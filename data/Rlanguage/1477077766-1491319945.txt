I posted a similar one for each of the debates over there. I was going to do the last one in ggplot, but just felt burnt out after watching the debate. Politics are exhausting, even if they are a good source of data. 
Definitely. There was a lot I wanted to do that didn't make it in. When I put it on my professional site I plan on cleaning it up a lot. Combine russian/russia/russians, pronouns, etc. This was more of a "welp, I am going to bed now" type of post. Also wanted to make it interactive to sort by percentage, number of words, and a slider to apply different filters (total frequency, and how much the other candidate had to say the word to be included).
Can't really answer most of your questions but as far as I know R does have visualization capabilities way beyond "basic." It's just a lot of work and questionable whether it's worth it.
It's similar to Python. It replaces Excel. It's useful for some tasks, burdensome and unnecessary for others.
Yeah that would be nifty. Would you use javascript and build it yourself or use some sort of interactive service/package like shiny?
Javascript, probably NVD3
Tableau user here as well, for me formatting data is much easier in Tableau. I really like how subsetting works in R, and creating dynamic columns is just easier. Plus, Tableau isn't reproducible. Great for working on the final product though. Also, with R you can dynamically create graphs that help you visually pinpoint what to really focus on. 
The issue is that you can't have dots twice. So you can only have one list of stuff in the dots argument to the function. Honestly if it was me, I would use the version with two vectors.
SQL - get data R - explore, visualize and model moderate sized data (approx 1M rows) Python - productionize data use If you're doing adhoc assessments you'll want to spend most of your time in R. It's faster than Python in terms of developer time due to its package libraries and visualization tools.
that's a good point. i just feel like a noob doing it this way, and was interested in learning about any best practices or something. thanks for your help!
The way lazyeval works is quite far from best practices, I think. It's very useful and helpful in some situations, but the way of thinking that it puts you into can really hurt you. I received a pull request from an analyst on my team last week that did this: for (var in c("list", "of", "vars") { q &lt;- paste0(var," == 1") dataframe %&gt;% filter_(q) %&gt;% do_something() } (note that it's `filter_()` not `filter()`) It's a great example of how the lazyeval syntax has warped this person's thinking so they can't think of a good solution and have to adapt their task to their way of thinking and not the other way around ;) The code review received a fail, btw :)
If you want to have your plot three-dimensional and animated, why not try [rgl](https://cran.r-project.org/web/packages/rgl/)? It will be hard, but it's definitely worth it.
*cough* the tidyverse *cough*
Are you referring to just one competition? If so, it makes sense that people who used the same successful techniques on that particular problem would all place well. 
I try to use neural networks in r for titanic but since i am not quite knowledgeable in the field yet, i was either too bad, like 30% accuracy or it just run for ever. So i need to comprehend neural networks well before implementing them. I guess thats what you should do too.
It will work but it will be slow. You can try it and see just how slow it will be. I know that one of my friends who's an AI researcher just upgraded his home setup from a 670 to a Titan Pascal and his computation times went from weeks to hours.
Okay, thank you!
Yeah I am referring to the RMS titanic, and Digit Recognizer one, both have people with perfect scores.
Not really, im still in uni so time is an issue. Along with uni courses, i am doing my dissertation, 30min-1hour per day learning python, some hours per week for a random project in r ( text analytics, machine learning etc) and reading on various topics for data science and machine learning, from coursera andrews ng course or something like that. So, not too much time for kaggle and knowing that i am not going to break in to first 100 is not very encouranging.
Not sure what you're getting at here?
The link is to a gif of a girl shaking her head, not a plot. In a (poor) attempt at humor, the sentence is phrased similar to Obi Wan when he first saw the death star.
Either it was updated or you were confused but it's definitely a picture of a plot now.
Another good resource here (specifically for genomics applications): http://varianceexplained.org/r/tidy-genomics-biobroom/
*the* Hadley? ~swoon~
1. Excel cannot open more than 1MM rows. 2. Excel takes FOREVER to perform a function on more than a moderate (1000) numer of cells. In the same time R can do 1000x as much. 3. Excel cannot handle any sort of non-basic statistical calculations. 4. Excel's mapping functionality sucks. 5. Excel's visualization capacities are different and in some ways limited. 6. Excel's ability to do joins/merges is highly limited. 7. Filtering and reshaping the data is fairly limited in excel If you're doing something really simple, Excel is a good tool, especially when you've got common shortcuts into muscle memory (font resize = alt + h + f + s, pivot table = alt + n + v + t) If you're doing much of anything else... no. For what it's worth I semi-automated 2 people's jobs using R. As in they went from a few hours a day of doing manipulations in Excel to running 3 scripts, getting coffee and coming back. My rule of thumb &lt; 1,000 rows - Excel &lt;1,000,000 rows - R &lt; 100,000,000 rows - Python. If you're somewhere inbetween you might be alright (e.g. 2 MM rows in R with only a handful of columns, I've got a few tasks that have grown to around this level)
I don't think you NEED a PhD... it's more about being of the caliber that you COULD get one. You do need the knowledge to be on par with someone who has an MS in stats/applied math and another MS in Computer Science.
I would imagine you're referring more to Data Science positions than some low level analyst? Because to be honest, what I'm most interested in is the sort of rote, mechanical scripting and automating aspects of this kind of work, as opposed to the higher level statistical inference and complex modeling that data scientists are involved in.
upvoted because this is the best answer. The link given works MUCH better and is more direct than the steps I listed. This line is key, it gives the bounds of the CI, from there you make your boolean. predict(model.lm, data, interval="confidence") 
http://stackoverflow.com/questions/1169456/in-r-what-is-the-difference-between-the-and-notations-for-accessing-the
Specifically, what is your difficulty? library("RODBC") to load the RODBC library. If you get an error there, you need to install it. Query&lt;-function(sql) { Channel&lt;-odbcDriverConnect(connection="DRIVER={Oracle in 11g_x64};UID=YOURID;PWD=YOURPASS;DBQ=//DBNAME:9999/HOSTNAME;",believeNRows=FALSE) df&lt;-sqlQuery(Channel,sql) close(Channel) df } This is the function I wrote to pass queries to ODBC. I stole the driver name from the ODBC configuration in Windows.
I think it's one of two things, you have either stored the pca model not the data in your pca.train object (or it contains multiple items, call just 'pca.train' to check), or you haven't applied the pca model you constructed on your training data to your test data set. I'm thinking it's the latter and you reversed test and train toward the end of your question. 
in R functions generally return the result and are not called for their side effects (with the exception of printing text and plotting). In that regard apply is like any other function: sum_vec &lt;- sum(vec) column_sums &lt;- apply(mat, 2, sum) (note there is colSums for the second function, I just thought it was a simple example) but I think you just want: x[,1] no need for the apply
For sure, x[,1] is clearly the way forward. I assumed OP was just messing around trying to understand apply(). The behavior with print() was odd enough I felt the need to check it out.
wizardry! no really how do you do this? 
Good catch! However that doesn't seem to fix it... I get the same error after changing future_days -&gt; days: Error: ggplot2 doesn't know how to deal with data of class uneval
You are using two different dataframes, so don't pass either of them to the initial ggplot() function, instead pass them to the separate geoms otherwise you get crazy errors. ggplot() + geom_point(data = linkedin_views, aes(x = days, y = views)) + geom_point(data = linkedin_extrapolation, col="green", aes(x = days, y = linkedin_pred))
First of all, reddit markdown messes up your code, I suggest to use the supported code feature and not do this lazy copy and paste, if you want us to help you (so we can easily copy and paste and not spend a day tracking what got messed up). Second, in `qexp()` function, the rate parameter has to be positive, you change your `lambda &lt;- 0.69`
I think it'll switch if you use fill instead of col.
Hahahaha, well I feel foolish. That did the trick. Thanks for your help! 
ok sorry, I thought you were OP, because I missed the *'d* in *I'd've* thinking that OP figured out he doesn't need the print...
Are the graphs regenerated?
They are reset every time the app gets closed. So they are regenerated in the sense that the graph gets created, but any saved locations would be lost.
That sounds interesting, I am going to look into a method of saving the graphs. I could actually just save the locations the user has selected, and use them as inputs to generate the graphs on the fly. One issue with data storage on shinyapps.io (which is what I am using) is found here: http://shiny.rstudio.com/articles/share-data.html
http://www.inertia7.com
Map the easiest route up Mount Everest?
It's referencing a slot: https://stat.ethz.ch/R-manual/R-devel/library/methods/html/slot.html
R packages can't be closed source. Here is the source of mgcv from CRAN: https://cran.r-project.org/src/contrib/mgcv_1.8-15.tar.gz
This error means, unsurprisingly, that your data is a class ggplot doesn't know what to do with. Despite looking like it in your example, your data isn't numerical, it's unevals. You need to plot numerical data.
have you tried running Rstudio as admin yet? (booting via right click)
I managed to get past that hurdle by changing permissions in the R folder, thanks. The packages still download to a temp file, but it's working anyway!
Probably not going to be popular on /r/Rlanguage, but R is not very good at image manipulation. If you want a free open source alternative I'd recommend python and scikit-image. If you're set on R, then have a look at the EBImage package on Bioconductor.
what the fuck, why, no thanks 
I literally just spent yesterday writing and putting together my own. Based on crosstables though, mostly so my client that needs quick numbers can look them up without distracting me. 
I plan on creating a bunch of web based tools to interact with data including creating the ggplot code. This is the first package I am releasing under the sift::r name. [Here is the larger tool](https://twitter.com/appupio/status/779825046353121280) I am working on, but it is farther out than I want it to be. 
Thank you! So I picked the wrong tool for the job.
If they're not too afraid of the terminal you could write an install script that installs R and the dependencies. Then make your R code into an executable script that takes arguments, so excel file as input, and an output location. A simple example would be `calc_cols.R`: #!/usr/bin/env Rscript # get command line arguments args &lt;- commandArgs(TRUE) input_file &lt;- args[1] output_location &lt;- args[2] # do stuff with input_file and write to output_location ... The users could then run this by `./calc_cols.R path/to/input.xls path/to/save/`. You could get a bit fancier using the optparse package or calling the `file.choose()` function. 
I'm a beginner too, without the benefit of knowing a damn thing about Excel, welcome! As mentioned, RStudio provides a sleek GUI to run R. In it, which you seem to be alluding to, is the "console" which I think of as a sort of live temporary way to either run bits of code or the place where script is printed. By default this will take up the left-hand part of the screen. In RStudio, begin by starting a new project, setting a working directory, and starting a new script. Once you've started a new script a new window will appear above the console. This is all obvious from the menu bar. It's in this script that you'll write code a la DataCamp. You can jump through the bits of code by pressing ctrl + enter, or run it en masse -just like DC. Good luck!
Try [this](https://telegram.me/joinchat/DP9KNQb7wfBptsdYnTh0Yw) Telegram chat. It is geared towards newcomers. 
The way you've defined the problem, it's logically unsolveable. You're asking the question "How can I get someone who isn't interested in doing something to do something?". You can't do that. You either have to get them interested, or change the nature of the problem to something they will be interested in. I would suggest you can do one of two things: the first is get them interested. This is a fairly simple marketing exercise. If the script could save them time and effort and give them a transferable skill, this should be easy. The second is to reframe the problem. If we were colleagues and you'd asked for feedback on this, I would suggest that your script is solving the wrong use case. Converting an excel file to another excel file is a fairly paltry endeavour. You don't really need R for that. You can aim higher. (Or alternatively, aim lower and use VBA or something) For example, why is the input data in excel format? Where does it come from, and can the source be persuaded to use an easier format for use in R? Perhaps they could provide an API to build the data on demand? Could they at least be persuaded to drop it on an FTP? Then you could set up a service to grab the data from the API or FTP, do the transformations, and then do something else useful with it. This would be far better than getting end-users to run R scripts.
[These instructions for making an R executable](https://www.r-bloggers.com/making-r-files-executable-under-windows/) seem relevant
Just heading out the house, so can only give partial response for now... &gt;Or alternatively, aim lower and use VBA or something Everyone at my work is still in the Excel paradigm so this is where I started. My actual task is to implement something roughly [like this](http://www.qihub.scot.nhs.uk/knowledge-centre/quality-improvement-tools/run-chart.aspx). VBA turned out to be a ill-suited tool for this task IMO. Because the basic building blocks of VBA are cell-references rather than more abstract constructs like vectors. So the VBA became verbose; obscuring the logic and inducing headaches. So that brought me to R and figuring out how to share scripts in a non-scary way.
This is a good example of what I meant by "aim higher", then. Why not just write the R code to produce the graph itself? It would be pretty simple. Then you can grab the input files off the FTP or whatever and produce the graphs in batch. &gt; how to share scripts in a non-scary way Also this is what I was talking about when I said "logically unsolveable". If we define that users are scared of the command line and using scripts (which you defined in the OP) then there is no solution that can satisfy both criteria - "sharing scripts" and "non-scary". It's axiomatic. So we would have to remove one of the assumptions - either stop "sharing scripts" and do something better, or reduce their fear through marketing.
I'm aiming for a general purpose tool, so data will be coming from different places. And the calculated fields will be used in different ways by different people (I.e. excel chart, JavaScript etc) So I'm just focusing on the essential problem for now. Thanks for your thoughts though (downvote wasn't me)
The R console is basically a terminal or command prompt for an R interpreter. Like the command prompt, when you type things, they will run. What you need if you want to develop scripts is an IDE, and RStudio is very popular as you've seen. There are alternatives such as Microsoft Visual Studio, which now includes R, and Eclipse/StatET, but these are less popular. If you have a favourite text editor you can also just download a plugin to add R support, most have it.
I'm aware of what you are saying and I myself use the strategy of creating an empty data frame, looping and rbinding, but maybe you could provide us an example? Also, you should post on r/rstats , it's much more active than this one.
It's typically easier to estimate how *much* space you'll need in the first place and then set values appropriately. As in, it's faster to say "give me 10000 rows of space" than "here's 1 row, here's 1 row, here's 1 row, ..." 10000 times. Could you show us an example of how you're using rbind? This isn't a case of "there's a better function to use" as much as "there's a better method to doing what you're doing."
Hello companion! I feel your pain RE Excel loving colleagues. My coworkers are largely analysts - but spreadsheets are still the default. Simply because they're ubiquitous and easier/less intimidating in the short term. Obviously, as complexity grows, they tend to become the more difficult option. Thanks for sharing your thinking process. It's helped crystallise the various options in my head. I'll probably go with your's and Hadley's recommendation of shiny. The one drawback is security (I work in health) so may need to fall back on source() for some coworkers.
rbinding to an empty table is an absolutely terrible pattern. Most times you're better off either preallocating your result table and then assigning directly into it (which I only bring up because it's more intuitive for people that take the rbind approach in the first place) but the better solution most likely involves lapply but without more details its hard to say. But if you think you're using rbind too much and you aren't using lapply then most likely the answer is "yes you're using rbind too much".
 z &lt;- sapply(1:100, function (i) x[(x$a==i)&amp;(x$b==y[i]),]) is the first step (I assume you meant y[i] not just y, or it doesn't do what you described). In general look at the apply family of functions. Ideally I would save x in a way that x$a isn't needed, but it slightly depends on what exactly you need in the results, i.e.: x &lt;- split(sample(1:100, 500), rep(1:100, each=5)) y &lt;- sample(1:100, 100) z &lt;- sapply(1:100, function (i) y[i] %in% x[[i]]) or (I haven't tested these but I think they work) z &lt;- mapply('%in%', y, x) would give you which y's match an x you could also get lists of the x's which match the y's and then subsequently use them to get the exact values, etc. it all depends on what you then want to do EDIT I just realized for your case there is an even nicer solution: z &lt;- x[x$b == y[x$a],] and that was it :-)
http://www.r-graph-gallery.com/portfolio/ggplot2-package/
Few if any of these fit the label “impressive”. They show off stock ggplot2 functionality, no more. For an impressive R chart, a better example would be the [beautiful story about NYC weather](http://blog.revolutionanalytics.com/2015/01/a-beautiful-story-about-nyc-weather.html) because it shows off ggplot2’s customisability.
Yes I solved it like that. 1. Applied PCA on train. 2. Multiplied PCA.train$rotation with train 3. Multiplied PCA.train$rotation with test 4. Ran knn on train and test
Chips , I see it , I use the wrong text 
Thank you very much for the link! And yes, would be very kind of you, if you can share your scripts too. Once again, thanks! 
Thanks, that solved my problem
Another problem. I want to calcualate the mean of a given pollutant. So I calculate it this way : print(mean(file[,c(pollutant)]),na.rm= TRUE) But still I see as output NA. I thought na.rm would strip all Na before it calculates the mean. Why still a Na as answer Roelof
is file a dataframe and pollutant is the name of the column? If so you have to put pollutant in quotes, like this: mean(file[, "pollutant"], na.rm = TRUE) or like this: mean(file$pollutant, na.rm = TRUE) you only need to use c() if you're selecting more than one column.
Are you just trying to show the relative density of the center? You can set the heatmap scale manually to over a greater area. Or you could try a hexbin? https://www.r-bloggers.com/5-ways-to-do-2d-histograms-in-r/ 
Sure thing! This should work, for the most part. You may need to play around with it to make it work perfectly with your data. Also, just change the Directory value to be your "C:/Documents/Your/CSVs/Here" Path &lt;- Directory The_TXTs &lt;- dir(Path,pattern = paste('\\.txt$',sep = ''),full.names = T) Short_Txts &lt;&lt;- dir(Path,pattern = paste('\\.txt$',sep = ''),full.names = F) x &lt;- 1 while(x &lt;= (length(The_TXTs))){ Dat &lt;- read.csv(The_TXTs[x], stringsAsFactors = FALSE, header = F) assign(paste('My_Data_',x, sep = ''), Dat) x &lt;- x+1 }
It's just so clustered the normal tools don't like it. Trying to figure out a way to scale the bins to make it look nicer than what you are getting, but not having a ton of luck. 
I think you need to increase the "nbin" argument to densCols() for the last figure you linked to on SO. BUt I'll warn you it doesn't look how the linked figure looks. Your data is much more discrete, therefore the colors appear to change in discrete steps as opposed to the gradual way in the link. EDIT: I tinkered with it and heres how I got it to look: [link](http://imgur.com/A9NYqPn) Here is the code: DT.sub = fread("~/Downloads/5qNdCHBS.txt") names(DT.sub) = c("x","y") colors.dmp &lt;- densCols(x = DT.sub[,x], y = DT.sub[,y], colramp = colorRampPalette(c("#000099", "#00FEFF", "#45FE4F", "#FCFF00", "#FF9400", "#FF3100")), nbin=128, bandwidth = 4) ggplot()+geom_point(data=DT.sub, aes(x = x, y = y, colour = colors.dmp), size = 2) + scale_color_identity()
instead of creating a data frame for each row, add each part of the row to an array via `ids &lt;- c(ids,idx)` and `nobs&lt;-c(nobs, sum(complete.cases(file)))` and then create the data frame at the very end via df = data.frame(ids, nobs)
For more html widgets see this persons blog: http://www.buildingwidgets.com/blog/ I love the sunburst widget. I use it for an interactive customer lifecycle analysis 
Sorry, I haven't really read your code, just scanned it for anything obviously wrong. complete &lt;- function(directory, id=1:332){ ... # your code up until the for loop here nobs &lt;- c() for(item in subset) { file &lt;- read.csv(item) ids &lt;- c(ids,idx) nobs &lt;- c(nobs,sum(complete.cases(file))) idx = idx + 1 # df = data.frame(ids, nobs) # you could also do the data frame here, but you would just recreate it everytime # so it's better to do that after the loop. } df = data.frame(ids, nobs) } This code should solve your problem. You didn't use nobs as an array but a single variable instead.
Yea that should work fine. You'd have to make input widgets that indicates what gets filtered in the variables and in which ones. So every time you add a new filter, the dataset used to plot the stations on the map will be subset to finally plot the exact one you want.
I don't have an account there, but with curl you want to first send to _username:&lt;user&gt; _password:&lt;pass&gt; to https://data.niwa.co.nz/data/authentication and grab the cookie. Then use the cookie when browsing the rest of the site/downloading. [Info on R + curl + authentication](http://stackoverflow.com/questions/2388974/how-do-i-use-cookies-with-rcurl) library(RCurl) #Set your browsing links loginurl = "https://data.niwa.co.nz/data/authentication" dataurl = "http://api.my.url/data" #Set user account data and agent pars &lt;- list("_username"="xxx","_password"="yyy") agent="Mozilla/5.0" #or whatever #Set RCurl pars curl = getCurlHandle() curlSetOpt(cookiejar="cookies.txt", useragent = agent, followlocation = TRUE, curl=curl) #Also if you do not need to read the cookies. #curlSetOpt( cookiejar="", useragent = agent, followlocation = TRUE, curl=curl) #Post login form html=postForm(loginurl, .params = pars, curl=curl) #Go wherever you want html=getURL(dataurl, curl=curl) #Start parsing your page matchref=gregexpr("... my regexp ...", html) #... .... ... #Clean up. This will also print the cookie file rm(curl) gc()
The problem is the nobs as second argument to the data.frame df Anyone a idea how to solve this ? When I look ids is numeric where nobs is a list. 
Try `df = data.frame(id = ids, nobs = nobs)`
This is quite tricky because of the myriad ways IT can screw it up. If your problem are the r packages, I would just create a script that can run itself and make a local repo and make necessary changes to rprofile. More fancy solutions involve packrat but it depends on whether you need it or not.
For some reason I didn't get a notification when you commented, but thanks so much! Been pulling my hair out trying to adjust contour plot lines
for the first, save it in a flat text file, i.e. csv or tsv file you can easily append to it and you can load it into excel etc. if your on linux create a cron job that calls your skript every day, on other systems I don't know, but there are probably similar tools/programs
Try using an AWS EC2 machine and putting R server on it. You can run it from a cronjob to output to S3.
Also try this: https://github.com/bnosac/taskscheduleR
maybe, but it is applicable to R, so hopefully I will get good answers.
&gt; When I put in https://data.niwa.co.nz/data/authentication to googe chrome i get a http 403 error. The visit was declined. The data service log in is located on the home page in a hidden panel that shares the same URL as the home page: Probably checking if you are accessing the page via POST or GET. &gt; the following line keeps returning error messages:"Error: unexpected input in " _" " I'm assuming this is because _username and _password required by NIWA? pars=list( _username="xxx" _password="yyy" ) I think that was just a formatting issue. Moving everything to one line and putting quotes around the vars seems to work pars &lt;- list("_username"="xxx","_password"="yyy") &gt;I'm assuming i should replace http://api.my.url/data with the actual data page i want to download? Yep!
So I can do summary(...., digits = 4) in my code and then it works when I call summary. and yes, this is the coursera course 
You realize that you must give summary() something to summarize, right? In the example you cite it would be cr. summary(cr, digits = 4)
Found another way to solve it. I put options = 4 as first thing in my function which is then override by digits = 5 on the line where I want 5 digits
Why bother? set.seed(50) df &lt;- data.frame( n = 1:10, row = rnorm(10) ) df[df$row == max(df$row),] Results in: n row 9 9 0.9755906
And there's df[which.max(df$row), ]
?which.max will get you what you want
i remember an econ professor once saying that that the best forecast of future oil prices is today's price; they move too randomly to forecast. I have no idea if that's true, but I would be cautious about relying heavily on any forecast. I did a quick google search, and [this article](https://www.federalreserve.gov/pubs/ifdp/2011/1022/ifdp1022.pdf) might be helpful to you. Finally, I have no idea if they're any good, but [this site](https://knoema.com/infographics/yxptpab/crude-oil-price-forecast-long-term-2016-to-2025-data-and-charts) seems to have data on World Bank, IMF, and other oil price forecasts.
One helpful things to realize about OPEC, is that they are a cartel. Cartels are a collusive oligopoly, and as such their prices are set in a Pareto superior set relative to the Nash Equilibrium for their market. 
R packages do not create namespaces in the way as python for example does, once you load a library all it's exported functions are copied into your main namespace! (not exported functions are of course separated) environments are the main way R handles it, people have also used package based suffixes (i.e. str_length, str_upperCase) and somebody is developing a package which lets you make Python-esque namespaces for your packages and possible script files. The last one sounded like the best solution, but I haven't come into the need yet myself, so I haven't researched it further.
If you're looking for a solution that does most of what you want without using additional packages you should look up aggregate. Try typing &gt; ?aggregate into your console for more information. Here's how I think you would use it in this case. &gt; #Generate some data &gt; Stocks &lt;- data.frame("Date"=rep(1:5,5), "Ticker"=rep(letters[1:5],each = 5), "Market Cap"=round(runif(25,2,3),1)) &gt; # Use aggregate &gt; aggregate(Market.Cap~Date, Stocks,sum) &gt; Date Market.Cap 1 13.2 2 12.6 3 11.5 4 12.5 5 12.8 If you then want to add that column to the side of your other table you could look into dplyr's left_join function.
Yes, with rollapply. 
Do you have any experience with statistics/probability theory? 
No correct values are 0 for df1 if that changes the need for looping :-p
Just merge them and then use ifelse(B==0, B=newcol, B=B) or something along those lines.... I'm thinking of it in data.table syntax, so you may need to adjust for your situation.
Well, first off, why would you simulate it? Monte Carlo simulation is useful when you are concerned about the path, not just the outcome. Whatever number of simulations you run, you're going to get whatever probability you set. 
Here's a solution that uses the tidyr and dplyr packages. library(tidyr) library(dplyr) df1_long &lt;- gather("variable", "orig_value", -ID) df2_long &lt;- gather("variable", "update_value", -ID) joined_df &lt;- left_join(df1_long, df2_long, by =c("ID"="ID", "variable"="variable") updated_df &lt;- joined_df %&gt;% mutate(final_value = ifelse(is.na(update_value), orig_value, update_value)) %&gt;% select(ID, variable, final_value) %&gt;% spread(variable, final_value) 
Gather is dope, thanks!
Quantmod with Yahoo API is what I use.
thanks, what do you write in the script to get the information:, Insert (Quantmod) or something else?
It will print to the console when you first run it, but will produce the function, "f", in your local environment - you'll be able to see this in the upper-right window of RStudio, if you're using the default layout. You can then call "f" as a normal function. 
Thank you, thank you, thank you, thank you, thank you, thank you, thank you, thank you, thank you, thank you, thank you, thank you, thank you, thank you, thank you, thank you, thank you, thank you. (Was the f(A,B)) i know its a bad way to do matrix multiplication, but is a different way to the same answer.
@omichandralekha I tried using the task scheduler (on windows 10) but I dont see the addin option. I also tried running the script as a batch file (so that I could potentially use the system scheduler to run it daily) but it doesnt seem to work as if I manually ran it.
[Have you looked into leaflet?](http://rmaps.github.io/blog/posts/leaflet-heat-maps/index.html)
Ok now I get the connection with linear programming. ehh, most constraints seem quite easy to implement. The only hard one is 1:9 only occurring once each and the only way I see that being implementable is if you use a vector of 81 to encode the nine numbers (which you seem to also propose) and after also checking that each slot of 9 for one position also only has one number, you can check the other things quite simply, so I'm unsure what your question is EDIT I first ignored the linear programming part and wrote this: I'm unsure what exactly you mean, if you call the matrix m (3x3) and the four circle numbers a (2x2), you can test the first constraint with: sum(m[1:2,1:2]) == a[1,1] by adding i/j to the indexes and looping them over 0:1 you can check all four constraints. If you want to test several matrixes at once, you can create a three dimensional array and use vector operations (you'll have to replace the sum function with writing out the additions) for the unique 1:9 you can look at %in% or ?intersect for some ways you could test it in a single line of code. 
Why is local not an option? If you desperately need server access across the net, I believe Rstudio has a server edition.
Have you looked into AzureML? That allows for R programming and no setup.
If you don't need a lot of horse power any Linux VPS hosting will do. Install RStudio Server, and R. You can SSH to it. Or open the ports and connect like a normal website.
I have this csv file, 4,000,000 x 42, tried reading with read.csv, took 3-4 hours and still reading, tried fread, done in 53 seconds, but that converts to data.table format, no clue how subsetting works in that, tried View("the data.table file"), just the column names showed up, even though manually calling out table elements - the data is present. So out of 4 million rows 600,000 have some values rest are NA, they are all scattered throughout, found each values index, now, using a for loop copying the information out from the data.table for the past 1/2 an hour. The [data](https://data.cityofnewyork.us/Environment/2015-Street-Tree-Census-Tree-Data/pi5s-9p35) I only care about lat &amp; long, the_geom and status, and not about the other 38 variables.
You can look into getting an EC2 AWS Machine and loading an RStudio R server on it. Something like this is good and easy to setup: http://www.louisaslett.com/RStudio_AMI/
It sounds like you're doing this incredibly inefficiently. Firstly, you want this data in a data.table (a tbl_df might be even better depending on what you're trying to do). read.csv is incredibly slow and shitty. I imported it using readr::read_csv() in around 40 seconds. Then you can drop the columns you don't need. This already drops the memory consumption from around 2.6GB to 600mb and this operation took too little time to measure with dplyr::select(). Finally, as you have seen, you seriously don't want to loop through this data. That's not how R is intended to work anyway. You need to be exporting your computation to C. What are you trying to compute? Why are you "copying out" information? Just conceptually that doesn't make much sense. You should learn how to subset data frames (or ask for help on it) rather than wait hours for your for your loop to process.
Oh I see! I'm sorry, I misread. Readr returns a tbl_df which inherits data.frame, so yes. 
When I think of density maps I think of using spatial interpolation methods like kernal density estimation, kriging, inverse distance weighting (IDW). These are great if you want to generate a predictive *continuous surface.* The example by /u/conogol has a section on "heatmaps", which seems like a good starting point boiled down to the basics. If you are looking for more theory and R scripting here are a few additional resources: * [Tutorial 1](https://mgimond.github.io/Spatial/interpolation-in-r.html) * [Tutorial 2](http://geostat-course.org/system/files/lewis_tutortPM.pdf) is a bit more tricky to follow and no images, but uses an inbuilt example data set in R * An [extensive powerpoint](http://geostat-course.org/system/files/geostat13_spatinterPM.pdf) with 100 slides of introductory background theory on different interpolation methods in geostatistics Alternatively you could consider creating a choropleth map, where you *bin your data into spatial units* (e.g. census tracts or census blocks, or create a uniform grid with same sized grid cells). Essentially you would end up with a count (or could get density) and color code it. (e.g. [NYC energy choropleth](http://ux.ubimix.com/choropleth/building-energy-consumption.jpg)). Your spatial units would depend on what would be most useful and what you are trying to compare within your tree data. I know this is an R subreddit, but quick plug for r/QGIS/ where you can do a lot of interpolation easily in a point and click environment with free software. That might be useful if you want just to play around much more quickly for exploratory data analysis, different color schemes etc. Edit: I was going to make quick examples in QGIS, but NYC data server doesn't appear to be connecting.
Took a quick break to follow-up. Here are some examples of kernel density estimation for this data using QGIS http://imgur.com/a/Ueh8z
Thank you for the help! 
Yeah this is more like it, I'll build something like this.
combn takes a vector. Are you doing acov &lt;- combn(a$column_name,2) ?
Ok, I see that is where problem occurred. I suppose, then, that combn() can not help me. What I want to do, is to use function (i.e. variation, and correlation test on pairs of variables - vectors). Do you know how to do this. Thanks :)
There could be a need for companies that can't do that, though - that could be interesting.
Primarily learnt statistics and data management in SAS. Got a new job, was verbally promised a SAS license, but never materialized due to budgetary constraints. Pretty much was forced to use R for analyses and automating support work for co-workers. Still would classify myself as relative beginner, maybe low-intermediate. Need to still gain better mastery over data management tools, but R has certainly grown on me even if run into frequent memory constraints. 
This course was designed for MPH epidemiology students, but it will certainly have relevant material, at least for a basic introduction. http://jennakrall.com/IntrotoRepi/
Not really sure what you mean by the POINT format (did a quick google search to no avail). Do you mean your lat/long are stored in a single column seperated by a space? In that case I don't know what the disadvantage to storing it in two seperate columns for lat/long would be. And "sp" is a package that does more than than just polygons, its the basis for handling a lot of spatial data in R
Government. I use R mostly for research. I work with mostly unstructured text data, but frequently combine with other data sources. Great for getting different data sources working together. For production mostly we use python.
Unfortunately this function create every combination between each number in vector. I only want to pair vectors. For example if I have vectors v1 v2 and v3, I want to pair this vectors (v1-v2, v1-v3, v2-v3) to conduct cor/cov test on it.
From what I understand, form data is very rarely sent in GET requests, which would explain why httr doesn't offer it as an option. It might be worth considering using query parameters instead of form data if you want to use GET requests. Having said that, it's doable (if a little painful) using the `curl` library: library(curl) url &lt;- "http://requestb.in/1hwyr2u1" handle &lt;- new_handle() handle_setform(handle, filename="example.txt") handle_setheaders(handle, Accept="application/json") request &lt;- curl(url, handle = handle) response &lt;- readLines(request) 
I have set of 5 vectors each of 284 elements. I want to pair this vectors, but with keeping order of this elements (to conduct covariation test on it) Code you proposed gives vectors of 80656 elements. So I supposed it creates not only pairs of vectors, but also pairs of each element in this vectors.
Thank you so much, this approach is working for me very well (but I use only first part of it) :). Problem solved. Once again Thanks!
per one of the comments in the stackoverflow question you made, your example doesn't look like a dataframe given the output it displays.. however given that your data is actually in a dataframe, then the paste function is likely the one you're looking for. library(dplyr) df&lt;- data.frame(a=c(rep(-1223, 5), -1224), b=c(476, 477, 475, 476, 477, 476)) (mutate(df, c=paste(a, b, sep=" ")))
hmm. yes that would create characters, and as.numeric would cause them to just translate back into NA. I feel like your solution shouldn't require you mutate a new column with both lat/long into a single data point while also remaining a numeric? I'm not too familiar with revgeocode so I'm not sure what you need exactly.. if you needed to input 2 pieces of data as one, have you tried something like as.numeric(df[i, 2:3]) ?
Same, trying out Material right now though.
Pastel on Dark. I wish it had something more similar to the Darcula theme in PyCharm though. 
big fan of this one so far. very aesthetically pleasing too
I wish we could theme the actual client! 
So, as a general point, when you want to ask for programming help, you need to produce a minimal reproducible example. This means that you have to provide the smallest piece code of code you could come up with that, when I run it, generates the same problem you're having. This includes providing your data, or a toy dataset. Bonus points for including what you expect the output to be. Now, you're lucky, because in this case the error message is completely self-explanatory: &gt;Error in bmerge(i, x, leftcols, rightcols, io, xo, roll, rollends, nomatch, : **typeof x.hs_gen (double) != typeof i.hs_gen (character)** So in x, hs_gen is a double, and in i, it's a character. Fix that. Also, you may want to look into the join functions in dplyr, they're very convenient.
&gt; You cannot do operations on a NULL or NA value. Maths simply doesn't work that way (what is x plus NULL anyway?) NA is supposed to be an exception to this, since it indicates "not sure". (1 + NA) is NA. (NA | TRUE) is TRUE. (NA &amp; TRUE) is NA. So as long as you limit yourself to logical operations or don't mind having more NAs in your output, NA is fine. NULL is another story.
It will depend on the function; there's no reason it couldn't remove NA rows or do imputation itself. Some packages provide that function using the model results to calculate the imputation, for example.
&gt; cesnsus geocoder Nope I did not. Thanks for the strsplit()! 
I really do believe that if your functions does both: has a side effect and returns something - it is doing something wrong. For pure side effect functions I tend to use `invisible(Map(...))` - it's not elegant, but keeps the R output clean. Regarding the side effect of changing the list itself - again my opinion is that it's not a correct way to use functional programming. In that case I would try to do something like l &lt;- Map(do_something_to_l, l) You can try looking at the `purrr` package - I think it has some functions for functional programming that directly deal with what kind of output you expect. In particular look at "Transformations" section of this page: https://github.com/hadley/purrr . But I don't use them, so can't help here.
 Few things are wrong. Use stringsAsFactors = FALSE i would advise to put your data in a reactive output by it selves so you can use it in different outputs. the icon doesnt exist anymore (see http://leafletjs.com/docs/images/leaf-green.png) and it isnt assigned in your code. "schoolqual" not schoolqual in UI as schoolqual isnt an object in your UI. have a read http://shiny.rstudio.com/articles/reactivity-overview.html sc &lt;- read.csv("BLschools.csv", header = TRUE, sep=",", stringsAsFactors = FALSE) library(shiny) library(leaflet) greenLeafIcon &lt;- makeIcon( iconUrl = "http://leafletjs.com/docs/images/leaf-green.png", iconWidth = 38, iconHeight = 95, iconAnchorX = 22, iconAnchorY = 94, shadowUrl = "http://leafletjs.com/docs/images/leaf-shadow.png", shadowWidth = 50, shadowHeight = 64, shadowAnchorX = 4, shadowAnchorY = 62 ) shinyServer(function(input, output){ output$map &lt;- renderLeaflet({ schqual &lt;- input$schoolqual school &lt;- subset(sc, sc$Rateoutof4 == schqual) %&gt;% leaflet(data = school) %&gt;% setView(lng = -73.98928, lat = 40.75042, zoom = 10) %&gt;% addProviderTiles("CartoDB.Positron", options = providerTileOptions(noWrap = TRUE)) %&gt;% addMarkers(clusterOptions = markerClusterOptions(), icon = greenLeafIcon, popup= ~paste("&lt;b&gt;", school$SchoolName,"&lt;/b&gt;&lt;br/&gt;", "Rating: ", school$Rateoutof4,"&lt;p&gt;&lt;/p&gt;")) }) }) shinyUI( fluidPage( titlePanel("NYC schools"), sidebarLayout( sidebarPanel( selectInput("schoolqual", choices = c("A", "B","C", "D", "E"), selected = "A", label="ws")), mainPanel(leafletOutput("map")) ) ) ) 
I'm, I think, at the point where I can do all the basic stuff, but can't figure out how to go on. I'm taking the swirl lessons just to make sure I'm not missing any important knowledge while catching up with my control theory class (which I also hope to make some money out of). I can do any statistical analysis, given the data, but that's barely programming, and I'm thinking "Why not use some more suitable language for anything else?" I saw that some guy made a game on R, but why? Why not just make the same thing on c++? How would one even go about, when making a game? Also, I can't express my appreciation in words. Thank you for the time.
Definitely use whatever language is most suited for the job. I use R for processing data, and like with the package I use it to serve processed data too. I really think as far as programming goes, Python or NodeJS are probably more worth knowing, but R can do so much with so little code which is WHY it is so great. [Here is an R script I wrote that formats some data](https://gist.github.com/MattSandy/054993a335674815b163ff32924288d3). With R usually what you are programming is something that formats data (unless you get into Shiny for R), so if you get good data running an analysis is a lot easier. [Here is something more in depth](https://www.r-bloggers.com/sentiment-analysis-on-donald-trump-using-r-and-tableau/) that is even more fun. I think what you are stuck on is what to make. [This might be useful too](https://www.r-bloggers.com/in-depth-introduction-to-machine-learning-in-15-hours-of-expert-videos/). 
For what its worth, I get many contracts as a freelancer from https://www.freelancer.com/search/projects/Statistics/?q=statistics I also program in C++ / Matlab / SAS and Spss . Then you could really build up your curriculum vitae . I just get contracts to keep my skills up so don't get to disappointed with the pay, its for experience. 
Before I forget , if your that good you should go through some of the Coursera classes on R. They really give you a good foundation to see what is basicallly the latest and greatest libraries to make your work easier
Ok, I can definitely follow your code, so that's good. It would have taken me quite a while to write a code that achieves those results, though (most of that time would be spent trying out random solutions until I got a reply on StackOverflow, probably). I'm positive I can do statistical and even regression analysis (which was what my R class was focused on) consistently, albeit slowly. It feels like I've reached the extent of R's functionality, and should move on to another language. The more I think about it, the more I'm liking Python.
I don't know if I'm good, though. Coursera looks like an exceptional tool, even though I can't pay for it. Also, searching for "statistics" came up empty... I saw some stuff I think I could do in Upwork, though. Thanks for the time!
[This](https://www.youtube.com/watch?v=FI_BUJPtCG8) tutorial using the dplyr package with baseball data might help you out. 
Would you be able to put this data on pastebin and PM me the link? It would make it a lot easier if I could see it.
There's a class on sabermetrics on EDX. It uses R. https://www.edx.org/course/sabermetrics-101-introduction-baseball-bux-sabr101x-0
I'm in a similar boat as you. I'm a biologist by trade, but want to delve deeper into statistical analysis with R programming to add a new skill to my career. I'm also a huge baseball fan, especially love it for the stats. A friend of mine gave me [this book](https://www.amazon.com/Analyzing-Baseball-Data-Chapman-Hall/dp/1466570229/ref=asap_bc?ie=UTF8) for a birthday gift and I've been working way my through it, albeit very slowly. So far (I'm only at Chapter 3), it's been easy to follow and a nice to guide through R. I'd suggest it. The edx course, that /u/sin7 suggested sounds interesting as well.
Thank you so much!
R’s default LOESS implementation *is* a robust LOESS.
Exactly what I wanted! Thank you very much! 
If I am understanding your data structure properly, you are talking about converting your data from long to wide format. It would be helpful if you posted some example data, but these two links may be of help in organizing your data: http://www.cookbook-r.com/Manipulating_data/Converting_data_between_wide_and_long_format/ https://www.r-statistics.com/2012/01/aggregation-and-restructuring-data-from-r-in-action/ 
I would look at the tidyr package. You will probably need to use the `tidyr::gather()` function to convert from wide to long. The gather function turns column headers into values.
Hi, please take a look at my response above. Would you still suggest the gather function?
Well in the sample data you provided you don't have the same level of information, so you need to decide how you are going to aggregate your data. Example Scenario #1 would be you assign the $ by Dept. from Table 2 to every row in Table 1 that has that same Dept. So, employee 1 would have a new column "Money" with a value of $100. It would also be $100 for employee 2 and $300 for employee 3 and 5. Or if it makes sense you could write a function to divide by number of employee per department. So the $100 is split equally by employee. Example Scenario #2: Another way to handle it would be to to aggregate your employee data to Department level. So you would sum employee 1 and 2 by day so there is only one entry per department per day (e.g. 150 total calls and 10 units training time, and new column of $100). At the end of the day you have different types of data, so before you worry about re-arranging the structure of your data you need to decide on what are the independent variables in your regression and what is the most appropriate way to categorize the data. This includes deciding what the most appropriate level of aggregation is for your variables. If you will be aggregating data, then you should consider if that aggregation is appropriate and theoretically defensible for your regression. Now all that aside, to answer solely the question of re-arranging your data. Personally, I think the easiest way would be to convert Table 2 from "wide" format (each dept is a column) to the "long format" in Table 1 (dept. # is now a value for a column). One way to do that is with the melt function from the reshape package, followed by merge function in base R to combine two tables by unique combination of date + department. Here is some example code. Others here probably have better and more efficiency strategies for data wrangling, but I will give it a go. If applied to scenario #1 you will get same $ value for every employee in the same department per day. ## import data *table2 &lt;- read.csv("table2.csv", header=TRUE)* *table1 &lt;- read.csv("table1.csv", header=TRUE)* ## Convert data frame from wide to long format, so each department a value in a column. Newly generated columned called "variable" unless you change name *table2.melt &lt;- melt(table2, id=c("Date"))* ## Merge data based on unique combination of date + department *final &lt;- merge(table2.melt, table1, by.x=c("Date","variable"), by.y=c("Date","Department"))* You would need to clean some stuff up, rename columns etc. but that is the core transformation assuming you are okay with example scenario #1. 
Wow I can't thank you enough for the time it took to write out such a thoughtful response. I was considering creating a "Money" column in table 1, but I wasn't sure if that was the correct step. I have some nominal data as well so I'm going to end up running an ANOVA, but the core of what you suggested still applies. Thanks again. That was extremely helpful. 
&gt; Now, you're lucky, because in this case the error message is completely self-explanatory: &gt; Also, you may want to look into the join functions in dplyr, they're very convenient. I've got to agree. `dplyr` will save you soooooo much time.
Model &lt;- lm(dependent + IV + IV + IV, data = data) Summary(model) Remove variables based off their p value is a good place to start. That's called stepwise regression! Also good for you for using R! Edit: added text. 
Thanks for the link. All I can find on there is that dark on light is better for reading comprehension, and one study found light on dark has reduced visual fatigue (though this may be caused by reduced screen flicker on older screens). Seems to be very far conclusive, and nothing about actual eye health, so I wouldn't change anything because of that.
Principle component analysis is helpful. If that doesn't interest you look at the drop1() function. https://www.youtube.com/watch?v=f9mZ8dSrVJA
Try using the `data_frame()` function. 
This worked great, thanks! 
Can you produce any scatter plots to see what the relationships look like between the IV and DV?
You should save the excel file as a .csv then read it into R. If your columns have headings, make sure to set that argument to True. Otherwise just use RStudio.
Use openxlsx instead - much faster and fully versatile IMO
Thanks, that's what I was looking for ! I've read the doc before asking, look like I have miss it !
I recommend the Intro to R free course on DataCamp, R-Bloggers, and some of the free tutorials hidden within DataCamp community section.
 DataSet &lt;- ... nr &lt;- nrow(DataSet) for(i in 1:nr) { if(length(grep("Officer",DataSet[nr][1]))&gt;0) DataSet[nr][1] &lt;- "Officer" else Print DataSet[nr][1] } I am learning R, so I just cobbled this together for you. Hope it helps.
There's also a coursera course from john hopkins about data science (and R) that is aimed at people who are new to programming in general and interpreting data sets
Best Video Course - hunt for a coupon. https://www.udemy.com/comprcourse If you're more of a reader. R for Dummies Intro to R PDF https://cran.r-project.org/doc/manuals/R-intro.pdf If you need a stats refresher. http://www.math.csi.cuny.edu/Statistics/R/simpleR/printable/simpleR.pdf http://cbb.sjtu.edu.cn/~mywu/bi217/usingR.pdf 
Thankyou, will look into these sources in depth
Thanks!
Is coursera free or $. Would prefer free resources
Just looked up that term. That sounds correct. The data already exists, however I need to use vim to open it. Essentially I need to make the data readable. My colleagues have figured out the code, but I want to try to achieve the same outcome on my own, rather than just copying thier code. 
Thanks mate, looking into that now
nice book list
you can do both, I did the free course and it works fine. Paying just opens up stuff like group projects and feedback on assignments etc
To aid mobile users, I'll link small subreddits not yet linked in the comments /r/Shiny: Shiny Pokemon CSS tests --- ^I ^am ^a ^bot ^| [^Mail ^BotOwner](http://reddit.com/message/compose/?to=DarkMio&amp;subject=SmallSubBot%20Report) ^| ^To ^aid ^mobile ^users, ^I'll ^link ^small ^subreddits ^not ^yet ^linked ^in ^the ^comments ^| ^[Code](https://github.com/DarkMio/Massdrop-Reddit-Bot) ^| [^Ban](https://www.reddit.com/message/compose/?to=SmallSubBot&amp;subject=SmallSubBot%20Report&amp;message=ban%20/r/Subreddit) ^- [^Help](https://www.reddit.com/r/MassdropBot/wiki/index#wiki_banning_a_bot)
Even easier, just use readr::read_file(), which returns one string with the file's contents.
You first have to tell R that your second column refers to time and that these values are from different days. Otherwise, why would R assume that 23.59 comes before 00.04? 
This sounds like a job for the apply function. https://youtu.be/f0U74ZvLfQo
Without reproducible data, this is hard to answer. Have you tried checking the dimensions of your data frames? dim(Trump_primaries) dim(Trump_election)
If you don't want R to sort the values, keep them as characters (instead of numeric), and then label your plot accordingly. table &lt;- read.table("/data.tsv", header = FALSE, as.is = TRUE) # keep second column as character plot(table[,1], xaxt = "n") #plot without default x tick labels axis(1, at=1:10, labels=table[,2]) # add your own tick labels 
primaries &lt;- read.csv("primaries.csv", header = TRUE, as.is=TRUE) election &lt;- read.csv("election.csv", header = TRUE, as.is=TRUE) as.numeric(gsub('%','',Trump_primaries[,"Donald.Trump"])) - as.numeric(gsub('%','',Trump_election[,"Donald.Trump"]))
A bit of a convoluted hack, but this get's it done: plot(as.POSIXct(paste(c(rep(1,5),rep(2,5)),table[,2]),format="%j %H:%M"), table[,1])
Generally points are plotted as scatter plots. If you plotted 1,2 and 2,3 your see the same thing as if you plotted 2,3 then 1,2. As obvious as that sounds, it's easy to forget. 
Any duplicates across the y will create extra x entries. 
you can use gsub() in conjunction with apply().
They both same same country names, just not equally many. Theres is no duplicates other than names, which it removes when i try to merge. It goes from a few hundred to 20k+ observations when i try to merge Edit: i found out something... length of the first data.frame=x, length of 2nd=y for every entry of y, i get x amount of y[1,1] and x amount of y[1,2] so im getting x*y total entries, but i dont know why edit 2: on top of that, i cant see in str(data.frame) what my country names are given as (it doesn't say "key" or anything to let me merge by=c("key") fx
How do i deal with this? my first dataset has the countries names, by rownames(x)=countries But my y data.frame has the countries in it by default, neither has any bind in str(x) or str(y) so i dont know what i should merge by? The countries are a-x in x and (b,d,f,i,j,k,m,x) in y, but i dont know what to merge by, in order to get them to merge from a-x, since i cant find the str of the names?
Maybe not optimal solution, but you could use charToRaw. charToRaw( "REDDIT" ) will give you 6 element vector with the letters as raw codes (integers), then just do the same for parameter char and compare.
Basically start with the harder problem: find the indices. Then, simplify the result (collection of indices) to the other problem: length of this collection == `how.many`. You've complicated the problem by making it return both results as part of a list. I... don't know if I would do that. But, regardless, let's use something that will make this easy for us. There's a function that is very similar to your request: `gregexpr`. It searches a string for a regular expression pattern and returns every index that matches. We'll use that to find the $which locations, then count. Since `gregexpr` also can be applied to a *list* of input strings, I've extended it to fit your use case. foo = function(strings, pattern) { search = gregexpr(pattern, strings) if (length(strings) &lt;= 1) { if (all(attr(search[[1]], "match.length") == -1)) { return(list(which = integer(0), how.many = 0)) } attributes(search[[1]]) = NULL result = list(which = search[[1]], how.many = length(search[[1]])) } else { result = lapply(strings, function(s) foo(s, pattern)) names(result) = strings } return(result) } LaLaLa = function(string, char) { foo(strings = string, pattern = char) } so I get &gt; LaLaLa("REDDIT", "D") $which [1] 3 4 $how.many [1] 2 but I can also get &gt; LaLaLa(c("REDDIT", "lower", "TINDER"), "D") $REDDIT $REDDIT$which [1] 3 4 $REDDIT$how.many [1] 2 $lower $lower$which integer(0) $lower$how.many [1] 0 $TINDER $TINDER$which [1] 4 $TINDER$how.many [1] 1 
I'm sure someone will give a better explanation but here's my go at it. For my method we have to do the following: 1. Break up the string you are providing by using the strsplit() function. 2. Use the which() function to find out the indices. 3. Repeat the same matching procedure from 2 and use the sum function to return the number of times the character you are searching for matched the individual characters of the string. I'm no Rtist, but this seems to output the results you were looking for: count_char &lt;- function(char,string){ ans = list() spread_string &lt;-strsplit(string,"")[[1]] char_eval &lt;-char==spread_string ans$which = which(char_eval ) ans$how.many = sum(char_eval ) ans }
Always make sure you are merging a smaller data frame to a larger one. You might be getting a lot of issues because you are merging your larger one into a smaller one. 
I want to eventually do some aggregation and regression analysis, but I need all 30 teams to be independently sorted before I can merge all the necessary data. I'd need to set them all to global environments so that I can then do the merging outside of the function. The idea is that I want to automate the sorting of all 30 entries, instead of manually inputting the respective team id, team name, and venue id. I just don't know how characters are passed to a function and how those character strings can be coerced with other character strings to create the name of a usable data frame. 
I probably would have just made a wrapper around gregexpr to convert the list result with attributes to just an atomic list result, and then a second function that looks at indices result and returns length. 
This isn't about pokemon, but about shiny apps for R.
Can you show the actual code you used
Depends on what package you are using "dataframe" is not base R please specify as i dont know what "dataframe" does without the package. Same example in base R below. #create data column1 &lt;- c("x","y","z") column2 = c("a", "b", "c") #combine data data &lt;- cbind(column1, column2) #change to dataframe data &lt;- as.data.frame(data) #remove "column1" change back to dataframe data &lt;- as.data.frame(data[,-1]) 
The more complex answer is that the RODBC object that's returned by your function shouldn't care whether or not RODBC has been imported or not, because it should use generic methods. It doesn't and therefore, in my opinion, the package is a bit poopy. The RODBCDBI package is an attempt to make a DBI-compliant interface for RODBC that you may find easier to use. But honestly I myself have given up on RODBC and moved to JDBC drivers and RJDBC for anything of any importance. I particularly don't like how RODBC handles errors at query time. Perhaps if you described your use case a bit more I could help more?
Try here http://polisci2.ucsd.edu/dhughes/teaching/MLE_in_R.pdf
So let's start with what you know. Do you know what the likelihood function is? Could you do it by hand for an exponential rv with a sample size of let's say n=2?
you really need to organise your data better http://r4ds.had.co.nz/tidy.html
Not sure what the use case is for this, but more power to you.
0.5.0
You can create your own blog or academic website/portfolio. R code can be embedded within the blog by simply using R Markdown.
Accompany it with this http://www.youtube.com/playlist?list=PL06ytJZ4Ak1rXmlvxTyAdOEfiVEzH00IK
THANK YOU!! the steps were helpful thank you
Thanks - I'll try it out tomorrow
So super sorry for not returning to you, I spent the next 48 hours working on a project! Ugh, I don't really know what to tell you, i tried leaving it for an hour or so, then restarting my PC, and when I ran the code again, it somehow worked, no changes, no nothing! But thanks for the help!
I say this all the time, but caret is your best friend. Here's a great little article on getting started with caret and naive Bayes http://machinelearningmastery.com/how-to-estimate-model-accuracy-in-r-using-the-caret-package/
hmmm.... Sorry I was on mobile earlier and couldn't look up the syntax. It looks like you are using plot.svm correctly in your original script. There are two things that might be happening. Your train.data could be in matrix format instead of a dataframe. But this is unlikely since read.csv typically returns a data frame. The other possibility is that your original data set only contains two columns - diagnosis and one other input variable. Is the error you get when running the original code something like Error in ncol(data): number &lt; 2 ?
I think I know what you're trying to do, but if I misunderstood please let me know. For the constructor to require 2 parameters, you'll need to give it 2 fields. GPS &lt;- setRefClass("GPS", fields = list( longitude = "numeric", latitude = "numeric")) thisCoordinate &lt;- GPS$new(longitude = 38.9, latitude = 77) 
The book probably wants the numerical data on the y-axis and the categorical data on the x-axis. Let's assume the numerical variable is age. This way you could see the age quartiles for people that answered yes and the age quartiles for people that answered no.
As far as I know, they aren't required in whole or part, although it is generally a good idea to have examples for your main exported functions. (fwiw, I'm the author of five packages on CRAN.)
Yup your right, thanks! 
Correct. There is absolutely no requirement to have any sort of testing for your package. It's a good idea but not a requirement.
Do JSS reviewers typically comment about that? I (naturally) had examples and the associated code to replicate them included with the JSS paper, but not automated tests embedded in the package.
I don't fully understand what you're asking here. My interpretation is that you want a function that returns 10 when given a depth between 0 and 5 returns 5 when given a depth between 5 and 6 returns 10 when given a depth between 6 and 10 Is that right? In that case your R function would look something like speed &lt;- function(depth) { if(depth &gt;= 0 &amp;&amp; depth &lt; 5) { 10 } else if(depth &gt;= 5 &amp;&amp; depth &lt; 6) { 5 } else if(depth &gt;= 6 &amp;&amp; depth &lt; 10) { 10 } } This function would work. A long if-else statement is fine for this purpose. If you have a lot of values or multiple inputs, it might be worth defining the relationship in another table or a data frame and then just looking up the value in the function.
This is the exact type of thing I was looking for. I guess I was unaware I could just use a if then statement. Like I said, I'm not very good with R. Thanks for your help.
It obviously depends, but this is an error you can get if you input a `NULL` or `NA` value. You may want to protect against those by having your first if statement handle it. Check your data to see if that's what's causing this issue.
What package are you using for the `SMA()` function?
So first things first, on Reddit you can format text as code by beginning the line with 4 spaces. The code you've provided has some syntax errors or other weirdness (it's not the right syntax for function definitions) so I'm guessing it's supposed to be pseudocode? It's pretty difficult to follow what you're trying to do. Are you just trying to define a step function? It would be simplest if you could provide some example data that described what input you want to give, and what output you expect, and then we can just help with solutions to your problem.
It's really hard to help you right now because you've only given a partial description of the problem. You probably don't need a function-within-function solution, I think, but I don't have enough info to know yet. It would be most helpful if you could provide a toy data set that demonstrates what you need to accomplish, I think. 
I think you're confusing yourself by trying to nest the one function inside the other. You *can* do that, but I think you need to **fix** your current "main" and "side" functions first before you look into how to declare a local function. Side = function(z) { if (z &lt;= -0.5 &amp;&amp; z &gt; -9.5) { 100 } else if (z &lt;= -9.5 &amp;&amp; z &gt; -19.5) { 200 } else if (z &lt;= -19.5 &amp;&amp; z &gt; -30) { 100 } } Note that `Side` is undefined for z &gt; -0.5 and z &lt;= -30. This will cause your function to return NULL and then probably crash later on. In order to clean up `Side`, I inserted closing parenthesis where you missed them on each if-check and also inserted extra space to make it clear you meant `z &gt; -9.5` as opposed to `z &gt;= 9.5` since you originally wrote `z &gt;-9.5`. It wasn't wrong, just confusing. Main = function(h, z) { A = 2 B = 3 K = Ks * A / B * h return(K) } &gt; and there are three distinct layers in this code. The top layer and the bottom layer having a higher intrinsic property 'Ks' which is proportional to the 'K' value. I have no idea what a "layer" is but it's clear to me that you haven't defined `Ks` yet. I'm assuming you want `Ks = Side(...)` where you pass some argument to Side but I have no idea which one is supposed to go in there. &gt; If I were to run something like: SideFunction(-4) I would expect to get 100, and if I ran it from SideFunction(-30) I would expect to get 200 -4 will return 100 but -30 will return NULL. As mentioned before, fix your bounds. &gt; So I want my main function to be dependent on both h(another function) and Ks (but I'm not sure if it's dependent on z or Ks). Is Ks a function or a variable? In closing, please never use one-letter variables. It's confusing as shit. 
Okay great, this certainly sounds like a job for reshape2. Just to be clear, I'm going to be assuming you have already read the data into an R data frame and that your data looks like this: |Origin|Destination|Time |-|-|-| |1|1|.5| |1|2|4| 2|1|.4 2|2|6 That form of data is known as "long" form and you want to convert it to "wide" form. You can do this with `dcast`. If `dat` is your data.frame, then you can do this: dat.wide &lt;- dcast(dat, Origin~Destination) Then, you can convert it into a matrix with: dat.wide.matrix &lt;- as.matrix(dat.wide) Just make sure that: 1. You have all numeric data, because a single character value will turn it into a character matrix. 2. You don't have any factor variables in your data frame. This usually happens when reading in a txt or csv file that contains character data (the default is to turn it into a factor), so if you don't have characters you probably don't have factors. You can check with `str(dat)`.
With that said there isn't much of a need to define Side as a function and instead you could just use if/else directly to define Ks
Certainly nested in that way, yes. But I think it would be acceptable to define Side globally if it turns out to be needed in other functions later on. It's a helper function, so keep it as a function and not as a if/else. It could potentially improves code re-use.
What is this Bing?
Okay good, then we don't need to worry about how to deal with those. If you did have instances like that, you'd need to decide if you wanted to average over those or whatever. But this makes it a very straightforward conversion from long format to wide.
That's defined in the very outer loop, though, so it will only be true on the first loop or whenever the particle changes directions. In each loop thereafter, I modify P in some way. That being said, there was an issue with some of my loops that I caught (and I will update it in the original post). My issue now is if I run walkprocess(100,1,0.5,0.5,100), I should, in theory, be averaging out the random walks of 100 different trials of 100 steps each. This means I should get a value somewhere around Sqrt(n L^2) = 10. My code consistently gives me something around 7.1, so something is off.
Can you explain the math a bit more? A good link or two on the calculations could work. I'm not totally familiar with the topic. I've gone through the code and can't find anything that seems like it's breaking what you're trying to calculate. I did find some parts that could've caused some issue (one is your use of `trials` as a function argument as well as a storage vector), but after fixing there wasn't any noticeable difference.
Hey there. This is my back of the envelope thinking. I can't say for certain, but I think you're forgetting that functions execute their code in a new environment. So when you call your for-loop, R creates an environment for the loop execute each iteration in, where you're expecting it to run in the global environment. So if you want to assign global variables from within a loop (or any function), I recommend using `assign(envir = globalenv())` rather than the normal assignment operator `&lt;-`. As a sidenote, I've found that eliminating loops from my R code prevents a ton of headaches. Right now you have more of a sub-routine than a function, because you're the one deciding where variables are going, how they are getting there, etc. If you can change that into a function where you're telling R only how to map from what you have to what you want , R will just return that and it doesn't matter what environment you're in. Let me know if this helps or if I can make it clearer. I'll actually be awake in a few hours.
You'll want to check out the lm() function. https://stat.ethz.ch/R-manual/R-devel/library/stats/html/lm.html
Tells you how long it's been since I've written a for loop in R. Thanks for the correction. = )
Okay, thanks a lot! Apparently, my biggest barrier was to translate the Excel terms I knew (in French!) to R in English. I figured everything when I was writing a previous comment to your answer (so thank you :).
Sure! [Here is an explanation of the root mean square.](http://mathworld.wolfram.com/Root-Mean-Square.html) Basically what it is is the sum of the squares of the various positions in the walk divided by the number of steps taken, all raised to the power of 1/2. 
Bing is the search engine created by microsoft do you know.
Also you should show your professor the paper (I'm on mobile so I don't have it but it's very famous) that looked at Excel's stats functions and showed that they were wrong. Even the function that just returns parts of the normal distribution doesn't work right. 
Well, I edited my code and the values I get now certainly make sense. I guess I had a misunderstanding about what the RMS value of a random walk was. Thanks so much for your help
I just took your and /u/TonySu's advice now that I understood better what you were saying and my code seems to work and give me the values I want. Thanks so much for your help! It is greatly appreciated.
Do you mean badly conceived only because of this or the function is written badly in general? I kind of only need the part with the boxplot so i can get the outliers. None of the stuff bellow concern me. Do you have something in mind that would work better for the same purpose? (labeling outliers with adjusted boxplot)
Try looking for Andy Field's book, "Discovering Stats with R". It's an excellent resource for doing beginner stats in R, and you'll most likely find parallel methods for the analysis you're trying to do in R. OpenOffice might also have the analysis capabilities you need, while more closely matching what your class-mates are doing: http://www.comfsm.fm/~dleeling/statistics/text5.html . Though if you can learn to do things in R, you'll probably be better off! Also, a professor in my school, who teaches statistics in the business school developed a package called LessR that makes it easier to do common statistical tasks, probably like the ones you're needing to use. https://cran.r-project.org/web/packages/lessR/index.html He even wrote a book around it: https://books.google.com/books?id=BVFWAgAAQBAJ&amp;pg=PR1&amp;dq=gerbing+R+less-R&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwidhr3fpuTQAhUKzGMKHTX7Bc4Q6AEINDAA#v=onepage&amp;q=gerbing%20R%20less-R&amp;f=false
haha, nice! I'll try to find it but if you remember the name I'll gladly take it :)
Thanks for the reference!
The sample data looks like CSV and you can do plenty with it. What message did you want to convey to management? [Simple CSV import](http://www.statmethods.net/input/importingdata.html)
Wow you need to be a bit more specific. What do you want to do? There are a million ways to visualize your data, so sure it can be done. If you have to transform the data you can use the tidyr package.
There are at least two other problems; namely, (1) that the function requires user input from the command line. This has no business inside the function, it should be supplied as a parameter. And (2) the function produces output via `cat`. Once again, the function shouldn’t do that. At most, it should use `message` (so the output could be silenced) but really it shouldn’t do that at all. In summary, this function massively violates SRP — the [single responsibility principle](https://en.wikipedia.org/wiki/Single_responsibility_principle): every function should have exactly one responsiblity/functionality. This is one of the most fundamental principles of software design, and I have never seen a good reason to break it. If you’re merely interested in labelling outliers, there’s no need at all for this bastard function. You can just get the outliers as follows: outliers = boxplot.stats(variable)$out And to mask these values as NAs in the original data: variable[match(outliers, variable)] = NA
Kirill has a LOT of experience and has been teaching data science for years
I’m not denying this but the linked “blog post” is simply Google SEO spam that is cramming as many hyperlinked keywords as possible into a text. Other articles by the same author all follow the same scheme, and [at least one](https://medium.com/@rodre/learn-how-to-create-modern-web-applications-with-the-successor-of-angular-js-60acb664a733) has been suspended by Medium for spamming.
Your data is in a good format. But if you want it in the format of your table, that's referred to as converting the data to a "wide" format. I'll use the tidyr package to convert the data from long to wide format in my example below, where I create some mock data to perform analysis on. library(ggplot2) library(tidyr) web &lt;- data.frame(student = factor( rep(paste0(LETTERS[1:20], "1"), each = 4) ), site = factor( rep(c("Google.com", "reddit.com", "amazon.com", "facebook.com"), 20) ), visits = sample(0:100, 80, replace = TRUE)) web$site &lt;- as.factor(tolower(web$site)) # all website strings in lowercase web_wide &lt;- spread(web, key = site, value = visits) # Data from long to wide format # Distribution of visits for all websites qplot(data = web, visits, binwidth = 5) # Total visits per website ggplot(data = web, aes(x = site, y = visits)) + geom_bar(stat = "identity") # Scatter plot of nr of visits by website and student (point colours) ggplot(data = web, aes(x = site, y = visits, colour = student)) + geom_point() # Distribution of visits to google.com by subsetting the long format data frame ggplot(data = web[web[, "site"] == "google.com", ], aes(x = visits)) + geom_histogram(binwidth = 10) ggplot(data = web[web[, "site"] == "reddit.com", ], aes(x = visits)) + geom_histogram(binwidth = 10) # Same distribution of visits to google.com by using the wide data frame ggplot(web_wide, aes(x = google.com)) + geom_histogram(binwidth = 10) ggplot(web_wide, aes(x = reddit.com)) + geom_histogram(binwidth = 10) Now this is all basic exploratory data analysis which should be among the first things someone learns when they start learning R. There are plenty of resources on the internet that will serve you better than a reddit comment ever can when it comes to learning this. Your question isn't so much asking for R-help, but in a roundabout way asking for reddit to teach you exploratory data analysis and R. The answer to your question in the original post is yes. The first step to identifying outliers is usually plotting a scatter plot. And to examine the shape and distribution of your data a histogram. I'd recommend this http://r4ds.had.co.nz/ or www.datacamp.com for a proper introduction to visual exploratory data analysis. *Edit: If you have 216 unique students and 414 unique sites you don't want to try and create a bar plot out of that. And I can imagine it might create problems when you try and make a bar plot in excel from that kind of data too (you're trying to create a bar plot with 216 bars). I'd stick to histograms and scatter plots in R. But beyond that I think you'd have to process the data a bit to be able to get worthwhile insights from it. For example group and/or categorize the websites based on visits. Or adding data for the type/category of the websites visited. In this case it is not to your advantage to have 216 and 414 unique categories which are to be plotted against one quantitative variable (visits), but in my opinion the data is enough to answer your original question (are there any anomalies/outliers). Use histograms and scatter plots. 
&gt; The next step is to calculate the site's internal PageRank with R, which I presume pulls data from my initial crawl. This is where you lost me. &gt; I'm using Windows (I believe this makes a difference). I don't see how. Could you please explain how your definition of "internal pagerank" differs from the standard algorithm: PR(u) = \sum_v PR(v) / L(v) for all v which link to u, for pages u &amp; v, where PR is pagerank and L is # of links If you're just asking how to compute pagerank given an adjacency matrix of web pages, then follow the formula I just described: every page starts at a uniform value of importance, and then you scan over every page and set its pagerank value to be the weighted sum I described above (pagerank over links for each neighboring page). Repeat until the values converge.
Was going to post this. Another alternative is to use sapply instead. Pro: Only needs one line. Con: Need to transpose. my_list &lt;- t(sapply(1:10, function(i) list(a=i, b=2*i)))
that would have omitted any observation with a variable reading NA. OP needed a specific combo of variables to read NA to be filtered. His comment is right. probably a way to do it cleaner, but it works.
Works! Thank you very much!
A neater approach might be be: filter_na_rows &lt;- function(df, col_names) { df_sub &lt;- df[col_names] df[rowSums(is.na(df_sub)) != ncol(df_sub), ] }
2 should be simple, lapply(yourListNameHere, ncol) 1 is going to be harder. I'm not sure lapply is the way to go here; applying something to vectors within lists within lists is weird and you're not comfortable with lapply. Just write a for loop. It's ugly but sometimes you're just not smart enough to do it the right way (I'm not either). rowsum() is a function, as is mean(), so that part at least should be straight forward.
Ok, Httr needs some unmet dependencies and it tries to tell you which ones. Sometimes it is not very clear and you might need to try installing different packages that seem to be the one(s) it asks for. The reason that this isn't always very specific is that different distributions chooses to package the libraries differently and the httr developers don't know your specific situation. In this case it asks for libcurl and openssl. The packages need to the "dev" version which means the version that can be used when compiling other applications (in this case 'httr'). Looking through the packages available to me on Kubuntu 14.04 (I use synaptic and searched for 'libcurl' and 'openssl') there is the package libcurl-openssl-dev Which would seem to check all the boxes. So, first check that you have this package or something that looks close enough, then install it and try again. You may experience that other unmet dependencies come to light and you will have to install those too. It's actually quite easy once you have tried it. One word of advice: write down which packages you installed and why (eg. 'libcurl-openssl-dev' because of 'httr') for later reference, you'll be happy you did a year from now when you upgrade and httr mysteriously stops working. I use the official R-repos that move a lot faster than Ubuntu's repos so I sometimes experience incompatibilities that are not easy to track down and this helps me understand the issue - even if it can't easily be fixed.
You need to install on the server probably. sudo apt-get update sudo apt-get install x sudo apt-get install y On mobile otherwise I would help more.
If you look closely in the error log you'll see this: Configuration failed because openssl was not found. Try installing: * deb: libssl-dev (Debian, Ubuntu, etc) ... So, now you look for *libssl-dev*, install it and try again - and so on - until you have installed all the missing dependencies. It's a very normal procedure. That's exactly how I do it.
You can create a ts object using the ts(). And us the plot() function and there is a method for time series objects already. Turning into a data frame and plotting with ggplot2 will give you more options. library(ggplot2) numSeries &lt;- c(1,4,3,4,3,2) tsObject &lt;- ts(numSeries, start = c(2015,1), frequency = 52) #set frequency arg to 12 for months, 4 for quarters etc... #option 1 plot(tsObject) #option 2 ggplotData &lt;- data.frame(value = tsObject, dates = time(tsObject)) ggplot(ggplotData, aes(x = as.numeric(dates), y = as.numeric(value)))+ geom_line() 
I get it, thanks a lot it's not the first time this happens it just is the first time a library was that crucial.
I would say it depends on the role and company. For a more senior position, specific proficiency in the firm's available statistics applications is expected. Particularly for contractors. For an entry level data science role, mathematical and analytic thinking, as well as company culture fit, are more important. What field and discipline are you trying to get into?
OK but my question was about whether R competence has any relevance to competence with other stats tools. 
If you have the means to, just check these programs out! I personally find STATA very easy to use compared to R and it is just a matter of sitting down and learning the format. There are several stats books that focus on multiple programs (for my field, polisci, there is a great book by Bailey called Real Stat that teaches R alongside STATA). A lot of this also depends on your field.
Udemy.com has a course by Jose Portilla that is great. Is called 'R For Data Science'. Just note that the base plot visualization section is lacking and the ggplot section for visualization MIGHT be outdated, maybe.
&gt;You don't mention R and stats. It's right there in the title. And the post. Also, looky what the R people say: &gt;R is a language and environment for **statistical** computing and graphics. https://www.r-project.org/about.html I suggest you add reading comprehension to your curriculum so you can understand the question that is put to you. Also, maturity - because you always had the option to ignore the question the first place.
Small multiples perhaps? 4 maps next to each other. At this resolution this looks like the "big picture" sort of thing.
Or using base-R df[,yardsaftercatch&gt;100]
I've written a few blog posts and some apps very similar to this idea. You might benefit from them. Specifically: - [Mimicking a Google Form with a shiny app](http://deanattali.com/2015/06/14/mimicking-google-form-shiny/) - [Persistent data storage in Shiny](http://deanattali.com/blog/shiny-persistent-data-storage/) - [App that collects user data and saves it](https://daattali.com/shiny/mimic-google-form/) ([code](https://github.com/daattali/shiny-server/tree/master/mimic-google-form))
Well this doesn’t really say much since `typeof(the_matrix)` is *also* `"double"`. It’s more informative to examine the dimensions: &gt; dim(the_matrix) [1] 3 2 &gt; dim(the_matrix[2 : 3, 2]) NULL &gt; dim(the_matrix[2 : 3, 2, drop = FALSE]) [1] 2 1 In other words: unless `drop = FALSE` is specified, indexing a matrix (or `data.frame`) will drop the dimensions (and other attributes, such as column/row names) of one-dimensional return values. This makes indexing of matrices in R infuriatingly inconsistent since the return type depends on the argument values. This was a terrible idea and modern libraries generally try to fix this (for instance, indexing in dplyr always returns the same type).
using prettyNum() should work.
Use xlim ylim flags for plot()
lol
Take a look at the unique entries under state: they all start with a space. So you'd want to use " IL" instead of "IL". 
Worked perfect, lack of sleep got to me.. cheers!
?rbind
 &gt; nrow(question_1) [1] 22452 &gt; nrow(question_2) [1] 5109 &gt; subject &lt;- rbind(question_1,question_2) &gt; nrow(subject) [1] 27561 Rbind does a good job combining the rows however with this new dataframe ***subject*** I loose what data is question_1 and what data is question_2. I would have to keep track of row indexes which is something I ideally don't want to do. Thank you for the suggestion though!
Then put a column in each data frame that indicates which question it is, then bind them?
As another user pointed out this is precisely the solution I was looking for. Thank you.
My current code: ggplot(during, aes(reorder(state, state, function(x) + length(x))))+ geom_bar(fill = "darkblue") + labs( x = "Year", y = "Total Number of Deaths") Works fine but I want to filter the graph so only states above a certain number deaths shows up (For a report so makes it easier to read) 
Shouldn't you be filtering on the total number of deaths then
I'm going to second your opinion that this is probably the most elegant solution provided here.
Well i am using a substitute of the regular boxplot - adjusted boxplot from robustbase package. But Management doesn't trust the Tukey's (the boxplot method, be it adjusted or not) method for some reason. I have heard of people detecting outliers with Cook's distance after performing a linear regression on their data. Is this really viable though?
You can alternatively subset your data before you ggplot it. Here's some pseudocode during_subset &lt;- subset(during,deaths &gt; 100 &amp; deaths &lt;400 ) Then ggplot(during_subset, aes......)
https://www.amazon.com/gp/aw/d/0199981949/ref=mp_s_a_1_1?ie=UTF8&amp;qid=1482241947&amp;sr=8-1&amp;pi=SY200_QL40&amp;keywords=real+stats+bailey&amp;dpPl=1&amp;dpID=41CMlOs8S8L&amp;ref=plSrch Here is the book on Amazon. Like I said, it is for my field, but I love it.
Thanks, and that is definitely a path I am considering. I just think quotes are ugly and unnatural to the typical user in this context. They're especially unnatural to a Stata user, my target audience. NSE aside, there are a TON of coding faux-pas in my function. I like to think I'm a good coder but I'm at a loss with some of these things.
First set your working directory setwd(filepath here) Save your file where your working directory is set Then read the csv. I'm naming it filename for this example. I believe it will be a data.frame by default filename &lt;- read.csv('nameoffile.csv',header=TRUE) Then view it to make sure it looks ok View(filename) 
I appreciate the help. How do I implement this into the ui code to display on the Shiny page?
If I'm understanding correctly you want to do a typical read.csv call before your server function so the data is loaded into the app. See [this article](https://shiny.rstudio.com/tutorial/lesson5/) for loading data and where to put those statements
Haha nice. Dplyr all the way. Also op, yes that is the correct answer. 
I definitely have no shot at this job. I'm just wondering what the goal posts are. I kinda sorta have my doubts that the job *really* requires truly advanced R, or stats, for that matter. Reason I say that is, IME, the higher-ups/decision-makers you're doing the stats for often don't understand them anyway, and don't trust them when the result isn't what they want to hear. So it's a bit pointless - other than getting your paycheck, obvs.
Likely machine learning. Or it could be something as simple as dyplr and tidyr with ggplot2. What constitutes advanced level in different sectors is probably vastly different.
+1 Wickham
Look into data.table() This can do a number of auto summarizations
+1 for dplyr/tidyr, just based on what OP said. R is a distinct skillset from stats/ML, they just often go together. edit: my point is if they NEED stats or ML, they should say so separately.
From this I would guess they mean using R for "machine learning algorithms and predictive analytics models" and R with "Hadoop ecosystem". Maybe a different definition that you get from say http://adv-r.had.co.nz
So "advanced level" is meaningless? It wouldn't surprise me. HR people often don't know the first thing about the jobs they write descriptions for.
If you have time series data, you can use the lag() function. If you don't, just subset the data manually: t.lag &lt;- 4 Data.differenced &lt;- Data[t.lag:length(Data)] - Data[1:(length(Data)-t.lag)]
&gt; Reason I say that is, IME, the higher-ups/decision-makers you're doing the stats for often don't understand them anyway, and don't trust them when the result isn't what they want to hear. Strongly, strongly depends on the company. I work in search analytics. Everything comes down to search accuracy, precision, and recall. All day, er'ryday. You can get any senior manager's attention by saying the phrase "increase OPS and improve customer experience". 
R is so fun to learn! Nice job starting out and good luck in the future! I vaguely skimmed over your code, and saw the following... card &lt;- sample(1:16, 1) if (card == 1){ square &lt;- 0 } else if (card == 2){ square &lt;- 24 } else if (card == 3){ square &lt;- 11 } else if (card == 4){ if (28 &gt; square &amp; square &gt; 11){ square &lt;- 28 } else { square &lt;- 12 } etc etc etc A fun/shorter way you could do this is to create array chance-cards and then pick from there, so card &lt;- sample(1:16, 1) chancecards &lt;- c(0,24,11,whatever numbers) square &lt;- chancecard[card] Since you put some special logic in the 4, 5, 6 chance card. you could do the big if else statements, or, card5 &lt;- c(rep(5,4),rep(15,10),rep(25,9),rep(35,9),rep(5,5)) if(card==5){ square &lt;- card5[square] } In case you don't know, rep(3,4), is the same as c(3,3,3,3), or 3 four times, so you don't need to type as much. Using vectors can usually save you from writing endless if statements, and I'm rather lazy, so I do this quite often.
Thanks! I actually just finished an MA in Poli Sci and want to get my feet wet in R before my PhD.
Awesome! I'm working on my PhD in polisci right now! PM me if you ever need more help!
Thank you! Good luck!
would you know why read_csv from the readr package works faster than using readRDS in quite a few situations? even with a much larger file size, readr still works better in my experience.
In a sense, intermediate level R programming, is advanced as compared to most other types of languages. An intermediate R programmer is going to be working with higher level analytical concepts and visualizations, just due to the nature and focus of the language. I suggest not getting hung-up on that word :)
If your app only computes certain things, you can precompute everything so that the app never needs to see the full data. So say summary statistics can be cached, even histograms, scatter plots, whatever. It'll make the app considerably more responsive.
Almost anything with NSE won't work inside a function without some maneuvering. That's why I say to avoid it! You either need to avoid it entirely, make two different versions of your function (one that has NSE and one without), or have a parameter for your function that tells the function whether the input should be grabbed via NSE or treated as character already. Avoiding NSE entirely is by far the easiest route and the one that leads to the least amount of headaches. Yes you pay the price of having to type quotes every now and then but it saves a million headaches.
seconding advise to use data table. in my experience speeds stuff up rather notably
Hmm, this is true! How would I precompute the plots and then use them in R?
Looks good to use! Only problem is the variable name i'm using for deaths by state is a factor, there is not INT or anything.. i'm new to R so if you could spread some light on this be very helpful
You will probably get better responses if you post more specific questions. I suggest trying to implement your application, and as you run into specific problems, post questions about those. As generic advice: I've found it useful to use colClasses when reading in CSV files, and saving/reading partial tables with saveRDS/readRDS.
I'm one of the co-authors of the [pacman](https://github.com/trinker/pacman) package. We opted to allow for NSE to make it easier on users so I know how to deal with this and where the headaches come from. Like I said I highly suggest not going the NSE route unless absolutely necessary. But if you want to go that route you could check out how we implement it in our package. I would suggest looking at the code for [`p_load`](https://github.com/trinker/pacman) if you want to get a handle on how we treat the input
Google is your friend. Here's one step: https://www.r-bloggers.com/accessing-bitcoin-data-with-r/amp/
&gt; Continually running script that on a server Assuming you are using a linux server, you'll want to run the script using a command like: $ nohup R CMD BATCH my_script.R &amp; `nohup` is a linux command that means "no hang up". If you are SSH'd into the box, it keeps the command from dying after you close your terminal session. It will also log stdout in the file `nohup.out`. The trailing ampersand pushes the job to run in the background. `R CMD BATCH scriptname.R` is just how you run R scripts from the command line, which you likely have never had to do before. If you want to run the script on a schedule, check out [kuri](http://kuri.io/) (R package) or `chron` (unix built-in) &gt; Web scraping data (real time) https://github.com/hadley/httr &gt; Analyze and output a graph to a file If by "graph" you mean "data visualization," you just need to specify a graphics device. Check out the documentation here: https://stat.ethz.ch/R-manual/R-devel/library/grDevices/html/png.html &gt; Based on analysis, use a site's API to make an update. I'm not really sure what you mean by this, but `httr` is probably what you're looking for. 
https://www.kaggle.com/fivethirtyeight/police-officer-deaths-in-the-us This is the dataset i'm working off, nearly everything is in factors so unsure how I would subset it to say deaths &gt;100 or so?
 Total_Number_of_Deaths_by_State &lt;- subset(as.data.frame(table(clean_data$state)), Freq &gt; 100) The key is to use table when working with factors.
Any websites that explain using them you'd recommend? 
Other than using ?table I would query your questions to google with "R table how do I ... site:stackoverflow.com" This might help too: https://www.stat.berkeley.edu/classes/s133/factors.html I would imagine your problem is more to do with all your data being factors rather than how to use table. Hope it helps.
never even heard of this before. What are the advantages over shiny?
I'd recommend reading [R in a nutshell](https://www.amazon.com/dp/144931208X).
Thanks!
From what I see, they do different jobs fastRweb seems to be for providing resources to a website, perhaps written in a different language, while shiny is for well, deploying an entire webapp from R. So if you wrote your website in HTML/JS and wanted to host a plot from R on it, you would use fastRweb
To be honest I've always wanted to do this myself but I've failed to really see the potential for making enough money in it. I really wanted to do arbitrage in particular, but there's no exchanges with enough currency pairs to make that possible, which is a shame because if it were I would more than certainly write some arbitrage algorithms.
So fastRweb is for building microservices?
Curly braces are used to dwnote the code that will be contained within a function. Regular brackets typically contain parameters that are being passed to a function.
sorry if I'm being dumb/annoying, but can you give me an example?
Testfunc &lt;- function (x) { Out &lt;- x *2 Return(x) }
what about codes with multiple lines? One thing I noticed when I watched the R Programming video was that codes with multiple lines within curly brackets aren't separated by commas. for example library(shiny) ui=fluidPage( sliderInput(inputId = "numb", label = "Choose a number", value = 50, min = 1, max = 100), plotOutput(outputId = "hist") ) server=function(input, output){ output$hist=renderPlot({ title="100 random normal values" hist(rnorm(input$numb), main=title) } )} shinyApp(ui = ui, server = server)
have you contacted the package developer? I find that more often than not people are responsive about 'active' projects https://cran.r-project.org/web/packages/MODISTools/index.html
If I use the command MODISoptions(MODISserverOrder=c("LAADS", "LDAAC")) , it should fix it. I'll still test it. Ty
Ok, reinstalling `stringi` helped, thank you. Yet I still wonder what may have caused this issue in the first place.
O'Reilly have a few R-related books - are there any others worth getting (e.g. the cookbook)?
There may be a way to do this, but it sounds like you might want to find a cleaner method of doing so. How are these scripts different, other than the output file location? If the only difference is the output file location, why don't you create a script with a file name as a parameter? For example, if you have a different data set for each one, you could make a script that could be called as such: Rscript simulation.R data1.csv results1.csv
You should be able to work with this using paste() with said variable/file names.
Great videos here for the absolute beginner: https://www.youtube.com/playlist?list=PLqzoL9-eJTNARFXxgwbqGo56NtbJnB37A 
Might be helpful to describe the scope of the project, how you are calling the R file and possibly the directory structure. You could programmatically setwd() in a loop of all of the R files, and then source() the same R script depending on the project. 
I get what you mean, but I didn't know if there was something that could have happened which could have caused the problems I'm having! Shall I paste the code? 
It may be helpful to describe a histogram similar to the way you described the bar plot. A histogram is not a bar plot and the language in the article suggests that a histogram is a special case of a bar plot. 
It would be helpful to post a full example of your code. For example, we don't know anything about the Barbados variable prior to the snippet that you included. Also, you are more likely to get help if people do not have to dig through other blog posts to reproduce your example. In addition, the traceback function will be helpful in debugging (or allowing others to debug). That being said, maybe a ggplot2 wizard will be able to fix your issue with ease, given the information you have already provided. 
The obvious thing to do is check that each variable you're passing in that call (lat, long, group, etc) are all declared and instantiated. It thinks you're trying to convert a base function into a vector and it knows that that's nonsense so you've probably forgotten to declare something. Given how none of that code snippet declares any variables besides `Barbados` it might be more than one missing variable. 
Yea, a lot of good suggestions here, but I think I might not have been clear with what I want to do. Lets say I have a Rscript called Test.R. What I want to be able to do is something like this: filename&lt;-function() filename &gt;Test.R The reason I seek to do this is because I have 100 Rscripts I am running on a high computing cluster as a batch. All of the files are named Test_1.R, Test_2.R, etc. I want to be able to simply use a function in each file so it can self reference its name when creating a directory to store output, instead of having to go into each file and type the actual filename for the creation of the new directory. 
I am doing something similar to this because I have a script on a share drive that multiple people run and that way we don't have to set the working directory. It does require you to click on the filename though. Also I am sure I copied most of this from stackoverflow at some point, but I don't have the reference. # Set the working directory using a window click filename = 'filename.R' filepath = file.choose() dir = substr(filepath, 1, nchar(filepath)-nchar(filename)) setwd(dir) rm(dir, filename, filepath)
If all of your columns are numeric, just find the (column) rank of your design matrix: qr(data)$rank or, equivalently, see if det(crossprod(data)) == 0 This, of course, will only tell you if your data are rank-deficient, which technically isn't the same as two columns being identical. (A linear combination of two columns could be the same as a third column, which is not what you're describing). A long, inefficient way to solve your problem would be something like: diff_mat &lt;- matrix(FALSE, 30, 30) for (i in 1:30) for (j in i:30) { diff_mat[i, j] &lt;- all(data[,i] == data[,j]) } And see which elements of the matrix are TRUE, via which(diff_mat, arr.index = TRUE) NOTE: If your variables have nasty decimal values, a == b might not work properly. You might want to replace the all statement above with all(round(data[,i] - data[,j], 4) == 0) Hope this helps. 
Thanks for the response! I'm not quite sure it's giving me the results I would expect. m7 &lt;- c(1, 2, 3, 4, 5) m8 &lt;- c(1, 2, 3, 4, 5)*2 m9 &lt;- c(1, 2, 3, 4, 5)*3 m10 &lt;- c(1, 2, 3, 4, 5)*4 m11 &lt;- c(1, 2, 3, 4, 5)*5 t0 &lt;- cbind(m7, m8, m9, m10, m11) qr(t0)$rank # returns 1 Even if you remove some of the multipliers, it still returns 1 from what I can tell. Is there anything that would output 0 if none of the columns match, and 1 if any of them do? Or, in the best case scenario, could it identify and return which columns match? Edit: also, I'm only working with numeric data in the data frame.
Doh. My fault. Actually your long version function works perfectly!!! No need for the linear one - was my fault for chosing a bad example. Thanks again for all of your help!
No problem! If you're interested, I just finished my function. You can find it here on my GitHub: https://github.com/TimothyKBook/rutilstb/blob/master/R/colMatcher.R Or, if you want, you can pull it down and see my documentation. You can get it with devtools::install_github("TimothyKBook/rutilstb") library(rutilstb) and see the documentation with ?colMatcher /{shameless self-promotion.}
Maybe try grep() with ignore.case, because spaces and case can affect things. 
I'll give it a shot when I'm back in front of my computer tomorrow then return and report. Thanks for the tip!
Yes if I can't fix it with the other commenter's suggestion I'll share my code with sample data. Thanks!
I tried this, but all it returns is "Filename is ". What exactly is the "--file=" suppose to be? I have seen multiple places where that is used, but not quite sure what it is meant to represent.
mmm just tried it and it worked...sorry, no idea why yours won't.
Check your Internet access when you are installing the package... 
Try changing your mirror and seeing if you can install from a different one. Run this and then try to install the package options(repos = c(CRAN = "http://cran.r-project.org/")) 
I'm a little confused since I thought my variables would be associated with the outcome, but not perfect fits. On the binary variables, I'll try taking some out tonight and see if that gets rid of the warning. I'm wondering if there is a way to include all of the variables through a different model because I needed all of these in one model.
These errors often occur in conjunction when one or more variables perfectly predict the outcome. What happens is that the optimizer breaks and there is really no maximum for the conditional likelihood function -- it is infinite. As others have noted, this is indicative of a problem called "separation" that is fairly common in logit/probit with dummy variables or factors. This is something of a catch-22. Separation indicates that one variable really (really really) matters in predicting the outcome. But, model estimates under separation are unreliable for the reasons given above. But, omitting the separating variable may induce bias if it is a backdoor path or confounds your variable of interest. For a good overview of the problem, see here: http://pan.oxfordjournals.org/content/13/2/157.abstract There are many solutions, typically three stick out: 1: If possible, just exclude the variable and put in the write up a discussion of separation. 2: Use a Bayesian logit model. 3. Use penalized MLE. 
You can download the package and install it from a local library if you can't connect to cran. That's one way I've gotten around this issue when I couldn't connect to cran based on the network. If you want help with this let me know.
Hi. I suppose it should work. Can you copy and paste the exact function that you wrote and is not working for you? That might help us.
Okay, I added the full function. Also, I had a faulty memory when I first made the post - I actually ended up using a combination of if and any (as opposed to a combination of if and for as I had said). 
Thank you!! This helps a lot!!
looks like you're trying to do this, divby0 &lt;- function(x,y) { if(y == 0) { print("UNDEFINED") } if(y != 0) x/y } the double if statement is unnecessary you'd clean that up with the following: divby0 &lt;- function(x,y) { if(y == 0) { stop("UNDEFINED: denominator is zero") } x/y } Now if you're looking to make it more robust by allowing `y` to be a vector, I'd consider some additional `if` statements to expand the code: divby0 &lt;- function(x,y) { # Determine the length of y leny = length(y) # if leny=1 then it's a single value if(leny==1) { if(y == 0) { stop("UNDEFINED: denominator is zero") } x/y } # obvious notes are obvious if(leny&gt;1){ # Do some other stuff } # Maybe other stuff } That'll take care of y being a vector or even a matrix, you've just gotta decide how to check for zeros in the denominator. You can do a similar thing for `x`. Other functions to consider are `class()` to tell you the class, `str()` to tell you the structure, and `dim()` for dimensions (note that `dim()` fails on single values). Look into input validation.
&gt; fairly common in logit/probit with dummy variables or factors. This is a problem for me, because I turned all the variables into factors so that I could see the odds of each level in each variable... Is there an alternate recommended format I can change categorical variables into for GLM? I've give them nominal values as a way to organize them in my model, but these are not indicative values since they're categorical levels. I attempted to create 2 separate models, one with binary variables and the other multinomial variables, but the same and new errors popped up for both: *1: glm.fit: fitted probabilities numerically 0 or 1 occurred* *2: In sum(weights) : integer overflow - use sum(as.numeric(.))* I've also tried taking out separate variables one at a time, which didn't work out. The whole thing's stumped me at this point... I haven't learned the Bayesian logit model, but looking at the package description [here](https://cran.r-project.org/web/packages/BayesLogit/BayesLogit.pdf), it doesn't seem like I can use multiple predictors in the command? Or does it work similarly to the original GLM formula?
If you can live with the value "Inf" instead of "UNDEFINED" then: x &lt;- c(3,6,9) y &lt;- c(0,2,3) x/y 
I did not know that dim() fails on single values... helpful to know because I've been writing control flows for a few analyses pipelines and been using dim for a lot of them, not knowing that this edge case would break my whole pipe
Very pythonic 
Here is a very simple solution that does what you would like: divby0&lt;-function(x,y){ ifelse(y==0,"undefined",x/y) } vec&lt;-c(0,1,2,3,0,6,8,0,20) divby0(10,vec) Results: [1] "undefined" "10" "5" "3.33333333333333" "undefined" "1.66666666666667" "1.25" "undefined" [9] "0.5" 
The latter. Readability is preferable if you learn how to use section markers and comments to make hundreds or thousands of lines of code more navigable.
I will use the first statement when I'm teaching R, because it's easier to parse out the different arguments. When I'm writing code for myself or someone else that knows R, though, the second one is definitely the way to go. 
Usually websites that house data will have data security policies that address this issue. If you're worried about it I'd recommend contacting someone from that website.
You should be fine if you're not downloading many times a day. You might want to take a look at the Quandl package as well. It looks like they have free access to a lot of the MSCI indices. This would be much easier than scraping data.
If it's publicly available, it's fine as long as you're not scraping it so you can sell it or display it publicly.
That's kind of what I was assuming.
Good idea. I've used Quandl before, I should've checked to see if they had the data.
[R for Data Science](r4ds.had.co.nz) - new book by one of the guys (Hadley Wickham) who work on RStudio. Seems to be a good book so far; I just finished the third section.
Why not be ablr to display it publically?
Web hits = cash money, and displaying it publicly takes away web hits
Ah I see makes sense. I was thinking along the lines of using the scraped data to create something original. Then pubically displaying the original work (which used the scraped data)
Sites often have Terms of Service statements that indicate what is permitted. If "bots" are completely forbidden, try sending an email and asking the webmaster. Since webmasters are generally worried about fake clones of their site being put up or sustained, rapid, repeated crawling, they probably won't care. Often the ToS will indicate that "excessive bandwidth use" or somesuch is forbidden. ... that said, I've generally ignored the ToS and made my scraper run gently, and (as someone else pointed out) used the data only for non-commercial purposes. If you crawl gently, you're extremely unlikely to be blocked, and being blocked is about the only actual response you'll see, and only if you're hammering hard enough that responsiveness is affected for other users. curl takes a --limit-rate &lt;speed&gt; arg, so if you're eg using download.file, you could do download.file(url, dest, 'curl', extra='--limit-rate 512k') to keep throughput at half a mb/s. 'wget' has something similar. Do that and sleep for a several seconds between requests. Edit: The old standard was to have a site.com/robots.txt file that listed the permitted and forbidden bots =)
Awesome, thanks, anything else to recommend? :)
The error returned is saying that the arguments to your function (in this case fish$fish_weight and fish$sex) differ in length. Each observation in `fish_weight` needs a corresponding value in `sex`. It might be worth just checking that your arguments are spelt correctly and aren't similar words (ie check it's not fish$weight or fish$gender). 
Thanks. That seems to have fixed it.
R isn't really the tool for this kind of work. Your best bet is probably going to be opencv which has python bindings or possibly matlab if you have access to it.
Under the hood I believe Rds is really gzip or bzip. So it's more compact. I think that a smaller file would be faster to read. 
https://stat.ethz.ch/R-manual/R-devel/library/stats/html/relevel.html
Going to second the Python + opencv suggestion. It's is your best and most straightforward option. MATLAB works too, but you'll want the image processing and computer vision toolkits. 
Are there suppose to be goals for learning R. please let me know what you think 
I frequently use combinations (and frequently get to R's limit with them), will try when I get home!
Let me know how you get on. I've got so deep into the project that I've run out of further possible speed optimisations to perform. The downside to this is that it's now very difficult for me to see the project with fresh eyes. When I say fast... https://github.com/mm0hgw/combnGen/blob/master/combnGen.Rcheck/combnGen-Ex.Rout https://github.com/mm0hgw/combnGen/blob/master/combnGen.Rcheck/combnGen-Ex.timings The example that takes the longest time is the one that compares the output of combn() to the output of combnG() over n&lt;-20 &amp; k&lt;-10. The ~1.7s of runtime breaks down into ~1.5s for combnG(), ~0.15s for combn() and ~0.05s to compare the results. Deliberately overflowing the L2 cache to match the output of combn() is atypical usage for the combnGen suite. The point is to provide the facility to both expand an index number into a combination, and to contract a combination into an index number. The index numbers take up less space in the L2 cache and the pipelines to and from main memory.
You might want to provide some context on the data. Maybe try running `head` to let us know what you're working with?
I can't see making it much easier. as_tibble(rownames_to_column(mtcars)) You could always write your own `as_tibble()` function that checks for rownames and calls `rownames_to_columns()`
What is your hypothesis? As /u/dataperson asked, what does your data look like?
Hi, please see my reply to /u/dataperson above!
Ah, it seems I've made a terrible mistake 😅 Try running head(your.data.frame) to see the top 5 rows or so of your data frame. I meant to ask so that we can get some context on what exactly we're working with. That said, whenever you have a dataset you should probably come up with questions you'd like to answer (ideally you'd have questions before sourcing the dataset, but I digress) so that you're not stuck. One question I often ask when I get stuck during an exploratory data analysis is "If I could know anything about this data without having to do any work, what would I want to know?" Hope this helps!
I will post there. Thanks!
Wait, what's this tibble you speak of? Edit...googling: From [CRAN](https://cran.r-project.org/web/packages/tibble/vignettes/tibble.html). Thanks OP! I'd never heard of a tibble. 
https://blog.rstudio.org/2016/08/29/tibble-1-2-0/
Because `as.data.table` does it like this (keep.rownames = TRUE), and `data.table` is the enemy (i'll just show myself out...)
What are the pros and cons of this vs data frames, and why ween people off rownames?
You have 2 replicates for control and 2 for experimental group. Following on to see other comments
I'm new to R coming in with stats/econometrics familiarity but solely Stata background. I'm using this book now and am pleased: http://www-bcf.usc.edu/~gareth/ISL/ I plan to move on to the follow-up book once I finish with the intro: https://statweb.stanford.edu/~tibs/ElemStatLearn/ Both of these books are available in pdf format for free, but i splurged for the $60 hardcopy to have a version to dog ear, mark up and highlight
A good place to get started with a very comprehensive set of tutorials is to download the swirl package for R. Then call the library and type swirl() to begin. It's great in that it's interactive so you're working on examples as you go. You'll get a good primer in a range of things like data structures, cleaning and exploring data, plotting, data manipulation, a full course on regression etc. Not too heavy on machine learning but I believe you should do the foundation stuff first and this package has it all in one place. 
Appologies in advance, on a plane with garabage internet so I can't get the links, if you think this is helpful let me know and I'll provide links later. Since you asked about libraries. You can do almost all existing machine learning operations for classification and regression via the caret library. It gives you a framework for implementing and hyperparameter tuning of Neural Netorks, SVMs, Ridge, Lasso, MARS, and a bunch of other stuff I can't remember of the top of my head. caret functions through repeated sampling while modifying model parameters to minimize or maximize a model evaluation cost function e.g. RMSE, basically what data scientists used to do for hours anyway, now we can run a bunch of caret scripts and grab a coffee (or browse reddit) while it figures out what works best. That being said, it's useful to know what the heck you are actually doing so as not to do something silly. For that you have to know the maths behind machine learning as opposed to just the code. For that, the coursera mooc from stanford by Andrew Ng is the absolute best blend of code and math I have come across. That course uses MATLAB instead of R, but that doesn't really matter. It's a good course to take for two reasons: one, the aforementioned prevention of silliness, and two, he spends a good deal of time talking about how using vectorwise operations can solve simulations equations very quickly. (That will make sense after you take the course.) Understanding how to do matrix and vector operations on your data (that is applying a function across your dataset instead of looping over it by row with a 'for' loop) is the learning curve that usually becomes a solid brick wall when folks go from development to R. And now that I've said all that, depending on your current language, R might not be the best language for you anyway. Pyhon just as accepted and typically runs faster. Scala gives you even more cred. And being able to implement even the basic machine learning via MLlib through a spark api (scala, pyspark, sparkR) is pretty much a guaranteed job. Since that last sentence was recommending SparkR in a paragraph saying you didn't need to use R, I think I've gone far enough. Let me know if you want to know anymore about anything I've rambled about.
It isn't a book, but there's a udemy course you may find interesting [here](https://www.udemy.com/data-science-and-machine-learning-bootcamp-with-r/learn/v4/). I've just started it, but it has a fair amount of machine learning towards the end of the course.
I don't see why not, it looks like R, Python, Scala, and several others all possess the capacity for ML
I have this: Machine Learning with R - Second Edition https://www.amazon.com/dp/1784393908/ref=cm_sw_r_cp_api_7TMEybJSEQZED I reference it often. Basic explanations plus use cases. Includes example code and data sources to get you going. Not in depth from a math/stat perspective but a great starting point. 
The first one looks like it should do the trick. Thanks.
Vlookups in excel are terribly inefficient compared to the same join functions in R. In tests I've done my machine, runtime is roughly 100x in excel vs R for the same join, so you would want to push that piece of processing into R/Python at the very least. R won't have any issues with joins / plotting a half million rows even with a very old computer, so you don't need to worry about loading it into a SQL database for example. Assuming you can do the data pull in R instead of Python, there's no reason you can't just write a single script that will do each of your 5 basic steps outlined and drop the graphs as .png, markdown, or even .ppt (using the 'ReporteRs' package) files into a folder on your machine. So you would just open the script and run it. Alternatively, you could make a .bat job to trigger the script on a weekly basis and drop the files in a folder, so you would only need to deal with uploading to your presentation portal in a manual way.
Spitfiring a quick flow here after a few beers so forgive if I miss anything... 1) Manually clean your log file, store in location. Import into R data frame. 2) Use googleAnalyticsR to query and get your GA data into R. 3) Use tidyverse to do any further cleaning/manipulation/joining for analysis prep. 4) Run regression model on the data and output results into final data frame. 4) Create graphs with ggplot2, connect to ftp in R and upload new graphs into your bootstrap site. You could even do some googleVis, plotly, or Shiny work to have it dynamically refresh depending on your web environment...if you want to get fancy. You could schedule a job to run this script at X time every week. Just have your manually compiled file ready to go by that time and boom. I would save some of the outputs along the way (csv or database) so you can keep logs of calcs etc.
Yeah - my goal is to completely remove Excel from the process. If I were to keep all of the data in cloud, what would be the best way? I currently use the googlesheets library. Would it be worth my time going to an SQL system? If so - why? 
Thank you for your reply! For the sake of making everything run in R I could re-write my GA pull. What would be your recommendation for cloud storage options?
Google Sheets may work for the POC. Take a look here for some cool options for using Google Cloud services and R for query/storage/reporting: http://code.markedmondson.me/digital-analytics-workflow-through-google-cloud/
Nice! I'll have to look into this after finishing Wickham's new book. Very envious of everyone who got to go to rstudio::conf as well. :D
 Proposition &lt;- function(Name, Address){ emailto(address=Address, msg=paste(Name, ", Ayy bb wan sum fuk", sep="") } for(i in 1:nrow(Girls)){ if(Girls$site[i] == sexfabb){ Proposition(Girls$Name[i], Girls$Address[1] } } That should achieve what you want
SQL is probably overkill unless you have another use case that's not in your OP -- I would probably start with dated and backed up CSVs and manage scope extensions as they come. Are you interested in eventually building this into a query-able data warehouse, maybe have multiple users accessing the data? If so SQL might be a good tool to look into. 400k rows is small for R, but somewhat big if you're using Google Sheets. Going 100% cloud based could be tricky, depending on your vision. You can store the data in Google docs, and just pull/upload from GDocs as needed, but you may run into cell count limitations and other performance issues if you need to scale. Amazon Web Services offers a fairly straightforward implementation, since you're pretty much just running your same R script on a remote server. There are a lot of cloud based database-as-a-service stuff as well, for example Google Cloud SQL, but I haven't used any of them.
1. Options goes in options() - which is basically your google auth credentials from Google Developer Console 2. When you use [] you're basically doing [Row,Column] so 23rd Row in this case, though it looks like your column name is wrong 3. That page has working code, but you'd need to sort your credentials first
The word choice "obscure" was a pretty bad one in the final version of that post; originally I went into what actually causes the most obscure errors, which is when the file is nonstandard (not RFC-4180) or even invalid in some places. I've seen all sorts of weird things like control characters get printed on single lines in 2 million row files. Then on top of that the csv file doesn't include its encoding, which can be a problem sometimes. Then finally the csv file doesn't include its column spec, so that has to be guessed as well. There are lots of strategies for guessing encodings and column types and they vary between tools and platforms so oftentimes the files end up being parsed improperly - or worse, parsed improperly some of the time on some systems. That's what I mean by "obscure errors". 
There are many options for different use cases. - R Packages. Suppose you write a function to produce a particular plot and you find yourself reusing this function many times with slightly different parameters. You might want to consider creating a R package to store this function. You would use `library` to load your functions like any other package, and you would be able to generate readable documentation for your work. See Hadley's book: http://r-pkgs.had.co.nz/ - `source` function. Say you have some logic that you would like to reproduce often, but maybe not indefinitely (i.e. 2 months from now). You can place this logic in a R script (.R) and run this code from a different script using the `source` function. For example, let's suppose you have two scripts `define_variables.R` and `print_results.R` `define_variables.R` x &lt;- 1983 y &lt;- 2 `print_results.R` source("define_variables.R") z &lt;- x * y print(x) In `print_results.R`, we have the values of `x` and `y` because we defined those variables in `define_variables.R` and used the `source` function to essentially run that script. - Parameterized RMarkdown. Let's suppose you have the same logic you would like to reproduce with a slight change in parameters. RMarkdown allows you to create a "template" document more or less, where you can pass in different inputs. http://rmarkdown.rstudio.com/developer_parameterized_reports.html 
I would also consider saving your data in CSV format in addition to binary R format. Binary R format makes it easy for *you* to read in data, but it is not the best format in which to share data with others. If you were working in Python, using binary format to save data would be frowned upon, because the standard library changes a bit more often than the R standard library does. 
The more I read from you guys, the more I understand I have a long way to go... I think that a package will suit my case. For now, I just copied and pasted different R Notebooks (I like the interface) and slightly modified each parameter as, for example, the name of the group I want to plot (healthy vs disease1 or healthy vs disease2) or the variable I want to plot from the same dataset (body temperature in healthy vs disease1 or lifespan in healthy vs disease1). My main concern is that, now that I started producing nice (imho) graphs with ggplot2, I noticed the problems I have in data- and project directory organisations.
Very nice tip, thank you. 
Learn to use Makefiles. It takes an afternoon to read the manual but the time is well invested. Once you have a basic makefile, rerunning a script is *also* just a click in RStudio. Or a simple command on the command like, whichever you prefer. But you will prefer the command line, because it allows you to trivially rerun more complex, multi-part analyses, something you cannot do with a simple button click in RStudio. Here’s an example of how you can organise a complete project with Makefiles: https://github.com/klmr/example-r-analysis — the project includes both reusable R modules and a report output generated from R Markdown.
For next 6 months I will be small. But I could have multiple clients after that - so I need to scale this in the future. May be best to build it out with multiple user capabilities now. 
Thank you for the help. It's challenging to start programming from the scratch as it seems very stupid and unprofessional (imho) to constantly go on the web to check the solution for problems I cannot solve. Time to learn Makefiles then...more food for thoughts. :)
I agree with all this advice ;)
First of all you should learn how to format your code: indent all lines four spaces: occurrencelocations &lt;- "C:/Users/Lauren/Documents/USE/redpandas.csv" occurrencelocations &lt;- read.csv(occurrencelocations,header=TRUE) head(occurrencelocations) maxent &lt;- "C:/Users/Lauren/Documents/maxent.jar" outdir &lt;- "C:/Users/Lauren/Documents/tryvt" gridfolder &lt;- "C:/Users/Lauren/Documents/clipped" backgroundlocations &lt;- "C:/Users/Lauren/Documents/USE/redpandapoints.csv" additionalargs="nolinear noquadratic noproduct nothreshold noautofeature" contributionthreshold &lt;- 5 correlationthreshold &lt;- 0.9 betamultiplier=seq(2,6,0.5) library("MaxentVariableSelection") VariableSelection(maxent,outdir,gridfolder,occurrencelocations,backgroundlocations,additionalargs,contributionthreshold,correlationthreshold,betamultiplier) The problem is that you use the variable (`occurrencelocations`) for the path and later load the dataset into the same variable. So after running `read.csv` the variable no longer holds the path but the dataset instead. And that is probably what (I'm guessing this is where the problem arises) `VariableSelection` is complaining about.
R can do this, and do it faster than Excel. I do not know what level of experience you have with R, so I will include some helpful informaiton. First, get R Studio. Next, I am going to assume your dataset is currently in excel, since you want to use it. The absolute easiest way to import excel into R Studio is by pressing "file" -&gt; Import Data set -&gt; From Excel -&gt; Select your file. Now your data set is imported into your environment (the upper right panel in R Studio) Please examine your data by clicking it. Scoll to the end, and check if R has imported blank cells (this happens with this import method, people might suggest other methods using packages, but its pretty easy to exclude the empty cells. If you intend to automate this task, like if it will be done often, then you will have ot revisit this.) To exclude the empty cells, or rather select only the cells you want, I will use Data as the name of the Data set: ` Data &lt;- Data[(1:the last row number you want included, ] ` Now you should have your dataset ready for analysis. When you're done, use this to export back to excel, if thats what you're into: ` write.xlsx(Data, "Data.xlsx", sheetName = "Data", row.names = True, append = True) ` Notes on your analysis: I am not sure about concatenating the number and the call reason. What is the use of having the number and reason in the same cell? That is not tidy, not tidy at all. If you already have the number and reason in separate cells, for the same observation, keep it that way. The if they have called in the last 6 weeks will lead you to the lubridate package. That will let you extract the week from any date, and that can be compared against the current date. Or you could do that all with the use of days, but it could be more work to figure out the logic. But I think lubridate might have the weeks begin on mondays and not Sunday's because its dirty dirty international standard which can go to hell USA USA USA To create a subset of only those observations that fullfill this 6 week rule, use something like this: `newdata &lt;- Data[which( code out your requirements)] ` The last portion of your query deals with forecasting, and that's a whole thing. So I will assume you did not mean forecasting, and that I have misunderstood 
Here's a mock up of what it sounds like you're trying to do. It took about a minute to run on my laptop: library(tidyverse) library(lubridate) cutoff_date = ymd('2016/7/1') data_frame(phone_number = sample.int(999999, size = 700000, replace=TRUE), reason_code = sample(LETTERS, size=700000, replace = TRUE), call_date = ymd('2017/1/17') - days(sample.int(300,700000, replace=TRUE))) %&gt;% mutate(in_next_6 = call_date %within% interval(start = cutoff_date, end = cutoff_date+weeks(6)), in_last_6 = call_date %within% interval(start = cutoff_date-weeks(6), cutoff_date)) %&gt;% group_by(phone_number, reason_code) %&gt;% filter(!any(in_last_6), in_next_6) %&gt;% summarise(calls_in_next_6 = n()) 
Your .onLoad() is a function like any other. Therefore, the variables you defined inside that function are stored in their own environment and will be destroyed when the function exits. (See the environments chapter in Hadley's free online book advanced R if you don't understand environments and functions.) Luckily, everything you want to do is easy to do without using onLoad. Just define everything in the main package namespace and then export your getter and setter functions. 
Thanks! For some reason I guess I mistakenly thought .onLoad had the special property of attaching the objects defined within it to the package namespace, but didn't export them. So yeah, I guess I'll just define all of these in the main package and not export the getter and setter functions (since they don't need ot be used outside of the package). 
Neither R nor Excel will process anything without your intervention, giving you a ratio of Infinity/Infinity. Apply l'hopital's rule to get a better estimate of the time savings.... Seriously, the tool's only as good as the user using the tool. I love R and data.table, but if you're great at excel, don't switch just because one's "faster". I can almost guarantee you'll make mistakes switching (in either direction). That having been said, R is the right tool for bigger data sets like that. 
Thanks alot. Currently, I have learn MySQL for database management. 
This is a trivial task in R. Really though, it sounds like you should probably be querying a database. 
Wow! thanks for your very thorough answer. I should have mentioned that I am familiar with R studio as I just completed an 'Introduction to R' module of a data analysis diploma, but the data sets we were working with were relatively small compared to this one. Regarding the concatenation, I should have said account number rather than phone number, but I might run the query based on this number alone. Adding the reason code was to see if a customer was phoning about the same thing over and over but I don't know if I trust the agents to use the reason codes consistently. Can I ask what you mean by the 'lubricated package'? Also the day of the week doesn't matter, we're looking for the main drivers of repeat calls and to identify customers who are phoning the help desk unnecessarily, so the days they phone don't make a big difference.
Thank you for this. I will need to take a few minuted to look all of the commands to interpret what you've put together but thanks for the help.
Yes as I've started to work with larger data sets I've come to realise Excel is the devil, which is why I've shifted to R!
I don't understand what you mean.
&gt; Excel has to move the numbers into new cells every time you delete a row R has to do the equivalent work. R *is* faster than Excel but not for this reason; and in fact deleting rows in a data frame is just about the least efficient fundamental operation you can perform.
As someone trying to transition from MATLAB to R this looks like a fantastic resource. Thanks!
What makes this guide “definitive”? More tangibly, why would one start an introduction into the language with irrelevant and uncontextualised low-level details like reserved words, operator precedence and constants? I could go on. The whole setup of this guide is not well suited to teach the language: It teaches control structures (including rarely-used ones like the `while` loop) before even teaching how to assign a variable. It teaches utterly useless concepts (like the obsolete reference classes) before mentioning plotting, which is an absolute fundamental in R. This isn’t how you teach programming effectively. The website claims it has the “Simplest programming tutorials for beginners”. I’m sorry but this it ain’t.
what's the benefit of learning bash scripting?
Automation. I’m a bioinformatician and I easily do more bash scripting than programming in all other languages combined. Not all (not even most) of my bash code is written to a file (most is ad-hoc usage on the command line) but it’s immensely useful.
What is the reason?
Is bash just the Linux terminal but in a script?
When testing for multicollinearity, you generally build a correlation map of all variables to be included in your model and then discard variables to your discretion. Summary holds no value to my knowledge. Perhaps you're referring to the adjusted R squared value.
For anyone finding this in the future - this works on my machine to get non-sampled GA traffic: install.packages("googleAuthR") install.packages("googleAnalyticsR") library("googleAuthR") library("googleAnalyticsR") #Go find your client secret and id - put them in the proper spot here options(googleAuthR.scopes.selected = "https://www.googleapis.com/auth/analytics") options(googleAuthR.client_id = "4058XXXX388-XXXXX5pgn4ciehoXXXX7jp41XXXXhjg6.apps.googleusercontent.com") options(googleAuthR.client_secret = "gBw5XXXcccpXXXLPJZXXXX7b") ga_auth() ## get your accounts account_list &lt;- google_analytics_account_list() ## pick a profile with data to query - 22 is my row number found in the data of account_list ga_id &lt;- account_list[22,'viewId'] #Find the gaid for the segment by logging at segs (Uncomment next two lines) #my_segments &lt;- ga_segment_list() #segs &lt;- my_segments$items segment_for_call &lt;- "gaid::-Rz97tkaS1KPGX61_8LILw" seg_obj &lt;- segment_ga4("TVtraffic", segment_id = segment_for_call) sessions_data_fetch &lt;- google_analytics_4(ga_id, date_range = c('2016-04-06', '2016-06-16'), segments = seg_obj, metrics = c("sessions","goal3completions","goal4completions","goal5completions","goal6completions"), dimensions = c("dayOfWeekName","year","month","day","hour","minute","segment"), anti_sample="TRUE")
Thanks for the reply, I'll certainly look into that. My only concern of all of these beautiful workflows is that in wet science the publication period, or at least the moment when you are making figures for publication, can be very distant from the moment you analyse the data. Even years can pass between the two, and you have no idea on which the results of the study will be and to which journal they will be submitted and/or published. But this is probably just me that I'm speculating on processes I've never seen at work IRL. The header.R with all the paths is very interesting. I imagine you first define the working directory, and then everything is relative to that directory, isn't it? Thanks again for the suggestions!
Yes I see your point on workflow with timing. Just try your best, or find something that works well for you. The numbering may not be the best in your instance due to the length of time between receiving data and publication. As for the header.R program this has the absolute path to a particular data location. If you look back at my header you will see I have it defined on my computer as: "C:\Users\Nick\data\of-dollars-and-data" where I have all of the data for my blog posts. Inside of the "of-dollars-and-data" folder I have additional subfolders that are also referenced in the header.R program. The nice thing about a header is that if you ever need to move all of your data you do NOT need to update every individual program afterwards. Since your programs call your header.R file and use relative paths, you only need to update the header.R file. It is very convenient in these instances.
Google: market basket analysis / association rule learning / apriori algorithm. KDNuggets write up: http://www.kdnuggets.com/2016/04/association-rules-apriori-algorithm-tutorial.html You can easily implement this in Excel, but I can't refer you to any R or Python resources. 
Learn both List and Apply functions via YouTube. Then use those functions to create (essentially) a workbook of the data. Good luck. 
Are the csvs in a consistent structure? When you say 'blank' do you mean it's just the header? If that's the case, you could use SparkR interface for spark 2.x (doesn't need hadoop) it's designed to read entire folders full of csvs and it is lazy compiled so you won't pull a giant dataframe into memory until after it's been filtered.
Hey thanks, this works but with a snag. I found a few files that don't have similar columns. Is there a way to do this but limit the number of columns to be read? Alternatively something a bit more flexible that just rbinds everything regardless of columns?
Look into 'rbind.fill' (??rbind.fill); should do the trick! 
It reads a folder using a spark session sql context, spark 2.x changed the syntax in sparkR a bit, but added additional csv capabilities. But since it's designed to be used in hadoop, where you can have petabytes of flat files, it doesn't care how many you add, but they will either need to have consistent names for the columns you do want to access or you need to ignore the headers entirely, which could be risky if you have inconsistent data. I would try to first run some basic housekeeping operations to get rid of the empty files, then you can try something simple like this to see if it works. (Once you have Spark installed, spark added to path, and the spark R bindings added you your .libpath) library(SparkR) #this will be present automatically after updating your .libpath SparkSession() df &lt;- read.df("file:///path/to/csv/directory/*", "csv", header = "true") You can also pre-define your schema with structFields so it's only going to try to read the fields you want, since you mentioned elsewhere additional columns were present elsewhere, you can set your schema manually before loading, along with saying "header = 'true'" this *should* (I'm not a 100% on this one) ignore the header and the columns you don't include in the schema: library(SparkR) sparkSession() schema &lt;- structType(structField("name", "string"), structField("info", "map&lt;string,double&gt;")) df &lt;- read.df("file:///path/to/csv/directory/*", "csv", schema, header = "true") Now you can do normal dataframe operations, just like in R just_one_column &lt;- df[, df$name] You can also run sql on your data set easily: createOrReplaceTempView(df,"tempTable") sql_results &lt;- sql("SELECT * FROM tempTable") I know SparkR may be a bit of overkill for this situation, but once you get it going in your environment, it's crazy powerful!
you could potentially concatenate the files using shell commands
Not a problem, happy to help. So are you planning on just leaving the csvs where they are or are you merging them each week then deleting? If you are leaving the csvs then spark is great for that, however you'll be up to 1.5 to 2 TB in a year, so you may need to start thinking about hadoop. Also, I would suggest python pyspark for this instead of sparkr since you mentioned memory issues. Look at Jupyter or zeppelin notebook configurations for python (if you haven't already) to give you a similar workflow as with rstudio (working in blocks of code instead of one giant compiled operation). If you are merging then deleting this process is fine as well, as long as you configure spark to spill to disk if memory overflows. I strongly suggest python or scala if you're doing this operation as they have less overhead and are easier to schedule.
Does anyone else find the name odd? I don't think of R as anything related to software development. Roger Peng's courses on Coursera were helpful from what I recall though.
I haven't read the book or seen the files. From a purely monetary point of view, I would say that getting the files/datasets might not be worth $20 (again - I haven't actually looked!). But in the R world, a lot of people write books and give them for free, and opting to pay for them is just a way to show your support. Hadley's book and many others are the same way
I was just telling someone this: any R book you buy will be outdated almost immediately, so keep that in mind.
The above is great advice. I would add: 1/ get generally comfortable with working in a UNIX environment 2/ rather than Java, learn Python and Haskell. Python because it is more common in data science and easier to get into than Java. Haskell because you are new to programming and you will find learning Haskell gives you a much broader idea about what computation means at such an early point in your career, before you are so invested solely in the imperative paradigm (which Java, Python, C++ fit into).
Both advanced R and the art of R programming are available for free online.
Ah, ok... I'm pretty new to R. I'm learning via this hobby project. &gt; What is your data? Text messages &gt; What are you trying to do? Just analysing - wordclouds, frequency of chats, time of day etc &gt; What package(s) are you using? tm, SnowballC, wordcloud, RcolorBrewer
Okay, great. A little bit more specificity would make it easier to help. So, what function (and from which of those packages) are you using to "remove stopwords" from your data? An example of how you're calling the function might help. And, specifically, what is the output of that function call and what would you like the output to be?
I'm not sure if this is quite what you want, but it seems like you want to use an outer product to get the y's into an n-by-n matrix. set.seed(0) x &lt;- matrix(sample(1:10, 16, replace = TRUE), nrow = 4) yi &lt;- sample(1:10, 4, replace = TRUE) yj &lt;- sample(1:10, 4, replace = TRUE) sum(x - (yi %o% yj))^2 This results in `[1] 439569` with my example. If so, then I see no need to define a function, but if you're so inclined: myFunction &lt;- function(x, yi, yj) { sum(x - (yi %o% yj))^2 }
Thanks!! Got it to work now... we used a lot of simple profanity, like "idiot" and "stupid" and vernacular forms of "dude" but they weren't showing up in the wordcloud. I assumed the stopwords library removed them. But specifically including them in your "keep" vector shows them in the wordcloud now. Thanks!
Thank you! I've downloaded R Studio (I think? And for some reason it downloaded two of them) as well as two packages my professor recommended (...I think). I'm trying to use data sets my teacher uploaded but can't seem to figure out how to do that. I've tried downloading the file and using &gt;data ("file name") but it responds that there is no such file. Perhaps I should be using a link to the data set online? What are your thoughts? (Note: reddit formatting doesn't show the little arrow, but that's what I used.)
If you've downloaded the data, you need to have it in your working directory and link to its place on your computer. To get the spot of your working directory you can use the line getwd() And you can change it in R Studio easily in the bottom right of the window. There are very easy tutorials for that online. Afterwords you should be able to get the data. 
Will do!
Hey man, you would have to be a bit more specific as to what Y hat is in your statement. Look I like baseball maybe PM me a picture of your assignment or something?
&gt; data ("file name") Sooooooooooo.... you're gonna have to become familiar with the concept of "functions" in programming languages. They're similar to functions in math. A function is defined, and then it can be used to do work and it takes inputs from the arguments/parameters you pass into it. You used the `data` function, which is (in fact) a function, but maybe not the one you want. Here's the help description for `data`: Loads specified data sets, or list the available data sets. I did that by typing `?data` into the console of RStudio. So `data` is good at loading datasets, but it can only load ones which are "available" in your R distribution. Type &gt; data() To see a table of the default R datasets, such as iris and Titanic, which are so commonly used as "example" datasets that they've become iconic in the dataviz world. To load a specific file, you'll need to know what type of function will read/parse the format of your file. Supposing it is a .csv file, then you can look at `?read.csv`: Reads a file in table format and creates a data frame from it, with cases corresponding to lines and variables to fields in the file. If you have a different type of file (such as Excel) you'll need to find a function that can read that type. 
This makes so much sense! Okay. My teacher doesn't specify what kind of files his datasets are, so how could I figure that out?
I'll be sure to check it out. Thanks!
Hey. I use R programming frequently and I am also no expert in it. But I like doing it because it's easy to use. Believe me you'll love it when you get used to it. After installing R Studio, google on how to install swirl package in R Studio. And then google on using the swirl package. It's the best learning package out there for R beginners. It's a walkthrough in R studio's command prompt on how to learn R. It makes you code by showing how to code and it's quite understandable. Install that and go through the package. I am sure you'll find R easy enough. Good Luck!
The simple answer is just to use a while loop, and repeat the check in each iteration. A more R-like approach, and probably much more performant for this, is to get the number of 'empty' rows using a vectorised function, assemble the required additional data frame and rbind it on in one go 
Everyone here recommends RStudio like it is the only R IDE out there completely oblivious to the harmful monopoly they help to create. I use RKWard and I believe it is way better IDE.
I'm not familiar with that but will look into it :)
I'll check that out, thanks for the input. Good luck with figuring that stuff out! :)
It isn't a harmful monopoly, because there are other editors. Not sure if you mean create a monopoly on features like notebooks, for that I could understand. It isn't like rolling out your own IDE would take that long, since R has all of those environment functions. 
datacamp.com. 
+100000 to that site!
If you're used to SPSS try Rcommander it's a GUI menu based system for R. 
If you can be more specific about what a "missing" row is, why it's necessary to add them, and where the new vector comes from, it's highly likely there is a way to do this that completely avoids using a loop at all - or, someone has made a package to solve the problem already. Or both. Also, isn't this a case where the evaluation isn't lazy enough? Since the evaluation of nrow(df) happens once, at the start of the loop - when it's needed to determine whether or not the first iteration should happen. It happens because 1:nrow(df) is actually a vector of all the integers between 1 and nrow(df) and the for loop runs once for each element. That vector has to be built at the start of the loop, and it could evaluate to an empty vector, in which case the loop wouldn't run at all. That's the reasoning behind this behaviour anyway. &gt; for(i in 1:5) print(i) #basic loop [1] 1 [1] 2 [1] 3 [1] 4 [1] 5 &gt; for(i in c(1, 5, 7)) print(i) #this time omit some integers from the vector - still works because it's the exact same process [1] 1 [1] 5 [1] 7 &gt; for(i in c()) print(i) #empty vector loops 0 times &gt; 
there are some spaces that say "NA" so we tried using the command na.rm = TRUE to remove them but not sure if that works. 
It looks like [this](http://stackoverflow.com/questions/7706876/remove-na-values-from-a-vector) is what you are looking for. So something like: *max(dataset, na.rm=TRUE)* 
&gt; If you can be more specific about what a "missing" row is, why it's necessary to add them, and where the new vector comes from without getting into too much detail, the dataframe, df, holds measurement data where each row is measurement data taken at each hour of a particular year. The for loop iterates over the dataframe and searches for missing rows of data and if it finds that a row is missing (an hour of data is not represented) it fetches the data from an external source and adds it to df.
I would expand the x and y axis to 0.5-4.5 expand_limits(x = 0.5)+ expand_limits(y = 0.5) + expand_limits(x = 4.5)+ expand_limits(y = 4.5) The circles are rather small and multiplying the size by a factor only changes the labels but does not increase the size of the circles. So I cannot figure out how to make the circles larger. 
Code is like poetry. It's not finished when there's nothing more to add, but when there's nothing more to remove. https://github.com/mm0hgw/combnGen/blob/v0.4.4/combnGen/R/hashChoose.R This whole file of code gained functionality transforming to this single function. https://github.com/mm0hgw/superChoose/blob/v0.2/superChoose/R/superChoose.R In the second version, the function is effectively vectorised, as the arguments are handed on to vectorised functions without interference.
The issue is with the shapefile itself. Most likely, the rest of the island chain is not part of the feature. You will want to create a non-contiguous feature by merging those features together. This can be done using the merge tool in ArcGIS or the merge selected features button in QGIS. This won't change the geometry of the islands, it will have the effect of grouping those features as one. That should allow the mouse over feature to highlight all the islands in that municipality.
How's this? https://www.r-bloggers.com/document-classification-using-r/
How are these repos relevant to OP?
I work in an office environment full of engineers who _love_ Excel (they are afraid of Access). And it is useful for presenting small data sets. The engineers can tinker with the filters and play around get a feel for what is going on. But then they inevitably try to keep some information in Excel and then expect me to perform some some complicated magic with it. But here's the thing. In Windows when you read Excel you use a built in part of the OS called the Jet. The Jet works with typed data but Excel is untyped. So it reads the first seven lines of your table and then guesses at a data type. Let's assume that your call reason code is mostly numeric, but occasionally you have one with a non numeric digit. The Jet will read those values as null. And I had a case just the other day where some engineer gave me spreadsheet with thousands of lines and way too many columns. A handful of the columns were blank for the first seven rows. Those whole columns were read by the Jet as nulls. So, if you data is already in Excel, do be careful. Make sure that nothing gets lost when you import into R. If it is being stored somewhere else, read from that source. But if you company is keeping this data in Excel, convince them to more it to a database or even Access.
The URL you gave redirects to HTTPS, but the HTTPS certificate that the domain uses [doesn't seem to be trusted on all platforms](https://www.ssllabs.com/ssltest/analyze.html?d=solarscience.msfc.nasa.gov&amp;s=198.122.199.235). Possibly the issuing certificate authority's certificate hasn't propagated to whatever certificate library read.table is using, or perhaps it's because the certificate is for xd12srv.nsstc.nasa.gov and this site is solarscience.msfc.nasa.gov. Probably your best bet is to ***temporarily*** turn off SSL authentication, since it doesn't matter in this case.
It should worry you that this is working since it means that SSL isn't working properly for you!
So, gotta start with the model. The model for this code is to generate nCk with guaranteed integer precision in as fast a time as possible. The first link is the corpus. The code there was run. It was observed that the middle function ran at a comparable speed to the more complex first function. Because it matched the model and was simpler, the middle function went on to be the only function in the second link. Of course, it's things that don't match the model that are of interest in statistics. The R code in your project will be doing: preparation of the data for comparison, calculation of the model for comparison or comparison of the two. The nature of the data in the corpus will dictate the necessary preparations, the reasonable models for comparison and sensible comparison techniques. If you know the task the code must perform, it's much easier to decipher the intent of the programmer. The requirements of the task would be the same if you were using a room full of undergraduates with slide rules rather than R. Measurement, expectation, comparison. It's the only way we prove things. 
I think you should look into dplyr – it sounds like this is exactly what you'd need. 700k phone calls (presumably 700k records/rows) really isn't all that much – are you sure you're going about this right? I'm working on a dataset of ~11 million rows right now using dplyr and a query usually takes 3 - 5 seconds. Also, even if it does take an entire day, why not just use AWS EC2 or a DigitalOcean Droplet? It's pennies per hour.
That's not a real shock. Unfortunately, I don't control IT policy on my machine. :-/
Can you provide more information? Also, I have found this cheat sheet to be very helpful for working with shapefiles and rasters in R. http://www.seascapemodels.org/data/ArcGIS_to_R_Spatial_CheatSheet.pdf
&gt; I don't control IT policy on my machine. :-/ Just a note, if you are concerned about this you can install `curl` for your own user (if necessary from source). It will install its own certificate store. If you configure your `PATH`s appropriately, R can be convinced to use these certificates.
That's just a nested list- pass it through unlist() to flatten it to a character vector 
I'm new to this. Could you please show me how? 
Just run unlist() with the output from the map call earlier - foo &lt;- map(tweets, function(x) x $text) bar &lt;- unlist(foo) And 'bar' should now be a character vector with all the tweets, suitable for passing into the sentiment analysis function 
Did you try my map_chr answer from earlier? That should mean you don't need this step
https://gist.github.com/tnvrsingh/d4112e7badd97f736f613ec19ffcb4a4
Yeah, that looks like it's probably emoji or other nonstandard unicode. I'm not really able to help much there, I'm afraid
Off the note but how do yiu pronounce dplyr?
It's based on plyr, which was named to call to mind pliers. So, D-plier. 
Try it in one line data.2012 &lt;- any.drinking %&gt;% select(state, location, bothsexes2012,females2012,males2012) %&gt;% mutate(difference2012 = males2012-females2012) 
I got rid of the emojis with the map_chr(tweets, function(x) x $text) Now I get this error: Error in tweetaftermap[, c(11:18)] : incorrect number of dimensions Calls: data.frame -&gt; colSums -&gt; is.data.frame Execution halted in this line: sentimentTotals &lt;- data.frame(colSums(tweetaftermap[,c(11 :18)])) Here's the updated file: https://gist.github.com/tnvrsingh/d4112e7badd97f736f613ec19ffcb4a4 
the underscores really convinced me for a second that this might be programming related
16gb isn't much RAM, really. You should upgrade. The only other way to reduce memory usage is to make sure you keep copying of the dataframe to a minimum, but I assume you're already doing that. R can consume a lot of memory when manipulating large data frames due to how many copies of things it makes.
What task exactly are you performing that requires all 20 columns to be in memory at the same time? Getting a representative sample depends on two things. Size and selection. The size is easy, opinion polls typically use a sample size of around a thousand from the population they're investigating. Selection is also easy. R provides a selection of random number generators. runif() is the one you want.
Yeah that's a classic example where a lot of RAM will be consumed - each operation is making copies of the dataset. It throws them away again and gc works as intended so you don't need to intervene to make the space free up once it's no longer needed. But the operation will still be very slow if it ends up needing pagefile space to make its copies while it's working and there's not very much you can do about that. [This stack answer](http://stackoverflow.com/a/612518) is a good overview of how this works in practice. You can do some finagling with what exactly gets passed around to keep the RAM usage down (for example, doing filter() and select() first to cut down the size of the dataset before doing mutate() or spread() - the exact best order of operations will depend on your task and your data) but it will be limited in effectiveness. Your only other options are more RAM and chunking your work into pieces.
I'm familiar with how to get a sample, the problem is that there are a few different variables in this set that I'd need to make proportional all at once, in order to make sure I'm building the calcs right. It's doable, just it's own little project. But clearing out columns I don't need for that particular calc is good advice, thank you. Is df$col &lt;- NULL the most efficient way to remove it from memory?
Anthropology/archaeology undergraduate research assistant. We use it, among other things, to analyze bones for evidence of stone tool use. 
It looks like I am mistaken. per u/fang_xianfu Edit: u/fang_xianfu did not say I am wrong, his answer just seems more righter than mine.
&gt; Is there no way to get an exact mean from the distribution? Use the formula. You can also calculate the mean using the densities s = seq(0.001, 100, 0.00001) sum(df(s, 2, 5)*s)/(1/0.00001) &gt; 1.651027 #about the same as 5/(5-2)
Your code doesn't do what you think it does. You can't `chain` columns with `|`. It only works on numerical, logical or complex. So by the off chance that your columns for the chemicals are logical indicators, you will be doing df%toxins %in% True or False. If the columns are character, your code will simply break.
a) put code into code blocks, see the formatting help b) your function should return something and you should then call the function instead of trying to access something you define inside the function c) nobody forces you to write the whole function in one line, trying to parse your code my best estimate is: pop.store &lt;- function(N0=100, a=.0005, f=.01, g=.00001, time.steps=100){ pop.store2 &lt;- matrix(NA,time.steps+1,1) pop.store2[1,1]&lt;-N0 for(t in 2:(time.steps+1)){ pop.store2[t,1] &lt;- ((pop.store2[t-1,1])+(pop.store2[t-1,1])*((((-a)*((pop.store2[t-1,1])^2)+((80*a)*(pop.store2[t-1,1]))-f))*(pop.store2[t-1,1]))+((g*(pop.store2[t-1,1])^2)*(pop.store2[t-1,1]))) round(pop.store2,digits=0) print( (pop.store2[t-1,1]) ) print(((((-a)*((pop.store2[t-1,1])^2)+((80*a)*(pop.store2[t-1,1]))-f))) ) print(pop.store2[t-1,1]) print( ((g*(pop.store2[t-1,1])^2)*(pop.store2[t-1,1]))) print("") } return(pop.store2) } results &lt;- pop.store(time.steps=3) plot(results,ylim=range(-1:1),ylab="Net Density-Dependence", xlab="N",type='b') head(results) your population never skyrockets, you instantly go into negative numbers because you subtract the squared population.... PS you don't need brackets around single terms...
I am on my mobile now so cannot give you sample code. Since you have not mentioned what you have tried it's hard to know where you are stuck and what help do you need? Do you understand how ggplot2 works? Do you know the Grammer of graphics? Are you familiar with polar coordinates? Can you draw a simple scatter plot in Cartesian coordinates assuming r as x and theta as y? If you do not know the answers to above then you need to start somewhere. Look up ggplot2 vignettes and examples and read up on polar coordinate system. 
Welcome, THE best way to learn is to do. So as others have said: explain what you did and where you think the trouble is. Give us a sample and do try searching online first. There is a crazy amount of tutorials online. Try r bloggers for instance. 
That’s expected: `save` is the wrong method here; it takes a list of R objects and saves them *under their current names*. So `.self`, in your case. What you actually want is `saveRDS` and `readRDS`. These save individual R objects. To load them, you need to assign them to something: myAccount$save('myAcc.rds') # Use saveRDS internally myNewAccount = loadRDS('myAcc.rds') --- On an unrelated note, Reference Classes is an outdated OOP mechanism in R. Unless you need to preserve backwards compatibility with some other library, don’t use it. Instead, use R6.
I have been working on the code in the image. I am still relatively new to R. It is my understanding that lapply works like a for loop, each item in the list is passed to the FUN then the results are stored in a list of the same size. Executing the above code though every element returned from my lapply is the same, the last set of data is duplicated for all the other answers. The for loop works as expected returning a different result for each. I added in the print statements in myfunc for troubleshooting. The for loop prints each successive gene like Gene_01, Gene_02,etc then a unique set of numbers. The lapply prints the correct gene string, but then prints the same set of numbers for each. I am a bit baffled by this behavior, what am I missing?
First of all, who posts code in an image!? I'm not typing all that out when I could just copy and paste it! Secondly, you need to make a minimally reproducible example. That is, an example that I can paste into a brand new R session and see exactly the problem you're having. You don't have that yet since you're loading an .Rdata file. Make a toy dataset that shows the problem if you need to. You also didn't show the code you're using to index the resulting data structures, which would show the actual problem. Perhaps your error is there rather than in this code. If I can't see the error myself, I can't help you. Since it seems like only myfunc() and the last 6 lines of code are necessary to create the error once you have a data structure to pass into that code, you could make this example more minimal too. The answer will be that the data structure you're iterating through is weird in some way. Perhaps some of the elements are objects and they have methods that don't return what you might expect them to return. But without seeing exactly what that data structure is, I can't help you.
Everything /u/fang_xianfu said. Also in general that guy posts some of the best comments on here, you could learn a lot going through his user page.
http://stat.ethz.ch/R-manual/R-devel/RHOME/library/utils/html/citation.html
I am currently working on a project with my professor and I am not entirely sure why he went with R5 other than mutability. I am sure R6 has this too. I am new to OOP in R so I have some research to do. But thanks for your reply and I will look into these functions!
Apparently not many people take the time to read through the text printed when you start R. Here is the relevant piece: R is a collaborative project with many contributors. Type 'contributors()' for more information and 'citation()' on how to cite R or R packages in publications.
Thanks for taking the time to help, but I am not sure I understand the solution. When I do lapply isn't "Gene_01" being sent to the function, followed by "Gene_02", and so forth. Then in myfunc this string is just inserted into the WGassociation call? The print(i) works so I am confused. 
This was a fun one to debug! TL;DR - this package sucks and the WGassociation function is absolutely terrible. The problem is because you're using get() in myfunc(). Actually the first time you run this code, the lapply doesn't run at all because there is no variable with global scope called i. &gt; ans2 &lt;- lapply(genes, myfunc) [1] "Gene_01" Error in get(i) : object 'i' not found The second time you run your code, it works, because the for loop creates a variable with global scope that's named i. &gt; ans2 &lt;- lapply(genes, myfunc) [1] "Gene_01" comments codominant dominant recessive overdominant log-additive Snp_01 - 0.71238 - - - - &lt;snip&gt; [1] "Gene_02" comments codominant dominant recessive overdominant log-additive Snp_01 - 0.71238 - - - - &lt;snip&gt; But because lapply doesn't change i, you always see the same result. You can see this better if you rename i in the for loop to something else, because that means that the lapply stays broken throughout multiple iterations. You can see the leftover variable called i in the global environment yourself after running the code. So what happens is your `get(i) ~ 1` call is eventually evaluated as part of a call to `model.frame.default` inside `WGassociation`. Because R is smart, and because the code is inside a formula, it has remembered that it should be evaluated inside the environment where the formula was created - that is, the local environment of `myfunc`. But the default argument for `get()` is `pos = -1` which means to begin searching 1 level up from the current level. This is where the for loop and the lapply methods diverge. In the for loop, this works - after a fashion - because the loop is creating a variable called i in .GlobalEnv as it's looping and it happens to have the value that we need. When get() runs it steps up one level from myfunc's environment and ends up in the global environment and finds the right value. Hooray! In the lapply example, get() steps up one level from myfunc, but it ends up in lapply's environment. There is no variable called i in this environment, so it steps up another level to .GlobalEnv and grabs the correct value from the leftover i from when the for loop ran on the last iteration. But it never changes i, so it repeats itself on future iterations. --- Now that we know what the problem is, how do we fix it? The short answer is I have no idea. The way you would ordinarily solve this is by signposting to get() where to go to find the variable, but this doesn't work. The reason it doesn't work is, in a nutshell, because the WGassociation function is really, really bad. Just... awful. In particular it does some absolutely bonkers computing-on-the-language shenanigans to create its call to `model.frame`. On line 10 it grabs its own call... on lines 13-15 it does some pretty ropy stuff to turn its call into a formula if it isn't one - but it only finds out if its argument is a formula by doing `grep("~",...)` on the argument code and not with is.call! - and then on line 19 it actually just straight-up swaps *its own call* for a call to `model.frame` and then evaluates it! It was at this point that I had to go for a lie down so I wouldn't have an aneurysm. I would guess that this function could be rewritten using the lazyeval package to be less silly, but it's not completely clear to me what the solution would be exactly. But it does mean that it's very difficult for you as a user to force where the evaluation will take place to make sure it happens inside myfunc. If you try to do it without passing WGassociation a formula, it will be turned into one using that bastard process on lines 13-15. And if you pass it a formula, you have already locked yourself in to a particular evaluation scope. So, honestly, I have no idea how to fix this, but it looks like your problems are caused by the author of WGassociation. I'd find a new package and ask the author to fix their problems if I were you. --- Here is an example I concocted of why this is so bad. Run this after running your code: &gt; x &lt;- "Gene_01"; WGassociation(as.character(x), data.s) Error in model.frame.default(formula = as.character ~ 1, data = data.s) : object is not a matrix The unevaluated call to as.character should not be being passed to model.frame! Why has the (x) been dropped from the code!? This is utter madness.
Thank you very much :)
It is amazing, thanks a lot :) 
Fun bonus with table using the "iris" data (pre-installed; you don't have to do anything to load it). table(iris$Species) table(iris$Species, iris$Sepal.Length) #note how awful this is table(iris$Species, iris$Sepal.Length &gt; 5) # much better!
I understand that you don't want the blank. That is how we learned during class, It is definitely easier to use the na.rm then put all the na's to blank. I think he was doing that to help us in future class sessions... Um, I guess I'm confused to what your code is doing. I see that you are creating a separate table for my age column, from there it seems like you are placing "ifany" into the NA so it won't be empty? Then you create a new dataframe named df of just that column? I'm trying to figure out a way where I can check how many null values (NA or empty) are in EACH variable without doing it for each variable separately. It seems like a for loop would be the most logical, I was thinking of creating a vector with all of the names of the variables and then iterate through them somehow..
&gt; x &lt;- c(1, 2, 3, NA, 10) &gt; y &lt;- c(NA, NA, 5, 1, 13) &gt; df1 &lt;- data.frame(x, y) #all this is just making fake data &gt; &gt; apply(df1, 2, function(x), length(which(is.na(x)==TRUE))) So if I am understanding this correctly I would create ONE date frame with all of my columns, 2 specifies I'm looking at columns, function(x) runs the specified function on my dataframe, and length will give me the amount of NA datamembers there is? I believe I need to count all of the NA datamembers AND all of the NULL or empty datamembers in each column so I would use: length(which(is.null(x)==T) 
If you're accessing it in a data frame (is `bo` in a rectangular format?) using `bo$Age`, it's most likely a vector column in a data frame. In theory, it could be a vector within a list, but otherwise it's hard to see how you'd be able to use the `mean()` function at all. Otherwise, yes, use `mean(bo$Age, na.rm = T)`. Skip trying to use "" to circumvent NA, and just tell the mean function to ignore NA values.
You could also do... colSums(is.na(bo)) and colSums(is.null(bo)) OR you could define a function: is.problematic &lt;- function(x) is.na(x) | is.null(x) and then colSums(is.problematic(bo))
Hmm ok I don't understand this. I ran as.POSIXct(time, format = "%H:%M:%S) but that adds today's date to the column as well, so now it looks like | TweetTime | | 2017-02-01 11:27:02 | I don't get why it has done that? I already had this format in my original "created" column. Does this mean that I didn't even need to split them out into separate columns to try and plot them? EDIT: Ok, while I don't fully understand it, I have been able to plot the graph once the time is converted to POSIXct format, using the code from the Stack Overflow questions I linked. My only problem is that i can't get the y-axis to start at midnight (i.e 00:00). For some reason it starts at 01:00. Do you know how I can specify the scale start at 00:00 ?
It doesn't make the variable global unless it can't find an object with that name. So in the following code, I define a variable `x` within the function, then use the `&lt;&lt;-` operator in a second function to modify the `x`'s value. If I call the main function and then call `ls()`, you can see there is no variable `x` defined in the global environment. myFunc &lt;- function() { x &lt;- 5 innerFunc &lt;- function() { x &lt;&lt;- x + 1 } innerFunc() x } 
[deleted] ^^^^^^^^^^^^^^^^0.6279 &gt; [What is this?](https://pastebin.com/64GuVi2F/72017)
[deleted] ^^^^^^^^^^^^^^^^0.2047 &gt; [What is this?](https://pastebin.com/64GuVi2F/62393)
Where do you work? :-p
I will give this a try, this looks like it will work for me! Thanks I appreciate the response.
Three comments. 1. `|` is a vectorised operation and the result will likewise be a vector. It’s never meaningful to use this in an `if`, and if somebody calls your function with a vector of length greater than 1, your code will fail. Use `||` instead. 2. The body of the code can and should be written as follows without any loss of readability (or performance): pdfX = function (x) { if (x &lt; 0 || x &gt; 1) 0 else 4 * x ^ 3 } (Some people prefer braces around the `if` conditional statements — go for it if you want. Personally I don’t buy the claim that it’s less error-prone.) 3. The function should be vectorised so that you don’t need `replicate`: pdfX = function (x) { ifelse(x &lt; 0 | x &gt; 1, 0, 4 * x ^ 3) } # Usage: pdfX(runif(runs, 0, 1)) … but apart from that your code actually works as expected.
Thanks for catching that. I was thinking after you replace the NA with "". test1 &lt;- c(1, NA, 2, 3, 4) # double test2 &lt;- c(1, "", 2, 3, 4) # character
Oh!, thanks you, did not know that, so, If I understood correctly, the code should be: SGD &lt;- function(...) { # Stochastic gradient descent # w &lt;- matrix(rep(0,3)) # ... update &lt;- function(x) { # Here we need to modify w w &lt;&lt;- # update w w } while (above.tolerance) { w &lt;- apply(data, 1, update) # ... } w } Is that right? Thanks for your correction.
You’re still updating `w` inside your `update` function now. Instead, `update` essentially shouldn’t exist; or rather, its name and purpose shouldn’t be to replace the variable `w` but to compute a result that is then assigned to `w` inside the loop.
Sadly no...
You need to add 'right = FALSE'. scores &lt;- c(65, 68, 68, 70, 71, 71, 73, 76, 77, 77, 79, 81, 82, 82, 83, 83, 83, 84, 86, 87, 87, 87, 88, 89, 90, 91, 91, 92, 93, 94, 94, 95, 98) hist(scores, main = "Test Scores of Students", xlab = "Percent Range", ylab = "Number of Students", breaks = 4, right = FALSE)
Yes, i look into it and it really is easy to use if you are familiar with the two concepts of product recommendation. I am still researching it though.. Still haven't tested it with the data i have.
I will try this when I get back to my computer.
 library(lubridate) if (day(mydate) == 3 &amp; month(mydate) == 7 &amp; hour(mydate)==0 &amp; minute(mydate)==0){ ... } 
right. we are using that package at the moment. we removed all other code as we found the above code snippet to be the root problem. still stumped.
Thanks for the help. I tried using the tips given in that Stack Overflow questions on the limits, but I keep getting an error, or else a really bad visualisation again. I think it's because you have to specify a date within the limits, but even when I try changing the range of the date, it keeps giving me either an error or an uncomprehendable visualization. I'll continue to look into it, thanks!
That did it - thank you. 
%&gt;% is the pipe operator, which you can get from the magrittr package. It takes whatever is on the left side of the operator and passes it as an argument to any function on the right side. This allows you to chain together a bunch of function calls. To steal an example from this page on [StackOverflow:](https://stackoverflow.com/documentation/r/652/pipe-operators-and-others#t=201702030023074638732) library(magrittr) 1:10 %&gt;% mean # [1] 5.5 # is equivalent to mean(1:10) # [1] 5.5 If you don't want to install magrittr (though you should!), just change the last lines of my code from: df$TweetTime &lt;- format(df$created, format = "%H:%M:%S") %&gt;% as.POSIXct(format = "%H:%M:%S") df %&gt;% ggplot(aes(x = TweetDate, y = TweetTime)) + geom_point() + scale_y_datetime(date_labels = "%H:%M") To: df$TweetTime &lt;- format(df$created, format = "%H:%M:%S") df$TweetTime &lt;- as.POSIXct(df$TweetTime, format = "%H:%M:%S") ggplot(df, aes(x = TweetDate, y = TweetTime)) + geom_point() + scale_y_datetime(date_labels = "%H:%M") As for the time-range: yeah, that was automatic. I did find [this though.](https://stackoverflow.com/questions/30607514/setting-limits-with-scale-x-datetime-and-time-data). Basically, to use my code as an example again: you have to define the hour interval in terms of today's date, like so: lims &lt;- as.POSIXct(strptime(c("2017-02-03 00:00","2017-02-03 24:00"), format = "%Y-%m-%d %H:%M")) And pass `lims` as an arg to the `scale_y_datetime` function, like so: ggplot(df, aes(x = TweetDate, y = TweetTime)) + geom_point() + scale_y_datetime(date_labels = "%H:%M", limits = lims) This lets me set a custom time interval for the y axis in the code above, but the year, month and day in the interval defined in `lims`have to correspond to the system date, for some reason. Hope any of this helps!
I think `&lt;&lt;-` will keep searching up through the environments looking for an object with the given label until it reaches a parent-less environment. If it doesn't find one, it will create the object in that global environment. Try this code: firstFunc &lt;- function() { secondFunc &lt;- function() { thirdFunc &lt;- function() { P &lt;&lt;- 9 } thirdFunc() } secondFunc() } fourthFunc &lt;- function() firstFunc() P fourthFunc() P 
The [official tutorial](https://shiny.rstudio.com/) is really good.
I have done quite a number of projects using R and Shiny and I haven't found much beyond the tutorial. There's the Shiny Configuration page: http://docs.rstudio.com/shiny-server/ And this is a good guide for hosting your own shiny server: http://deanattali.com/2015/05/09/setup-rstudio-shiny-server-digital-ocean/ But don't focus on Shiny, R is where the value lies. So learn R. Shiny just makes it easy to host analyses and visualizations. But you need to be able to make stuff worth hosting.
I have my Biostats book floating around somewhere in r and was where we did GLMM. Let me look for it tomorrow and if I can find it I'll forward it to you.
Is Pond a numeric identifier? Convert it to a factor and then try plotting.
try converting Pond to a factor first. Amap$Pond &lt;- as.factor(Amap$Pond)
I've used [plumber](https://github.com/trestletech/plumber) to make an API that a website uses to do analysis. Its worked out well for me. 
That was the answer! Thanks for the help!
Check out the dplyr package! # install.packages('dplyr') library(dplyr) If you want to reduce the mpg dataset to a table of unique manufacturers and models, you can do it like this: # install.packages('dplyr') library(dplyr) data(mpg) distinct(mpg, manufacturer, model) The output will be a table that looks like this: # A tibble: 38 × 2 manufacturer model &lt;chr&gt; &lt;chr&gt; 1 audi a4 2 audi a4 quattro 3 audi a6 quattro 4 chevrolet c1500 suburban 2wd 5 chevrolet corvette 6 chevrolet k1500 tahoe 4wd 7 chevrolet malibu 8 dodge caravan 2wd 9 dodge dakota pickup 4wd 10 dodge durango 4wd # ... with 28 more rows If you want to calculate summary statistics by group, you can do it the following way: foo &lt;- mpg %&gt;% group_by(class, year) %&gt;% summarise(mean_displ = mean(displ)) The output of `head(foo)` looks like this: Source: local data frame [6 x 3] Groups: class [3] class year mean_displ &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; 1 2seater 1999 5.700000 2 2seater 2008 6.466667 3 compact 1999 2.292000 4 compact 2008 2.363636 5 midsize 1999 2.720000 6 midsize 2008 3.114286 Cars are grouped by type of car (class) and year of manufacture and engine displacement (displ) is calculated for those levels.
I'm not entirely sure what you're trying to do from the description. The aggregate() function is handy for grouping by variables and applying basic functions. `aggregate(data = mpg, hwy ~ manufacturer, FUN = summary)` `aggregate(data = mpg, hwy ~ manufacturer + model, FUN = summary)` I'm using formula syntax above, telling R to group by manufacturer and model and give me a summary() of the variable hwy. A different way of writing that without formula syntax is: `aggregate(x = mpg$hwy, by = list(mpg$manufacturer, mpg$model), FUN = summary)` Getting a tally of how many observations there are of a certain manufacturer type: `aggregate(data = mpg, hwy ~ manufacturer, function(x) length(unique(x)))` This can also be done through the dplyr package, which should be more similar to sql syntax. library(dplyr) mpg %&gt;% group_by(manufacturer) %&gt;% summarise(mean = mean(hwy), sd = sd(hwy), twentyfifthpercentile = quantile(hwy, 0.25))
If you're used to SQL, OP, then dplyr is definitely what you want. Read about it here: https://cran.rstudio.com/web/packages/dplyr/vignettes/introduction.html.
This doesn't sound like a problem that R is well suited for - R is made for statistical analysis, not web-based applications. Sure, there are R packages that interface with the web, but I would really recommend trying this in a language like Python.
Probably the right tool for this task is Selenium + some other language like perl or python. If it is possible in R, it is going to be a truly awful kludge.
Hey, by the way i see that the recommenderlab package can create a system that is based on rating. Is there a way to create one without having rating? For example i do not have all products of my company rated by customers. I just have the sales volumes of these products. Will it be sensible to use the sales volume as rating?
any way to stop this spam? it interrupts my feed read.
Report them. Eventually it'll end
RSelenium is pretty decent. I mean it's probably not quite as nice as working with Selenium + Python or Java but it's gotten the job done for me quite a few times.
Here is a link from someone trying to download data where buttons clicks needs to be simulated. http://stackoverflow.com/questions/29185501/r-language-how-to-make-a-click-on-webpage-using-rvest-or-rcurl
Free course for learning how to use R with databases
 You can use assign(x,value)
I've no idea what your code looks like, but why not store your sims in a list instead of assigning them to vars? You could access and modify the list from within your loop.
The package vignette says it works with rating data and 0-1 data, so I don't know what would happen if you used sales volume. When we haven't had ratings, we have used a threshold with sales volume to get 0-1 data. For instance, if 10% of a customer's basket contained a particular item then that item will get a 1 and all items below will get a 0.
Give us psuedo code. There are several methods here that depend on what you are doing. `assign()` sends things to an environment of your choosing, appending the new person to a list of people and cycling that through your algorithm's loop also sounds reasonable. There are other methods, it all depends on your code.
You should think about reducing the number of data frames and related vectors and try to focus on creating a single data frame with a column for each variable and a row for each observation. Don't worry about trying to save space by using relational DB normalization approaches. R is very efficient at storing columns of data with lots of repeated values.
Sometimes I found this problem at batch processing various dataframes. (reading, reshaping and analyzing). My solution was to see dataframes as data, using a list of dataframes as in this sample code: dfnames =c("one","two","three") dataframe = list() for (name in dfnames){ # create dataframe dataframe[[name]] &lt;- data.frame(cbind(rnorm(100),rnorm(100))) # modify dataframe dataframe[[name]][1,2] = NA } View(dataframe[["two"]]) 
While this isn't an R solution, you might be able to get most of what you want done with pdftk builder, which is a Windows freeware utility tool 
I have looked into it recently, basic R cannot load pdfs and no package (I found) exists to help you do it. Depending on what you have/want, you can do it with pdftk or latex. In the case of pdftk R might be helpful in creating the pdf page you bake onto the other ones. For latex you might also be able to generate the .tex files automatically with R. In my case I did it by hand, since I just needed to label several figures in different ways.
The best thing to do at that point I think is to just keep on finding interesting problems to work on. Once you can create a script to do it, systemise and functionise it. Once that's done, turn it into an API and make it into a package. Add tests, new features, make it play well with other system components. 
The loop has nothing to do with the problem. Sounds like you could use a refresher in debugging your code. A good strategy (at least until you learn some more advanced ones... but even then...) is to run through your code line by line and actually run it. Your function doesn't take any input so we don't need to define any variables. So let's run that first line. n &lt;- readline(prompt="Enter an integer: ") Let's provide some input and check if it's what we expect it to be &gt; n &lt;- readline(prompt="Enter an integer: ") Enter an integer: 9 &gt; n [1] "9" &gt; str(n) chr "9" were you expected a numeric? This is where your issue lies. 
Use lists. They can really help make your workspace less cluttered. 
Start tutoring in r. I've found myself learning so many new things in r because I basically get to see the homework of classes or real life problems that people deal with. Plus you get paid. 
The r-bloggers website helps me a lot when I have some problem doing a R project. I didn't realize it was updated that often. Thanks !
Is this your first programming language ? If so, stop using R write a way and get yourself a decent language. R is awesome at what it does, teaching you good programming practices is not one of them.
Eh. Just because you were taught shitty programming practices doesn't mean that that's the only way to do it in R.
Packages! It sounds like what you need is a personal package for miscellaneous use. Packages are really easy to write nowadays with Hadley Wickham's excellent and free online book. You don't have to publish your package on CRAN or even github if you don't want to, just install it from a local source. My setup looks like this: I have a code/ directory under home. (I use a mac. If you're on windows your setup my look different.) under code I have an R-packages directory and I keep all my packages there. They are so easy sometimes I'll build one for a specific use case because it's kinda nice to have all the logic abstracted away. I recommend starting with your most basic function that you use a lot. Mine was a colors function. I keep using the same colors that I'd mix in RcolorBrewer over and over so I wrote a function to save six colors and put it in a package. Get that one function working and you'll learn a lot about package development. Then just add from there! Hadley's package book: http://r-pkgs.had.co.nz Very useful step-by-step blog post: https://hilaryparker.com/2014/04/29/writing-an-r-package-from-scratch/
Not OP but thanks for this! I didn't realize making one was so easy. 
Excellent answer! much obliged...
I find the answer to this question never gets past personal preference 
I have a student github account, where I can keep private repos. Is that what you suggest?
War never changes... language wars that is.
I mean, that's a true statement. But do you really think R is a good language to start with ? 
There's no battle. Both are good. R is a little more domain specific with a steep learning curve. Python's more general, and easier to pick up on. Can we stop arguing about it now? I didn't think so.
Well said, I love both, and use both, but I believe it's more than preference. The more you use each the more you will understand which tool is best for each problem
Every language has some weird inconsistencies. Do you have any in mind that you think cause too many issues?
If you want them private, bitbucket and gitlab offer free private repos. 
Just having the package in a git repository locally saves you a lot of headaches in the long run. External backups is a separate thing, which is also good to have.
Univariate optimizations are doable using the optim() R base function. Write a function, specify the parameter to be optimized, and run it through. [Heres the documentation](http://stat.ethz.ch/R-manual/R-devel/library/stats/html/optim.html)
This looks like a standard example of linear programming to me. Quick google shows you can use [lpSolveAPI](https://cran.r-project.org/web/packages/lpSolveAPI/index.html). I found [this tutorial](https://ecreee.wikischolars.columbia.edu/file/view/lpSolveAPI+Tutorial.Rmd) to be very helpful and managed to write a solution in 20 minutes.
Perhaps this question can help you: https://stackoverflow.com/questions/41995627/long-to-wide-table
with r you always want to use the question prefix before a function ?matrix. Look careful at all the function arguments and their default options. Also look at the Examples and Value (which tells you the output class and what it contains)
The Github student account, as OP mentioned, also allows that.
Ah, wasn't aware. I just use the regular one for my public stuff. 
Would changing the variable to a factor help? df$discreet_variable &lt;- as.factor(df$discreet_variable)
Sounds like they shouldn't be factors then. Is splitting between whole numbers a problem, though? If this variable is higher, go right, if lower, go left. Isn't that the way it should be? 
Discreet variables try not to attract too much attention. Discrete variables take a countable number of values.
https://rud.is/b/2017/02/09/diving-into-dynamic-website-content-with-splashr/
Why not call it log4r 
ok but what about R
Her hips and legs kind of make an "R"...? 
Looks like most of this was taken from: https://www.datacamp.com/community/tutorials/r-or-python-for-data-analysis
The `data_grid` object is a data frame with about 17.5M rows. That is a lot of data for a computer to process.
As the other poster suggested, your grid is simply way too fine: Try this instead, plot came out just fine for me when I did it... data_grid = expand.grid(ShotsFacedPerMatch= seq(min(league$ShotsFacedPerMatch) - 0.5, max(league$ShotsFacedPerMatch) + 0.5, 0.25), ShotsFacedPerMatchConceded= seq(min(league$ShotsFacedPerMatchConceded) - 0.1, max(league$ShotsFacedPerMatchConceded) + 0.1, 0.002)) (edit: btw - cool plot)
Thanks, it's been through good criticism to get to this point. I had no idea the data table would mesh itself first rather than use the data explicitly given so I didn't even think this would be the issue. Thank you for your help! Last question: what does the sigma represent and why would I use 0.33 v 0.67 v 1?
sigma of 0.67 represents approximately 1 standard deviation of the mean in the Gaussian distribution. More information is available here: https://en.wikipedia.org/wiki/68%E2%80%9395%E2%80%9399.7_rule 
Well I was going by the popular log4j Java library 
The content has been inspired by an article from KD Nuggets as mentioned at the end of the post.
In general people don't like doing homework for someone else. At least tell us the problem you're having so we can point you in the right direction. Do you not know a good place to start with the math? Do you have a solution but don't know R? What? 
It's not homework and like I said I'm just trying to look for random numbers to sum to another number. I know with rnorm I can do this for one number but I wasn't sure if there was a way to get multiple random numbers with different criteria. 
Pretty sure this is what you're looking for: * https://en.m.wikipedia.org/wiki/Dirichlet_distribution * http://www.stat.washington.edu/~rje42/lca/html/Dirichlet.html You won't be able to specify SD, but if you specify your alpha as your target means (divided by 100), it'll always sum to 1 (so just multiply by 100). If you really need all those constraints satisfied, you could try using MCMC to generate a sample. I suspect your distribution is over-specified and you can't just pick arbitrary variances out of the air as a constraint on dirichlet samples, which is essentially what you're doing. Maybe you could specify a variance for one component, or maybe even all but one, but I seriously doubt all. 
https://projecteuler.net/archives
Where can I find solutions to those problems, if I am stuck somewhere?
It’s a nice visualisation but it doesn’t make the correlation between the length of the address and its Fleisch–Kincaid grade level, or the number of words per sentence, very clear. The first plot strongly hints at a correlation but falls short of showing one. And it might even suggest one where none actually exists. Furthermore, I don’t see the use of the local regression line, or the confidence interval. In this context, they are more likely instances of [chartjunk](https://en.wikipedia.org/wiki/Chartjunk).
Sorry I can't give you more feedback, I'm not an auth pro and have never tried to implement any of the stuff I pointed you to.
Try figuring out solutions to the questions posted under the r tag at stackoverflow. Don't look at other solutions until you finish yours. Compare your solution to any other posted solutions and see if theirs is better. If so use that as a learning experience. 
No worries, thanks.
I would just google "project euler problem x language". I know I've done that with other languages like Ruby and had it turn up results that helped. I don't know a specific site for that though, no.
What do you want to be able to use R in the future for? Wrangling, stats, GIS, data cleaning? 
If you can swing it, a subscription to Safari (O'Reilly) books an excellent bang for your buck and has virtually ever R related book you could ever need/want. There is an app that allows you to download books and video tutorials to your phone/tablet and supports highlighting. 
I would recommend using apply (or lapply if x is a vector): http://astrostatistics.psu.edu/su07/R/html/base/html/lapply.html. Just make a function for which(jarowinkler(x[i], x[-i]) &gt; threshold) part.
Hey SecretAgent, thank you !!!!!
cheers!
Guys, thank you all !!! I downloaded the Udemy app and will get around with that. You can download videos for offline usage ... a really cool feature ... so I can watch a video instead watching out of the window of the train ...
This whole kind of `values &lt;- c(values, ...)` construct is really bad in a loop. The reason for this is that every time the vector gets another item added (it "grows"), R has to do some slow operations. This makes your loop slow as balls. At the very least you should start by allocating an empty vector of the correct length by changing your line `values &lt;- NULL`to `values &lt;- vector(length = 10)` and changing the `values &lt;- c(values, ...)` line to `values[i] &lt;- ...`. See chapter 2 of [the R Inferno](http://www.burns-stat.com/pages/Tutor/R_inferno.pdf) for more information on this. But of course, as other commenters have noted, this is a time when you don't need a for loop at all. It's not strictly speaking "better" in this case, since really under the hood R is just doing a for loop. But it is more convenient, because it will do the construction of the result vector for you without you having to worry about allocation or any of that rubbish. Which apply function you use depends on what output you want and what input you want to give - sapply for example is given a vector or matrix and returns the same thing. lapply is given a list and returns a list. Other apply functions work in different ways.
I don't think you should mention mode() at all - it's only included for S compatibility
Homework questions aren't really appropriate for /r/Rlanguage , you won't learn anything by having someone else provide answers to it all. I'd recommend using [RStudio](https://www.rstudio.com/) and then installing the [Tidyverse](http://tidyverse.org/). &gt; install.packages('tidyverse') If you need to do manipulations on your data frame or summarise it then [dplyr](https://cran.r-project.org/web/packages/dplyr/index.html) is your friend and reading the [vignette](https://cran.rstudio.com/web/packages/dplyr/vignettes/introduction.html) is highly informative. You might also consider providing useful feedback to the course co-ordinator to include an indication of the nature of the assignments and the need for some level of understanding/knowledge/other module requirements.
I don't think assigning to storage mode is great practice either. I think an as function is clearer
[removed]
No, I know bayesbootobject isn't a function. I was giving that as an example of what summary() would be giving a summary of. For example, bayesbootobject &lt;- bayesboot(data$variable1, mean, R=4000) summary(bayesbootobject) Nevertheless, I am still learning fundamentals of coding in R. Thanks for the link! I will look into it 
Since you are being honest, I'll be blunt. Study and pass the exam on your own merits. My integrity is worth more than $10. You might want to think about how much yours is worth. How do you know that your professor won't ask a few questions about the code to test your mastery? He/she will see right through your deceit quickly. Most schools have severe penalties for plagiarism and academic dishonesty, exactly because they also value their integrity. 
Honesty and integrity is fine and good getting over the University crap I have to put up with is worth much more. I mean he is giving me 2 days to do this exercise and I don't see much of a difference between you doing it and asking a friend to do the same. I can't possibly belive he is not expecting something like that.
I'd guess the professor would give you some basic exploratory data analysis task. If you followed the swirl() basics course and then read the dplyr vignette you'd probably be able to learn enough R to get you through in 1-2 days. Like the other guy I'm not going to help you cheat, but more than willing to help you learn.
Ok, thanks! I googled and found out how to use URISource for PDFs.... now to figure out how to pass it is just a txt file not a PDF Thanks just enough, thank you 
[deleted] ^^^^^^^^^^^^^^^^0.9630 &gt; [What is this?](https://pastebin.com/64GuVi2F/17866)
There are multiple change point detection packages in R. There is also one for multivariate though i don't recall exactly which package this is-possibly ecp. I don't think that it can handle categoricals but you could one-hot encode them first as well
try persp()?
Just separate starting at the first word starting with a lower-case letter. You do not have to do anything super complicated like the other post suggests.
That only works if we can correctly assume the titles are lower case, and that doesn't even hold in the data OP gave. If the list of possible titles is limited and reasonably small, it might be more expedient to create a list of titles and use that to match and then remove the titles from the names.
Oh! I just learned this the other day! strsplit(x=, split="___", fixed=T) x is the character object you want to split so make sure they are characters split=the characters you want to split by so " " in your case This produces a list which you need to turn into something useable so you need to use matrix() and unlist(). matrix(unlist(strsplit(x="dataframe$col", split=" ", fixed=T)), ncol=__, byrow=T) Only tricky thing is you may end up with different number of columns since your names and titles are varying lengths so not sure how you would address that. There's also substr() which allows you to start and stop by a particular character...but that isn't uniform for you either. 
You could turn everything into lower case using tolower()
That worked - thanks!
&gt; teaching me how to find the information on my own. And that is the most important skill you can hope to develop as it is independent of subject specific knowledge and will stand you in good stead for the rest of your life. Note that even highly experienced R users google for solutions all the time, e.g. /u/hadley ... &gt; [I google for #rstats code all the time!](https://twitter.com/hadleywickham/status/757919786344984577) ...and Roger Peng.... &gt; [There's just too much to know. I would be surprised/skeptical if anyone *didn't* do this.](https://twitter.com/rdpeng/status/757984394061676544)
I'm with you. Its HARD. But it also teaches you to problem solve without being babied. I got halfway through and got super stuck with the cacheing part. That was about 6 weeks ago. I'm planning to get back to it when I have a couple free days to really focus.
Agreed. Except that I enrolled in the course to be taught information, not to be given challenges that forced me to teach myself. I think it would have been much easier had I any experience with R, but so much of the basics were just not covered in the course. Again, I can't tell if that was the point of the course or not. Is is best to learn early on how important Google and Stack Overflow are? I guess time will tell. At least it's over and I can progress to the next course. On the plus side, my frustration with Coursera has led me to find other resources. It's pretty crazy how helpful the R community is if you simply ask your questions correctly. I found posts that provided not one solution to a problem I was having, but multiple options depending on the situation. I also just started with the tutorial on Data Camp tied into Kaggle. It's been fun so far. No new information yet, but the format is entertaining.
Yes, being able to google is an important skill but it is not something you should have to deploy frequently in a well designed course. The point of a class is to guide you through the topic in a structured manner, so at a minimum you have a strong scaffolding to hang future knowledge on. 
I am really, really tempted to send this to an old employer who (long story) forced me to take multiple "technical skills assessments" where no aids whatsoever were allowed, and zero information on what would be assessed was given beforehand.
They're very good if you can power through them (I did 3/4 courses at a time). But most of the time-sink comes from trying to figure out the problems yourself. The advantage is that it teaches you how to manage being thrown in the deep-end. I still think the best way to learn is just forging a project yourself, maybe using the IMDB/OpenData packages. Connect to the API -&gt; query data/create functions to query data -&gt; manipulation -&gt; display. I think one of the biggest problems is that most tasks you're given have no context, data analysis is much easier when you have a brief of sorts and an understanding of the underlying data. Good luck anyway.
Thanks everyone! I found this tutorial that has more or less given me what I wanted using the timelyportfolio/parcoords library: http://www.buildingwidgets.com/blog/2015/1/30/week-04-interactive-parallel-coordinates-1 
I used to be a mentor for the course (I quit because I got tired of trying to help people deal with lousy course design over and over). The course coordinators and the mentors know this is a problem. People complain about it in every single section, and they launch a new section every 1-2 weeks. The party line is that this is not a bug, it's a feature - they are helpfully teaching you how to self-teach using Internet resources. That's pretty clearly BS but no one has any intention of changing it. The track the class is part of claimed at least when I started that it required no programming experience, but the intro R class has some programming experience as a pre-req. 
don't complain about something you got for free. I took the course and I am satisfied, I got 100million times more out of it then what I paid for.
Which course specifically.?the one from Johns Hopkins? That course is supposed to be one in a series, so if you skipped the earlier ones that could be contributing to your struggles 
That course is garbage. As a senior data dude in one of the world's largest investment management firms (top 5), I suspect that Johns Hopkins series was cobbled together quickly with the intention to sell seats and a brand name certificate. I understand the need to use google and for data professionals to show a little spine and grit, but this class doesn't seem to have that designed into the course structure. I'm always on the look out for good courses to provide guidance to our summer interns, and I recommend mostly anything on the edX platform. Also, I highly encourage replicating projects from other tools with R. Academically, all my projects were completed in matlab or SPSS. Transitioning to R was merely learning how to drive a different vehicle. Conceptually I knew where I wanted to go and which roads to take. 
I took the courses in order.
request a refund, I didn't pay and while I think there is a lot more they could have covered overall I don't think the class was bad. Most of the issues I've came across I've found a solution to either by asking online or referencing in a R programming book.
They've gone to a new system where you pay for a month of full access instead of by course. Course 1 was very good, and I'm gonna wait see how the other two courses I complete this month are before paying for another month. I do want to support them if everything is up to snuff. 
I took the class about 6 months ago, I have noticed that the course itself has changed in the past few months.....
I remember doing the nuts and bolts module and finding it very useful. Concepts were and explained and then you learnt by doing. Was it a perfect wealth of knowledge? no. But it is excellent for a free resource.
Not sure what the bitching is about. I thought the course was great. I now am very comfortable in R.
This is my plan!!!
Agree. The order of the material and the way it is presented is not designed in any particular way that would help people actually learn. The way it was taught just didn't make much sense to me.
That's not much to go on, but generally you will need to - get stock data, probably using getSymbols() - change the stock prices to returns using CalculateReturns() - then use chart.Rollingperformance() using whatever function you're using within the call to chart.Rollingperformance() ex: library(quantmod) library(PerformanceAnalytics) getSymbols('SPY', from = '2005-01-01') SPY_adj_returns &lt;- calculateReturns(Ad(SPY)[-1,) chart.RollingPerformance(SPY_adj_returns, width = 12, FUN = 'mean') More information would help, but that should get you started. The performance analytics package also has a lot of 'chart.something' functions. Take a look at them.
Thanks a ton! Just one question, what does the "key" refer to in this scenario? (I'm pretty new to R so I apologize in advance for my ignorance :P ) Edit: Im asking bc I get the following error code: &gt;*"Error in eval(expr, envir, enclos) : object 'key' not found"*
&gt; If the list of possible titles is limited and reasonably small, it might be more expedient to create a list of titles and use that to match and then remove the titles from the names. Do you know how to do this. I mean i can delete the titles or replace them but i dont know how to extract them in such a manner
I get: &gt; Error in UseMethod("gather_") : &gt; no applicable method for 'gather_' applied to an object of class "function"
Could you give more details about this data?
Did you assign yr dataset to `df` ? Type `df` in the console and see if the output is a table. If the output is a function body then that's your problem right there. If yer data is assigned to a different variable, then replace `df` with that.
Each feature attribute represents a relationship value for the corresponding stock. The relationship value is defined by strength of hidden and non-hidden connections.
Thanks. Why do you adjust the returns before doing the chart? 
If your data frame is called "x", your numeric column is called "score", and the column of strings is called "group", then: t.test(score ~ group, data = x, paired = TRUE) However, with 12 strings it seems like you might be looking for an ANOVA? If so, just replace t.test from above with lm, save the result into a variable, and run anova() on that variable. 
I already ran the ANOVA, I'm looking for which one is different now. Thank you very much for the code, I had been looking for a while and hadn't found what I had been looking for.
Wouldn't you run a tukeyHSD then?
http://www.onthelambda.com/2014/02/10/how-dplyr-replaced-my-most-common-r-idioms/
I did it the way i did because it was the way I knew how to do it. Ill definitely check out purrr though, thank you.
No problem. The big bold text is supposed to be ashed out comments. ¯\_(ツ)_/¯ 
Give me about an hour or so and I can pm some code and the results.
getSymbols returns open/high/low/close/volume/adjusted. The adjusted price accounts for splits and dividends. This makes a pretty big difference, especially if you're looking over a relatively long time period. Here are a couple of examples. The black line in each chart shows the opening price, the colored line shows the adjusted price. The first shows how a split change the price over a short period of time, while the second shows how dividends skew the price over a longer period of time (~10 years). http://imgur.com/a/f3QSQ This is what not using the adjusted price can do to returns: [SPY over the past 10 years](http://imgur.com/a/FCEt7) It looks like dividends accounted for ~40% of returns.
You might find some of these references useful... * [Biostatistics for Biomedical Research](https://biostat.mc.vanderbilt.edu/wiki/Main/ClinStat)[Biostatistics for Biomedical Research] by Frank Harrell * [An Introduction to Statistical Learning with Applications in R](http://www-bcf.usc.edu/~gareth/ISL/ISLR First Printing.pdf) by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani * [The Elements of Statistical Learning : Data Mining, Inference and Prediction](http://statweb.stanford.edu/~tibs/ElemStatLearn/printings/ESLII_print10.pdf) by Trevor Hastie, Robert Tibshirani and Jerome Friedman
enthusiastic instructions, you say: #I'm so pumped about functional R programming! It's the best #Let's make this a function so it's super-easy for you to use on all kinds of data! Your going to kickass and take names! karmoeba_graph&lt;-function(x,calculation=sum,...){ #include the ... to pass extra arguments require(ggplot2) #make sure ggplot2 is loaded FUN=match.fun(calculation) #make what is set to calculation a function class new.df&lt;-data.frame(Sums=apply(x,2,calculation),names=colnames(x)) #old code. now using apply for more flexability g &lt;- ggplot(new.df, aes(names,Sums)) g + geom_bar(stat="identity",...) #the ... lets you pass more arguemnts } #use the ... to color by names. Colors are amazing! Your going to wow everyone! karmoeba_graph(df,calculation=sum,aes(fill = names)) #or change calculation to "mean" to graph means. You can do sd for standard deviation, whatever! YES! OH YES! karmoeba_graph(df,calculation=mean)
Eh, forget the pm, I'll just put it here for /u/False_3 If you have an anova, like this using the Chick Weight based on diet dataset in R: aov.weight&lt;-aov(ChickWeight$weight~ChickWeight$Diet) summary(aov.weight) which gives: Level|Df|Sum Sq|Mean Sq|F value|Pr(&gt;F) :--|:--|:--|:--|:--|:-- ChickWeight$Diet|3|155863|51954|10.81|6.43e-07*** Residuals|574|2758693|4806 Since it's significant, we can run a Tukey's honest significant difference. TukeyHSD(aov.weight) which gives us: comparison|diff|lwr|upr|p adj :--|:--|:--|:--|:-- 2-1|19.971212|-0.2998092|40.24223|0.0552271 3-1|40.304545|20.0335241|60.57557|0.0000025 4-1|32.617257|12.2353820|52.99913|0.0002501 3-2|20.333333|-2.7268370|43.39350|0.1058474 4-2|12.646045|-10.5116315|35.80372|0.4954239 4-3|-7.687288|-30.8449649|15.47039|0.8277810 If the p values are below 0.05 then you have significant differences between the groups. The agricolae package has a HSD.test that makes an ordered table for this, but for some reason it's giving me a null set right now so... Edit: table formatting Edit2: Got HSD.test working install.packages("agricolae") library(agricolae) HSD.test HSD.letters&lt;-HSD.test(aov.weight, "ChickWeight$Diet", group=TRUE) This gives a bunch of data, but what we are interested in is: trt|means|M :--|:--|:-- 3|142.9500|a 4|135.2627|a 2|122.6167|ab 1|102.6455|b Since groups 3, 4, and 1 have the same letter they are not significantly different. Since group 1 and 2 have the same letter they are not significantly different.
I'm glad! If you want to learn you can go over u/hadley's [chapter on functional programming](http://adv-r.had.co.nz/Functional-programming.html). Super, super well written and quick to learn. 
Thanks for sharing!! I'll definitely check it out. I just started tinkering with R and I have to say I'm incredibly impressed with the potential it has. Occasionally I get a little overwhelmed with it, but I've had a fair amount of luck using a combination of online sources and the kindness of redditors (like yourself) to little by little guide my progress. Thanks again for everything
SQL for database querying A little bit of SCALA for when I start using Spark. Also, this question would be better suited for r/datascience
Python for just about everything day to day. Java for some videogames I make as a hobby. MATLAB for when I want to kill myself. 
Checkout the description for these books 1. 'Financial Analytics With R' 2. 'An Introduction To Analysis Of Financial Data With R' 3. 'Option Pricing And Estimation Of Financial Models With R' 
* PHP - Scripting / Cron Stuff * NodeJS / Javascript - Integrating with R over web sockets, building data analysis tools which are used internally, controlling hardware, long running processes, multiplayer games/applications. * MySQL - Querying stuff * C++/Java - Honestly forgot most of what I learned
The books SecretAgent recommended are great but very technical. I would also check out this: https://www.otexts.org/fpp 
xts, PerformanceAnalytics, packages, and rollapply function are extremely useful and powerful.
Free is always great! Thanks
I have replaced several subscription modeling engines with Shiny servers running on my firm's network that all run out of R. As someone else said, xts makes anything time series a breeze. It also can do matrix math much faster than Excel, simply because you don't have to visualize it (arrays in Excel are wonderful but also complete shit). I can very easily do a PCA on yields to measure the level, slope, curvature factors for modeling the yield curve. Monte Carlo simulations are also much faster in R, especially for time series. If you want to do a 5000 iteration simulation on a time series that is 400 observations long, you would need 2,000,000 cells in Excel just to generate the random numbers. That would freeze the application, most likely. You can also do much more advanced machine learning optimization algorithms, etc etc. R is Excel on steroids, but only if Excel were a baby and R was Arnold Schwarzenegger 
Can you give a few examples of what you use Python for? I have it downloaded, but I never have a reason to open it because I can do my task quickly in R.
Mainly gluing stuff together and creating pipelines. One example is watching for new data produced by some lab equipment, so for that I use the [watchdog package](https://github.com/gorakhargosh/watchdog) to monitor for file changes in a certain directory, then parse some metadata and stuff, then fire off the jobs to AWS with [boto](https://github.com/boto/boto3) or submit them as jobs on a compute cluster. I also prefer working with any matrices in numpy compared to R.
I had to do something like this ages ago when labeling a map. I eventually split everything by the spaces and then used an "if" test before the label to identify the odd cases where the name was either 1, 3 or 4 'words' long. This strsplit() function looks brilliant.
Some times ago I have an idea on investment and then found that R is one of the best language to present it. It may not be the best strategy but I think it’s funny, so I would like to share it with you. Any idea/comment is appreciated. Note: the evaluation is on Taiwan stock market, and I am planning to evaluate it on the U.S stock market later (plz forgive possible inaccurate wording for not being a native English speaker)
So Ahnold is really a mere baby on steroids?
you need to first input the data like so: values &lt;- c(5,17,4,10,2,45) The arrow assignes the stuff on the right to the thing on the left. The c() is used to include a combination of things. from there, figure out how to use mean()
Thank you:)
Do you know anything about basic statistics? It's a serious question. With only a few numbers this wouldn't take long to check. I encourage you to do the basic calculations in R to start getting a feel for it. This would have been an easy one to google, so I'm curious what you tried first, but I'll walk you through it: For the variance, take the squared difference between each number in your sample and the mean. Then, add all of those numbers together and divide by n - 1 (so, 5 in this case because you have 6 numbers in your sample). To assign values in R, use &lt;- rather than an equal sign. You can read more on this but just get that in your head to start. So, rather than having to type mean(values) over and over again, you can type xbar &lt;- mean(values) and then use xbar from then on. To access each value you entered, you can use []. So, values[1] would be equal to 5, and values[4] would be equal to 10. You might start working your way through checking the calculation with: num1 &lt;- (values[1]-xbar)^2 num2 &lt;- (values[2]-xbar)^2 ... sums &lt;- num1 + num2 + ... + num6 sums / (6 - 1) Not the fastest way to do any of this, but it will start to get you familiar. And - some people don't like it, but I like the swirl package to get started. install.packages("swirl") library(swirl) Then, do what it say. If something doesn't make sense in R, you can type a question mark followed by the thing you're trying to learn for some info. So, ?sd will give you the help page for the standard deviation function.
Discovering rollapply was a godsend for me.
This may be of some help in regards to importing multiple csv files: http://serialmentor.com/blog/2016/6/13/reading-and-combining-many-tidy-data-files-in-R Can't offer help on the plotting aspect because I have no idea what your data looks like or what kind of plot you want.
You should be able to do apply(table, 3, mean) (replace mean with the function you want to use)
you need a loop to get the filenames in the directory loop over each one and (open, plot, print) you can google all of this step by step and have it going. I've done this at work maybe 50 times. You could alternatively concatenate the data together using rbind() if it's all isomorphic and add a distinguishing variable, then do the plot loop separately. I would probably write a helper function to plot the data frame, then a helper function to prepare the data for plotting, and then nest those into the file reading loop or something just because I like functions. 
Well, if i knew how to plot my data without uploading it in the environment i wouldn't be posting my conundrum on Reddit :-)
Same as you plot any other data. If it’s in a list `my_list`, just do `ggplot(my_list[[i]]) …` for a given list index `i`.
Free vs $$$ Hmmmmm. Also the customization you can do with Shiny with just R/ggplot or especially R + HTML + JavaScript far outclasses Tableau in every way excluding ease of use. Think of it like GUI vs Command Line
I've used shiny for smaller (~30 users) internal products. For bigger customer facing products, we ended up going with JS and OpenCPU. I'm not sure if that decision was based on performance or lack of willingness to buy Rstudio Server Pro or necessity of keeping the JS team busy... Probably some combination of all of the above.
Shiny is built on top of bunch of modern web JavaScript libs, vega.js for drawing and selectize.js for nice ui controls. You can take a glance at [plotly R library](https://plot.ly/r/) which has been designed following similar approach. But in comparison plotly with shiny, Shiny is integrated with dplyr and magrittr packages, and plotly can 3D plots. Well, there are many js vis libraries we can await to get bound with R, like d3. What's lib is better, depends on the case, I think itsn't worth to dive deeply into one thing.
Only books. 'Web Application Development With R Using Shiny: 2nd Edition'. Just know you should know how to visualize data beforehand. If you don't 'ggplot2 Elegant Graphics For Data Ananlysis: 2nd Edition'.
I just started getting into analytics. So thank you for the ggplot book suggestion! 
In addition to Shiny Server, RStudio has recently released a new product called Connect which allows you to easily deploy Shiny apps and R Markdown reports. NASA and eBay are among the companies using it. 
You probably want to get rid of the for() loop and the grid.arrange. Look into [facet_wrap](http://docs.ggplot2.org/0.9.3.1/facet_wrap.html) in ggplot2. That should do what you're trying to accomplish here, in more R-like way.
Additionally: &gt; Gi &lt;- ggplot(data=tds, aes_string(x="Well", y=date)) + geom_bar(stat="identity") Trying to create multiple objects with iterated object names in this manner will not work. Each *for* loop will simply update the object with the literal name *"Gi"* with the data from the current iteration, as opposed to creating a new object named *"G**i**"*. 
Shiny consultant, nice gig!
using dplyr is probably better but a quick and easy way would be to create a new dataframe by subsetting for babies that weigh less than 2500g, ie newbirthweight &lt;- subset(birthweight, birthweight$weight &lt; 2500) and then see how many observations are in the dataset, which is how many babies weigh less than 2500g. str(newbirthweight) Then divide this number by 50, 000 to get the proportion.
On mobile so I'm thinking more than set code but.. You could filter the dataset into 2500.birthweight&lt;-filter(birthweight$weight &lt; 2500 then length (birthweight) and length (2500.birthweight) 
Yeah, they could make it into a list or something though g &lt;- list() ... g[[i]] &lt;- ggplot() .... grid.arrange(grobs = g,ncol=10)
Yeah, only reason I didn't suggest something along these lines was because I figured *facet_wrap()* covered OP's specific use case (...and I'm lazy). It is probably worth noting though :)
lst &lt;- list() Loop or something that creates your data For (i in shebang){ lst[[i]] &lt;- create a dataframe }
give it a try, you might like it
Still gives me this error: "Error: ggplot2 doesn't know how to deal with data of class character"... All i want to do is upload all my files (tables), do the same visualization on them and export that visualization into a pdf. For some reason i get that error. The other problem i encountered was that when i export them automatically like this i get empty while pdfs with nothing in them instead of receiving the plot into the file. Actually here is all of my code: library(xlsx) library(ggplot2) library(scales) a &lt;- list.files() for (i in a) { assign(paste("data",i,sep=""), read.xlsx(i,sheetIndex = 1, as.data.frame = TRUE)) } listt &lt;- ls() listt &lt;- listt[listt != "a" &amp; listt != "i"] for (i in seq(listt)) { options(scipen=1000) ggplot(listt[[i]], aes( Z_PERC_GSR, Z_PERC_NSR))+ geom_point(aes(colour=ifelse(listt[[i]]$LOF&gt;quantile(listt[[i]]$LOF, prob = 1 - 2/100),"blue","red")))+ geom_density2d()+ labs(list(title = "Visualization of Ouliers", y = "Total discounts over Net sales revenue", x = "Total discounts over Gross sales revenue"))+ scale_colour_discrete(guide = guide_legend(title = NULL), labels = c("Outliers", "Not outliers"))+ theme(plot.title = element_text(hjust = 0.5))+ scale_y_continuous(labels = comma)+ scale_x_continuous(labels = comma) dev.copy(pdf, paste("plot_", i, sep="")) dev.off() }
Okay... listt &lt;- lapply(ls(pattern="data[A-Z]+"), function(x) get(x)) This fixed the issue... Now all that remains is - how do i export every plot that my loop makes?
I think when your using ls() your just getting the names of the data frame, not actually putting the data frames in a list. 
Based on what blog posts we see from Jeff Leak and Nathan Yau, yes. Do you have the link to Hadley's representation regarding the cupcakes?
you need to use ggsave ggsave(filename=paste("file",i,".jpeg",sep="")) hope that helps
&gt; how do i export every plot that my loop makes The simplest way, when using ggplot2, is to use `ggsave`: p = ggplot(…) + … ggsave(filename, p) This can be done in the loop as well, of course. Regarding your list: good approach. You can abbreviate the `function` part though, because your function is simply equivalent to `get`. Hence you can write: listt &lt;- lapply(ls(pattern="data[A-Z]+"), get) … but R even has a dedicated function for that, so a shorter version would be: listt &lt;- mget(ls(pattern = 'data[A-Z]+'))
Fantastic. I'm glad to help out any time
the functions outputs an object and that doesn't have a name, i.e. a &lt;- fun(xz) b &lt;- fun(xz) What you appear to want to do is assign a global variable based on the input, but apparently also not a input string but the name of the inputed variable? That is as complicated and convoluted as it sounds and can super easily break etc. So if you want to write good R code, try to avoid assign and use functions as they were intended with return(...)
Thanks for the tip! I'll have to do some testing to see what functionality this gives me. Do I have to re-run that every time I make a code change? 
I am almost at this point where I am considering taking my packages apart to get to this state, but I would rather not! I enjoy having a proper dev environment but it kills my focus if I have to wait to re-run my scripts something I didn't mention is that I was trying to make a shiny app as an R package too, and it really kills my productivity to have to kill the Rshiny server each time I make a code change for this reason
Yes, though it often runs faster than install() followed by library(), because it does less.
I went and tried to make some workarounds, and I tried to bundle it as a little "template" here https://github.com/cmdcolin/shinytemplate This seems to address the issue I was having with having to re-install the library each time, I think now it simply gets code changes every time I refresh browser which was the goal :)! I may have missed something again, but if anyone has ideas, please contribute to the repo!
I will bookmark this page so i can come back to the code when i start making the app. The lof logic is a bit more subjective then a solid 1.5 value for a cut off, but that is another topic. Again, thank you for the help! :-D
If you've already loaded your library then after `build()` followed by `install()` your rebuilt library is reloaded and you don't need to use `library()` again.
How can I get this?
Thanks for the reply, yes that's what I need. I've been able to do it using the melt function from the "reshape" package: df &lt;- melt(myData , id.vars = "V1", variable.name = "foo") and then plotting df.
Here is my personal favorite way to do it, assuming the matrix's columns have names: myData &lt;- matrix(ncol = 650, nrow = 10) Y_Values &lt;- noquote(paste(colnames(myData),sep = "",collapse = "+")) Here is an alternate way to do it if you want to have any pattern of column names. Generate_Names &lt;- vector() for(Each_Column in 1:ncol(myData)){ A_Name &lt;- paste('V', Each_Column, sep = "") Generate_Names &lt;- c(Generate_Names, A_Name) } Y_Values_Alt &lt;- noquote(paste(Generate_Names,sep = "",collapse = "+")) Just paste the Y_Values vector into the y= section of ggplot :) It does what you asked :p Let me know if it worked!
[removed]
Make an account, login, and claim the book. You can then read it online or download it in a variety of formats!
Try this chunk, it loads 3 of 4 cores for me. library(parallel) no_cores &lt;- detectCores() - 1 cl &lt;- makeCluster(no_cores) parLapply(cl, seq_len(10000), function(exponent) { s &lt;- 0; for (i in seq_len(1000000)) s &lt;- s + 2 ^ exponent }) stopCluster(cl) gc() For Windows I use "Microsoft R Open" R distribution, which supports multicore out of box.
The code you provided also works for me, however, I don't really know how to adapt it to my situation since I'm such an R-noob . Didn't know about the Microsoft R Open distribution you suggested, though. Thank you very much for bringing that up, definitely going to look into that one. 
Do detectCores() - 1 for sure, or you will have problems operating your machine. I don't know the package you are trying to use, but just because you create a cluster doesn't mean your software is going to take advantage of it. You either need to use a package that has parallel computing built in, or look at putting your logic into an apply style function and using one of the functions in the doParallels library.
How about something where you set the threshold manually? You can always replace it later with some clever function for finding the threshold. threshold &lt;- 50 hrdata &lt;- mydata[2:1000, 1] peakr &lt;- which(hrdata &gt;= threshold) points(peakr, hrdata[peakr], pch = 1, col = "red") text(peakr, hrdata[peakr], hrdata[peakr]) There is an argument within text function (I think it's pos) that allows you to offset the label.
I studied psychology and use R everyday for (almost) everything! I won't do it for you but I will offer help if you want it.
This is exactly why I stick to base R.
I think it's looking for "LOFC_Han" in y.
It doesn't work like that. Get good and become a Shiny consultant. That's how you make money with it.
Does the Microsoft thing work for all functions or do you have to specify that you want to use it through apply or dplyr etc? 
Alright, thanks for the clarification. Guess I'll have some reading to do to get a better understanding first. 
regarding your second question: only few symbols have two colours, most have one regarding the first, if you use numbers for colours there are only 8 colours and then R starts reusing them, this can then appear rather random. Instead create a vector of colours you like and use the numbers as index to the vector (i.e. colour_set[vol$colour]) The code should do what you intend, but double checking that with a small simple example can help (and be used here as a reproducible code)
It would be easier to help you if you provided an example of your input data. You can do this easily by creating a subset of your data (if you think it's neccessary), pass it to the `dput()` function and paste the output here.
Use `points(..., col=ColorsVector[as.numeric(SomeFactor)], ...)` where ColorsVector contains color names, and SomeFactor is a table column.
OK, thank you all, I managed to do it by saying: colour&lt;-as.character(vol$color) then just using it on the plot as col=colour
Python still stores objects in memory. But I think bigmemory and biganalytics operate by reading in chunks of data at a time, doing calculations and storing results, and Python is much faster than R at looped tasks. I tend to use packages only when strictly necessary so most of my code is in base R, meaning good portability/compatibility and minimal need to rewrite on updates. bigmemory, ff and so on might do the job, but what happens when you need to speed up the code for production or some major update happens? These issues are better solved by switching to a more appropriate language and using its base functions.
But that is the thing, it doesn't! It prints the error, but keeps going with the lines after the stop.
Correct me if I'm wrong but bigmemory stores C++ pointers in the environment and saves the data on your hard disk rather than physical memory. I've never had issues compartmentalizing the data, I don't think I'll ever be working with &gt;10GB at a time. If I do I'll probably learn a more appropriate language. doParallel helps a lot with for loops. I tested one yesterday, a 13 minute for loop was completed in 3 minutes by a foreach dopar loop
How do you execute your script?
Usually I'm running it in RStudio but as a test, I opened the script from the normal R console and choose to run all lines. It still happens: &gt; print("This is fine") [1] "This is fine" &gt; stop("stop here") Error: stop here &gt; print("should not get here") [1] "should not get here" &gt; 
I guess the bigmemory method amounts to the same thing, it will use pointers to keep track but read the data in chunks for analysis. I don't think there's really any other way conceptually when the dataset size exceeds RAM. As an alternative, you could store the data in say a SQLite file, create indexes and send SQL queries from R using RSQLite package to subset and perform (limited) operations on the data in situ. Still some form of chunking may be needed but at least you have full control over it. Thanks for the doParallel tip. I'll make a point of testing R with/without doParallel and Python.
&gt; Or, if the code at the end your script does not serve any practical function, just comment it out. It does serve a function and is working, but I was testing an earlier part of the script, so I just wanted a way to exit out after it went through the part I was currently working on. &gt; Also, you can source your script in Rstudio by having the script open and pressing Ctrl + Shift + S. I played with this a little bit and that seems to give the result I was expecting. I thought `source()` was just used for including other files for use in the script you were working on. Also, I didn't realize "Run all" meant "Run all even if an error is thrown". As I was going through things in RStudio I found an even better option! `CTRL + ALT + B` will run from the beginning to the current line. That's exactly what I should have been doing all along! Thank you for helping me sort this out :-D
If you write up some example data and use this to illustrate your question, then you'll get an answer
I did... Here it is again. library(dplyr) df &lt;- mtcars df$gear &lt;- factor(df$gear) result &lt;- df %&gt;% group_by(gear) %&gt;% summarize(sum_vs = sum(vs), count_vs = length(vs)) prop.test(result$sum_vs, result$count_vs)
Try using %in% instead of == and you don't need unique. So you should do: subset_df2 = df2[df2$Name %in% df1$Name]
Maybe I'm not understanding correctly, but wouldn't be easier to add to each df a column that indicates which df the df is apart of, then combine both dataframes. Now you'd have an extra way of subsetting all of the df1 names out, excluding df1 Jane. 
First, your formatting is awful. Please use line breaks and tabs properly so your code is readable. Second, do not call any variables inside of a function that are not inputs defined by the function. `function(x)` does not create a variable named `z` at any point. That will fuck your shit up. I also have no idea what `f(x)` is. If you're asking for help, provide all the code backing it or a reproducible example. Third, closure typicaly means you're indexing a function. Which is exactly what you're doing. But that's not your only mistake. Here is what your code is supposed to look like: spvgm = function(x){ x[which(x &lt; 4086] &lt;- x[which(x &lt; 4086] * 0.5 x[which(x &gt;= 4087)] &lt;- 0.5 return(x) } x = 0:8000 output = spvgm(x) plot(output) Your biggest mistake is `lines(spvgm)`. You're asking R to plot your custom function. The function is a method, not a numerical value, so obviously there is no way to plot it. It's like saying `lines(sum)`. Edited code.
Your solution won't work because `if` isn't vectorised. Also, these `return`s are unnecessary syntactic noise. The `return` function in R is used for exiting a function early. Since every expression in R has a value, it's consistent that the last expression in a function is its (return) value. No need to signal that redundantly.
&gt;[**R tutorial: Joining data in R with dplyr [3:24]**](http://youtu.be/M_mt0vrP_hg) &gt; [*^DataCamp*](https://www.youtube.com/channel/UC79Gv3mYp6zKiSwYemEik9A) ^in ^Science ^&amp; ^Technology &gt;*^802 ^views ^since ^Nov ^2016* [^bot ^info](/r/youtubefactsbot/wiki/index)
&gt; Also I don't see why always using return() wouldn't be best practice Because it’s utterly useless and clutters the code. I’m guessing you’re not otherwise in the habit of putting things that have no effect into your code. Put more pointedly, do you usually add `+ 0 * 1` to each arithmetic operation in your code? Or `&amp;&amp; TRUE || FALSE` to each logical condition? Because that’s the equivalent of using unnecessary `return` function calls. &gt; unless it is somehow slower. Performance isn’t the only consideration when writing code, and it isn’t the most important one by far: correctness, readability and maintainability are. However, calling the `return` function is of course *also less efficient*, since it’s, well, an extra, unnecessary function call.
&gt; Lol so clearly marking the exit and output of a function inhibits readbility? No, but doing so *redundantly* may (and does, in this case). In fact, if you need `return` to mark function exits for you then this is an indication that your function is too complex, obfuscates the logic flow, and should be decomposed into smaller bits. [Just look at the code you wrote yourself](https://www.reddit.com/r/Rlanguage/comments/5xqjpq/if_statement_help_plotting_it_after_gets_an_error/dekbxjg/) — clearly, `return` is completely unnecessary to show (1) where the function exits, and (2) what it returns. &gt; Lmao redditors choose to fight on the dumbest fucking hills I recommend reserving disdain and ridicule for situations where you’re sure that you’ve actually grasped the matter at hand. And even then — it’s hardly productive.
&gt; clearly you're being driven by something other than a desire for conciseness and efficiency Yes: I’m driven by a desire to make people write better code, and to understand code better. It’s as simple as that.
I have used a heaviside function for similar code: &amp;nbsp; [1] http://stackoverflow.com/questions/40085772/how-can-i-make-a-heaviside-function [2] https://r-forge.r-project.org/scm/viewvc.php/pkg/fBasics/R/utils-Heaviside.R?view=markup&amp;root=rmetrics &amp;nbsp; These two links should help
Base R simply doesn’t seem to have a solution. There’s a discussion with several solutions [on Stack Overflow](http://stackoverflow.com/q/11095992/1968). The recommended solution is to use the function `permn` from the ‹combinat› package, or `permutations` from ‹gtools›.
 Gi &lt;- This is not doing what you think it does. It is assigning to a constant variable named "Gi". You want assign(paste("G", i, sep=""), ggplot(...))
 prob_lotto = function(k, n, i) { x = sapply(replicate(i, diff(sample(1:n, k)), simplify = F), function(x) { ifelse(test = all(x &gt;= 1), yes = "Ascending", no = ifelse(test = all(x &lt;= -1), yes = "Descending", no = "Neither"))}) return(table(x)/i) } Your function can basically be written as one liner. Loops are more verbose than *apply series function. Try not to write nested loops in R; they are slow and messy. Draw random integers with sample instead of loops + runif + ceiling. There might an bug there; you can't generate 1 as a lotto number (didn't watch video, maybe this is not supposed to be possible). Some benchmark, n = 1000 you = function(u) {probability_of_ordered_lottery_numbers(5, 100, 10000)} # Has printing stripped me = function(u) {prob_lotto(5, 100, 10000)} microbenchmark(you(), me(), times = 10L) Unit: milliseconds expr min lq mean median uq max neval cld you() 177.03440 179.1430 196.0304 182.07153 183.56858 321.77610 10 b me() 23.57808 24.4182 24.7195 24.61698 25.03249 25.75062 10 a n = 10000 Unit: milliseconds expr min lq mean median uq max neval cld you() 5112.6863 5240.3371 5528.8342 5623.742 5715.2489 6033.3105 10 b me() 245.1867 255.6297 269.2602 258.657 292.5351 298.7594 10 a n = 100000, your code doesn't run in reasonable amount of time. Unit: seconds expr min lq mean median uq max neval me() 2.875089 2.889185 2.966155 2.929949 3.021 3.162175 10 See nested loops are bad for scaling.
Thank you! That is far more elegant than what I have. In the future, I hope to utilize the various *apply functions more and more. I'll have to look a couple things up in your code, before I fully understand what it is doing. I did run it with various numbers of iterations and it definitely works! 
Commented # same function arguments prob_lotto = function(k, n, i) { x = sapply( # Feeding this into sapply, what is this? A list of without replacement lotto draws, also take first difference replicate(i, diff(sample(1:n, k)) # Telling sapply to not simplify results to a matrix, retain list structure , simplify = F), # Here is the function to test for descending/ascending, operate on each element of list function(x) { # If all first difference &gt;= 1, ascending, if all &lt;= -1, descending, etc ifelse( test = all(x &gt;= 1), yes = "Ascending", no = ifelse( test = all(x &lt;= -1), yes = "Descending", no = "Neither" ) ) }) # Generate an output, just tabulize the count and divide by total iter. Print whateveryou want here with results from x return(table(x) / i) }
wrap a which() around your condition Your code doesn't actually return rows where there are NA (the actually data itself). It actually fills all variables in rows where at least one logical check is equal to to NA all to NA. Be careful when working with NA in R; see the results of following: `TRUE&amp;NA; 1+NA; NA^2; mean(NA); which(NA)`
You already got a decent alternative via /u/p0olp0ol but I'll comment on some of your code directly. tix_asc[i] &lt;- if(length(asc_match[asc_match==TRUE])==k) {TRUE} else {FALSE} - It's fairly common to see in beginner's code something that looks like `if(condition){TRUE}else{FALSE}` but note that you'll only return TRUE if the condition was TRUE and you'll only return FALSE if the condition was FALSE. So the entire if statement isn't necessary and you could have just used `condition` directly and avoided the unnecessary `if` - You might be interested in the `is.unsorted` function. - `asc_match[asc_match==TRUE]` once again you don't need to compare to TRUE and could just use `asc_match[asc_match]` directly. With that said the larger context of that is: `length(asc_match[asc_match==TRUE])==k` so really you just want to count how many TRUE values there are. With a logical vector you can use `sum(asc_match)` to get the number of values that are TRUE so `sum(asc_match) == k` would accomplish the same thing. And with that said in the larger context you really want to know if all of the values in the vector are TRUE. And that is what the `all` function is for. So `all(asc_match)` is really all you need there (but if you look into `is.unsorted` that might get rid of this chunk of code entirely). - To my eye it looks like you never use the values calculated as `my_tix_matrix` and `final_matrix`. You could either get rid of the lines or use them as you intended to.
Can you provide some kind of reproducible arguments for how you are running this function? Otherwise, try printing out mypeople[row1 - 1, col2] and seeing what that is equal to. I'd suspect it's equal to nothing and that's why your boolean expression is failing. Also, in the future, try harder than just pasting your function to a forum with the error and asking "what's wrong, I've tried everything." Show what you've tried, make it reproducible and maybe write some comments. Help us help you.
In regards to time series, your frequency is the number of cycles of observations in your time frame. If I had a year of data, given in months, that would mean my time series has a frequency of 12. I'm not sure what your professor is asking. If a time series has a frequency of 1, then it only has one iteration in its time range, which doesn't seem quite right in terms of what data you are collecting. 
consider using break() in parts of this function as a debugging method, or another form of checking just above the if statement which checks the value of mypeople[row1-1,col2] == "I" and breaks off when it encounters a non boolean. then you inspect the dataset further what is happening, e.g. you get what value of row1, col2 is and find what is fucky in the data at that point.
When we place the code: print(mypeople) in line 14 and then close off the function brackets we get: http://imgur.com/gallery/v3e0d
When we place the code: print(mypeople) in line 14 and then close off the function brackets we get: http://imgur.com/gallery/v3e0d
mypeople[row1 - 1, col2] is NA so '==' returns NA which if cannot work with print or save mypeople in every for loop step (or execute the function code by hand). To see at what point you don't replace the NA with a char, which causes the issue.
Figured out quite the inelegant solution. I flipped the order of the columns so the algorithm in combn() created a "mirror image" of the combinations...
&gt; is.unsorted wtf
I prefer to do this dplyr-style: library(dplyr) DRINKING_WATER %&gt;% filter(authority == 11 &amp; Sodium &gt; 21 &amp; Turbidity &gt; 0.4) It's much easier to read than base R in these situations, I think; you don't have to repeat DRINKING_WATER all the time; and the hanging comma isn't an issue.
What's the matter?
Yes, never seen it. Doesn't check the opposite which I guess is because this checks unsorted 'without the cost of sorting it.'
Kaggle is too hard (IMO). I think its the toughest form of programming challenge.
I guess my question was not clear. I am not looking for tutorials/courses. I am already aware of those. My question was specifically on programming exercises. Or Programming Kata types of resoures.
Eh, still. I think she can get the basics of R if she does a few simple problems, like the iris and titanic. But yeah, I understand what you mean. Some of the challenges there require pre-processing, and the methods you run into problems with the methods you want to use sometimes.
Emojis are very useful for s-a. In fact smileys can act as unsupervised yet known quasi-labels!
did you even read his comment? fuckin lol 
Surely there is a way to decode them back to a representative token word at an early stage in your munging. It's worth the effort. In fact I like to split my munging/leaning up into two parts - steps over the whole document as a string, then steps over the tokens. Doing a map back to a word might muddle with that a little but try it and run your stuff with emojis and without - just set up a boolean + if to turn it on or off. 
Ok can you check my code: https://github.com/tnvrsingh/senti-Twitter-script/blob/master/senti-Twitter.R
You could wrap the script around an observeEvent which runs when the user input changes
[DataQuest](https://www.dataquest.io/) Click on projects.
would it use diag()?
fixed - data&lt;- cbind(data, do.call(rbind.data.frame, lapply(data$location, geocode))) 
When you ask any programming question in any forum, it's good practice to include the input you want to give, and the output you expect to receive. As it stands, your description is incomplete, because I have no idea what exactly you expect your output data structure to look like, nor what the structure of `data` is except that it contains a column called `location`. What does `geocode` return for example; giving two values can be tricky depending on how exactly you want the data to be structured.
yeah, fair response and you are quite right. In this context I didn't explain in that level of detail because I assumed that everyone know what geocode did, and would be able to assume the expected input/output of the function. After reading this I realised that actually the ggmap library is probably not as commonly used as I initially thought.
I wasn't familiar with the function, but that's not so important. It's more that even if I was familiar with it, I couldn't know for a fact that that's what you were using. When you're talking about bugs, or other problems you've encountered, it's often because you've misunderstood something or failed to specify something properly. For example, if I gave you advice on `ggmap::geocode` and it turned out you were using `dismo::geocode`, my advice isn't going to be much help.
My guess is that you mean to pass your G and T data as the params `g` and `t`. You can do the following: bon.test(t = c(1,2,3,4), g = c(10.0,11.0,12.0), alpha = ...) And replacing values appropriately. The c() function takes a series of scalar values and concatenates them into a vector. 
If you're on a unix system, apparently the --grow option to the zip command can do this
Sadly, Windows. 
Considering you're able to express the logic clearly, why not use the ifelse function? 
Maybe git bash gives you this functionality. You should use it anyway, it's incredibly useful. https://www.git-scm.com
Hmm there should definitely be a line for Trt, try replacing your code with: aov(Knowledge~Period+Trt*Pre_Post, data=AttitudesStack). What you have should work since it's just the longer way of writing what I've put above. Let me know if it still doesn't give a trt effect. 
Sure, how can I send it over?
And is the data balanced, so the same number of observations for each variable?
As a mom, I totally empathize with your sleep deprivation! I tried out the lm and it's giving me the t values for each period. I have roughly one semester of stats work in R studio in my experience folder and am trying to finish my master's thesis in environmental science and education. Unfortunately, my advisor took a position at another university out of state and is unable to sit down with me and really take a hard look at the data. My stats professor has been very helpful, but I feel bad pestering him with questions when I am not even his grad student. We did some linear modeling in R, introduced ANOVA/ANCOVA, multiple comparisons, and a dash of others. Learned alot but it's still new to me and deadlines loom ahead. :( Thank you again for all your help so far. Just having someone to troubleshoot with has been a godsend.
Yeah, so linear regression uses t values to determine significant predictors. Now that you've saved your linear model, you can perform an anova on it, but correct for the unbalanced design. So you should do: install.packages("car") library(car) mod1 &lt;- lm(y~x1+x2) Anova(mod1, type="III") where you replace mod1 with your model. Another thing to consider is, I'm not sure how your study was designed so if this doesn't work I can take a good look at your data if you wanted. 
I suspect that this is the path that I'm going to have to take. Unless I create the files in the TMP folder and ignore them. 
dnorm gives the density - not the probability.
That is incorrect. &gt; dnorm(0,0,.1) [1] 3.989423 Probabilities must be between 0 and 1. If density is the same as probability how do you rectify that densities can be greater than 1? You can't. Density is related to probability but it is very much NOT the probability of observing that exact value (at least when talking about continuous distributions).
If you are familiar with dplyr, one way to do this is: data &lt;- mutate(data, Q1 = ifelse (is.na(Q1), Mean, Q1)) If not, I believe this is the same: data$Q1 &lt;- ifelse(is.na(data$Q1), data$Mean, data$Q1) A simple loop to replace many columns: Q1 &lt;- c(2, 3, NA) Q2 &lt;- c(NA, 4, 5) Q3 &lt;- c(4, NA, NA) Q4 &lt;- c(3, 2, 1) Mean &lt;- c(3.3, 3.2, 4.55) data_missing &lt;- data.frame(Q1, Q2, Q3, Q4, Mean) data_filled &lt;- data_missing vars &lt;- names(data_missing)[1:4] for (variable in seq_along(data_missing[vars])) { # For each column, for (observation in seq_along(data_missing[[1]])) { # and for each row: data_filled[observation, variable] &lt;- ifelse ( is.na(data_missing[observation, variable]), # If data are missing, data_filled[observation, 5], # Then replace in with column 5 (Means) data_filled[observation, variable] # Otherwise replace with itself ) } } 
Check class(table) You will find that 'table' is a matrix, not a data frame. Matrices can't be subset by name - only by position. You can fix this using: table &lt;- data.frame(rbind(row1, row2, row3)) The reason why the for() loop method works is because it is subsetting the data set by index position rather than name, which does work with matrices. Hope this helps.
In this particular instance (looking at the density at the mean) you do get that special relationship. But it isn't true in general &gt; dnorm(0,1,.1) [1] 7.694599e-22 &gt; dnorm(0,1,1)*.1 [1] 0.02419707 And how would you explain the density for let's say a uniform random variable on the support [2, 2.0001] which takes a value of 0 outside that range and a value of 10000 in [2, 2.0001]. The density is very much related to probabilities but this is a counter-intuitive point that I know takes a while to understand... For a continuous distribution the probability of observing any given value is 0. In other words if X is a continuous random variable then P(X = 1.3) = 0 and P(X = 0) = 0 and P(X=1)=0 and it doesn't matter which value I choose for the right side of that equality so P(X=x) = 0 for any value of x. Density is not the same thing as probability except in the case of a discrete distribution. It's highly related but it isn't the same thing. I mean... I can make more arguments for why density is not equal to probability if you want but I'm intrigued and want to see what other arguments you can come up with in favor of your ideas. It might help me when teaching this stuff again to see the misconceptions that are out in the wild.
There are lots of reason to call dnorm. But please clarify what you mean by "assuming a discrete distribution" in your last line. Because the normal distribution is not discrete. If you want a discrete version of the normal then you might use dnorm to create that function but you wouldn't use it directly. And I think the semantics here are greatly important because I don't think OP truly understands the difference. And to be honest your wording is troubling. You say things like "probability of drawing exactly 0 in a standard normal distribution" but the probability of drawing exactly a 0 in a normal distribution... is 0. If you tried to tell me the probability of drawing a 0 from a standard normal distribution is approximately .3989 then I would take that to me that approximately 40% of the time I would get *exactly* 0. But hell when making random draws from a standard normal I only get draws that are a max of .1 away from 0 about 8% of the time. I can understand not caring too much about semantics but I do think it is an important distinction and if you understand the difference why do you treat me like I'm the idiot when calling out the distinction?
colClasses if you can go through the effort of defining a vector of all the classes. Or the most likely reason it's getting read as a character instead of numeric is there's some non numeric value in that column. Additionally it sounds like you already have the data in a data frame the way you want it in another script, then you can just use save() and load()
Any chance you could share the data or a subset?
Also you can convert all columns with df &lt;- sapply(df, as.numeric)
how should we know based on what the labels are placed to help you? you obviously have to explain where you get the data from, and if you are talking about the position, or the scale (since at the moment you don't scale anything, which is done with 'cex' if that is your question)
I'm not sure about those directions, but anytime I am faced with a missing data problem I first see what the underlying missing mechanism is (MAR, MNAR, MCAR). Mean imputation will lead to biased estimates, regardless of it is just one item per person or multiple. Multiple imputation is what I personally use, or full-information maximum likelihood if I am handling anything on the latent level. These are modern missing data techniques, and while more cumbersome, they give more accurate point estimates and a smaller chance of possibly committing a type one error. 
Yeah the instructions for this thing was written in 1996, I also wonder what the reasoning was. I'll definitely look into those techniques, however, as these are questions that I've been pondering lately.
"building tools which can help people in removing the friction removed in working with data" wut?
Unfortunately not all of them are numeric. Most of them are categorical.
always a solution :) df &lt;- sapply(df, function(x) {if (sum(is.na(as.numeric(x))) == length(x)) {x} else {as.numeric(x)}})
try gsub()
Please provide a minimum working example. We dont have your data and it is easier that we can recreate your problem. Also, avoid using 'attach'.. it can cause some confusing when you are not strict in your naming convention
if it is a factor try plyr's revalue.
This has nothing to do with R as a language. This is a statistics question. The keyword you want to search for is something along the lines of `pearson correlation confidence interval', which brings us to psychometric::CIr.
If you have two variables and you want to check the correlation to see if it's significant than you need to run cor.test(df1, df2)
Thanks a lot for your explanation. I really appreciate it. I will edit my comment with the correct symbol. 
Dark high-contrast theme: Final (copy and paste into css file). [Here's](http://imgur.com/a/ADsd3) what it looks like. It's actually more vibrant than how it shows up in imgur though. Try it out! .ace_gutter { background: #000C1E; color: #dbdbdb } .ace_print-margin { width: 1px; background: #246 ; } .ace_editor { background-color: #111111; color: #FFFFFF } .ace_cursor { color: #FFFFFF } .ace_marker-layer .ace_selection { background: #303030 } .ace_selection.ace_start { box-shadow: 0 0 3px 0px #002240; border-radius: 2px } .ace_marker-layer .ace_step { background: rgb(127, 111, 19) } .ace_bracket { margin: 0 !important; border: 0 !important; background-color: green; } .ace_marker-layer .ace_active-line { background: rgba(0, 0, 0, 0.35) } .ace_gutter-active-line { background-color: #0058a5 } .ace_marker-layer .ace_selected-word { border: 1px solid rgba(179, 101, 57, 0.75) } .ace_invisible { color: rgba(255, 255, 255, 0.15) } .ace_keyword, .ace_meta { color: #00e9ff } .ace_constant, .ace_constant.ace_character, .ace_constant.ace_character.ace_escape, .ace_constant.ace_other { color: #ff0000 } .ace_invalid { color: #ff0000; background-color: #800F00 } .ace_support { color: #80FFBB } .ace_support.ace_constant { color: #EB939A } .ace_fold { background-color: #FF9D00; border-color: #FFFFFF } .ace_support.ace_function { color: #d000ff } .ace_storage { color: #ff1900 } .ace_entity { color: #ff1900 } .ace_string { color: #3AD900 } .ace_string.ace_regexp { color: #80FFC2 } .ace_comment { font-style: italic; color: #993000 } .ace_heading, .ace_markup.ace_heading { color: #C8E4FD; background-color: #001221 } .ace_list, .ace_markup.ace_list { background-color: #130D26 } .ace_variable { color: purple } .ace_variable.ace_language { color: #FF80E1 } .ace_meta.ace_tag { color: ##00ffff } .ace_indent-guide { background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAACCAYAAACZgbYnAAAAEklEQVQImWNgYGBgYHCLSvkPAAP3AgSDTRd4AAAAAElFTkSuQmCC) right repeat-y } .nocolor.ace_editor .ace_line span { color: #00fff6 !important; } .ace_bracket { margin: 0 !important; border: 0 !important; background-color: #00235b; } .ace_marker-layer .ace_foreign_line { position: absolute; z-index: -1; background-color: #000000; } .ace_marker-layer .ace_active_debug_line { position: absolute; z-index: -1; background-color: #7F803C; } .ace_marker-layer .ace_find_line { position: absolute; z-index: -1; background-color: #324E66; } .ace_console_error { background-color: #324E66; } .ace_keyword.ace_operator { color: #3880ff !important; } 
&gt; It kinda distinguishes between equals to inside and outside of functions. I guess I came to R after learning over languages first, so I never saw the need for this distinction. 
Once I discovered the alt - it sped up my coding a bit. It's really nice.
There's no difference, but `&lt;-` is a bit more readable. It also prevents other people from judging you haha
I like it. It's like a tiny arrow that pushes the value into the variable. Honesetly, I wish it was a single key on the keyboard`←`, so it could replace `=` as an assignment operator everywhere, freeing up `=` to be an equality operator. Plus, a decent number of data analysis websites forget to explicitly point out when they're writing R code, and `&lt;-` is an instant indicator. That being said, use `=` if you want to. There's nothing wrong with it.
No. You might be stuck in the notion that "=" means "assignment" from other languages, and R's assignment using "&lt;-" is just some unique quirk thing R programmers do because they are not good computer programmers. "&lt;-" operator has distinguishable meaning and intent to "=" in R. In fact, it prob is the opposite where people should stop using "=" for assignment, if you want better style.
&gt; some unique quirk thing R programmers do because they are not good computer programmers Never said that. 
&gt; It kinda distinguishes between equals to inside and outside of functions. For me, that’s a reason to use `=` instead of `&lt;-`: you *don’t want* to distinguish between these two situations. For instance, in dplyr’s `mutate` function, `mutate(x = 2 + y)` means “assign `2 + y` to `x`”. Shouldn’t we use the same syntax here as in other assignments? It seems to perform assignment. But we use a different operator than for other assignments. This certainly trips up beginners (advanced programmers know of course what’s actually going on).
I think we can all agree that &lt;- and = are both superior to -&gt; except for in piping situations (where I've been told it's useful, but still think it sucks aesthetically). 
There is probably a more elegant way of doing this and I'd probably put it in a tiny function, but this should work for you: # Use lubridate package require('lubridate') # The input you provided myDate &lt;- '2014-02-01 00:00:00.739166+01' # Grab the timezone offset by splitting the string myTZ &lt;- unlist(strsplit(myDate, '\\+')) myTZ &lt;- as.numeric(myTZ[2]) # Delete msec and TZ myDate &lt;- gsub(x = myDate, pattern = '\\.[0-9]+', replacement = '') # Convert the date and then add the timezone offset myConvertedDate &lt;- strptime(myDate, '%Y-%m-%d %T') myConvertedDate &lt;- myConvertedDate + hours(myTZ) myConvertedDate Note that this will produce an offset of your current timezone. If you want it as an offset of UTC then set your timezone first.
Basically, the &lt;- means that you are assigning a value to an object in the environment (whether the global environment or inside your function) and that object in the environment can be called later. The assignment within a function call does not allow the assigned variable (in this case your x column in the data frame) to be visible by itself in the environment.
But is this an important distinction? I argue (strongly) that it’s utterly irrelevant: whether you perform your assignment into an R environment or another object shouldn’t (and mostly *doesn’t*) matter.
ROC curves are intended to show how your choice of a cutoff for a probability will yield different true positive and false positive rates. Bottom line, you're prediction is only two unique values, 0 or 1. It should be a range of probabilities if you want to use an ROC curve. Make sense?
In the end, it comes down to personal preference. I'm not strongly for or against it, it just is and I've gotten used to it. In the end, your code is your own and you can do whatever you want with it :)
It seems you're putting the wrong things into your ROC function. As /u/rwrmf has implied, the model you're using should produce an array of continously distributed decision values (not necessarily probabilities). These decision values are then thresholded, and each value below the threshold is assumed to belong to the first of two classes, while every value above the threshold is assumed to belong to the second of two classes. That is, every point on the ROC curve represents a different threshold that resulted in a different prediction which in turn yielded different True and False positive rates. You can't construct an ROC curve from a single prediction (that just gives you a single point in the ROC space). Since you didn't post the relevant parts of your code it's hard to say where you're taking the wrong turn. Make sure to read the documentation of the functions you're using for the ROC curves and understand what exactly they're expecting from you. 
And even in that situation you can obviously use `=`, despite what lots of online resources claim: system.time((x = 1)) Though to be fair the parse tree for this expression is slightly different, and the actual expression being timed here would be `` `(`(`=`(x, 1))``, rather than `` `=`(x, 1)``.
Is it possible to implement the same thing using rvest? 
Can you give an example of grouping? Say if someone wanted to run the same analysis on a per-group basis. Or would that be something from the apply family?
Grouping as implemented by dplyr is pretty simple in concept. Take this example data: &gt; set.seed(50) &gt; df &lt;- data.frame(a = 1:5, b = c("a", "a", "b", "b", "b"), c = rnorm(5)) &gt; df a b c 1 1 a 0.54966989 2 2 a -0.84160374 3 3 b 0.03299794 4 4 b 0.52414971 5 5 b -1.72760411 You can use the dplyr function `group_by` to group the data frame and then the function `summarise` to run functions on each group, for example: &gt; df %&gt;% group_by(b) %&gt;% summarise(mean = mean(c)) # A tibble: 2 × 2 b mean &lt;fctr&gt; &lt;dbl&gt; 1 a -0.1459669 2 b -0.3901522 You can substitute a lot of functions for `mean(c)` here, or write your own.
Wait until you discover `-&gt;`, like `8 -&gt; k`.
This is cruel. 
Thank you! The column I want split has non-numeric data (the column is "nationality" with two options: British and Australian, so I'm trying to compare the British subset with the Australian subset of that makes sense). Is there anything else I need to add since the data is non-numeric?
No, I manage to send the report to gmail and using the gmail API who is way more simple than the Bing API I can upload the report to my database.
Try the swirl package
Ah, you are correct. I interpreted your statement to mean "something to learn by myself." 
You might want to add how you are reading it in (e.g. which package) and if possible a reproducible example. I haven't heard of text truncation yet, at how many letters does it stop, is there a special symbol at that position that might cause the issue?
What I am using now is read_excel, but I am100% ready to use a different function to do this.. I just keep failing to figure out what. 
Merge(df1, df2, by = VIN, all = TRUE) would be the most bare bones way you could do it. I'm having a bit of a hard time exactly visualizing your data, but that should be a start. This is essentially an inner join as you would do in SQL. Left or Right joins are done with all.x being a left join and all.y being a right join. If the fields aren't named the same thing, you can use by.x and by. y to identify the field names in the respective data frames.
You really need to provide example data when you ask for help like this. If you can't share your real data, create a toy dataset that still demonstrates the difficulties you're having. There is a lot of easy-to-confuse terminology in your post that means I have no idea what kind of data you're describing. Here are some examples of things from your post that I don't understand how they are represented in the data: * VIN * unique VINs * transporting a VIN * move * duplicates for moves * moving the same VIN * times By far the easiest way to avoid this confusion (explaining the meaning isn't really necessary, and I will not be able to become an expert in your domain in the time I will be able to devote to helping you) is simply to provide some examples of the inputs you have, and the examples of the outputs you expect, and we can help you get from A to B.
How married are you to the data.table format? Because this is super easy with dplyr and doesn't result in code that looks like an accident in a punctuation factory: &gt; dt &lt;- data.table(start_position = c(12345, 12378, 72399, 72677, 72780), + end_position = c(12350, 12399, 72401, 72701, 72799), + group = c("grp1", "grp1", "grp2", "grp2", "grp2")) &gt; dt start_position end_position group 1: 12345 12350 grp1 2: 12378 12399 grp1 3: 72399 72401 grp2 4: 72677 72701 grp2 5: 72780 72799 grp2 &gt; library(dplyr) &gt; dt %&gt;% group_by(group) %&gt;% mutate(start_diff = start_position - lag(start_position)) Source: local data frame [5 x 4] Groups: group [2] start_position end_position group start_diff &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; 1 12345 12350 grp1 NA 2 12378 12399 grp1 33 3 72399 72401 grp2 NA 4 72677 72701 grp2 278 5 72780 72799 grp2 103 &gt; dt %&gt;% group_by(group) %&gt;% mutate(start_diff = lead(start_position) - start_position) Source: local data frame [5 x 4] Groups: group [2] start_position end_position group start_diff &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; 1 12345 12350 grp1 33 2 12378 12399 grp1 NA 3 72399 72401 grp2 278 4 72677 72701 grp2 103 5 72780 72799 grp2 NA If you are attached to the data.table format, there is a package called dtplyr that might help, but I'm not familiar with it. Also, props for a really well-asked question! :)
Yeah, I'd do that with one table then. Join on your table that has costs for dates so each VIN-date combination has a cost attached. Then group by VIN and sum the costs. It would look like: &gt; library(dplyr) &gt; moves &lt;- data.frame(vin = c(1, 1, 2, 3, 4, 5, 5), date = as.Date(c('2017-01-01','2017-01-02', '2017-01-01', '2017-01-02','2017-01-01','2017-01-01','2017-01-02'))) &gt; rate_card &lt;- data.frame(date = as.Date(c('2017-01-01', '2017-01-02')), rate = c(153, 155)) &gt; moves %&gt;% left_join(rate_card, by = "date") %&gt;% group_by(vin) %&gt;% summarise(cost = sum(rate)) # A tibble: 5 × 2 vin cost &lt;dbl&gt; &lt;dbl&gt; 1 1 308 2 2 153 3 3 155 4 4 153 5 5 308 
&gt; code that looks like an accident in a punctuation factory: Totally stealing that line.
Thanks. You're missing a `]`. Also, we need this calculated by group. `dt &lt;- dt[, distance := start_position - shift(start_position, 1), by=group]`
I will assume it works. I won't get to it for a couple of days but I'm sure this will be enough of a jumping point to use the documentation. :) Thanks for all your help 
So do you think R Visual studio will take over Rstudio at some point ? 
Hi - No :). RStudio is a fantastic IDE and will be around for long time! RTVS basically gives you another option, especially if you're using VS already.
Is this available on Mac? 
What have you tried? My first guess is `getGOParents(go1769308.mf)[1][["GO:0003824"]][["Parents"]][1]`
Yeah - List[[1]][[2]][[3]] - would access the third list in - Also try: unlist(your.list) And it should open things up
My interpretation: For each unique customer id, you want to assign a random datetime. I'm not sure how you want to generate the datetimes, so I'm just going to assume it's from this century. I'm going to generate a random day between 2014 and today from a uniform distribution. It's been 1178 days since then. I can then convert it with as.Date(). This saves me from accidentally using February 31st or something like that. as.Date() returns dates in the "YYYY-MM-DD" format. I don't know the name of your dataframe, so I'm using "mydata". # I don't like destroying things, so I'm going to make a new column instead mydata$newdate &lt;- rep(NULL, nrow(mydata)) # for each unique id for(i in unique(mydata$customerid)){ # Generate random day as number of days since 2014-01-01 randdate &lt;- as.Date(runif(1, 0, 1178), origin = "2014-01-01") # Assign to all rows with current customer id mydata[mydata$customerid == i, newdate] &lt;- randdate } There are plenty of other ways and this one isn't particularly elegant, but it should work. It should also be pretty easy to read - we're going through each unique customer id, and assigning a random date to all of the dates with that id. I have absolutely no idea why you would want to do this. Like, none at all. Wouldn't it be easier to just remove the column? Or pretend it doesn't exist? 
Oh, right, it should be: mydata[mydata$customerid == i, mydata$newdate] &lt;- randdate If you need the date to be correct, why are you generating random dates?
for some reason it doens't create a new column and if i try to use the existing date column it removes it. I really can't see where the problem in your code is lol. The date that is used right now is incorrect and i can't substract that from the current date. I hope you can still help me
Thanks! I don't know why I assumed the dot in data.frame was anything special. It just seems batshit crazy to me after years of C++ and python! I take it underscores are not allowed in variable names in R?
That's mostly for historical reasons - the underscore wasn't allowed in variable names &gt;10 years ago
?ifelse works on vectors, just set up the 3-4 conditions when you want what to happen inside each other 
Historically, underscores weren't allowed but periods were, so it became a thing. Personally I don't like periods in names for anything - variables, columns or list elements, function arguments, class names, whatever. The reason for this is that in one of R's object-oriented systems (btw, R has three OO systems, which should make you laugh coming from C++ and Python! :P) the period in a variable name *does* have a special meaning. You might have wondered if you'd ever looked at the code for a function like `print` what the UseMethod function is. &gt; print function (x, ...) UseMethod("print") &lt;bytecode: 0x0000000005da3848&gt; &lt;environment: namespace:base&gt; This is part of the S3 OO system. What this does in a nutshell is look up the classes of the argument to `print`, and then look for a function called `print.&lt;classname&gt;` and run that instead. If you type `print.` on the command line in RStudio (don't press enter, just look at the function list that pops up) you'll see a list of all the print methods for various classes. So, this makes . a special case in variable names, and it means that the function called `print.data.frame` is ambiguous. Is it the `print` method of the `data.frame` class, or the `print.data` method of the `frame` class? Your guess is as good as mine!
This might answer some of your questions: https://google.github.io/styleguide/Rguide.xml 
Nope, from my experience it works on dates, characters, numeric, maybe more... it even names the elements of the list by the value it was split on. It's quite convenient :) I typically use it on stock codes in order to calculate lagged/shifted values and then re-consolidate the list of data frames back into a single data frame with dplyr::bind_rows.
I was kind of thinking of a subreddit that dedicated to only sharing links to articles that related to R Programming but I couldn't make up my mind sometimes. When I made up the rules I kind of couldn't choose between only articles or let user ask for help. It is easy to ask for help in the same place of where the articles are. Nevertheless, I also like your opinion of centering everything into one subreddit, but I do also like a subreddit that mostly contains good articles or links to good articles and discussions that are only related to those articles that were shared. It just sometimes, I can't choose. I know that can sometimes make a fool out of myself but hey what the heck right?
In my opinion, when I program in R, the &lt;- help me be able to distinguish between a function and an actual variable declaration. For example, this would help you, when you re-evaluate your code for errors. a &lt;- file(atmp, "w") cat(file = a ...) If there were thousands of lines of codes, sometimes that can help you plenty in being able to distinguish varieties of thing. It may seem my statement does not has very much affection. However, it is mainly because there are only two lines of code for demonstrating.
**Here's a sneak peek of [/r/rstats](https://np.reddit.com/r/rstats) using the [top posts](https://np.reddit.com/r/rstats/top/?sort=top&amp;t=year) of the year!** \#1: [The xkcd package for R](http://xkcd.r-forge.r-project.org/) | [15 comments](https://np.reddit.com/r/rstats/comments/5v6z2l/the_xkcd_package_for_r/) \#2: [Announcing RStudio v1.0!](https://blog.rstudio.org/2016/11/01/announcing-rstudio-v1-0/) | [10 comments](https://np.reddit.com/r/rstats/comments/5ak26h/announcing_rstudio_v10/) \#3: [Top 50 ggplot2 Visualizations - The Master List (With Full R Code)](http://r-statistics.co/Top50-Ggplot2-Visualizations-MasterList-R-Code.html) | [13 comments](https://np.reddit.com/r/rstats/comments/5np36c/top_50_ggplot2_visualizations_the_master_list/) ---- ^^I'm ^^a ^^bot, ^^beep ^^boop ^^| ^^Downvote ^^to ^^remove ^^| [^^Contact ^^me](https://www.reddit.com/message/compose/?to=sneakpeekbot) ^^| [^^Info](https://np.reddit.com/r/sneakpeekbot/) ^^| [^^Opt-out](https://np.reddit.com/r/sneakpeekbot/comments/5lveo6/blacklist/)
i would not want to and wasn't intended to fracture the R's community. I just wanted to create a community for R's programmer to have a place for sharing. In fact, I really like R and the concept of vector data structure. I think the concept of vector data structure is really great for a graphic rendering engine. In my opinion, I think R can be a superior programming language for a graphic rendering engine if R is heading in that direction. It is not a bad direction. With the current era, games and graphic applications all requires 3D and R's vector data structure is capable of handling multi-dimension storing of value which can translate to xyz positions. Nevertheless, sorry for making it feel like I intended to fractured the community.
This is easy. You can use `list.files` to return a vector of file names. You might also find the `recursive` and `include.dirs` arguments useful. Once you have a vector of file names, you just need to write a function to handle one file name. Your function just needs to do three things: 1. Build a new file name with a different extension. I recommend `tools::file_path_sans_ext` for this job. 1. Read in the file - use `readRDS` 1. Write the object to the new name. I recommend `readr::write_csv` over `write.csv`. Then you just use sapply to apply this function to the vector of file names. Do note however that an RDS file doesn't necessarily contain a data frame and thus `write_csv` might error. You should probably have a trycatch in your function too to catch those errors and do whatever recovery steps you want it to. 
Add a criteria to the start of your if that uses is.na to check for NA values and ignore them too. Also, don't use a for loop for this :(
Thx i'll give both a look.
Not the author of the packages, but the author of the blog.
It's possible to read each file in as a data.table to make sure it's a csv file. 
Apologies, I misunderstood previously. Yes, you're correct. Thank you. 
You can do some basic checks like typeof or class to make sure it's the right type before writing. But then if you are fairly sure and this is a one time thing just use a try catch and deal with the exceptions manually. 
I'm thinking that they don't actually EQUAL each other at the cross-over points - they just get larger or smaller than the other, and the lines connect these points making it look like they actually have the same value. Consider - subtract (or just see which is larger) one vector from the other, and at the sign changes - you'll have your cross-over points.
R is vectorized, so you don't need to run loops. You can use: counter = sum(x$1 &gt; x$2, na.rm = T)
Here's a version with the base system data.frame( sapply( levels(as.factor(strsplit(paste(df$var2, collapse = ","),",")[[1]])), # For each of the different values within df$var2 function(x) grepl(x, df$var2) # See if the letter appears in each var2 row ) ) Output: a b c d q w z 1 TRUE TRUE TRUE TRUE FALSE FALSE FALSE 2 TRUE TRUE FALSE FALSE TRUE TRUE TRUE You can wrap the grepl bit with as.numeric if you want to turn them into 1s and 0s but functionally they'd be the same. 
You might want to make sure that you understand what the pipe operator (%&gt;%) actually does. In this case, I don't think it makes sense that you are using it with your for loop. This is what you are doing: x &lt;- leaflet(data=airports)%&gt;% addMarkers(~long,~lat,popup=~as.character(info))%&gt;% addTiles() z &lt;- for(x, j in 1:nrow(flights)) ... See the problem? If you don't, look up the for-loop syntax. 
Holy... This actually worked. I am surprised because my RStudio has a mind of its own sometimes. Thanks a bunch!
I am just as puzzled as you are :-D
It would be easier to cbind them into a matrix then change the class to data.frame. 
Thanks :)
I don't know what lrm does, but try: ` test &lt;- lrm(Y~ x+z, data=text)` ?
Yeah it still throws similar error... lrm is from the rms package
apply works on a matrix (or more general array) not a data.frame, dat is therefore transformed and $ is list and data.frame specific, '[' works if I remember correctly (you can pass '[' a name) EDIT given the second part of your question ?lapply it returns a list (which can contain whatever you want), apply returns a vector, matrix or array 
Do you want to classify based on some text in the column? From looking at you example, you want all cases where an n appears to be coded as 1? Use the stringr package id &lt;- c(1,2,3) var1 &lt;- c("a,b,n", "b,w,a", "n,a,o") df &lt;- data.frame (id, var1) library (stringr) df$var2 &lt;- str_detect(df$var1, "n") Gives you: id var1 var2 1 1 a,b,n TRUE 2 2 b,w,a FALSE 3 3 n,a,o TRUE
I used these for a project last year and thought they were really fun. 
*Please* read the documentation of the function before doing anything else.
You are getting a ton of errors because your code contains a ton of errors. Line 3 is missing a ). You have 4 ( and only 3 ) on that line. You are also missing some kind of symbol on that same line between the 162 and the Team1[4]. The HomeP line has 10 ( and 12 ) so it's just all screwed up. The way I debugged this, just to help you for next time, is to remove all the code except that which was necessary to generate the first error. The "multiple errors" are just because those errors are screwing up all the later expressions. 
This a good example of why building and testing code one functional piece at a time is a great idea. Not only do you catch problems along the way, but it keeps you alert so you don't write the same problem multiple times. 
R has very well documented packages, and usually full of examples that explain things. You'll probably get a better understanding of what's happening that way so you don't make this mistake again. 
Please provide a MWE, a small dummy dataset and the code you are running... looks like you may have too NAs in your data and the function is complaining about it...
Another useful thing that can help you debug these sorts of issues is to use RStudio (if you aren't already). It includes bracket highlighting, so when your cursor is next to a ")" it will show you the matching "(". The newer version of it also includes squiggly red lines (like Microsoft Word's spellcheck) that will alert you to any syntax errors like missing brackets and so forth.
What you're saying is a little bit like the teacher who says read the whole textbook before asking me a single question. It's not necessarily that people are lazy, but it is almost *definitely* easier to get the answer from someone than it is to read the whole doc. Further, some people will respond better to understanding another person explain it than having to read poorly worded and flippant documentation.
It's really not at all accurate to compare r documentation to a textbook. The documents are usually like 3 PDF pages or so for simple things. I get what you're trying to say but I just don't agree that it applies here. 
Trying to learn R right now and I do this. Like, I'm borderline OCD about checking the code I write. Half the time it still comes out wrong, but hey, beginner probs.
Pretty much *any* half way modern code editor contains bracket highlighting, not just RStudio.
The script doesn't run successfully without instantiating `json.comments` first. Your successful "test" was most likely because `json.comments` was already instantiated in your working environment before you re-ran the script. Trying to run the script on an empty environment threw this error: Error in rbind(json.page$data$children$data, json.comments) : object 'json.comments' not found It's a neat idea, thanks for sharing the script! But it's also a lesson in testing your code. There are certainly more robust ways to test your code, but a quick "dry-run" can often reveal lurking errors. `rm(list=ls())` will clear the working environment of all declared objects and loaded packages. Try running your script again, after clearing the workspace with `rm(list=ls())`.
Thanks, I completely forgot about the environment, this is the first time I've had experience with that. Fixed!
This comment is spot on. R can essentially reproduce html code for interactive maps using the leaflet or mapview packages, but yea...if there's no R server fetching the data every time you interact with said map, they gave you a fool's errand. If they want you to build a page from scratch, that's a different story, and should look into shiny. 
Alpha.subset is only 25 rows, but you're trying to index the first 100 rows. Switching to `test &lt;- 1:5`, say, lets the code run without errors, though I think you probably want more data.
use shiny
or you can use an r script to create an rmarkdown report
Okay thanks! I am new to R and was using this tutorial for basics: https://www.datacamp.com/courses/reporting-with-r-markdown How do you save *rmd* file and a *css* file in R(programming language) to run. Do you know of other good tutorials online? Is this typically done in R Studio? 
Do I save the files as text and open in R(programming language)?
I only have access to R (programming language) at work. Cannot use R Studio right now even though it is free 
you can use the Rmarkdown package (I think the function is also called Rmarkdown) to compile your .Rmd file into .md/.html/.pdf files. This works independent of RStudio (that just nicely integrated everything into its gui) For dashboards I'm not as sure how it works, but that should also be independent.
What is your ideal output? I use ReporteRs to output to powerpoint because that is the format people want to see my data in.
As I seem to say perennially in these cases, you need to provide a *reproducible* example. I should be able to paste your code into a terminal and see the exact error you're experiencing. If you can't share your real data, share some toy data that shows the problem. I'm not aware of a case where those should be different, but that doesn't mean you haven't found one :) 
With what little information you provide one possibility is that you're getting NAs in your condition. You can avoid that by wrapping the condition in a which statement. In particular as.numeric is prone to returning NAs,, but any NA will result in the condition being NA. 
You have to test the value after it's tranformed to numeric. I don't normally use 'subset' but i strongly suspect it filters NAs, the 'raw' condition does not. Any string that doesn't directly translates to a number (including empty string!) will be translated to NA. So if it's an NA issue and you use the first method you have to wrap the whole condition in a 'which' statement. EDIT Just reread your reply. That command doesn't make sense. You're checking the number of rows in an object that doesn't have rows, and that's what the test shows you (NULL), not whether there is the same number of NAs. You should use 'sum' for that. Still, it would not tell you the full story unless you also convert to numeric before you test for NAs. 
Really appreciate your effort in fixing the code, so its always == inside an if statement when testing checking against input values, am I right. I am a newbie so I have more questions: 1. Can I use **else if** more than once ? 2. At the end of if statement, do I use **return()** or **stop()** ? If so under which situations ? Yea I know its quite silly. I couldn't come up with something better.
Well, this has nothing to do with machine learning... If i knew what the pattern in my data was i wouldn't need classification algorithms like glm, random forest etc. I just don't know how to do it with text. Right now i don't know the pattern in my data. I can't just assume that something is a 1 because it has a certain letter in the text column. The same goes for 0s..
I don't see Carson Palmer twice, but I do see other players listed more than once. Could you use test &lt;- df[unique(df$NameY),] It seems unlikely that two players would have the same name and draft year. /* It also looks like you're joining two datasets. You might have more luck cleaning one, and then using one of dplyr's join functions. left_join or inner_join might be what you're looking for.
http://imgur.com/a/9cwoY .. Just missing why they don't match on the name attribute
Definitely. Use RSelenium. It will let you scrape via browser. The chat appears to load and keep 100+ lines of chat history on the screen. The steps would be something like: have RSelenium load the chat, have it sleep for 30 sec - a minute, scrape/store the chat lines, repeat the previous steps, and then parse the data + remove duplicates via the unique chat ID when you are done scraping. See the screenshot: http://imgur.com/a/ktmIl - Each chat line has a unique ID and embedded data on the username and chat content. Edited my original answer as the chat does not keep an infinite number of chat lines stored.
Cool thanks! Those steps sound logical, I'll give it a go
Done that myself earlier, pasted name year and height together and was left with just a few players. only problem left now is the grammer differences between the datasets like use of . and ' in players names. Thanks for the help guys!
Alternatively, if you want it in a dataframe: database1&lt;- data.frame(Firstnames, Lastnames) database1$Fullname &lt;- paste(Firstnames, Lastnames, sep=" ")
I do not think rvest will be able to natively grab the content of the chat since it renders via javascript. You will need the javascript to render the HTML and then grab it, which can be a pain in the ass. RSelenium + PhantomJS should be able to get you there... library(RSelenium) rD &lt;- rsDriver(browser = "phantomjs") remDr &lt;- rD[["client"]] remDr$navigate("https://poloniex.com/trollbox") remDr$findElement("id", "trollboxTable")$getElementText() 
That's great.
The above is not ideal as only a few lines are pulled in. The below gets you the full chat as of the page load and saves it locally to be able to pull in and parsed with rvest. Unfortunately, R has some character issue with the formatting into a string and fucks it up. Python probably excels at handling that kind of thing. rD &lt;- rsDriver(browser = "phantomjs" ) remDr &lt;- rD[["client"]] remDr$navigate("https://poloniex.com/trollbox") source = remDr$getPageSource() sink("source.html") source[1][[1]] sink() file.show("source.html")
they are, and I find them especially suited for presentations
What did you search for on Google? I thought that I was decent at searching for thing, but for some reason searching for things about R is difficult for me.
"how to color bars based on variable ggplot2"