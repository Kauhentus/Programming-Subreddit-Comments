I will give you a recommendation that maybe many will disagree but it helped me to go from 0 to something. 1) For R I recommend you R for dummies. Is a really good book, written in a concise and entertaining way, and it provides an excellent overview of the data structures and its capabilities. If you don't understand data structures in R is hard to move on. 2) Once you have done R for dummies you could move on to An Introduction to Statistical Learning with applications in R, you can find it for free [here](http://www-bcf.usc.edu/~gareth/ISL/index.html). Really good for the basics of machine learning/data mining techniques. 3) For statistics, Andy Field's book is really good, but I agree with you that you need some background knowledge of R (R for dummies is enough for this) hope this helps
Thanks for the feedback. I will be looking into that ;)
A second nod for introduction to statistical learning. Partially because you can access it for free and I think some of the intuition behind statistical techniques is pretty good. I didn't think it was that good for the actual R code though. I would also suggest Max Kuhn's applied predictive modeling. Lots of examples and a good intro to the fantastic caret package. 
I'm thinking that R Programming For Data Science would be a much better replacement to the Art of R Programming. I found the Art of R Programming sort of disorganized, and also it also seemed to me that programmers from a C background would understand it better. I hadn't learned prior programming languages but I was pretty intermediate in R, and the Art of R Programming just seemed to do a bad job at explaining things. R for Data Science is a bit more well suited for beginners, and I think it helps you skip over a lot of unnecessary stuff. 
Add the na.rm = TRUE that was previously suggested. 
First, check that you have the correct data. In the console, both type `plotdata` to see it printed, as well as `str(plotdata)`to see the structure printed. The first will print the data nicely, whereas the latter will print information such as (my own data) 'data.frame': 10 obs. of 2 variables: $ x : int 4 14 25 39 51 61 72 86 98 108 ... $ y : chr "1" "1" "1" "1" ... In this example, pay attention to the second variable `y`; it might print as `1` but in reality, it is a character vector without numeric meaning.
Ok thank you so much! This was indeed the issue.
Levels have to do with factors, which are meant more for categorical variables than continuous numeric variables. For instance, if I had test scores for a class of students, a Gender variable would have the levels Male and Female for the students. Or, I might have numeric data on the effect of 10 different drugs; the Drug variable would have 10 levels, one for each drug tested
The levels (of a factor) are basically which different values your factor could be. (It basically shows you the unique values in the factor, but it can be overwritten and include more than only those.) Note that when you do ?levels in console, it is actually showing you the function levels(), which let's you see and assign the levels of a factor. There should not really be a clear reason as to why you would want them removed.
They're intended to be identical to 'levels' of a categorical variable in a statistical model - I.e, dummy variables. They're part of the 'factor' class identity. If you're not jumping right into stats on your data, then I'd advise working with 'character' classes. The 'stringsAsFactors' argument to e.g. 'read.csv' is useful for this.
I agree that’s it’s hard to imagine a reason to remove factor levels, but one could convert it from a factor to a character type. 
Haha, I forgot about something so basic, you just solved my problem with this last question! Thank you for your reply! These are not integers, but floating-point numbers, where someone used comma instead of dot. I was to deal with this, but as I had problems with loading the file at first (not only wasn't sure about quotation "" needed, but also forgot that \\\\ in the path is required, instead of \\ ), I actually forgot about this after loaded the data, then simply tried to start working on this. Now I added *dec = ","* as *read.table* parameter and it works now. And no levels were generated, as I have numbers now. Hooray! And thanks again :-)
Thank you for reply, I think I understand now. I was really confused and not sure why do I even had those, but looks like part of my problem was that I loaded numbers in the wrong way (comma is supposed to be decimal separator, but I didn't added parameter to fix it, while loading the data).
Thank you for explanation! Looks like I just messed up and loaded data in a wrong way (there's supposed to be a dot, instead of comma), so as these were not numbers (while I thought they are and couldn't understand why it generates levels and what they really are), the levels appeared. I fixed it now and learned more about setting them manually :-)
I might suggest (assuming you already know some basic stats): 1) Introductory Econometrics, Wooldridge - a well-written, accessible undergrad text on regression analysis 2) Statistical Inference, Casella - a very popular graduate-level stats book 3) R for Data Science/Advanced R, Wickham - two great books by a very well-know data scientist. Easy and fun to read, does not require a background in programming, though it helps if you already know a little. 4) The R Book, Crawley - I have this one, but unlike the others I have yet to read it. However, it is very well-rated and fairly popular, per my understanding. Of course there may be a better combination of books for your purpose, but these are the ones I'm familiar with, and should be enough to get you where you want to go. Godspeed!
No problem! I get data with euro decimals sometimes so that's what it looked like. You should definitely still look into the factor/character difference, it's important to know!
Is it a continued like (starting with a +)? If it worked before it should work again. 
Ahhh that makes a lot of sense, definitely a good place to start looking. Unfortunately, as it stands right now, I don't have enough familiarity to even know when syntax might be wrong. Going forward that'll be the first thing I check. Thank you very much for the response! I'm off my laptop at this point but I'll fire it up tomorrow and see if that's what caused the issue.
It did start with a + on the far left side. I think the problem that I had was syntax related, I tried out a new function (more related to my class) and started a new file without saving. Then I re-entered the first function from my previous file and must've typed something wrong. Thank you for the response as well!
I think you were in "continue" mode... ie you hadn't closed a quote, parentheses, brace, etc. R is hard. I love it, but I felt like crying when I was getting started.
There's an R for finance tutorial on datacamp. Your finance professor is cray. Two software AND finance in one course? Clearly a coke head. 
No prob! In this particular case, the issue is the space between the function name and the parentheses. In R, functions are called with the parentheses connected to the name, like so: function(x,y) And for functions without arguments, you'd do: function() In either case, the parentheses are part of the function call, not separate. Whitespace, indentation, etc. are more rigidly and literally interpreted in R than in other languages, and it's more similar to Python in that way. As long as you are diligent about the code as you're typing it (i.e. proofreading as you go), this won't usually be too big of an issue for you.
The class your taking doesn’t give you any help on basic R usage?
That's easy with a t-test. a&lt;-c(87,86,79,82,78,87,84,81,83,81) b&lt;-c(89,85,83,87,76,90,85,78,85,84) As for the test, use the `t.test()` function. You should be able to figure it out based on [this page](https://www.statmethods.net/stats/ttest.html) and [the documentation](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/t.test.html). I am not giving the answer since this is a homework problem.
You don't actually say what you used to calculate the scores. If you want help, you have to actually explain what you want.
Can you instead of writing to csv files write to a socket connection?
What about instead of writing new csv files, appending on your poll. Then instead of file reader use something like readlines() on the file connection. 
Try options(scipen = 999). It sets the "penalty" for scientific notation, ie. the point at which large or small values are converted to scientific notation.
Your question isn't really an R language question. Rather, it's more of an operating system (OS) question. What OS are you running R under on the HPC? I've run Windows Server, OS X, and Linux on HPCs. You're using the right command.(`setwd()`), but without knowing your OS and directory structure, we can't help you. If you run `getwd()` at the R prompt, it will give you a clue to the directory structure.
Linux OS
Ok. You have the general concept of the idea right. Linux doesn't use drive letters like Windows. In general, on Linux systems `~` is a shortcut for your *home* directory.
so I would do setwd("~/csv/") ?
Do you already have a directory named *csv* in your home directory? If so, then yes, that will work. However, as a general rule, you shouldn't be using setwd() in your code because then your code isn't portable. If you insist on using it, then make the working directory a parameter, like so. WORKING_DIR &lt;- "~/csv" setwd(WORKING_DIR) You can also create a new directory from within R using the `dir.create()` function, but again is recommend that you create the directory externally so as to keep your code portable. 
My code isn't formatting correctly, perhaps because I'm on mobile.
I'm by no means an expert and am not totally sure what your goals are but this looks like something that may be easier to do in python.
Did you import the data with `read.csv`? It sounds like one of the problems you're having is the old `stringsAsFactors` gotcha. If you use `readr::read_csv` or set `stringsAsFactors` to `FALSE`, it might fix the issue you're having here. I don't think you actually want a factor with 20k levels. But I could be wrong. See: https://simplystatistics.org/2015/07/24/stringsasfactors-an-unauthorized-biography/ Additionally, there's a way to `mapply` across a data frame. That might make more sense for what you're trying to do? (e.g., perform an operation across a data frame and extract a couple of vectors from it.) https://stackoverflow.com/questions/15059076/call-apply-like-function-on-each-row-of-dataframe-with-multiple-arguments-from-e https://stackoverflow.com/questions/12300565/return-mutiple-values-from-apply-function-in-r
Monte Carlo simulation is what you want. See for example https://bayesianbiologist.com/2012/06/11/simulating-euro-2012/ and http://srome.github.io/Making-Fantasy-Football-Projections-Via-A-Monte-Carlo-Simulation/.
Thank you!! :) It works after I make it numeric.
thanks
thank you!
No, I read it in via VCorpus(). You're right that I don't want a factor with 20,000 levels. Those are actually 20,000 unique words found in that column. I'll look into this mapply() though. 
&gt; Try converting the whole column to character first, rather than just the term you're storing in current_term. This has made matters worse. My output is now the following: terms_vec freqs_vec percentages_col cumulative_percentages_col 151 no surprise 10 0.0619047619047619 29.9 [1] "no surprise" [1] 10 [1] "no surprise" [1] 10 [1] "!!!???" [1] "no surprise" "no such" "no sense" [1] 10 6 66 &gt; Edit: also this is one of those questions that would be really helped with a sample of the data included. The intent of what you're trying to do is also rather unclear. Why are you looping instead of just subsetting with a regular expression? Most operations in R can be vectorised, so you should almost never loop over the rows of a dataframe. I'm relatively new-ish to R and I haven't yet worked out why my attempts at subsetting with subset() or the [] way sometimes don't work. I tried both and neither would work here so I had to resort to the loops to get the job done. Also, for some reason it wouldn't allow me to do: current_term &lt;- as.character(DF[i, 1]) current_freq &lt;- as.numeric(DF[i, 2]) vec_matched_terms &lt;- c(vec_matched_terms, current_term) vec_matched_freqs &lt;- c(vec_matched_freqs, current_freq) instead of: vec_matched_terms &lt;- c(vec_matched_terms, DF[i, 1]) vec_matched_freqs &lt;- c(vec_matched_freqs, DF[i, 2]) 
I'd do yourself a favor and take the 4 hours to complete the course "Introduction to R" from Dstacamp. It's free.You'll have a firm grounding in data types, vectors, matrices, data frames, lists, and functions and be light years ahead of your class. 2 hours on Saturday and 2 hours of Sunday and you'll have gone through a great R crash course geared towards data science.
I just so happen to have a 3 day weekend, I'll take this advice! Thanks for the resource, dude!
If you're working a lot with plots, ggplot is the perfect tool. It's bundled as part of the tidyverse or shiny (can't remember which), which this book will have you download in the first few pages. 
Right on, sounds like "open can here's your class" so I'm definitely excited to check it out! Looks like this weekend will entail quite a bit of R for me, haha. It seems like you're really familiar with this stuff, do you mind if I message you from time to time with conceptual questions similar to this post?
There's much more to learn about R after this class, but I think it will go far enough to get you on solid ground with your class applications. Wouldn't let it spook you, either. Usually in a class like this you're going to be taught what you need to know anyway. 
Datetimes can be a bit weird. One approach is to group by the case_id, summarise the max end datetime, and left join that back to the original dataset. This will give a new column where each row is the max end time for the case. From there, simply subtract the columns (making sure they are all formatted as datettime). df &lt;- read_csv("data.csv") df %&gt;% mutate(start = mdy_hm(start), end = mdy_hm(end)) %&gt;% left_join(df %&gt;% group_by(case_id) %&gt;% summarise(seg_end = mdy_hm(max(end)))) %&gt;% mutate(remaining_time = seg_end - start)
Thank you for the reply! This is close to what I want, but it's not giving the right answer - since I'm trying to figure out the difference between events in each case id, not the max one. So with case id = 1, I'd want 310 seconds (~5 minutes) between process 1 and 2, then 165 seconds (~2 minutes) and process 2 and 3.
I think one of these might be what you are looking for: df &lt;- read_csv("data.csv") df %&gt;% select(1,4:6) %&gt;% mutate_at(3:4, mdy_hm) %&gt;% group_by(case_id) %&gt;% mutate(activity_gap = difftime(start, lag(end), units = "mins") ) df %&gt;% select(1,4:6) %&gt;% mutate_at(3:4, mdy_hm) %&gt;% group_by(case_id) %&gt;% mutate(activity_gap = difftime(start, lag(end), units = "mins") ) %&gt;% group_by(case_id) %&gt;% summarise(remaining_time = sum(activity_gap, na.rm = T))
I'm not getting it to work. I think it's become too complicated with the time portion. It might be easier to tell based on the UNIX time alone. So it would just be 1396375110 - 1396374800 = 310 seconds between process 1 and 2 for case id =1 , and so on. Thanks for the help!
If it can get me through this class for now, that's awesome. R is prolific enough that I'm glad you can't leaarn it in a weekend. I'd imagine everyone would do it if that was the case! I don't spook easy but that will definitely be fuel for the fire when I want to give up. Thanks for the insight!
I'd be happy to field questions, with the caveat that I'm really new to R too. I may not have answers, but I'm always looking to learn more. 
This isn't pretty, and might not be what you're looking for, but give it a try: df &lt;- df[, 1:4] names(df) &lt;- c('id', 'start', 'end', 'procedure') df2 &lt;- gather(df, key = 'start_end', value = 'time', 2:3) %&gt;% arrange(id, time, procedure) %&gt;% group_by(id) %&gt;% mutate(time_left = time - min(time)) %&gt;% group_by(id, procedure) %&gt;% mutate(time2 = time - min(time)) df2$time4 &lt;- ifelse(df2$time2 !=0, df2$time2, df2$time_left) Take a look at the last three columns of df2, one might be what you're looking for. If you need the final product to look like the data that you posted, I think you'll have to use `spread()` to undo the `gather()`. 
It looks like what I thought was the problem, isn't. I'm having an issue similar to https://unix.stackexchange.com/questions/408355/running-r-script-via-shell-script-syntax-error-near-unexpected-token/408356 according to the error logs, but adding #!/usr/bin/env Rscript to the top of my script didn't fix it. [This is the script that I've been running to figure out where the HPC writes my data.](https://pastebin.com/raw/nSG6TWpx) 
Linux tres-l1 2.6.32-642.15.1.el6.x86_64 #1 SMP Fri Feb 24 14:31:22 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux /share/apps/R/MicrosoftR-3.4.2/lib64/R/bin/R
Ok. That's a good sign. You're definitely running Linux, and you have R in your path. I don't know if it's a valid R installation, but we can find out quickly. Run the following command and let me know what happens. `R`
https://pastebin.com/raw/qDgPzEVX
Excellent! Your version of R is working. Now that you're in the R environment, run the following command and let me know what it returns. `getwd()`
Ah, I think I understand now. Typing R gets me into R within PuTTY, and I can look at the directory structure as well as run my scripts. Also, my admin got back to me and made a copy of one of my scripts and fixed it so it ran, so I've got a working example to use too. Thanks so much for the help! Got me onto the working path I needed.
Try minimizing your example to the smallest possible example — as it is right now, I’m not sure anyone can realistically help without significantly going out of their way. I’d suggest looking into the reprex package.
I'm sincerely glad that I could help! Your investment in R will definitely pay off. Let me know if I can help any other way!
[removed]
 df$B[grep("foo", df$A)]
Does each customer have at least one row that is not NA?
Well as of right now I'd like something that can play MIDI, help visualize MIDI notes in piano roll form, and can write MIDI files. So I'm thinking about the traditional MIDI capabilities of a DAW and am ignoring all the audio signal processing stuff that DAWs do. Its only because of the project I've been working on that I can point out limitations of current MIDI handling capabilities. This came up for me during a project I'm doing where I try to automatically identify the melody track in a midi file. I've had to keep my copy of Reaper running in a separate window to verify that my program is selecting the melody track. It'd be nice to get the playback and track selection to happen within R. Any ideas about how to do that? Maybe a r package that could act as a wrapper for an existing, opensource, c++-based DAW... I don't know. I'm only still learning how to program. 
 &gt; df &lt;- data.frame( + X1 = c(NA, "customer1", NA, NA, "customer2", NA, NA, NA), + X2 = c(NA, NA, 1, 3, NA, 5, 7, NA), + X3 = c(NA, NA, 2, 4, NA, 6, 8, NA), stringsAsFactors = FALSE + ) Your data, it looks like. &gt; cust &lt;&lt;- "" `&lt;&lt;-` is a special "global" assignment operator. The variable survives whatever scope you're in. &gt; df$X1 &lt;- vapply(df$X1, function(c) { + if(!is.na(c)) { + cust &lt;&lt;- c + c + } else { + cust + } + }, c("")) `vapply` column 1 to another character vector. If it's NA, copy in the last seen customer name. If it's a customer name, leave it there and cache the value. &gt; df[!is.na(df$X2) &amp; !is.na(df$X3),] return a data frame containing only rows where columns 2 and 3 aren't both NA. X1 X2 X3 3 customer1 1 2 4 customer1 3 4 6 customer2 5 6 7 customer2 7 8
`grepl` but yeah.
Here's a ggplot2 solution library(ggplot2) ggplot(xc, aes(x = Output, y = value)) + geom_boxplot() 
Put what you’re trykmg to do in English please 
The %in% operator is already vectorised. It checks for each element in the first argument if they are present in the second, returning a logical vector. Thus you can just: matched_terms_vec &lt;- terms_vec1[terms_vec2 %in% terms_vec1] 
I just want the subset of terms_vec2 that appears in terms_vec1. I'm attempting what /u/Japhiri has suggested already though, it's running now...
OK thanks, trying it now...
You could also try: df1 &lt;- as.data.frame(v1) df2 &lt;- as.data.frame(v2) merge(df1,df2) I think that would do the same thing. If the names are different you can use by.x = "column name" arguments. http://clayford.github.io/dwir/dwr_05_combine_merge_rehsape_data.html 
FYI the apply functions are just wrappers around for loops. They aren’t vectorized. 
This will be quicker because repeatedly concatenating vectors in R is expensive. &gt; terms_vec_1 &lt;- c("ankh", "beta", "canary") &gt; terms_vec_2 &lt;- c("something", "anything", "beta", "canary") &gt; matched_terms_vec &lt;- terms_vec1[vapply(terms_vec_1, function(e) { + e %in% terms_vec_2 + }, c(TRUE))] &gt; matched_terms_vec [1] "beta" "canary"
Japhiri's answer is how you should learn to think about "simplifying" what you are doing by using vector instructions. In this case you might also want to look at ?intersect or ?setequal (they will kick out duplicates in your matched_terms_vec however).
What does 'not vectorized' mean? 
Comparing 2 lists in R can be done like this: I found this: &gt; A = c("Dog", "Cat", "Mouse") &gt; B = c("Tiger","Lion","Cat") &gt; A %in% B [1] FALSE TRUE FALSE &gt; intersect(A,B) [1] "Cat" &gt; setdiff(A,B) [1] "Dog" "Mouse" &gt; setdiff(B,A) [1] "Tiger" "Lion" And &gt; length(intersect(A,B)) [1] 1 &gt; length(setdiff(A,B)) [1] 2 &gt; length(setdiff(B,A)) [1] 2 
Vectorized functions are optimized for vector operations. I suggest reading the relevant chapter in R Inferno. It’s a free PDF. I’m on mobile now so I can’t quickly link to it. Maybe someone can comment the link on my behalf. 
Wow that's insane. I've got some serious tinkering with my code to do. This relates to a question I just signed in to post though. I'm calling a function I've made where the set of operations is identical every time, but the times required to run this code is not constant or even close to it - the repetitions I've done range between 0.19 seconds and 0.32 seconds to complete. They don't involve any random sampling, just take the same input and return the same output. What is it that causes the difference in run-time? I wondered if it could be that my laptop is running other stuff in the background, but when I look at my task manager there is still plenty (&gt;50%) of unused memory available.
This is a good question. It is the reason I usually use microbenchmark to benchmark my code. Microbenchmark allows you to repeat the code a bunch of times (default is 100x) and reports the min, max, mean, median, and sd of the run times. This allows you to get a better idea of what a "normal" run time for the code is. I am not a sophisticated computer scientist, so I don't actually know all the reasons behind the variability of run times. Perhaps it has something to with the physical proximity of the memory it has to allocate? I couldn't find any information in 60 seconds of google searching. Let me know if you find out why! 
I agree. For me, I would like to see much more of the math and computer science behind many of the answers I read. In this case, I am not 100% sure I am the right person to explain the things you suggest. I understand THAT the function is vectorized, and I know how to use this various approaches and time them. I have even learned which approaches are usually faster. I don't totally know why, though. I understand that the benefit of vectorizing a function is to reduce duplication (the memory only has to be allocated once, for instance) and I have even seen write-ups where people have proven that a well-written for-loop can be just as fast as a vectorized function when all the memory is allocated correctly in the beginning, etc. I don't know how to do this reliably on my own, so I use the wrapper functions that i am comfortable with and trust the smart ladies and men who wrote the C++ code behind the wrapper! Would you be willing to write up something about this logic to help explain how a beginner could go through the process I went through?
I'll just post it as a new thread now... 
If you're running the same exact calculations over and over, your system might be doing some behind-the-scenes optimizing every time you execute your code
Hmm. I do notice that if I do 10 or 100 repetitions, the mean time is two orders of magnitude greater than if I do 1000... 
I don't think it has to be as deep as the computer logic (though that would be an interesting explanation as well), but at least for this one: "The problem with your initial question [i.e. Fibonacci's] is that your for loop is taking the task and serializing it (having the computer work on it one piece at a time. Some R functions take advantage of modern processor's vectorized cache (the ability to do multiples of the same instruction all at once) to speed up that process. You [Fib] seem to notice that because you're already asking about Xapply functions. As pointed out, those are just wrappers for "for". As Infreq pointed out, there are smart people writing wrappers for C codes that take advantage of the cache speedup. Knowing how/when to do that is the key, and this is one of those cases. Always keep in mind that there are multiple ways to wind up at the same solution in code. %in% can handle that vectorization, and rather than compare each string to each other string by yourself, in can tell you "yes, I found a match." (Note 1. I don't know how it does this, and 2. it only tells you that there was a match. Many of my problems come in the form of I need something much more complex with this, and I still struggle in optimizing those.) A couple things may be speeding this up, in your original code, you check every token, (which may be what you need? we don't know, this is where things could get complicated) but it sounds like you just want to see what's the same. This means that the intersection of the set of words is all you're looking for. That's one function call, but all you get is the set of words that intersect (I stress that because you may want to do more with each individual word later, so it's good to keep in mind what information you lose when you optimize). 1 function call vs a loop of N function calls will speed your code up tremendously. 2 for loops (which is most "beginner" programming) would be N^2 (really NxM where M is the size of the second vector). If something seems like it's taking a long time, it's probably taking too long (from a beginner's standpoint) and it's good to break your code into pieces and 1) see what can be optimized with different approaches, and 2) go back to the planning stage and ask "what am I really trying to accomplish, and can it be done as a whole rather than piecemeal?"." (Here code would follow, and I have other answers where I break it down in a similar fashion) I don't know if that helps, but that's more what I'm looking for, pointing out where my error is, what I should be thinking about, and then how to achieve it.
What's meant by benchmarking?
"Seeing how long my code takes" Benchmark - "a standard or point of reference against which things may be compared or assessed." https://en.wikipedia.org/wiki/Benchmark_(computing)
Ahhh.
I'm on mobile, but try using "wrong$" as your pattern in the sub function. The '$' matches the end of the string.
 ‘gc’ returns a matrix with rows ‘"Ncells"’ (_cons cells_), usually 28 bytes each on 32-bit systems and 56 bytes on 64-bit systems, and ‘"Vcells"’ (_vector cells_, 8 bytes each), and columns ‘"used"’ and ‘"gc trigger"’, each also interpreted in megabytes (rounded up to the next 0.1Mb). If maxima have been set for either ‘"Ncells"’ or ‘"Vcells"’, a fifth column is printed giving the current limits in Mb (with ‘NA’ denoting no limit). The final two columns show the maximum space used since the last call to ‘gc(reset = TRUE)’ (or since R started). ‘gcinfo’ returns the previous value of the flag. This is the important part of the documentation. The integer numbers are in 'cells' and one NCELL is 56 bytes and one VCELL is 8 bytes. The total space is shown in the column next to these in Mb. The 'max used' column is the max used since the last garbage collect. The 'gc trigger' is the amount of space in which a new garbage collect will trigger. R should be taking up about as much memory as adding the two 'mb' in the 'used' portion together. 
Thanks
I am not sure I understand your question but is this what you are asking? combined &lt;-c(col1, col2, col3...) combined &lt;- t(combined)
Sounds like an integer optimization problem (allocation based on constraints) - something Excel solver is great for, though there are R packages I understand the process would be much more convoluted
It looks like the site you're trying to scrape is being rendered with javascript. This probably means you can't get the job done with rvest. I've heard of two approaches to scraping javascript-rendered pages: `Rselenium` or `phantomjs`, but haven't tried either.
 library(tidyverse) # example data dat = data.frame( aColumn = c("i1406, j9875, r4563, f5674", "l5764, r4563", "r9786, r4563, p9876, t5674, f4563")) dat = dat %&gt;% mutate(aColumn = str_split(aColumn, ",")) %&gt;% unnest() # colx # 1 i1406 # 2 j9875 # 3 r4563 # 4 f5674 # 5 l5764 # 6 r4563 # 7 r9786 # 8 r4563 # 9 p9876 # 10 t5674 # 11 f4563
try unlist
OK many thanks.
https://gist.github.com/Bergvca/c1df8e579005e3cd82e8d3c8b009403a
Not sure i got what you are looking for, but this will match only the cell which contain "wrong" and only "wrong" and will replace it by nothing string[grep("^wrong$",string)]=""
[This blog post](https://davetang.org/muse/2014/04/14/merging-multiple-data-frames/) should get you where you need to go.
It looks like you have three dataframes smashed together. You should split it into three separate dataframes df1 &lt;- data.frame(term = df$terms1, count = df$count1) df2 &lt;- data.frame(term = df$terms2, count = df$count2)... And then try a full_join or an rbind, then group_by(term) %&gt;% summarise(total = sum(count, na.rm = T))
not sure what's going on in your answer....what are col1, col2...? There is only one column in the input and should be only one column in the output. 
Oh ok I think I understand now, on my computer your question was formatted differently. It looks different on my phone.
aColumn &lt;- t(aColumn)
doesn't seem to be helping... R is seeing each row as one long string.
The important parts are `str_split` from the `stringr` package, and `unnest()` from `tidyr`. If you somehow can't get the packages, you can make do with `stringi` (the underlying, but less user-friendly version of `stringr`): library(stringi) dat2 = data.frame(aColumn = unlist(stri_split_regex(dat$aColumn, ","))) 
Excellent, thank you!
you need to split the strings into lists before unlisting. unlist(lapply(dataframe$column,function(x){strsplit(x,",")})) `strsplit` splits a string given to it by the delimiter (results in a list), `lapply` applies strsplit to each element of the list (results in list of lists) and `unlist` flattens the given list into a vector. This will work in base R. note: data.frame is just a list of lists with equal lengths but a column in a data.frame can be a list of lists with variable lengths. Read about vectors, lists, nested lists and data.frame. 
Look at if... else if... else statements. I think the code below does what you want. if(x &lt; -10){ print('x &lt; -10') } else if(x &lt; 0 ){ print('-10 &lt;= x &lt;= 0') } else if(x &lt;= 5){ pirnt('0 &lt; x &lt;= 5') } else if(x &lt;= 10) { print('5 &lt; x &lt;= 10') } else { print('x &gt; 10') }
Try this if(x&gt;5){ if(x&gt;10){ print("5&lt;10&lt;=x") }else{ print("5&lt;x&lt;=10") } }else{ if(x&lt;0){ if(x&lt; -10){ print("x&lt;-10") }else{ print("-10&lt;=x&lt;=0") } }else{ print("0&lt;=x&lt;=5") } } You have to be careful with brackets. Also, look into using ands and ifelse
this is an great lesson on why it's important to have proper alignment in your code it makes it a lot easier to catch mistakes like this. the else on line 13 does not have a matching if statement to go along with it. Also as /u/ExcellentOdysseus pointed out instead of using so many if statements you could really clean up your code by using "and" (&amp;) in your if statements so instead of this if (x &gt; 5) { if (x &gt; 10) { } } else if(x &lt; 0) {} you could use if (x &gt; 10) { #some code here } else if (x &lt;= 10 &amp; x &gt;= 5) { #also check out the between function in the dplyr package #&lt;some code here&gt; } I didn't type out your full code but if I rewrote your code using this style it would be a lot nicer to read and easier to catch mistakes such as yours. See if you can convert your code to use and statements and use good indentation. It will allow you to write code that is a lot easier to manage.
When I try this I get: ## Warning: NAs introduced by coercion Do you think this will matter if I keep on combining many individual tables?
As you wrote your code, the last `else` stood alone, i.e. it did not come after an `if`. This is due to the previous if-statement not being closed with a `}`. As others have pointed out, this is a good example of why formatting (and indenting) your code is important: it enables you to find these errors.
Ahhhhh got it I see my error now. Thanks so much for the help, I'll be sure to keep spacing and indenting in mind moving forward. 
what do you mean by underlying data ?
WTF you asked for the answer to your homework? Please don't ask people to do your homework for you. This is not the place for that. Also PLT: you could have simply copied and pasted your code into a R session and figured out the issue yourself from looking at the error message and properly formatting the code yourself.
Just write a function to calculate the sample SD. You need two values which can all be derived from your data. * Sum of squared differences from the mean. * One less than the sample size Write a function for the first and evaluate. 
Repeat the value by the number of occurrences, then take the standard deviation of that. sd(rep(df$value, df$occurence)) 
&gt; This was from a class exercise Not homework, my teacher's code in the powerpoint was incorrect. Good to know about the homework though. 
Let's say the value in the j-th row is yⱼ and the number of repetitions of it (i.e. its frequency) is fⱼ Also let's talk about variance, you can work out sd from that. The ordinary Bessel-corrected formula for variance is the sum of squares of deviations from the mean divided by n-1 With frequency data, *n* is just the sum of the frequencies n= ∑fⱼ , so that leaves us with the sum of squares of deviations from the mean. I assume you're already able to calculate the mean (since you didn't ask about it). So you need to calculate what you'd get if you could compute ∑ᵢ (yᵢ−ȳ)^(2) but you have a count of repetitions of y-values rather than y actually repeated that many times, so that's ∑ⱼ fⱼ (yⱼ−ȳ)^(2) So now the formula for variance is: ∑ⱼ fⱼ (yⱼ−ȳ)^(2) / [∑ⱼ fⱼ - 1] And you just take the square root to get standard deviation. If you're dealing with grouped (binned) data instead of the actual y-values and trying to apply this approach by replacing all the values with the bin-centre, this will underestimate the standard deviation.
You're technically correct. The best kind of correct. But top commenter with the one-liner for expanding the operations is the most usable for OP. I CHALLENGE OP TO COMPARE THE TWO METHODS!
I've got a long way to hack it into place doing [this](https://pastebin.com/raw/qtvHYSk7) which I can then pull the data from via lines4[2:5,]
Check out the round function. 
I had a look at how the table was generated (type `mediate:::print.summary.mediate` in your R, and something like this should work: x = summary(Mediation) smat &lt;- c(x$d0, x$d0.ci, x$d0.p) smat &lt;- rbind(smat, c(x$d1, x$d1.ci, x$d1.p)) smat &lt;- rbind(smat, c(x$z0, x$z0.ci, x$z0.p)) smat &lt;- rbind(smat, c(x$z1, x$z1.ci, x$z1.p)) smat &lt;- rbind(smat, c(x$tau.coef, x$tau.ci, x$tau.p)) smat &lt;- rbind(smat, c(x$n0, x$n0.ci, x$n0.p)) smat &lt;- rbind(smat, c(x$n1, x$n1.ci, x$n1.p)) smat &lt;- rbind(smat, c(x$d.avg, x$d.avg.ci, x$d.avg.p)) smat &lt;- rbind(smat, c(x$z.avg, x$z.avg.ci, x$z.avg.p)) smat &lt;- rbind(smat, c(x$n.avg, x$n.avg.ci, x$n.avg.p)) rownames(smat) &lt;- c("ACME (control)", "ACME (treated)", "ADE (control)", "ADE (treated)", "Total Effect", "Prop. Mediated (control)", "Prop. Mediated (treated)", "ACME (average)", "ADE (average)", "Prop. Mediated (average)")
It’s virtually impossible to diagnose the problem like this. You need to debug the problem yourself, by successively deleting code from (a copy of) your package source, until it starts working.
You need to define the data frame OUTSIDE the loop, ahead of it, and use a join or rbind to stick the imported data on to it from inside the loop 
&gt; It’s virtually impossible to diagnose the problem like this. That's the sense I get. Sometimes 'devtools' problems are easily diagnosable l.e., but in this case, &gt; You need to debug the problem yourself, by successively deleting code from (a copy of) your package source, until it starts working, and thus isolating the issue. this approach is what I'll be doing. Thanks
This works faster, cheers.
Rbind works. if you have a bunch, look into do.call(rbind....) and read them in as a list. I have to find my old code for this, but the for loop will be slow when you have a ton or if you have large frames.
That does seem to work. Can you explain or point me to somewhere that explains the d1/d0/z1/z0/etc?
Run in the console or before the function: options (scipen=999). Feel free to change 999 to 5 or 6 or something
Have you tried saving it as a pdf? I find plots in windows normally much worse unless you save them in some vector format.
I ended up just pulling directly from the summary once I learned about object ocmponents (so useful!) instead of feeding it into another variable for tabling (e.g. 'summary(Mediation)$d.avg,summary(Mediation)$d.avg.ci,summary(Mediation)$d.avg.p,summary(Mediation)$z.avg,summary(Mediation)$z.avg.ci,summary(Mediation)$z.avg.p,summary(Mediation)$tau.coef,summary(Mediation)$tau.ci,summary(Mediation)$tau.p,summary(Mediation)$n.avg,summary(Mediation)$n.avg.ci,summary(Mediation)$n.avg.p') which extracts the 16 values I need/are shown in the default summary(Mediation). This also fixed a separate issue I had where the p-values were being output in either scientific notation or with &lt; symbol. Now R rounds the p-values to zero since they're so small.
d1/d0/etc. is just what whoever wrote the package decided to call things.
I ran the commands again today and suddenly the plot is much clearer. I don't know what might have caused it
I think this is exactly what you're after. V8 should do it for you. https://datascienceplus.com/scraping-javascript-rendered-web-content-using-r/ 
&gt; applying sapply What does this mean? It might help if you posted the source. Use four spaces indent to format code on reddit.
Provide us an example dataset.
It's .txt
fread should work. You may have to specify if it's tab delimited, coma separated etc. but fread is usually smart enough to figure it out. #comment #install.packages("data.table") #install package remove "#" library(data.table) df &lt;- fread("C:/YOUR_FILEPATH/test.txt")
yup, I think it's also one of the quickest for reading in files i'm very happy with it ( don't use base r for that). That being said i like using packages by hadley wickham for data manipulation (plyr, dplyr, lubridate)
This gets all csv in folder and makes df of them multi_files &lt;- dir(pattern = "\\.csv") multi.df &lt;- lapply(multi_files, read.csv, header = FALSE, stringsAsFactors = FALSE) Temp.df &lt;- do.call(rbind, multi.df)
Does this work? lapply(dataframe$statenames, StateInfo) I don't have R access rn, can't try.
Agreed, fread works best. You can change the nrows argument to select the number of rows too. But beware that it automatically imports data as data.table class, which behaves differently than data.frame class, so you can also add this argument in to avoid that : data.table = FALSE.
Depending on the situation, I would possibly not even store that function in the environment. lapply(unique(df$states),function(state) { write.csv( any_drinking[which(any_drinking$state == state),], file = paste("output/",state,".csv", sep=""), row.names=FALSE) }) 
You can save any object to the disk with saveRDS. Then load it back in on any pc saveRDS(myplot, "plot.rds") myplot = readRDS("plot.rds") 
R is useful for visualization when you cant create what you want using the standard graphics built in. You can actually use R in Power BI to help with this, so you can create plots in ggplot2 and show them in power BI. R is also excellent for more general data work. Power BI/Tableau are good tools once you have a nicely formatted, cleaned up data set. R is a tool to help you get that. And you can of course do things like machine learning in R, and not in Tableau/Power Bi.
If you are using ggplot, ggsave() will save the most recent ggplot object that was created. You can specify what file type.
What do you mean by “send”? Your entire description is unclear; yes, you can write a plot object to stdout in various representations and capture it. But you didn’t describe how you did that, or how it fails. So we really can’t tell you how to fix your code.
PDF('mypdf.pdf') Plot (x,y) Hist(z) Dev.off()
 png("Picture.png") # Plotting stuff dev.off() check your working directory, there should be a png there that you can send.
+1 for data.table package. Matt has optimized the read and write functions as much as possible.
I discovered data.table about a year ago after more than a decade using R almost every day. It has change how I use R. I subscribed to this subreddit about 3 days ago and almost every post I have an urge to reply with "well first install data.table then..."
Thanks for taking the time out of your day to respond. But, I sort of meant without writing to a file. FileIO is way slower. So far, dev.capture and writePNG/writeBinary seem to be the most promising options.
I do apologize. Really any method that doesn't involve writing to a file first would be welcome. Stdout/console is where I'm aiming, as I'm planning for this script to be called by an external program, and that would really help, but I'm willing to work around whatever I have to.
How would this loop be changed so that it uses the 'parallels' package and take advantage of multiple cores? Would I put the entire loop inside a 'parLapply()'?
 require(doParallel) registerDoParallel(#insert number of cores here #typically 50% of available cores is recommended) x &lt;- foreach(i = 1:nrow(table), .packages = #string of package names here .combine = #rbind, cbind, c (as a string, i.e. "rbind") # however you want to combine each iteration's output) %dopar% {# loop here}
library(tidyverse) D&lt;-read_csv(“somefile.csv”) D 
This should work. Happened with me two days back. Just increase the size of the graphs view in rstudio and try to run the code again 
That is wrong. 1. You must use foreach() not for() 2. foreach() is like apply() in that it outputs 
You can make the log messages yourself using cat() 
What about errors and warnings and such?
The difference is, as i have already said that ACL runs the .R script with a CMD command. After the script finished the CMD closes. This .R script is ran many times which respectively means opening a console, running the script and closing the console. The option of leaving the console open is stupid. That's why i need a way to record the messages that the .R environment gives me - warnings, errors and such. If the environment closes after finishing i have no chance of seeing the error (if any). Anyway, i solved this by using sink(), i stumbled upon an example that works for me.
Are you on Linux?
If he still has large bits of data stored in variables then gc() will not really help. Running something like this will give an idea of the large objects sort( sapply(ls(),function(x){object.size(get(x))})) 
It is RStudio.
Lots of open devices? What happens if you type dev.off() ?
&gt; Error in dev.off() : cannot shut down device 1 (the null device)
&gt; fff &lt;- function(x){ object.size(get(x)) } &gt; sort( sapply(ls(), fff) ) fileName_1 fileName_2 output_fileName pkgFile input_file_1 input_file_2 120 120 120 120 216 216 input_file_dir_1 input_file_dir_2 output_file output_file_dir url end_time 216 216 216 216 216 312 start_time input_output_vec documents results fff file_2 312 472 1608 3384 10136 377970192 file_1 output_tdm combined_file 790559864 1135692144 1140638392
That's the answer. To remove an object once you're done with it you can rm it, e.g. rm(file_1) It won't help the ram problem, but if you're not already using it look at the data.table package, you might find there's some pretty good speed improvements you can get. 
http://r4ds.had.co.nz/
Thanks!!! I need knowledge for Data Science domain.
No, sadly i work on a Windows computer because we use a software that is only compatible with Windows.
Forgive the rushed reply. You might wanna look into using the dplyr package for this. That way you could do it like this: library(dplyr) # Example data: my_data &lt;- data_frame( id_var = c("A","A", "A","B","B","B","C","C","C"), measure = c(1,2,3,4,3,3,4,5,1) ) You could then pick out the max measure for each level of `id_var` doing something like this: max_data &lt;- my_data %&gt;% group_by(id_var) %&gt;% summarise(max_measure = max(measure)) The content of `max_data` then looks like this: &gt; max_data # A tibble: 3 x 2 id_var max_measure &lt;chr&gt; &lt;dbl&gt; 1 A 3.00 2 B 4.00 3 C 5.00 
I was considering grouped summaries but I was hoping there was a simpler way of doing this within my ifelse statement, but yes, creating a new grouped variable with the max values is an option. Thank you very much for your help! :) 
dplyr is nice and all, but R has built in functions for these kind of things. aggregate(measure ~ id_var, data=my_data, max)
Take a look at some [Bookdown books](https://bookdown.org/) 
Ahh right, I'll certainly be adding rm() to my bag of tricks. Also I was told about data.table() on this sub the other day and fread() certainly has made my life easier. 
`` library(data.table) my_data &lt;- data.table( id_var = c("A","A", "A","B","B","B","C","C","C"), measure = c(1,2,3,4,3,3,4,5,1) ) max_data &lt;- my_data[, .(max_measure=max(measure)), by=id_var] `` ``` id_var max_measure 1: A 3 2: B 4 3: C 5 ```
Can you give a better example? your example doesn't have delimiters that help distinguish each field.
_Wintermute that's perfect thanks 
You should probably break the problem down as follows: 1. Given an email, parse into a dataframe (or list) containing three fields: `date`, `subject`, `body`. 2. Given a column of characters, how can I normalize/clean the text so that `I'm` and `i'm` are equally counted when you... 3. Figure out how to load a character vector into a word cloud. As others have noted, without a specific reproducible example we can't really help. I'd also look into the `tidytext` package.
Thank you for the help. Please see my edit
Thank you for the help. Please see my edit
You could store them as a list of functions, but then you'd have to access them using the list... `listname$functionname()` for instance. 
Oh, I see! Yeah, that could be painful. But maybe not.
It sounds like what you want is [{modules}](https://github.com/klmr/modules/); it’s a lightweight alternative to packages. Organising your functions into different files will automatically make them available in separate namespaces when loaded in R. See the README file and the vignettes for detailed info.
I found [this link](http://www.sthda.com/english/wiki/text-mining-and-word-cloud-fundamentals-in-r-5-simple-steps-you-should-know) very helpful when I was trying to solve a similar problem.
I'll give that a shot as well, thanks for the suggestion!
I too have a similar problem I am trying solve for in R studio. Please be sure to update if you find a solution.
Packages are the correct way to store R code, if you know how to make them, regardless of the application. They force you to document, force you to utilize versioning, and interface very well with version control using git. Look into Hadley Wickhams tutorials on it - the 'roxygen2' package makes it really easy.
The Johns Hopkins specialisation is the one I've done (actually I'm right at the end of it now, closing in on the end of the final project). I didn't think Coursera offers a different data science course - this one is a collection of 10 modules. I started it based on what people were saying in the (or, a) reddit data science sub and I certainly feel like I've got a lot out of it; I'd never touched R before and only had C++ under my belt before that so it's been worth it for that alone. And it introduces techniques that I'd not used before, like machine learning. 
Okay, i will give it a try.
Assuming both dataframes contains comparable columns ('city' and 'region'), you can use `merge(df_a, df_co, by=c('city','region'))` or use the package dplyr for `inner_join(df_a, df_co, by=c('city','region'))`. Both will return a data frame with all rows that overlap between the two data frames, and all columns from both. If you want all addresses, including those that do not match in coordinates, use `all.x=TRUE` for merge, or dplyr's `left_join`. On mobile so I cannot check exact syntax.
Would it be too much work to create it in ggplot2 variable and then wrap it in a ggplotly() function? p = ggplot(...); ggplotly(p) I'm not sure of your specific task/project, but I always start in ggplot2 and then throw it to a ggplotly function.
That works great, thank you very much!
You don't need the explicit comparison operators in `dplyr::filter`; a comma (`,`) is implicitly `&amp;`, and if it's just one variable, you can use the `%in%` operator as "|", as u/nobadchainsmokers showed. I'm also not 100% sure, but I don't think you're using your first filter condition, `n()`, or the call to `replace` correctly: the first will always evaluate to the row count, unless you group the data first, using `group_by()`; and the latter needs a variable assignment.
what is the %in%? I recall seeing that and I might have used it. Also, does n() == 1 only select a single row? I need it to pick every row that matches because in some cases there are as many as 5 tests/rows for a single person, like 131. I'll tweak form what you and ryapric said and see what happens. THANKS
upvote for tidyverse!
Good to hear from someone, who's taken. By the way, how long would it require to finish the entire modules. I have around 2-3 months to finish before applying for job. Would it suffice? 
Let me see if a understand correctly.. I have a linux kernel inside my Windows 10 distribution, right? I read a bit about it and the impression that i am getting is that the link between the subsystem and Windows is not really there. I need to be able to transfer files from one to the other.
Output redirection also works in cmd.exe, no need for Bash.
I have been working on a solid port of Hedgehog, a quickcheck inspired property based testing library. I think it can really help eliminate bugs. I'm looking for people to try it out and make comments about the API before I commit to CRAN. Cheers
It looks pretty awesome. I'll try it out once I get to the tests phase of my next project.
We are getting off topic here. I don't need a Linux server or Subsystem for my R prototyping...
That’s exactly my point: contrary to what the initial comments said, you can do this redirection directly in Windows.
It’s the negation operator: it returns the opposite of a Boolean input. (!True = False; !False = True)
So then on the assessment the function was turn the opposite of the ind object and then the mean function only took the TRUE ones to make the calculation?
I think two of the best courses for R are from Kirill Eremenko. You can find them on Udemy. I did both the intro and advanced courses. He does an amazing job explaining everything. Would definitely recommend checking those out. 
You can do it in 2-3 months with some effort. I started off at a rapid pace with no experience of R and got through the first few modules in fractions of the required times, but as things wore on I had stuff like Christmas and new year get in the way, and started reading more about the unfamiliar topics, so my pace slowed. I'm still not quite done. If you do start it you can message me when you get stuck, although there is a semi-helpful discussion forum for the course.
Are you referring to [this intro](https://www.udemy.com/r-programming/) course? And can you elaborate a bit more on why you found it useful and the important features in the intro and advanced courses? Also how long did it take for you to complete it? 
That is the one. I found it useful because the intro course does a great job if teaching the basics and actually teaching you about R itself. Other courses I've seen just tell you what functions do what or how to do something without much background. I cannot remember where the intro ended and the advanced began, but by the end of advanced you can clean data, analyze, and visualize it. I was working a full time job while doing it so it took me about a month to do the advanced course and a couple weeks for the intro. 
Ok. I'll look into this course then. Looks like it's available for a more affordable price as well. And were you referring to [this course](https://www.udemy.com/r-analytics/) or [this course](https://www.udemy.com/machinelearning/) as the more advanced one? Do you feel the advanced course is a must to code efficiently in R? Also do you include your Udemy certificates in your Resume/LinkedIn profile? And does it have any value when added?
I've been struggling with the JH Coursera course, tbh. I find it very difficult to follow along with programming teaching materials that give examples like, 'x -&gt; c(1,2,3)' and don't contextualize how you might use the info in 'the real world', nor give smaller practice problems along the way--the content can be very hard to absorb. In this JH course, you mostly watch a university-style lectures with very sparse slides and lots of unnecessary exposition*, then are thrown off the deep end when it comes to your first major programming problem. It can be quite defeating. The best part of the whole thing was the [Swirl](http://swirlstats.com) tutorials, tbh. And you can access those for free by installing RStudio and downloading that library. Plus, it's a monthly subscription, whereas with Udemy you buy forever-access to courses for a fraction of the monthly Coursera charge. ...I think I just talked myself into quitting this course. I don't mean to be too hard on the creator, because I'm sure teaching a university-level course in MOOC format is incredibly difficult. But if you're the type of person who finds it easier to learn in an applied manner, using Stack Overflow and books like 'Learn Python the Hard Way', you might find this course a bit hard to take. * Do I really need to know that R is a version of S language in order to start coding? No. Good to know, but I'll likely pick that kind of information up later on, when I need to know it.
I would second the Data Camp courses (datacamp.com). I started with the Coursera courses because they were free (you can pay for them, but I didn't), but there is no interaction with the course. Data Camp's platform is great and you watch a video and then answer questions or write code to solve the problems.
I'd just use mean(dataset$column, na.rm = T). This will remove any NA values and calculate the mean from the remaining values.
I took the free version of the Coursera course about a year ago without any other training in R, my only other experience in any programming languages have been in school. I think the course is great considering that I didn't pay a single penny for it. The class does leave a lot of important subjects out that I had to learn on my own, mostly through googling, reading R programming books at the library and a lot of practice. The course by no means will make you an expert but you can't beat the price so if you are seriously motivated I would recommend taking it. P.S. I originally wanted to find a university/college that taught R programming but of the courses that I could find that used R, the emphasis of those courses was on teaching statistics and not learning R. That is why I ultimately decided to go with the Coursera course as I could tell from the course description that the focus of the Coursera course was on R programming and not statistics.
You might look at what Prof. Gerbing at PSU has been doing. He has a package called LessR to help his students do stats. http://www.lessrstats.com/ https://cran.r-project.org/web/packages/lessR/ I don't know him personally, but he might be open to giving you advice on building your functions into a package for teaching stats: https://www.pdx.edu/sba/david-gerbing 
I've subsetted it 26 times, for each possible first letter. Significantly quicker. fread() has been helpful too.
I cannot praise Datacamp enough. I’ve taken the UT Austin stats course (with R) and I found Datacamp to be less challenging but way more informative for learning R.
Yup
I completed the entire data science specialization with JHU on coursera. I liked it. The projects and the course material helped me feel competent with using R for doing data wrangling, visualization, exploratory analysis, creating reports, interactive data products, For the R programming course specifically, the most helpful aspect of the course are the swirl exercises. 
In addition to the course, Data Camp has projects on R. You can use the projects to gain experience. 
This was my experience too. I passed every course until about halfway and switched to data camp. I like it a lot more.
Sure. I'm planning to start this month. &gt;If you do start it you can message me when you get stuck Thanks! 
Another big vote for data camp. 
I agree with this. I’ve done both his courses and it he helps tremendously. Very easy to follow, has “homework” that is very helpful to do. I moved on to R For Data Science afterwards and I believe it was a good move. Kirill’s courses gives enough information for you to start learning more on your own.
I took several of the courses in this specialization (but not the r programming one since I already knew the language) and it’s honestly been the most useful thing I’ve done outside of school. I use things I learned in the class (shiny, rmarkdown, data visualization with leaflet/ggplot2) everyday in my work. From what I’m reading, the R programming course may not be the best, but once you get a grasp on the language I would highly reccomend circling back on this specialization, particularly for the shiny course. I developed a shiny app for that course that really helped me differentiate from the crowd during my job search.
What are you trying to achieve? I’m not an expert but R is for data analysis where python is more of a general purpose language 
I'm trying to understand R classes via an analogical example from something I think I know.
[removed]
The terms “functional” and “procedural” are conventionally opposites. OOP is compatible with both.
Most people who use R aren't writing packages. R's OOP features are mainly so users can use the same generic functions on objects from different libraries with library-specific behavior. Unless they're making a package, I'd be very surprised to see someone defining new classes in R code.
I think a screenshot of the data would be helpful, but I guess if you have a column of numeric data and a column of time units you could loop through the subset of time units that are not correct and multiply the associated numerics by the proper factor? I'm not familiar with a way to tidy your data in this kind of higher level all while reading it into R. I'd just load the csv, then do your finagling.
I ended up writing a 4 lines gawk script instead. But the data and the unit were in the same column.
Brilliant 
If you want to contain it in R, str_extract_all (stringr package) is pretty great at returning the regex match. I used that to format twitter's created_at column which looks something like "Thu Apr 06 15:24:15 +0000 2017" to a datetime. 
They could start by cutting down on the commercials... hatchacha
 Inner join merge(df1, df2, by=”common_key_column”) Outer join merge(df1, df2, by=”common_key_column”, all=TRUE) Left outer merge(df1, df2, by=”common_key_column”, all.x=TRUE) Right outer merge(df1, df2, by=”common_key_column”, all.y=TRUE) www.r-bloggers.com
a left join would be all.x = TRUE, and are you using the base merge() function?
what does it look like when your entry is 15 minutes vs 5 seconds? do it say "15 minutes" or "5 seconds"? or is it more like 00:15:00, and 00:00:05?
&gt; do it say "15 minutes" or "5 seconds"? Exactly this.
what does it look like when your entry is 15 minutes vs 5 seconds? do it say "15 minutes" or "5 seconds"? or is it more like 00:15:00, and 00:00:05?
Well, the idea is equal distribution based on both continuous as well as categorical variables, the same way a stratified random sample would represent the population, I want each bin to represent the total population. I am not sure if I am clearly defining my problem.
Nevermind, I figured out how to ask google and stackoverflow had it. for anyone curious, here is an answer with 2 solutions that could work: https://stackoverflow.com/questions/4290672/add-a-dynamic-value-into-rmysql-getquery 
Check out this guide, it should get you started on importing worksheets by name. http://readxl.tidyverse.org/articles/articles/readxl-workflows.html
Corey's solution is better than Gavin's. Avoid pasting values into SQL at all costs: it's a classic SQL injection vulnerability. You should always use bind variables. 
[Obligatory](https://xkcd.com/1171/)
I always hear that repeated, but I never have problems with regex. I use one particular style so frequently that I made a [blog post](https://rlang.io/keyword-searches-from-comma-separated-terms/) about it and just got a message from a client to run it again today on some things. 
I consider myself pretty handy with regex as well, but most people aren't. The other day, I blew a coworkers mind when I constructed a simple regex in front of him without referencing any docs. It's a really confusing technology to some people, and definitely suffers from "write once, read never" across implementations, which I think makes it more mystifying. Maybe it's because data scientists (i.e. the people I work with) are accustomed to fairly readable code (R/python/SQL), which regex generally isn't. 
How do the data scientists you work with clean their data without regular expressions? I remember working on some sort of bank log dump (not sure what it was exactly, but my friend asked me to help him for some personal financial software he was working on) and there was no way to parse it without a whole slew of nested loops or a regex. Side note: I can't believe MySQL still doesn't truly support regex in the way MariaDB does. 
My philosophy is to always try to "practice how you play". If you're in the habit of not allowing SQL injection vulnerabilities in code that isn't client facing, you'll be that much less likely to create vulnerabilities in coffee that is. More importantly, you're also future-proofing your code in the event that something you intended to never be client facing for some reason becomes client facing. Maybe coffee you wrote gets wrapped into a library, or someone blindly copy-pastes it. Who knows. Better safe than sorry.
Thanks, but I was more wondering if there was a nice regex function that allows to change a part of the match. This is the gawk script I ended up using. #!/usr/bin/gawk -f BEGIN { FS=","; OFS=","; print "Time","City","State","Shape","Duration in seconds","summary"; } match($5, /([0-9]+) seconds/, a) {$5 = a[1] " seconds"; print;} match($5, /([0-9]+) minutes/, a) {$5 = a[1]*60 " seconds"; print;} match($5, /([0-9]+) hours/, a) {$5 = a[1]*3600 " seconds"; print;}
Those are really good points. Thanks!
What type of file is it before you read it in and what type of file is it in R? If it's data it's possible it could be coercion of data types when you read the file type in. There will be a difference in size for example between a column of numbers as class character and a column of numbers as class numeric, float, integer etc. 
Is it a dataframe? If there are columns that are numbers what class are they? 
The files have six columns, the first contains character strings, the second integers, the rest are decimals. But I haven't specified this to fread(), I'm not sure if it treats everything as characters. I'm looking at ?fread now and it isn't clear on this, but it does say: &gt; Value A data.table by default. A data.frame when argument data.table=FALSE; e.g. options(datatable.fread.datatable=FALSE). So it must be a data.table.
Ok the image is better. What you have is what I was invisoning as the separated dataframe I thought the transaction ID was "123 OP". Could you just do something like this? I'm assuming the transaction ID is unique. df %&gt;% filter ( unique(ID)) Base code version might look something like this: df[!duplicated (df$ID),] I can't replicate this bc I'm not on my computer but let me know if this is what you were looking for. 
This is all pretty normal behavior. glm objects, for instance, have some environment pointers that make the objects take up much more space in memory than the object size. I was having fun last week with a 600MB file that is a 5GB R object taking up 30GB of RAM. Oh, and fread doesn't treat everything as character. It does try to figure out the actual data type using the first... 1,000 or 10,000 rows (on mobile and don't want to go look). So, there are attributes that can make your R object take up more space than the file size, particularly when the file is compressed (though your txt isn't). Format just matters, and it's not specific to R. Save a .csv file as an Excel xlsx. You'll notice that "same" data is smaller in the xlsx because it's compressed.
First off, as noted, `object.size` is a *very imprecise* estimate of an object’s size. Secondly, you are comparing apples and oranges: on the one hand a text file, which contains a *textual representation* of the data, and on the other hand your data as represented in R. To illustrate the difference, take numbers; in a plain text file, a number typically takes log10 *N* bytes (i.e. one byte for every decimal place). In R, every number takes a fixed number of bytes (usually 8 bytes, plus some fixed overhead per vector). Similarly, a text takes up something like one byte per character in the text file. But R performs *string interning*, which means that if you store the exact same text twice, it probably gets only stored once in memory. And that’s just for two simple data types. Complex data types tend to diverge even more between their convenient textual representation *vs.* what they look like as dynamic objects in memory.
I can do that, but then I need to remove the OP and the BP, since the BP indicates a reversal of the original OP. I will give up on an clean way to convert columns 10:14 to abs and do them individually. Question. Does unique remove both items of a duplicated pair, or just the duplicate. I need to get rid of both which is the part I don't know how to do. I've dealt with duplicates before where I needed to keep one.
I believe unique will only list ones that are not duplicated so it should give you what you are looking for. This should be the clean way you are looking for. If you want to get rid of the BP/OP column you can df$column&lt;- NULL
Hmm OK. Well for my purposes it doesn't matter what the size is outside of R, only once R has the object. However, I'm now faced with another irritation: the units of the number returned by object.size(). Supposedly I can specify the units by placing object.size() inside print(), so print(object.size(x), units = "Mb"). However, this isn't working, for example, for a 1435 KB file x (before R reads it in), doing print(object.size(x), units = "Mb") gives me the number 2.175. Whereas for a 31 KB file I get the number 33032. How do I force the units to be the same in both cases? 
Okay after exploring the data a little more I think I need to solve the problem differently. What I'd like to do is remove rows that match on a few variables. If I convert the negatives to positives, then the rows will match on all but two variables. The TRANS_CODE where the first is OP and the second is BP, and the ICN. I am trying like hell to figure out how to filter while ignoring the two columns that are unique without removing them. When I use distinct() with just the rows that are the same it gives me a dataset that is missing the unique columns. I'm now trying to see if I can index rows where DMHID matches, the the abs(oneofthePRICE) match and the TRAN_CODE doesn't match so I have rows I can then remove. It vaguely reminds me of something I did in DataCamp that I can't find. https://imgur.com/a/x2p4L
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/gQitX1I.png** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) 
&gt; Whereas for a 31 KB file I get the number 33032 You almost certainly forgot to specify the unit when printing that size. I (correctly) get `0 MB` when creating an object of that size. Apart from that, I strongly recommend not to use the legacy units (“Mb”) but rather SI or IEC units (“MiB” or “MB”, respectively; see the `object.size` documentation).
I've checked it again, definitely with "MB" for both files, and still the same result. Setting them both to "KB" doesn't result in different numbers either. The actual lines of code I'm using are as follows: x &lt;- print(object.size(TDM_single_letter), units = "KB") vec_size_of_files__single_letter &lt;- c(vec_size_of_files__single_letter, x) y &lt;- print(object.size(TDM_double_letter), units = "KB") vec_sizes__current_second_letter &lt;- c(vec_sizes__current_second_letter, y) 
It's hard to tell when you didn't give more specifics on what you're trying to do and what you've tried, but the simplest solution is often to just download the file first, then read it in, rather than trying to make R do something it usually doesn't. If you really want, you could write a short function to do both for you: dl_read_rest &lt;- function(url) { session$files$download(path = url,filename = "my_file.csv") return(read.csv("my_file.csv")) }
My suggestion is to just represent those bases as letters (characters) as it's pretty easy to test for differences in single characters as is but if you must represent the bases as numbers you can use the [R hash package](https://cran.r-project.org/web/packages/hash/hash.pdf). Which has the ability to act as a "python-like" dictionary.
I think you need to convert the corpus object to a term document frequency matrix before the wordcloud can be made. try TermDocumentMatrix() or DocumentTermMatrix(). it should help.
It’s a bit unclear to me what your end goal here is — in particular, I don’t see why you need to change the representation at all, and how having numbers instead of letters here would help. That said, the “R way” of representing strings as numbers is to convert them to a factor variable. You can then use `as.integer` to get the numeric values out. Here’s a simple example: amino_acids = setdiff(LETTERS, c('B', 'J', 'O', 'U', 'X', 'Z')) sequence1 = c('M', 'D', 'H', 'F', 'V') # … sequence1 = factor(sequence1, levels = amino_acids) # numeric codes: as.integer(sequence1) #&gt; [1] 11 3 7 5 18
I’m interested in learning more. Can you elaborate on this unwanted variance you see when comparing across strings?
It generally doesn't make sense to coerce a big.matrix to a vector. The entire point of the bigmemory package is to be able to work with data that is larger than will fit in memory. Vectors have to be entirely resident in memory so converting any reasonable big.matrix into vector will throw an outofmemory error. At this point you really have two options. If your data will fit in memory, don't use bigmemory at all: data &lt;- read.csv("test.csv", header=FALSE) km &lt;- kmeans(data, 5) If your data is actually too large to fit in memory, you'll need to re-implement the kmeans algorithm in a way that doesn't require a vector. This should be relatively easy since big.matrix does expose operations very similar to regular matrix (without actually being a matrix). Though, after peaking at the source of `stats::kmeans`, it looks like the core algorithm is implemented in fortran so i'm not sure what sort of performance you'd get from a pure R rewrite.
thanks! after some research, i think i need to use a different algorithm. `clara` looks promising, since it can deal with much larger datasets
Ahh, that does work, many thanks.
Look into using a Kruskal-Wallis test, which doesn't assume normally distributed data
Are you saying you have only 12 observations? It's hard to get any reliable statistics from that.
&gt; Do longer variable names have any significant impact on how quickly code runs? Virtually none. Descriptive names are good. But too long names also hinder readability: simultaneously strive for precision and conciseness. For instance, don’t encode types in names unless necessary, and avoid mentioning *uninformative* types, such as `vec` in your case. Next, avoid filler words: `file_sizes`, not `size_of_files`. Etc.
I can’t help you solve this problem the way that you want to. But the general approach I use for aggregating by factors - in other words, a specific ceremony your awards column - is dplyr’s group_by function. I’d load the tidyverse library and do something like data %&gt;% groupby(award) %&gt;% summarize(actors = n_distinct) to get a data frame indicating how many actors were at an awards ceremony.
How about a for loop: for(i in 1:(nrow(dt)-1)){ if(dt$actor[i] == dt$actor[i+1]) print(paste(dt$actor[i], dt$actor[i+1])) } 
yeah good point, he could change the while loop to a for loop. I suppose what he'd want if he went with a for loop is a nested for loop, since he wants a column of all actor pairs.
The most apparent consequence in my work is that it drops attributes like names(). Note that the default for the shortcut operator '[' is drop = TRUE...
I'll plug mixOmics, particularly if you're interested in the sparse variants of pls and plsda.
I've just used the `pls` package but I think `caret` has this functionality too.
On the other hand, expect a lot of 'drawing the rest of the owl' if you pursue a career in data science. Being able to learn independently is critical.
Not interested in sparse, much more interested in good graphics. Thanks though.
I don't think I understand what you mean by that. Can you elaborate?
What are you using to plot the curve? base, lattice, ggplot2?
Using ggplot, I plot a ribbon with the ymin as 0 and the ymax = density(x). Incidentally you can also plot the relevant rejection regions by setting your xmin and xmax at Inf and/or the min or max of x at the probability you need. 
I've done it several times using `polygon`. What are you unsure about?
As another user said, you can use the `polygon()` function. There is a [somewhat sophisticated example on this page](http://willitreplicate.com). It might be overkill for your needs. Let me know if I can help.
With the weird red/blue/bold formatting? Probably easier to just export your data and apply the formatting in excel tbh. 
Check out the openxlsx packages if this is a regular report export to Excel, but if this is for one-off stuff I'd follow /u/Dokugumo 's advice and just export and do the formatting in Excel. If it's not for Excel, look at the kable/kableExtra packages. I generally use something relatively close to their default, but I think you can do this sort of formatting with kable. The flextable package should allow you to do this as well, but I don't like the default as much. Still, if you need the kind of flexibility that allows blue text and red borders, it's an option.
Why not just do it with html https://cran.r-project.org/web/packages/R2HTML/index.html
For your prob argument in `sample`, just supply `c(pC,pD)` and the function will ensure the probabilities sum to 1.
Exemple: winner of the mach 1 and the winner of mach 2 are going to make a new mach, let's call match 5, how i pick the result of this two firsts machs and apply on the new function.
I cannot solve this for you right now. So instead, try this: how would you do this on paper? Just, step by step? Try doing it using variables. Hint: arrays, or in R, a named vector.
Hey sorry I was at a debate this weekend. Here's the code I'm doing and what I'm trying to do: #actual code thats not working #goal is to mark area under curve between 79 and 120 lbound &lt;- (79-94)/6.8 ubound &lt;- (120-94)/6.8 curve(dnorm(x,94,6.8), xlim=c(70,120), main="normal density") cord.x &lt;- c(lbound,seq(lbound,ubound,.01),ubound) cord.y &lt;- c(0, dnorm(seq(lbound,ubound,.01)),0) polygon(cord.x,cord.y,col='skyblue') -- #code example that I am following curve(dnorm(x,0,1),xlim=c(-3,3),main='Normal Density') cord.x &lt;- c(-2, seq(-2,-1,.01),-1) cord.y &lt;- c(0, dnorm(seq(-2,-1,.01)),0) polygon(cord.x,cord.y,col='skyblue') 
Hey sorry I was at a debate this weekend. Here's the code I'm doing and what I'm trying to do: #actual code thats not working #goal is to mark area under curve between 79 and 120 lbound &lt;- (79-94)/6.8 ubound &lt;- (120-94)/6.8 curve(dnorm(x,94,6.8), xlim=c(70,120), main="normal density") cord.x &lt;- c(lbound,seq(lbound,ubound,.01),ubound) cord.y &lt;- c(0, dnorm(seq(lbound,ubound,.01)),0) polygon(cord.x,cord.y,col='skyblue') -- #code example that I am following curve(dnorm(x,0,1),xlim=c(-3,3),main='Normal Density') cord.x &lt;- c(-2, seq(-2,-1,.01),-1) cord.y &lt;- c(0, dnorm(seq(-2,-1,.01)),0) polygon(cord.x,cord.y,col='skyblue') 
Hey sorry I was at a debate this weekend. Here's the code I'm doing and what I'm trying to do: #actual code thats not working #goal is to mark area under curve between 79 and 120 lbound &lt;- (79-94)/6.8 ubound &lt;- (120-94)/6.8 curve(dnorm(x,94,6.8), xlim=c(70,120), main="normal density") cord.x &lt;- c(lbound,seq(lbound,ubound,.01),ubound) cord.y &lt;- c(0, dnorm(seq(lbound,ubound,.01)),0) polygon(cord.x,cord.y,col='skyblue') -- #code example that I am following curve(dnorm(x,0,1),xlim=c(-3,3),main='Normal Density') cord.x &lt;- c(-2, seq(-2,-1,.01),-1) cord.y &lt;- c(0, dnorm(seq(-2,-1,.01)),0) polygon(cord.x,cord.y,col='skyblue') 
In general R likes columns and you will be happier in the long run if you get comfortable with columns instead of rows. That being said; your first step made a matrix with 20 columns, each with 10 integers. Your second step transposes the matrix, and samples each of the 10 (now) columns. You can skip the transposition and directly sample the *rows* of the original matrix by setting the second parameter to "2": apply(data, 2, function (x) sample(x, 5, replace = FALSE))
I'll take a look when I have access to my main computer. RemindMe! 12 Hours "Shading normal PDF"
I will be messaging you on [**2018-02-18 11:16:42 UTC**](http://www.wolframalpha.com/input/?i=2018-02-18 11:16:42 UTC To Local Time) to remind you of [**this link.**](https://www.reddit.com/r/Rlanguage/comments/7y0yf9/how_can_i_shade_under_the_curve_for_a_normal/) [**CLICK THIS LINK**](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[https://www.reddit.com/r/Rlanguage/comments/7y0yf9/how_can_i_shade_under_the_curve_for_a_normal/]%0A%0ARemindMe! 12 Hours ) to send a PM to also be reminded and to reduce spam. ^(Parent commenter can ) [^(delete this message to hide from others.)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Delete Comment&amp;message=Delete! ____id____) _____ |[^(FAQs)](http://np.reddit.com/r/RemindMeBot/comments/24duzp/remindmebot_info/)|[^(Custom)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[LINK INSIDE SQUARE BRACKETS else default to FAQs]%0A%0ANOTE: Don't forget to add the time options after the command.%0A%0ARemindMe!)|[^(Your Reminders)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=List Of Reminders&amp;message=MyReminders!)|[^(Feedback)](http://np.reddit.com/message/compose/?to=RemindMeBotWrangler&amp;subject=Feedback)|[^(Code)](https://github.com/SIlver--/remindmebot-reddit)|[^(Browser Extensions)](https://np.reddit.com/r/RemindMeBot/comments/4kldad/remindmebot_extensions/) |-|-|-|-|-|-|
Did you even *look* at the values in `cord.x`? You're plotting x-values between about -2.2 and 3.8 there. Now what are the x-values in your `curve` plot? It did *exactly* what you told it to, which is to draw your polygon about 6 inches to the left of the edge of your plot window. Of course you can't see it. 
I tried both with direct scores and my z score. The example was confusing because the data was the same as the z score, so I wasn't sure what was going on there. I'll quickly SS what happens when I change them all to just pure data
Check out the kable and kableExtra packages.
Smells like homework. 
so what have you tried
[removed]
not necessarily an application for this example, but check out the tidyverse packages by Hadley Wickham. It gets you away from using apply and loops and it's all in a very nice framework
Is this a data frame? What are the column names, and what do each contain? What do you get when you run print(str(data.frame.name))?
 library(dplyr) result &lt;- df %&gt;% filter( origin %in% c("Antigua", "Barbados", "Jamaica") | destination %in% c("Antigua", "Barbados", "Jamaica") ) "|" is used as the "OR"-operator
There are a few ways you could do this. One way in base R: You could create a vector of destinations you want, and then check if your rows in certain columns are contained within them using %in%. e.g r$&gt; data origin destination 1 Antigua Barbados 2 Barbados Slough 3 Slough Barbados 4 Jamaica Antigua 5 Scunthorpe Slough r$&gt; wanted &lt;- c("Antigua", "Barbados", "Jamaica") r$&gt; data[data$origin %in% wanted | data$destination %in% wanted, ] origin destination 1 Antigua Barbados 2 Barbados Slough 3 Slough Barbados 4 Jamaica Antigua 
I get: 'data.frame': 96065 obs. of 4 variables: $ Country_destination : Factor w/ 194 levels "","*","**","Afghanistan",..: 4 4 4 5 5 5 5 5 5 5 ... $ Country_origin : Factor w/ 216 levels "","Afghanistan",..: 91 93 143 40 53 59 91 93 127 149 ... $ Year : int 2016 2016 2016 2016 2016 2016 2016 2016 2016 2016 ... $ Passengers : int 1 33 59737 11 3 3 23 17 2 1 ... NULL
That worked! Then I realized I didn't organize my problem properly - I got too many results with countries outside the region (e.g. people going from Norway to Antigua). So I switched to the "&amp;" operator and that worked. Thanks!!!!!
&gt;I'm given a 2x2 table and I'm told to make a list, but I do not know if I am doing it correctly. &gt;Column 1 is Name and Column 2 is "Courses. &gt;Column 1/Row 1 is "James" and column2/Row1 is "Math, Physics, Chemistry". &gt;Is my code below correct? I feel it's not because when I try to get the "Physics", with [[1]][[2]], it doesn't work &gt; &gt; &gt; a &lt;-list(name = "James", Courses = c("Math", "Physics", "Chemistry")) &gt; b &lt;- list (name = "Peter", Courses = c("English", "History", "Sociology")) &gt; Student_Data = list(a,b) You're trying to get "Physics" from a list (`Student_Data`) by subsetting "too hard". `Student_Data[[1]]` returns only "James"; there's no `[[2]]` to return in a vector if just "James", so that's why it doesn't work. I'd offer some alternative approaches, but I'm not sure if you need your list(s) structured like this, or where the student names are the list names, and the courses are the elements.
Is the memory still allocated if you end the R script from within itself rather than with sudden death from Task Manager?
What do I add to a script to end itself? I never end a script in the same way.
&gt; .rs.restartR() From what I read, this command should do the job all on its own. Does it work without rm(list = ls()) ?
It didn't, I tried that on its own first. 
You haven't stated which Operating System you are using but if its GNU/Linux then this is the expected behaviour, memory is occupied until it is needed and that &gt;3Gb of RAM will retain the RStudio/R session until something else on your system needs it. This is a "good thing" as it means starting a new RStudio session will be faster since it loads from RAM rather than starting anew. Its also why you do *not* need to kill apps under Android (which is running the Linux Kernel and GNU userspace tools under the fancy UI). An overview is [here](https://lifehacker.com/5650894/android-task-killers-explained-what-they-do-and-why-you-shouldnt-use-them). You can read a lot more on the Linux kernel memory management... * [The Linux Kernel/Memory - Wikibooks, open books for an open world](https://en.wikibooks.org/wiki/The_Linux_Kernel/Memory "The Linux Kernel/Memory - Wikibooks, open books for an open world") * [The Linux kernel: Memory](https://www.win.tue.nl/~aeb/linux/lk/lk-9.html "The Linux kernel: Memory") Thus, under GNU/Linux at least, you don't need to free up memory, it will be freed up when a new application wishes to use it. No idea how M$ handles this but suspect it may be similar since its an efficient approach. Look for details of memory allocation/handling in the operating system you are using since that is where the problem will lie.
&gt;Have you any idea what could explain being able to release the memory using &gt; rm(list = ls()) &gt; .rs.restartR() &gt; &gt; but that released memory being re-occupied again if RStudio is closed then re-opened? Are you saving your session at any point in your script (likely before `rm(list = ls())`)? I suspect it would still be worth checking out and understanding how your system allocates and frees up memory and avoiding growing vectors within loops will likely solve a lot of problems for you.
Because windows 10. In your case because Microsoft gave up after 7 and devs forgot how to properly manage memory allocation. Try opening a bunch of pdfs or browsers to see if you can push r out of memory but if it's present after am actual reboot microsoft has really failed hard.
You've probably got the "Restore .RData into workspace at startup" enabled. Try disabling it.
&gt; What should I do? `gc()` — forces garbage collection.
This. Rstudio restores rdata by default. Uncheck the box next to it in global options. I agree with Hadley wickham that this is important... start each new r session with a fresh environment. 
Ahh great thanks, this did the trick. RStudio is opening quickly again. 
It was that I had "Restore .RData into workspace at start-up" in global options enabled. Unticking that box solves the problem.
Good to hear it was simple to resolve.
Just do the same process again. png(filename = "plot01.png") plot() # Your first plot dev.off() # Close the file png(filenname = "plot02.png") plot() # Your second plot dev.off()
The file doesn't yet exist to already be opened, also there's masses of free space and they're not large files that I'm trying to write.
Doesn't seem to be an R issue. I just tried to rename a text file to "aux.txt" and Windows 7 wouldn't let me. Says "The specified device name is invalid".
I've been doing that, it definitely falls over when attempting ".../au/aux.txt". I'm attempting "auX.txt" now to see what difference it makes. 
Ugh. This is going to cause a minor headache...
Aux dot anything (or nothing at all) is a device name, an MSDOS device name :) You might want to skip 'aux', probably some others. (errmm... PRN, LPT, it's been a while, I forget)
I am not a CS person but any means, but I've worked with data that has well over 100k cells and most things never took more than a minute or two. There isn't much information here, but if what your doing is taking a long time, you may have accidentally crated an infinite loop.
It would help if you included... * The time taken to complete for you. * Your code (ideally formatted by indenting with four spaces) More generally if you want to look at where there are bottlenecks in your code you can use profiling to work this out, some articles to get you started... * [Profiling with RStudio – RStudio Support](https://support.rstudio.com/hc/en-us/articles/218221837-Profiling-with-RStudio "Profiling with RStudio – RStudio Support") * [Profiling and benchmarking · Advanced R.](http://adv-r.had.co.nz/Profiling.html "Profiling and benchmarking · Advanced R.")
Add a printed index inside the loop to test the speed. for (i in 1:50000) do { *Stuff* print(i) } Afterwards you can estimate the duration.
Thanks. I will do that. As i mentioned, I have only started with some simple calculations over a small sample of data that is really not reflective of what i want to achieve at the end of the project. Hence, the question before the start (i know, getting too ahead) I just wanted a ball park estimate of time taken usually for such a function.
Catch 22 since it will depend on your code, but in general its likely to not take too long as thats not a particularly large data frame.
Sorry about that. The dataset features the following input columns (a)day (a)min temp (b)max temp (c)wind speed (d)precipitation (50k+) and 15 other columns (that i intend to use further along) Ex: For day 84, i calculate the cumulative values of the above till day 84 ,perform the below and store them: calculated column 1 = (min+max)/2 x windspeed calculated column 2 = (min+max)/2 x precipitation This is done for all the rows. The cases check the day of the week, and conditional checks for previous calculated values of wind, temp and precipitation. That said, i just wish to understand the what will be the approximate time taken?And if at all, that is substantial.
Cool. Thanks
Given, it's not very elegant. But it works.
Do these directories exist already, or are you forcing R to create them?
You can specify the type of column within read_excel() [it should be in the help documentation on read_excel]. The issue could be that there is 1 (or more) values not behaving in the same way, and as such throwing off the guessing of read_excel on column type.
I think in this case your best bet to ensure you're not losing data is to run row counts or unique(dataframe$customer_id) over whatever you're joining and adding those values together. Or to stop duping you can do dataframe[!is.duplicated(dataframe$customer_id),] etc... I'd like to hear more about your problem so I can help.
use paste0()
data &lt;- c(1,2,2,3,5,3,6) data[duplicated(data)] This will give you the unique values of the ID's that are duplicated. In your case it might be dataframe[duplicated(dataframe$client_code),]
Thanks i will try it 
Oh the fun of receiving data in changing formats. You could put all the flags in one data frame and use that as a lookup table against the imported dataset using customer I'd as a key. You would have two data sets to check against each other, just make sure every customer I'd that is in the import is also in the lookup table. Once the two are set and checked you could either merge or conduct further analysis as a relational db. This way you're never loosing the import, and you can maintain the customer I'd list separate of each new import, if this is a regular report.
Sorry let me clairify. I used POSIXct since I was working with hourly data, it would probably be easier for you to use the date class. But do something similar.
Database. Hist tables. Stats before and after transformations. 
It wasn't me that downvoted, I don't disagree with what was said merely added my tuppence to the conversation 
Unfortunately I have close to a billion rows of data, so I'm hoping to avoid using a loop. Let me know if you find a better way as well :D
Unfortunately I was hoping to know what specific days I was out of stock so that I can calculate potential lost sales, where I can use a seasonal sales figure for the specific day. I think I'll try your way though, since it looks like what I'm trying to do isn't possible. Thanks! :D
Yeah I have a lot of rows too not quite a billion but a lot so I'd like to no longer rely on for loops.
Oh wow, it seems to be the issue described here: http://promberger.info/linux/2011/08/08/ggplot2-error-invalid-argument-to-unary-operator/ Tried it and it worked - at least this time. Huh.
Check out the [padr](https://cran.r-project.org/web/packages/padr/vignettes/padr.html) package. It will help you `pad` the time series (i.e. fill in the implicitly missing values). Edit: typo
Well, congrats on finding the answer to your own question then. =)
In the future could you format you code using the four spaces method? It makes it easier to read
Sure, can you give me an example to follow?
Just like this: myPlot&lt;-ggplot(data)+ geom_point(mapping = aes(x = independent, y = dependent), col = "blue")
Thanks! That does look much better.
at one point I thought about using the [doparallel](https://cran.r-project.org/web/packages/doParallel/index.html) package which would make it easier to convert from a regular for loop to a parallel for loop but never got around to trying it.
I'm new to R, can you explain how exactly I would enter my data into this? Say I have 1 data set that is 1, 2, 3, and the other data set is 5, 10, 15. How would I enter this and perform the anova test?
I feel like this reply isn’t a proper illustration. ;-) @/u/candleflame3: Compare &gt; like this; and &gt; ␣␣␣␣like this which renders &gt; like this; and &gt; like this 
You are getting an error because the last iteration of the sequence is trying to grab an element from the vector in position one greater than its length. 
Ok. When i is set so that it indexes the last item in the array vectorpls, trying to refer to vectorpls[i+1] will cause a problem, because i+1 is not in the array. vectorpls[i] is the last element that can be referred to.
the solution here is to use a while loop that looks something like this testFunction&lt;-function(vectorpls){ i = 1 while(i&lt;max(range(vectorpls))){ if(vectorpls[i+1]&lt;vectorpls[i]){ print(vectorpls[i+1]) } i&lt;-i+1 } } 
As /u/jowen7448 commented, the error occurs because `vectorpls[i+1]` does not exist when `i` equals the number of elements in `vectorpls`. You should however exploit that R is vector based: differences &lt;- vectorpls[-1] &lt; vectorpls[-length(vectorpls)] which(differences) print(vectorpls[differences]) Note however there is a 1 off index issue here, depends on which way you want to look at it. R has also the function `diff` which calculates the difference between an element and the following element. We can use it as such: differences &lt;- diff(vectorpls) &gt; 0 which(differences) print(vectorpls[differences]) 
The guide is intended for embedding a custom format into a *package*. You can create a similar effect, albeit for a single Rmd document, by specifying your tings directly in the yaml: --- title: Yaml stuff output: html_document: toc: yes theme: lumen css: `system.file("/resources/css/styles.css")` header: `system.file("/resources/header.html")` --- ## Header 
I didn't know I could do that. This is a good example of "If you're writing functions in R, you're doing it wrong"
as /u/mrgumble pointed out, a while loops is not the solution. If anything you would just shorten the for loop by one: seq_along(vectorpls)[-length(vectorpls)]
Because `seq_along` just numerates 1:n, you can shorten it to `seq_along(vectorpls[-1])`.
Nope, writing functions in R is very useful and can help separate different problems. Writing functions, in any programming language including R, helps you to focus on a specific issue and reuse your solution. It is also helpful when using e.g. `sapply` and friends. What it *is* an example of, is writing loops in R are for novices rarely the best solution. There are however numerous cases when a good ole fashioned loop is the solution, such as when relying on the result of a previous iteration (and not just a neighboring element in the vector).
Take a look at https://github.com/jennybc/googlesheets/blob/master/R/gs_auth.R
If you've been using C based functions or libraries you might be experiencing zombie handles. https://randomascii.wordpress.com/2018/02/11/zombie-processes-are-eating-your-memory/
`df[logical WHERE condition, column_name] &lt;- assignment` For example: `df[df$origin=="Antigua","country_code"]&lt;-'ag'` Note: make sure you had stringsAsFactors=FALSE when the dataframe was created. Or you will need to do a bit more work.
using dplyr:: and forcats:: df &lt;- df %&gt;% mutate(country_code = fct_recode(origin, 'ANT'='Antigua', 'JAM'='Jamaica')) ... and so on, specifying how you want origin mapped to country code. Let me know if this isn't what you had in mind!
Thanks. &gt;make sure you had stringsAsFactors=FALSE when the dataframe was created I didn't. How do I change that? 
Just tried that for a few of the countries and looks like it worked! Thanks! xxoxoxoxox!!! Of course I just realized I have another problem related to Sankey diagrams... ugh...
A number in the date column like 40573 is called the "origin date" which, if I remember correctly, is the number of days past the origin date of January 1, 1970 (1/1/1970). I learned that on Udemy :)
Well you can just add the option to the data.frame statenent and rebuild
I'm such a noob I don't know what means.
Isn't this problem more appropriate for an INNER JOIN where you take your country code-&gt;origin mapping table and join it on?
Can you show us what code was used to create your dataframe? If you did it like this: df &lt;- data.frame(origin = origins, iso_code = iso_codes) then you'd want to do this: df &lt;- data.frame(origin = origins, iso_code = iso_codes, stringsAsFactors = FALSE) Alternatively, you can set that option to be false by default for that script by adding this line to the top of your script: options(stringsAsFactors=FALSE)
You should be able to do this with group_by and summarise from dplyr. df %&gt;% group_by(id) %&gt;% summarise(n(), sum(Bill), mean(bill))
df is tour data frame. No $ required for columns. %&gt;% is the pipe operator. Pass object on left to function on right as first argument. 1:5 %&gt;% mean == mean(1:5)
Ah neat. So it seems if I set that df equal to something I can keep it in memory in that form. Is there a way for me to pass along something that isn't getting grouped as well? (I'd like to have the user ID in the final dframe ideally in this format: ID | number_transactions | sum_bill | mean_bill 
The columns you called group_by on will be preserved when you summarise.
Thanks all. That was insanely fast, loops are awful.
Google “dplyr cheat sheet” and “tidyverse” .
The issue with combn, unless there's an argument I don't know about, is it only shows unique combinations. So for my example it would only return (1,2,3).
Your, not the.
That’s what I get for not paying enough attention to your post. To get permutations of a vector (where order matters), I like permn from the combinat package. 
Thanks a lot! I created a local package using your instructions and the following [checklist](https://simplystatistics.org/2015/10/14/minimal-r-package-check-list/). Everything seems to be working accordingly after I replace 'output: custom_html_format' with 'output: packagename::custom_html_format'.
Check out the help for tidyr::nesting()
Exactly what I needed. 
Ok, glad you got it figured out. Just for reference, in case it comes up again, read.csv also has a parameter for stringsAsFactors, so you can do the following: df &lt;- read.csv("myfile.csv", stringsAsFactors = FALSE) Also, if you put `options(stringsAsFactors=FALSE)` at the top of your script, it will work for read.csv as well as data.frame.
expand.grid then filter where the row sums to 6
permn is perfect, thanks!
In the data.frame statement, just add stringAsFactor=FALSE as a parameter after you specify the columns
Have fun.
To add to this for future readers you can do your own integration with caret. A method type in caret looks up a model via getModelInfo(method). If you create a list with all the same components wrapping your model fit, prediction etc you can use it within caret. To get a feel for what's necessary just look at the output of getModelInfo("lda") for example
I’m interested in working with you in this - I know R and I know about k-means but probably not enough to implement my own. However, I’m in Philly for a week. What’s your project’s timeline?
Here's what I've done in the past: If n is the number of rows in my data, I initialize an (n x k) matrix to 0. I'll call this cluster_matrix. As data x is added to cluster y, I make cluster_matrix[x,y] = 1. Once clustering is complete, I use which() to extract the members of each cluster. From there, it's trivial to assemble, say, a data matrix containing the members of a given cluster.
If you didn't save your plot to a variable you might be able to do it with P &lt;- last_plot() P + theme(legend.position="none") Keep in mind you'll have to re-render your plot anyway, so if it's rendering time you're worried about this won't help.
You could add the the clusters to your whole data. Given the results are in the order you put the input, you can cbind the cluster number and then merge the centroids to your data. Then a simple row wise calculation would give you distances 
Thank you. My Google skills obviously need some work :)
That is kind of wasteful - you only need a vector - set cluster_vector[x] = y when you add data x to cluster y.
What method did you use to create your plots, base graphics or ggplot2?
[removed]
What do you mean by “desktop” and “some library”?
You could always left join to a sequence of all the dates possible. As in, create: x &lt;- data.frame(date = seq.Date(from = as.Date("2011-07-22"), to = as.Date("2011-12-31"), by = "days")) then: dplyr::left_join(x, your_data, by = c("date" = "ADate"))
OK, then which step do you have trouble with? And why go the detour via the desktop rather than directly saving them data into the target directory? Anyway, to save a variable to an RDS (not RData; avoid!) file, use the `saveRDS` function. And use `readRDS` to read the data into R again. Here’s why to use RDS instead of RData: RDS represents a single R object; RData represents a set of objects. It’s useful to store a whole environment; for single objects, RDS provides better control.
Is it not alright to append/write to the csv when I'm not sure how long the operation will take? I'm testing my script on a 30min node so when that time is up it ends all the process. Without the script writing as it runs, I won't know how far it was able to run.
All of those functions depend on the initial reactive "my_list". So you don't need all those extra reactives throughout, since they depend on a reactive object they will update automatically. I'm sure your probably has something to do with this. Why don't you take this into R and run it as regular code/variables without the shiny conversions. Do you see the same error?
Hmm. Wouldn’t it be difficult to do that on only one period?
Use an rstudio server
I do seem to need the reactives, because... well, I'll go step by step: list_length &lt;- length(my_list()) &gt; Warning: Error in .getReactiveEnvironment()$currentContext: Operation not allowed without an active reactive context. (You tried to do something that can only be done from inside a reactive expression or observer.) whereas the following allows the app to run successfully: list_length &lt;- reactive({ length(my_list()) }) Now I have my renderUI(): output$suggestions_fileName2 &lt;- renderUI({ str_vec &lt;- vector() for(i in 1:list_length()){ current_str &lt;- paste(my_list()[[i]][[4]], "-gram file used is ", my_list()[[i]][[3]], ".txt", sep = "") print(current_str) } }) # end of renderUI() &gt; "1-gram file used is a.txt" output$suggestions_fileName2 &lt;- renderUI({ str_vec &lt;- vector() for(i in 1:list_length()){ current_str &lt;- paste(my_list()[[i]][[4]], "-gram file used is ", my_list()[[i]][[3]], ".txt", sep = "") str_vec &lt;- c(str_vec, current_str) print(paste("str_vec =", str_vec)) } }) # end of renderUI() &gt; "str_vec = 1-gram file used is a.txt" output$suggestions_fileName2 &lt;- renderUI({ str_vec &lt;- vector() for(i in 1:list_length()){ current_str &lt;- paste(my_list()[[i]][[4]], "-gram file used is ", my_list()[[i]][[3]], ".txt", sep = "") str_vec &lt;- c(str_vec, current_str) print(paste("str_vec =", str_vec)) } }) # end of renderUI() HTML(paste(str_vec[1], sep = '&lt;br/&gt;')) App displays "1-gram file used is a.txt" output$suggestions_fileName2 &lt;- renderUI({ str_vec &lt;- vector() for(i in 1:list_length()){ current_str &lt;- paste(my_list()[[i]][[4]], "-gram file used is ", my_list()[[i]][[3]], ".txt", sep = "") str_vec &lt;- c(str_vec, current_str) print(paste("str_vec =", str_vec)) } }) # end of renderUI() HTML(paste(str_vec[1:str_vec], sep = '&lt;br/&gt;')) &gt; Warning in paste(str_vec[1:str_vec], sep = "&lt;br/&gt;") : NAs introduced by coercion &gt; Warning: Error in :: NA/NaN argument Also if I try print(length(str_vec)) I get &gt; Warning: Error in : $ operator is invalid for atomic vectors What do I make of this?
[This link is a good explaination of why you need collapse instead of sep.](https://gist.github.com/briandk/d9231ba1e2603eed0df1). Run these two locally to see the output in R and how HTML will then handle them: paste(str_vec, sep = '&lt;br/&gt;') paste(str_vec, collapse = '&lt;br/&gt;') &gt; Then none of this text appears in the app. Also if I try &gt; &gt; print(length(str_vec)) No idea why this is, but str_vec is not what you expect it to be for some reason. Without the code you ran in this instance I can't figure it out.
`R CMD BATCH my_script.r`
The issue looks like it could be at the function muCalcuation. It doesn’t look like it’s receiving the values from the input sliders. It might work to call the input ids directly in the function. muCalculations &lt;- function(){ 1 - ((input$lambda) ^ input$powers) } That might explain why it isn’t responding to the input values. I’m not near my computer so I’m not able to confirm at the moment. Hope that helps!
Do you have already have a server, if so what OS is it running? If it's linux, or you need to use AWS/DigitalOcean (in which case run linux), then you should be using a [cron job](https://en.wikipedia.org/wiki/Cron) to call your Rscript.
&gt; # generate trials based on input$powers from ui.R &gt; muCalculation &lt;- function(lambda, powers) {1 - ((1 - lambda)^powers)} &gt; probability_at_lambda &lt;- sapply(0.01, muCalculation, seq(0, 100, 10)) You call your muCalculation() function incorrectly, you need to call: &gt; probability_at_lambda &lt;- sapply(0.01, muCalculation(lambda, powers), seq(0, 100, 10)) Your Rsession will display the related errors.
Perhaps OP can use a Raspberry Pi with Ubuntu for his needs?
Many thanks for your answer. It's still not working for some reason. Error: 'muCalculation(lambda, powers)' is not a function, character or symbol
Doesn't have to be Ubuntu, I run [Arch linux ARM](https://archlinuxarm.org/) on my Pi's and have R installed.
This is not an R issue but a stats issue. Shirt answer is that all entries in the 7th column probably are exactly the same. The correlation is calculated as the covariance between x and y, and then scaled by the variance of x and the variance of y. The standard deviation is simply the square root of the variance. The variance tells us how diverse the entries are. If they are all the same, variance equals zero, thus standard deviation equals zero. And you can not divide by zero. The covariance tells how much x and y changes together, but on a common scale. If y never changes (variance = 0), we cannot say anything about how x changes and covariance is zero. The correlation takes the covariance and scales it down to a value between -1 and 1.
It's standard operating procedure to recommend datacamp. It would start off slowly for you but surely you'd learn something? Probably the only resource that best fits the bite sized requirement that I know of. There's plenty of R content on coursera, designed to take 4 weeks or so per course. Could probably finish most coursera courses in a few days per.
Thanks so much!
I think this probably is what I need but I'm struggling to find examples. 
Page not found.
I did this by having the function recalculate Everytime a user inputs data using the observe function.
Sorry I'm afk right now on my mobile device. But ??observe should give you some info. Don't quote me on this but it should be something like this. observe(input$userinput, { function(isolate(input$userinput))}) What this will do is call "function" after the user makes any change to input$userinput.
I found that simply using R to do things is the best way to learn. I typically find an example, and adapt it to my work. As previously mentioned, I also endorse datacamp for structured learning. Furthermore, look at data.table on datacamp
http://adv-r.had.co.nz is a great resource at the level you're describing. Read it start to end or pick at bits that you find interesting. It has exercises which are good, but practicing on real or toy problems of your own invention is best.
Thanks very much!
This combined with tidyr::fill() would make for a nice, easy to understand solution.
How would this handle passing the previous objected returned by function() back to itself?
It would re-evaluate the expression so effectively it would throw out the old result because it is no longer valid. 
Hmm. Well here's what I'm trying to do. I have an app with a text box. It's for predictive text, so the user types something into this box, and the app suggests some possibilities for the word the user is currently typing. To do this it scans some files of words/sequences of words, and subsets all the rows that match what the user has so far typed (or part of it). As the user continues typing, it's not always necessary to re-open and re-subset an entire file - this thing will run faster if I carry forward the previous subset so that that can be subsetted further based on further user input. Is what you've suggested here going to allow me to do that?
Thanks for taking the time to answer my question! From what I can gather the stream_in function requires the {JSON_strings} to each be on a newline, but mine are separated by a comma between each. Will this be a problem?
I'd suggest your start with something simple and see how far it gets you, eg the following regex: [^.] snow [^.](\d*\.?\d*) (?:to|-) (\d*\.?\d*) If that is to imprecise, add other phrases from the sentences or units. 
I do something similar in Python. It's easier to scrape the table ([example](https://forecast.weather.gov/MapClick.php?lat=41.1863&amp;lon=-80.9652&amp;lg=english&amp;&amp;FcstType=digital)) than parse the unformatted text.
When you add Cuiyu Liao (CL) datas, you introduce cases where CL has only 2 common movies with Claudia, Mick and Toby. And that makes that the matrix has now only 2 complete rows where where the data for Gene are the same "Snakes on a Plane=3.5" and "You Me and Dupree=3.5". Either you add data to CL for the movies "Superman Return" and have 3 complete rows, or you use the "pairwise.complete.obs", but then the correlation values are not comparable between them. It's the computing of the independant correlation each of these [plots](https://imgur.com/WxHfQgC)
Try [profiling your code](https://support.rstudio.com/hc/en-us/articles/218221837-Profiling-with-RStudio) to work out the bottlenecks. Of you can't speed up your code (and 20-30 minutes isn't that long in the grand scheme) its also pretty cheap to purchase cloud computing time to do your grunt work. You could buy a VPS and set things up yourself or there are canned packages such as those available from [Amazon](https://aws.amazon.com/blogs/big-data/running-r-on-aws/). This would prserve your *NIX like working environment and combined with say RMarkdown reports and say [mailr](https://cran.r-project.org/web/packages/mailR/index.html) you can send yourself the results.
You could install ssh server on your windows machine, then set up a script to ssh into the windows box and run your Rscript on windows command prompt.
I remote into my desktop to run scripts. 
yes
I have something that appears to work. This is how my server() function starts off: list_holder &lt;- reactiveValues(dat = NULL) my_list &lt;- reactive({ x &lt;- predict_ending_wrapper(input$my_string, input$num_suggestions, input$largestNgramOnly, isolate(list_holder$dat) ) list_holder$dat &lt;- x return(x) }) So what's happening is, the function predict_ending_wrapper() is called when the user first types a letter, the last parameter it takes being a NULL value. But for each subsequent letter typed, the list created by the previous call to predict_ending_wrapper() is used as the last parameter instead of the initial NULL value. However, I blundered my way to this solution based on [this related example on stackoverflow](https://stackoverflow.com/questions/37116347/how-can-i-use-a-previous-reactive-value-in-a-reactive-expression-in-shiny) and I don't fully understand why it works. Is there any chance you could spell out what's really happening here?
Check for ?strsplit You can use the following code to split your string. strsplit(string,split='-', fixed=TRUE) Put que result in a char vector and use vector[1] for the first and vector[length(vector)] for the last. I'm not with R now, but I believe this will além.
your guess is as good as mine, like I said I'm pretty newb to this myself.
Wtf is going on here. If you go on stack overflow and post this and link to it here I will help solve your problem. But I don’t understand what you’re looking to do. If you post on SO use dput(head(df)) so I can play with your data. Or post it here in a comment or something. 
I feel too stupid for SO. I guess I can try to figure out how to post there.
Please don’t! Loads of beginners post their questions and it’s much easier to read and post. Just post some of your data and clearly state and end goal or maybe an example of what it should look like, and I’m happy to dig through it and help out. 
It's partly the beginner thing, it's also that the actual posting is confusing. I am doing it right now but trying to figure out how to include the data is what I'm having trouble with. The dput thing just makes a super small version of the set that doesn't really have enough data. I mean I can describe it in words well enough. But while I was posting this a friend mentioned case_when so I did some googling and found a solution. df_accent &lt;- df_orig %&gt;% select(`hs_pos_m_time_First Click`:rp_neg_f_att_accent)%&gt;% mutate(type = case_when( hs_pos_m_att_gender &gt; 0 ~ "hs_pos_m", hs_neg_m_att_gender &gt; 0 ~ "hs_neg_m", mw_pos_m_att_gender &gt; 0 ~ "mw_pos_m", mw_neg_m_att_gender &gt; 0 ~ "mw_neg_m", rp_pos_m_att_gender &gt; 0 ~ "rp_pos_m", rp_neg_m_att_gender &gt; 0 ~ "rp_neg_m", hs_pos_f_att_gender &gt; 0 ~ "hs_pos_f", hs_neg_f_att_gender &gt; 0 ~ "hs_neg_f", mw_pos_f_att_gender &gt; 0 ~ "mw_pos_f", mw_neg_f_att_gender &gt; 0 ~ "mw_neg_f", rp_pos_f_att_gender &gt; 0 ~ "rp_pos_f", rp_neg_f_att_gender &gt; 0 ~ "rp_neg_f"))
Okay, so the policy here is that you need to post the R part of your question and you need to post example code of what you’ve tried so far. Include that in your post and you’ll get a better response. 
Excellent. Thank you very much. I will have a good day taking this one apart and learning some new verbs and structure.
Thank you for your help. This gives me something to work with and a place to go look now. 
And if you want to automate that you could `touch` the `restart.txt` file using a [cron job](https://en.wikipedia.org/wiki/Cron).
Umm, I'm feel like I'm totally a robot, but k looks undefined to me. What value(s) should it take in the do function? You're not passing in a k parameter / argument that I see. 
OP, this is the way to go, but note that strsplit outputs a list, so you need to either unlist the result or extract the one (and only) element of the list to get your character vector. vector=unlist(strsplit(string,split='-')) out=c(vector[1], vector[length(vector)] vector=strsplit(string,split='-')[[1]] out=c(vector[1], vector[length(vector)] 
How are you calling `doClaraParrallel`? And have you loaded `library(parallel)` before trying to run it? 
The R Shiny website contains an extensive gallery of examples that show the working code next to the apps. [This would probably be the best place to start](https://shiny.rstudio.com/gallery/). [This example dealing with file input may be useful to you](https://shiny.rstudio.com/gallery/file-upload.html). What sort of output are you looking to get?
Can you please restate your question specifying what you want as your IV(s) and DVs? It is a little unclear to me whether you are asking how you would use time as an independent variable, or if you are somehow trying to think of it as a DV? 
Well, as I said I already have the code, but I would like a few plots
I might be missing something, but wouldn't the "merge" function work here? Something like: q1q2 &lt;- merge(q1, q2, by.x = "project", by.y = "project", all = TRUE) 
Woops that was a horrible typo. I meant to say: 2 IVs, both categorical 1 DV, continuous. The model should be used to predict the DV based on the 2 IVs. I actually need to make this so I can assess the distribution of the residuals (distance of my data points from the line produced in the model) so I know whether to use a parametric approach (rmANOVA) or some other nonpara approach. 
Thanks for your response. This would work if I am trying to combine the columns together, but I am trying to paste the rows together so that the new data frame has the rows from Q1 and Q2 together.
Sorry, should've read more carefully. I think you want the bind_rows() function?
Your `col="red"` seems to be specified inside `WMA()` not inside `lines()`
You're fine. Is that in a particular library? Can't find it. If you are referring to rbind, it does not work because the column names are not the same
I'm not in front of a compute right now. But try looking into joining functions with dplyr. Should be as simple as left_merge(df1, df2). I haven't used them in a while, though, so not sure what problems may arise.
Thanks, but are you referring to left_join from the dplyr package? Assuming yes, I don't think that could help me achieve what I need above. I could be wrong though.
oh damn. i've been staring at it so long i just couldn't see it. thanks, it's working now.
I'd suggest you go to try to run Rstudio from the command prompt. Maybe then you will see some more useful error message.
Can you please provide more info like the crash log if available? PM me I can help you out maybe 
Honestly? Not really... the vast majority of R users (myself included) are 'secondary programmers' who use programming as just another toolkit. That being said - R's documentation is actually pretty good. If you have experience with other OOP/scripting languages then you can probably pick it up pretty quickly. It's not complex, just pretty idiosyncratic. Stack overflow helps a lot, since there's more 'real programmers' there.
Check out this comment I wrote in another thread. https://www.reddit.com/r/datascience/comments/7yg61i/comment/duh6n02?st=JEDV4GC2&amp;sh=94355eb1
That's my understanding yes. Generate a vector filled with ones and twos.
The "must-reads": * [Hadley Wickham's Advanced R](http://adv-r.had.co.nz/) - this free book will teach you the advanced concepts, including environments, metaprogramming, and parsing expressions, * [The R Inferno](http://www.burns-stat.com/documents/books/the-r-inferno/) by Patrick Burns - this one is rather untypical, in that it tells you what to _avoid_ to be a successful R programmer. Learning statistics/data science with R: * [R for Data Science](http://r4ds.had.co.nz/) by Hadley Wickham, * [Discovering Statistics Using R](https://uk.sagepub.com/en-gb/eur/discovering-statistics-using-r/book236067) by Andy Field.
 library(tidyverse) Q3 &lt;- merge(Q1, Q2, by = c("Customer", "Project", "OtherInfo"), all = TRUE) Q3 &lt;- select(Q3, 1, 2, 4, 5, 6, 7, 8, 9, 3) Q3 &lt;- arrange(Q3, OtherInfo) That gives me the same output as what you requested, just with NAs introduced instead of the 0s. 
It is worth noting that RStudio is not R. It is an R IDE and there are a lot of other IDEs for R out there. I personally hate RStudio and wholeheartedly recommend you try RKWard if you have issues.
The best book for that I would say is **The Art of R programming**. From the "who is this book for" section: &gt; Many use R mainly in an ad hoc way—to plot a histogram here, perform a regression analysis there, and carry out other discrete tasks involving statistical operations. But this book is for those who wish to develop software in R. Second one (from the ones I've read) is **Advanced R** which is I think already linked somewhere in here. And I haven't seen a book who tries to teach both statistics and R and does it in an acceptable manner. So probably R and statistics should be learned separately. But that depends on how deeply you want to get into statistics.
Yeah, the problem solved, thanks bro.
A part of your debugging process, you should try plain vanilla R from [cran.r-project.org](https://cloud.r-project.org/). That has to be already installed I think for Rstudio to work.
Make sure R and Rstudio and your library are installed on your c drive and not a shared or network drive which they may be by default on a work machine. Performance suffers greatly when they are.
This, sort of. I'm a programmer with a few languages under my belt, and I still find R impenetrable, despite *several* attempts. I disagree it's "not complex". If you're not a data-analyst or statistician, the learning curve is really steep. You might be better off taking a few courses on statistics, and learning R along the way. You can DO programming in R, but it's not really a programmers language.
I'm curious why you say this -- there's nothing particularly "statistics-y" about R as a language, in my mind. It's a functional language with some optional OO stuff strapped on top of it and some syntactic idiosyncrasies. 
Try replacing print(xy) with just xy.
Could you provide a [minimal working example](https://gist.github.com/hadley/270442)?
Awesome, can't wait to take a look. (On phone at min) is it based on other common bootstrap themes? I was wanting to put a package like this together but haven't found the time yet. 
This is a great task for the dplyr package and the pipe function. Assuming that the data is in a dataframe, the ram column is numeric, and the CPU column is lowercase, something like this should work: df %&gt;% filter(ram = 16 &amp; cpu == "intel")
Thanks - let me know your thoughts when you take a look. At the moment there are mostly bespoke themes and one immitation of Flatly, but you can use custom settings to easily create your own theme similar to one from Bootstrap.
Will do. I'm off work tomorrow for my fiancées birthday so will take a look Wednesday morning 
dplyr::case_when() perhaps.
ram = 16 should have the double equals sign for comparison rather than assignment. The "base R" way of selecting the rows would be square brackets indexing, something like: pc[pc$cpu == "intel" &amp; pc$ram == 16,] Although will work poorly if you have any missing data in cpu/intel. If `table(pc$cpu)` gives OP what they want, but just for cpus, it sounds like maybe just the cross-tabulated frequency would do. You can get that with: table(pc$cpu, pc$ram) 
Hmm, maybe we've had different experiences... I don't even do stats in R, but rather learned it for the purposes of matrix manipulation (I.e. a free alternative to matlab). What part of it do you find "impenetrable?" Im pretty good at R relative to my peers, and I think that's because I started talking to actual programmers about it...
Make a Major Product Type lookup table. It should have two columns, one should be the full ProductType, the other the MajorProductType you want to use instead. Each row is one the 30 product types. Then the following one-liner should replace the product type with major product type: df$ProductType &lt;- MPTtable$MajorProductType[match(df$ProductType, MPTtable$ProductType)] `match` finds the rownumbers in the lookup table where the producttypes from your data are found. The rownumbers are then used to index the MajorProductTypes column from the lookup table. In this case they are assigned over the top of the ProductType column, which may not be a good idea. This may not work well with NA's. A left_join using dplyr would be simpler and more robust, but requires dplyr. df &lt;- dplyr::left_join(df, MPTtable) 
This would be perhaps simpler than the other suggestion to create a lookup table and join. The case_when() statement can be combined with %in% to reduce to three cases. 
&gt; &gt; # this binds the rows together, adding NAs where no data is available &gt; your_data &lt;- bind_rows(data1, data2) I've been learning R for the better part of a year now, can I still play the newb card when I call a package a library? 😅 I think I still have 5 out of the original 10 punches left. There is always a dplyr command that exists to solve my existing issues. I am happy I do not need to make a overly complex function to accomplish this task :)
Good stuff, thank you. 
wow i had no idea something like that existed that will help so much with data cleaning. Do you know any good resources to easily reference the rest of dplyr? beyond just the cheatsheet i mean.
 df &lt;- df[Year==2017,] Compare don't assign
Hi all thank you for reaching out, it seems to be running fairly stable today :)
Whoops, good catch on assignment vs comparison!
No, it's the operator for equivalency, regardless of object class.
Well same problem. It just replaces stuff that don't match the condition with NA 
Try: df &lt;- df[which(df$Year == 2015), ]
Yes... can you post a more reproducible example? Based on the limited information you've given, I'm 100% sure the which subset should work. But, maybe your data structure is different from what I'm imagining... 
df = subset(df, Year != 2017) ? 
Honestly when a package is that dense with good stuff it can be worth it to just browse the list of functions and checking out the ones that sound interesting. https://www.rdocumentation.org/packages/dplyr/versions/0.7.3
Yes, in fact 0 is probably more likely to be the wrong place to start than the right place for most data. https://www.vox.com/2015/11/19/9758062/y-axis-zero-chart
Yes. The only time it is strictly necessary to have the x-axis intersect the y-axis at y = 0 is if you a) have something important where y = 0, or b) have bar charts.
The X axis is arbitrary in terms of Y - likewise, the Y axis is arbitrary in terms of X. For any value of Y, you can literally have an infinity of possible X values - so it really doesn't matter. If you're drawing a graph, put your axes wherever they make the most sense. If your lowest Y value is 1000 and your highest is 1010, it wouldn't be very useful to start your Y axis at 0. The 0 point is arbitrary.
If you're asking about visualization best practices, I agree with the majority of other commenters--it's absolutely fine to start it somewhere other than 0. If you're plotting a monthly time series across five years and the data ranges from 200k to 250k, you're losing a lot of interpretability by forcing 0 to remain in the plot. Now, there are times when removing the 0 can be misleading. For example, you can over-exaggerate the delta in a graph by snapping the Y-axis to a narrower range. It's important to use discretion and present the data in a way that's both interpretable and persuasive (if you're trying to persuade) but still ethical.
Have you tried legend() ?
As everyone has said, you can start Y (or X) anywhere you want. But always make sure to label the axes--especially if they don't start at zero, because that's the assumption that everyone makes.
oh wow i had no idea how deep it actually was. thanks for pointing it out!
Learn sed 
Maybe regular expressions?
Not only analysis, but the data visualization is fantastic. You can also make live apps for dashboards as well, pulling directly from databases. Just curious, why do you need a business proposal for a free tool?
Thanks, I'll do some research on the live apps. It's more of a formality. It's a government job and we have to run it on a VM that anyone on the team can connect to.
I switched some reporting for a client over to R and it was much easier to handle record/observation dependant information with lookahead/lookbehind. Things like where an observation that matched x and y criteria, where we only want to keep the first/last occurrence of that happening with a certain timeframe. The SQL queries were getting convoluted and long. Plus, merging information from external data sources was straightforward. I set up [R as a websocket server](https://github.com/MattSandy/httpuv/blob/65dc6097ef07ef46d433bd7b522fedd235a0426f/demo/json-server.R) which had it return json encoding back to the web page, and populate information with VueJS. You can always go the Shiny route as well.
Just because r is itself free doesn't mean it's free per se to an organization. Depending on how locked down they keep their environments, there will be costs associated with deploying it and maintaining it. Also, a lot of employers wouldn't necessarily want only one or two in people in the organization to know how to use a tool. If those people leave, what happens to the work they leave behind? Do you now have a project in production that no one knows how to maintain? There's a lot of things to consider with having r being used in an organization, so it makes sense someone would need to present a business case.
Got it, that makes sense, I was just curious. The apps are called Shiny, btw.
&gt; Just because r is itself free doesn't mean it's free per se to an organization. Depending on how locked down they keep their environments, there will be costs associated with deploying it and maintaining it. To be fair, that's going to be the case for any new software system that's deployed. So it might be good in a business-case analysis to compare something like TCO (Total Cost of Ownership) between R/Rstudio/Server/Shiny and competing options like Alteryx, Tableau, etc... and to also add something like R done "in house" vs. contracting with a company to provision and manage the installation.
 library(XML) library(RCurl) theurl &lt;- getURL("https://www.sec.gov/Archives/edgar/data/320193/000032019317000070/a10-kexhibit2112017.htm",.opts = list(ssl.verifypeer = FALSE) ) df &lt;- readHTMLTable(theurl)[[1]][-c(1,2),] names(df) &lt;- c("Foo","Bar") https://stackoverflow.com/questions/1395528/scraping-html-tables-into-r-data-frames-using-the-xml-package
That's... more money than I have.
I'm a PhD student and I analyse my data with R which I'm new to. I guess I would donate my whole scholarship! (now try to figure out whether the scholarship is so small or I spent so much time with R!) 
You're kidding, right?
I expect that is true for the vast majority of R users, but have a suspicion it is also somewhat tongue in cheek.
Did you try ??linetype or ?linetype (if ggplot is loaded) when I do that it appears to pull up examples of all possible lifestyle values. 
Gosh. Thanks a lot Wusuowhey. Can't believe it is that easy!
&gt; a Just exploring the package now, first very minor point. When you install a package from github via devtools it doesn't build the vignettes automatically. I would either build the html and include it in the source, or in your README have `devtools::install_github( ... build_vignettes=TRUE)`
More generally, R has very good built in documentation. Running ?[functionname] will provide you with help files for all of a base (and any well dveeloped package) function's arguments, including defaults.
Good point - I had trouble building vignettes in the source package and over time forgot to fix it. Looking back, I'm not sure if the vignette is really required if just a duplicate of the GitHub readme instructions?
I potentially agree but if I am working on the train I can't see your readme. I quite like the look of the themes but I'm a little confused by the implementation. Your R functions paste the CSS code. For the set themes would it make sense to have the css files in an inst directory and load them in? Similar to how the style for shinydashboard is implemented. 
Fair point - the vignette would continue to be available offline once downloaded through a package. I will look into building it as part of the source. Avoiding local css files, and opting for style code being wrapped into R functions, was a key feature to allow users the ability to use the set themes as an example or starting point for creating their own options. Anyone can copy the settings used for a set theme and tweak it to their preference without requiring css knowledge.
Fair enough on css. I will have another look tomorrow with that in mind. Hope you find more people interested in your package. I tweeted your blog post out as I think it would be good if more people contributed to making styling easier. have you come across the dull package? Its essentially shiny but based on bootstrap v4 which might open up some more interesting style options. It's very much experimental it would seem at the moment though. 
Either, sed is a bit easier to learn and get job accomplished, buyback is a bit more powerful if they want to use it later.
Also, dplyr's count(). df %&gt;% count(x)
This a comment I posted in another thread. https://www.reddit.com/r/datascience/comments/7yg61i/comment/duh6n02?st=JEJ52YJS&amp;sh=28532ecd
Thanks for the direction.
The reason I said awk is because it has variables, arrays, and assignment. So its programming model is a lot more understandable. I couldn't even imagine now to do this in sed.
[Learn x in y minutes](https://learnxinyminutes.com/docs/r/) helped kickstart me coming from another language. After that http://r4ds.had.co.nz/ is worth checking out. 
You could save your R-markdown document as a html/pdf/docx file. They won't be able to run any of your code or get at the raw data, but it should work as a report.
thanks for the advice, I thought about this but figured there had to be a better way
Talk with the person you are doing the analysis for. Maybe he only wants the results and figures, in which case send PDF with explanation and no code. If he wants code, send a script that was used to produce the pdf side by side. If he wants to be able to run the code himself, see the results and figures, and doesn't know R - use something that he knows.
Mark down is the R way to do it. I don't think there is a better way to present your work. But if you want to try, you can use Jupyter notebooks with R kernel. But I would prefer Rmarkdown as it works better with R.
You can author interactive Rmarkdown documents (really just shiny applications) and host them on shinyapps.io. It's a bit more involved than regular Rmd, but it can be very useful. The intended audience won't need R installed, just web access. https://rmarkdown.rstudio.com/authoring_shiny.html
You will not regret it! And I swear by DataCamp, man.
Your requirements as you've written them seem contradictory. "What's the best way for me to share my R code with a non-programmer who doesn't have R installed?" - the answer is, there isn't one, because that person doesn't want your R code. If they wanted your code, they would not consider themselves to be a non-programmer in a way that was materially relevant to sharing your code with them, and they would have R. Since they don't have R and say they're not a programmer, that means they're uninterested in code. Perhaps if you could clarify the reason why the person is interested in seeing the code, it would cut this Gordian knot?
In short I'm trying to show a nontechnical superior how simple R can be to solve a particular problem they are having. 
i'm not too familiar with garch modeling. but this seems to be a pretty extensive resource for a package for this https://cran.r-project.org/web/packages/rugarch/vignettes/Introduction_to_the_rugarch_package.pdf
Could you give me an overview by means of a step by step guide getting VaR and ES estimations of log returns of any asset through GJR-GARCH modelling?
`df %&gt;% count(x, sort=TRUE)` (just learned about that argument)
I'm trying to get an overview no a micro step to step guide. 
That's the plan without the shiny overview. I want to document the whole process and share via blog post. 
then you should be the one to work through it
On the function arguments, I wondered if it's worth setting default values within the function definition. That way if anyone forgets to include an argument it wouldn't cause the function to break.
Set your input$whatever to a reactive context: https://shiny.rstudio.com/reference/shiny/latest/reactive.html You will then have to call inFile() instead of just inFile, or set it if you want to keep the code as is. inFile &lt;- reactive({input$layer}) if (is.null(inFile())){ return(NULL)} data &lt;- brick(inFile()) You should also probably else if your if statements, but I don't quit understand what you're trying to achieve with those return(NULL)'s.
Thanks a lot ,questionquality, very helpful!
I think it's because you're adding a newline even on the last iteration of the loop. Why not just do the lazy thing and use as.list(names(m)) though?
&gt; as.list(names(m)) Hi Tergon, I need to output the msg in that particularly defined format, instead of list format. Thanks for your suggestion!
Nice! Thanks a lot !
`cat` doesn't concatenate strings (`paste0` does that!), it prints them. In other words, your code immediately prints each line, and finally prints the empty `msg` object (the documentation of `cat` tells us that its return value is `NULL`).
Use the rgamma function in R?
Look at the 'paste' function - giving it a vector and a single length character should do exactly what you need
Seriously, OP is making this way more complicated than it is. It's just: paste(my_const, my_vec)
Lol R is weird like that, lots of built in functions that do specific things that happen to be common operations. Sometimes you don't know a function exists till someone tells you! A recent one for me was 'which.max' and 'which.min' - I've been using complicated subsets with 'which' and max/min for years but was someone watched me do it recently and leaned over to be like "you know there's a shortcut for that right?" Mind. Blown.
if you've run a region of code before, you can re-run it by pressing ctrl+shift+p. this works even if you've edited the code. no selection required.
Not sure about your specific situation, but have you considered storing this specific code in another file? Just open a separate file with the code, have it point to your data, and run it there. I only mention this because when I was new to R I hadn’t considered it, and would have these long files open with different sections of code. Breaking it into multiple files can, *depending on your situation*, make it much more manageable.
Var&lt;- (1/n)sum((x-u)^2) Am I missing something? Is this a trick question?
Think about the definition of variance: the average squared deviation from the mean. So, first you need to find the mean of your data: m &lt;- sum(x)/length(x) Oops --- only use sum right? Stupid requirement but why not. Length is the same as the sum of the number of positive numbers, negative numbers, and all 0s: n &lt;- sum(x &lt; 0, x == 0, x &gt; 0) m &lt;- sum(x)/n OK now we need the squared deviations from the mean, so we just subtract each item from the mean and square it: dev &lt;- (x - m) ^ 2 Now we just need the mean of these deviations: var &lt;- sum(dev)/n
It kind of is a trick yeah. Your answer presupposes n and m, and OP can't use anything but sum (and operators I'm assuming). So you need to substitute an implementation of length that only uses the sum function (in my other comment I implicitly converted to logical and summed those), and then you need an implementation of mean which uses sum and the new n variable. Otherwise your answer would suffice!
It's incredibly easy in RKWard.
This is because x == 1 returns NA wherever x is equal to NA. If you give the logical indexing function (`[`) a NA value it spits out a NA row in the output - one for each missing value in df$x. There is a base R way to do this neatly when you have missing values: subset(df, df$x == 1) Subset treats NA values as FALSE for logical indexing. You can also do: df[df$x == 1 &amp; ! is.na(df$x), ] This works because NA is a valid logical object, so if the logical test is not ambiguous it evaluates fine. In this case NA &amp; FALSE gives FALSE. Here's a nice stack overflow answer on when you might want to use base::subset vs. dplyr::filter (link)[https://stackoverflow.com/questions/39882463/difference-between-subset-and-filter-from-dplyr]
Honestly I think that the R ecosystem is such that we can generally assume our desired operation has been attempted by someone else, and may even be a default/primitive. My recent discovery was [`rownames_to_column`](http://tibble.tidyverse.org/reference/rownames.html)--was really nice to find out I don't need to re-implement this across projects. 
Update: I have been made aware that the [rio](https://github.com/leeper/rio) package is a far better documented &amp; designed R package that accompishes the same goal as readit, and more. The lesson here should be: practice better Google-fu before you reinvent the wheel...
Also take a look into e-books, in my experience you can sometimes even download a pdf of the book, so you always have a reference available. I personally like Hadley Wickham’s books. While they focus on his R packages I found the step-by-step format and exercises helpful. 
Hadley Wickham’s R for Data Science. It’s free online and there’s a paperback on amazon if you want!
Second Wickham's books, they are really great!
Here’s a comment I wrote in another thread. https://www.reddit.com/r/datascience/comments/7yg61i/comment/duh6n02?st=JEQEQLHA&amp;sh=c6dec59e
Get "R for Everyone" by Jared Lander!
[removed]
[removed]
[removed]
[removed]
[removed]
[removed]
The just want to echo u/spuds_mckinness suggestion. Every other book will teach you the basics of R well enough, but they’ll do it in a way you actually use for a while. I started with the book of R and jumped around a lot until I found something that worked for me. There’s a lengthy section on matrices, programming convention, base plotting, and a lot of things you won’t be able to get up and run with if you want to do data analysis. Too many books treat R as a regular programming language(like java or C++). But why come to R to learn computer science? R is a data analysis environment more than anything else and should be treated with practical examples. R for data science does this in spades. There are good exercises in there(some I found challenging) and a lot of good information on how to use the tools Hadley Whickam created for R, which will be enough for most of your purposes as you start out.
[removed]
[removed]
[removed]
[removed]
[removed]
The R Inferno by Patrick Burns is amazing. It's a book about trouble spots, oddities, traps, glitches in R. Gets you into some excellent habits.
Looks like a few people have already mentioned Wickham's "R for Data Science". That's the one I used starting out, and I found it easy enough to follow. After that, I started on his "Advanced R" book, as well as Crawley's "The R Book".
You did not give enough information to help you
do you have a reproducible example?
Wat
Definitely not the best way to do it (someone with good dplyr knowledge will definitely one up me here), but this will get the job done d &lt;- c("a",1,2,3,"b",1,2,"c",1,2,3,4,5) d &lt;- append(d,"Termination") breaks &lt;- which(is.na(as.numeric(d))) neato.list &lt;- list() for(pos in 1:(length(breaks)-1)) { group.name &lt;- d[breaks[pos]] group.vector &lt;- d[(breaks[pos]+1):(breaks[pos+1]-1)] neato.list[[group.name]] &lt;- data.frame(Group = rep(group.name,length(group.vector)), Value = group.vector) } df &lt;- rbindlist(neato.list) 
Are the groups always characters and the elements always numeric? If so you could write a for loop. Pretty inelegant, but it'll work.
[removed]
 groups &lt;- Reduce(function(x,y) if(is.na(as.numeric(y))) y else x, d, accumulate=TRUE) data.frame(groups, d)[d!=groups,] Might be a bit complicated to understand but essentially this checks elements of `d` in pairs and when the next element is numeric - it takes the previous one. If the next element is not numeric - it uses that element instead.
Thanks! Its elegant yet tricky. I haven't used the Reduce function until now. So, to check my understanding, the first element of the "groups" vector is "a" because the first comparison was function(a,1). In that function, the *if* statement evaluates to FALSE because "1" can coerce to numeric. So the function then defaults to the *else* statement, which takes the first of the two argument to function(a,1), which is "a". PHEW! That really is tricky. TIL.
You get points for being the first. I know for loops are taboo around here but they might not be prohibitively slow for my specific application. Your code gets me very close to a solution. However I can't use it precisely as written because you generate the group.vector using the colon operator. I can't expect all the elements within a group to be successive integers. The elements could just as easily be c("a",3,1,2,"b",102,33,2,... etc). My bad for not specifying at the outset what the code needed to do.
In practice the groups and elements are strings that I can tag as one or the other using regex. Your code will still work in that situation. Plus its simple to understand. If I can't wrap my head around the vectorized approaches others gave then I'll probably do it like how you suggested. Thanks!
You're right it does! Apologies. I got you mixed up with someone else's code. Thanks for your approach. 
No problem, it is a wonky bit of code. I think it might be faster than a typical loop though by predefining the breaks. That reduce bit looked pretty interesting on that other person's code. 
No problem. Depending on how much data you have, this is probably a slower solution as well. Main advantage is readability, and that it's super easy haha
Outputting a list of the rows of the data frame seems counter-productive, so I assume you're asking for a new data frame with every other row from the original. This is pretty easy with the tidyverse dplyr library. library(dplyr) #Loading the library cars %&gt;% # "Piping" the data frame into subsequent functions mutate( n = seq_along(speed), # Creating a row-number counter i = ifelse(n %% 2 == 1, 1, 0) # Binary variable of even = 0 / odd = 1 ) %&gt;% filter(i == 1) # Selecting only odd row numbers The `n` variable assigns an incremental observation number to each row. The `i` variable checks whether the number is even or odd (through the [modulo function, `%%`](https://en.wikipedia.org/wiki/Modulo_operation)), and the filter() call selects odd-numbered observations. Change `filter(i == 1)` to `filter(i == 0)` to select even-numbered observations. 
A simple way to do it. Set a TRUE/FALSE filter to keep every other row: df # Pretend df has 52 rows, and you want rows 1,3,5,7, etc... filter &lt;- rep(c(TRUE, FALSE), nrow(df)/2) df_filtered &lt;- df[filter, ] You replicate a list of alternating TRUE/FALSE statements. Subsetting a dataframe with TRUE statements will keep the rows, FALSE will drop.
See my edit comrade
The legend is the little box that tells the viewer the difference between different parts of your plot (ie red means this, blue that, and that dotted line is the mean, or whatever). If you want to label your axis (and you should), you can do it with `labs(x = "Age", y = "Eggs per gram")`, and if you want to label the year facet, you can do `facet_wrap(~Year, nrow=1, labeller="label_both")`
[removed]
What formats does your CAD program read? png? svg would be my best guess, knowing next to nothing about CAD.
Collapse?
Depending on exactly what you're looking for, you could try: c(5 + 5, 5 - 5, ... ) or list(5 + 5, 5 - 5, ... )
Knittr is doing the rendering from R markdown to html. The chunk option you are looking for is results='hold'. This is detailed in the manual for Knittr here: https://yihui.name/knitr/options/#text-results ```{r echo = FALSE, eval= TRUE, results='hold'} 5 + 5 # return 10 5 - 5 # return 0 5 * 5 # return 25 5 %/% 5 # return 1 5 %% 5 # return 1 5 ^ 3 # return 125 5 ** 3 # return 125 ```
You could paste the results all together and format the string as you want, but then you'd be doing the work of the rendering engine.
you can't make this video in ggplot2. you can make a waffle chart using emojis for a person with clever use of ggplot2 and the emojifont package though. 
I'm not sure if you get derive the exact value, but you can get a numeric estimate. The median is when the F(x)=0.5, so you want to minimize the distance between 0.5 and the integral of your pdf from 1 to x. So, if you define a function that looks at the absolute value between your CDF and 0.5, then you want to minimize that. cdf &lt;- function(x) x^2 - 2*x + 1 min_func &lt;- function(x) abs( cdf(x) - 0.5 ) nlm( min_func , p=1.5 ) ## Double-check cdf( 1 + sqrt(2)/2 ) 
A more "numeric" solution , that would work with more complex distribution as well would be : pdf &lt;- function(x) 2*(x-1) cdf &lt;- function(x) integrate(pdf(x),1,x)$value optimize(f&lt;- function(x) abs(cdf(x) - 0.5) , c(1,2))$minimum
Thanks. It works wonders!
Thanks. This is what I was looking for!
Yes. You can do that with Shiny
yes
You can do whatever you want in shinny. But some of this things will need more work then others.
I'm sure someone could recommend a better way, but one way would be to make 2 columns just using normal markdown, and then in the left column include the a copy of the code block as just display the code. And in the right column include a code block that just displays the output. I'll cook up an example in a few just let me fire up R.
Yeah, I almost used `integrate` for the generalized solution, but in the past I’ve had it give wrong solutions. I don’t recall the function, I think it was an “edge case”, something like where I didn’t know the bounds, so I tried to go a bit beyond what I thought they’d be. So I figured I’d give my solution in terms of the CDF, figuring that someone can either derive it or use `integrate` to approximate it.
you can with htm https://pastebin.com/2S4CShAF
Do you have some example code?
Another possibility, depending on your needs, are to x^n or log(n) your variable in color aesthetic to make the color change in some non linear way. This can often differentiate colors in a useful way. 
One idea is to use some transformation of the data for the color scaling - say, a logit or probit or Fisher z. This works if what you have is bunching toward 1 and -1. Sometimes there's even a model-based justification for using such a scaling. 
You want to use the "duplicated" function which will return a logical vector. your_data[!duplicated(date),]
Is average Joe going to work with data? If so, what could average Joe be planning to do with it? An employee in sales or marketing wouldn't benefit at all.
x2 - R is a FUNCTION-al language, and it operates smoothest when you break tasks up into manageable chunks, organized in functions. IMO it's the best compromise between process generalization and task completion speed.
summary() ... looks at output in confusion
Even in sales or marketing there is lots of data to analyse.
Seconded. There are actually tons of sales and marketing positions that *require* Python or R (or tools like SPSS).
Yeah for your basic non data role that sometimes has to do repeat analysis, being able to run a simple r script could save them a lot of time.
this comment is underrated 
Here's a handy resource with code: https://www.r-graph-gallery.com/portfolio/ggplot2-package/ I would recommend using the library ggplot2 for your data viz purposes in R.
Got it. Had to use library(lubridate)
Average joe can use R in whatever their respective field is to improve on current systems or reveal better practices to better their respective field and become more than average Joe. Statistics is a great tool but combined with the power and relative ease of R can unlock so much even to the average Joe
I've used R many times for basic file management and string manipulation, which helped for contact clean up when exporting stuff from Apple. It's also useful for any API access, which is great for government data (exploring crime data before you move to a city for example). There are packages for oddball tasks like optimizing the impact of your Reddit posts. There are utilities for converting colors between color types (RGB to hex) which could come up if the average Joe is blogging and wants to just look up a damn color they like from Excel. R is my go-to for almost all technical tasks, so being good at R it makes it easier for me to use it for anything. 
I actually used r to analyze my budget for last year. I used text mining techniques to assign expenses to budget categories and looked at changing trends over time. I also calculated a few optimizations for payoff strategies. 
Im not familiar with KDE, what is it?
i can't find the tweet now but I saw something similar showcased the https://github.com/thomasp85/transformr package It's work in progress so you might want to tweet at the author https://twitter.com/thomasp85
Well... bigger limits 
If the average joe uses excel then there is a lot R can offer: 1) It's free 2) Does not crash like excel 3) Amazing graphing capability (ggplot) 4) Gives you scale - do it once and then re-use code 5) Amazingly unbelievably power to create pivot table like analysis 6) As you develop your R skill, you then dare to imagine solutions to problems that you never would have dared to imagine C'mon mate, join the club. 
Shouldn't it be `summary(...)` ? (-:
Things like lookups are extremely fast in R vs excel. 
+1 Could you clarify what you mean for 'create pivot table lime analysis' though?
Specifically I want the bars on each end to remain, but I don't want the dates associated with them. x1&lt;- as.Date(c("2017-01-01", "2017-02-01", "2017-03-01", "2017-04-01", "2017-05-01", "2017-06-01")) y1 &lt;- c(1,2,3,4,5,6) plot_ly(x = x1,y = y1, type = 'bar')%&gt;% layout(xaxis = list(tickmode = 'linear', tick0 = as.numeric(as.POSIXct("2017-02-01", format = "%Y-%m-%d"))*1000, dtick = "M1") ) r plotly
your_data[!duplicated(date)] works if your_data is a data.table.
That's the guy, but it looks like it was the particles package. https://twitter.com/thomasp85/status/968593132311982082
Ahh okay. Thank you
[This](https://cran.r-project.org/web/packages/imager/vignettes/gettingstarted.html) package can be used to make histograms of a image, with some fiddling you could do the same minus the animation.
The plot function is generic, meaning that it looks at the types of data you give it and decides on the appropriate plot. If one of your variables is a factor (i.e. categories) and the other is numeric then you will get a boxplot. Try coercing your data with as.numeric() before passing it to plot. Your final comment mentioned lubricate. The native date-time class in R is called POSIXt. Tournament can get a scatterplot vs time if you first pass your time variable through as.POSIXct.
Hell, I barely use it for stats beyond summing things up and taking means, but I find the way it can manipulate/display data to be invaluable. Great for interfacing with websites and scraping data from tables, or even from PDFs.
Dear Rlanguage, I'm new to R and haven't discovered all of it's wonders yet, but I hope to expand my knowledge! Could you help me recreate a similar graph? I don't understand how I can display the weeks for each subjects, and how to add the little squares to the lines.
What *data* do you have? The "little squares" are data points. Subject 1 had an event at about 4, 6, 10, 12, 15, and 28 months (roughly interpreting). The creator of this plot had access to the fact that those events occurred, and when they occurred.
Posting what your data looks like would help, but here's a starter: library(ggplot2) ggplot(df) + geom_point(aes(x = Time, y = Subject, shape = as.factor(event)) + geom_segment(aes(x = 0, xend = max(event), y = Subject)) I'm sure that will need some tweaking. Look into ggplot2, geom_point, and geom_abline.
This is completely false. In Sales Ops and I use R weekly. I know many reps I support use it or other similar tools. 
Average Joe here, I work for a tech giant in Sales Operations. My job doesn't necessarily require data-wrangling skills, but R sure is a time saver. I second those in the thread who say it's basically advanced excel. Anyways I deal with lots of people internally who are great with data, and lots who are HORRIBLE. R can save me days of Excel work (transforming messy/untidy data from a rep), with just a few lines of code. I use it for string-matching to find duplicate data in our CRM, and to check for dupes on system IDs over 16 characters, which Excel has trouble with. If you are doing anything with data over 10k rows, or you have a Mac (Excel for Mac is pure trash) I highly suggest learning a bit of R just to save you time and headache. 
I was observing and plotting the trends in polling data and I had to use the command lubridate(library) to get it to exit the boxplot function. Thanks for the help!
I meant using the dplyr library functions group_by and summarise. Use it daily in my work. Permanently moved to R a year ago. R has a lot on offer. Don't wait.....just do it!!!
This one: https://github.com/nolanlab/citrus? what do you use it for?
Oh yeah, I am in the transition period, I still use Excel for the quick one-off requests, but my goal is to not open it again within a couple of months!
My data consists of 7 subjects, who were followed for 40 weeks. They underwent MRI scans roughly every 4 weeks. We assessed these MRIs for events. All of the subjects had them, but some more than others (and some also had multiple events in one MRI scan). Some subjects did not undergo all 10 scans (for example, one only had 3 scans, or 12 weeks in time). So, like in the figure, I need to display all the subjects separately, and show how many weeks they participated, and when events occured.
Thanks for the suggestions! I will look into them. So, my data consists of 7 subjects, who underwent MRI scans roughly every 4 weeks. Most of them were followed for 10 weeks, but not all of them. Some were only followed for 12 weeks (3 scans). We checked these scans for certain events. All of the subjects had events, but some more than others (some even multiple events in one scan).
Nah, if you hit the limits you just go download more RAM for your R.
Hey there! Everyone's asking for sample data because it really helps us understand how to get from A (your data) to B (the graph). Describing the data is helpful but it's much easier &amp; actionable for us to understand if you show us an example of that the data looks like. The data doesn't have to *actually* be your data--you can totally anonymize it. An example would be: subject|scan_num|event_freq|date|right_censor :--:--:--:--:-- 1|1|6|01-01-2016|0 1|2|3|04-01-2016|0 1|3|7|07-01-2016|1 2|1|1|01-01-2016|0 2|2|8|04-01-2016|0 2|3|2|05-01-2016|0 If we can see what *shape* your data is, that will make answering the question much simpler because we'll know what sorts of aggregating, filtering, and selecting would need to be done to get to the chart you want.
Hey there! Everyone's asking for sample data because it really helps us understand how to get from A (your data) to B (the graph). Describing the data is helpful but it's much easier &amp; actionable for us to understand if you show us an example of that the data looks like. The data doesn't have to *actually* be your data--you can totally anonymize it. An example would be: subject|scan_num|event_freq|date|right_censor :--:--:--:--:-- 1|1|6|01-01-2016|0 1|2|3|04-01-2016|0 1|3|7|07-01-2016|1 2|1|1|01-01-2016|0 2|2|8|04-01-2016|0 2|3|2|05-01-2016|0 If we can see what *shape* your data is, that will make answering the question much simpler because we'll know what sorts of aggregating, filtering, and selecting would need to be done to get to the chart you want.
Hey there! Everyone's asking for sample data because it really helps us understand how to get from A (your data) to B (the graph). Describing the data is helpful but it's much easier &amp; actionable for us to understand if you show us an example of that the data looks like. The data doesn't have to *actually* be your data--you can totally anonymize it. An example would be: subject|scan_num|event_freq|date|right_censor :-|:-|:-|:-|:- 1|1|6|01-01-2016|0 1|2|3|04-01-2016|0 1|3|7|07-01-2016|1 2|1|1|01-01-2016|0 2|2|8|04-01-2016|0 2|3|2|05-01-2016|0 If we can see what *shape* your data is, that will make answering the question much simpler because we'll know what sorts of aggregating, filtering, and selecting would need to be done to get to the chart you want.
On my way out so I can't properly format it but tick0 can just be tick0 = "2017-02-01". Also, change dtick to dtick = "M3". It will get rid of your tick labels at 1 and 6. However, it might not arrange the labels exactly as you want. 
A simple learning method would to be to have it build up a Bayesian strategy tree. Basically , play the rules and when it comes to a choice, it makes the decision on which column to drop a chip into based on the known probabilities of winning with that move. Then have it update the tree based on conclusion of game. Over time that tree should get more and more information. It maybe an idiot at first but it will eventually learn the tree and destroy you every game. Not simple but not overly complicated either, especially if you can figure out how to write and update the tree to a disk, so it doesn't have to start stupid everytime.
Why use Excel, can you not create databases for the data. Much easier to manage and query.
It sounds like you've tried nothing and it hasn't worked. What (code) have you tried so far? Have you loaded the maps library? Have you looked into the maps package? 
[removed]
I found the problem, I never loaded the map(library) or whatever. Long story short I had to install the map command.
Thanks for the response. I actually figured it out. I needed to make it dynamic so the first and last are always gone no matter the number of observations so I turned everything into character factors and made the first and last dates “ “
Depends, if you have the time and drive learning python or Java may ultimately help more for customization and flexibility. If you want to automate reporting or build dashboards and have less time, R may be your best bet (a lot of R packages transform R code to other languages).
Lime analysis
Thanks, I'm between Python and R. Python seems a lot more intimidating for sure.
Yeah, I can appreciate that! I think you can probably do most anything you would need to do in R. There are so many packages, it is just a matter of finding what you need and learning how to use it. Here is a good resource on cleaning data: https://cran.r-project.org/doc/contrib/de_Jonge+van_der_Loo-Introduction_to_data_cleaning_with_R.pdf I may also recommend looking into the Power Query tool and Get and Transform, especially if you use a lot of pivot tables https://www.youtube.com/watch?v=ajFWkl8eTmQ
Thanks!
That is really hard to read, but make sure the as.numeric is the first function to touch the column from your data frame. Try switching the cut and as numeric
If it makes a differences, I literally copied all the code and messages I received in R when the code didn’t work. So some is code some is R responding. Sorry
Please try learning data.table and shiny I developed crazy spreadsheets that used c++ to trade on the open market (pairs trading and gamma scalpers). I wish I had learned r sooner and there had been things like shiny and RStudio. Pm me if you want a more in depth answer. 
R is way easier to use to manipulate data frames than Python with Pandas IMO. Checkout this cheat sheet. https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf Also get SWIRL for R. It has some very nice tutorials for doing data wrangling. 
Truth. 
Always learn R!
Now that I know R, doing the tasks you're describing would make me want to shoot myself. So I guess I'd vote yes
if you're using VBA daily, you could easily get into both Python and R to do the kinds of things you're talking about. i just took a basic R class and also did most of the examples in Python just so that i'd know how to do the work in both. it was not a big deal at all, and i come from a similar background to you.
So this is what an example of my data could look like: ID | Scan1 | Scan2 | Scan3 | Scan4 | Scan5 | Scan6 |Scan7 |Scan8| Scan9| Scan10| Firstscan| Lastscan |Event1 |Event2 |Event3 |Event4 |Event5 |Event6| Event7 |Event8 |Event9| Event10| Event11 |Event12| Event13 |Event14| Event15| Event16| :- | :-: | -: 1 |0 |4 |8 |12 |16 |20 |25 |29 |34 |37| 0| 37 |4 |20| 25 |29| 37 |37 |NA| NA| NA| |NA |NA| NA| NA| NA| NA| NA| 2| |0 |3 |7 |11 |15 |20 |25 |30 |34 |40| 0| 40| 30| |NA| NA| NA| NA| NA| NA| NA| NA| NA| NA| NA| NA| NA| NA| NA| 3| 0| 3| 6| 11| 17| 20| 25| 29| 34 |38| 0| 37| 0| 20| 29| 29| 38| NA| NA| NA| NA| NA| NA| NA| NA| NA| NA| NA| 4| 0| 4| 11| 14| 23| NA| NA| NA| NA| NA| 0| 23| 11| NA| NA| NA| NA| NA| NA| NA| NA| NA| NA| NA| NA| NA| NA| NA| 5| 0| 4| 8| 12| |16| 21| 25| 30| 34| 39| 0| 39 |12 |NA |NA |NA| NA| NA |NA| NA| NA| NA| NA| NA| NA| NA| NA| NA| 6| 0 |4| 9 |14 |NA |NA| NA |NA| NA| NA |0| 14| 0 |4| 4| 4 |4 |4 |4| 9| 9 |9 |9| 9| 9| 9| 9| 9| 7 |0| 4| 9 |14| 18| 22| 27 |32 |39| 41| 0| 41| 18 |NA| NA| NA| NA| NA| NA| NA| NA| NA| NA| NA| NA| NA| NA| NA| 
This is an example of my data: https://docs.google.com/spreadsheets/d/1K_9Y_6cy5ktezF6L2HALWRCeCg6qufGrN1aonxhLqbs/edit?usp=sharing
Would you care to share with the rest of us?
I was going to offer to speak with him on the phone. A lot of the selling points are user specific. 
That data is a mess. Maybe someone here is willing to put a lot of effort in, but it's not something I can do in a free post here. Good luck. If you ever have control over recording data, go for one observation per row. patient, week, event_number Multiple rows per patient are fine (and preferable).
Why could the example you present not be addressed with a simple SQL query, if the data is all in a relational database?
Can he run R on data that he doesn't have the privileges to though? 
Well, that part is not going to change either way, unless he gets a hook to the database
200k-300k is quite small to some people. R is fine for this much data but may not scale well to 'big' data. In fact, I always have thought with the functionality of dplyr and the entirety of the tidyverse, R is the best for analytics. Especially if you are a data vis guy (ggplot2 has a bizzllion extensions now that sit right on top of the base package nicely). I will say though that when things get big, R has problems. I just tried to load up a file with about 8 million rows and R was simply too slow so I am going to use Python + Spark as I will be adding a lot more data to this. Even 8 million rows I do not consider big data. In a few months, I have to look at data which is a size large enough to contain hundreds of millions of rows. R would not even work for this. As in it simply could not handle it. If you do need to scale up a lot more and start having memory issues on your pc you can look at R packages which run code in parallel. But do not think about any of this stuff until your boss asks you. Focus on the tidyverse and you will streamline your life. Begin to take a tidy approach to everything you do and you will pump out clean and reproducible analyses. Good luck with all your endeavours! 
For R it seems to be the rule of thumb that if you have to ask if you need it, the answer is yes.
Is there an approach for doing interactive data entry in R?
Yes! Look into the tools that MS is doing with R, they might fit into your current workflow well. 
Thanks, this is great. I'll look at all this.
Assuming the organisation is running Outlook for mail, it's possible to set up rules and even a bit of VBA (though it's awful) to automatically watch for that email with the data file, extract the attachment, and invoke a batch file that can launch R. It's a kludge, but a nice lazy-making one.
Shiny.
Wow this is suddenly a huge load of information! Thanks so much for this! Looks like I've got tons to read up!
You mean with a js table? I haven’t done that for quite a while, but didn’t find it as user friendly, enter value, hit enter, automatically in next cell etc, as editing with Excel. So I tend to still edit tables in excel then read them in. Do you have any recommendations/updates?
This article may be of interest... [Persistent data storage in Shiny apps](https://deanattali.com/blog/shiny-persistent-data-storage/ "Persistent data storage in Shiny apps") You can also use [Google Forms](https://www.google.com/forms/about/) to design a form to capture your data which is then stored in a Google sheet. The [googlesheets](https://cran.r-project.org/web/packages/googlesheets/) package can then be used to read this data into R and work with it.
Depending on what you are doing I would advocate that databases are still better for capturing data, although for small work/studies this may be over kill. You can't deploy Excel on the web to capture data either (a common method these days), but see my reply below on how to capture data on the web using R (via Shiny or Google Forms).
Thanks!
&gt; Depending on what you are doing I would advocate that databases are still better for capturing data Yes, of course, for almost all cases. My reply was merely about replacing Excel with direct R equivalents. If ever there’s an R package that provides a ready-made solution for general-purpose DF editing, my answer would change. But as it stands, creating an editing form for tabular data with a comparable UX to Excel (or other data entry applications) requires substantial effort in R.
https://en.wikipedia.org/wiki/Kernel_density_estimation
**Kernel density estimation** In statistics, kernel density estimation (KDE) is a non-parametric way to estimate the probability density function of a random variable. Kernel density estimation is a fundamental data smoothing problem where inferences about the population are made, based on a finite data sample. In some fields such as signal processing and econometrics it is also termed the Parzen–Rosenblatt window method, after Emanuel Parzen and Murray Rosenblatt, who are usually credited with independently creating it in its current form. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/Rlanguage/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^| [^Donate](https://www.reddit.com/r/WikiTextBot/wiki/donate) ^] ^Downvote ^to ^remove ^| ^v0.28
"Rather it will replace your need for Visual Basics/VBA/Excel completely" I would disagree with this line of your answer: IMO R *cannot* replace spreadsheet software. Spreadsheet software is damn fast to get data in, transform it, visualize it and share it. Excel does a simple job done reliably &amp; quickly which is suitable for small projects, or one-off reports. Why write a reproducible code if you won't run it twice? On the other hand, its limitations are obvious: working with over 100k rows you'll see the loading pointer a lot and when the complexity of your project gets from light/low/draft to 'medium' size it'll blow up in your face when trying to maintain it. To give an example, I usually spend 2-3 hours in the start of every project in excel : in this time one can get a prototype or parts of the project up and running to get an initial feel of it - maybe not with all the data of the final product. Some formulas here, some formulas there, up to 6-7 pivots, and max 15 lines of (macro produced/google copied) VBA code for this part to avoid the spreadmart effect. Then I do the transition to R / Python / databases if needed to get the more powerful data manipulations, reproducability, programming logic, easy integrations to other programs, statistics &amp; predictions, pretty visuals, etc. It's easy to get data into R/python as it's a single read-csv command. Nevertheless, for simple/prototype/initial/draft work, copy/paste to excel is still 100x faster. ...as for the rest of your reply +1 enilkcals. Completely agree on the R comments and that one should not take courses on VBA.
Hey there. So I think you have a lot of work to do before you can get to this plot. First you need to get this data in a tidy format. This isn't an R limitation, by the way--if you were sticking this in SQL or Tableau you'd need to do the same thing the same way. In the words of Hadley Wickham, "Like families, tidy datasets are all alike but every messy dataset is messy in its own way." Like the other guy who posted, it's probably going to be too much work for me to get you from your data to that plot. BUT I can give you some pointers on shaping your data to make it easier for you to get there (or to post to stackoverflow :)) First, your columns should be variables and your rows should be observations. So all your patients should be in a single column. Scans should also be in a single column, like scan_num. That way, there will be a row for each scan/patient combination. Since the x-axis mentions time in months, you'll want a date column. If there are specific events that you want highlighted, those should also be in a column. Wickham's tidyr package will help you get there. You can read about it [here](https://cran.r-project.org/web/packages/tidyr/vignettes/tidy-data.html).
For a square matrix X, the following code snippets will work for the first 4 questions. Don't have any time to do the fifth or the bonus, but will do so later if no one else replies. * dim(X) * Matrix::isDiagonal(X) * length(unique(diag(X))) * A &lt;- X; diag(A) &lt;- NA; mean(A, na.rm = TRUE)
If you do want to look at a heatmap, you don’t need to write a new function. `heatmap()` will work fine. To stop it from reordering or rescaling anything, for a matrix `m`, try `heatmap(m, Rowv=NA, Colv=NA, scale=“none”, revC=TRUE)`
Did you ever find a decent solution here? I'm looking for the same thing.
Many great responses here. But I am going to go in a different direction -ask your boss for an Alteryx license. 
That sounds crazy. Are you not allowed to directly interface with the database?
Yes. Also check out foverlaps in data.table and you'll realize that there're things you wouldn't even consider if SQL were your only tool. I use it to match business events at a point in time to business info over a period of time. So if something happened in 2009 I want to know the state of that business for whatever fiscal year during that event. As you mentioned I also like to do "how many times did another (irregular) event happen x days before the first event". It's hard in R, but unthinkable in SQL. 
Yes but Python has more support across the board so it is useful to not ignore it. 
well this would not work for me as the matrices are often too large to view without scrolling.
Thanks! I did write a heatmap function after all. [example output](https://github.com/SchmidtPaul/useful/blob/master/img4.png?raw=true)
Is there something wrong with scrolling?
Thank you so much!!
Text to Columns on ";" delimiter
This is very helpful. Thank you. Is there a way to scrape the content of just one panel of the accordion, rather than an entire accordion? I'm thinking about how to have fewer steps in cleaning the text scraped.
I agree, but the point was to learn a systematic framework of how to solve his problem. Granted I was drunk when I made that comment and probably could have contributed something a bit more useful. Nice solution above by the way
I think the View() function should do this, but can't recall actually using it on a matrix. Does what you describe for data frames. 
Well you’re getting a list of text back — just select the element(s) you want and discard the rest. You can also do this in the selector directly — use Developer Tools to get the path of your target element.
As a long time shiny user from the beginnging, I can understand the frustration. I discussed with Ian Lyttle on my latest podcast [episode](https://r-podcast.org/25) about best ways to get started, and we both highly recommend the effective reactive programming videos from shinydevcon (linked in the list). Once I truly understood the nuances of the concepts it changed everything for me.
Thanks.
An R pod cast? Subcribbed 
I'm not a machine learning person or anything even close, but it sounds like you might need it
I've tried various versions of Python connectors over the years, this is the first one that worked for me. 
I guess wrapping something is the way to go. Supercollider maybe?
Hmm. So I generated your example table using the following: oq_missing &lt;- matrix(c(1,NA,3,4,1,2,NA,4), nrow=2, byrow=TRUE) oq_missing &lt;- data.frame(oq_missing) And I ran exactly what you did, and I think It worked? I removed the ```[ind[,1]]``` because it didn't seem to be doing anything. It replaced the first NA with 2.67 and the second with 2.33. When I ```View()``` the data frame, it now has ```2.00000000``` and ```3.00000000```, but I can't seem to figure where it is changing those values (especially in your case). 
don't have a computer on me but out of curiosity is ~year a discrete variable in this context
I’m not sure what you’re trying to do with rollapply but you want apply(). The 2 parameter specifies iterate across columns rather than rows (1). apply(ret, 2 function(x) rollapply(x, ...))
Its a retention model, so it takes the sum of the last 3 values in one of the d columns (d1, d2,d3, etc...) and divides them but the sum of the total accounts that signed up on those same days (acc column). so as you move across the days columns (d1,d2, etc...) you need to shift the 3 day window up 1 in the acc column since the people that started yesterday havent had a second day to return yet. This continues across the columns 
Ah. I see thank you!
Took a new stab at it ###Column data must be stored in a list rather than a matrix since the ###vectors have different lengths ############################################# cols = list(c1 = sample(20,10), c2 = sample(20,9), c3 = sample(20,8), c4 = sample(20,7)) cols ############################################# ###Sum the last three measures of each columns ############################################# N=3 sumcol = sapply(cols, function(z) sum(z[(length(z)-N+1):length(z)])) sumcol ############################################# ###Create A vector ############################################# A = sample(20,10) ############################################# ###Divide by last 3 in A looping up each time ############################################# coloverA = NULL for(ii in 1:length(cols)){ offset = ii-1 #How much to move up A one ach iteration i1 = length(A)-N+1-offset #lower A index i2 = length(A)-offset #upper A index sumA = sum(A[i1:i2]) #sum A values coloverA[ii] = sumcol[ii]/sumA #divide colsum by A sum } coloverA 
The original code works; the changes in number format was throwing me off. Thanks!
nevermind doesnt work with my actual data
facet_wrap should accept character vectors for your variable to facet on as well. Try + facet_wrap("year") You should also try coercing your year variable into a factor, within your ggplot syntax or before hand. Try these troubleshooting tools... # Outside ggplot solution... table(orlando_combined$year) # If you find anything unexpected here, fix it orlando_combined$year_fac &lt;- factor(orlando_combined$year) # Coerces year into factors, may appear out of order # Inside ggplot solution... (less safe, I can't test this without a reproducible example) + facet_wrap(~ factor(year))
I've got a project going where I use the tuneR::getMidiNotes function and spruce up the tuneR::lilyinput function so that I can write midi files. I send the midi file through a pipeline that looks like readMidi %&gt;% getMidiNotes %&gt;% prepare_input %&gt;% lilyinput2. I end up with a .ly file that I can use to generate a pdf score and a midi file with my alterations. Here's the project: [lilyinput](https://github.com/areeves87/lilyinput). Pretty bare bones right now but it sort of works and I can spend time tweaking it if need be. Appreciate any and all feedback.
I think that this should work, but I'm not able to test it at the moment. the.animals[the.animals$name %in% target.animals$name,]
So this gives me all the data I need, plus some. It catches every individual I need but includes their "full record," including information from other years. So I get all of the records for G008, from years 2000-2017. Can I add another statement that will evaluate the years too? 
Yes, put them in parentheses and separate them with an &amp;
This feels close!! I created my target.animals dataframe like this: &gt; name &lt;-c("G008","G040","G074", "G110","G111", "G115", "G118", "G119", "G120", "G126", "G127", + "G128", "G129", "G130", "G131", "G132", "G150", "G151", "G152", "G153", "G154", "G155", "G301") &gt; sex &lt;- c(2,1,3,2,1,2,1,1,2,1,2,2,2,2,1,1,2,2,2,1,2,1,1) &gt; years &lt;- list(2006, 2006, c(2009, 2010),2008, c(2008, 2012, 2013, 2014), 2011, 2011, c(2011, 2012, 2013, 2014), + c(2012, 2013), c(2012, 2013), 2013, c(2013, 2014), c(2013, 2014), 2014, 2014, + 2014, 2013, c(2013, 2014), c(2013, 2014), c(2013, 2014), 2014, 2014, 2014) &gt; &gt; target.bears &lt;- data.frame(name = name, sex = sex, years = cbind(years)) And now when I use merge: &gt; testmerg &lt;- merge(the.bears, target.bears, by.x = c("name", "loc_year"), by.y = c("name","years")) Error in bmerge(i, x, leftcols, rightcols, io, xo, roll, rollends, nomatch, : typeof x.years (list) != typeof i.loc_year (integer) So I guess using a list/cbind to create my dataframe means that R cannot interpret the "years" column as an integer? 
Without the parentheses it seems to work, what do the parentheses do? test &lt;- the.animals[the.animals$name %in% target.animals$name &amp; the.animals$loc_year %in% target.animals$years,] 
So I think to accomplish what you want, you are going to need to clean up the second data frame, so that each year has its own row (so animals with multiple years will have multiple rows). I accomplished that like so: library(reshape2) library(tidyr) # subset only the name and year column from the second table data_sub &lt;- target.animals[, c(1,3)] # Separate the year column into 4 columns - use separate from tidyr data_sub &lt;- separate(data_sub, years, into = c("y1", "y2", "y3", "y4"), sep = ", ") # convert the data frame from "wide" to "long" using melt from reshape2 data_sub_melt &lt;- melt(data_sub, id = "name", measure.vars = c("y1", "y2", "y3", "y4"), value.name = "year", na.rm = T) # remove var column data_sub_final &lt;- data_sub_melt[,c(1,3)] Your data for the second table should now look like this: name | year ---|--- G008 | 2006 G040 | 2006 G074 | 2009 G110 | 2008 G008 | 2008 G040 | 2011 G074 | 2010 G008 | 2012 G008 | 2013 G008 | 2014 For the final step, you can use data.table to join the tables, by setting the key values on both tables to name and year: library(data.table) # convert data frames to data.table type main_data &lt;- setDT(the.animals)[] subset_data &lt;- setDT(data_sub_final)[] # set the keys for both tables to prepare for the join setkey(main_data, name, loc_year) setkey(subset_data, name, year) # Join the tables - I believe you want an inner join but I suck at these # and I'm too lazy to build the toy data, so you may need to mess around with it a bit # inner join main_data[subset_data, nomatch=0] Hopefully that works. 
What you are describing is a join. Dplyr makes an inner_join() that does just this and is very performant. library(tidyverse) the_animals &lt;- read_csv("name, bear_ID, location_n, x, y, loc_date, loc_time, loc_year, loc_month, loc_day, fix_type G001, 21202, 161, 494000, 5820000, 2000-05-24, 11:01:00, 2000, 5, 24, 2 G008, 21202, 590, 512000, 5830000, 2000-09-01, 19:01:00, 2000, 9, 1, 3 G001, 21202, 621, 498000, 5830000, 2000-09-13, 07:01:00, 2000, 9, 13, 2 G074, 21202, 66, 493000, 5830000, 1999-05-11, 13:02:00, 1999, 5, 11, 3 G001, 21202, 31, 489000, 5800000, 2000-04-26, 11:01:00, 2000, 4, 26, 3 G001, 21202, 63, 49000, 5830000, 1999-05-10, 21:01:00, 1999, 5, 10, 3") target_animals &lt;- read_delim("name| sex | years G008 | 2| 2006 G040| 1| 2006 G074| 3| 2009, 2010 G110| 2| 2008 G111| 1| 2008, 2012, 2013, 2014 G115| 2| 2011" , delim = "|", trim_ws = T) inner_join(the_animals, target_animals, by = "name")
Check out my reply. It uses inner_join() instead of merge and also includes code to create those datasets.
I second this method. You need to tidy the dataset first. 
You can simplify this a lot by using `strsplit` with `unnest` from {tidyr}. No need for `separate`/`melt` with hard-coded columns.
Here’s the {tidyr}/{dplyr} way key = target.animals %&gt;% mutate(years = strsplit(years, ', ')) %&gt;% unnest() result = semi_join(the.animals, key, by = c('name', loc_year = 'years')) This requires that the `loc_year` and `years` columns are both of type `character`. If `loc_year` is a numeric type, you can make the `key$years` column matching by adding `%&gt;% mutate(years = as.integer(years))` at the end of the definition of `key`. Another thing: change the variable names; don’t use dots in variable names, they should be reserved for S3 lookup!
Nice use of `separate_rows`, didn’t know that. A `semi_join` is probably more appropriate than an `inner_join` here.
Very true... I meant to use semi_join() but was distracted. Thanks!! 
The parentheses are not needed in Python here, either. Saying `a in foo and b in bar` works.
You are going to have better luck is you post some "toy" data or illustrate your question. I think I can asnswer this one though: &gt; ...is there a way to query columns, so let's say if column job contains sales, update industry to marketing This would be pretty simple with base R subsetting: Data$industry[data$job == "sales"] &lt;- "marketing" Assuming I am interpreting that correctly. Cheers.
You can use the sqldf library to run SQL statements that treat your data frames like database tables.
To subset your data you use []'s. df[df$id == 999,] This will give you the rows of your data frame where the id is 999. If you want to then replace value in the column "jobs" for this subset you would use: df[df$id == 999,]$job &lt;- "sales" In your example you are replacing the name column for any rows that has "John Smith" in the name column and "WI" in the state column. You could also do this using: df[df$Name=="John Smith" &amp; df$State=="WI",]$Name &lt;- "John Smith1" It sounds like you might want to replace several rows at once matching multiple criteria. If you had a list of id's, say: ids &lt;- c(999, 998, 997, 996) You can subset your data frame by these id's, and replace the "jobs" column for any rows that match these id's: df[df$id %in% ids,]$job &lt;- "sales" 
Tidyverse gives you database table join operations (and so much more). library("tidyverse") df &lt;- data_frame(name=c("John Smith", "Joe Blow"), id=c(100, 103), state=c("WI", "IN")) jobids &lt;- data_frame(id=c(100,101,102,103), job=c("sales", "finance", "HR", "IT"), industry=c("marketing", "retail", "food service", "manufacturing")) result &lt;- df %&gt;% left_join(jobids) 
Thx I will try this out
key method to use is "findInterval", here's an example x &lt;- runif(10,0,120) y &lt;- round(runif(10,200,800)) bins &lt;- seq(0,120,by=15) inds &lt;- findInterval(x,bins) data.table(x,y,bin=bins[inds])[,.(sum(y)),by='bin']
Really feels like a missed opportunity to use an actual image of RStudio in the header image...
What exactly is your confusion here? This should be pretty straightforward. Just fit the model in a regression fold, evaluate on the held out observation, rinse and repeat for each observation. 
Don't you need to do it for every possible combination in best subset selection?
Not only is it *not* RStudio, it's some type of html/php (which would be really odd to use RStudio to code html/php.)
You make a variable called mYd, so it's in memory. Then you ask sqldf to look for a column called mYd. Then rsqlite tells you that column doesn't exist because its not a column in df3, it's a value in memory. You to either 1) have the value of mYd in your query string using something like: paste("SELECT a.* FROM df3 a WHERE (a.Yield &gt; ", toString(mYd), ";") 2) or actually make a mYd column and put it in the data frame. THen your query would work. 
Thank you. I just learned a number of important things. I gave up before your hints and got to work with a subquery too. sqldf("SELECT * FROM df3 WHERE ( Yield &gt; (SELECT avg(Yield) FROM df3) )" )
The avg function returns a mean, not a median.
for real? I just have two on the side you sure?
https://www.google.com/search?q=tic+tac+toe Try playing a few rounds against google. It’s pretty good. Any ideas about how to make a tic tac toe program line this one?
Median is not an SQL function so I had to improvise with Avg.
It is a solved game though, so every game should be a draw. 
For that, you'll have to modify the program so that it takes the player's input and, based on that, picks the optimal tick for it to win.
https://xkcd.com/832/
Shall we play a game?
I guess I should have said every game is a draw when playing the correct move. If you are playing random moves, then sure, x has the advantage. If you are playing correct moves then it is a tie.
I'm guessing the matrices are not symmetric, so you can't use the singular value decomposition. It's possible that one of the matrix libraries in C++ (Armadillo or Eigen) would improve performance, but they tend to use the same underlying BLAS or LINPACK algorithms as R. I don't think Armadillo has a matrix power function however. Personally, I've found that Rcpp generally does lead to a slight improvement in matrix multiplication. One advantage is that you could pass in the extremely large matrix by reference, whereas expm likely requires making a copy. You can start by modifying something like [Eigen's matrix power function](https://eigen.tuxfamily.org/dox/unsupported/classEigen_1_1MatrixPower.html). It's not too difficult to use with Rcpp and RcppEigen. 
It makes it print out the result of the assignment. Just a shorthand. Saying res=tapply(df$count, df$term, sum) res does the same thing
Ahh right, cheers 
I realise this might not be helpful if you are determined to use R for this, but I was trying to do something similar last year and reeeaaaally struggled. Every method I tried didn't actually give me what I needed. In the end I asked my partner who is a developer (I'm an ecologist) and it took him just a few minutes in SQL and most of that was him trying to understand what I wanted.
This is a perfect case study for dplyr and piping.
Just some thoughts, if you're running a lot of aggregates, subgroup sums, use dplyr. I try to stay away from it just because it can lead to some issues with package delendency when you are programming workflows . Your desired output in dplyr: DF &lt;- df_3 %&gt;% group_by (term) %&gt;% filter (term =="aaa"|term =="bbb") %&gt;% summarise (count = sum(count)) %&gt;% as.data.frame () In base r: DF &lt;- aggregate(count ~ term, data = df_3[df_3$term == "aaa" |df_3$term == "bbb",], function(x) sum(x))
Yup. I had no dun reading that without pipes.
check this out. I belive it will cover your needs https://cran.r-project.org/web/packages/kableExtra/vignettes/awesome_table_in_html.html
Check the knitr package out. If you’re using rstudio, you can just knit it to a word doc.
How You 'knit' it?
I’m sorry, I forgot and am not near a computer right now. However, it looks like you want to keep the entire string up to the ‘@‘ symbol. You should be able to use: Df$textcolumn &lt;- stringr::str_replace(Df$textcolumn, “@“, “”)
As in "will run in parallel"? That's pretty use case specific. In general anything iterative, in lists, or subsetable can be used with the par*apply functions. However, using an HPC will be more difficult depending on the scheduler used. I've spent hours just getting the right MPI libraries to work....
depends on the method, realy, for many itterative algorithms yous smallest pvalue depends on a number of iterations
You could try [patchwork](github.com/thomasp85/patchwork).
Ahh excellent, thanks.
It’s unclear what you’re asking. {ggridges} works fine with data that spans more than a year (why wouldn’t it?) but note that by default it plots *densities* so plotting a time series isn’t really meaningful. You need to show what data you’re attempting to plot, and how.
My guess is OP is talking about a 12 month constraint in the y variable and my guess is it’s because they did something like aes(y = month(Date)). No way to know without any code though ¯\_(ツ)_/¯
You dropped this \ *** ^^&amp;#32;To&amp;#32;prevent&amp;#32;anymore&amp;#32;lost&amp;#32;limbs&amp;#32;throughout&amp;#32;Reddit,&amp;#32;correctly&amp;#32;escape&amp;#32;the&amp;#32;arms&amp;#32;and&amp;#32;shoulders&amp;#32;by&amp;#32;typing&amp;#32;the&amp;#32;shrug&amp;#32;as&amp;#32;`¯\\\_(ツ)_/¯`&amp;#32;or&amp;#32;`¯\\\_(ツ)\_/¯` [^^Click&amp;#32;here&amp;#32;to&amp;#32;see&amp;#32;why&amp;#32;this&amp;#32;is&amp;#32;necessary](https://np.reddit.com/r/OutOfTheLoop/comments/3fbrg3/is_there_a_reason_why_the_arm_is_always_missing/ctn5gbf/)
good bot
Good bot
Thank you, guepier, for voting on LimbRetrieval-Bot. This bot wants to find the best and worst bots on Reddit. [You can view results here](https://goodbot-badbot.herokuapp.com/). *** ^^Even ^^if ^^I ^^don't ^^reply ^^to ^^your ^^comment, ^^I'm ^^still ^^listening ^^for ^^votes. ^^Check ^^the ^^webpage ^^to ^^see ^^if ^^your ^^vote ^^registered!
Sounds like a simple use-case for some REGEX
`num_values &lt;- as.numeric(gsub("[^0-9.]", "", non_num_values))` takes each element from the vector non_num_values and deletes characters that are not 0-9 or ".", then coerces to numeric
That makes a lot of sense and is a simple enough solution! Thank you! Would this still capture the "Mill." and "Thou." given there is a "." present or nah? I would run the code but at work currently. 
Yes it would leave all "." characters regardless of position and how many are present
Interesting, I've never heard of Omegahat -- will definitely check it up. I am aware of http://www.renjin.org -- which I checkin on from time to time but I think it's not ready for widespread use. My personal reason for OP was that I feel a lot of Java developers who are rolling of the varsity assembly lines or write Java for a living self-exclude themselves from trying to learn statistical machine learning. I think the thought of learning a new Programming language can be quite daunting. So I wonder if one could take some of the lessons learned in the R and Python ecosystem and see if workarounds can be found in Java. Some of my personal thoughts were: 1. Lack of a REPL (substitute with Groovy) 2. Lack of a statistics oriented graphing library 3. Verbosity (come up with an API that is [fluent!](https://en.wikipedia.org/wiki/Fluent_interface)) 
Also I just re-read your question, and the reason the original table gets updated, is that data.tables work by reference (rather than creating a copy) which is way more efficient when you are working with large data sets. [This stackeoverflow(https://stackoverflow.com/questions/10225098/understanding-exactly-when-a-data-table-is-a-reference-to-vs-a-copy-of-another) answer does a good jub of getting into the technical side of things.
{data.table} is counter-intuitive because, unlike most other objects in R, assignment doesn’t actually copy the table — it merely creates a reference. This can be good for efficiency (potentially fewer unnecessary copies) but makes the code harder to reason about. You need to use the [`copy` function](https://www.rdocumentation.org/packages/data.table/versions/1.10.4-2/topics/copy) to suppress this behaviour: flCensusLog &lt;- copy(flCensus)
 server &lt;- function(input, output){ output$AhtPlot &lt;- renderPlotly({ if(input$barGraph) { graph1() } else { graph2() } })
Tidyr::separate_rows() is designed for this. 
Thanks! Knew it was easier than I was making it lol
`do` is your friend here: my_data %&gt;% rowwise %&gt;% do({ tibble( Role = .$Role, Personnel = .$Personnel %&gt;% str_split(",")) }) 
Create a new variable before the ggplot Call like: cust_lims &lt;- “c(0,100)”. Then pass that into both lims and labs where you want it. 
so if you follow some tidy data principles you could do this with just ggplot 2 in b4 observations and variables the length of the observation in terms of Y, the position in terms of X. The caps (dot and arrow) can be specified in ggplot2. the axis options too, there should be an option. trend line and legend are also doable. 
Here is the ggplot2 master list: http://r-statistics.co/Top50-Ggplot2-Visualizations-MasterList-R-Code.html hit control+f and type 'Dumbbell Plot' - essentially a better version of this plot (which suffers from overplotting and poor choice of colour). That will get you started in the right direction. Tweak that code I would say. 
yep, ggplot should do. Here's some basics you can tweak: # load packages library(lubridate) library(dplyr) library(ggplot2) # make up some data startDate = ymd('2014-09-01') endDate = ymd('2015-03-31') timeWindow = as.integer(endDate - startDate) measureInterval = 3 # a measurement every __ days date = seq(startDate,endDate,by='days')[1:timeWindow %% measureInterval == 0] year = year(date) k = length(date) preM = 38; preS=8 # mean, sd of pre-medication diffM = 12; diffS=5 # mean, sd of difference pre = rnorm(k, preM, preS) post = pre - rnorm(k, diffM, diffS) myDat = data_frame(date, year, pre, post) # plot it myDat %&gt;% ggplot(aes(x=date))+ geom_linerange(aes(ymin=post,ymax=pre))+ geom_point(aes(y=pre), color='gold')+ geom_point(aes(y=post), fill='royalblue4', shape = 25)+ geom_smooth(aes(y=post), color = 'royalblue4', linetype='dashed', method='loess',se=F)+ facet_grid(~year, scales='free_x', space='free_x', switch ='x')+ scale_x_date(name='Time, mo', date_breaks='1 month', date_labels='%b')+ scale_y_continuous(name = 'mPDS, Points', limits=c(0,100), breaks=seq(0,100,20), labels=seq(0,100,20))+ theme_linedraw() 
Actually I think it would be easiest in base r. Do you have the data?
Are you still having trouble installing the latest version of olsrr?
Your question applies to any discipline, not just R programming. With any technical speciality, there is a certain minimum knowledge level you need in order to be able to ask a question succinctly and to get a helpful response. For R specifically, you're going to need to understand the different data types. The elementary types that are common to all programming languages, such as *character* (usually called *string* in most other programming languages), and *numeric*, which is a superset that includes types such as *integer*, *float*, and *logical*, are fairly straightforward to understand. However, to be successful with R, you really need to understand the compound data types such as *vector*, *matrix*, *data.frame*, and *list*. The other concept that is critical for R success and gives beginners a lot of trouble is the *factor* data type. Take an online introductory course or pick up an introductory book that explains these concepts. Once you have a basic understanding of them, your frustration level with R will decrease substantially and you'll be much more productive.
Yep. The odd thing is that I learned a decent amount of tools through a stats course I took at my university where they gave us most of the code for each lab we were working on. So I'm in a weird spot where I've done linear regression, ANOVA, and made tons of histograms, qqplots, etc. without actually understanding what I was doing. I'm going through the swirl tutorial lessons to pick up on all the little pieces I missed out on in between those lessons and it's helping a lot with filling out my understanding of why things work like they do.
Read the [Data Structures](http://adv-r.had.co.nz/Data-structures.html) and [Vocabulary](http://adv-r.had.co.nz/Vocabulary.html) sections from Hadley Wickham's Advanced R to get the foundation. 
https://imgur.com/a/Tg36E So this is just bit of what I have... So how should i write the code so that I can regress for year=2000 and state=AB??? I can see that my regression would look like model1&lt;-lm(CR~MW+Pov+...,data=data) but dont really know how to only do it for specific time with specific state
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/Ii02K51.png** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) ^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20dwz75m1) 
Thanks. That looks like an immensely helpful site - Bookmarked! Like I said above, it's amazing what I was able to do being spoonfed without knowing what I was doing. Thanks for sharing that resource.
You might also find Garrett Grolemund and Hadley Wickham's [R for Data Science](http://r4ds.had.co.nz/) useful too.
Should you not be doing latent growth curve modelling? Or if you really want to do regression use SEM. Because otherwise you'd have too many dependent variables. One for each year. 
...insanely impressed that you were able to re-create that completely. i probably should've mentioned earlier but i want to separate out by patient (i.e. pt 1,2,3 etc.). I figured out how to do it using dumbbell but am getting an aes error here: # make up some data health &lt;- read.csv("health_PN.csv") # plot it myDat %&gt;% ggplot(aes(x=health$Patient,group=health$Patient))+ geom_linerange(aes(ymin=health$pre_PN,ymax=health$intra_PN))+ geom_point(aes(y=health$pre_PN), color='gold')+ geom_point(aes(y=health$intra_PN), fill='royalblue4', shape = 25)+ geom_smooth(aes(y=health$intra_PN), color = 'royalblue4', linetype='dashed', method='loess',se=F)+ theme_linedraw()
Your blog is seriously useful. Thank you.
You probably shouldn’t use lm() and summary() if you don’t know what p values are. All of your variables probably could use some kind of scale since it’s increasing things by .006 and .005. It means significance at various levels.
If you read the table with stringsAsFactors you already have a numeric value for the columns with strings in it. given data is your data, the code to select a specific state should be something like `data = data[which(data$state == 'yourState'), ]` It is also possible to just interpret the state as another independent variable, but this could lead to an overall worse model.
Yeah I am
It really doesn't matter but it's to differentiate when you use = to associate functions with values. 
Thank you - that link is genuinely helpful :-)
Ta!
I never use &lt;-. I think it's awkward for a number of reasons.
Because most people do not have an APL keyboard with a single assignment operations key &lt;-. But historically, the use of = as the assignment operator tends to be the source of confusion since statements like x=x+1, while perfectly acceptable in many languages, is algebracially nonsensical and is cognitively dissonant with the way of implementation at OP code level. Where &lt;- is closer to the p-code usage of what is conceptually is going on. But it has been used so long and we still don't use APL keyboards so it really doesn't matter except to very new programmers.
I used to be a ‘&lt;-‘ user all the way until I learned that it didn’t matter if I used the equal sign. For me, I switch between R and Ruby a lot and it’s just a personal preference now. If you like the assignment operator, there is a keyboard shortcut for it: option (alt) + ‘-‘
That was me for the first few years. Now I only use arrow assignment. For one thing it's the only thing that works within system.time. And if something takes long enough for me to use system.time, you can bet I want to save the result. But also, after a while it's just more clear. Now r studio makes it easy to make the arrow with one keyboard combination. 
I just use ESS and have Ctrl+; mapped to &lt;-
&gt; statements like x=x+1, while perfectly acceptable in many &gt;languages, is algebracially nonsensical and is cognitively dissonant &gt;with the way of implementation at OP code level This is why I use &lt;- ; I know = isn't the same as == but it makes it easier to keep mathematical formulas and logical conditions separate from assigning variables. Also its how the people I learned from did it. 
&gt; one thing it's the only thing that works within system.time Often repeated but still wrong. Just use extra parentheses: system.time((this = works()))
“&lt;-“ is referred to as the “gets” operator. So the following x &lt;- c(2, 3, 5) means: x gets the vector c(2, 3, 5). R is a variant of the S language. The S language uses “&lt;-“. So, the gets operator in R is just syntax that was inherited from the S language. Using “&lt;-“ or “=“ is just a preference. I personally don’t like “&lt;-“ since I started programming in C++. I’m used to “=“. 
Hi there. So the above plot represents winners, by region, of Nobel prizes by year. Each dot represents an instance where at least one Nobel was won by that region that year. (Ra-ra Europe, right?) Anyway, I would like to have the y axis tick labels be presented in descending order in relation to the amount of dots along their respective axes, instead of just in alphabetical order. plot_region &lt;- ggplot(df, aes(x=Year, y=Region, group = Region, colour = Region)) + geom_point(size=1) + coord_fixed(ratio = 3) + theme(axis.text.x = element_text(color="#999999", size=5, angle=90), axis.text.y = element_text(color="#999999", size=7, angle=0)) plot_region I spent about 3 hours last night trying different things, trying to get it to work. Notably, I have tried scale_y_discrete() with no success. Any suggestions would be greatly appreciated. If that issue can be resolved, I am also intrigued to know if the legend on the right can be explicitly rearranged. Thank you very much.
Did you try ordering your df inside ggplot call by y?
I am not sure because I don’t quite understand what you mean. Perhaps. Can you explain a bit more? Thank you. 
Either x or y has a single value after the subset. You can just run the subset function and check how many values are populated. 
Since you're already giving the data to ggplot in the first argument, in the aesthetics call you can just specify the variable names like so: `ggplot(subset(iris, Species == "setosa"), aes(Sepal.Width, Sepal.Length))` When you specify the full data frame (150 rows) before the variable name via `$`, ggplot is expecting something the same length as the data given (the subset with length = 50) but instead `aes` has something with length = 150. 
Ta!
You may use fct_relevel() to change a factor order, see ?fct_relevel for more details
&gt; the use of = as the assignment operator tends to be the source of confusion since statements like x=x+1, while perfectly acceptable in many languages, is algebracially nonsensical This never struck me as a good argument: code *isn’t (conventional) algebra*, and intentionally uses different symbols. There’s no actual risk of confusion. (There *is* a risk of confusing `=` and `==` in `if` conditionals, but since `=` is legal R, this situation isn’t helped by using `&lt;-` elsewhere).
Is it a shortcut if you use the same amount of keys? Haha. 
Custom operator function to assign whatever to the global environment. Probably best to not use in production.
Holy fuck. You're awesome.
Hi. Thanks. I found an answer that works (see my edit above), but I will look into this function for the future. 
When I need to manually rearrange a categorical axis for a graph I do it by ordering the levels in the dataframe first: Mydata$Variable &lt;- ordered(Mydata$Variable, levels = c("First", "Second", "Third")) This would work for you if you know what order they need to be in. This may be lazy, or inefficient, or not proper in some way, but it works very well for me! Just remember that if you then want to analyse this data your ordering will be whatever you told it to be.
I see "levels" bit — Does this only work with factors? (if so, I will try with as.factor()) Thank you.
I haven't tried it with anything other than a factor so I can't answer that, but I always work with catagorical variables as factors. 
forcats::fct_relevel() or forcats::fct_reorder()
I'm not sure your model really makes sense, are you sure you want to include an interaction when you haven't included those variables in the model? What about setting the intercept to be subject specific, random = ~1| subject?
Your presentation is a little confusion. Please clarify: Is `light` constant or does it vary across `subject`. Right now you're fitting light (and an intercept) effect per subject. This isn't typically what people want. Try using: random = ~ time|subject 
I just started learning R, but I can't get used to write &lt;-... I guess I'll stick to =...
Thank you for your reply. Light varies across subjects. But not every participant has every light condition (of which there are 5). So it is an unbalanced design, for which I read you should use a mixed model. I will try the "random = ~ time|subject" line you suggested.
Thank you for your reply. Doesn't "light * time" include the main effect of both variables? What do you mean by variables not included in the model? I tried "random = ~1| subject" at first, but is that sufficient? I want the model to reflect the fact that half of the subjects have data for light conditions 1 and 2, but the other half have data for light conditions 3 to 5.
I believe you probably already know that this kind of unbalanced design will not yield very meaningful results unless you have a very large number of subjects (e.g. at least hundreds).
read.table(data, sep = '\t', header = T). You probably are reading in the header as part of the data set.
Hi, I'm a bot that links Gyazo images directly to save bandwidth. Direct link: https://i.gyazo.com/8e402f956262dc7aeb5a496d8f37cc3c.png Imgur mirror: https://i.imgur.com/9L3phj6.png ^^[Sourcev2](https://github.com/Ptomerty/GyazoBot) ^^| ^^[Why?](https://github.com/Ptomerty/GyazoBot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/u/derpherp128) ^^| ^^[leavemealone](https://np.reddit.com/message/compose/?to=Gyazo_Bot&amp;subject=ignoreme&amp;message=ignoreme)
Could you in lude your code? There are many possible issues. - are you inputting a dataframe, or the column of the dataframe? - did your data parse correctly? There might be a letter or punctuation mark in the column you're using, causing R to interpret it as words rather than numbers - does your data look right? Maybe there was an error during import and you've got a garbled mess instead of numbers.
Hi, I'm a bot that links Gyazo images directly to save bandwidth. Direct link: https://i.gyazo.com/8e402f956262dc7aeb5a496d8f37cc3c.png Imgur mirror: https://i.imgur.com/EVd39Bl.png ^^[Sourcev2](https://github.com/Ptomerty/GyazoBot) ^^| ^^[Why?](https://github.com/Ptomerty/GyazoBot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/u/derpherp128) ^^| ^^[leavemealone](https://np.reddit.com/message/compose/?to=Gyazo_Bot&amp;subject=ignoreme&amp;message=ignoreme)
I would definitely pick up a book (or some other reading) on longitudinal data analysis. If there are five (discrete) light groups, it will be pretty difficult to model the heterogeneity between patients in their response, unless you have a lot of data -- even more so if not ever patient experiences all 5 levels. I would start with random = ~ 1 |subject and random = ~ time|subject. These are fairly standard models and people who know this area will understand what you're doing. I'm not entirely clear on your comment about anova in the OP. If the light|subject model is as I think it is (5 levels), the weird result might just be a bad model fit (5 correlated random effects) + fixed effects ~= 20 parameters. That said, I'm not sure I understood what you did.
hist(m1$L1) Will create a histogram of the first column. You can repeat this for all of the columns. You can use par(mfrow=c(2,3)) to make it so there are 2 rows and 3 columns of plots so you can see all of them at once (use par(mfrow=c(1,1)) to put it back to normal). hist(as.vector(m1)) Will create a histogram of the entire dataset (disregarding row names and column names, this will just lump all of your data together). I'm not sure which one you actually want. You can't do a single histogram for an entire data set, you can only do it for numeric vectors (columns in data frames are vectors). For your errors, note that "numeric" is a type of vector in R. Character vectors also exist, which is why R specifies numeric and not just vector. Once you loaded your data in (m1 &lt;- read.tsv...), you don't need to load it in again. m1 is your data. It's there. Adding the code from read.tsv won't help anymore.
Thanks a lot for this, having a function generating a histogram of columns of the data set is what I what, is there anyway that I can make the function so that when I input the cell line name (colname) it will produce a histogram with the corresponding data. for example: myfunction(L1) will produce a histogram like the code hist(m1$L1)?
You can. It would take a very small amount of work. However, any effort you put into it would be wasted. just typing the histogram code is pretty simple. hist(m1$L1) hist(m1$V1) Is already basically no code. I have no idea why you would want to make it any harder than that. Writing your own functions is a little bit more advanced, and you really have to understand function arguments, various ways of subsetting dataframes, and generalizability. You have to walk before you can run.
Thanks.
OP is correct "light*time" is equivalent to "light + time + light:time"
Don't run multiple tests, more fundamentally though there is very often little point in testing for deviations from normality. See the response at here (as well as the rest of the thread).... [Is normality testing 'essentially useless'?](https://stats.stackexchange.com/a/2501) (Harvey Motulsky who's response I've highlighted is author of the book *Intuitive Biostatistics*). Rather plot your data by each pH group and give it a quick eye-ball for being roughly normally distributed. If you expand on your hypothesis (e.g. the mean number of species is influenced by the rough estimate of pH) then you might receive more advice on how to proceed.
In order for a mixed model to be useful, you need at least two repeated measures of a parameter to be escalated per subject (more is better). As the variance in the parameter to be estimated increases you'll need ever more subjects. The challenge becomes even greater when your model is non linear, such as in the case of pharmacokinetics. [This article](https://onlinelibrary.wiley.com/doi/full/10.1111/jvp.12473) has an excellent overview of the topic.
Yes, I have 8 measurement points for each subject for each light condition they completed (some 2 of 5, others the remaining 3 of 5). So I think a mixed model would apply in theory, but as you said I probably have too many parameters in my model considering that I have less than 50 subjects in total. Also, thank you for the article. It was very hepful in understanding the purpose and limitations of mixed models.
&gt; header = T Please don’t use `T` and `F`, since they are normal variables that can be overwritten by the user. Invest in three extra characters and use `TRUE` and `FALSE`.
I'm currently taking the Data Science Specialization on Coursera and they're teaching R for all of it so far. I'm sure that'd be something to look into. 
There isn't an official R certification. Just work on R projects and put them on GitHub. Employers want to see your work, not your certificates.
Okay sounds good I can appreciate that.
FYI - you can audit the Coursera Data Science specialization, but you won’t get access to the assignments for practice. 
DataCamp is good and cheap and educational
New here, MOOCs?
To gather your data in to the expected tidy format you can use tidyr's gather function: library(tidyverse) df &lt;- read_csv(file = "~/Downloads/data.csv") %&gt;% gather(frame, value, 2:4) Continue with the example with this tidy data and you should be fine
What is the feeling towards edx.org
Thanks for the response. I don't know why this is so difficult for me, but I'm still somewhat lost. That command neatens up the data, but how and where do I use "df" within the rest of the example? How do I remove tweenr frame interpolation from the program w/o screwing up everything? I really appreciate the help, for some reason I cannot seem to understand R data structures or breakdown very well, it's so different than other programming languages.
We're you able to solve the issue ?
[MOOC](http://lmgtfy.com/?s=d&amp;q=mooc)
If it only works with factors that's fine because you can order a data frame's columns any time you want df &lt;- df[, c(1,3,4,2,5)] as an example
Aren't set elements all unique because..it's a set? So no element would occur the most number of times, as each possible element occurs 0 or 1 time. Maybe you want the most common element in a list, in which case table is useful.
Mode &lt;- function(x) { ux &lt;- unique(x) ux[which.max(tabulate(match(x, ux)))] }
https://dplyr.tidyverse.org/reference/tally.html
Sure! That's pretty simple, actually. Just replace `split(.$frame)` with `split(.$group)` and modify the `ggplot()` aesthetics. Full code: library(tidyverse) library(tweenr) library(gganimate) df &lt;- read_csv(file = "~/Downloads/data.csv") %&gt;% gather(frame, values, 2:4) %&gt;% transmute(group = Year, values, frame = factor(.$frame, levels = unique(.$frame))) %&gt;% split(.$group) tf &lt;- tween_states(df, tweenlength = 0.02, statelength = 0.001, ease = c('cubic-in-out'), nframes = 30) p &lt;- ggplot(tf, aes(x=frame, y=values, fill=frame, frame= .frame)) + geom_bar(stat='identity', position = "identity") gganimate(p, interval = .1, title_frame = F, filename="#288_barplot_animation.gif", ani.width=480, ani.height=480) Best of luck! Edit: full disclosure: it's midnight and i just got back from the pub, so i'll have to check i got this right tomorrow. 
Thanks again! One last thing, and no worries if you don't answer this till later, is it possible to make the ggtitle() coorespond to the year in the data. Something like ggtitle(tf[,1]) but have it change for each year.
Yes. For example, if it’s a line plot but you have missing data on holidays, the line will disappear on those days. The axis won’t be affected. 
maybe save the results of summary() as a variable and then pass that variable to round() ? Alternatively, try placing round() inside of summary() ? This idea sounds worse, but it is low-cost. 
Sounds like your life would be made much better with tidytext.
A little reading might go a long way: https://cran.r-project.org/web/packages/textstem/textstem.pdf https://cran.r-project.org/web/packages/corpus/vignettes/stemmer.html
Hey thanks a lot for the suggestion. Here is what a simple version of what I am trying (just checking for plurals): for (i in 1:NROW(words)) { grep(paste(words[i], "s", sep=""), words, value=TRUE) print(i) ##to monitor progress } Is this what you had in mind? The whole job would still take a day or so, but it is certainly going faster.
So the performance hit is only a factor if you’re modifying one of Rs collection data types inside a for loop. But if you were doing something outside of computation, like iteratively saving files to a directory, loops would be fine. 
nice!
Ahhh this is an interesting idea. So I could just write the results to a txt/csv file then?
not working but thanks
Take a look at the [`tabulate` function](https://www.rdocumentation.org/packages/base/versions/3.4.3/topics/tabulate): which.max(tabulate(data)) will give you the most occurring number. For this, `data` needs to be numeric. Alternative you can also run `table` and extract the result: names(which.max(table(data))) This is a bit more effort but also works with non-numeric data.
Use a stemming library. However, to immediately improve your code, precompute the derivative words, rather than re-computing them in every loop. You don’t even need a loop for this since `paste` is vectorised: derived_s = paste0(words, 's') Or, for all derivations at once: derived = unlist(lapply(words, paste0, c('s', 'ed', 'd', 'ly', 'ing'))) And now you can simply use set operations to remove these derived words from your word list. Again, no explicit loop, and no need to manually create a `to_delete` vector: stems = setdiff(words, derived)
These are compound expressions. Take them apart to see what’s going on. First off, let’s replace `c(a, b)` with a variable, `x`: x = c(a, b) Now. The first expression is x[-2] [-3] # = (x[-2])[-3] That is, we first subset the vector `x` with `-2`, which returns everything but its second element. *The result of that* is subset with `-3`, returning everything but the third element (*note*: this is the third element of the *modified* vector, not the third element of `x`!). The second expression is: x[2] [3] # = (x[2])[3] So we first subset `x` with `2`. The result of that is a vector with a single element, which corresponds to `x`’s second element. You then attempt to subset *that result* with `3`. And taking the third element from a vector containing a single item doesn’t make sense. So it fails.
You == chief. This is extremely helpful. Will try this afternoon and update my post with an edit. Thanks. 
I've been taking the course on DataCamp right now and am almost done with the free course they offer
So, I just looked at it and I'm super stoked about all of the courses they offer
Ok, good. I didn't want the plot to 'compress' as if those dates didn't exist.... they exist, there's just no data, and its important to show the dearth as it is the collected data.
That seems about right but the vectorized solution by guepier seems better so go with that. 
It is not correct. c(a,b)[-2][-3] will concatenate vectors a and b and remove the 2nd the 4th element of the original vector a. for example a = c(1,2,3,4) b = c(5,6,7,8) c(a,b)[-2][-3] &gt;[1] 1 3 5 6 7 8 This is not really a property of vectors, as such, but rather a consequence of function calls that R performs to do subsetting. subsetting operation x[-2] is actually a function call '['(x,-2). Now, with understanding lets see what you do with this expression - c(a,b)[-2][-3] Again, since any action in R is a function call lets rewrite this expression as such '['('['(c(a,b),-2),-3) First, you concatenate two vectors (using an example above) c(a,b) &gt;[1] 1 2 3 4 5 6 7 8 Now, you use this longer vector to remove its second element - [-2] as in '['(c(a,b),-2) &gt;[1] 1 3 4 5 6 7 8 Now you use this new shorter vector to remove its third element - [-3]. Notice, now that its third element now is not 3, as you would expect, but 4, because you have removed 2 already in your [-2] call. 
Yes. That is what I did. I trimmed the 450k word list down to 38k and now can iterate in peace with my for loops (and grepl-ings like a noob. Thanks. 
Can you apply a conditional with lapply? Thank you 🙏 
I do love a bit of pedantry, and upvoted your answer accordingly. But you and I both know that this is unlikely to be what OP is after (your answer even acknowledges this).
I was just looking through documentation, and realized that while I can do set operations on vectors in R, I can't actually guarantee that a vector is a set - IE: i can't construct something to which I can add objects, but will guarantee uniqueness. It looks like you can get a "sets" package on CRAN, but I'm amazed sometimes at how R doesn't have certain things I take for granted in many other languages as part of their core system. 
Can we get a quick tldr intro to docker containers, since it's new to r. Thanks
Yes! Here's an example... though this might not be the efficient way to do this particular thing. coins = sample(0:1, size = 10, replace = TRUE) coins ## [1] 0 1 0 1 0 0 0 0 0 1 sapply(coins, function(x) {if(x == 0) {"Head"} else {"Tail"}}) ## [1] "Head" "Tail" "Head" "Tail" "Head" "Head" "Head" "Head" "Head" "Tail" lapply(coins, function(x) {if(x == 0) {"Head"} else {"Tail"}}) ## [[1]] ## [1] "Head" ## ## [[2]] ## [1] "Tail" ## ## [[3]] ## [1] "Head" ## ## [[4]] ## [1] "Tail" ## ## [[5]] ## [1] "Head" ## ## [[6]] ## [1] "Head" ## ## [[7]] ## [1] "Head" ## ## [[8]] ## [1] "Head" ## ## [[9]] ## [1] "Head" ## ## [[10]] ## [1] "Tail"
Ah I see, Thank you so much. Also are these kind of questions frowned upon? I don’t mind the downvotes, but I just want to make sure that I’m following unspoken rules.
Excellent!!
I recommend that you start with this [comparison of Docker vs Kubernetes](https://www.devteam.space/blog/kubernetes-vs-docker-comparison-of-containerization-platforms/). The key concept up understand is *containerization*. Although Docker was revolutionary, their crazy high monetary valuation caused them to make some really dumb moves to try and start generating revenue. I don't believe they'll ever recover. On the other hand, Kubernetes sprang out of Google, so the pressure to generate revenue from the product wasn't there. Rather, as Kubernetes grows, it will become more valuable to the cloud service providers.
Yeah if you need a collection of words that is one idea. The issue has to do with Rs data types being immutable, which means that any time a variable or collection member is modified, the whole object is really being copied and reproduced internally. My understanding is this is a common feature of functional languages like R. With this knowledge, the idea of writing more idiomatic R becomes more important. So you’d probably want to take a functional programming style approach to this problem. I would suggest boning up on the libraries others suggested, and seeing how it performs. Send me a PM if you want some help writing/refactoring this code. 
Thanks a lot, will do. I am diving deep into library(tidytext) et al. for the time being. Be well. 
The arrow is always assignment while the equals can be used for named parameters. The code above evaluates as if you did this: red &lt;- 'poe' vectorMap &lt;- c(purple="Tinky Winky",red, yellow="laa laa") 
indeed, it works just the same. red even exists as an object outside of the vector. AND you can still assign a name to it as a parameter: &gt;vectorMap &lt;- c(purple="Tinky Winky",pink=red &lt;- 'poe', yellow="laa laa") 
The rationale is that everything is a function in R and gets evaluated as such. '&lt;-' is a function used for assignment.
Actually one last thing, if you'd be willing to help. Would you mind explaining what your first command does in a bit more detail. Particularly transmute, the %&gt;% operator (I think that does nesting right?), and the "." prefix in certain parts. I'd just like to understand how the data is being manipulated into the right format. Thanks!
This doesn’t explain it since `=` is *also* a function, same as `&lt;-`.
There are many ways, but I’d use dplyr’s mutate with case_when function. On mobile now, so unfortunately can elaborate more only later.
Ifelse()?
Iris[iris$species=='whichever',sepal.width] + Iris[iris$species=='whichever',petal.width] .. This should give the idea about selecting particular data from a data frame and anything on top of this, I'm pretty sure you can figure it out. Happy coding! 
Iris[iris$species=='whichever',sepal.width] + Iris[iris$species=='whichever',petal.width] .. This should give the idea about selecting particular data from a data frame and anything on top of this, I'm pretty sure you can figure it out. Happy coding! 
this is super super helpful. I really appreciate you posting this link. Hugs. 
How are you liking it? If you can afford the monthly sub, you can probably get a decent certification in 3 to 4 months (analyst/ data scientist) if you really hunker down. If you can afford the year sub there’s a lot of good coursework and you can take your time.
Wow - such a clear summary!
This needs to be an Rmd file or a Jupyter notebook! 
Since there's so much to do in so many different ways I don't know you can have a general kind of language competency test. But the easiest way would be to ask "what are five very common, basic tasks in my field and one in depth problem?" And trying those out in the most optimal, annotated, and readable way. If you manage that from scratch I'd say you're in a good place.
This is only true for languages without proper certifications, like Python
Hmm, ok I may be speaking out of my limited experience then. I never had an employer ask about my certificates, they ask about my projects. I have a couple from Coursera and one from Udacity. The Coursera certificates are extremely easy to obtain. You just need to get ~50% on the quizzes and so many people cheat on the projects since everyone publically posts their code on github. When I did the peer grading on Coursera, half of the people copied their solution from github. It baffled me why people would cheat for a certificate. 
Hey, dopadelic, just a quick heads-up: **publically** is actually spelled **publicly**. You can remember it by **ends with –cly**. Have a nice day! ^^^^The ^^^^parent ^^^^commenter ^^^^can ^^^^reply ^^^^with ^^^^'delete' ^^^^to ^^^^delete ^^^^this ^^^^comment.
I think this a pretty good interview question to know how much someone knows about R (or any language for that matter): What do you hate the most about R? What's your answer?
Coursera is not a skill or expertise certification, it's a website that sells courses and gives you a certification that you completed a course, nothing more. No company will ever tell you "hey you're a Java developer with SCJP certification, we need to pay you more", you need to tell them how qualified you are and why they need to pay you more because of that. Udacity is barely worth something, most companies don't believe in that nanodegree. There's no way to compare a Microsoft, Sun, Oracle, SAP... Certification with a course sales website. The exam is paid and you need to prove your skills without cheating, it's nothing like Coursera.
the fact that position 1 in an array is index 1 instead of index 0
I agree that the network/database admin certificates are valued in the industry. But you didn't list any for programming languages. Is there even one for R? 
A lot of things are pretty great about R, the main "problem" with using the language is the lack of others using the language at all or even using it well. I work with analysts that write godawful R code and (what I hate) argue with me about having to learn anything new or improve.
duck, i did it myself. Thank for all supporters out there! :D
I'm not that familiar with R but I listed Python as one language that doesn't have an official certification, so any other course certification is worthless as well.
 vec &lt;- sample(1:100, 10000, replace = TRUE) seq &lt;- c(1, 2, 3) lseq &lt;- length(seq) lv &lt;- length(vec) matches &lt;- sapply(1:(lv-lseq+1), function(i) all(vec[i:(i+lseq-1)] == seq)) sum(matches) 
[removed]
care to explain what you do in line 5 and why? (matches &lt;- ...) and also pm me your paypal adress, as it seems to work.
Your pattern expression is set to identify a single digit (\\d) immediately followed by the end of the string ($). Try adding a + (one or more) after the \\d.
There are three mistakes. First off, because your whole regex is inside braces, it's interpreted as a single (negated, because it starts with `^`) *character class*. Remove the outer braces to fix this. Secondly, you currently only a a single letter, followed by a single digit. Add quantifiers. Thirdly, you're asked to return indices, not string values. Don't pass `value = TRUE`. To `grep`.
Clear! Thank you.
sapply applies the function in the second argument (here a lambda function, so everything after the space of function (i) ) to a vector doing it the number of times that a string of that length appears in the vector (lv-lseq+1). So since the function is to check that the string is identical (by going through the vector one at a time and checking the next two [i:(i+lseq-1)]), it'll have exactly the same amount of "TRUE"s as there are matches. So if you sum a vector of booleans with sum(matches) it'll give you the number of trues.
You can either change the size of the plot itself (like if you're saving it to an image file, save it wider) or you could try adding "\n" after the y-axis text to force a line-break to make space. Example: ggplot(...) + labs(y = "ACME\n")
You're looking for the "font-family" https://www.w3schools.com/cssref/pr_font_font-family.asp
Ah, ok, I've managed to get a new font in place. Excellent. However, I'm struggling with getting my text in the right position. How do I adjust the vertical position of a &lt;h1&gt; or a &lt;p&gt;?
hey, thanks for the good explanation. is there an even simpler solution? For example the german lottery system can be easily compared to a unique drawing (=seq) this way: #pick 6 numbers from 1 to 49, 100k times lottery = replicate(100000, sample(1:49, 6)) #draw a sample to use as the "winning" ticket/set of numbers draw = sample(1:49, 6) apply(lottery, 2, function(i) sum(i %in% draw)) but due to the difference in sets (one is paired in groups of 6, the other is paired in either singles, if done with replicate, or just one big bulk of data, if done as presented in Sway's reply) I can't seem to figure out to do it any simpler than Sway.
Take a look with `str(FI)`. It will output the entire structure. If it outputs too much, try `str(FI, max.levels=1)`.
Using `str(FI)` I found [this](https://www.reddit.com/r/rstats/comments/8cp8f5/r_retrieving_information_from_a_s4object/dxgrp5s/?context=1): &gt;Using this I found FI@quality is a dataframe containing the columns support and count and FI@items is an "itemMatrix in sparse format". It seems the rows in both are linked by an index. If I could combine these two objects into one dataframe I would have the result I am looking for. Unfortunately, I don't know how. Are you aware of a method?
You need to use double slash // on windows machine
Thank you! I changed it to the following but get an error...but I think it has to do with the audio file instead. Any thoughts on that by chance?: &gt; gl_speech("C://Users//smars/Documents/eric.wav",sampleRateHertz = 44100)$transcript Request failed [400]. Retrying in 1.6 seconds... Request failed [400]. Retrying in 1 seconds... 2018-04-17 13:53:47&gt; Request Status Code: 400 Scopes: https://www.googleapis.com/auth/cloud-platform Method: service_json Error: API returned: Must use single channel (mono) audio, but WAV header indicates 2 channels. &gt; Here's what the wave object looks like: &gt; readWave("C:/Users/smarshall/Documents/eric.wav") Wave Object Number of Samples: 411248 Duration (seconds): 9.33 Samplingrate (Hertz): 44100 Channels (Mono/Stereo): Stereo PCM (integer format): TRUE Bit (8/16/24/32/64): 16 
I have no experience in this but since the error message states that you need a single channel audio file, you’d need to extract the same from your current wave file. A quick google search shows that there’s a package called tuneR which claims to be able to do this https://www.rdocumentation.org/packages/tuneR/versions/1.3.2/topics/Mono-Stereo
First, as /u/engti said, for directory path names, "[use double slash `//` on windows machine](https://www.reddit.com/r/Rlanguage/comments/8cxy7y/googlelanguager/dxionpe/)". If you use just a single `\` character, the interpreter is trying to use an *escape sequence*. By default, Windows will return file paths with the `\`, so I always convert them to `/`, which you can do with a call to `gsub()`, as [discussed in this Stack Overflow post](https://stackoverflow.com/questions/17605563/efficiently-convert-backslash-to-forward-slash-in-r). Linux/Unix use the `/` for file-paths. Although Windows doesn't use this convention by default, Windows does recognize it, and it's a better approach because it will make your code more portable. Second, I recommend that you not use the following construct. gl_speech("C:/Users/smars/Documents/BG.wav")$transcript Instead, consider the following approach: #Take note of how I swapped the *backward slashes* for *forward slashes* FILENAME &lt;- "C:\Users\smars\Documents\BG.wav" SpeechObject &lt;- gl_speech(audio_source = FILENAME) MyTranscript &lt;- SpeechObject$transcript My proposed approach has a few more lines of code, but from a software engineering perspective, it's easier to understand the logic of your code, and it's also easier to debug. 
Look into [shiny](https://shiny.rstudio.com/tutorial/)
https://shiny.rstudii.cim/tutorial It's also relatively easy to learn
If you already know Python/Django, you could have Python call a shell script to run some R code.
I'm happy to see a recommendation for Shiny as the first comment. All other factors being equal, I think Shiny is the best way to go. However, if the users need Tableau like functionality and there will be nobody to maintain an instance of Shiny after the OP leaves, [Plotly](https://plot.ly/) is another option to consider.
Thanks for the suggestion! I'd looked into that briefly but I wasn't sure how much that service was oriented towards displaying data that imported on the backend versus just manipulating data a user uploads on the front end, which I need the latter. Is that still possible with Shiny? 
Yes r shiny makes it easy to have a user upload a csv or xlsx file into the GUI, and then run your script, and then be able to download some output file if you want put that into Tableau. However, if you learn shiny you're not far away from learning parameterized reports and skipping tableau altogether.
I would NOT use Shiny or any other web tool. The more code you add the more complexity there will be to debug things in the future when something inevitably breaks. No web server will stay up forever and eventually someone will be scratching their head wondering where to start digging through all this code to figure out how to fix things. If I were you I'd ask my colleagues if they can handle running a simple python script. Have the script fetch the data and run the subsequent R program on that data to produce the output file. If you keep it simple someone may even decide to look at the code and take it upon themselves to automate it!
Thanks for the input, I will look more into that. I'd love to use Shiny as a data visualization tool but I'm not sure if my team will be interested in learning how to manage that. Right now they can manage Tableau which is why we're using it. 
Honestly I think it should be fairly easy to [input a csv](https://shiny.rstudio.com/gallery/file-upload.html) and [output another csv](https://shiny.rstudio.com/gallery/download-file.html) with regular changes via a shiny app. Keep us posted on how it’s going. 
Simple etl? Can you use tableau maestro?
I wrote a shiny app that used data stored on the server. It stored essentially what was a massive book and then the app was based off the contents of the book. It worked off user input and data stored in the back...so I'm pretty sure it could do what you wanted here. The only issue with shiny is having to pay if you have a lot of users...but that's not really an issue I've had to look into. I've never made an app that I told more than a handful of people to check out.
I’d recommend going to google cloud first and setting up a new project. Then call this API using the API explorer for easy debugging. Also, there’s another API called “long...” which will accept &gt;1 min samples. Finally, I think google recommends you use FLAC - it’s more efficient - but WAV is also ok. 
Look into the package adehabitatLT, it's a package used to analyse spatial-temporal animal movement data. 
Since nobody mentioned it, you might want to check opencpu (https://www.opencpu.org/). It s robust, independant from rstudio and possibly not as greedy than shiny
Hey, GeylangFTW, just a quick heads-up: **independant** is actually spelled **independent**. You can remember it by **ends with -ent**. Have a nice day! ^^^^The ^^^^parent ^^^^commenter ^^^^can ^^^^reply ^^^^with ^^^^'delete' ^^^^to ^^^^delete ^^^^this ^^^^comment.
It’s hard to say what’s going wrong with your first piece of code: since the result is empty, presumably the function couldn’t recognise any speech in your audio. Ensure that the audio file is on mono format (stereo isn’t supported). As for your piece of code, I’m not sure what you were attempting here. You’re using [`system.file`](https://www.rdocumentation.org/packages/devtools/versions/1.13.3/topics/system.file) wrongly: the function isn’t supposed to be used with an absolute path (what would it do in that case, anyway?). Always inspect your variables! Here’s what your `test_audio` looks like: &gt; test_audio &lt;- system.file("C:/Users/smars/Documents/eric.wav",lib.loc ="C:/Users/smars/Documents/") &gt; test_audio [1] "" In fact, the error message you’re getting is telling you exactly the same thing: &gt; Error: Path '' does not exist If you’ve got the absolute path to the file, don’t use `system.file`.
&gt; Stereo PCM There you have it: [the `gl_speech` documentation states](https://www.rdocumentation.org/packages/googleLanguageR/versions/0.1.1/topics/gl_speech): &gt; All encodings support only 1 channel (mono) audio.
What's the business model behind this? What about the license?
You will have more success when researching this if you call these model frailty models. They are equivalent. 
Not really. Sometimes they use a different distribution for the frailty terms. They aren't used as much in time to event analysis when compared to mixed models in other areas. survival::coxph does them too, but I think they are gamma. Marginal models have tended to be more popular in this area. Therneau's book is getting a little dated but it's still a good reference to these types of extensions to the cox model, particularly when using s-plus/r. 
Thanks for the explanation! It's a lot clearer for me now. I'm actually pretty new to stats, especially in the field of survival analysis. Just came across the online copy of Therneau's book too, will be doing quite a bit of reading up.
I'm also interested in the license. Is it FOSS? What about the toolkit? Electron crap or something normal like Qt5/gtk?