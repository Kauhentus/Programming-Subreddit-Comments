I've uploaded the files here: [https://drive.google.com/drive/folders/1cxZxaYD8FUmYJNZnWEdBiSuGG8r8hjQ9?usp=sharing](https://drive.google.com/drive/folders/1cxZxaYD8FUmYJNZnWEdBiSuGG8r8hjQ9?usp=sharing)
To elaborate on the problem: I have 8 independent variables and 3 dependent variables ("casual", "registered", and "count). The models created have each of the dependent variables included as independent variables for the others. For example, "casual" and "registered" are included as independent variables for "count". I'd really like to avoid creating three separate dataframes to create 3 different sets of models, but I'm starting to wonder if that's what I need to do.
turn it into a function
Maybe something in purrr [https://github.com/rstudio/cheatsheets/blob/master/purrr.pdf](https://github.com/rstudio/cheatsheets/blob/master/purrr.pdf) 
The real beauty of the apply family is their ability to handle user defined functions.
I see that if you run the following: functions&lt;- function(x) { c(Counts = length(x), Mean = mean(x), StandardDeviation = sd(x)) } output&lt;-tapply(iris$Sepal.Length, iris$Species, functions) You get output that appears as a list. However, how could you instead create something that looks like the following (or convert the list output: Species Count Means StandardDeviations setosa 15 5.006 0.3524897 versicolor 21 5.936 0.5161711 virginica 3 21 6.588 0.6358796 
Even better, function composition! 
I have not looked at your source code, but normally, you can use a `formula` to create the model, if you don't use your dependent variables as independent variables within the formula, this should be no problem. independent &lt;- names(data_frame)[!names %in% dependent] formulas &lt;- lapply(dependant, function(x) paste(x, "~", paste(independant, collapse=" + "))) This should yield the independent variable names, if the dependent are set, and the according formulas for every dependant variable as list.
Hey, panda\_yo, just a quick heads-up: **independant** is actually spelled **independent**. You can remember it by **ends with -ent**. Have a nice day! ^^^^The ^^^^parent ^^^^commenter ^^^^can ^^^^reply ^^^^with ^^^^'delete' ^^^^to ^^^^delete ^^^^this ^^^^comment.
Hey /u/CommonMisspellingBot, just a quick heads up: Your spelling hints are really shitty because they're all essentially "remember the fucking spelling of the fucking word". And your fucking delete function doesn't work. You're useless. Have a nice day! [^Save ^your ^breath, ^I'm ^a ^bot.](https://www.reddit.com/user/BooCMB/comments/9vnzpd/faq/)
Join using the ID only, then filter using your date conditions. Unless your tables are massive, in which case you’re better off using data.table package 
&gt;Thanks, that could be an option and something I didn\`t think of. I will take a look at it. The problem is the data will get massive when I apply it to the whole sample.
What is the data type for firstSteeringTime? Integer, numeric, time, date/time?
Consider this: how do you want to represent the results? You've got `4n` results, where `n` is the number of species. Although not with `tapply`, the dplyr package allows you to express it as iris %&gt;% group_by(Species) %&gt;% add_count() %&gt;% group_by(n, add=TRUE) %&gt;% summarise_at(vars(Length), funs(mean, sd, p99=quantile(., probs=.99)) or something like that. I dunno. I'm on my mobile....
https://community.rstudio.com/t/tidy-way-to-range-join-tables-on-an-interval-of-dates/7881
How would the absolute value ever be less than a negative?
data type is numeric
That's true. I think I've fixed it. The code below seems to do the job: &amp;#x200B; `mutate(EarlyResponses = FirstSteeringTime &lt; mean(FirstSteeringTime) - sd(FirstSteeringTime))`
Pull the date vector from the target table. Sort it. Sort table 2 by date. Add column to table 2 for closest future date from the vector(use the sorting to your advantage, this shouldn't require many comparisons. Keep an index of the last date used and increment when the date is behind). Join tables using that date.
&gt;Thanks, I think this solved my issue: &gt; &gt; &gt; &gt;`df1$start_date &lt;- df1$date_f + 183 df1$end_date &lt;- df1$date_f + 540 library(fuzzyjoin) yy &lt;- fuzzy_left_join( df1, df2, by = c( "ID" = "ID", "start_date" = "date", "end_date" = "date" ), match_fun = list(\`==\`, \`&lt;\`, \`&gt;=\`) )` &amp;#x200B; I wrote a more detialed answer on [SO](https://stackoverflow.com/questions/55537032/joining-data-by-date-based-on-time-period-condition/55549479#55549479).
I often have to use range joins and unfortunately tidy solutions are way too slow for my applications. The one using fuzzyjoin can take several minutes if not hours. My go to solution is to use data.table. The relevant function `foverlaps` is here: https://rdrr.io/cran/data.table/man/foverlaps.html
I think you have some good answers, but let me just compliment you on an extremely well-asked question. Thanks for putting some effort and thought into it!
&gt;Thanks for the foverlaps suggestion! I will take a look at it. I did "solve" the issue using data.table in the [SO](https://stackoverflow.com/questions/55537032/joining-data-by-date-based-on-time-period-condition/55549479#55549479) post, which is lightening fast! but this data will increare in size so perhaps the foverlaps function is the best way. &amp;#x200B;
Thanks! I think I was more detailed in the Stack Overflow posts. The way I see it, is if I want help from people who have no obligation to help me, I should *at least* put this effort in to making myself/problem clear.
check your variables. specifically, type `AVGWEEKLY` at the R console, and then type `FEMALE` likewise. Why aren't they the same length? Did you fail to enter one of the numbers in one of those vectors?
PSA: This post leads to an affiliate marketing link. Here's affiliate-free link: https://www.humblebundle.com/books/coders-bookshelf-books * Strike 1: The Twitter account associated with the given link (https://twitter.com/Rcentralhub/status/1114529100113371136) has nothing to do with R and has been renamed 1Purple_hub. * Strike 2: The affiliate marketing query string is obfuscated by two url shorteners: Twitter's t.co as well as bit.ly. * Strike 3: Plugging the shortened-urls into a url expander (urlex.org), the full affiliate url is: https://www.humblebundle.com/books/coders-bookshelf-books?hmb_source=navbar&amp;hmb_medium=product_tile&amp;partner=purplehub&amp;hmb_campaign=tile_index_4 I see this pattern pretty often. Humble Bundle affiliate marketers make Reddit submissions that link to a tweet so that affiliate query strings in the url are obfuscated. Usually the associated Reddit account's post history has mostly single-sentence responses. And they're often not contributors to the subreddit or any subreddits with related content.
Is that Linux book any good?
FYI. The Art of R Programming is a terrific book for tips and tricks in base R. Although I prefer Tidyverse over base R for many things, I would still highly recommend the Art of R Programming book. 
Step one: write appropriate code to read in one file appropriately. You should be able to do this with some trial and error with the various read/scan functions. Step two: use purrr or something to read all the files in. Suppose for step one you have a function `read_parksinson()` that reads in an appropriate file and returns a data frame containing that record. Then you can do, presuming you're in that directory and they're all called something.txt: files &lt;- dir(pattern = "*.txt") df &lt;- files %&gt;% purrr::map_dfr(read_parkinson) 
This is pretty disappointing if you're only after R related material. The book available covers only base R. Not even ggplot2. Give this a miss unless you're also interested in the other books.
Yeah if you look at ?Syntax you'll see that special % operators are higher priority than negation
To expand on this a bit more... I made a couple of .txt files andput them in a folder "pd files". The `read_delim_helper()` should work so long as the files are set up exactly as described (i.e., with the ":" as the delimiter and with no headers) and if the variables are the same for each file. At the end of this the variables are all saved as character. Not sure if there is an easy way to do this other than to recode each value individually. If that's too annoying it might be a good idea anyway to save a copy of the the combined data frame and then use the `read_delim()` to bring it back in with the variables updated. You have a copy for later and an updated file. library(tidyverse) ## This is easy if all of the files are saved in one folder and are the only .txt files there pd_files &lt;- dir("pd files", full.names = T) ## This reads in the files and spreads out your variables ## I had to remove the white space after reading it in read_delim_helper &lt;- function(.path) { read_delim(.path, delim = ":", col_names = F, trim_ws = T) %&gt;% spread(X1, X2) } ## This will only work if all of the variables are the same in each file ## Variables saved as characters, will need to change these pd_df &lt;- map_dfr(pd_files, read_delim_helper) ## Saving a copy of the combined file might be a good idea ## Reading it back in automatically adjusts the column names write_delim(pd_df, "combined_pd.txt", delim ="|") read_delim("combined_pd.txt", delim = "|")
You have created a function that uses global variables `n` and `data`. Note that `data` is the name of a base R function so there is some risk that you are trying to do math on a raw function... not good. I try to use `dta` instead for avoidance of doubt. Anyway, you need to show us how those global variables are defined.
great, thanks for that tip. I changed my variables to dta. n = length(dta) and dta is normally distributed data which I got. There are 1000 data points. &amp;#x200B; &amp;#x200B;
Yeah. Calling length() on your variables as routine debugging output when you're sorting out a new function is something that can save you a bit of sanity.
Nicely put together. 
Note the 'Value' returned by `mle`. https://www.rdocumentation.org/packages/stats4/versions/3.5.2/topics/mle You're going to have to do something with that object.
Your question is too vague, you should state what area of application are you interested in. You may find the [CRAN Taskviews](https://cran.r-project.org/web/views/) of some use. A lot of things can be found through simply searching too, what searching have you already done as you don't indicate you have done any.
What do you mean by the longest code? You mean most lines? Asking about R specifically? There are shiny apps more than 500 000 lines. I am not sure people think or calculate it alot since you would split your code among multiple files when it becomes long. 
Multivariate or Multiple Regression?
`textstat_frequency` is not a function in "Base R" (meaning it does not come with your regular R installation). The most likely explanation is that: 1. You did not install the package which contains this function. `install.packages("quanteda") ` https://github.com/quanteda/quanteda 2. You did not load the library/package before trying to use the function. `library(quanteda)` 3. When restarting R/RStudio, you need to reload `library(quanteda)` every time. Alternatively if you do not want to load the package you can use the function by writing `quanteda::textstat_frequency()`. By doing this you tell R where to look for the function (its "namespace"). Functions can be named the same name in different user created software packages. To minimize the risk of conflicts you only load in the libraries you wish to use, or alternatively specify the namespace in the form of `package_name::function_name()`.
This is an initial idea, without knowing more about the data. There are probably some factors in the test data that are not in the training data. Having the same columns is great, and it's what you want, but if the model hits something it doesn't quite know what to do with then you could run into some problems. You may want to look at the factor levels that pop up in str([test.data](https://test.data)) and str([training.data](https://training.data)) and see if they match as well.
Thanks so much for your help! I appreciate it! 
Thanks for your reply! They're exactly the same as far as I can see. They were selections taken from the same dataframe. This is the test set: &gt; str(hotel_reviews_testset) 'data.frame': 3000 obs. of 15 variables: $ Hotel_Address : chr "Gran Via de les Corts Catalanes 678 Eixample 08010 Barcelona Spain" "Avenida Bogatell 64 68 Sant Mart 08005 Barcelona Spain" "Wrights Lane Kensington and Chelsea London W8 5SP United Kingdom" "Balmes 142 146 Eixample 08008 Barcelona Spain" ... $ Additional_Number_of_Scoring : num 158 168 1172 143 1243 ... $ Review_Date : Date, format: "2016-09-06" "2016-08-01" "2016-05-19" ... $ Average_Score : num 8.4 8.2 7.8 8.1 8.1 8.9 8.5 8.4 8 7.7 ... $ Hotel_Name : chr "Grupotel Gran Via 678" "H10 Marina Barcelona" "Holiday Inn London Kensington" "Catalonia Diagonal Centro" ... $ Reviewer_Nationality : chr " Ireland " " Egypt " " United Kingdom " " United States of America " ... $ Review : chr " Great location and very clean and comfortable Perfect for a trip to visit the city as very central Would highl"| __truncated__ " Reception staff didn t really go the extra mile to make me more comfortable" " Could not afford to drink in the hotel too expensive " " Third bed was a pullout sofa not so comfortable Shower glass showed signs of wear and tear pool was tiny No te"| __truncated__ ... $ Review_Total_Word_Count : num 27 15 12 41 103 39 51 5 11 6 ... $ Total_Number_of_Reviews : num 1664 1579 5945 1918 6608 ... $ Total_Number_of_Reviews_Reviewer_Has_Given: num 5 9 2 4 6 1 3 1 3 25 ... $ Reviewer_Score : num 8.8 7.5 9.2 7.5 6.3 10 9.6 8.8 6.7 8.3 ... $ days_since_review : chr "331 day" "367 day" "441 day" "347 day" ... $ lat : num 41.4 41.4 51.5 41.4 51.5 ... $ lng : num 2.173 2.193 -0.193 2.157 -0.18 ... $ posneg : Factor w/ 2 levels "0","1": 2 1 1 1 1 1 2 2 1 1 ... And the training set: &gt; str(hotel_reviews_training) 'data.frame': 3000 obs. of 15 variables: $ Hotel_Address : chr "Bond Street Mayfair Westminster Borough London W1S 2YF United Kingdom" "Jaume I 6 Ciutat Vella 08002 Barcelona Spain" "Petersplatz 9 01 Innere Stadt 1010 Vienna Austria" "La Rambla 128 Ciutat Vella 08002 Barcelona Spain" ... $ Additional_Number_of_Scoring : num 164 30 173 226 42 417 302 269 524 431 ... $ Review_Date : Date, format: "2017-01-10" "2017-04-10" "2016-09-15" ... $ Average_Score : num 8.2 7 8.5 8.1 7.7 6.7 8.2 8 8.2 7.9 ... $ Hotel_Name : chr "The Westbury A Luxury Collection Hotel Mayfair London" "Gran Hotel Barcino" "Hotel Wandl" "Hotel Serhs Rivoli Rambla" ... $ Reviewer_Nationality : chr " United Kingdom " " Israel " " United Kingdom " " United Kingdom " ... $ Review : chr " Room service as usual was expensive but 4 for a Mars bar " " location was excellent" " Small room with no external window and bathroom" " Location was wonderful and the breakfast " ... $ Review_Total_Word_Count : num 14 4 9 8 3 17 10 4 41 32 ... $ Total_Number_of_Reviews : num 715 448 1799 1957 1160 ... $ Total_Number_of_Reviews_Reviewer_Has_Given: num 1 7 3 4 13 20 7 4 2 3 ... $ Reviewer_Score : num 9.6 3.3 9.2 8.8 6.7 9.6 10 7.1 5.4 7.5 ... $ days_since_review : chr "205 day" "115 day" "322 day" "143 day" ... $ lat : num 51.5 41.4 48.2 41.4 45.5 ... $ lng : num -0.146 2.178 16.37 2.171 9.119 ... $ posneg : Factor w/ 2 levels "0","1": 1 2 1 2 2 2 1 2 2 1 ...
What package are you using for svm? 
They said multivariate in their post.
But usually when somebody doesn’t even know where to start, they don’t actually mean multivariate. 
I figured they meant they didn't know where to start with R. They said mutlivar a few times, and I know plenty of social science stats people who are trying to learn R now.
While that’s all true, OP doesn’t even have data... which makes me again think otherwise.
Oh, I just found what exactly they have and want to be a little unclear.
I don't do much prediction, but just an idea. Should you include the label column on the test data?
I just realized that too. It's an easy enough fix, but that could throw things off quite a bit.
So I tried these, but I am still getting the same error message. Any ideas why?
Look up `for` and `while` loops, they should provide you with what you need!
Thank you
I tried it and it works, but now it just predicts everything as positive, so the accuracy is kinda worthless :(.
Is the training set balanced? If not, maybe an SVM is not the best choice. Try to work it out as a anomaly detection problema instead maybe. 
I just completed the exercise as a little practice of my own. If you are stuck I can aid you in getting the solution. I'm not sharing my solution directly though, as that hinders the learning process :)
It's impossible for us to say without guessing unless you provide some more info. When asking questions related to programming it is good practice to post a reproducible example, or at least 1. Include the code which lead to the error. 2. Include some information on what system you are running and which packages are successfully attached/loaded. To provide info about the second point above you write `sessionInfo()` in the R Console after you have attempted to load your packages and run your code. `sessionInfo()` will provide info about which packages are loaded, which system you are using, what version of R/packages you are using. In order to successfully install the package which contains `texstat_frequency()` you need a C++ compiler. If you're using Windows you should install Rtools: https://cran.r-project.org/bin/windows/Rtools/ (version 3.5). That's a possible reason for why you are not able to successfully install the package.
Thanks for the offer but I figured it out as well, I also found out about the pattern function that I could use in the for loop. Thanks so much for the help 
Maybe clarify the difference. Multiple means one y variable and many X's. Multivariate means many Ys. 
Yeah it just seemed irrelevant unless they we're asking for the wrong thing. Maybe ask what it is they need first.
[removed]
I just noticed you did a 50/50 split for train/test. Did they tell you to use that ratio? Usually (not always) you want a bit more in your training than in your validation set. I'm partial to 80/20 myself, but meh.
You will have much better luck if you post your code. It is most likely an error in your logic, but it's impossible to know without the code.
Hard to help without the code, but I would check exactly what data you are writing. Say if your code is 'write.csv(df,file_path)' then try running 'View(df)' (may need RStudio for this) or 'summary(df)'. That way you can confirm that a) the data were subsetted properly and b) this subsetted data is what is actually being used in the write function call. My guess is you have subsetted your dataset propeely, but have not overwritten the old data with the new subset (or are using the old data variable name instead of the new)
The issue is not the '&lt;=', but the combination of two logical tests without an '&amp;' (R's AND operator). Try (Z&gt;-1 &amp; Z&lt;=1). I'm not sure if splitting it up is correct or incorrect, however - hopefully this gets you moving in the right direction either way 
Thank you very much. I put that in and am getting an answer different than what I was getting before so I think that splitting them up was incorrect. I appreciate the help.
No problem. I would look into the pnorm function more however, as I don't believe passing a logical (ie. is X largest than Y) is correct actually - instead I believe you need to pass it the Z values you want p values for (try running ?pnorm for more details on the function)
I think you just want pnorm(1) - pnorm(-1)
 CIfun&lt;-function(x,p) { xbar &lt;- mean(x) xq &lt;- qt((1-p)/2, length(x) -1, lower.tail = FALSE) xvar &lt;- sd(x)/sqrt(length(x)) c(xbar - xq*xvar, xbar + xq*xvar) } &amp;#x200B;
```CIWrapper &lt;- function(x,p) { c(CIfunlower(x,p) CIfunupper(x,p))}```
Just wrap them in a third function. CIfun &lt;- function(x, p) { result &lt;- c() result["CI_lower"] &lt;- CIfunlower(x, p) result["CI_upper"] &lt;- CIfunupper(x, p) return(result) } Note that you have a typo in your second function though.
I actually think I figured it out, there was a column that was lengthy chunks of text and I think there were "hidden" or at least subtle line breaks and enters and for some reason that doesn't matter inside R but during the export, that suddenly makes new lines instead of deleting the row that was supposed to be gone. Still unsure on the mechanics but when I export without the text-heavy column, it comes out correctly.
Try quote=T in your writing call. It should surround any text cells in quotation marks so when you open it in a spreadsheet program it will keep everything that is in quotation marks inside a single cell, instead of parsing out the line breaks.
The easiest solution is probably to use the function as.POSIXct. For example, if your start date is January 1st, 2018 and the end date is December 31st 2018, then &gt; time &lt;- seq(as.POSIXct("2018-01-01"), as.POSIXct("2018-12-31"), length.out = 365) will generate an array containing every day in 2018, and &gt; plot(time, 1:365) will create a plot with dates on the x-axis.
As far as I know, there is no way to use the api without getting a key, but don’t be intimidated by the use of the word app in that process. Once you have registered a developer account, you can create a new “app” that will generate an api key for you. Once you have that key, you can use it for pulling data in your analysis. You shouldn’t have to interact with the twitter dev portal again unless you forget your api key.
Hey man, The "application" is just a term. I registered for one to use the api and I literally just put reddit.com as the app url and callback url.
Thanks, that's good to know. It seems a bit of an overkill process, but I guess they are just being cautious and trying to crackdown on twitter data being used for Russian bots and the like.
I had a similar concern when playing with the twittR package in an R class last fall. I figured I'd go ahead and apply while the Prof was going over the code... Twitter approved my access before the end of the class. The access application is 100% not a big deal.
Others already answered your question, I just want to point out that the TwitteR package is more or less deprecated at this point, and it's advised to use the rtweet package instead: https://rtweet.info/
Alright, you are definitely correct. I've been looking more into it over the past 24 hours or so. Thank you for your help!
I believe you are absolutely correct. Thank you for the help it is greatly appreciated!
Great! Do you understand why?
Something along the lines of the pnorm finding the probability of the value being less than what I put in. So pnorm(1) = probablilty of Z being less than 1? Is this correct?
Yes so you are finding area to the left. So you find the area to left of 1 and then take away area to the left of -1 which leaves the area between them!
Thank you for the follow up! Though I guess I'm caught up on why pnorm(1) is first if the problem has it second. Is it so the value isn't negative?
Hello, I am seeking syntax for filling in years/quarter that weren't entered in the dataset. The picture illustrates the given data and the desired data frame.
Kinda new to it but I think dplyr or data.table has functions to do it (maybe merge and spread) https://www.rstudio.com/resources/cheatsheets/ There is also dcast and melt but i forget their package and dcast is harder to use.
There’s probably a better way to do it, but you could always just craft the times that you would like (i.e. year, quarter combinations) using a loop and then merge the data across.
Please, everyone, stop using twitteR. The maintainer is deprecating it and has recommended everyone use rtweet for years.
Thanks for the advice. I was unaware of rtweet.
I would create a vector of all the dates I was after, and then join them together.
This is the approach I would take as well.
It sounds like your date field is being treated as a string or factor. You need to convert it to an actual date. Btw, I highly recommend learning ggplot2. Yes, it’s a bit of a learning curve at first. But once you get the idea, it can be a lot easier to get things done than using base graphics.
Related to this, once I'm retrieving tweets using rtweet, how to remove smileys. It appears in my text as something like this &lt;0/&gt;&lt;/00&gt; etc.
I don't think your Dell's GPU will be much help. For your personal computer, you may want to look into basic parallel processing like "library(parallel)". For your cloud setup, you could try putting that GPU to use with RPUD and RPUDPLUS, I think both handle clustering. Here's two quick examples that discuss these packages (and refer to each other) : [https://devblogs.nvidia.com/gpu-accelerated-r-cloud-teraproc-cluster-service/](https://devblogs.nvidia.com/gpu-accelerated-r-cloud-teraproc-cluster-service/) [http://www.r-tutor.com/gpu-computing](http://www.r-tutor.com/gpu-computing) &amp;#x200B; I also stumbled on this presentation that seemed to have a trove of additional resources, github tutorials, and more: [https://www.arc.vt.edu/wp-content/uploads/2017/04/GPU\_R\_Workshop\_2017\_slidy.html#1](https://www.arc.vt.edu/wp-content/uploads/2017/04/GPU_R_Workshop_2017_slidy.html#1)
Thanks for your reply, the link leads to just a titlepage, though.
This package seems to provide a (rough) implementation of K-means for NVIDIA GPUs: https://github.com/src-d/kmcuda
Ah, specifically k-means. Great, thanks a lot!
You say that "you have written the algorithm". Have you tried using an already written knn-function? For example, the package "class" has one. These are often optimised a lot more than the ones you would write yourself. Or maybe look into the "Fastknn" package. I've never worked with it, i just found it in a search.
It is a presentation, Press cursor right
Thx, how come I didn't try that?!
Now I see that I have misled you and perhaps a lot of other people. I meant I had written the enclosing algorithm. For k-means itself I use the native kmeans function from the stats package. Thanks for the hints, I will look at the functions, you mention.
Hi! tidyr::complete was invented for this. For more granular control see tidyr::expand or tidyr::crossing
Which language is the native function written in? If written in R you can write your own in C++ using the Rcpp package.
OP may need to be a bit careful. I'm not super familiar with the difference between GPU and CPU runtimes for this sort of thing, but, if s/he's on a stock Dell desktop, chances are the CPU is actually halfway decent and the GPU is little more than a "display adapter" kind of chip.
That would definitely work if I knew C++ :) Thanks anyway.
This is true. In addition, since the ops data is a time series, the package “tsibble” (for time series tibble) is even more relevant. When you use as_tsibble() itbhad an argument that completes any gaps with nas.
As this is CUDA, OP wouldn't be able to run it on the laptop anyway. So the only option mentioned by OP would be AWS.
i highly suggest you do the R Studio Shiny tutorial. it'll take 1 hr max and you'll understand how reactive components work.
Which tutorial? I was trying to work it out with a working application of a friend and the cheat sheet.
huh? then ask your friend to explain to you how shiny works.
I don't want to. It's not like we have a counselling relationship going on.
You could check if Y-X == 0. As you point out, if it's not 0, there has been a change.
I don't remember if the library is stringr or stringi but I have been using &amp;#x200B; tweetDF$modtext &lt;- gsub("\\\\p{So}|\\\\p{Cn}", "", tweetDF$modtext, perl = TRUE) &amp;#x200B; to clean emojis out of my tweet texts
Couldn't you just do a matched fold change? or percent change? Y/X *100 or Y-X/X for each sample. then do a one way ANOVA test to compare the means for each supplement
That sounds awesome- you made the most out of the event! I went to Rstudio in Austin and the R community is so welcoming!
Highly suggest what the other guy already suggested. It seems that you don't know some fundamental stuff and they are easy to learn. Give it a try! Have being said that, you have two problems on server: 1) (minor) After you sunset any dataframe on your reactive context, you have to repeat it's name as to say to the chunk that you have these values. Also, you don't need to put it's value to the same dataframe. You can use any name and it will not save these in the objects created, for example: reactive \{ aux2 &lt;- subset of aux aux2 \} 2) (major and what's causing your error) Using input$something only works inside a block that is reactive, like the ones you have with aux, aux2 and aux3. "But what should I do when the app starts and the is no value?" The widgets on UI already have this problem solved inside it. They have an argument you can put after "choices" that is **selected**. So, instead of creating a if with exist, you just need to get these values of aux1 and aux2 (country1 and country2) in a argument selected after choices in selectInput. Finally, I would say that you don't need these 3 reactive dataframes, as you actually just need one, but it doesn't matter as your data look small. Also, search about **req** function of the shiny package. You can use inside reactive contexts to make things like your plot to only load after your inputs have some value. Probably check that ggplot too. Looks kinda weird
Thanks a lot. I will try that
Not only it is CUDA, it is also UNIX-based, so AWS is the way.
[http://lmgtfy.com/?q=shiny+tutorial](http://lmgtfy.com/?q=shiny+tutorial)
Thank you!
Have a look at the sink() function.
This is definitely what people often do. Not considered a hack. If you have pieces of code you find yourself re-using often, you may want to consider making them into an R package, just for your own use. [There is a great free online book to walk you through the process](http://r-pkgs.had.co.nz). It’s easier to keep organized, and easier to share with colleagues if you need to. (I used to think that “making an R package” was synonymous with “publishing a package on CRAN” — but this is not the case. You can make a package that you just keep on your local machine.)
Trying to manage a few r scripts and functions using by sourcing files is probably fine for a while. Letting the R package system do it is scales better and supports tests more readily. E.g. a project that implements a one off CLI utility as an R package that will never be published to CRAN: https://github.com/Display-Lab/fraisty ...and I need to finish the readme.md
thanks - rather than it being a class or something it's more a logical section of code. Say I have a file with 500 lines in it, but lines 200 - 300 can meaningfully be described as "fit-model-and-test", then perhaps it makes sense to have that saved into a file and loaded by source? That's more the case that I'm referring to than making a package. but thanks for the link
I am new to R but have some experience with Python. One thing I profoundly dislike about R, and in which AFAIK only Matlab is even worse, is precisely how it doesn't handle namespaces properly. &amp;#x200B; In Python it is natural to do import very_famous_package_everyone_uses as vf vf.fun(my_input) import my_libraries as mk # this doesn't require compiling your file nor making it into a package, so it's much more straightforward ml.fun(my_input) This is natural, good practice, leads to readable code and makes it much harder to accidentally overwrite a function name. Yes, you can always refer to it with library::function(), yes, R gives some warnings when loading a library, but, still, it's far from ideal. &amp;#x200B; As for your question, have a read here (including all the comments): [https://stackoverflow.com/questions/55521517/how-to-import-an-r-file-and-assign-an-alias-to-it-like-import-myfile-r-as-mf](https://stackoverflow.com/questions/55521517/how-to-import-an-r-file-and-assign-an-alias-to-it-like-import-myfile-r-as-mf) In R you can do this for packages: library(namespace) registerNamespace('ggp', loadNamespace('ggplot2')) data(iris) ggp::ggplot(iris, ggp::aes(x = Petal.Length, y = Sepal.Length)) + ggp::geom_point() and this for external files which are not packages: tstEnv &lt;- new.env() sys.source(file = "my_test_script.R", envir = tstEnv, toplevel.env = tstEnv) tstEnv$testfun("it works") or: source(temp, local = my_alias&lt;-new.env()) To be honest, I am not too clear on the difference between the last 2; if anyone could shed some light, it would be very useful.
 ``` outfile &lt;- file.open("/tmp/foo", "wt") cat( myvar, file=outfile ) ``` Use append=true in subsequent calls to cat() to write more vars to the file.
As someone who first started out in C and C++ this behavior is familiar to me and is called function overloading. IMO I like this behavior because there are times when I'd like to force the use of my function over that of a base function. Not everyone will like that idea but its great in my opinion.
As of my comment, there are only 4 comments in this thread, and they all contain good advice. If I'm developing a workflow where I don't know exactly how the full work product will turn out, I start with a single file and just build from there. Once it gets large enough that it spans more than *3 screens* on my display, I know that it's getting too large, so I'll refactor it (if necessary) and create a new script. Rather than referring to the first file from the second script, I'll create a third **main file** that calls the other files as needed using the `source()` command, which you referenced. If I am going to use the code repeatedly or have my coworkers use it, I'll develop a package. As others have mentioned, creating a package is not nearly as hard as it might sound. Before coding in R, I did a lot of coding in C++. Even after 10 years of coding in R, I still get frustrated sometimes with R's strange implementation of features compared to other programming languages. # Handling multiple "translation units" in R vs more traditional programming languages In C and C++, you always have a `main` function that controls the overall flow of your program. Then, you have other files that perform units of work that get called by the main function. The basic structure of C and C++ encourages you to break your program into units of work. ## Option 1: R's `source()`Command (Difficulty: Easy) By contrast, R encourages you to put all of your work in a single file. Even so, R does have facilities that allow you to break your code down into smaller sub-units just like C or C++. The first option is what I mentioned earlier and which you already understand: the `source()` command. ## Option 2: Using R's *package* facility (Difficulty: Advanced) The second option is a package. As I mentioned earlier, creating a package in R is not nearly as hard as it sounds. With the exception of the documentation, which is optional, but critically important if you plan on releasing your code to anyone else, creating a package in R is basically implementing a program structure similar to how C, C++ or Java implement multiple translation units for a large program. ### Namespaces vs Environments C++ has the concept *namespaces*. They are very intuitive and fairly easy to use. R has the concepts of *environments*. They are not nearly as intuitive as C++'s namespaces, but they are very powerful. In Hadley Wickham's *Advanced R* book, he has [a great chapter dedicated to explaining R's environments implementation](http://adv-r.had.co.nz/Environments.html). Based on the knowledge you demonstrate in your original post, at this stage, I highly recommend that you set aside an hour to read this chapter in Hadley's book on environments to understand the basics of them. ## Option 3: RMarkdown and `knitr::read_chunk` (Difficulty: Intermediate) Also, I highly recommend that you consider using RMarkdown and RNotebooks. Incorporating them into my workflow has made me far more efficient. Before them, I'd write code, revise it, then write a report for others. Now, I "document as I go." Often, I wind up with a publishable report and there is no need to write a separate report after I "finish" my analyses. ### Calling external code chunks with `knitr::read_chunk()` One really nice feature of RMarkdown/RNotebooks is the ability to use *code chunks*. When using RMarkdown, you **should not** be using the `source()` command. Rather, you should be using [knitr's `read_chunk()` function](https://yihui.name/knitr/demo/externalization/). I believe it's a far superior option to R's `source()` function. For these reason, often, even if I don't think I need RMarkdown, I'll still use it anyway just because I like the `read_chunk()` functionality so much. The previous link I gave you explains how to use `read_chunk()`, but I really like [this blog post from 2014 that explains the functionality](http://zevross.com/blog/2014/07/09/making-use-of-external-r-code-in-knitr-and-r-markdown/). I always opt for the ```&lt;&lt;ChunkName&gt;&gt;``` syntax (the first example) because it's far more flexible than the *alternative syntax* (option 2). Option 1 allows you to combine multiple external code sections in a single code chunk in any order you like, whereas option 2 only allows one external code section per chunk. --- **TLDR**: R has three major facilities for managing code across multiple files. They are listed here in order from least (simplest to implement) to most powerful (hardest to implement, but still not as hard as you probably think). 1. `source()` command 1. knitr's `read_chunk()` function, which is very useful with RMarkdown/RNotebook 1. Packages ---
When asking people to help with your homework show your work and say where you get stuck/what your problem is.
which behaviour do you like, R's or Python's?
 Error: object 'file.open' not found Is this function from a package?
R, though I have had very little experience with python so far.
I'm really not a fan of using source files in this manner. I either use source files as a full script meant to be run in a fresh R session or as a place to define functions and maybe some new constant variables. If these files get big enough, I'll put them into a package. Treating source files as a function to be run on variables in the global environment of the existing R session isn't really great practice. Be liberal with defining new functions to reduce repeated code, but don't treat source files themselves as functions. If a source file doesn't work when sourced in a fresh R session, it probably isn't the best use of a source file.
&gt; If a source file doesn't work when sourced in a fresh R session, it probably isn't the best use of a source file sounds reasonable... maybe this is why it felt a bit 'off' whilst going through it I'm not sure. Thanks
There are a variety of base set functions that will accomplish what I think you are trying to accomplish, but i wanted to share this solution to your code because i find it hilarious : (set1 + set2) != 1
Alright, I have this, but I do not know how to find the posterior density. \# Exercise 3 \# Values of bt bt \#prior distribution prior\_bt=1/length(bt) \#likelihood likelihood=rep(0,length(bt)) tt=seq(1,length(bt)) for (i in tt){ likelihood=dpois(1,bt\[i\]) } likelihood \#Prior\*likelihood weight\_bt=prior\_bt\*likelihood \#Posterior distribution posterior\_bt=weight\_bt/sum(weight\_bt) posterior\_bt \#probability density hist(prior\_bt, main = "Deansity", freq = "False") \#Posterior density curve(posterior\_bt, add= True, col="red")
In Python you can just import the function from the module directly to get the overloading behavior
It's not \`\`\`file.open\`\`\`. It's \`\`\`file\`\`\`.
The behaviour is called \*overriding\* (or, more correct, \*shadowing\*), not \*overloading\*. Overloading doesn’t exist in R but similar results can be achieved via generic method dispatch (S3 or S4).
Check out the [{modules} package on GitHub](https://github.com/klmr/modules). It’s designed to solve your precise issue. Be aware that I’m currently working on a new version of this package which will not be compatible with the current version. This is necessary because over the years I’ve identified flaws and improvements in the current version.
And there’s no need to use it. Simply pass the filename as a string to the `cat` function.
I would try to keep everything in functions. It helps keep the workspace clean, and then you can just call fit_model_and_test(df,params). Large projects where I am writing one-off functions I will sometimes keep the functions in a list. I source everything at the top, which loads the libraries, functions, and what essentially are the constants. Any sourcing partway through probably shouldn’t be additional functions being defined. I bet someone has a better style guide for that type of structure though.
Remember that it is possible to run Linux on a laptop.
look at this [link](http://www.sthda.com/english/wiki/ggplot2-axis-ticks-a-guide-to-customize-tick-marks-and-labels) specifically the section "Format the text of tick mark labels" which talks about using the package "scales" which has a few handy methods.
When you made the plot you specified the x-axis. I would check a few things around that: Does your x axis variable actually have a decimal in it? What is the interval of your x-axis? Or did you specify a minimum number of points? It would be really helpful if you could show us the r code you used to generate the plot
Writing things out on pen and paper is underrated. Spell things out plainly. Break the question down into the simplest components that you can, and figure out how to use R to solve each simple component, then put it all together.
Yup. I often write a simple flow chart before starting to write anything in my script.
It sounds like they want you to learn to think outside the box. Like the other comment, that's all about breaking the problem down in smaller pieces that you can solve. If you have pieces you _can't_ solve, try to break those down even further.
But what if you're so bad at mathematical reasoning that you can't even get there? Cause that's where I'm at. I've taken super detailed notes and have annotated all of my labs and assignments to make sure I know what each part of my script is doing. The problem is just I can't reason of any of these things myself. Even the smaller steps sadly-I'm just not creative enough yet apparently.
Use the write.table or write.csv functions in base R. Or, I prefer using fwrite from the data.table package.
I think this is the best answer. For larger projects consider making a package.
Honestly there is something really similar to that in the sample file so just look at that.
Sounds like iterations will be your friend. Get familiar with sapply, lapply (and unlist), and for loops.
You already know what length and mean are... What you probably skipped over are the basic data structures in R, which will give you flexibility in thinking about solutions. http://adv-r.had.co.nz/Data-structures.html Try to make it through the class without getting too discouraged. Sadly I dont think reinventing length() has lasting value.
So if you had a list of numbers on paper, how would you work out the length and mean? Think about that for a minute.
I completely share your sentiment with regard to namespaces. I just want to point out that packages are already their own namespace, you don't need to register a new one unless you want to rename it. But you can easily refer to ggplot2::ggplot() or whatever, without loading the entire package and without using registerNamespace(). That said, if it's a package you use all the time but still want it namespaced with a shorter name, your approach is totally reasonable.
cheers &gt; someone has a better style guide for that type of structure though yeah - i should look to see if there's a beginners style guide or something now i think of it thanks
I put my results in a data frame before writing to a csv with write.csv
I think you'll have to go to a 'long form' using 'gather' and then put the category column as the 'fill =' part in aes
It's bad practice because it's impossible to get proper IDE support for this. For example with RStudio, if you had ``` source("declare_x_and_y.R") sqrt(x) + sqrt(y) ``` Then the IDE will tell you `x` and `y` are missing. In general you will be missing a LOT of support from `lintr` which gives you significantly higher risk of bugs in code. The interdependencies between scripts becomes extremely difficult to manage in the long run if you ever need to make changes. In my opinion you have two options, if you are doing this to isolate a reusable chunk of code, then write a function. If you are doing this purely to not have to scroll through massive amounts of code and be able to focus on specific areas, then use [code folding](https://support.rstudio.com/hc/en-us/articles/200484568-Code-Folding-and-Sections) in RStudio. Specifically you can have ``` # model and fit ---- loads of code... # plot ---- more code... # summarise ---- just a few more lines... ``` Then RStudio will allow you to collapse entire sections, you can also easily navigate to sections using the "Show document outline" button on the top right of the editor panel. If you want to be able to run the code block by block without having to manually highlight them then you can create a [RMarkdown notebook](https://rmarkdown.rstudio.com/lesson-3.html) and use the code chunks.
I'm no R expert and you just got me thinking with your example...for length could you use dim(), nrow(), or ncol()? And for mean, sum(df$V1)/ncol(df). That seems a little simple maybe but i don't see why not? I'm in a bioinformatics lab using R a lot and, like what others have said, making flowcharts is immensely helpful.
off topic but can you please share where you get these exercises, from what book?
Our professor created these exercisrs
First things first - and I don't mean to be harsh - but you gotta change your mindset to a can-do attitude. Coding definitely takes some creativity. However, you can give it as many tries as you want and you can just try different things with no consequence. Don't focus on doing it right in the first go. Focus on doing anything, seeing where it takes you, and tossing it out or refining it. Neither of those is wasted time: learning what _doesn't_ work can be really valuable too. Let's stick to the example of counting the length and mean of a vector without the standard functions. I'll try to guide you through it step by step. Lets say our vector `v` is `1, 5, 3, 0, 6` while discussing this problem in words. Normally, on paper, how would you calculate the mean?
Hi, I'm not familiar with ts objects but I think that plotting time-series is limited. Why not storing your data as a data frame and then using all the possibilities of plotting in base R (or ggplot2)? In base R, the steps to format an axis are roughly: plot(df$Year, df$Amount, xaxt= "n",type="l") # xaxt="n" suppresses x-axis axis(1, df$Year, format("%m/%y")) Note: If you follow my suggestion, be sure that Year is a Date object: df$Year&lt;-as.Date(df$Year, '%m/%d/%y") See [here](https://www.r-bloggers.com/date-formats-in-r/) for formatting dates See [here](https://stackoverflow.com/questions/4843969/plotting-time-series-with-date-labels-on-x-axis) for similar questions
Thx, but I will use AWS eventually, so it would be great to make a trial run on my own device, but it is not a must have (besides the fact that I do not have NVIDIA GPU).
Yeah, I wanna say I'm questioning the value of this class for you. Why are you learning R, what do you want to use it for? Because many different people use R every day without thinking about how `length()` works. The other advice about writing it out and thinking it through, that's all completely true. But it sounds like you might be in a class that expects more programming experience than you have. Which is fine if you want to push through but also reasonable if you want to try learning another way. Which is a long way to say, either way, don't let this discourage you!
I keep seeing posts like this here and on the other R subreddits. Is it just me, or is this an assignment really early on that only serves to make R more frightening? Thinking outside the box is good, but if I was on my third class I’d still be getting to grips with vectors (I was admittedly a very slow R learner). Is this how R is normally taught?
One really cool resource is Rosetta Code: http://www.rosettacode.org/wiki/Category:Programming_Tasks They have tons of programming tasks, solved in a huge variety of languages. One thing you can do is browse some of the simpler tasks and see how they get put together. You'll probably even find R solutions for some of the things you're tasked with - though they may use the functions you're not supposed to. However other languages will have to code the algorithms more explicitly. Be sure to maintain your academic integrity and credit/cite properly if you use this as a resource.
Hmm. Well this sounds like a stupid class. Implementing the mean is good for understanding the statistics but implementing length seems pointless. Two ideas come to mind. For a 1 dimensional array v (eg a vector) of length n , create a boolean vector of length n where the ith value in the array is true if v[i] ==v[i]. Basically this is a vector where everywhere it's true and now add them all up. Here's a simple example: v &lt;- as.vector(1:100) #length(v) == 100 bool &lt;- v==v #length(bool) == 100 sum(bool) #100 My other idea: v &lt;- as.vector(1:100) i = 1 while(!is.na(v[i +1]){ i &lt;- i +1 } print(i) #100 This will test the next element in a vector and see if it's NA. Assuming your vector had no NA elements to begin with, this will continue adding 1 to i for each element in v. Note the '#'s are comments, meaning it's not code. Maybe they want you to know how to code, not just analyze data. However if that's the case, it's probably should've been covered in a seperate class bc data analysis is already a huge topic to even begin to cover in one class!!!
Also, again it seems like the profs expect you to already be a competent programmer. Unless you a skipped a programming prerequisite or something like that, that's an unrealistic expectation. (which might be intentional - hoping to push the class?) Do your best, ask for help when u need it, read ALOT about R and programming, practice and don't worry about not measuring up. Get as much out of the class as you can, and realize the profs may be asking for unrealistic results. Oh and maybe make some friends in the class? If your struggling, probably someone else is struggling as well.
Here's a couple of places to start learning. &amp;#x200B; Here's a place to run R (and other languages) in a browser. It's more of a data science focus but still plenty of ppl have code posted publicly: [https://www.kaggle.com](https://www.kaggle.com) &amp;#x200B; Some books: [https://r4ds.had.co.nz/](https://r4ds.had.co.nz/) [https://adv-r.hadley.nz/](https://adv-r.hadley.nz/) probably best to use a reference material. Read what you need to solve a specific problem. &amp;#x200B; Some interactive exercises: [https://www.codecademy.com/learn/learn-r](https://www.codecademy.com/learn/learn-r) [https://www.datacamp.com/home](https://www.datacamp.com/home) &amp;#x200B; datacamp is kind of annoying bc it's sorta teasers for free and the rest you'd need to pay for. I've never use this code academy course but others were great as an introduction to a new language! &amp;#x200B; Oh and if you need help cleaning data: [https://cran.r-project.org/doc/contrib/de\_Jonge+van\_der\_Loo-Introduction\_to\_data\_cleaning\_with\_R.pdf](https://cran.r-project.org/doc/contrib/de_Jonge+van_der_Loo-Introduction_to_data_cleaning_with_R.pdf)
It sounds like every introductory programming course, rather than a stats course. Mine (programming courses) definitely included learning how to find the length of a list, and the language we were using didn't include for/while loops. Recursion all the way. The purpose was to teach us to problem solve differently, as most of use were already familiar with for loops. One of the assignments in week 2: implementing "if even, calculate this, if odd, calculate that" without using an if statement. Hint: modulo math operators exist. We also learned to implement a crude type of image compression from images stored as matrices, and wrote a function to determine if two strings were in alphabetical order. Not because we needed to, but because we needed to learn how to handle information in certain structures under certain constraints. We were learning to program, rather than just how to write code. /end nostalgia I loved R from the beginning of my stats courses, but that's because I had learned to program on four other languages already. In one course we had one lecture that taught us the basics of vectors, but that was fine because vectors were like the lists/arrays that we already knew, and vectors made things even easier. That's why programming courses were pre-requisites to statistics courses, and I strongly support that requirement.
I definitely understood R better once I had learnt how to code. But imo I think that is an issue with how R is taught - we forget how it felt to know nothing, and go too fast for new users. Especially stats courses that just assume users will get the programming.
there is a [thread](https://www.reddit.com/r/datascience/comments/b9rjeq/advice_regarding_leaving_a_bi_job_for_a_ml_job/ek7eh7l?utm_source=share&amp;utm_medium=web2x) discussing a career change to data analytics
Thanks, but I'm a beginner, telling me to put them in a data frame means as much to me as telling me to put them in a μαγειρευτής ριζιού *wink*
Thanks.
Thanks.
You'll probably want to learn some SQL as well, but that'll be easy for you if you know R. I have a B.S. in biology but had some coding classes and have gotten interviews with just R + SQL. Good luck!
That’s encouraging news. Thanks for the reply!
I got into data science via modeling in the sports domain too; a common path, I think. I wanted to predict game outcomes, which led to quite a bit of education on data manipulation, analysis, and modeling. Keep going!
Depending on where you are living, a lot of consulting firms are looking for people no matter the background. I myself coming from a background of economathematics but I had a little experience with coding in general. Just try to find smaller companies with flatten hierarchies looking for people and you‘ll find something. But be sure to read the job description, Data XY can mean a lot of different things and not everyone of them is what you are looking for.
Ok... I have to start by saying this function does not seem like a good idea. I feel as though you are violating one of the fundamental principles of functional programming, functions should not have side effects. You might want to rethink your solution design to avoid this approach. Now... if you must, you could investigate the use of the eval class of functions. There you can programmatically construct a line of R code to execute, giving you full control of the variable name.
[This stackoverflow](https://stackoverflow.com/questions/1741820/what-are-the-differences-between-and-in-r) page has a pretty good discussion about the differences between the two assignment operators and the pros and cons of using each one. Personally, I use '&lt;-' because it's what the style guide in my department requires.
You’re entirely right that `&lt;-` doesn’t offer technical advantages over `=`. [The two are equivalent in power](https://stackoverflow.com/a/51564252/1968), their only difference is operator precedence. The real reason why `&lt;-` is still widespread is simply for consistency with existing R code bases, since `&lt;-` predates `=`. Any other reasons you might stumble across, such as the claim that `=` can only be used at the top level, are simply incorrect. Use whichever you prefer. Style consistency is a good reason to keep using `&lt;-`. When I switched to using `=` some years ago I made sure that I kept all my projects internally consistent: old projects keep `&lt;-` (even when I edit them), new projects use `=`.
Thank you for connecting me to that stack overflow post. I read through it, rather quickly admittedly. I am still not buying. The solution with the most upvotes suggests that one reason for using "&lt;-" is so that parameters clutter your global scope? I do not want that functionality. Parameters are parameters and as a programmer I should understand they only live inside the scope of my function. &amp;#x200B; As for this style guide business. I do understand the need for consistency. However, I have the same beef with the style guide authors. Why dictate to others such an unnecessary inefficiency? One that introduces parsing ambiguities? (`h &lt;- 7` vice `h &lt; -7`)
Of course I fully agree with your advice. Just a quick note that you do not need `eval` or similar to make OP’s code work. It’s sufficient to retrieve the unevaluated argument passed to the function (via the `substitute` function), and deparse it (= convert it to a character string).
Most of the answers in that thread are atrocious. But you seem to misunderstand the highest voted answer: it does *not* recommend to use `&lt;-` in function calls, it’s merely pointing out the syntactic differences between `=` and `&lt;-` inside a function call (albeit without explaining the cause). This isn’t an endorsement of either style of assignment, and `=` directly inside a function call *does not perform assignment* (contrary to what the answer says in its very first sentence).
There are many things I dislike about R, but, honestly, if you get "aggravated" over the use of `&lt;-` vs `=`, maybe you might want to, how shall I put it, reconsider your priorities? :) I get more "aggravated" by namespaces, and how it's all too easy to overload functions when you are importing libraries.
&gt;how it's all too easy to overload functions when you are importing libraries [That *never* happens.](https://twitter.com/emilynordmann/status/1116371158838972417) 😉
point taken
With regards to keybinding, in RStudio it's a modifier + "-" (modifier is Alt in Windows, Option in macOS). In emacs+ESS, it's whatever you want it to be (conventionally, "_"). I don't find this tedious.
I just find it silly that we have an old notation what was relevant only historically and we have gone so far as to make keybindings to make it easier to use old / outdated assignment operators. &amp;#x200B; Lets just use the "="...
You've got it the other way around - the keybindings (at least for emacs) have existed before anyone considered them outdated.
You're really getting angry over an arbitrary stylistic convention for a programming language? Further, R *gives you a choice over which one you want to use!* Here is a list of assignment operators in various languages: a = b a := b a &lt;&lt; b a &lt;- b a ← b a =: b a : b https://en.wikipedia.org/wiki/Assignment_%28computer_science%29#Notation If you think one of these is "correct" you need to get out more.
I think the argument OP is making is that, regardless of how little overhead the key binding offers, the effort for the `=` key on a standard QUERTY keyboard is even less (and assignment is an extremely common operation, so this adds up). Using a key binding might not be “tedious” but it’s also entirely *unnecessary* given that there’s a ready alternative.
Same here! I actually had a similar background to OP (tenured math faculty). Decided academia wasn't enjoyable. I had reasonable R skills, reasonable Python, "Meh" on Java etc and managed to find an entry level spot. It can be done for sure! ALso, the sports part is what drove me too, pretty neat stuff.
I agree that writing four characters " &lt;- " (to avoid the "less than a negative number" syntactic ambiguity) is a real pain. However, the "modern" solution of allowing "=" to be used outside of function call argument specification simply confuses the fact that they behave fundamentally differently inside the parentheses of a function call than they do outside. I for one wish they had never permitted using equals as an assignment operator, or that they had used some other symbol for argument specification... but they didn't. - Used inside a function call the right side of `=` is lazily evaluated and associated with an argument, even if no such argument ultimately is available to assign it to. This can have huge implications on behavior of the program. - Used inside a function call " &lt;- " will be part of the expression that is not evaluated but will make a change to the calling environment when it finally is evaluated and has nothing to do with argument matching. Once I got my head around this distinction, changing back to using "=" outside of function call parentheses became a whiplash... I often spread function calls over many lines in R and some function calls work with code blocks that switch the context out of the parameter assignment syntax even before completing the function call so always treating these operators as distinct really helps code readability. The use of assignment in calling the `microbenchmark` function is an example of a useful application of " &lt;- " in calling a function: library(microbenchmark) x &lt;- 1:100 microbenchmark( sum = xs &lt;- sum( x ) , xsd &lt;- sd( x ) , times = 10000 ) stopifnot( 5050 == xs ) The use of "=" in the `dplyr::mutate` function is a case where parameter specification is being used to simulate assignment (the language is not doing this... the `mutate` function code is) within the scope of a data frame: library(dplyr) DF &lt;- ( data.frame( x = 1:4 ) %&gt;% mutate( y = x^2 ) ) stopifnot( "y" %in% names( DF ) ) This behavior conveniently allows less typing than base R would require by using the very real difference in meaning between " &lt;- " and "=" to advantage, even though it may seem inconsistent to require "=" in this case on the surface. I don't expect you to _like_ the extra typing (I don't) but if you intend to use R well rather than hack at it then you need to convey your intent clearly and the "style" recommendation helps others (and your future self) see your intent.
Thanks.
I suppose that's why people like Python's philosophy of 'one way to do it' - someone makes that choice for you. (On the other hand once you get into numpy/pandas that philosophy is out the door.)
My preference is igraph.
&gt;\[I wish\] that they had used some other symbol for argument specification True, but I’m not convinced that this is actually a problem. Rather, the problem is the confusing documentation. The same “problem” also exists for other characters with overloaded meaning. For instance, parentheses have at least three distinct syntactic meanings in R: * To group expressions (which in fact calls the function `(` in R; that is, `(a + b)` is equivalent to `` `(`(a, b)``). * To call functions … `f(x)` * In control statements … `if (condition)` But for some reason nobody ever makes a fuss about this. Because *it’s simply not a problem*. The only issue with `=` is that people get taught wrongly that assignment works differently in function calls. If we stopped teaching people wrong things and fixed the misleading R `?assignOps` documentation this problem stopped existing. &gt; Once I got my head around this distinction, changing back to using "=" outside of function call parentheses became a whiplash I honestly have a hard time believing this because I used to teach R and none of my students *ever* had this problem, and I taught them the distinction early on. They just accepted that `=` means different things in different contexts, and moved on with their lives. And your mention of `dplyr::mutate` etc is, in my view, one of the best arguments *in favour* of using `=` over `&lt;-`, since the whole point here is that it syntactically emulates assignment (even though we (should) all know that it doesn’t really perform assignment). It creates a very nice syntactic consistency that is completely lost if you use `&lt;-` for other assignments.
Your second two examples of parenthesis usage are in fact the same. Whether you "believe" it or not, the cognitive cost of losing track of the difference between assignment and argument matching is real in R. Delayed evaluation is an important behavior to keep track of, and syntax can help or harm readability.
&gt; Delayed evaluation is an important behavior to keep track of Sure but the `=` operator has nothing to do with delayed evaluation, that happens for every (non-`BUILTINSXP`) function call, regardless of whether you specify a named argument or not. &gt; and syntax can help or harm readability. I am fully on board with this but I see no evidence that this is the case here, and I see plenty of evidence that it isn’t. The issues come from being taught falsehoods, not from potentially-but-not-actually confusing syntax.
It would probably be easiest to generate eg a list (or whichever data structure you prefer) of all permutations and then impose your rules by looping over the number of elements in that structure.
100% agree. I really have no idea why it is done and I feel peer pressured to do it in my code. Maybe it's time to take a stand.
I think this is pretty a non issue, use whatever you want as long as you are consistent throughout your code. The argument \`=\` vs \`&lt;-\` is pretty much the same argument as \`camelCase\` vs \`snake\_case\` (extra keystrokes vs readability), just even less relevant. I prefer to have the subtle distinction between declaring function arguments and assignment. It also makes the code a bit more skim-able for me (though I guess that is because I am used to it?) and more searchable (but to be honest I cannot remember ever searching my code for assignments). So the upsides of using \`&lt;- \` are pretty slim and come down to taste, but so are the downsides. The main argument for \`&lt;-\` is that is a convention in the R community, and conventions are good. You do not have to like them, but if everyone follows them it makes things a bit easier for anyone.
I can see it being an easy transition. Can spin teaching mathematics into being able to break down complex material to people who aren't familiar with the subject. Definitely a plus with consulting. Tons of positions available for data analysts. Work on some projects and make a small portfolio on Rpubs. Multiple projects on baseball data can be fun too. Creates good talking points during interviews. Have some projects with some nice looking graphs. &amp;#x200B; DataCamp is a good site for R programing. It has a Data Analyst skill track. You can get certificates of completion you can put on a LinkedIn profile. Courses are taught by creators of some of the packages and use tidyverse. It's a little expensive, but after a free trial, they usually drop the price 50% off. I still use it. &amp;#x200B; People mentioned SQL, but it really depends on the place you're working. If you're familiar with Dplyr notation in R, you're halfway there with basic SQL queries. Datacamp also has a couple courses on SQL basics. &amp;#x200B; R also has packages where you can connect to your SQL server so it can all be done in RStudio. Can also upload your results if need be. Most of the time, I'm pulling queries from SQL and doing all of the data work on my desktop.
I really appreciate the response. This reinforces articles I’ve read mentioning how more functional R is with the dplyr tool. I’ll check into some of the resources you mentioned and start deciding on specific projects to work on. Many thanks.
Same, but [graphlayouts](https://github.com/schochastics/graphlayouts) seems really interesting with that "Radial Layout with Focal Node" option.
I usually always load the "tidyverse" package. It's a collection of Hadley Wickham's most popular packages. Dplyr makes for easier code to read too. Which is good for when people review your code.
yeah, it's kind of weird to do this instead of, like, doing it a more idiomatic way, like, defining it so you behave like this: TotalLength = total(iris$Sepal.Length) There are times when people have good reason to exit the functional paradigm, or do so whether it's a good idea or not - like PRNGs, for most use cases, might as well have a global state, so in R the default is to work with PRNGs that have global state.
Hadley Wickham is a saint for providing top tier resources for free. I’m still working through “R for Data Science.” It’s been easy to follow and understand. If I can land a job in this field I’ll certainly be donating. Are there any data visualization packages you especially prefer? So far I’ve just messed around with ggplot and been surprised with it’s diverse functionality.
ggplot has mostly everything you need. I usually save code chunks of some graphs. There are additional packages that improve ggplot. gridExtra makes it easier to put completely different graphs on the same grid. ggthemer has additional themes to make the graphs look nicer. Rcolorbrewer has some good color palettes. There are other packages that create interactive plots. Plotly being one of the more popular ones. https://plot.ly/r/ There's pretty much a package for almost anything you can think of. The problem arises if you need to put code into production. Because libraries are dependent on the version of R, it complicates things if you're deploying outside of an R server. Python is a lot more friendly in this aspect. R is better for data analysis, data visualization, and doing stuff locally, but Python is better for putting code in production. Especially if you work in a cloud service such as AWS or Azure. R is starting to catch up though.
See this document: https://rmarkdown.rstudio.com/lession-6.html
Depends on what they want you to know ... Are they doing some work that's ETL related? Or is it a bit more complex around modelling?
I think you need args[1] instead of x?
Data camp has a pretty good Intro to R tutorial and [Youtube](https://www.youtube.com/channel/UCpcJNrQyW3Ge7w9-dmijW9Q) has quite a few resources for learning R as well. I'd also find some data related to work and try to apply skills as you learn them to analysing that data.
SuperDataScience has a great course and I was able to use ggplot in under a week.
An old professor of mine offered an intuitive explanation. &lt;- can be described as an assignment operator. If I assign an object called “variable” to be a vector c(5,6,7), then I’m assigning that vector a name. But mathematically “variable” does not equal c(5,6,7). The vector c(5,6,7) does = c(5,6,7)
If you want to quickly get an overview of what you can do in R, DataCamp is useful. Although I'm not sure how much the subscription is. Wouldn't take too long to make it sound like you know what you are doing -- until you have time to learn it more thoroughly. &amp;#x200B; I also might suggest using the phrase: "I've mostly used R for \[insert thing that isn't thing you have been asked to do\]"
You can start with SWIRL , Read data wrangling with r by boehmka side by side , When you get comfortable with R , you can ask again if you some more specific needs
Haha good advice (seriously). I will probably be saying that a lot
Wow this looks great. Ill definitely give this a shot
I use '&lt;-' extensively, but I do so with purpose. As much as some people hate it (but far from all), I like to assign variables in arguments, And R is fantastic for this, &gt; a &lt;- function(x){x^2} &gt; a(b &lt;- 2) [1] 4 &gt; b [1] 2 &gt; a function(x){x^2} &gt; a(d = 4) Error in a(d = 4) : unused argument (d = 4) &gt; a(x = d &lt;- 4) [1] 16 &gt; d [1] 4 It might look nicer with a realistic example, but...
Good call. I'll see if I can wrangle up a relevant data set
So it looks like they have a $23/mo option. You think its worth it to get this just for one month?
Definitely more on the modeling end
This is a great free book about forecasting in R. It touches on a lot of easy to implement models with all the R code included. https://otexts.com/fpp2/
[https://www.youtube.com/channel/UCMdihazndR0f9XBoSXWqnYg](https://www.youtube.com/channel/UCMdihazndR0f9XBoSXWqnYg) &amp;#x200B; Idk what you are going to be doing exactly but this channel has been quite helpful for me. She has supplemental materials on her website as well.
To what extent is needing to know R fluently a requirement to do the job? Unless you were hired to specifically develop things in R (specifically), I imagine it'd be more important that you can get the job done, period-- in which case, you can learn as you go and focus on establishing a strong foundation now.
Okay you're going to have to do some power reading. There's a couple of books for this but one that will help you out that Ive enjoyed so far is "Practical Data Science with R" by Nima Zumel &amp; John Mount. This really will take you majority of the way of being able to identify what types of models are used when, and how to use them in R (amongst proper presentation and workflow). They sell this book at MIT so it is fairly credible. You may be able to find a free version somewhere, but if not you can google discounts for Manning's website that sells the book. Take comfort in knowing that there is no end game in this industry, you'll always be adding knowledge to your toolkit so the expectation is that you have strong analytical skills as a prerequisite before coding (that part you can always look up, you just need to know what questions to ask)
You've an uphill climb, but it is doable if you focus 110% on the task at hand. Ditch everything else and get moving. &amp;#x200B; 1) Get yourself through the syntax (variables, operators, control flow, etc.). Print out cheat sheets for reference. 2) Learn [data.table](https://github.com/Rdatatable/data.table/wiki/Getting-started) . Getting fluent at this will justify the claim you've made. 3) Try to implement excel workbooks you are familiar with into R. &amp;#x200B; Pour your questions onto here and stack overflow. Hire a tutor to help you. Reading is reading, but projects are learning.
Yeah this would be the best way if you can stick to functional programming but if there is a good reason to break out of it you could use assign like this --- total = function(x, varName) { assign(varName, sum(x) , envir =.GlobalEnv) } total(iris$Sepal.Length, TotalLength) --- Sorry for the bad formatting. I'm on the phone.
They have the course on Udemy for like 12-30 bucks for lifetime with 30 day no question warranty. &amp;#x200B; [https://www.udemy.com/r-programming](https://www.udemy.com/r-programming)
It is short and gets you started with simple tools.
Here is one approach, &gt; f &lt;- function(x) {.GlobalEnv[[x]] &lt;- 1} &gt; names(.GlobalEnv) [1] "f" &gt; f("ood") &gt; names(.GlobalEnv) [1] "f" "ood" &gt; ood [1] 1 R also allow for you to assign to the parent environment.
R for data scientists, is a good free text book that teaches you all the basics and is actually quite well written
R Studio cheat sheets (https://www.rstudio.com/resources/cheatsheets/) + caret
Ah the common sitcom premise. You need a friend wholl give you live R advice as you try to copy what they are saying mid meeting which will lead to hilarious misunderstandings at the time. Better yet if your friend is actually a Python person but agreed to help, thinking it can't be too hard to switch. Eventually all will be revealed and you'll be fired but not before realizing all this actually made you a decent R user.
R for Data Science by Hadley Wickham. It's completely online. https://r4ds.had.co.nz/
Are you questioning $23 when you’ve gotten a position by lying?
I would start with this, gives you the most bang for your buck
You can try cramming, but R will take anywhere from several months up to years to become proficient in, not days. Everyone learns at a different pace, but I spent two solid years programming in R for 4-8 hours a day before I started really feeling like I knew what I was doing. Granted, this was my first programming language, and I didn’t take advantage of any resources like DataCamp or O’Reilly books which would have helped accelerate the process, but me learning was the result of pure sweat equity. Others may disagree, but I feel like your expectations are similar to becoming a fluent Spanish speaker with no accent in two weeks. It’s nearly impossible, but that doesn’t mean you can’t pick up just enough to navigate your way through a different country and fake it until you make it. Best of luck to you!
So, to be clear: - You outright lied in your resume and interviews, and got a job for it (presumably at the expense of someone who *does* know R), - You couldn't be bothered to learn until 2 weeks before your start date, and... - You don't even want to spend $23 in some of the suggestions people have put forward. Good luck, bud...
No kidding. OP sounds like a complete piece of work.
Trying to understand your workflow here - why do you want it "typed into Notepad" instead of, say, reformatted into a text file of the appropriate shape?
A cheap solution, if you can work with the text file solution, is to do something like: dta = tibble(date = c('010119','020119'), customer = c('Home Depot', 'Lowes')) write.table(dta, "filename.txt", eol = "\n", sep = "\n", quote = FALSE, row.names = FALSE, col.names = FALSE) This is getting a little cute, but, there you go. Verify that this should reproduce your data exactly.
Figure out what you want to do in R, find an example, hack it to work for your case. I got my R intro in a few days that way, the rest is relatively simple once you become familiar.
I want it typed into notepad to verify it works. The data ultimately has to be manually keyed into a system of ours with thousands of rows.
I wish it were this simple, see my comment to your first post.
This isn’t an argument for (or against) `&lt;-` since you can do the same with `=`, for better or for worse. &gt; a = function (x) x ^ 2 &gt; a((b = 2)) [1] 4 &gt; b [1] 2 &gt; a(d = 4) Error in a(d = 4) : unused argument (d = 4) &gt; a(x = (d = 4)) [1] 16 &gt; d 4
So I’m genuinely curious: Who the fuck are the people who upvote this? You do realise that this means it is displacing other, more worthwhile content on the front page in the subreddit, right?
Best answer, I completely agree. Except for the last sentence. Completely disagree with that.
Yeah, if you have to manually key stuff into something, native R does not have access to the elements that you need to make that work that I know of. While it may be possible to extend R by having it plug into other tools and frameworks, that would involve installing things and that's already indicated to be a problem. You may see whether VBA in Excel can do this; it can do a lot and is likely to be available for your use if you have Excel.
Thanks! Yeah I’ll admit the last sentence was a bit facetious of me.
To be fair, this sub is small enough that all posts are on the front page for a few days. But this is indeed highly unethical behaviour on OPs part.
R can use Java robots class through `rjava` this is how `rMouse` works I believe and that package also khas keyboard functions. I must say this is a bizarre workflow though. You might have a better time writing a snippet to a file an looking at it, or pasting to your clipboard using something like `clipr`
 Server A.1: 3.4.4 Server A.2: 3.5.2 (Docker) --- Server B.1: 3.4.4 Server B.2: 3.5.1 (Docker) I am the one setting up the servers though.
Sounds like a job for WinAutomation, not for R.
Its going to be rough. If anyone else on the team knows R itll be hard to write code that they wont look at and go "jeeeeeeesus". If nobody else knows R you may not run into any problems, but dont expect to start pumping out valuable work immediately.
I use Rocker Docker containers myself. You can get whatever version you need. https://hub.docker.com/u/rocker/
I use (close to) the newest version. If my admin doesn't stay up to date then I install it in userland (e.g. with Anaconda).
If you're on windows you can try the [KeyboardSimulator](https://github.com/ChiHangChen/KeyboardSimulator) package. But how did you manage to get R but aren't allowed Python?
Local machine, I stay around 1-2 versions behind just so all my packages have time to catch up. In terms of an HPC situation, I believe our handles it by allowing (certain) users to create environments and install whatever version they want.
Base R developers and tidyverse developers have in general been very good at maintaining backward capability. There are few instances where upgrading version of base R will break anything.
It's interesting you brought this up. I found the keyboard simulator package not too long before I posted my question here but for some reason won't let me pass strings as arguments. If I need it to type the letter 'a' it can do that, but if you want it to return 'hello' as one string it yells at me. Am I doing something wrong?
I use the latest stable version and will just roll back a version if I ever can't install something I need to. I use R every day as a data scientist at Microsoft. Mostly tidyverse packages.
What are some examples of your errors? I bet you're missing system dependencies, not running out of memory.
I've never done graph stuff in R, but from what you're asking, could these be formulated as properties of nodes? You can look at epidemiology models where nodes for example are vaccinated or not, or more complex models where nodes have different infection rates. Could this maybe be used more as a vertex property rather than trying to use organisations as nodes? If there's only 1 organisation per person this would be much simpler, and you could even just visualise it with highlighted vertices (if the number is small)
I am not sure what is the difference is between a vertex and a node? There are 100 people and 20 organisations so it is quite small.
sorry there's no difference I was being loose with the terminology. I've barely done any graph theory but what you could do, at least in most libraries I know of, is create a vector of people with unique values per organization and use that as some type of property to do analysis. I'm not sure what information you're trying to gain, but this is essentially a "community" problem I think? https://en.wikipedia.org/wiki/Community_structure
There are many options to analyze that data. With network science, you could for instance transform that data into a bipartite graph and analyze it based on the corresponding adjacency matrix, which will be square as usual. You could also work on a projection over one of the dimensions (either a graph of people linked if they share an organization, or a graph or organizations linked if they have common people). Another approach is to tackle the problem as a recommendation problem and apply matrix factorization techniques. You could for instance use SVD to obtain the people-features and organization-features matrices. This would work as a profile of people and organizations in a common space that would be comparable. It really depends on what you want to know.
It only does one key at a time, you’d have to loop through a string. I suspect this design makes it possible to have “Enter” as a input to press the enter key rather than type the word “Enter”.
This should be doable in igraph, you just need to create appropriate vertex attributes to distinguish your two types of vertex. The only way I can think of to do it, because I am not an expert, is to create your graph from an edgelist, rather than adjacency matrix, and supply the vertex attributes seperately. I would create a seperate dataframe listing all the people and organisations in one column (called name or id or similar), and then another column specifying if that name is a person or organisation (called type or similar). Then when you create your graph, you specify the vertex metadata=your dataframe, as per https://igraph.org/r/doc/graph_from_data_frame.html I had to do this recently, the hardest bit for me was creating and then converting the matrix, which was a dist object, into an edgelist in a way that preserved the vertex ids. But you may find that a doddle with your data! The other option would be to use the package bipartite, as your network is effectively two tiered. https://cran.r-project.org/web/packages/bipartite/bipartite.pdf This has a suite of built in network descriptors that can be calculated, but it is designed for biological species interactions, so they may not be as useful. The graphs are relatively easy to make, but you would definitely need to supply an edgelist rather than adjacency matrix for this package. They also have a specific sort of clunky look, which may not be the aesthetic you are going for ([example](https://ars.els-cdn.com/content/image/1-s2.0-S0065250415000355-f02-09-9780081009789.jpg) from a book written by a colleague, here's [another](https://www.cell.com/cms/attachment/2007962415/2030682266/gr1.jpg)).
Hey, BeesBeware, just a quick heads-up: **seperate** is actually spelled **separate**. You can remember it by **-par- in the middle**. Have a nice day! ^^^^The ^^^^parent ^^^^commenter ^^^^can ^^^^reply ^^^^with ^^^^'delete' ^^^^to ^^^^delete ^^^^this ^^^^comment.
Hey /u/CommonMisspellingBot, just a quick heads up: Your spelling hints are really shitty because they're all essentially "remember the fucking spelling of the fucking word". And your fucking delete function doesn't work. You're useless. Have a nice day! [^Save ^your ^breath, ^I'm ^a ^bot.](https://www.reddit.com/user/BooCMB/comments/9vnzpd/faq/)
Hey BooCMB, just a quick heads up: I learnt quite a lot from the bot. Though it's mnemonics are useless, and 'one lot' is it's most useful one, it's just here to help. This is like screaming at someone for trying to rescue kittens, because they annoyed you while doing that. (But really CMB get some quiality mnemonics) I do agree with your idea of holding reddit for hostage by spambots though, while it might be a bit ineffective. Have a nice day!
The bipartate graph sounds interesting and I tried to read up a little bit about it. If I understand it correctly it is just what I need as it is similar to a normal graph but it can have two different groups of vertexes. But how would I convert a 100x20 matrix of 1 and 0 to a bipartate graph? I thought SVD was dimension reduction. What can it tell me about the features in this case? I don't really have a question yet, first this is mostly exploratory and I want to visualise how the different people are connected to the different organisations.
Just build a matrix of 120x120 where the first 100 nodes are of one type and next 20 nodes are of another type. Set it to 1 whenever there is a link. Links will only exist for nodes of different types, because of the nature of your data. So, the first 100 nodes will only have a one to any of the other 20. That's just what a bipartite graph is. Your initial matrix can also be interpreted as an incidence matrix, where either one type or the other is the edge (regarding the projection I was talking about, not the bipartite graph). Regarding SVD, the features are called latent features, which means they are not interpretable, but they usually capture some sort of semantics and they can be used to compare a person with an organization, answering questions like "would this person be a good fit for this organization?", the quality of which would obviously depend on the quality of the data. For an exploratory analysis, I would honestly use Gephi instead of R. Then you can switch back to R for other, more complex stuff, particularly when you have a specific question you won't be able to solve with Gephi, like an experiment with random walks that is easier to solve with linear algebra in R. I would suggest you prepare the adjacency matrix in R, and then, using igraph, just convert it to a graph object and save it as GML for example. Load the GML into Gephi, do your exploration there and then you can go back to R by exporting back to GML. igraph can compute most of the metrics Gephi can (if not more), but Gephi is a better visualization tool and therefore more useful for exploration. Good luck! :-)
&gt; I would create a separate dataframe listing all the people and organisations in one column (called name or id or similar), and then another column specifying if that name is a person or organisation (called type or similar). So I have a data frame with two columns, one with both people and organisation names and one which specifies which of them are people and organisations. What is the rest of the data frame containing?
Nothing, that dataframe is just there to specify the vertex attributes. The rest of your data, which was in an adjacency matrix, would be stored in a separate object taking the form of an edgelist. For example: my.edgelist.df is your edgelist created from your matrix my.vertex.attr.df is your new dataframe with your two columns, defining a vertex attribute (person or organisation) my.graph = [graph_from_data_frame](https://igraph.org/r/doc/graph_from_data_frame.html)(my.edgelist.df, directed=FALSE, vertices=my.vertex.attr.df)
Another question, as I am playing with this I am noticing that the command *keybd.press**('a')* prints the letter "a" in the screen where my code is. How do I get it to print the result in Notepad? Looping through each string that I need returned to notepad seems kind of inconvenient but I'll take it, beggars can't be choosers.
But R is free though :)
Lots of assumptions here. Just wondering how it stacks up against so many great free options that people have suggested. Would be happy to pay if its worth it though
Awesome thanks!
Haha I don't think anyone else there knows R so hopefully I'll be able to fake it until I can learn enough to be of value
Good advice. I always find that I learn best by doing it anyway
From what I understand, its just being able to use it so I can make predictions/recommendations based on the data they collect. I don't think that requires me to code, but I don't really know
Haha jeez you guys seem to be personally offended that I would even ask this question here... Honestly when I posted it, I obviously needed help, but I also thought it would be an interesting way for people to think about the most efficient way to learn R, given a time constraint. I actually thought it would be a question that people would enjoy thinking about
Is the requirement to make recommendations in the literal "this is what I suggest and here is why?" Or is it to build recommendation engines? Either way, if you need to use R, you are obviously going to write code. But if you need to do the former, R is just a means to an end. If the latter, then R is both a means and an end. Big difference.
The behaviour as described here is unethical and you presented as the truth. People don't like unethical behaviour. But there is value in the answers which is why I didn't join the downvote brigade. Honestly the way to make people think about the most efficient way to learn R would be to ask that. If you add embellishments, people will also think about the embellishments. Kinda like asking programming questions with a minimal reproducible example when you think about it.
Just to be clear, the scenario I'm in is very real and true. But I also thought the audience I was asking, people who are knowledgeable/interested in R, would consider the question to be engaging as well. Personally speaking, when I'm competent in something I enjoy sharing my knowledge with others. From my POV, posing this question would be nothing more than a way to share valuable information. It's honestly hard for me to even understand the state of mind of the commenter who was actually angry about people upvoting this.
This. I was reasonable with R. I knew how to build some basic models and do some tweaking/tuning. The first data set I got was swiss cheese and took quite a while to fix up before it was even usable. I guess the question for OP then is how comfortable you are doing the other parts of this. Proper imputation, etc. That's a thing that can be a giant pain in the butt.
True, but what's the value of your time?
 #say a is the row player#### a &lt;- matrix(sample(-10:10, 4), nrow=2, ncol = 2) #say b is the column player#### b &lt;- matrix(sample(-10:10, 4), nrow=2, ncol = 2) #define funciton finder &lt;- function(x,y){ #null hypotheses#### x.weak.dom.str.yielding.row &lt;- NULL y.weak.dom.str.yielding.column &lt;- NULL weak.dom.str.yielding.cooridnates &lt;- NULL #weakly dominant strategies#### if(all(x[1,]&gt;=x[2,])) { x.weak.dom.str.yielding.row &lt;- as.integer(1) }else if(all(x[1,]&lt;x[2,])) x.weak.dom.str.yielding.row &lt;- as.integer(2) if(all(y[,1] &gt;= y[,2])) { y.weak.dom.str.yielding.column &lt;- as.integer(1) }else if(all(y[,1] &lt; y[,2])) y.weak.dom.str.yielding.column &lt;- as.integer(2) #strategy pairing#### if(!is.null( length(x.weak.dom.str.yielding.row)+ length(y.weak.dom.str.yielding.column) )) { for (k in 1:max(length(x.weak.dom.str.yielding.row), length(y.weak.dom.str.yielding.column))) { weak.dom.str.yielding.cooridnates &lt;- c(weak.dom.str.yielding.cooridnates, x.weak.dom.str.yielding.row[k], y.weak.dom.str.yielding.column[k]) } if(all(length(weak.dom.str.yielding.cooridnates)%%2==0, !is.null(length(weak.dom.str.yielding.cooridnates)))){ return(weak.dom.str.yielding.cooridnates)} }else print("No WDSE") } #run for given matrices finder(a,b) #redefine matrices and rerun a &lt;- matrix(sample(-10:10, 4), nrow=2, ncol = 2) b &lt;- matrix(sample(-10:10, 4), nrow=2, ncol = 2) finder(a,b) I formatted it for you. `x.weak.dom.str.yielding.row` and `y.weak.dom.str.yielding.column` are only not NULL, when every row of the first column is bigger or equal OR smaller than the row of the second column. Could you explain what you are trying to achieve and how you want to achieve it? Are x and y the payout of the two players dependent on what strategy they are choosing?
on your question, you are right. x is the payoff of the column player, that means he will play one of two rows and their payoff will be determined what the row player chooses as their strategy. If a strategy weakly dominates (that is, one row or column is greater than or equal to the other), that strategy is chosen and equilibrium is thusly reached. I will try the code ASAP. Thanks a ton!
Oh no, I did not change your code, I just reformatted it for better readability.
Oh no, I did not change your code, I just reformatted it for better readability.
Oh, thanks anyway 😁
Can you provide working and non-working static inputs? Or seed values so we can reproduce and troubleshoot more easily?
Have you looked into SWIRL
You show up in a community, telling people "hey I am not member of your community and stole a position from a member by pretending to be so. now help me become an actual member so I don't have to face consequences of my actions" Note that a high proportion of R users are academics who have lower tolerance for this sort of stuff. I am sure you have reasons for whatever you do but it's bizarre to me that you are so surprised that people might be upset.
thanks OP!
What's SWIRL?
Its a library in R , commonly known as Statistics with interactive learning in R . Its a very good resource for anyone who starter to learn R
If you can't do the work, you should probably just drop the class.
I have an 85 in the class, the final is take home, and there's only 4 weeks left. Why would I drop.
You really should do you own work dude, it’s how you learn. Also, you do realise that most universities will kick you out for paying someone to do assessment work for you?
Even if your title is about tSNE plots, I understand that you want help with merging the two dataframes. Assuming that you are only processing one plate at a time, or that you treated all of them with the same compounds in the same wells, I would not overthink it and create a common variable before the merging. all_data$well_coord &lt;- paste(all_data$well_row, all_data$well_col, sep = "_") positions$well_coord &lt;- paste(positions$well_row, positions$well_col, sep = "_") df &lt;- merge(all_data, positions, by = well_coord)
Just the 19-digit permutations (without rules) will number around 121 quadrillion options: https://www.wolframalpha.com/input/?i=19p19
Do you need the actual list of permutations? Or do you need the count of them? Just the 19-digit permutations (without rules) will number around 121 quadrillion options: https://www.wolframalpha.com/input/?i=19p19 One approach might be to first enumerate through the *combinations* of your 19 "digits", and if the combination passes the rules, then do the permutations on that (assuming your rules are only based on content, not order).
You can't go wrong following Hadley Wickham's style guide for R: http://adv-r.had.co.nz/Style.html He's written many of the more popular libraries used in R.
&gt; he code worked the first time but after I changed the matrices and reranthe cod This makes me think that some variable(s) that should be reset when the code starts is not actually getting reset.
Oh boy, I need to brush up on my combinatorics.
Thanks a bunch. Your reply is super-helpful. It worked magic. Now I can feed my tSNE (apologies for the misleading title of my query) with the new matrix. Thank you again!
Since someone answered the meging question I'll just throw in that I use the Rtsne package a lot for performing tSNE at work. It has a simple interface and can be configured to run on multiple cores. Once it is finished running you can easily plot the results with ggplot2 or any other plotting package you prefer.
What class is this for?
You have to simulate the actions to bring notepad into focus. It’s a mouse and keyboard simulator, simulate exactly the actions you would have taken to do what you would do manually.
Can you explain this statement? if(all(length(weak.dom.str.yielding.cooridnates)%%2==0, !is.null(length(weak.dom.str.yielding.cooridnates)))){ return(weak.dom.str.yielding.cooridnates)} }else print("No WDSE") The length of ```c(NA, 1, 2)``` would always be 3 and as such, there would always be a remainder of 1. Also, you're setting this up to be a loop but I'm not sure what ```for (k in 1:max...``` would result in. You'd keep growing the ```weak.dom.str.yielding.cooridnates``` but I'm not sure you'd want to do it the way you're doing it.
Aha, that makes it more fun I guess.
Also note you can do merges on two columns which skips the paste step: df &lt;- merge(all_data, positions, by = c("well_row", "well_col"))
One more note. I think you can simplify strategy paring to this. Also note, this uses my code above where I use ```NA_integer``` instead of ```NULL```. #strategy pairing#### weak.dom.str.yielding.cooridnates &lt;- c(x.weak.dom.str.yielding.row, y.weak.dom.str.yielding.column) if (sum(is.na(weak.dom.str.yielding.cooridnates)) == 0) { return(weak.dom.str.yielding.cooridnates) } else { print("No WDSE") }
I just have to make something work for another 3-4 months (crossing my fingers). If I needed it for longer than that I would look into it.
Using lists for this is a terrible plan. Use a vector of cards (character) and use the `sample` function to choose random indexes to pick cards. Read `?sample' for the answer to your question.
I agree with jdnewmil. cards &lt;- data.frame(suit = factor(rep(1:4, 12), labels = c('Heart', 'Diamond', 'Club', 'Spade')), card = rep(2:13, each = 4)) cards[sample(1:nrow(cards), 2) , ]
https://stat545.com/bit001_dplyr-cheatsheet.html
The figures in [this chapter](https://r4ds.had.co.nz/relational-data.html#understanding-joins) of R for Data Science might help you better understand joins, and will show you the syntax if you're planning on using `dplyr`. If you want compare all the data from your second .csv file `df2` with the entries in your first .csv file `df1` which share the same company name, this would look something like: library(dplyr) df2 %&gt;% left_join(df1, by = "company_name") Not entirely sure what you mean by cluster ID. If you need a unique ID for each entry which share a company name, doesn't the company_name column already fulfil that purpose?
Thanks. I will utilise this. And, any thoughts about assigning cluster ID to exact match records ? Imagine, I’ve superhero CaptainAmerica repeats 10 times. How can I cluster or tokenise them into one unique ID ? This workflow would help me to identify records by cluster ID, filter them &amp; remove duplicates
Imagine, there are repeated company names. How can we capture them or group them by ?
In my opinion, you don't need unique IDs for that. The company_name column should already allow you to do anything you want to do. For instance, if you want to have a look at a specific company: df2 %&gt;% filter(company_name == "abc") Or if you want to consider each company as an independent group, to calculate average profit for instance: df2 %&gt;% group_by(company_name) %&gt;% summarise(profit = mean(profit))
https://stackoverflow.com/questions/1299871/how-to-join-merge-data-frames-inner-outer-left-right#1300618
Thanks. I will definitely utilise this.
I just noticed I accidentally linked to a specific answer, but all of the top answers to that question are really good.
Group by in dplyr? You could also something like Dataset[column == ‘Captain America’] &lt;- cluster.id And repeat for each value. A custom function will help this go more quickly.
Well.... excellent post. Also, is there a way to identify partial match company name &amp; perform join ? Example: ABC Corporation would identify ABC Corp as its match &amp; perform join. Any thoughts ? Appreciated
Hmmm.... I’ve an ample amount of company names so this work flow may not work. Can we perform unique function with multiple columns ?
I’d so some research into the “apply” family of functions.
The best solution would be to have a lookup table for all abbreviations or variations, so like one row would be "ABC Corp" in one column and "ABC Corporation" in another, and maybe a row for "ABC INC." n one column and "ABC Corporation" in another. You would then to use the lookup table as an intermediate table in joins. Of course, that's not always possible if you have a ton of variations. When making a lookup table isn't feasible, the fuzzyjoin package will let you join based on a function of your choosing (as opposed to just using the == function). https://cran.r-project.org/web/packages/fuzzyjoin/fuzzyjoin.pdf http://varianceexplained.org/fuzzyjoin/
While what? If it is while day does not equal Friday you could do it like the below code. &amp;#x200B; dias = c("Lunes", "Martes","Miercoles", "Jueves", "Viernes") x &lt;- 1 &amp;#x200B; while (dias\[x\] != "Viernes") { print(dias\[x\]) x &lt;- x+1 }
sorry if i cant explain properly, but i struggle with both R and english haha. Thanks for your answer. What i meant is that i', asked to achieve the same thing i did with the "for" function, but using the "while" function. I have to get to the same result.
The while loop is a common programming pattern. There is a condition evaluated at the start of the loop and if its true you run through the loop, if it evaluates false then the loop is terminated and you move on. As the programmer you need to add logic so you iterate through all the days. You can use an iterator variable: i (it increases one at a time with each loop) Then you just need to tell the while loop when to stop. You could do this based on the length of an object: dias_list &lt;- c("Lunes", "Martes","Miercoles", "Jueves", "Viernes") number_of_dias &lt;- length(dias_list) i &lt;- 1 while ( i &lt;= number_of_dias ) { print(dias_list[i]) i &lt;- i + 1 }
&gt;dias\_list &lt;- c("Lunes", "Martes","Miercoles", "Jueves", "Viernes") number\_of\_dias &lt;- length(dias\_list) i &lt;- 1 while ( i &lt;= number\_of\_dias ) { print(dias\_list\[i\]) i &lt;- i + 1 } thank you so much!!!!!
A few possibly obvious questions: Is the rmd load doing the same thing as the script load, i.e. literal load and nothing more? Including libraries. What are the source file types? Are the files hosted/stored in a location where you would not expect variance due to e.g. network performance?
The Rmd is loading libraries, the data, and executing various functions. Source files are .txt, using read\_tsv (tidyverse) to read them in. It is on a remove server, however, I did do a test shortly after the quick document was rendered to see if perhaps fewer people were processing things on the server at that time. I loaded a single file into the environment and it took nearly as long to load the one file as it took the entire Rmd to knit. Hope I was able to answer your questions... Still somewhat new to R.
The single file test is also loaded using the same function (tidyverse::read_tsv)? If so, and nothing else is different, I am not sure why you would see such significant differences. Without looking at the code, the only thing I can suggest is to isolate variables. Is the rmd loading the file using a bare call to read_tsv? If you don't knit the document, but run the chunks up to and including the load, can you verify it is loading the same data? Also consider a comparison using a local copy of a single file to 100% eliminate network / server load as variables.
Do you have knitr cache enabled?
One of my stats courses used [OpenIntro statistics](https://www.openintro.org/stat/index.php) for the course. It's open source - aka free - plus it has [labs in markdown.](https://www.openintro.org/stat/labs.php) Also, they have [additional resources available for teachers](https://www.openintro.org/stat/teachers.php). I liked the book and loved that it was free online and really cheap in print!!!
This is more of a intro to stats (using R for labs) than an intro to R thing. So, if this is not for an intro stats course, it might help but likely wont introduce that much of the programming language.
You could use some modular division. Maybe something like this, &amp;#x200B; x=c(floor(b/60), b%%60) paste(x\[1\],x\[2\],sep=":") and have it loop through your values. I'm new to R, hope this helps.
time objects in R are usually stored as dates along with times in R so I'm not aware of any packages that will help you with that. I would use modular arithmetic as /u/swolepatrole suggests.
Take a look at lubridate "durations" (vs intervals or periods).
A few things that maybe might help if possible. 1. What are you doing with these large `.tsv` datasets? Do you load them, do some data preparation, and then run visualizations or models on them? If so, you could do whatever data preparation beforehand in a separate R script, write the summaries to file, then have your Rmd load those. If you can do this, this will save you a huge amount of time. Your Rmd would just want to load the smallest summaries, models, etc. and do the presentation output part. Then, when you need to knit, you're just updating the presentation output, and you can run the larger data processing parts in other R script(s) when you need to. 2. When reading in large datasets, reading in data into R in binary format will be much faster. Every time you read in a .txt file via `readr::read_tsv` you have to parse the entire file and map it a location in memory. A simple easy speedup would be to in a separate R script, read in the .txt files and write out a serialized form of the data using the `saveRDS` or `readr::write_rds` functions. You could also compare the serialized .rds formats to just using the `data.table::fread` function, a much faster replacement for `read.table`.
Why do you add \`(1 | Subj\_ID)\` within the R function? &amp;#x200B; To format your code, switch to markdown and prepend \` \` to every line of code.
It looks like it's something to do with the time-series aspect of it and follows the pattern here: https://stackoverflow.com/questions/35628204/incorporating-time-series-into-a-mixed-effects-model-in-r-using-lme4 I was going to suggest that maybe the time data needs to be made into a time-series object, and after searching found the link above . This one shows the same pattern https://stats.stackexchange.com/questions/71087/analysis-of-a-time-series-with-a-fixed-and-random-factor-in-r , but one of the answers recommends using nlme (or lme4 if autocorrelation is not an issue).
👆perhaps?
You are correct. Looks like you did not specify time as a random effect in your r mixed model. Try + (Ztime | Sibj_ID) instead. It will include intercept by default (I am 95% sure), check your output to make sure. If that doesn’t work, let me know!
Thanks for you help and yeah more info on getting my stuff from spss to R would be most welcome! I've been working in spss for years but switched to R because it's more versatile. Getting all of my work ported between the software packages has been a challenge though
https://r4ds.had.co.nz/dates-and-times.html using the `lubridate` package. As the other post mentions, use modular division (search `%/%` and `%%`).
You could use `mapply`and pass your **Row** and **Col** columns as vectors to it. Let me know if you want more of an explanation!
The `[` accessor can simply accept an N-by-2 matrix and return a vector of element values. Read ?`[`
As others have mentioned, modular division will work. However the easiest way is to add the numeric to the midnight timestamp. For example “2019-03-18 00:00:00” + num_seconds will produce a timestamp. This should be really easy to use as a label. Check out scale_x_date() in ggplot for some ideas.
I was able to make it run by changing the data to a data frame instead of tibble and setting minCell = 1: condif &lt;- lordif(as.data.frame(Resp), Condition, criterion = "Chisqr", alpha = 0.05, minCell = 1) But that produced this error: Iteration: 500, Log-Lik: -101.335, Max-Change: 0.00098 EM cycles terminated after 500 iterations. ERROR: The following items had negative slope parameters. 5ERROR: The following items had negative slope parameters (5). Error in probgrm(theta, DISC[i], CB[i, ]) : slope is missing or negative Afraid I can't make heads or tails of that.
I have no experience with this, but maybe this gives some insight. Maybe also try the example in the function documentation as a positive control? &amp;#x200B; library(lordif) Anx_adj &lt;- structure(list( condition = c("T2", "CO", "T1", "T2", "T1", "T2", "CO", "T1", "CO", "CO", "T1", "T2", "T2", "T1", "T1", "T2", "CO", "T1", "T2", "CO"), R1 = c(2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 2, 1, 3, 2, 2, 2, 2, 1, 1), R2 = c(3, 2, 1, 1, 1, 1, 1, 1, 3, 2, 3, 1, 2, 1, 1, 2, 1, 2, 3, 2), R3 = c(1, 1, 1, 1, 3, 3, 3, 1, 1, 3, 3, 2, 1, 3, 3, 3, 3, 3, 3, 3), R4 = c(3, 1, 1, 1, 2, 3, 1, 1, 3, 2, 3, 1, 3, 1, 2, 3, 3, 1, 1, 3), R5 = c(2, 1, 1, 1, 3, 1, 1, 1, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 2), R6 = c(3, 1, 1, 1, 1, 1, 1, 1, 1, 2, 3, 2, 3, 1, 3, 3, 3, 3, 1, 1), R7 = c(3, 3, 1, 1, 2, 3, 1, 1, 2, 3, 3, 3, 3, 1, 3, 3, 2, 3, 1, 1), R8 = c(3, 2, 1, 1, 3, 1, 1, 3, 1, 3, 1, 1, 1, 3, 1, 1, 3, 3, 1, 3)), row.names = c(NA, -20L), class = c("data.frame")) # this fails lordif(Anx_adj[, 2:9], Anx_adj$condition) # Error in collapse(resp.data[, selection[i]], group, minCell) : # items must have at least two valid response categories with 5 or more cases. # Try with more cases? ---------------------------------------------------- # first confirm you at least get the same error with a sample of same rows as # original set.seed(10) samps_rep_20 &lt;- replicate(20, sample(1:3, 8, replace = TRUE)) mat_20 &lt;- t(samps_rep_20) group_20 &lt;- rep(Anx_adj$condition, length.out = nrow(mat_20)) # this at least runs, but with a different error message lordif(mat_20, group_20) # Error in collapse(resp.data[, selection[i]], group, minCell) : # items must have at least two valid response categories with 5 or more cases. # let's try with more cases set.seed(10) samps_rep_200 &lt;- replicate(200, sample(1:3, 8, replace = TRUE)) mat_200 &lt;- t(samps_rep_200) group_200 &lt;- rep(Anx_adj$condition, length.out = nrow(mat_200)) lordif(mat_200, group_200) # at least we get a different error message? # Iteration: 25, Log-Lik: -1743.742, Max-Change: 0.00008 # ERROR: The following items had negative slope parameters. # 2,4ERROR: The following items had negative slope parameters (2,4). # # Error in probgrm(theta, DISC[i], CB[i, ]) : slope is missing or negative
Ah thanks! Didn't realize data was not a data frame. Hurdle crossed, new hurdle introduced :).
Quick question, once the script gets going, is there any way to stop it once it gets trucking? Because the cursor selects another window.
Check out which.max function.
It's a `table()` error...This is due to the fact you are creating a tibble instead of a base::vector, which is due to how your assignment of class to your data. 1. So drop `class = c("tbl_df", "tbl", "data.frame")` 2. You have 3 levels of conditions you need just 2 (i.e., `condition = c(rep(0,10), rep(1,10))`) 3. Also your `minCel = 5` will cause an error because you don't have enough cases at each level of response (you need more data)
You’re close. `rbinom(n=1, size=10, prob=0.5)` gives you the total number of heads observed by 1 person who flipped a fair coin 10 times. Hopefully that is a good enough clue to help you figure it out.
ah OK excellent so then I think rbnom(n=10, size=5, prob=0.5) should do the trick. Is the value for each one of the cells then out of the 5 flips, how many are successful = 1? So if the value of a cell is 5, then one can interpret it as 5/5 flips were heads?
Yup, that is the right interpretation!
OK great thank you so much. And if I may ask, what would be the easiest way to turn this into a function to assess what happens when I change the probability value? I want to be able to sum up the values across the different cases under different probabilities, but I do not know the best way to turn it into a function
What I always do when figuring out how to write a function is to first write the code for a single case — like here, write out everything you need to do for prob=0.5, from start to finish. Then, copy and paste that code, and replace the parameter you want to vary with some variable (say `p`). Then wrap the whole thing in a function statement. So, like, your code for prob=0.5 might look like tmp &lt;- replicate(1000, rbinom(n=10, size=5, prob=0.5)) #other code as needed, summarizing tmp in whatever way you need it summarized #ultimately you will get whatever you want as the final output for prob=0.5 — call it final_output Then you want to vary the `prob` argument. So set `prob=p` and make the whole thing a function of `p`: myfun &lt;- function(p) { tmp &lt;- replicate(1000, rbinom(n=10, size=5, prob=p)) #other code as needed, summarizing tmp in whatever way you need it summarized #ultimately you will get whatever you want as the final output for prob=p — call it final_output #then return what you want to return: return(final_output) } Then you can call `myfun` for each value of `p` that you want to check.
What happens if there are ties for the max value in ColA? Do you expect that the ID will always describe the same value, or is an arbitrary running index of some kind?
hm...something to think about
If there's more than one instance of the max value, the above codes print all instances where ColA = max ColA value
In R a data frame can have a column that is a list, which is essentially a less structured data frame. I think that is what you would need here. Here is my solution that seems to do the trick: a &lt;- data.frame( name = c("Cat", "Lizard"), type = c("Mammal", "Reptile"), a1 = c("meowing", "climbing"), a2 = c("sleeping", "changing colour"), a3 = c("eating", "eating"), a4 = c("playing", "laying eggs") ) b &lt;- data.frame( name = c("Coffee", "Cake"), type = c("drink", "food"), b1 = c("caffeine", "flour"), b2 = c("water", "eggs"), b3 = c("beans", "water") ) send_all_but_first_2_to_rest &lt;- function(df) { nc &lt;- ncol(df) df$rest &lt;- apply(df, 1, function(r) list(r[-(1:2)])) df[, -(3:nc)] } rbind( send_all_but_first_2_to_rest(a), send_all_but_first_2_to_rest(b) )
use rvest instead
Try `test &lt;- content(resp, as = "text")` instead?
Load up rvest webpage &lt;-read_html(‘enter your url here’) data &lt;- html_nodes(webpage, ‘enter css selectors here’)
There is `row_number()` from `dplyr`. result &lt;- teams %&gt;% transmute("Position" = Position, "responsibilities" = list(team[row_number(), 2:ncol(team)])) Does that work?
Next time, please give us the sample frame and function to work with. See `dput`. ``` library(data.table) # function declarations ==== greatCircleLat &lt;- function(lat,lon){ if(is.na(lat) | is.na(lon)){ lat } else { lat*100 # I don't know what your algo is } } greatCircleLon &lt;- function(lat,lon){ if(is.na(lat) | is.na(lon)){ lon } else { lon * 100 # I don't know what your algo is } } # data instantiation ==== dt &lt;- data.table( Lat1 = c(19.0506727,58.8721035,33.0233529,54.8721035), Lon1 = c(-3.9991256,-1.4004125,-5.0988341,-4.4004125), Lat2 = c(92.7133187,54.746791,NA,NA), Lon2 = c(-6.7593169,-4.47984,NA,NA)) df &lt;- as.data.frame(dt) # using data.table dt[ , F_Lat := Vectorize(greatCircleLat, c('lat','lon'))(Lat1,Lon1)] dt[ , F_Lat := Vectorize(greatCircleLat, c('lat','lon'))(Lat2,Lon2)] # using data.frame df$F_Lat &lt;- Vectorize(greatCircleLat, c('lat','lon'))(df$Lat1,df$Lon1) df$F_Lon &lt;- Vectorize(greatCircleLat, c('lat','lon'))(df$Lat2,df$Lon2) ```
Try tibble::enframe() then you can retrieve column names using the colname function
How do you interpret the 5th element of Type in relation to the Year and Group values? Does it correspond with all of them? Does it correspond with the 5th Year value and not to any Type value?
&gt;dplyr Damn, no, I can use the function, like "row"=row\_number() but I still can't cut that dataframe to be the nth row rather than the whole frame
I can’t even read how many lines the code has
I am using the geosphere::gcIntermediate function for the great circle center point determination. One of my major issues was that only certain rows will need the center point analysis and those with empty Lat2/Lon2 should just populate the Final\_Lat &amp; Final\_Lon values with Lat1 &amp; Lon 1.
For your first question: look into a technique called *rejection sampling*, aka *acceptance-rejection sampling*. Or, if you can find an expression for the inverse CDF of your distribution, look into *inverse transform sampling*, which is computationally more efficient but requires a bit of calculus and algebra legwork, first.
The density belongs to a Gamma distribution.
Well, can you see where they list the code about facet_wrap? If you remove that, what would happen? Let us know. And then, if you're thinking, well, now there are only 3 lines but I want six, consider your factor(persons). Currently, there are how many levels in that factor? What kind of factor would we want to generate that could *double* the amount of factors in person, one for each level of camper?
Hey thanks for the speedy response! If you remove the facet wrap line then the chart becomes one but the data is all messed up -- there are still only four lines, so I think a few changes need to be made elsewhere in the code. To clarify I only want four lines in the chart; I want the model to be if the populations were merged (I don't care whether they were camping or not).
You might be able to try something like: ``` bad_rows = which(df$A == 'red' &amp; df$A == df$B) if(length(bad_rows) &gt; 0) { df = df[-bad_rows,] } ```
What is this "best" metric you speak of? Fastest in general? Simplest for interactive use? Fastest when the data are sorted? Most re-usable when programming? Least likely to cause your instructor to think you cheated in answering a homework problem?
Fastest, sorry - should have clarified. My dataframe has a couple million rows
http://adv-r.had.co.nz/Subsetting.html Look at the data frame section, they show you how to select rows from a data frame using a boolean expression. That's the first part, now you just need to create a boolean expression that satisfies "colA has the same value as colB"
Thanks I'm trying 'which' now
Ohh, forgot the Hadley approach (which is quite readable and easy to code: Thank you Hadley): ``` df = df %&gt;% filter(!(A == 'red', A == B)) ``` I did not try this, but it would be something close to this.
I have started my timer to see how long it takes somebody to now post the obligatory `data.table` approach. :)
Using data.table I'd just do this: library(data.table) a &lt;- data.table('v1' = c('red','blue','green'), 'v2' = c('green','blue','green'), 'v3' = seq(1,3)) &gt; a v1 v2 v3 1: red green 1 2: blue blue 2 3: green green 3 a &lt;- a[ v1 != v2 ] &gt; a v1 v2 v3 1: red green 1
If I were you I'd delete the actual code that shows the answer. The OP reeks of "do my hw please". I just wanted to give OP a push in the right direction without telling them exactly what to do.
Humm. I did not read it that way. I do agree that this would be a good homework problem though. I, along with my other colleagues run into things like this everyday and I could see having a discussion of how to address this issue with them. So, it is unclear if that is what the OP is doing.
I don't know this package, but I believe you also have to then remove the camper variable when making the regression model with the zeroinfl() function (in both places), i.e. m1 &lt;- zeroinfl(count ~ child | persons, data = zinb, dist = "negbin", EM = TRUE) Otherwise you have two predictions for each child number when you remove the facet_wrap.
`renderTable` and its associated `tableOutput` are used for displaying a data frame in your app. It sounds like you just need to create a reactive version of your data, but filtered and grouped using `input$country` and `input$facet`. Then use this reactive version of your data later in your ggplot. Does that give you enough of a hint? Also, [RStudio's shiny tutorial](https://shiny.rstudio.com/tutorial/) is a great resource for starting to learn shiny.
Not quite enough. I tried the following, but it's stil not working, as the reactive data filtering is not filtering as soon as an input is changed, as I thought it would and should. &amp;#x200B; aux_data &lt;- reactive({ shinydata %&gt;% filter(country == input$country) %&gt;% group_by(year) }) output$plot &lt;- renderPlot({ ggplot(aux_data(), aes(y = suicides.100k.pop, x = year, colour = input$colour)) + geom_point() + facet_wrap(~input$facet) }) What's wrong with that?
I think you've correctly created the reactive data frame. There might be something else wrong in a different part of the app. Do you actually get a plot to render? Because `input$colour` and `input$facet` are strings I had do some things differently in my ggplot code. Checkout my minimal example and see if you can spot where you might have an error. library(tidyverse) library(shiny) shinydata &lt;- tibble( country = sample(c("England", "France", "Italy"), 500, TRUE), year = sample(2000:2010, 500, TRUE), suicides.100k.pop = abs(rnorm(500)), gender = sample(c("M", "F", "O"), 500, TRUE), area = sample(c("Urban", "Rural"), 500, TRUE) ) ui &lt;- fluidPage( selectInput("country", "Country filter", choices = unique(shinydata$country)), selectInput("colour", "Color by", choices = colnames(shinydata), selected = "gender"), selectInput("facet", "Facet by", choices = colnames(shinydata), selected = "area"), plotOutput("plot") ) server &lt;- function(input, output, session) { aux_data &lt;- reactive({ shinydata %&gt;% filter(country == input$country) }) output$plot &lt;- renderPlot({ aux_data() %&gt;% ggplot(aes_string("suicides.100k.pop", x = "year", colour = input$colour)) + geom_point() + facet_wrap(as.formula(paste("~", input$facet))) }) } shinyApp(ui, server)
I needed to quote the input arguments and used this %&gt;% before the ggplot. Thank you.
No problem! Just for your understanding, your error probably did not lie in not using `%&gt;%` before `ggplot(...)`, it must have been the other thing. All `%&gt;%` does is put the thing on the left of it into the first argument of the thing on the right. It is equivalent to writing `ggplot(aux_data(), ...)`.
You could split your data into two different datasets ahead of time. Then you could call two geom_line() functions with the same ggplot, but specifying both datasets inside of the geom_line(). The downside of this is that I can be tedious to specify all of the components in one legend.
Going to bed now, but I will tackle this in the morning if you still need help. I just built an NB classifier that I plan to upload as an example soon.
You error is saying that each vector needs the same factors. You can see what factors you have by running: `levels(nb_train_predict)` `levels(test$ME)` You probably want to set train$ME to be a factor before fitting a model, ie `train$ME = factor(train$ME)` You will have to do this for the test data as well when you try to generate the confusion matrix. Given that you said: &gt;#table(nb\_train\_predict) &gt; &gt;#the above line outputs &gt; &gt;#&gt; table(nb\_train\_predict) &gt; &gt;#&lt; table of extent 0 &gt; It looks like you have not predicted anything. Whats the value of data.nb?
I would take a closer look at how you specified the model in the ‘lm’ function.
 mod &lt;- lm(bleh~blah, data = Merged_datasets, na.action = "na.rm") abline(mod)
It still comes up that "only using the first two of 163 regression coefficients"
What does `str(Merged_Datasets)` return?
Omg thank you so much, forgot to change it to num thanks dude.
I figured that was the issue. Calling `lm()` on categorical data converts each unique category into its own predictor, 163 in your case...
How have you run your GLM? Paste your code and it may be obvious to others how to do this, as many returned objects have methods that allow you to get additional statistics. See for example [R: Analysis of Deviance for Generalized Linear Model Fits](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/anova.glm.html "R: Analysis of Deviance for Generalized Linear Model Fits")
fit2 &lt;- glm(gmean \~ Species \* NaCl, data = finalday, family = binomial, weights = seedtotal) summary(fit2) Anova(fit2)
Well if you want the F-statistics read the page I've linked you, you want to specify the `test="F"` in your call to `anova()`, i.e. anova(fit2, test="F") You can also use [Markdown](https://www.reddit.com/wiki/markdown) when posting on Reddit, and in so doing you can indent your code by four lines to get it be rendered... fit2 &lt;- glm(gmean ~ Species * NaCl, data = finalday, family = binomial, weights = seedtotal) summary(fit2) Anova(fit2) You could also post output as no one else reading it will have your data, you might find some have the time to make useful comments on that.
Appreciate the help !
data.nb is supposed to be the niave bayes for naiveBayes(ME ~ ., data = train) but since this is the first time ive tried this I'm not certain if I'm doing that right.
Errors :/
Take a look at the dplyr package and the tidyverse in general. Based on what you're describing, I don't see any reason that you couldn't read in your csv as a dataframe and then use `filter()` to subset what you want using logical operators. Chapter 5 of [R for Data Science](https://r4ds.had.co.nz/) is a good introduction to dplyr and should get you started.
With only a couple million rows, most of the methods should take about the same amount of time. You can try several and time them and decide if that delay matters to you.
Functions in R return as output their last line. So I think your function is returning the output of `closeAllConnections()` which is `NULL`. Then when you do `summary()` of this function's output you are basically running `summary(NULL)`. From what I can tell your function is designed to write plots to an output file. If you want it to also return the `islands_sp` object you can add `return(islands_sp)` as the final line of your function. Let me know if I can explain further!
Hi! Thank you for your reply. I tried moving around the closeAllConnections() and tried to return(islands\_sp) but my code ended up with an error saying that there was *no function to return from, jumping to top level.*
I think I'm understanding the problem correctly, but let me know if not. You could grab the index of the first and last observations with a peak value and then subset the rows of your data using a sequence: start &lt;- min(which(data$position == 5)) end &lt;- max(which(data$position == 5)) data5 &lt;- data[start:end, ] plot(data5, type = "l") # or maybe: data5 &lt;- data[(start - 1):(end + 1), ] plot(data5, type = "l") Does this seem to work?
Oops, sorry `islands_sp` only exists in the function used in your `lapply`. Maybe you need to return `list_of_Polygons`? I guess I'm not sure what exactly you want the function to return, but the point is whatever you think you are taking `summary` of should be the thing your function returns as output.
That'd be an advanced use of filtering in my opinion. I'm pretty sure what OP wants is two things: 1) detection of inflection points and 2) subsetting data after detection. OP, my general approach to detect those changes is to do two rolling diffs. So if we have sawtooths like your plot: x = c(1,2,3,4,3,2,1,2,3,4,3,2,1,2,3,4,5,6,7,6,5,4,3,2,1) diff2x = diff(diff(x)) Then where diff2x &gt; 0 + 1 is where the internal valleys are valleys = which(diff2x &gt; 0) + 1 How we use this info depends on what sort of data you want your "sets" to be. I envision a for loop that goes through 1:(length(valleys)+1), grabs the appropriate data, and saves it to a csv named by the loop index, if that's what you want. Of course, the valley detection above only works for smooth data. Any noise breaks it.
To answer op's original question you can use group_by() then split() but I think /r/MercuryPitchforks' idea is easier to learn if you've never worked with grouped data.
I really appreciate it! This is my first time working with the SpatialPolygons so I needed some guidance.
I don't believe your problem is directly related to using `SpatialPolygons`, if it is I won't be much help as I have never used it. So are the last 3 lines of your function: dev.off() closeAllConnections() return(islands_sp) ?
yes, those are the last 3 lines.
Thanks everyone. Here's how I got the indices. Now I just have to split my dataframe with those indices. It's not pretty: n=1 indices=rep(0,100) end=length(Position...mm)-1 for (i in 1:end){ if (Position...mm[i]&lt;0.1 &amp; Position...mm[i+1]&gt;0.1){ indices[n]=i n=n+1 } } indices=indices[indices&gt;0]
Two options: ``` data &lt;- data %&gt;% group_by(decile) %&gt;% summarize(average = mean(value) / mean(data$value)) ``` ``` data &lt;- data %&gt;% mutate(valuemean = mean(value)) %&gt;% group_by(decile) %&gt;% summarize(average = mean(value) / valuemean) ```
Thanks for the reply, doing it like that would have been better than my solution. I'll keep it in mind for next time.
This script works for me. See if you can spot maybe where yours could have the problem. Sorry for rewriting some parts of it, but I think it's safer to define `lines_vector` outside of the function given that has the same name as one of the function inputs. I also yanked out `whichPlot` for simplicity since it seems like you haven't written the other `if` block yet. #load the packages library("sp") library("rgdal") library("maptools") #input data: llCRS &lt;- CRS("+proj=longlat +ellps=WGS84") lines_vector &lt;- MapGen2SL("https://raw.githubusercontent.com/dobriban/spatial-data-with-r/master/auckland_mapgen.dat", llCRS) plotLinesOrPolygons &lt;- function(lines_vector, llCRS, filename = "lines.pdf") { #pulling the list of Lines lns &lt;- slot(lines_vector, "lines") #write on this pdf pdf(file = filename) islands_auck &lt;- sapply(lns,function(x) { crds &lt;- slot(slot(x,"Lines")[[1]],"coords") identical(crds[1,],crds[nrow(crds),]) }) #Polygons islands_sl &lt;- lines_vector[islands_auck] list_of_Lines &lt;- islands_sl@lines list_of_Polygons &lt;- lapply(list_of_Lines,function(x) { # Convert the first (and only) Line in the Lines to a Polygon: single_Line &lt;- x@Lines[[1]]@coords single_Line_ID &lt;- x@ID single_Polygon &lt;- Polygon(coords=single_Line) single_Polygons &lt;- Polygons(list(single_Polygon),ID = single_Line_ID) }) islands_sp &lt;- SpatialPolygons(list_of_Polygons, proj4string=llCRS) #plotting the graph plot(islands_sp, main = "polygons", col = "Red") dev.off() closeAllConnections() return(islands_sp) } summary(plotLinesOrPolygons(lines_vector, llCRS)) Does that do the trick?
Thanks!
Thanks for the reply. If I understand it correctly, I guess the problem is it never is exactly an integer. I got my indices somehow now.
Thanks, I think with the which-function I could have gotten my indices easily.
Wow, yes! It works now. I plan to add more to this and you helped a lot. Thank you so so much!! I was stuck on this and was getting frustrated. I'll try to search why I wasn't getting the correct input.
Well yeah but your table of predicted results is empty so do you have a valid model?
It could be. Also, in the future it's best to post a reproducible example in your questions, meaning that the code you post should be able to be run on any computer regardless of local files. You'll find that the answers you get will be a lot more helpful. Once I found that your data is actually on github it made it a whole lot easier to help you. You're welcome! Happy to have helped!
Glad you got it worked out. For the future, a thing you should keep in mind: Pre-allocating the vector indices is fine if you (1) know the number of peaks and (2) expect a giant vector, BUT if either of those aren't true, it's much simpler to just append it: (before the loop) indices = c() (in the if statement) indices = c(indices, i) This means you don't need your last line or the n variable.
Great, thanks a lot!
what could you possibly gain from a software cert?
I have absolutely no idea, that’s why I thought I’d ask
In my eyes, an actual project, interesting github repo, or other parts of your portfolio would be worth 10x a certification.
 I don't know the nature of your projects but why don't you post the code on github and list your profile url on your Resume? I don't think there is any legit R or Python certification so the best way to prove your skills is to provide examples.
Employers would rather see R projects than certifications. Have you done any (and I mean aaaany) projects in R? If you are just breaking into the language, then it wouldn't hurt to get the certification, but for the most part employers would much rather see "I did X at my old company/school in R" than "I have this R certification of nebulous authenticity/requirements". I always point people to Kaggle because of the kernels (heavy R usage with many good practices). If you aren't into predictive modeling, then Datacamp is probably the best-known R course that employers would accept.
Thanks for answering, I’m not quite sure how I’d build up a portfolio...would you mind elaborating on that? I’m about to start my masters in neuroscience and was asked to learn R, Python and become familiar with MATLAB. I’m not sure how I’d go about finding an actual job (or what that job would be for that matter) if I have no experience and will have to really market whatever I end up doing in the masters. I’m assuming I’ll be given most of the analysis work (alongside an actual person in bioinformatics) and won’t have to deal with the wet lab component too much. Help?
Ahh, I didn’t know that I could do that (I haven’t started anything yet, still learning. Should have something in my lap starting September) Would you happen to know how I’d go about finding an actual job? What would be “entry level”? Is there something that I should be working towards? I know it’s all very vague and relative but I need some direction right now :)
Ooh, good idea for a project, I think I'll try it myself. &amp;#x200B; RemindMe! 5 days
I see! So as long as you describe what you’re doing you should be okay? I don’t have any experience yet (I’ve replied to some of the other comments and I think that shows where I’m at right now, if you’d like to read) I’ve started learning with DataCamp but it seems pretty basic and there’s a lot of hand holding involved. If I practice using Kaggle, would that be something I should tell employers about? (I’m sorry that all this sounds very naive, but I’m not sure where to start or go right now)
Are you using R in your masters program? Maybe aim to use R for your masters thesis? Start with that. Use github to version control your homework and thesis project. Add a simple README.md to explain to visitors what you are doing. Start a blog and write more about what you are doing. The basic principle is to just share with the world the things you create. Create stuff that people are interested in or can use. I think you will be surprised at how much value it will bring to your career. It certainly worked for me.
I will be messaging you on [**2019-04-23 18:15:38 UTC**](http://www.wolframalpha.com/input/?i=2019-04-23 18:15:38 UTC To Local Time) to remind you of [**this link.**](https://www.reddit.com/r/Rlanguage/comments/beopoj/help_creating_a_twitter_botwatcher/el7djzf/) [**CLICK THIS LINK**](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[https://www.reddit.com/r/Rlanguage/comments/beopoj/help_creating_a_twitter_botwatcher/el7djzf/]%0A%0ARemindMe! 5 days) to send a PM to also be reminded and to reduce spam. ^(Parent commenter can ) [^(delete this message to hide from others.)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Delete Comment&amp;message=Delete! ____id____) _____ |[^(FAQs)](http://np.reddit.com/r/RemindMeBot/comments/24duzp/remindmebot_info/)|[^(Custom)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[LINK INSIDE SQUARE BRACKETS else default to FAQs]%0A%0ANOTE: Don't forget to add the time options after the command.%0A%0ARemindMe!)|[^(Your Reminders)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=List Of Reminders&amp;message=MyReminders!)|[^(Feedback)](http://np.reddit.com/message/compose/?to=RemindMeBotWrangler&amp;subject=Feedback)|[^(Code)](https://github.com/SIlver--/remindmebot-reddit)|[^(Browser Extensions)](https://np.reddit.com/r/RemindMeBot/comments/4kldad/remindmebot_extensions/) |-|-|-|-|-|-|
Use crontab to run script (via Rscript) every f.e. 15 minutes. Grab tweets from stream and filter them to last 15 minutes. Then on that sample do the job. Use mailR package for send mail with alert.
Yes, my supervisor needed more people that were interested in analyzing his large sets of data and I’m assuming that’s what my primary responsibility will be. I’m not how I’d incorporate that into my thesis but if I can then I plan on doing it because I’d like to use the degree for bioinformatics and the like. Start a blog? I’ll look into github, does this place also offer some platform for me to blog or does that go somewhere else? “Create stuff” hmm...so whatever I’ll be doing for my masters? Can I ask you about your career, where you started and where you are now? You can message me if you’re not comfortable with this being public (and you don’t have to answer if you’re not comfortable with that) Thank you!
A certification without a substantive github or something similar only emphasizes how little experience you have. If you have experience, your work does the talking. Do you see a theme here?
Yes, after reading all the other comments as well I came to the conclusion that I won’t get a certificate. I’m just trying to figure out where to start and how to profess :)
Just get started on the work for your Masters and you should start to get some inspiration. You'll want to version control your code, so that lends itself to git, and subsequently github. But none of this will make much sense until you get into the actual work.
Is there anything you think I should be mindful of before I start? I just want to be prepared and not make any avoidable mistakes
I'll check out crontab, thanks. I might not use mailR because I can just have the Twitter bot message my irl Twitter account. I plan on sending the twitter bot instructions for it to execute also.
See https://blog.prokulski.science/index.php/2018/03/21/jak-zrobic-twitterowego-bota/ - my blogpost about writting twitter bot in R. Post is in polish, but you should understand the source code. Or just use Google Translator :)
Focus on one of R, Python, or Matlab (in your case). I assume this is your first foray into programming, so your first language is going to take the most time. You not only have to learn the syntax, version control, and data structures, you also have to learn how to think (differently)
Look at what v is again. Also why is your function argument the loop index? I think you want "for i in 1:k".
As somebody that writes R code nearly every day for work, I'd hate to work to see somebody's code like this, but I'm sure you're a beginner so you're learning. Like MTMD36 says, you're not properly looping. However, I would add that using some better variable names can help a lot. I took a shot here: ```values &lt;- rnorm(5000) num_reps &lt;- 100 bs &lt;- function(values, num_reps) { mean_values &lt;- vector('numeric', length = num_reps) for (i in seq_len(num_reps)) { sample_vals &lt;- sample(values, replace = TRUE) mean_values[i] &lt;- mean(sample_vals) } q_limits &lt;- round(quantile(mean_values, probs = c(0.025, 0.975)), 3) paste0( '95% bootstrap interval for the mean: ', q_limits[1], ' to ', q_limits[2] ) } # now test it bs(values, num_reps)```
Enter twice for a line break!
What's the problem? It looks like it rendered as code-formatted just fine for me?
Hmm, it’s coming up for me fine on mobile, was all clumped together when I saw it earlier on pc. Oh well OP’s gone now!
It seems like we are pretty much on the same path. When working on your thesis, try to identify parts of your code that other people can use for their own purposes turn them into packages. This allows you to easily re-use your own functions in different projects, saving you from copy-pasting your own code and allows other people to benefit from your work and something that future employers can see. For instance anything [here](https://github.com/topics/ogan-bio) is something I have used in my project but were useful enough on their own to be their own package. Other than this I have project repositories that uses functions from these packages along with more specific functions that reside in the project repo to replicate my work. In addition to that if you have any hobbies you can use a language of your choice to code for that. I have a growing codebase for dungeons and dragons play for instance.
If you want to treat each month-year combo as it's own factor, you can create a new column that combines the two and make that into the factor. In `tidyverse`, `df %&gt;% mutate(month_year = paste(month, year, sep = "_"), month_year = factor(month_year))` Though, I think letting the month factors repeat would be more typical in regressions. You may want to capture some type of seasonality that will be lost if you lump it with the year, I think.
Thank you! I appreciate your point on the seasonality but the purpose of my regression is due to various minimum wage changes that took effect so I need to separate those from just the effects of each month. Once again thank you very much this worked perfectly!
Check out swirl and/or the RStudio chest sheets.
[This is a really good course](https://www.udemy.com/share/100YzAAkEfc19TTXw=/) . Not free, but super cheap and well done; focused on analytics.
Datacamp can get you started with the super basics, swirl can help too. At some point (might as well do it now) you'll want to pick up R For Data Science by Hadley Wickham, it's basically the Bible for R users.
Thank you! I will definitely keep all this in mind when I start :) can I see this dungeons and dragons codebase?
Find a dataset you like and just start manipulating it. Just google the things that you don't know as you go. I'm convinced that this is the best way to get familiar with R in a hurry.
Thank you, we already have some datasets from the assignment, so I might just start working with those.
Thank you, the book sounds great. Right now I need to learn the super basics so I'll start with the datacamp stuff, then move on. However I will need to check that book out too, it sounds very interesting.
[Yep](https://github.com/topics/ogan-dnd)
On the subject of careers, here's what I care about from an analyst perspective: can you solve problems, figure things out, and do I believe you have genuine interest in continually learning? The best way to quickly assess these characteristics at an initial review (resume) stage is to look at a portfolio. That usually means a repository. It's actually the exception that we have any submissions that include github links. To me, that is a huge plus. Just don't be that person who forks a bunch of repositories with nary a line of original source code solely for resume purposes :)
Thank you :) I’ll keep this in mind. No idea what I should start looking for, I was thinking I’d get a part time/contract job so that I could have more experience but I wonder if those things exist.
This is what i did when i was picking up R. It was super convenient being able to learn R on the terminal and swirl is fairly motivating as well in its feedback.
[https://r4ds.had.co.nz/](https://r4ds.had.co.nz/) a version of the book is available free online.
If you show a specific example it would be easier to help. `...` can be used in many ways so it is often not used consistently. Normally if `...` is used in a function, you would see something like, "arguments that are passed to this other function", in that case you should look at the documentation for that other function. Sometimes `...` is used so that the function can take a variable number of arguments. In that case `...` itself should be documented accordingly. There are a few less standard ways to use `...` but the point is, in all cases documentation may be poor so the best way to understand what something does, apart from the documentation is to either look at the code and see where it goes (you can display the code of a function by calling it without parentheses), or asking around
I agree. Really, the only way to truly know, what does the `...` argument does for sure is to look at the source itself.
you could do some kaggle competitions publish your code and methodology on github
You can use ?function to know the arguments.
What I have recently found myself doing is (even after a year or two) get a book you like and/or TWO computers (if you are learning by web). Set up the book or computer Right next to each other. Set up the book so that you dont need to man handle it every two seconds. I literally follow along line by line as I walk thru the web examples or book ideas. Setting up two computers means you Have to type the commands Not copy/paste. This is my kind of 'Monkey see, monkey do'.
RStudio has recently broken all ties with Datacamp due to a sexual harassment issue.
Save all your work to [www.rpubs.com](https://www.rpubs.com)! The site is part of Rstudio and the ide allows you to do it from your Rstudio. (it's free!) By saving it to there spot you can always call others to look at it too.
Don't pay! Go to Coursera! Cousera has 'financial assistance', in other words, if you honestly tell them why you are broke and can't pay they WILL wave the fees. Read the small print. All you need to do is write up your reasons in their form.
I m specifically asking about ... arguments.
I was reading the example code For dbplyr in it the author uses the copy_to function and passes the time=T argument but looking at "?copy_to" there is no mention of that argument so I have no idea what it does.
Whoa seriously? Didn't know that.
you are taking the absolute value of 0.1, not the values in your matrix. abs(mymatrix) &gt; 0.1 vs mymatrix &gt; abs(0.1)
Try abs(mymatrix) &gt; .1.
Yep that worked! Can't believe I goofed that up. thank you.
This was a dumb mistake on my part. Yes this worked - thanks!
We have all been there - just need a different set of eyes.
Hey, I'm pretty new, but wouldn't it be the case that the since `abs(0.1) = 0.1`, you're effectively trying the same formula? In other words, the second formula isn't taking the absolute value of the number you're testing for, it's taking he absolute value of 0.1.
no worries, dude. I've spent many an hour looking for shit like that. Once ran an entire piece of code trying to maximize an objective function, except that I had put a minus in front. spent way longer than I care to admit looking for that minus...
https://www.r-bloggers.com/r-three-dots-ellipsis/amp/
Thanks but this does not tell me how to look up what those parameters do.
I litterally made that same mistake last week.
ls.str('package:quantmod')
Can you provide example code?
if you look at https://cran.r-project.org/web/packages/dbplyr/vignettes/dbplyr.html toward the bottom the author uses the copy_to function and passes the argument temporary = FALSE to it but ?copy_to does not document that argument so I have no idea what is happening.
that's not a unique aggregate function this is a grouping issue. create a new column where years between 2009-2012 is one group and 2014 is another group and then group by storeid, newcol.
That is what the after variable I created with this is dat$after &lt;- as.numeric(dat$year == 2014) . Poor naming I admit.
I think I solved it by using this. &amp;#x200B; dat2 &lt;- dat %&gt;% group\_by(storeid,after,province,winstore) %&gt;% summarise(value = mean(value))
ah sorry i missed that. if i understand your data correctly then just add all the variables you want in the final output into the group by. since theres no variation in those other columns within storeid it should give you what you need.
I'm not clear on what you mean by average the years' data. Do you want all of those years' data averaged into a single row, or do you just want the number? Also, if you want it averaged into a single row, you need to decide what you want to do to 'average' other fields like the province field. dat.2014 = dat[which[dat$year == 2014],] dat.2009.2012 = dat[which[dat$year == 2009 | dat$year == 2010 | dat$year == 2011 | dat$year == 2012],] Then you have two data frames you can do whatever you want with.
I just remembered I could do that. lol. I feel dumb now. facepalm.
👍
https://dbplyr.tidyverse.org/reference/copy_to.src_sql.html the only place I saw copy_to without the temporary argument listed is in an early version of dbplyr. Perhaps the documentation needs to be updated to reflect this.
In most cases when you want to retain all rows you want a simple `mutate()` instead of a `summarise()`. ``` dat %&gt;% mutate(new_value = ifelse(test = year == 2014, yes = value, no = mean(value)) ```
Yes but this isn't the first time that I've seen something like this happen. IMO quite a few functions in other libraries don't document ellipses so it's often hard to tell what they do and sometimes you don't even know that the argument exists.
It's obvious that he understands the idea of bootstrapping enough such that he can just write a one line function and which would actually take him 30 seconds. He is not using bootstrapping exhaustively just enough for his one application. Could you do the same if the need arose?
Is that what you want? You are now calculating the mean value based on values of storeid, after, province, and winstore. I thought you want the mean by storeid only. &amp;#x200B; I think you want to use mutate not summarize. &amp;#x200B; `dat2 &lt;- dat %&gt;% filter(dat$after == 0) %&gt;% group_by(storeid) %&gt;% mutate(value = mean(value)`
The actual function he's writing is a wrapper around the `cor` function so when he calls the `boot` function on his dataset it'll only return the first row instead of two rows. To avoid creating this extra function you'd have to sunset the data in the `data =` argument.
https://r4ds.had.co.nz/functions.html#dot-dot-dot https://adv-r.hadley.nz/functions.html#fun-dot-dot-dot
Did anyone have the need to speed up the video?
\*subset?
use rsympy
You can use [```uniroot```](https://stat.ethz.ch/R-manual/R-patched/library/stats/html/uniroot.html) to solve it.
The mgcv package has an implementation for ANOVA, so you can make comparisons between models. As I understand it, you should keep/drop terms after such a comparison. I would only include an interaction if covariates of a lesser degree were also included; I think for all intents and purposes, you should treat it as you would a linear regression ( as you are doing). I try to get the gcv score as low as possible...sometimes switching model types (like switching to poisson regression) yields the biggest benefit. Remember, te() and ti() can be used to create tensors. I think ti() is useful for interactions? Like, ti(x,y) + s(x) + s(y) will allow you to see the smoothed effect of x,y together and separately. Also, you can set different basis functions for each smoothing term. It really is a fantastic package with many knobs to turn!
polynom also works!
Newtons method if you’re willing to do some work
So, this is my code so far but im getting this error after I run model=lm(Ytilde\~0+Xtilde) : **Error in model.frame.default(formula = Ytilde \~ 0 + Xtilde, drop.unused.levels = TRUE) :** **variable lengths differ (found for 'Xtilde')** &amp;#x200B; &amp;#x200B; attach(data) as.numeric(as.character(AVGWKLY)) #CHANGES ALL LETTERS TO NA as.numeric(as.character(FEMALE)) as.numeric(as.character(DISABL1)) as.numeric(as.character(DISABL2)) as.numeric(as.character(EDUC)) &amp;#x200B; AVGWKLY = na.omit(AVGWKLY) #REMOVES ALL NA EDUC = na.omit(EDUC) DISABL1 = na.omit(DISABL1) DISABL2 = na.omit(DISABL2) FEMALE = na.omit(FEMALE) &amp;#x200B; n = length(unique(AVGWKLY)) n T= length(unique(YEAR)) T Y=log(AVGWKLY) X = cbind(FEMALE,DISABL1,DISABL2) X &amp;#x200B; demean = function(FEMALE,AVGWKLY) { xtilde = rep(0,length(FEMALE)) # same length for (i in unique(AVGWKLY)) { xtilde\[AVGWKLY==i\] = FEMALE\[AVGWKLY==i\] - mean(FEMALE\[AVGWKLY==i\]) } xtilde } Ytilde = demean(Y,AVGWKLY) Ytilde Xtilde = apply(X,2,demean,AVGWKLY=AVGWKLY) Xtilde model=lm(Ytilde\~0+Xtilde)
is `nrow(xtilde)` the same as `length(AVGWKLY)` Also please format your code by prepending 4 spaces and switching to the markdown editor. :)
attach(data) as.numeric(as.character(AVGWKLY)) #CHANGES ALL LETTERS TO NA as.numeric(as.character(FEMALE)) as.numeric(as.character(DISABL1)) as.numeric(as.character(DISABL2)) as.numeric(as.character(EDUC)) AVGWKLY = na.omit(AVGWKLY) #REMOVES ALL NA EDUC = na.omit(EDUC) DISABL1 = na.omit(DISABL1) DISABL2 = na.omit(DISABL2) FEMALE = na.omit(FEMALE) n = length(unique(AVGWKLY)) n T= length(unique(YEAR)) T Y=log(AVGWKLY) X = cbind(FEMALE,DISABL1,DISABL2) X demean = function(FEMALE,AVGWKLY) { xtilde = rep(0,length(FEMALE)) # same length for (i in unique(AVGWKLY)) { xtilde[AVGWKLY==i] = FEMALE[AVGWKLY==i] - mean(FEMALE[AVGWKLY==i]) } xtilde } Ytilde = demean(Y,AVGWKLY) Ytilde Xtilde = apply(X,2,demean,AVGWKLY=AVGWKLY) Xtilde model=lm(Ytilde~0+Xtilde)
what do you mean by "is nrow(xtilde) the same as length(AVGWKLY)"
Yes they are the same. Just checked. Why do you ask?
Am I missing something? Is this a troll post? x &lt;- 5 What is the general problem you're interested in that's nontrivial? Are you trying to solve systems of linear equations? solve(2, 10) # [1] 5
No, not a system of linear equations. But your way of doing it is simple and to the point! Don't overthink it.
Please format the code, it is really a pain to read in this form.
Because the error implies different sizes either within Xtilde itself or Xtilde and Ytilde
Because the error implies different sizes either within Xtilde itself or Xtilde and Ytilde
It's hard to follow without my PC handy, but I'll try to point you in the right direction. You're error states that Xtilde has a different length, so first step is to check that Xtilde and Ytilde are the same length. It looks like Ytilde is a vector (ie only one column), and Xtilde is a matrix (you applied demean to the columns of X, so your output will have the same number of columns as X). You likely need to apply you lm for each column. I don't think this is what you want though, I imagine you want a model that includes all the variables in Xtilde. For that you will need to specify in your lm formula all of the column vectors, instead of the entire matrix.
and how do i specify in my lm formula all of the column vectors?
 p1 = Northeast / (Northeast + 6782 + 47043) Problem isn't with prop.table; I don't know how to calculate that either.
Wow... you desperately need a better beginner introduction to R than whatever you have had so far... perhaps https://rstudio-education.github.io/hopr/ ? attach(data) - Using attach this way is a terrible strategy for handling data, because it encourages you to think about the columns of your data set separately, when in fact the values in each row need to work together as a combined set of values. - There is clearly something missing here, as `data` is the name of a function in Base R, so I am sure you must have done something to create a new variable called `data` in your workspace. - To avoid confusion with the Base R `data` function, I use a different variable such as `dta` or `DF` (there is also a base R function called `df` and capitalization can distinguish these). as.numeric(as.character(AVGWKLY)) #CHANGES ALL LETTERS TO NA - The comment is only half right... this expression returns such a numeric vector for display at the console, but it does not make any changes to the `AVGWEEKLY` variable that will be accessible in the code that follows. AVGWKLY = na.omit(AVGWKLY) #REMOVES ALL NA EDUC = na.omit(EDUC) - Indeed, if there are any NA values in these columns from when you imported them (but not from your conversion above), those values will be removed by these statements. However, the new versions of `AVGWEEKLY` and `EDUC` may have different lengths. This is a problem because analysis functions like `lm` assume the fifth element in one vector relates to the fifth element in the other vector, but if one of these was removed then you might be lining up the original fifth element in one with the original sixth or seventh element of the other... which is nonsense at best. In any event, if one vector ends up of different length than the other you will get an error later... and you did. Y=log(AVGWKLY) - This suggests that `AVGWKLY` is floating point numeric, though you did not confirm that using as.numeric like you (tried to) do above. for (i in unique(AVGWKLY)) - This suggests that `AVGWKLY` is discrete, like an integer or a categorical variable, which is very different than a floating point numeric. Anyway, if you keep the data in a data frame and remove entire rows when individual elements are `NA` then you will keep the values lined up: # using as.is=TRUE here avoids needing to apply as.character to columns dta &lt;- read.csv( "yourdata.csv", as.is = TRUE ) str( dta ) # you haven't given us the data, but you can learn what you imprted dta$FEMALE &lt;- as.numeric( dta$FEMALE ) dta$DISABL1 &lt;- as.numeric( dta$DISABL1 ) dta$DISABL2 &lt;- as.numeric( dta$DISABL2 ) dta$EDUC &lt;- as.numeric( dta$EDUC ) dta2 &lt;- dta[ complete.cases( dta ), ] dta2$Y &lt;- log( dta2$AVGWKLY ) # function to subtract means by group fnDemean &lt;- function( v, g ) { ave( v, g, FUN=function( x ) { x - mean( x ) } ) } dta3 &lt;- as.data.frame( apply( dta2[ , -1 ], 2, FUN = fnDemean, g = dta2$AVGWKLY ) ) model &lt;- lm( Y ~ 0 + ., data = dta3 ) However, because you grouped by AVGWKLY and subtracted out the mean of Y on each group, Y is always identically zero. Since I cannot figure out what you really want to accomplish, I will stop here... the error should be handled now... but I will say that from the labels I would have guessed that all of your explanatory variables are normallly treated as categorical (factors), and taking the log of the variable you are also grouping by also makes no sense to me. I hope you have some kind of local statistical expert (teacher?) who you can consult with to get back on track.
In case u/natched's comment isn't clear, you need to remove region from the data frame (you could rename the rows with the region vector instead). The function prop.table expects numeric inputs.
Thanks for posting this - looks like a good round-up of approaches. I tried to work with the report on Thursday, too, but ran into some trouble getting pdftools::pdf\_text() to work. It returned a vector of "\\n"s, one for each page. I actually haven't used pdftools before - has anyone ever run into anything similar? In the end, I used something else to get a plain text version and then read that into R. Took long enough, though, that I haven't had time to return to it -- maybe this week. Would love to try to extract the names and build some sort of coappearance network from the text.
Why do people try to import this particular report into R? Do you import all pdfs into R?
If you use R I don’t see why not.
I can’t tell if you actually have a genuine interest in R, but if you do, this is by no means the first R text analysis/sentiment analysis tutorial to use Trump or Trump-themed data. http://varianceexplained.org/r/trump-tweets/
An ideal job for the `ave` function: infill &lt;- function( b ) { m &lt;- mean( b, na.rm=TRUE ) b[ is.na( b ) ] &lt;- m b } auction2 &lt;- transform( auction, cleanbid = ave( bid, bidder, FUN = infill ) ) The `ave` function is designed to manipulate groups of values without collapsing the answers like `aggregate` does.
I've never used it myself, but the `here` package seems like it might be what you're looking for.
You posted a link about people analyzing the report in R...
Not sure what you are asking... I work in an academic library in a digital scholarship/digital humanities capacity, and I have been trying to hone my skills re: text analysis with R for related reasons. So I was mainly interested in this as a challenge/exploratory exercise -- in other words, can using R to do topic modeling/chart word frequencies/do sentiment analysis raise questions about the report that might not be immediately evident? It's always nice to see others thinking through similar questions.
The way to avoid un-portable use of `setwd` is simply to not use it. That may sound flippant, but I am dead serious. RStudio doesn't set your working directory... it gets set by virtue of how you start the software... by the operating system. I will come back to this below. When you work from a command shell (e.g. `bash` or `cmd`) it is common to use the `cd` command to set your working directory before you set to work on files there. Amazingly, the working directory is a non-issue within the code when you follow this sequence. You just need to reproduce this invocation sequence when you are using a GUI. When you fire up a software development IDE from a GUI file browser using the application icon, it is started with no associated filename argument so any apparent continuity of previous working environment is remembered global to your OS login. On the other hand, if you double-click (or select an "Open" item on a context menu) on a filename then the OS will normally change directory to where that file is before invoking the corresponding program. A good application will run with that configuration... while one that tries to hold your hands too much may disregard that hint from the OS. RStudio takes the hint when you start it with an Rproj file as the file argument. Rgui takes the hint when you start it with an RData file (but beware of objects in RData files that hide base R or contributed package functions). Note that the OS will usually have some kind of "properties" option for how to invoke a program to open a particular type of file. Sometimes these properties will be set to use a fixed directory when invoked... when it has to be pointed at the software installation directory to work that program won't work with the above approach (but I usually migrate away from such software before long). If it is configured to point at your home directory then you can probably clear that setting and the OS will set the current directory to the file's directory before running the program. Having said all that, I don't use VSC but as a quality editor it by default shouldn't mess with your directory unless you play games with your setup. If you just set VSC to open one of the files (README.md?) you keep in your project directory and make all of your R code assume that is the current working directory (even if the R file is not in that directory) then you should be ready to go after you double click the README.md file.
Thanks for the reply! When I call auction2, it is just returning a copy of auction. Am I missing something? Thank you.
I appreciate your very detailed explanation but I'm afraid I don't understand what to do. I get that I'm doing it wrong but how do I do it right? &amp;#x200B; I have the following code that I use to run a series of other scripts. I would like to open one file and then run this code. How do I to that?: `library(rstudioapi)` `project_directory &lt;- dirname(getActiveDocumentContext()$path)` `setwd(project_directory)` `source(file.path('code_set1.R'))` `source(file.path('code_set2.R'))` `outputs_of_function = function_in_code_set1(flag_1=TRUE, other_flag=FALSE)` `function_in_code_set2(some_flag=TRUE,` `various_arguments="values")` &amp;#x200B; `other_function_in_code_set_2(some_other_flag=FALSE,` `various_arguments="values")` Right now in R Studio all I need to do is open this one file and run the first 5 lines to set everything I need, then I can run the functions below it and make it manipulate data, clean it, graph it, do cool stuff to it, but the data itself is in sub-directories, on SQL servers, etc. Right now it all works fine by setting directories like it is shown above but unfortunately that is a stop-gap solution while I'm developing it. Eventually this will probably run headless in Linux.
Stack Overflow
It sounds like you are not using the Project development approach in RStudio. - Open RStudio. Just above the upper right pane should be a pull down menu that says "Project: (None)". - File|New Project - Since you already have a working directory, use the Existing Directory option. Browse to that directory. Do NOT use the home directory "~". - Click "Create Project" An Rproj file will be created, and RStudio will set the working directory to the one you specified. Eliminate all code that uses `setwd` to that location from your R code... I recommend never changing the directory, but if you do always use relative paths to do so. To return to working on that project, double click on the Rproj file. From
I use R once in a while. This article was tweeted by one biology professor who's area of interest has nothing to do with R or CS. This attracted my attention, because his interest seemed to have political underpinnings. And, likewise, that lady who wrote the blog article also chose Muller report out of millions of other pdfs due to its political nature. This is a subconscious way to try to "find something" in it, IMO.
/r/rstats
Just to make sure I understand: 1. You want to do something like find all of the values where bid is NA for someone like pereluzi00 and replace them with the mean of all of pereluzi00's bids? 2. Is there any bidder that has 0 information known about them? Or do we have know at least 1 bid value for every bidder?
Thank you! I didn't know it works that way. I think this will help me a lot.
I can't tell from your description, but it sounds like you are trying to have columns of different lengths. R doesn't allow that. You can however make a list of lists.
There's nothing wrong with choosing one of millions (I mean, are we really talking about the entire extent of existing pdfs, I suspect that would be in the billions?) on the basis of being interested in the content, or am I misunderstanding what you're trying to say or imply?
It's a number of people playing around with a trendy dataset. It's a somewhat interesting data-set. I grant that I would find analyzing the prose of a novel more interesting. But, there are a lot more people that are interested in the Mueller Report. This trend is essentially attempting to use R to decipher what I imagine is a dense read (haven't looked at it). Overall, it's mostly people counting word frequency. As for a biology professor tweeting about it...so what? My professor tweets about political stuff with no scientific ramifications all the time. &gt;This is a subconscious way to try to "find something" in it, IMO. This is how I would describe your post here. It seems to me that these people on twitter are mostly practicing/showing off their R skills with a popular data set. None that I saw express any opinion on the matter one way or the other.
The PDF was not readable text. If you opened it up you probably wouldn’t have been able to select text either. You could have used OCR, but people posted various text versions pretty quick. Tesseract got about 200 pages through before I found a better PDF.
1. Correct 2. If there is no data at all for the selected bidder, I want to remove that row because it contains insufficient data I actually was able to figure this out but thanks for reaching out!!!
This seems like a poor way to represent data for either computation or display. But maybe the following will be close enough to be useful: dta &lt;- read.table( text = "pitch_1 pitch_2 euclid_dist 400010-FF 493247-SI 3.631515 493247-SL 493247-SI 3.125729 349193-FT 493247-SI 4.619143 150037-FF 493247-SI 4.137001 218596-FF 493247-SI 2.536352 218596-SI 493247-SI 1.438356 346800-FF 493247-SI 2.410959 218596-FS 493247-SI 1.915009 346800-FC 493247-SI 3.103196 136602-FF 493247-SI 4.305645 400010-FF 493248-SI 3.631515 493247-SL 493248-SI 3.125729 349193-FT 493248-SI 4.619143 150037-FF 493248-SI 4.137001 218596-FF 493248-SI 2.536352 218596-SI 493248-SI 1.438356 346800-FF 493248-SI 2.410959 218596-FS 493248-SI 1.915009 346800-FC 493248-SI 3.103196 136602-FF 493248-SI 4.305645 ", header = TRUE, as.is=TRUE ) library(dplyr) library(tidyr) library(purrr) dta2 &lt;- nest( dta, -pitch_2 ) sz &lt;- 3 dta3 &lt;- ( dta2 %&gt;% mutate( data = map( data , function( DF ) { DF$id &lt;- ( seq( 0, nrow( DF ) - 1 ) %/% sz ) + 1 DF &lt;- nest( DF, -id ) mutate( DF , txt = map_chr( data, function( DF2 ) { paste( DF2$pitch_1, collapse = ", " ) } ) ) } ) ) ) ( dta3 %&gt;% mutate( data = map( data , select , -data ) ) %&gt;% unnest )
I think its Twitter. Hadley Wickham and #rstat and so on. I kept picking up things from Twitter and fetl overwhelming, which has made me Twitering much less now just to get away from all the \_R\_-related stuff.
How can I do that? Any help with commands, please?
R-bloggers is pretty large, but fragmented obviously.
&gt;The actual function he's writing is a wrapper around the &gt; &gt;cor thank you. What is the meaning of ''wrapper around the cor'' ? &gt;so when he calls the boot function on his dataset it'll only return the first row instead of two rows. Basically the boot function will just make the bootstrap for the first row. So he writes a function to make also the bootstrap on the second row? Sorry but I don't get it... this is a correlation, the bootstrapping should be made among all the possible results of correlation index between the to rows
&gt;Did anyone have the need to speed up the video? Anyhow, I honestly wouldn't do this, as the essence of writing a function is to simplify. The presenter ended up passing corboot as a statistic - this would not be helpful to someone else using the function but creating the need for extra look-ups. ok, but why just not to utilize ''boot'' and that's it?
&gt;why just not to utilize ''boot'' function in R, and that's it? without having to write a new function?
First, create the kk data frame with only the uninsured and insured columns. Then you can name the rows: col.names(kk) &lt;- Region To be clear, you don't *need* to name the rows in order for prop.table to work, but it certainly makes interpretation easier!
Thank you. &gt; It really is a fantastic package with many knobs to turn! It can be quite overwhelming when you're going through all of the options but the manual is good and Simon Wood's book has some really good examples in it so it's manageable.
R-bloggers is just an aggregator of blog posts written elsewhere by other authors, which then slaps lots of ads and popups on poorly-formatted reposts of the articles. There’s no community involved, and I’d rather we didn’t give it even more traffic.
We’re going to have to agree to disagree because I find it a valuable resource to access a wide-ranging mix of news, commentary, and tutorials aimed at all levels.
The coursera R courses are a good place to start. Theres enough resources to help you with the entire program if you get stuck. AFAIK it is free, you just dont get a certificate without paying. From there I would suggest just diving into datasets. See how professionals explore, analyze, model data. Try to replicate it. How fast you get proficient depends on how much you practice, if you have prior programming experince and how well you understand the logics and syntax. Ive been using R on and off for 2 years. Just recently started using it daily for my job. And i still feel like theres sooo much more to learn and question my proficiency.
I also started with Coursera - it was a good basic start. However, they spend alot of time teaching "base R" but in the real world most people use packages to do the work of base functions (ie: tidyvers::filter() instead of subset() ). Datacamp is also a good resource, albeit not free. I began using R almost daily in my job and felt like it took about a year before I felt "proficient". Even now, there are so many things I don't know and there's so much more to learn.
This isn't possible for everyone, honestly the best possible way is to figure out how you can use R in your job/your studies, then do that. I was quite good with Excel and Tableau but I started struggling through tasks in R that would normally be a breeze for me otherwise. If balancing the short-term hit in productivity is a concern, you could try replicating an analysis you'd already done in a tool you're familiar with on your own time. The key with this advice is just to make sure you're doing stuff that's relevant to your day-to-day because it will resonate so much more. Yeah you can tool around with stuff on r/datasets (which is admittedly fun) but then you'll have to figure out how to apply what you've done in a real-world context. Not impossible by any means, but it'll require more motivation and effort. Now, this advice is useless if you just don't have relevant work you can do in R--if you're contemplating a complete career change, for example. In that case, go ahead and audit the Coursera class or something like that but always think about how you can apply what you're learning in other contexts. It's so easy to fall into the "[tutorial treadmill](https://medium.com/@aryanjabbari/learning-to-code-get-off-the-tutorial-treadmill-5a56a564bb04)," so to speak, and you'll find your retention will improve dramatically if you focus on application.
Do you have prior programming experience? &amp;#x200B; [https://swirlstats.com/](https://swirlstats.com/) is good for basics.
Check out https://r4ds.had.co.nz/ https://bookdown.org/robinlovelace/geocompr/ If you have a twitter, definitely ask on #rstats the community is LOVELY and really helpful
https://play.google.com/store/apps/details?id=com.datacamp
&gt; figure out how you can use R in your job/your studies, then do that. Completely agree, there is nothing better than necessity to motivate your learning. I switched jobs last year and have had to learn Python (and to read Java) having used R for a number of years after having consciously switched from Stata and am doing fine, I still search for solutions in the docs of Pandas and Scikit-learn and use Stackoverflow, but I'm learning through doing. If /u/wtrmln88 doesn't have a job/study/project then I'd recommend working their way through [R for Data Science](https://r4ds.had.co.nz/)
Unfortunately many tutors are leaving datacamp in droves at present because of their poor handling of managerial sexual assault (search news for details).
Lots of really helpful replies. Very generous of you. Thank you.
Great response. Really appreciate it. Thank you.
Great response. Really appreciate it. Thank you.
Great response. Really appreciate it. Thank you.
You're welcome. One thing I forgot to mention is that its well worth learning how to use [Git](https://www.git-scm.com/), I've found [Conversational Git](http://blog.anvard.org/conversational-git/) to be useful, but have only really started learning it now I'm working with others and using it collaboratively as intended. Good luck and enjoy the learning experience, its very satisfying when things work.
Yeah, the struggle pays off, especially if you can resist the nagging "I already know how to do this in another way" urge. Also, great suggestion on the R4DS book--definitely support that recommendation.
No sweat! Good luck :)
If you're in social science: http://datascience.tntlab.org/
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/sabermetrics] [How to run a for loop where data is weighted then run it through a function all results output into one new df? \[baseball data related\]](https://www.reddit.com/r/Sabermetrics/comments/bgu0ck/how_to_run_a_for_loop_where_data_is_weighted_then/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
Yes!
Ditto
They resolving all those individual error messages for the dependencies. So try to fix mvtnorm first.
Sort of. I'm pretty sure pred1 * pred2 means pred1 +pred2 + pred1:pred2 (: indicating the interaction of pred1 and pred2). The random slope is correct. I'm pretty sure it's just redundant (like specifying 1 + ...) but I'd have to run a model to check.
For reference, I’m a Python guy, but I’ve messed around with this same kind of stuff. If I understand your question correctly, I can explain how to do it in python and you can try and translate to R. Dataframes (if they’re like Python) don’t like being updated. You can, but it’s prone to errors. Instead, I would store your RE24 in an array. If you still want to reference it like a dataframe, you can define a class that just declares the spots in the array (0, 1, 2...n) as the column name (base_state, runs, etc.). Arrays are much easier to update and iterate over. After you’ve completed your calculation, transform the array into a dataframe and rename the columns appropriately. Dataframe are great for querying and pulling information from. Arrays are better for storing and updating information constantly (and most other data types are better than dataframes for this if you’re more comfortable with something else).
This first sentence is correct... The `1` (intercept) in the fixed effects isn't necessary (though it won't hurt anything to put it in there), and the same is true for `pred1 + pred2`. When you have `pred1*pred2`, that tells R to *also* include the main effects for `pred1`and `pred2`. The only reason you'd need to explicitly put them in there individually is if you used a colon to indicate the interaction as mentioned above. But you then said "the random slope is correct"... Not sure if you meant "the random *intercept* is correct" (which is true), but there is not a random slope.
try to run on an older version of R
Woops. That's my bad. I meant random intercept.
Agree but how?
Ok will inform them in github. how to run older version of R?
go to [rcran](https://cran.r-project.org/) and download + install an older version of your OS, then run the older version
Are you passing the params directly to the render function or are they in your YAML?
Install mvtnorm by itself first
^(Warning in install.packages : package ‘mvtnorm’ is not available (for R version 3.4.4))
Well now research that error message for that package. You probably should update your R as well. I think it's up to 3.5.2 or 3.5.3 now
Both, I have them defined in the YAML as defaults but I also passed them into render to override the defaults.
`sudo apt-get install r-base` ^(Reading package lists... Done) ^(Building dependency tree) ^(Reading state information... Done) ^(r-base is already the newest version (3.4.4-1ubuntu1).) ^(0 to upgrade, 0 to newly install, 0 to remove and 3 not to upgrade.)
You need to update your sources.list after every major update. Check cran for the correct link.
Good job!
On mobile but it’s something like options(scipen=9)
What I would do is define a new discrete variable with say 10 classes. Any ratio from 0 to 10 would be class 1, 11 to 20 would be class 2, etc. Then rather than plotting it with ‘Ratio’ you could plot it with ‘Class’ or whatever you named it.
Try specifying the points fill colour within the aesthetics of your geom\_point command. Also set the gradients fill colour to whatever you need map + geom\_point(aes(x=long,y=lat,fill=Ratio) + scale\_fill\_gradient(low="#green", high="red") &amp;#x200B; If you want to customise your colours further see this tutorial, color brewer has some great 2 color gradients [http://www.learnatschoolschool.com/tutorials/how-to-access-r-color-brewer-directly-from-r/](http://www.learnatschoolschool.com/tutorials/how-to-access-r-color-brewer-directly-from-r/)
If every level is a different color, check to see if the ratio is treated as a factor instead of integer. First you need to convert it to integer before you can apply any of the other suggestions.
First, do check to make sure “Ratio” is a numeric variable, not a factor or character. Using `geom_point(aes(c=long, y=lat, color=Ratio))` should, by default, give you a nice gradient of blues, if Ratio is actually numeric. The fact that it doesn’t give you a gradient makes me think it’s not actually numeric. Once you’ve checked that, look at `scale_color_gradient()` or, for preference, `scale_color_viridis()`.
Remember that there is a difference between the precision that R uses when *printing* numbers and the precision that R uses when *computing* numbers. Print precision can be controlled with the `scipen` and `digits` options. `digits` controls how many significant figures a number is rounded to, and `scipen` controls the threshold before scientific notation is used instead of regular decimal notation. You can also use the `digits` argument to `print()` to specify a one-off value for the option. [https://stat.ethz.ch/R-manual/R-devel/library/base/html/options.html](https://stat.ethz.ch/R-manual/R-devel/library/base/html/options.html) [https://stat.ethz.ch/R-manual/R-devel/library/base/html/print.default.html](https://stat.ethz.ch/R-manual/R-devel/library/base/html/print.default.html) However these options only affect how R displays numbers and not how R computes numbers. Your floating point operations will produce the same mathematical results regardless of the values of these options. Remember that these operations aren't perfectly accurate. Running `0.1 + 0.2 == 0.3` returns `FALSE` and this may catch you out! [https://0.30000000000000004.com/](https://0.30000000000000004.com/)
R uses standard double precision internally. The smallest non-zero normal value (the value does not compromise precision) is 2\^{-1022}. Many systems can actually deal with smaller numbers with lower precision (called [subnormal numbers](https://en.wikipedia.org/wiki/Denormal_number)), and often the smallest value possible is 2\^{-1074} (around 5e-324). &amp;#x200B; The actual issue here seems to be the format of printing out the results as noted by /u/[fang\_xianfu](https://www.reddit.com/user/fang_xianfu).
Take the base 10 log off the p value.
It works on R version 3.5.1 on my windows 10 tho. Is it required older version or newer version of R?
Numeric (specifically float) not integer. But yeah, it's probably this.
Newer, my bad
Well darn, that was all I had to offer
Well thanks for trying, I will try to run with the defaults to see if that works.
probably
Yea. For example, when using `dplyr` you can use backticks to select column names with a space when applying filter or select operations. Single and double quotes are used to denote strings.
If your colum names aren't standard (have spaces or accents) then you can use ```df$\`yey lmáo\```` with backticks. In fact, that's the way the autocomplete does it. Also, in RStudio, syntax highlight won't color backticks text as strings.
If your colum names aren't standard (have spaces or accents) then you can use ```df$\`yey lmáo\```` with backticks. In fact, that's the way the autocomplete does it. Also, in RStudio, syntax highlight won't color backticks text as strings.
Not only when using dplyr. Basically a backtick is used when calling an object that has spaces in its names. U can also use it to create objects with a space in its name for example ``` `a vector` &lt;- c(1,2,3) ``` would be doable.
Single quotes define characters and you can use them anywhere that can take in character inputs. That includes using [ to subset things for instance. Back ticks on the hand is used to encapsulate symbols so you can use them in places where a name/symbol is expected. For instance dplyr functions or calling a list element by $. What backticks allow you to do is to encapsulate complex names that normally would not e valid names. For instance if your list names includes spaces or starts with a number or has other properties that would normally disqualify it from being a name you can write it in back ticks and all will be fine.
A single-quote is essentially deparsed as a double-quote. A backtick is used to refer to inherent symbols or illegal symbols in r. Inherent symbols-- like a plus sign. You can't name an object a plus sign because R already uses it. If you want to modify or refer to \`+\` you need to use backticks. An example: `identical("+",'+')` `[1] TRUE` `identical(\`+\`,'+')` `[1] FALSE` You often see it in packages in the tidyverse (i.e., ggplot, dpylr) given how functions are called. You can also use this to create in-fix functions... a pipe is a great example of this %&gt;%. An example: `\`%plus%\` &lt;- \`+\`` `1 %plus% 1` `[1] 2`
What!! Hah, time to create some ridiculous variable names
Back ticks usually denote column names, for example if you have a column name that has a space in it, and you’re trying to clean it up, you should use `column name` or you’ll end up with a lot of problems
Thank you for your help. After changing my columns into numeric from factors as the others have advised me to, I tried to follow your advice by running map + geom_point(aes(x=LONGITUDE, y=LATITUDE, fill=Ratio) + scale_fill_gradient(low="green",high="red"), data=test3) However, it returns Error in FUN(X[[i]], ...) : object 'lon' not found Do you have any idea on what may be the problem? Thank you for your help.
I did this just today! I used the package VennDiagram and closely followed the example linked [here](https://stackoverflow.com/questions/54363499/venn-diagram-for-non-numeric-entries-to-be-shown-in-subsets). Not sure how many regions you will have, but the hardest part was figuring out how to index v (using v\[\[#\]\]$label) to get certain strings/lists to show in their corresponding regions. Hope that helps
This is awesome thank you!
You can also use emoji
The word you're looking for is permutations (re-orderings of a list). Try `permutations()` from the `gtools` package: permutations(4, 4, c("a", "b", "c", "d")) [,1] [,2] [,3] [,4] [1,] "a" "b" "c" "d" [2,] "a" "b" "d" "c" [3,] "a" "c" "b" "d" [4,] "a" "c" "d" "b" [5,] "a" "d" "b" "c" [6,] "a" "d" "c" "b" [7,] "b" "a" "c" "d" [8,] "b" "a" "d" "c" [9,] "b" "c" "a" "d" [10,] "b" "c" "d" "a" [11,] "b" "d" "a" "c" [12,] "b" "d" "c" "a" [13,] "c" "a" "b" "d" [14,] "c" "a" "d" "b" [15,] "c" "b" "a" "d" [16,] "c" "b" "d" "a" [17,] "c" "d" "a" "b" [18,] "c" "d" "b" "a" [19,] "d" "a" "b" "c" [20,] "d" "a" "c" "b" [21,] "d" "b" "a" "c" [22,] "d" "b" "c" "a" [23,] "d" "c" "a" "b" [24,] "d" "c" "b" "a"
You’re linking to a question that already has a solution posted, and which furthermore doesn’t really seem to match the question in your title. What exactly do you want to know? Be sure to [post a reproducible example](https://reprex.tidyverse.org/articles/reprex-dos-and-donts.html).
I wanted to know how to save the output from that solution to a new data frame. (I know, i should've explained it better in my post.) Thanks for the guide, it'll come handy the next time.
In that case: Just assign it to a new variable before performing subset assignment: dat3 = dat2 dat3$spec[i1] = dat1$spec[match(dat3$Prey[i1], dat1$Prey)]
Thanks a ton for the help! Got it finally.
As the other answers show, **they are fundamentally different**. 1. (Single or double) quotes denote a *character string literal value*. 2. Backticks are used to delimit *variable names*. Unfortunately R muddies the water because it allows us to use character string literal values in some contexts in which it expects a variable name. Just to fuck with us. Consider: "x" = 1 Most people are surprised to learn that (a) this is valid R code, and (b) it creates a variable `x` and assigns the value 1 to it. Obviously this is confusing and there’s no reason to *ever* write this code. Why does R allow it? I have no idea but [the reason is probably “for compatibility with S”](https://twitter.com/WhyDoesR). Unfortunately some people (even good R programmers) actually use this “feature” when working with “strange” identifiers. By “strange” I mean what R calls “non-syntactic names”, i.e. names that contain otherwise invalid characters (spaces, etc.). This is mostly used to work with data column names that contain spaces, or to define operator functions. The *canonical* R way of defining an operator is as follows: `%&gt;&gt;%` = function (a, b) { … } Here backticks are needed to use an otherwise non-syntactic name, `\`%&gt;&gt;%\``. Unfortunately some people use a quoted string instead of names with backticks here, even though [the R documentation discourages this](https://stat.ethz.ch/R-manual/R-devel/library/base/html/Quotes.html). The same rules apply when passing parameter names, including to the functions `c`, `list`, `data.frame`, and to {dplyr} functions. Whenever you see this code such as this … data.frame("foo" = 1 : 10, "bar baz" = 1 : 10) … it’s appropriate to (mentally or actually) replace the quoted strings by variable names, applying backticks as necessary (i.e. *only* around the name that contains a space, not around the syntactically valid name!): data.frame(foo = 1 : 10, `bar baz` = 1 : 10) Given the confusion around this topic, **I strongly urge people not to use strings when they mean identifiers.**
Got it, thanks. Although had to reinstall all the packages, what a pain ...
I think the above answer with `permutations()` is what you are after. But the second part of your question got me thinking about the below solution. If you want to sort in all possible ways you could index the result of `order()` like this. apple &lt;- c(5, -2, 8, 0) rank &lt;- c(4, 3, 1, 2) # largest, second largest, smallest, second smallest rank &lt;- c(4, 1, 3, 2) # largest, smallest, second largest, second smallest o &lt;- order(apple)[rank] apple[o]
Are LONGITUDE and LATITUDE, the names of your coloumns? Or lat and long like you said above?
Disclaimer: I have never specifically tried to put arrows to locate a position so this is my best guess. You will probably need to include the arrows as a separate layer. Check out this [link](http://www.nickeubank.com/wp-content/uploads/2015/10/RGIS3_MakingMaps_part1_mappingVectorData.html#multiple-layers-with-spplot) which provides step by step instructions for adding graphics to your maps.
LONGITUDE and LATITUDE are the column names for dataframe test3. I apologize for the confusion.
This may be helpful as well. Or maybe not. :-) [http://stat405.had.co.nz/ggmap.pdf](http://stat405.had.co.nz/ggmap.pdf)
Use str(data3) to check if it’s numeric maybe?
I kind of remember having this same issue while creating my graph, I’m pretty sure it was something to do with how the data was formatted, eg numeric. I can’t remember exactly and I’m at work, but I’ll have a play around when I get home and see how I fixed it!
If you’re comfortable with some network analysis, Kate provides a great tutorial with some examples that might be helpful for your case: https://kateto.net/network-visualization
When you say "producing something with a distribution with a t-stat of qt(.95,29)" I suspect you're doing something wrong. Without seeing code, it's hard to know how you're drawing your conclusion. Look at a histogram of your random t sample results. Increase your sample size and look at a histogram of that. Then change your degrees of freedom and look at more histograms.
Here is my code as is. generate_t_stats = function(number_of_simulations=5000, sample_size=30, df=5, shift = c(.2,.5,.75,1), alpha = .95) { # universalize it set.seed(1) #initiate an empty vector that will hold all of our t-stats and get top our percentage for the cut-off of our vector t_stat_list = vector() vector_cutoff = round(alpha*number_of_simulations) #make sure that on low numbers of simulations we don't return an empty vector -- useful for testing if (number_of_simulations &lt; 100 &amp;&amp; vector_cutoff &lt; number_of_simulations) { vector_cutoff = vector_cutoff - 1 } #build t_stat_list -- once I can get a sane number out of this I need to add the shifts using the ncp parameter on rt. while(length(t_stat_list)&lt;number_of_simulations) { #generate our random sample generated_t_sample = rt(30,5) #generate a test statistic from generated_t_sample sample_test_stat = sqrt(sample_size)*mean(generated_t_sample)/sd(generated_t_sample) #append our generated test stat to t_stat_list t_stat_list = c(t_stat_list, sample_test_stat) } #sort that list from smallest to largest sorted_t_stat_list = sort(t_stat_list, decreasing = FALSE) #remove everything but the top (1-alpha)% cut_off_sample = tail(sorted_t_stat_list, n = number_of_simulations-vector_cutoff) #this function is a sanity check to make sure our t_stat_list is removing the vectors as expected top_95_stat = quantile(t_stat_list, .95) print(top_95_stat) #initiate a vector that will hold all shifts all_t_stat_lists = c(t_stat_list) #return all of our t_stat lists return(all_t_stat_lists) }
Thank you for this...
So like how many samples are you looking at before making that conclusion? Are you sure you're coding right? set.seed(20190426) x &lt;- rt(5000,5) quantile(x,c(0.05,.95)) # 5% 95% #-1.996466 2.058196 qt(.95,5) #[1] 2.015048 x &lt;- rt(30,5) quantile(x,c(0.05,.95)) # 5% 95% #-2.096501 1.999801 qt(.95,29) #[1] 1.699127
Hrmm... I posted my code in the comment above above (but here is a link just in case you didn't see it https://www.reddit.com/r/Rlanguage/comments/bhqazx/how_does_rtndf_work/eluxlfh/). What I have to do is compare the power of a test of a wilcoxon signed rank test with a t-test using resampling. The steps were outlined as: 1) generate 5000 different samples from a t-distribution where n=30 and df-5. 2) I then need to find the t statistic for all 5000 sample sets (150k values all together). 3) With the 5000 t statistics I am supposed to find the value where 95% of my 5k t-statistics are below that value (I believe this is the quantile function) 4) I am then supposed to compare the Wilcoxon signed rank test stat with this. 5) repeat the test with medians at .2, .5, .7 and 1. (haven't written that part of the code yet since it isn't making sense using the median at 0)
You can do this with the windows task scheduler. This link answers it very good. https://stackoverflow.com/questions/2793389/scheduling-r-script
Thank you for your time. All columns of dataframe test3 is numeric. I was able to produce the plot I want on non-map through: ggplot(test3, aes(x=LONGITUDE, y=LATITUDE, color=Ratio)) + geom_point(size=5) + scale_color_gradient(low="green",high="red") However, map + geom_point(aes(x=LONGITUDE, y=LATITUDE, color=Ratio) + scale_fill_gradient(low="green",high="red"), data=test3) still produced Error in FUN(X[[i]], ...) : object 'lon' not found &amp;#x200B; Which is funny because map + geom_point(aes(x=LONGITUDE, y=LATITUDE), data=test3,color="darkred") produces the plot on map correctly.
https://cran.r-project.org/web/packages/taskscheduleR/readme/README.html
I use task scheduler and batch scripts quite a bit. If there's a better way, I'm open to it, but it is simple.
On a linux you'd use cron jobs for this.
If on linux, use crontabb.
 I'd just like to interject for a moment. What you're referring to as Linux, is in fact, GNU/Linux, or as I've recently taken to calling it, GNU plus Linux. Linux is not an operating system unto itself, but rather another free component of a fully functioning GNU system made useful by the GNU corelibs, shell utilities and vital system components comprising a full OS as defined by POSIX. Many computer users run a modified version of the GNU system every day, without realizing it. Through a peculiar turn of events, the version of GNU which is widely used today is often called "Linux", and many of its users are not aware that it is basically the GNU system, developed by the GNU Project. There really is a Linux, and these people are using it, but it is just a part of the system they use. Linux is the kernel: the program in the system that allocates the machine's resources to the other programs that you run. The kernel is an essential part of an operating system, but useless by itself; it can only function in the context of a complete operating system. Linux is normally used in combination with the GNU operating system: the whole system is basically GNU with Linux added, or GNU/Linux. All the so-called "Linux" distributions are really distributions of GNU/Linux.
Nice!
Oh how I love the GNU/Linux bot
Does your computer have to be turned on for this to work?
Yes, nothing works if it's off.
Yes, of course. Anytime you want to schedule something you basically want to run this code on a separate, non-personal computer. This is usually something we call a server. This server should always be running, ready to start a job once the scheduled time is reached. Learning to get your code from your personal machine on a server in a reliable, consistent way is a great skill to have! To even do this, you'll want to learn a bit about either sftp or git (what I recommend), and how to navigate the operating system of your server. e.g., if your operating system is a headless linux server (meaning command line only), you'll start out slow but learn a lot of great skills.
Link to crontab guru: [https://crontab.guru/](https://crontab.guru/)
I said the exact same thing. Why didn't I get a bot visit?
Check out `gganimate`. Never had a use case so never used it yet myself but as I understand you just dump the data in and plot like you would a regular ggplot then separate different frames like you would separate facets
The scheduleR package makes it easy to create tasks in Windows task manager
There seems to be conflation between ```quantile(rt(30, 5), 0.95)``` matching up with ```qt(0.95, 5)```. The ```rt()``` is producing random data in a simulation. With a sample size of 30 data points, it's less probable to get the 95th percentile that ```qt(0.95, 5)``` produces. Also, as far as your code, you could simulate the data all at once with replicate and then use the apply function to get the t statistic in place of the while loop. Or at the very least change the while loop into a for statement as you know exactly how many times it should loop. Replicate looks like this replicate(number_of_simulations, rt(sample_size, df)) And apply could be something like this: apply(data_set, 2, function (x) sqrt(sample_size) * mean(x) / sd(x)
https://cron-job.org/en/ ?
Thank you so much!
Have you tried your approach? Do you run out of memory? It's asking a lot to have people to download a 223 mb file to make suggestions on a better function. It'd be better to break it down to a simple question with sample data using dput. I'd recommend using data.table. There's almost too much piping. The other thing is that you could change your ```ifelse()``` statements to return ```TRUE``` or ```FALSE``` instead of 0 and 1. Turn this: df &lt;- df %&gt;% dplyr::arrange(game_pk, at_bat_number, pitch_number) %&gt;% dplyr::group_by(game_pk) %&gt;% dplyr::mutate(final_pitch_game = ifelse(pitch_number == max(pitch_number), 1, 0)) %&gt;% dplyr::ungroup() %&gt;% dplyr::group_by(game_pk, at_bat_number, inning_topbot) %&gt;% dplyr::mutate(final_pitch_at_bat = ifelse(pitch_number == max(pitch_number), 1, 0)) %&gt;% dplyr::ungroup() Into: dt &lt;- as.data.table(df) dt[, final_pitch_game := (pitch_number == max(pitch_number)), by = game_pk] dt[, final_pitch_at_bat := (pitch_number == max(pitch_number)), by = .(game_pk, at_bat_number, inning_topbot)] And your next dplyr statement would turn into this: dt[, runs_scored_on_pitch := stringr::str_count(des, "scores")] dt[events == 'home_run', runs_scored_on_pitch := runs_scored_on_pitch + 1] dt[, bat_score_after := bat_score + runs_scored_on_pitch)] setorder(dt, game_pk, at_bat_number, pitch_number) dt[, final_pitch_inning := ((final_pitch_at_bat == TRUE) &amp; (inning_topbot != shift(inning_topbot, 1, type = 'lead')))] dt[is.na(final_pitch_inning), final_pitch_inning := TRUE]
I don't think that data.frames are prone to errors when being updated. The knock against updating data.frames is that it typically copies the memory to make the new one. Arrays and matrices can be faster. But data.frames are more convenient because everything is in one place.
RStudio has a Community forum where this problem may already have been discussed... but if you were going to ask in reddit wouldn't r/rstudio be a better place?
Sorry I didn't know that subreddit. There are too many R related subreddits.
Thanks for the suggestion. I will check out gganimate.
Ok, thanks for posting and sorry for the delay. When you generate your `t_stat_list`, you fill it with t test statistics which were derived from a sample centered at 0 with 30 observations. Therefore, your `t_stat_list` will look like a t distribution centered at 0 with 29 df. Fundamentally, a t distribution is a fat-tailed normal distribution, so your `sample_test_stat` doesn't care how you generated your sample. In fact, it would probably pass normality assumptions in the real world because it would look normally distributed with maybe a couple outliers.
Indeed there are. Try posting screenshots as well. That will help people understand what’s going on.
You have to download base R first. So do that if you haven’t
http://adv-r.had.co.nz/OO-essentials.html
Thank you it's a good source, is there a way to pass a `list` to the constructor and then the constructor will assign properties based on the list's properties. let's say ```r setClass("Person", slots = list(data="list", propOne=data$something ... )) ``` ?
I have downloaded and reinstalled both the applications twice.
[https://imgur.com/VYd9SJl](https://imgur.com/VYd9SJl)
Faced similar problem twice. Removing everything from RStudio and reinstalling all again solves the problem.
I did that twice. Will try again!
Mine was a Linux system. Apt remove didn't work. I had to remove every traces of RStudio manually.
Here: https://towardsdatascience.com/how-to-do-that-animated-race-bar-chart-57f3a8ff27a8
Thank you!
The latest version of adv R has a chapter on R6, which I think is a nice OO system in R that is relatively intuitive and feels a bit more like OO in other languages. (Just my two cents). [https://adv-r.hadley.nz/r6.html](https://adv-r.hadley.nz/r6.html)
R6 if anything but consider if you really do need objects, I feel like doing OO in R is not the right way to go about doing things.
&gt; compare the power of a test of a wilcoxon signed rank test with a t-test using resampling. Okay, good, that’s clear. &gt; The steps were outlined as Are these the steps your professor outlined, or your summary of the steps? I’m having difficulty following them because, like Peter-Cottontail said, there’s some fundamental inconsistency (e.g. you’re wanting n=30, but df=5, this does not happen). I’m hesitant to write out a set of steps for how to go about simulating the power of a t-test and Wilcoxon signed rank test, because I’m not entirely certain what your prof wants you to do, so I don’t want to send you on a goose chase. If you can clarify or screenshot the directions, I think folks here would be able to give you some better pointers.
Note that using R6 may confuse people who are unfamiliar with it since it passed by reference on assignment. Apart from s4 and R6, you can also have a simple list with that is returned from a function and write custom methods using functionName.className syntax.
Thank you!
Thank you!
`ggplot(as.data.frame(applematrix), aes(x = USA, y = Australia))+` `geom_path() +` `geom_point()`
Hi, Thanks for the reply! But I want to connect the data points according to the row ( like dot 1 connect to dot 6), and have all points on the graph ( so 10 data points) .
Hrmm.. yeah. here is a screenshot of the directions. Sorry it took so long. https://drive.google.com/file/d/1vJ9t2hlY1-VDAYmF8vVwk7S_vQpeIg64/view?usp=sharing
Try countries &lt;- c( "USA", "Australia" ) DF &lt;- data.frame( Group = factor( rep( 1:5, levels=1:5), Country = factor( rep( countries, each=5 ), levels=countries ), Apple = apple ) ggplot( DF, aes( x=Country, y=Apple, group=Group ) ) + geom_line() + theme_minimal() (untested)
doesn't work :( but thanks!
wait. i dont get this. are USA and Australia the x,y coordinates or is this a single dimension that you want to connect by lines?
Did you remove absolutely everything RStudio related? Just uninstalling might not work as it’ll leave some configuration files about.
Huh, that’s odd your prof wants you to sample from the t-distribution. Did s/he give you the values of *n* and *df* to use? Ordinarily, we take a sample, and when we get a t-stat the *df* is known to be *n*-1. Anyway: I assume your code is working on implementing step 1, yes? Usually when I write loops, I try to write the “guts” first. That is: I ignore the fact I need to do something 5000 times, I just want to get it working for ONE replication. Once I (think) I have the proper code, then I’ll put a loop around it, and modify as needed, e.g. to save the results. In your case, this means simulating a set of data from the t-distribution (using `rt(n,df)`) and calculating the test statistic. You already did this, but you have it in a while-loop, and there if some extraneous stuff before and after. For example, the `quantile` function works properly without sorting beforehand. And like cDidsM, I’d use a for-loop rather than a while-loop. The reason? We know exactly how many replications we want: 5000. That `vector_cutoff` bit is unnecessary. To bring in the shifts, you don’t need to fiddle with the non-centrality parameter. You can just add the constant to your data. For example: - `rnorm(100, 5, 1)` follows a Normal distribution with mean 5, and sd 1 - `rnorm(100, 0, 1) + 5` Also follows a Normal distribution with mean 5, and sd 1 For mean and median at least (I forget if this applies to all measures of location), a constant shift in the data will correspond to the same shift in the measure of location. That said, you don’t necessarily want to bring the shift in at that point. Take the two steps in sequence. First accomplish step 1. This is getting you the sampling distribution of the t test statistic under the null hypothesis, and from it the critical value at which you’d determine whether or not to reject the null hypothesis. The second step uses that same value. You don’t repeat step 1 for each of the different medians, you do it only once, get the critical value, and save it to re-use. For the second step, you generate a dataset (in a similar way as before, but now apply the shift). Then using this dataset compute the t test statistic and compare it to the previously-simulated critical value, and using the same dataset run the Wilcoxon signed-rank test. The Wilcoxon test doesn’t use the simulated critical value, your prof said you know the null distribution for this test, so you use that directly. So for each replication in step 2, you’ll get two results: One for the t-test, which uses your result from step 1, and one for the Wilcoxon test. That’s probably a lot to absorb. I’m trying to clarify what your prof wants, without doing the assignment for you. Let me/us know when you’ve had another crack at it.
Ahhh!! I should make it into a single dimension and connect them by line ! It I’m still not sure how to draw the line, like 1 and 6 ...
This question is not about the R language...it is about a statistical procedure encapsulated in an R package. Try r/rstats.
&gt;Question\] Curve fitting using LASSO Will do, thanks
I thing the segment function will do what you want
This does work, and I believe it is what you are after. However, it relies on having previously defined the apple object from your code. Here it is again in a self-contained form so that you can see the required data organization: library(dplyr) library(ggplot2) library(readr) "pair, Country, Apple a, USA, 1 a, Australia, 6 b, USA, 2 b, Australia, 7 c, USA, 3 c, Australia, 8 d, USA, 4 d, Australia, 9 e, USA, 5 e, Australia, 10" %&gt;% read_csv() %&gt;% ggplot(aes(x = Country, y = Apple, group = pair)) + geom_point() + geom_line()
If you're familiar with ggplot2, you should use geom_path()
not sure what's going on because I would need more code but can you please add spaces between arguments and around the assignment operator? Your formatting is hurting my eyes. analysis &lt;- function(file, max.or.min = "max") analysis(file = file.to.analyze)
is `paste0()` the first time you refer to the argument `max.or.min` in the function? Your problem is somewhere else in the function. Here's a minimal example of just that part of the functionality and what it would output: analysis&lt;-function(file,max.or.min="max") {paste0(max.or.min, ".")} analysis('stuff') # [1] "max."
i tested u/jdnewmil and u/endaemon 's solution and it works. `library(ggplot2)` `apple &lt;- data.frame(country = as.factor(c(rep("USA",5),rep("Australia",5))),` `value = 1:10,` `group = c(1:5,1:5))` `ggplot(apple,aes(x = country, y = value, group = group))+geom_line()` &amp;#x200B; You can also try making an arc diagram which can be found [here](https://www.r-bloggers.com/arc-diagrams-in-r-les-miserables/).
&gt;The actual function he's writing is a wrapper around the cor function so when he calls the boot function on his dataset it'll only return the first row instead of two rows. To avoid creating this extra function you'd have to sunset the data in the data = argument. Hello, sorry what is ''wrapper'' ? What does it mean? Why should he return only the first row instead of 2 rows? what is the purpose of this?
&gt; It's obvious that he understands the idea of bootstrapping enough such that he can just write a one line (is one line fancy?) function and which would actually take him 30 seconds. He is not using bootstrapping exhaustively just enough for his one application. &gt; &gt; Could you please tell me why he writes a new function instead of to simple use 'boot( )''
Figured it out. It just seems to be syntax error. map + geom_point(aes(x=LONGITUDE, y=LATITUDE, fill=Ratio) + scale_fill_gradient(low="green",high="red"), data=test3) caused error but map + geom_point(aes(x=LONGITUDE, y=LATITUDE, color=Ratio), data=test3) + scale_colour_gradient(low="green",high="red") worked.
Without details about OS and configuration it's hard to say.
Yep you are right. Don't know why I wrote integer 😁
Yes, that is the first time I refer to the max.or.min argument. The first thing the function does is add on a "." to make downstream functions easier.
It's hard to help without seeing the body of the analysis function (or at least a good spec) and a sample of the data you're working with. Is this a file you've opened? Is file a file read into a dataframe? Are you writing to a file? How do you handle the file opening? Is it a dataframe you're modifying and reassigning to the same variable? Are you assigning with &lt;- ? Or =? You can just make up some data that is structured like the actual data if it's sensitive or large.
Without seeing some more code (a minimal reproducible example, but we might be willing to work with some extra fluff), it's impossible to say.
Excellent work!
- You appear to be calling the `analysis` function from within itself. This is recursion, and if it ever works it will fail with a stack error because no work will ever get accomplished. - `file.to.analyze` is not among the arguments you have defined, so the function will look in your working environment for variables that match that name, and your `file` and `max.or.min` arguments are getting ignored. - `file` is the name of a commonly-used base R function... should not cause problems right away, but confusing. As the other commenters have noted, a reproducible example is probably needed before we can answer your question. https://cran.r-project.org/web/packages/reprex/index.html (read the vignette)
I wish I had seen this question years ago, I ended up learning Python because of this..
Probably easy tiling? btw they're not just an R thing: https://www.stickermule.com/uses/hexagon-stickers
&gt; Probably easy tiling? Yes, definitely. But, I would like to know a little more.
Sorry for the late reply. It works now. I removed both R and RStuido. Somehow it wasn't working when the first time I reinstalled but it works now.
What makes you think there is more?
Sorry I didn't respond to this. I just buried my Dad, so I'm still in recovery mode.
Hello there! I just wrote an article on R, dealing with a few basic programming standards if you want to build a sustainable project/package. Tell me what you think!
That's it. It's the highest one-sided normal polygon that tesselates.
Overall a nice article but the section on return values is entirely backwards. R is a *functional programming language* and therefore has the following self-consistent rule: Everything in R is a value. Every expression, including every function call. You don’t “access the value of the last variable” as the article claims. A function *always* returns the value of the last *expression* it executed. The consequence may be the same but the different framing leads to a different way of thinking about expressions and values. Contrary to what the article claims, it’s *good* and *idiomatic* to write code like this. Implicit return values are the equivalent of explicit returns in other programming languages. It’s entirely idiomatic in R to write the following code: add_numbers = function (a, b) { a + b } By contrast, `return(…)` in R is a *function call*, with the meaning “abort the current function prematurely (and return my argument’s value).
Yes!
Thanks for your answer! I will update my article on the functional aspect of R, so that people like me - not used to this way of working - won't commit mistakes. As well for the `.Rprofile`, really good advice of yours! Regarding magrittr, this arbitrary limitation is a suggestion to keep good readability. I didn't phrase it well, will change it too. Thanks again, really good to read such helpful comment!
That's interesting to consider return as “abort the current function prematurely (and return my argument’s value)”. I knew that about return values, though recently I've taken to explicitly returning values anyhow because it's just slightly more readable to me, with no room for surprises.
Here's one way to do this. Essentially you're looking for `separate()` from the `tidyr` package along with some string manipulation from the `stringr` package. library(dplyr) library(stringr) library(tidyr) df &lt;- df %&gt;% mutate(coordinates = coordinates %&gt;% str_replace("c", "") %&gt;% str_replace("\\(", "") %&gt;% str_replace("\\)", "")) %&gt;% separate(coordinates, into = c("lat", "long"), sep = ",", remove = FALSE, convert = TRUE) First you need to get rid of the parentheses and the "c" in the coordinates column, then you "separate" the column on the comma. Hope this helps, let me know if you have any questions
This worked perfectly! I am also grateful that you explained why I'm using the 3 packages, too. Thank you very much, it was very helpful.
Mostly good stuff... but some picky points: ↓ for naming standards, note that `data.frame` violates the S3 usage of periods so don't follow that pattern... avoid using periods unless you are using them for S3 methods ↓ for writing `c("value")`... all data in R is stored in vectors, and `"value"` is nothing more than a single-element vector, so concatenating a vector to make a vector is... pointless. ↓ there is only one true way to test for `NA`... the `is.na` function, and it is intrinsically vector-valued... always... because all values are vectors. If you have a need to mark a non-existent vector, use `NULL` and the attendant `is.null` which always returns a single element (so is friendly to the `if` function). ↓ RStudio autocompletes... R applies partial matching in certain cases as you discovered. The distinction is context... autocompletion is an interactive behavior that occurs during editing, while partial matching occurs as the code executes. If `$` partial matching is a problem, use `[[` instead. ↓ implicit return values... I agree strongly with r/guepier on this. * re pipes: if you are using `dplyr` you don't *need* to use `magrittr` unless you want the fancy variants it offers. As for switching between pipes and successive variable modifications, I agree in most cases but there is one very significant performance optimization that is not available to pipes: partial vector modifications: i &lt;- 1:10 v &lt;- rep( "blue", 10 ) v[ i %% 2 ] &lt;- "red" This mutates `v` in place, while in a pipeline this operation inevitably allocates a completely new block of memory. * FYI: Being a Python user you probably prefer `=` over `&lt;-`, but these do behave differently within a function argument list, so my preference is to stick with `=` only for setting arguments. Others may just prefer to use a subset of R syntax by avoiding using `&lt;-` ever, which of course excludes the possibility of using it in function calls. If you do use `&lt;-`, always put spaces on both sides of it to avoid syntax confusion with unary minus.
na.rm should ignore value with NaN. Can you provide a reproducible example of what you are doing?
What do you want to do? You could use ggplot2 in R, save as svg file and import that into Blender.
Would help to see some other code prior to this for context ...
I'd like to make animations of my simulations.
Is one of the column a date or time? I think you have a different date format in your different files or R is having trouble making dates out of one of them.
Is one of the column a date or time? I think you have a different date format in your different files or R is having trouble making dates out of one of them.
yes it is. I've added some more code now so you can see the columns
I don’t know anything about Blender - but people making animations in R seem to use gganimate quite commonly.
I agree with everything you've written. But a small clarification on your last point: Saying that the two operators behave differently in function calls is slightly misleading. [The *operators* behave identically in function calls](https://stackoverflow.com/a/51564252/1968). It's just that the syntactic token `=` has an additional meaning inside a function call, which is to set a named argument. This can be avoided by simply putting parentheses around the assignment inside the function call: f((a = 1)) f(a &lt;- 1)) These two are identical.
For reasons that are not entirely clear to me now, I think update events happen after the code block is executed. You may have to use `future` to make this one work. On the plus side, it'll have the added benefit of supporting multiple users at the same time.
Two possible problems are you've called the wrong variable name (or misspelled it) or that you've overwritten the data somehow. Does it display correctly if you just call: `stargazer(CrimeStatebyState, type="text")`
What Dr5penes said, is where I would start checking. Most likely you have an issue with a timestamp_datetime column in one or more datasets. We need to see the class of that field from each input table to properly start troubleshooting.
Blender and R are very different things... it's kind of like asking when Photoshop is coming to R. Within R, you can render a 3D model and save out a 2D image (or animation created from a sequence of images). Check out the `rgl` package. But the process is clunky at best compared to the control over rendering that you get from dedicated software like Blender. Alternatively, you can export a 3D .obj file from R and edit/animate/render in Blender. Google turns up various packages for exporting .obj files but I've never personally worked with any of them. Maybe Blender can import animation keyframes (that you would export from R) but I'm not sure.
Hard to know what you are looking for when you give no details about the model you are talking about. I have made animations of particle movement using ggmap and gganimate. &amp;#x200B; https://twitter.com/trjpphelan/status/1064485609341890560 &amp;#x200B; These could be an option for you. &amp;#x200B; If you are looking for really complex animations and rendering, then why not export your data from R and do this work in Blender?
R is specialized in statistics, and while a package to make 3D models in R would be awesome, it would still not beat a specialized open source alternative that has been in development for many years. However making 3D plots is certainly possible, look at the plotly package and many others that other comments have mentioned.
This largely depends on what you are trying to accomplish. Examples below with slightly modified DF1 to illustrate an inner vs. left/right outer join: # Base approach ----------------------------------------------------------- df1 &lt;- data.frame( Sample = paste0("Sample", 1:4), Gene1 = c(1, 0.5, 23, 0), Gene2 = c(5, 16.7, 0.4, 0), Gene3 = c(2, NA_real_, 8, 0), stringsAsFactors = FALSE ) df2 &lt;- data.frame( Sample = paste0("Sample", 1:3), Time = c(5, 5, 6), Treated = c("Yes", "No", "Yes"), stringsAsFactors = FALSE ) # do you want to bring over df2 data to df1 using 'Sample', only matching? # this is an inner join merge(df1, df2, by = "Sample") # or all keep all x, even if no corresponding match in y? # this is a left join merge(df1, df2, by = "Sample", all.x = TRUE) # data.table approach ----------------------------------------------------- library(data.table) dt1 &lt;- as.data.table(df1) dt2 &lt;- as.data.table(df2) # right join dt1[dt2, on = "Sample"] # left join dt2[dt1, on = "Sample"] # inner join dt2[dt1, on = "Sample", nomatch = 0L] # bring over matching from dt2 where dt2's `Treated` value is equal to "Yes" dt1[dt2[Treated == "Yes"], on = "Sample"] # Sample Gene1 Gene2 Gene3 Time Treated # 1: Sample1 1 5.0 2 5 Yes # 2: Sample3 23 0.4 8 6 Yes Note that if you indeed have thousands of genes, and these are laid out as a column per gene, you may want to reshape your data. Or you may want to use a different data structure (possibly a matrix). It all comes down to your requirements.
Glimpse all 4 datasets. Is the date time column the same class on all 4? You can use parse_datetime() to make sure a column is a date time
This could be what you're after. https://stackoverflow.com/questions/55905927/is-there-a-function-to-scrape-the-notes-sections-of-powerpoint-slides
Ok, I've done that. I've got a column called 'timestamp_datetime' in each of them. It was originally created in Python, then exported to csv and then into R. That column looks the same in each one, but do you think the Python origin is the issue?
This is always tricky because it depends on what possible patterns you need to handle. x &lt;- c( "123 Somewhere St, Someplace, MA 12345, USA", "54321 Somewhere St, Someplace, MA 12345, USA" ) # any run of 5 digits patt_simple &lt;- "\\d{5}" # any run of 5 digits where right after you have # a comma, a space, and one or more letters # terminating the string patt_lookahead &lt;- "(\\d{5})(?=,\\s\\D+$)" # extract regmatches(x, regexpr(patt_simple, x)) regmatches(x, regexpr(patt_lookahead, x, perl = TRUE)) Note the simple pattern will return `54321` for the second example. But the lookahead will work properly. Neither will work if you have zips that look like e.g. 12345-1234. And the lookahead will only work if its assumptions are met (see comments in code block).
Worked like a charm! Thanks!
In day-to-day reading of R code I find your distinction between the syntactic token and the operator takes too much time to process, particularly now that `mutate` and `summarize` functions frequently spread argument matching across many lines. The below example is a bit artificial, but I (and many other R programmers) find that using `=` for argument matching and `&lt;-` for assignment makes easier to read code. Clearly there is a separate group of R programmers such as yourself who don't find this to be a problem (and there are some distinguished members in your group)... but the proponents of `&lt;-` in R coding standards are not "lying" about its value when they choose to include this requirement. x &lt;- 1 # global is 1 f &lt;- function( x ) { cat( sprintf( "x argument is %d\n", x ) ) } f( x = 2 ) #&gt; x argument is 2 x # global is still 1 #&gt; [1] 1 #f( x = x = 3 ) # disallowed by parser f( x = x &lt;- 4 ) # allowed by parser #&gt; x argument is 4 x # changed to 4 #&gt; [1] 4 f( x = ( # argument matching x = 5 # assignment allowed by parser ) ) #&gt; x argument is 5 x # changed to 5 due to equal sign within paren #&gt; [1] 5
I'm on the phone but you could do it in sevreral steps. 1. split it into different columns at the commas 2. that would give you the state and zipcode 3. And then use a simple number extractor for the column with the state and zipcode.
Sometimes r randomly seems to interpret dates as factors so you have to convert them to strings and then to before you convert them to dates.
Yes, I think so
The way R deals with loops is really memory inefficient. The preferred solution these days is through something like the purrr package (or you can use do() in dplyr but it's deprecated). I think a combination of these functions with maybe even multiple cores is going to be your best bet.
I have used RGL to plot sedimentary cores in a 3D space. It has tools to play animations which can be rotated in real time and all that, but if the number of particles is high it will probably be very resource intensive. Not sure if you can export frames from it, but googling that package might be a start.
If you don't care about them being dates its easier to convert the date-times to strings than the other way. But it's not hard to parse dates, see my previous post for which function to use.
can you post an example and the error message with it?
Are you specifying the data? It’s not clear from your post. As in data %&gt;% group_by()
Yes.
Yes.
No error message, it just does not give me an ordered tibble.
I just tried with group\_by(data, column), yet it still does not work. The two ways should work the same anyways.
A reproducible example would help us help you.
I think you need to summarize() after you group\_by(). If I remember right, group\_by just tags the tibble with the variables that all future dplyr verbs should use to group by. So your code should look like: &amp;#x200B; tibble %&gt;% group\_by(...) summarize(...)
It's not quite clear what you are expecting but group_by doesn't change how the data looks, so if you group_by and then view it won't look different. You could use arrange if you expect an ordered tibble to view.
Also make sure you're looking only at American Addresses, zip codes change format from one country to the next.
Ordered in what sense? Are you trying to sort the data? What are you expecting group by to do?
What the function usually does. Group by values in that variable. Sort~group, yes.
`group_by()` doesn't reorder your dataframe, it just allows you to set groups so you can perform future operations at the group level. It sounds like you're looking for `arrange()` instead?
Seems like it. I must have mistaken one for the other. Thank you.
You would need another %&gt;%. So it would look like tibble %&gt;% group_by() %&gt;% summarize(). Other than that all good
The dataframe is grouped. That’s why you don’t see an error. It is working, but you need to add %&gt;% summarize(x, y, ...) and decide what x, y, etc. variables that you would like to group by. This unseen grouping is important when joining dataframes. Can cause issues depending on what you are grouping by.
No worries, glad I could help!
As u/Lifebyrd said, `group_by()` won't change anything visually. If you want to know if your df is grouped, you can use `is_grouped_df(df)` or `print(df)` as suggested in [this stack overflow thread](https://stackoverflow.com/questions/42655746/determine-if-a-tibble-is-grouped-or-not). `print(df)` will tell you at the top of the response how the data is grouped.
I'm only working with a specific geographic area currently (Boston) so I don't think I'll run into any issues with foreign postal codes.
Your best bet is to have R output the results in a file (csv or some sort of binary) and then use Python to read in the data and generate your scene in Blender. From there you can animate the camera and so forth....
Currently on the phone but I‘s suggest str_extract with pattern to whitespace followed by two letters (maybe only MA if no other states in data) then multiple numbers (\d* I think it is).
Can I use another IDE or does it have to be RStudio?
It's probably for his/her homework. It might have to be in RStudio.
I bet it’s the classic R and RStudio mixup.
Thanks. I was hoping it would be possible to skip the Python part, but it seems that's not in the cards.
Thanks for the tip. I'll check it out.
That's a decent approach. I'll have to look into it. Thanks!
Professional data scientist for 6 years in a senior position at a prominent tech company. 65$ an hour.
If the deliverable is a .R file, how could you tell?
Your post history seems to tell a different story.
I have rejected candidates solely on the basis of claiming to know 'RStudio' in lieu of R. I can't think of a stronger way to signal a lack of experience than mistaking the IDE for the language. Except maybe failing to recognize this fact and offering $65/hour while claiming to be a senior level resource.
DITQIS$Decision is a vector, the formula should just be column names and passed a dataframe containing them. It's a special case. lm(Decision ~ mn.calculation, DITQIS)
Yeah another IDE should be fine
Try Googling "rgl animation"...
They will likely need to combine it into one data frame first (I believe mn.calculation is not in the DITQIS df). DITQIS$mn.calculation = mn.calculation lm(Decision ~ mn.calculation, DITQIS)
A couple things: 1. A Monte Carlo simulation is a very general thing, any sort of simulation that incorporates randomness. I think you think the specific type of simulation you are doing is what it means to do a monte carlo sim, but really you are just doing a single specialized case. So asking things like "any parameter about the drift" doesn't really have to do with the fact that it is a monte carlo simulation, but rather with your simulation setup, which apparently has a constant drift rate. To your question, "can" you change your drift parameter? Well, sure. You can do whatever you want. Do whatever is interesting to explore your problem. If you think there is an interesting reason to adjust your drift parameter, and can argue for it being a reasonable thing to do, then do it. How do you do it? Depends on your code. Of which you have shown none.
The image() function is the most straightforward if you can get your data in matrix form
Are you running out of memory? I usually get away with writing intermediate products to disk and trying to keep my environment as clean as possible, including freeing up system resources. It slows things down considerably, but it at least makes R not crash. The other thing that helps is utilizing function enclosures so the garbage collector can do its job effectively
Yeah I’m running out of memory and that’s when I run the script once on the fun dataset I had to break things down further — and now I want to run it thousands of times.
try wrapping your script in a function enclosure and pass it to an lapply. it shouldn't matter how many times you run it as long as your data product isnt growing every time you run it - if it is, then write it to disk (or csv with append = T)
Thanks, I'll look into it!
Hello! I made a small summary about simple basic questions and answers about the meaning of the term high dimensional data. Very simple and very basic. I hope that you find useful.
I would suggest `geom_tile()` from `ggplot2`. There are also ways to customize the color scale that the plot uses. library(tidyverse) fires &lt;- read_csv("forestfires.csv") fires %&gt;% mutate(X = factor(X), Y = factor(Y), X = fct_rev(X)) %&gt;% group_by(X, Y) %&gt;% summarise(area = sum(area)) %&gt;% ggplot() + geom_tile(aes(Y, X, fill = area)) + geom_text(aes(Y, X, label = area))
I spot a small typo: visualice
You're right, thanks. Fixed my original comment.
If only there were someway to display the head of a data frame 🤷‍♂️
You could order the dataset by BMI then plot the first 5 countries in the plot, using data$BMI[1:5] (it might just be data[1:5,]. Maybe have a play about with that code. I am using my phone and can’t format properly
hows this? library(tidyverse) data %&gt;% top_n(5, Mean_BMI) %&gt;% mutate(Country = forcats::fct_reorder(Country, desc(Mean_BMI)) %&gt;% ggplot(aes(x = Country, y = Mean_BMI)+ geom_bar(stat = 'identity', position = position_dodge()) # + and add errorbars if you can # geom_errorbar(aes(ymin = Mean_BMI - se_BMI, ymax = Mean_BMI + se_BMI, width = .33)
Thankss for the effort! But I think I found a solution with arrange(data, desc()) function!
In case it isn't obvious for OP (second language, etc.), the correct spelling is 'visualize'.
In addition to the solutions you have provided, people also aggregate dimensions computationally to fit a 2-dimensional scatterplot using a variety of algorithms. Principle component analysis (PCA) plots and t-distributed stochastic neighbor embedding (t-SNE) plots are two of the most common in biology.
R is one of the common scripting languages I’ve encountered on large climate models, although Python is still far more common It’s a very inefficient language though - if you want to do any production-scale analysis you’re probably going to want to run the C/FORTRAN subroutines directly and skip the overhead from R
https://opensource.t-mobile.com/blog/posts/r-tensorflow-api/
Yea, for sure. I usually do my heavy lifting in SQL and once I get manageable statistics, I use R to process (opting for R over Python for the visualizations and statistical packages; opting for R over C for the ease of maintaining the code). I seem to be the only one in my office who uses it in any capacity though, which is disheartening.
Well this is awesome. Thanks for the link!
Does everyone else use Python?
Using the name `data.frame` as a data variable is a remarkably poor choice. That is the name of a commonly-used base R function and your variable may interfere with your ability to use it. Since `df` is also a base R function, I recommend using `dta` or `DF` for these kinds of variables. Any reason why you didn't use `subset( unemp,t &lt;= 41 )` ? Can you post the output of `dput( head( unemp ) )` ? Also in the future it will help if you indent all lines of code by at least four spaces so Reddit will leave the code alone.
Hi thank you for the suggestion but the outcome is still wrong. AND when I run dput(head( umemp)), it says "Error in head(umemp) : object 'umemp' not found" &amp;#x200B; I remodeled it: rm(list=ls()) #rm(list of objects) removes all objects from memory &amp;#x200B; unemp &lt;- read.csv ("unemp1.csv") &amp;#x200B; df &lt;- c("year","t","gdppc","votpc","repub","reces") &amp;#x200B; n &lt;- 41 subset( unemp, t&lt;=41) &amp;#x200B; \# Time Series start=1976 end=2016 frequeucy=12 &amp;#x200B; unemp &lt;- lm(unemp\~t+gdppc+votpc+repub+reces, data=unemp) #model summary(unemp)
R is a general-purpose language and you could build anything you like in it. Whether you should or not is a different question. It's very possible with enough care in designing the infrastructure to scale a shiny app to hundreds of concurrent users, for example. It really depends what you're trying to do with your personal website, what you're trying to demonstrate or learn, and what you're intending to put into it. I really like Django personally, but mostly for applications are are essentially providing a nice interface over a database. Those kinds of sites are for people to do things like update data mappings and access information, though, which might not be a use case of your personal website. If you want to make a tech demo of different analysis and visualisation techniques that you know, and expect a small number of users if you speak at a conference or something, then there are several options for hosting using R that will be very easy. I guess what I'm saying is that "personal website" is too vague and you should be more specific.
It's really up to your machine and the computational intensity of the process. With the advent of async programming in shiny, serving concurrent users did become feasible even for computationally intensive processes as long as the machine has enough resources to sustain the subprocesses. If you have too much money, shiny server pro has load balancing with multiple active sessions. Without any of that I used to have multiple sessions of the same app and the website html secretly redirecting to a random one. This approach has SEO issues but there's probably a right way to do it that I didn't bother to do. In all likelihood, none of that will be needed though if the processes that the users triggers are done quickly. Note that this is for interactive websites. If you want a static blog, shiny is not the right tool for the job. Look at `blogdown` if that's what you want. It is fun to work with.
Personal as in I would be blogging about my community and uploading some visualizations hopefully. But despite my ambitions it is unlikely I will be having 5000 refreshes a minute any time soon :) https://bookdown.org/yihui/blogdown/ This website is really compelling
Yes!!! I was looking at blogdown and i imagine I might upload a few interactive things, but it is unlikely I am uploading like tons of that kind of stuff. https://bookdown.org/yihui/blogdown/
With blogdown, performance isn't an issue. At least not from R's side. The output of that is a static HTML website. Anyone number of users can visit that at the same time as long as your server is happy to continue serving
So you think I could accomplish my goals and make a semi pretty enough website?
It depends on what you mean by building a website in R etc etc. If you're building a static site with blogdown/pagedown/pkgdown/etc, it just depends on who you're hosting it with and their limitations. Something like github pages or netlify will take blogdown sites and in theory have essentially whatever capacity you can throw at them, but I imagine that they cap free users at some point for their bandwidth. Just like any site. If you're talking interactivity, depends on what you're doing. If you're talking using plotly for interactive graphics, I think that's browserside, so you're fine. If you're talking shiny, there you've got scaling problems. And probably what they're referring to.
[Here is my website](bsolomon.us) The website is written using Jekyll and includes apps written in R and served using a shiny server. The whole thing is hosted on an Amazon ec2 server. The most traffic Ive ever gotten is a couple hundred views an hour, so I don't know how it would do with high traffic, but it suits my needs quite well!
Probably browserside. How much does web hosting cost these days? It would be very cool to have my own domain ♥️
How much is your bill usually?
Prettyness of a website is mostly dependent on your CSS wizardry and/or picking a good theme. That wouldn't really be effected by the platform you use.
With netlify or github pages you can host there for free essentially and point to your own custom domain, see here for github: https://help.github.com/en/articles/quick-start-setting-up-a-custom-domain You just have to cost of your own domain, which is like $12-20/yr or something like that.
I don't think I would call blogdown an "R website" and that's not what people are talking about when they say you won't be able to support many concurrents. blogdown is a static website generator. When people say "R website" it's usually implied that R code will be executed on the server.
Like $15, so it was cheaper than the basic shinyapps.io plan, though they now have the starter plan. Either way, I was easily surpassing the active user limit on the free plan. I have been meaning to apply my credits to AWS from the git hub student developer pack (https://education.github.com/pack) though...
Python or *shudder* PL/SQL
I have been summoned to the discussion. Mouse brains is correct - shiny is awesome and I do have a few small websites running off R/shiny (such as https://cranalerts.com/ and https://daattali.com/shiny/) that cost me $5/month using digitalocean (if you want instructions on that, let me know, but I'd rather not that article uninvited because it has referral codes) However, for the vast majority of personal websites, even that is more than necessary. If you can get away with a static website, you can just use jekyll or hugo or anything similar. I personally use jekyll with github hosting - it's all free and very easy and convenient. You can use my theme https://deanattali.com/beautiful-jekyll/ if you're looking for a place to start
To delete all missings: df = df\[!is.na(df$t),\]
Thank you both of you, I made the change.
I will check this in order to complete the article.
I have just one machine learning model in production with R. It's productionalized via SQL Server. I built it in R Studio and then SQL calls the R script via it's built-in integration with R. It works well, but its kind of clunky to write R code as a straight up string within T-SQL.
City of Chicago has the RSocrata project https://digital.cityofchicago.org/index.php/archive-open-data/ At useR! some years ago (maybe 2014?) there was a speaker from City of Chicago talking about how they used some daily modeling of restaurant inspection data (including complaints) to determine where to send inspectors. Basically the person managing the inspectors (not a data scientist) was able to point their browser to a shiny dashboard which would update along with the database contents.
My team has deployed TF models to production in almost the exact same fashion. While not ideal, it does work.
thank you very much
thank you very much
I'm curious, what type of ec2 instance have you found to be the best for hosting shiny apps?
I used your fix but am getting same errors for this regression regpd &lt;- lm(dt.calculation\~ mn.calculation,data = DITQIS ) print(regpd) summary(regpd) &amp;#x200B; Error in model.frame.default(formula = dt.calculation \~ mn.calculation, : variable lengths differ (found for 'mn.calculation') any idea how i can fix it?
The Tempe Bike Count infrastructure is R.
I was working in a job where we used R + Shiny for financial app for banks.
Call length on each of the variables.
This was a big letdown for me, when the introduction of Machine learning services happened. I expected the implementation to be really different..
We deploy Shiny apps in containers for our production environment. Our primary production environment is .NET, but I've moved all of our BI stuff (financial reporting, KPIs, etc.) into Shiny dashboards. I also have some scheduled scripts that run stats on our orders and we use that information to supply accurate turnaround times on our services and fee estimates. As far as working with a team, I'm currently the only active contributor on my team to the R projects, but that is changing. I've been writing and maintaining my own R library (hosted in a private repository) that sets up a bunch of functions and environment variables to interact with our databases and S3 storage. That way when other team members do start contributing, they don't have to figure out how to connect to our SQL server, or re-write any common functions to retrieve common data sets. The idea there being to make it as easy as possible to just work on the functionality of the apps, without fussing over proper ways to open db connections in R and dispose them correctly. I also enforce certain standards by doing that, ie. I want everyone to use data.table syntax, not dataframes, so all of those functions return data.table objects for them. Works amazingly well.
Not sure if this is the case here, but is it possible that you have NAs in either of your variables?
First, get rid of the argument data = DITQIS. It is unneeded. Call DITQIS without assigning the model to that data. You will need to do this if you want to predict something with you lm later. Next, you need to exclude the NA values. I have shown this below. regpd &lt;- lm(dt.calculation\~ mn.calculation, DITQIS, na.action = na.exclude ) &amp;#x200B; This should work. If not reply with more code and I will help.
If they are both in that data frame they should be the same length. A better "Call" would be a summary of each variable. With 70 values table would also work. Length won't hurt but may not solve anything.
At least half of analysis is knowing what your data consists of. You should be using the `str` function frequently, as in `str(DITQIS)`. When asking for help you should be providing at least a few rows of data in a re-usable form like `dput(head(DITQIS))` generates. Also, if you indent your code examples by four spaces reddit will leave it alone and it will be easier to read.
either you response variable or the predictor is living separelty in the global environment and probably has NAs in it. rm(list=ls()) and then just load the data DITIQIS and run this model.
It really depends on how much "lifting" needs to be done. R is not, as a rule, an efficient language. If you're building production-level tools for a handful of users in a closed environment, R can be a fine tool for that. If you're building a SAAS product to serve thousands of simultaneous users in a distributed server environment, then you want to consider a more appropriate language like C, C#, or .Net.
Is this homework or an interview question?
Hint, this is known as the *cumulative product*, which is a special case of the so-called *prefix scan*. Armed with these terms you can consult the (R) documentation.
It's unclear how your input is supposed to lead to your desired output. You failed to describe the necessary intermediate steps.
Something like this should work. library(data.table) x = 'abcdefghijk' lx = nchar(x) df = data.table('triplet'=rep('',lx-2),'out'=rep(0,lx-2)) for (i in 1:(lx-2)){ df$triplet[i]&lt;-substr(x,i,i+2) df$out[i] &lt;- i } out_string &lt;- paste0(df$out,collapse='') fwrite(out_string,'path/to/file.txt')
Not a loop, but it might give you a clue about how it could work: y &lt;- c(x[1], prod(sample(x, 2)), prod(sample(x, 3)))
Neither. I am a PhD Student in Biostatistics and I am building an S(Susceptible)E(Exposed)I(Infectious)R(Recovered) model and when forming the likelihood for each infected person I require some of the idea I started the post with
Thanks. Only reason I was thinking loops was because the vector x in my situation is of length 10000 infected individuals. So I want to write a short code to get the job done. I just gave a simple example when I defined x to make it interpretable to everyone.
Thanks, I was having trouble with the matrix but I was able to get it thanks to this. I was missing the summarise() function!
A quick recursive solution. You'll need to handle multiple inputs with something like `lapply`: x &lt;- "ABCDEFGHIJKLMNOPQRSTUV" f &lt;- function(x, res = list(), iter = 0L) { l &lt;- nchar(x) if(l &lt; 3) { return(res) } nm &lt;- paste0("iter", iter) res[[nm]] &lt;- substr(x, 1, 3) f(x = substr(x, 2, l), res = res, iter = iter + 1L) } paste(seq_along(f(x)), collapse = "") # 1234567891011121314151617181920
Yep, I understand that. I was just giving a non-loop example and leaving it up to you to extend the code to be used in a loop. Let me know if you still can't get it and I'll be willing to help more.
This seems like your calculations will go in overdrive if you are going for 10000! Calculations
Ok, I was going to answer your question, but I cannot get around the camera image. I suggest that in the future you take a screenshot of your screen and submit that for your questions. After some thought, I think I know what is going on. You do not open Reddit on your browser and instead use the phone app. Thats got to be it. Sorry for going on about this, but I see it all the time on Reddit in this subreddit and the Blender subreddit. &amp;#x200B; Ok, now I can address your question: &amp;#x200B; You should look into the \`scale\_fill\_continuous\` function. There is an option where you can specify the breaks and the labels. I would also recommend that you use a more diverse color map such as Viridis. You can just add \`scale\_fill\_viridis\_c()\` to your ggplot command.
Yes. Probably will use a super computer for the whole task wit a SLURM batch file. Just need to think about a short simple code first.
Can you use the `cumprod` function here?
So cumprod will multiply in a certain sequence so 2nd element is product of first two elements. Here there is no order. only we know that 2nd element is product of two terms(any two terms not necessarily first and second)
Maybe I'm not understanding the problem correctly then. Here is what I was trying to get at with my non-looped code, but this doesn't take long to run on a vector of 10,000 numbers. x &lt;- sample(1:10, 10000, replace = TRUE) y &lt;- sapply(seq_along(x), function(i) { if (i == 1) return(x[i]) prod(sample(x, i)) })
&gt;No you are on the right track. I excluded a lot of technical details here. I actually have times of infection of individuals which is being generated by a model. Then those times of infection get plugged into a hazard and survival function for likelihood construction. That is where this concept comes in. But there are a few functions working here so I feel a supercomputer would help if I am trying to build an epidemic of size 10000.
I see, sounds like a cool project! If you are really worried about time you could even remove the `if (i == 1)` check from the loop and just do `y[1] &lt;- x[1]` after. I wouldn't expect this to speed it up too much, but it is redundant to check on every iteration.
Thanks.
Try: colnames(test2pay)[1] &lt;- "PatientID"
Set it up as a Boolean conditional - 73 samples of TRUE/FALSE on whether they touched the object with gender as an associated factor. Of the top of my head I think a z-score would work just fine
In your example, what is the desired output?
As a further question, what order of magnitude are your x values on here. The product of 10000 numbers greater than 1 is likely to lead to an Inf result.
You're asking whether a categorical variable (gender) predicts another categorical variable (car-touching). Chi-squared is sensible.
This is absolutely a chi^2 test. There are ~~several~~ many tutorials on this. Here's [one](http://www.r-tutor.com/elementary-statistics/goodness-fit/chi-squared-test-independence) that looks fairly intuitive as far as the R code.
Instead of a chi-squared test, you might want to go with a logistic regression. Since you want to know if gender predicts whether or not the rat touched he toy car, the behavior is a yes/no answer. The glm function in lme4 can handle this very easily. Just use sex as your predictor and specify a binomial distribution (family=binomial).
It is to form a likelihood function and then estimate the parameters for the chosen parametric family. Since this is survival analysis I am using exponential, weibull, gamma distributions and checking which family gives best parameter estimates
He might not have the conceptual foundation to understand and justify GLMs. A chi squared seems better fitted for this type of class (for a thesis I agree).
I understand that. What I mean is, in your example what do you expect y to look like? E.g `y=...`
To be fair mate that's exactly why I asked a question beginning "how to" :P
My point is that your description makes it impossible to answer this question. Your question is “how to?” — my question is “how to *what*?”
How to write to a file the digits which are assigned to different triplet codes (e.g. ABC = 1, BCD = 2...), given a string (e.g. ABCDE). This code and output doesn't matter it is just the simplest example. The string 12345 supposedly corresponds to the ABCDE... because ABC = 1, BCD = 2 etc.
&gt;The string 12345 supposedly corresponds to the ABCDE... because ABC = 1, BCD = 2 etc. OK I still don’t get it: The string `ABCDE` is composed of the overlapping 3-grams `ABC`, `BCD`, `CDE`. So shouldn’t the output be `123`, not `12345`? And, again, what’s the desired output for, say, `TEST`?
I haven't played around with it too much and, honestly, I'm probably not the best to ask. My instances have always been t2.micro and work well enough for me.
Yeah the output for ABCDE should be 123, ABCDEF would be 1234, DEFGH = 456 etc
And `TEST`?
It is arbitrary at this point, so maybe 69
OK but the problem is that the computer doesn’t do “arbitrary”, the computer follows fixed instructions. So *you* need to decide on some rule.
I know the computer doesn't do arbitrary, but my example is ABCDEFGHIJKLMNOPQRSTUV, not ABCDEFGHIJKLMNOPQRSTUVTEST.
Well in that case the simplest answer to your question is, as I wrote in a previous answer: writeLines('1234567891011121314151617181920', 'filename') Do you see how useless this is? If you’re unable to generalise your description there’s no helping you. OK, that’s it from me. You’re clearly not interested in actually getting help, or learning something.
Well I am not bothered - looking through your posts you seem to be a very inflammatory person and everyone else understood what I meant. Once I get past the dummy dataset stage, this is being used on the genetic code which has a set number of triplet combinations so no it doesn't matter how TEST will be converted.
&gt;everyone else understood what I meant That’s clearly not the case: None of the posted solutions works outside your arbitrarily confined example. Especially, as you’ve now revealed, for codons. &gt;this is being used on the genetic code Then, for fuck’s sake, why didn’t you say so initially? You’ve wasted everybody’s time here because for the genetic code a solution would look fundamentally different (for one thing, there’s no overlap). I should know, I literally did a PhD on it.
Sorry for the late reply but that worked well, thanks!
```cumprod(c(x[1], x[sample(2:length(x))]``` If you need x to match the order, do ```x &lt;- c(x[1], x[sample[2:length(x))]``` ``` y &lt;- cumprod(x)``` It'd be a good idea to have an I'd associated before you sort on x.
I don't think `data1[, -c(1)] %&gt;% nest()` is doing what you expect. You might need to do: data1 %&gt;% group_by(Experiment) %&gt;% nest() Also, I believe PCA only works on numeric data?
Well its hodge podged together from an example I found for doing PCAs on tibble data... Thanks though, I'll try that. I'm not trying to do the PCA on the conditions but still want to keep the conditions so I can use them to label the final PCA plot to visualise the results. E.g. colour would be experiment and shape for exposure if that helps give context for what I want to achieve.
I made some posts about the pros and cons of R and Python here: &amp;#x200B; [https://www.reddit.com/r/rstats/comments/bavyo8/two\_examples\_on\_how\_the\_documentation\_for\_r/](https://www.reddit.com/r/rstats/comments/bavyo8/two_examples_on_how_the_documentation_for_r/) &amp;#x200B; [https://www.reddit.com/r/Python/comments/bauvvf/python\_for\_data\_science\_what\_do\_you\_love\_about\_it/](https://www.reddit.com/r/Python/comments/bauvvf/python_for_data_science_what_do_you_love_about_it/) &amp;#x200B; Why do you prefer ggplot to, say, matplotlib + seaborn ? You don't really explain. The tidyverse plotting is great if you agree with its 'opinionated' approach, but terrible to deal with if you don't. Eg try to have 2 y axes on one chart. I hear, but don't fully agree with, the theoretical arguments against them, but the fact remains that there are many real-life situations where you simply need two axes. &amp;#x200B; You could talk about how poorly R implements namespaces, and how easy it is to overload some functions in R when importing packages. &amp;#x200B; Or about how R is orders of magnitude faster in reading xlsx files. &amp;#x200B; Or about how PyCharm is an incredibly more advanced editor than R Studio &amp;#x200B; Or clarify that the larger number of packages available for one language is totally meaningless to 90% of its users. &amp;#x200B; Or about how Python was born as an OOP whereas classes and objects were added to R as an afterthought. &amp;#x200B; Or about how Python now allows some kind of type checking. &amp;#x200B; Or about how (not) great it is that R uses c() to create arrays, and has a number of odd reserved keywords like that; actually, it's worse than that, you can overwrite them, which makes for some fun debugging. &amp;#x200B; Honestly, your article seems really shallow and not particularly informative.
That’s a lot of work for expressing the exact same as paste(seq(nchar(input) - 2L), collapse = '') In particular, the result of the function `f` is unused in the code; the only thing that’s used is string length (minus 2).
Yes it is :) &amp;#x200B; I even considered adding the output length to pre-allocate. Then I realized that if I did that, the solution would basically be what you said. Which led me to the same question as you asked re: why does OP want to do what it seems he is trying to do. But I'd already written the function, so I figured I'd post it.
Not sure if this fits what you're looking for but check out the [TidyTuesday repository](https://github.com/rfordatascience/tidytuesday) \- they post a dataset every week and folks use it to practice/test new skills, etc., but mostly toward data visualization (and especially w/ggplot and related tidyverse packages). Participants post on twitter using #tidytuesday and share their plots and code which means you can find plots you like and then peek and see how they did what they did. I've found it really useful myself.
Check out Kaggle. Specifically their ‘Titanic’ challenge which may be relevant.
GLMs really aren't too difficult to wrap your head around (at least for binomials), and the design is simple enough. There's only one predictor, and one observation per subject so no need to mess with any random effects structures. This data might actually be a good introduction to generalized linear models. But like you said, it's probably more than what's required for an ethogram.
I think there is a pipeline that allows you to manipulate the data frame without passing the manipulation back to the dataframe. Its something like %&gt;T%. This would allow you to then use select(-‘experiment time’ , -‘exposure’) to remove the columns temporarily.
Looks like you're not the only one who noticed this. I don't have a solution, but maybe if you follow this issue hopefully someone will have a fix soon. https://github.com/rstudio/rstudio/issues/4572
I don’t have a solution, but I’d recommend getting comfortable list.files() instead.
This!
Also check out David Robinson!! He creates an hour long video for every Tidy Tuesday set where he just goes in blind and does an original data analysis, answering questions he creates in order to write a blog post on it. They’re extremely good and interesting and he has a good personality! I really recommend it. Extremely great way to learn
I think this is what you're going for. Since you're doing this with nesting you'll need to preserve the columns of Experiment to Gene somewhere then unnest those too. your_df &lt;- tibble(Experiment = c("A", "B", "A", "B"), Time = c(1, 1, 2, 2), Exposure = c("Y", "N", "Y", "N"), Gene1 = "text", Gene2 = "text", Gene3 = "text", Stuff = c(1, 2, 3, 4), And = c(17, 20, 15, 8), Things = c(24, 51, 54, 8)) exp_pca &lt;- your_df %&gt;% nest() %&gt;% mutate(excluded_data = map(data, ~.x %&gt;% select(Experiment:Gene3)), pca = map(data, ~prcomp(.x %&gt;% select(Stuff:Things), center = TRUE, scale = FALSE)), pca_tidy = map2(pca, data, ~broom::augment(.x, data = .y %&gt;% select(Stuff:Things)))) exp_pca %&gt;% unnest(excluded_data, pca_tidy) exp_pca %&gt;% unnest(excluded_data, pca_tidy)%&gt;% select(-.rownames) %&gt;% summarize_at(.vars = vars(contains("PC")), .funs = var) %&gt;% gather(key = pc, value = variance) %&gt;% mutate(var_exp = variance/sum(variance)) Hope this helps. If this wasn't what you were going for let me know.
I used to use autohotkey for this. Don't have the code anymore but essentially you can configure it to modify your clipboard and paste where your cursor is whenever you type "rpaste" for instance.
Had not run across his videos yet - they look great, thanks!
They are! I recommend the “Golden Age of Television” one. It’s really good
This one &gt; https://www.kaggle.com/c/titanic
Here's a gallery of workflows and examples of using R in production that you might find interesting: [https://solutions.rstudio.com/](https://solutions.rstudio.com/), including scheduled/parameterized R Markdown reports, REST APIs, Python/Kubernetes, etc.
I'd recommend switching over to using something like the `here` package. One problem with doing things with absolute paths like you're talking about is that it makes your code quite fragile. If, for example you want to share your code with someone, they would have to go in and manually change all those paths. `here` offers a very sustainable solution for this and most other problems involving paths. You can check out a guide on the package [here](https://github.com/jennybc/here_here) and download it easily with install.packages('here') It's great! You'll love it.
I don't know how to crosspost on mobile, but someone just posted a free workshop-esque thing in r/rstats that sounds exactly like what you're looking for.
Having difficulty spelling consistently will make using R rather difficult. You misspelled your own object name and did not notice that on your own. You still have not indented your code by 4 spaces so Reddit will leave it alone. And we still don't know what data you are dealing with.
Indeed. This is why I prefer robust coding approaches.
If I understand your problem correctly, I would advise you start using `normalizePaths`.
This method isn't intended for production or replicablity. But for data exploration it can be quick and easy. Try out the function: file.choose() It opens a GUI and let's you browse files, when you select it outputs the file path. Next use that function in this way: path &lt;- file.choose() This will store that path so it can be called later. No if you need to use that path in a function, say for example read.csv(), just insert the path object instead of typing out the path. read.csv(path, header = T) Give that a go and let me know if you have any quesh
preprocessCore is a Bioconductor package, meaning it has a slightly different install process than packages on CRAN. The installation instructions are here: http://bioconductor.org/packages/release/bioc/html/preprocessCore.html
I have never seen this feature, as I have been using RStudio since it came out. What I have seen is the tab-autocomplete feature inside quotes, which uses '/' instead of '\\', but it doesn't insert 'file:///' notation. Perhaps you were using some add-in? Note that this is probably the wrong place to ask... you should be in r/RStudio.
Nope, go to an earlier version of RStudio and test it out yourself.
thank you!
PS what the etiquette for posting same in other r-channels?
Here's a solution that might work use readClipboard() https://community.rstudio.com/t/paste-windows-path-converting-backslashes-to-slashes-automatically/8851/2
If you want something like sklearn, consider mlr: [https://mlr.mlr-org.com/](https://mlr.mlr-org.com/). Most popular non-deep learning algorithms are implemented in mlr, see [https://mlr.mlr-org.com/articles/tutorial/integrated\_learners.html](https://mlr.mlr-org.com/articles/tutorial/integrated_learners.html) for a list.
Caret is a good go-to library for training predictive models
R is excellent for doing predictions. Take a look at the Task Views section to see what types of analysis you want to perform. http://lib.stat.cmu.edu/R/CRAN/ the caret package is often a good place to start http://lib.stat.cmu.edu/R/CRAN/index.html and it has an introductory pages at https://cran.r-project.org/web/packages/caret/vignettes/caret.html And http://www.rebeccabarter.com/blog/2017-11-17-caret_tutorial/
R is fantastic for this.
How do you want to perform these predictions? R is built from the ground up for exactly this type of thing.
R is good for this and so is sklearn. There tends to be packages for techniques in R, rather than one package that does everything, although caret does a good job of standardising approaches.
Three options: 1. Run locally on another instance of RStudio 2. AWS using the RStudio AMI 3. If you have access to a Linux server, use the Rocker docker images
Thanks for replying. &amp;#x200B; 2) Doesn\`t AWS charge per hour? if this takes 2 weeks then it might be quite costly right? 3) I do have access to a Linux server from a professor (not sure how much he uses it), I am in a different city to him however. Is it possible to virtually run the R script on the Linux server and get it to download the files directly to a dropbox folder?
I just checked some pricing, AWS charges 0.023$ per GB. If I need to process 300GB it would only cost what, $5.75?
why would you even use R for this?
Because I am an economist and I suck so the only think I know is R. There is some very niche packages which helps me locate and download these specific documents also which is only available in R (not even Python).
I think I will go with AWS using R server, looks relatively simple. Thanks again for your suggestions.
ok but why not just use a simple shell script?
The code I have has to; 1) go to the main web page and query it for a given ID company (lets say Google with ID = 0001288776 ) &amp;#x200B; [https://www.sec.gov/edgar/searchedgar/companysearch.html](https://www.sec.gov/edgar/searchedgar/companysearch.html) &amp;#x200B; 2) Get taken to this page &amp;#x200B; [https://www.sec.gov/cgi-bin/browse-edgar?company=google&amp;owner=exclude&amp;action=getcompany](https://www.sec.gov/cgi-bin/browse-edgar?company=google&amp;owner=exclude&amp;action=getcompany) &amp;#x200B; And again filter by ID = 0001288776 &amp;#x200B; 3) Get taken to this page [https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&amp;CIK=0001288776&amp;owner=exclude&amp;count=40&amp;hidefilings=0](https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&amp;CIK=0001288776&amp;owner=exclude&amp;count=40&amp;hidefilings=0) &amp;#x200B; Collect the relevant information on the company and filter the "filings" part of the table by " SC 13G/A". Grab the link that takes me to this page: &amp;#x200B; [https://www.sec.gov/Archives/edgar/data/1288776/000031506615003299/0000315066-15-003299-index.htm](https://www.sec.gov/Archives/edgar/data/1288776/000031506615003299/0000315066-15-003299-index.htm) &amp;#x200B; Then open up "filings.txt" &amp;#x200B; [https://www.sec.gov/Archives/edgar/data/315066/000031506615003299/filing.txt](https://www.sec.gov/Archives/edgar/data/315066/000031506615003299/filing.txt) &amp;#x200B; Save this file and from there I can process the relevant information. &amp;#x200B; There is a few helpful packages in R which helps with all this and I have written something which calls upon these R packages when needed. (It just seems to work in R since I am scraping tables of information for many of the links and saving this information - even though it might be better in another language)
You probably don't need a very powerful instance. A t3.small instance, for example, is 2 cents an hour - would run you just under $7 for two weeks.
Thats awesome thanks for the suggestion! I am registering now for AWS and will try it out with a few observatiosn on the virtual R server and see how it goes. (wish I would have just done this from the beginning.
&gt;Given the time it took to download the 500 companies, I cannot afford to be without my Rstudio for a week or more. You can have multiple R Studios open at once. Or you don't even need to use R Studio, you can run your script in its own R process outside R Studio. I would also see if your department has computing resources available for something like this. By the way, look at `rdrop2` package for interfacing with Dropbox.
look at `rdrop2` package.
Thanks for the \`rdrop2\` suggestion! I will certainly look into implementing it if I can save the file there. &amp;#x200B; I currently have Rstudio installed on a pretty poor laptop (its fine, it does the job I need but nothing high-level). Since I am constantly downloading for approx 2 weeks it should be connected to the internet constantly, the home internet is fine but a university internet connection (or a dedicated AWS connection) would be more reliable, if the internet breaks then I would have some additional problems (I am happy to pay a small fee to eliminate this potential problem), multiple r studios open will affect the performance of my poor laptop (I dont want rstudio to crash on one instance because I am overloading it). I have a computer at the university but I am in a different city for much of the time.
Yeah, that makes sense. I haven't done this, but I think this should be feasible on the Amazon free tier, since you won't need a lot of power, just bandwidth: https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/free-tier-limits.html Using that package, then, it'll plop it right on your Dropbox.
You were spot on that the double arrow &lt;&lt;- is accessing a global variable outside of normal scope. The idea of "cache matrix" is that you're storing stuff there instead of constantly recomputing it. This is sometimes called memoization (NOT "memorization" with an r) The empty parenthesis function definition means there are no arguments. It's written that way as a one liner. If it were my code style, I'd wrap the function body in curly braces. Does that answer most of your questions?
This is one of those times when formatting things in code blocks really helps with comprehension, but I'm also confused about the *reasons* that this code was made the way it was. I've added braces as @featherfooted mentioned, which also clears things up somewhat. makeCacheMatrix &lt;- function(x = matrix()) { m &lt;- NULL set &lt;- function(y) { x &lt;&lt;- y m &lt;&lt;- NULL } get &lt;- function() {x} setInverse &lt;- function() {m &lt;&lt;- solve(x)} getInverse &lt;- function() {m} list(set = set, get = get, setInverse = setInverse, getInverse = getInverse) } cachemean &lt;- function(x, ...) { m &lt;- x$getmean() if(!is.null(m)) { message("getting cached data") return(m) } data &lt;- x$get() m &lt;- mean(data, ...) x$setmean(m) m } &amp;#x200B; Two questions: 1. In the cachemean function, it looks like this function accepts the 'cachematrix' object as an input, so I assume you have another function that makes a `list` object with a `setmean` and `getmean` methods? The cachematrix object you defined in the first function only has `get`, `set`, `getInverse`, and `setInverse` functions. 2. Is there a clear benefit to doing it this way? I don't see one. Setting global variables is a notorious source of bugs, and in this case, the risks of issues seems to outweigh the benefit. The reason that functions have their own namespaces and scopes is so they can be portable. This configuration seems sure to cause issues. If I were you, I would just define functions for all of the tasks you need to do, and then call them without assigning things to the global environment.
I recommend t3.small also. Use this AMI -- it is very good and has built-in dropbox integration. http://www.louisaslett.com/RStudio_AMI/
One option is to use Amazon and Louis Aslett's AMI: http://www.louisaslett.com/RStudio_AMI/ It spins up with integration to dropbox and all kinds of other great features.
R is fantastic for this you motherfucker!
Lolol
"Why don't you learn this whole other technology instead of the one you already know"
Probably
Use Google Cloud Platform. it gives you 300usd free for the trial. It's enough to run a VM for months and with enough space to store hundred of thousands of your files. Otherwise, use Microsoft Azure (also with trial money but not as much) or digital ocean (also with trial money). They only caveat is that you will have to learn to use Bash on Linux, but is not difficult. Look for YouTube tutorials of how to setup your VM.
I am tasked to do it this way :/ and to your #1 yea i had to change it careless error... still learning!
Yes it does so it that makes way more sense!
I think you'll need to forward all of these questions to your professor's office hours because it's very difficult for us to decipher the *purpose* of this code. There are two functions being defined here, `makeCacheMatrix` and `cachemean`. It doesn't seem like get/set mean are even used in the `cachemean` function so I have no idea how they're being used in main script either.
Thank you for your feedback and input, MonthyPythonista! Your brought up some valid points and I will check your posts with your thoughts on R and Python for my follow-up post in English.
Looks interesting but why would I or a company want to pay for this when the Rstudio IDE and jupyter notebooks are serving people well? What added value does the nextjournal have for the average user?
We see multiple value propositions: * In Nextjournal you can easily collaborate in a team in real-time (think Google Docs-style editing code and content together) and securely share secrets between your team members * Everything in a Nextjournal notebook (code, content, all installed packages and all data) is automatically versioned all the time. You don’t have to rely on separate version control systems. You can simply go back to any previous change and restore it. * You can plug in other programming languages if you need them. You can use multiple programming languages in the same notebook, e.g. R + Python together. * You can export a notebook’s full runtime environment as reproducible Docker image and share it e.g. with colleagues that are not on Nextjournal. * It runs in the cloud. You can scale up your hardware as you need it. Nextjournal offers multiple options including GPUs (which is helpful for some Machine Learning tasks). Open Science is always free at Nextjournal (and will stay free!) so feel free to sign up and just try it out for yourself. We still haven’t figured out exactly how our paid offerings should be but I’d love to get more feedback from the community on this.
No worries thank you so much!
I don't recall how the `raster` package works exactly, but I'm familiar with `imager`. Looking at `as.cimg()`, it has a method for RasterLayers, so you may try doing `as.cimg(band1)` to get it to the type expected. Or, if you know how to convert to a matrix and call it, say, `x`, you can do it as: imgx = array(x,dim=c(1882,2726,1,1)) imgx = imgx %&gt;% as.cimg then you have as `imgx` an object of the type `imager` expects.
Love the idea. 1 quick questions and 1 comment: 1. In what folder are data sets stored when uploaded? -&gt; read.csv("...?/table.csv)? 2. I would really need the cmd+z Undo command! cheers
How will this function with respect to the necessity for privacy? Can I run a NextJournal server within my institution? Also, does it play nice with various packages?
1) When you upload a file it is stored in our content-addressed storage. This allows us to version your data together (and in context) with the contents of your notebook. To insert a reference to such a file, you can use the "insert path to file" command (Cmd+E) in any code cell. Btw, the opposite of this is our magical "/results" directory. Whenever you write files to it (e.g. via \`write.csv(...)\`) it will also be put into content-addressed storage and can be referenced in any code cell (no matter what programming language it uses). You can take a look at our quickstart notebook under "Files" for a more detailed description: [https://nextjournal.com/help/quickstart](https://nextjournal.com/help/quickstart) &amp;#x200B; 2) Cmd+Z is available! Type something then hit Cmd+Z to undo.
&gt;How will this function with respect to the necessity for privacy? Everything that you write in Nextjournal is private until you publish it. If you have a [paid plan](https://nextjournal.com/pricing) you can further restrict to whom you publish. Secrets are stored encrypted. &amp;#x200B; &gt;Can I run a NextJournal server within my institution? Nextjournal is a hosted product but we offer on-premise installations for enterprise customers. If you are interested in the details of this, feel free to get in touch via our [website](https://nextjournal.com). &amp;#x200B; &gt;Also, does it play nice with various packages? Yes! You have full access to the whole runtime environment. This means you can install and configure any package you want in whatever way you want.
Changing the max ram usage can be valuable for certain applications.
How can you do this?
For RStudio panes so that the left side has only the script and history (which I always have minimized) and the console is moved to the top right pane. I believe I got this layout from a post on Rbloggers and I cannot use RStudio any other way now. [pics of setup](https://imgur.com/a/ZRgzePo)
never thought of that...
Don't be a noob. Everyone knows the script goes on the right and the console goes on the left!
[https://support.rstudio.com/hc/en-us/community/posts/115000475648-Memory-limit?mobile\_site=true](https://support.rstudio.com/hc/en-us/community/posts/115000475648-Memory-limit?mobile_site=true)
I do it similar, but I like to pop the text-editing panel out into its own window. At least when I have multiple monitors set up. If I'm just on my laptop, I do it this way. Helps a lot when I'm trying to (painstakingly) get a plot just right for a paper or something.
Turn Vi keybindings on and blinking cursor off
I'd use conda to manage r libraries where possible. It will save tons of time over building
https://www.rdocumentation.org/packages/geosphere/versions/1.5-5/topics/distm
change the background colour to blue, less exhausting
If my current memory limit in R is "1.759219e+13" then I shouldn't NEED to increase it right?
lol