It's already done and can be used: http://docs.juliadiffeq.org/latest/analysis/parameter_estimation.html#abc_inference-1 https://github.com/JuliaDiffEq/DiffEqBayes.jl/pull/40 I'm just spacing out release announcements so we don't drown everything else out / write too much. But go ahead and use it: the software is already release, just the announcement isn't.
Thank you, will check it out!
JuliaDB has a bytecode serialization that's use for its out-of-core stuff to make it fast. It's not much of a storage format though. BSON.jl should be speedy? I don't think anyone ever benchmarked all of the options together though.
Have you tried [Parquet](https://github.com/JuliaComputing/Parquet.jl) or [Feather](https://juliadata.github.io/Feather.jl/stable/)? They are both columnar formats based on the same idea and designed for efficient reading. Both have Julia implementations but also work with other languages.
Thank you for the suggestions and I will look into them in the future because I think for now the DataFrames + CSV ecosystem is a bit broken. I updated DataFrames.jl and CSV.jl and I can no longer load the csv file with CSV.read() and then save it in a different format. I can still use readtable() but it results in columns of type Missings.Missing even for columns that do not have missing values and I don't know how to convert the columns to types that can be passed to other packages (such as Distributions) that do not accept Missings.Missing. The data become useless because I don't know how to do basic operations with this type of columns. I also don't know how to clean the missing data per column, in case some columns have missing values in different rows, something that at least from the documentation is easy in JuliaDB. I wanted to load the raw data, save them in an efficient format and then find basic descriptive statistics (e.g. mean value of a variable by one or two variables usually by year and industry) and save the results which takes a couple lines of code in Stata and a few seconds. 
Yes.
Huh? My question was not a yes or no question, unless I'm missing something here..
Julia allows you to pretend that it doesn't care about types. But, when it gets to actually run your code, the compiler will specialize your code to the types that you are using when you call your functions. So Julia is not strongly typed like java, where all types must be declared beforehand, but also not weakly typed like ruby. I guess one could say julia is two languages in one: it is a weakly typed language that compiles to a strongly typed language in run time.
I was hoping someone wouldn't do this nonsense. An answer like this isn't clever or helpful.
Sorry, you're right. I gave a more complete answer below once he responded.
Julia is strongly typed. The compiler can infer types at many occasions, so that one doesn't have to specify them explicitly, but the types are still very much there. Compare to Haskell, where you can write "f x = 2 * x + 1" without declaring types, and the compiler infers them. Or to C++, where you can nowadays declare almost any variable as "auto", and have the compiler infer the type. JIT compiling is a red herring; the semantics of Julia code does not depend on when or whether it is compiled. The main point is that it is impossible in Julia to accidentally re-interpret an integer as a string, and that you can place constraints onto variables that you cannot later circumvent.
&gt; the semantics of Julia code does not depend on when or whether it is compiled in that case, why there is dynamic dispatch?
This is awesome. Nice work.
&gt; Sorry, was being a bit snarky and giving you a not very clear answer to a not very clear question (Yes, julia is either strongly or weakly typed). What is it you actually want to know about? I'd like to know if Julia silently promotes types, e.g. is `1+2.3` an error or is the `1` promoted to `float`? Is `1+"x"` an error? Is the type of `a+b` the same as for `b+a` for all `a` and `b`? Is Julia memory safe? &gt; I suspect you will get a better answer (from someone that knows a hell of a lot more about it than me) I find it interesting that nobody has (as yet) given what I'd consider to be a decent answer. 
 $ julia --lisp; _ ; |_ _ _ |_ _ | . _ _ ; | (-||||_(_)|__|_)|_) ;-------------------|---------------------------------------------------------- &gt; initial-reserved-words (begin while if for try return break continue function macro quote let local global const do struct abstract typealias bitstype type immutable module baremodule using import export importall) &gt; 
The answer to these questions is a bit complicated, because julia allows you to play it loose or to be super strict. You will get much better performance if you're super strict for reasons better explained by people who are experts... But here's some examples: I'd like to know if Julia silently promotes types, e.g. is 1\+2.3 an error or is the 1 promoted to float? The answer to your example doesn't actually answer the question... julia&gt; 1 + 2.3 3.3 But this isn't really "silent" promotion \- there's a method explicitly generated for \+ that promotes the Int. If you want to define a type\-stable sum, you can definitely do that: julia&gt; strict_sum(a::Int, b::Int) = a + b strict_sum (generic function with 1 method) julia&gt; strict_sum(1, 2) 3 julia&gt; strict_sum(1, 2.2) ERROR: MethodError: no method matching strict_sum(::Int64, ::Float64) Closest candidates are: strict_sum(::Int64, ::Int64) at none:1 Stacktrace: [1] macro expansion at /Users/kev/.julia/v0.6/Atom/src/repl.jl:118 [inlined] [2] anonymous at ./&lt;missing&gt;:? Of course, this would be annoying if you want to have \``strict_sum`\` for all types of numbers, but there are some convenient ways to do this. For example: julia&gt; function strict_sum{T&lt;:Number}(a::T, b::T) a + b end strict_sum (generic function with 2 methods) julia&gt; strict_sum(1.2, 2.2) 3.4000000000000004 julia&gt; strict_sum(1.2, 2) ERROR: MethodError: no method matching strict_sum(::Float64, ::Int64) Closest candidates are: strict_sum(::Int64, ::Int64) at none:1 strict_sum(::T&lt;:Number, ::T&lt;:Number) where T&lt;:Number at none:2 Stacktrace: [1] macro expansion at /Users/kev/.julia/v0.6/Atom/src/repl.jl:118 [inlined] [2] anonymous at ./&lt;missing&gt;:? julia&gt; strict_sum(Float32(1), Float32(2)) 3.0f0 julia&gt; strict_sum(Float32(1), Float64(2)) ERROR: MethodError: no method matching strict_sum(::Float32, ::Float64) Closest candidates are: strict_sum(::T&lt;:Number, ::T&lt;:Number) where T&lt;:Number at none:2 Stacktrace: [1] macro expansion at /Users/kev/.julia/v0.6/Atom/src/repl.jl:118 [inlined] [2] anonymous at ./&lt;missing&gt;:? Next question: &gt;Is 1\+"x" an error? yup! julia&gt; 1 + "x" ERROR: MethodError: no method matching +(::Int64, ::String) Closest candidates are: +(::Any, ::Any, ::Any, ::Any...) at operators.jl:424 +(::MicroLogging.LogLevel, ::Any) at /Users/kev/.julia/v0.6/MicroLogging/src/core.jl:89 +(::T&lt;:Union{Int128, Int16, Int32, Int64, Int8, UInt128, UInt16, UInt32, UInt64, UInt8}, ::T&lt;:Union{Int128, Int16, Int32, Int64, Int8, UInt128, UInt16, UInt32, UInt64, UInt8}) where T&lt;:Union{Int128, Int16, Int32, Int64, Int8, UInt128, UInt16, UInt32, UInt64, UInt8} at int.jl:32 ... Stacktrace: [1] macro expansion at /Users/kev/.julia/v0.6/Atom/src/repl.jl:118 [inlined] [2] anonymous at ./&lt;missing&gt;:? But the only reason is because, as the error suggests, no method defined for \+\(::Int, ::String\). In principle, you could define a method if you wanted to \(note: it's not recommended that you extend methods on types that you didn't create, because it can muck with other people's code \- it's called "type piracy" \- but you can do it\). julia&gt; import Base.+ # have to explicitly import to extend a function julia&gt; +(a::Int, b::String) = "$a$b" + (generic function with 267 methods) julia&gt; 1 + "a" "1a" julia&gt; "a" + 1 ERROR: MethodError: no method matching +(::String, ::Int64) Closest candidates are: +(::Any, ::Any, ::Any, ::Any...) at operators.jl:424 +(::Complex{Bool}, ::Real) at complex.jl:247 +(::Char, ::Integer) at char.jl:40 ... Stacktrace: [1] macro expansion at /Users/kev/.julia/v0.6/Atom/src/repl.jl:118 [inlined] [2] anonymous at ./&lt;missing&gt;:? julia&gt; +(a::String, b::Int) = +(b, a) + (generic function with 268 methods) julia&gt; "a" + 1 "1a" &gt;Is the type of a\+b the same as for b\+a for all a and b? Yes, but again, that's only because the authors of Base defined it that way. You can see me violating it above when I defined a method for \+\(::String, ::Int\) but not \+\(::Int, ::String\). Functions don't have to return the same types \- there's nothing in principle wrong with the following function: julia&gt; function stupid(x::Int) if x &lt; 0 return "Negative!" else return x end end stupid (generic function with 1 method) julia&gt; stupid(4) 4 julia&gt; stupid(-3) "Negative!" But, if you want your code to be performant, writing type\-stable functions is highly recommended. And for the most part, the authors of the language are sticklers for high performance, but they allow users and package authors to do things that aren't performant. &gt;Is Julia memory safe? Someone else needs to answer this one. I think the answer is "yes by default, no if you want to explicitly be unsafe," but I'm not sure. I highly recommend [reading the docs](https://docs.julialang.org/en/stable/manual/) on types and methods \- they're very readable and will probably answer many of your questions better than I can. &gt;I find it interesting that nobody has \(as yet\) given what I'd consider to be a decent answer. The question is not particularly specific, since as mentioned in the SO post I linked, the term "strictly typed" is not very precise. I think people have given answers that are as good as one might expect given the vague nature of the question. 
You're at the limits of my knowledge - I'm a biologist that writes code, not an expert by any means. I know there's some tooling like macros that will warn about the changes in your functions and even a way to spit out the stuff the compiler see to inspect it (check out the "performance" section of the docs there are a bunch of tips there). As to inferred static types and algebraic datatypes I don't know what that means. Really, check out the `types` section of the docs... It's narrative and very thorough with lots of examples - I learned most of what I know from that document.
Wow, I love this.
An easy way to access these is: in the terminal press `Tab` consecutively twice.
Useful line from the unstack docs: &gt; rowkey : the column with a unique key for each row, if not given, find a key by grouping on anything not a colkey or value So omitting the rowkey works in this case: unstack(df, :cell_population, :value) 
&gt;What does the inferred static type system look like, e.g. does it have algebraic datatypes? It tries not to. While Julia v0.7 will have special code generation to make union types faster (which will be great for dealing with missing data), it will see when it can keep a concrete type and then try to make types as concrete as possible. `@code_warntype` shows you the full typing inferred in your code and adds big warnings for any uninferred portions. Traceur.jl is a nice tool for diagnosing typing issues. As you write more Julia though, whether there is a type-instability becomes pretty obvious, and then it's more of a coding choice. &gt;Sounds like Julia does overloading but not promotion. If it is memory safe then I'd call that strongly typed, the same as Python. Yes. Julia is all about overloading. Generic functions like `f(x)` actually auto-specialize down to concrete types, so if you call `f(2)` it generates and calls `f(x::Int)`. It then does this the whole way through to generate a concretely typed code when it can, otherwise it boxes and creates algebraic data types (unions or Any) to hold the result. "Promotion" is actually just overloads that are type-stable with a determined result, for example `+(x::Int,y::Float64)` is a function that converts `x` to a `Float64`and then does `+(x::Float64,y::Float64)` which is then an LLVM instruction. So yes, it's strongly typed. That strong typing, coupled with multiple dispatch and the function auto-specialization is what lets it generate about the same code as C compiled with clang, which is why the speeds come out "like C" (and in actuality, you get the same LLVM IR so it's not just like C, it is the same target as C+clang).
It's weakly typed at parse time, but the Julia AST is typed (that's what's shown by `@code_warntype`) which happens before compilation (`@code_llvm`) 
Cool! I hadn't heard of your InternedStrings.jl package but it sounds very useful for some applications.
Another one is [StringCases.jl](https://github.com/djsegal/StringCases.jl) which I made for changing the cases of strings (e.g. camelcase) While, I also like [EnglishText.jl](https://github.com/TotalVerb/EnglishText.jl) for singularizing and pluralizing words
Another one like StringCases and EnglishText.jl is [Humanize.jl](https://github.com/IainNZ/Humanize.jl)
I would love to see that animation in there!
Small point: the link to the official julia website in the second paragraph links to https://blog.cerebralab.com/julialang.org instead of julialang.org
assign_value should be named differently, since it doesn't assign anything. Just julia maybe ? You can also write julia_set as a one liner: julia_set(height, width, c) = [julia(x, y, width, height, c) for x = 1:height, y = 1:width] Otherwise you really want to include an image in the article.
True, I should probably simplify that function. Initially I left it as the for since I was trying to use @parallel inside of it, but the computations are fast enough that parallel just slows it down. I will probably re-write it as such, looks much better.
&gt; I would love to see that animation in there! Didn't include them since they would slow down page loading for people with bad connections, I did provide links to them right after the code to generate them. &gt; As a general point, there are a lot of very introductory articles. That's fine. But if you want to make a mark, write an introduction to your specific field with Julia. For example, I do stuff a lot with (stochastic) differential equations, and so there's a lot to write about there. For you, if your thing is fractals, writing articles showing how to calculate fractal dimension, fractal estimation of diffeq solutions (for detecting chaos) with DynamicalSystems.jl, etc. would be great ways to give a very unique take on Julia which can be a great introductory source to people like you. Actually I'm fully in the dark when it comes to geometry/topology. I just though fractals would be good for an introductory article since they are fun to play around. I'm actually writing a series of articles that are more related to what I use the language for (which is focused on how to use it on very large &amp; high dimension datasets to implement algorithms that run relatively fast without needing highly specialized hardware). However, I wanted an "introductory" article for the series that brings in some of the concepts I'd be working with and is easy to follow. I also don't necessarily have a specific field in which I work, I usually just do whatever pays the bills, so I'm not sure I could bring out a very specialized article that is of any use. But I do agree with your feelings overall, and I am trying to make the next Julia-related article[s] I released more focused towards tackling a hard "real world" problems in Julia, it's just that writing such an article takes a long time and there's many ways to screw it up or produce something that's subpar or untrue.
A static image would be fine. That said your image is way too large (8k times 8k pixels) for a 800px grid. I'm not sure how the png function works, maybe it exports at 300dpi or something.
This is quite lightweight, but heavy on the graphics.
Nice work! I enjoyed reading your perspective. I’d agree that some graphical content would be nice, but don’t go mad, perhaps, like I do.... A few typos: eg “gorse mistakes” (unlucky, a spell checker wouldn’t complain about that!). 
Interesting first article, thanks. I'd put at least an image of the generated Julia set in the article. Your Julia code might seem short compared to Rust and C++ but it feels quite verbose to me. The loop tends to look simpler as a tail recursive function (OCaml/F# code): let rec juliaLoop c z i = if i &gt;= 255 || magnitude z &gt; 2 then i else juliaLoop c (z*z+c) (i+1) I wonder what this looks like in Mathematica/Wolfram Language... 
Ty sir. 
If only Atom weren't so sluggish and slow on older laptops. Using Juno, miss the responsiveness and speed of Sublime Text.
I hear that... but I am almost always using a superpowered desktop for solving SPDEs, and so Atom is a breeze on that :P. Another option is VSCode which is supposedly a bit more responsive, but lacks some of Juno's nicest features.
I am quite happy with VSCode. Out of curiosity, what Juno features are lacking in VSCode?
I downloaded the free version of JuliaPro. Do you know if/how that relates to Juno? They are both built on top of Atom; which do you prefer?
Hah well that makes it easier. Thanks
VSCode doesn't have Juno's debugger. It doesn't have Juno's tree display rendering system. It doesn't have Juno's workspace or docs viewer (last time I checked). It doesn't have Juno's profiler integrations. It doesn't have Juno's nested progress bars. Those are the things off the top of my head. Don't get me wrong: VSCode is nice. But it is objectively not feature-complete with Juno.
I thought the debugger was not working. Is it functional with v0.6? 
It's there and documented. It works fine on v0.6 via ASTInterpreter2, and has been for almost 9 months now? http://docs.junolab.org/latest/man/basic_usage.html#Using-the-Debugger-(experimental)-1 It doesn't have breakpoints yet, but you can make it break at current curser which ends up being about the same thing.
I usually prefer Option B.
Both are perfectly reasonable. Here are a few questions you might ask yourself to help decide: * Is the method you're asking for something that has a canonical definition, and which might be useful elsewhere? For example: comparison, conversion to string, etc. If so, then asking users to define that method for their types might be beneficial. * Or is a user likely to want to pass in the same type of data to your function multiple times, and with different functions to be used? If that's true, then it doesn't make sense to have a canonical definition, and you would want to go with option B. The Base `sort` implementation does both option A and option B, because many items have a "natural" ordering, but users will still often want to use some other ordering from time to time. By default, it compares your items with `Base.isless`, which you can implement for your own type. But if you want to sort using some other comparison, you can simply pass in a different comparison function with the `lt` keyword, as in `sort([1, 2, 3], lt = (x, y) -&gt; x^2 &lt; y^2)` 
Oh, sorry for any noobish mistake I may have committed. First time dabbling with CAEs, and not much experience with Julia either.
As awesome as Jupyter is, it is an extremely different IDE than RStudio.
&gt; What is a CAE? (I'm guessing Convolutional Auto Encoder ) Guessed right :) &gt; are you translating a model from another framework (like TF) into Flux? Nope, just trying my hands on this. Ideally I'd like to try both upsampling and transposed convolution, but I just can't make it work. The example script I posted is a subset of what I'm trying to do, but I have no idea why it doesn't work.
Maybe I'm misunderstanding your code, but it looks like your loss function here is trying to calculate the mean squared error between the model's prediction from an x input, and that x input itself, instead of the true result \(maybe y\). Maybe you meant for the loss function to be something like this instead? loss(x) = mse(model(x), y)
It's an (convolutional) autoencoder, that's why the prediction is compared with the input itself.
It's a convolutional autoencoder (CAE). That's why the prediction is compared to the input itself. AEs try to reconstruct the input signal from a reduced representation.
Oh, whoops, you're right, I guess I read your post too quickly! Sorry about that.
The authors hangs out on Discourse and Slack. You could raise an issue on GitHub. 
I really like Flux.jl. http://fluxml.ai/ It's being actively developed, has really concise syntax to make building NNs easy. I found it much easier to use than any other ML library that I've used. Since it's written in Julia instead of calling kernels written in C, it has the usual advantages like the fact that you can use Julia defined functions anywhere and introspect / add features at will. It has the JuliaComputing backing so I expect development to keep going strong. Flux is still missing some things, but that's the only negative I can say. By the end of the GSoC though I would expect it to be one of the most feature-filled with tons of different models / tutorials to build from.
SOLVED (I GUESS): So, the `data` argument expects iterable structures, and both `x` and `y` can be concatenated with `zip`. Just like in the autoencoder example at the model zoo, using `zip` solves the issue. @epochs 10 Flux.train!(loss, zip(train), opt, cb = evalcb) Haven't explored it much further though (for one I'm not entirely sure what `zip` actually accomplishes, will have to delve into the code).
thanks, will ask on slack then.
Mxnet.jl is probably your best bet
*wondering
I was wondering while wandering 😉
Uhhh I'm pretty sure Knet is being actively developed. 26 days is not that long of a time to not have a commit dude. Probably the same story for Mocha but I don't use it.
I've used Flux, MXNet, and Knet, and they are all great. I think Knet is probably the fastest, but it is more low-level. 
In theory it's possible. It's still a work-in-progress. You can follow the repo here: https://github.com/JuliaLang/PackageCompiler.jl Most simpler codes can already use it. More involved packages need modifications to work with it. But there are cases which show it is possible on large codes. Makie.jl is a full plotting library with OpenGL integration completely written in Julia and it can statically compile via PackageCompiler.jl. Some of the v1.0 tooling like the better LLVM.jl integration and Cassette.jl will definitely make this easier.
This seems quite promising: https://github.com/NHDaly/ApplicationBuilder.jl
How does it compare to keras+tensorflow? 
Usability-wise it's amazing how Flux is. Extension-wise, it's hard/impossible to extend Tensorflow, while Flux is easy to extend (you can put anything in there, Mike put a [differential equation solver as a layer](https://twitter.com/ChrisRackauckas/status/995710429161701376)). Benchmarks and GPU wise? That isn't known right now, but the devs are working on it. That's the easy part though and can always get tweaked. 
arrange it so you can put using Structs.jl in your source code you can even set the module path at runtime push!(LOAD_PATH, "/path/to/source") 
DifferentialEquations.jl's native Julia solvers allow arbitrary number types, so AD and tracking works through the whole ODE solver. Why would you do it? There are very good reasons why that are hard to explain but will be shown later.
So I need to manually add the current working directory to LOAD_PATH, and then import the module with using? That seems very hacky 
It seems to work with [julia-vim](https://github.com/JuliaEditorSupport/julia-vim), but if you're not running that then you'll need this: autocmd BufNewFile,BufRead *.jl set ft=julia in your .vimrc so that vim knows it's dealing with a Julia file. The other slight anomaly is that you need to switch over to the terminal and type a 'G' so that the terminal follows the Julia output from REPL. I'm fairly used to using Nvim-R, which is great for R. Nvim-R has a lot more functionality. For example I've configured '\dj' and '\dk' to check the head() and tail() of objects under the cursor. vimcmdline is far more basic - but it's nice to be able to press space to send a line of code to the REPL.
Or you need to put Struct.jl in your existing module path. There are plenty of hacky things in Julia for it to work in existing file systems on multiple OSes. I do find it strange it doesn't look in pwd for a module by default. That must have been a conscious decision at some point with a rationale.
Totally not my area of expertise, but maybe these could help: "Relative and absolute module paths" in https://docs.julialang.org/en/stable/manual/modules/#modules-1 , and https://github.com/JuliaLang/julia/issues/4600 (specifically, the many comments with different current worksarounds). 
&gt;We have enough problems with sexism and recruiting women in this industry, creating such an obviously sexed name is just cherry on the icing. If you think just calling a language a very common female name is the same as choosing a "sexed" name perhaps *you* are the person with the sexism issue, if you associate everything that has a female ring to it with sex, hm? Think hard about this, mate, and chill. Julia is a perfectly good name for the language and it's really dumb to think this is sexist or sexed or really anything other than an easily pronounceable and easily remembered name quite different from your, if you don't mind, *horrible* suggestions of "Sharp-J" or "SKEB" which are hard to say and hard to remember gibberish.
You're also a guy. What if they called it "Jared"? How would you feel if you were in a DS team and you constantly had to name-drop "Jared"? Imagine you were the only guy in a team with 19 other women. And the language was co-created by women. Besides your name, that would be the only "guy" name thrown around daily. It would stand out to you psychologically, whether you'd like it or not. 
It's not named after a woman. Gaston Julia was a French mathematician who studied fractals, a contemporary of Mandelbrot.
That's not what Stefan said at JuliaCon '14. But whatever, good luck with your moral crusade.
I think this kind of posting is counter productive and should be banned. It's meant to trigger a reaction, not cause a discussion (even the title is an imperative). If you really wish to pursue this childish endavour do so by emailing the maintainers. However, Julia is a wonderful name, it's short, catchy and in this context actually refers to a guy (see Julia sets). Fun fact, the most popular people named Julia, are by far those from a Roman family, known for producing amongst others, a lady named Caesar (or Iulia more accurately, but names and pronounciations and letters evolve in wierd ways, I don't think "J" was even a letter at the time, but it's closer to how they'd pronounce it)
If you believe this strongly enough, open an issue on github and talk to the language maintainers directly. 
Ahm, good bot I guess. That is, if you are wired properly and can understand context.
You make an excellent point, and I definitely think this should be addressed. I can’t tell you the number of people who dreamed of being plumbers, but left the profession because they were named John or Lou. Back in the real world, Julia giving a talk about Julia might start with a corny joke about her name as an icebreaker, and that *will be* unfortunate, but it won’t be that bad.
It would be fine if it was named Jared. It's a computing language name. Do you also find a problem with the Ruby language? 
Call it Akobo, sounds bad ass to me
Look at OP’s post history before falling for the bait lol
 julia&gt; x = 1:10; y = x.^2; y[x.&lt;5] 4-element Array{Int64,1}: 1 4 9 16
Thank you! would have never figured out the dot after `x`.
Thank you! ..was wondering if there is a place where such type of syntax is documented? Official docs introduces the [find](https://docs.julialang.org/en/stable/stdlib/arrays/#Base.find-Tuple{Any}) method with limited set of examples.
`.` is to element-wise (broadcast) any operation.
Thank you!
It would, though I struggle to see how it could be done. Maybe adding a bonus to to score for each ingredient with a nontrivial amount \(that would be MIP\) 
Hey thanks for the tip. I will check my Julia version first. In terms of evaluating the using Luxor line—how would it get missed if I just shift+enter to run the body of code? And when I get back to my computer I will edit with the actual code. Thanks again. 
Thanks for fielding this! I’m out on a hike at the moment... 😅
Hi there. That did indeed work, but now I am getting a 'failed to precompile' error. I updated the main post above to show the new problem, if you have a minute. Thanks for your help.
First suggestion, back out and try again. ie quit Juno/Atom, start a fresh session. Sometimes errors disappear (and were caused by some interaction that's hard to reproduce.) Second suggestion: do a `Pkg.update()` at least once. Third suggestion: do installations at the REPL in the terminal, and post results... If a package isn't precompiling, that might be because other packages it requires also don't precompile. Typically, I find that if Homebrew and Cairo successfully install and precompile, Luxor will too. Fourth and subsequent suggestions: (!) Try a fresh installation of Julia (in case your current one has become corrupted; try as another user; restart your machine; update the OS; etc. You probably know the rituals... :)
So I got the program to run in Atom by installing the package via repl and then restarting atom, as you suggested. Thank you. But, as opposed to when I run it from my terminal Julia repl, in Atom, even though it runs without error, it produces no output. Any ideas on that one?
At this point you’re probably going to have to specify in detail the machine, OS, version numbers, directory paths, the code you’re trying to run, etc, since it looks like your installation isn’t working the way it’s expected to. That way we can establish what’s different between my system and yours. (Very few installations are identical.)
Good news! but as I see -this is only "v0.7.0-alpha" ( Alpha version - Pre-release ) https://github.com/JuliaLang/julia/releases
The title is false. Please fix it to say that 0.7-alpha was released. The alpha phase has started but v0.7 has not been released.
&gt; It seems the progress bar for 1.0 has been replaced with 0.7, which they describe as "An intermediate release feature-equivalent to 1.0 but with deprecations." Yes, that has always been the plan.
0.6.3 was just released, as well as 0.7-alpha. Still exciting. Bad title.
Haven't we had Julia 0.7 alpha for a while now?
One of the problems with Reddit is that titles can never be edited. The advice to error-prone OPs is to delete quickly and re-submit before upvotes and comments accumulate... 😕
I was interested to read the list of 0.7, mostly because I had never used most of the constructs! I started in 0.4 and my code was breaking with every release I kid of gave up maintaining it. Quite annoying when my finals were coming around and I needed my old libraries. Anyway, great to see the 0.7/1.0 milestone and thanks for some explanation. Now I can learn Julia again and fix my damn code.
I like the arguments deconstruction thing. Looks very functional.
I agree breaking is annoying. I have a 0.5 library I wrote, that I really like the idea of, and I am yet to work out how to port it to 0.6. To be fair though that library uses undocumented reflection functionality. On the other hand my first ever julia package was written for 0.3. It work on every version from 0.3\-0.7 \(though I've not checked 0.7 in a while so it might have broken\). It is a very simple package though I also picked up some of the first 0.3 code I ever wrote 3 week ago \(a short function\), and mange to get it working without too much trouble. I am definitely looking forward to less churn, on that kinda thing.
[My code](https://github.com/lawless-m/XlsxWriter.jl) doesn't do anything particularly fancy, it's just a wrapper to pycall. I'm sure it is easily fixable. The main problem is I don't understand the error message (well, I know what the words mean, I just don't understand how it comes to be - and I think it is different on 0.6 and 0.7) I think it's mainly the switch from type to struct. At least now I have the confidence to spend time fixing it without it breaking again soon. I don't mind the churn really, I'd rather that than cruft and workarounds 
well it comes with version numbers starting with zero. btw the problem goes deeper than breaks. you might want to rewrite a few things to take advantage of the new stuff.
Nice work Lyndon. This is a very good summary of the user-facing changes. My favorite change is the broadcast implementation change (to lazy type wrappers instead of at parsing time, making it easier to overload), but that's a pretty niche thing :). 
This one I didn't see the benefit. If you're handing a more complex data structure to a function, why not keep interacting with that object as-is, than do some transformation/unpacking? Seems like each interaction would be a bit more taxing, but it would also reinforce the object's nature, a little redundancy in the self-documenting code we all strive for. For example, f(Point(x,y)) getting unpacked into floats x and y instead of staying a Point object gets rid of a lot of "reminders" and leaves x and y "vulnerable" to being interacted with without the Point syntax as an intermediary. Is there a tenet of functional programming or Julia that I'm missing?
It's super useful when, for example, you want to operate on things which naturally come in pairs or tuples, like when iterating over a dictionary. For example, the `do`-block syntax makes it easy to call a function on every element of a collection (like .each in Ruby): julia&gt; foreach([1, 2, 3]) do x println(x) end 1 2 3 but if the thing your iterating over is a collection of pairs (like a Dict), then it's a bit awkward to access just one part of each pair: julia&gt; d = Dict(:a =&gt; 1, :b =&gt; 2) Dict{Symbol,Int64} with 2 entries: :a =&gt; 1 :b =&gt; 2 julia&gt; foreach(d) do kv key, value = kv println(key, "=", value) end a=1 b=2 In Julia v0.7, we can unpack the key and value directly in the argument to the `do`-block: julia&gt; foreach(d) do (key, value) println(key, "=", value) end a=1 b=2 
For the type to struct you can just search and replace "type" to "mutable struct". I would also advice using `PkgDev.generate()`to create the standard folder and file structure (src, test, ...) for you package.
thanks. The new Pkg3 stuff sounds intersting
I was so confused! 
But why put the unpacking in the function call, instead of just outside or just inside the function? That's kinda more what I was after. The trade-offs don't seem to offer any advantage, and it strikes me as weird enough (compared to other languages) syntax-wise that it won't prove very intuitive to people learning Julia.
Python 2.7 has the same tuple destructuring. I find it pretty convenient since it's one less line of code. If you use languages like Haskell and Ocaml, tuple destructuring actually seems pretty basic. Haskell and Ocaml does destructuring for all types.
And it's not all that useful in Python, either, really. It saves a few lines of code here or there, but it's the putting it in the function call that's got me. I just don't see the gain in putting the unpacking there instead of anywhere else. Maybe it's the first time I've found syntax subjectively distasteful.
I suggest it is better to wait for Femtocleaner to be updated, then let it fix things like \`type\` and \`mutable struct\`, 
I am looking forward to that too \(It is on my list of things that should get their own posts\) \(The 0.6 change broke TensorFlow.jl until Mike fixed it by TakingBroadcastingSeriously.jl\) 
A common pattern for me is: \`\`\` map\(enumerate\(zip\(xs, ys\)\)\) do \(ii, \(x, y\)\) #code end \`\`\` And that is now possible. In 0.6 I would write that: \`\`\` map\(enumerate\(zip\(xs, ys\)\)\) do arg \(ii, \(x, y\)\) = arg #code end \`\`\` Temporary variables that don't have meaning of their own are not great
That strikes me as a time to iterate directly over the indices, C++ style, instead of all the nested calls to build the iterated object in one line with the loop call. Is there something about those constructions that gets optimized in Julia? Maybe it avoids mutation/state changes? I'm afraid I'm turning into a hidebound old fart. Is this an early symptom? Can it be treated? Did we catch it in time?!?!
No, to my knowledge there is no special optimization of this. Infact I suspect it is slower (BenchmarkTools.jl `@btime` would be the way to find out). I personally find this easier to read. But I completely understand others disagreeing
Thanks for tolerating my questions. I'm still concerned about your silence on my prognosis, though. I'm afraid I'll start chasing whippersnappers off my lawn during the next full moon, and complaining that we had dated folders for version control back in my day. XD
that was great!
A table showing the results would be nice.
[Imgur](https://i.imgur.com/rtjYUoj.png)
This was offered as an explanation: https://nullprogram.com/blog/2018/05/27/
Is this notebook available somewhere? I liked the video quite a bit, but I would like to look at the lines of code of these examples myself to better digest it.
Both VSCode and Atom are both based on Electron, so if this acquisition were to happen, Electron would still be supported and given Atom's community following, I doubt it would wither if MS pulled the plug. Disclaimer: I haven't done much developer on the Julia plugins for either editor, so take the following with a grain of salt. Corrections and suggestions are welcome. As for porting Juno, from what I understand, it uses a custom IDE interface called Ink while VSCode uses the Language Server specification to support IDE\-type features. Unfortunately, VSCode prohibits plugins from creating custom widgets and styling, so some of Juno's features, like inline execution, may not be directly portable; however, there is already a Language Server for Julia on VSCode which is fairly good. Atom recently added its own IDE interface which also uses the Language Server specification. Personally, I would really like to see Juno and LanguageServer.jl join forces \(if they haven't already\) so improvements to a single project could benifit both editor. 
If there is a unification perhaps it is not so bad. Juno and VS Code with the Julia extension are currently the best Julia editors out there. If they join forces into a single project it will probably advance much faster.
[There you go](https://github.com/alanedelman/YouTubeNotebooks/blob/master/Automatic%20Differentiation%20in%2010%20Minutes.ipynb).
I do not like Atom at all, so switching to VS Code would be a good thing in my eyes. 
Thank you!
Very nice! Perfect for someone who just gets started with ML in julia.
Nice overall but the intro could have some discussion of the comparative strengths of Julia for ML, maybe look at the Philosophy section of Knet manual. Now it sounds a bit like "yet another ML tutorial", it's not very clear why one should care. Maybe cite https://julialang.org/blog/2017/12/ml&amp;pl and Flux.jl
&gt; Nice overall but the intro could have some discussion of the comparative strengths of Julia for ML I feel like this is what I wanted it to be, and, considering the article you linked was one of the inspirations for this post, just makes this feeling stronger. I should probably cite it, I always forget to keep a list of citations and just add them hap-hazardly at the end. That's why I chose to hilight stuff like the ability to chose hardware (CPU or GPU) used for computation by just changing the array type and the ability to use higher level abstractions for defining the model's loss function and gradient without using a complicated "compile at runtime" framework. After all, essentially all the code is Julia code. Which is in stark contrast with a Tensorflow/MXnet/Pytorch model, where the code is essentially just a list of instructions for the framework to build and train the model. But may I'm miss-interpreting what you are saying, or maybe I'm doing a shit job at getting that point across in the article.
&gt;Or maybe I'm doing a shit job at getting that point across in the article. I think the core of the article is fine, it's just the intro that should motivate a bit more the subject. It's probably not clear to everybody that Tensorflow-like frameworks have some issues and that Julia might be a good solution.
Wow, I'm impressed by how clean the code is.
I hope this helps: [Symbolic math with Julia](http://wiener.math.csi.cuny.edu/~verzani/tmp/julia/symbolic.html).
 using SymPy @syms t @symfuns x y A = [1 2; -1 1] Y = [x(t), y(t)] Yp = diff.(Y, t) dsolve(Yp - A*Y) gives Eq(x(t), 2*(C1*sin(sqrt(2)*t) + C2*cos(sqrt(2)*t))*exp(t)) Eq(y(t), (sqrt(2)*C1*cos(sqrt(2)*t) - sqrt(2)*C2*sin(sqrt(2)*t))*exp(t))
A simple and neat introduction, thanks for the link. I'm having trouble with Practice questions under "Integration" though. The Practice questions in other parts work fine, but in Integration, the first question gives a different answer not among the choices (2*log(log(x))), the output for second one (209952) that I get from the REPL is deemed Incorrect by the site, and the third integration just dies with a `PyError` (`&lt;type 'exceptions.AttributeError'&gt;`). Am I missing something or the exercises wrong for that part alone? 
And in case you ever need to numerically solve a matrix equation: http://docs.juliadiffeq.org/latest/tutorials/ode_example.html#Example-3:-Using-Other-Types-for-Systems-of-Equations-1
Similarly, for algebraic systems: @syms a b c A = [2a b; a*c c^2] b = [1, 2] x = simplify.(A\b) gives (2*b - c^2)/(a*c*(b - 2*c)) (c - 4)/(c*(b - 2*c))
brilliant! thanks, i'll try to go over everything to see what works and what doesn't, lets see if i can finally uninstall that resource needy bastard that is matlab
I would think you could do: [readtable(path * "/" * f) for f in files] to avoid `cd`? I suspect that data preparation and pre-processing would be an important first step, before loading data into your program.
yes, if there are different numbers of columns in each row, then it sounds like the data needs a touch of pre-processing - something like awk could do this fairly easily and probably concatenate into one csv too.
Thank you soooo much! Everything works with CSV package.
The best resource to learn Julia is to pick up a project you need done, and start asking questions [on the Julia Discourse](https://discourse.julialang.org/), [in the Julia Slack](https://slackinvite.julialang.org/), or [on StackOverflow](https://stackoverflow.com/questions/tagged/julia-lang). Sure, [the manual](https://docs.julialang.org/en/stable/index.html) or [my workshop notes](http://ucidatascienceinitiative.github.io/IntroToJulia/) can be good resources, but learning a language requires moving there and trying to use it. You won't learn it until you start asking questions and learn your own workflow.
Also, check out the [Intro to Julia](https://www.youtube.com/watch?v=o6GOc6Sxy9Q) video. This one is from yesterday, they're presented live once a month (with a Live chat to ask questions in) and I've found them very useful to get a feel for the language. It also has instructions to follow along with the code interactively, and they're presented starting at a beginner level. 
You might want to check out this [Intro to Julia video](https://www.youtube.com/watch?v=o6GOc6Sxy9Q). You don't even need to have Julia installed on your computer to follow along.
#### [Intro to Julia](https://www.youtube.com/watch?v=o6GOc6Sxy9Q.) ##### 527 views &amp;nbsp;👍36 👎0 *** Description: Julia: Looks like Python, feels like Lisp, runs like FortranThis is a repeat of our (monthly) intro to Julia tutorial on June 8 that will show you som... *The Julia Language, Streamed live 14 hours ago* *** ^(Beep Boop. I'm a bot! This content was auto-generated to provide Youtube details.) | [Opt Out](http://np.reddit.com/r/YTubeInfoBot/wiki/index) | [More Info](http://np.reddit.com/r/YTubeInfoBot/)
Try coursera or edx
Sounds like a fine approach to me in the "tidy data" spirit. However it's a bit convoluted given how simple the problem is when thinking in terms of row sums. Another solution in Julia would be to simply loop over columns and add 1 to the new column each time a value above 100 is found. That's an example where the code would be relatively short and intuitive, and would probably beat dplyr/DataFramesMeta by a large margin in terms of performance.
Maybe there could be a code-review section for beginners. Something like [this!](https://codereview.stackexchange.com) . I don't think it's necessary to respond to every code review query as that won't be feasible (and people will start posting homework problems ) however having some reviewed code for others can be helpful. 
There is a Julia tag for CodeReview which I have answered for. There's also a #helpdesk channel in the Slack. So these things already exist in the Julia community if you want to use them! But the best way to get a code review is to do a PR to a package. If you do a PR to a package, then you'll get a deep and thorough code review from someone who knows Julia well (if to Julia Base, it could be the core developers of the language themselves). Don't deny yourself this learning opportunity.
This is a blog post from the Juno Google Summer of Code project. I am quite impressed by the DiffEq solution treeview: this is a very nice way to investigate types in the REPL! And the linting + performance displays are looking great!
Thanks - yes I'm sure that would be a lot faster, though I understand that the dataframe object has some inefficiencies. I'm guessing it would be a lot faster to loop 'downwards' first, then across? However, for the type of data analysis I do, I much prefer a clear syntax, like the tidy approach. In base R (i.e. using direct subsetting of the object), it can be quite easy to introduce some nasty errors, and I end up with object-naming hell.
What's going on here? A full Julia rewrite of the FEniCS stack (that would be \*so\* nice)? Or just a Julia \-\&gt; PyCall \-\&gt; DOLFIN/UFL/etc.? Will any work be done on the dolfin\-adjoint stuff too?
It's the latter: Julia -&gt; PyCall -&gt; DOLFIN/UFL/etc. The first GSoC on it got it working, now this one is getting the full tutorial to work, along with getting Julia-based tools the ability to interact with the resulting operators and solutions (for plotting). 
It's just stylistic but I find the function to be a bit long and messy (lot's of if's). Maybe you could make method for the "if m.captures[3] != nothing" and use dispatch instead of the conditional. I would even do another function for the regex, that return variables with proper names instead of using the "m.captures[3]" which aren't very telling. 
Thanks, I can see what you mean, I've split the code that updates the tree into a separate function, leaving the main function to deal with only file-handling and parsing. And I've updated the regex to use more meaningful capture group names, that does make a big difference to readability. In addiiton to the readability issues, do the semantics of the program make sense? I've never had to use a `mutable struct` before, is its usage here jusitfied and done properly? Storing what's essentially a tree structure into a `Dict` (even if it's called `tree`) feels a bit weird, but seems to make sense here - since the data comes in as disparate nodes and subtrees that connect later - does this seem like a good way of doing it? And is using `Nullable`s still considered good practice (or should I be using a `Union{Prog, Void}` or something)? Is `.value.name` the right way to access a `Nullable` variable's properties, or is there a better way? 
I don't see anything on the Nim website discussing features that are the reason I use Julia. Julia is for developing high quality fast libraries, quickly with dynamic programming with the option to compile, with lots of numeric computation libaries built in. From the looks of it, Nim is designed to be more of a systems level language that's a clean wrapper for C. If that's the case, I think Go is more of a competitor for Nim than Julia. To me, the main competitors for Julia are Python, R, and Mathematica/Wolfram.
Nim is just pretty different. There was an attempt at a REPL, but [it was killed because it was pretty fundamentally incompatible with the language's design](https://github.com/nim-lang/Nim/issues/5886). Julia is all about interactive scientific usage, so this is a pretty big difference between them. This does mean that Nim currently has better support for building binaries than Julia (I mean, it compiles to C so it literally builds the same binaries as C), but that's well on its way to changing with the Julia core developer focus being on the compiler since the v0.7-alpha dropped. Nim does have some nice things going for it, but it doesn't have the scientific computing focus. Nim's standard arrays are value-types instead of reference types, so more akin to StaticArrays.jl than a Julia array. That means the built in array isn't really designed for "big data" the same way a Julia Array or a NumPy array is. Multi-dimensional array handling isn't built in (and it [doesn't look like it will be either](https://forum.nim-lang.org/t/230)), and while someone has been [building a linear algebra library](https://forum.nim-lang.org/t/1918), it requires you to link to BLAS on your own, link to GPUs on your own, does have Julia's sophisticated broadcasting system or matrix types and factorization handling, etc. But I think that looking at linear algebra points out that it doesn't really have the "scientist" focus. If someone had to know that they had to install or "link" to some BLAS program, Julia/MATLAB/NumPy/R would have never gotten ground. One major assumption about the audience is that they want to do science and not software development, and handling binaries is the kind of hurdle that the audience doesn't want to handle. Nim does not have these features built in, and it only has elementary libraries that require you to take charge yourself. So this may be something a C++ programmer would be interested in, but when you also include the lack of real interactivity, this is not something I would compare against tools like MATLAB which are focused on scientists doing science. That's not to say it's a bad language. Nim is a pretty cool language, and in many respects is similar to Julia. But when there's a full language tracker asking ["Are we scientists yet?"](https://github.com/nim-lang/needed-libraries/issues/77) and they leave off entire fields like differential equations (the only mention of it in the thread is... from a Julia developer), this tells me that the community wants to check a checkbox of "we do science" but isn't really focused on building the complete scientific libraries of Julia. There's a big difference between linking to LSODA and LAPACK's QR, and going deep into allowing specialized sparse LU-factorizations and adaptive timestepping for stochastic PDEs. Julia has the latter, Nim doesn't have the former yet and doesn't have a dev crew focused on doing it.
Thank you for the detailed response! Fantastic, as always. Yeah looking at Nim's stuff online it definitely isnt as focused on scientific computing as I thought it was. I guess "Python, but faster" is a pretty big umbrella. Makes me realize how thoughtful Julia's design is. Prioritizing REPL, easy linear algebra, not to mention package development. On a selfish note, glad I'm not investing time into a language with too many direct competitors that could win out. 
Hello there, I'm a data scientist and the author of the scientific package in Nim with the most features at the moment: [Arraymancer](https://github.com/mratsim/Arraymancer). Just to give you something to play with, here is an example of using Arraymancer + Plot.ly to [track training of a convolutional neural net on MNIST](https://github.com/Vindaar/NeuralNetworkLiveDemo/raw/master/media/demo.gif), source repo [here](https://github.com/Vindaar/NeuralNetworkLiveDemo). The documentation is [here as well](https://mratsim.github.io/Arraymancer/). Also I do have experience with Julia for data science, though i only played with it for a weekend, see my [Kaggle Titanic challenge.](https://github.com/mratsim/MachineLearning_Kaggle/blob/master/Kaggle%20-%20001%20-%20Titanic%20Survivors/Kaggle-001-Julia-MagicalForest-0.79904.jl) Let's start with how Nim and Julia relates to each other from my (biaised) point of view: - Both are aiming to provide expressiveness, elegance and speed without compromise. - Julia focuses on numerical computing while Nim is general purpose. - Both have a performance-oriented and benchmark addicted community - Metaprogramming is a huge part of both languages and their appeal. Aka reducing boilerplate as much as possible. continuing on numerical computing in a second post
Nim for numerical computing. Let's start with the flaws: - Jupyter - Ecosystem, libraries and traction to build Strengths: - Wrapping C at no cost - Easy sell to those dissatisfied with Python performance given the syntax similarity - Dependency-free, you can ship a binary or shared library with no dependency and no need to install plenty of packages, deal with virtualenv or similar things
isn't statical typing also supported with Julia?
&gt; Are people writing physics papers in Nim right now? I (the author of that tutorial) started to use Nim instead of Python for my work on non-smooth contact dynamics. Nothing yet available nor published, but hopefully it will be one day :) Why did I choose Nim over Python? I wanted to see if I can do it, and I like the speed improvements. Why did I choose it over Julia? Because I'm much more familiar with Nim.
Here is a physics paper in Nim: https://arxiv.org/abs/1612.02750 QEX: a framework for lattice field theories, presented at the 38th International Conference on High Energy Physics, August 3 - 10, 2016, Chicago Source code here: https://github.com/jcosborn/qex Also Nim for computational biology: https://academic.oup.com/bioinformatics/advance-article-abstract/doi/10.1093/bioinformatics/bty358/4990493 Source code here: https://github.com/brentp/hts-nim 
I'm using it right now - login with github option (top right corner).
Thank you. It is very strange: I am constantly redirected to the free/paid plan page and no Google/GitHub/LinkedIn log in options is available to me at this time.
It's a Safari web browser problem: using Firefox or using my mobile phone I can see the log in options.
such things are usually caused by half of the cookies being there, but some missing. try delete all relevant cookies from the browser. as a test, you can use a clean browser.
https://juliaimages.github.io/latest would be a good start.
I suggest the Julia package for [Tensorflow](https://github.com/malmaud/TensorFlow.jl) for your models and maybe [Images.jl](https://github.com/JuliaImages/Images.jl) for loading and preprocessing images. There are other packages for machine learning ([JuliaML](https://github.com/JuliaML), I assume you like to use machine learnign for computer vision), also pure julia implementations, but Tensorflow is very powerfull and fast. Don't worry about that the most Tensorflow tutorial are in python. It's very easy to transfere it and you are able to use the very nice julia features to build your graph.
Oh my god! that belongs to r/blackmagicfuckery I played some time ago with Cxx.jl but I didn't know it was so great. Now I need an excuse to use it. There are good examples in the Readme of the repository.
Thanks a lot!!!
Thanks a lot!!!
Or are you just pleased to see me?
You should see the kind of numerical power I have in my pocket... 
Do you have ∈ on your keyboard?
\\in probably
\in then tab. I think that's right...
Is this SSH or native Julia?
And, if it's native Julia, souce please. Couldn't find it on fdroid.
native!
It's a little bit of a hack at the moment, but you need termux for android, and then its-pointless's private repository (where you can also install R from). Have a look [here](https://github.com/termux/termux-packages/issues/58). Once you've add the repo it's actually fairly easy. I think just an apt install julia (do pkg search julia first, then install what you see). At the moment I can't get DataFrames working, but base julia functionality is there.
Any differences in performance? 
Eeee!!!!! I got it working :) Thanks dm319 and its-pointless!!! I am so happy, programming julia on the bus :)
What is being measured here? For a system like this, the linear solving step should dominate the runtime. So is it only the assembling part that is showed? If not, is the exact same linear solver used? Where is the code to reproduce? 
It is Tutorial 1 of the FEniCS manual as noted in the post. So it's the runtime to assemble and solve. Here's what the FEniCS.jl code looks like: https://github.com/JuliaDiffEq/FEniCS.jl/blob/master/test/tutorial1.jl So if you run the two tutorial codes you can time them. The "Julia_x" code is just assembling and solving in Julia which I am not sure is currently shared.
In general for data stored column-wise it's much more efficient to work column by column. Depending on the situation, this can be quite short an intuitive, or not.
yes it's remarkably fast as a loop. You're right that this just doesn't need the split-apply-combine approach - a procedural approach is fine.
Thanks for spotting this Chris! I'm quite chuffed... (as we say over here)... :)
I was installing it by downloading the arm binary version from julialang.org and the fedora script for termux. Works nicely! https://github.com/nmilosev/termux-fedora https://julialang.org/downloads/
last time when i participated in a discussion about this, a question came up. with Nullable, you can have function a(i::Nullable{Int}) function a(i::Nullable{Float32}) but with this new Missing, one can not have function a(i::Union{Missing, Int}) function a(i::Union{Missing, Float32}) incidentally, i'm not saying this is a problem. an argument can be made that if the value is missing, it should make no difference whether it would be an Int or Float32 if it existed. in most cases, you would just do function a(i::Int) function a(i::Float32) function a(i::Missing) but anyway, it is a difference worth noting.
I think that part of the motivation for this is that the compiler had gotten better at handling small unions
i get that. i'm considering semantics. as an example: you can have function null_to_zero(a::Nullable{Int}) # returns 0 for nulls function null_to_zero(a::Nullable{Float32}) # returns 0.0 for nulls but it is not possible with the new way. again, i don't mind it. just pointing out.
Julia is in Ubuntu repositories. But old version.
I don't think it affects uptake at all. Especially as you can try it at Juliabox.org without installing anything
The problem is that Julia uses a patched LLVM. Julia v0.6 used a patched LLVM 3.9 which is quite an old version. The reason is because Julia has been great for numerical precision work, which means it tends to find bugs in the compiler. A lot of those got upstreamed and now Julia v0.7 uses LLVM 6.0 which is the newer LLVM. This may be something that works directly with the package manager's LLVM, which would make it work well. Package managers like you to have your dependencies be other packages in the package manager, so sideloading a different version of LLVM is what made these repositories not work well with previous versions of Julia.
Oh that's fascinating! I wonder if this is why I had so much trouble trying to install the R tidyverse on my phone - had to switch between GCC and LLVM for various packages. Guessing you could sideload LLVM in a snap or flatpak though? Sounds like it'd be a lot easier than trying to get into distro repositories even without the LLVM requirement.
It's that through plan 9 in user space? I haven't heard of many people using acme for development - what do you like about it?
Not seeing it [here](https://packages.ubuntu.com/bionic/allpackages), am I looking in the wrong place?
Yes. Probably my favourite features are full hypertext and combined with the plumber, pattern matching. I'm proud to say the screenshot on [wikipedia](https://en.m.wikipedia.org/wiki/Acme_\(text_editor\)) is of my screen 
**Acme (text editor)** Acme is a text editor and graphical shell from the Plan 9 from Bell Labs operating system, designed and implemented by Rob Pike. It can use the Sam command language. The design of the interface was influenced by Oberon. It is different from other editing environments in that it acts as a 9P server. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/Julia/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
Yeah I get the feeling that 'a::Union{Missing,T}` as a function input variable for dispatching has slightly different semantics than it as a variable.
My only complaint would be that `missing` is rather long winded compared to `NA`, but then so is `nothing`. `∄` would be a nice Unicode alias.
I think Rob Pike is a genius, so I'm going to have to check acme out sometime.
Its was Nikolas Wirth's set of ideas from Oberon implemented in Plan 9. "Everything is a file" is the part missing from the Plan 9 in user space world. It is a force multiplier.
Not sure what this one is showing, `apt search julia` in my Artful shows `julia`, `julia-common` and `julia-doc`. But again they are old (0.5).
I totally understand the Plan 9 team's disappointment that Unix and Linux became so popular. I think Ken Thompson thought Unix was ok, but Plan 9 was even more elegant and a natural progression. But linux had the gnu licence, and I think that is what made the difference. That group of programmer's were very much of the 'cathedral' type of software development. We make something beautiful, and you can use it if you want. Someone asked Rob Pike if he could change the colours in acme, and he said no. There's been a lot of pressure on the Go team to introduce features the community wants, but, so far, Go are resisting. Ken was very sceptical of Linux ever being written well given the number of people sending in patches - and he was probably right to begin with. You think of all the OS's being developed at the moment - the BSDs, Haiku, Fuschia - you would have thought people would be interested in a modern Plan 9.
That's strange. I recently moved from 16.04 to 18.04. It wasn't in 16.04, and nothing comes up in synaptic. apt search julia returns this: julia-common/now 0.4.7-7build2 all [residual-config] (none) There isn't anything else and this doesn't look like a package to me. Maybe they put it in 17.10 and dropped it for 18.04? Or maybe my repos are set up strangely.
I was there in 2000, on 9fans, I've worked with Brucee and the Limbo team on paid products. Yeah, Gnu is Not Useful killed the ecosystem and The Lucent Lawyers did the rest.
it would be a misuse, wouldn't it? that symbol means "does not exist"
Somebody knowns what would be the equivalent of Matlab/Octave's `nanmean(array,2)`, i.e. the mean just over the second dimension of an array ignoring missing values?
\`mean(skipmissing(array),2)\`?
Awesome. Yes. I'm a big fan of gnu - both the license and the ecosystem - glad it survived the lawyers too - but I'm sad that I didn't get to see what plan9 would have turned into. Also Amiga - I was the original fanboy. I have to go to therapy sessions re: M$.
It would be great if it would be that simple, but I get an error ``` julia&gt; array = [missing 1; 2 3] 2×2 Array{Union{Int64, Missings.Missing},2}: missing 1 2 3 julia&gt; mean(skipmissing(array),2) ERROR: MethodError: no method matching mean(::Missings.EachSkipMissing{Array{Union{Int64, Missings.Missing},2}}, ::Int64) ```
So you want the plotting and the regression to be in the same step? I'm not sure that's feasible. It is probably to use `lm` from the `GLM` package and then treat the plot separately. 
Thanks, I have found a way to do that in some steps without GLM package! :)
Try unixstickers, they donate back to projects.
I'd start by saying that both languages are awesome, have a lot of great features and are modern and highly productive. However, the first thing to bear in mind is that Julia targets numerical computing problems whereas Nim is a general-purpose (systems) programming language. As such from the start, the Julia development community have invested a lot more effort in providing tools that make it easier to write code for numerical computing. This means that at least at first, you will find Julia much easier to start with. For example, you won't need to worry about linear algebra libraries or creating your own vector, matrix, array objects and functionality. That stuff is designed to work out of the box in Julia. Julia also has an interpreter, which is useful to allow you to develop your numeric code live. However as already mentioned, both Nim and Julia are able to interoperate with C, (Nim natively with no overhead), in addition, you can also call C++ code from Nim, in fact Nim has better interop with C++ than any other language. This means that you don't necessarily need to create everything from scratch in Nim. From a language perspective, both languages are a significant departure from standard approaches. Julia is a dynamic programming language that has great support for types/template/metaprogramming - better than some static languages, and has fantastic performance when compared to other dynamic and even other static programming languages. It has support for multiple dispatch - great advantage in writing object oriented code. One thing I have noticed about Julia however is that creating multiple threads comes with a very high memory footprint. On my system on startup Julia (0.7.0) takes up 125 MB. That's almost x5 an R instance and x50 of a Python instance. Nim is a static programming language also has native support for multiple dispatch (multi-methods) and supports types/template/metaprogramming programming. Nim is more of a trans-piled language since it translates to C, C++, or JavaScript rather than having it's own native backend compiler. It has a very powerful compile-time system with macros, and allowing you to create your own pragmas and use them in very powerful ways with functions, as well as features that allow you to mark objects to be treated differently with respect to an already flexible garbage collector. Nim treats objects oriented programming holistically breaking away from the class/struct syntax of other languages and provides a subtle and intuitive approach to create the behaviour you want for your object including, inheritance, reference, and garbage collection v.s. manual memory management. I found this awesome feature poorly documented in the tutorial/documentation - which is kind of a running theme for a lot of Nim's very powerful and advanced features. Perhaps the documentation is still in development. But in short, Nim gives you great control over low and high level programming features. My most important tip if you are new to Nim is for goodness sake, don't use tabs, and if your scripting editor pops them in automatically, set it up to use spaces instead - read the style guide. Believe me, it will save much head-scratching! My advice is to try them both! But if you've never written a numerical library in a low-level static compiled language before, expect to favour to Julia. If you have written low level numerical libraries before and you are looking for a powerful language to work with that gives you lots of control at the very low and high level, then you'll probably favour Nim (or some other suitable systems language). The path you choose is also highly dependent on your capability. If you are a computer scientist or very comfortable with advanced computer science concepts you'll get much more out of Nim (particularly from the prgama and macro system) than someone who is more of a pragmatic programmer. This is not to say that Nim is not pragmatic or that Julia doesn't have deep concepts, it's that they are designed to be used by different types of people, Julia the analyst-programmer and the Nim the developer. From a semantic point of view, however, I think both languages are similar to each other despite the static/dynamic differences, and I suspect that they would work quite well together. A programmer could probably transition quite well between these two languages. Hope this is useful, I've tried to focus on different aspects from those already covered by other commenters.
If you’re looking to write a compiler and making a native executable is a must for you, I’d say that Julia with the current state of PackgeCompiler is probably not quite there yet. However, I’d recommend you look into either the Racket programming language or Common Lisp. Both are fantastic languages for writing compilers. Racket has better tooling for writing a compiler but Common Lisp is faster so my recommendation would depend on how important performance is to you. 
It seems like there should be an easier way to do this, but this works: mapslices(x -&gt; mean(skipmissing(x)), array, dim)
From the `skipmissing` REPL help: skipmissing(itr) Return an iterator over the elements in itr skipping missing values. Use collect to obtain an Array containing the non-missing values in itr. Note that even if itr is a multidimensional array, the result will always be a Vector since it is not possible to remove missings while preserving dimensions of the input. ...so the solution involves judicious use of `mapslices` mapslices(x -&gt; mean(skipmissing(x)), array, dim) Seems like it'd be nice to provide this functionality as a keyword argument to `mean/median/std/etc...`, or as a convenience function to improve discoverability. For reference, Numpy &amp; Matlab both provide `nanmean, nanmedian, nanmin, nanmax, nanstd, nanvar, nansum`
Julia compilation process is multi-stage: the source code is first pre-compiled to some intermediate form. When arguments are provided to functions, native code is generated for this combination of arguments (in a way, it is similar to C++ templates, instantiated only when needed). The result of native compilation is saved within one session. However, it is lost between the sessions. The pre-compiled intermediate form is cached between the sessions (as far as the module has `__precompile__()`). PackageCompiler allows to save the native compilation results. Alas, I was not successful with it either (on Windows 10). As Julia 0.7 is on the way, I would wait and try it again on a new version. Julia has another peculiarity that you need to be aware of if you are writing a parser/compiler: while it has garbage collection it is slow. For numerical computations in general it is not a problem: everything is allocated right at the beginning and then re-used. In case of more CS-like functional-style programs, memory allocation and deallocation may hinder the performance. 
I can second Racket/Common Lisp. I would sway towards CL a bit more though as general tooling (debugger, inspector and macro-expander in SLIME) is better and available libraries are more mature.
as far as I can tell, unixstickers don't sell julia stickers anymore. Did you look? 
I believe once you input the PDE, finite difference is used to solve the PDE. It appears to be the underlying algorithm the DiffEq package uses. https://github.com/JuliaDiffEq/DiffEqOperators.jl
This sounds right, but you should also be able to implement something yourself for this problem. Could you post the DOI of the paper maybe, so I could take a look. I work in CFD and the equations are quite similar, I might be able to help a bit.
Here's the DOI: https://doi.org/10.1093/rfs/14.1.113 It seems like a very standard model, so I thought I'd ask if somebody has already implemented something that can handle this. Thank you for your input!
first give it a shot with a first order explicit in time central finite difference discretisation in S. https://en.wikipedia.org/wiki/FTCS_scheme This should work, not very well, but a good first attempt.
Discretize in space to get a set of ODEs. DiffEqOperators.jl can help with this. Just central differences the P\_ss term and then upwind difference P\_s. Then use the Euler method for their explicit method, or ImplicitEuler will give you the implicit method. However, you probably don't want to use either or their methods which are very inefficient and low order. I would start by using the same spatial discretization and then applying the CVODE\_BDF method to it which will be much more efficient. I'll have a whole workshop on this at the next JuliaCon BTW, but for that now hopefully that's enough information to help you build this. It shouldn't take much work to do. See the PDE in this tutorial as an example for discretizations: [http://nbviewer.jupyter.org/github/JuliaDiffEq/DiffEqTutorials.jl/blob/master/Introduction/OptimizingDiffEqCode.ipynb](http://nbviewer.jupyter.org/github/JuliaDiffEq/DiffEqTutorials.jl/blob/master/Introduction/OptimizingDiffEqCode.ipynb) though I really need to write a more in-depth one.
Upwinding may not be required, depending on the coefficients of the laplacian term.
[GitHub link](https://github.com/JuliaDiffEq/DifferentialEquations.jl)
Isn't Julia's parser written in Racket? 
Scheme. 
This change also fixes the Julia issue that bothered me the most personally: if in a comprehension with nested loops any of the non-outermost ones was over an empty sequence, you'd get an error.
This was such a great read, thanks!
This is the difference between the typed vs untyped `null`. See e.g. https://github.com/davidanthoff/DataValues.jl which has a type like: struct DataValue{T} hasvalue::Bool value::T end
function n2z(a::Union{Missing,T})::T where T (a === missing) ? zero(T) : a end
The existential and universal qualifiers are in a restricted section of unicode and Julia doesn't like parsing it, IIRC
Julia works quite nicely (not quite perfectly for distributing code, however), with singularity containers.
Use import: import PyPlot; plt = PyPlot plt.plot()
why not PyPlot.plot()?
I guess because it looks similar to python this way?
&gt;plt = PyPlot Make that \`const plt = PyPlot\` if you want type-stability further down in functions which use the calls.
Thanks for your help, how would I do if I wanted to have plotly in one cell, statplots with another backend and Gadfly in yet another cell ? Thanks !
1. This kind of support question is not really for Reddit, try discourse [https://discourse.julialang.org/](https://discourse.julialang.org/) or Gitter [https://gitter.im/JuliaLang/julia](https://gitter.im/JuliaLang/julia) where Julia programmers hangout and will be able to help you. 2. I can't make head or tail of your question (I've build GLM solvers from scratch). Take your time to describe what you are trying to do. Firstly, what is your target variable? Univariate? Multivariate? Is is a binary or multinomial categorical variable or just floating point numbers and if so what are the range of the numbers? Is the data size very large, how many explanatory variables and how many observations? What kind of distribution and link function have you decided to use? What version of Julia are you using? 3. If the data is not huge you may want to give R a try - standard GLM is the kind of stuff R does in its sleep. 4. Why are you trying to feed in a DataFrame and an array into GLM function!?
This is a great blog post. Posts showing off the various modes of distributed computing in Julia are in short supply - particularly for Julia 0.7. One question is whether there are any plans to reduce the memory foot-print of Julia threads which are orders of magnitude of Python and much larger (x 5) than R?
Thanks, this would be a useful function in the standard library!
Exactly. Also to illustrate how to use this pattern with longer library names like DifferentialEquations where you don't want to type DifferentialEquations.solve() all the time.
Thanks I'll check it out, I didn't know where to turn to exactly. I'm coming from Python and when you use the sklearn API, you can use a DataFrame as an input with different types of features, even categorical, as long as they're encoded. I'm using a housing dataset and I was doing some EDA in Julia because I'm a beginner and it seemed like a nice challenge. This was relatively easy. But now I'm trying to use machine learning for price prediction (my target value) with Ridge and Lasso but I can't seem to make it work. I've scaled my data to some extent and label encoded the features I won't be hot encoding. That data is small, train is 1456x76 and test is 1459x76, even after encoding of target variable it doesn't go over about 1450 by 230. I was trying to use the most basic regressions first to get a hang of what's going on in Julia since I'm a newbie. Thanks for your help!
Can you clarify the expected behavior you want from a categorical variable using GLM? Do you want it to act as dummy variables? 
Yes, I'd like to use dummy encoding on those categorical values but I'm open for suggestion if a different type of encoding can help :) There are 25 categorical features in my dataset, out of 76 features.
categorical variables as dummies works out of the box for me on 0.6.3 and `GLM`. What versions are you using? CategoricalArray is the way this is done with `GLM` and `DataFrames` now. ``` df = DataFrame(y = rand(100), x = CategoricalArray(rand([1,2,3,4,5],100))) lm(@formula(y ~ x), df) &gt; StatsModels.DataFrameRegressionModel{GLM.LinearModel{GLM.LmResp{Array{Float64,1}},GLM.DensePredChol{Float64,Base.LinAlg.Cholesky{Float64,Array{Float64,2}}}},Array{Float64,2}} Formula: y ~ 1 + x Coefficients: Estimate Std.Error t value Pr(&gt;|t|) (Intercept) 0.363469 0.0758999 4.78879 &lt;1e-5 x: 2 -0.0216355 0.105648 -0.204788 0.8382 x: 3 0.15596 0.101532 1.53606 0.1278 x: 4 0.0739177 0.0953118 0.775536 0.4399 x: 5 0.173424 0.0967538 1.79242 0.0762 ``` 
Using 0.6.3 too ! Well I'm not really sure what I'm looking after then? Ideally I would run my regression on all my variables at the same time or should do that separately for each feature and build a response matrix ? Sorry if that doesn't make sense but coming from scikit learn you can just run the regression with a Data frame as input 
Perhaps you should look at the Julia ML libraries? Like Flux.jl, I don't know anything about machine learning, but perhaps that library has what you need. 
I'll look into it, thank you ! Any good library and tricks I should know about ? 
I'm not too sure about it but I'll try, if I get good enough at Julia I'll try to see if I can emulate Dataframes regression from the sklearn API
&gt; Ideally I would run my regression on all my variables at the same time or should do that separately for each feature and build a response matrix ? I think that's more of a statistical question rather than a programming one. You can use a GLM on all of your variables in a multivariate analysis, or go through each as univariate. You can use a step-wise procedure to try to optimise what is included in your final model, or things like lasso/ridge like you mentioned, which are more complex statistical methods to arrive at a final model. Maybe you should post a minimal example so we can see what you're trying to do? Or even the python code with some minimal data?
I'd like a multivariate analysis if possible because some of my variables are correlated so step-wise doesn't seem to be the best course of action :/
10AM PDT happens when this comment is 10 minutes old. You can find the live countdown here: https://countle.com/oX224411oF --- I'm a bot, if you want to send feedback, please comment below or send a PM.
In Julia, this is commonly done by just defining all of your functions on \`AbstractString\`. I.e., your functions work for any string regardless of implementation details. Then via multiple dispatch you can define a version of the function specific to \`MyString\` which overrides the behavior in a specific case. This is how most generic programming is done in Julia, with the \`Number\` and \`AbstractArray\` interfaces being the big examples. You can go as far as letting your entire generic code packages specialize on small function overloads and get new behavior just by putting new types into existing code. This blog post showing SymEngine symbolic expressions as "Numbers" into DifferentialEquations.jl's ODE solvers explains this: [http://www.stochasticlifestyle.com/fun-julia-types-symbolic-expressions-ode-solver/](http://www.stochasticlifestyle.com/fun-julia-types-symbolic-expressions-ode-solver/) Using this style to structure generic codes is something I have been calling type-dispatch design and describe some of the tricks and techniques in this post: [http://www.stochasticlifestyle.com/type-dispatch-design-post-object-oriented-programming-julia/](http://www.stochasticlifestyle.com/type-dispatch-design-post-object-oriented-programming-julia/)
Adding on to what Chris said, in Julia a function that actually specifies `::String` as the type of its argument should be one that really really wants an actual String, not some String-like object you defined. Most functions don't need something which is precisely a `::String`, but merely something string-like. So those functions should accept `::AbstractString`, which is the abstract type of which `String` is a concrete subtype. If you define your own `MyString` as a subtype of `AbstractString`, then your type will naturally work with anything that was expecting an `AbstractString`. It will *not* work with a function that expects precisely a `String`, but that's kind of a feature: it allows the author of that function to be sure they will *actually* get a a `String` when they require it. Of course, the real world is not perfect. You may find functions out there in the wild which ask for a `String` but would actually be perfectly happy with `AbstractString`. Those functions won't accept your `MyString` type. The only remedy there is to fix those functions. 
I don't think that works quite the way I was hoping. The blog post was still quite informative however, the `@def` macro in particular solves a more minor issue I was having. You also come up with much more creative examples than I do - I always seem to be using apples and oranges! The method you were talking about, if I understand it correctly, requires defining all methods for the interface of the abstract type. I'm not interested in creating new implementations of some kind of string and really just want to use an existing concrete subtype of AbstractString, but with distinct types for every logical/semantic/I-don't-know-the-best-word type in the program, this gets to be kind of burden. For example, right now I'm working on some NLP work and I'd like to create a new string type for each stage of the processing, ensuring all steps are called and in the correct order. I would have to create a passthrough method for each methods in the AbstractString interface right? Speaking of which, is there a good way of figuring out what the interface of an abstract type is? If so, perhaps it would be possible to write a macro to generate the passthrough methods?
Yeah I probably should have used a slightly different bit of sample code. I was trying to indicate I didn't want to create a new sort of implementation for a string and was interested in reuseing an existing concrete type, but as an entirely new type just for the sake of type safety. I can't come up with a better one-liner right now so I'll give a longer one. newtype Apple_Count &lt;: Int end newtype Orange_Count &lt;: Int end apples :: Apple_Count more_apples :: Apple_Count oranges :: Orange_Count apples + oranges #error apples + more_apples #works The intent here is to use the actual Int implementation for these types, but make them fail to type check if you try to do something illogical like add apples to oranges. Granted, the actual implementation in that case doesn't matter, so being able to make them parametric would be even better. That seemed like asking even more however, so I didn't get into it.
I believe what you want are traits, as provided by the [SimpleTraits package](https://github.com/mauro3/SimpleTraits.jl)
Hm, I don't think that solves this but it does offer some really interesting abilities once the main thing is solved. So, I'm wanting to do something like create a new type for raw text, lowercased text, text with non-alpha characters removed, etc. I was thinking I'd need to create a distinct sequence (must lower first, then remove non-alpha, then...) but with traits I could allow the sequencing to be unimportant but still ensure all the processing is applied. Very cool! However, I still have the issue of how do I even create all these types in the first place. The only option I'm seeing so far is to subtype AbstractString every time, but that means absolute loads of boilerplate to actually define the interface methods for AbstractString for every single one. As this sort of thing I'm wanting to do to nearly all of the parameters and variables and whatnot, this gets really unmanageable. Is there a good way to get Julia to give me the interface methods for a type? If so, maybe it would be possible to macro it?
https://discourse.julialang.org/t/restrictive-type-aliases/10546 I (and others) have been calling what I think you are thinking of **Restricted type aliases** For `S` being a restrictive type alias of a concrete type `T` , We would have for purposes of Dispatch `S &lt;: T`, and further that `S` is strictly more specific than `T`. and `fieldnames(S)==fieldnames(T)` and `fieldtypes(S)==fieldtypes(T)` This would mean the behavour for the following definitions: ``` foo(::T) = "AlphaF" bar(::T) = "AlphaB" bar(::S) = "BetaB" buzz(::S) = "BetaZ" ``` and for `t` and instance of `T` and `s` and instance of `S` ``` julia&gt; foo(t) "AlphaF" julia&gt; foo(s) #fallback via subtype like dispatch "AlphaF" julia&gt; bar(t) # notice that it is not overridden "AlphaB" julia&gt; bar(s) # More specific than T "BetaB" julia&gt; buzz(t) # Not defined -- only defined for restricted alias MethodError(...) julia&gt; buzz(s) "BetaZ" ``` Is that what you were thinking of? It is not a thing you can do. I wish it was. It is nonbreaking so it is possible in 1.x. There are hacks to uses reflection and metaprogramming to regenerate a bunch of methods, and there are other hacks for other things. It all boils down to varying degrees of fragileness to varying changes More generally see the discourse thread I linked at the top. 
None of the parallelism mentioned here in this post was using threads. But in anycase, the PATR PR that is referenced largely replaces the whole threading system. so that will certainly change things. In general the expectation is that a lot of optimization will be occurring 1.x timeframe. I'm not deep enough on the inside to know exact plans; and I suspect the core devs haven't even got much certainty on it themselves; being that they are focusing now on getting 0.7 and 1.0 out really soon
Yes! This is exactly what I was talking about. Thanks for the term and link. Unfortunate to hear this is the case, but I was pretty sure it would turn out this way. I was hoping there might be a "nice" hack out there, but ah well. I'll check em out anyway. These restricted type aliases (or whatever the term used by $lang is) are always the first thing I look for in a language so I'm pretty used to being disapointed. Fragile hakcs are actually kind of a step up from most.
For this specific case, you could probably use the Unitful.jl package (https://github.com/ajkeller34/Unitful.jl). I don't know if you have other use cases in mind for what you are trying to do which wouldn't fit naturally with Unitful, but you could probably take a look at the Uniful package and see how they implement what they are doing. 
The standard way of doing this right now is to just write all functions on the abstract type. That's not a fragile hack: it's the exact same thing if you just make your new concrete type have the fields of the other one. Sure, it's not as pretty though.
Oh, I see now. I had misinterpreted my reading of SimpleTraits. I thought it allowed one to be more specific than mere type information but I was mistaken. Perhaps a better option (though its dynamic instead of static) is contracts. There's a package called Contracts.jl and I've made a branch of it that is updated to work on 0.7 if you're interested, here's [my branch](https://github.com/MasonProtter/Contracts.jl). I've submitted a PR but don't know when it'll merge. With contracts, what you'd do is something like islowercase(s::String) = s == lowercase(s) isuppercase(s::String) = s == uppercase(s) @contract function foo(s::String) requires(islowercase(s)) ensures(isuppercase(result)) uppercase(s) end @test foo("abc") == "ABC" @test_throws Exception foo("ABC")
[here](https://discourse.julialang.org/t/what-is-the-interface-of-abstractstring/8937/2) is a discourse thread that probably has the answers you need. With regards to inheritance, there are metaprogramming solutions for this, especially if your new string type contains a concrete string as a subfield. [here](https://discourse.julialang.org/t/forward-all-methods-of-a-structure/11783/2) is another discourse thread with answers. 
Just wanted to say thanks for asking this question! It's something I've been wondering about for quite some time, but couldn't quite put it into words properly. Hoping this can be added post 1.0; even though there are workarounds/other workflows which work as Chris says, I think this would be a very useful feature which would just be quick and easy to use
Check out bio.jl org. A lot of effort has already gone into great implementations of some of these things. 
You have tab on your phones keyboard? 
I'm concerned that Julia is already gunning for Python before Python has even fully displaced older science/numeric codebases. I'm not against the article, indeed, I see Julia as making positive strides as a language, and I expect it to start taking over Python's numeric mindshare in a few years. I'm concerned about the appearance to "outsiders", people who need numeric analysis but aren't technical or up-to-date enough to make an informed decision. There are a lot of hidebound organizations still clinging to outmoded codebases, tools, and languages that are resisting change out of habit. Now, just as it looks like Python is finally gaining momentum and the transition from old crap to Python, which sets the stage for Python to Julia (since Julia directly addresses some of Python's longstanding and difficult to remove shortcomings), is getting "disrupted" by these sorts of articles playing up Julia's capabilities as a comparable alternative. They confuse that momentum and possibly serve as an excuse to put off the long-overdue conversion to newer, better tools, so said hidebound orgs can keep clinging to their anchors and "wait to see who wins". I'm all for Julia taking its rightful place, and this article summarizes the amazing and rapid progress the language has made, but those of us who know should have a coherent argument for modernizing, to fight the complacency of twenty or more years of rotting tech stack.
Finally! That said, I hope they hold off on 1.0 for a few weeks, to give the ecosystem time to catch up. Right now some of the headline packages are not fully working.
Nice
Hey, fully\_strapped, just a quick heads-up: **publically** is actually spelled **publicly**. You can remember it by **ends with –cly**. Have a nice day! ^^^^The ^^^^parent ^^^^commenter ^^^^can ^^^^reply ^^^^with ^^^^'delete' ^^^^to ^^^^delete ^^^^this ^^^^comment.
I suggest you reconsider your position before continuing to assert untruth as truth. https://www.grammarphobia.com/blog/2015/08/publicly-publically.html
Well that's what I always assumed they would do, but the mad lads just pulled the trigger about half an hour ago!
How is this a fair comparison? It looks like in the python/numba code you're creating a list at each function evaluation, but in the julia code you're passing both input and output arrays via arguments. Is the comparison really between numba and julia or between scipy ode and julia's ode? I mean, you already gain 10% speed improvement in the numba version if you drop the square brackets at the return statement.
I think this release fixes the Julia issue that annoyed me the most: comprehensions didn't work unless all the loops except possibly the outermost were over nonempty sequences.
[This blog post](www.stochasticlifestyle.com/finalizing-julia-package-documentation-testing-coverage-publishing/) has a decent run down. I just used to it when I was figuring out how to add [my first package that makes corner plots](https://github.com/kilianbreathnach/CornerPlot.jl) to Pkg. I feel you on the python dependencies though. I've been using PyCall but I have a custom virtual environment that I need to specify in ENV["PYTHON"] and I don't know how I would make a deployable package out of any julia code that relies on this.
Hi. Passing Arbiter of English Grammar, here. If you can link to some text written by yourself within the last 100 years which includes a use of the word 'publical', I will judge this dispute in your favor.
Ha, and I just installed 0.7.0-rc2 two days ago. Time to download...
Hey, fully\_strapped, just a quick heads-up: **publically** is actually spelled **publicly**. You can remember it by **ends with –cly**. Have a nice day! ^^^^The ^^^^parent ^^^^commenter ^^^^can ^^^^reply ^^^^with ^^^^'delete' ^^^^to ^^^^delete ^^^^this ^^^^comment.
delete
Here's a quick intro: [https://www.youtube.com/watch?v=i5iGVkI7XOI](https://www.youtube.com/watch?v=i5iGVkI7XOI)
congrats!!
I have just tried to install and use the package Plots and Julia 1.0 fails at precompilation. Can anyone confirm this?
Yup, that's right. The packages are still working on updating to the new version. The goal of 1.0 was to stabilize the language — now package development can begin in earnest, knowing that they're depending on a stable version.
\[removed\]
It's a missed opportunity though to make a good first impression on a wider circle of people. A month on 0.7 to get the package universe in shape and you could capture a lot more people who will see and try Julia 1.0 for the first time now. Many many of the headline packages linked to on the julia homepage don't work right now. So everyone from a domain with a non-working headline package who decides to check Julia out now, based on the coverage of 1.0 being ready will have the experience: * Go to website. * Download 1.0 * See that for some reason the documentation link goes to 0.7 (and assume it's outdated). * add mydomain.jl, get a bunch of compilation errors. * Give up and maybe decide to come back later if they remember. That's not a great first impression. And to figure out that the Package they are interested in might work in 0.7 is not exactly obvious either.
Woah, did I miss the 0.7 release? Is this really 0.1? 
I appreciated John Myles White's take on it: https://twitter.com/johnmyleswhite/status/1027365024258506752 It's definitely a trade-off. Packages have always lagged for a few months after each release, and even longer to completely remove deprecations. At the same time, we wanted 0.7 and 1.0 to be as similar as possible... and also allow folks who are ready to begin using 1.0 now.
After this major milestone, it will gain widespread adoption of some major company is backing this project. Kaggle kernel support and Google colab like environment(JuliaPro is not quite there - no GPU support and more geared towards students). I love the language and I have been following Julia since 0.4. It has tremendous potential as a data, scripting and general purpose language. Really excited because the core maintainers will now be focused on the compiler internals after 1.0. 
1.0 is basically the same as 0.7 except with deprecated features from 0.7 removed, so it didn't take much longer.
I agree. I am also unsure on how to install IJulia on an existing Python + Jupyter + Jupyter Lab system. I used to set the ENV\["PYTHON"\] = "/path/tol/python" to avoid the process to download and install a miniconda environment, but I am not confident now to explore it. I am nonetheless optimistic and welcome the 1.0 milestone.
This is true, but imagine the hand-wringing in the community if 1.0 had been delayed more. People would have lost trust in the core development team, which is arguably worse. 
Is it practical to use zero-based indexing in Julia? I find one-based indexing annoying.
Yes, there's OffsetArrays.jl (which lets you use arbitrary indices, not just zero starting).
Honestly, in idiomatic julia code you almost never index arrays in a way where 0 vs 1-based indexing matters. If you are looping over all indices of an array for example, you want to use something like &gt;for i in eachindex(arr) rather than &gt;for i in 1:length(arr) etc etc.
&gt; Try Julia by downloading version 1.0 now. If you’re upgrading code from Julia 0.6 or earlier, we encourage you to first use the transitional 0.7 release, which includes deprecation warnings to help guide you through the upgrade process. Once your code is warning-free, you can change to 1.0 without any functional changes. The registered packages are in the midst of taking advantage of this stepping stone and releasing 1.0-compatible updates. &gt; The single most significant new feature in Julia 1.0, of course, is a commitment to language API stability: code you write for Julia 1.0 will continue to work in Julia 1.1, 1.2, etc. The language is “fully baked.” The core language devs and community alike can focus on packages, tools, and new features built upon this solid foundation. This is one of the first bits of writing in the announcement IMO nothing spurious about it. Now you can start writing Julia 1.0 code. it's pressing the "go" button on the ecosystem development, which happens in a more ad-hoc, organic way. That's a pretty big announcement for your immediate clade of contributors and is a sign of near ripeness for us. At any rate most of the useful packages are still looking for the Right Way To Do Things and giving them the extra time helps a lot
Congrats! Happy to see this.
I've never experienced this. It sounds like you used to not experience. Try upgrading (your version of|to) iTerm2
Is your numerical work multithreaded? Then Ryzen hands down. Anandtech actually has a benchmark for this. 3D Movement Algorithm Test v2.1: https://www.anandtech.com/show/12625/amd-second-generation-ryzen-7-2700x-2700-ryzen-5-2600x-2600/9 
I would go with the Ryzen since it has more cores, and I like parallel computing and algorithms. But that is just my personal preference, they both look like really good CPUs. For the GPU you want an Nvidia card, since those support CUDA. I have a GeForce GTX 1050 Ti and it works fine :) 
Can you do any serious computing on that GPU? 
I’m curious if the benchmarks differ if you can make use of Intel’s MKL.
If I use python, it doesn't happen though. $ python &gt;&gt;&gt; Then I hit `ctrl+d` and it goes back to the shell
Actually, if you want 64 bit precision it's not a great choice if you want to really exploit a GPU for scientific computing you should buy a Tesla not a GTX. [https://devtalk.nvidia.com/default/topic/995849/does-the-gtx1060-support-double-precision-/](https://devtalk.nvidia.com/default/topic/995849/does-the-gtx1060-support-double-precision-/) This links talks about GTX 1060 but it applies to all GTX family. I have worked with low level operations with CUBLAS and CUDA MAGMA, and the amount of data in a matrix to beat Intel's MKL is insane and it is because the GTX series are not designed to do scientific computing.
What about the Asus - ROG STRIX X470-F Gaming?
Yes, you are in a shell inside a shell. One Ctrl+D exits Python's IDE, another would exit the shell (bash, zsh, etc)
OK, I have an older version of Julia, which I installed with a different process. I actually have a binary on /usr/local/bin/julia and my shell is using it to start the REPL (both iTerm2 and terminal.app). The alias might be interfering with the way the shell is interacting with the Julia's REPL. Maybe remove the alias and try to copy /Applications/Julia-1.0.app/Contents/Resources/julia/bin/julia to /usr/local/bin or some location in your PATH. Alternatively, remove the alias and add /Applications/Julia-1.0.app/Contents/Resources/Julia/bin to the PATH in your bashrc or other resource file.
If you're doing neural network stuff with the gpu, a 1050ti is perfectly fine for most stuff. It has more ram than the 1060 3gb model which greatly helps computation (despite the fact that there's less Cuda cores). I've been using mine for a while now and it works very well. Occasionally, there's some stuff to tune like micro-batch size but it does very well. Also you can mess around with the cudanative.jl library if you go with nvidia. If you need a bunch of double precision or half precision floating point performance and you're not relying on Cuda libraries, an rx580 might be a better value. The consumer grade nvidia gpus have hampered double and single precision I believe. 
Most numerical libraries like LAPACK and BLAS are slower on AMD from what I understand. 
your alias is the problem. `exec` means (roughly--it's more efficient than this:) "run this and then exit". You're telling your shell to quit after julia quits. Just remove the exec from that alias, or update your PATH instead. You don't need those quotes either. alias julia=/Applications/Julia-1.0.app/Contents/Resources/julia/bin/julia or: export PATH=$PATH:/Applications/Julia-1.0.app/Contents/Resources/julia/bin but check that you don't already have a julia in your $PATH
Unless you plan to not use this machine as a personal desktop, I would recommend using cloud services to do your numerical work.
It does make it slow, but it's the best we have right now if you need a solver. SymEngine.jl is much faster but doesn't have an equation solver (i.e. it cannot find the X s.t. ...). That's about it for now, though there's work going on for more.
Thanks for you answer! I'd be doing mainly matrix manipulation (with no need for an equation solver), so I think SymEngine would fit? 
Can you recommend any GPU cloud computing service?
The package Rewrite.jl was just presented at JuliaCon 2018 https://youtu.be/53AS7uxBMRs .
#### [Algebraic Simplification Using Rewrite.jl](https://youtu.be/53AS7uxBMRs) ##### 227 views &amp;nbsp;👍10 👎0 *** Description: Tweet ShareTerm rewriting is essential to a wide variety of fields, including elementary, boolean, and abstract algebras. Because existing symbolic si... *The Julia Language, Streamed live on Aug 8, 2018* *** ^(Beep Boop. I'm a bot! This content was auto-generated to provide Youtube details. Respond 'delete' to delete this.) ^(|) [^(Opt Out)](http://np.reddit.com/r/YTubeInfoBot/wiki/index) ^(|) [^(More Info)](http://np.reddit.com/r/YTubeInfoBot/)
oh that sounds cool too, does it handle matrices? 
I've used AWS EC2 for GPU computing (playing around with deepstyle back when that was popular). It's pretty decently priced. You can get a 4xNvidia [K520](http://www.nvidia.com/object/cloud-gaming-gpu-boards.html) machine for about $0.80/hr if you use spot pricing in US-West-2. These days, they have elastic GPUs that you can hook up to any old EC2 instance. I haven't tried that out, but I would imagine that you can get slightly better pricing with that. Ultimately, whether you go with a cloud service or buy your own hardware is up to you. Here are some benefits and drawbacks worth considering: The benefit of owning your own hardware is that needing to run it 24/7 is usually cheaper, and you can re-purpose it for other uses down the line (or sell it to get some of your money back). Some drawbacks of buying a system is that it will take up space (you gotta keep it somewhere), and GPUs get pretty hot when you use them for GPGPU workloads. For me, the key benefit of using a cloud service for a workload like this is that I don't actually have to live with a fixed amount of computing power. If I have a workload that takes 2 hours on one GPU, but only 1 hour and 10 minutes on 2 GPUs, I can easily just spin up two GPU instances (or a single dual-GPU instance) and get the same results in about half the time and at roughly an equal cost. A second benefit of using such services is that you learn *how* to use the service, which itself can be a pretty valuable skill to have on your resume. Another important benefit is that you can immediately start working on code that coordinates the efforts of several computers, allowing you to write things in a distributed way from the get-go. The downside of cloud, of course, is that you're living as a tenant. Any time spent computing costs more than just the electricity you're using. Leaving things running by mistake can be costly, as can writing very inefficient code. And, of course, you don't get to keep any of the hardware when you're done.
That's an awesome overview, thank you! I have two additional questions however: (1) did you ever try to use multiple GPUs vía MPI on a cloud service? Is the experience similar to an HPC cluster? (2) where does Julia for into this workflow?
MPI always had too much of a learning curve for me. All of the multi-GPU stuff I have done is single tasks on a multi-GPU machine. I haven't used a dedicated HPC cluster either, so I can't really compare the two. As for where Julia fits into this workflow, the native [passwordless SSH](https://docs.julialang.org/en/v0.6.0/manual/parallel-computing/) cluster feature of Julia is where you can make use of Julia across multiple networked machines. With pretty much all cloud services, you can predefine SSH keys to be installed so that the machines are ready to start working soon after you spin them up. Note that since Julia, OpenCL, and CUDA are not typically default packages, you will likely need to do some initial legwork in customizing a linux image that you can deploy to your worker machines. Having said all of this, take it with a grain of salt: I have not needed to do heavy numerical work for my job for a long time. This is just how I would go about making the most of my time/money, knowing what I know from my experience in other areas of industry.
[It's working now.](https://imgur.com/a/50W7sTU)
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/UBUX8ir.png** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) 
Thank you!
Where can I learn about the algorithms for symbolic manipulation? 
I'm not familiar with JuliaInExcel, but it looks like it's one of the (few) packages that's not open source. There are a bunch of packages for working with tabular data though, including ones what will read and write to xlsx. I'm not sure if julia computing has any sort of student rates available, but their Enterprise offerings are one way that they pay the bills.
Ahh, I can see why that would be attractive, but also why it would be a bear to maintain. Makes sense that this would be the sort of thing that corporate clients would pay for :-/
Why particularly for development?
Precompilation of the package(s) occurs once for all, although there is (unfortunately) the same familiar JIT compilation/startup time: `julia&gt; @time using Plots` `9.696986 seconds (14.81 M allocations: 796.325 MiB, 6.37% gc time)` `julia&gt; @time plot(rand(5))` `21.568198 seconds (42.85 M allocations: 2.095 GiB, 8.37% gc time)` :-(
Thank you so much for this tutorial. I mainly use python for data science, so my question is: is Julia much faster than python when dealing csv files on a single machine ? Like loading csv, EDA, data wrangling etc. ?
If you're familiar with python, xlwings accomplishes the same thing. You have to script outside of excel but can utilize the functions as normal excel functions.
False. Julia is more succinct in any side-by-side comparison I do or find. For example, this one is pretty comprehensive in the basics: https://cheatsheets.quantecon.org/ As for one-based indexing, no. Which one is better is entirely domain dependent. In most algorithms in numerical analysis (interpolations, differential equations, numerical linear algebra) and data science I tend to see the algorithms nicely written in one-based but require indexing changes to handle zero-based. In fact, there's some mental gymnastics which are required to make zero-based sensible in the field: https://stackoverflow.com/questions/29287224/pandas-read-in-table-without-headers &gt;In order to read a csv in that doesn't have a header and for only certain columns you need to pass params header=None and usecols=[3,6] for the 4th and 7th columns: Unlike Python, it's unambiguous in Julia when you tell someone that the height data is in column 3. That's not an insignificant fact.
www.xlwings.org
Yeah, I saw the site, but could not find the actual addin file download.
I might be mistaken, but my understanding is that what you are asking is impossible for all but 1) matrices of dimension 5 or less or 2) very special cases of matrices. This is simply because eigenvalues are defined as solutions to a polynomial equation.
Here you go: [http://docs.xlwings.org/en/stable/installation.html](http://docs.xlwings.org/en/stable/installation.html) Read the entire page, it mentions the add-in at the bottom (there are 2 steps to installation). Actually I think it also works with Julia, may want to look into the docs for that.
Is being succinct an advantage? C can be succinct as well and very hard to understand. I personally prefer clarity to 1-line code.
True, but certainly more blogs/documentation on concurrency in Julia is needed once that part of the language is fully agreed upon. I am excited about 1.0 which I am currently running!
If on version 7.0 or later, start Julia, then press `]` to go in "package manager mode", then type `update` then `add PackageName` for each package you need.
Is there a point in me checking out version 1.0 if several packages won't precompile?
An if statement?
I want it to print the expression that failed and raise an exception just like @assert. Swift has both "assert" and "precondition" with the latter always executing no matter the optimization level.
Will Julia not ever optimize an if statement away? Its common in C to remove 'dead code' that "can't ever happen", and noticed when the optimization is on the path of a vulnerability.
Yeah, I noticed that oddly long time it takes to plot. Any idea what's going on?
Let me know if [this](https://en.wikibooks.org/wiki/Introducing_Julia) is helpful, or not. I'm always interested in getting feedback...
I was thinking OP was talking about more common optimization methods, such as removing test code from production environment. Removing an if statement requires proving it is useless, which makes wanting to keep it useless as well.
Here's one of the ways that C compilers sometimes prove that a test is useless: int n = 0; while (1) { if (n++ &lt; 0) break; } compiler: hmm. So you gotta, a, a number. And you add to it. That means the next number is always larger than the previous number. So the possible ranges of this number is `[0, infinity)`. OK then. compiler: hmm. so this test is asking, uh, is this `[0, infinity)` number ever going to be less than 0? no of course not. says so right in the range I just proved. QED. now, I'm not saying that *good proofs* aren't possible, but C compilers have bad proofs and they (because they're used as C++ compilers which have mountains and mountains of templates) extremely eager to remove dead code.
I'm finding most packages don't work with version 1.0.
I found out today if you’re going to use `Pkg.add(“*insertpackage*”)` you have to first type `using Pkg`
1.0 came out only a few days ago, so its understandable that not all packages have caught up yet. 1.0 means the language is stable, in a few weeks time everything will work much better (and more updating of packages is done every day)
That's a tough question to answer. For reading in CSV files, i would imagine that some of the python libraries are more robust (but check back in a few weeks once more packages have upgraded to 1.0. For data wrangling and manipulation, there are really two options, `DataFrames` and `JuliaDB`. I would imagine that the performance of both are comparable to pandas. 
Thanks for your reply. I have this question because I have heard many times that Julia is super fast. Maybe it’s faster in other tasks?
The biggest thing I noticed so far is the startup time. This is marvelous! 
When a package does get updated, do I then do: update &lt;package_name&gt;? Anything else beyond that?
Depends on what you want to do. Probably easier to give it a couple of weeks. Packages are rapidly catching up right now.
Most of the resources out there for v0.6+ should be pretty reasonable. There are obviously some changes going forward, but it's not that far off. I did find a couple links for you, though: v1.0 compatible: https://github.com/bkamins/The-Julia-Express v0.6-ish: https://learnxinyminutes.com/docs/julia/
This does break though, it is an example of the compiler compiling away a test you may want to keep.
Julia really is that fast, and with JIT compilation, it can even be faster than C! That said, `pandas` and `dask` are not really written in python. They are essentially C++ libraries with python bindings, meaning all of the heavy computation for those packages is done in C++. Additionally, those libraries have been around for a while, so the writers of those libraries have taken care of a lot of edge cases and small optimizations. We may have not written implementations for all of those yet, but we are getting there. That isn't to say that DataFrames and JuliaDB aren't as fast or faster, just that they might not uniformly be faster. You can always benchmark some of it yourself and see if julia is faster. 
Amazing! Thanks
The documentation is also very good, and you'll have to read some of it sooner or later: https://docs.julialang.org/en/stable/
running &gt;Pkg.add('DataFrames') broke for me. Immediately got turned off the language. (And yes, I looked up the package manager and managed to install a few libraries, but the setup experience still gives a bad first impression).
they released v1.0 before libraries could update. this takes a few months probably. I read they had real good reasons to do it. the main thing is that it's simultaneous with the 0.7 release IIRC
Because DataFrames is not v1.0 compatible yet. Did you read any information about the v1.0 release? It's quite clearly mentioned many places that packages are only now catching up to the new language version, and v0.7 is a safer bet in many cases (it's a transitional release from v0.6 to v1.0 that doesn't have the deprecation warnings removed). The community also has a friendly community you could reach out to over at [discourse.julialang.org](https://discourse.julialang.org) or their slack channel that you can find the link to on discourse.
What kind of tutorials? Basic language features, or?
\+1 for Linux for the cleanest experience, but I will say that many packages take pride in running CI on linux, macos and windows.
I work primarily with Windows. In general, everything (I needed) worked on Windows. Julia uses a clever system of getting binary dependencies (`WinRPM`) where it is overall difficult to predictably compile them on Windows. Having said this, the transition to `1.0` is very recent and not all the packages have been updated yet. In particulat, `IJulia` doesn't work on Windows at the moment, from issues it seems that the reason being the updated version of protocol in Python installation. So, hopefully it will be resolved soon.
I wonder why Julia 1.0 break on that. Couldn't just print something like "module require v0.6 and not compatible with 1.0" ?
It's understandable that people would be put off by the package manager not working, especially when on the website, the version advertised to be downloadable is 1.0. It should be made clearer imo, for now I'm waiting for packages to be ready and compatible and the ecosystem to be a bit more consistent.
What is each of your "Silicon Valley" character counterparts? [alanedelman](https://www.reddit.com/user/alanedelman): &gt;I am not sure about counterpart, but I do identify with getting lost in a data center. :-) el. oh. el.
This isn't the AMA - click the title to go there...
D'oh Thanks :)
:) It's odd that there was nothing in this sub about it... :)
My experience has been Linux &gt; Windows &gt; MacOS. Installing some foundational packages on MacOS can be broken or very fiddly.
In this example, the break is removed as dead code. The `n++ &lt; 0` can't ever happen, so its block can't happen either. A C compiler that removed the test as always false, and then caused the block to always fire, would be buggy in a way that even the idiot who wrote the 'optimization' above could understand.
I'm the maintainer of a Julia package, so trust me, it's annoying that many people's first impression with my package is that it doesn't work. So I don't disagree, it could have been handled a lot better. However, if you're new to Julia, I guess you have seen one of the announcements, and the ones I've read all go through the story about v0.7 as a transitional version, that you should use it until the stuff you need is ready for v1.0.
I knew about it because I browse reddit and I followed on the website but one isn't as avid a follower, I can still understand the confusion, I'll go back to it in a month when I have more time!
See you in a month. Remember there's [discourse.julialang.org](https://discourse.julialang.org) if you run in to any problems, but let's hope everything has settled a bit by then.
thanks I will!
Julia 1.0 is really a great release. What I miss the most compared to other languages is a debugger. Are there any plans for developing a debugger (and keeping it up-to-date with julia releases)?
Thank you! This worked.
Are you saying there is NEVER an example where when you add 1 to n, it couldn't become negative? (hint, what happens when n is as large as it can be?) 
I didn't think it was possible, but you're managing to read me as saying the opposite of what I'm actually saying. Try reading the words as they're written instead of guessing my biases based on tone and word-choice.
Yeah, sorry again at that. Given the tight integration with LLVM, I am kinda surprised there isn't a @keep annotation which says "don't optimize this away"
That is the wrong place
Hahahah! 
I don't get it...
You should submit a PR!
A package manager that can't tell you about incompatibilities between versions of dependencies isn't really worth much, especially to new users. I can't see Julia gaining much traction if this is the on-boarding experience.
You got to be kidding me!!! Arrays starting at [1]?!!!! HERESY! I love you guys.
This is the kind of material the community should produce. One of R’s strengths is its immense wealth of learning materials. If Julia can come close to that it’s future is bright.
This is the kind of material the comnity should produce. One of R’s strengths is its immense wealth of learning materials. If Julia can come close to that it’s future is bright. 
Wow amazing thx for the info :) really neat.
one is 2\^(2x) and the other is (2\^2) \* x 2x is short hand for (2\*x) rather than 2\*x
Thanks for the answer. It's helped me out a lot. Would you be able to let me know if it is possible to use this method to calculate second derivatives too? Or how, I'm really stuck here.
Hey! I believe in order to calculate higher order derivatives you need to nest ForwardDiff calls. This is how I would take the 2nd derivative of a sin function at a point x0 x0 = pi/2 ForwardDiff.derivative(x -&gt; ForwardDiff.derivative(sin, x), x0)
[This list](https://discourse.julialang.org/t/juliacon2018-videos-on-youtube/13734) of videos from this month's JuliaConference is an easy way in, perhaps. I don't understand most of what people are talking about, so that's an indication that these might be aimed other than at beginners... :) 
I think that's got it working. Thanks for your help :D
&gt;I was wondering whether Julia could be a good choice for my work. It seems like a perfect job for Julia, it was initially made specifically to avoid this two language issue. You can do all the standard matrix operation in-place (so no memory allocations). You could even try automatic differentiation, there's pretty good packages for that. Here's a playlist of tutorials on the Julia youtube, maybe it's a bit basic for you though: https://www.youtube.com/watch?v=4igzy3bGVkQ&amp;list=PLP8iPy9hna6SCcFv3FvY_qjAmtTsNYHQE The method section of the manual can be useful too: https://docs.julialang.org/en/stable/manual/methods/#Defining-Methods-1 This can be useful too: https://cheatsheets.quantecon.org/ One issue is what version to use, Julia 1.0 was just release and the package ecosystem hasn't fully transitioned yet. At the moment it's recommended to use Julia v0.7 (which is the same as 1.0 but with less compatibility issues) but you could even go with v06.4. 
Honestly, the Package instability probably makes it one of the more frustrating times to get into Julia right now. Should all be good in a month. Fingers crossed.
It's a bit of a communication error to put 1.0 in front, most packages have issues with people complaining about it. But on the other hand it forces peoples to update their packages.
Readability? It is closer to how actual math formulae are interpreted
Julia sounds like exactly the right tool for this; one of its primary design considerations is to remove the "two language problem" you just described. I think you will find it expressive and performant. The stereotypical but probably correct advice is to start by just...building what you want to build. Learn as you go as needed. I'm not sure how to answer your question about libraries. Extensive set of libraries for what? In some domains such as mathematical programming (JuMP.jl) and differential equations (paging /u/chrisrackauckas), I believe Julia's packages are best-in-class. For "most" things, Julia has a high-quality, usable, and evolving native package. Said package won't have all the exotic features of a more mature equivalent in another language, but it does the vast majority of what you need. That said, there's no denying the ecosystem is smaller than Python's. It's also worth noting you probably need fewer packages in Julia than in C++/Python. In Python you need packages which are sneakily written in some low level language for speed, and (from very very limited personal experience) I recall using packages in C++ because doing things efficiently and safely yourself is somewhat painful. Julia is fast and low-pain, so...less need. Lastly there are excellent language bridges such as built-in calling of C and Fortran [here](https://docs.julialang.org/en/stable/manual/calling-c-and-fortran-code/) and [PyCall.jl](https://github.com/JuliaPy/PyCall.jl). Maybe also calling C++ via [Cxx.jl](https://github.com/Keno/CXX.jl)?
I’d have to dig the exact link out of the docs, but I’m pretty sure short handed multiplication has a lower precedence to explicit operations which is what’s causing the difference between your last 2 examples.
Libraries for matrix algebra I meant. Generally, a hand-written implementation of say matrix multiplication or SVD decomposition will have poor performance. So, when doing anything you need libraries like Numpy or Eigen (c++) which have been optimized. So I was wondering whether we have such a developed ecosystem for Julia as well. Correct me if I am wrong, but the little that I have seen of Julia, we dont have this library issue (atleast for matrix manipulations). 
Ooh! that looks really neat. And that cheatsheet is a god-send. 
I am super not an expert on this, but, as far as I know: - By default Julia uses OpenBLAS. - This is the same as C, Fortran, Go, Lua, and some Numpy. - You can also compile Julia with Intel's MKL which may give better performance.
I think 2x is parsing like -x when exponentiation is involved. Its confusing and seems like it might be a source of errors.
Yes, Julia has *excellent*, best-in-class linear algebra support. It's among the most core focal areas of Julia from the very beginning. In addition to built-in BLAS support, you should check out the package StaticArrays.jl, which has super-fast small fixed-size arrays, that are *much* faster than ordinary arrays. Base Julia also has support for lots of special smart array and vector types types, such as ranges, diagonal, symmetric matrices, etc. etc. etc.
you can't please everyone every time. julia tries to be natural. when you write 2x, you really don't want the 2 and the x to behave like separate entities. but in the second case, when the operator is explicitly given, this does not apply, and operator precedence takes over. your other example in the comment is yet another use case, 2x\^2, which parses as 2(x\^2) because this is how polynomials are usually written: 3x\^3+5x\^2+x+7. so it is basically the principle of least surprise.
Awesome stuff. I am teaching second year ODE with applications in the fall semester. I hope to use your package for some interactive sessions. Hopefully IJulia/Jupiter has worked out all the bugs. 
Yeah fingers crossed. For those who don't know, it's just Windows now and it can be tracked here: [https://github.com/JuliaLang/IJulia.jl/issues/693](https://github.com/JuliaLang/IJulia.jl/issues/693) I'm sure it will be worked out in time for fall courses.
Don't know the solution to your problem but the [Julia discourse](https://discourse.julialang.org) is much more active!
Thank you! I'll try there as well. 
Unless you are __actually transcribing a mathematical formula__ into your code, just stay the heck away from coding 2*x as 2x. Your future self will prefer it that way, as more readable, well after you've forgotten what you wrote. 
Been looking into it recently too, similar background. Warning: Julia 1.0 is frustratingly useless right now, most libraries haven't been updated to work. Go with 0.6 for exploring the available ecosystem.
You can interact with SQL server over http via xml. So now you have 3 problems :)
i don't see how 2x is less readable than 2*x. if anything it is more readable. this of course does not hold for things like 2height, in this case, use 2*height
I think that this syntax is especially useful for cases like this: a / 2x instead of a / (2 * x). Inside complicated expressions this can really clean things up. I certainly use it quite a bit.
2 is not a julia question, but general RDBMS question. the answer depends on the use case, and both have benefits. sometimes you don't have DDL access to a database, and asking for the database owner to create your procedures is a hassle. on the other hand, you can change a stored procedure without deploying a new software, it is one additional abstraction layer. there is no real performance issue with any of these, since you can reproduce the exact steps a procedure would do with SQL. in my experience, keeping it simple is the way to go for most applications. that is, not only you should use SQL directly, but also try to shy away from overly complex SQLs, if at all possible, especially vendor extensions. it makes it easier to switch platforms. it might also be a good idea to abstract data access in your application, so you can even switch to non-relational data sources. but your mileage may vary.
Is there a benefit to doing it that way, as opposed to like an ODBC connection?
Yeah ODBC.jl definitely works with 0.7 and I don't get any warnings on load so it probably works on 1.0 as well. The README looks a bit out of date.
Awesome! Thanks! I look forward to playing around with this package; hopefully I'll be able to replace some of the antiquated VBA and .NET code at my work with Julia at some point.
&gt; 2 is not a julia question, but general RDBMS question. It was only a Julia question in the regard that I wasn't sure what performance costs come with using Julia to pass explicit queries rather than keeping the logic in the database (e.g. what kind of efficiency is lost when parsing the syntax from Julia and whatnot). &gt; there is no real performance issue with any of these Thank you! That's exactly what I was wondering! &gt; it might also be a good idea to abstract data access in your application What do you mean? (Pardon my ignorance; I'm not totally familiar with some of the concepts you refer to)
by abstraction, i meant an additional logical layer. for example instead of doing some hypothetical rowset = exec_sql('select a, b, c from x where d = $p') xlist = process(rowset) you introduce reader functions like function get_xs(p) rowset = exec_sql('select a, b, c from x where d = $p') return process(rowset) end and while at that, you try to refactor to be nice and consistent. unify some similar calls, etc. the benefit is that if you ever need to change the backend, the necessary modifications are isolated and also probably easier.
Ahhhh I see what you mean! Thank you for the advice!!
There was for me, I was using a client program "Red Prarie". It's an inventory &amp; invoicing system built on top of SQL Server, so no ODBC was available. But I did a MITM and found out it was plain HTTP &amp; XML. Saved us a heap of cash because they charged you for running reports on your own data. You could run SQL queries using the client but it only returned 1000 rows. Although I didn't know that when I did it. I was using it to check the reports the IT dept. had screwed up because they wouldn't believe they were wrong. It was only a $1m dollar difference !
Damn, good job! As someone who works in an IT department, I cannot describe to you how well I understand the sentiment that your IT department "wouldn't believe they were wrong." I've basically got a secret alliance with our QA department to help them call my bosses out on their BS. Lol 
Julia has the foundation to be great in ML. Compared to python, some tools are missing but are being developed rather quickly. As mentioned knet is a great deep learning framework. Julia also has a dedicated and well maintained package for solving MDPs and POMDPs called POMDPs.jl which as far as I know, is one of a kind. 
@[ChrisRackauckas](https://www.reddit.com/user/ChrisRackauckas) would you know how to plot direction/slope fields with VegaLite plotting?
I don't think VegaLite can do arrow plots. At least they don't demo any: [https://vega.github.io/vega-lite/examples/#scatter--strip-plots](https://vega.github.io/vega-lite/examples/#scatter--strip-plots) . I would use Plots.jl or PyPlot.jl for that.
Sure I can do that, but I still don't see how to plot the vector field. I don't mind submitting a PR to improve the docs here (http://docs.juliadiffeq.org/latest/basics/plot.html) if you can get me started
You just evaluate the derivative function on a grid.
offhand only [this](https://github.com/sambitdash/PDFIO.jl) springs to mind...
 You should use @time instead of rolling your own. You are including the compile time of the code in your benchmark. Even then your need to warm start it, not cold. Julia JIT compiles functions the first time they are run. So do main() @time main() and you'll see I wrote this lot before I noticed that: Use IOBuffer instead of StringIterator Use a Tuple{Vector{Op}}() instead of a Tape I would use a Type for each Op instead of an enum and let the despatch do the work rather than a big switch. Not sure it will help with speed but you're over specific in your types. The JIT can determine the return type of "next", for instance, because it returns an element of a String. I suspect your Array{Op} might be being boxed because it's a more general type than its actual contents. Try a Vector{Op} 
Julia 1.0 and 0.7 are a mess right now because many libraries still have yet to be updated. Try it with Julia 0.6 first. If you still have the same issue, then it is a problem with your code. If not, it is a library bug.
Yeah, it's a bit like the early days of the Python 2 -&gt; 3 transition... You can actually keep all the versions side by side. No need to delete anything. You will need to reinstall the modules for each version though with `Pkg.add("module-name")`!
By the way I tried a Type for each Op with multiple dispatch and it was ludicrously slow.
pdftk https://www.pdflabs.com It's a command line application.
Especially since: julia&gt; Array{Int64, 1} == Vector{Int64} true &amp;#x200B;
Nice, thanks for the info! I'll give it a try
Yep. Everything I have tried, except using Vector, has made it slower
You'd have it installed on the server that is processing the pdfs and call the command there. Then send the processed pdf to where you need.
 SQL server is a database. So your workflow would involve accessing the pdf in the database (the one you are interested in), calling pdftk to modify the pdf, and then updating it in the database. You'd have it installed on the server that is processing the pdfs and call the command there. Then send the processed pdf to where you need. Invoking pdftk via command line is similar on Windows, Linux, Mac. Similar syntax. There are examples on the website. The beauty of languages like Julia and Python are that they let you work seamlessly with your command line utilities to create scripts. The language itself doesn't need to do everything. It just needs to let you easily interface. [Here's](http://blog.leahhanson.us/post/julia/julia-commands.html) a blog post talking about running shell commands from within Julia. 
Yes, you have some type stability issues. \`Array{T}\` isn't strictly typed, but \`Array{T,N}\` is (if T and N are given of course). I would just recommend writing \`Vector{Op}\` to make things easier. Additionally, this shouldn't be necessary &gt;res::Array{Op} = \[\] instead just do \`res = Op\[\]\` to build it as strictly typed the first time. There are a few other cases where what you are (probably inadvertantly doing) is double allocating because you're changing the array type. For example, ``` mutable struct Tape tape::Array{Int32} pos::Int32 Tape() = new([0], 1) end ``` that constructor is bad because `[0]` is a `Vector{Int64}` so this struct is both not type-stable to access and has to re-convert the input. ``` Op(op::O, v::Int = 0) = new(op, v, []) ``` is another place where you want to specify the element type of the empty array: ``` Op(op::O, v::Int = 0) = new(op, v, Op[]) ``` &gt;By the way I tried a Type for each Op with multiple dispatch and it was ludicrously slow. Yes, that's a bad idea because then all of your arrays become non-strictly typed. Multiple dispatch should be used mostly for compile-time controls and not runtime handling. 
Yes in general, no in this case. Plots + GR is already updated and working on v1.0. This issue is specific to the user and should be reported. 
Precompilation doesn't actually store any precompiled native code, so for small functions it won't have much of a difference. PackageCompiler.jl in most cases is actually still partially JITing unless you work out all of the snope compiling bits (I think Makie's compile did this). This is all being worked on but there isn't a holy grail quite yet.
Presumably, it'd be *easier* to precompile a couple of simple functions than a full-blown package. 
Good to know! CC: /u/Gabcab 
Thanks for letting me know! I'll submit a bug report
I think at this point it’s probably easier to write these functions in another language that can be compiled and build your shared library. 
It's not very well hidden state... From the menu, Julia -&gt; Open Workspace to see all your variables. They persist across all files because they are attached to a running instance of Julia (this is really quite useful when you are editing multiple files in a project that are all linked together). Julia -&gt; Open Console to see the Julia console (the instance running) where you can access all your variables. The variables only disappear when that instance of Julia is stopped and restarted. (From the menu, Julia -&gt; Stop Julia, Julia -&gt; Start Julia.) If I remember correctly, the default layout for Juno is to show the Workspace and the Console automatically. &amp;#x200B;
As it was pointed out, this is not Juno's issue, but how Julia works: when you define something in a global space, it persists there within one running instance of Julia. I guess, the big difference here is that the file is not the module like in Python. It's closer to MATLAB/Octave/Scilab. If you develop something big, maybe a package, the point would come when you do want to re-define some constant value (e.g. a `struct`) or erase a global state. My workflow with this is to put all the code into a module, "run" the file with this module and -- this is crucial! -- `import` the module into REPL or test file. If you use `using`, there might be a conflict with already imported names (I still can't figure out how to resolve it in Julia, in Common Lisp, for example, there are restarts in the debugger that let you either replace existing names, or symbols, or keep the old ones, one-by-one or all at once).
In my particular case, the flow is more like grabbing a template PDF from our file system. network, selecting the data from SQL that I need to fill it in with (each row corresponding to a single PDF's data), and outputting the filled PDFs to a predefined location on the network. I see your point, though; if a tool exists for the command line but not for Julia, you may as well just use that command line tool *from* Julia (assuming Julia is even needed for the process). It isn't as nice as being able to keep all of the code and dependencies in one language, but it'd certainly work.
Although I lack the expertise to fully understand it all, I've read in a couple places that precompilation in Julia doesn't quite fully compile. Do you know, if I precompiled the Julia functions, if I'd be able to make them .exe files?
Sorry, I was sloppy with language there. I meant it would probably be easier to Ahead of Time compile just a couple of functions than a big messy module. Ie. package compiler would probably work well for this task. 
I haven't been able to get Turing.jl (and indeed most packages) to work in Julia 1.0 . Anyone had any luck with this so far?
I should make a note that it's not updated right now: https://github.com/TuringLang/Turing.jl/issues/462 . 
In addition to for loops can also use comprehensions/generators: [A[i,i] for i=j:j+2] sum(A[i,i] for i=j:j+2) You could also write an iterator yourself. https://julialang.org/blog/2018/07/iterators-in-julia-0.7
This was written a year ago but I just discovered it and thought it was a very good introduction to Julian-style programming.
This is very unreadable in mobile.
Don't worry, as I said, I have no shortage on other (more practical) programming problems to try Julia against. I chose this example for context because it's simply the best illustration of my question. 
&gt; Getting diagonal elements is relatively straightforward, you can use `diag` to get a specified diagonal from the matrix It creates a copy though, you can't `diag(A) = [1,2,3,4]` for example and expect the diagonal elements to change. Comprehension won't work either - you are creating a new array with it. Would it make sense to have a view of diagonal? That would make access to it much easier. I'm not sure if there any caveats.
Use `diagind`, as mentioned in the post: `A[diagind(A)] .= 11` sets the diagonal to 11. `v = @view A[diagind(A)]`creates a view into the diagonal. `diagind` is a type of range (so no unnecessary allocation), and you can also choose the `k`th diagonal if you want.
I mean the following case: ``` julia&gt; A = [1,2,3,4,5,6] 6-element Array{Int64,1}: 1 2 3 4 5 6 julia&gt; reshape(A, 2,3) 2×3 Array{Int64,2}: 1 3 5 2 4 6 ```
The Wikipedia page on [Row- and column-major order](https://en.wikipedia.org/wiki/Row-_and_column-major_order) is quite helpful on this. Take a look at the first diagram on that page and you'll hopefully see the difference. Julia - column-major (numbers go down the columns in order) julia&gt; M = reshape(1:9, (3,3)) 3×3 reshape(::UnitRange{Int64}, 3, 3) with eltype Int64: 1 4 7 2 5 8 3 6 9 julia&gt; M[3, 2] # 3rd row, 2nd column 6 Python - row-major (numbers go along the rows in order) In [1]: from numpy import * In [2]: M = arange(1,10).reshape((3,3)) Out[2]: array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) In [3]: M[2, 1] # Indexing is the same as Julia except for zero-based rather than one-based indexing (3rd row, 2nd column) Out[3]: 8
**Row- and column-major order** In computing, row-major order and column-major order are methods for storing multidimensional arrays in linear storage such as random access memory. The difference between the orders lies in which elements of an array are contiguous in memory. In a row-major order, the consecutive elements of a row reside next to each other, whereas the same holds true for consecutive elements of a column in a column-major order. While the terms allude to the rows and columns of a two-dimensional array, i.e. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/Julia/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
There's something wrong with your formatting. Look here, it moves down the column, filling elements. In what sense does this seem to be row-wise? julia&gt; A = [1, 2, 3, 4, 5, 6] 6-element Array{Int64,1}: 1 2 3 4 5 6 julia&gt; reshape(A, 2, 3) 2×3 Array{Int64,2}: 1 3 5 2 4 6 julia&gt; reshape(A, 3, 2) 3×2 Array{Int64,2}: 1 4 2 5 3 6 
From [Wikipedia](https://en.wikipedia.org/wiki/Julia_(programming_language)): &gt; Julia has Unicode 11.0 support, with UTF-8 used for strings (by default) and for Julia source code, meaning allowing as an option common math symbols for many operators, such as `∈` for the `in` operator. I'm not a friend of these design decisions. One of the reasons I don't use Perl 6.
My company (a large multinational corporation) uses it in other departments. They swear by it. Its gaining traction at least
Is it mainly used in R&amp;D at your company?
I think so (like I said, a different department). I think they use it for signal processing
Did you check to see if matplotlib is installed for your python3 interpreter? Or try anything else the error message suggests?
It's up to an individual package how to manage its own non-julia dependencies, as I understand it. There are "packages-that-target-packages" (I'm not really sure how to say that succinctly) such as [BinDeps.jl](https://github.com/JuliaPackaging/BinDeps.jl) or [BinaryProvider.jl](https://github.com/JuliaPackaging/BinaryProvider.jl) that provide a conventional way of accomplishing this sort of thing, though.
https://docs.julialang.org/en/v1/stdlib/LinearAlgebra/ ``` using LinearAlgebra ?lu ``` Sparse matrice are supported. Banded Matrices (via BandedMatrices.jl http://juliamatrices.github.io/BandedMatrices.jl/stable/) also work, probably more.
&gt;ARe there any reliable LU-decomposition algorithms in Julia 1.0 that can handle big matrixes? If your matrix is sparse then Julia uses a sparse LU from UMFPACK which handles large matrices. Though for large enough matrices IterativeSolvers.jl is recommended of course.
[https://docs.julialang.org/en/v1/manual/arrays/](https://docs.julialang.org/en/v1/manual/arrays/) Under the Supported Index Types subsection, read the subsubsection on Cartesian indices. The link to the blog post shows some cool, 'julian' ways of using them.
The using statement in this example is just not needed. Btw I would suggest to use Julia's slack channel #helpdesk or Julia's Discourse forum for questions like that.
Can you please elaborate? Why is it not needed? I'm still trying to understand the language and how to properly package up my code to enable others to use my functions. If I give someone my .jl file, how will they load in my modules?
Because you're in REPL
I read that page prior and I don't see it mention anything about don't use modules in REPL.
That's the point right, you don't need to use module in REPL, if you declared them in REPL
You need `using .MyModule` (note the `.`) to clarify that you want the `MyModule` defined in your current workspace and not some package called `MyModule`. julia&gt; module MyModule x() = "x" export x end Main.MyModule julia&gt; using .MyModule julia&gt; x() "x" 
Thank you so much. How do I package up a source file defining a module that I want to distribute? For instance, let's say I created my own API that I want others to use. I assume there is some way to provide my files such that they can import the modules without that dot.
See [rdeits's response](https://www.reddit.com/r/Julia/comments/9bmuzh/using_does_not_work_on_my_own_modules_with_v100/e54qkxd/).
Not sure if this is the exact rationale, but MATLAB uses a backslash as follows: A\b == inverse(A)*b Julia seems to adopt this whether A is a matrix or a scalar. In the case that A is a matrix, it is solved in various ways according to the properties of the matrix, but the actual inverse is probably not solved for behind the scenes. 
Maybe it is a tradeoff between speed vs accuracy. 
yep, it solves for x in Ax = b without doing the inversion. Check out the now famous "[don't invert that matrix](https://www.johndcook.com/blog/2010/01/19/dont-invert-that-matrix/)" post by John Cook.
It's an example with the purpose probably being having everything on one screen so that people can explain to OP how the `using` command works for local modules. That's why everything's in REPL. I rarely downvote comments but this one truly does not add anything to the discussion and in fact only served to confuse IOP more than they already were.
Well, your criticism is deserved. But OP needs help in asking the right question, not answers taking their question at face value. Quote "How do I package up a source file defining a module that I want to distribute?" I suspect "using .MyModule" is the answer to the wrong question. OP wants to distribute their code, not work with submodules of Main. My actual advise to "use Julia's slack channel #helpdesk or Julia's Discourse forum for questions like that" still stands.
By far the easiest thing to do is to make a git repository (we usually use GitHub but you don't have to) named `MyPackage.jl`. You need at least two things in that repo: 1) a file `src/MyPackage.jl` that contains your module definition and your code, and 2) a `REQUIRE` file that lists what other packages you depend on. That `REQUIRE` file must at least contain a single line indicating what minimum version of Julia you need: julia 0.7 # or whatever version you want People can then add your package with `pkg&gt; add https://whatever#master` Here's an example of what a Julia package looks like: https://github.com/MikeInnes/Lazy.jl You can "register" a package as well. A registered package can be installed by name (`pkg&gt; add Lazy`) instead of only by git URL. There's a great GitHub bot called attobot that makes registering packages super easy: https://github.com/attobot/attobot . Before you register your package, you should make sure you're happy with its name and that you have a nice set of tests to make sure it works. Finally, there's a useful tool called `PkgDev` that automates creating the basic package structure. Currently it only works on Julia v0.6, but it will be working on v0.7 and v1.0 as soon as this PR is merged: https://github.com/JuliaLang/PkgDev.jl/pull/144 
Thanks, that's exactly what I was looking for. I was only considering scalar context and was .. perplexed.
Also, the Julia [discourse](https://discourse.julialang.org/) and [slack](https://slackinvite.julialang.org/) are excellent resources. 
Multiplication is not necessarily commutative (e.g. [matrix multiplication](http://mathworld.wolfram.com/MatrixMultiplication.html), [quaternion multiplication](http://mathworld.wolfram.com/Quaternion.html)). See [division algebra](http://mathworld.wolfram.com/DivisionAlgebra.html). For these cases, \[;\\frac{A}{B};\] is ambiguous. Division and inverse division operators enable the expressiveness to avoid this ambiguity. 
https://docs.julialang.org/en/v1/manual/parallel-computing/ It seems the Celeste project (the petaflop thing) is just using threads: https://github.com/jeff-regier/Celeste.jl/blob/dda1a5d7582220fcb82dd7464f125613a085eed3/src/ParallelRun.jl#L119
I saw that. It's pretty impressive
I saw this comment from one of the core devs in their recent [AMA](https://www.reddit.com/r/IAmA/comments/97jdb9/weve_spent_the_past_9_years_developing_a_new/): &gt;Julia already has OpenMP-style for loop multithreading, but we're nearly ready to merge a [pull request](https://github.com/JuliaLang/julia/pull/22631) that implements M:N mapping of tasks onto hardware threads, which will make Julia's threading system similar to Go's but tuned for extreme computational performance rather than for writing concurrent servers (although you can do that as well). As we see the number of cores on CPUs go up and the amount of memory per core go down, this will increasingly be a huge competitive advantage over other dynamic languages that don't have any plausble answer to multithreading. &amp;#x200B;
It seems that your comment contains 1 or more links that are hard to tap for mobile users. I will extend those so they're easier for our sausage fingers to click! [Here is link number 1](https://www.reddit.com/r/IAmA/comments/97jdb9/weve_spent_the_past_9_years_developing_a_new/) - Previous text "AMA" ---- ^Please ^PM ^/u/eganwall ^with ^issues ^or ^feedback! ^| ^[Delete](https://reddit.com/message/compose/?to=FatFingerHelperBot&amp;subject=delete&amp;message=delete%20ID_HERE) 
Yeah, this is the answer. The devs want this pull request done now, by like v1.1. This will make Tasks the central piece of multithreading. 
It shouldn't be equivalent. In cases where `A` is singular, `inv(A)` does not exist, but `A\B` still returns a solution, which is the least squares one. It's more like the pseudo-inverse.
This is a splendid thing. Many thanks for doing this.
https://docs.julialang.org/en/v1/stdlib/Statistics/#Statistics.mean
First, you need to add `using LinearAlgebra`. Then you can use `I` as the identity matrix when you need it. For example: julia&gt; A = [1 1; 1 1] 2×2 Array{Int64,2}: 1 1 1 1 julia&gt; A + I 2×2 Array{Int64,2}: 2 1 1 2 
Which platform? I do not have any luck on Windows last time I checked. Works fine on Mac and Linux. 
The problem on Windows is known: https://github.com/JuliaLang/IJulia.jl/issues/693
Alternatively, if you want to preserve namespacing: import Statistics Then you can use it a la: Statistics.mean(A) I assume Julia supports import shortening (import x as y) but I haven’t tried.
`import Statistics: mean`
The only shortcoming with that is if I need lots of things from within the package I've got to pollute the import section with heaps of stuff. `Using` is cool, but getting into the habit of `import`instead makes your code orders of magnitude more maintainable.
I mean you got the bare `import` I wasn't about to repeat it. :P
Haha fair enough.
After the buzz about Julia 1.0, I decided to give it a try. The first problem I faced was not being able to run IJulia. I found a workaround and that was to install the master branch of the IJulia. If you dig around the issues section of the github, you should find it somewhere there. That got the IJulia working. &amp;#x200B; But other than that, none of the popular packages work as of today. The best I could do is, 1+1 = 2 &amp;#x200B;
&gt; Matrix{Float64}(I, 2, 2) I understand the need for the type information, but that is such an ugly syntax for a basic operation like this. Feels more like one of those weird numpy calls arising from the constraints of Python, than like normal Julia. 
I agree that it's ugly, but your comment about it being a "basic operation" got me curious about why they deprecated it (I don't really use the stuff in `LinearAlgebra` very much and hadn't actually noticed this deprecation until now) and I found a couple things to read (a [github issue](https://github.com/JuliaLang/julia/issues/23156), a [discourse thread](https://discourse.julialang.org/t/why-eye-has-been-deprecated/12824), and another [discourse thread](https://discourse.julialang.org/t/eye-in-julia-0-7/9820)) and the deprecation makes sense to me. `I` seems better than `eye` for most situations since you don't need to allocate a whole array for what is indeed a basic operation, but `one` can create an identity matrix if you need one. julia&gt; using LinearAlgebra julia&gt; a = rand(2,2) 2×2 Array{Float64,2}: 0.795106 0.956368 0.846418 0.681767 julia&gt; a + I 2×2 Array{Float64,2}: 1.79511 0.956368 0.846418 1.68177 ​julia&gt; one(a) 2×2 Array{Float64,2}: 1.0 0.0 0.0 1.0 But, like I said, I'm not really too affected by this change, so I'm probably easy to convince.
Looking at my karma, this sub-reddit is only for praising Julia. Got it.
Your comment is just harsh given that there are already great packages in 1.0. Check Distributions, Plots, Flux, DifferentialEquations, DynamicalSystems, OnlineStats, HTTP, DataFrames. A lot of these developers are doing this hard work for free and are updating packages at an amazing rate. Also, all of the amazing features in base available to you if you wanted to contribute to the package ecosystem.
What is the BIGTOKEN in http://localhost:8888/?token=BIGTOKEN ? Some environmental variable? The notebook() function does not give me the token to login in the REPL so I have to launch the notebook directly through the jupyter binary in the .julia folder every time I use it. If BIGTOKEN works, that is really convenient. 
Speaking of Plots, I'm yet to plot the official tutorial. As an user, I'm not just randomly trying libraries. I'm testing libraries that might replace my existing stack. So far I'm still at level 0. I understand the breaking changes in the new version. The developers knew the 1.0 is going to break most of the libraries. Why didn't they work to make sure that the libraries are updated as well? If they knew this is going to happen, why create so much buzz prematurely? Do you think it's helping when new users are trying 1.0 and finding that nothing is working other than the standard library? I don't think so. MIT media machine is putting on a good show of promoting the language, but it's too early and going to hurt the growth. I would help to the ecosystem if I had the experience and expertise on the language. But you know what, I can't have these because I can't make anything work.
&gt;The developers knew the 1.0 is going to break most of the libraries. Why didn't they work to make sure that the libraries are updated as well? Do you think the devs and the community isn't aware of issues and isn't working on them ? It just take time, how long did Python took to go from 2 to 3 ? But I do agree that v1.0 should have been put out with some word of caution about its status (even one of Julia dev also said so). &gt;I would help to the ecosystem if I had the experience and expertise on the language. You can still help by testing and reporting issues/bug, although most big packages are aware of their issues already (so you need to check that the issue isn't reported already). What never helps is negativity. It's like if someone hand you free cake, you eat it and say: "there's not enough sugar, you didn't told me I should add some, that's just ridiculous!". 
&gt; It's like if someone hand you free cake, you eat it and say: "there's not enough sugar, you didn't told me I should add some, that's just ridiculous!". It's not ridiculous if it's just a free cake. But when it's sold as one of the best cake with the most balanced sweetness of all the cakes out there, it is ridiculous. &amp;#x200B; Julia is sold as a well-thought-out language for technical computing. When I started using that, it turned out to be not-so-well-thought-out. I should have kept my expectations at bay before believing the marketing. My bad.
Note that most of the 1.0 issues are with the ecosystem, not with the language.
Reminds me alot of the general direction Lazy.jl takes. Cool. 
Hey, TheJelle, just a quick heads-up: **alot** is actually spelled **a lot**. You can remember it by **it is one lot, 'a lot'**. Have a nice day! ^^^^The ^^^^parent ^^^^commenter ^^^^can ^^^^reply ^^^^with ^^^^'delete' ^^^^to ^^^^delete ^^^^this ^^^^comment.
I'll have a look at it. thanks
Looks like it's working on Windows now, this issue has been closed.
It's working now.
Fyi they've fixed the bug so it's working now.
Indeed it is fixed.
One of the authors here, thanks for trying it out! First the bad news, currently there is no out-of-the-box implementation for TrueSkill. Now the good news, but it is definitely possible and I agree that it would be a great addition to have. We could eventually make a demo out of this. I'll try to sketch an angle of attack and along the way explain some of the design choices we made by answering your questions. - TrueSkill requires a hard thresholding when comparing player performance. Currently we only implemented a soft thresholding contraint, namely the `Sigmoid` node. [This demo](https://github.com/biaslab/ForneyLab.jl/blob/master/demo/5_expectation_propagation.ipynb) exemplifies the use of the `Sigmoid` node in a practical setting. Hard thresholding could be modeled by an additional `Threshold` node that models a step-function, analogous to the `Sigmoid` node. Additionally, this requires the implementation of update rules for this new node type, which will also be similar to the `Sigmoid` updates. With these implemented, it would be possible to derive a TrueSkill algorithm with ForneyLab. - The usage of variables in ForneyLab differs slightly from other probabilistic programming toolboxes. In ForneyLab, variables are used to define the model, where `@RV x ~ GaussianMeanVariance(2,3)` will define a prior for the variable `x`. After constructing a message passing algorithm (e.g. through `algo = sumProductAlgorithm(x)`), ForneyLab will return the code for a function `step!(...)` that (by message passing) computes the posterior marginal distribution for the variable `x`. This is the main difference with other probabilistic programming languages; where others run the algorithm and directly return the posterior, ForneyLab returns algorithm code, which you then run yourself. Here, the `step!()` function accepts your data and returns a dictionary of marginals, where `marginals[:x]` will contain the posterior marginal distribution for your variable `x`. - After initializing a `marginals` and/or `messages` dictionary, the `step!(data, marginals, messages)` function can be run multiple times. Because `step!()` operates in-place on these arguments, this yields an iterative execution of your algorithm. Again, see the [expertation propagation demo](https://github.com/biaslab/ForneyLab.jl/blob/master/demo/5_expectation_propagation.ipynb) for a practical example. While these answers might not explain directly how to set up the TrueSkill algorithm, I hope they illustrate some of our design choices. Time permitting, I hope at some point to write a demo for this. And if you try it yourself, please let me know how it works out. 
I totally agree - I have the same experience with the BioJulia package for bioinformatics and Plots, which doesn't work. It's kind of a bummer to celebrate the official release by trying to learn the language, but not being able to import any interesting packages because they don't compile. &amp;#x200B; That being said, It'd be a shame if that one single mistake of a premature 0.7-to-1.0 release leads you to not try the language. I hacked a simple kmer counter and FASTA parser together with Julia 1.0 and it was absolutely amazing. Write in your calendar to check out Julia again in 4-6 months when all the big packages has been fixed :)
Thanks for this, it's very helpful. I'll definitely have a go at implementing TrueSkill. Also, is it okay if I pm/email you further about this? The reason I've been looking into TrueSkill is that I/the research group I'm part of have been looking at using a slightly generalized version of the TrueSkill algorithm for quality scaling, but we weren't entirely sure about how feasible the method is.
&gt; With R, Julia and Matlab, documentation seems to be always very clear if I need to do any additional steps to start using the commands and functions. I guess that comes with being much older and established languages. If you think the documentation can be improved, please help by making a pull request!
This is about as useful as shouting "If you think the design of the railings can be improved, please forge some new ones!" to someone who's just been washed overboard a ship.
As OP did not provide background, https://en.wikipedia.org/wiki/Tuple_space 
I omitted that because anyone who doesn't already know what a tuple space is wouldn't know the answers to my questions anyway.
It's not your imagination. Julia has been out of beta for maybe a week. The documentation was sparse and seemed to be somewhere at people moving over from other languages in the previous version, and that seems to have carried over. It will get better in the future, but for now there will be some growing pains. Don't be afraid to keep posting questions on here or stackoverflow. For a beginner, your best bet is almost certainly to seek out community support. You might also consider getting a book. If you're fortunate enough to have access Safari Books, there are some decent Julia books on there. .
Please do, in order to improve usability it's also important for us to get some user-feedback. After some time one tends to develop some blind spots ;) 
even if they do not know the answer to your question, at least they could potentially understand your question. and then understand someone else's answer.
Point conceded.
There are things in Base that are not exported (see [list of exports](https://github.com/JuliaLang/julia/blob/release-1.0/base/exports.jl) and `peek` is one of them. You can do `import Base: peek` or just qualify its usage with `Base.peek`.
&gt; There are things in Base that are not exported and peek is one of them. Quick question: why isn't `peek` exported?
Thanks! I assume the reason there's no entry in the official documentation is because there's no docstring associated with it?
Hey, thanks, but I feel it stills leave me confused. For example, there is a page in Standard Library called 'The Julia REPL'. Of course, that is not a package/module. I just wish the documentation was more clear and consistent. I shouldn't be complaining, I recognize the hard work the whole community is doing to improve the language, and that if I complain about this, maybe I should be doing a pull request, but I am a very new user and afraid I'll mess things up. I don't even know how git works very well. Anyways, thanks a lot!
Just guessing, but perhaps they just don’t want to swamp the default namespace with too many things, and they consider peek to be of use so infrequently that it shouldn’t be imported by default.
The standard library pages of the documentation should probably say that you have to `using` them. That seems like a good fix.
There is a PR opened here: https://github.com/JuliaLang/julia/pull/28811
 &gt;For example, there is a page in Standard Library called 'The Julia REPL'. Of course, that is not a package/module. It's a bit of a detail but REPL is a module (there's some examples of what you can do with it). It's just that's it's already loaded since it powers the actual REPL. You can see all the stdlib modules here: https://github.com/JuliaLang/julia/tree/master/stdlib
You can check this maybe, it looks pretty good: https://discourse.julialang.org/t/julia-language-a-concise-tutorial-updated-for-julia-1-0/14574
thanks!
thanks for the clarification!
It's one thing that is very nice with Julia; a lot of things are just written in Julia and you can play with them easily.
You should always feel free to ask questions -- people actually enjoy answering them, particularly if they're easy! (I'd answer more questions if I knew more of the answers ...) The moving of functions from the main Julia system to the standard libraries happened fairly late on in the 0.6 -&gt; 0.7 development cycle, so that's certainly one area where the documentation hasn't always managed to keep up 100%. Obviously it's useful for future users if places where confusion occurred were listed or mentioned somewhere. For example, if you found any errors in [this](https://en.wikibooks.org/wiki/Introducing_Julia) I would be very grateful. Doesn't have to be a "pull request" or anything, just a note. :) 
I will take a look! Thanks!
It should definitely be noted that using statement is required. They do have some weird policies. For example, they removed the rref function (computes reduced row echalon form of matrix) which is a standard function accross many different environments and is very important, especially in education. Discussion is avilable at https://github.com/JuliaLang/julia/pull/9804 3 years later, it still isn't included in LinearAlgebra package...
Hey, Millkovic, just a quick heads-up: **accross** is actually spelled **across**. You can remember it by **one c**. Have a nice day! ^^^^The ^^^^parent ^^^^commenter ^^^^can ^^^^reply ^^^^with ^^^^'delete' ^^^^to ^^^^delete ^^^^this ^^^^comment.
delete
If you're reading the documentation for a package, it should only tell you what that package does. It shouldn't be responsible for teaching you that packages must be imported before use, that seems to be a basic language feature of Julia (common across almost every language) that should be included in the core Julia documentation, not repeated in every third party library. 
Not the OP, but I do have access to Safari Books, are there any you would especially recommend? I took a look and most of the titles seem to be from Packt, which doesn't have as good a reputation as O'Reilly.
If you are interested in graph visualization, check out the JuliaGraphs github organization and NetworkLayout.jl (https://github.com/JuliaGraphs/NetworkLayout.jl). NLP looks hard to get into right now but check out https://github.com/JuliaText And check out the github groups in the bottom of this page (https://julialang.org/community/) to explore packages in terms of topics. Also https://juliaobserver.com/ to explore packages (organized by categories on the right).
I would personally prefer to see Julia specialize to scientific and numeric computing rather than try to generalize and appeal to everyone.
I was like that initially, because the scientific computing community is pretty great, lots of smart people there, which leads to generally better decisions made in terms of code structure, community organization and so on. But I figure if other fields use Julia, I will have a wider set of libraries to choose from if I wanted to use some esoteric functionality in my Julia code (yes I use Cxx, RCall, PyCall, etc). Plus multiple dispatch is really nice and I would like to see how it can be used in other fields like web dev in a nice way.
Something to think about: is there domains in which multiple dispatch would be a big advantage, problems that can be subdivided into subtypes that interact together in a complicated way (it's not only the pairs that matters) ? UI programming maybe ? Other domains that Julia could excel at is dynamic compiler related things (since LLVM is easily accessible) and metaprogramming. Stuff like CUDAnative seems promising. Maybe WebAssembly ? 
I use Julia as my go to. The nearest I have come to "scientific computing" is using JuMP ot a bit of statistic. I do mostly text processing - web scraping etc. mostly logging into some website and moving data from HTML to Excel. Or processing Excel workbooks produced by colleagues and processed somehow. Add in some SQL and it's all business office data wrangling.
i'm using it for algorithm prototyping. something like a proof of concept, which later can be implemented in another language if needed.
Bad bot
This reminds me of a Larry Wall story about Perl. Perl obviously specializes in working with text data and for awhile had no support for binary data at all. However a later version of Perl finally did get support for binary data and that opened up the whole world of the internet to Perl. After all, the internet works with a tiny but essential binary protocol, followed by kilobytes if plain text after that. By generalizing the language, Perl became an even bigger success at its strengths. Julia is an excellent scientific programming language with how potential. However speaking as a software engineer it's hard to work into a production workflow. If it's going to be used in a production system then it either needs to handle all the business logic as well, or at the very least integrate well with the other languages being used for that purpose. Because of this, I think the best thing for Julia would be to have a simple yet capable REST server library. That would allow Julia applications to run as microservices that can be used by a business application without having to worry about the details of it.
I think scientific computing nowadays is so complicated, that there is no big difference with general purpose computing. The priorities may be different sure. But in the end I think we'll end up with a really amazing general programming language. Actually, I think we should are already there with more libraries to come.
Is this a good time for a beginner to get into Julia or I’d be better off sticking to Python?
Does anyone know what the web scraping package-scape looks like for Julia? That’d my domain besides data cleaning, viz, and web apps. 
The biggest problem for Julia right now, in my personal experience, is that is has not reached critical mass. I work in engineering/numerical programming, and Julia is by far the most suitable language for almost everything I do. But I cannot use it, except for in pet projects. My colleagues have heard about it (from me, but they are moving to Python), but few of my peers in my engineering domain. *None* of our customers know of it, or would remotely consider it. Everyone is either on Matlab, or moving from Matlab to Python. I cannot develop Julia code for people who don't know it, and can't use it. In order to change this picture, Julia needs more users of all stripes. If it starts going mainstream, then other engineers start using it, or at least will know of it, customers will become aware of it, and will have employees who master it. Furthermore, serious data processing now includes moving data around on cloud servers, interfacing with instruments, creating GUIs, serving results on webpages and auto-generating written reports. I'm not saying Julia cannot do any of the above, but I want to highlight that in order to stand a chance against Python in the engineering domains, it needs to handle the "full stack" of "general programming tasks", otherwise we just have another two-language problem on our hands.
I don't know...the quality of software that's published along with papers by academics is low because it's usually not worth the time to make it good. I feel like you're extrapolating that to all software published by academics.
I'd wait a month or so if you're using stats or visualization packages. 
JuMP, DiffEq, SUNDIALS, LightGraphs, IPOPT, etc. Long-term supported software can come out of academia, it's just rare and not incentivized. I would say that most code you see from academics isn't software, and that's the problem. The people who do make well-supported software tend to write good stuff. 
Focusing on scientific computing allows it to have a stance in bikehsedding and API disputes. If anything, the API has to be good for scientific computing. So for things like "should unicode be allowed?" once can argue from the PoV of a scientist because that's an established use case. That's good to have. That doesn't mean it should be bad for general purpose computing, but building with an explicit use case in mind is a good way to make generally good decisions.
I think that one of the greatest aspects of Julia is that in attempting to create a language that was flexible and dynamic enough to do any sort of scientific computation imaginable, they ended up making a language that is flexible and dynamic enough to be relevant and useful in nearly *any* domain. The fact that Julia supports generic functions, macros and compiler modification on a first class footing means that Julia can effectively have whatever syntax, interfaces and evaluation models you can dream of and they're not necessarily difficult to build. 
If you want to gain any traction from the Python user base, it’s going to have to be good at some general purpose things. What I mean by that is: one of the reasons I don’t use R at work is that I can’t use it for general scripts the way I can use Python/JS etc. I have half a dozen AWS Lambda’s doing various tasks written with Python that I’d love to replace with something good. I spun up 2 web services using Python frameworks (Flask and Sanic) and I’d love to be able to replace them. Basically, if it can’t do a variety of other jobs and fit into a modern workflow, adoption is going to be severely limited, and you’re going to have a hard time building that community momentum (in terms of number, quality and range of libraries built). Julia does numeric/scientific computing brilliantly, being able to do some general software engineering things well as well won’t compromise that, only improve its standing.
&gt; in order to stand a chance against Python in the engineering domains, it needs to handle the "full stack" of "general programming tasks", otherwise we just have another two-language problem on our hands. Exactly this. If you have a great number crunching solution, but I can’t easily spin up a web server/put it in a docket container/etc to show people or integrate it into an existing software engineering workflow, people simply won’t use it.
It really depends on what you are looking for. While I'll grant that Packt isn't necessarily at the O'Reilly standard, their books are still perfectly reasonable references, if you can find one that covers what you need. That being said, if you just want to learn the basics, Learning Julia by Anshul Joshi and Rahul Lakhanpal covers a lot of the fundamentals pretty well, has a lot of example code, and is generally easy to read. If you have more specific needs, most technical books have a "Who this book is for" section. Find a few books that seem like they might be ok and read that section from each of them. Don't forget, though, that even the recent books will all still be written for pre-1.0 versions. There will be some inevitable inaccuracies you will have to contend with since, by many accounts, 1.0 breaks backwards compatibility with the previous versions. Most of that should be easy to find on the Julia homepage or stackoverflow, but it's going to be an issue until the 1.0 boss start rolling out. 
More users also means trying to appeal to a wider range of people and wider range of possible applications. There are always tradeoffs. I also use ASP.NET just like you do. If I want to build a website, I will use ASP.NET. Data scraping? Python. Desktop application? Electron. Time series analysis? R.
I, for one, am currently using it to automate some processes at work that don't necessarily require advanced scientific/numeric computing, but mainly just string and file manipulations right now. While other languages could be used to do the same thing, using Julia comes with 2 advantages for me: 1. If later I decide I want to incorporate some ML or data mining into the process, it'd be relatively straightforward to do so. 2. My company is a bit light on people with real programming chops, so if we decide on using a scripting language as a tool, it has to be relatively easy for others to learn/debug, it has to be able to do a broad range of tasks (so we don't have to have people learning new languages for every type of process or analysis we create), it has to be efficient, and it has to have interoperability with other systems/languages. Essentially, Julia is capable of being a sort of "full stack language", which is exactly what we need in order to keep things consistent and usable across people in our IT department.
Thanks! And thanks for the amazing great work!
I think that it should be _great_ at scientific computing, but it _also_ needs to be kind of good at many other things. So that's the challenge, and it's a tall order. But I am optimistic that it will be.
Also, someone has ported the excellent open source book, "How to a think Like a Computer Scientist," to Julia 1.0. Check it out here: https://benlauwens.github.io/ThinkJulia.jl/latest/book.html
https://discourse.julialang.org you can ask here. More people will answer
One average GPU will far exceed the effort of putting together several computers, even if you already have them. For a simple example, it might even take more time to run on your cluster, since you’re likely not going to do tons of code and hardware tuning 
For most deep learning problems, you really just want low latencies between your data and gpu and a fast gpu/cpu. Where multiple rigs are used, they're very closely networked together and generally just used for tasks like image augmentation. I doubt you would see much of an improvement if you networked computers together. I went through the same thought process a few years ago but in the end, it wasn't really worth the effort (unless your other computers are also very good).
You shouldn't use Types to emulate if conditions. Using a switch like in the current code will always be faster for this type of code.
Yeah, I noticed that when I implemented OPs program using codes, it was 10x slower. Which is sad because the code looks much nicer. 
&gt;Because of this, I think the best thing for Julia would be to have a simple yet capable REST server library. That would allow Julia applications to run as microservices that can be used by a business application without having to worry about the details of it. As someone not all too familiar with software engineering, could you expand on what you mean by this and how it'd be beneficial? Is that something you think Julia developers should make, or something a company should set up if they want to use Julia in a production environment? (Pardon my ignorance; some of this stuff is a bit outside my realm of expertise)
&gt;The fact that Julia supports generic functions, macros and compiler modification on a first class footing means that Julia can effectively have whatever syntax, interfaces and evaluation models you can dream of and they're not necessarily difficult to build. Pardon me if this is a silly question, but couldn't that level of freedom pose issues in terms of interoperability and readability? Customized types come to mind as an issue I've run into before; the ODBC package, for example, reads in datetime values from SQL as a specific, customized type that the package XLSX doesn't accept, and making the 2 play well together is nothing short of a troubleshooting nightmare. I love how flexible the language is, but I am a bit concerned about the future of package development and maintainence for it; it seems like things could get *really* messy really quickly once Julia starts getting more popular, and more diverse groups of folks start creating popular packages.
Essentially Julia developers (not necessarily the language developers but Julia library authors) should create a library with either an embedded webserver or something like python's WSGI which let's Julia work with a webserver like nginx. With this, companies can develop their numerical applications in Julia and expose it as a service that their main application can use without all the business logic having to be in Julia. For an example, say a company wants to detect customer churn. They have a whole web interface for their analysts to look at customers and their info which includes a percent likeliness to leave the service within the next 6 months. This number is determined by a neutral net written in Julia running on a server. To pass in the customer, the Julia code also includes a simple webserver that accepts a POST request including the customer fields which the main webserver for the business application can call out to. This separates the Julia code from knowing how to display the info, validate end user credentials, etc, while also separating the business code from having to implement the neutral network.
Small matrix multiplies do not parallelize well over distributed setups. You want large enough matrices so that each node has a pretty complete matmul (well, set of dot products) to do efficiently and long enough so that the data transfer cost is minimal. So this would only work for large large network. But, even then, you'd want to put a GPU on each node if you're at that size.
Ahhh I think I understand; it's basically just setting things up to use Julia as the engine for the computations that other applications can call out to. Thank you for explaining that! This is a bit off topic, but I'm curious, where do you think the business logic should be stored? A coworker of mine is adamant about keeping *all* our logic in SQL views/stored procedures, even in situations where SQL really doesn't seem to be the best/most efficient option. I understand that you generally want to do your data manipulations on your database server, but there are some things that SQL just flat out isn't good at (complex string manipulations or aggregations, and pretty much anything that'd generally require iteration come to mind). In your ideal setup, where would the business logic reside?
I'm not an expert on that, but general advice is to leave business logic outside of the database. Personally, I prefer to keep the SQL as "dumb" as possible (except for performance bottlenecks) and implement all non-relational logic in my code. However, I've heard of enough cases of people putting that kind of logic in the database, that there must be some (maybe short term) benefits towards doing so.
The biggest benefit I can see is just that the logic is basically all in one place and in one commonly known language. In a smaller place like mine, where we don't really have any "true" programmers, the ubiquity of SQL is particularly nice since it means we can all read, validate, and debug each other's code. But, of course, it's limiting. That's why I'm currently trying to get some of my coworkers to learn a new language that we could use as a "glue" language, and I'm hopefully Julia could be that language (or at least Python or something).
Isn't custom types a problem for all languages where you can make your own classes or types? I work with C++ a lot and I am very familiar with that problem, where you have to do conversions all the time if you're using more than one library. There are efforts to lessen this problem in Julia since you can create small extendable libraries for a single purpose that hopefully will be used by everyone who want to do that single purpose. For example you have ColorTypes.jl which contains just color types, Colors.jl containing functions for working with colors e.g. whitebalance and colorspace conversions, StaticArrays.jl for working with small arrays that can be stack allocated, GeometryTypes.jl that contains definitions of simple shapes and triangular meshes, Distances.jl that contains implementations of lots of distance measures, Distributions.jl that contains probability distributions, LossFunctions.jl that contains implementations of loss functions used in machine learning, and lots of other small packages like these. These packages are used widely in the Julia ecosystem and because of Julia's ability to work with generic types, they can work with many different types. For example, the functions in Distances.jl can work with any subtype of the abstract type AbstractVector. 
Actually, the presence of generics means that this isn't actually much of a problem. The idea is that when a user or package maker or Julia dev defines a type, it should be subtypes of some "abstract type" (which may itself be user defined). The idea is that most functions should be defined only in reference to an abstract type. For instance, instead of making a function like `avg(x::Float64, y::Float64) = (x + y)/2`, I should instead write `avg(x::Number, y::Number) = (x + y)/2`. Now, if I make some new numeric type, say a `Quaternion &lt;: Number` object, then I don't need to defined `avg(x::Quaternion, y::Quaternion)`, I can just define what `x::Quaterion + y::Quaternion` and `x::Quaternion/y::Real` mean and then my `avg` function will automatically work on `Quaternion` types! In practice, a good example of this is dual numbers. Dual numbers are objects written like `x + yϵ` in analogy with complex numbers but instead of `i^2 = -1`, we define ` ϵ^2 = 0`. The use if dual numbers is that they allow us to take derivatives of a function! ie `f(x + yϵ) = f(x) + yf'(x)ϵ`. So this is used to take derivatives of Julia code in packages like ForwardDiff.jl. So if I make a package that exports a function like `avg` do I need to worry about the ForwardDiff people's dual numbers not working on my code? No! As long as I just define my `avg` function in terms of abstract `Number` types, the ForwardDiff people should be able to do all the basic definitions needed so that they can pass a dual number into `avg` and take its derivative! I don't know if that was very clear or not, let me know if it makes any sense. 
The fact that that code in that blog post worked blows my mind. Really amazing. 
That some languages are better than others in specific domains is mostly a function of the library ecosystem and not of the language itself. The language features that make Julia attractive for scientific computing also make it attractive for other domains, but the libraries may not be as developed as for scientific computing.
As mentioned by others, this is not a good idea.. However, you will probably be experimenting with different setups, and obviously you can do that in parallel
I was using Python's HTMLParser via pycall. I have, though, been implementing a Pure Julia version. It's not quite ready for general use as it doesn't decode CDATA &amp; Entities atm. but if you're interested I'll make it available. My PC at work is locked down and I can't use github because of the annoying SSL thing.
If your individual data point and network are very big you might be able to get a little speedup, by doing the gradient calculations for different examples on different machines and averaging them. I don't think there are many situations where this will give much speedup because you still have to send the gradients to all machines to sync up the networks. I don't have experience with parallel computing in julia, but I it'll be pretty involved to get working. What's much easier (almost trivial) to parallelize is the hyperparameter search. To find good hyperparameter settings you will have to train many networks and doing that in parallel can be done easily. I think people often this it by running independent instances of their program with different parameters, so you wouldn't even need the parallelization features of Julia, although that might make your setup somewhat more organized.
Well, those scientists who feel like they can contribute to a young, modern programming language are generally more competent at programming and also in the minority in the scientific community. So it doesn't really follow that just because scientists on average are bad at programming that every scientist who contributes to open source software is also bad at programming. 
I have been having some success using threads over batchs. Basically when you compute your likelihood or loss you can distribute the different examples in your mini-batch. I guess you could do `sum(pmap(loss(...)))` but I don't know if that would play well with auto-differentiation.
I think I understand where you're coming from, but I'm not sure. With regard to the example I gave, are you essentially saying that, for whatever function I want to use that doesn't take ODBC's datetime type, I have to add in a definition for it and/or its abstract parent type? Were I running it through my own function, that'd be no problem. But, at least as a relatively novice programmer, I'm fairly hesitant to modify the source code of one package in order to make the types of another work well with it. Since I didn't build either the type nor the function, it's difficult to assess what the possible subtle ramifications could be of modifying the code.
The idea is basically you just need some common functions to convert your ODBC date time to Unix Datetime which will be provided by the ODBC package and everything should just magically work. 
&gt;The cases when you _do_ get problems, are when you interface with code that has too strict type requirements, which is quite common, especially with new users. Personally, I always try to make code as generic as possible, with the least possible amount of type restrictions. I suppose that's the issue I've been running into, then (stupid XLSX, being all strict like a jerk). If generic types are as powerful and flexible as people say, I'm not sure why anyone would want to define their functions so strictly unless there was only a very narrow set of types that it'd even be conceptually applicable for; yet, I keep managing to find packages with functions that simply don't accept any abstract types for their arguments, even when there's no apparent reason they'd need to be so specific. I need to start using better packages... Lol
cross post to /r/programming ?
done
Yup, and IMHO they pointed that gun right at their toe and blasted their entire damn foot away the instant they settled on indexing from 1 (to allegedly win over Matlab converts).... because most of the useful code currently in existence on this planet indexes from 0 making this integration a major PITA if you are trying to quickly build-up a broader ecosystem around your new language. &gt; it's not a convincing enough reason for Python people to switch. Yup, Python does everything I want it to do in scientific computing and more. The MAJOR selling point for Julia over python is performance. However nobody doing serious scientific computing (i.e. Julia's target audience) is spending substantial clock cycles on the python interpreter anyway. The bulk majority (by cpu-time) of my code is spent on calls into other libraries that do the actual heavy computational lifting (e.g. blas/fft, etc.). The python is just there to quickly set up the problem, wrangle the input data, feed it to the correct lower-level functions, and then analyze the results. So the interpreter speed, GIL, etc. have almost no bearing whatsoever on real-world performance. In the exceedingly rare case that I do actually need to write my own custom high-performance bit I can just pop into C++ (where again, the major advantage over Julia is that decades worth of everything else on this planet are already compatible with C++), and then EASILY bind it back via SWIG/cffi/Cython/Boost.python, whatevs. ------------- The performance advantage is overplayed versus the reality in the real-world and IMHO there is little to no upside to Julia in scientific computing. However there IS a huge upside to remaining in python. The ubiquity and comprehensiveness of the rest of the python ecosystem! If I need some database library up front to wrangle the data, there are tons for python. If I need a module that writes my results to a powerpoint file on the other end, python has me covered, etc. etc.
I think that's where the author wants to make a point. Why have to use 2 languages to do stuff (for eg. python for scientific computing and c++ for low-level performance-oriented stuff?). Python is definitely ahead of the competition in terms of adoption and libraries, but I still think that Julia takes the cake when it comes to performance in general (compared to Python at least) and it's relatively new so I'll give it the benefit of the doubt that it could become the de facto high-performance language within the next decade provided dev continues the way it is today. On a side note, I like to read blog posts like this that compare performance: [https://modelingguru.nasa.gov/docs/DOC-2676](https://modelingguru.nasa.gov/docs/DOC-2676)
&gt; Python does everything I want it to do in scientific computing and more. That's not true for me. Are there adaptive solvers for stiff SDEs and DDEs? Are they differentiable programs which are autodiffable? That's just the first think I need. There weren't any libraries for this kind of stuff (other than RADAR5 for stiff DDEs if you want to directly use Fortran) when I started using Julia, and they still don't tend to exist outside of Julia. Maple has had some improvements though in terms of DDEs but not SDEs. Python? Not even close. It's differential equation ecosystem is pretty bare even in comparison to MATLAB. The other pillars of scientific computing are optimization and linear algebra, and you can say some very similar things about block-banded matrix support in Julia vs Python, and MINLP solvers. Python's package ecosystem has done great in data science, but it has only done okay in scientific computing. &gt;In the exceedingly rare case that I do actually need to write my own custom high-performance bit I can just pop into C++ It's not rare for package developers. I have mentioned before that the main advantage of Julia is for people writing packages, not necessarily the people using packages. Though for people using packages, generic algorithms and type handling has many advantages but those require more in-depth knowledge of Julia to fully utilize. For example, it's not possible to use SciPy routines on a GPU without extra copying, but you sure can with many Julia packages.
I had a case recently involving relatively simple statistics where Python was about 1000x slower than Julia (even with numba). I don't think people realize how limiting the language can sometimes be. Of course you can go on the hunt for the perfect library for your use can and if you are lucky learn how to use it, but in Julia you just write down your problem using basic, generic building blocks that you already know well, and you are done.
&gt; Why have to use 2 languages to do stuff (for eg. python for scientific computing and c++ for low-level performance-oriented stuff?) Because there is no holy grail here. Abstractions are cognitively useful for humans, and abstractions intrinsically always have a performance cost. So anything that gives you the flexibility to optimize performance on bare metal AND compatible enough to call into existing software is going to be a major PITA to program in on a daily basis (think C, CUDA, C++, etc). Anything that you want to live with 24/7 in a REPL or write a quick script is going to lack the verbose bits you really need to write high performance low-level code. While you can certainly shoehorn those two disjoint modalities into "one" language, the bits of that language you need to write an arbitrary fast parallel reduction from scratch or call directly into an Intel MKL function will look *very* different from the high-level abstractions you actually want to use every day to analyze/wrangle data, etc. Again, Julia doesn't just have to beat pure python. It has to beat Python + all of the existing highly optimized low-level libraries people in the scientific computing field actually use for all of the heavy computational lifting along with all of the easy-to-use abstractions in python for those libraries. So nobody in the HPC field actually cares about the GIL or the fact that a pure-python fibonacci function is slow. &gt; could become the de facto high-performance language within the next decade Not a snowballs chance in hell. It's a chicken and egg problem. Julia has to be SUBSTANTIALLY better than existing solutions (e.g. Python + C/C++ + Fortran &amp; existing numerical libraries) in order to compel any real investment in moving to it. Everyone I know in the field of scientific computing was SUPER psyched about python a decade ago because (as an entire ecosystem) it had some EXTREMELY compelling features over existing solutions of that time (e.g. IDL, MATLAB, pure C/C++). It was free, cross-platform, high level, had passable plotting, array/matrix, scientific, I/O, optimization, etc. libraries, interfaced nicely with existing C/C++ libraries/code, had a REPL, was better at OO code than Matlab/IDL, etc. etc. A decade ago, everyone and their dog was moving/binding every piece of scientific code over to python like gangbusters.... and this was not only limited to scientific computing. I also get to use the database interface library someone in the webdev community developed for python a decade+ ago, etc. I've talked to plenty of colleagues about Julia in 2018, and the sentiment is always along the lines of "That's nice I suppose... but I don't see the payoff if the only thing it buys me is *maybe* some future ability to write a small performance-critical portion of my code in Julia instead of how I currently do it in C/C++.... oh, and I also have to give up a bunch of functionality in the meantime because I use sqlalchemy or pptx, or opencv, or tensorflow, or whatevs, AND also re-adjust my brain and code to 1-based indexing but only for the part of the time when I'm not interfacing with C / python, etc."
I see your point and am inlcined to agree with your views on this issue. But having Julia as part of one's skills stack won't hurt.
I'll have to look more into doing things like that and setting things up for code to be portable between coworkers. As much as I want to learn Julia and become proficient/competent with it (even as a non-programmer, Julia's potential is patently obvious), at times all of the concepts and practices I need to know seem a bit overwhelming/daunting from the perspective of someone that's used to having all the functions/utilities they need practically handed to them with languages like R, SQL, VBA, Python, etc. I imagine a significant portion of that is likely just due to it's relative infancy and smaller following, though.
Indexing from 1 may be annoying but most code shouldn't be using indexing anyway.
It’s a learning process. Most people don’t write nice generic code on their first try and there’s nothing wrong with that. Depending on your field, many utilities will just be handed to you in Julia though, it’s very much a batteries included language in many domains. Though of course, due it it’s infancy it’s not hard to find a domain where Julia has little support. The nice thing though is that the barrier of entry to roll your own tools is vastly lower in Julia. 
\&gt; because most of the useful code currently in existence on this planet indexes from 0 making integration with Julia a major PITA if you are trying to quickly build-up a broader ecosystem around your new language. &amp;#x200B; really? Have you ever actually tried this? It's not that hard.
Things are changing. Ecosystem is moving to containers with server-side hooks that abstract all of the interop concerns away. You'll write a function in a single language and have it deployed by an abstract scheduler. &amp;#x200B; Tensorflow is terrible, and second mover is going to win this. Been playing around with Flux, and it's a joy. Watch this space.
IMHO, I just think your points can be both valid and humble if you chose to offer it as such. Its an informed opinion based on your experience, not right not wrong. 
&gt; really? Have you ever actually tried this? It's not that hard. Yeah everyone I know who has been doing this has already gone from 1 to 0 to 1 to 0 indexing multiple times during their career (e.g. Fortran -&gt; C/C++/IDL -&gt; Matlab -&gt; Python, etc.). Conceptually it's simple, but in practice it is always an absolute nightmare, esp since being off by 1 can be simultaneously subtle in certain situations, while outright crashing your software by overflowing an allocation in others. The WORST is when you have to keep both conventions in your mind simultaneously (e.g. I'm porting this stuff over to Julia... So for the next year parts of my software stack is going to be in C/C++/Python and some of it in Julia, and every time I toss a vector or array over the fence I have to think about it for a few seconds and then sometimes still get it wrong)
&gt; IMHO, I just think your points can be both valid and humble if you chose to offer it as such I do not. I humbly apologize for any discomfort this decision may cause you.
For 1. - Go in the folder you want to create your package - `]generate MyPackage` - `]dev MyPackage` (you can also put a full path or a git address here) - `using MyPackage` The `dev` command adds the package to your current environment (should be the default) so you need to do that only once. For 2. you can use `using Pkg; Pkg.activate("package")` (most commands are in Pgk), or use the `pkg` String literal : pkg"activate package". Most of this stuff is in the doc, but there's a lot to take in, so it's a bit confusing.
It's probably also worth explaining that DL matrices, big as they are, are still "small" compared to really big matrix multiplication which a lot of scientific stuff does. &amp;#x200B; One could split up DL inference across nodes, by layer, fairly painlessly. However, for backpropagation training, you generally have to carry the matrices with you, and calculating the gradients needs to wait for full forward and backward pass. &amp;#x200B; There are penalties to distributing minibatches and map/reducing their gradient update into a full batch, as in, convergence success is less guaranteed and convergence speed takes a cap to the knee, but if your data size is big enough, it can be worth it. Distributed tensorflow does this "automagically" - as in, when it works. If Flux could do this transparently that would be really cool. &amp;#x200B; Finally, there is this thing where you can potentially assign single nodes to self-train on layers, with periodic gradient updates. This dramatically reduces the communication time, but there is some penalty. If I had time, I would work on getting this to work with flux, since it seems like it would play very nicely with julia's parallelization.
&gt; Abstractions are cognitively useful for humans, and abstractions intrinsically always have a performance cost. Not necessarily. Zero runtime cost abstracts exist in Julia and many other languages like C++, Haskell, etc. It's all about code generation. Sure, when you look at programming through the lens of Python there are no zero runtime cost abstractions, but that's why we're moving beyond Python.
&gt; relatively simple statistics I'd be very surprised if numpy or scipy didn't have what you were looking for. These are not obscure libraries. Julia itself is a less popular tool than either of these libraries
Thanks! It's also all so different from v0.x too
I've written C libraries for julia twice and not once did I run into an off-by-one error. If anything I have a worse problem because python's range() syntax is terrible...
I do a lot of scientific computing related to material science. I did not understand most of the discussion there. So whatever the advantage of using Julia are, these does not concern me. At all. &amp;#x200B; For me, python does do any heavy computing or anything. It helps me doing stuff faster than I used to do without using python. Also the barrier for doing stuff related to my domain is extremely low when using python. For people like me, no matter how fascinating the language is from low level computing stuff, doesn't give us any compelling reason to start using Julia.
You can index from 0 in Julia if you prefer. You can index from anything in Julia. It handles it for you automatically.
&gt; indexing from 1 Kill. Me. Now.
&gt; abstractions intrinsically always have a performance cost Not in Julia. There are libraries using tiers and tiers of abstraction that get compiled down to efficient code with zero overhead. Julia also had decent support for lower level programming constructs: https://github.com/RelationalAI-oss/Blobs.jl So Julia really can do high level and low level stuff. Then there are the libraries. Sure, there is little advantage for very basic stats and machine learning tasks, But Julia is on the cusp of having a huge competitive advantage in other parts of the data science pipeline and more advanced modeling: Data wrangling: Query.jl Is expressive like dplyr and generic and fast in a way that pandas cannot ever hope to be. Modeling: Differentiable programming with Flux.jl, probabilistic programming with turing.jl and the upcoming cassette based 'autgrad all of julia' is going to far surpass anything python can do, in both usability and power. This will be particularly salient as ML moves away from simple dataflow constructs. Plus, using tables.jl you can process any table like source with and generic transforms with the formula interface. See https://julialang.org/blog/2017/12/ml&amp;pl Deployment: Small binaries and web assembly compilation is on the roadmap https://github.com/MikeInnes/Charlotte.jl https://github.com/JuliaComputing/static-julia Front end: Julia's generic programming, metaprogramming and future Web assembly will obviate the web two language problem See this as a start: https://github.com/JuliaGizmos I see a fairly large leap over python given all of the above. 
or you can make height(s::Square) = s.width width(s::Square) = s.width height(p::Poly) = p.height width(p::Poly) = p.width commands equals becomes equals(p::Poly, p2::Poly) = height(p1) == height(p2) &amp;&amp; width(p1) == width(p2) &amp;#x200B; The problem is you want to use only width in square, but without having a way which you can use width and height in a square which works.
Group photo is Julia 6.0, what?
Actually have degrees in physics, mathematics, and computer science. Most important bit is to be consistent (even if you made the wrong choice). Second most important bit is to speak the language of the thing you are programming. If you are programming a computer, then there are a LOT of good reasons to start indexing at 0 (esp. if you are programming in a lower-level language), which is why all of the good numerical libraries post-fortran are written in languages that are 0-indexed. If you want to interact with those libraries in any meaningful, non-abstract capacity you are going go nuts having to look at a chunk of memory, or a loop, or whatever where the index shifts by one depending on which side of the fence you are currently looking at it from. 
Julia Taiwan [https://www.facebook.com/profile.php?id=1787971081482186&amp;ref=br\_rs](https://www.facebook.com/profile.php?id=1787971081482186&amp;ref=br_rs)
The question isn't really about solving the equals problem, it's about the exploding combinatorics of having to specify every : function foo(a::Subclass, b::Superclass) and function(b::Superclass, a::Subclass) in the Java example you end up with every subclass having a method for every other subclass (which is the cartesian product). In Julia you have a function for every pair - the same cartesian product 
Somebody correct me if I'm wrong, but even in mathematics, natural numbers are also debated. Some say they start from 1, some say they start from 0. &amp;#x200B; For example, you take that they start from 0 as fact, but I was taught in school that natural numbers start from 1 and it is the set of whole numbers that starts from 0.
&gt;Ok I'll give you it's a little more contentious in math than I originally let on On the contrary it's not contentious at all. Literally everyone in mathematics agrees that the natural numbers start at 0. &gt;mostly because who starts labeling the variables at 0? Natural numbers have nothing to do with labelling variables. You can label variables starting with 1 while still regarding the natural numbers as starting with 0. &gt;More generally, I've found those who do analysis to prefer this distinction; whereas, set theorists prefer to identify the natural numbers with the non-negative integers. Discrete mathematicians just seem to be all over the place with almost no agreeance on the matter. The natural numbers start at zero by definition. People probably use 'natural number' to mean 'positive integer' all over the place, through many many papers, but they're being technically imprecise.
[You](https://en.wikipedia.org/wiki/Natural_number) [don't](http://mathworld.wolfram.com/NaturalNumber.html) [know](http://www.chaos.org.uk/~eddy/math/found/natural.html) [you're](https://math.stackexchange.com/questions/283/is-0-a-natural-number) [talking](https://www.merriam-webster.com/dictionary/natural%20numbers) [about.](https://www.math.brown.edu/~jhs/frintch1ch6.pdf) Please stop telling me about my field that you obviously do not work in. 
**Natural number** In mathematics, the natural numbers are those used for counting (as in "there are six coins on the table") and ordering (as in "this is the third largest city in the country"). In common language, words used for counting are "cardinal numbers" and words used for ordering are "ordinal numbers". Some definitions, including the standard ISO 80000-2, begin the natural numbers with 0, corresponding to the non-negative integers 0, 1, 2, 3, …, whereas others start with 1, corresponding to the positive integers 1, 2, 3, …. Texts that exclude zero from the natural numbers sometimes refer to the natural numbers together with zero as the whole numbers, but in other writings, that term is used instead for the integers (including negative integers).The natural numbers are the basis from which many other number sets may be built by extension: the integers, by including (if not yet in) the neutral element 0 and an additive inverse (−n) for each nonzero natural number n; the rational numbers, by including a multiplicative inverse (1/n) for each nonzero integer n (and also the product of these inverses by integers); the real numbers by including with the rationals the limits of (converging) Cauchy sequences of rationals; the complex numbers, by including with the real numbers the unresolved square root of minus one (and also the sums and products thereof); and so on. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/Julia/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
&gt; Because there is no holy grail here. Abstractions are cognitively useful for humans, and abstractions intrinsically always have a performance cost. This is pretty much where your entire argument falls apart. One of the great things about Julia is that you _do_ get super-lightweight (or even totally free) abstractions, so you can work in a high-level abstract style and also get good performance. There seems to be widespread misunderstanding that for you need to drop down to 'C-like' programming style for performance in Julia, but that simply isn't true. This misunderstanding might be related to the common advice that you don't need to avoid loops to get performance in Julia. Occasionally, Julia code looks a bit low-level, but that's just because you're doing custom operations where no abstractions have been implemented, it's not because those abstractions wouldn't be possible or performant. &gt; So anything that gives you the flexibility to optimize every last bit of performance on bare metal AND compatible enough to call into existing software is going to be a major PITA to program in on a daily basis (think C, CUDA, C++, etc). Anything that you want to live with 24/7 in a REPL or use to write a quick script when up against a deadline is going to lack the verbose bits you really need to write high performance low-level code. I believe you have dismissed the possibility of such a language prematurely.
Wikipedia is, as per usual, wrong. Wikipedia is like the newspaper: you read it and think it seems very interesting, then you read something you actually already knew about and it's inaccurate nonsense, then you go to the next article you didn't already know about and think it's interesting again. It superficially looks accurate because it has references, but it isn't. Wolfram Mathworld is almost as crap as the stuff Wolfram himself writes. etc. etc. It's wrong. There's a meme that natural numbers are ambiguous and could be either. It's wrong. Every modern text book on mathematics is quite clear that the natural numbers start at zero. No discussion, no ambiguity. The natural numbers start at zero, and that's that. Calling it 'your field' and claiming I know nothing about it, then citing fucking Merriam-Webster and MathUnderflow as sources, that's a yikes from me dawg. I have a mathematics degree. I am doing a PhD in pure mathematics. I know what a fucking natural number is. And zero is one. Always.
&gt;People in scientific computing tend to be the worst at software engineering from my past experience. Software engineering doesn't exist. There's no generally accepted definition of what constitutes good engineering practice in the software 'industry'. &gt;They are so focused on solving technically challenging problems that they completely lose sight of the user experience for ordinary users. The idea that solving technically challenging problem is less important to good software engineering than 'ordinary user experience' is ludicrous.
Please tell me about your thesis. 
Why would I tell some random internet stranger the details of my thesis? 
The 1- vs 0-based indexing fundamentalism has me rolling my eyes hard. It's a superficial discussion about an unimportant design choice, that lets people express strong opinions without needing any deep understanding (bikeshedding, basically.) It's a lazy and weak argument. 90% of the time 1-based indexing is irrelevant (because you just use `for x in y`, or `for i in eachindex(y)`) or marginally better, since it is more natural and intuitive to start counting things at 1; but the advantage is slight, and the adjustment to 0-based is easy. Then, there's a mix of cases when 1- or 0-based has a larger advantage over the other, and the ratio of the mixture depends on your domain. I've experienced cases where 0-based indexing allowed me to create elegant and simple code, and I've had cases where it was a major bug-causing annoyance. The extreme focus on and dogmatic pronouncements upon the advantages of one type of indexing over the other is, frankly, embarrassing, and more than anything shows that the persons screaming about it is either unwilling or unable to inform themselves about the more important features of the language.
Could you be more specific? What was it that you were doing? Do you have sample Julia and Python code?
Thank you for the information ! How do you publish your changes after that? Should I still use PkgDev/PR to Metadata.jl, or did it change ?
I don't have samples on hand but I was basically evaluating the pmf of a Multinomial in a loop (iterative algorithm).
&gt;I have a mathematics degree. I am doing a PhD in pure mathematics /r/thatHappened
Awesome job! That's pretty damn cool!!
Oh piss off you ignorant little shit.
May I ask, what are your preferred packages for DB connection and Excel output? I've foubd ODBC and XLSX packages to not work too well together 
I wrote a wrapper for XlsxWriter - https://github.com/lawless-m/XlsxWriter.jl (Sorry, I haven't pushed my updates for 0.7 onwards to it (the TLS1.1 thing prevents me using Github at work on my locked down terminal)) - I have plans to make it a proper module now 1.0.0 is out. I just got a new terminal with 8Gb RAM (I was on 4!) so I'm going to get a VM running so I can use Linux ..... life in the Fortune500 For SQL Server I use the HTTP connection which uses XML. We use the Red Prarie Warehouse management System so I'm note sure if that works with vanilla SQL-Server. https://github.com/lawless-m/PickingLogic/blob/master/RPClient.jl 
I get that, but, one of the ways around the combinations is making sure you have the methods you would use to make things similar. &amp;#x200B; This is just one of the ways of handling it.
Agreed.
I'm looking at my post right now and I haven't said "I think that ME is a valid resource". I just said that in two different environments, I have observed the same thing, i.e. inconsistency.
The people who think 0 should not be included are idiots. That should resolve the confusion. 
Have you tried uninstalling/reinstalling?
I tryed rm and remove. Then add in both cases but it didn't help.
Have you tried uninstalling and reinstalling *Julia*?
Fresh install for my girlfriend and I uninstalled it and reinstalled it again. &amp;#x200B;
&amp;#x200B;
# Building IJulia [https://photos.app.goo.gl/qobRJMsJuJt3RDX47](https://photos.app.goo.gl/qobRJMsJuJt3RDX47) This is the error I get while building IJulia
I messed with this for 2 nights. [This](https://github.com/JuliaLang/IJulia.jl/issues/739#issuecomment-419605998) finally worked for me.
That's also working for me. Thank you :) using Conda # you may need to "add Conda" at the pkg prompt first rm(Conda.ROOTENV, recursive=true) # remove your existing Conda installation ENV\["CONDA\_JL\_VERSION"\]="2" Pkg.build("IJulia") # re-install jupyter
You have a typo. \`[Pkg.build](https://Pkg.build)("IJUlia")\` won't work because the package manager is case-sensitive. Try \`Pkg.build("IJulia")\`.
Good eyes! 
I think they (we) could definitely do with some more Windows-based testers. Come and join in the bug hunt!
Isn't this a problem where type promotion is of interest? I.e., to compare two objects they need to be the same, so take your approach of "strictly enforce commutativity between the same type" and then include a catch all that does type promotion to a common type. This is most obviously seen in the arithmetic of Julia; what do you do when you have \`1 + 1.0\`? \`\`\` julia&gt; @which 1 + 1.0 \+(x::Number, y::Number) in Base at promotion.jl:313 &amp;#x200B; julia&gt; @which 1.0 + 1.0 \+(x::Float64, y::Float64) in Base at float.jl:395 \`\`\` If you look at the underlying function in promotion.jl it is implemented as \`\`\` \+(x::Number, y::Number) = +(promote(x,y)...) \`\`\` Not sure if this helps you but this seems quite a reasonable pattern to me. (Though if I remember correctly, I've seen criticism from the core developers about the implementation of the promotion system.)
You might need an explicit call to `Revise.revise()` in your code. 
As soon as Parquet.jl hits 1.0 compatibility, I am *so* keen to rewrite my Keras + Tensorflow python project for work with pure Julia + Flux.
though not a direct solution, you can use Julia almost anywhere you have a web browser using https://juliabox.com/
Thank you. I know but sometimes I don't have internet but still want to work.
It's not just windows. I think the 0.7 release with depreciations and shortly after the 1.0.0 release fucked the entire ecosystem. A newcomer will likely download 1.0 and then see that most of the packages don't work. So downgrade to 0.7 and get spammed with depreciation warnings. Currently the whole ecosystem is a mess. And that is sad, because I do think that there is still a lot of potential for the language. And it doesn't help that pkg.julialang.org is stuck on version 0.6.3
Well, you're not wrong, although a month can make a lot of difference. I think the core devs took a calculated risk, weighing up the benefits of a stable core language, new package manager, and a newsworthy release against an ecosystem in flux. Perhaps it will work out OK, or perhaps some users will be put off for the rest of their lives, which might be shame... :]
https://github.com/chakravala/Fatou.jl - works on 0.7, so there's a good change it works on 1.0. 
https://stackoverflow.com/questions/21152051/difference-between-nx1-array-and-n-element-array-in-julia/21152587#21152587 should help.
i don't know which version you are using, but 1.0 definitely does not give you these values. i get a one dimensional array for both: julia&gt; typeof([1 2; 3 4][1, :]) Array{Int64,1} julia&gt; typeof([1 2; 3 4][:, 1]) Array{Int64,1} that is, these are not 1x3 or 3x1, but rather only one dimensional, size 3. you can get matrices if you want: julia&gt; [1 2; 3 4][:, 1:1] 2×1 Array{Int64,2}: 1 3 julia&gt; [1 2; 3 4][1:1, :] 1×2 Array{Int64,2}: 1 2 
I unfortunately have to agree with this sentiment. I have just learnt 0.6.3 about six months ago and I was kind of excited when Julia 1.0.0 was released, but after couple of uses, I feel like random stuff gets broken constantly. Also, since there is very small community, I feel like not much help is available. I love this language and love how fast it is compared to Python or Matlab, but compatibility is definitely an issue for me currently. I hope it gets better soon. My biggest worry is that this will be happen at least few more times. Stuff gets upgraded, other stuff breaks down, panic for a bit and wait for smart people to figure it out. I feel like this is just going to alienate the casual user base like myself. 
If having to think about indexing is the hardest part of your job, clearly you need a more challenging job.
You could replace Python and Julia in your comment with any other two languages popular in science (Fortran, Matlab,...) and there probably is a scientist somewhere who would say exactly that.
You are correct. I interpreted it wrong. It returns one dimensional arrays. I may have to reconsider my way of thinking in Julia, as pointed out by Nuaua, Julia does not differentiate between a column vector and a row vector. Maybe I can use your suggestion to use a range to get a 2 dimensional array.
I guess the main issue is that I differentiate between row vectors and column vectors and Julia does not. However searching a bit more it seems the LinearAlgebra.Transpose type mimics the behavior of a row vector.
&gt; Julia does not differentiate between a column vector and a row vector actually it does now :) there is a dedicated row vector type which is essentially a vector, but it remembers that it represents a row, so it behaves accordingly
there is some black magic called lazy adjoint: julia&gt; [1,2]' 1×2 LinearAlgebra.Adjoint{Int64,Array{Int64,1}}: 1 2 julia&gt; [1,2] * [1,2]' 2×2 Array{Int64,2}: 1 2 2 4 julia&gt; [1,2]' * [1,2] 5 
currently, Fatou does not work on 1.0 due to a compatibility issue with PyPlot. I'm the developer of Fatou, I did not have time recently to resolve this issue and nobody has complained about it up until now; however in the near future i intend to resolve the issue that is preventing Fatou from functioning in 1.0 see here for discussion: [https://discourse.julialang.org/t/mysterious-stackoverflow-error-with-type-constructor-when-using-pyplot-in-conjunction-with-reduce/14050](https://discourse.julialang.org/t/mysterious-stackoverflow-error-with-type-constructor-when-using-pyplot-in-conjunction-with-reduce/14050/9)
Maintainability, ease of modification for end-users, and (depending on some factors) performance.
Thanks a lot
When your whole software stack is in native Julia, not only is it fast but you get a lot of cool features by utilizing type-specialization and multiple dispatch. You get things like pervasive autodifferentaition and the ability for users to take control of internal calculuations via array types (i.e. auto-recompilation of internals to the GPU via GPUArrays). A full story is written out here: http://www.stochasticlifestyle.com/why-numba-and-cython-are-not-substitutes-for-julia/
The good think with Julia is that it can be used for scripting (which you describe) but at the same time have the performance of C/C++!! This may be a combo that you don't need, but Julia will substitute Python in a few years. And by "few" I mean like no less than 5 years and no more than 15. So, for people getting now into scientific computing (Master students and PhD candidates), Julia is something that if they learn now it will advance their careers in the future. Otherwise, they will be required to learn Julia in the future and they won't be early adopters of the language. That said, I did say 5-15 years and each scientific disciple may adopt Julia at different times. I am a PhD candidate and a bioinformatician and my main area of study/expertise is Molecular Dynamics, which I use for drug design. As far as I know, Molecular Dynamics are being utilized by material scientists as well. So, regarding material science and Julia in particular. [OpenMM is a MD algorithm, or more accurately package](https://journals.plos.org/ploscompbiol/article/file?id=10.1371/journal.pcbi.1005659&amp;type=printable), that utilized Python as well as C/C++, and is a project that could be benefited by a programming language like Julia. And speaking of bioinformatics, at the moment two programming languages are being utilized at bioinformatics; Python and R. Python is used for its scripting abilities and mostly by us structural bioinformaticians and R for statistics and mostly by those working with biostatistics, with micro-arrays (that are dependant at biostatistics) and at the genomic level (that also utilize....biostatistics). Julia's profile will make it, in 5-15 years, the one and only bioinformatician language.
Overhead and optimisation potential are the 2 big ones that spring to mind. The ability to do things in Julia that can’t feasibly (or at all) be done in Python (check out Jump, Flux and the symbol manipulation libraries for some great examples) are another very big, very important reason.
JuliaBox gives you the solutions to each exercise in that same folder.
No it doesn't for the introductory lessons 1-12. What you are referring to is that for the "calculate_pi.ipynb" it does provide "calculate_pi_solution.ipynb". But I am not there yet. I can't even data structures!
Using PyCall to access XlxsWriter was 4x slower for me than using python directly. Its only creating XML.
I've done this course https://www.coursera.org/learn/julia-programming 
This isn't like an interactive webpage or anything, but I found that (atleast for me) using projecteuler.net to get better at a programming language works great. It's just a page where they have hundreds of actually difficult tasks. The tasks also teach you how to optimize your code really well. tl;dr Try doing the tasks on projecteuler.net 
Okay, so as a recent Julia learner, I agree, the tutorials on juliabox are not really meant to teach one to code. At best, they're aimed at coders comfortable in some other language. While it's not a course per se, there's an open source book called How to a think Like a Computer Scientist that is excellent, and may be just the ticket for you. (It was the first programming book that I read that actually made any sense, for what it's worth.) It's free, very easy to read and follow, and has been ported to many programming languages, including Python and, very recently, Julia. For Python, there's an interactive website where you can read the book, perform the exercises, and get results all in the same page. Check it out at http://interactivepython.org/runestone/static/thinkcspy/index.html. It's based on Python 3, BTW. The Julia version is not interactive, but there basic lessons are all the same, and it does seem to be based on Julia 1.0, though I haven't gone through much of it yet. You can find it at https://benlauwens.github.io/ThinkJulia.jl/latest/book.html. If you want to find out more about the book or see the full list of languages its been ported to, you can check it out at https://greenteapress.com/wp/. 
What I don't like about coursera is that it requires "commitment". At the moment my schedule is very fluid and uncertain and I can't commit like that.
The best interactive learning is to pick a project you like, join the Slack channel, and ask people when you have questions. Honestly nothing beats a chat channel.
What helped me the most was to follow the tutorial notebooks of packages i wanted to use (provided they were high-level enough). [DifferentialEquations.jl](http://docs.juliadiffeq.org/latest/tutorials/ode_example.html) and [DynamicalSystems.jl](https://github.com/JuliaDynamics/Tutorials-and-Resources/tree/master/Youtube_JuliaLang_tutorial) have great tutorials. This method helps me learn how Julia works outside of the package (since most of your coding will be in standard julia anyway), while still working on something I find interesting and which may be relevant to my research.
Exercise\_solution.ipynb
I am literally looking right at the solutions... https://github.com/JuliaComputing/JuliaBoxTutorials/blob/master/introductory-tutorials/intro-to-julia/Exercise_solutions.ipynb
&gt; I also hate the stale method of reading a book. You need to read better books or adjust your thinking. Is this how you approached learning biology, too? Judging by your remarks on your past formal education in programming, you never learned the fundamentals. Languages are trivial to learn once you understand computation (with a handful of exceptions which require unusual concepts). Forget Julia for now — it’s a great tool, but you won’t get a lot out of it. Learn the basics with a simpler language. I highly recommend Structure and Interpretation of Computer Programs. There’s a printed book, a free web book, and video lectures. They’re brilliant. It’ll take you several weeks but will give you a foundation of computing knowledge for life. If you get stuck early, go through How to Design Programs, then return to SICP. When you finish, you’ll be able to pick up Julia just by skimming the reference and you’ll have a much better understanding of what it offers and why. I’ve spent the last few weeks working with bioinformatics code, and can confidently say that most scientists writing that code are terrible programmers with bad taste. You have a chance to break out of that mold and actually wrote _good_ scientific code. Don’t blow it by chasing some fancy learning UX.
If you are starting new then start with Julia. It's the best and fast. But if you want neural network and machine learning packages then python. I have a hand made beginners note made for Julia. If you decide to learn Julia then let me know, I will share it with you. And if you need any help in Julia, don't hesitate to ask me. 
Biology is different from coding. Coding is a skill and no book will teach you a skill. The same goes for any skill, including those associated with biology. So, no matter how much you read about it, you can't get any better at cell culture with books, you have to actually do it. As for "the basics", I think I learned the basics when I was being taught C, which if you notice I separated from the rest. That said, I will look into the book you suggest in the future. Lastly, I don't intend at writing an algorithm. There two kind of people; a) those who make the bioinformatic code and b) those who use it. It is hotly debated if both should be called "bioinformaticians". At EU Academia we both are considered bioinformaticians but at the US industry only the first group is considered a "bioinformatician", the others (we, who use the algorithm) are considered "computational biologists" I do not intend at becoming a "bioinformatician", I intend at staying a "computational biology". So, you will never have to deal with my messy code. As a computational biologist though, I am required at maintaining your code, adjust it to my needs (if required), create scripts that will automate things and will link different algorithms of the same pipeline together.
I use all all three of them — Python, R &amp; Julia. All of them have advantages and disadvantages in different areas. Python is extremely versatile and more of a general-purpose programming language than R and Julia. It has a massive ecosystem and its qualities extend far beyond scientific computing. I think R as a language is horrible but it is really good for doing some quick &amp; dirty proof of concept works. I use it mostly as a playground for something that will later evolve into something bigger. What I like about R's libraries is that they are mostly created by researches that are experts in corresponding fields. Documentation is often really good and instead of just examples, it contains overview of used methods along with references to relevant research articles. Julia is "new" (it appeared in 2012), but it is still gaining traction. As a language, it is very well designed and offers some novel ideas. It has some state of the art libraries, but in my experience, documentation is often lacking which is understandable since it is a new language. There is no wrong choice here. I would recommend you talk with colleagues/mentors as I find this to be extremely important. You want to "be in sync" with other people in your field. If programming language lacks good libraries relevant to your field, this might be a huge factor. However, just because you choose one language as a starting point, this isn't a final decision. They have a lot of things in common (especially Python &amp; Julia), so it's not like transition is going to be a huge one.
/u/keepitsalty, /u/dark_frog fair enough, but still.... First of all, exercise 2.2 asks to &gt; store the results in `c` and `d` respectively but the answer creates no "c" or "d" variables. Then at exercise 2.1, after copy/pasting the correct answer (which was my answer minus the variable) I still get the following error message!! &gt; The source of the following cell has changed, but it should not have! JuliaBox would have been so great if better maintained! I suppose I will try tomorrow Julia 0.6.2 instead of Julia 1.0, to see if Julia 0.6.2 is any better. By the way, what are their differences?
&gt; Coding is a skill and no book will teach you a skill. Nonsense. If your assertion were true, then lost skills, documented in books, would not be rediscovered and relearned (crafts, arts, cooking techniques, and HEMA all come to mind). Skills _always_ involve theory and practice. Books provide theory. Good books also provide guidelines for practice: they're called exercises [*]. You don't need an online verifier to see if you solved a programming exercise correctly, it should be reasonably obvious. And if it's not obvious, then you need to work harder. Real-life problems do not come with on-line correctness verifiers, you'll have to do the work of figuring out if you got it right or not. Just by reading your comments here, I guarantee that you did not learn the basics when you learned C. Knowing conditionals and loops and their syntax is not even remotely what I meant. Even the best C book available (K&amp;R) does _nothing_ to teach code modularization, a sense of proper levels of abstraction, algorithms and runtime analysis, elegant techniques, good taste, or anything else required to write non-toy programs. Since no one can predict the future, it seems likely that your notions about how and where your code will be used will not survive their first encounter with reality. The first person who needs to maintain it is _you_, and the better you are at it, the less you'll have to struggle (1) with your own mistakes, and (2) remembering what you did six months ago. If it makes you feel any better, the computation skills I'm talking about are way easier to pick up than the biology you already know. You don't need a degree in "computer science". But if you will be writing _any_ code for a living, then you do need to dedicate some time to learning the right set of basics. [*] I shouldn't need to note this, but just in case: not every skill can be learned by every person _only_ from a book. Some things are poorly documented in books. Some things are difficult enough that live feedback from an instructor is essential or at least speeds the learning process. Learning to speak a foreign language only from a book is impossible for nearly anyone — though understanding that language's grammar is hella helpful. Learning to program is absolutely _not_ in any of these categories.
good to know. Thanks.
cool, thanks. I am feeling motivated to make some native packages now. 
thanks for the link, was helpful.
If you are new to both statistics and programming, I would strongly suggest sticking to something well-established - I would suggest R. You will probably find all you need in some existing R packages. Julia libraries are still in non-existent or in a development/experimental stage. Python is a good option as well, but for statistics R is, IMHO, still much better.
Here's a sneak peek of /r/statistics using the [top posts](https://np.reddit.com/r/statistics/top/?sort=top&amp;t=year) of the year! \#1: [The trump problem](https://i.redd.it/kqnvc5wcrh101.jpg) | [34 comments](https://np.reddit.com/r/statistics/comments/7h23he/the_trump_problem/) \#2: [Bing results. Where stats and fashion meet.](https://i.redd.it/xzd3c5ka9myz.jpg) | [4 comments](https://np.reddit.com/r/statistics/comments/7dopht/bing_results_where_stats_and_fashion_meet/) \#3: [The scariest Halloween monster of them all!](https://i.imgur.com/oZYJSIU_d.jpg?maxwidth=640&amp;shape=thumb&amp;fidelity=medium) | [29 comments](https://np.reddit.com/r/statistics/comments/7aqv39/the_scariest_halloween_monster_of_them_all/) ---- ^^I'm ^^a ^^bot, ^^beep ^^boop ^^| ^^Downvote ^^to ^^remove ^^| [^^Contact ^^me](https://www.reddit.com/message/compose/?to=sneakpeekbot) ^^| [^^Info](https://np.reddit.com/r/sneakpeekbot/) ^^| [^^Opt-out](https://np.reddit.com/r/sneakpeekbot/comments/8wfgsm/blacklist/)
Thank you for your input. &gt; R as a language is horrible but it is really good for doing some quick &amp; dirty proof of concept works. This is the general consensus, and why I have been considering Julia as it is claimed to be "less messy". However, I think *quick &amp; dirty* is just what I need. &gt; What I like about R's libraries is that they are mostly created by researches that are experts in corresponding fields. This indeed a crucial point. I am no statistician, and being able to rely that the libraries function as intended is indeed important.
Dude, just download a local copy and work through the exercises and check your work, why do you care that it's all specific? If you use Julia 1.0.0 all the of exercises and workbooks should work. 
What I think I hear you saying is that you don't feel comfortable coding in general. Julia is both relatively young with a small and niche community. Given this it may be a while before something similar to what you are looking for is developed for Julia specifically. It may be more helpful to first spend time learning another language- Python is a good choice - and then spend time translating those skills to Julia. Once you learn one programming language and are comfortable to some degree, learning others is a relatively straightforward process. For example, once you understand control flow, functions, lists, and etc. in Python, you should be able to check for the Julia syntax to do the same things. One thing I would ask is if you can describe why you need Julia specifically. If Python will do for your application it may be more worth your time to use existing resources for learning a language with a larger community and more support behind it currently. 
You won't care about elegance or speed of code if all you are doing is using computer to produce graphs or do descriptive statistics for example, as most of your code will consist of various small blocks of code that will probably be in the following format: 1. Do some statistics 2. Plot results It won't matter if code executes in 1ms or 1ns. However, for someone that is specialized in for example, fluid dynamics, it's not the same if simulation runs in 20 hours or 10 hours. I would recommend you to install RStudio and jump right in. I also like Jupyter Lab with R kernel.
Seems like Base.search has been deprecated in favor of findfirst / findlast. https://github.com/JuliaLang/julia/issues/26145
I think you should look at something like Datacamp or Dataquest to learn python. It’ll cost some money but a few months should be enough to progress without it. As you say, it’s the language used in your community, and once you are comfortable with Python learning Julia will be easy. I’m not sure why you would need Julia, but at your point you should really learn Python. Julia will be there once you’re ready.
You're asking this in /r/Julia, which is likely to give you a biased answer. Maybe one of the statistics subs would be better. What do the people in your department and field tend to use? I'd focus on that. That way people near you will be in a better position to review your work, and the results you produce are more likely to be in line with what people in your field are used to seeing. What does the statistics department at your school like to use? Since it might be a good idea to hire or at least consult with statistical experts as you design your studies. Ultimately, it all comes down to how much time you have and what you want to focus on. Every hour you spend learning one language is an hour you can't spend on another language or on advancing your research.
It's actually a cross-post from r/statistics. Since Julia is relatively new, I figured many people here probably had prior experience with another language and could give some as advice about Julia's strengths and weaknesses compared to R or Python. SPSS seems to dominate my field of research, and is the most used at my university, but I find it too limited for my purposes. I was also advised against it by researchers in my department that has long experience with it. Our university is relatively small and new, so we don't really have a statistics department. My section's statistician is proficient in R, so she could probably be of most help if I stuck with that.
Actually I am trying to learn both languages simultaneously. I already have started learning Python at codecademy and [now I have another option](http://interactivepython.org/runestone/static/thinkcspy/index.html) as well. As for why I need Julia? [Because I am sure that Julia is the bioinformatic programming language of the future and I want to be an early adopter.](https://www.reddit.com/r/Julia/comments/9fcbw0/the_julia_challenge/e668xj2/)
"lost skills" are **rediscovered** by applying them. &gt; Skills always involve theory and practice Skills involve mostly practice and less theory, which is what I described when I mentioned cell culture. &gt; Some things are poorly documented in books. I have find that most programming books are a mess. Not that there aren't any good ones, but most are a total mess. &gt; Learning to speak a foreign language only from a book is impossible for nearly anyone — though understanding that language's grammar is hella helpful. Learning to program is absolutely not in any of these categories. I beg to differ. Programming languages fall in this category. And this is where interactive learning comes in. It adds the level of an "instructor" (the "instructor" is automated), while keeping the self-learning process, which allows someone to learn with his/her pace. 
For some extra incentives, the type system makes it sooooo much easier to keep track of what’s doing what, what everything is returning etc etc. Even when you’re not doing anything super complex, the added clarity is so good. The vectorisation and built in timing utilities make writing cool performant stuff (without needing to do all sorts weird hacks) so much better.
From a comment in the linked thread: &gt;but similar (or even better) can be achieved with the other two, especially python. Good god that's an annoying thing to keep hearing. Yeah, I can get decent performance in Python, if I go and rewrite everything in C and use mad hacks in my Python everywhere. Or I could just write it once in Julia and get top tier performance *from the start*.
String is array of chars no?
Don't learn both at the same time. Pick one and stick with it for a few months. Doing both will only slow things down unnecessarily. And for someone in your position, with barely any programming experience, there is absolutely no reason to be an early adopter. Python and R have an immense amount of learning material for pretty much anything you can think of. Make use of that. It will take years if not decades before Julia replaces R and python. If you really want to learn two languages at the same time pick either python and R and do Julia on the side. But Julia simply lacks the material you seem to need due to its young age. 
Julia's performance seems to be the upside. Another comment claims this isn't really an issue before you have [100M+ rows](https://www.reddit.com/r/statistics/comments/9gvres/which_softwareprogramming_language_for/e67a4z1). I am working with the PISA dataset, where we are talking thousands of rows and hundreds of columns. Would you see a great difference in performance between Julia and R on such a dataset?
I’ve had R choke on a hundred thousand rows and under 50 columns.. As in, grind my computer to a complete halt as it runs out of memory. Scaling up is plausible, but is only ever a bandaid solution. I’ve got a project at work at the moment written in Python, it has to process at least a million lines in ~40 columns, it’s doable but I end up waiting ~30mins (with multithreading-technically multi-processing). That execution time goes down only slightly when I use bigger EC2 instances (currently training models on P2 and P3 sized instances). I’m suspicious about the 100m+ rows claim, if your whole language is faster, you don’t need to wait until you get to that size to see gains. Personally, I’d go straight to Julia because I like the language features and type system better, and I’ve got better things to do than wait for things to complete because my not-built-for-performance-language still hasn’t finished doing things.
T= "123456789" t = SubString(T, 3, 4) t[2:3] "45" 
This seems to solve the problem for custom string indices for any index &gt;= 1, but at the moment, it seems impossible to have a solution with indices &lt;= 0. Plus, I think a solution similar to OffsetArrays.jl would be less hackish than having to SubString each String.
Definitely read this: https://docs.julialang.org/en/v0.6.1/manual/strings/ This will give you a good overview of how strings are represented in Julia.
It's not meant for learning from scratch, but [exercism.io](http://exercism.io/tracks/julia/exercises) has practice problems in Julia and mentors that'll provide feedback on your solutions. You might wanna check those out once you've learned the basics.
&gt; Julia libraries are still in non-existent or in a development/experimental stage. That's a of an exaggeration, Distributions.jl is much better than anything in R in my opinion. R has more domain-specific stuff, but when it comes to basic statistics, I find that Julia is already ahead. 
I depends a lot on what you want to do: if you want to load a table and compute mean and standard deviations almost any language will do. If you want to write down a relatively complex custom model and fit it, R will struggle.
I opened a issue on Github and Tim Holy got back to me. He said I needed to use the following. It totally works. while true Base.invokelatest(hello, "world") Revise.revise() end https://github.com/timholy/Revise.jl/issues/194
What's the file?
I have almost no luck in using CSV in 1.0 I think there might be issues with it.
Same here. The version from a month ago was working fine but all of the new updates cause some issues. I can still read small (&lt;1k rows) csv files with CSV.read, but it blows up if try to import a larger file.
Thank you
I got an upgraded terminal this past week - 8Gb instead of 4gb - woo. I'm not an official developer at work - in fact no-one is. I'm pushing to get my role officialised. So I got Qemu running yesterday and have a Debian install to use. I left it compiling Julia 0.6.4 when I went home. (Debian has 0.4.3 as the packaged Julia). I did push my 0.6.x compatible XlsxWriter code to Github - it might work with 1.0.0, I don't know. I tried installed 0.7 and 1.0.0 at home but some Julia packages fail to install because they haven't been updated. see https://discourse.julialang.org/t/cannot-pkg-build-conda/13192/4 The advice in Discourse is roll back versions. But anyway, I'm on the way to 1.0.0. 
I am from material science as well. I mostly do Density Functional Theory. There absolutely zero chance that Julia can replace use Fortran in the field. Even for the scripting, Python is well established. May be in 5 years, we can talk about it again.
I couldn't get lots of the inbuilt functions to work once I had read in my CSV either. I'm guessing something in the 1.0 update broke it. Hopefully it gets fixed soon as it's rather an important feature for data science.
Just as a heads-up, the Python track gets so many submissions that the mentors usually can't give feedback on side exercises. Not sure how long the core exercises take. The backlog isn't that huge given the amount of mentors, so I'd guess 24-48 hours.
 From: https://github.com/JuliaLang/julia.git It looks like the answer to your question is: push!(LOAD_PATH, "/Path/To/My/Module/") There's additional information in the above referenced page under the heading "Module file paths".
Do you have an example of how you would do this without introducing ambiguity?
I think that you might be right, but there's also the inherent problem that promotion isn't being used to resolve ambiguity.
CSV.jl is kind of a shitshow. Maybe try CSVFiles.jl
I have it running. See [here](https://www.reddit.com/r/Julia/comments/8rci8l/numerical_computing_in_my_pocket/).
CSVFiles seems to work, thanks.
Yea I have the same problem.. I've side stepped this problem by dropping into python and using pandas via @pyimport pandas as pd. 
I mean, he just said it was a pain in the ass. And really, I can see how it could be if you're trying to rewrite a program from one language in another with a different base, especially when, as he said, sometimes a missed correction won't throw an error but rather continue on inaccurately calculating things. A pain in the ass like that can be a big enough hassle to keep people from making the switch, because not only do they have to rewrite everything, but if they accidentally think in the base they're used to for a moment, it can cause massive issues. That said, I'd hope someone would learn and become familiar enough with Julia before trying to port all of their other code so that it really wouldn't be an issue (as base-1 indexing would be the new normal for them).
&gt;I'm a trained mathematician and we have quite different ideas about where the natural numbers start. This just made me cackle out loud in a crowded place. Thank you for that.
Could you provide some (preferably freely available academic) sources for your claims?
Might I inquire, why is it impossible, in your view, that Julia could overtake Fortran in your field? 
If you think about it, C (1972) came long after Fortran (1957). Compared to Fortran, C had much more convenience in terms of writing code. But developers of DFT tools haven't moved into C yet. And this is 2018. That is because, well, 1. the tools are written by physicists. They write code with the language they are comfortable with and 2. Fortran has very optimized and parallelized matrix operation libraries. So if they didn't move to C, I don't see why they will move to Julia.
I've been doing some more reading over types and promotions. I think this helps a lot actually. To some regard, I think there is still some friction for me in going from an OOP language like Java and Ruby to a strictly procedural language. Thank you for posting this. Do you know why there was criticism though?
Ahhh I C... (look, I got jokes!) Doesn't sound like it'd make too much sense for those in your field who're comfortable with Fortran to make a switch then. 
“Run Julia in your Browser” ...okay
I just noticed that and now I am doing the Python track independently. 
But the packages are still on 0.6
It might not be the answer you're looking for, and maybe you had solid reasons to do things the way you described, but have you considered saving (some of) the configurations of the system in an array and then using the @animate macro to save a video of the simulation? Something like this: anim = @animate for i=1:numframes Plots.scatter(Xhystory[i], Yhystory[i], leg=false) next!(prog) # increment the progress bar end mp4(anim, "./videos/file.mp4", fps=15) &amp;#x200B;
Hmm, I played around with @animate once or twice, but I was more focused on getting the algorithm to work than I was with learning how to properly use macros. Maybe now that I know the output should be right it's time to start learning how to properly use Julia
You dont need to be afraid of using macros. They work very similar to functions. Writing macros, on the other hand, requires a bit of learning about julia's parsing. 
Taken from the author's [website](https://www.sas.upenn.edu/~jesusfv/research.html): "Bottom line: Matlab and R have improved a lot, Python is still awful, and Julia rocks!"
This is an update to [their original paper](https://www.reddit.com/r/Julia/comments/28gzgp/a_comparison_of_programming_languages_in/). Taken from the author's [website](https://www.sas.upenn.edu/~jesusfv/research.html): &gt;Bottom line: Matlab and R have improved a lot, Python is still awful, and Julia rocks! &amp;#x200B;
This code looks a lot better than the one I saw before. Good update! I found the code here, and here's a few things I noticed. [https://github.com/jesusfv/Comparison-Programming-Languages-Economics/blob/master/RBC\_Julia.jl](https://github.com/jesusfv/Comparison-Programming-Languages-Economics/blob/master/RBC_Julia.jl) They test with `include("RBC_Julia.jl"); @time main()`. This includes compilation time. Julia without compilation time is about half of the time of what they report. Here's the results on my computer: 2.383807 seconds (2.89 M allocations: 670.501 MiB, 7.10% gc time) 1.248532 seconds (2.22 k allocations: 528.563 MiB, 5.17% gc time) So this is over-estimating the time by including compilation. The reason why this is important is because if you change the test to be a more expensive model converging to higher accuracy etc., you still get the same compilation time, so this constant overhead is over-estimated in microbenchmarks like this as compared to actual practice. That said, this "microbenchmark" isn't too micro, so it is mostly overshadowed and Julia does well despite measuring compilation. One thing to note is that the compilation flags for other languages are noted, but not for Julia. [https://github.com/jesusfv/Comparison-Programming-Languages-Economics#compilation-flags](https://github.com/jesusfv/Comparison-Programming-Languages-Economics#compilation-flags) Julia should be ran with `-O3` like is done with the other languages. `-O3` is not the default. There are some optimizations which are missing. A simple one is that `@inbounds` should be placed on the `while` loop, and that does give a small time reduction. But a more in depth profile is interesting. About half of the time is spent on: valueProvisional = (1-β)*log(consumption)+β*expectedValueFunction[nCapitalNextPeriod,nProductivity] This is interesting because most of the time here is spent in log. It is known that Julia's log is a bit slower than some system libms, and this is something people are working on. The next significant part of the profile is maximum(abs.(mValueFunctionNew-mValueFunction)) What's interesting here is that all of the time is spent in the allocation, but the allocations are completely unnecessary. Instead of using broadcast, this should be using a generator expression to then iterate through the calculation without actually creating an array, this looks like: maxDifference = maximum(abs(x-y) for (x,y) in zip(mValueFunctionNew,mValueFunction)) This simple change is a pretty large cut to the runtime: 2.151167 seconds (2.41 M allocations: 297.580 MiB, 6.01% gc time) 1.066446 seconds (1.70 k allocations: 179.115 MiB, 2.54% gc time) When I then add `@inbounds` I get: 2.081820 seconds (2.41 M allocations: 297.675 MiB, 5.85% gc time) 0.961019 seconds (1.70 k allocations: 179.112 MiB, 1.89% gc time) Then there's expectedValueFunction = mValueFunction*mTransition' Instead of using a lazy adjoint here, it's ever so slightly faster to instantiate the adjoint matrix before multiplying: expectedValueFunction = mValueFunction*copy(mTransition') 2.038555 seconds (2.42 M allocations: 298.675 MiB, 6.10% gc time) 0.946127 seconds (1.96 k allocations: 179.196 MiB, 1.95% gc time) So the final runtime result, 0.946 seconds on my computer, is less than a second and much faster than the C++ code. Instead of using `@time` for this, it really should be using BenchmarkTools.jl for `@btime` get a more statistically correct timing setup. I'll make a PR with these fixes.
Great analysis. Stuff like this does wonders for growth of Julia.
https://julialang.org/learning/ You can find a lot of resources there.
aah the \`using Pkg\` haha I guess Pkg is not bound into the base namespace by default anymore! Haha that got me. I'm like why is "Pkg.\* not working anymore!!?"
julia&gt; using LinearAlgebra julia&gt; Diagonal(ones(3,3)) 3×3 Diagonal{Float64,Array{Float64,1}}: 1.0 ⋅ ⋅ ⋅ 1.0 ⋅ ⋅ ⋅ 1.0 &amp;#x200B; &amp;#x200B;
I agree
FYI, some tutorials are for V0.6, not the current release (v1.0), some features or functions may be deprecated and produce errors.
Very impressive analysis. How do you profile in Juno? Do you use Profile from the stdlib? Any Atom packages for profiling? If so, how did you come to the conclusion that "all the time is spent allocating" for that particular expression? I'm not aware of how to see the breakdown of allocation.
Hey, sorry if it's a stupid question but why does Python at the bottom of the table in the page that you linked using Numba was able to get a faster result than Julia? But the original version got 90s+? Is python in Numba really that fast?
hm, interesting. i expected that writing maximum(abs.(mValueFunctionNew .- mValueFunction)) would fix this line (note the fused minus). but though it is faster, your zipped version is even faster (on a 10000 sized Int64 array): orig 47.068 μs fused 29.041 μs zipped 18.018 μs also the fused version allocates some memory while the zipped does not. i don't get what's the problem with the fused code.
 abs.(mValueFunctionNew .- mValueFunction) That generates a `tmp` array and then `maximum(tmp)` is called on that array. A generator is a lazy expression for how to loop through a collection, and then `maximum` is called on that iterator without ever generating a temporary.
I'm not the author, so I can't give you a definitive answer. (However, you might be able to find out for yourself by trying out the code on the author's [GitHub page](https://github.com/jesusfv/Comparison-Programming-Languages-Economics).) My understanding is that you can get Python to run really fast using Numba and/or Cython -- the catch is that in doing so, you give up a lot of what makes coding in Python so easy. Of course, I know very little about Numba and Cython, so I could be mistaken.
As an economist who writes this code or variants of it an absurd amount of times, these are some very helpful optimizations. In particularly the rewriting of maxDifference. One thing I think is worth mentioning is that the Julia code performs very well even without small optimizations like this. Many economists have the mindset of "my time is more important than computer time" (which is true to be fair) and aren't willing to invest the time required to learn how to write all these little optimizations. A large selling point (for me talking to people) is how easy it is to switch from MATLAB and still naively write code that works well. Of course, the more one uses Julia the more one realized how suffocating MATLAB was..... but that happens over time. While I'm writing this post, I should also take the time to say that DifferentialEquations.jl is excellent. I use it often when I'm working on continuous time models, and it has impressed a few of my colleagues with what it's capable of. Appreciate the work that you and the rest of the contributors have done.
It really is a good intro. It is not super demanding in terms of time. If you can commit a few hours a week, you should be fine. 
Yo, can I tell you that that this is really usefull. I essentially write bigger version of these models, stuff like this is extremely usefull. I'm gonna try the way you take the sup-norm because I do the same as he does just on a way, way bigger array. 
What do these mentioned packages lack? Could you give an example of this data and transformed output?
If this is really performance sensitive to you, then you can get even better performance by writing a custom function that uses looping and `@ inbounds`, and perhaps even multi-threading. A simple (non-threaded) loop shaves off almost 20% run-time compared to the generator version.
As I understand it, it can be compiled down (there’s a couple of options here, down to a sysimg, or down to a stand-alone executable), but this toolchain is still in progress and I don’t think it’s fully compatible with 1.0 *just* yet. As it improves, the compilation story will be a lot better than Python as well, because rather than the “freeze” approach the python “executable” approaches have taken (where the whole interpreter and stuff is just bundled together, it will be an actual, machine code executable.
Awesome, sounds like I know what language I'm learning next.
I haven’t played with the interop/FFI stuff yet, but they’ve gone to a lot of work to make that super useable and easy, so if you had stuff you needed to call out to in Python, or wanted to, it should be straightforward to do so in Julia as well! To also get you a bit more keen: I’ve nearly finished my first project at work in Julia and I cannot count the number of times where there has been something in Julia (api/language/etc design, functions and packages, etc) where I’ve read the documentation and gone “holy shit yes, this is exactly what I’ve needed/wanted/been looking for”. There are so many little things that are just so good, and they really add up and make a difference.
how is this even possible? generally speaking, it can be dependent on user input which types show up in function parameters. this can be only handled by either disallowing type instability or throwing runtime errors in case of missing code.
A fully type stable program could compile down completely. A general Julia program would have to include the jit. After all the language has eval.
The PRs to fully switch to 0.7/1.0 have just been merged, see [https://github.com/JuliaLang/PackageCompiler.jl/pull/116](https://github.com/JuliaLang/PackageCompiler.jl/pull/116) and later. I've successfully used PackageCompiler on 1.0.
Really? Shit yes! You have just made my day!
Exactly. You can explicitly turn off the JIT using r/https://github.com/JuliaLang/PackageCompiler.jl/blob/66b3fd16182712997c3510453c2078a8d5c3800f/examples/program.c#L28 if you know you won't need it. At runtime, Julia will tell you if you're calling functions with missing native code. &amp;#x200B; PackageCompiler uses SnoopCompile to generate a list of methods to bake into the system image. IMO this part of the PackageCompiler workflow is a little awkward, since the SnoopCompile script is a Julia script, and if you use \`build\_shared\_lib\` with a less-than-trivial driver C program, you may have to write the same program in Julia to make sure that SnoopCompile marks all the relevant functions for compilation into the system image.
&gt;in(a,b) = a ∈ b =&gt;(a,b) = Pair(a,b)
i think he wants to define a new infix function, for example define like(a,b) and then use it as a like b
Currently it's not possible for arbitrary ASCII characters, you can use only symbols as infix operators. `in` and `isa` are an exception to this rule.
related docu section: https://docs.julialang.org/en/v1/manual/variables/#Allowed-Variable-Names-1 Operators like + are also valid identifiers, but are parsed specially. In some contexts, operators can be used just like variables; for example (+) refers to the addition function, and (+) = f will reassign it. Most of the Unicode infix operators (in category Sm), such as ⊕, are parsed as infix operators and are available for user-defined methods (e.g. you can use const ⊗ = kron to define ⊗ as an infix Kronecker product). Operators can also be suffixed with modifying marks, primes, and sub/superscripts, e.g. +̂ₐ″ is parsed as an infix operator with the same precedence as +.
Hey how do you recommend where to put @inbounds function? If I wrote for loop like this, @inbounds for i=1:100 stuff end. This works, but would maximum(abs(x-y) @inbounds for (x,y) in zip(mValueFunctionNew,mValueFunction)) this work as well? Would love to just try it out myself but no access to Julia currently :( Thanks! 
use Cairo.jl perhaps? https://github.com/JuliaGraphics/Cairo.jl/blob/master/samples/sample_text.jl
Maybe you should have a look at IndexedTables.jl https://github.com/JuliaComputing/IndexedTables.jl An other interesting package for dealing with N-dimensional data could be NamedArrays.jl https://github.com/davidavdav/NamedArrays.jl but anyway all feature that you are looking in a timeseries libraries is lacking I understand perfectly that support for N dim data is lacking both in TimeSeries.jl and in Temporal.jl But that's just my opinion I think something like http://xarray.pydata.org/ but being able to deal with timeseries is missing in the Julia ecosystem. But you should also noticed that Python Pandas Panel have been deprecated https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Panel.html
Great read, I did not know about the function composition. Reminds me a lot of lisp.
Same here. Was also surprising to see this, and coming from the Haskell world, love this!
&gt; Julia was designed for parallelism from the ground-up, and provides built-in primitives for parallel computing at every level: instruction level parallelism, multi-threading, and distributed computing. The Celeste.jl project achieved 1.5 PetaFLOP/s on the Cori supercomputer at NERSC using 650,000 cores. This is really cool, I didn't know about it.
but how does one get this composition operator they mention? is it just /degree? Couldn't they have also designated a plain old ascii character for this operator?
Please file specific issues if you find problems like this. DataFrames is actively maintained for years -- we've just had a long release cycle at some point when redesigning missing values support.
A new release has been tagged recently which changes the implementation a lot, so give it another try, and file issues if it still doesn't work.
This was a very early on when they were still figuring out how to handle zeros, Nan, and null storage. The memory leak issues were from loading csv it was not really optimized at that time 
I had the same question, I ended up copying the character from the website itself into my REPL. Definitely need a shortcut for this character. 
I tested it on my Mac right now and it worked fine. Did you recently install the Mojave update? If yes, try to run: $brew update $brew upgrade This should hopefully fix your problems.
I'm no Pkg expert but I'm guessing both JuMP and DataFrames have a shared dependency (or dependency of dependency...) and that's where the incompatibility comes from. I don't know if you've seen this [post](https://discourse.julialang.org/t/jump-and-julia-0-7-1-0/12964) about JuMP and Julia 1.0, but if not, you should check it out!
Yeah. The parser and the runtime aren't in Julia. The Base library is entirely Julia though, and then the packages built on it are mostly Julia as well. To a normal Julia user, what this really means is that everything from a standard install of Julia is written in Julia, except the pieces from the type system (DataType, typeof, etc.) and the `Array` type. (There's probably one or two more things)
Curious that `(sqrt ∘ +)(3,5)` works but `(√ ∘ +)(3,5)` does not, causing `ERROR: syntax: "∘" is not a unary operator`. This despite the help saying \sqrt is equivalent to sqrt.
thanks! ill give a read!
I want to separate the constraint to iterate from j = 1 to 6, because i want to apply a different constraint on j=7, then apply the same constraint as j=1 to 6 on j=8-13. It seems like if I have @constraint(m, a\[i in NAMES\], sum(x\[i,j\] for j in TIMES) &lt;= 1) . It iterate through all column of TIMES.
This looks like code from a JuMP model. I don't know what `a[i in NAMES]` is meant for -- what are you trying to do with that? I think you want to write: `@constraint(m, sum([x[i,j] for j in 1:6]) &lt;= 1)` or just `@constraint(m, sum(x[i, 1:6] ) &lt;= 1)` 
Easiest way is to use a `for` loop and loop through each value of i. A better way is to do: `@constraint(m, [i=NAMES], sum(x[i,j] for j in TIMES) &lt;= 1)` which is close to what you did but without the `a`. (Also, this seems oddly familiar. CS524 with Wright?)
Julia compiles during the first run. Your benchmark @time includes the compilation. To measure running code do main_loop(N); @time main_loop(N); 
There was a licensing problem with the library which is used to download Julia modules through a proxy. This license was not compatible with Debian - so actually you should ask about the upstream, Debian. I have not checked on the Debian Julia list recently - however I believe the licensing problem is solved.
Starting from the "fast" version, you don't need the global distance = 0.; energy = 0.; so get rid of those. You do utilize the cache array `dx` though. If you want to keep that a global, you need to make it const dx = zeros(3) In order to make your functions type-stable. Otherwise, inside of your functions the compiler cannot know that the type of `dx` will not change (since it's in the REPL so you can change it any time) and so your performance decreases quite dramatically. I would recommend just passing the work vector into the functions instead of using a global, but at least `const` will fix the performance. However, inside of the functions you're not actually making good use of the cache array. For example dx = bodies[i].x - bodies[j].x This constructs a new array and assigns `dx` to it. What you want to do instead is update the values of the currently existing `dx` array. You do this with broadcasted equals. However, to completely get rid of the allocations you want to broadcast all element-wise expressions, so dx .= bodies[i].x .- bodies[j].x or the shorthand @. dx = bodies[i].x - bodies[j].x which means "broadcast everything". Another place where this comes up is in `dx .= zeros(3);` which is actually creating an array, which instead should be `dx .= 0.0` or `fill!(dx,0.0)`. If you go and clean up all of these small allocations then your code will be quite a bit faster. However, instead of taking that approach you might instead want to use StaticArrays.jl. If you do this and define your bodies like # Neptune Body(@SVector [1.53796971148509165e+01, -2.59193146099879641e+01, 1.79258772950371181e-01], 0.0, @SVector [2.68067772490389322e-03 * DAYS_PER_YEAR, 1.62824170038242295e-03 * DAYS_PER_YEAR, -9.51592254519715870e-05 * DAYS_PER_YEAR], 5.15138902046611451e-05 * SOLAR_MASS) then using out of place operations will utilize stack-allocated arrays and thus not cause any heap allocations, so in that case `dx = bodies[i].x - bodies[j].x ` will be perfectly fine. This will actually be faster than using standard arrays too. 
Currently there's version 1.0.0-ubuntu5 in 18.10
This last point is quite important. `Array{Float}` is an abstract type, and that is very bad to have inside a struct. Definitely use the above suggestion, or the (even nicer) alias `Vector{Float64}`.
Thank you, I know this is probably basic stuff but this really helps me understand how Julia handles in-place vs. copy operations. I'll give those changes a try soon and see what happens. 
Great to know, thank you!
I somehow got Julia 1.0 in a fresh Ubuntu Installation a week ago, from the default sources. I ran into problems adding packages as Julia had issues accessing github servers. Googling fixes did not help. This is not the answer to your question but this is perfectly fine to make it via the official binaries (and you will have better control over versions)
I downloaded the original code and when I ran the C sequential code I managed to get an execution time of \~14 seconds on an Intel Xeon CPU E5-2697A v4 (Compiled with icc 19). I tried the author's Julia implementation and got around 30 seconds. Both with the 512x512 test case. This was with no tweaks to the C code. Adding the `restrict` keyword to the `x` and `x0` parameters of the `computeNext` function dropped the C version of the code to ~9 seconds.
Yes, `Array{Float64, N}` is an `N`-dimensional array of `Float64`s. If `N` is 1, it is a vector, which has a type alias, called `Vector{Float64}`. If `N` is 2, you can say `Matrix{Float64}` in stead. I think these aliases are better and more readable, and recommend using them.
Reading that, it looks like the only reason for recommending the Julia binaries is that there are some patches for llvm that haven't made it to an official release yet. There's also a fork of libuv and though it's not clear why those patches are in a fork instead of going upstream, that page makes it seem like less of an issue. Anyway, I'm wondering, once LLVM releases with the full patchset, will distribution binaries become recommended? Really not a fan of not getting updates through my package manager. A substantial reason I use linux+OSS is so that I can rely on the package manager to keep my software up-to-date.
Even with GCC, I got 23 seconds on the same hardware by compiling with O3. bash-4.2$ diff -r ./ ../Heat2D_MPI_Solving_C-original diff -r ./Makefile ../Heat2D_MPI_Solving_C-original/Makefile 2c2 &lt; CC = gcc -O3 -Wall --- &gt; CC = gcc-mp-7 -Wall bash-4.2$ make explicitSeq mkdir objSeqDir gcc -O3 -Wall -c explicitSeq.c -o objSeqDir/explicitSeq.o gcc -O3 -Wall -c explUtilSeq.c -o objSeqDir/explUtilSeq.o gcc -O3 -Wall -lm -o explicitSeq objSeqDir/explicitSeq.o objSeqDir/explUtilSeq.o bash-4.2$ echo -e "512\n512\n100000000\n0.1\n0.1\n" | ./explicitSeq Size x of the square Size y of the square Max. number of steps Time step Convergence Time step too large in 'param' file - Taking convergence criterion Time step = 9.462671653e-07 Convergence = 0.100000000 after 26394 steps Problem size = 262144 Wall Clock = 23.870000000 Computed solution in outputSeq.dat
Another major reason was the rapid development of Julia pre-1.0, which doesn't mesh well with the release cycle and package manager philosophy of Debian and Ubuntu. But I don't think there's any intention to change to a package-manager-based installation procedure even post-1.0.
Ugh, I'm really sorry but it looks like I got the "slow" and "fast" versions mixed up above. Although now the slow one isn't crazy slow like it was before, so I'm not sure what to make of that.
&gt; There's also a fork of libuv and though it's not clear why those patches are in a fork instead of going upstream, that page makes it seem like less of an issue. The patches are always attempted to be upstreamed, but that is a slow process. Right now, the patched versions are recommended for correctness. &gt;Anyway, I'm wondering, once LLVM releases with the full patchset, will distribution binaries become recommended? Hopefully. As it stands right now though, Julia is a great bugfinder its dependencies, so there's usually some patches trying to be upstreamed.
&gt; &gt; There's also a fork of libuv and though it's not clear why those patches are in a fork instead of going upstream, that page makes it seem like less of an issue. &gt; &gt; The patches are always attempted to be upstreamed, but that is a slow process. Right now, the patched versions are recommended for correctness. Sorry, didn't mean for that to come across as accusatory about upstream contributions. Just meant literally that the LLVM stuff was explained right there but the libuv stuff wasn't. &gt; Julia is a great bugfinder its dependencies I feel like that's the story of my history with computing in general.
I tested it out tonight and that simple change made a huge difference in improving the speed of the slow version! It's now my fastest version, except the one I found online that's a [highly optimized version](https://github.com/ecarl65/programming_benchmarks/blob/master/nbody_perf.jl).
Thanks so much, u/ChrisRackauckas , for the suggestions and for what that taught me about Julia. I must be doing something wrong still, however, because when I implemented both of these ideas it actually made things slower! The implementation of your first ideas, with the `const` and broadcast operations, is [here](https://github.com/ecarl65/programming_benchmarks/blob/master/nbody1.jl). It was slower than the first version it came from by less than 2x. But the second one, in which I used `SVector`s, is [here](https://github.com/ecarl65/programming_benchmarks/blob/master/nbody2.jl), and it was excruciatingly slow, orders of magnitude slower. 
I forgot to mention this in the original post (since edited), but the fastest version of this program, by far, is one I found on some forums. You know what it does that makes it fast? It seems to avoid the use of vectors or arrays altogether wherever possible. That's pretty disappointing to me. In my mind that's most of the reason to use a language like Julia, that it should be fast with arrays and vector/matrix math. For reference here is the fastest version of the program: [https://github.com/ecarl65/programming\_benchmarks/blob/master/nbody\_perf.jl](https://github.com/ecarl65/programming_benchmarks/blob/master/nbody_perf.jl)
Julia _is_ fast with arrays, it's just that scalars are faster. It would be disappointing if that were _not_ the case, because that would mean performance was limited to that of a dynamically sized, heap-allocated data structure. But you can do something else that's cool. Look up StaticArrays.jl. They are sort of scalars disguised as arrays (actually, they are tuples with fixed size known to the compiler). They behave like vectors and matrices, but are super-fast. They are exactly what you should use in cases where you work with small, constantly-sized vectors. I think there's a chance you can even improve on the performance of the linked 'optimized code', if you use StaticArrays, and make `Planet` a `struct` instead of a `mutable struct` (then you'll have to 'replace' each planet for every `advance` instead of updating in-place, but that can sometimes be faster). One more thing, the linked 'optimized code' uses a lot of unnecessary type annotations inside the functions. Don't copy that, it's just noise.
I also see that you have `bodies` as a global, which the functions get to work on. Don't do that. Pass it as an argument, like this: `advance!(bodies, dt)` `advance!` should work on the `bodies` vector in-place, and should therefore have a `!` at the end of the name. Same thing with `offset_momentum`. Avoid globals.
OK, looking more at your code, even ordinary arrays should _much_ faster than what you're seeing now. You still have `Array{Float64}` in your `Body` fields, which is _really_ bad. And you create an incredible amount of unnecessary temporary arrays. In fact, you seem to not do _any_ in-place array operations _anywhere_! You can _massively_ speed up your code, without even using StaticArrays.
Some other useful package could be: - https://github.com/JuliaArrays/AxisArrays.jl - https://github.com/JuliaAudio/SampledSignals.jl
It's not that overflow warnings are turned off but that they don't exist in the first place. Overflow is part of the defined behavior of the `Integer` types; it's expected behavior.
*clears voice* I want safe arithmetic!
Why would anyone want that, apart from performance reasons?
well, performance reasons
https://github.com/JeffreySarnoff/SaferIntegers.jl
I believe the argument is that if you don't need the speed offered by standard integer arithmetic then you would just use `BigInt`. 
Do you do things in the REPL where you would notice a difference in performance if overflow checks were enabled by default?
i would not want the repl behave differently. it does not even make too much sense to me, everything runs from the repl, isn't it? and if you just want direct math expressions, well in julia everthing is a function call, 2*2 is a function call to *. a matrix multiplication too. or a complete differential equation solving. repl means everywhere
I can't speak for the developers but I suspect it was because Julia was designed to be a high-performance language rather than a safety-critical one, and integer overflows are so rare that it was deemed more important to keep the performance than gain a bit of safety in edge-cases. 
With the Julia version 1.0.1 update, the compatibility issue with Reduce and PyPlot is resolved, so the Fatou package should be working now, but I have not publicly committed all my updates to it yet. &amp;#x200B; Later this week, I am going to push an update, but Julia 1.0.1 does resolve the fundamental issue with Fatou.
Alright, the update has been pushed to github, it will be registered now. Have fun! please share your Fractals!
Your `bodies` array was still a global. You could use `const` on that but I highly suggest localizing your variables: that's good style in any language. In addition, you missed a broadcast. Here's the mutating form: https://gist.github.com/ChrisRackauckas/67dd77e0288ac507b656f3c794bb00c7 That's dramatically faster. You can probably take it from here for the StaticArrays version.
If you're using integers bigger than 1000000000000000000 then you should be aware of that, most people aren't. 
It was an integer overflow during assignment/conversion: &gt; This shutdown occurred 36.7 seconds after launch, when the guidance system's own computer tried to convert one piece of data -- the sideways velocity of the rocket -- from a 64-bit format to a 16-bit format. The number was too big, and an overflow error resulted. Source: https://around.com/ariane.html Still an integer overflow. Guarding for integer overflows, either at runtime, compile time or both, is still a really, really good idea.
no it was not. i know the case inside out. it was a *conversion* error, which is markedly not an overflow, and it was actually *caught* by a runtime check which caused the problem. without the runtime check, it would have been fine.
Well that escalated quickly. but seriously I think there is something in this for readers of all levels. Like if it is obvious and easy to you, scroll down to the next bit. 
Yes, you have defined the fields of `Body` to be `Vector{Float64}`. If you then construct a `Body` with `SVector`s they just get converted back to `Vector`, which takes a _lot_ of extra time, with no gain, so everything just gets slow. You have to redefine `Body` to accept StaticArrays.
I completely agree that runtime checks may not always be helpful, and that if you don't know what to do in an error condition, you might as well not check it. However, if the conversion from 64-bits to 16-bits is an "integer overflow" or not, I believe is a matter of definition. If the conversion had went through, one could say that the 48 lost bits had "overflown", since they did not fit in the "bucket". Potato potato. :)
The composability of julia code has nothing to do with the composition operator. And everything to do with the dispatch and type system.
Optionally typed does not mean supporting functions with optional arguments. 
good stuff. i would like to make understanding #2 mandatory before anyone starts to develop a library. i'm not sure i would wander into the realms of #3 and #4 though. as a side note: i get that this is an example. but a programmer should first think: do i really need this? do i really need a function that turns a scalar into an array? what i just ass a number down the line and see how it works? what if i, as a library developer, write code that is not picky and can eat scalars just as well? i don't mean making it understand scalars. i say the exact opposite: make it so that it does not care. if it does care, check if you have a good reason, or maybe you put up artificial barriers? it is fascinating what duck typing can do if you trust it.
but the conversation is markedly not about that, but the weird fact that you try to argue for runtime checks citing a case in which runtime checks caused millions of dollars damage.
I love you and your writing, but you could do with a proofreader casting an eye over your text before you publish... I'm always available at no cost! :)
Great content! Few recommendations: 1. Proofread 2. Left-align the text or enable hyphenation. 3. Write better introductions.
No, I'm arguing for something along the lines of: * Having BigInt (or something similar) as the default number type * Having Int as a number type that can be used for the special cases where people want performance and/or rely on integers overflowing. Also, I believe runtime errors **in the REPL** is a good idea.
to that i reply: no (at least, i hope so. not my decision.)
There were a few spelling mistakes. But given this post is available for you to read for free and the content is good. And the spelling didn't detract from the soul of the post.
Dang, excellent point. I had totally missed that. Thanks! I guess I don't totally understand `StaticArrays`. It seems like you can't iterate through them or set values individually? For example, in this throwaway code here is what I get: julia&gt; dx = SVector{3,Float64}(zeros(3)) 3-element SArray{Tuple{3},Float64,1,3}: 0.0 0.0 0.0 julia&gt; dx += 1 3-element SArray{Tuple{3},Float64,1,3}: 1.0 1.0 1.0 julia&gt; dx[2] = 2.0 ERROR: setindex!(::SArray{Tuple{3},Float64,1,3}, value, ::Int) is not defined. Stacktrace: [1] error(::String) at ./error.jl:33 [2] setindex!(::SArray{Tuple{3},Float64,1,3}, ::Float64, ::Int64) at /home/ecc/.julia/packages/StaticArrays/Ze5H3/src/indexing.jl:3 [3] top-level scope at none:0 So vectorized operations work but not individually indexing. I'll have to see if I can massage the code to work with that, but right off the bat I have lot's of code that iterates and sets things individually, which is now giving me an error. I remember the tricks to vectorizing stuff in Matlab, but it's been a while. I'll have to dust off the cobwebs. function offset_momentum(bodies) for i = 1:length(bodies) for k = 1:3 bodies[1].v[k] -= bodies[i].v[k] * bodies[i].mass / SOLAR_MASS end end end
Looking at [this](https://stackoverflow.com/questions/47580303/incrementing-a-staticvector-at-an-index) StackOverflow answer it seems I could use array comprehensions. I'm familiar with that from python, but it seems kind of kludgy. I think perhaps the right answer is to stick with the correct inplace operations instead of using `StaticArrays`? With all your help I finally got that down to the minimal amount of allocations, and it was a **huge** difference in the number of allocations and memory used (eg a few dozen allocations versus millions, a few hundred bites used versus hundreds of MiB). 
You have two main types of arrays, S-arrays (`SVector`, `SMatrix`, etc.), and M-arrays. The latter are mutable, and the former are immutable. You can indeed iterate over both, and access individual elements with indexing, just like with ordinary arrays, but you cannot assign to elements of the S-arrays. It's not hard to fix your code to handle S-arrays, though. For example `offset_momentum` becomes: function offset_momentum(bodies) for i = 1:length(bodies) bodies[1].v -= bodies[i].v .* (bodies[i].mass / SOLAR_MASS) end end
they're probably referring to optimization in the JuMP library. &amp;#x200B; [https://jump.readthedocs.io/en/latest/quickstart.html#objective-and-constraints](https://jump.readthedocs.io/en/latest/quickstart.html#objective-and-constraints)
If I have a dictionary (read from XML) that has both strings and symbols as their keys, is there an easy way to switch them all to one or the other? 
I use the DelimitedFiles package. The readdlm() function that was global in 0.6 got moved to that package in 1.0. I see you got the dependency issue resolved. So you could compare both CSV and DelimitedFiles, depending on your needs.
To switch to all string keys ~~~~ newdict = Dict() for (k,v) in xml_dict newdict[string(k)] = v end ~~~~ or to switch to all symbol keys ~~~~ newdict = Dict() for (k,v) in xml_dict newdict[Symbol(k)] = v end ~~~~
that's a good alternative, thanks!
&gt;perhaps with the option "perhaps"? You're suggesting that you'd want to \*prevent\* us from optimizing our code?
A smart enough compiler could optimize away unneeded integer overflow checks, while keeping the performance.
But that's a string, isn't there a base16 int type? 
I would love to make this a thing. Currently the Holodeck team is comprised of 3 undergraduates working part-time around our studies. We don't have the bandwidth to do this ourselves, but we would be happy to support anyone that wants to make this happen!
Can you provide a minimum working (well, not-working) example?
` a = Any[1 2 3; 4 1 6; 7 8 1] ` to replicate the data as it is extracted from readdlm ` inv(a) ` which returns error ` a = convert(Array{Float64}, a)` Hmmm the above code actually works now for some reason. I don’t know what I was doing wrong before. I’ll probably delete this post or have it removed 
And we can argue if checks for integer overflow checks were needed I don't know much about compiler optimizations, but the guys who develop Julia do. If this were possible and worthwhile, I'm sure they would have done it.
Thanks for the good info, I somehow missed this. Julia just isn't for me. I would use C or Assembly for performance, and combine it with Python, OCaml, Go, Haskell, C++ or Common Lisp for composition and structure. If I were to use a "mathy" language, I would always choose one that behaved as "mathematically" as possible, even when performance was sacrificed. Better algorithms is usually the key to better performance, and rarely the small details (like unsafe but faster integer arithmetic), in my experience.
parse() takes a string and turns it into the value. So when you do parse(Int, "60456", base=16) you are doing: &amp;#x200B; 16 \* 1 + 256 \* 5 \* ... &amp;#x200B;
Seems nice but it doesn’t work for online notebooks https://discourse.julialang.org/t/juliabox-adding-a-package-asks-for-credentials-and-fails/15224/7
Try the Python Markdown extension. It allows you to refer to your variables in the Markdown cells. If you're on juliabox, it's already installed. Not sure if it comes bundled with other forms of Jupyter Notebook. https://jupyter-contrib-nbextensions.readthedocs.io/en/latest/nbextensions/python-markdown/readme.html 
On windows, DLLS are looked for in (roughly): 1. Apps .exe directory 2. current directory 3. system dirs 4. %PATH% More: [https://docs.microsoft.com/en-us/windows/desktop/dlls/dynamic-link-library-search-order#search-order-for-desktop-applications](https://docs.microsoft.com/en-us/windows/desktop/dlls/dynamic-link-library-search-order#search-order-for-desktop-applications)
Well the changes to PATH are not visible to julia REPL - env["PATH"] does not return the ArrayFire path. Which get my head scratching because when I'm simply running command line it is there same if I run git bash. So I guess this is more of a windows question - why julia REPL launched using a shortcut has different PATH.
it is rather unfortunate Julia doesn't support a graphics library.
Never heard of optical flow, but it seems a lot like Particle Image Velocimetry (PIV). Perhaps try looking up packages that do that and you can modify?
Found this https://github.com/JuliaImages/ImageTracking.jl 
Awesome, thanks!!
Maybe the FileSystem module is what you're looking for? [Here is a link to the relevant part in the doc](https://docs.julialang.org/en/latest/base/file/#Filesystem-1)
Commands like `ls()`, `pwd()` and `cd()`, `mkdir()`, `run(&lt;command&gt;)` are available in base. More on that look in https://docs.julialang.org/en/v1/manual/running-external-programs/#Running-External-Programs-1 and https://docs.julialang.org/en/v1/base/file/#Base.Filesystem.pwd
Hi mate, If you ask a personal advice with a little bit experiences, I say you Julia for two principal reasons. Firstly, if you study Econometrics, the Julia package is really good. I think everything is implemented with a good intuition and there are lot of good tutorials. If don't, implement by yourself to understand better the underlying concepts. Secondly and it's little bit more personal, Julia is a great performant and intuitive programming language. Don't forget it's 100 times faster than Python or R. If you have any questions. Cheers.
Maybe Base.Sys can be useful: julia&gt; names(Sys) 23-element Array{Symbol,1}: :ARCH :BINDIR :CPU_NAME :CPU_THREADS :JIT :KERNEL :MACHINE :STDLIB :Sys :WORD_SIZE ... :isapple :isbsd :isexecutable :islinux :isunix :iswindows :loadavg :total_memory :uptime :which
It seems like it is a known issue https://github.com/JuliaIO/MAT.jl/issues/90
Imo, Julia is a great programming language in many ways. But perhaps it's still a little immature to compete with the ease of using Stata. Actually, I'm in the same place as you are (I have an econometrics course this semester and we're taught stata in this course) but I chose to do my econometrics works in R simply because there are many great tutorials available already. If you use Julia instead of R, you'll end up reimplementing a lot of the functionalities of R packages in Julia, a process by which you'd get really good at Julia, but you will also never be able to compete with the ease of having used stata in the first place. Using R will prove to be a breeze simply because of the sheer amount of discussions already made on many of the questions that you will likely end up asking about R and econometrics. On the plus side, R has shiny while Julia has genie which is nowhere as mature as shiny is. 
It does work with if you pull in the PRs that fix 0.7 deprecation warnings and 1.0 error. So for now you could just `dev MAT` and apply those patched using git.
I’ve loaded matlab files (with some work) using scipy. https://docs.scipy.org/doc/scipy/reference/generated/scipy.io.loadmat.html Maybe you could use python to convert to a numpy format and load that with something like NPZ.jl? I’m not sure if it will work for your case, but it is a possibility.
I have never worked with Stata. I have worked with R, Python and with Julia, though not as much with Julia as with the other two. Here are my (very subjective) recommendations based on my own experience. Use R if 1. you're using methods for which there are already existing libraries, 2. your coding consists mainly of cleaning data and automating the application of existing methods, 3. you're developing some new models or methods, but you expect your code base to be smallish, or 4. your intended users are statisticians. Use Python if, 1. you're creating new models from scratch and the project is large,, 2. you don't mind reimplementing methods that are already in R, 3. you prefer a modern object orientated language with clean and easy to read syntax, 4. you're planning on using some machine learning tools, or 5. your intended users are computer scientists or developers. Use Julia if, 1. you want your code to be "close to" the way you write your maths, 2. you need performant code without resorting to wrapping C++ code, 3. you don't mind reimplementing methods that are already in R, 4. you need easy automatic differentiation, 5. you don't mind using a programming language that is new and has a slightly different style to most other languages, 6. you and your intended users love MATLAB but can't afford it any more. Obviously, some of the points I make are not relevant to "homework". For homework, R is probably what you want. R makes doing simple things really easy, provided that you are willing to learn the commands. But if you are going to develop things from scratch, then Python and Julia are likely a better bet. Doing things from scratch is actually a great way to learn econometrics. Since Julia code is "closer" to maths than Python, I'd say it's a really good choice for that sort of thing. Also, Julia hit version 1 recently, so you no longer need to worry about major changes to the language. Python is likely overkill for homework. I do, however, think it's a worthwhile language to learn. IMHO, it's also an absolute joy to code in. In terms of places to ask questions... StackExchange and CrossValidated are great as long as you know how to ask a good question. If you can't produce a minimal working example (MWE) of your problem, then Reddit forums are often more forgiving. Wherever you ask your question, MWEs and proof of own effort at solving the problem are always welcome. Of course, all of this is just my opinion. Hope this helps and do let us know what you end up deciding on. TLDR; R is great for doing simple things quickly, like homework. If you want to gain a deeper understanding of econometric methods, code the maths from scratch in Julia. Where you ask for help is less important than how you ask for help.
Genie is similar to Django, not Shiny. Interact.jl is similar to Shiny and is getting pretty mature, but missing the easy deployment of Shiny Server Pro.
Referring to variables in double curly braces in a markdown cell doesn’t work in juliabox
Okay it is working now thanks. Do you have example code of how I can use a for loop to populate a table like I would in latex?
Well you can put the for loop right in the Markdown if you want, as long as you can make it a one-liner. You could try something like {{for x in range (1, length=4) println(x, "\n"); end;}} Notice that I used semicolons to separate the commands to make it work. You can get your loop the way you want it in a code cell, then easily concert it to a single line and paste it into the double curly braces. 
Do you need `value`? julia-1.0&gt; x = Date(2015, 06, 06) 2015-06-06 julia-1.0&gt; y = Date(2016, 12, 29) 2016-12-29 julia-1.0&gt; (y - x).value 572 julia-1.0&gt; (y - x).value / 7 81.71428571428571 
Thank you for that. I will try it out.
i'm quite sure there is a function to get that, maybe convert. accessing data members is not nice. 
Yes, so this might be better? julia-1.0&gt; import Dates: value julia-1.0&gt; value(d) 572 
yes. struct members can be thought of as implementation, and the functions are the interface.
Behind the scenes. DateTimes are actually 64bit ints julia&gt; typeof(Dates.value(now())) Int64 julia&gt; Date(0xffffffffffffff) -165227855484411-18085043209519168-7378697629483820658 julia&gt; 
That is really helpful to know, thanks. The ().value tip from another commenter was a good solution. 
I'm an R user but curious about Julia developments. I currently use Rcpp for running simulations in R and it's pretty fast, something worth comparing. Also, I use data.table which is very fast. Is this going to be benchmarked as well?
At what level is this class? I am going to be contrarian and say that learning Stata (or SAS) is a worthwhile investment in addition to Julia, Python or R and so on. I personally find the approach of those two to be much easier to understand when dealing with a large and messy data sets than the approach of Python or Julia. I cannot really comment on R, it is somewhat between those two. In my mind the workflow would be: 1) Use R, Stata or SAS for data cleaning and simple statistical analyses 2) If you need to code your own estimator, then use Julia, Python, Gauss, Matlab, Fortran or whatever floats your boat. I would use Julia, Fortran or Haskell, but that just me. Learning Stata well is something that a professional economist have to know if you are planning ever to collaborate with anyone. I don't know about the econometrics packages in Julia, I have done only numerical (non-econometric) simulations with it. Appeal to unverifiable authority: I am an Economics Ph.D. with 15+ years of professional experience.
parens instead of brackets (f(I) for I in 1:n if something)
\&gt;I currently use Rcpp for running simulations in R and it's pretty fast, something worth comparing &amp;#x200B; Wouldn't that be simply comparing to C++ ? &amp;#x200B; \&gt;I use data.table which is very fast &amp;#x200B; I have no idea how I would use that for the raytracing benchmark ;) But It would be fun to do some more benchmarks in other domains! Since I'm not very familiar in R, I'd need some R examples though. Would you be willing to supply some R examples you want to benchmark? You could use Nextjournal, and I can add a julia version to your article ;) 
Thank you for the information. I've decided to do homework in Stata because the code of seminars is provided and it's pretty close to what we need to do in homework. Also, I'll try to use Julia just to learn it. It's kind of exercise: I get results in Stata and then try to replicate it in Julia. I'm not going to do it with all my code, only basics
That's easy, thanks
[This](https://lectures.quantecon.org/jl/) may be of interest to you.
Yes, I've already know about this one. It's how I met Julia
Here's a good link for that. Looks like Julia is on the list but not yet tested... https://github.com/Rdatatable/data.table/wiki
You need to change the PATH on the user environment settings.
Perhaps try inside of brackets?
I have a few benchmarks here: https://github.com/jonathanBieler/ScientificComputingBenchmarks.jl It would be interesting to add more statistics and bioinformatics, since it's what R is often used for.
It's called a generator and many functions can take one as an argument: sum(i for i=1:10) Which is indeed faster than allocating an array.
data.table is mostly C code called from R and is quite fast, faster than Julia's DataFrame at many things. I wonder if a Julia port of rdatatable's C code would be as fast.
Could be. H20.ai is building a data.table package for Python that is showing promise
Whoops :( Thanks a lot mate! 
&gt; data.table is mostly C code called from R and is quite fast, faster than Julia's DataFrame at many things. I wonder if a Julia port of rdatatable's C code would be as fast. Part of it is just that Julia's DataFrame is not very type stable. It was made to be highly highly flexible: you can change the types of columns at will, use missing values for any type of data, and naming is mutable. This comes at a performance cost though. data.table naturally is fixed to using only a few data types and only a few data types allow for NA in R. So this is a case where a Julia package has made a conscious choice for improved functionality over speed. You can handle this somewhat in algorithms by using function barriers after indexing in order to only dynamic dispatch once, but this is not the type-stable Julia code people are used to. JuliaDB is the fast alternative for that fully typed kind of code, and it of course comes with some reduced flexibility (but some other interesting features to make up for it).
`ctrl-j` + `ctrl-k` will kill the current Julia process and start a new session. It effectively clears the workspace. But it also clears everything else, like the packages you've loaded. `ctrl-j` + `ctrl-c` will clear the console. `ctrl-l` also clears the console and works in the REPL if you're not using Atom/Juno. 
I used workspace() in the past. I don't know if it still works.
Awesome stuff! We just need to push to get things working in Julia 1.0, I'm looking at you OpenCL. CUDA is awesome but not everyone has the architecture. Getting everything working with GLAbstraction would be great!
What editor are you using?
This is how I would do it: artithmeticExpression(a, b, c) = any(op -&gt; op(a, b) == c, [+, -, *, /])
This is a submission to a code challenge at some platform. A similar one is [codewars](https://www.codewars.com/). So, it's a web editor ;)
Now benchmark them! Fight fight fight! ```julia julia&gt; using BenchmarkTools julia&gt; function orig(a::Int,b::Int,c::Int) for eq in [+, -, *, /] eq(a,b) == c &amp;&amp; return true end return false end orig (generic function with 1 method) julia&gt; funca(a, b, c) = any(op -&gt; op(a, b) == c, [+, -, *, /]) funca (generic function with 1 method) julia&gt; funcb(a,b,c) = any(op(a,b)==c for op in [+,-,*,/]) funcb (generic function with 1 method) julia&gt; @btime orig.(rand(Int, 100),rand(Int, 100),rand(Int, 100)); 101.956 μs (2007 allocations: 47.59 KiB) julia&gt; @btime funca.(rand(100),rand(100),rand(100)); 94.476 μs (813 allocations: 31.13 KiB) julia&gt; @btime funcb.(rand(100),rand(100),rand(100)); 100.571 μs (913 allocations: 35.81 KiB) ``` and the winner in efficient code is u/leovailati ! congrats! Now the question is why is the lambda slightly faster than the generator expression? 
It's Codefights/Codesignal
&gt; Now the question is why is the lambda slightly faster than the generator expression? I guess is my method is unintentionally better at short circuiting the computation. I think the generator expression evaluates all operations and then passes the resulting boolean array to the `any` function, which is not the case when you use a lambda: the `any` function consumes the boolean values directly. Not sure why the original function would take longer though.
 function arithmeticExpression(a::Int,b::Int,c::Int) map((+, -, *, /)) do eq eq(a,b) == c &amp;&amp; return true end return false end &amp;#x200B; Scalar performance: @btime arithmeticExpression(a, b, c) setup = begin a = rand(1 : 10) b = rand(1 : 10) c = rand(1 : 10) end 1.252 ns (0 allocations: 0 bytes) &amp;#x200B; Broadcasted performance: julia&gt; @btime arithmeticExpression.(a, b, c) setup = begin a = rand(Int, 100) b = rand(Int, 100) c = rand(Int, 100) end 684.516 ns (4 allocations: 4.34 KiB)
Usually the idea it's to install Jupyter Notebook to your local machine. The online one is just a demo to try before installing your own. Google has an online version that integrates with Google Drive. Check it out here: https://colab.research.google.com/notebooks/welcome.ipynb#recent=true 
Oh thank you this looks interesting, say I upload a csv to my account does colaboratory io to and from the drive?
If you are aiming for speed, consider simplicity: ```julia f4(a,b,c) = a + b == c || a - b == c || a * b == c || a / b == c @btime f4.(rand(100),rand(100),rand(100)); # 1.521 μs (7 allocations: 6.97 KiB) ``` For comparison on my machine the funca takes ``` funca(a, b, c) = any(op -&gt; op(a, b) == c, [+, -, *, /]) @btime funca.(rand(100),rand(100),rand(100)); 164.160 μs (813 allocations: 31.13 KiB) ```
Same thing in F# with less code and static type checking: let inline arithmeticExpression a b c = a+b=c || a-b=c || a*b=c || a/b=c 
Same thing in F# with less code and more static type checking: let inline arithmeticExpression a b c = a+b=c || a-b=c || a*b=c || a/b=c 
I guess I should have mentioned that I did do exactly that. However it still does not answer my first question, or my updated second question. It is exceedingly rare that I am required to examine the source code of a library to determine that library's API in any language. Is this the standard I should expect from Julia libraries?
But the Sublime answer was also partially correct - OP seems to be using it a Sublime editor mode, going by the dropdown choice in the top right corner.
MATLAB.jl maybe?
As mentioned, it does not.
I like the why julia section
Why limit this to `Int`? Surely all of these operations can be done on Floats and other types also. As I understand Julia has some sort of type inference built in. 
one technique I use, I have my Github projects in my my home and I use different environments so : cd(ENV["USERPROFILE"] * "/GitHub") unshift!(LOAD_PATH, "XlsxWriter.jl/") 
Either directly include `a.jl` into b (if it's a submodule of b) or make `a` a real package. To make a real package in v1.0 you just need the `ModuleName/src/ModuleName.jl" (see https://docs.julialang.org/en/v1/stdlib/Pkg/index.html#Generating-files-for-a-package-1) and then you can `]dev path_to_ModuleName` it.
Good point. I'll have to think about organization some more. 
If you're interested in differential equations, we have a pretty active community in JuliaDiffEq. We can find an appropriate topic that suits your interests. Just come [chat in the chat channel](https://gitter.im/JuliaDiffEq/Lobby) (or #diffeq-bridged on the Julialang slack).
either what /u/tarrosion recommended, or you can define f(a::T) where T &lt;: Tuple = f(a...) and then just use f.(A)
For core Julia, you could§§ consider tackling one of the issues labelled [good first issue](https://github.com/JuliaLang/julia/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22), and you can also find many areas where the documentation could be improved (more tests, cross-references, etc). Read the [contributing info](https://github.com/JuliaLang/julia/blob/master/CONTRIBUTING.md) before you start, and remember to make a branch before you make changes... :) As you move away from the central core of Julia, there are many more opportunities to improve code, tests, and documentation, and you'll often find that introductory and tutorial information needs both adding and updating (because the authors have been concentrating up to now on developing the packages' functionality). Since versions 0.7 and 1.0 were released only as recently as August, there are still a number of places where v0.6 code/documentation is still being used - these would be useful places to start, and your contributions would be valued! (For example, you'll find `linspace` still in use, although it was replaced with `range`...)
If you want a mini-project the best place to look: [GSoC Ideas Page](https://julialang.org/soc/ideas-page.html). Note that some work might have already been done in the last GSoC however, you will get an idea of what kind of work is available, you can communicate with the mentors on slack. Also try to look for different groups [here](https://julialang.org/community/) scroll down below to the Github Groups section, find the groups that interest you and start communicating. 
Yes, thank you, that answered my question. (The answer below too)
Try reshape(A,7,10,1) and then repeat(A,1,1,N)
`repeat(A, inner=(1,1,N))`
 julia&gt; A = randn(7, 10, 3); julia&gt; B = randn(7, 10); julia&gt; C = A .- B; julia&gt; C[:, :, 1] == A[:, :, 1] - B true and so on 
It's better to use broadcasting since repeating allocate memory for no good reason.
Not only needless allocations but also needless copy operations(read/write). 
Excellent, thanks!
Perfect, thanks for that.
This kind of thing is why it's worth investing time and effort in Julia. 👏👏
Maybe you have to install GLPK: https://askubuntu.com/questions/349582/installing-and-running-glpk I'm not using Linux so I don't know that this is actually a fix. 
I still have some stickers lying around. PM if i can send you some
[The paper](https://arxiv.org/pdf/1810.09868v1.pdf) \[PDF\].
[removed]
Thank you - yes I have installed GLPK, but Julia doesn't know about the solver GLPKSolverLP. I'm not sure whether it's a problem of the binaries being in the wrong place, or a permissions issue, or something else. I'll have another fiddle - maybe re-installing might help.
Hopefully I'm not way too late for the party, lets generate the function! using Base.Cartesian function func_generated(a,b,c) @ntuple(4, op) = (+,-,*,/) @nany(4, i -&gt; op_i(a,b) == c) end a, b, c = rand(Int,100), rand(Int,100), rand(Int,100) @btime func_generated.($a,$b,$c) Which gives us: `1.180 μs (8 allocations: 320 bytes)`
That was a mistake on my part, due to ignorance of the nature of the delay. Loading matplotlib and showing a plot in python takes 1 second on my machine, vice 25 for Julia/Plots
I will take you up on that. All my stuff is backed to github so a PR is possible even. 
Plots in particular is the main issue. It just needs to nospecialize more things. Makie.jl is the next generation plotting package and it fully precompiles, but DiffEq isn't setup with it yet. Soon though. We are working in chats to update our recipes
That package looks great from the GH page. Currently won't install due to an error building `ImageMagick`.
It's taking 22 seconds before catching syntax errors: These should be caught immediately.
Just tried it on Windows. I won't call it fast. It feels lethargic just like other plotting libraries on Win10 laptop
Did you compile it to the system image?
Lurker here. Yep, that works. Thanks!
No. Didn't know I had to do that. So Pkg.build doesn't do it
Unless you have a certain field or application in mind, a lot of the core structure and "style" of Julia is covered in [Think Julia](https://benlauwens.github.io/ThinkJulia.jl/latest/book.html#_preface) , have fun !
Thanks ChrisRackauckas
Some talks from JuliaCon: https://www.youtube.com/watch?v=XWIZ_dCO6X8 https://www.youtube.com/watch?v=7KGZ_9D_DbI https://www.youtube.com/watch?v=Z2LtJUe1q8c&amp;t=1303s
Go into the Julia-client package settings and enable fallback renderer
I am trying PackageCompiler with Makie, but I came across these instructions: # If Julia doesn't start for you anymore, consider doing: # using PackageCompiler; PackageCompiler.revert() &lt;- not well tested How would you be able to issue these Julia commands, when Julia does not start any more :-) Maybe that's why the author mentioned "not well tested" :-)
That did the trick, thanks very much. I thought I remembered trying that in the past and it not working. Maybe not or maybe updates since then have helped. Thanks again!
Awesome, now I have something new to play with. 
You’re welcome. I was going nuts over this same problem a month or 2 ago and one of the developers helped me solve it. It was indeed a combination of updating the atom packages and enabling that fallback renderer. Happy computing!
I hate how it's the same thing over and over again, they need some next step tutorial
You need to do: convert(Union{Float16, Nothing}, nothing) Nothing is the type of nothing, you are trying to convert the type itself (instead of the value that has this type) into another type.
Whoops. Thanks!
Btw: Julia looks like it's shaping up to be a dominating factor for numerical computing in a few years.