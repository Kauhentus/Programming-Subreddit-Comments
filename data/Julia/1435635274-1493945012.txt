Great writeup! I think a link to this would be of interest to a lot of people on julia-users, some of whom might not read reddit. I'm curious if you've benchmarked your library against Parsec or some similar library -- wondering about the overhead of the stack and dispatch system in particular.
Yeah, it sounded like the author didn't know how floats are supposed to work.
Most of these are unsurprising, but from a mathematical perspective I'm not so sure that Inf==Inf is such a great idea considering there are so many different infinities. Although I can't think of a single case where I've had problems because of this behavior in any language, and MATLAB does the same thing. Still, maybe it would be better to raise an error when comparing infinities so you would be forced to deal with the issue each time in a way that makes sense for the problem. What do you (likely more experienced programmers) think?
Awesome, look forward to going to these again! Thanks for prodding! There was some talk of a weekend julia "hackathon", this fizzled out, but IIRC there was some interest.
I think if you actually wanted to work with different cardinalities of infinity, you wouldn't use the floating point numbers.
He's using BigInts, which allocate sizable GMP data structures for all operations. Also note julia&gt; a = BigInt[1 1; 1 0]; julia&gt; @which a^10000 ^(A::Array{T,2}, p::Integer) at linalg/dense.jl:170 And looking at the source code there (I'm on master, 2cbb72a) that uses `power_by_squaring` so does O(log n) operations instead of the O(n) loop.
That threw me for a loop a while back. To force integer division you have to use div(0,3)
I could not agree more! What Julia needs most sorely is documentation and tests. I have major trust issues with a language that fails it's own test suite half the time. A programming language has to be dependable and understandable. I really really hope hat Julia can outgrow this, and emerge mature and stable with great documentation and tests. If not, I have a hard time advocating Julia to my colleagues, who are less willing and able to work around regressions and dive into the source for documentation.
in my experience, making temporary variables to make the code "cleaner" leads to significantly more memory allocation and longer run times. it's very easy to do. you can't be sure if it's a fair comparison unless you see the code.
This was written 7 months ago, things have gotten considerably better since then. A handful of really motivated contributors like Katie Hyatt can make a noticeable difference here - if you want to see things get better, don't moan about it, just pitch in and help make them better. Most of the CI failures Julia hits are intermittent problems that people have a hard time reproducing locally - not easy to debug. And Julia doesn't really follow classic git flow, the stable branch is the previous release branch. If you expect a stable experience and aren't contributing to development of the language, you shouldn't be using Julia master.
That is great news! Thank you for the heads-up!
you understand it's still in development? of course it often fails tests. it's often being changed! it really sounds like you (and your colleagues) should wait for the 1.0 release.
The future is without the `@doc`: """ # docstring with *markdown*. """ function f() # blah end Docile can already read this, and I think it's working in base right now.
You understand how testing and branching/merging works? You don't check broken or untested code into master. That's what development branches are for.
i really think you'd be happier with a different language / culture... why use a language you're indignant and butt-hurt about? honestly. you clearly have different priorities / style / attitude. why get an ulcer? go and do your own thing somewhere else.
I don't even know how to respond. I *love* Julia! It is an awesome language, that is developing rapidly into the next big thing in my field (scientific computing). At some point, the totality Julia will grow beyond the carrying capacity of single brains, and at that point, tests will have to make sure that changing one place does not break other places. Similarly, Julia will attract more third-party development, and documentation is necessary so *they* don't break existing code. Thorough testing and comprehensive documentation are safeguards against human error, and provide guidance for future developers. They are not a question of "style" or "priorities" or "attitude", but necessary scaffolding for growth. And as you say, Julia is still in active development, so growing pains are to be expected. Getting a handle on documentation and testing is part of the process of developing a language from a cool new toy into a grown-up language that can be used for large-scale projects. I don't doubt that they will get there, eventually. And the sooner they do, the more likely it is that people will use it.
if you *love* it, why are you so dogmatic? why are you so indignant and assertive? why use entitled, self-righteous blackmail ("if you don't do it my way i won't recommend you to my friends...")? is that how you treat other things you love? or do you give them space to do things their own way? what makes you *so sure* you know better than the people who are involved? for that matter, why systematically downvote someone with a different point of view? possibly related: there's a thing called shu-ha-ri https://www.google.com/search?q=shu+ha+ri&amp;ie=utf-8&amp;oe=utf-8 - one way of understanding it is to see that experts are more open to change and variety than learners.
This is good progress but I would like them to update the performance chart comparing it to C at http://julialang.org/. 
I don't think that's likely to be included in base. Have you seen [Grid.jl](https://github.com/timholy/Grid.jl) and its soon-to-be-replacement [Interpolations.jl](https://github.com/tlycken/Interpolations.jl)?
Here are the few reasons why I want it more up to date: * No one really uses Python 2 unless they are maintaining legacy code. Python 3 should be used for the benchmark * Go is at 1.4.2 * Interested if there was any speed sacrifices in the new Julia version. * I think some people would probably want Scala and Rust for comparison
Wow, why are you so aggressive ? bastibe is just saying that documentation and tests are important, and that many potential users can be scared by poor documentation and lack of thorough testing. The Julia community seems agree with this, I think you know the issue numbers regarding this topic, and all the work being done. Even if 1.0 is not here yet, it is still valuable to hear input from people who are solely or primarily users, and there is no need to be so negative and aggressive. Let's keep it open and constructive! tl;dr; you are reading bastibe's comments way too negatively.
Well, having the division operator returning an integer when performed on ints can lead to weird results, too. I don't expect 5/2 to return 2, and having a function always returning the same type when given the same input types is more consistent.
I will look for it soon. Just try not to set it on the 2nd and 3rd Wednesday. Most people go to the python meetup.
So far I like the start it looks clean, my only question is the format. The python format tells you what you will get back in return (which I find helpful). 
Python 2 is still more prevalent than Python 3. In my environments the only time I get to play with Python 3 is if I am coding it by myself, while everyone else around me exclusively uses Python 2 so I must use it in all our collaborative projects. 
I just notice the reason why other than legacy code is libraries are STILL not all up to date. One of the annoying problem with python 2 is you have to put `.encode(utf-8)` on almost everything.
I don't know what's going on with the scrolling on this website, but it's awful enough to make the article unreadable.
https://github.com/mikeizbicki/subhask#subhask- Check out the comparison hierarchy.
The guy behind that site spams the hell out of LinkedIn. Don't give him reasons to start posting on Reddit as well. 
I recommend using [Homebrew](http://brew.sh) and installing Julia from there (instructions [here](https://github.com/staticfloat/homebrew-julia/)). Then you can run `brew update &amp;&amp; brew upgrade julia` to update. Your packages are not affected by Julia updates since they are installed in your home folder (under .julia/v0.3 or .julia/v0.4). To update your packages, run `Pkg.update()` from the Julia REPL.
My first post has instructions for installing on Homebrew. I don't know about Juno but I guess you can point it to your Julia install.
I haven't seen the second link. Sorry. May I bother you with an extra question? Why homebrew? What advantage does it have over the kosher dmg install? Because I really screwed my installations of python because of homebrew (I didn't know what I was doing, just copying and pasting commands)
With Homebrew I don't have to bother going to the website and checking if there's a new version, updates happen automagically whenever I update my other packages. You probably screwed your Python install because you did not know what you were doing :) I have 5 or 6 different Python environments on my machine and they all work. The trick is to update your environment variables accordingly (basically your PATH and PYTHONPATH) whenever you are using/installing stuff to a different Python environment.
Thank you sooo much. 
Yeah, I didn't know what I was doing. I see your point about easy updates. I will look into it. Thanks!
We've organized a meetup on [Aug 13](http://www.meetup.com/Bay-Area-Julia-Users/events/223908654/). Following is the lede &gt; We are pleased to have Jeff Bezanson, one of the creators of Julia speak to us on the future of Julia, as well as Kyle Barbary. Kyle will review the state of Julia astronomy packages and the small but growing JuliaAstro organization. He will review the prospects for adoption of Julia in Astronomy. Kyle is a Cosmology Data Science Fellow at Berkeley Center for Cosmological Physics. &gt; Jeff will speak on future directions and speculations in the base Julia language. He will present on the type system, higher-order programming, static analysis, and more. Jeff recently finished a PhD at MIT and is now helping to build Julia Computing. Michael also made me a co-organizer; I'd be happy to hear suggestions for meetings. For example we'd love to hear more JuliaCon talks when speakers visit the Bay Area; ping me on [meetup.com](http://www.meetup.com/Bay-Area-Julia-Users/members/6328024/) or send me an email at chris.peel at ieee.org 
Static package compilation just landed! Hnnnng! It's opt-in on a per-user basis for now, it seems.
It's in the roadmap in OP? https://github.com/JuliaLang/julia/pull/8745
If you don't mind: what is users behavior file? Thank you
I want to call the a Fortran library/subroutine (DVODPK) from Julia. DVODPK takes as its first argument a pointer to a function (which provides derivatives of some ODE you want to solve). Since DVODPK is called from Julia and the derivatives function is also written in Julia, I have to pass it a pointer to a Julia derivatives function using `cfunction`. Now, if I want to pass data into this derivatives function (which I do), in typical Fortran style DVODPK gives you two arrays RPAR and IPAR which you can fill with floating point and integer values, respectively, that the derivatives function can use. For performance reasons, I want to pass a bunch of references to arrays to the derivatives function. But IPAR is an array of 32bit integers and I'm on a 64bit system so they can't hold pointers. So the easiest way for me to pass pointers is to disguise them as floating point numbers by reinterpreting the 64bit address as a Float64 and storing them in the RPAR array. Then inside the derivatives function I convert these "floats" back into 64bit pointers and dereference them to arrays which I make use of.
2.7.x is very much alive. While most of the popular libraries from 2.x series have been ported to 3.x, there are still people who are used to using 2.x and such. I think 3.x will gain some tractions now that most popular libraries have been ported. But I disagree with your assessment that it's just for maintainability, it's for libraries, user base and familiarity mostly imo. The only instances that I've seen for maintainability is redhat promising to maintain 2.4 series for 10 years or something as part of their version support package plan, IIRC from an article. 2.7 is the last in the 2.x series btw for new features. I know this is anecdotal but most programmers I've talked and worked with chose 2.7.x over 3.x . I've been out of the market for a little over the years though, I doubt that preference have changed. The rift between 2 and 3 is still there, and I think it'll be there for a while. It's been there since the first release of 3 and will be here for awhile still. 
Have you put anything in `.julia` yourself? If not delete the directory abs try again, if so trying deleting `.julia/v0.3`.
Cool, I'm glad it worked. 
You're welcome!
Macros don't actually dispatch on their argument types; the type annotations in your signatures above serve only as assertions. Indeed, it is convention not to include any type annotations in macro definition argument signatures. In that case it is certainly possible to pass literals to a macro: julia&gt; macro global_const(name, value) info("@global_const: $name = $value") end julia&gt; @global_const n 20 INFO: @global_const: n = 20 Macros can indeed be exported from modules, but you export the name with the `@` prepended: `export @static_global_const`. As far as your particular use case is concerned, you can actually achieve the same effect with a `let` block: const n = 20_000_000 let const tmp1 = Array(Float64, n) const tmp2 = zeros(n) global fn_with_static! function fn_with_static!( input_array :: Array{Float64,1} , output_array :: Array{Float64,1}) fill!(tmp1, 1.000) fill!(tmp2, 0.234) for i=1:length(input_array) output_array[i] = input_array[i] + tmp1[i] + tmp2[i] end end end function run() input_array = ones(Float64, n) output_array = Array(Float64, n) fn_with_static!(input_array, output_array) @time fn_with_static!(input_array, output_array) end This gives me julia&gt; run() 118.123 milliseconds Using the `let` block is probably preferable, since it is simpler and truly does not create any global variables. EDIT: formatting blargh
 function fn_with_static!( input_array :: Array{Float64,1} , output_array :: Array{Float64,1}) @static_init tmp1 Array(Float64, n) @static_init tmp2 Array(Float64, n) fill!(tmp1, 1.000) fill!(tmp2, 0.234) @inbounds @devec output_array[:] = input_array + tmp1 + tmp2 end let const tmp1 = Array(Float64, n) const tmp2 = Array(Float64, n) global fn_with_let! function fn_with_let!( input_array :: Array{Float64,1} , output_array :: Array{Float64,1}) fill!(tmp1, 1.000) fill!(tmp2, 0.234) @inbounds @devec output_array[:] = input_array + tmp1 + tmp2 end end I added `@inbounds @devec` but this change is reflected in `fn_without_static!`.
Hmm. I don't know. Looking at julia&gt; macroexpand(:( @devec output_array[:] = input_array + tmp1 + tmp2 )) quote ##len#6603 = length(output_array) for ##i#6604 = 1:##len#6603 output_array[##i#6604] = Devectorize.get_value(input_array,##i#6604) + Devectorize.get_value(tmp1,##i#6604) + Devectorize.get_value(tmp2,##i#6604) end end suggests that it might have something to do with the behavior of `Devectorize.get_value`, so you could investigate that. Another option is to use `@code_typed fn_with_let!(input_array, output_array)` and `@code_typed fn_with_static!(input_array, output_array)` to examine and compare their intermediate-level representations. 
When I was working in Julia heavily last year, this was my biggest issue - no good tooling (that I could find). I tried Juno, but it was too foreign and too buggy. So I stuck with Vim and printf-style debugging, which really made things a pain. Things aren't perfect on the Python side of the world, but at least they have some decent tooling in place. I'm well aware that Julia is young, and all this will come. Just wish it was here already. :-)
If you're already familiar with R and Emacs Speaks Statistics, Julia and Emacs Speaks Statistics works most the same. Bigger picture, if you're a language polyglot, Emacs is well worth learning. It supports everything, and usually pretty well. I say this as someone who clocked ~15 years in vi/Vim and only about 5 in Emacs. IDEs can also be great for a handful of languages, but you'll wait a while before a mature IDE supports up and coming languages like Julia. For me, Emacs is something of a local maxima--offering the most creature comforts short of an IDE, but supporting everything under the sun. There's a new Mastering Emacs book that I'd get after spending an hour with the built in tutorial to get the basics of movement down. You'll want / need to customize your Emacs.
My general Julia editor is Atom with the Hydrogen plugin which lets me interact with an IJulia kernel from inside the editor. After you install IPython and IJulia on your system, the general workflow is just open a .jl file, and execute a line of code with Command-Alt + Enter to display the output inside the editor. It will also display plots and such. I will generally use Atom for small calculations/projects, or prototyping parts of larger projects, and then switch to good old command line debugging to polish out the final few bugs. I prefer Atom+Hydrogen over Juno because more people are developing themes and plugins for it. For instance, the latex-completions plugin does the REPL thing where \alpha completes to α. 
I used to use Julia-studio all the time until they stopped supporting it. Tried Juno but not. A big fan, still looking for a decent IDE. Sounds like emacs/vi/vim might be best bet in the ear future
I use vim and the Julia terminal, nothing else.
By the way did you also see [this mail](https://groups.google.com/forum/#!msg/julia-users/W-H6ZxjXYSI/RWnc6bk6e-EJ)? Probably the most complete write up.
That's the solution that didn't work for me! I found some more information in the docs than I think there used to be. The docs [ here](http://julia.readthedocs.org/en/latest/manual/packages/) give a little more information than they may have used to when I was trying it before. They have some instructions for offline installation. 
Still not working. I downloaded the ZIP file and just created the directory myself. The packages are where they belong but I can't Julia to recognize them. I think I need to create a REQUIRE document, but I don't know what that should look like.
So... I used pmap instead of DArray to parallelize. Below are some timings: 800x600 with 1200 iterations pmap: 69.60744009999999 DArray: 74.20881680699999 800x600 with 5000 iterations pmap: 122.60959561899999 DArray: 231.68992265400001 1920x1080 with 1200 iterations pmap: 320.675840137 DArray: 330.68787382899995 1920x1080 with 5000 iterations pmap: 546.000590829 DArray: 1011.809776215 ~~I think~~ it is faster because it spawns a process for each pixel if there is a worker available instead of giving an equal chunk of the image to each worker. This way it scales better if using multiple computers with different performances. What I noticed from those runs is that copying data around and spawning the functions on the remote workers takes a lot of time. Larger images with a lower iteration count will be faster with DArrays while pmap will be faster with higher iterations since the copy time will be less significant when compared to the time the worker was working. Instead calling a function to just a single pixel, calling a function to process a small chunk if pixels (5, 10, 50, idk) would require moving less data. Maybe calling a whole line per worker. This way less time will be wasted spawning things and moving data across the LAN. For small iteration counts the CPU wont even reach the full clock speed since it don't spend enough CPU time. Considering that as you zoom in one needs a higher iteration count, pmap is the way to go here. You wont need a increasing resolution as you zoom in. I wonder how else it can be done. Maybe using @spawn and using isready(r::RemoteRef) to see if there is an available worker. The pmap code is available [here](https://github.com/h3nnn4n/julia_mandelbrot_set/blob/master/pmap_mandel.jl) -- edit -- I have a couple of old computers laying around in my room and I decided to put them to use. I took and old celeron, installed arch linux, compiled Julia and used it to help the calculation. Using the old computer did speed up the time from 493.175 to 465.248 seconds, which shows how pmap is superior when working with computers working on very different speeds. The old computer didn't hold the fast ones down. 
I use sublime text for code and iJulia for testing. Tried juno it was buggy and not much support. I have found nothing that graphs stuff and displays it well.
Someone asked a similar question about game dev a while back, and the second half of [my response](https://www.reddit.com/r/Julia/comments/2zzbin/julia_game_dev/cpnuhlx?context=3) is still pretty relevant. In short, go for it if you're interested in playing around with the webstack and figuring things out for yourself – it's a great learning opportunity – but if you want to throw a web app together without the tooling getting in your way it's not the right choice. (Whether or not it becomes a great choice for that stuff in future is up to people like you ;)
After some hair pulling I figured out that My smooth colouring was working right and my histogram normalization was messing it. So I disabled it. I also implemented an anti aliasing, which worked rather well. Here is a sample: [1920x1080 with anti aliasing](http://i.imgur.com/z7iUdvz.png) [1920x1080 without Anti aliasing](http://i.imgur.com/lqVYllw.png) only the pmap version has anti aliasing.
Another tool that can be indispensable for prototyping and scripting is the Jupyter (formerly IPython) notebook, although the biggest disadvantages of it at this time are that you have to use a browser, you can't easily configure keybinds, and the save format is JSON so it's not accessible outside Julia. It's pretty useful if you have a VPS though. If you don't, it's very easy to use https://www.juliabox.org/ Though as of now, Atom + Hydrogen seems has the most ecosystem support.
So the Angels WERE Dragons!!! Well, shit. Then, does that mean Jesus is/was a Dragonborn?
Julia FTW \o/
I have the same problem (in a mac). I got ERROR: symlink: file already exists (EEXIST). How do you delete .julia or .julia/v0.3? I cannot find it in Finder. 
I parallelized the [code](https://github.com/h3nnn4n/Juliaception/blob/master/lines_pmap_julia.jl) using pmap but splitting lines instead of pixels and I got a *major* speed up. The running time for a 800x600 image: 43.634030470000006 The running time using the line splitting for a 1920x1080 image: 24.049079525999996 
This started as a community project on [github](https://github.com/chrisvoncsefalvay/learn-julia-the-hard-way) but has mutated into a solo (commercial?) enterprise. There's also Leah Hanson's forthcoming Julia book, so the rush has started. There's currently one Julia ebook available at present, so there will soon be three! With Julia's rapid pace of development, though, a paper version will quickly become out of date.
AFAIK you need to unzip the zip file. You can do this into memory instead of to a file, if you have enough ram, but either way you need to unzip the file.
Python: def read_zip_file(filepath): zfile = zipfile.ZipFile(filepath) for finfo in zfile.infolist(): readerData = csv.reader(finfo) return list(readerData)
Is `github.com/fhs/ZipFile.jl` not useful? I've not used it myself. 
They must've been paid upfront... :)
In the past few days there's been more and more discussion among core developers of switching the `VERSION` string from `-dev` to `-pre`. Though it seems imminent, I cannot predict precisely when it will happen. EDIT: see for instance [here](https://github.com/JuliaLang/julia/issues/11536#issuecomment-129296975).
I found on Github [1] that the processes on different nodes need to be able to communicate without ssh tunneling. All my serves are behind a firewall, so this is not possible. I guess I'll have to wait until the fix on the github link is pulled... https://github.com/JuliaLang/julia/issues/6256
Jeff Bezanson's talk: "The base language, future directions and speculations"
Great presentation. I have very high expectations for Julia. I want to see it even greater and better. Why is the comments turned off for the video? D:
And now a lot more! https://groups.google.com/forum/#!topic/julia-users/RzEn0PoOFp4 https://www.youtube.com/user/JuliaLanguage
Very basic setup: Sublime Text + REPL. That's it.
Now I cant speak on youtube about how cool juliacon is D: 
well if you know that the parameter is fixed just fit against y' = y - fixed_par * var1 ~ par2 * var2 + par3*var3
In an linear model, sure, but that isn't going to work when the linear index is inside a non-linear link function such as in the logit model. 
in that case you will probably have to use a penalty function in the optimization/lagrange multipliers 
It is a shame that on some videos only the talker was recorded, but not the slides :/
Thanks, `Task` is a better solution. I've seen functions that return a `Task` type, and then I've seen functions like yours that only use `produce` and don't return anything (rather, they return `nothing`). Is one style preferred over the other?
The Task is pythonic, but not very Julian. I think your original solution is great! A custom iterable type is *way* more efficient than a Task.
Agreed, a custom iterable type is more efficient (by about one order of magnitude, on my computer - the context switches seem very demanding ^(^for ^now)). I don't think it's un-Julian per se, I like to use Tasks for exactly this sort of thing (lazy iterators).
You can do the same with Julia or any other programming language. You save as a .jl file in julia and you can open the file again. You can also create a module/package where you can call you your function and run it.
Saw you posted on /r/Juno but I'll reply here since it's more visible. Unlike R or Matlab, Julia won't generally load code automatically for you. (The exception to this is juliarc.jl, which you can open with the "open julia startup code" command from Juno/LT. This is useful for utility functions, but you probably don't want to work on projects in this file.) The best way to make your code easily accessible and reusable is to put it in a package (you can find docs on that in the Julia manual). Then you can just load it from anywhere with `using MyPackage`. The other option is to keep your working code together in a folder, and have a top-level file which `include`s all of those sub-files. If you open the top-level file and press `Ctrl-Shift-Enter` to run it then it will load all of your code at once. (This is basically just an ad-hoc package.)
Thanks- It looks like packages are the way to go. Hopefully I can begin to actually do some real things in Julia by way of Juno. Cheers!
What are you saying about context switches? Are you saying that "generators" or `Tasks` will become more efficient eventually?
I wonder why not focussing all energy on fixing/ completing one version (e.g. 0.4)? Why do people already work on 0.5 and 1.0?
Thank you so much for taking the time to answer this question. To be perfectly honest (just my 2cents), I find this approach confusing. I understand that there is one "next version" (0.4) which has defined items, plus a future version (0.4x) that has new features/ bug fixes. However, I am not sure why other versions are needed (next defined version vs. wishlist: yes/ no). You may say now that some people have different ideas that would make up a very different version? Then I would ask again why people are working on different versions. Also, when I look here, https://github.com/JuliaLang/julia/milestones, it seems that the other versions have closed tickets already (95 closed items for 0.4x, but 79 closed items for 0.4). So can you see that some people will think that the contributors work on several versions at the same time? Again, there may be some reasons for this, but it makes me wonder how structured the Julia movement is (I am wearing my marketer hat here - just concerned about market signalling). May I suggest the three labels: 0.4, 0.4x, and issues for discussion (because they do not seem to fit in with the present version). Sorry for being overcritical here. I am just trying to share my feedback (again just one opinion). PS: I am aware that the Julia product is open source - thanks for all the work guys.
As a software engineer, I don't see anything confusion about that approach. In fact I think it is a really good one. People are going to open issues (thankfully); sometimes they are about bugs, other times about improvements or new features. It is important to prioritize, but also to build up and communicate a long term vision with each other and the community. Also, I'm guessing that community members are going to work on issues that they understand and are interested in, regardless of it's priority in the overall agenda. So closed tickets for future versions don't surprise me. Bottom line, I really disagree with your statement. Then again I'm just a Julia user and am not associated with it's core development
The best milestone: [Powers of two](http://i.imgur.com/y003A1q.png)
I hope this is not received as too confrontation, but "why do you care" ? What I mean by that, is: if it helps the developers develop, isn't that reason enough? There are a lot of work to be done on Julia before they can reach v1.0. To keep things rolling, and to make sure that some of the recent changes are actually embedded in a "stable" version, there is a wish to finish v0.X versions now and then. We are now at v0.3 as the stable release. The next release is v0.4, and that already has a lot of new things, so it is quite important to add them to a stable release, such that Julia users and Julia developers do not diverge too much. So the community has said: a, b, c, and d needs to be done before we can make a new "final" (modulo bug fixes) v0.4 version. However, there are still some items on the wish list for the future. Some of them should be implemented in v0.5 (that is after work on v0.4 ends), and some should be implemented but are not so important, so they get a (for example) v1.0 flag. It's something we want, it's something we need, it should be in v1.0, and we need to discuss it now - but it's _not_ urgent.
I am sorry, but this statement does not help or answer my question.
Because different groups of people have different interests, even if there are specific goals for v0.4, v0.5 and even v1.0 (maybe kind of a wishlist). Sometimes 'someone' wants 'some feature' that is tagged for v0.5 or other version. Why not let that 'someone' work on that feature of his/her need? Maybe most of the goals of both version could be reached at once, and the project could jump right away to v0.5. All depends on the participation of the people. 
It answers the title and the "what the status for Julia 0.4 is?"
I've used Matlab for years but was able to completely migrate to Python/Numpy. I could get all the computational performance I needed by using cython and even compiling my own CUDA kernels and exposing the API to Python. I found all the libraries I needed for my applications and then some. However I am now starting to do my development in Julia, primarily because I don't need to bother with the cython step, and because it's array syntax is superior's to Numpy's clunky syntax. Moreover the functional language approach to numerical problems is far more natural. 
You can check the checksum of the file you downloaded against what we have listed on the website ([MD5](https://s3.amazonaws.com/julialang/bin/checksums/julia-0.3.11.md5) checksums, [SHA256](https://s3.amazonaws.com/julialang/bin/checksums/julia-0.3.11.sha256) checksums) if it doesn't match I would worry, but if your local file's checksum does match, it's a false alarm. The virus name that your scanner has given julia.exe denotes that julia.exe isn't matching a known virus signature, it's just raised some red flags due to some internal heuristic scanning done by the virus scanning engine.
Surprise me Python Pypy and Go is faster than Julia in some instances. But my guess the code is not written to be optimized.
Does it include startup time and compilation time? Seems like it.
The matmul code at least is not very good, a simple loopy version is more than 10 times faster.
Julia isn't statically compiled, it loads a lot of large libraries by default. More of those will be made optional over time.
Yup, it sure seems like it's constant-folded. Julia will constant-fold it, too, if `n` is constant: julia&gt; function f() a = 0 n = 500 for i = 1:n for j = 1:n for k = 1:n a += i a += j a += k end end end a end f (generic function with 1 method) julia&gt; f() 93937500000 julia&gt; @code_llvm f() define i64 @julia_f_21593() { top: ret i64 93937500000 } It's quite impressive that Numba and Rcpp manage to do constant-folding with a variable `n` (perhaps it's a "link-time" sort of optimization, in which case Julia will do similarly with an `@inline` annotation), but I'm pretty sure the benchmark is simply testing if this particular optimization happens. Meaningful benchmarks are hard.
What happens when you use types for this example in Julia? Then would it run on par with Numba?
When the times don't scale with `n` you should figure that the calculation is being optimized away... IMO This is why you should use python's timeit rather than rolling your own (presumably the same applies for R).
I agree. Also, as far as I know, Rcpp is not to R in the same manner Numba is to Python. Saying one can make R faster by using Rcpp, is almost like saying one can make R faster by using C++ instead, which is more or less valid for other languages which can somehow call C++ (Although Rcpp seems to make that really easy).
Tools like PyPy and Numba are really impressive engineering efforts, but getting Python to consistently achieve C-like performance is really, really hard, and remains an unsolved problem. (Even Google, makers of the incredible V8 JS engine, gave up on trying to do this.) Unfortunately, people are going to keep having to use C and Cython for anything non-trivial for a while yet. Then of course there's the classic 'R/Python is fast if you write C/++'. Well, yes, obviously, just like Brainfuck is great for web servers if you write Ruby. Crucially, since you're *not* using Python now you win performance at the cost of all the productivity benefits of using a high-level, dynamic, interactive language – when the goal of keeping both performance and flexibility is exactly the reason Julia exists. Admittedly, Julia misses a trick in this benchmark. Numba and C++ implement constant folding, which runs the code at compile time and effectively turns your benchmark into `f(5) = return 1308423` or whatever. The fact that the R and Python times don't go up with `n` should be a pretty big giveaway that you're not timing anything meaningful here. All this benchmark actually tells you is that Julia is running the code while R takes 40μs to call into C++ *and nothing else* – put it that way and it sounds a little less impressive. **Edit:** Worth pointing out that not only can Julia also do calculations at compile time, but it gives you complete control over it: macro f(n) f(n) end @time @f 100000 # Always takes 1μs If you were looking for an (albeit pretty meaningless) 40x speedup over R, there you have it.
hi all. im the person who wrote that blogpost. i think you make some fair points here, so id like to rewrite parts of the post because of it. meaningful benchmarks are indeed hard, but i am reading some good suggestions for improvement here. i am not against julia (should i have given that impression). in fact; i find it a very interesting project! i merely wanted to write this blog post to show that there are python or R approaches to achieve similar speeds without going too much overboard. some areas of improvement that come to mind after reading this thread: - firstly, it would be fair to check performance with constant 'n' in julia against numba/Rcpp. i wasn't aware of the constant-folding optimisation being a thing but it would be a bit bland to use this as an argument against julia's speed. so this needs to be added. - i suppose it is also fair to say that my benchmarking should be sharpened a bit (understantement). i naively put the numbers in because the results felt as if they instantly came in. using the 'benchmark' library in R should already help (Rcpp uses it to do it's benchmarks). if anybody has a good tip for python/numba id be all ears (i am using the %%time magic now, does %%timeit work differently?). what is the best method to time julia? - third, I am not at all a power user of Julia. more a curious individual who has dabbled with Julia in his spare time and has gotten curious about the project. in Amsterdam there isn't a meetup or a community of such that I could exchange thoughts with. I have an impression of the language that might be completely of; that julia is primarily being used in academia, less in commercial production. is this true? 
Numba pretty consistently achieves C like performance on numeric code.
Do you think Julia is ready for report based individual analysis IE not plugging into production systems? When will it be ready for either that or production? Is it stable on windows? 
Sure, you could use it for analysis and report generation today. If something's missing that you would be able to find in Python, there's always PyCall.jl. You could use Julia in production today if you're careful and you do a fair amount of testing. Keep the versions of everything pinned, test thoroughly any time you update dependencies. Windows support has some rough edges, not all packages work or are actively tested there, but that has gotten a lot better over time. If you need to integrate Julia with a Windows application that is built with Visual Studio, that might be tough (depending on how things are connected) and could use more work and testing. Julia is compiled using GCC (MinGW-w64) on Windows right now.
Constant folding is things like 2+3 -&gt; 5. I guess this optimization is called loop replacement, but that is a general name. I don't think this particular optimization has a specific name. I know that at least GCC can do this optimization, so it could be that those packages are using GCC under the hood. The math for computing such sums is actually quite interesting. It's also not trivial to implement as an optimization because you need to deal with overflow so that the formula has the same overflow behaviour as the loop.
this.. this is the right answer... you should submit this to the juno devs.
I would like to see a DeepDream example in Mocha.jl, but I haven't taken the time to learn enough about neural networks yet to do it.
Great stuff! Really well presented, and a great package it seems.
Humanism on speed
Numba is the only speed up tool I use, but it does require big rewrites at times. A lot of what makes numpy/python elegant and fun is not in the subset of numpy/python supported by numba.
Maybe I should go back to try it then. The functions I tend to implement were making heavy use of numpy broadcasting rules. Concretely, the last thing I accelerated was the following code: def right_hand_side_numpy(y, _): phases = np.exp(1.j * y[:size_of_system]) return np.append(y[size_of_system:], input_power - damping_coupling * y[size_of_system:] - np.imag(phases * np.dot(phase_coupling, phases).conjugate())) All of the variables other than y are closures, and so compile time constants. A factor of two faster than the numpy version is the one making use of scipy sparse: def right_hand_side_sparse(y, _): phases = np.exp(1.j * y[:size_of_system]) return np.append(y[size_of_system:], input_power - damping_coupling * y[size_of_system:] - np.imag(phases * phase_coupling_sp.dot(phases).conjugate())) To turn that into numba requires hand rolling the sparse matrix multiplication. But the compiled numba code beats the scipy/numpy version by another factor of two. What is not shown below is the necessary set up to define all the variables closed over. def right_hand_side_for_numba(y, _, phases, temp, ret): # phases = np.exp(1.j * y[:n]) for i in xrange(size_of_system): phases[i] = np.exp(np.complex(0., y[i])) temp[i] = 0 # temp = phase_coupling_sp.dot(phases) for i in xrange(number_of_links): temp[left_index[i]] += np.complex(phase_coupling_real[i], phase_coupling_imag[i]) * phases[right_index[i]] # ret[:n] = y[n:] # ret[n:] = input_power - damping_coupling * y[n:] - np.imag(phases * temp.conjugate()) for i in xrange(size_of_system): ret[i] = y[size_of_system + i] ret[i + size_of_system] = input_power[i] - damping_coupling[i] * y[size_of_system + i] \ - (phases[i] * temp[i].conjugate()).imag return ret Now, given that function calls to numba function from numba functions are fast, the additional code for numba would be worth it, if I would also finally sit down and implement an ODE stepper in numba. But here's the rub, now I'm rewriting library code in order to make full use of numba. This code was written a while ago though, I fully intend to keep using numba, too, and it will get less rewrite heavy with every project.
well, I mean 0.4.0-rc1 of course, not 4 :)
cool man
Not that I consider Julia globally not readable or understandable though. But more often than not, simplicity is chosen over "easiness". I've read just yesterday for instance that `using foo` will stay just because some don't like to go up and down their code to add explicit imports. These persons wouldn't use fully resolved names either... That's done in complete disregard of current best practices (this namespace pollution is usually forbidden or at least frowned upon in modern languages). In the case of Julia, that very pollution is indeed encouraged and constitutes the default import behavior. Yes this leads to simple code writing but **not** to easy reading or understanding! Yes there are checks to avoid importing the wrong function without noticing it, but that won't help anyone to understand the whole picture, quite the contrary. You write code once, but it's read many times. What if newspapers began releasing prints in the form of a very long stripe of paper??? It's simpler and cheaper to print! What if newspapers kept citing sentences from each others to cut down printing price? If sentences are cited explicitely, it's already a pain. If they are not and you are supposed to keep track of the citation order, no one would bother to read it. Some would use special devices in the hope these would help to read things more easily (if there are no bugs). I'm just saying that in this case, the editor/writers should do something to make the news readable for everyone without requiring a dedicated device. If that's a pain for the journalist, then let them write what they want but ultimately translate that to something everyone can read and understand. Stop giving the opportunity to write dreadful code for very little benefit. Implicit import in the REPL are understandable, but in anything lib-like, just do right now what everyone else figured out 20 years too late: implicit imports are bullshit. I'd like not to wait that long for Julia to figure that out. One suggested way to mitigate the obfuscation coming from that is the use of "proper" tooling/IDE to **read** code. Tooling is here to help people write programs, not reading them (or this is a proof that the language is rotten, *I look at you C and Fortran*). If you really don't like explicit imports nor full names, why don't you **write** an IDE pluggin to suggest autoadding explicit imports? That's what IDE are for. But please, don't trade code clarity and expressiveness for a very limited advantage in code writing simplicity: if you find something painful to write, pick an IDE. If you find something painful to read, pick another language. Another way would be to let julia rewrite implicit imports as explicit import a la gofmt.
Remember what Julia's focus is: scientific computing. A lot of Julia code is going to be throwaway code. Plotting a graph, running a small simulation, that kind of thing. Increasing the friction for those kind of tasks in the name of good software development practices is not a good thing. Java-style IDE that address the shortcomings of the language by expanding into massive amounts of code is neither good for readability nor for writability.
It's also true that a lot of Julia code is expected to not be throwaway code, which, in any case, if often not thrown away. Calime's complaint about namespace pollution could also be ameliorated by allowing local imports a-la D, which would be a small change to the language. 
The answer to simple performance questions is "It depends, you must measure", and "Use knowledge about the system to ensure that you measure what you think you measure". Precompilation will certainly help because we can cache the parsed version of the source, but it is it is computationally impossible to compile all the methods for all possible arguments for a generic function. If the module contains proper precompile(func, (argT, argX)) calls, you might be able to avoid hitting the JIT on first execution, but I don't know how to get that list.
Thank you for the answer, but I may need some further clarification. I understand that if all my functions allow for arbitrarily typed inputs, it is impossible to compile all possible versions. The reason I am asking is, that it seems impractical for the user to have to run everything once, and then run the calculations that they need. Say I wanted to estimate some statistical model with a lot observations and features, then it would seem weird that the user would have to run: dfs = small_data_set fit(dfs) df = real_data_set fit(df) if the user wants the second run to be fast. Often I might just want to estimate a model once, but it might be very slow if the functions involved have not been used yet. But as far as I can understand from your post, this can to some extent be handled by carefully specified pre-compilation?
Agreed. This kind of complaint should be addressed by a lint-checker, not baking Java-style bureaucracy and inflexibility into the language. Users of Julia will want to work fast and clean up later. Users writing heavy-duty mathematical code (like myself) will be especially annoyed if they are constantly forced to write all the explicit namespace boilerplate they have to write in other languages. It makes the code look ugly and un-mathematical, which is really annoying in early development and prototyping.
First, again no offense intended, but **stop telling me what Julia users ARE supposed to do**. Or back it up with proofs (scientific ones, not what you do or what your coworkers do). The problem with such belief (i.e. everyone/the majority of users is working as you said) is that you use it to justify things. Let politics justify things out of thin air: bring proofs or don't say what "users of Julia will want". As well, if that was really in the mind of Julia's creators, why developing Julia in Julia? It's a big code base. Why allowing the users to split a module into different files? For throw-away code? Really? Why spending time to provide compilation ahead of time? Yes, some modules are slow to load, but why would they, since they should be small? In any case, these modules could be added to Julia sysimg... ----- As I said, no offense intended: "mind-reading" is a common bias and I, too, fall for it a lot. :) ------- Now, that said, I don't mind if people prefer fast iterations/coding for throw away stuff. This is fine. But currently in Julia there is no distinction between "quick and dirty" and "general good coding practices". And more often than not, Julia design tends to favor ease of use instead of good practices. There is a world between recommending horrible practices and Java's view of it. Basically my point is: - most of current Julia's decisions, justified by ease of use, were designed with single-person-coded throw away code in mind; - there is no proof that people want nor will use Julia this way (actually if Julia live up to its claims, it can become far more than that); - some bad coding practices are currently recommended/forced upon users (implicit imports being one of them); - when the bad design is specifically underligned, it is discarded by referring to what they know people want/need, by proper tooling or by reffering to the reason why they did it or one advantage it provides. So it seems that Julia devs are not really good at solving problems globally instead of locally (this is a bias shared with all of us humans :)). With the same behavior, I could solve many problems and get away with it: people throwing junk on the street? why not allowing other people to cut the hand of the offender? That would solve the problem: people would be too scared to throw away anything even. As a side effect, it's much harder for the offender to do it again ! Now mayor, you know a very oractical way to help keeping your streets clean ! :D Jokes appart, if you find this stupid then maybe you'll see the problem with the current way people justify Julia's design. More importantly, this failure to see problem chains and the complete trade-off for each design decision lead to poor design. Remember that there are usually many ways to obtain the same effect. Also keep in mind that as far as problem chains are concerned, it is far less troublesome to cut the chain at its roots instead of patching the leaves... Patching the leaves leads to more overall complexity (here complexity in the language) and introduce new problems that will need to be patced. Julia devs: please wake up and look at Julia from afar! I really don't want a committee-designed language. Like you (or so it seems), I want a practical language to get things done. Python succeeded: it is easy to read, easy to write and while they tried very hard to be easy to use, they also succeeded in conveying most of the basic development best practice. Until now, Julia failed to do it, yet it is possible. Gosh, for this matter (and some others), the go approach would work very well: make Julia more clearly defined. Just don't allow implicit imports in anythin but the REPL **but** provide a jlfmt tool which would transform sloppy source code to proper Julia before executing it. This would allow people to write their shit at home and proper format everything before release. A jlfmt tool would also expand implicit imports to explicit ones without having to do anything manually. This would also enforce the basic best practice related to code formating without manual intervention. This is close to the linter thing but more automated. This is also one idea, there are many others... Then why resurrecting 30-year-old bad practice?! ----------- NB: implicit imports are only the tip of the iceberg. I find Julia too complicated for its own sake. Of course, not as much as C++ and related dialects, but a lot of it comes from either bad design or failure from evaluating trade-offs...
Ah, I get it. I was just being stupid there. I thought the code would actually run slower the first time, but of course it is only the compilation that increases the time. In my silly mind I thought all the involved functions would be "slow" the first time, but that not the problem. If I understand you correct the following happens: I load GLM and run fit(...) after some data processing (say there is no pre-compilation in GLM). If I run fit two times in a row, the second time around, the total time will be less. However, this is only because there is an initial compilation, all the computations take the same amount of time both the first and second time around. I have no idea why I had never seen it like that. I guess I should dig into the docs and read some more.
Ok, I used the answer to go even deeper without even answering. Let me fix that. I'm also a scientist. Of course, when writing math-related code, it's much better to write function directly. One could argue that explicit imports or fully resolved names are uneeded or less readable. And I **agree** (well, mostly... m.cos does not seem that horrible). The problem is not that you should not implicitly import anything. The problem is that implicit imports are the standard import way: *using* is used everywhere and you can't get a public/private separation if you are not using it. *using* is also the default recommended everywhere. So while directly using *cos* is no big deal, having modules not exporting a *open* function because it clashes with the one in Base is a pity. So basically, with the current system, people are acting like in C... Either by not exporting stuff (therefore, you can't tell what is public or private) or by using prefixes... What were the namespaces made for, again? However on the top of that, with the current system, you can still have clashes with math functions... Fortunately, these shouldn't fo unnoticed, but then you have to figure out how to correctly import stuff... 
Your energies would be better directed to code, documentation, and tooling contributions than rants on reddit. If you want to see practices change, help change them. As one particularly good example - people will use an auto-formatter once someone writes one that works well. Lint.jl already exists but isn't widely used yet, a few PR's to important packages to encourage incorporating Linting into their development workflows/CI/testing/etc would be almost certainly be welcomed. It's just a matter of software engineering hours.
I'm suggesting (by using the implicit import example) to stop a little, step back and try to get a better idea of the whole picture. You suggest to ignore that (for whatever reason) and to implement what I proposed right away to solve this local problem. So basically you are suggesting to fix a road because my bicycle manufacturer chose to sell bikes without a tire, just with air chambers. I'd rather find the real problem to fix instead of fixing what seems locally fixable. *I know I'm the one who suggested the formater, but that's to counter fallacious "arguments" like "nothing can be done, just use import and hope others will do". By providing an alternative, we move from " nothing can be done" to "do it yourself", but it's still not good enough because I don't even know if that's a reasonable answer to a real problem.*
The import system in Python is definitely very flexible. Most of the complaints I've heard about Python are that the "best practices" there lead to things being so heavily namespaced that you need to add new, highly granular imports any time you change your code in the slightest. There's some happy medium between there and the no-namespaces-whatsoever world of Matlab (or R I think?).
That seem to do it! Thank you so much. If you havn't, I can file an issue, and see what the community thinks should be the result of hcat of a dense and sparse matrix. edit: I opened an issue: https://github.com/JuliaLang/julia/issues/13130 
There's a pretty strong desire to avoid bifurcating the way the language works into "REPL Julia" and "library Julia." Remember the reason Julia exists is to solve the 2-language problem.
Transhumanism, then? I can see that, yes.
https://github.com/JuliaLang/julia/blob/release-0.4/NEWS.md
import statements can be made in a block, restricting namespace pollution. Silly example below pulls the import into an if. Many languages besides D have such a feature. void main() { bool verbose = true; if (verbose) { import std.stdio; writeln("Hello, world!"); } } 
Read over the [parallel section](http://julia.readthedocs.org/en/latest/manual/parallel-computing/) in the [Julia manual](http://julia.readthedocs.org/en/latest/manual/) but also check out this [great post](http://stackoverflow.com/q/27677399/803954) 
I get it. So it seem like writing `f{T&lt;:AbstractString}(d::Dict{T,Float64}) = keys(d)` would be much more reasonable. I didn't really think about what types were concrete vs. abstract before... just that i was dealing with something string like. Is the best way to tell if something is a concrete type to just use `subtypes(T)`?
Correct me if I am wrong, but I believe that it's also worth mentioning that parametric types are invariant. So even though you could pass an ASCIIString to a function which takes an AbstractString, you can't pass a Wrapped{ASCIIString} to a function that expects a Wrapped{AbstractString}.
Possibly, but conventionally the names have `Abstract` in them, so that's a pretty good indicator without having to write any code. Do you have a need to programmatically determine if a type is abstract? From my experience, I'm only using Abstract types when writing functions, I've never needed to check for one.
Thanks. I initially wrote my function signature with String not AbstractString. No need to programmatically determine if something is abstract... just to help me reason through what I'm doing. Thanks
Well, numba treats closure variables (variables that are in the local scope at function definition time) the same as global variables: as compile time constants. So if you do a = 5 @jit(int32(int32)) def f(x): return x + a b = f(5) a = 0 c = f(5) then b and c will both evaluate to 10. The global variable becomes frozen at compile time. The same happens with closed over variables. This is nice, so we can compile just the function we need at run time.
&gt; If it were a magic bullet general-purpose solution to how slow Python is, wouldn't it be more widely used? How do you assess how widely it is or isn't being used? Cloudera has used blogged about using it with Impala. Many, many research labs and academic departments are starting to use Numba and its commercial version in their research, to target GPUs. Not everyone tweets or blogs about what packages they're using. :-)
Hi Peter. Very true, public download numbers would probably be the best gauge, if those are being collected via conda (or pip? 8200/month according to pypi). The most worrying thing I've seen about numba is that it apparently doesn't handle user-defined types? https://groups.google.com/a/continuum.io/forum/#!topic/numba-users/TVal1Xfw6kw Being fast on basic scalar built-in types and dense N-D arrays thereof is a starting point, but library authors need specialized data structures.
I believe that it refers to optional type annotations. Long story short: If you choose to provide a type annotation (&lt;arg&gt;::&lt;Type&gt;) to a function argument, they can provide context during compile-time for optimization or error-checking (type-safety). example: foo(bar) = ... foo(bar::Integer) = ... both may do the same thing, but the latter tells the compiler that foo only takes integers.
Type annotations will do nothing for your performance. Firstly, you are summing over rows instead of columns. Julia is column major order so summing over rows will lead to big cache misses. Secondly, you are using vectorized operations that are quite slow in Julia. Fortunately there is the Devectorize package: see https://github.com/lindahua/Devectorize.jl Thirdly, for a tight loop like this, it might be worth to turn off bound checking with the @inbounds macro. Original code: julia&gt; A = rand(10^3, 3) julia&gt; @time dmatrix_julia(A); 0.289717 seconds (5.47 M allocations: 296.921 MB, 28.33% gc time) This version: Pkg.add("Devectorize") using Devectorize function dmatrix_julia_opt(xyz) n = size(xyz, 2) dmatrix = zeros(n, n) @inbounds for i = 1:n for j = 1:i-1 @devec r2 = sum((xyz[:, i]-xyz[:, j]).^2) dmatrix[i, j] = sqrt(r2) dmatrix[j, i] = dmatrix[i, j] end end return dmatrix end julia&gt; @time dmatrix_julia_opt(A'); 0.005364 seconds (10 allocations: 7.653 MB) which is 50 times faster. And they give the same answer: julia&gt; norm(dmatrix_julia(A) - dmatrix_julia_opt(A')) 0.0 So now you can do stuff 50/3 = 17 times faster than Numba :)
It's also worth noting for future reference that *both* generating a subarray via `getindex` (e.g., `xyz[:, i]`) and using the pure operator `.^` allocate new arrays. That's why avoiding them as above helps to alleviate the performance difficulties. 
The former, however, will likely change for Julia 0.5 - returning a view instead of a copy.
As far as I have understood, in v0.5 it should loose the singleton dimensions automatically, but what until then? What is the best way? Edit: lose...
Use `vec` julia&gt; vec(sum(A,2)) 3-element Array{Float64,1}: 3.54732 5.35543 4.39051 This will not allocate any extra data but instead give you a "view" of the same data as if it was a Vector.
You can use the squeeze function to get rid of singleton dimensions. You do have to specify which dimension though. squeeze(sum(A,1),1) squeeze(sum(A,2),2)
https://github.com/JuliaLang/Interact.jl Is a good source of inspiration. 
Thanks. It looks like it requires IJulia. I'll have to see how it goes.
I've literally just installed it and am running through a few things. 
Just for completeness, you can get a much faster numba version if you unroll the loop instead of using array slicing. Below I have the original implementation and `dmatrix_numba2` which unrolls the inner loop. They produce the same result, but for an array with shape (1000,3) the first implementation takes 2.5 seconds on my machine, while the second takes 6 ms. I'm using numba v0.21.0. @nb.jit def dmatrix_numba(xyz): n = xyz.shape[0] dmatrix = np.zeros((n, n)) for i in range(n): for j in range(i): dmatrix[i, j] = np.sqrt(np.sum((xyz[i, :]-xyz[j, :])**2)) dmatrix[j, i] = dmatrix[i, j] return dmatrix @nb.jit def dmatrix_numba2(xyz): n = xyz.shape[0] m = xyz.shape[1] dmatrix = np.zeros((n, n)) for i in range(n): for j in range(i): for k in xrange(m): dmatrix[i,j] += (xyz[i,k] - xyz[j,k])**2 dmatrix[i, j] = np.sqrt(dmatrix[i, j]) dmatrix[j, i] = dmatrix[i, j] return dmatrix Also, for reference, I downloaded Julia 0.3.11 and ran the devectorized code posted by Furrier (https://www.reddit.com/r/Julia/comments/3moyw6/computing_distance_matrix_with_julia_and/cvgvnk2) and `dmatrix_julia_opt` took 6.7 ms on my machine using Kyle Barbary's 'timeit' pkg. It's nice that Julia has the devectorize macro, which is a nice convenience, but it looks like numba can beat it (at least on my machine) if you know what you're doing and unroll things by hand. 
If you 'devectorize' numba by hand, it's very competitive with your Julia implementation. See my comment (https://www.reddit.com/r/Julia/comments/3moyw6/computing_distance_matrix_with_julia_and/cvm3x09)
There is data in the url. So I would need to write a parsing script in another language. Which would be fine, but I feel like I shouldn't have to do that. 
How is your code organized? Is it not possible for you to put it in a module and use precompilation? 
True, precompiled packages has been a great addition to Julia in v0.4 (which should be out in its final version soon). What kind of dynamic plots are you looking for? I'm not sure when you used Julia last, but a lot of improvement has been made to plotting packages. Maybe check out Interact.jl https://github.com/JuliaLang/Interact.jl (btw you are probably thinking of javascript with the .js endings ;))
&gt;After the first import does that carry across starting and stopping Julia? Eg running a script and then running another? 
It seems that interactivity is still a big deal for Julia. I wonder if there would be value in the JIT compiler running LLVM bytecode first, then using unoptimized native code when that compilation finishes as a task, and finally using optimized native code once that is generated. Rust (and likely other languages) share the problem that LLVM's full optimizations take a long time to run and people point their finger at the language itself.
Sure, that's kind of the point :)
You wouldn't, but that isn't what I'm saying. Julia code is generic which is the big difference. Because of that you can't compile every last bit natively. I'm also not suggesting recompiling every time someone uses something, just that when there is a recompile due to changing something, 3 tasks could be spun off at the same time so the code could start running immediately and then run fast once the final compilation is done. What creates this problem is that LLVM does lots of optimizations but these are not built for interactivity. So Julia is probably not the problem with long compilation times when making changes, it is actually all the LLVM optimization passes. 
It really is nice for the thing I guess you had in mind. Before, if you ran multiple scripts from file, it would have to load over and over - super annoying.
It has nothing to do with language design. Julia is being compiled to LLVM byte code. That could be interpreted, compiled without optimizations or compiled with optimizations. LLVM isn't just bytecode in and native code out with no way to change anything.
I understand that, but haven't the language designers chosen the specific types of optimisations that LLVM does when it compiles? I understand that there is a trade off between speed of compilation, the optimisations that it can do in that time, and the run-time speed of the code. This is all a learning experience for me, as I don't think I've ever tried using s language that is this new before. 
+100 I would love a convenient way to compile standalone executables. That would get me one step close on using Julia for work
It's not so much that, but it would be good to be able to use some java analogies in order to figure out how types, modules, functions, and methods all talk to each other, and how to have variables in a "class" and how to define their behaviour and the like. I'll go and mock up some examples.
Hooray! This is very exciting news! I'm looking forward to updating from the pre-release version to release later this evening.
Can't promise to be nice about your book, but I am biased towards both pipelines and Julia. Can't help but think you will want your book to be compatible with the latest version (0.4). Me, workflows &gt; decade ; Julia &lt; year.
I'm waiting for an updated benchmark (on the front page) 
https://github.com/JuliaLang/JLD.jl
Great!! Are you in any way associated with SOASTA? If yes, then can you explain your experience using Julia for the data science implementations.
Well, you are right, I fatfingered my answer: I meant Interact.jl+PyPlot.jl or Interact.jl+Gadfly.jl are not working reliabily. I can't remember exactly, but it was something along those lines: I could get Interact.jl to work on julia 0.4, but not PyPlot.jl (that worked like a charm under 0.3), as for Interact.jl+Gadfly.jl, I was just not able to run the examples on Interact.jl.
Okay. They are very popular packages in the Julia community, so they _should_ be working. If you ever have another go at it, please report any errors/problems.
I'm going through the [JuiaCon 2015 videos](https://www.youtube.com/playlist?list=PLP8iPy9hna6Sdx4soiGrSefrmOPdUWixM). Some of these provide insights into what companies are using Julia and why they do so. Particularly interesting is a presentation by [Jack Minardi](https://www.youtube.com/watch?v=PluP562Xdv4) of [Voxel8](http://www.voxel8.co/). He makes a great case that Julia is critical to his company's success, as it helps them quickly develop high performance code. It's a good presentation to catch if you're preparing to pitch use of Julia to management. (I have no affiliation with Voxel8, though I'd certainly like to own one of their printers.)
I guess not, but was this presentation taped?
You could also try to do compute several per cpu using the @simd macro. http://docs.julialang.org/en/release-0.4/manual/performance-tips/
Julia really shouldn't be cloning into system32. What are the environment variables `HOME`, `HOMEDRIVE`, and/or `HOMEPATH` set to?
I found Juno to be pretty buggy so I switched to Sublime julia
At least `HOMEDRIVE` and `HOMEPATH` should be set by default by Windows. Something is wrong with your system configuration if you're missing those.
Make sure you are running Julia with `julia --color=yes file.jl`
That worked. Why isn't that enabled by default?
Probably depends on what terminal you're using.
&gt; Has anyone had a similar experience with namespaces in Julia? Similar to what?
ugg. thx. screwed up the link. https://luthaf.github.io/julia-some-criticism.html
Modules are namespaces. You don't need to bring everything into scope. My recommendation would be to either call them by the module, e.g. `MyModule.fun()` or import only the functions you need. [Short doc snippet on the matter](http://docs.julialang.org/en/release-0.4/manual/modules/#summary-of-module-usage).
Acceptation? What? 
I think something like this was already filed on github.
What do you mean? Isn't it a correct english usage? I am open to corrections of my writing!
I am not complaining it is hard, I am complaining It is not automatic, and I have to remember to create `modules` by myself. And this add some boilerplate code for nothing.
&gt; And this will probably get even easier in 0.5 with #4600. That's one of the major problems with Julia (as a project this time, not a language)... 1. each time yiu encounter a problem, you tend to fix it directly by introducing new features 2. this new feature induces some problems of its own or when coupled with other features 3. when people underline the problem and its relationship with a feature, you respond that the feature is all good since it solves problem X (even if it induces problem Y, Z, T and Omega) and you start again at 1. trying to find solutions to all these problems. I genuinely think that you don't see it but this kind of local problem solving tends to make things difficult for you and for the people who intend to use the language not because of just one design decision but a pile of them which makes the language overly complicated as a whole compared to what it could easily be.
There *is* one benefit to having module creation be an expression. You can theoretically do interesting things with them with macros. Underneath the hood julia is really just a lisp with weird syntax.
Well, I learned something as well then. 
New features are complex so they introduce errors (bug) and design errors (coupled bugs and mind strain). The concept of missing feature is tricky itself (is lacking RAII, environments, contracts, classes, etc. considered missing features?). What I was trying to say is that by adding features just by looking locally, you get something intrinsically more complicated than if you try to solve the problem globally... **People (Julia devs included) spend their time paving roads, putting asphalt, fixing cracks, even putting sun umbrellas at both side of the road to protect them from the heat of the sun just because they didn't figure out that the tyres of their car were missing!** Which is more complicated and expensive? Buying 4 tyres or making sure all the roads everywhere are in a perfect shape? The funny part is when people figure out that breaking will always break the inner tube even if the roads are perfect... This is neverending and makes the whole thing more complicated than it should. **People think that finding solutions is the most difficult part. Actually the most complicated part is to find the right problems... The 5% of root problems whose fixing would resolve 90% of all the problems encountered in the project**. 
You are right that I have yet to develop a large library. I'll let you know how it went when I do so. I'm undeterred by your comments on the grave difficulty of developing with Julia.
I agree with almost everything you said, and yet Julia is missing some needed features. In my opinion a couple of these are fixed-size arrays and reference counted types (as an alternative to lazily garbage collected types). The second may not be needed but I think fixed-sized arrays would definitely be a key thing to have.
This is some good constructive criticism and much of it is on point. My biggest issue is with the assertion that the first three points won't be addressed in any way. I also think that the difficulties that these issues cause for building larger systems are highly overstated. There are definitely annoyances, but they are not show stoppers.
Matches shell syntax and it's often necessary to mix different levels of quoting in shell command arguments? Backtick quotes are on the table to be changed to a macro extensible mechanism like custom string literals though. I've yet to see any projects written in Go or Rust that actually solve problems I have, with the exception of docker where I've never needed to look at the source. I have a much easier time reading and reasoning about Julia code with reflection pointing me straight to where things are defined than trying to figure out backtraces of deeply nested OO hierarchies in all of the Python or C++ code I've ever tried to debug. Dispatch is far more interesting for code structure and interactions than file and module organization. Cleanliness of exports is a matter of good practices, at the moment many Julia users are coming from a Matlab or R world where it's a total mess and Julia improves tremendously on that (and will get better over time as a social matter). Julia is probably the wrong language for **your use cases,** but that doesn't mean you need to continuously vehemently rant about how its import system and code organization will forever make it wrong for **everyone else's use cases.**
&gt; one thing to handle "series of numbers" God no. Interesting problems have structure. Especially when you need to solve them at scale.
Yes but does this burden need to be on the language user head every time someone tries to use Julia? Wouldn't it be better to think about a way to abscract these differences in a way that structure can be represented without having to rely on 5+ different types? At a time, there were distributed arrays for parallel work. Since distributed stuff need more bookkeeping compared to serial stuff, so a new type is a clean and straightforward way to get that. However from the user point of view, a Array and a DistributedArray is the same thing: a series of numbers. So I am fine if the `@parallel`, `@everywhere` macros and all the parallel constructs use DistributedArrays in the background. However this is typically something a user shouldn't have to see as a default. But to be able to get this level of abstraction needs to really think about the design and to accept alterations in the language to some degree.
&gt; I have a much easier time reading and reasoning about Julia code with reflection pointing me straight to where things are defined than trying to figure out backtraces of deeply nested OO hierarchies in all of the Python or C++ code I've ever tried to debug. You've got it all wrong, this is a social problem, not a technical one: just send pull requests to change that to something more sensible. :P (and class-based OO does not only have drawbacks even if they are not trendy anymore) No, what I'm trying to say is that for scientific computing at large-ish scale, Julia is far from Python/Go in term of readability. If I'm saying this here is not because I don't like Julia, quite the contrary: I like it. But the vision of Julia is basically to give all users (not only computer scientists), the access to a high level language that is perfomant enough to avoid the 2-language problem. Right now, Julia allows people to do clean-ish big libraries and software. The julia program itself is like that and that's really great. In theory C and fortran can be almost as clean. In practice, all the scientific projects I've came across (except 2 of them) are a whole mess of spaghetti code, yet it is what you call a social problem. The thing is they are to big to be ignored yet too brittle to alter them yourself. When Python came, this was mostly delightful: the code is of higher quality and is generally readable. At the beginning, Julia mostly felt like that... Then I looked at third-party project and I see all the fortran mess again. Yes it is better than actual fortran because it is of higher level but the code really is crappy. I find it a shame because Julia is not far from encouraging people to write better code by design just by trading some arbitrary decisions for others.
 S to the P to the aghetti SPAGHETTI!
&gt; Wouldn't it be better to think about a way to abscract these differences in a way that structure can be represented without having to rely on 5+ different types? Yes. Abstraction design in the standard library types is a continuous and ongoing process. With strings and structured arrays there are design proposals that have been laid out for potential improvements, and now need proof-of-concept implementations. But arrays are core data structures and have many subtleties and special variations for different purposes, and trying to hide that complexity behind a single does-everything type so the user doesn't have to think at all about how to appropriately solve their problem is in the realm of sufficiently smart compilers. Julia changes constantly. I'm not sure what your central complaint is any more - Julia won't change from multiple dispatch to class-based OO, and will in all likelihood not be adopting a 1-file-per-module convention. But most other things are up for discussion and development since Julia is not at 1.0 yet. Things change when people make convincing cases and implement those changes, by having a presence on github. Reddit commentary doesn't accomplish much of anything other than making a reputation for yourself.
By the way, I'm not coming to Julia from MatLab/Octave, which I've barely seen and not learned. If anything, I'm coming to Julia from Python. Prior to adopting Julia, I had spent about 6 months learning and using Python. (This was my second attempt to get to know and like Python. My first was long ago, when Python was in version 1.5. Then I got as far as building GUIs with Tkinter before leaving it to pursue Perl, which had much better library support at the time.) This second attempt to like Python came from my growing frustration with R. I've built some rather complex raw data to finished report projects with R but found it frustrating to do things that do not fit naturally into a data frame. I liked (but didn't love) Python3 (Python2 not so much) and appreciated the efforts of the Python community to develop tools for scientific computing. But these didn't satisfy and I found myself looking elsewhere. Searching for alternatives to MatLab, I stumbled into Julia and found a tool that fit my work style so well that I've adopted it with gusto. I don't find Julia's source code at all difficult to explore and understand. When I encounter something that I don't understand I turn first to the documentation, and if I don't get a quick answer there I don't hesitate to look at the relevant source, which is almost invariably helpful. I can't say the same of any of the other languages that I've used. (Most often I look at source on github, where the search repository function is helpful. Otherwise I check it out locally with emacs or less.) By non-standard string types, do you mean such constructs as `r".*foo$"` for regular expressions? I rather like this way of invoking macros. If you see this practice as problematic, how so? 
Your answers are really helpful. Thank you. Regarding strings, by themselves this is not a problem. Quite the contrary actually: it is convenient. However just imagine for a second that you come to a codebase where there is one macro every third line and where not even a single string is "raw" (i.e. made of string macros). This is a real pain because I, as a contributor, have to figure out what each macro is doing everytime. Yes, `r".*foo$"` is convenient but this convenience comes with a high mind strain... One of the core principles of Julia is to keep it simple, right? One of Julia's recommendation is to avoid macros when a function is enough, isn't it? Then I would largely prefer to feed all regex-related functions with a raw string, just like I'd rather use raw strings as commands just because the detail of expansion and encoding does not map to what 99% of the people want to do: describe information as a chain of characters. So while I understand the reasons and the convenience, I really think they are outweighted by the drawbacks (complexity being one of them). At worse, contructing methods (constructors) could be used, if at all needed.
I understand. Thank you for your input both of you! It may take a little while but I'll definitely come up with something interesting!
Comments are also a great way to discuss and to change one's mind. Actually I'm not aggressive because I want to (I'm sorry for that). This is only my (unconscious) way to confront ideas: finding answers all by oneself is generally fine but then it is difficult to figure out when one is wrong. The problem I have with the mailing list is this urge to tell people to implement stuff first and talk later (like what you did just now with github). While I perfectly understand that having hundreds of people suggesting stuff without doing anything for it is not very useful (actually it can be a waste of time if the same suggestion is done again and again), I still think that programing skills is not the only thing that counts for software developement. Sometimes, discussing with other knowledgable people can prove to be as important as implementing something right away. My centrals complains for the moment are that the large-program story in Julia is broken (IMHO) and that too many problems are only fixed locally without ever discussing the trade offs of old decisions. But I'm quite confident that I can do something about it now.
There's a serious issue with signal to noise ratio on the mailing list and reddit/hn commentary, and an unfortunate "armchair language designer" tendency where newcomers will show up and demand everything change right away to fit their own vision, which contributors have grown justifiably weary of. Julia's a young language but it doesn't change overnight and at the drop of a hat - it takes effort. If you have a well-articulated point, people might listen, but more likely it will get drowned out by uninformed idle discussion. You build up social capital in the community through participation, and people will come to value your opinion more over time. Occasionally technical proposals will come in and stand on their own merits or the previous background of the individual (see David Moon's proposals for reworking macro hygiene), but this is the exception rather than the rule. Julia is malleable, but only by the people who are actively doing work on it.
Excellent. Good luck and have fun.
Ah but `r".*foo$"` is much more than just a way to invoke macros: ``` julia&gt; typeof(r".*foo$") Regex ``` To the question, "Why not feed all regex-related functions with a raw string"? I would respond with a question, how does a function know that it is regex-related? I hope that (most of the time) the Julia answer is that a function is regex-related when its inputs include a `Regex`. A beauty of Julia's generic function/multiple dispatch paradigm is that one can create string processing functions that respond differently (and appropriately) depending upon whether their inputs include strings, `Regex`, or something different. (I once was delighted when I fed a `Set` of characters to such a function and found that it did exactly the right thing. The details escape me; it was a situation where neither a string nor a `Regex` was quite right for the job, but the `Set` invocation was perfect.) I've never thought of `r".*foo$` as just a convenience. To me it is first and foremost a `Regex`, and as a former Perl user, I'm very comfortable with that. I'm afraid I don't understand how this way of doing things results in complexity. It's just speculation on my part, but I have a feeling that if you get this idea (finding that the complexity drops away, leaving insight) that you'll be much more comfortable with Julia. That `r".*foo$"` is a macro is just a implementation detail (albeit a very clever one).
I won't argue much more. Thanks again for your time and your explanations. As I already said, this has been truly helpful. There is a very blurry line between what string macros are (metaprogramming) and what they are generally used for (constructors). Eventually there are two different things here: using a string macro (which is just convenience since raw strings also exist) and efficient dispatch (which requires types as mere flags instead of embedding this flag withing the function arguments or using a set of different functions). I'm wondering if we can do better than forcing the developer to think about which type of string subtype he should use to get a function work (i.e. using raw/verbatim strings for everything). But well, I've been working more with numbers than strings... And as I stated implicitely further above, using string macros to get string-like types does not look that bad in the end. It just still feels weird to get a matrix as a string output (even more as it is redundant).
It has been fun, and I've learned from having to think carefully about Julia as I've responded to your comments. I do not share your concerns and even see advantages in some of the things that disturb you. I wish that I were a bit better at getting my thoughts across. I hope that you to continue work with and contribute to Julia. And I encourage you to seek a shift in perspective when thinking about Julia. Julia isn't doing things in the *old* way of Fortran and C versus the *new* way of Python; its doing things in a different, and certainly modern, way. Please do try to understand and embrace generic functions and multiple dispatch. I get the impression from some of your issues that you are being hampered by a different mental model of functions. Viewed from an appropriate perspective, Julia becomes easy, natural and almost effortless. That's what I meant when I spoke of *cognitive fit* in my initial comment on the thread. 
The error it's giving is because at some point you are attempting to treat an Int64 as a Drain.pores. At first glance it looks like the line: pores[i].coord = coords[i,:] is the culprit. You reference the pres constructor like an array which is probably getting treated like a call to it's constructor. Perhaps you meant to construct an array of pore objects first? if pores on that line was meant to be a variable not a reference to the type you defined above then you never initialized it as a variable.
You might want to check out Julia [packages](http://pkg.julialang.org/) that deal with problems similar to yours for some inspiration [JuliaFEM](https://github.com/JuliaFEM/JuliaFEM.jl) perhaps.
In that case, here are some things I noticed while reading it: &gt;One have to explicitly declare a new module should be "One has" &gt;This have two consequences should be "This has" &gt; code organization do not follow file organization. should be "does not". (the same thing a few lines down in "The fact that code organization do not follow...") &gt; This may change when Galium.jl become usable should be "becomes". (Also, the period is missing at the end of this sentence.) &gt; But this model is bad, and add a cognitive charge should be "adds" These are essentially all the same mistake: Using the infinitive when it should be the third person singular form of the verb. &gt;But what append if we already have another variable with the same name? should be "what happens". "to append something" means to attach something to the end of something else. &gt;you do no get full control over what append Same thing here. &gt;You can not refactor easily your code should be "You cannot easily refactor your code" ("cannot" is one word, and the adverb "easily" is in front of the verb.) &gt; But scientists notoriously use not enough unit tests. Probably not wrong, although "But scientists notoriously don't use enough unit tests." sounds more natural to me. &gt; Automagically generation of code is nice for high-level construct should be "Automagic". "generation" is a noun, not a verb, and therefore has to be modified by an adjective, not by an adverb. &gt; This is a list of smaller flaw should be "flaws" &gt; and when you have used lldb once, it is hard to go backward should probably be "it is hard to go back" &gt; (multi-dimmensionals) arrays are first-class citizen should be "multi-dimensional" and "citizens" &gt; multiple dispatch is a very nice way to implement reuse code This can be fixed in at least two ways: "implement code reuse" or "implement reusable code". In any case, two main verbs should not be juxtaposed. &gt; I will still use Julia, but not for large codebase should be "but not for large codebases" Also, a potential formatting issue: &gt; This have two consequences: - modules are big; - code organization do not follow file organization. should this be an enumeration?
Corrected, thank you! I do find myself doing these mistakes very often: missing 's' at the third person, and using append instead of happen -- the later may be because I do too much programming in english, and not enough writing in english.
Try `Pkg.checkout("JuliaParser")`
Juno tells me to download the newest LightTable on startup :/
I'm not 100% sure what this means for julia (maybe nothing in the short-term?), but from speaking to people at the SF Julia Meetup last night it seems like it's going to be a Big Deal.
Did you `Pkg.add(“Jewel”)` ? 
What does threading mean for your everyday user?
With "threads" your computer can do multiple things at the same time (or seemingly at the same time). If you do multiple things at the same time, it can be faster that doing them one at a time. --- For certain algorithms this gives scope for substantial performance gains (especially if the machine your working has access to a large number of threads e.g. has a GPU\*). \*GPUs are a special case which may have their own issues, but I suspect this work "helps"... ?? Not sure.
But how does this differ from running julia with -p flag ?
Ah. Like we do with shared arrays. So i guess this would dramatically change how we do parallel processing in Julia? 
With Jupyter (formerly iPython Notebook) and the IJulia kernel, you can download a notebook as PDF via LaTeX, or download as Markdown.
yes I did
nice post! we recently had a kakuro challenge in /r/dailyprogrammer: https://www.reddit.com/r/dailyprogrammer/comments/3g2tby/20150807_challenge_226_hard_kakuro_solver/ that julia blog post provides a great treatment of the problem and solution. if you're looking to put it to use, check out that above challenge.
I completely agree. One thing to add though is that "Using" keyword should just be considered harmful same way as "*" is considered bad practice in Python. Only use "Import mymodule" and then "mymodule.myfunc()". The problem is that "Using" is commonly found throughout the examples and in many libraries so it's more to do with the culture and less likely to change.
I don't understand what the solution is there.
Is it possible? Definitely. Is there a native library to make it easier? Depends on how easy you want and how messy the website you want to scrape is. Check out http://github.com/JuliaWeb. Alternatively, you could use PyCall to punt to a Python library like BeautifulSoup.
&gt; Setting `JULIA_PKGDIR` to some place local got me past this issue. Is your home folder on a network drive?
The Julia side of things doesn't present any problems. Most of the work is to do with navigating the stuff you get back. Here's a quick scrape of Reddit: using JSON, Requests url = "https://www.reddit.com/r/Julia/" firstrequest = get("$(url)/.json") jd = JSON.parse(Requests.text(firstrequest)) after = jd["data"]["after"] counter = length(jd["data"]["children"]) c = [] for i in 1:counter url = jd["data"]["children"][i]["data"]["url"] id = jd["data"]["children"][i]["data"]["id"] title = jd["data"]["children"][i]["data"]["title"] author = jd["data"]["children"][i]["data"]["author"] flair = jd["data"]["children"][i]["data"]["link_flair_css_class"] created = jd["data"]["children"][i]["data"]["created"] push!(c, (url, id, title, author, flair, created)) end for post in c println(post[3]) println(" ", post[4]) println(" ", Dates.unix2datetime(post[6])) end 
I installed Julia directly onto the C Drive.
Your user's home directory is what I'm asking about, not where you installed Julia.
I think one mistake is not having an official plan for GUI/IDE/Plotting. These are really essential for day-to-day work and relying purely on the community isn't so great. Just having a plan for those - from the start - like for other aspect of Julia (like the REPL), would have helped imo, even if you do rely on the community to do the actual work.
If that's set to anything. If it isn't, then what are `HOMEDRIVE` and `HOMEPATH` set to?
Julia is not 1.0 yet. Stability means not changing, which means not gaining any new features like multithreading, C++ FFI, a debugger, fast array indexing, fast closures, etc etc etc. The language is for early adopters. Once the language design is more or less "finished" at 1.0 there will be a stronger commitment to stability and things like IDE's and blessed packages will be more important to get right. Julia doesn't really need users right now, as much as it needs contributors - engineering hours going into submitting pull requests and creating high-quality packages. That's not to say IDE work can't go on in parallel before the language is 1.0 (I believe https://github.com/JuliaLang/julia/issues/12941 is the blocking issue on making Juno support 0.4 nicely), but is an easy-to-use IDE the best place to spend engineering hours right now in terms of gaining contributors to the language or authors of new packages? I'm pretty sure Mamba.jl has separate branches for 0.3 support vs 0.4 support, so the newer branch should work fine.
I completely agree with this. I tried to do a multi-person research project in Julia a few months ago, but after I had written it with PyPlot, it wouldn't work on windows without a lot of work, and it just wasn't worth it. I eventually just output the data into a mat file and had separate matlab scripts to plot it. This is a very important aspect to get right.
Lost functionality being Julia Studio? All of the Julia Studio code is open source, if you want it back you know what my response is going to be here. The package ecosystem is much more stable and closely monitored for breakage now than it was 18 months ago. If you aren't contributing code, funding, or well-written bug reports to the Julia ecosystem, then sorry to say you only matter as advertising. Word of mouth is useful to grow a community, but if you're not happy with what's available to you then no one is asking you to be dishonest and overly complimentary about Julia's maturity.
Julia studio as a symbol for a well-working IDE. Along with other packages and features (Juno) that were working once but stopped working at some point. Please don't get me wrong, I have the utmost respect towards the Julia project - I am just trying to provide some feedback. You are right, I may just be some WOM or advertising factor. But this can be very important for a new product to take off. To be honest, by now I have read some replies to feedback from other people (sometimes quite harsh comments). I understand that people are very involved and can get defensive. But to be honest, now I also wonder if this attitude of "if you want that, code it yourself" (instead of saying, thanks for the feedback, we will think about it") may be what could break Julia's neck. 
I won't think about it, there's zero chance of me personally working on an IDE. It's just not at all on my personal priorities list. The priorities of the people doing the work determine what gets done. We hear the feedback, but again, at this stage in development our priorities mean more to us than yours. It's frank honesty. To change the situation, participate and become one of the people doing work. Or become a paying customer of Julia Computing to sponsor development of things you want. We don't need to rapidly grow the user base right now, we need to rapidly finish all the known work items on the list towards getting Julia to 1.0. Help or you're just a distraction right now.
Gadfly
Perfect! This is amazing! Thanks
I suggest to hold off on heavily using dataframes as it currently has some speed problems.
IDE - I've been working with Julia through emacs and the support is fantastic. From latex notation through latexsub to embedded plots in babel-mode, code completion (this still doesn't work perfectly for me but at least seems to work well for base), and REPL support, it makes learning and using Julia a pleasure. Also, Gadfly seems to be working much more smoothly and quickly since I began using 0.4 (though this may be package specific and not because of using a newer version). I would strongly recommend upgrading to .4 and using emacs.
Vim support is really good too. It doesn't have REPL support, but maybe someone could implement it with NeoVim terminal mode. 
Hi I work with Brian Smith on Mamba which you mention does not work anymore but I am not sure I understand what you mean. It has always been up and running just fine :) Feel free to post any issues you have we are very responsive! https://github.com/brian-j-smith/Mamba.jl http://mambajl.readthedocs.org/en/latest/ PS: if you are referring to the "tests failling on 0.4" on the website I think that is an issue with the testing service installing Cairo correctly. We will look into it. I continuously use it and all of its features just fine. 
BTW: I also find it sad that some constructive and neutral comments seem to be voted down by some people here (just because it does not comply with their opinion). Listen to other people - you learn more from dissatisfied customers/users than from the ones who claim everything is great. 
The discussion has yet to produce anything actionable. I get that you're frustrated things aren't stable - julia probably isn't for you yet if you want a perfectly stable finished product. Julia should only be used by a small circle of people until it's mature and needed features are implemented. R and Python will always be better at appealing to the masses than Julia will, and that's okay. We know what needs to be done because stakeholders who are funding and contributing to development have made their priorities abundantly clear. Overselling to the R studio crowd doesn't help us get there. People who've been around both communities for a long time will tell you as much.
Don't use master. The release branch has been quite stable. If you have counterexamples, please report them. Test coverage has improved dramatically over the past year. Documentation is kind of a mess right now but being worked on. If you want a fixed set of features that never change, old releases and pinned package versions satisfy that.
i think you're "concern trolling" because it's not what you want. go away. use something else.
I understood some of that. It looks pretty neat! 
I'm with you. The lack of a supported IDE, debugger, and graphing system is keeping a lot of people from using Julia. When I'm explaining my excitement about Julia to my coworkers, this is always what they ask, and it's hard to convince them to overlook these missing pieces. I'm surprised by how many snobs there are in this thread saying that Julia isn't ready for users yet. The Julia website is definitely geared towards encouraging users to try the language, and that's been the general vibe in all the Julia threads I've read this far. I'm not a language developer but I decided to give Julia a try about a year and a half ago. I liked it a lot and ended up starting my own package that I continue to support today. I think there's definitely a correlation between the number of Julia users and the number of Julia developers, and new users should be courted as such. Also, the core Julia developers that I know would never come into a thread and tell people to go away for making suggestions. /u/JabbaJabba1, please do your best to ignore these people. They are right in that open source projects usually lack strict organization and developers work on what's interesting to them, and that's fair. But now that several of the core Julia developers have formed a small company around using and supporting Julia, I'm hoping that supported plotting, debugging, and development environments will become more of a priority. At my company, the lack of a debugger sadly makes Julia a non-starter, but none of us know how to write debuggers or low-level language code so we're left waiting. If there was a debugger we probably would develop a new project in Julia to begin the switch from Matlab, and probably would contribute bug reports and pull requests along the way. As far as the loss of Julia Studio, that was being developed and supported by a company called Forio. As far as I can tell, the Julia developers that were working there left, and Forio then abandoned all of their supported Julia work. I used Julia Studio for a little while before realizing it actually offered nothing interesting, and decided I liked working in a text editor and the REPL a little better. I haven't tried the newer IDEs that have come along, but I agree that it's a very nice capability to have once you start working with large projects with multiple developers. Edit: I'll also just add that I haven't had any major issues with stability, but I almost always stay on the latest stable release except when testing the compatibility of my public package. I don't see any reason to leave the stable branch. The stability of each package is another issue, but many of them have integrated tests that help to maintain stability across versions and I haven't had much trouble here, either. But the packages I use most heavily are my own. :)
Its great to hear what you've experimented with (and I'd love to hear others chime in as well). 
Thanks! Just figured I'd share so just others could see what's out there
Mmh, what happened to the spirit of open source here? And I would like to say that it is not true that any new user does not spend anything. You spend time: time installing things, time learning new syntax, new structures etc. These are opportunity costs (I could have spent this time learning something else) and whether someone feels rewarded for their efforts will determine how much they continue, support, and positively promote a product (here new software language). It's good to hear that different customer groups seem to be interested in a well-working IDE. I wish I could contribute to it, but I am afraid my programming skills would not be sufficient here. I am looking forward to Julia 1.0. 
I use atom plus language-julia, minimap, and editorconfig. And then I use the os terminal with tmux so I can have more than one pane side by side. 
Thanks again - I edited my original entry to make sure that it was not working for me only.
You don't need any specific packages in general, it really depends on what you do in your project. If you need JSON use JSON.jl, if you need plots use Gadfly or Winston, if you want to do web stuff use HTTPClient.jl, etc. Think about what you need in your project and search the packages or mail list for suitable packages. Ask yourself if you really need a package too, sometimes it's actually faster to do things by yourself.
Thanks for sharing your experiences. I can imagine that not all core developers represent the attitude of telling users to go away. Forum discussions can become a bit emotional - and different people have different skill sets :) 
What are you doing in Julia at this point in its life that would use all of this? I don't mean to be condescending, I'm genuinely interested.
Not too much, I just don't like writing code in a plain text editor then running. At work I wrote a small simulation in Julia. Then at home I've started working on audio processing code and using it as a review of my linear algebra so I've been writing functions to do like eigenvector calculations and stuff. No heavy development but just a clean place to write and test some code
Julia devs are generally very nice, they often answer "noob" questions on the mailing list.
Okay I added those to my "user variables for user". Or should I put them in system variables?
Might also be using `JULIA_PKGDIR`
What is JULIA_PKGDIR? I apologize about all my questions... I really do appreciate your help. I'm moderately familiar with things like environment variables from installing packages in R and Python but I'm having a lot more trouble in Julia for some reason.
Yet another environment variable that Julia uses to override the default setting of where `Pkg.dir` should be. See https://github.com/JuliaLang/julia/blob/b8b351720eb6375e1f36fae8cfcb713de13e6f10/base/pkg/dir.jl#L10
Okay I got it to work! I'll make a new thread if anyone has this same problem.
To be honest I keep going back to using plain old vim to edit julia. Combined with slime and tmux I get pretty much everything I need. 
This kind of support encourages me to keep an eye on Julia. I downloaded it and played around with it several months ago. But I am not a good enough programmer to deal well with it at that stage. Seeing this kind of support means that the core team will have the opportunity to get it ready for guys like me. My appreciation to the Moore foundation
I posted here asking for examples in the past, and no one has provided them yet. Someone should create a single program that calls a hello world function from all the languages julia is compatible with as an example. I read the docs, and don't understand them enough to implement.
Were you just using Numpy for that? I'm not an expert in Julia, but the linear algebra package is probably calling some C code that may be very similar to what Numpy calls, just the abstractions are different. If you want some speed up here, you'll probably have to install some highly optimized BLAS package, preferably compiling it yourself. Both Numpy and Julia should be able to make use of that, but someone who knows more than me about Julia would need to tell you how to do that. Edit: now, for the other stuff you are not using Numpy for, my bet is that Julia is much faster at those parts. Edit 2: are you sure you are not running out of memory? Julia wouldn't help you with that. Dense matrices can use a lot of memory.
&gt;Were you just using Numpy for that? Yes &gt;If you want some speed up here, you'll probably have to install some highly optimized BLAS package, preferably compiling it yourself. Both Numpy and Julia should be able to make use of that, but someone who knows more than me about Julia would need to tell you how to do that. What is a BLAS package? &gt;Edit: now, for the other stuff you are not using Numpy for, my bet is that Julia is much faster at those parts. Do you think Julia is useful for data mining purposes? I do a lot of that with pandas in Python. &gt;Edit 2: are you sure you are not running out of memory? Julia wouldn't help you with that. Dense matrices can use a lot of memory. I have 8 GB of memory on my work laptop, that may be the case it does turn into gigantic matrices. Is there any workaround for this other than more memory? Like would this happen even if I wrote all this in C at the pointer level?
&gt;This is right - the claim that Julia is faster than python means primarily native Julia and python code, which does not spend most of its time calling external libraries. Numpy and Julia's linear algebra libraries definitely spend most of their time in C and fortran code. Ah, that may explain why they both crapped out on me at similar sizes. Numpy actually worked a bit faster as well. But it was amazing to me how quickly Julia could print through each row (I did a loop to print through 30,000 rows and Julia did it super quickly).
Still, remember that I/O is always slow as hell, but Julia is probably much faster at that than pure Python. For instance, the printing code in Python will probably be much faster if you do something like `map(print, rows)`.
[BLAS](https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms) is a specification for Linear Algebra libraries and the bigger implementations of BLAS are ridiculously fast. I know Numpy is compatible with some of them, so if one of those is compatible, Numpy will use them instead. However, if you are using Windows, you are in for a though time, as it is rather complicated to get them working there. It's been a while since I've used Julia, but I'm sure it has something similar to pandas that implement records and such. I think Julia is a very powerful language, but I'm waiting to really start using it. I've had a lot of trouble trying to make code that is as good as the standard library in Julia. For example, Julia built-in insertion sort allocates something like 144 bytes of memory. Even when copying that insertion sort code from the Julia code base, this "new" code uses several megabytes, and I haven't been able to figure what am I missing. So, my current problem with Julia is that it lacks the learning resources Python has. If I don't understand something in Python, I can probably find 100 different sources explaining it at various different depths. For the workaround, it highly depends on your data. You are using dense matrices, but are they actually dense? If a very large portion of the entries are zero, you can (and should) use [sparse matrices](https://en.wikipedia.org/wiki/Sparse_matrix) instead. Edit: depending on the transformations you are doing with your matrices, you might actually be able to use an SQL database to do them. They should be able to handle large amounts of data much better, at least.
Can you post the code? Or something representative and similar? Make sure you don't run Julia code with global scope variables. Most likely for working with large arrays you'll need a little bit of refactoring to cut down on copying and allocation of temporary results. There are in place versions of most linear algebra operations you can use.
Here's my example, and I can show you how I parallelize in one: The C function (with more details than you probably need since you are just calling a library): // gcc -shared -fPIC -O3 optimize.c -lgsl -lm -lgslcblas -o findP.so double* findPolicyInCPricesPreviousK(int kInitial,int qInitial, int z, double theta, double delta, double beta,double* vK, int nK, double* vQ, int nQ,double* vZ, int nZ, double* currentValueZ, double* transition, double betaGov,double fixedCostMean, double fixedCostVar,double wage,double capitalShare, double floatationCost, double elasticityOfSubstitution, double aggOutput,double adjustmentCostParameter,int previousK){ ... double *results = (double *)malloc(sizeof(double)*7); results[0] = policyK; results[1] = policyQ; results[2] = highestVal; results[3] = bestProbability; results[4] = bestLabor; results[5] = bestDividend; results[6] = bestCost; return results; } Julia Caller, where state private is just a composite type to make everything neater function findPolicyCPrices(state::statePrivate) q = state.debt z = state.z model = state.model currentValue = state.currentValue answerMatrix = Array(Float64,model.nK,7) previousK = 0 for k in 1:model.nK #k q z theta delta beta vK nK vQ nQ vZ t = ccall((:findPolicyInCPricesPreviousK,"/home/gcam/china/findP.so"), Ptr{Float64} ,(Int64,Int64,Int64, Float64, Float64, Float64,Ptr{Float64}, Int64,Ptr{Float64}, Int64,Ptr{Float64}, Int64, # cvz1 cvz2 cvz3 trans betaGov meanFC varFC iter wage capShare floatation elasticity aggOutput || w and z are reduced by 1 (-1) due to 0 based index in C / 0.0 as mu value Ptr{Float64},Ptr{Float64},Float64,Float64,Float64,Float64,Float64, Float64, Float64, Float64, Float64,Int64),k-1,q-1,z-1,model.theta,model.delta,model.beta,model.vK,model.nK,model.vQ,model.nQ,model.vZ, model.nZ, # currentValue,model.transition,model.betaGov,model.fixedCostMean,model.fixedCostVar,model.wage,model.capShare,model.floatationCost,model.elasticity,model.Y,model.adjCostParameter,previousK) if t == C_NULL error("NULL") end answerMatrix[k,:] = pointer_to_array(t,7,true) previousK = floor(Int64,answerMatrix[k,1]-1) end return answerMatrix end Then to parallelize my C function I just do: input = [statePrivate(q,z,model,currentValue) for q in 1:model.nQ, z in 1:model.nZ]; resultVector = pmap(findPolicyCPrices,input) 
`.dylib` stands for "dynamic library", and is the file extension used for dynamic libraries on OS X. It is the OS X version of a Windows ".dll" (Dynamically Linked Library) or a Linux ".so" (Shared Object).
From his answers, I'm guessing that he has several of these 10k x 10k matrices existing at the same time. That would explain the memory usage.
True; however I should note that in most cases, you really want your regression matrices to be "skinny"; a highly overdetermined regression problem is typically better than a merely determined one; you usually want many times more observations than variables. Something closer to a 10,000 x 100 matrix is much more palatable to me.
Agreed. Gadfly might be moving in that direction, but for a language that emphasizes efficiency, I think ggplot2 (on which it is based) is a relatively poor model. It's slow, the "grammar" is awkward, and contours are as close it gets to 3D. I think Julia should be held to a higher standard, aspiring to the beauty of Matlab/Matplotlib with the simplicity of R's **base** graphics. But at the very least, graphics need to become part of the standard library to make it easier for a new user to get up &amp; running quickly, and wanting to contribute that much sooner. 
Did you try it? Did it work faster? I'm curious because I've been advocating for a while that the way to write fast Python code is to write the least amount of code possible, calling as many built-in function as possible, since those are implemented in C.
Regardless of language/implementation, it's likely that your algorithm can be improved. If you're interested, I suggest posting it as a `gist` somewhere, as it sounds like there is a way to rephrase your operations without using so much memory.
Have a read of this paper D. York, N. Evensen, M. Martinez, J. Delgado "Unified equations for the slope, intercept, and standard errors of the best straight line" Am. J. Phys. 72 (3) March 2004. There is some matlab code here http://au.mathworks.com/matlabcentral/fileexchange/26586-linear-regression-with-errors-in-x-and-y/content/york_curve_fit_0_01/york_fit.m
(X'X) ^(-1)X'Y 
I mentioned that because I know that with this method you can do this. But if there are other methods implement it would equally sufficient.
Aaaah gotcha. Yeah OP, I misread your question, that is only for y-errors.
Sorry, asked the question also at another place ... sorry for the confusion ...
This looks nice. It looks like you really have a wrapper to `python nbconvert` plus an include. This is a pretty simple and straightforward approach—can't believe I hadn't thought of it. I suggest rewriting this to take advantage of the fact that Jupyter/IPython notebooks are just JSON. You should be able to traverse the JSON and pull out the cells that contain code and evaluate them without writing anything to file.
https://github.com/stevengj/NBInclude.jl
Thanks, I didn't notice it while googling for something like this. EDIT: unfortunately, trying to compile it on a cheap server where I host my notebook fails due to not enough memory. So I guess I'll stick with my simplistic solution ;)
So I figured out how to change them (ENV[""] = ""), but when I do it through Juno, it automatically creates a /v0.3 path wherever I send it, whereas if I do the exact same thing from the command line version of Julia, it makes a /v0.4...how do I stop Juno from making this incorrect path automatically? 
Maybe you wrote bad Julia code? Hard to say without seeing it.
Took a look at the Vega package. Haven't gotten around to using it quite yet but it looks really solid. Lots of visualization options, and being able to create custom themes is a big plus. I may well be making this my plotting library of choice. Thanks for the good work!
Not ignoring that at all. I agree that's incredibly important, and maintaining a low barrier to entry to convert users into potential contributors is a huge strength of Julia. What I'm reacting negatively to is the "everything is awful" sentiment. If this post were phrased as "how do I help make it better," I would have had a completely different reaction. Julia needs people who are willing to participate, and of course those all start as users.
There has been a surprising amount of chaos in the IDE world. At various points in history you'd just make an emacs mode and/or a vim mode, or an eclipse plugin and be done with it. Not so simple any more. Julia Studio was developed by a third party and discontinued. Nothing we can do about that. I don't think it ever had a large amount of functionality though. Then there was excitement about lighttable, but then that was basically discontinued. Next people jumped on atom, and I don't really know anything about it but some people seem to have some reservations about it. Add to that the fact that many developers (like me) are happy with emacs/vim/sublime and a terminal, and I think it explains the situation. I agree that it would be best for the julia world to rally around making one good IDE. In theory we (other maintainers &amp; I) could announce which IDE this will be, but reading the room a bit I don't think it would work at this point. There's too much uncertainty on what the right thing is. Another major factor is that many people feel IDEs need to or will move to the web, but there is no consensus on how this should work. It's an awkward point in history where many people still like client-side GUIs, yet many developers are reluctant to invest in them. And I know it doesn't fix anything just to say this, but OSS projects aren't single actors. For all I know tomorrow somebody could announce yet another Julia IDE, and it wouldn't be entirely fair to describe that as "julia moving in yet another direction". Finally, please let us or appropriate package maintainers know about regressions. We try hard to fix them.
&gt; What I would like to see is a stable core feature set, that is tested, maintained, and developed with care and deliberation. We have that, but (1) the small core of the language is really too small to be useful to anybody, (2) even that still needs to evolve. However, that evolution will not be willy-nilly. There is a *small* list of big-ticket items that need to happen there, and then we will have a pretty stable base language. Time spent on care and deliberation is exactly why I'm not posting on julia-users all day.
&gt; Even when copying that insertion sort code from the Julia code base, this "new" code uses several megabytes That is very strange. Since I doubt the name of the source file has much effect on performance, could you describe what you did more specifically? Also you probably know about this already but I'll link to it anyway: http://docs.julialang.org/en/release-0.4/manual/performance-tips/
I had read that when I was doing my tests. There was no type instability and no global variables being used. However, when using `@time` to time both my implementation of insertion sort, Julia's built-in and the copied code of the built-in insertion sort, both my code and the copied built-in failed to perform in any manner similar to the built-in function. Specifically, the `@time` reported that the built-in always allocated a constant amount of memory, while both my code and the copied code were using a linear amount of memory (however, I was pretty sure it wasn't allocating anything besides a temporary variable). I'm pretty sure I'm possibly missing something really obvious here (or not), but I couldn't find any explanation to what was happening in those tests. I would love to find an explanation for what was happening, but I couldn't find anywhere anyone was talking about an issue like this. Unfortunately, I neglected to post it to any groups asking about this.
Thanks for the link. Seems like a nice one! Would take it out for a ride :) 
Ok. Thanks. My mistake.
-1.0
I think so I was reading the doc and the table only showed uint and int. The floats where in a separate table only showing bit number.
Reference for metaprogramming (and macros) in Julia [link](http://docs.julialang.org/en/latest/manual/metaprogramming/). The first paragraph explains the main difference.
no. in C you're restricted to replacing strings in templates (roughly; in c99 it's actually turing complete (if you add a driver for recursive evaluation), but not exactly accessible). in julia you get the full power of the julia language to generate the code. julia is like using c programs to generate c programs. not like using the c pre-processor.
I was thinking of [conda itself](http://conda.pydata.org/docs/), which the package you linked to seems to work with. I haven't used conda to manage an environment for Julia, so I'm not able to evaluate the need for Conda.jl.
No one has packaged Julia or any Julia packages for Conda yet as far as I'm aware, which would have to be done first for this to work. You can set an alternate JULIA_PKGDIR to install packages to a non-default location.
Your regex is wrong: julia&gt; rgx = r"([^\.]+).ipynb" r"([^\.]+).ipynb" julia&gt; ismatch(rgx, "notebookipynb") true But you should probably use `endswith` here anyway (there's just no need for a regex): julia&gt; endswith("notebookipynb", ".ipynb") false julia&gt; endswith("notebook.ipynb", ".ipynb") true
Learn `python` with cython for work as julia is still in development and architectural choices are still being made. Python has full developed libraries to work with stats and data analysis. With python you can easily pick up Julia later. If you want to be ambitious work on a julia library.
You shouldn't be language dependent. In other words don't only be able to work in R or SAS or Python or Julia or whatever. Be able to use several. Everyone I work with can program in Python, it is pretty much the common tongue of programming languages. Even if Python is at its high point I doubt it'll be irrelevant anytime soon.
Yeah, thanks! It was as easy as I feared.... I forgot the `include("FEM.jl")` command.... One more thing different from Python... And you are right, it is `export` in the module... So thank you a lot, you did just push me the necessary little bit... Thanks 
Hm.... this sounds strange... But after checking again, it seems to work like you say... now I am really confused. So I am back where I started, except it does work now... I came from Python to Julia and I thought it would be straight forward to put the file into the same directory as my notebook and use it from there, without needing to do anything more. I did not know that I needed to add the path manually but well.... now it works.... Thank you a lot!
What is going on here is that when you do ```include("file.jl")```, all the code in file.jl is read by the parser. (Exactly what *that* means is a bit mysterious too.) The important part for you is that symbols defined in file.jl are now available in the current Julia session. So you can do ```using MyModule``` to load the module. Adding a directory to LOAD_PATH tells the Julia parser that it should look there for modules, which get searched for when you do ```using ...```. But the criteria for exactly when a module is found are not well-described. You could probably find an answer to that if you looked deeply into some of Julia's code, but the method I just described is easier, I think. I would encourage you to look into the documentation for creating your own packages, in the main Julia docs.I don't know if that method would fit with Sage cloud, but it's really quite simple, and saves you from having these kinds of head-scratching moments.
Just HttpRequest, but it's not a framework. I've been thinking about making a full opinionated framework with automatic sessions and templating and stuff, it's really needed. 
Not so much a framework, but there's Mustache.jl for templating and Mux.jl for middleware.
0.5?? Man I am not paying attention to the newsgroup.
These are great points. 
Wow, I can't believe I'd not found Mustache. It feels like close-by there's a way to wire these up to give a flask-like syntax wrapper for Mux with templating.
Ugh... I just went to try Julia and ran into a wall just evaluating an expression in Juno. It now works for me. Here is the exact batch file I use to start Juno: SET HOMEDRIVE=C: SET HOMEPATH=\Users\MYUSERNAME SET JULIA_PKGDIR=C:\juno-windows-x64\resources\app\julia\packages C:\juno-windows-x64\Juno.exe What's particularly frustrating if you work in a networked environment is that your HOMEDRIVE variable may already be set to a network volume. This does not work but it will waste a lot of your time copying files to it first; which is also a PITA to get rid of later. Hopefully this improves in the future by not relying so much on environment variables, or at least relying on JULIA_* variables only. Barring that, it should use the Windows API to find the users actual home directory, because I guarantee the average user isn't going to have time for this.
I don't see much of a speedup from declaring constants, but I do get a noticeable improvement by converting x and y to Array{Float64}.
This seems the best the fastest on my machine, I'm not sure why though, it's a bit suspicious. @inbounds disable bounds checking in arrays. A = rand(2000,2000) x = rand(2000) y = rand(2000) function f(A, x, y) @inbounds for k = 1:2000 for l = 1:2000 if x[k] &lt; .1 &amp;&amp; y[l] &lt; .1 A[l,k] += .1 end end end end You don't have global variables problem here because you pass them all as argument.
Also for me that is faster. It takes 3ms with disabled bounds check.
I'm unfamiliar with Python numba@jit. Does it do any multi threading? EDIT. [No](http://numba.pydata.org/numba-doc/0.21.0/user/faq.html) 
I've also tried @simd for the outer loop. It took 3ms.
That is true. However, for my real problem I cannot do this. The problem I posted was meant as toy surrogate problem. But is indeed faster. 0.7ms.
It is not quite my use case but it takes 0.5 ms. Really fast.
It looks like the major optimization that Julia is missing is hoisting the `x[k]` access out of the inner loop. On my machine, your `f` takes 14ms. function g(A, x, y) for k = 1:2000 xₖ = x[k] for l = 1:2000 if xₖ &amp;&amp; y[l] A[l,k] += .1 end end end end Manually hoisting the `x[k]` access out of the inner loop is a 2.6x speedup, which gets us close to Numba. Hopefully some day, Julia will be able to do that optimization. Now with the `@benchmark` macro from [Benchmarks.jl](https://github.com/johnmyleswhite/Benchmarks.jl), we're able to see the difference between `BitArray` and `Array{Bool}`: julia&gt; @benchmark f(A,x,y) ================ Benchmark Results ======================== Time per evaluation: 13.96 ms [13.82 ms, 14.11 ms] julia&gt; @benchmark g(A,x,y) ================ Benchmark Results ======================== Time per evaluation: 5.44 ms [5.32 ms, 5.55 ms] julia&gt; @benchmark f(A,convert(Array{Bool},x),convert(Array{Bool}, y)) ================ Benchmark Results ======================== Time per evaluation: 6.65 ms [6.51 ms, 6.79 ms] julia&gt; @benchmark g(A,convert(Array{Bool},x),convert(Array{Bool}, y)) ================ Benchmark Results ======================== Time per evaluation: 3.85 ms [3.62 ms, 4.07 ms] That last one should be comparable to your Numba timing. It looks like this is a case where the size advantage of BitArrays doesn't outweigh the additional complexity of element accesses. Adding `@inbounds` adds another incremental speedup, but it's not nearly as large as hoisting `x[k]` or converting the BitArrays to Array{Bool}s.
I've not looked into it, but this optimization is probably being blocked because LLVM doesn't know that the BitArray storage is independent from with the storage for `A`. In fact, it's possible to make them alias, in which case modifying `A` will change the elements of `x`.
This is as fast as I'm getting it: function g(A, x, y) @inbounds @simd for k = 1:2000 xₖ = x[k] if xₖ for l = 1:2000 if y[l] A[l,k] += .1 end end end end end These are my times: julia&gt; @time f(A, x, y) 0.010314 seconds (4 allocations: 160 bytes) julia&gt; g(A, x, y) julia&gt; @time g(A, x, y) 0.001025 seconds (4 allocations: 160 bytes) This is roughly a 10x speedup (on `Version 0.5.0-dev+1526 (2015-11-30 04:16 UTC)`, my nightly pull scipt broke somehow.)
Quite interesting. Also numba uses LLVM.
thanks! 
In contrast to plain Python, the numba code actually also jit compiles the function (after @jit) to native code using llvm, just as Julia does. The difference is that the numba jit compiler does not support the whole of Python. Performance should be similar. Using bool arrays and @inbounds (numba does not boundscheck either) in Julia, it is a tiny bit faster on my machine: function f(A, x, y) @inbounds for k = 1:2000 for l = 1:2000 if x[k] &amp;&amp; y[l] A[l,k] += .1 end end end end function main() A = rand(2000,2000) x = convert(Array{Bool,1},rand(2000) .&lt; .1) y = convert(Array{Bool,1},rand(2000) .&lt; .1) f(A, x, y) @time f(A, x, y) end main() 
Check out [RobotOS.jl](https://github.com/phobon/RobotOS.jl)
Julia works on Linux so first we can test it on Raspberry Pi 2. One needs to first built a GPIO library similar to the one in Python. Later add modules to for interacting with different sensors and motors (PWM, camera, LED displays etc). Create a library and share it with others!
I'm working on something of the sort, but it's still quite experimental: https://github.com/jonathanBieler/GtkIDE.jl
&gt; Juno is not well polished. Huh? Do you have any specific issues?
Currently the best options are - IJulia - Juno - Atom with Hydrogen - Emacs Speaks Statistics To be honest none of are as feature rich and polished as Rstudio. I think IJulia is currently the most polished and easy to use of the options, since it can piggy-back on all the work that has gone into IPython and Jupyter. You can even run R inside the same notebook interface. To try it out all you need to do is Pkg.add("IJulia") using IJulia notebook() 
I also like using tmux + slime + vim
We are very close to having good ARM support. The things that need to happen are upgrade to llvm 3.7+patches (should happen in a few days), complete ARM ABI in ccall, and fix a couple of other failing tests, where there are x86 specific assumptions. I am personally hopeful that Julia 0.5 will have arm support.
Really? This would be quite wonderful. Do you know which issuse(s) on github track this progress? Also, any chance of pre-compiled binaries? Building from scratch takes a _long_ time on an ARM board. 
I started wrapping Mraa for the Intel Edison a while back, but never got to finish a test. It should also support RPi IO as well: https://github.com/sjkelly/Mraa.jl My original plan was to use the Edison+Jupyter+Julia+Mraa as an education platform but the costs were too high and the performance too poor to be feasible.
All ARM related issues are tagged with the arm label, and they will mostly get addressed as I said above. https://github.com/JuliaLang/julia/labels/arm The slow compile times are because of major performance regressions in LLVM, which is why Julia hasn't moved to 3.7 yet (except the arm port, since ARM support only appeared in recent releases of llvm). However, many of Keno's fixes are now in llvm 3.7.1, and we will include some additional patches to bring performance and memory to acceptable levels. This is mainly going through reviews, and will merge very soon. Once these things are done, we will probably also get a buildbot going, so that we can have nightly binaries, and at least some form of CI testing on travis. From there on, we need to get Aarch64 working, get folks aggressively trying things out and reporting issues, etc. and hopefully we will have arm ready to ship in julia 0.5.
It's a bit hard to install at the moment (I need to make a proper package) I'm fixing some issue on linux currently. Otherwise it's just a Jula script, so it uses whatever Julia version you are starting it from.
got it: http://k4webcast.mediasite.com/Mediasite/Play/2529ebcb20794942874d5c277c5dcc981d
Thanks. I guess I could make this a function. I was expecting Julia to have some initialization syntax for this.
Thanks for this. I've submitted the pull request to add it to the package directory, and made most of the changes you recommended. It's my first open-source contribution, and it feels like I've just performed magic!
Executing from the editor instead makes sense. Looking forward to seeing the further development
What use case does an interactive editor have? Like doesn't code always have to be reusable? So running it one line at a time seems unproductive. Or is it useful for stats or something?
In short, you can use it as a REPL and try things out to get an expression right before plugging it into your code. Also, you can use an interactive mode like that as a sort of notebook where you aren't trying to structure everything into a complete program at that time.
I just wanted to say that I finally caved and tried Atom yesterday, and it's just a really nice editor. Everything just sort of clicked for me and I was productive with it right away. Granted, it still has the same performance trade-offs for large files, etc. and it looks slower than Sublime, but it's such an intuitive editor that I can forgive it all of that. 
The Ink plugin seems nice, but the Hydrogen plugin already does inline execution (including plots). Admittedly, Ink has some improvements, such as tree views for complex data, but Hydrogen is already an effective liaison between Atom and IPyton/IJulia. I'm just worried about a duplication of effort. On the other hand, having a proper debugger with breakpoints would be so nice.
&gt; Like doesn't code always have to be reusable? No. c.f. Perl. ;}
Really incredible work, it looks like it has come a long way in interactivity and feedback.
The people behind LightTable stopped caring about LightTable.
It's true that many statisticians, among others, use programming languages more as tools to do stats than for writing reusable programs – so yeah, in those cases an interactive R-Studio or IPython like model is more effective than an edit-compile-run-debug cycle. Aside from that, I personally find that live-editing makes me *tons* more productive – if only because I don't have to reboot the entire IDE every time I make a change. More productive me means more features for Juno more quickly, so it seems like it would be a worthwhile investment even if it wasn't useful for anyone else. [More details on the workflow here.](https://github.com/JunoLab/atom-julia-client/blob/master/docs/workflow.md)
No, I'm expressing concern at where limited developer resources are being directed. Focusing on return for engineering time invested is important at this stage.
Yeah, well they don't take orders from you or me. They can do what they like.
A question: would anybody here want to talk about Julia? Seems like there are some experienced Julia users here. I am in the organisation of PyData Amsterdam and we'd love to have a speaker from the Julia community talk about the project! Please send it to somebody who might like to give a talk about Julia. http://pydata.org/amsterdam2016/cfp/
&gt; It does not display the REPL within the IDE. Do you mean that Julia's REPL stayed opened, or that nothing was printing in the console ?
:) It's awesome that the best scientific language it's starting to reaching maturity or starting to start to reaching maturity 
Hmmm. They they really should have used the inbounds macro since they did so in cython.
You're right, I'll add it. I am by no mean a Julia expert.
Now this is not the end (of Julia reaching maturity). It is not even the beginning of the end (of Julia reaching maturity). But it is, perhaps, the end of the beginning (of Julia reaching maturity). :D With apologies to Winston Churchill.
the simd macro could also be used
I have added it, it does speed Julia significantly on this benchmark.
Just tried this out and it works really well! Would be great to get plotting working too (gadfly or pyplot) either inline or in a new pane. Anyone know if this is being worked on?
Maybe this can get around your firewall, "Seaborn is a Python visualization library based on matplotlib. It provides a high-level interface for drawing attractive statistical graphics."
I'm generally happy with Gadfly.jl. You can try to use pyplot.jl in conjunction with conda.jl 
I will have a look at this
I have only used it briefly, but it's nice for investigating the data. The examples were quite useful for me.
It is true that Gadfly becomes faster after the first run. I am recently having problem using Gadfly as it depends on so many packages. The build fails on one or the other package. Since I am behind the firewall, it needs lot of time to resolve. As Julia is competing directly with R, I think native plotting will make it easier for new people moving from R to Julia, especially from corporate world how sit behind firewall. 
As an alternative, take a look at [Gaston](https://github.com/mbaz/Gaston.jl). The only requirement is having gnuplot installed. Use the unreleased master branch, though, since it's had tons of bug fixes since the last release.
The Plots.jl package is very nice. 
Actually [RStudio](https://github.com/rstudio/rstudio) is open source and made mostly in Java. Only 1.9% is R. Would it require a lot of work to replace the backend engine with Julia?
Some of the conversions need to be explicit, as well as subtracting 'A' before doing the modulo: bytestring((UInt8["STRING"...] - UInt8('A') + 2) % 26 + UInt8('A'))
This also enables [efficient comprehension syntax](https://github.com/JuliaLang/julia/pull/14848), 0.5 is going to be big.
That seems quite nice! Very "mathy".
Ordinary? Partial? Any algebraic constraints? How large? Are the derivative functions closed-form algebraic expressions? Julia is really good for this kind of thing since it has great support for automatic differentiation, which you can use to efficiently get exact Jacobians if you're interested in implicit methods.
The majority of our work will deal with PDEs. We are in very preliminary phases so I don't know specifics in terms of sizes and whether they are closed-form algebraic expressions. We are not concerned with analytic or exact solutions. Solutions utilizing numerical methods should be fine.
First off, make sure you're up to date; julia-client and Atom.jl are both on 0.3, and those versions should give better error messages if they can't find your Julia install, for example. Also, check the console (Packages-&gt;Julia-&gt;Toggle Console) for output. Precompiling Atom.jl takes a while – up to ten minutes on Windows – so you might just think it's hanging because of that. As an alternative, you could also try out the [bundles](https://github.com/JunoLab/atom-julia-client) we've recently put up. They're not battle-tested yet but in theory should have everything set up out of the box, so you can get going within a few minutes.
Didn't realize I needed a package installed in Julia, it's working now. Thanks a bunch!
Guess I should make the switch then.
the optimization community maybe, the Stanford optimization research group is big into it.
If you're dealing with PDEs and any form of significant boundary/initial conditions or complicated geometries you are probably going to have to rely on numerical methods. Writing robust, valid numerical solutions is hard. When you say your work is in optimization it seems to me that you might want to consider already developed, and vetted, software packages that you can base your work on. I'm not sure of Julia libraries that can help you in this respect, but hopefully others can point you in the right direction. Without more info about the types of problems you're trying to solve I would say that you should try to find out what your colleagues are using and see if that will suit your purposes. If you are interested in learning and developing your own numerical methods then Julia is probably a pretty good choice, although Python and C++ are probably more stable.
If you want to solve differential equations as part of an optimization problem, you can use [JuMP](http://www.juliaopt.org/). It is fast, but you have to discretize the PDE manually. ([Example with ODE and finite differences](http://www.juliaopt.org/notebooks/JuMP-Rocket.html)) The python package [Pyomo](http://www.pyomo.org/) is slower, but it can discretize differential equations automatically, using finite differences and pseudospectral methods. ([documentation](https://software.sandia.gov/downloads/pub/pyomo/PyomoOnlineDocs.html#_dae_toolbox))
Thank you, I'll take a look at Pyomo as well. I think that we'll need the ability to discretize the differential equations automatically.
Plots.jl (https://github.com/tbreloff/Plots.jl) deserves mention here. You can use the same code to generate plots from Gadfly, PyPlot, Plotly, etc. I'd like to see the "default plotting option" just be a default plotting syntax like Plots.jl and a user can freely change the backend. Edit: I missed that Plots.jl was already mentioned, but really, check it out.
Good point, sorry. I will update with dot styles.
[removed]
thank you!
So.... is it toxic waste, or an effort to clean up toxic waste? I could see either meaning ascribed to the term.
Pornography is exclusionary because (most of) it objectifies women as sex objects, but this is a gender-neutral reference to sex. Unprofessional, sure. Exclusionary? I'm not seeing it.
Could you elaborate on this, please? I am curious as to what you mean.
Ignoring the hyperbole of the blogger, I fail to see the difference between Julia's package ecosystem and R's. R's is bigger for sure, but Julia's is 99% as solid in my experience. Do people complain about Julia's package ecosystem?
Could you point us to that discussion?
Main language is "R"? Speaks volumes for his ability to evaluate ecosystems, which is not even nearly an appropriate metaphor placing technology as a type of biological/chemical process. Might as well say Julia is in trouble due to pesticides just like the bee population. It's not even wrong, it is incomprehensible.
Sure, what is unclear? 
Because I had never heard about "superfunds", and looked it up. "Superfund or Comprehensive Environmental Response, Compensation, and Liability Act of 1980 (CERCLA) is a United States federal law designed to clean up sites contaminated with hazardous substances and pollutants." I mean, look at the list of superfunds in Louisiana: https://en.wikipedia.org/wiki/List_of_Superfund_sites_in_Louisiana Afterwards, I realized it could be a "positive" thing as in a poisoned place that is being cleaned, but I'm not sure that was what the author was going for.
In a comment, the author of the blog post said about the Julia ecosystem: "On the ecosystem I've found it to be a bit sparse (not surprising; 800+ packages in as long as the community has been around is phenomenal work!)". So I believe it was just a comment on the current sparsity, and not about being a poisoned place.
I did some work to create Julia bindings for the Lego Mindstorms Ev3 as part of a class project. Low-level bindings code is here: [rdeits/Ev3.jl](https://github.com/rdeits/Ev3.jl) and documentation is here: [rdeits_18.337_report.pdf](https://github.com/rdeits/julia-ev3-report/blob/master/report/rdeits_18.337_report.pdf). Unfortunately, I was never able to get Julia running natively on the Ev3 due to its pretty limited ARM processor. Instead, I used ZeroMQ to remotely control the Ev3 from a laptop running Julia. But I'm excited to hear that Julia 0.5 may have full ARM support.
If they are in a foreign non-english speaking country for readability or if they are using mathematics.
They are used pretty extensiviely in the Distributions.jl package. For example look at the [normal](https://github.com/JuliaStats/Distributions.jl/blob/master/src/univariate/continuous/normal.jl) distribution 
My favorite uses are all the super and sub-scripts. E.g., `Fⱼ = Eⱼ + ϵⱼₖₗ * ẋₖ * Hₗ` or `χ²` or `x̂′`
Personally I think they are perfect... inside modules. I use them all the time to have consistent use of symbols between a paper and the accompanying code. However, I do not like to expose them in scripts and code I expect other people to run and "play" with. I also do not like being given a script with unicode characters I might not know how to type on my keyboard. Inside modules they increase readability, but in the REPL they simply slow you down (in my personal experience...).
&gt; Why pick an arbitrary spelling / different name rather than just use the correct symbol? Because it's a PITA to write them out on a standard keyboard, although I could just set an abbreviation in vim or commit hook in git to do that for me.
Yeah, but like I say code is read a lot more than it's written. It's definitely worth setting up your editor for latex-unicode e.g. the julia-vim plugin. It's 2016: Unicode FTW. Embrace it!
Wow, maybe I could finally learn APL!
Good work on the repo. (Starred!)
I just want to be able to write libraries for R in julia... that'll be the day :')
If it helps you or anyone else who will be reading the code understand it more clearly, then go for it? Underscores are sometimes used as "placeholder" variables with no useful name, a la (_, thingIwant) = f(foo, bar) for a 2-output function, or likewise in function definitions where some methods might ignore some input arguments.
Thank you for explaining, But, aren't the placeholders in variable names different from the digit separators?
Good I am not a big fan of Matlab due to experience with licensing and speed issues in the past.
&gt; aren't the placeholders in variable names different from the digit separators? Yes, these are just separate stylistic reasons you may see underscores various places in Julia code. It's also just a part of identifiers, either for `long_underscore_names` or `_private` naming conventions.
This is not recommended if you want the code to continue working in future versions of julia, as the internal representation of strings is likely to change.
rotate("PQRSTUVWXYZ', 17) gives a LoadError: syntax: invalid character literal.
Thank you for putting effort into this. It looks like it has great potential.
If you are interested in contributing your optimizers to a Julia package, take a look at MXNet.jl 
sure, It would be an honour to commit to MXNet.jl - the question though is how to start? 
Also, if you're generally interested in optimization routines, see if any of the packages in the JuliaOpt organization could be of interest. http://www.juliaopt.org/
Yes, it should be ``` rotate("PQRSTUVWXYZ", 17) ``` Strings use double quotes in Julia.
Thanks. I'm mostly using it to develop it, so it is somewhat usable, but there's still a lot of work to be done.
Thanks for the tip, didn't make a difference though. Oddly enough I have no problems running an interactive Octave shell.
Bummer! Well, if I get a chance to, maybe I'll try installing Julia on my work computer and see if I can reproduce it. I'll get in touch if I figure anything out.
I've had this issue for a while in Windows 7. Looking one more time for a solution, I just stumbled on [this comment](https://github.com/JuliaLang/julia/issues/5271#issuecomment-121928871). Lo and behold, ESS works now. For Julia 0.4.3, I saved this code: using Base.Terminals: UnixTerminal using Base.REPL: BasicREPL, run_repl type PipeTerminal &lt;: UnixTerminal in_stream::IO out_stream::IO err_stream::IO end run_repl(BasicREPL(PipeTerminal(STDIN,STDOUT,STDERR))) and added to my .emacs: (setq inferior-julia-args "-L &lt;path-to-the-file.jl&gt;")
That was it. Brilliant, many many thanks!
How about at least to bytecode like java? Recompiling everything every time I run something has got knobs on. 
I'm building a system where performance of callbacks is somewhat important, and I expected these changes to be of help in that. However, running this test case (on JuliaBox), it seems 0.5-dev is signifcantly slower than 0.4.3. function applyall(funs::Vector{Function}, reps=1000) for i=1:reps for f in funs f() end end end funs = Vector{Function}() for i=1:1000 push!(funs, ()-&gt;nothing) end for i=1:4 @time applyall(funs) end On 0.4.3 I get: 0.008823 seconds (232 allocations: 12.766 KB) 0.003282 seconds 0.003119 seconds 0.003135 seconds On 0.5-dev I get: 0.026008 seconds (294 allocations: 18.022 KB) 0.018319 seconds 0.018292 seconds 0.018262 seconds So roughtly 18ns per call, which is pretty terrible. I noticed 0.4.3 seems to call through a function pointer loaded for each element in the vector, whereas 0.5-dev calls through jl_apply_generic. Is there some way to fix this? Maybe a function type with more signature specificity?
So that's not idiomatic, in julia you're supposed to name function height(what::Tree) .... But if you want to force it, something in this spirit should work. Type Node .... height::Function end Node(....,yourFunction,...)
ok, so there is no equivalent of methods defined on a type. thanks! (actually, tree.insert(node) happens to read really well to me)
if it's not idiomatic, it doesn't make sense to try to go around the language's intent since that would probably create more headaches than it's worth in your example, are you using a Constructor? http://docs.julialang.org/en/release-0.4/manual/constructors/#outer-constructor-methods
What's the performance like vs hand written loops?
For this you probably want a Tree type (containing the root node?), and naming you module differently. You can also export functions out of the module so you don't need to add "Tree." everywhere. It's also more Julian to use Base functions on your type, for example insert! or push! instead of add_children!. This allow to have more generic code across packages.
seems like modules are useful for namespacing (among other things) in my case, i'm using the add_children method to do multiple things (append child into parent's collection &amp; set the child's parent). i think this convenience method makes sense, the alternative (doing multiple actions manually) is more error prone. what do you think?
great points, it is better to use standard conventions than creating your own one-offs that people have to learn.
Just to be sure, are you printing your output? example.jl: print("Hello\n") a = 3 +54 print(a) will only show "Hello" and "57" when run from command line. 
&gt; atom You can now get inline evaluation in atom using the [julia-client](https://atom.io/packages/julia-client) plugin (see set-up instructions there). It works fabulously for me and gives a very nice workflow.
A single throwaway line in a long blog post about a guy quitting R development? This is not a very good submission.
Have you tried [ExcelReaders.jl](https://github.com/davidanthoff/ExcelReaders.jl)? It looks to be well-tested and wraps the python package xlrd, which supports xlsx.
You could also try PyCall with openpyxl (see http://www.python-excel.org for more python packages).
Nevermind, I've solved the problem using SymPy. However I'm curious whether it is possible to do in Julia or not. I've already read https://github.com/JuliaLang/julia/issues/2636 so it seems that there is no general facility for symbolic computation. However, is it possible to have a simple implementation of surds (like the one mentioned in the post)?
I was hoping that somebody had written a package already for something similar but didn't have my hopes up after reading the issue on Github (see my other comment). Unfortunately, it was a fairly small task to do so I didn't bother trying to come up with a Julia solution and went with SymPy instead.
Just so you know, you can also do `@doc [topic]` in the REPL. Definitely not as quick as `?`, but it's good to know about.
Totally agree with that. I'm writing a library and want stuff to be allocated on the stack. The only way to do this is with immutables wrapping tuples and tuples are a pain to work with. I just want normal mutable stack allocated arrays... :(
~~i'm not sure i understand. julia's arrays are stack allocated and mutable.~~
Julia arrays are allocated on the heap. Which is expensive if you want to rapidly create / destroy them.
Update: https://ironholds.org/blog/an-r-update/
This seems useful, I ran into the problem where I wrote a script using Gadfly and Dataframes, and then found that when plotted in a Jupyter notebook, it slowed my web browser to a crawl, so I went back to PyPlot and native arrays.
I've read through most of this. It is a pretty interesting look into the prototyping process when using a language of this nature. Perhaps the biggest takeaway from this all is that had he had to do an edit-compile-run-debug loop, he may not have finished in time or would not have been able to make as many insights in the time allotted. Being able to nimbly make a change and see what happens and then quickly do it again helps with the prototyping and lets one "fail faster" so one can succeed sooner.
I prefer tmux + slime + vim, then you get a Rstudio style split screen where you can send commands from a text file over to the julia REPL. 
I realize that this is bikeshedding, but I noticed that the author, who works on the Julia compiler, ignored the Julia style guide and went with camel case for almost all of his code. I happen to disagree with the Julia style guide suggestions, but I think if I were going to 'rebel' I'd use snake case (rather than go for long nondistinguished conjoined words, like Julia and German), which is at least accepted by the style guide. Having three styles is inelegant. I think we should follow the style guide as much as possible.
I think you can do `?Point.x`.
That doesn't print out the docstring for field x. When I do that at the repl, I get "Point has fields: x and y".
Sounds like you might have mispelt – it shows the docstring for me on Julia v0.4.4, but shows that message if I do e.g. `?Point.m`.
Your heuristic is a good starting point, but it lacks some robustness. I'm not familiar with exactly how `isprime` checks for primality, but presumably it sequentially goes through possible prime factors of `n` and checks if they actually are factors. The problem with stopping after a certain amount of time is that you're checking all of the low possible prime factors first, but there's nothing that makes those numbers more "special" than higher prime numbers (that is, the number you're testing could just as easily have large or small factors). There are methods that make this line of reasoning more robust though, such as probabilistic primality tests like the [Miller-Rabin test](https://en.wikipedia.org/wiki/Miller%E2%80%93Rabin_primality_test).
&gt; I noticed you are doing a huge calculation in a loop so that you are calculating 6380! (a constant) 3190 times, lmao. I did open by saying it was "strictly for illustration." All those redundant calculations of 6380! still execute more quickly than @time isprime(factorial(big(6380))+6389) It only reinforces the point if it wins the race on square wheels.
Thanks a lot. I will try to adapt to Julia's way of handling matrices, but the mindset of Matlab is still strong. For visibility in case anyone else has the same question, there is "repeat" for Julia which can function as repmat for dimension greater than two. repeat(A, outer=[n1,n2,...nk] 
Julia basically compiles most of its runtime at startup. Thus it takes a while to start, but runs very fast once it is running. Python does not compile, thus has a fast startup, but runs slower. Julia is working on caching the results of the compilation, thus making only the *first* startup slow in future versions. 
Same problem with same installation. I call it a compiler problem because a one-line program takes just as long as a complex one, there is no sensitivity to the source. A 100,000 line program of simple integer arithmetic should compile instantly.
[removed]
I think some versions have that disabled on Windows due to missing backtraces when the compiled code is used.
Why would you have to do derivatives manually if you were using JuMP? How complicated are the ODEs you're solving? You're going to get better performance and better utilization of the problem structure if you provide the optimization solver with full knowledge of the system you're trying to optimize, rather than hiding parts of it inside a black box function evaluation.
I'd like to support both shooting methods (which require solving ODEs to evaluate the objectives and constraints) and collocation methods (the idea you're suggesting). I'm assuming that JuMP won't be able to compute the derivatives for the shooting methods, so I will have to do so myself. The ODEs will be provided by the users of the trajectory optimization toolkit, so I don't know their complexity or structure in advance.
Even shooting methods require discretization of some kind, so you can implicitly include all the intermediate discretization variables in the optimization problem.
Thank you, the following code went from 5 seconds to instantly with the following julia --precompiled=yes perm.jl word = "normal" perm = permutations(word) k = 0 for p in perm q = join(p) @printf "%s\r\n" q k += 1 if k &gt; 10000000 break end end 
But shooting methods still require that you differentiate IVP solutions, which I assume that JuMP can't do. What I want is to hand JuMP a multiple input, multiple output "black box" (as well as its derivatives). The user defined function support I see in JuMP is not designed for multiple-output "black boxes", and the mailing list posts I've seen suggest that JuMP's developers aren't looking to extend support until the code base has matured some more. Also, another problem I realize I may have is duplicate ODE evaluations. Evaluating the objective and constraints (as well as their gradients) will probably require solving the same IVP, but MathProgBase is set up in a manner that makes it hard to re-use computations between the objective and constraints. If function evaluation time is significant (which it may or may not be), I'll probably need to add some form of ODE result caching to my implementation to speed things up.
Express the IVP as constraints and variables. No need to solve it outside of the optimization problem as a black box if the differential equation functions are given (by the user, whatever) - assuming the solver can handle equality constraints, it's going to be good at solving implicit systems of equations, and the distinction between "IVP solution" and "optimization solution" is artificial and unnecessary. You can get better descent properties by allowing evaluations that aren't feasible according to the IVP as intermediate evaluations in the optimization problem.
I'm not sure I fully understand you. Are you referring to collocation methods? Part of the reason I want to do these optimizations is to investigate shooting methods versus collocation methods, so I feel that I need to implement both.
That's a fairly simplistic description of direct collocation, limiting to a first-order approximation. You can easily use higher-order methods and variable step sizes in a collocation approach, at which point it's completely equivalent to multiple shooting except that you don't have to treat the integration as a black box or accumulate derivatives in any special way for integration steps vs shooting boundaries. The distinction is artificial and doesn't buy you much in a world where sparse solvers and automatic differentiation are available, easy to use, and you don't have to think about them.
You'd have to fix the maximum allowed number of variables you expect to ever need, but the locations of the intermediate discretization points don't need to be set beforehand or at fixed spacing. You could even do "error control" constraints like a CFL condition to deal with spacing of the time steps. And when those error control constraints are easy to satisfy and the dynamics are well-behaved, many of the variables can just result in trivial conditions grouped at the edges in a way that the sparse solver would effectively eliminate them. Almost anything you can do in an external integration routine, you could do as part of the optimization problem formulation. Implementing classical shooting methods in the conventional way with black-box separate integrators will require user-defined derivatives in a way that's a bit experimental with JuMP, but what I'm getting at is the existence of shooting methods in their classical form is just an artifact of the tools - fancy integration libraries were historically more widely-known and easily available than fancy sparse optimization and automatic differentiation tools.
Super useful to know, thanks. Can you point me to where I can read more about this?
https://github.com/JuliaLang/julia/blame/ccc8fc91662a919390855f554df7137182911e25/src/init.c#L76-L81 https://github.com/JuliaLang/julia/pull/11640#issuecomment-118012119 Since master is on LLVM 3.7 I think backtraces might theoretically be in a better shape now, but I think it still needs some work in codegen/LLVM to fix all the way.
Which university? I used it briefly for a parallel computing application that never fully came to fruition so I don't think I have much helpful feedback.
This is one of the reasons why I chose to start looking more seriously into Julia. I made it to the top 10% in this contest and this was using C++. Obviously, having a fast language will not help if you have a bad algorithm, but all other things being equal (and I used similar methods to solve the problem), I found the fact that the 2nd place was implemented in Julia quite impressive. Add to that my interest in machine learning and wanting to try out something new and the choice seems inevitable :-)
When I read the OP they mentioned the deprecations in 0.4 I could only imagine what that would be like in a large scale production environment. For now I'm using Julia more or less to play around with at work, (I also like Gadfly's plotting, although I would like jpg support, haven't gotten PNG via Cairo to work on Windows yet) Going to be interesting to see Julia grow the next few years
Good. But those "solutions" aren't really full. There are many details and the actual implementation of proper_divisors(), factor() etc are key on the actual learning and solving the problems
It uses garbage collection so it might not really well suited for heavy real-time applications that need smooth run-time like games. 
What is general-purpose computing? Every program you write has a purpose, and different tools will suit that purpose better or worse. I'm not trying to be pedantic here. A language which is good for writing a web browser might not be great for writing machine learning code. Is the great-for-browsers language general purpose? Anyway, Julia is very fast and quite expressive, which makes it appropriate for a wide variety of tasks. But as /u/Staross points out, there is garbage collection, so if you need very fine-grained control of memory usage and when/how cleanup happens, Julia probably isn't the right choice. There are tons of packages and bindings (pkg.julialang.org shows us pushing 1000 packages right now), but the exact package or binding you need may not yet exist. Short answer: Julia is awesome for many purposes well beyond scientific computing, but I probably wouldn't use it to write a video game from scratch. Of course I'm not crazy enough to try to write a video game from scratch, so that may be the problem!
I know only a little about Julia, but I'm told that, while the language implentation doesn't yet offer a wide range of libraries for various services like GUIs or graphics, it can easily call existing libraries written for C. So if you're willing to 1) separate your app's main code from the support code that interacts with those libraries, or 2) mix those foreign function calls into your main code, then yes, Julia can be used to do anything.
What kind of typo? You can either fork, fix and pull, or simply open an issue and hope someone else has the time to fix it - but most people are already very busy with the aspects that are dear to them. https://github.com/JuliaLang/julia/blob/master/CONTRIBUTING.md has a section on improving documentation.
Julia is aimed at high-level programming. As far as I am aware, you cannot (currently) compile it into a standalone executable, and its runtime includes garbage collection. Those two facts alone make it unsuited for the following tasks that you mentioned: * Kernel code * Video games (garbage collection and consistent frame timings don't work well together) For kernel code and realtime code you generally need a language with a very lightweight runtime (C, C++, Swift, Rust, etc...). As /u/tarrosion mentioned, there is no such thing as a "general purpose" programming language. Every language has its weaknesses (and every good language has its strengths). A language that is suited to writing kernel code would not be suited to solving the problems that Julia is designed to solve.
GitHub makes the fork/fix/PR cycle really easy, especially if you use the web editor for quick fixes such as this one. Wouldn't recommend the web editor for more extensive changes though.
Thanks! I made the change. It was a lot more simple than I had expected. 
This does not work. The cmd only blinks and I can see an error message shown. Actually, I can get it to work when I move my script.jl to path/to/julia. But, I really want to run the script located in a different path. This works when I move my script (model.jl) to C:\Users\Owner\AppData\Local\Julia-0.4.3\bin: cd C:\Users\Owner\AppData\Local\Julia-0.4.3\bin julia model.jl This does not work: cd C:\Users\Owner\AppData\Local\Julia-0.4.3\bin julia C:\Different\Path\To\model.jl 
I cannot even see or read the error message, the command prompt closes too fast.
Didn't you try running this in the command line manually first? Don't try to do two new things at once.
I can't wait till windows finally gets a legit shell this summer....
Haha, that's awesome.
Considering the post-apocalyptic setting of the show, maybe Julia needs an [Apple-style clause](http://www.cultofmac.com/140224/itunes-eula-wtf/) in its licensing.
I don't have a ton of experience with open source projects, but the folks running this one are great. I'm pretty new to julia, but I wrote a simple little package for running BLAST (a very popular sequence search algorithm) within bio.jl, and several members of the team helped me throughout. The gitter chat is super active and friendly too - can't say enough good things. 
I always thought best way to learn a language was to have your own project/idea and start trying to implement it in the language. 
I would agree, but after years of toying with various languages I realized that any language with an existing ecosystem develops a certain feel to it, and the best way to get the hang of this feel is to familiarize yourself with good code in the language. I have written too much of C-like code in Python when I was starting out, so I would have definitely appreciated someone to put me on the right track from the get go. 
On the mailing lists the julia standard library is often cited as high quality, idiomatic code. OP could have a look in there. For example, almost all of the code for dealing with arrays is handled in Julia code. edit: I should probably point out that the standard library lives within base/ in the source distribution.
How about [A Month in Julia](https://github.com/DataWookie/MonthOfJulia)? 30 or so short sessions exploring some practical areas using Julia. You can probably do the first "week" or so in about an hour or less.
You've pretty much described the ideal use case for Julia. In my experience Julia is as expressive as python and a joy to work with. Without any performance tuning it is ~1.5x slower than C and relatives, but of course this depends on the problem. I've also found that optimizing Julia performance is pretty painless. The built in profiler and timing macros are useful, finding type instability (and helping the compiler/yourself with type annotation) works well, etc. In my own research, I've found that the slight (often approx 0) performance hit from using Julia rather than C++ is more than outweighed by the productivity increase (faster development, etc) of using Julia. Two notes: - If you ware writing code that somebody else will need to run in years, Julia is not the language for you. The base language is almost-but-not-quite stabilized; the package situation is still developing. There are &gt;900 registered packages, but the community is still settling on best approaches and occasionally "deprecates" previously mainstream packages. - If you want to sometimes solve those NP-complete problems to optimality e.g. as benchmarks, then you *absolutely* want to be using Julia. Check out [JuliaOpt](juliaopt.org) and in particular JuMP, which makes interfacing with solvers such as CPLEX and Gurobi stupid easy. 
Based on just this snippet, it's not at all clear that the recursion terminates or that f() returns anything before finishing the for loop. Check those things. 
It does terminate, and return something every time. It's a crappy example, I'm sorry. Also, the problem seemed to only be there when the function was a locally-defined closure, just making it global and passing it the extra parameters as arguments fixed the problem.
some good response https://news.ycombinator.com/item?id=11695095
The current version is 0.4.5.
Mailing list announcement: https://groups.google.com/forum/#!searchin/julia-users/dsge/julia-users/SpiE77AL4kM/z_nEvRvgCwAJ
Let's micro benchmark a cold start to "hello world" on a Jit compiled language. Really, how dumb can you be?
I really love how far the R and python functionality in Julia has improved over the past 2 years! 
You might want to check out my [DifferentialEquations.jl](https://github.com/ChrisRackauckas/DifferentialEquations.jl). I am making a bunch of solvers for standard differential equations. Right now I have finite element implementations for heat and Poisson equations, including stochastic variants. I'll be adding a bunch of stochastic differential equation solvers this weekend, and then will be doing some finite difference methods, starting with heat and Poisson, and then implementing some solvers for Stokes equations. I am on a mad rush to get these things all in (some for homework, some for research, some to have to present at a conference in the next month) so the development speed is quick, and convergence tests are part of the package to ensure correctness. Let me know if there's an equation/method you're interested in. Put an issue in and we can discuss what implementation we should look at.
Do you mean behavior like this? julia&gt; function f(x) x[1] = 47 end f (generic function with 1 method) julia&gt; myVec = [1,2,3] 3-element Array{Int64,1}: 1 2 3 julia&gt; f(myVec) 47 julia&gt; myVec 3-element Array{Int64,1}: 47 2 3 julia&gt; function g(x) x = [1,2,3] return true end g (generic function with 1 method) julia&gt; g(myVec) true julia&gt; myVec 3-element Array{Int64,1}: 47 2 3 I believe the interpretation here is that references to variables are not the same as variables. So x = [1,2,3] is creating a new object [1,2,3] and setting the symbol :x to point at that object. On the other hand, x[1] = 47 is syntactic sugar for a call to setindex!. As for whether people are happy or not, well, that's subjective. I'm happy, but to each their own. I think this behavior is quite common across programming languages. How would you like it to work?
Haven't tried it myself but you should have a look at [ChoroplethMaps.jl](https://github.com/penntaylor/ChoroplethMaps.jl). Vega.jl also has a [cloropleth function](http://johnmyleswhite.github.io/Vega.jl/choropleth.html), however the map is currently [fixed to US Counties/States](https://github.com/johnmyleswhite/Vega.jl/blob/642e129beaac3add826aa4718af957678910620e/src/derived/choropleth.jl#L12-L35). It seems to be reading topojson behind the scenes, so perhaps not difficult to modify. For geojson there is [GeoJSON.jl](https://github.com/JuliaGeo/GeoJSON.jl), though this is currently not directly tied to visualisation. 
I'm new to Julia, but one of the reasons I'm converting is because the numerical syntax is so much more intuitive from a mathematics perspective. It seems silly but, as someone who does math all day, even just being able to write "3x" rather than "3*x" makes the code much more legible and less error-prone.
I used MXNet.jl for a CV / ML project and was happy with the results: https://github.com/bonsairobo/mxnet-neural-style MXNet has solid tensor shape inference and error messages to go along with it. There's also Mocha.jl that's written by one of the MXNet authors. And here's a list of what look like fairly modular stats packages (more along the lines of what you mentioned in your post body): https://github.com/JuliaStats
[removed]
I'm the creator and maintainer of [ChoroplethMaps.jl](https://github.com/penntaylor/ChoroplethMaps.jl). It doesn't support geojson at the moment, but it shouldn't be terribly difficult to add in a geojson provider. Feel free to file an issue at the repo, and provide some sample data I can test against.
If I understand it correctly, you shouldn't loop over pmap. pmap takes a function and an array, and apply the function to each element of the array, calling free workers when needed. It already manages workers for you. if you want to call a function on a specific worker you can use remotecall.
It does make sense, I'm not sure what the issue is. 
Why not define a worker function for pmap? Something like the following: function myfun(a::Int64, b::Int64, c::Float64; runs::Int64=1) d = max(a, b) args = [(r, d, c) for r in 1:runs] err = pmap(myfun_worker, args) return err end @everywhere function myfun_worker(args) r, d, c = args srand(r) return silly_compute(d, c) end @everywhere function silly_compute(n::Int64, x::Float64) randn(n) .- x end myfun(6, 7, 3.0; runs=10) As it relates to your example, myfun_worker would perform most of the work of learningExperimentRun, silly_compute is like your agentInit, getLearnerErrors, etc., and myfun is a convenient wrapper function for the whole process.
For anyone reading this in the future, it seems the issue was [(kinda) resolved on Google groups](https://groups.google.com/forum/#!topic/julia-users/R64TDEVKBvs).
If I understand your question correctly, I think you're looking for function f!(z) for i=1:length(z) z[i] = 25; end z[:] = z + 5 end 
Should be the same if you do things correctly.
In general loops in Julia are very fast, but it's always going to depend on your particular application. This paper does some benchmarking in the context of computationally intensive economics models. http://www.econ.nyu.edu/user/violante/NYUTeaching/QM/Spring15/Documents/comparison_languages.pdf A quick summary (I rounded some of the values below) They find C++ and Fortran to be the fastest. Julia is 2.7 times slower Matlab is 10 times slower Python with PyPy is 44 times slower Python with CPython is 150 times slower R is 250 times slower (after compiling) and 475 times slower without compiling. Matlab, R and Python all were able to be speed up to being roughly comparable with Julia if you are willing to do a hybrid programming approach with some parts written in Mex/C++/Numba. Matlab with Mex 1.5x R with Rcpp 3.66x Python with Numba: 1.57x
If anyone wants to give it a try, here's a link to their code: http://economics.sas.upenn.edu/~jesusfv/RBC_codes.zip (taken from http://economics.sas.upenn.edu/~jesusfv/research.html) I'd be interested to see how much Julia has improved since v0.2 (but sadly I don't have time to give it a try this week)
Can you please provide the context? Exact name (+syllabus) of the course would be a valuable hint ;) 
There are separate subs for homework, but they don't give straight answers either
See: https://groups.google.com/forum/#!msg/julia-users/egVN31WJMAo/_7zQaunoAwAJ https://groups.google.com/forum/#!topic/julia-users/uTu3G28evbE
Thanks! This is exactly what I wanted to discuss.
Also finish your English homework.
I've not used Julia from C but in the docs further down you can see how to call the julia funciton in C. Can you use the c-printf to print this value? Does that go to the file/stdout? http://docs.julialang.org/en/release-0.4/manual/embedding/#calling-julia-functions
info@juliacomputing.com http://juliacomputing.com/ Or ask on the julia-users mailing list https://groups.google.com/forum/#!forum/julia-users
Thanks!
I guess the folks who understand this are upvoting and moving on, while those who don't (like me) don't know what to comment... I lost track of things around "a new language (within Julia) that would take general graphical models (in abstract form) and ‘compile’ them to Julia code", everything after that read like /r/VXJunkies to me. :(
I'm in the DC area and have done work involving that and could give a talk. Also you might want to check out http://quant-econ.net/
Thanks for the reply but I found a speaker. And thank you for the link.
or `floor(pi, 0)`.
Huh, weird. What Julia version? The output style is supposed to be platform-independent, but maybe there was some weirdness in LLVM 3.3. Please try with a recent (0.5 nightly) build, and file a bug if no improvement.
version 0.4.5 (2016-03-18), Windows x64 Actually I found this in src/disasm.cpp: &gt; unsigned OutputAsmVariant = 0; // ATT or Intel-style assembly where 0 = AT&amp;T and 1 = Intel. Just tried the 0.5, of course it still shows AT&amp;T
The C assembly version just shows setting the arguments into the stack and `call`ing `sprintf`, and the Julia output actually is the entire body of `sprintf`, so if course it's longer. What an idiotic article.
I've been working on some Julia code since 0.3.12 (I think). So far, there have been very few changes required in my code after each upgrade. It has been far easier to follow the upgrades with my existing code than it was to move the same code from Python 2 to Python 3. Having moved my own code from Python, I have to recommend at least giving it a shot. The speed increase for me was totally worth the time spent porting the code over.
I want to reimplement some of the library features in another way to best meet my current needs. And in my opinion the best way of doing this is to "undress" the language and see what it can do. In addition, it is a good way to learn the internal design of the language. But unfortunately, in the case of Julia, it is impossible or very difficult. Even the language developers can not (or do not want) really say anything about this. Instead of answering my question, they try to convince me that I do not need this - "Hey man! Why would you do this? The library is very good! You can't do better!". Of course, you can dig into the source code to deal with this, but it's not worth it. 
Thank you! It's really informative :)
Thanks! Yes so this is definitely the next step for the package, getting these models to scale to as large of datasets as possible. The LDA algorithm that I used is the standard variational Bayes algorithm used by Blei in his original 2003 paper, which basically gives the user complete access to the data in all nodes of the PGM. It looks like Mallet probably uses collapsed Gibbs sampling, which I believe reduces the memory and computational overhead considerably, but at the cost of losing knowledge about certain posterior distributions. I'm not sure about this but I think in general MCMC uses less memory because it doesn't introduce variational parameters the way VB does, but I could be mistaken about this as I don't know that much about MCMC methods. All things being equal, variational Bayes is generally supposed to be faster than MCMC, and there do exist versions of collapsed variational inference for LDA, which I believe have been shown to outperform collapsed Gibbs sampling in terms of speed (since MCMC is unbiased you will never beat its accuracy as time -&gt; ∞). As for trying it out on larger datasets, the largest dataset I have at hand is ~330k documents with ~15.5k lex, and just running a few iterations at 10 topics it looks like it would train in maybe a few hours on a dual-core 2.5ghz powerbook with 4GB of ram. But yes GPGPU support, collapsed variational Bayes and possibly also stochastic coordinate descent are definitely the next step for this package. If you have a larger dataset lying around you could definitely try it out as a comparison with Mallet and post your benchmarks (I don't know Java myself unfortunately), as I would be curious myself. 
**Challenge:** So I have just uploaded an HMTM folder to the TopicModelsVB.jl repository. The hidden Markov topic model (HMTM) is a model I hoped to implement which takes word order into account. Unfortunately I was unable to successfully optimize one of the coordinates in the objective function. Included in the folder is • HMTM.jl: Runs as is and simply leaves the problematic coordinate fixed. • HMTMREADME.md: Explains how to integrate the HMTM.jl file into the package (it's easy) so that you can try your hand at completing this model. It also contains a few of my own personal thoughts on the problem. • HMTMVB.pdf: A set of very comprehensive notes explaining the algorithm and exactly where I got stuck (warning: they are math heavy). • PC Dataset: A new dataset of ~12k articles from *PC User Magazine* 2004 - 2012 which preserves both word order and stopwords. You'll of course get complete credit for completing this algorithm (your name at the top of the code or w/e you want really), however if you do decide to submit a solution the code will fall under the same MIT open-source license that the rest of the package is released under. Bon chance!
I had the same idea as you, maybe we could collaborate at trying to convince them into helping us with it? Could you please bring me into your discussions with them? I am just starting reading up on the mailing list links from above, maybe you could help me not waste time :).
The first link from above is what you need. I tried to explain my needs, but gave up. English is not my native language and sometimes it is hard to me to explain a certain things. As for our problem, in the library sources I found something called intrinsic functions. For example, in the int.jl file you can find that integer multiplication is implemented by using three functions: box, unbox, mul_int. When I tried to inspect them by typing their names in the Julia's REPL, I got this: julia&gt; Base.mul_int (intrinsic function #5) julia&gt; Base.box (intrinsic function #0) It seems that all basic low-level functionality is implemented using the intrinsic functions. So, the authors are lying (a little) when they say that all language features are implemented in the standard library and without this library you can't live. I am sure you can. With the intrinsic functions. Unfortunately, no one could (or they simply don't want) say anything about this "magic" functions. The only guaranteed way to understand this - digg into the source code or, maybe, do some reverse engineering. But I am afraid that even if you can understand this low-level API and can build something on its basis, the next Julia release could ruin all your efforts. I mean that this API is not public and potentially is not standardized, so it may vary from version to version.
What are the current alternatives in Julia?
Currently? As far as 'drop in replacement' goes? There really isn't one. Either they are for Cuda or openCL only, and I've not seen anything which just overrides all the standard Julia operators like ArrayFire does. Once there is one, you will likely find it ... at either https://github.com/JuliaGPU or https://github.com/JuliaComputing The finalizes are a bit of a pain, but, I don't think there is a better way of doing things around this (unless the Julia garbage collector gets rewritten again... and even then, the ability to throw stuff out as you need it within a block is pretty compelling.) If anyone else has one, I'd be happy to give it a go and report what I find :) *edit* you CAN use mxnet... using MXNet a=rand(Float32,200,100); # generate a (200,100) random matrix in julia A=mx.copy(a,mx.gpu(0)) # copy the matrix to GPU No.0 and return the reference to it b=rand(Float32,100,300); B=mx.copy(b,mx.gpu(0)) C=mx.dot(B,A) # 2D Matrix Multiplication but it looks a bit more painful. and http://www.admin-magazine.com/HPC/Articles/Julia-Distributed-Arrays talks about Julia picking it up natively at some point, maybe. 
Thanks for the analysis. I've been considering using ArrayFire for my topic modeling package, I'm curious if you know the standard way to extract bit types from AFArrays. For instance if I have an AFArray vector `a`, when I try to get the value at the first index `a[1]`, it still returns an AFArray. The hack I've found is to basically do `sum(a[1])`, but this isn't particular elegant. 
Wouldn't it be better and more educational to code from scratch the network architecture and the learning algorithm?
I haven't had to, I'll check when I get home tonight if I can find a nice way.
Care to explain what finalize and serialize do? The only thing the docs say is `Register a function f(x) to be called when there are no program-accessible references to x. The behavior of this function is unpredictable if x is of a bits type.` Have you tried to use https://www.techpowerup.com/gpuz/ for GPU monitoring? 
Finalize normally gets called by the garbage collector when it is being thrown out of memory (because nothing is using it anymore). it may be used to clean up file handles or whatever other resources the object is using. In this case, it recycles the memory on the graphics card so other things can use it. So, you should call it whenever you are no longer using something on the card, so it can get rid of it (since the garbage collector is way too slow to call all the time, and it can't see how full the graphics card is, so it may get called way too late). Since I am typically using the card to its fullest, I need to get rid of everything I can as soon as I can. I sadly can't use something outside of arrayfire to check the graphics card memory, since... arrayfire keeps hold of the memory to be used for other arrayfire collections. Allocation takes time, so it does this for speed. Unfortunately, it means that other things can say 'you have no card memory left' when there is plenty to be used. 
Is that the only talk missing?
Most if not all the talks are in now.
Pick something you want to do (i.e. a class project, or a research project), convince yourself it's a good enough idea to start the project in Julia, and get far enough that you have to finish it in Julia. Then you'll know the language :) The reason why it's important to do it this way is because you'll never be "fastest" with a new language right away: you already know all the tricks for something else! It's only after you have to fight through and learn all the tricks that you come out the other side with a new tool.
People may also be interested in this Juliacon talk: https://www.youtube.com/watch?v=Vd2LJI3JLU0
Thanks!
One or two of the tutorials aren't up yet.
Yep, I know that it can have specific meaning in some areas, but I was talking about the broader term. Though, I'm specifically interested about some finite field arithmetic.
Thanks for taking the time to respond. 
Holy fuck, that looks nice! 
Do you run into problems with batch windows? If not then a rewrite would probably not be recommendable. If you have problems start by finding the programs that perform worst and check for poorly written implicit pass-through. Ex converting a numerical value using a SAS function, forcing a comparison to be made on the SAS server rather than in the database engine. 
I can stick to editing the existing code for a lot of the scripts, but some of them look like they basically need to be completely redone anyways. So I'm wondering, if I have to redo some of the scripts anyways, would code written in Julia be faster?
You can experiment with both but don't forget that the code you write needs to be maintained. If Julia is not widely used in your organization then stick with SAS. 
Yeah, I know SAS is in decline in the era of Julia/R/Python but its all about making sure the business runs as smoothly as possible. You could do the Julia build as a proof of concept though. It could make you look like an innovator in your company.
If nothing else, it'd at least be good practice using Julia in a production environment. That'd be a good side project and, if it works well, maybe it'll attract some attention.
Can you give a rough idea of what sort of processing you are doing? I use Julia in a production environment, and for one fairly basic data ingesting task I parallelized ingesting 1k files of total size 1 GB and got it down to 80 sec. The previous implementation was not optimized (to put it mildly) and took multiple days.
It's primarily importing, combining, and exporting data sets. There's also some naming stuff, averaging, and conditionals mixed in there (waaaay too many if-else statements; considering using a dictionary instead). Really, nothing too complex. And the combined data don't seem to get any bigger than about 1.5gb. Fun fact, in one of the scripts, there were about 50 separate calls to PROC SQL to grab variables *from the same dataset*. 
My example was pretty similar. You can certainly do this much more efficiently in Julia. I don't know anything about SAS but that can presumably also be optimized a lot.
In the current code, there's plenty of moving data. It's actually pretty challenging to keep track of what's what at which point. There are also a lot of pretty pointless imports, exports, and formatting. I've never heard of DS2 before. I'll try looking into that, though; that could be very helpful.
Look at code moving first, along with code clarity. 
I suggested a desirable intrinsic value of sample code not optimised and not refined. Appropriate for educational use but not for say running some serious task for which artificial neural networks can be used (E.g. Pattern recognition or classification).
Did you come to a decision? I have a similar set of constraints at my work. The nice thing about SAS is that it is easy to optimize. There aren't a lot of tricks. Clean up the data steps, eliminated unnecessary procedures and your basically optimized. How fast does it run when you've done that level of optimization?
If you want to use both without downloading two programs: $ julia --lisp
TIL this exists.
I gonna learn lisp at some point. But julia is more lurking right now, in terms of practicality. Could you expand on what you said. What part of lisp other than metaprogramming is mind expanding? or more specificaly, what has lisp that Julia does not have is mind expanding? p.s. I did learned haskell - not that deep as it is a very long road. So functional programming is not new to me.
Same content. Just a different UI + [works offline](http://devdocs.io/offline).
I really liked this! Well done!
I asked around in gitter.im and irc and the general recommendation was either plotting, GLVisualize.jl or use the vtk output: https://github.com/jipolanco/WriteVTK.jl
I don't think so. You could use Gtk.jl to build your input forms but it would require a bit of work. You could try PyCall to use guidata.
You could maybe look at https://github.com/barche/QML.jl which is wrapped using CxxWrap.
Karpinski talked about Julia 1.0 at JuliaCon in June 2016, [video here](https://www.youtube.com/watch?v=5gXMpbY1kJY). If I remember correctly he basically said: - Julia 0.5 in August/Sept 2016 - Julia 0.6 will essentially be 1.0-alpha - Julia 1.0 target release date is before JuliaCon 2017, i.e. roughly mid June 2017
I agree with you that learning Julia first makes more sense. And while there is more to lisp than metaprogramming, I'd say about 50% of the mind-expansion comes from metaprogramming - once you truly understand and wrap your brain around it. When you do get to learning Lisp, the easiest starting point today is probably Racket, which is well-maintained and goes a long way towards making Lisp easy to install and use. 
It's nice to see people blogging about Julia but isn't all of this covered in the documentation?
Yup, but it takes a while to parse through it all. I just wanted to provide a tl;dr to make it easier to figure out.
This worked. Thank you.
I can second the question with Julia 0.4.6 - the latest stable as far as I know (or almost the latest by now). The same problem is when using Plots with PyPlots back-end: the time for the first plot to appear is almost a minute. After this it's fine, until you restart Julia...
Julia 0.4.6
I agree.
There's definitively something wrong with your setup then. Like you don't have precompilation enabled. When you install a package and use it for the first time (or do Pkg.update()) does it tells you "Precompiling..." ?
OP is speaking about *using Gadfly* not function calls. It shouldn't take more than 2 sec on 0.4.
Oh. I am really interested in Comp. Neuro. so Ill definately check all of that out! Thank you so much!
Thank you! Ill definately try to do those things!
In math, I just completed Calc III and im onto ODE. I have yet to do Statistics, and Im just beginning to learn about compilers, as the languages i previously learned didnt have the optimizations that Julia has ( i know Javascript, Python, Elixir, and Go ). I am interested in any science, especially Comp. Neuro. 
Can you pipe the file through sed before reading it? Something like sed -e 's/\t$//g' &lt; file &gt; file_notrailingtab (warning: untested! Do not trust this verbatim!)?
That's a fantastic idea!
should i set it up?
I think so. Though, we'd probably want to figure out how to keep it updated with which ones are being worked on and which are done so people aren't unknowingly reinventing the wheel.
Sounds good to me!
Rarely is the problem in open source that people are looking for ideas; rather, developers are in short supply and so is their time. So basically, you're going to get people who wish there was a specific package, request it, but not find anyone who is going to implement it (because if they were interested, had time and had the skill, they'd have likely done it already). I would think the better thing would be for a relative newcomer to pitch an idea and see if they get get a 'mentor' of sorts to help them through roadblocks. At least in that case, the requester adds to the developer supply of labor, instead of just hoping someone will implement their wishes.
I'd guess it's not the intended behaviour. You could try raising it as an issue on github. If it is the intended behaviour, the maintainers will likely be able give an explanation of how to get around this. If not, it should be a pretty easy fix.
Oh, I was totally unaware of this. Thank you! 
Thanks for the update!
Thanks. Great reply.
4 line packages lmao. 
That's unfortunate, because the grammar of graphics approach to plotting is amazing.
Is this not already possible with the Juno or web stack tools? With what people have already built it seems that if it hasn't already been done, it could at least be pieced together using existing tools.
I have no personal use for it but go for it, it will look good on your resume.
Started working on a Node-based "JSCall" library today: https://github.com/mindbound/JSCall.jl
I suspect it is because you are giving the compiler the problem of working out the return type. f(x::Int64, y=1) = x * y even though the y defaults to an Int it could by anything so Julia has to box the result http://docs.julialang.org/en/release-0.4/manual/performance-tips/#code-warntype because GScale is boxed then it percolates upwards Although it is 2h48m long, this video is worth watching https://www.youtube.com/watch?v=szE4txAD8mk (to save a few minutes, download it and watch it in VLC at 1.5x :)
Ok, I can work with that. This function is buried three levels deep in a monster that's numerically integrated over a volume, so I'll test the difference with keywords. I didn't expect the nested functions to give me trouble, but sometimes it's the weird things that kill you
In Julia 0.4x, the compiler can't infer the types of named parameters. They have a high overhead, so, use them in making an easy to use API, don't use them for performance-sensitive inner loops. Source - the Julia high Performance book. 
This is not true. julia&gt; f(x::Int64, y=1) = x * y f (generic function with 2 methods) julia&gt; @code_native f(1, 2.0) .text Filename: REPL[6] pushq %rbp movq %rsp, %rbp Source line: 1 vcvtsi2sdq %rdi, %xmm0, %xmm1 vmulsd %xmm0, %xmm1, %xmm0 popq %rbp retq nop Declaring types on function arguments has almost never any impact on performance.
Usually that kind of scoping is nice for convenience, but if you check the LLVM/native code it has a slight overhead. Instead, for your innermost computations, pull that function out of the other's scope and pass in all of its arguments. That will make it compile to something really clean, and in many cases like the ones you showed, will probably end up inlining.
Because Julia uses JIT. The first time you call a function, during the execution of your program, it will be compiled. &gt;In computing, just-in-time (JIT) compilation, also known as dynamic translation, is compilation done during execution of a program – at run time – rather than prior to execution.
Great answer. Just to reinforce the point, you can make globals "fast" by making the promise that their type won't change with `const x = 1`.
I tried that, but it keeps turning my int into floats
 Any[1,2,3,123.0]
you can convert convert(Array{Any}, [1,2,3.0]) but what is the benefit of this? why do you need an array of Any in the body?
See this discussion: http://stackoverflow.com/questions/39380285/how-to-declare-array-type-that-can-have-int-and-floats/39380464#39380464 You probably want to do Any[1,2,3,123.0], but this will make your digit function slow. If you're on v0.5 and you want to instead work element-wise on your array, you probably want to write the function for numbers (instead of arrays) and broadcast it, using the . syntax: digit.(Any[1,2,3,123.0]) will then apply digit separately to each number, keeping its appropriate type. 
Nice. I'm a recently new user. I'm a data analyst in Supply Chain Management / Logistics. I recently attended the 7th International Conference on Computational Logistics. Not much Julia talk in that tbh but my paper for ICCL 8 will be done in Julia. 
Any time comparisons to share?
Thank you for your response. This is the analysis I was considering julia&gt; @code_warntype(sqrt(1.0)) Variables: x::Float64 Body: begin # math.jl, line 144: return (Base.Math.box)(Base.Math.Float64,(Base.Math.sqrt_llvm)(x::Float64)::Any)::Float64 end::Float64 where the ::Any is highlighted in red 
I had the same issue (I think) with atan2. I have a piece of code that allocates a lot of memory for no reason I can see. Maybe it's a bug in 0.4, 0.5 doesn't have the Any. julia&gt; @code_warntype sqrt(1.0) Variables: #self#::Base.#sqrt x::Float64 Body: begin return (Base.Math.box)(Base.Math.Float64,(Base.Math.sqrt_llvm)(x::Float64))::Float64 end::Float64
Note that this is a part of a much larger issue right now about standard tooling: https://github.com/JuliaLang/julia/issues/18389
Looks like it's $49 USD
i thought it would be (finally) the 1.0 release. What about the CHANGELOG ?
Congratulations! I will upgrade immediately
thanks !!
PowerPC support? C++ FFI? No more having to deal with ASCII and UTF-8 separately? Looks like I'm about overdue to start moseying on back to Julia.
Ouch, my package must be getting properly broken by now.
What's the best way to update? Just installing again? 
I'm not sure but I'd like to know too. Now when I start atom to edit a text file it starts julia and other plugins...
NURBS
Yes, you can definitely change which version of Julia is being booted. You just need to open the package called "Julia Client" and change the path.
Yes, you can find them here https://github.com/JuliaCI/BaseBenchmarkReports/blob/6d82a5518a25740eef4abde8359ea3cdbc630375/0350e57_vs_2e358ce/report.md (The results are not very easy to read on Chrome unless you install the 'wide github' extension)
In Packages &gt; Julia &gt; Settings, set the path directly to Julia. v0.5 and v0.6 (and v0.4) are in different folders, so switching between versions is just switching the number in the path.
Hard to compare since it now uses -O3 optimization (higher optimization). There are some new type-inference issues being worked out, but if you avoid them then it should be faster.
It does work for me out of the box. The readme is out of date I think. Or maybe is necessary for some users. But you can use it with just Pkg.add("Cxx").
Almost... I mean, this will speed them up but there is some non-intuitive behavior that this can cause because it doesn't just promise that their type won't change: the compiler will also act as though the value won't change. I put an example of this problem in here: http://www.stochasticlifestyle.com/7-julia-gotchas-handle/ Thus I'd recommend putting them in a function instead of declaring variables as constant (unless the variables are actually constant).
Ah, ok. thanks. Always one more rabbit hole :)
Something that got me several times (even though I've been using Julia for some time now) is forgetting to pass a variable as argument after playing with some code in the global scope, e.g. x=2 N=5 a = 1 for i = 1:N x += a end turning into : function f(x,a) for i = 1:N x += a end end And then I spend an hour trying to figure why my code is slow. I wish there was a way to make using global variable without declaring them an error.
I just quit the REPL frequently. In Juno it's Ctrl+j Ctrl+k, and since it uses a process server there's no startup time. If you do that, you'll get an error since N is no longer defined.
I usually have N defined somewhere (alongside x and a). It just seems like something the compiler should be able to tell you about.
Interesting but I'm still waiting for something like scikit-learn for julia. This example even has to implement a simple kNN manually.
I agree that there should be a way to to on compile-time errors. Right now they are mostly disabled (except for parsing errors), and so most errors are run-time. This means good tests are required.
The authors seem to be omitting the square root for their Euclidean distance calculation, both in the formula in the text and in the provided code.
If you look at the source in Base they wrap the routine name with a macro: @blasfunc($gebal) Defined as: if vendor() == :openblas64 macro blasfunc(x) return Expr(:quote, Symbol(x, "64_")) end openblas_get_config() = strip(unsafe_string(ccall((:openblas_get_config64_, Base.libblas_name), Ptr{UInt8}, () ))) else macro blasfunc(x) return Expr(:quote, x) end openblas_get_config() = strip(unsafe_string(ccall((:openblas_get_config, Base.libblas_name), Ptr{UInt8}, () ))) end So it seems you need to call zpbtrf_64_ or something like that.
It doesn't matter because x^(2) is monotonic, so it doesn't change the sorting order.
This is true, but they should call it the quadrance or squared distance then. It is the right thing to use, but calling it the distance should be avoided. 
What system? Have you tried running it with admin privileges? You also might need to restart and run it directly upon startup.
I got the same error, x86_64, and I run with sudo privileges.
Code available at: + https://github.com/djsegal/julz
I love it!
Yes. JuliaBloggers uses the RSS feed and it will re-blog any post from PkgUpdate. I still need to workout the frequency of publication and whether to post each one to the Julia subreddit.
Good to know. Just curious -- what university is this at? I only know of a few that are using Julia at the moment. Edit: Nm, figured it out. 
I'd just remove any line that consists only of whitespace. You could do it with sed or awk or perl or whatever you wanted to.
[Screenshot of the setup and Pkg.status.](http://i.imgur.com/Va0PqsH.png) Then I restart the console, using JuMP and try a function I know will be different in the dev version. Then I print the source to show the file is the dev version but not the code being run. [New sourcefile not what is imported.](http://i.imgur.com/fmKdLOF.png)
`fname` in source code is not a keyword argument and this is consistent with the error message. If you try `writeLP(m, "output.txt", genericnames = false)`, will it work?
Did you consider using [sshfs](https://linux.die.net/man/1/sshfs)? It just mounts a remote filesystem via ssh on a mount point of your choice in your local filesystem, which should then allow you to &gt; do something like _readcsv("/path/to/the/file/on/the/machine/I'm/ssh-ing/into.csv")_ (except you'd have something like _"/sshfs/mount/point/path/to/the/file/on/remote/machine.csv"_)
Julia has scikit-learn https://github.com/cstjean/ScikitLearn.jl/blob/master/docs/examples.md 
EDIT: Disregard this ~~Alternatively, use the `@assert` macro.~~ ~~`@assert color in [...] "Not a valid color"`~~ ~~I personally think this is a bit more readable, since it's easy to spot and clear what you're doing.~~
This is the correct way, imo.
No. Pkg.* afaik works on the user's home. I dont know about IJulia though. 
Definitely not. `Pkg` and `IJulia` are likely only needing sudo because you previously launched Julia with sudo privileges — which means that `Pkg` created its repository folder with sudo privileges — which means that you can no longer do things as your normal user. Have you done any package development? If not, just trash `~/.julia` and start again — without sudo at all this time.
I'm going to try this first. Edit: so far so good.
maybe you should ask them? julia has some support for functional style. but same goes for many other modern languages.
You'll need more information about what caused the MethodError. Probably one of the input types is wrong?
It's not a high bar nowadays, but usually you'd call any language that has functions as values that you can pass to higher-order constructs like map, filter, etc a functional language, at least if using those constructs is common and idiomatic. Not changing state or modifying parameters is known as purity, Haskell is a *pure* functional language. But since this terminology is a bit contentious, it's best to just not argue about it and move on.
The word "functional" as applied to languages is pretty overloaded. What you're talking about – programming without explicit mutation – is often called "pure functional" programming. But on its own [functional](https://en.wikipedia.org/wiki/Functional_(mathematics\)) originally means a higher-order function, i.e. one that take functions as arguments. As well as those things, it's also about programming style and design. Do you think of your program as mutable objects interacting (like Python, Java) or as functions operating on data (like Haskell or Clojure). On a technical level you really couldn't do a lot of functional programming in C in a reasonable way; `malloc` and `free` are evidently side-effectful and it's pretty infeasible to build things like persistent data structure without memory management. But conventions and culture matter too. You could program functionally in Python if you wanted but you'd be going against the grain (both in terms of language support and community style), whereas in Julia these patterns are very much encouraged. Again, it's a fuzzy thing, but on many of these axes Julia is closer to Haskell than to Python; it's functional because we use it that way.
Thanks for the reply! What I meant was that it doesnt work when you use it in a .jl file and run it on cmd.
Because you can pass functions as variables. ys = [1:5] double = x -&gt; 2x zs = map(double,ys) # [2,4,6,8,10] Julia has all of the parts needed to do functional programming Where you are getting confused is that you are not restricted to ONLY functional programming.
i'm not an expert, but i would posit that true functional programming requires two things: automatic currying and immutable values. in julia, you can do currying, but only explicitly. and without mutating values, the performance will be extremely poor.
&gt; maybe there is a better way? I just realized the correct way is to do: `C:\&gt; julia --color yes foo.jl` :P
I think that is more complex. Why can't you install julia on the other machine? IMO you should focus on this instead of moving big files around, sshfs is very useful too and it seems it worked for you as a temporary solution. But all this started because you couldnt install julia, but I assure you, you can. Just download the generic linux binary if you are on linux of course, untar and run. You dont need to bee root or have sudo to do this. 
Thank you for the reply. I got this advice from some coworkers as well and looked into it. It turned out the hard drive of the machine in question was dying and a permanent solution was reached that is similar to your suggestion. 
 function tokenize(text) split(text, [' ', ',', '.'], keep=false) end function countWords(text) results = Dict() for word in tokenize(text) results[word] = get!(results, word, 0) + 1 end results end countWords("I have a cat, I have a dog. my dog is called fred, my cat is called cfer") 
so... lets use that on a dataframe Pkg.add("RDatasets") using RDatasets: dataset, datasets wordLists = map (x -&gt; countWords(x), convert(Array, datasets()[:Title])) then some kind of foldl on the word lists to aggregate them. no doubt there are better ways of doing all this, but this is the first off the top of my head.
or... maybe better.... flatten{T}(a::Array{T,1}) = any(map(x-&gt;isa(x,Array),a))?flatten(vcat(map(flatten,a)...)): a function countTokens(tokens) results = Dict() for token in tokens results[token] = get!(results, token, 0) + 1 end results end countTokens(flatten(map(tokenize, convert(Array, datasets()[:Title])))) which gives the right answer.
Since Julia is built to interoperate seamlessly between languages, one may argue that even though Julia is not yet fully matured yet, especially regarding the library ecosystem, that it's a good move to switch to Julia now as a main language, not forgetting and practicing calling external libraries when needed.
Julia is better as a language for developing. So if you goal is to implement your own algorithms and pipelines, Julia is the way to go. The language itself is better and also faster, so you won't have to go back to C or fit your problem into crazy vectorization scheme. However if you just want to use packages and call different tools, and get things done without having to look under the hood too much, MATLAB or R are probably better at this moment. In theory you can do everything in Julia, but in practice you can encounter issues that might require you to get your hands dirty (contributing to packages, developing your owns, fix bugs, read the source code, etc.) if you want to make things work. Julia also is also a bit more complicated imo, you'll learn a lot of computer science terms if you read the docs. Which can be a good thing.
One may also argue that since Julia hasn't been released ([it's still in beta](https://www.moore.org/article-detail?newsUrlName=bringing-julia-from-beta-to-1.0-to-support-data-intensive-scientific-computing)) that Julia is not yet fully matured :)
To add to that, the IDE for Matlab is pretty valuable and allows for intuitive data import, especially from excel. For example, notice here that [one drags the file into the workspace](http://i.imgur.com/jprvkNg.png), and this action instances the [import data tool](http://i.imgur.com/zr2HdKQ.png) which can then auto-generate an import function or script. If OP needs to "get stuff done" I think I'd go the Matlab (or R) route for now, especially since if they have issues their university clearly has support. That being said, Julia's great, if OP can learn it in parallel then do it.
Matlab's editor is really nice yes (pre 201x anyway, the new interface is awful imo). The ability to easily make GUIs is also nice, for some projects it can be a huge time saver.
Check settings -&gt; packages -&gt; julia client, what does it say for the julia path?
That is strange, as far as I know Julia evaluation is handled by the `julia-client` package. Double check the Julia Path by going to Preferences &gt; Packages &gt; julia-client &gt; Settings. On iOS it should be */Applications/Julia-0.5.app/Contents/Resources/julia/bin/julia*. If that looks alright maybe try reinstalling this particular package.
the path is `/home/nils/julia-3c9d75391c/bin/julia` should i redirect this to the path shown in the terminal and delete the other folder? Also: How can I update the julia release from 0.4.5 to 0.5 in terminal? 
TL;DR Julia is changing fast, but if you're careful it can be a good tool for data analysis. In my experience (I've done chemistry data analysis, plus some programming centric projects) Julia is acceptable for day to day use. Plots.jl and PyPlot.jl are fantastic for creating figures and DataFrames.jl is amazing. Also, LsqFit.jl has regularly come in handy. If you learn Julia in tandem with Python, you can usually use PyCall.jl to supplement Julia's nascent package ecosystem. I've found Julia easy to learn, but I've also found myself having to devote a bit of time every week to keep up with frequent changes in packages and syntax. That being said, things become unstable around new releases (like all software). For example when Julia went from 0.2 -&gt; 0.3, many packages wouldn't run properly on either version for a few weeks. Things are getting better, going from 0.4 -&gt; 0.5 was only slightly uncomfortable. I would suggest updating packages and Julia versions slowly, perhaps only during semester breaks or when something is intractably broken. On the same note, any code you write now may be unusable by the time 1.0 comes out due to syntax changes. Another difficulty is being unable to collaborate; I doubt your colleagues will know Julia, so you may have more support when using MATLAB. You will also find more sample code and mature libraries for MATLAB, Python, and R. 
yes, i guess i installed both (oops). If I symlink the folder, it is still required and i cant delete it right? I symlinked `julia-3c9d75391c` to my `/usr/share/bin/julia` and changed the path in atom to that.
You can't delete the original file or the link will die, but it should be trivial to recreate the link if you'd rather not have `julia-3c9...` in your home directory. I would actually suggest instilling it through the PPA (see the Ubuntu section on http://julialang.org/downloads/platform.html) that way apt-get will handle Julia updates as well.
Was thinking this too, but they warn on their website that it's a community version so I got confused
The PPA is not recommended any more. If it updates on you, it's not possible to downgrade it back.
In a technical sense, yes, because when things like scikit-learn are missing in Julia you can get them via PyCall. But at this point in time, if the project is just pulling a bunch of libraries together, Julia just gives you different syntax and extra friction. Depending on what your goals are, the benefits may not outweigh the costs. For example: If you want to try implementing basic models like KNN or a neural network from scratch and get it working "for real" on a dataset, Julia's a really great choice for that. If you want to contribute and improve Julia's libraries so that it's the best choice for data munging in future, go for it! If you just want to learn how to productively get data out of a database, do some cleanup and visualisation and throw some pre-cooked models at it, Python or R are still your best bet.
thanks for the reply! It works, but I would rather I avoid using for loops as this part of the code will be already in a for loop and called often. I would really like to use the DataFrame indexing to filter the results. 
With the new broadcasting syntax in 0.5 (I expect that) you'll be able to do df[ismatch.(r"aaa",df[:A])] (note the dot after ismatch). This is going to be morally the same as gotfork's answer, though, and it's very far from clear to me that you'll be able to *avoid* a for loop, as opposed to *hiding* it like I do here.
I tried that already, I get the following error MethodError: no method matching size(::Regex) It seems like its expecting two arrays to index over instead of one array to apply the same regexp over its elements EDIT: I found the problem in implementing that one...for some reason the regexp needs to be placed within square brackets to work, i.e. df[ismatch.([r"aaa"],df[:A])]
I also thought that would be a good approach, but apparently broadcast can't tell that a Regex is a scalar -- not sure why. 
I'm not sure that you can do this without allocating an array of bools at some point (or without allocations that take up the same space). Even if using broadcast worked here, it's basically doing the same thing. You could try asking at the general users list, perhaps a wizard there knows a trick with generators or something. Edit: O(N) ain't that bad: using DataFrames df = DataFrame(A = String[randstring(6) for i in 1:100000], B = [i for i = 1:100000]); @time df[[ismatch(r"aaa", s) for s in df[:A]], :] # 0.039369 seconds (127.34 k allocations: 2.811 MB)
Why replace Google groups?
See [this](https://groups.google.com/forum/#!topic/julia-users/Ov1J6MOVly0) google groups thread
It works, but you can avoid unnecessary allocations. function replace_nan{T}(x::Array{T}) for i = eachindex(x) if isnan(x[i]) x[i] = zero(T) end end end function replace_nan2{T}(x::Array{T}) x[isnan(x)] = zero(T) end function time_eachindex(x) @time replace_nan(x) end function time_logical(x) @time replace_nan2(x) end x = rand(50,50,50) [x[i] = NaN for i = (1,5,7,15,25,70)] x1 = copy(x) x2 = copy(x) # compile... time_logical(x) time_eachindex(x) # time time_logical(x1) time_eachindex(x2) edit: fixed a mistake (see below)
It seems you can make use of the [WorldBankData](https://github.com/4gh/WorldBankData.jl/#examples-of-country-searches) package for this. For example, to get the ISO3C of any country with "denmark" in the name: WorldBankData.search_countries("name",r"denmark"i)[:iso3c] 
Thank you, that looks just right :)
Hm, replace_nan doesn't actually replace the NaN values (tested on julia 0.5.0). After running the script above x1 == x2 False x1 == x True 
Ah, sorry. Fixed a mistake, but the point is still the same. If you're curious, the mistake was comparing to NaN. This is not viable, as NaN == NaN evaluates to false (yay floating point numbers...). You need to use isnan(x). Sorry for the confusion. 
Plz, add this link to http://julialang.org/learning/ 
who cares, right ?
http://docs.julialang.org/en/release-0.5/manual/control-flow/#man-short-circuit-evaluation The *and* and *or* operators in Julia are `&amp;&amp;` and `||`.
A quick google search for "julia syntax logical operators" suggests using '&amp;&amp;' instead of 'and' [source](http://docs.julialang.org/en/release-0.5/manual/control-flow/#man-short-circuit-evaluation)
Those are the standard 'and' and 'or' symbols in most languages. Python is the only really popular language I know of that writes logical operators out in words. Also, it's probably useful to know that the reason "and" is &amp;&amp; and "or" is || instead of &amp; and | is because those are reserved for bitwise "and" and bitwise "or." So something useful to know whatever your programming language.
huh. I have only written in Python and Lua and they both use 'and' and 'or', so it never really crossed my mind to not just use the words. The more i know.
Expressions, not statements! &amp;&amp;, ||
what do you see there? it gives Float64 to me, although through a typeassert, as the getfield returns a union (which seems OK in this case)
It is probably quite important to really understand why you are getting red text. Type instability means that for a given set of input *types*, Julia is unable to infer the return type of a function. You are hoping that Julia will be able to infer that you will only allow `i` to be 1 or 2, and as a result, the return type should be Float64. I'm guessing here, but you probably first tried to write function getindex(X::Foo, i::Integer) 1 &lt;= i &lt;= 2 || throw(BoundsError(X, i)) return getfield(X, i) end And got the inferred return type `::Union{Float64,Val{true}}`. This is because for a given type of `i` (Int64), Julia cannot know what type getfield will return. Julia does not realize that you already checked if `i` was 1 or 2. As a result, you added the type assertion. What does that give us? julia&gt; @code_warntype(X[1]) Variables: #self#::#getindex X::Foo{true} i::Int64 #temp#::Bool Body: begin unless (Base.sle_int)(1,i::Int64)::Bool goto 4 #temp#::Bool = (Base.sle_int)(i::Int64,2)::Bool goto 6 4: #temp#::Bool = false 6: SSAValue(0) = #temp#::Bool unless SSAValue(0) goto 10 goto 12 10: (Main.throw)($(Expr(:new, :(Core.BoundsError), :(X), :(i))))::Union{} 12: # line 3: return (Core.typeassert)((Main.getfield)(X::Foo{true},i::Int64)::Union{Float64,Val{true}},Main.Float64)::Float64 end::Float64 This is a type stable function, as the return type is tightly inferred. Your worry is then probably the red text after the getfield-call. My guess is that it probably won't matter. But it is important to understand that nothing you have done so far changes the fact that Julia simply cannot infer that getfield will return Float64 based on the types of the inputs. With the above in mind, try to run the following immutable Foo{T} x::Float64 y::Float64 misc::Val{T} end function getindex(X::Foo, i::Integer) if i == 1 X.x elseif i == 2 X.y else throw(BoundsError(X, i)) end end X = Foo(1., 2., Val{true}()) @code_warntype(X[1]) What does the output of the last macro call tell you?
is this what you mean ? julia&gt; fieldnames(OLS) 3-element Array{Symbol,1}: :model :mf :mm julia&gt; fieldnames(OLS.model) 3-element Array{Symbol,1}: :rr :pp :fit julia&gt; OLS.model.rr GLM.GlmResp{Array{Float64,1},Distributions.Normal{Float64},GLM.IdentityLink}([2.0,3.0,7.0,6.0,9.0],Distributions.Normal{Float64}(μ=0.0, σ=1.0),GLM.IdentityLink(),[1.9721522630525295e-31,0.4899999999999996,2.5599999999999987,1.210000000000001,0.040000000000000424],[1.9999999999999996,3.6999999999999997,5.4,7.1000000000000005,8.799999999999999],[1.9999999999999996,3.6999999999999997,5.4,7.1000000000000005,8.799999999999999],[1.0,1.0,1.0,1.0,1.0],Float64[],[1.0,1.0,1.0,1.0,1.0],[1.0,1.0,1.0,1.0,1.0],[1.0,1.0,1.0,1.0,1.0],[4.440892098500626e-16,-0.6999999999999997,1.5999999999999996,-1.1000000000000005,0.20000000000000107]) 
Yes, that's exactly what I was looking for. Thanks!
The efficiency cost of boxing the return from getfield seems bad for just retrieving a value. So either forcing type inference to happen, or replacing getfield with the Julian equivalent of`(Float64*)(&amp;X)[i]` EDIT: I replaced it with: getindex(X::Foo, i::Integer) = (X.x, X.y)[i] and its the same speed as accessing a tuple directly for me 
I guess you don't mean julia&gt; 'Z' &lt; 'a' true Staross in another post suggests \`, you can't type ` as a symbol because it is interpreted but you can make it into one s = Symbol("`") same with the other [ASCII](http://www.asciitable.com/) below 32 So you could even get clever and use the Substitute character sub = Symbol(Char(26))
&gt; (Int('a')-1) Clever, I wish I had your mind. So backtick is less than a. I was able to run a sort with the following. julia &gt; string = "abc`" julia &gt; join(sort(split(string,""))) "`abc" On a side note: I wish julia had one function to sort a string. The `join`,`sort`,`split` is tom-foolery.
'\0' less than any other char. you can use it in strings like "hello\0"
` is a backtick. ~ is a tilde.
Thanks, I edited my above post.
apparently it is a sorta bugish https://github.com/JuliaLang/julia/issues/18852
Jeff Bezanson says there that behavior is intentional, I wonder what the reasoning is. Maybe its just a difficulty in guessing what type the resulting empty array should have?
Seems like a type stability issue yes, this line says: &gt; this is a simple way to make this function type stable https://github.com/JuliaLang/julia/blob/e94007ea13f963e545534028dc967697f7267649/base/iterator.jl#L659 I guess the start function would suddenly returns something else and fuck with all the iteration code, so they need to refactor some of it to avoid the issue. Otherwise can't you just do something like : [f(a,b,c) for a=1:N for b=min(a+1,N):N for c=min(b+1,N):N] 
empty arrays have types too, but you use empty ranges, which are also typed. there should be no problem calculating the correct result (a nicely typed and shaped empty array). probably it will work eventually, you just caught a snapshot when it isn't. (that annoying 0 at the beginning of the version number.)
Oh, now that I'm not on my phone I could comfortably follow that link. It explains everything, thanks.
`reshape` doesn't create copies in 0.5, but indexing does. There are actually two ways of doing this: Wlayer1 = reshape(view(theta, start:finish), (28, 28)) Or you can equivalently reshape the *indices*. I don't think we have quite as many optimizations for this, but it'll behave identically: Wlayer1 = view(theta, reshape(start:finish, (28, 28)))
You can do @inbounds begin foo = 3 bar = 5 ... end or @inbounds function f(x) bla end From http://docs.julialang.org/en/release-0.5/manual/performance-tips/#performance-annotations, note after summation. 
If you want to use @inbounds to index into the 'inputs' vector, you need to be absolutely sure that it's impossible for inputs to have zero length. Out of bounds subscripts can lead to crashes or silent corruption. With your current implementation, it's a little too easy for this situation to occur, for my comfort level at least. So I would just get rid of that @inbounds. Note that the bounds check in the indexing call will be elided anyway if you run julia with --check-bounds=no, which you use if you want to squeeze out more performance and you're sure that everything is working properly. To see this in action, look at the difference between the native code for f(x) = x[1] when running julia with --check-bounds=no and without (using e.g. @code_native f([1])). 
Never mind, just realized that split will always return a vector of nonzero length.
Julia is a dynamically typed language. You can always stick whatever you want in a variable. 
Function arguments only determines what function will be called. It doesn't put restrictions on those arguments inside the function. 
Hmm, I didn't remember this being the case actually. TIL.
Does this give the maximum performance since the compiler knows at compile-time the input type? If so, how does it decide what types "red, blue, green" are? Does it assign integers to them?
Thanks!
Like /u/pint, I can't comment on CFD specifically, but I've been using Julia in my research for about a year now, and I've found it to be an excellent tool. As for a few of your specific concerns: * interfaces: It's true that Julia does not let you *enforce* that a particular input satisfied an interface, but the language is full of interfaces like start(), done() and next() for iterators, getindex(), setindex!() for array-like objects, and so on. Once you learn to use these interfaces, it becomes amazingly easy to add new behaviors to existing types or create new types with the behaviors you want * managing large codebases: Julia allows nested modules, so you can organize a large package with discrete sub-components without their namespaces colliding. It also makes it very easy to split functionality into Packages, which makes code re-use and sharing very easy. In essence, I think a more Julian approach is, rather than having one huge package, to have several well-defined packages which work together to accomplish your larger goal. * object-oriented programming: yup, Julia will require you to change the way you think about OOP. It's just different, and Julia code is more likely to use encapsulation rather than inheritance. But there are a lot of arguments in favor of that behavior anyway. I found that I *really* missed Python's OOP for about two months when I started working in Julia, and then never really missed it afterwards. But enough about potential downsides. I think the huge potential upside of Julia is the ability to quickly prototype ideas (since it's a nice high-level language) and then iteratively refine your ideas and implementation into something that's as fast as C *without ever changing languages*. Not all Julia code is magically fast, but it really is possible to get C-like speed while also having incredible flexibility and expressiveness that would be very difficult to achieve in C. 
I've run into a few hangups with informal interfaces in Julia, though. First, they aren't always well-documented. This has gotten better over time, obviously, but it still isn't perfect. Second, if an interface changes, you find out at run time. That's unacceptable to some people, but obviously not everyone (Python is the same way, and Python has obviously had a lot of success in the technical computing space). Third, it's tricky to export an interface because of the first two hangups. So, for example, creating a single interface for a particular kind of computation means committing to solid documentation and clear releases and release notes. This isn't necessarily the norm in all areas of technical computing. Overall I think the OP's impression is pretty accurate. In my experience, Julia has better ergonomics than C/C++, but not as good (yet) as Python and some others. In particular, I find the standard way of developing packages (where you work directly on a Git checkout controlled by the package manager) to be pretty terrible. However, this is probably not a concern for many "application" style projects where the resultant code isn't meant to be distributed as a library.
i don't see how a dedicated interface construct is any better than individual functions. if you change the interface definition, you will only get errors if you use the members that has been changed. exactly the same as with individual functions. the only difference comes from the JIT compilation, causing errors only showing up at runtime. but then you praise python, which is also not a compiled language.
julia does not enforce, but you can, if you are adamant on it. you can check if a method is there for a given type-tuple. with generated functions, you can even pull this test to compile time. why would you do that? well, dunno, maybe a nicer error message. not much else can be done in the JIT/interpreted world.
Need a high res PDF version I can post up in front of me.
Have you read the whole manual? I found it good at providing a solid base level of knowledge so you know what questions to ask docs/people when you're trying to do something. From there I would look at good code examples from well-documented packages (Tim Holy writes really good Julia code, for example). Most of all I would try your own projects; the advanced features are just tools for problem solving, so it's good to learn to use them when you have an actual problem to solve.
I'll follow those suggestions. Problem solving is a good way to learn, of course. I find that there is often a paradigm behind the design of a good piece of software that you don't see just reading about the pieces. Even a brief explanation from the designer can make an aha moment
I praised the general software development ergonomics around Python. I meant for the last paragraph to be separate from the discussion of interfaces. Having interface definitions of some kind makes static analysis possible, for one thing. This is why Python, for example, has added optional type annotations. The Python interpreter more or less ignores the type annotations, but they are available for tooling and static analysis. For another, even if the error shows up at run time, the error message is going to be more useful if the interface is defined in code rather than in documentation, and interface-implementation mismatch errors will be easier to resolve since you can just go and look at the actual, defined interface. Keep in mind that I'm a software engineer by trade (I've done some academic work as well, including a small amount of CFD). Partly as a result, my bias runs heavily toward maintainability and tooling. In my experience, many people doing "science" value flexibility and rapid development over long-term maintenance considerations and code re-use. This is a perfectly valid point of view and probably stems from the fact that the goal of custom scientific software is usually to get some result, not to distribute the software to end-users (and even in the latter case, the end-users are usually other scientists). This probably goes a long way toward explaining why Python has been so successful in this area. Julia is great because it attempts to split the difference, providing significant benefits (especially in terms of speed) to everyone in exchange for some mild (and diminishing over time) discomfort for everyone.
Start trying to do something and whenever you get stuck ask for help in the Gitter channel. You'll learn fast. https://gitter.im/JuliaLang/julia
The Plots.jl package with the PlotlyJS backend will do this. using Plots; plotlyjs() plot(x -&gt; x^2 - 10x + 1,[-10:10]) x -&gt; x^2 - 10x + 1 is an anonymous function. [-10:10] sets the range of x values to plot for (and the starting window size for the plot, although you can zoom in and out once it appears).
To do this, you might also want to use IJulia using IJulia; notebook() (Or run it in [JuliaBox](http://JuliaBox.org))
You can use UnicodePlots as a backend to Plots.jl too. using Plots; unicodeplots() plot(x -&gt; x^2 - 10x + 1,[-10:10]) That's the same code @Cortwatsjer posted for plotlyjs(), but with the backend changed. That's the magic of Plots.jl which makes it so easy to use different libraries. You should give it a try. Also FYI, there is GGPlots.jl for a grammar of graphics ggplot2 style syntax for Plots.jl. I don't use it, but those who like GoG might want to give it a try.
So why is changing the value allowed? Shouldn't it give an error? It would be nice anyway, I think, to have a way of specifying that the type of a variable will never change (and throwing an error if it does). Because many of the issues of speed in Julia come from type instability, this would provide a far easier mechanism for beginners like me to write performant code than @code_warntype.
Awesome, thank you!
thanks for your examples, and the nice links. I might be missing something, but I have always found various functions in the scipy stack to be pretty composable. I have also used the scipy.optim and scipy.integrate together in a similar fashion to what you point out. I do agree that the zero-cost nature of these high level features is a great advantage of julia for fast computation and prototyping of these sort of "laptop" problems. As far as parallelization is concerned, it does not seem to me that MPI style SPMD parallelism is a first-class citizen in the julia world, even though it is supported to some extent in 3rd party packages. It is clear to me how to launch of bunch of "embarrassingly" parallel jobs using @spawn, but it is not clear how to do the kinds of interprocess communications which are ubiquitous in HPC applications using native julia constructs. Nor do I trust the julia parallel constructs to be as fast as tuned MPI libraries. This raises the issue the packages in the "JuliaParallel" stack are just not as well maintained or feature complete as their python/c/fortran counterparts. Right now, julia seems to be targeted at speeding up the sort of problems which used to be done in MATLAB. For sure, this covers a great portion of scientific computing, but it does not seem to be intended for HPC applications. My concerns would be allayed if any of the HPC heavy-weights at places like LLNL used julia, or if there was series of well documented examples using julia to solve basic PDEs like the poisson equation at large scales, but as far as I am aware there is not even a proof of concept using julia for such things. 
Yeah, I've enjoyed using Julia and the tooling ecosystem is coming along pretty nicely. I just think it's important for people to understand what they're getting today, not just what they might get tomorrow. I think this is important for any young language.
Same here. I love the idea of Plots.jl, but I didn't manage to do more advanced stuff (using quiver for instance).
Thanks
&gt; It depends on how you are using Julia. In an interactive setting, it can be &gt; really annoying to have to restart Julia when you load a code file with const declarations a seccond time. I agree completely. But that strikes me as a quality of implementation(s) issue. There are a number of ways to address it. A compile time flag enabling improper but convenient behavior is one easy way. From the POV of someone reading a language spec, a const declaration should have a reasonable meaning.
Related is this post from JuliaComputing's blog: http://juliacomputing.com/press/2016/12/14/2016-11-28-celeste.html I don't know if this stuff has an open source repository though.
Can you also use Gadfly with Plots? And what exactly is the difference between Plotly and Gadfly? As I understand it now, Gadfly is native Julia plotting library while Plotly is just interface to python code. Does that mean that Plotly is just temporary solution ? Would you pick one over the other in certain situation?
Oh this is great. I have heard about it before but didn't realize how great it is up until now. Even though it is not ideal for some fast checking on function graphs I might use it to write some Latex documents with runnable code... Thanks!
Yah. I totally agree that the main issue is library support and the fact that julia is a young language. I also agree that there are costs associated with using FFI to libraries like sundials or PETSc. I have never heard of sparso, and will be sure to check it out. I guess I don't know if these iterative linear algebra packages use as much black magic for efficiency as something like BLAS or LAPACK does. Hopefully 5 years from now a suitable HPC stack will be in place for julia, but I am concerned that the split personality between the embarrassingly parallel "big data" parallelization and SPMD will be reflected in the libraries that people build.
Have you checked `@code_warntype` to know that it's not due to a type inference bug?
Could you use BenchmarkTools on the concatenation part of your code to confirm that this is the issue ?
i do eploratory data analysis and often write new individual scripts. Everytime i import modules like using Plots using DataFrames df = readcsv("foo.csv") plot(df["A"], df["B"]) it needs ages to import the modules at the start of the script. 
precompilation is a one time thing. yes, it takes a while, but one can treat it as part of the installation process. like, what, 20s to install a module? that's fantastic. i spent hours trying to install c++ libraries, often ending up failing. in comparison, julia install times are luxury. however, it seems that you have problems with loading times or compile times, not precompiling. "using" the modules you want, and then calling some functions can take a second or two every time. i guess this is what bothers you. however, i like to think what i gain. no other environment gives you what julia does. you pay a second every morning for it. i'm in.
Are you remembering to start julia with julia --precompiled=yes file.jl that makes a big difference for me on Windows 
I'm on ubuntu, but I usually use the juno ide which doesn't take any additional agruments when starting the Julia kernel or executing code
`@time using Plots` takes 11 seconds for me and `@time using Gadfly` even takes 100 seconds. I don't have a julia session running all day so I need to sit through this very often....
find returns the list of index of nonzero elements of an AbstractArray. Here BitArray is a subtype of AbstractVector{Bool} and the zero of Bool is false so find on BitArray returns the list of index of true elements, that is, the list of prime numbers.
Thanks for the tips--they've been helpful. I forgot about @code_warntype and BenchmarkTools. After redownloading an earlier version and comparing, it seems like there's a bottleneck here at least: function CM1vn(n,J) [CM2n(i)^J for i=1:n] end CM2n is a function that returns a BigFloat, but it actually runs slightly faster in 0.5.0 than 0.4.7. The code above, in contrast, runs 7 times slower in 0.5.0 than 0.4.7 for some reason. I can't believe that's such a short function, but I'm trying to map the code as closely as possible to a published algorithm. I'm going to look at it in more detail but thought I'd post here just with an update. Just for completeness, this is CM2n: function CM2n(n) r1 = collect(BigInt(0):BigInt(n)) r2 = n-r1 sum( [binomial(x,y) for x=BigInt(n), y=r2]' .* ((r1/n).^r1) .* ((r2/n).^r2) ) end Also, I've tried versions with explicit type declarations in the function arguments and it doesn't seem to affect the slowdown. I didn't benchmark that explicitly though, so maybe I should try that again. EDIT: I figured it out. The comprehensions in 0.5.0 were of different dimension than in 0.4.7, which then changed the dimension of what was being summed over, which was then exacerbated in a different function. 
It's easy to forget to wrap your benchmarking code in a function, and to make sure every function gets run once (to JIT compile it) before benchmarking. I'd suggest using [BenchmarkTools.jl](https://github.com/JuliaCI/BenchmarkTools.jl) to avoid all of those problems and get nice reliable timing results. 
a-ha, ok i get it now, thanks. i really do appreciate the explanation.
It looks like you're running into that problem even when your modules are precompiled (as in your example). 6 seconds to do `using Plots` does seem excessive (on my macbook air this takes about 0.9 seconds). If you keep running into this, I might suggest posting about it on https://discourse.julialang.org/.
Yes, it also needs about 6 seconds even if I start with `Julia --precompiled=yes`. It's a shame because I work on my scripts on and off and start new workspaces all the time which leads to Julia being kinda slow. This is holding me back from experimenting more with Julia 
The picture is of Julia Martin, a DC lawyer.
In general, if you are worried about how the compiler works and parse code versus run-time code, then you are dropping half-way out of the language towards x86 assembler. Macros can be used to help performance but it is not their primary purpose. To address your issues directly see, for example, http://docs.julialang.org/en/release-0.5/manual/performance-tips/ hth
primary or not, they can increase performance, in particular they can bring computation to compile time, which results in faster run time code. the prime example would be @evalpoly or @nloops and co. you can also do loop unrolling with macros.
Kind of an aside from the question, but what is your opinion on the @view macro? I don't really love the idea of changing a bunch of lines of code to include that if it isn't pretty likely that slices will become views by default. I currently like view() and setindex!() and other tools like that because I am confident that they'll do the same thing in 1.0. But if you think that it is pretty likely that slices will become views then maybe it makes sense to make the syntax change. 
Interesting! I'll look into those. Thank you very much. 
Julie.israelian@hotmail.com
I don't know what your problem is but you can inspect python objects from Julia instead of dot notation one uses symbols into an array so look at the req object println(req[:markup]) I can guess that either req is wrapped and you need to send in a child object or the call to get is failing altogether so try also println(fieldnames(req)) and then look at the fields, see if an HTTP status can be found or you can edit the python /usr/lib/python2.7/site-packages/bs4/__init__.py and add prints() in there 
hehe well I'm happy you got it solved. I've done a bit of work with PyCall, so I've been there
For the sake of this exercise I didn't even try any numeric code. The program is just: function main() for x in ARGS println("Hi: $x") end end main() I compile it with build_executable("app", "app.jl", "target", "native") This results in these files: -rwxr-xr-x 10416 app lrwxr-xr-x 17 libLLVM-3.7.1.dylib -&gt; libLLVM-3.7.dylib -rw-r--r-- 20912656 libLLVM-3.7.dylib -rw-r--r-- 38708 libamd.dylib -rwxr-xr-x 35326644 libapp.dylib -rw-r--r-- 25588419 libapp.ji -rw-r--r-- 350120 libarpack.2.dylib lrwxr-xr-x 17 libarpack.dylib -&gt; libarpack.2.dylib -rw-r--r-- 38752 libcamd.dylib -rw-r--r-- 22544 libccalltest.dylib drwxr-xr-x 102 libccalltest.dylib.tmp.dSYM -rw-r--r-- 47000 libccolamd.dylib -rw-r--r-- 956684 libcholmod.dylib -rw-r--r-- 34104 libcolamd.dylib -rw-r--r-- 377616 libcurl.4.dylib lrwxr-xr-x 15 libcurl.dylib -&gt; libcurl.4.dylib -rw-r--r-- 18508 libdSFMT.dylib -rw-r--r-- 2131172 libfftw3.3.dylib lrwxr-xr-x 16 libfftw3.dylib -&gt; libfftw3.3.dylib -rw-r--r-- 26700 libfftw3_threads.3.dylib lrwxr-xr-x 24 libfftw3_threads.dylib -&gt; libfftw3_threads.3.dylib -rw-r--r-- 2210020 libfftw3f.3.dylib lrwxr-xr-x 17 libfftw3f.dylib -&gt; libfftw3f.3.dylib -rw-r--r-- 26788 libfftw3f_threads.3.dylib lrwxr-xr-x 25 libfftw3f_threads.dylib -&gt; libfftw3f_threads.3.dylib -rw-r--r-- 278092 libgcc_s.1.dylib -rw-r--r-- 1623064 libgfortran.3.dylib -rw-r--r-- 820004 libgit2.0.24.0.dylib lrwxr-xr-x 20 libgit2.24.dylib -&gt; libgit2.0.24.0.dylib lrwxr-xr-x 16 libgit2.dylib -&gt; libgit2.24.dylib -rw-r--r-- 491740 libgmp.10.dylib lrwxr-xr-x 15 libgmp.dylib -&gt; libgmp.10.dylib -rw-r--r-- 4598680 libjulia-debug.0.5.0.dylib drwxr-xr-x 102 libjulia-debug.0.5.0.dylib.dSYM lrwxr-xr-x 26 libjulia-debug.0.5.dylib -&gt; libjulia-debug.0.5.0.dylib lrwxr-xr-x 26 libjulia-debug.dylib -&gt; libjulia-debug.0.5.0.dylib -rw-r--r-- 2468776 libjulia.0.5.0.dylib drwxr-xr-x 102 libjulia.0.5.0.dylib.dSYM lrwxr-xr-x 20 libjulia.0.5.dylib -&gt; libjulia.0.5.0.dylib lrwxr-xr-x 20 libjulia.dylib -&gt; libjulia.0.5.0.dylib lrwxr-xr-x 25 libmbedcrypto.0.dylib -&gt; libmbedcrypto.2.3.0.dylib -rw-r--r-- 314096 libmbedcrypto.2.3.0.dylib lrwxr-xr-x 21 libmbedcrypto.dylib -&gt; libmbedcrypto.0.dylib lrwxr-xr-x 22 libmbedtls.10.dylib -&gt; libmbedtls.2.3.0.dylib -rw-r--r-- 151476 libmbedtls.2.3.0.dylib lrwxr-xr-x 19 libmbedtls.dylib -&gt; libmbedtls.10.dylib lrwxr-xr-x 23 libmbedx509.0.dylib -&gt; libmbedx509.2.3.0.dylib -rw-r--r-- 74860 libmbedx509.2.3.0.dylib lrwxr-xr-x 19 libmbedx509.dylib -&gt; libmbedx509.0.dylib -rw-r--r-- 467852 libmpfr.4.dylib lrwxr-xr-x 15 libmpfr.dylib -&gt; libmpfr.4.dylib -rw-r--r-- 70081856 libopenblas64_.dylib -rw-r--r-- 177056 libopenlibm.2.3.dylib lrwxr-xr-x 21 libopenlibm.2.dylib -&gt; libopenlibm.2.3.dylib lrwxr-xr-x 21 libopenlibm.dylib -&gt; libopenlibm.2.3.dylib -rw-r--r-- 172436 libopenspecfun.1.3.dylib lrwxr-xr-x 24 libopenspecfun.1.dylib -&gt; libopenspecfun.1.3.dylib lrwxr-xr-x 24 libopenspecfun.dylib -&gt; libopenspecfun.1.3.dylib -rw-r--r-- 556696 libpcre2-8.0.dylib lrwxr-xr-x 18 libpcre2-8.dylib -&gt; libpcre2-8.0.dylib -rw-r--r-- 9056 libpcre2-posix.0.dylib lrwxr-xr-x 22 libpcre2-posix.dylib -&gt; libpcre2-posix.0.dylib -rw-r--r-- 285232 libquadmath.0.dylib -rw-r--r-- 190380 libspqr.dylib -rw-r--r-- 171096 libssh2.1.0.1.dylib lrwxr-xr-x 19 libssh2.1.dylib -&gt; libssh2.1.0.1.dylib lrwxr-xr-x 15 libssh2.dylib -&gt; libssh2.1.dylib -rw-r--r-- 4392 libsuitesparse_wrapper.dylib -rw-r--r-- 13540 libsuitesparseconfig.dylib -rw-r--r-- 728808 libumfpack.dylib -rw-r--r-- 37757960 sys-debug.dylib drwxr-xr-x 102 sys-debug.dylib.dSYM -rw-r--r-- 36092004 sys.dylib drwxr-xr-x 102 sys.dylib.dSYM Running time: $ time ./app x Hi: x ./app x 7.14s user 0.49s system 93% cpu 8.113 total 
You can invoke Julia functions directly from other languages most likely including whatever your GUI is written in. So then you spin up Julia when your guy starts, then you just call your functions with no overhead. 
It was removed from Base and moved to StatsBase: http://statsbasejl.readthedocs.io/en/latest/empirical.html#histograms If you try to use hist in 0.5 it will warn you about it: julia&gt; hist(randn(100)) WARNING: hist(...) and hist!(...) are deprecated. Use fit(Histogram,...) in Stat sBase.jl instead. 
Solution [here](https://github.com/dhoegh/BuildExecutable.jl/issues/28): JULIA_SYSIMAGE=libapp.dylib ./app The startup time is now 0.3 s.
`Histogram` lives in StatsBase, and you can fit it to your data using the `fit` function: h = fit(Histogram, randn(1000)) The above returns a StatsBase.Histogram type with *edges* (accessible via h.edges), *weights* (h.weights) and *closed* (h.closed).
What is the type of your dictionary? Or more precisely, what kind of object is `dict[i]`?
 Array{Int64,1}
What is it that's in the dictionary? What do you get when you do `typeof(dict[i])`?
I tried thinking of a better way to do this, but maybe it is better if you just saw whole function. This is a work in progress stemplot. You can reproduce the plot by putting this in a .jl file, include the file in your julia REPL then use the command ``stemplot(rand(1:200,80))`. There are many problems still, but I am just trying to do some formatting. The problem line is `println(pad, i, lpad(symbol,2), dict[i])` function stemplot( v::Vector; symbol::AbstractString="|", ) (left_int,leaf) = divrem(sort(v),10) i = left_int[end] stem = [] # Create a range of values for stem. This is so we don't miss # empty sets. while i &gt;= left_int[1] push!(stem,i) i -= 1 end # generator function to be used in the Dict. Looks up the leaf corresponding to the stem function f(i) index = find(left_int .== i) leaf[index] end # Dict where key == stem and value == leaves. dict = Dict(i =&gt; f(i) for i in stem) [dict[i] for i in stem] # Print the results as a stem leaf plot println("\n") pad = " " i = stem[end] while i &lt;= stem[1] if isempty(dict[i]) println(pad, i, lpad(symbol,2), "") else println(pad, i, lpad(symbol,2), dict[i]) end i += 1 end # Find a non empty stem-leaf pair to use for the key #is_stemleaf = false i=1 while i == 1 key_stem = rand(stem) if isempty(dict[key_stem]) == false key_leaf = dict[key_stem][1] println("\n",pad,"key: $key_stem$symbol$key_leaf = $key_stem$key_leaf ") i=0 else i=1 end end end 
That's great! It has been something I have been stuck on for some time. Thanks for the help. Your code is much cleaner than mine. 
Stefan [explains it well](http://stackoverflow.com/questions/23480722/what-is-a-symbol-in-julia)...
in these cases, you can think of them as "things with a name". in other languages, you would probably use enumerations or constants for this. they offer nothing else than you can check for equality, like f(a) = a == :big ? 1000 : a == :small ? 10 : error("dunno what is $a") f(:big) # -&gt; 1000 f(:small) # -&gt; 10 this construct is also used heavily in metaprogramming, but that's for another day.
But Julia is not your typical statically compiled (statically typed) language. By introducing 'true' constants, you'd harm it's interactive powers (and the ability to change / patch your code at runtime). For me, it would cripple Julia. The in-the-works solution (recompile dependencies) is IMO how similar cases (assumptions we held while compiling code are no longer valid) should be handled.
Julias focus is, afaik, to be great general purpose language (recently for example, some functionality -fft i think - went from base to some math package, to not impose dependencies in case it's not needed; although number crunchers cried, I salute it as the right step for the language). HTTP/2 support helps when you are doing http (not only servers, but also clients). It helps in cases where you'd've used multiple connections, but both sides need to support it.
I wrote an interpreter for the DUP esolang in Julia and came across the same problem. I put the variables in a Dict and for debugging I wanted a bracket free printing routine. It’s pretty easy to format the output as you like: Here’s an example Dict of variables and their values. In DUP, the variables can be alphabetic or numeric. julia&gt; vars=Dict('a'=&gt;3,'b'=&gt;7,0=&gt;3,1=&gt;2,4=&gt;99) Dict{Any,Int64} with 5 entries: 0 =&gt; 3 'b' =&gt; 7 4 =&gt; 99 'a' =&gt; 3 1 =&gt; 2 Output all variables e.g. in the format `key → value`: julia&gt; for(j,k) in vars println("$j → $k") end 0 → 3 b → 7 4 → 99 a → 3 1 → 2 If you want the output sorted by keys: julia&gt; for(j,k) in sort(collect(vars)) println("$j → $k") end 0 → 3 1 → 2 4 → 99 a → 3 b → 7 
That is a cool solution. Thanks for the post. 
First: find out what HTTP *is*. Hint: it's just a way for to computers to communicate, there are dozens of such things. How each interactis with Julia is not that interesting from a Computer Science point of view. There are basically two features: async or sync. The web is a dull/dumb place really, science wise. 
It's a symbol. Symbols always evaluate to themselves, so :dog will always be :dog, and you can use them for keys or dict keys and it's faster than using a String and uses less memory since the size of a symbol never changes, is always known, and is typically smaller than most strings. Thanks for using Julia
Because each `Dict[x]` is an `(x =&gt; value)` pair. A working alternative would be, for a Dict named `vars`: a=sort(collect(vars)) for n=1:length(a) println("$(a[n][1]) ¯\\_(ツ)_/¯ $(a[n][2])") end You can separate keys and values this way. This results in the print format: key1 ¯\_(ツ)_/¯ value1 key2 ¯\_(ツ)_/¯ value2 ...
[removed]
[removed]
This looks very promising! Are you the owner/maintainer? I get errors on most of the packages that I click upon, and I think that it would be better to expand the sometimes abstruse abbreviations of the categories names on the right…
[removed]
[removed]
I'd add LinearAlgebra.jl as well. For example, it has some blas bindings I use a lot, that are not in Base.
I did a small update and added it (as well as NLsolve).
The common way is to provide le binary using a deps/build.jl the package you are looking for is BinDeps.jl you may also look at Tony Kelman's talks at JuliaCon on Youtube
Nice looking and helpful site. 
It scrapes ~5 times a day. Something got screwed up yesterday and that's why Gadfly (for example) wasn't on the list. ----- Still working on improving the robustness of the workers. I just wanted to get some users, so I wasn't working in a vacuum.
cool, I was just curious
Hey there! I'm glad to see this worked - I helped with the "how do I add headers in mux" problem :) Once you have it running though - the maintenance is super easy (which you say in your article). Anyway - I'm super happy you got there in the end!
[removed]
This is cool! You could do some cool things with a tool like this, like searching through all package docs. Do you have a particular goal you're aiming for or is it just for fun?
How is that better than Pkg ?
/u/fully_strapped Example: If I run two different projects in rails, one with 4.2 v and another with 5.0.1 v I can switch between the compatible ruby versions easily with a [rvm](https://rvm.io/). It isn't a package version control it is a Environment Version, I guess (but I could be wrong in this point), they are two different things. 
An nvm clone for Julia could be really handy. For most things I end up using Docker to isolate my execution environment but that is sometimes overkill if I just want to run a simple script that is broken on newer versions of Julia.
Loops are replaced by a function that acts on each element of a vector in series. [Here is a better explanation](http://www.cs.cornell.edu/courses/cs1112/2016sp/Exams/exam2/vectorizedCode.pdf). 
So it's like calling .map? 
[Examples](https://matlabcompat.github.io/help.html) from their github page. 
`.jl` files are just plain text/utf-8 files. I am not sure how the format could be changed just by downloading and uploading. What do you mean by “completely changed”? Does the plain text change to binary format? Do you mean, you download the file in Julia, and upload it back, and the format has changed? Then I would assume that you maybe changed the file format from e.g. utf-8 to a binary format in the process? I think you need you give us a more specific description of your problem.
No doubt it is the difference between endline characters for unix / windows / mac. if it is, use dox2unix to fix that http://macappstore.org/dos2unix/
I thought about this possibility, too. But you can simply define/read files using any endline chars in Julia (`readdlm` would do the job, for example). Besides, it isn’t really a complete change, and I know of no editor that isn’ t able to read files with any of the endline configurations or change them arbitrarily. I would think that a “complete change” sounds rather like downloading a utf-8 file that somehow erroneously got uploaded in a binary format, probably be reinterpreting e.g the bytestream differently.
You mean big endian vs. little? That would be something! Op should really give a less vague description of the problem, how it happens and how the resulting files differ. As far as we know it could be everything. ;) 
everything!, sure. A combination of political unstability + sunspots, combined with static charge from the cat 2 countries over caused a bit to be flipped in a machine in Belarus in an alternate universe. Which caused a race condition, forced it to attack an alien race's computer / biological network which then...... *insert 20 pages of random crap here* and when the professor woke up from the sleep inducing raygun the gnome was firing at the talking snake, he has HIS memory scrambled, and installed the wrong os on the uni machine. which caused your problem....
Thanks everyone, I fixed the problem. I just had to rename my file in .pynb format 
Oh, you meant Juliabox. That's just an environment in which you *can* run Julia, not where everyone runs Julia. .pynb is an extension short for "Python Notebook" My Juliabox files are called ".ipynb" - which I think is Iron Python Notebook None of these are actually files that Julia itself runs, unless it is via the IJulia packages (which is how to run the Notebook system locally).
That’s why I thought we’d need more information about what the actual problem looked like.
Haha, no worries. It was so over the top that it was hard to interpret. In the times of president Trump it’s hard to tell satire from reality. Get my upvote! ;)
Thanks 
You’re welcome! Glad you could figure it out.
[removed]
Yes, I have the same problem here, it's the reason for the idea, I will start it this month and announce here and julia's forum when I have done. 
To be honest, I tried numba and the code got no speedup at all :\ I gave a try to Cython but couldn't compile my code for some reason and decided to give up. Anyways, since you're probably more proficient with Julia. Is it practical to do the interfacing on the other way? (calling my python custom classes/methods from Julia) I wouldn't mind at all to do it that way - it would be a good excuse to migrate to Julia actually :P 
pyjulia is unfortunately not as well maintained as the other direction, using PyCall to call into Python from Julia. That works really well and is very actively maintained and developed.
If numba gave you no speed up, it's likely that it didn't manage to compile at all and used the fallback. Try compiling in nopython mode: @jit(nopython=True) Julia python interop works as Staross has noted, but not as reliably as I would have liked. Which is why I have stayed with python+numba for now.
I really have no buisness posting, since I am not an expert in Julia or Python. Is it possible that numpy is automatically removing (aka squeezing) the single dimension that is resulting from your slice operation and Julie is leaving that dimension there (expect for the case in the first example where the singleton dimension is last). That's my guess anyway.
What version of Julia are you using here? In Julia 0.5, singleton dimensions are now dropped no matter what order they're in (so all of your examples give a 2x2 array). In 0.4 and earlier, singleton dimensions were only dropped if they were trailing indices.
[removed]
Tim Holy (https://github.com/timholy, http://holylab.wustl.edu/) is a huge contributor to julia and renowned neuroscientist. 
You can easily change the formatting of selected text in the document text by choosing &lt;a href="http://bit.ly/2eVqrDq"&gt;database sql books &lt;/a&gt; a look for the selected text from the Quick Styles gallery on the Home tab. You can also format text directly by using the other controls on the Home tab. Most controls offer a choice of using the look from the current theme or using a format that you specify directly.
Not to be rude, but just how far did you "dive in"? To say that Julia is merely intended to be a Python/Numpy replacement with higher execution speed is incredibly shortsighted. Julia is inspired by numerous languages (some of the big ones being Lisp, Python, R, Fortran, and C/C++), and is primarily aimed at highly scalable performant numerical and statistical computing while retaining a pythonic like ease of use. Julia already has most (all?) of the features you ask about and despite not having reached a 1.0 release, execution speeds that in many instances are nearly equivalent with C/C++/FORTRAN. Intel Labs is heavily invested in the Julia language and its role in the future of HPC. Intel's ParticleAccelorator.jl library adds an additional layer of optimization on top of Julia's already excellent built parallelization methods resulting in code that is sufficiently fast for even the most demanding HPC tasks. That said, I don't think Julia is ever going to be a replacement for properly hand optimized FORTRAN/C code (but then again that was never its goal). Julia is probably best thought of as HPC for the masses at 9/10th (or better) of the performance, trading a big of speed for massive gains in user friendliness. For some further reading: http://www.intel.com/content/www/us/en/events/hpcdevcon/high-productivity-language-track.html#juliainparallel https://github.com/IntelLabs/ParallelAccelerator.jl/blob/master/README.md 
&gt; This is the reason for my question because I cannot see why one would choose Julia for HPC scientific computing instead of some other language. Basically it offers you c-like performance while keeping the high productivity of languages like Python or Matlab. One of the main goal of Julia was to solve the two languages paradigm in scientific computing.
People often only (re)write the performance critical parts (which is usually a small fraction of the code), or call into existing libraries or tools. Scientific computing also often requires a lot or trial and errors (trying different models, approaches, etc), so it's much faster to first mess around in a high-level language and then rewrite the performance critical parts in C if necessary once you know exactly what you want to do. That said there's still quite a bit of friction there, and that's what Julia wanted to solve.
&gt; Do people actually write an application in Python and then convert it to another language for production? yes. reason: the first time around you're still figuring out the algorithm and need to iterate forward to the best solution. in addition, rewriting gives you a chance to break out of some dead-ends. but it's painful.
You're assuming I learned Julia in order to do this task. I didn't. However, Julia also does this job with ease. I've been paid to program in Julia (current), Python, Java, C, Limbo, C++, PHP, Perl, various Microsoft Basics, T-SQL, PgSQL, rc shell, and JavaScript; can write code in Forth, Pascal, Matlab, Lua, various assemblers (6502, AVR, Z80); have a passing familiarity with Lisp, Prolog, COBOL, Fortran, APL and Haskell; and can pick up the basics of most other languages in a couple of days. If you don't want to use Julia then don't use Julia. 
I want to be paid to program in Julia! Also that's an impressive list. Tip of the hat to you. 
When I was working on my PhD in comp neuro I used Julia a lot. I also wrote some code for fun, which I dumped in my github https://github.com/jostmey/RestrictedBoltzmannMachine https://github.com/jostmey/DeepNeuralClassifier
Yes. I use Julia as a replacement for things I would have written in C/C++/FORTRAN. For reference, the library/packages I have built are those of JuliaDiffEq, culminating in DifferentialEquations.jl. The reasons are quite numerous. For one, what you are saying about dynamic typing doesn't matter in libraries. The dynamic parts of Julia are great when you need them, but you can just not use them in numerical libraries and you're fine. In fact, I documented quite extensively how to get code which is pretty much equal in speed to C/C++/FORTRAN: http://www.stochasticlifestyle.com/7-julia-gotchas-handle/ But the more important thing is, using the correct algorithm is vastly more important in scientific computing than using a code which has a few extra optimizations. Sure, "the most optimized" code is needed for something as basic as a linear algebra library because everything is built off of it, but all higher level problems are more dominated by the choice of algorithm. This is why there's so much research in the fields! Using the correct optimization, differential equation, iterative solver algorithms gives huge speedups that micro-optimizations can never even approach (orders of magnitude difference). Julia can get you pretty much all the way in terms of micro-optimizations, letting you use FMA, SIMD, turn off bounds checking, etc. But let's say DifferentialEquations.jl's ODE algorithms were 10% away from the theoretical optimal C/FORTRAN implementations (I would be surprised if it's that high, it's probably much smaller if existent at all). Technically, I could then re-write these functions to get that extra 10%. Now let me ask... why did nobody do this in the last 20 years? We're stuck with old FORTRAN codes like dopri5 which haven't done this, and which DifferentialEquations.jl bests in benchmarks on the same algorithm by about 10x. And that's not even the full extent: in recent years there have been new and more efficient algorithms than the Dormand-Prince algorithm of dopri5, and so using the `Tsit5()` algorithm is not only a more efficient than implementation than the existing C/FORTRAN codes, but also a more efficient algorithm choice. Technically, someone could re-write this in C/FORTRAN, but it can be can take an excessive amount of time to do which is why nobody has done it. And then what you get out in the end by using Julia is just a crazy amount of features for free. Because of the type-genericity and auto-dispatching when duck typing, it's literally as though the entire library was written with C++ templating. I am not as familiar with C++ so I don't know the details, but I hear that writing and maintaining a large library which is generic through templating is very difficult. And the biggest evidence that it is difficult is that you don't see people doing it. Things like deal.ii and FENICS don't support arbitrary number systems (arbitrary precision), or allow for you to do crazy things like use an ApproxFun-type (like chebfun discretizations) as numbers. But in Julia, this is all free, and easy to maintain. The kicker is zero-overhead abstractions: http://www.stochasticlifestyle.com/modular-algorithms-scientific-computing-julia/ Not only does Julia let you implement tons of algorithms with pretty much the same performance that you'd get in C/FORTRAN, with a crazy amount of free features, but then you can provide all of these features together in one interface. Then users can swap out linear solvers, nonlinear solvers, differential equations algorithms, etc., even from different packages, all inside a scripting language, and the abstracting for handling this is very simple to setup and is a zero-cost abstraction to users. In the end, a user can pick an algorithm that specifically handles every detail in a way that specializes to their problem, which will give a massive speedup over any general function provided by a C/FORTRAN library. And lastly, macros/string macros let you define your own universe/language. This allows you to hide the fact that all of this is going on, and essentially let an undergrad copy paste from the homework assignment and get an answer which uses all of this together. This is why I think that Julia's best usage isn't even as a scripting language, but as a language for the development of numerical libraries. I mean, I wouldn't say Julia is as easy as say MATLAB, because there are times where Julia will error when MATLAB will just "give you what you want" (with a cost, of course). So I'm actually on the other end: using Julia to just call people's packages? Meh, then it's in the same boat as Python/R/MATLAB, and you'll likely get similar results (most of the time is spent in packages, so the speed isn't a big issue. Though the fact that your user-defined functions are compiled and then sent to package functions is a big deal some times). However, using Julia to build huge libraries you would have wanted to build in C/C++/FORTRAN? I think this is a no brainer, and I think people will come to Julia for these packages. There are a few things which aren't as rosy in Julia-land. The lack of static compilation is not good. Build a library in C/C++/FORTRAN, and you can easily bind it to any scripting language. Do the same in Julia? Tell people to use Julia? This story does need to get better. The good thing is, design-wise this is all possible (since everything just compiles to LLVM IR anyways), there's already experimental successes here, and the changes to Base to make it more practical have already happened (as part of the CUDANative changes). So, this should be a near future possibility. Also, the parallelism story does need a bit of work. `@threads` and `@parallel`/`pmap` is magic that works really well, but the standard networking that it doesn't isn't as fast as MPI. There needs to be an easier way to move between the levels of parallelism, and I know this is in works (and a Julep should be released "soon"). But, to end: &gt;Currently, I see Chapel as very well suited to replace the dinosaurs C++/Fortran but unfortunately its progress is very slow and the community quite small. How about Julia? Julia hasn't even had its v1.0 release, and it's already had more contributors to its Github than projects like Python, SciPy, etc. It's developer community is huge. Why? Because Julia is mostly written in Julia, so anyone can develop Julia. This is Julia's ace in the whole: everyone can help out with Julia development, and that fact has already been a major factor in its early success. This final reason is why I am confident that Julia can solve any of its remaining problems.
You can use `BigFloat` in Julia to get arbitrary precision. But I doubt R uses something like that without your explicitly asking for it. Also, double check that your Julia output is giving you all the digits stored in the Float64 and it's not just a matter of truncated output wherein the underlying data is actually correct.
Thanks, I will try the BigFloat. I will look up in the docs how to see the actual data and not just the output.
Thanks for bringing that. Up these are all problems that I would not have thought of on my own. 
Are you using Juno? If so, click on the number: it doesn't show the whole output by default.
Re-read the question. Printing an array uses a "nice" print that doesn't show all of the decimals, even in the REPL. Try `println(x[1])` and you should see a lot more.
Explanation: Unlike most every other language, Julia does _not_ play fast and loose with boolean typing. If you index with `Int`s, then it's a linear index; if you index with `Bool`s, then it's a logical index. Julia doesn't try to read your mind since implicit, under-the-hood operations like converting one to another can lead to subtle bugs. The only real reason this kind of logical indexing works in Matlab is because as a language, Matlab doesn't give a shit about typing and very strongly encourages you to just make everything a matrix of `double`s. Julia has a very strong and amazing type system, so there's no point in playing stupid tricks like Matlab has to resort to. Disclaimer: You need features that are not in any stable release yet, namely broadcasting functions like `Bool` with the `.` operator. Will be available in v0.6. PS: Also, in Julia you should just write A = [1 2 3; 4 5 6; 7 8 9] ind = Bool[1 0 1; 0 1 0; 1 0 1] A[ind] = 7 or better yet ind = [true false true; false true false; true false true] Ie, represent your data with the correct type.
Broadcasting is available in v0.5. It just doesn't fuse operators like `.*`
you can also check out **sum_kbn**
&gt; Does Julia in its current form fulfill these requirements? Large and complex applications have been written in many languages which were not well suited for it, so Julia easily meets that requirement. IMO, Julia needs some work before I'd say it's fantastic or outstanding for that goal. Modules, packages, namespaces, and all that need further refinement. That's not a slight on it's developers; they're exploring new ground here. They can't just steal all the work from Dylan so they'll need to come up with their own ideas. Like you, I've looked closely at Chapel too. It's a very promising language, but development is slow and it's far from being stable.
&gt; sum_kbn I was not aware of the function. I will try it. 
[removed]
I have also noticed that the developers are very reluctant to implement new language features and when people suggest new ideas they always say that there is no need for that because one can get the same result following some other approach. Well, using this logic one can claim that the language features C offers are enough and there is no need for object oriented programming or functional programming or any other set of features because they can all be simulated in standard C! Another thing I dislike in a language is the use of macros. When you see people writing code with lots of macros that means the language has gaps and it needs further work.
&gt; I have also noticed that the developers are very reluctant to implement new language features and when people suggest new ideas they always say that there is no need for that because one can get the same result following some other approach. That's strictly false. Look at every new release. New features are implemented daily. It's just newcomers to the language have weird priorities because they haven't learned what's available in the language yet. The answer in every thread isn't "no, go away", it's "you don't really need it, here's an alternative, submit a PR if you want it". The biggest example is interfaces and inheritance. Lots of design decisions are already in place to allow for it to be implemented (look at the whole `struct`/`mutable struct` PR), but people have been busy with the type system overhaul and broadcast fusion because those are more important, and inheritance/interfaces can just be done with macros in Julia so it's far down the priority list. But... if you submit a PR, sure, it'll get done quicker.
&gt;What are GC pause times like on average in Julia? Would it be suitable for soft real-time use? I get GC pauses. The GC is generally fast, but has latency in my applications. For what I do, that doesn't matter. But I am not sure if it would work well with soft real-time use. It's enough to cause hiccups between benchmarks, that's for sure (as in, if you run the same small code 100000 times, you will have a few much longer than the others due to the GC being used). &gt;Are there any knobs to twist in favor of latency? Not that I am aware of. You may want to open an issue for this? Doesn't look like there's an issue for this at all; https://github.com/JuliaLang/julia/labels/GC Someone who knows more about the GC would also be much more likely to comment there, or at the development section of the Discourse: https://discourse.julialang.org/c/dev
I like the new keywords better, but of course there will be some work to update existing packages etc.
Ummm, I thought I knew what the keywords meant until this, can someone explain what they're for?
You can usually avoid the GC if the main loop is non-allocating, but if it's hits you'll see it. Example: julia&gt; for i in 1:100 @time begin A = rand(100,100) B = rand(100,100) C = A*B end end 0.000255 seconds (9 allocations: 234.703 KB) 0.000176 seconds (9 allocations: 234.703 KB) 0.000199 seconds (9 allocations: 234.703 KB) 0.000179 seconds (9 allocations: 234.703 KB) 0.000171 seconds (9 allocations: 234.703 KB) 0.000167 seconds (9 allocations: 234.703 KB) 0.000158 seconds (9 allocations: 234.703 KB) 0.000151 seconds (9 allocations: 234.703 KB) 0.000160 seconds (9 allocations: 234.703 KB) 0.000189 seconds (9 allocations: 234.703 KB) 0.000142 seconds (9 allocations: 234.703 KB) 0.000168 seconds (9 allocations: 234.703 KB) 0.000144 seconds (9 allocations: 234.703 KB) 0.000149 seconds (9 allocations: 234.703 KB) 0.000162 seconds (9 allocations: 234.703 KB) 0.000156 seconds (9 allocations: 234.703 KB) 0.000161 seconds (9 allocations: 234.703 KB) 0.000145 seconds (9 allocations: 234.703 KB) 0.000150 seconds (9 allocations: 234.703 KB) 0.000157 seconds (9 allocations: 234.703 KB) 0.000213 seconds (9 allocations: 234.703 KB) 0.000160 seconds (9 allocations: 234.703 KB) 0.000217 seconds (9 allocations: 234.703 KB) 0.000153 seconds (9 allocations: 234.703 KB) 0.000165 seconds (9 allocations: 234.703 KB) 0.000144 seconds (9 allocations: 234.703 KB) 0.000147 seconds (9 allocations: 234.703 KB) 0.000184 seconds (9 allocations: 234.703 KB) 0.000157 seconds (9 allocations: 234.703 KB) 0.000181 seconds (9 allocations: 234.703 KB) 0.000161 seconds (9 allocations: 234.703 KB) 0.000148 seconds (9 allocations: 234.703 KB) 0.000162 seconds (9 allocations: 234.703 KB) 0.000146 seconds (9 allocations: 234.703 KB) 0.000190 seconds (9 allocations: 234.703 KB) 0.000177 seconds (9 allocations: 234.703 KB) 0.000163 seconds (9 allocations: 234.703 KB) 0.000171 seconds (9 allocations: 234.703 KB) 0.000175 seconds (9 allocations: 234.703 KB) 0.000167 seconds (9 allocations: 234.703 KB) 0.000159 seconds (9 allocations: 234.703 KB) 0.000168 seconds (9 allocations: 234.703 KB) 0.000251 seconds (9 allocations: 234.703 KB) 0.000166 seconds (9 allocations: 234.703 KB) 0.000173 seconds (9 allocations: 234.703 KB) 0.000166 seconds (9 allocations: 234.703 KB) 0.000152 seconds (9 allocations: 234.703 KB) 0.000157 seconds (9 allocations: 234.703 KB) 0.000168 seconds (9 allocations: 234.703 KB) 0.000150 seconds (9 allocations: 234.703 KB) 0.000147 seconds (9 allocations: 234.703 KB) 0.000146 seconds (9 allocations: 234.703 KB) 0.000126 seconds (9 allocations: 234.703 KB) 0.000172 seconds (9 allocations: 234.703 KB) 0.000173 seconds (9 allocations: 234.703 KB) 0.000183 seconds (9 allocations: 234.703 KB) 0.000162 seconds (9 allocations: 234.703 KB) 0.000168 seconds (9 allocations: 234.703 KB) 0.000160 seconds (9 allocations: 234.703 KB) 0.000173 seconds (9 allocations: 234.703 KB) 0.000167 seconds (9 allocations: 234.703 KB) 0.000156 seconds (9 allocations: 234.703 KB) 0.046953 seconds (9 allocations: 234.703 KB, 99.61% gc time) 0.000209 seconds (9 allocations: 234.703 KB) 0.000146 seconds (9 allocations: 234.703 KB) 0.000159 seconds (9 allocations: 234.703 KB) 0.000207 seconds (9 allocations: 234.703 KB) 0.000152 seconds (9 allocations: 234.703 KB) 0.000138 seconds (9 allocations: 234.703 KB) 0.000160 seconds (9 allocations: 234.703 KB) 0.000137 seconds (9 allocations: 234.703 KB) 0.000133 seconds (9 allocations: 234.703 KB) 0.000143 seconds (9 allocations: 234.703 KB) 0.000172 seconds (9 allocations: 234.703 KB) 0.000148 seconds (9 allocations: 234.703 KB) 0.000141 seconds (9 allocations: 234.703 KB) 0.000165 seconds (9 allocations: 234.703 KB) 0.000142 seconds (9 allocations: 234.703 KB) 0.000147 seconds (9 allocations: 234.703 KB) 0.000147 seconds (9 allocations: 234.703 KB) 0.000143 seconds (9 allocations: 234.703 KB) 0.000160 seconds (9 allocations: 234.703 KB) 0.000131 seconds (9 allocations: 234.703 KB) 0.000157 seconds (9 allocations: 234.703 KB) 0.000148 seconds (9 allocations: 234.703 KB) 0.000141 seconds (9 allocations: 234.703 KB) 0.000154 seconds (9 allocations: 234.703 KB) 0.000189 seconds (9 allocations: 234.703 KB) 0.000156 seconds (9 allocations: 234.703 KB) 0.000145 seconds (9 allocations: 234.703 KB) 0.000162 seconds (9 allocations: 234.703 KB) 0.000147 seconds (9 allocations: 234.703 KB) 0.000155 seconds (9 allocations: 234.703 KB) 0.000183 seconds (9 allocations: 234.703 KB) 0.000197 seconds (9 allocations: 234.703 KB) 0.000211 seconds (9 allocations: 234.703 KB) 0.000174 seconds (9 allocations: 234.703 KB) 0.000161 seconds (9 allocations: 234.703 KB) 0.000189 seconds (9 allocations: 234.703 KB) 0.000159 seconds (9 allocations: 234.703 KB) 
The upgrade path is very simple though. 
Will it solve this problem though? Modern-day HPC requires that you write a CUDA kernel in a version of C to do the heavy lifting. If what remains is glue code and data munging, which you are _not_ going to do in C, then you need two languages and what does Julia get you that e.g. Python didn't already?
&gt; Modern-day HPC requires that you write a CUDA kernel in a version of C to do the heavy lifting. That's a bit of a stretch, but of course if specialized hardware require a specialized language then you are kind of forced to use that. But in the case of CUDA there's already some native implementation (Julia is based on llvm so it seems they are able to directly compile Julia code targeting the GPU). https://github.com/JuliaGPU/CUDAnative.jl
Does Pkg.build(HttpParser) do the same as Pkg.add(HttpParser)? I would think you couldn't rebuild the package if it never was installed in the first place.
Have you tried reinstalling all of the packages? Like clear out anything to do with Julia in Atom, then Pkg.rm("Juno"). Unfortunately I can't remember the exact issue but I know I had trouble getting this to work initially myself. I think once I cleared everything Juno and Atom/Julia out then reinstalled it, it started to work for me.
I suggest you read up a bit more about Julia, as the whole idea of Julia is to solve the performance issues traditionally associated with script languages. With a regular script language you don't know the type of a function argument when a function gets called. However with Julia, the type or the arguments is determined at the point of function call. Then Julia looks up in a table a chunk of optimized machine code made for exactly those argument types, and runs that. If it doesn't already exist the Julia JIT will compile it. It can further this analysis so that the machine code for functions it calls is further inlined. Look e.g. at how `2 + 3`, is executed in Julia. If you step through with a debugger you will see that lots of dynamic looking code gets called. But if you look at the machine code Julia JIT will generate for this, it turns into a single assembly instruction for adding two integers. At the Julia REPL you can write e.g. julia&gt; @edit 2 + 3 This will open a text editor and show you the Julia code executed for this. It looks like this: +{T&lt;:BitInteger}(x::T, y::T) = box(T, add_int(unbox(T,x),unbox(T,y))) Looks inefficient and complicated for something that simple right. But if you look at the generated assembly code it is just a function call using leaq to add the integers. julia&gt; @code_native 2 + 3 .section __TEXT,__text,regular,pure_instructions Filename: int.jl pushq %rbp movq %rsp, %rbp Source line: 32 leaq (%rdi,%rsi), %rax popq %rbp retq Source line: 32 nopw (%rax,%rax) If you define a function doing this, e.g. called `add` you will notice the call to it gets optimized away and Julia turns it into the same machine code. julia&gt; add(a, b) = a + b add (generic function with 1 method) julia&gt; @code_native add(2, 3) .section __TEXT,__text,regular,pure_instructions Filename: REPL[7] pushq %rbp movq %rsp, %rbp Source line: 1 leaq (%rdi,%rsi), %rax popq %rbp retq nopw (%rax,%rax)
Hey everyone, I got it working and just wanted to update yall. I needed to install the xcode command line tools to get httpparser built in Julia. Thanks for the help!
Yup just did!
fixed. thanks for catching that!
Lots of people seem to be using attobot, as seen by the METADATA PRs. I use it quite a bit now. It can't do everything, but it sure is easy.
1) macro arguments are seperated from the macro call by spaces. julia&gt; @identity (x=2) 2 2) Your macro returns what the expression would return. If you use the macro on an assignment, the assigned value is returned. A function or type definition returns the function or type. To get what you want you'd have to eval the expression inside your macro and return that: julia&gt; macro identity(a) return eval(a) end @identity (macro with 1 method) julia&gt; e=2 2 julia&gt; @identity e=1000 1000 julia&gt; e 1000 julia&gt; f(x) = x f (generic function with 1 method) julia&gt; @identity f(x) = x + 1000 WARNING: Method definition f(Any) in module Main at REPL[98]:1 overwritten at REPL[99]:1. f (generic function with 1 method) julia&gt; f(1000) 2000 
Macros should return expressions, not values. If they return values, they will compile once and that'll be what they return: not really what you want from a macro (unless you only use it in the top scope of a module or the REPL... but that's a different story).
one more piece of advice. if you want to see what your macro actually does, use this syntax: macroexpand(:( @identity e=1000 )) you get the expression the macro returns without executing it.
[removed]
[removed]
[removed]
[removed]
[Matlab](https://www.mathworks.com/help/matlab/ref/fftw.html) and [Julia](http://docs.julialang.org/en/stable/stdlib/math/#Base.fft) both call [FFTW](http://www.fftw.org/) for Fourier transforms, so besides minor differences in overhead, both should have the same performance when given the same function call. That said, there are still ways to get ahead with Julia. If you know your input array is real, you can use `rfft` for a significant speedup (a factor of 4 on my machine for a 10000x10000 real array). Also, if you'll be repeatedly performing FFTs on arrays of the exact same size and element type, you can use `P = plan_fft(A)`or `P = plan_rfft(A)` to precompute a linear operator `P` that can then be applied with `Â = P * A`. 
This is spot on. It should be noted that this is a pretty common thing too. For these kinds of functions which have a canonical C/FORTRAN implementation; Julia, MATLAB, SciPy, Mathematica, etc. all will simply just call that library. Here it's FFTW. For matrix multiplication its a BLAS. Etc. In these cases, you cannot expect much of a speedup because it's calling literally the same function. However, Julia has much stronger FFI, and so can expose more of the true C interface in order to get the full potential. Here you see it via `plan_fft`, but with BLAS you can call the in-place BLAS functions, etc. That's where you'll get gains for things like this.
No, since the results are not identical. `RFFT` [takes advantage of conjugate symmetry](http://docs.julialang.org/en/stable/stdlib/math/#Base.rfft), so the length of the transformed complex array's leading dimension is approximately half that of the input real array. Any operations on `Â` obtained from `rfft(A)` need to take this into account. An illustrative example: using Plots t = linspace(0,2pi,2^4) const A = [sin(t1)*cos(t1/2)+cos(2t2)+randn()/10 for t1 in t, t2 in t] heatmap(t,t,A) Ahat1 = fft(A) Ahat2 = rfft(A) heatmap(real(fftshift(Ahat1))) # Note symmetry along the first dimension heatmap(real(fftshift(Ahat2,2))) # Symmetric part not computed or stored: more efficient
Well... the *information* is identical, it is just that the `fft` version explicitly calculated parts that it didn't need to. The `fft` calc *could* use the `rfft` function and then mirror the result to get something in the `fft` format. Though I suppose I understand why that isn't the case.
Right, it saves half the storage. Any user advanced enough to make a distinction between `fft` and `rfft` should be able to work with `rfft`'s output. I agree that it seems reasonable for `fft{T&lt;:Real}(A::Array{T,n})` to be computed with `rfft` and then mirrored by default, since it'd save on processing even if storage is no more efficient.
FFTW (which all three languages use for DFTs) caches plans internally. The plans [can be destroyed](https://github.com/JuliaLang/julia/blob/master/base/fft/FFTW.jl#L403-L411) during garbage collection, so it's a good idea to explicitly store plans with [`plan_fft`](http://docs.julialang.org/en/stable/stdlib/math/#Base.plan_fft) if you want that behavior.
[removed]
All my experience is just installing Julia on Raspberry Pi 3 with Ubuntu Mate on it. So, my idea was to actually use Raspberry as a desktop computer, rather than just to deploy something on it. Overall, it worked more or less. But some packages didn't work, including PyCall, due to their reliance on system libs and sometimes there were version mismatches. With PyCall it was quite critical, since it meant no IJulia, no PyPlot and hence no Plots (at the time I didn't try it with PlotlyJS). Also, Atom didn't run under Raspberry, so no Juno IDE. The only way I found to develop Julia under Raspberry was to use Emacs with julia-mode, but it doesn't have all the features.
This is excellent work and much needed within Julia. Keep up the good work!
my julia.bat (yeah, I know) @echo off julia --precompiled=yes %* Which reduces startup time considerably Another way is to use Jupyter (which is the system http://Juliabook.com uses) julia&gt; using IJulia julia&gt; notebook() and you can edit your code in a browser window (which is handy for plotting with JS plot etc.) and take advantage of partial compilation 
Thanks for the correction. I edited it to say these things are C/FORTRAN, since generally the lower level libraries tend to be in those languages. Though I do see FORTRAN written like this still. The change probably hasn't completely caught on.
If you just want to speed up your testing process, definitely program in some sort of REPL. I write my scripts emacs and copy paste into the repl to run parts. In emacs, the copy-a-block, switch to REPL, paste, run combo takes 4 keyboard presses, so it's quick. I'd like to run the REPL inside of emacs but AFAIK there's no decent julia mode for it yet.
I don't know what verilog is so coming upon this package I feel quite lost. A sentence or two of explanation and link to its webpage on the front page of your github would be helpful to people like me.
I really really like using Juno. It uses the Atom text editor along with some extra packages to make using Julia about as simple as using MATLAB. And it's prettier!
Have you tried ESS (Emacs Speaks Statistics)? It's mostly built around R, but it also supports REPLs in several other languages including Julia (also Stata, JAGS, and BUGS). http://ess.r-project.org/ 
[removed]
[removed]
I have. It's great for R, but quite buggy for julia unless something's changed in the past 3 months. Thanks anyway!
&gt; --precompiled=yes that has been the default since 0.5
Hi all - I'm the former Founder and CEO of Staffjoy, and I was the primary author of this library (which served customers in 2015). I'm happy to answer any questions. You can also read more about this repo and our experience with Julia [on the official Julia Discourse](https://discourse.julialang.org/t/staffjoy-scheduling-algorithm-in-julia-now-open-source/2468). 
[removed]
You can get the precompiled packages by looking at the .julia/lib folder. Each precompiled package has its own aptly named .ji file.
Thanks!
/u/lusion sorry but I think I doesn't express myself right. the tool don't generate any scaffolding. It provide you to easy work with different Julia versions in the same environment. So If you're using julia 0.5.1 but your projects broke when you wanna test a new version like the 0.6.0-pre-alpha and respective new functionalities, juliavm offer you a easy way to have more than one julia versions on your computer, so you can have your projects who run only in 0.3.11 version of julia language, and also run your new application using the newest version without make your hands dirty and install/uninstall the language each time you wan't work in a project which support a different version. EDIT: GRAMMAR
Oh wow you did all this in last one month? That's pretty amazing and much needed.
I think /u/lusion meant to post about [julz](https://www.reddit.com/r/Julia/comments/5xtxpo/julz_a_julia_framework/) And I agree it should handle more substantial stuff. It's just the package is at v0.0.1 and stuff like that needs community input
Yes, it's a better approach. 
right now: + automatically loads all src files + exports funcs/types/etc that share name with files + wraps every file's test sets in a global test set + generates files &amp; tests using boilerplate templates + is usable through terminal (you can easily generate/destroy files) in the future: + standardize how you use people's code and how they use yours + have packages that add custom subdirs (e.g. `solvers`, `compilers`) that behave in certain expected ways + adds place to store configuration options (i.e. I want to use algorithm A in some package by default **or** I want to use mysql for the database) + configure bootup process (i.e. first set some variables, then load packages, and finally wrap up setup) this is by no means a comprehensive answer. i just thought it was necessary to start a feedback loop
I wanna mean, it's a better point of view, but isn't the same way to manage the envs ....
Hi, thanks for the interest. I have wrapped the project into a Julia module, which makes it extremely easy to play with. For example, you can clone, load the module and then call test_svm() for testing SVM. Every machine learning algorithm implemented now have a test function for testing purpose. Also, any contribution is welcome, thanks!
Updating status: Added uninstall option and supporting 32 bits architecture. 
Ok, that is amazing, Jump code is always soooo very cool, and Factorio is a perfect test bed :) I wonder if we can use the optimiser to do layout....
what exactly is linear programming? 
Its a way of finding optimal values for linear functions with many dimensions. As you can imagine, finding the number of optimal factories with 10 chained recipes to reduce the number required to produce 10 Prod3 modules is a lot of space to explore. Linear programming does it in a way that makes it easier to find. The name programming is not a sort of computer programming - you can do it by hand
It's a constrained optimization problem where the objective to minimize and the constraints are both linear.
Awesome julia. I really need to learn it (again)!
What would you expect for `a` as a 3x3 and `b` as a 2x2? a = reshape(1:9, 3, 3) b = ones((2,2)) Would you expect [2 3 4; 5 5 6; 7 8 9] or [2 3 3; 5 6 6; 7 8 9]
The expression `(1:i for i in size(b))` can be more precisely and succinctly expressed as `indices(b)`. I think that's about as good as you're going to get: a[indices(b)...] .+= b
only so familiar with julia, but could you just write a function for this kind of "matrix addition"?
&lt;-- This 
&gt; However, I don't know the last nonzero power in advance, so I can't just have long sparse array and be done with it (practically speaking, I probably could) Actually, it's pretty simple to estimate this reasonably well. You just need to estimate the number of primes less than your maximum possible input. Primes are funny creatures whose density decreases roughly as the natural log, so the number of primes up to x is approximately given by pi(x) ~ x/ln(x) So for 64 bit the factors array would need to be about 4.1582853e+17. That's obviously massive, but a sparse array will have no problem with it whatsoever. Furthermore, no 64 bit number can have more than about a dozen or two unique prime factors, so this array will always be _really_ sparse. 
you're asking people to click your downvote button?
Oh boy. I play a lot of factorio and I work with Integer Linear Programing. Since a while ago I have been wanting to do something for the game. 
The [LightGraphs](https://github.com/JuliaGraphs/LightGraphs.jl) package includes the [PageRank](https://en.wikipedia.org/wiki/PageRank) algorithm. See also the [Pagerank](https://github.com/purzelrakete/Pagerank.jl) package, which is written for an older version of Julia but shouldn't be too hard to upgrade.
You want [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf). Take your array of sentences and let's call them documents and the list of documents a corpus. For each document, compute the document frequency of each word. Then for each document, compute a tf-idf vector that is the list of the tf-idf values for each word in the document. Then when given a new document, you can compute the cosine similarity of the new document to each document in your corpus, and also rank the relationship of each document in your corpus to your new document. Edit: By the way, this is not the Google PageRank algorithm, but I'm pretty sure that's not what you want. Google's PageRank works by ranking documents based on their links to other documents. A document with more links to it is ranked higher than another document. It's essentially a graph algorithm that computes a ranking of "most central nodes". Then when you use a search term, the matches to that term are ranked by the PageRanker. This is a bit different since the necessary requirement is the "links" to compute the PageRank. Your sentences don't have links to each other, so I'm not sure how a PageRank would be implemented.
I don't code in Julia, and never did, but tried to support it. It seemed simple to support `using` and `include` statements, as well as those `REQUIRE` files, which just list a bunch of external dependencies. It seems to be working well in my personal tests browsing random Julia repos on GitHub, but feedback from actual Julia programmers would be great.
I see it also supports `import` statements as well. This works really well. Nice tool. Thanks!
Macros are like creating a special dialect of a programming language making it difficult for someone to understand and further develop existing code. Also, when developers require the creation of macros to satisfy their needs, this is an indication that the language lacks important built-in features.
This is a blog post which criticises *numerical programming* performance claims, as well as many other aspects of a large and complex project, based on a cursory timing of `println("hello world")` and a dislike of 1-based indexing. Folks, there's nothing to see here, let's move on.
I think you just made me excited for Julia again, after a period of not hearing much about it. How good is Julia for general purpose programming? 
&gt; You just can't make runtime safety "free", so this is a good middleground and a good design decision unless you're asking for the impossible. I wonder if this is actually true though, as it would be in Python, but not necessarily in say, Haskell. I'm still pretty new to Julia, but I am very impressed with it's type system, which I think could be further leveraged to give a lot more runtime safety at compile time. The cost then is more fussy types, so possibly Julia is still making the right tradeof though, just not so certain about the "impossible" bit. &gt; So for the vast majority of people, the fact that Julia's parser is written in femtolisp just doesn't matter. No doesn't matter, but still is a bit dissatisfying, especially considering how lispy Julia is :) I think it'll eventually be in Julia.
ps: you can check for such errors by looking at the lowered version: @code_lowered +(:a, :b)
Ah, OK, thanks a bunch Edit: also it works
&gt; Python code for mathematics (NumPy/SciPy) is well established as being very verbose. The last time I did an analysis in Python I felt like I was writing a book! 
&gt; if there's a runtime safety check it has to have a nonzero cost. Not necessarily – the compiler can optimise away various kinds of checks it can prove at compile time. I think the OP's post was arguing for a way to parse C headers and emit the `ccall` wrappers with appropriate type and argument number checks; that would make C interop safer without compromising performance. This may have been a valid criticism at the time, although it looks like Clang.jl provides this now.
No, it is "treat this as a constant, and thus you can assume that the value will not change". Example: julia&gt; const a = 1 1 julia&gt; f() = a f (generic function with 1 method) julia&gt; f() 1 julia&gt; a = 2 WARNING: redefining constant a 2 julia&gt; f() 1 You told it that `a` is a constant, it realized it could inline the value and just make a function which returns the value (1). It will do this the first time it compiles the function. Then even if you change it, you will not change the compiled code for `f`. But now you have to remember, what is the value of a matrix? A value of any array is its pointer. That is why `a=rand(5);b=a;b[1]=10;a` will return 10: since changing `b` changes `a` since all you've done is set the pointer (instead of `b=copy(a)`). So, the compiler will know that its value is some pointer. But, the good thing is that information comes with type information. So the compiler can type the variable inside of any compiled function, knowing that you gave the guarantee that it will not change. But the values of the array, i.e. not the pointers, so information about the size and elements, are runtime information. So this information cannot be optimized on. Instead, you can use something like a `StaticArray` or a tuple which has, as compile-time information, the size. This can help with code optimization. A constant `StaticArray` is probably what you want.
Have you profiled it? Where is all the time spent? There are tons of temporary variables here. Julia v0.6 will help when fusing some broadcasts here, but in general this algorithm should be changed in a few places. `flowvals + beta*EVals` that's a bunch of temporaries. `V-Vold` temporaries. `Vold = copy(V)` unnecessary to copy: just `copy!`. You can get some pretty extreme speedups by changing the amount of allocations going on there. There are likely a few tutorials detailing this, and if I'm not busy I might write something more detailed here later.
 for i in 1:N_a, j in 1:N_l, k in 1:N_a @inbounds EVals[i,j,k] = EV[j,k] end works too
Looping like that should be a bit faster: for k in 1:N_a for j in 1:N_l for i in 1:N_a @inbounds EVals[i,j,k] = EV[j,k] end end end Otherwise the maximum(abs(V-Vold)) seems to be taking quite a bit of time, you could also devectorize it. Edit: tried it, and it doesn't seem to do a big difference. function absmax(V,Vold) m = V[1]-Vold[1] @inbounds for i in eachindex(V) x = abs(V[i]-Vold[i]) if x &gt; m m = x end end m end You also have a small type instability because you declare error as an Int error = 1 -&gt; error = 1.0
I don't know about tutorials, but the Julia community (here, stackoverflow, the discourse forums) are typically pretty good about responding to questions about performance. If you can find a specific example of your own code that's not performing as you'd like - ask how to make it faster, and folks will usually respond. Then you'll learn working on your own problem, which is probably better anyway :-)
One thing to note is that MATLAB is "cheating" a little bit here if most of the time is spent in the vectorized operation. This is because MATLAB has implicit parallelism (multithreading) for element-wise operations. So it's probably multithreading a lot of this, while the Julia code you wrote is only using a single core. And MATLAB is fairly efficient when in vectorized code (it's just calling C-code). Because of this, you may only get close to 1x with MATLAB with single-threaded Julia here. (there a lot of allocations to cut, and a type-instability to get rid of here) Currently the best way to handle this is to unroll the vectorized computations to loops, and then `@threads @inbounds` them. However, there should be an `@threads` that works for broadcast in the near future.
Yes, functions are constant global variables. This is why you can call a function without importing into a local scope! Because they are considered constant, there isn't a performance problem though. The fact that they are constant, plus other code showing how constants compile, is what led to the infamous issue 265 which was solved in Julia v0.6. Remember, in Julia functions are just variables. `f(x)=2; g=f` is fine!
Also, check out the `learning` section of the Julia website. Has references to blog posts, tutorials, and books.
Working through this now. Thanks!
Why not use ParallelAccelerator to fuse and parallelize the vector operations?
Giving build errors right now. I'll look at it more tomorrow. Thanks.
Nice tutorials. Thanks!
The same is true of the Int types. I assume that this is done in the name of efficiency; a vector of Float64 will consist of an array of 64 bit doubles and can be processed as such without worrying about conversion.
Abstract types aren't based off of conversions, it's based off of actions. For all intents and purposes, for mathematics (arithmetic, linear algebra), a `Float64` "has the same actions" as a `Float32`. So if you replace a `Float32` with a `Float64` in most functions, it will act the same, and just have a difference in precision. In Julia, I find that it is better to think about types by their traits and actions. How do they behave? Thinking about "what they have" fits object-oriented programming more (i.e. classifying by fields, and here, number of bits), while thinking about how their actions work fits multiple dispatch because duck-typed functions will "just work" if the actions are defined.
Good points
Why don't you do it in Matlab?
Thanks, I've read a bit about optimisation in Julia, but some of it seems rather obscure like pre-allocating outputs. I'm not sure when this will give a boost. 
In theory Julia can be as fast as other languages, but in some rare cases you can hit some issues. Without code we can't say much more.
in my limited experience, if you need really high speed, you need to employ a very specific style that is not that natural and concise. and you need to use @code_lowered and @code_llvm / @code_native quite a lot.
You shouldn't need to do much at all. Type-stability is just something that usually happens naturally: just don't change the type of the argument. And pre-allocating when you work with big arrays is something you have to do in any language to get good performance, and pretty much guarentees type-stability. If you just put `.`s around then devectorization isn't needed in v0.6. Quite frankly, I don't know what you're talking about, at least when it comes to scientific computing and data science (outside of this domain, the same principles still hold, but there might not be as many built-in language shortcuts).
The root cause here is [issue #15276](https://github.com/JuliaLang/julia/issues/15276). Comprehensions use anonymous functions. Anonymous functions are "closures" over the variables they use. It can be hard to determine if the function will need to "reach back out" into the original scope to modify the original variable. If it does end up modifying the variable, then many optimizations cannot apply. That re-assignment is making it a bit harder for Julia to determine if it can optimize this case.
That makes sense. Thanks!
If you have small number of functions I would recommend to make wrapper with extern "C" (google on it) decorations and use julia build system to compile the code (make a package). By default (better check) julia passes pointers to the C functions thus you can edit allocated arrays in julia from the C code. Also Cxx looks much easier to install than a year ago, which could make to wrap C++ easier. 
I've found Cxx.jl is super easy to use except for support of std::tuple
I would also consider using the muladd function. On x86 machines this should invoke the fma opcode which will give you faster performance AND higher precision. for idx = 1:length(Values) @inbounds values[idx] = muladd(beta[idx], Evals[idx], values[idx]) end
It is supported on Windows, but the latest update to the Pkg resolver is having issues.
This is not accurate. It's the dependencies between packages that have become more complex and harder to solve recently. The latest updates to the solver have actually fixed some of the issues, but most changes are in julia 0.6 only, and haven't been backported to 0.5. On 0.6, there were no issues reported as far as I can tell.
Did you try setting `ENV["JULIA_PKG_RESOLVE_ACCURACY"]=2` or more?
I've done that, and am following their directions exactly (I followed the steps written in my OP).
Thanks. I tried that, and get a long message, ending with an error when I enter the second line (pasted entire message here: https://justpaste.it/15lp5). The error: ERROR: failed process: Process(`ipython notebook --profile julia`, ProcessExited(1)) [1] in pipeline_error at process.jl:476 in run at process.jl:453 in notebook at /home/notParticularlyAnony/.julia/v0.2/IJulia/src/IJulia.jl:177 in notebook at /home/notParticularlyAnony/.julia/v0.2/IJulia/src/IJulia.jl:176 
Holy crap. Yep that's an old version I'm using (I installed it through the Ubuntu software center--that was a mistake). Installing newest version now. [Edit: Yep, that was the problem. Holy smokes. Thanks for the help! In my OP I wrote "I'm sure I'm doing some stupid noob mistake"...well...yep.]
glad to help with two of my favourite FOSS at the same time!
yes, I tried this. I went up to 10, but it still doesn't work. 
I use NLSolve to solve for a system of non-linear equations and I cannot pass parameters in the function I'm solving unless I do the following (example from the Github page): type LotkaVolterra &lt;: ParameterizedFunction a::Float64 b::Float64 end f = LotkaVolterra(0.0,0.0) (p::LotkaVolterra)(t,u,du) = begin du[1] = p.a * u[1] - p.b * u[1]*u[2] du[2] = -3 * u[2] + u[1]*u[2] end So, I do use functions but I simply pass the parameters around using constants, which to me seems the easiest workaround and it makes the files clearer. Even excluding the issue with NLSolve, I would need to unpack the parameters (instead of p.a-&gt;a) which is unecessary extra work. My question is whether there is a performance penalty using constants or any other downsides because from the codes that I've seen everyone uses composite types to pass parameters. To give you an example, these are the parameters that I need to pass: const nage = 15 # maximum age const nw = 9 # number of working periods const nj = 30 # discrete states of ex ante heterogeneity (food preferences + streneousness of job) const n = 0.03 # population growth #---------------------------------------------------------------------------------- ## auxiliary variables #---------------------------------------------------------------------------------- const nvar1 = nj*(nage-1) # size of savings vector const nvar2 = nj*nw # size of labour supply vector const nvar3 = nj*nage # size of food and composite good consumption vector const nvar = nvar1+nvar2+nvar3 # size of the vector for first order conditions #-----------------------------------------I----------------------------------------- ## final goods production #---------------------------------------------------------------------------------- const α = 0.33 # share of capitaltalare of capitaltal const δ = 1-(1-0.07)^(80/nage) # depreciation const A = 5.5 # TFP #---------------------------------------------------------------------------------- ## preferences #---------------------------------------------------------------------------------- # U = utility function # c = composite food consumption # f = food consumption # l = labour supply # Ω = utility from BMI (humped shaped-so there is an ideal weight) # U = c^(1-γ)/(1-γ) + ef(j)*φ*f^(1-σ)/(1-σ) + ν*(1-l1)^(1-η)/(1-η) + Ω # Ω = ω0 + ω1*BMI + ω2*BMI^2 const σ = 2.5 # CRRA coefficient foodfood CRRA coefficient foodfoodnt foodfoododfood const γ = 2.1 # CRRA coefficient composite good const η = 3.0 # CRRA coefficient labour supply const β = 1.01^(80/nage) # discount factor const ν = 3.5 # relative weight of labour supply const φ = 0.01 # relative weight of food consumption const ef = [1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0] # types of preferences with respect to food consumption const ω0 = 40.0 # parameter of the utility derived by BMI (constant): const ω1 = 0.22 # parameter of the utility derived by BMI (linear) const ω2 = -0.01 # parameter of the utility derived by BMI (quadratic) # ---------------------------------------------------------------------------------- ## BMI #---------------------------------------------------------------------------------- # BMI = BMI(-1) +ξ*f -METl*l*BMR-MET*(1-l)*BMR [BMI= previous BMI + (calories in) - (calories out)] # probability of survival = 1-(p0+p1*BMI+p2*BMI^2+p3*BMI^3)*d [BMI increases the probability of death non linearly] const ξ = 5.0 # food consumption into calories for the weight determination weight determination const METl = [1.5 2.5 3.5 1.5 2.5 3.5 1.5 2.5 3.5 1.5 2.5 3.5 1.5 2.5 3.5 1.5 2.5 3.5 1.5 2.5 3.5 1.5 2.5 3.5 1.5 2.5 3.5 1.5 2.5 3.5] # degree of job streneousness [MET = metabolic equivalents of tasks] const MET = [2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0] # MET for leisure (1-l) const BMR = Array(readtable("data_bmr.csv", header=false)) # resting metabolic rate const p0 = 7.721 # parameter of BMI hazard risk (constant) const p1 = -0.6427 # parameter of BMI hazard risk (linear) const p2 = 0.01905 # parameter of BMI hazard risk (quadratic) const p3 = -0.0001638 # parameter of BMI hazard risk (cube) const d = Array(readtable("data_death.csv", header=false))# used the data from the probability of death const w0_10 = [19.53066 20.90843 21.9316 23.13361 24.13637 25.30629 26.5954 28.92573 32.29603 33.26636] # initial level of BMI for type i) # initial level of BMI for type iii) ## household productivity #---------------------------------------------------------------------------------- const ey = [linspace(0.5,1,5); linspace(0.95,0.7,5) ] # exogenous age-dependent productivity profile const Phi = -0.00 # parameter for the endogenous labour productivity exp(Phi*BMI) #---------------------------------------------------------------------------------- ## health care spending #---------------------------------------------------------------------------------- # households face a compulsory lump sum health care spending "shock" which is described as: # m=μ*(bmi-21)+μ0 # Since the government covers a significant fraction of household's health care spending, the out-of-pocket medical spending is (1+taum)*m, where taum is the subsidy rate const μ = 0.01 # elasticity of additional medical spending with respect to excess BMI (deviation from the average BMI) #const μ0 = 0.0 # the average is calculated endogenously, in order to reflect per capita health care spending #---------------------------------------------------------------------------------- ## government #---------------------------------------------------------------------------------- const gy = 0.2 # government spending Gng G const τr = 0.46 # capital income tax const τc = 0.2 # composite good consumption tax const τf = 0.097 # composite good consumption tax const ζ = 0.29 # replacement rate for pensions (net) const τm = -0.83 # health care spending subsidy Then I can simply call: solution = nlsolve(getss,x0)
&gt;The Julia documentation encourages the use of constants instead of globals for well-known reasons, but it's not clear if it's just better or optimal. It's optimal. It will be a compile-time constant. It will inline into functions, and if a function is all constants, it can compile to a constant. They are a good tool to use, and feel free to use them when you want without worrying about performance.
That's quite a lot of parameters! Using constants like this is probably fine for what you're doing. The standard old trick is to use closures. `f(x,p)` is a function with parameters, `g = (x) -&gt; f(x,p)` wraps the parameters in there, given that `p` is already defined. But lots of parameters can make this messy. Another option is https://github.com/mauro3/Parameters.jl. You can wrap all of this into a type. You can use my function parameterization trick (well, thank ihnorton for the idea) instead with a parameterized type: p_type = MyParameters() # Assuming you used Parameters.jl type MyFunction{P} &lt;: Function p::P end f = LotkaVolterra(p_type) (p::LotkaVolterra)(t,u,du) = begin du[1] = p.a * u[1] - p.b * u[1]*u[2] du[2] = -3 * u[2] + u[1]*u[2] end Now `f.p` is your type which contains all of your parameters. You can also just enclose arrays if you want. Actually, you can put any type in there, so don't worry. And if it's an immutable type, it's an essentially zero-cost abstraction (though using a mutable type for parameters is still extremely cheap and I believe you wouldn't be able to even measure the cost). DiffEqBase has something which automates the any-type function parameterization wrapper: pf = ParameterizedFunction(f,params) Then `pf` is essentially what I showed above, where `f` is the ODE function. This is made specifically for ODEs, but it wouldn't be difficult to make one just for optimization problems. We have it in the DifferentialEquations.jl plan to make supporting this kind of thing better: https://github.com/JuliaDiffEq/DifferentialEquations.jl/issues/146 Feel free to comment/feature request there. We just added a PyDSTool wrapper for bifurcation plotting, and now I want to make something for solving for steady states that's more automatic. Note that directly using a nonlinear solver isn't that great... there are much better methods to get steady states of dynamic equations. I am thinking there should be a `SteadyStateProblem(f,u0)` where `f` is the ODE and `u0` is the initial guess, and we can just extend `solve` to have algorithms that give back steady states for this. Or, could you suggest a design that would be helpful to you? Maybe some macro that makes things easy (though Parameters.jl kind of has that covered), or generation from DataFrames or something? Maybe something that can read a file and build a type which has those constants?
Thank you very much for the reply. I need to digest a bit the information and work with the methods in practice to see which one is more "convenient". I have to admit that coming from Matlab, even the solution that I posted above is elegant enough for me because I don't need to pass all these parameters one-by-one every time I call a function (which was also prone to typos). Somewhat unrelated, I posted a few days ago regarding another issue and the NLSolve didn't converge. I had made a typo, which we both suspected was the issue and now everything works like a charm. I want to thank you because if I didn't feel like there is a community to ask questions, even if it's not mature yet like Matlab, I would have given up on Julia over the frustration from a silly typo. Not only it is much much faster, I managed to upscale the model which was not feasible in Matlab. 
A related question - will this whole recompiling deal be gone by 1.0?
Those are interesting benchmarks. I would still use sed s/_$// and grep -c '\^&gt;' because they are easier to remember and type How fast is 'time cat filename.fasta &gt; /dev/null' I suspect that I/O is the dominant factor in your timing
&gt; So SymPy.jl is a wrapper for calling python's SymPy, and SymEngine.jl is a wrapper for C++ SymEngine? &gt; So SymPy is writing in pure python, and SymEngine in C++. I assume SymEngine would be faster than Indeed that's the case. SymPy is a bit more developed still though. It doesn't have equation solving (i.e. finding the roots of a polynomial) or simplifying yet. But if you can avoid requiring a solver, it does well. They should get a solver soon...
I showed this in a bit more detail in a new blog post: http://www.stochasticlifestyle.com/fun-julia-types-symbolic-expressions-ode-solver/
If you fancy a real speed up, try OpenCL https://github.com/JuliaGPU/OpenCL.jl 
Hey! Right now I'm working on trying to recreate an IVGMM estimation with a non linear relationship (Poisson Distribution) assumed between the Explained Variable and the regressors (Example 6) http://www.stata.com/manuals13/rgmm.pdf Right now, I'm just trying to sort out how to program my error terms and a GMM estimator(S bar). I might be making a mathematical error with the way I'm setting up my arrays/matrices. 