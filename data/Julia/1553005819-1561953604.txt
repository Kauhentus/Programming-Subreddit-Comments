this problem is much harder than a forum post can cover. floating point operations come with rounding errors. matrix operations are notorious for magnifying the problem a lot. of course you can omit rounding, for example move to rational numbers, possibly over BigInt (so complex over rational over bigint, yikes). but it will slow down things enormously. certain decompositions are developed to mitigate the problem.
Thanks, that helped a lot :) 
You cannot tell which number is missing without looking at all of them, unless they are sorted, which is probably not the case.
Have a cluster I manage with 18 nodes, 64 cpus each primarily used with Julia. I develop locally, FTP into the server to upload and ssh to run my script. Interested in seeing another method. 
Interested!
That sounds like a pretty gnarly rig! Our solution for that would be to install our application (URL below for more info) on your custom machine(s), so that you can deploy jobs there in no time at all. We have worked with a few ML researchers and material scientists who are using it that way. Alternatively, if you eventually need more compute power than you have locally, you'll be able to purchase time on cloud, or on another custom machine somewhere else in the world. &amp;#x200B; [https://galileoapp.io](https://galileoapp.io)
I have a dockerfile that goes into Kubernetes via our CI/CD pipeline at work. The dockerfile itself is very straightforward, I can probably share it here if anyone wants it.
Advocate of the devil here; can't jupyter do something like this? Don't work over ssh enough usually I just upload and run if there would be an alternative that would be great!
So would this be something along the lines of kubernetes, but with a focus specifically on HPC? 
pandas and ggplot? In Julia land they are juliaDB (kinda...) for doing panda like things over bigger datasets, and all kinds of plotting systems, mostly based around grammar of graphics. &amp;#x200B; [http://docs.juliaplots.org/latest/](http://docs.juliaplots.org/latest/) [http://gadflyjl.org/stable/](http://gadflyjl.org/stable/) etc etc &amp;#x200B; I'm not sure I would say they are a replacement for spreadsheet programs, but they do pandas and ggplot like things. I mean also including [https://github.com/JuliaPy/Pandas.jl](https://github.com/JuliaPy/Pandas.jl) and [https://avt.im/blog/2018/03/23/R-packages-ggplot-in-julia](https://avt.im/blog/2018/03/23/R-packages-ggplot-in-julia) which ARE pandas and GGPlot. &amp;#x200B; &amp;#x200B;
I want to use native julia packages. Which julia packages do you recommend for spreadsheet and plot?
Plots.jl is the old and mature plotting library, but it needs a bit of time to load (the first plot is slow). While Makie.jl is still young, it can be precompiled, therefore its time-to-first-plot is smaller. I used Plots a lot (but like a year ago) and I really liked it. Also I think that julia is ready for your use case as most of the "main" packages support v1.x.
well, I'm using JuliaDB, Gadfly, Plots, and DataFrames, but.... I didn't use Pandas or ggPlot. So... I'm not in the best position to compare the 2. I can say that for my use cases, it is working just peachy, and that for some extremely large datasets, it is a god sent. However, I'm a bit of an oddball, as I didn't come here from Python and R.
So, Mesos?
Plots.jl for simple things, Makie.jl for more fancy things. They are both very functional.
\&gt;How about basic plots? Is it easy to create basic plots with julia? Yes, Plots.jl is what you want. If you need more complicated things then Makie.jl will do it for you. \&gt;draw plots on terminal and web browser? So I'm not 100% sure what you mean by draw plots on terminal but there is UnicodePlots.jl which is surprisingly functional and easy to use: [https://github.com/Evizero/UnicodePlots.jl](https://github.com/Evizero/UnicodePlots.jl) Is this what you mean? \&gt;How does it stack up against pandas and ggplot from python? Are you asking performance? It will probably be at least as fast but will definitely have much nicer syntax but that's just my opinion. \&gt; Is julia suitable for a lightweight spreadsheet usage? That's the hardest question you asked. It can certainly do everything a spreadsheet program can do and be faster about it but for most people I know who rely mainly on spreadsheets I would say Julia (or python or anything else) is kinda overkill. But I don't know your exact use-case.
&gt;A stupid-simple way to deploy Python scripts on cloud, a cluster, or on a friend's machine. Yeah, great question. You can read more about our solution at [galileoapp.io](https://galileoapp.io). In short, yes there are a variety of ways to solve this problem. But none of them are as easy, or as general-purpose as Galileo. If you're interested in being an Alpha user, I'd love to send you the app!
It's along the lines of Kubernetes in that we use containers to send compute jobs (specifically, we have Docker running on the backend). Our focus right now is on researchers (typically academic researchers), since they have the clearest need for it right now. But the tool is actually general-purpose. So you could install Galileo on an HPC cluster to make it easy to send jobs there, or you use Galileo to instantly access cloud instances or friendly machines. If you're interested in being an Alpha user, I'd love to send you the app!
[`SDL.jl`](https://github.com/jonathanBieler/SimpleDirectMediaLayer.jl) might be what you're looking for.
vegalite looks also good for spontaneous basic plotting, just pipe your dataset to it using the queryverse packages [https://github.com/fredo-dedup/VegaLite.jl](https://github.com/fredo-dedup/VegaLite.jl) the piping philosophy greatly increases usability and makes data manipulation in spreadsheet programs easily replaceble. on the queryverse for data science: [https://www.youtube.com/watch?v=2oXSA2w-p28](https://www.youtube.com/watch?v=2oXSA2w-p28) However, using languages is never as interactive but also more reproducable than spreadsheet progs, so julia is only in part a replacement. for quick and dirty clearly one off data manipulation on small data, spreadsheet, albeit i prefer to avoid it to have a valuable worklearning experience, is unbeatable. use the right tool for the job is what i think. thinking of fast free office, i like the chinese wps office spreadsheet program, which gives to me a more snappy experience than libre office calc. [http://wps-community.org/downloads](http://wps-community.org/downloads) (i guess your on linux)
Let me get back to you on this as I love to try new things.
&gt; So I'm not 100% sure what you mean by draw plots on terminal but there is UnicodePlots.jl which is surprisingly functional and easy to use: https://github.com/Evizero/UnicodePlots.jl &gt; Is this what you mean? Yes &gt; I don't know your exact use-case. My use case is comparing prices of items and converting currencies for comparison when I buy something. When I build a machine, I buy parts from different continents which use different currencies.
In that case I'd say yes, Julia sounds like a good option for you.
You can also try Gtk.jl, Makie.jl, Nuklear.jl, Blink.jl, ... Which is best depends a bit on what you want to do exactly.
That seems awesome! Does anyone know if it works nicely in vscode? 
I will test that at work today and report back.
This is amazing! So much progress and so many cool tools, fantastic work Julia team!
I'm currently at work baby I got my head over a debugging issue right now. This is so desperately needed.
From the article: &gt; If you have a different favorite editor than Atom—or sometimes work in remote sessions through a console interface—you can alternatively perform debugging via the REPL. There are two REPL interfaces: Debugger offers a “step, next, continue” interface similar to debuggers like gdb, whereas Rebugger aims to provide a console interface that is reminiscent of an IDE. So - I’m hoping the answer is yes? I’m going to try it out over the weekend. 
u/jeffdujon
Not yet. [https://discourse.julialang.org/t/debugging-julia/22147/22](https://discourse.julialang.org/t/debugging-julia/22147/22?u=cossio)
Fantastic news! So happy for Julia.
Can you post what you are executing to get that error?
 Pkg.add("DataFrame") 
Strange that folder should exists. Maybe it failed to initialize at some point. The content of the General should be this I think: https://github.com/JuliaRegistries/General You can download it. 
Using that resulted in the following error: ERROR: The following package names could not be resolved: * DataFrame (not found in project, manifest or registry) Please specify by known `name=uuid`.
Seems like your whole package system is messed up, I would rename your .julia folder to .julia.old, restart Julia and do `]status`, maybe it will fix things.
Ok, may do a full re-install with 1.0.3
I think you are missing an ‘s’ Try Pkg.add “DataFrames”
0.19 is still beta. You can install with the following: ] add JuMP#v0.19-beta2 (and then hit backspace to exit the package manager mode)
What version of Julia are you running? JuMP v0.19 runs on Julia v0.7 and higher, so you might encounter this issue if you have Julia v0.6. JuMP v0.19 is now stable and no longer in beta, if I am not mistaken. &amp;#x200B;
julia&gt; VERSION v"1.0.3"
Seems like it still uses an older version of JuMP, JuMP v0.18.5+ #7c115a2
Actually i just uninstalled JuMP and did a Pkg.add("JuMP") and was able to get v0.19. I'm not that familiar with Julia's package manager. Maybe try uninstall first? Or maybe Pkg.update()?
Not necessary, but harmless, I expect. For any non-existent package, you'll see: (v1.1) pkg&gt; add EnderFuckingWiggin ERROR: The following package names could not be resolved: * EnderFuckingWiggin (not found in project, manifest or registry) Please specify by known `name=uuid`. 
I would try Pkg.update(), I am not sure why it defaults to an older version of Julia.
Perhaps you can try getting help from https://discourse.julialang.org/. Majority of the julia community hang out there. 
This is great! I relied on Rstudio's debugger often, so this was the one thing I've been missing.
Guess i will have to use REPL interface for now then. Really love the progress they are making though. 
You can precompile Makie.jl. It's not as mature as Plots (yet) but there's a plenty of examples.
Plots.jl runs just about as fast for me using GR as MATLAB
So I’m somewhat inexperienced, but the issue I’m running into is that the Control Systems package uses Plots to do its plotting internally. I can change my plotting backend within Plots, but I am lost on using something like Makie instead.
I precompiled Plots. Using packagecompiler.jl
Hi Big-Nash: I was wondering if you could tell me any good tutorial on julia econometrics. I want to use Julia for my Wooldridge econometrics. My professor uses R and R seems excellent. But I am also trying do the same estimation in Julia. Could you please give me any link or text (like economertics with Julia) or tutorial site. Thanks for help.
Julia is not OOP. Python is already a replacement.
Because you can overload getattr, you can get the dot syntax relatively cheaply. Dynamic dispatch is a subset of Julia's multiple dispatch. Inheritance is possible. Interfaces are also possible. Julia is OOP enough for most things. 
Does it speeds things up a lot ?
Yeah. It sued to take me 3 seconds to start DataFrames. After compilation it starts instantaneously. The process is still quite brittle at the moment and I can't seem to reliably pre-compile many packages together like this. But where I can it works beautifully.
how is that relevant?
Python is one replacement, Julia another. Having experience with both, I vastly prefer Julia; not just for speed, but for its central programming paradigm. Multiple dispatch is head, shoulders, hips and ankles above single dispatch OOP, imho.
Did a small update to the blog yesterday if you're interested. More reasonable code and some extra visualization.
IMHO, typical OOP abstractions do not suitable for scientific computing. Screw microsoft for popularizing it. For sure it is better than just fortran 77, but fact is fact - advanced OOP is horrible for scientific computing. It is much better to achieve polymorphism and encapsulation with different abstraction models. p.s. and by the way, python is a not good OOP example as well. Look no further than smalltalk heritage, e.g., ruby
Wrong
I'm a huge Julia fan. Switched nearly completely from R. But the plotting ecosystem is still very... Nascent. Frankly, I do most of my plotting in R, still. I would kill for a drop in lattice replacement, though I have hopes for Makie. I do not like the pyplots style interfaces plots.jl apes. Personal preference, of course.
I see you reply very constructively... probably I should not engage, but I cannot stop procrastinating at the moment. Browsing some of your reply to other questions, well you seems to compare OOP with "procedural" approach, and claim that OOP is much more productive! hehe, well you are right, it is. So if you would compare, say C++ to say C, yes, C++ is more productive! win win. But there are other merits and approaches. In scientific computing there are two different communities (well in very simplified terms, there are more, and it is a spectrum, blabla bla bla). a) people who cares about performance. Better yet, parallel performance b) people who cares about productivity. People from crowd (a) use C or fortran (and openMP, MPI, cuda, etc). they optimize cache to death, and they target specific machines with million cores (see at for example PETSc devs). For them advanced OOP usually a sacrifice of performance (e.g. inheritance). There are still people who do it..., but that is because they want to not suffer! haha. People from group (b) care about productivity and they use python, etc. Well, for that matter, there are much more productive languages, and they are not popular because of many reasons. Examples: Ocaml, Haskell, Common lisp, etc. For example 100 lines of haskell is like 1000 lines of your-typical "productive" C++. The problem is that you can teach a monky to use Java and smart monkey to use C++, but it will not work with more advanced languages. And there is a third group! they want to combine performance and productivity - oh boy, it is hard... Example - julia. They try to not sacrifice performance, and still be productive... the main problem of julia (in my opineon) is the not so great (fast/usable) distributed memory parallelization model. Even shared memory parallelization is not as polished. There are other projects - for example chapel! they do very well with everything in my opineon and they target more hardcore HPC/ million cores guys, but they lack adoption... &amp;#x200B; Overall, OOP is not "bad" (and it is very cool for some problems, like writing a gui widget library) and it is defiantly better in terms of productivity than "C" procedural approach, but there are much better abstraction models (ocaml functors, or haskell typeclasses, or racket crazy macro system where you can do ANYTHING, etc.) However, too many bad programmers learned how to use it, and use it every where - where needed and where not needed. And they use not even good version of OOP (java like OOP). So I think it is very bad think that OOP became too popular, making more beautiful parts of computing less visible.
Well, I was mobile and couldn't adequately respond. It's no more relevant than the original post really. Saying something is a replacement for something else is kind of silly. Python is closer to MATLAB and it's free, that's all. Outside of Simulink Python does everything, it's just not a compiled language. Either way, I would never use a non-OOP language if I didn't have to--personally.
 OK you’re a good sport. I was mobile so I couldn’t adequately answer. I’m just going to reply to everything here. IWhen I originally started programming it was all procedural (I’m a EE who learned C first). After I spent some time with some good object oriented programmers, I almost never want to do anything procedural—it’s almost always more natural to have objects. Noone should ever use Fortran or C if they have C++ for performance. The only exception is in environments with microcontrollers with limited ram or something like that. Usually, the programs are simple enough not to need OOP (though classes are just natrural). If you get any speed up in C or Fortran it is seriously not worth the developer time. Using CUDA and other things like that for raw number crunching are in their own category and you use them when you seriously need to. IMHO it’s not really a question of whether or not of how you think. People who don’t think in OOP just lack the experience. In most cases, it’s just better as someone who’s done plenty of both. As you saw in my other posts I was really curious about the core uses of GO/Julia. Most of the people just “don’t like OOP” it’s really because they just don’t understand it in most cases. One guy on my other posts gave me a really good idea about what they are used for. It is for performance when you don’t want to use C (or C++). Really the reason they are chosen over C++ is just that they are simpler. C++ is huge and complex, not easily portable, not self-memory managed etc. Go/Julia are simple to learn, fast and when you use them on a DB or server, you’re just doing simple procedural stuff to data in many cases probably so it just makes sense, because why would you use C? You just wouldn’t if you care about developer time. We still use C because it’s lightweight and fast and there are special applications where you still should. We still use C++/C because of the infrastructure and it’s long term maturity. Everything (like Linux, many embedded systems.) is already written in C/C++ so it stays around. I am still young and I likely will never see a C++ replacement. By the way, using vanilla C++ is pretty easy. In terms of data science/scientific computing, trust me when I say – OOP makes sense in many cases but that was a very general comment of yours. Basically, OOP with a few simple levels of inheritance/objects are all you need in most cases and it’s really not that complicated. Example: look at pandas dataframes in Python. They know how to plot,display,serialize,output themselves to different formats, and when you look at one in a console there is a lot more to their implementation in terms of their efficiency in memory versus what you actually see, which is just a table with rows and columns. I have seen HPC with thousands of cores too. They run C++, Fortran, Python, Perl, Java. All of the code runs, but I understand why Julia and Go exist. BTW, you’re English was good.
&gt; I would never use a non-OOP language i don't want to ruin your day, but oop is kinda not cool anymore. the world moved forward.
The point is, I believe, that Julia is now a viable replacement for Matlab, just like Python is. The main 'weak point' is plotting, which has still not stabilized, and is (imho) not up to Matlab's standard in that regard. I certainly disagree that it is silly to say that something can be a replacement for something else. What do you even mean by that? It's especially useful to suggest a free and open-source replacement for an entrenched, expensive language, that many students are 'stuck with'. As for insisting on sticking with singular dispatch OOP, that's up to you. You are missing out.
You can't move on if you never learned it.
Multiple dispatch is kind of cool. It's not a reason to pick a language imo. Yeah I see what you are saying. You meant Julia is a capable alternative (not replacement). Sorry if it seems I was being mean at all. 
As for Python being closer to Matlab, that depends. When it comes to linear algebra and array handling (which is pretty much the core of Matlab), then Julia is a lot closer than Python, fortunately. Syntax-wise, I find Julia closer than Python in general, but there are areas where Julia is more different (notably, the class-based OOP stuff.)
Oh, I disagree! Multiple dispatch is pretty much the *main* reason to pick this language, with performance in second place.
All you're doing is getting rid of if statements right? In languages like Python or Ruby you can just pass anything to any argument
i don't know what are you referring to here, but i grew up on it. i thought it is The Truth.
hmm, so are you talking about HPC/scientific computing or about programming in general? those are kinda different things... &amp;#x200B; "IWhen I originally started programming it was all procedural (I’m a EE who learned C first). After I spent some time with some good object oriented programmers, I almost never want to do anything procedural—it’s almost always more natural to have objects." again, you are comparing OOP to procedural approach. I admit, OOP almost always more expressive, unless you know some tricks. OOP is not the only way to get abstraction, and admittedly not the best. Sometimes algebraic datatypes are best to manage complexity, sometimes monad transformers, sometimes modular functors, sometimes continuations, sometimes metaprogramming (I am talking about real metaprogramming - not C++ templates, which is awful implementation of a good idea). The list goes on and on... The problem is there are so many more beautiful waves to solve specific problem and most programmers do not know them. But they are there, scattered in other advanced languages or in scientific computing papers. OOP is over popularized, so it is forced everywhere. For example, racket is one of the most expressive languages, and it has OOP with classes and all. But no one cares, because language is so expressive, you can write your own "OOP" in racket in like 100/1000 lines of code! Or more efficiently, racket programmers create abstractions which are specifically targeted for problem itself, making it so expressive for specific problem, it is ridiculous... but such advanced programming is rarely used in scientific computing crowd... with some exceptions: for example FFTW is written in OCaml, where they do very sophisticated C code generation for specific machine, so this FFTW is fastest ever DFT... Good luck to write something like this in C++ - almost impossible, unless you someone like Donald Knuth. The point is most programmers are often ignorant that OOP is the only way to abstraction. &amp;#x200B; About ten/hundred thousands of cores (I do it on my current job): it is complicated, and python and julia is almost always a bad choice. &amp;#x200B; about C++ - it is ugly mess :) do not get me started... only templates can be criticized for hours... how else you can take a beautiful idea of parametric polymorphism which was well used and beautifully implemented in ML family languages around 1975!!!!! (yes, that long ago) and make it so ugly and error prone! did not C++ crowd leaned nothing from C macroses bugs? ugh &amp;#x200B; C is more elegant and small, yes it is not a high level language like C++, but if you choose it, you know you chose low level languages. and if you know how, you can do magic. &amp;#x200B; about C++ in scientific computing - in most cases it is just "better C" no templates and no inheritance etc. And large scale scientific computing codes, usually do not need inheritance anyway and it also makes things slower for something you do not need (and templates are too scary to use). &amp;#x200B; about julia: it is not general purpose (in my opinion). the focus on scientific computing, so they do not have inheritance to be able to optimize memory layouts more efficiently etc etc etc (they whole forums discussing why, smart people who knows what OOP is and much more about compilers). However it has other advanced features. multiple dispatch you probably heared of... and of course julia is reflective almost like lisp (well not quite), so you can add language features you miss (e.g., in common lisp, whole OOP is implemented via macroses, and its OOP system is considered as one of most sophisticated OOP systems out there... google CLOS if you are interested, you can do such magic with classes and objects, no java/C++/C# ever dreamed of. for example for mixins in C++ you would need to use templates... any way). for other things, julia has lexically scoped closures.. sure they are in python as well, but in python you would need to pay for this with slowness... So julia is not as simple as it seems, just because it does not have proper OOP, and most reason that the compromises julia made is optimal for HPC (not general programming mind you). The only thing I do not satisfied is - distributed memory paralelism, so you kinds stuck with MPI even in julia &amp;#x200B;
Well, no. But, even if it were only that, I'd be all for it. I am willing to bet that I have spent 30% of all my time in Matlab writing "if second input is a double, then if the third is a vector, do this, else if the third is blah." (Actually, this also goes partly for Python) Also, it is infeasible to go in and edit someone else's code (or even my own) to add behaviour for new types (or just different old types). The ability to add external methods is amazing, and has removed so much boilerplate code.
That's cool. 
Yes quite a bit...
What in R did you use? 
Delete the folder " C:\\\\Users\\\\MY\_NAME\\\\.julia\\\\registries\\\\ General\\\\" restart Julia and run \`\]up\`. It happened to me as well. I think the registries are corrupted. But you just need to delete it and start again.
&gt;pes for Makie. I do not like the pyplots style interfaces plots.jl apes. Personal preference Have you tried vegalite.jl?
Yay thank you!
cjsandy83: "I would never use a non-OOP language" also cjsandy83: "Multiple dispatch is kind of cool. It's not a reason to pick a language imo"
I always moderately preferred lattice (e.g., to ggplot), though I've used both base, lattice, and ggplot a lot over the years, and liked all of them well enough.
It's on my radar, and I \*am\* meaning to try it (haven't yet, though.)
I can't get this to download on 1.03. Anyone having similar issues?
Why?
It's art. Some people like it and others don't.
&gt; For example, unicode in the scripts, that's odd. Just... don't use it then? &gt; Also, working with strings is painful! Like what? Due to the fact that it is UTF-8? 
so the question is, why do you not see how λ∈A is nicer than in(lambda, A) #or whatever the syntax is ? because if this is the question, i have no clue how to answer that. who else than you can tell you why do you think something? introspection helps, probably.
Julia has it's origin in scientific computation, and mathematicians and scientists simply recognize math symbols and greek letters faster than reading a variable name. And they also know from muscle memory the latex code, so typing them in any julia editor would not take any extra time. It's a particular domain in which having self-documented and concise variable names just happens to be using unicode and making it the same as any scientific paper in latex. I particularly never use any unicode variable or function name unless it's a well known mathematical notation. I only use basic string processing so I didn't reach any wall, but I feel that if you can be more specific about your troubles (especially in the julia discourse that has many core developers, or maybe a github issue), you can help them expand the use cases of the string library.
because you don't want to
See also julia-intellij that takes advantage of ASTInterpreter2 to achieve debugging.
This probably doesn’t help you much but I had a quite easy time getting plots to compile by just following the example linked as „instructions“ on the github page. It’s for CairoMakie but just using :Plots,:GR in the command worked for me. It’s basically just compile_incremental(:Plots,:GR) and that already worked for me without issue so that’s probably what you also tried. Works for me on Linux 
On windows 10 i had to do compile_incremental(:Plots,:GR,force=false) and then locate the compiled sys.dll and manually copy to where the sys.dll was
I have just tried it again and it worked on my computer. Depending on where you installed julia, there might be a permission problem overwriting julia system image. Maybe try not to force with `force=false`. IIRC, it should give you a command to run julia with the newly created system image at the end of compilation process. You might also want to look at [Fezzik.jl](https://discourse.julialang.org/t/ann-fezzik-jl-brute-force-sysimg-building-for-julia/21424) which does not use package tests to find which methods to compile but uses your workflow instead. 
It's useful to know that it's working on other machines and not an isue with the overall package. The offending error is [1] pipeline_error at ./process.jl:705 [inlined] It looks like there is an open bug on this, so perhaps I'll just have to wait for it to be resolved!
What is your version? There's a package compile cache in newer versions.
You can install juno without relying on the julia-pro bundle, an use it with your current Julia install. You should look for the proper way to do this on the github readme of Juno.jl.
Overall how is your transition from MATLAB to Julia going? I am debating trying it - I'm working away in R2018b at the moment... I have a lot of existing code in MATLAB and making such a transition is a big jump.
When you are using JuliaPro, you need to authenticate once before downloading packages with a JuliaPro login. When you try installing a package, a sign on tab should open in Juno for you. 
I actually tried to do this last night, and it failed in atom when installing the julia client. The setup at my job is a nightmare, I had similar issues with importing python packages.
Yup and I did log in when I was trying to make it work with JuliaPro. I was receiving the error after authenticating.
Not exactly what you asked for, but plotting directly from GR.jl brings almost no overhead. Not so straightforward with styling, but I use it for quick plotting where format isn't vital. 
That's super useful, thanks! I think that's a good approach -- use GR for fast simple plotting, then use the full Plots package when I want to do something fancier. As an aside, do you know how to permanently set the GR background ("figure") color to white instead of transparent? I'm using Juno, and with the dark plots panel it's somewhat hard to see the plot when it's transparent with the dark blue background.
It's going well! I've been a Matlab user for probably 5 years, and am wanting to switch for philosophical reasons (namely the open-source-ness). Occasionally I'm frustrated with aspects of Julia, but I think that the payoff will be worth it. I think that if I grind through 6 months or so with Julia I'll be as comfortable with it as I am with Matlab. Plus, I think that the language is very elegant, and will force me to think more like a computer scientist -- I feel like Matlab makes it easy to be a bad programmer!
Glad it worked for you! I also don't know how to deal with this, before it had per default a white background. :/
Also, maybe is worth asking in the Discourse forum of Julia, usually I get very friend and supportive feedback from the community also there! 
As someone who is very new with both Juno and Julia, i have no clue how i get this to work. Can someone enlighten me?
If your code is something like calculating eigenvalues of a large matrix, then you are basically just calling out to to an optimized C or Fortran library. Julia isn't going to be any faster than that, and is in fact itself calling out to a similar library. (For eigenvalues of *small* matrices, or matrices with structure, like diagonal etc. Julia can be *extremely fast.*) If you are just calling canned C-code, you have little *speed* to gain from Julia. The advantage starts coming out when you are doing things repeatedly, and for example using in-place operations, or when you want to implement your own algorithms with custom data structures. But aside from performance, Julia is just delightful to work with, and multiple dispatch is *amazingly* expressive. I think that is at least as important as raw speed.
I wouldn't say Julia makes only sense if your program is heavily iterative, especially considering it's use in HPC and long running processes in general. Though I'd say being heavily iterative is just the correct way to develop in dynamic languages in general, after all you're trading the type safety for something. Julia focuses on solving the "two language issue", in which you use a high level language for productivity and interactivity (as glue logic) and a low level language for the actual processing. I'd say the immediate advantages to that is: 1. You don't have to learn another language to understand and modify the performatic code, and you don't need to deal with stuff like the FFI or working with multiple environments in parallel like mypy/numba that can't support all the language. Debugging is much easier as well. 2. Code in Julia integrates well with code in Julia. For example you can auto-differentiate code in Julia directly, but Python cannot do that with libraries that are written in C/C++ like numpy, pytorch and tensorflow. Also you can make different libraries play together much easily, like flux + diffeq, and adding unplanned functionality by creating custom types like lightgraph, StaticArrays and CuArrays. 3. The high cost-benefit of adding new functionality without compromising performance allows you to experiment much more with the algorithms. If your library didn't add a functionality, you can just write that functionality directly in a normal function (especially easy thanks to Julia multiple-dispatch and duck typing) and not worry about it being slower. Would you even know where to start adding some new functionality to numpy or a new model for scikit-learn in a performance sensitive way? Though you can consider it narrow since it favors package developers and more advanced users since most users will never really need to do more than those libraries already provide. But I do enjoy the freedom to make my tools adapted to me instead of having to adapt to the tools. 
Julia's a good general-purpose language as well. Not just for scientific computing.
I'd say that Julia is in a worse condition than most general purpose computing languages. The main issues I've encountered are very limited standard library (why is there no stack nor queue nor linkedlist?), experimental threading api, lack of local constants (like final ... in Java or val in Scala), lack of built in immutable collections and tools (Juno, debugger) which still feel rather immature. The language itself is almost fine, but the tools and standard library could definitely see some more love. Furthermore, I think there should be more marketing towards the "general purpose" programmers, so that a community can be established. Anyway, I really hope that the problems I mentioned will be resolved in the next few years. It would be a shame not to utilize Julia to its fullest potential.
What's wrong with the stack and queue and linkedlist from here: [https://github.com/JuliaCollections/DataStructures.jl](https://github.com/JuliaCollections/DataStructures.jl) ?
There are several things: 1. They're not built in, which is a very minor issue, but personally I prefer a batteries included approach. 2. Their methods mutate the passed collection, which isn't necessary for an efficient implementation. 3. I find the docs laconic, which again might not be an issue for everyone.
I am not really familiar with the code, since I don't use those types of data structures, so I cannot comment about mutation (but in general, mutation is more efficient). But I can say that being built-in is not a requirement, and it will probably never be built-in. In many languages you need things to be built-in to have good performance, but that is not the case with Julia. External or home-made is every bit as performant as built-in. In fact it is a deliberate decision to *remove* things that were previously built-in, in order to slim down the language.
Well, most popular general purpose languages are popular because they are already mature for use, so it makes sense they are currently in better condition. Especially since Julia indeed spent it's pre-1.0 phase (which was less than a year ago) focusing on filling a very particular niche which gave it some leverage to now try expanding (it's hard to start a popular new language from basically nothing and without even support from major tech players or having a mature library infrastructure like JVM and BEAM as a starting point - though it did have the LLVM and C/R/Python FFI). I do think it's mostly a documentation and discoverability problem (besides maturity). One of Julia main strengths is that custom types are just as good as built-in types (since those are just Julia as well), so it does make sense to let the community build and maintain the optimal structures for each purpose. For example, if you're adding stacks and queues, what about Dataframes? JuliaDB? StaticArrays? They are arguably more frequently used than stacks, queues and linked list (especially since arrays already cover a lot with push!, pop!, prepend!, append!). And then there are LazyArrays, CuArrays, OffsetArrays for those who can't deal with 1-indexed arrays... It could very well bloat the language, especially one with strong focus on extensibility.
I fully agree that maturity will resolve many issues almost on its own. On the other hand - I don't really think that extending the standard library bloats the language. On the contrary, I'd say that having a huge (but well designed) standard library allows programmers to create code that is easier to understand and maintain, because that allows you to create idiomatic, reusable code. Otherwise there's a risk that different people will use non-compatible implementations of data structures. It's literally the Lisp curse coming back to claim Julia.
I definitely can't deny Julia is vulnerable to the Lisp curse to some extend, it's the risk involved in any powerful language. It's much easier for anyone to create alone a competing array/dataframe/... in Julia, so the community may never join together in one main implementation like python with pandas and R with tidyverse. Even if they can all play well together, none will ever reach maturity and become as feature complete as it could. But I think the community is aware and is organizing stuff to make it easy to concentrate efforts, for example the group focused on Arrays: https://github.com/JuliaArrays/ And many domains (scientific domains though) we see a winner library (DifferentialEquations, JuMP, Queryverse) or at least a small number (Flux and Knet). What is needed is probably some kind of reference book/material (not the manual, since that is about the core language) that works as a guideline of the "right tools" and "right way" for any common task, that could be updated when a competing library actually becomes the new standard. 
&gt; 1. They're not built in, which is a very minor issue, but personally I prefer a batteries included approach. Given how subjective that is, that feels like an unfair criticism. Just as many people would argue against having a large standard library that's fully batteries included. I think Rust actually takes the best approach here: sensible sized standard library, and then separate but "blessed" packages that function as the standard. Julia seems to organically have something similar. Even cooler is that I've never seen a language where cross-package functionality and awareness is so fantastic. &gt; 2. Their methods mutate the passed collection, which isn't necessary for an efficient implementation. Julia has a strong focus on performance, having them mutate makes a lot of sense. At least unlike python, the mutation methods feel a bit cleaner and more sensible: do you know how many times I've been bitten by Python's "does this mutate or return a new copy? Who knows, let's just run it and find out if your variable went to null!" The constructor functions are usually exceedingly straightforward to use, so if you did want a non-mutating version, that should be quite straightforward to implement.
Out of curiosity, what would be the utility of non-mutating push and pop? Aren't stacks and queues inherently and necessarily mutable data structures? 
Using immutable data structures allows you to save a lot of space in certain applications. You would need a different set of functions with a little bit different semantics, but the utility would be similiar to their mutable counterparts with the ability to share the data freely between functions/threads/states/whatever. If you find yourself making copies of structures to make sure your program works correctly, then you would certainly benefit from immutability.
My criticism surely is subjective, but I can't see it being a disadvantage. A programming language is a tool and, as an user, I'd like to be able to use it in the most efficient way. To address your point on performance, I'd like to point out that in general purpose programming you don't always focus on performance. And since Julia is already performance oriented, it wouldn't be silly to write a piece of code in a way which yields worse performance, but is much easier to reason about and much less error-prone. If performance even becomes an issue, it would be easy to use a mutating version. And honestly, I think that it's much easier to fix performance by scaling a more fp-oriented code than by replacing immutable structures with mutable ones. But that for sure would depend on implemented algorithms and hardware. I find your comparison to Python unjustified. There may be some exceptions (the standard library is huge...), but I've found out that whereas most functions in Python make a copy, most methods modify the object. In a similiar way, Julia has a convention that uses "!" suffix for input-mutating methods. Therefore, the solution for this artificial problem has already been there for quite some time.
There is ```setcolorscheme(t::Int)``` I think
About the immutable structures you could check: https://github.com/JuliaCollections/FunctionalCollections.jl Though even the README makes it clear that there is still a lot of work required.
I'll take a look at it, thanks :-)
Depends on what kind of TDA you need (cubical, Rips, Cech, alpha, witness, etc). For Rips, there exist imo two public competitive systems: Gudhi and Ripser (both C++), and possibly eirene (depending on your notion of "competitive" and desired polish; eirene is certainly very innovative and written in pure julia). Gudhi is a giant template library with lots of features, but best used if you are proficient in C++. Ripser is a very lean command-line tool, and often considered king-of-the-hill in terms of raw speed, memory consumption and usability. I am aware of two public julia wrappers for Ripser and none for gudhi. These are https://github.com/mtsch/Ripser.jl and https://github.com/bbrehm/Sparips.jl. Sparips is actually something very different (an approximating preprocessor), but also contains a wrapper for Ripser (you can simply set the approximation factor to 1.0 in order to compute precise diagrams). Then there is the more general and probably more polished but uncompetetive https://github.com/wildart/TDA.jl. I think that will often be slower by 1000x than Ripser, but it has cool features that the wrappers for Ripser lack (integral homology, CW complexes, ...). Aside from that, there are javaplex, dionosys, etc. These projects have no julia wrappers and are uncompetitive with respect to speed, but have certain features you might need (e.g. zig-zag filtrations). If your problems are &lt;200 points, and 1-dimensional homology is enough for you, then https://github.com/wildart/TDA.jl is probably the most polished tool. Up to 1k points / 1-dim homology, https://github.com/mtsch/Ripser.jl is probably most accessible. For larger data than 10k points and/or 2-dimensional homology, you should try https://github.com/bbrehm/Sparips.jl (which computes approximate persistence diagrams). Take the thresholds with several grains of salt: I know people who routinely use unaided Ripser with 10k points, and don't know precisely what sizes and dimensions https://github.com/wildart/TDA.jl can handle (maybe you should just ask wildart). If you are interested in 0-dimensional barcodes, then any tool will work to any reasonable size of data (this can be solved via [union-find](https://en.wikipedia.org/wiki/Disjoint-set_data_structure); consider saving time by coding your own solver in an afternoon, instead of spending more time integrating a library into your project). 
**Disjoint-set data structure** In computer science, a disjoint-set data structure (also called a union–find data structure or merge–find set) is a data structure that tracks a set of elements partitioned into a number of disjoint (non-overlapping) subsets. It provides near-constant-time operations (bounded by the inverse Ackermann function) to add new sets, to merge existing sets, and to determine whether elements are in the same set. In addition to many other uses (see the Applications section), disjoint-sets play a key role in Kruskal's algorithm for finding the minimum spanning tree of a graph. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/Julia/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
But a stack that cannot be mutated? How is that even stack? It's also hard to see how you can save space by copying instead of mutation. I am vaguely familiar with concepts from functional programming, but my understanding is that performance and memory use are considered an Achilles heel.
Imagine a stack implemented using a linked list. You can use head to get the first element and tail to get all elements except for the first one. Then you can use push to insert an into the beginning of the stack. So all you have to do is to replace a mutating function call with an assignment operation - your stack/list is a value returned by a function. That's not much different from current approach, but there are some advantages. If everything is immutable, you don't really have to copy anything. It's safe to share suffixes between lists, because a suffix is an immutable list on its own. So instead of making copies, you simply treat everything as a value. In some scenarios, this approach will be ineffective. That's fine - mutable collections can be used instead. I'm not a zealot of functional programming. I don't want to replace every mutable structure with an immutable one. I just think that mutable structures should in general be discouraged in general purpose programming. There is a reason why more and more functional features are included in mainstream languages such as C#, C++ and Java. There are reasons why people use Scala, F# or Clojure. Julia could definitely shine between them as a simple and powerful language. 
I'd say if you copy the structure a lot (including distributed scenarios), immutable is better since you don't need a copy of the structure for each reference, you can have every variable pointing to the same position in memory without worry. If you mutate a little, it can still be optimized in memory by the compiler (for example keeping just the diff). If you mutate the data a lot and you don't share it then mutable is certainly better (immutable is still somewhat easier to reason and test in my opinion, but I can't imagine having some multi-gigabytes dataframe having to make copies at each operation). Julia itself has structs immutable by default, and it usually makes sense. For example, creating a new complex number is very fast, so there is no real benefit in changing each field separately, you can just create a new number in every operation. Most common operators (+,*,map,...) also doesn't mutate and even though they are everywhere in the language they are very fast. Something I hope they eventually add in Julia is a method to facilitate working with a little larger immutable structs, for example a syntax sugar to create a new struct changing only one field, though that might encourage exposing the internals of your struct instead of creating interfaces through methods.
&gt;Would you even know where to start adding some new functionality to numpy or a new model for scikit-learn in a performance sensitive way? This is a good point. The amount of time I've spent wrangling with cython to get my code to work alone probably justifies working with Julia more. I guess most of my typical use cases until recently are suited just fine for the existing python libraries. &amp;#x200B;
[A non-affiliate link to Amazon.](https://www.amazon.com/Julia-Programming-Operations-Research-Changhyun/dp/1798205475/)
I mean, is this book actually good?
I'll check these out. Thanks a bunch!
I have the first ed. Not sure I learned enough about Juila or OR from it. Table of Contents Frontmatter Introduction and Installation Simple Linear Optimization Basics of the Julia Language Selected Topics in Numerical Methods The Simplex Method Network Optimization Problems General Optimization Problems Monte Carlo Methods Lagrangian Relaxation. Parameters in Optimization Solvers Useful and Related Packages
I've linked a Jupyter notebook where I show some strange behavior between `sin`, `sinpi` and `sind`. For instance ```julia sin(π) = 1.2246467991473532e-16 sinpi(1) = 0.0 sin_d(π) = 0.0 sin(2π) = -2.4492935982947064e-16 sinpi(2) = 0.0 sin_d(2π) = 0.0 ``` where `sin_d(x) = sind(x*180/pi)`. Does anyone know why `sinpi` and `sind` return 0 and `sin` does not? `cos` behaves similarly but shifted by a factor of pi/2.
And yet the kindle is 3x the amount of a physical copy
I'm not sufficiently familiar with Julia to know if this answer applies, but the following is generally true of IEEE-754 arithmetic. When you calculate `sin(pi)` you are computing the `sin` of a floating-point approximation of pi. In other words, what you are feeding into `sin` is not exactly pi but an approximation to it. As a result, the value you get out is not `0`, because you are not taking the sin of pi, you are taking the sin of an approximation to it. I suspect that when you compute `rad2deg(pi)`, you are getting exactly 180 as the output. `sind(180)` is exactly 0, matching the mathematical operator.
I suppose my thinking is why implement it like this? If you redefine `sin(x)` to use `sind(rad2deg(x))` then you get the desired `sin(pi) = 0`. My only thought is that `sin(x)` is optimized for speed. If this is the case I would be curious to see if the speed up is great enough to justify the odd behavior.
I’m sure sin(x) is optimized for speed, and roundoff errors are a fact of life so I can see it not bothering some people. However, I can’t recall having seen this in other languages (but I don’t know). It’s possible other languages check if sin(x) is “supposed” to be zero (i.e. if the argument x is an exact multiple of pi to machine precision) and return zero. You might check the behavior in python/numpy and/or python/math. You might also check how sin(x) is implemented in the Julia souce code. Most other languages (e.g. python) probably use the C implementation (which might be a compiler specific implementation). Finally, you might dig into the implementation in gcc for comparison. To me, this would be a fun little mini project, but maybe you’re looking for a quick answer (still glad you asked, though, I’m curious of the answer).
&gt; I would be curious to see if the speed up is great enough to justify the odd behavior. What is odd about the behaviour? Do you mean what is odd about the implementation? The behaviour seems completely sensible to me. As for performance: sin_d(x) = sind(rad2deg(x)) x = 0:1e6 # float range xpi = x .* π # float range xi = 0:10^6 # integer range julia&gt; @btime sin_d.($xpi); 50.994 ms (2 allocations: 7.63 MiB) julia&gt; @btime sinpi.($x); 8.845 ms (2 allocations: 7.63 MiB) julia&gt; @btime sinpi.($xi); 819.865 μs (2 allocations: 7.63 MiB) As you see, the speedup is particularly dramatic for integers, since then you *know* from the type itself that the answer is zero.
This line: julia&gt; A = Array{Array{Float64,4}} Sets A equal to the type, not an array of the type. A::Array{Array{Float64,4},1} Declares A as the type or tests for the type to match if already declared. julia&gt; A = Array{Float64,4}[] Creates an empty array with elements of type Array{Float64,4}.
You forgot a dimension, and also to actually instantiate the data structure. Here's what I've got, and a solution for your case: julia&gt; A = Array{Array{Float64,4}} Array{Array{Float64,4},N} where N julia&gt; typeof(A) UnionAll julia&gt; A = Array{Array{Float64,4}}() ERROR: MethodError: no method matching Array{Array{Float64,4},N} where N() Closest candidates are: Array{Array{Float64,4},N} where N(::UndefInitializer, ::Int64) where T at boot.jl:416 Array{Array{Float64,4},N} where N(::UndefInitializer, ::Int64, ::Int64) where T at boot.jl:417 Array{Array{Float64,4},N} where N(::UndefInitializer, ::Int64, ::Int64, ::Int64) where T at boot.jl:418 ... Stacktrace: [1] top-level scope at none:0 julia&gt; A = Array{Array{Float64,4},1}() 0-element Array{Array{Float64,4},1} julia&gt; push!(A, zeros(1,1,1,1)) 1-element Array{Array{Float64,4},1}: [0.0]
Here: `A = Array{Array{Float64,4}}` You've defined A to be a **Type** of Array of Arrays, rather than an actual Array of Arrays. Hence the error: &gt; ::Type{Array{Array{Float64,4},N} where N} To initialize an array try this instead: `A = Array{Float64,4}[]`
Thanks a lot!
Thank you! Of course it had to be that obvious when you know the answer.
If i extend my array to Array{Float64, 5} how should i the extend the array?
The odd part is you expect the result to be one thing and it is another. It is even more odd when this result is readily obtained using alternate methods that don't wildly impact performance. It is a small issue but what criterion do we use that says this method is accurate enough? There are conceivably even faster methods that produce less precise results. This is an arbitrary level of precision that is set for a slight boost in speed. What is even more baffling is that it seems `sin(x) = sinpi(x/pi)` would be an overall better option. It doesn't suffer a loss in precision and is faster at large `x` ```julia julia&gt; @btime sin.($xpi) 29.368 ms (2 allocations: 7.63 MiB) 1000001-element Array{Float64,1}: 0.0 1.2246467991473532e-16 -2.4492935982947064e-16 ⋮ 1.9892802124160583e-11 1.0164920800596326e-10 -2.231912181360871e-10 julia&gt; @btime sind.(rad2deg.($xpi)) 54.064 ms (2 allocations: 7.63 MiB) 1000001-element Array{Float64,1}: 0.0 0.0 0.0 ⋮ 0.0 0.0 0.0 julia&gt; @btime sinpi.($xpi./pi) 6.112 ms (2 allocations: 7.63 MiB) 1000001-element Array{Float64,1}: 0.0 0.0 0.0 ⋮ 0.0 0.0 0.0 ``` and it yields higher precision at a slight decrease in performance over the range of `[0,2pi]` ```julia julia&gt; x = range(0,2,length=Int(1e6)) 0.0:2.000002000002e-6:2.0 julia&gt; xpi = x.*pi 0.0:6.283191590371176e-6:6.283185307179586 julia&gt; @btime sin.($xpi) 12.649 ms (2 allocations: 7.63 MiB) 1000000-element Array{Float64,1}: 0.0 6.283191590329834e-6 1.2566383180411618e-5 ⋮ -1.2566383181059309e-5 -6.2831915912202336e-6 -2.4492935982947064e-16 julia&gt; @btime sinpi.($xpi./pi) 19.228 ms (2 allocations: 7.63 MiB) 1000000-element Array{Float64,1}: 0.0 6.283191590329834e-6 1.2566383180411618e-5 ⋮ -1.2566383180482417e-5 -6.283191590365234e-6 0.0 julia&gt; @btime sind.(rad2deg.($xpi)) 21.675 ms (2 allocations: 7.63 MiB) 1000000-element Array{Float64,1}: 0.0 6.283191590329835e-6 1.256638318041162e-5 ⋮ -1.2566383181040476e-5 -6.283191591140316e-6 0.0 ```
You can use `hcat` and `vcat`: julia&gt; a = rand(10, 4) 10×4 Array{Float64,2}: 0.646398 0.103303 0.446857 0.758945 0.133645 0.730726 0.222393 0.312995 0.573502 0.976366 0.845645 0.0752874 0.021735 0.880273 0.643184 0.64648 0.00257909 0.587017 0.760047 0.547313 0.139898 0.157461 0.478678 0.952736 0.151423 0.625977 0.186488 0.906813 0.395232 0.408281 0.7073 0.905398 0.176136 0.7557 0.27853 0.616033 0.00920773 0.732319 0.13071 0.810438 julia&gt; hcat(a, rand(10)) 10×5 Array{Float64,2}: 0.646398 0.103303 0.446857 0.758945 0.280022 0.133645 0.730726 0.222393 0.312995 0.420971 0.573502 0.976366 0.845645 0.0752874 0.880808 0.021735 0.880273 0.643184 0.64648 0.938316 0.00257909 0.587017 0.760047 0.547313 0.365564 0.139898 0.157461 0.478678 0.952736 0.091522 0.151423 0.625977 0.186488 0.906813 0.102063 0.395232 0.408281 0.7073 0.905398 0.00427332 0.176136 0.7557 0.27853 0.616033 0.844768 0.00920773 0.732319 0.13071 0.810438 0.0903006
&gt; The odd part is you expect the result of `sin` to be zero at multiples of `pi`. This is intuitively true, but does not apply to floating point arithmetic. When you type `pi`, you are getting an approximation to the mathematical constant pi, and the sin of that approximation is not zero. When you do `sinpi(x/pi)` you are causing a rounding error that effectively rounds numbers close to pi (the mathematical constant) to exactly pi (the mathematical constant), which gives you exactly 0 for numbers near pi, such as `pi` (the floating point value). This rounding error may not always be desirable, hence why it is not implemented by default.
I understand floating point approximation. My point is that Julia carries pi as an irrational. Extended precision is possible which alleviates the issue you are raising.
There doesn't seem to be a method for `sin(::Irrational{:π})` for some reason. If you add one you do get the expected behaviour. Now I'm not sure how useful that would be, realistically. `Irrational` types decay to floats almost everywhere they're used.
``` julia&gt; g(x) = mod2pi(float(x)) == float(pi) ? float(0) : sin(x) g (generic function with 1 method) julia&gt; g(pi) 0.0 ```
I'm pretty surprised that you consider 1-2 *orders of magnitude* a slight boost in speed. I would call that an *insane* speed boost. Also, `sin` needs to have acceptable accuracy across all values, not just close to values of the form `sin(pi*x)`.
&gt;Also, `sin` needs to have acceptable accuracy across all values, not just close to values of the form `sin(pi*x)`. I'm confused by this statement. Do you mean when `x` is an integer?
No, not just for `x` integer. I have not tried to explain this before, so please bear with me. If you are multiplying `x` with `pi`, you are actually multiplying two floats, not the float `x` with the transcendental number `π`. Maybe you expect it to be the closest float to the correct product, but that's not really the case. There are floating point roundoff for both of the numbers as well as for the end product. So, `sinpi` will help you find the sine value that is closer to sin(π\*x) when you interpret π as a Real, transcendental number. But if you then say, well, let's always use `sin(x) = sinpi(x/pi)` instead, for all inputs. In that case `sin` would be second-guessing the input value. Instead of accepting the input as-is, it would be saying, "you probably want the sine of a slightly different number, namely some float multiplied with the transcendental number π. Here, let me help you tweak that number." So whenever you really, actually want the sine of some product of a float and (the transcendental number) π, use `sinpi`, it will be more accurate. But if you want the most accurate sine value of some float, use `sin`.
Any float can be recast in terms of `pi` ```julia julia&gt; sinpi(1.0/float(pi)) == sin(1.0) true ``` My point is that `sin` has issues. I hope this example will illustrate my frustration ```julia julia&gt; api,bpi = float(pi)-eps(float(pi)), float(pi)+eps(float(pi)) (3.1415926535897927, 3.1415926535897936) julia&gt; sin(api),sin(bpi) (5.66553889764798e-16, -3.216245299353273e-16) julia&gt; api,bpi = float(pi)-eps(float(pi)), float(pi)+eps(float(pi)) (3.1415926535897927, 3.1415926535897936) julia&gt; sin(api),sin(pi),sin(bpi) (5.66553889764798e-16, 1.2246467991473532e-16, -3.216245299353273e-16) julia&gt; a,b = 1.0-eps(1.0),1.0+eps(1.0) (0.9999999999999998, 1.0000000000000002) julia&gt; sinpi(a),sinpi(1.0),sinpi(b) (6.975736996017264e-16, 0.0, -6.975736996017264e-16) julia&gt; adeg,bdeg = 180.0-eps(180.0),180.0+eps(180.0) (179.99999999999997, 180.00000000000003) julia&gt; sind(adeg),sind(180.0),sind(bdeg) (4.960524086056721e-16, 0.0, -4.960524086056721e-16) ``` And because we know that `sin` is a symmetrical function the tiny steps away from `pi` (or 180) should sum to zero. ```julia julia&gt; sind(adeg) + sind(bdeg) 0.0 julia&gt; sinpi(a) + sinpi(b) 0.0 julia&gt; sin(api) + sin(bpi) 2.4492935982947064e-16 ``` The above example(s) show that `sin` is performing inherently worse at these points than the other methods.
Maybe, but if you are interested in those points, you should use `sinpi`. The more interesting question is, which function has the best *uniform* error bound.
And, BTW, `abi` and `bpi` are not symmetrical around π (the transcendental number): julia&gt; api, bpi = prevfloat(float(pi)), nextfloat(float(pi)) (3.1415926535897927, 3.1415926535897936) julia&gt; [api, bpi] .- big(π) 2-element Array{BigFloat,1}: -5.665538897647979338920592604638282308209749445923078164062861980294536250318213e-16 3.216245299353272984468460740088280191790250554076921835937138019705463749681787e-16
I agree it is an interesting question. So which one is it and how do we know? 😄
I'm pretty sure `sin` is better: julia&gt; x = π .* rand(10^6); julia&gt; sinerror = abs.(sin.(x) .- sin.(big.(x))); julia&gt; sinpierror = abs.(sinpi.(x/pi) .- sin.(big.(x))); julia&gt; count(sinerror .&lt; sinpierror)/length(sinerror) 0.385337 julia&gt; count(sinerror .&gt; sinpierror)/length(sinerror) 0.012455 So, the ordinary `sin` is more accurate 39% of the time and less accurate 1% of the time. Maximum errors are julia&gt; maximum(sinerror) 8.730185143122814480515249108533383727301230384187443205887407709508142865345659e-17 julia&gt; maximum(sinpierror) 3.047857499752289867437363421227888328447486690171592329297240031305266167004758e-16 So ordinary `sin` has lower max error too.
Actually, I think I am ready to summarize. The point of `sinpi` isn't that it is more accurate close to multiples of pi, but that you can use it to calculate the sine if values that cannot be expressed in floating point. `sin(3pi)` isn't different from zero because `sin` is inaccurate, but because `3pi` is inaccurate.
Does using `sin` as the reference impact your conclusion? julia&gt; f(x) = abs(sin(x) - sin(big(x))) f (generic function with 1 method) julia&gt; g(x) = abs(sinpi(x/pi) - sinpi(big(x/pi))) g (generic function with 1 method) julia&gt; h(x) = abs(sin(x) - sinpi(big(x/pi))) h (generic function with 1 method) julia&gt; j(x) = abs(sinpi(x/pi) - sin(big(x))) j (generic function with 1 method) julia&gt; cnt_lt(f,g,x) = count(f.(x) .&lt; g.(x))/length(x) cnt_lt (generic function with 1 method) julia&gt; cnt_gt(f,g,x) = count(f.(x) .&gt; g.(x))/length(x) cnt_gt (generic function with 1 method) julia&gt; cnt_lt(f,g,x) # f is sin - sinbig, g is sinpi - sinpibig 0.500103 julia&gt; cnt_gt(f,g,x) # f is sin - sinbig, g is sinpi - sinpibig 0.499897 julia&gt; cnt_lt(f,h,x) # f is sin - sinbig, h is sin - sinpibig 0.774505 julia&gt; cnt_gt(f,h,x) # f is sin - sinbig, h is sin - sinpibig 0.225495 julia&gt; cnt_lt(f,j,x) # f is sin - sinbig, j is sinpi - sinbig 0.544376 julia&gt; cnt_gt(f,j,x) # f is sin - sinbig, j is sinpi - sinbig 0.009642 julia&gt; cnt_lt(g,h,x) # g is sinpi - sinpibig, h is sin - sinpibig 0.543875 julia&gt; cnt_gt(g,h,x) # g is sinpi - sinpibig, h is sin - sinpibig 0.010143 julia&gt; cnt_lt(g,j,x) # g is sinpi - sinpibig, j is sinpi - sinbig 0.776987 julia&gt; cnt_gt(g,j,x) # g is sinpi - sinpibig, j is sinpi - sinbig 0.223013 julia&gt; cnt_lt(h,j,x) # h is sin - sinpibig, j is sinpi - sinbig 0.502317 julia&gt; cnt_gt(h,j,x) # h is sin - sinpibig, j is sinpi - sinbig 0.497683 I ran though and did some comparisons. It seems that the choice of reference does have some impact.
ForwardDiff.hessian
using ForwardDiff function f(x) mu = x[1] gamma = x[2] lambda = x[3] theta = x[4] return ... end h = x -&gt; ForwardDiff.hessian(f,x) x0 = [1,1,1,1] hess = h(x0)
But did you read my other, later post? (Below, I try to use the convention that italics, like *x,* signify mathematical (Real) numbers, while `x` is a float, *sin* is the mathematical function, while `sin` is a computer program.) The reason I'm saying that `sin(x)` is more accurate is that `sinpi(x/pi)` isn't even *trying* to be accurate. It's calculating the sine at a different point. So, the utility of `sinpi` is not that it's more accurate near multiples of pi, because it *isn't.* The point of `sinpi` is that you are able to ask it for values of the form *xπ* (or at least `x`\**π*). We are asking different questions of the sine function, which is why we are unable to see eye to eye. You are asking "what is the sine value of this number, which is expressed in terms of *π*?" While I am asking "what is the sine value of this float?" The way to get an answer to the first question is to use `sinpi`. But if you use that for calculating the sine of general floats, you will get a less accurate answer. The best answer is then given by `sin(x)`. One way to show that, for floats `x`, `sin(x)` is more accurate, *even near multiples of pi,* is to use your own example: julia&gt; pif = float(π); julia&gt; x = [prevfloat(pif), pif, nextfloat(pif)]; julia&gt; big(π) .- x 3-element Array{BigFloat,1}: 5.665538897647979338920592604638282308209749445923078164062861980294536250318213e-16 1.224646799147353177226065932275001058209749445923078164062861980294536250318213e-16 -3.216245299353272984468460740088280191790250554076921835937138019705463749681787e-16 julia&gt; sin.(x) 3-element Array{Float64,1}: 5.66553889764798e-16 1.2246467991473532e-16 -3.216245299353273e-16 julia&gt; sinpi.(x./pi) 3-element Array{Float64,1}: 3.487868498008632e-16 0.0 -6.975736996017264e-16 julia&gt; almost_exact = big(π) .- x 3-element Array{BigFloat,1}: 5.665538897647979338920592604638282308209749445923078164062861980294536250318213e-16 1.224646799147353177226065932275001058209749445923078164062861980294536250318213e-16 -3.216245299353272984468460740088280191790250554076921835937138019705463749681787e-16 julia&gt; error_sinpi = sinpi.(x/pi) .- almost_exact 3-element Array{BigFloat,1}: -2.177670399639347575404459044734412019399385679354643934564809534588556187833472e-16 -1.224646799147353177226065932275001058209749445923078164062861980294536250318213e-16 -3.759491696663990542563806379719460385830476979059946623058966871706496375287696e-16 julia&gt; error_sin = sin.(x) .- almost_exact 3-element Array{BigFloat,1}: 2.764667309787495847375811193296251986726104051180819911783088006636894911587047e-32 2.994769809718339554641594267875450189973339414651230218759663483116995990870467e-33 -2.165713347843827936447492339721161948731436168250573868031155310013495713412953e-32 Since we know that, mathematically, *sin(x)* for *x* near *π* is approximated by (*π - x)*, you can see that `sin(x)` is closer to the correct answer.
I read this first version of this book. Quite a basic book, nice as general overview. &amp;#x200B; I would like to go through "Algorithms for Optimization" as well, this is also written with Julia in mind. &amp;#x200B; [https://mitpress.mit.edu/books/algorithms-optimization](https://mitpress.mit.edu/books/algorithms-optimization)
You've convinced me. Thank you for this excellent explanation.
Thanks. I didn't fully get it myself, before going through this discussion. So, useful for me too.
Inside the \`for\` loop there is a separate scope. Notice that \`i\` is not defined outside the loop, and if you define some other variable \`x =42\` inside the loop, it will not be defined outside either. In addition to putting it in a function (this works because everything inside the function has the same scope), you can also put \`global\` in front of the expression inside the loop. This tells julia that you intend for the \`num\_expects\` variable inside the loop to refer to the one in the global scope . See also: [https://docs.julialang.org/en/v1/manual/variables-and-scoping/index.html](https://docs.julialang.org/en/v1/manual/variables-and-scoping/index.html) https://stackoverflow.com/a/51930866/3742902
If you write it in function everything works as intended: ```function f(x) num_accepts = 0 for i in 1:x num_accepts += 1 end num_accepts end``` In order to run your code in the REPL you need to "declare" `num_accepts` as `global`: ``` global num_accepts = 0 for i in 1:2 global num_accepts += 1 end ```
Thanks, makes a lot of sense
https://docs.julialang.org/en/v1/stdlib/LinearAlgebra/index.html#LinearAlgebra.Symmetric Storing the matrix usually isn't the concern. This wrapper mostly handles the BLAS interactions.
I was a quite surprised by your results with respect to MH. So I coded up a simple one from scratch and I have a few comments. I'm unsure why you chose the InverseGamma parameters of 2 and 1/8 for the prior distributions for alpha and beta. I ended going with 30 and 90 after seeing the extremely low acceptance rate with your parameters. I don't think that 1000 iterations is nearly enough for the MH to work. I did 20000 with 10000 as burn in and it runs in about 3.5 seconds. I don't know how the MH function you used works, so I might be wrong. In terms of recovering the distribution I was able to get very close. I've linked the distribution plot below, it could be better if I up the acceptance rate I think. https://imgur.com/x17qiDk
Inverse Gamma with (2, 1/8) is a distribution with infinite variance, so can help explore the tails if needed. The 1/8 value is pretty arbitrary like you've discovered in your code. The MH function I've used comes from Turing package. I've not looked in detail how they actually implement that sampler. Your code looks great though, have you tried seeing how many effective samples you got out of 20000 iterations?
Ah yes, I plotted the histogram of a IG sample with those parameters and kept getting horribly large outliers, the infinite variance explains that now thanks. I haven't looked at effective sampling, its not something I'm familiar with, but will take a look now.
ESS coded up, slight changes but pretty much the same as https://github.com/tpapp/MCMCDiagnostics.jl/blob/master/src/MCMCDiagnostics.jl 1211.41 for the alphas and 1277.05 for the betas.
I don't know much about the inner workings of Juno unfortunately, but in general you'll probably get a better response to these kinds of questions on [the Julia forums](https://discourse.julialang.org). Maybe try there?
Thanks for the redirect. :). I will ask over there.
I'm using notebooks for a quick look and in the end create a module/package using Revise but yeah sometimes need to rerun the REPL which isn't the best but not too bad in my opinion.
If you have one file that includes the others, you can add any amount of files and Revise will track them. You can also put multiple modules in a file, or multiple files in a module, which might give you fewer components to handle from the REPL. I know some people have a notebook-like workflow using Juno, which lets you work directly in the file you're writing.
I use a combination of different techniques. Something that is good to know is that you can always manually eval into a module using: @eval Module begin ... end You can easily write a function that does some custom reloading into your module, if Revise doesn't do what you want. Some tools used to be able to switch the console mode to evaluate directly into a module, but I don't know if that still exists.
That first point was something that I was thinking about last night but forgot to test. I think I might give that a shot.
For package development, a scratch file plus Revise and Juno. Scratch file acts sort of like a REPL, competed functions go into the module file and the stuff I'm tinkering with in the scratch file to check things can be modified for tests. For data analysis, I use weave markdown in Juno. It's like a jupyter notebook but without the need to start up a server and much nicer to version control. Plus you can output a notebook or a PDF if you want to.
`setcolorscheme(2)` works pretty decently, isn't all white but is pretty good on dark backgrounds...
I'm curious if you've ever run into this issue. &amp;#x200B; #in MainModule.jl MainModule include(file1.jl) include(file2.jl) #insert code using all modules end #in file1.jl Module1 struct sometype end in file2.jl Module2 include(file1.jl) struct F2struct{T&lt;:Module1.sometype} var::T end &amp;#x200B; Suppose now that I'm in the repl and I have included the above 3 files. If I instantiate sometype in the REPL, it sees: 1. Main.Module1.sometype 2. Module2.Module1.sometype &amp;#x200B; If I try to instantiate F2struct in the REPL using an instance of Module1.sometype, I get an error because F2struct expected Module2.Module1.sometype, not Main.Module1.sometype. &amp;#x200B; I've already removed all modules except for the main one which now includes all of the files. As a consequence every item has a relationship such as Main.MainModule.X, and all my code can interoperate, but this dirties up my workspace with a bunch of variables. &amp;#x200B; I think there are certainly some improvements to be made in terms of ease of use--especially when it comes to attracting newcomers to the language, but even with these inconveniences, I am enjoying Julia.
Thanks :)
Off topic: what school are you in, I want to transfer to your school?
you need generators, the ones that look like &lt;expr of i&gt; for i in 0:200 and then well folding in this case is just a sum, but there are a number of such aggregation functions that work on such generators
I think the simplest way is this [Imgur](https://i.imgur.com/805hAvf.png)
You can simply iterate `A = [1 0 ; 0 1]` `for i = 1:200` `global A += b^i * P^i` `end` However, this is cumbersome and should be discouraged in a language like Julia (if it isn't obvious by the fact that you have to use the ugly `global` keyword to update A). The more elegant, simple, and understandable solution would simply be `A = sum([b^i * P^i for i = 0:200])` which you can verify gives the same output by observing `b^0 = 1` and `P^0 = [1 0 ; 0 1]`.
using LinearAlgebra out = zeros(2,2) out = out+I for i = 1:200 out += b\^i .\* P\^i end &amp;#x200B; So this is the solution that I came up with real quick. There are tiny little things you can change, but overall this still does what you want. I use the LinearAlgebra package because this allows me to use the identity matrix by just calling I. This is is pretty nice because it can map to any size you give it. I initialize my final value by creating a matrix of zeros. In practice you wouldn't want to do it this way because making an empty matrix like this can take up a lot of memory if your problem is large, but since this is a 2x2 matrix there is no problem. I then do the first part of the equation where it is I + . . . . out is now the identity matrix. Here is where the loop comes in. You want this from 1:200 because thats how many terms your equation calls for. "out +=" is just a fancy way of storing your previous value and having the next value add to what you have stored. This could be replaced by a sum but then you'd have to store all previous values in an array and that just taking up memory when you could do it much more efficiently this way. The the rest of the equation b\^i .\* P\^i is just representing the rest of your equation. Notice that it is .\* and this is because you have to do element wise multiplication of your scalar value b, too a matrix of P sized 2x2. If you don't want to use the LinearAlgebra package just create your own identity matrix. Ill let you figure out what lines you have to replace if you want to do it this way
@sagrada-muerte I agree with their comment. In terms of traditional loops you would see it in the form I have, but with Julia their last line solution is the most elegant. If you're just starting to code this line may be a littler harder to read and understand, but it does what you're looking for the best. So to reiterate A = sum(\[b\^i \* P\^i for i = 0:200\]) is the best way to do this.
I study economics at the University of Oregon. I'm in an experimental course where we're using Julia to calculate optimization problems (we're not quite there yet). Generally, the department uses R for data manipulation but this specific course requires a different language (hence I'm learning Julia)
I really appreciate all the help! I was on the right track in my thinking but it just didn't manifest itself in the code.
Brilliant! thanks so much.
Worth mentioning that the `global` keyword (what you need to get around "hard" local scoping that was enacted post-0.7) is not needed if you're in a Jupyter notebook (as you appear to be), because the `SoftGlobalScope.jl` package is basically loaded by default.
The solution is to use `PackageCompiler.jl` to AOT-compile Plots (basically, eliminate the "time to first plot" problem.) This is exactly the use case PackageCompiler was built for. Brings that delay down to the order of milliseconds. The [PackageCompiler repo](https://github.com/JuliaLang/PackageCompiler.jl) should have some examples, and this is definitely one I've seen.
Depending on how fancy you want to get with your loop you could also exploit the fact that the terms can be generated by multiplication with a previous value instead of recalculating the exponents at each step.
You can just leave you the square brackets and write it as A = sum(b^i * P^i for i = 0:200) That way, you are summing over the generator instead of materializing the whole array first, and then summing over it.
So, this is definitely a job for an ordinary loop. You can use generators, but the generators you have seen here have a problem: performance. Generators normally have fine performance, but notice that here they are re-doing the same work, over and over: First, calculate P\^1, then P\^2, then P\^3, over and over. So, use a loop instead: function powersum(P, b, n) P1 = one(P) # this creates a diagonal matrix of ones b1 = one(b) # make the number one of the same type as b S = P1 for i in 1:n P1 *= P # this is the same as P1 = P1 * P b1 *= b S += b1 * P1 # same as S = S + b1 * P1 end return S end Now you only need to do a single matrix multiplication (\`P1 = P1 \* P\`) at each iteration. BTW, if you *really, really,* like generators, don't use square brackets, `[]`, because they will make a vector before summing it, instead of just accumulating a sum. So instead of sum([b^i * P^i for i in 0:200]) you should write sum(b^i * P^i for i in 0:200) Let's compare performance: julia&gt; using BenchmarkTools julia&gt; @btime sum($b^i * $P^i for i in 0:200) 123.868 μs (2094 allocations: 228.78 KiB) julia&gt; @btime powersum($P, $b, 200) 33.055 μs (601 allocations: 65.73 KiB) As you see, the loop is much faster. But still, it's much slower than it needs to be. The reason for that is that at each iteration a new matrix, `P` is created, which causes memory allocations. There are two ways to fix this. The hard way is to use in-place multiplication (it's not *very* hard). The easy way is to use StaticArrays. These are super-fast arrays that you can use instead of small matrices (up to maybe 10x10 or 20x20). Check this out: (v1.1) pkg&gt; add StaticArrays julia&gt; using StaticArrays julia&gt; Ps = SMatrix{2,2}(P) # we make a static matrix out of P 2×2 SArray{Tuple{2,2},Float64,2,4}: 0.4 0.6 0.2 0.8 julia&gt; @btime sum($b^i * $Ps^i for i in 0:200) 10.674 μs (0 allocations: 0 bytes) 2×2 SArray{Tuple{2,2},Float64,2,4}: 9.24558 24.0146 8.00488 25.2553 julia&gt; @btime powersum($Ps, $b, 200) 1.175 μs (0 allocations: 0 bytes) 2×2 SArray{Tuple{2,2},Float64,2,4}: 9.24558 24.0146 8.00488 25.2553 Both the generator and the loop are now *much* faster. The loop is more than 100 times faster than the original generator. Don't be afraid of loops in Julia. They are not discouraged. You can *often* get fast and clean code without loops, but you can *always* get fast code with loops.
And, BTW, for this particular problem, you can even get the solution much, much faster. What you are calculating is a geometric sum, so you can use the formula for geometric sums: using LinearAlgebra # to use the uniform scaling (identity matrix) I function psum(P, b, n) bP = b * P return inv(I - bP) * (I - bP^(n+1)) end If you don't want to load the LinearAlgebra library, you can use `one(bP)` instead of `I`. New benchmark: julia&gt; @btime psum($P, $b, 200) # using ordinary matrix P 1.302 μs (19 allocations: 2.92 KiB) 2×2 Array{Float64,2}: 9.24558 24.0146 8.00488 25.2553 julia&gt; @btime psum($Ps, $b, 200) # using StaticMatrix Ps 53.626 ns (0 allocations: 0 bytes) 2×2 SArray{Tuple{2,2},Float64,2,4}: 9.24558 24.0146 8.00488 25.2553 As you see, the fastest code here is more than 2000 times faster than the original generator. Though, perhaps using the geometric series formula is a bit like cheating here ;)
It's definitely not the *best* way, since A = sum(b^i * P^i for i = 0:200) # without [] is definitively better. But as i argue in a post above, a straight loop is probably the best.
This is clearly for a class. My guess of they're working up to that.
That's a very nice feature of Julia I wasn't aware of, thanks for pointing it out
[you can run the profiler and find out for yourself](https://docs.julialang.org/en/v1/manual/profile/index.html#Memory-allocation-analysis-1)
Yes, that is exactly the point, see here [https://docs.julialang.org/en/v1/manual/arrays/#Generator-Expressions-1](https://docs.julialang.org/en/v1/manual/arrays/#Generator-Expressions-1) It lazily produces the values one by one by creating a recipe for the list, so no there's no allocation of temporary arrays.
Ah I was under the impression that [] were needed. If not that’s great. I saw your explanation in another comment
The brackets are needed when you want to collect the values into an array, and it's therefore called an array comprehension. If you don't want to collect, you just skip them :)
julia startup is slow. you can minimize it by including a RUN step in the Dockerfile to invoke a small jl script that precompiles the used packages.
what should this script contain?
well i'm not really knowledable in this, but maybe just a *using X* for all packages. but there is also Base.compilecache
QuantEcon maintains a good set of Julia Docker images. See https://github.com/QuantEcon/docker
Ay I’m grappling with this exact issue at work as well! Firstly, you don’t need to activate your environment like that, there’s actually a much cleaner way: julia —-project main.jl -O3 From the same directory as your project.toml will run your `main.jl` in your current environment. You can also use `—project=@.` to be more explicit. The `-O3` tells the compiler to be more aggressive with its optimisations. Next, in your container build step, have a step that causes precompilation of your code: RUN julia —-project -O3 -e “using mypackage” If you’ve got most of your code setup as a module you import into your script that you run, then this should trigger precompilation and build the cache at this stage, rather than runtime. If that’s still too slow, start looking into packagecompiler.jl I’m sure there’s some other stuff that can be done to minimise resource usage and docker startup time, but this is only what I’ve implemented so far.
Great thanks I will try that
This is true, but the dude teaching this class is a math/physics major from Stanford who got his Phd in economics. When you come to him with more advanced thinking/knowledge on the subject I think he gets a chub. &amp;#x200B; I've never seen someone so stoked to explain how x = y\*inv(A)
I would disagree with this. Here are a few other uses cases which encourage Julia use: - If you want your code to look like math (say, if you're a research scientist.) The unicode support and incredibly easy method overloading makes this stuff a breeze (among other features.) Don't underestimate the usefulness of having code that's easy to write and read. - If you want an "accessible" language with powerful metaprogramming support. You can forget about metaprogramming in things like MATLAB, and it's clunky at best in Python. Not so in Julia. LISP is the gold standard for this, of course, but is "inaccessible" to many for a number of reasons. - If your program makes heavy use of types and dispatch (i.e., you could expect vastly different behavior based on types of arguments), Julia is a no-brainer. Note that "speed" doesn't factor into the above; that is, there are plenty of reasons to choose one language over another which are orthogonal to speed.
`maximum(A, dims=3)`.
\[Transpiling Julia to C – The LLVM-CBackend\](https://juliacomputing.com/blog/2016/03/10/j2c-announcement.html) (on Julia Computing) It's from 2016, so maybe there is a more recent source now.
Yeah builddir is ginormous... I'd recommend uploading to Google Drive and emailing the teacher about your special scenario. Also, you have to create builddir with matching architectures or it won't run, for example if you work on a mac and teach works on linux, it won't run. So I'd get your code on GitHub or something for proof it's done and then start re-implementing it in C. That's your best bet. Teach is gonna want to be able to read code, transpiles are typically not very readable.
Yeah, I was aware of it needing to be compiled on the architecture it’d actually be run on, and went through a great amount of effort to install Julia and compile my script on a Windows machine for him. Still pretty upset that it didn’t “work” considering all the effort I went through for that alone. As for readability, I’m working with the understanding that the C code would basically be treated as a kind of byte code (really not sure if I’m using the right terms here) and the actual.jl script with all of my documentation and my write-up for it would be what was graded.
My open source project might be helpful: https://github.com/markhalonen/sauna-sim I deploy this to AWS ec2
I strongly agree with your opinion. The loading time of Plots.jl or Pyplot spoils all excellent language design and fast performance of Julia. I really regret that Julia itself is very fast, but if we make some small program, Julia is extremely slower than python because of plot loading time. I'm suffering from this problem for a long time, and that's why I can't recommend Julia to other people. &amp;#x200B; I tried precompiling Plots.jl by using PackageCompiler.jl, but I failed it. I hope precompiled Prots.jl will be included in Julia distribution, someday.
You can try grabbing the generated LLVM IR and use that directly? Yes, Julia is a dynamic language and right now its binary building creates a binary which holds its environment as well just in case it needs to compile code.
Hey all, i'm rather new to Julia, i'm interested in the language for both data science and game dev. I've been having a lot of trouble with graphics libraries lately, so I thought i'd take it down to a lower level and do some ncurses bindings. I found talking with C pretty easy. I encountered one weird gatcha where the library in my OS wasn't actually a real .so but a reference and was throwing some errors. Overall the process was pretty painless.
you have symbolic calculations and float{64} calculations. They don't match and have different use cases. The float{64} should be close to symbolic, but to be honest, I don't expect real word data to be exactly pi hence don't expect sin(x) to be 1 (or 1.0)
After much struggle to get the most basic example going. Here's an opengl example of just a triangle that works with Julia 1.1. There's a lot of broken code out there on google search.
you'll find more here: [https://github.com/Gnimuc/Videre](https://github.com/Gnimuc/Videre) (although not all examples are updated to Julia 1.0)
I would check out the following: * [Julia Docs - SIMD](https://docs.julialang.org/en/v1.1/base/simd-types/) * [SIMD.jl](https://github.com/eschnett/SIMD.jl) (has some examples of using `llvmcall` to explicitly create SIMD function)
Thanks anyways! Great! Do you know if all SIMD instructions from Intel are supported? I need the latest AVX2 support. Something I need to check.
Unfortunately I do not know which instructions are supported or not, but if it is it should be documented somewhere. The instructions you mentioned (AVX-512) seems to be rather specific to some Intel processors. It might be good to check if LLVM itself has access to these intrinsics, in which case you should be able to call them using the methods described in your intrinsics [link](http://kristofferc.github.io/post/intrinsics/) or the `SIMD.jl` [link](https://github.com/eschnett/SIMD.jl). If none of the above is possible, you could create a small C library that wraps the intrinsics and just use a `ccall` to access the functions.
I don't think `ccall`ing with the C calling convention would work, as then you'd have call overhead any time you want to use a SIMD instruction. The `-emit-llvm` workflow followed by `ccall`ing with the `llvmcall` calling convention described in the blog post sounds like the way to go for SIMD instructions that are not in SIMD.jl.
I'd enjoy conversation on idiomatic data oriented design in Julia. In the patterns I see in projects in its current form, I don't think it would scale very well for larger projects.
data.function() is not idiomatic at all. especially not data.function(data). this is nothing but the old oop habit, there isn't any rationale behind it
OOP is not a syntax of calling a function
I honestly don't like it since we already have the dot operator (for both broadcast and as a struct field accessor, the latter even overridable). So it could generate some confusion for example with: struct A f::Function end g(a) = a b = A(g) b.f(4) And second because it doesn't really saves characters (maybe one space). If we are talking about syntax sugar I'd favor something for dictionaries, especially with symbol keys, like the usual {a: 43, b: 2} instead of Dict(:a =&gt; 43, :b =&gt; 2) (though it has to be well thought so you can apply to any custom dict just as easy), some facility to create new mutable maps/structs with one field changed like elixir's %{map| field: newvalue} or finally some official equivalent to Match.jl or MLStyle.jl (especially for macros and error handling). Though by the suggestions you can clearly see my bias.
Your examples don't seem to capture what my suggestion does. You seem be suggesting `static functions` on structs, not `methods` of a struct. ``` b.foo() ``` would pass b as the first parameter to foo, in your examples that does not happen.
Sorry, the first example is meant to say your suggestion would cause an ambiguity since example already works in Julia. If you define the above plus a function f(a::A, Int64) then the compiler would be completely unable to decide if you want to call the field or the external function. And even if you only define one of those, anyone reading the code would have to backtrack to know which one was implemented to understand what the code is calling. Your idea could technically work, but it would need to use an another operator, not the dot, since programming languages should avoid allowing syntax ambiguities with contextual resolution.
I don't agree personally given my assumed rarity of people putting functions in structs. That said, i'd be totally fine with another operator. Let me show you an example of an issue i'm facing. Here's some code for a game i'm writing. ```julia mutable struct PositionComponent x::Float32 y::Float32 end world = World_create() World_registerComponent(world,PositionComponent) player = World_createEntity(world) World_addComponent(world,player,PositionComponent(1,1)) ``` Notice how I have to specify the struct name firstly to disambiguate it from other function. With Uniform Function Call Syntax, this could be shortened to: ```julia mutable struct PositionComponent x::Float32 y::Float32 end world = World_create() world.registerComponent(ositionComponent) player = world.createEntity() world.addComponent(player,PositionComponent(1,1)) ```
The example code you start with looks like what someone who wants to do OOP might write in C where there is no function overloading of any sort. Not even single dispatch, so definitely not multiple dispatch. That is categorically unjulian. The idiomatic Julia version of that would probably go something like tbis: ``` abstract Component struct Position &lt;: Component x :: Float32 y :: Float32 end function register!(w::World, ::Type{C}) where C &lt;: Component ... end world = World(...) register!(world, Position) player = createEntity!(world) add!(player, Position(1, 1)) ```
If we are talking about idiomatic Julia, World_registerComponent(world,...) would be unusual since multiple dispatch is about separating the concept of action from the concept of actors. You don't have Real_sum and Complex_sum for real and complex number (unless the action was fundamentally different to the point that you would want to use both to the same actor in different contexts, and even then some people might just use datatypes to differentiate like convert(Int64, 1.0) or just keep them in different modules and use import). Basically registerComponent(world, PositionComponent) should be perfectly readable to anyone using Julia (and not OOP languages in usual, including functional languages), and in general lots of people prefer not having to think if something is func(a) or a.func() in those languages (though I know in your syntax there is no issue since both are equivalent). But as something that I truly think your syntax helps is on method discoverability in REPL/Editors. You could use world. &lt;TAB&gt; to get all available methods, which is certainly really nice.
Why do you use these function names with „World_register_component“ instead of just „register_component“. Seems unnecessarily verbose. You mention something about that you have to write it this way. Why?
Context. This is how C does it. When libraries get big having mountains of single word names does not help.
It changes the order one needs to read lines of code to understand the code, and on group project can have two ways of achieving the same thing struct foo end ufcs(f) = return foo() w1 = foo().ufcs().ufcs() w2 = ufcs(ufcs(foo())) I don't like it.
The order works just like the pipe operator, and in fact that suggestion is basically Elixir's pipe which always takes the first argument of the following function but using a dot instead. w3 = Foo() |&gt; ufcs |&gt; ufct And with the PR of the new syntax for anonymous functions (if merged), it could work like (a |&gt; foo) for foo with 1 argument or (a |&gt; foo(_, arg)) for multiple arguments if someone really wants to write that way.
But in your unified function syntax you have the same issue. There you use world.register(...) which isn’t that different from register(world,...). In both cases you would define a single word name function. Your proposal just changes the way this function is called. What you are proposing also privileges a single argument of a function whereas with multiple dispatch there isn’t really one single input the method “belongs” to. If you are really interested in having this specific behavior I would suggest you write a macro that translates something like v1.add(v2,v3) into add(v1,v2,v3) Here is an ugly example: macro unified(expr) v1 = expr.args[1].args[1] v = expr.args[2:end] func = expr.args[1].args[2].value return :($func($v1,$(v...))) end This only works for simple cases and does not allow any chaining. It should work for the examples you posted though.
"clean" is very strong word. Large projects in mainstream OOP languages are usually not clean and very painful to refactor. "Clean" libraries are mostly can be found in something like Haskell due to very strong type guarantees. And composability is ridiculous. Or in Ocaml with functorial modules. If you look to something julia like, look at common lisp - people usually do not write documentation there, because users of libraries are expected to read code as documentation.. Can you imagine something like this in C++ world? And julia is very influenced by common lisp... and it scales. Julia is young to have too many projects, but large one already exist and very impressive ones. Good example is DifferentialEquations.jl. Author ( @ChrisRackauckas ) of this library writes a lot of nice blog posts about julia (including how to structure julia code). Look around his blog. One pointer [http://www.stochasticlifestyle.com/like-julia-scales-productive-insights-julia-developer/](http://www.stochasticlifestyle.com/like-julia-scales-productive-insights-julia-developer/)
I like that even less. next it will be |&gt;[n] takes the first n arguments otoh mayne this opens the door to partial application / currying which might be a good thing
Well, you can sort of do that already with slightly different syntax: |&gt;(n) = arr -&gt; arr[1:n] |&gt;(3)([1,2,3,4,5]) # [1,2,3] And you can see the discussion here: https://github.com/JuliaLang/julia/pull/24990 I think it's positive since it's a pattern commonly used in Julia and it already works well in many languages (I think even Ruby just added something similar as a shortcut for blocks), but of course, people can write some unreadable stuff with it when it would be better to use the more verbose version that properly documents the arguments.
The lack of this in Julia is one of my biggest beefs in the language. It's way too common to have to do something unreadable like `dropdims(mean(data, dims=3), dims=3)`. With universal function call syntax, it could be `data.mean(dims=3).dropdims(dims=3)`, and there's a more clear sense of a data pipeline. In my opinion D does this best with their anonymous function templates, so you get pipelines like ```D data.filter!(x =&gt; pred(x)) .map!(x =&gt; foo(x)) .fold!((x, y) =&gt; bar(x, y))(seed); ```
Thanks, that was an interesting read. I dislike when "number of keypresses" is a metric. I much prefer "can I understand the code I checked in 3 months ago".
You can do it right now with pipes, such as: https://github.com/tkf/Transducers.jl Though right now for your example you'll need to either create a specialization of the |&gt; operator (like the library above), use a macro (like Pipe.jl) or use anonymous functions: result = data |&gt; x -&gt; mean(x, dims=3) |&gt; x -&gt; dropdims(x, dims=3) Or if the syntax I mentioned above is merged: result = data |&gt; mean(_, dims=3) |&gt; dropdims(_, dims=3) # pipe result = (dropdims(_, dims=3) ∘ mean(_, dims=3))(data) # composition with the \circ operator Or alternatively of course: data_mean = mean(data, dims=3) result = dropdims(data_mean, dims=3)
From what I have seen most real time applications just try to not allocate any memory that might trigger the garbage collector. StaticArrays helps a lot in this regard. I think one of the showcases is a robotics group at MIT that does the controller for a humanoid robot in Julia. Such a controller runs at 100-1000 Hz apparently. https://juliacomputing.com/case-studies/mit-robotics.html
The Julia ecosystem hasn't been built outside of basic web development, robotics, and data science. So you're probably better off just using languages with established ecosystems, especially for gaming right now.
Hilariously, (of course) depending on your use case, I'm able to get \~300Hz *generating and solving* a convex problem at each instance using JuMP via a hopelessly naive solution. It takes a hard hit when GC runs (lowering to around \~80Hz), but this is still far more than enough for many applications. It's just insane that Julia is so fast that I haven't found the need to reimplement it in a more efficient way, even though this approach is clearly wasteful (and would be dead in the water in, e.g., Python).
Thanks for this, I'm rather new to Julia, and this helped me understand a bit what people are talking about
Hm, thanks for you feedback, this has given me something to think about, perhaps I can take advantage of submodules of my package to get the same searchability
Great observance, thanks for this feedback
It's possible, and something I'm actively investigating, too. The big issue common with a lot of non-C++ languages is the inadequate compatibility with C++. This is by no means the fault of Julia - C++ ain't compatible with *itself* half the time, let alone with other languages - but it makes things harder given how much of the game development world currently revolves around C++; you'd have to put effort into either bridging Julia and C++ (the Qt.jl or whatever it's called folks are tackling that front) or reimplementing things specifically for Julia.
Well the thing is that Unitys Core itself relies on manual memory management (obviously, because no game would run smoothly otherwise). C# is used as a scripting language, the code that the user writes is exposed to automatic memory management but as you said yourself that should not be a big deal. So as much as Julia strives to close the barrier, I suspect that this is (at least for now) the only solution: to use Julia as a kind of "glue/scripting language" that calls manually managed C functions. I know it is behind the Julia mindset but I see no other way around it, atleast for those kind of projects. For me this is no big deal, I know Julia and the syntactical strengths it has to offer. On a side note: Blender also uses Python as a scripting language, but the 3D Core of Blender is written in optimized C. Unfortunately I have no idea around the ccall procedures and if they are even able to force manual memory management that the C code tries to invoke (I mean wrappers around the free() calls).
As Julia matures I think we'll start to see more attention and more solutions for things like (soft/hard) real time requirements etc.
Start writing a script that contains a "main" function/entrypoint and just start writing the logic in there. This usually means I make quite a lot of progress through the task at hand. As I work I'll start pulling out loops or re-used code or reused patterns into their own functions and compositions of functions. Start adding type signatures to functions as their use/reuse solidifies. Once everything works end-to-end, refactor into "library" and "business logic" portions. Makes maintenance and reading a bit easier; also makes it easier to come back later and extract all the library level stuff that's used across projects into its own relevant package. My work setup consists of VSCode, a terminal running a couple of tabs: one with a running instance of Julia to test/whiteboard things on, the other for running my code from scratch easily. Sublime merge for git commits. In my Julia instance I use OhMyREPL heaps because it's so very pretty. Don't make much use of Revise, but I've been meaning to. I tend to avoid notebooks like the plague because I find they tend to attract way too much hidden state/out of order magic/poor software engineering practices etc.
Has anyone tried this? I've not futzed around much with Julia and this seemed pretty interesting. Unfortunately, I ran into so many errors (I think about dependancies not installing or building correctly) that I eventually gave up.
Hey thanks for checking it out and letting us know! Can you post the error(s) you're seeing? I know if you install Oceananigans on a computer without an Nvidia GPU you get a bunch of warnings as it tries to install GPU related packages but things should still run fine on the CPU. This is an issue we're actively trying to fix but needs some upgrades to the Julia package manager unfortunately: https://github.com/climate-machine/Oceananigans.jl/issues/178
Scientific computing is also one of the major established ecosystems of Julia, and that can be quite relevant here.
Hi, I'm currently running Ubuntu 19.04 and I have an NVIDIA GPU. What I expected to be able to do is: apt install julia julia ] pkg&gt; add https://github.com/climate-machine/Oceananigans.jl.git julia firstmodel.jl (where firstmodel.jl is the first example in the readme.md) Adding the Oceananigans package failed for a number of missing dependencies which in part it seemed like I needed use apt for (e.g. hdf5 stuff). Also because I had the NVIDIA GPU and (some of) the Cuda framework but didn't have cuDNN I had to do the silly NVIDIA manual install gyrations and I got the impression I should have done that first. Anyway, I went through a handful of iterations of adding the Oceananigans package in julia, running the example script, finding dependency errors, trying to add that dependency with either julia's package manager or apt... rinse, lather, repeat. I developed the impression that forcing julia to build various dependencies manually got me further along than just adding the Oceananigans package ... but ultimately I got stuck because I have no idea wtf I'm doing. Last night I purged a great many things that were installed from me trying various projects; so I will try again today and give you actual error messages rather than my dim recollections. Though I will install all the dependencies I can dredge out of my shell history before I start. Also FWIW, I did try to install the JuliaPro IDE but for some reason it didn't play well at all with my system.
 CUDA-enabled GPU(s) detected:Internal error: encountered unexpected error in runtime: BoundsError(a=Array{Core.Compiler.BasicBlock, (32,)}[ Core.Compiler.BasicBlock(stmts=Core.Compiler.StmtRange(first=1, last=7), preds=Array{Int64, (1,)}[32], succs=Array{Int64, (1,)}[2]), Core.Compiler.BasicBlock(stmts=Core.Compiler.StmtRange(first=8, last=14), preds=Array{Int64, (1,)}[1], succs=Array{Int64, (2,)}[5, 3]), Core.Compiler.BasicBlock(stmts=Core.Compiler.StmtRange(first=15, last=20), preds=Array{Int64, (1,)}[2], succs=Array{Int64, (1,)}[4]), Core.Compiler.BasicBlock(stmts=Core.Compiler.StmtRange(first=21, last=21), preds=Array{Int64, (1,)}[3], succs=Array{Int64, (1,)}[7]), Core.Compiler.BasicBlock(stmts=Core.Compiler.StmtRange(first=22, last=28), preds=Array{Int64, (1,)}[2], succs=Array{Int64, (1,)}[6]), Core.Compiler.BasicBlock(stmts=Core.Compiler.StmtRange(first=29, last=29), preds=Array{Int64, (1,)}[5], succs=Array{Int64, (1,)}[7]), Core.Compiler.BasicBlock(stmts=Core.Compiler.StmtRange(first=30, last=47), preds=Array{Int64, (2,)}[4, 6], succs=Array{Int64, (2,)}[9, 8]), Core.Compiler.BasicBlock(stmts=Core.Compiler.StmtRange(first=48, last=49), preds=Array{Int64, (1,)}[7], succs=Array{Int64, (0,)}[]), Core.Compiler.BasicBlock(stmts=Core.Compiler.StmtRange(first=50, last=50), preds=Array{Int64, (1,)}[7], succs=Array{Int64, (1,)}[10]), Core.Compiler.BasicBlock(stmts=Core.Compiler.StmtRange(first=51, last=52), preds=Array{Int64, (1,)}[9], succs=Array{Int64, (1,)}[11]), Core.Compiler.BasicBlock(stmts=Core.Compiler.StmtRange(first=53, last=53), preds=Array{Int64, (1,)}[10], succs=Array{Int64, (1,)}[12]), Core.Compiler.BasicBlock(stmts=Core.Compiler.StmtRange(first=54, last=54), preds=Array{Int64, (1,)}[11], succs=Array{Int64, (1,)}[13]), Core.Compiler.BasicBlock(stmts=Core.Compiler.StmtRange(first=55, last=56), preds=Array{Int64, (1,)}[12], succs=Array{Int64, (1,)}[14]), Core.Compiler.BasicBlock(stmts=Core.Compiler.StmtRange(first=57, last=65), preds=Array{Int64, (1,)}[13], succs=Array{Int64, (1,)}[15]), Core.Compiler.BasicBlock(stmts=Core.Compiler.StmtRange(first=66, last=71), preds=Array{Int64, (1,)}[14], succs=Array{Int64, (2,)}[17, 16]), Core.Compiler.BasicBlock(stmts=Core.Compiler.StmtRange(first=72, last=74), preds=Array{Int64, (1,)}[15], succs=Array{Int64, (0,)}[]), Core.Compiler.BasicBlock(stmts=Core.Compiler.StmtRange(first=75, last=82), preds=Array{Int64, (1,)}[15], succs=Array{Int64, (2,)}[19, 18]), Core.Compiler.BasicBlock(stmts=Core.Compiler.StmtRange(first=83, last=84), preds=Array{Int64, (1,)}[17], succs=Array{Int64, (0,)}[]), Core.Compiler.BasicBlock(stmts=Core.Compiler.StmtRange(first=85, last=85), preds=Array{Int64, (1,)}[17], succs=Array{Int64, (1,)}[20]), Core.Compiler.BasicBlock(stmts=Core.Compiler.StmtRange(first=86, last=87), preds=Array{Int64, (1,)}[19], succs=Array{Int64, (1,)}[21]), Core.Compiler.BasicBlock(stmts=Core.Compiler.StmtRange(first=88, last=88), preds=Array{Int64, (1,)}[20], succs=Array{Int64, (1,)}[22]), Core.Compiler.BasicBlock(stmts=Core.Compiler.StmtRange(first=89, last=89), preds=Array{Int64, (1,)}[21], succs=Array{Int64, (1,)}[23]), Core.Compiler.BasicBlock(stmts=Core.Compiler.StmtRange(first=90, last=90), preds=Array{Int64, (1,)}[22], succs=Array{Int64, (1,)}[24]), Core.Compiler.BasicBlock(stmts=Core.Compiler.StmtRange(first=91, last=92), preds=Array{Int64, (1,)}[23], succs=Array{Int64, (1,)}[25]), Core.Compiler.BasicBlock(stmts=Core.Compiler.StmtRange(first=93, last=93), preds=Array{Int64, (1,)}[24], succs=Array{Int64, (1,)}[26]), Core.Compiler.BasicBlock(stmts=Core.Compiler.StmtRange(first=94, last=94), preds=Array{Int64, (1,)}[25], succs=Array{Int64, (1,)}[27]), Core.Compiler.BasicBlock(stmts=Core.Compiler.StmtRange(first=95, last=95), preds=Array{Int64, (1,)}[26], succs=Array{Int64, (2,)}[29, 28]), Core.Compiler.BasicBlock(stmts=Core.Compiler.StmtRange(first=96, last=98), preds=Array{Int64, (1,)}[27], succs=Array{Int64, (0,)}[]), Core.Compiler.BasicBlock(stmts=Core.Compiler.StmtRange(first=99, last=102), preds=Array{Int64, (1,)}[27], succs=Array{Int64, (2,)}[31, 30]), Core.Compiler.BasicBlock(stmts=Core.Compiler.StmtRange(first=103, last=103), preds=Array{Int64, (1,)}[29], succs=Array{Int64, (1,)}[32]), Core.Compiler.BasicBlock(stmts=Core.Compiler.StmtRange(first=104, last=104), preds=Array{Int64, (1,)}[29], succs=Array{Int64, (1,)}[32]), Core.Compiler.BasicBlock(stmts=Core.Compiler.StmtRange(first=105, last=107), preds=Array{Int64, (2,)}[30, 31], succs=Array{Int64, (1,)}[1])], i=(0,)) unknown function (ip: 0x7f62ed96d57e) unknown function (ip: 0x7f62ed944901) jl_throw at /usr/bin/../lib/x86_64-linux-gnu/libjulia.so.1 (unknown line) jl_bounds_error_ints at /usr/bin/../lib/x86_64-linux-gnu/libjulia.so.1 (unknown line) getindex at ./array.jl:731 jfptr_getindex_1594.clone_1 at /usr/lib/x86_64-linux-gnu/julia/sys.so (unknown line) jl_apply_generic at /usr/bin/../lib/x86_64-linux-gnu/libjulia.so.1 (unknown line) replace_code_newstyle! at ./compiler/ssair/legacy.jl:80 optimize at ./compiler/optimize.jl:212 typeinf at ./compiler/typeinfer.jl:35 typeinf_ext at ./compiler/typeinfer.jl:574 typeinf_ext at ./compiler/typeinfer.jl:611 jfptr_typeinf_ext_1.clone_1 at /usr/lib/x86_64-linux-gnu/julia/sys.so (unknown line) jl_apply_generic at /usr/bin/../lib/x86_64-linux-gnu/libjulia.so.1 (unknown line) jl_apply_with_saved_exception_state at /usr/bin/../lib/x86_64-linux-gnu/libjulia.so.1 (unknown line) unknown function (ip: 0x7f62ed92b2f3) unknown function (ip: 0x7f62ed92e9d6) jl_fptr_trampoline at /usr/bin/../lib/x86_64-linux-gnu/libjulia.so.1 (unknown line) jl_apply_generic at /usr/bin/../lib/x86_64-linux-gnu/libjulia.so.1 (unknown line) unknown function (ip: 0x7f62ed945259) unknown function (ip: 0xffffffffffffffff) CuDevice(0): GeForce GTX 980 WARNING: could not import GPUifyLoops.@launch into Oceananigans ERROR: LoadError: LoadError: LoadError: LoadError: UndefVarError: @launch not defined Stacktrace: [1] top-level scope [2] #macroexpand#32 at ./expr.jl:92 [inlined] [3] macroexpand at ./expr.jl:91 [inlined] [4] docm(::LineNumberNode, ::Module, ::Any, ::Any, ::Bool) at ./docs/Docs.jl:509 (repeats 2 times) [5] @doc(::LineNumberNode, ::Module, ::String, ::Vararg{Any,N} where N) at ./boot.jl:451 [6] include at ./boot.jl:317 [inlined] [7] include_relative(::Module, ::String) at ./loading.jl:1044 [8] include at ./sysimg.jl:29 [inlined] [9] include(::String) at /home/Bhima/.julia/packages/Oceananigans/vUV58/src/Oceananigans.jl:1 [10] top-level scope at none:0 [11] include at ./boot.jl:317 [inlined] [12] include_relative(::Module, ::String) at ./loading.jl:1044 [13] include(::Module, ::String) at ./sysimg.jl:29 [14] top-level scope at none:2 [15] eval at ./boot.jl:319 [inlined] [16] eval(::Expr) at ./client.jl:393 [17] top-level scope at ./none:3 in expression starting at /home/Bhima/.julia/packages/Oceananigans/vUV58/src/poisson_solvers.jl:170 in expression starting at /home/Bhima/.julia/packages/Oceananigans/vUV58/src/poisson_solvers.jl:146 in expression starting at /home/Bhima/.julia/packages/Oceananigans/vUV58/src/poisson_solvers.jl:146 in expression starting at /home/Bhima/.julia/packages/Oceananigans/vUV58/src/Oceananigans.jl:159 ERROR: LoadError: Failed to precompile Oceananigans [9e8cae18-63c1-5223-a75c-80ca9d6e9a09] to /home/Bhima/.julia/compiled/v1.0/Oceananigans/hU93i.ji. Stacktrace: [1] error(::String) at ./error.jl:33 [2] compilecache(::Base.PkgId, ::String) at ./loading.jl:1203 [3] _require(::Base.PkgId) at ./loading.jl:960 [4] require(::Base.PkgId) at ./loading.jl:858 [5] require(::Module, ::Symbol) at ./loading.jl:853 [6] include at ./boot.jl:317 [inlined] [7] include_relative(::Module, ::String) at ./loading.jl:1044 [8] include(::Module, ::String) at ./sysimg.jl:29 [9] exec_options(::Base.JLOptions) at ./client.jl:266 [10] _start() at ./client.jl:425 in expression starting at /home/Bhima/model.jl:1
Sorry you had to go through all that. The only dependency I can think of that might need an `apt-get` would be `hdf5` (or maybe `hdf5-dev` in Ubuntu/Debian). This is kind of an annoying binary dependency as we want to produce NetCDF output (which relies on the HDF5 library) as it's by far the most common file format used in climate science (and oceanography + atmospheric science). I just checked and the GPU Google Cloud images I use seem to come with HDF5 pre-installed so I suppose I never had to worry about this. Thanks for bringing this up though, I will add a note about HDF5 to the installation instructions. A few things I would check just to make sure: * That you're running on the latest stable version of Julia (1.1.0). Looks like you have at least 1.0.0 since `]` gives you `pkg&gt;` at the REPL. I usually run from an older Debian image on Google Cloud that comes with CUDA but Debian's Julia version is ancient so I grab the latest Linux binary from https://julialang.org/downloads/ * That running the `nvidia-smi` command shows your GPU (to make sure the CUDA toolkit is correctly installed).
Ah interesting, thank you for posting this. Never seen this before... Julia is definitely seeing your GPU as it prints `CuDevice(0): GeForce GTX 980` which is good news. What version of Julia are you running? My first suspicion is that you might be on Julia 1.0 (since you used `apt`) but some of the features we use require Julia 1.1. I will open a feature request so that a warning/error is printed when running a Julia version that is too low. And I'll update the installation instructions to make this point clearer. If you're on Julia 1.1, then it might be something else but I'm very interested in figuring this out. It's really bad if people can't install the package lol...
julia version 1.0.3
Ah I think that might be an issue. We'd really appreciate it if you could try with Julia 1.1 and let us know if works there. I'm really sorry that we didn't make this clearer or check for the Julia version when loading the package, totally an oversight on our part.
Yeah I think using `apt` to remove Julia 1.0.3 and then installing the Linux binaries might be the way to go so you know you're not mixing Julia versions. Might be good to also `rm -rf ~/.julia/` to delete all the 1.0.3 stuff (like installed packages, precompiled stuff, etc.). Also, if you download and extract the Linux binary, you might also want to add the Julia binaries your `$PATH` by adding something like `export PATH=$PATH:~/julia-1.1.0/bin` to your `.bashrc` if you extracted the Julia binaries to `~/julia-1.1.0/`.
PS: Just added that version check so hopefully if it was the version, it'll print a more useful message if other people encounter the same issue. Thanks again Also, I'm updating the GPU example of the README with an actual GPU example.
OK. This was my issue. Now that I removed ~/.julia/ and use the generic Julia 1.1 binaries it all works.
The first question is really, what kind of game, and is it for commercial release, on what platforms, etc, but realistically, if your goal is to actually make and release a game rather than, make and release a game ***with Julia****,* you should probably look elsewhere. &amp;#x200B; You mentioned Nim, which will be a much better choice for both performance and portability. You could wrap raylib (there are a few wrappers on github but no idea how mature they are). &amp;#x200B; Have you looked at Godot, it's GDScript has optional typing now and is Python-like. I think Godot is pretty great. It is even possible to wrap their API with Julia but the path of least resistance will definitely be to just use GDScript.
Awesome! Sorry again for all the trouble you had to go through, but thanks for posting so we could fix things! I'm still updating the README with an actual GPU example (it'll be a 3D version of the rising thermal bubble CPU example). Just a heads up that if you try running the rising thermal bubble example you will need ffmpeg installed for the plotting library to produce the final movie (going to add this note to the README as well).
FWIW, trying the examples forced me to manually add a few packages using julia's package manager. I don't know if that's a big deal but I sorta expected that adding the top level package would have installed anything the examples needed.
That's a reasonable expectation and another oversight on our part, thanks for pointing this out. I'm guessing the main package would have been Plots.jl, and maybe ArgParse.jl if you tried running some of the more advanced examples. Yeah I think we argued about this among ourselves that we may not want to depend on a big package like Plots.jl as we only use it for the examples. I think a good compromise would be to have e.g. Plots.jl installed when the example runs. That way we avoid a heavy dependency and only have it be loaded at run time if needed. I just opened a GitHub issue about this so we can implement something to this effect. Still working on the improved GPU example, should have it done later today: https://github.com/climate-machine/Oceananigans.jl/pull/196 Thanks again for all your help!
I just meant typical application of Julia in 3d game development, as I am certain that 2d game dev would be certainly possible today (even Python can do this with fairly good performance). And yes, you have got a point with the GDScript, ans that is what I meant: either using Julia natively (which is not that good performancewise) or use it as a wrapper language
use a for loop, a SharedArray, and @parallel
specifically, like ``` H=SharedArray(zeros(n)) @parallel for i=1:n H[i] = stuff that's hard to type on my phone end
&gt;@parallel I'm using Julia 1.1.0. I get this error: `UndefVarError: @parallel not defined` Do I need a special pkg? PS: I like your answer. I had to smile\^\^
`using Distributed`
It changed to @distributed
also, you probably want to read through [at least some of the parallel documentation](https://docs.julialang.org/en/v1/manual/parallel-computing/) at some point.
`include("file.jl")` where `file.jl` is your file.
Thanks a lot then how can i give a specific path where my file is loaded.
`include("path/to/file.jl")`
Thank you realy so much \^\^
Hey, stoarmy, just a quick heads-up: **realy** is actually spelled **really**. You can remember it by **two ls**. Have a nice day! ^^^^The ^^^^parent ^^^^commenter ^^^^can ^^^^reply ^^^^with ^^^^'delete' ^^^^to ^^^^delete ^^^^this ^^^^comment.
Hey /u/CommonMisspellingBot, just a quick heads up: Your spelling hints are really shitty because they're all essentially "remember the fucking spelling of the fucking word". And your fucking delete function doesn't work. You're useless. Have a nice day! [^Save ^your ^breath, ^I'm ^a ^bot.](https://www.reddit.com/user/BooCMB/comments/9vnzpd/faq/)
Hey BooCMB, just a quick heads up: I learnt quite a lot from the bot. Though it's mnemonics are useless, and 'one lot' is it's most useful one, it's just here to help. This is like screaming at someone for trying to rescue kittens, because they annoyed you while doing that. (But really CMB get some quiality mnemonics) I do agree with your idea of holding reddit for hostage by spambots though, while it might be a bit ineffective. Have a nice day!
Hey /u/BooBCMB, just a quick heads up: No one likes it when you are spamming multiple layers deep. So here I am, doing the hypocritical thing, and replying to your comments as well. I also agree with the idea of holding reddit hostage though, and I am quite drunk right now. Have a drunk day!
Not exactly what the op asked for, but you may want to take a look at juno ide and Julia pro. It is a complete Julia ide based on atom (which is an hackable text editor similar to sublime text). It is able to interface with the Julia repl and it makes easy to run chunks of code without having to wrap it up in scripts.
Tonight I published my first package in Julia, I had to weed through a lot of confusing info before I found Registrator. It made publishing super simple! Such a great feature of Julia
I was in awe of my REPL tonight ``` julia&gt; a = :(1+1) :(1 + 1) julia&gt; a.head :call julia&gt; a.args 3-element Array{Any,1}: :+ 1 1 julia&gt; eval(a) 2 julia&gt; a.args[2] = 3 3 julia&gt; eval(a) 4 ```
&gt;Thank you :)
The manual has a well written tutorial on Expr and Macros. Julia even has an equivalent to reader macros (Non-Standard String Literals) that allows one to parse any arbitrary string at compile time and return code, and a unique (at the very least I'm not aware other language that has it) macro that works on types (generated functions) and allows to easily create complex dispatch rules (and is the basis for Cassete, and is used from debugging with MagneticReadHead to source to source automatic differentiation with Zygote). https://docs.julialang.org/en/v1/manual/metaprogramming/ and how Julia code appears in the Expr: https://docs.julialang.org/en/v1/devdocs/ast/
That seems like a function from the indexing interface on Julia 0.6 https://docs.julialang.org/en/v0.6/manual/interfaces/#Indexing-1 Try lastindex on Julia 1.0+ https://docs.julialang.org/en/v1.1/manual/interfaces/#Indexing-1 Since Julia just got stable (1.0) less than a year ago, it's still common to find code that is not compatible anymore.
Do you have any knowledge how fast evaled code runs vs non eval?
Should be the same speed. The eval will trigger a compilation phase (which will cause that lag you see whenever you call a function for the first time) but the runtime should run the same compiled code. Though I have not seen benchmarks to be sure.
Lol are you from the 90s?
I'm often (and I probably can't do it well) to wield the power of macro that manipulates the code. But I imagine it's powerful in the right hands.
It's brand new. We used to have attobot, RIP. I only have experience publishing a package in one other language (python), and yes, the julia experience is waaay nicer
endof has been deprecated but otherwise you can use: parentmodule(sin) or @which sin(1)
Avx 512 was possible in 2017 https://juliacomputing.com/blog/2017/09/27/auto-vectorization-in-julia.html . I imagine it still is but julia won't generate avx512 code unless you're running on a processor that supports it and I don't have any. It should work though.
Thanks. If anyone wants to pair program with AVX SIMD instructions, I would be interesting.
Just to give an example. In Tcl, this is done like this. First one creates a tcl\_obj value. This takes some time. But then you can pass values between C and Tcl with almost no overhead, since just the pointer is passed between C and tcl. So if you switch 100 language betwenn C and Tcl, you just pay the price one (for creating the tcl\_obj). I'm doing research projects. Also experimentation with SIMD and auto vectorization.
Well, if you look at e.g. \`@edit Base.cconvert(Ptr{Cdouble}, rand(3))\`, you'll see that it's just the identity function (this call will be optimized away). But \`ccall\` actually calls \`unsafe\_convert(argtype, cconvert(argtype, argvalue))\`, and the relevant method of \`unsafe\_convert\` is not completely trivial: &amp;#x200B; \`\`\`julia unsafe\_convert(::Type{Ptr{T}}, a::Array{T}) where {T} = ccall(:jl\_array\_ptr, Ptr{T}, (Any,), a) \`\`\` &amp;#x200B; That said, it's still very fast: &amp;#x200B; \`\`\`julia julia&gt; using BenchmarkTools julia&gt; @btime Base.unsafe\_convert(Ptr{Cdouble}, Base.cconvert(Ptr{Cdouble}, x)) setup = begin x = rand(3000) end 1.381 ns (0 allocations: 0 bytes) \`\`\`
I was wrangling with packages in JuliaBox a month or so ago. The main issues are that (a) they use a locked-down version of the standard registry (or METADATA, in the case of 0.6), and (b) the whole free service tier is being deprecated. In terms of alternatives for running Julia without a hassle, you could try (a) using a JupyterHub service like [this one](https://pims.syzygy.ca), which is available to anyone with a Google account; (b) using Google Cocalc (you have to run a few commands to install Julia to it, but it works fairly well after that; the commands are in a discourse post I could link you); (c) running a local Docker image, or (c) doing stuff locally on your machine. There are some good [lecture notes](https://lectures.quantecon.org/jl) that describe the setup process. Also, I would recommend moving to Julia 1.x if at all possible. It's where all the action is with all the new infrastructure and speed gains. And in terms of package support, with v0.6 you're basically stuck with maintenance on existing packages (maybe), and even then nobody is prioritizing v0.6 support AFAIK.
Thanks heaps. It's a project at uni and for the subject we're stuck with 0.6.4. I emailed the lecturer and he said to try JuliaPro, which is downloading now, but if I'm still having problems after that I'll try those. Thanks for your help.
No problem. JuliaPro I think also uses a nonstandard set of packages (or it bundles the packages inside the application, or something...) So we didn’t use it for our Julia v0.6 lectures. I think the cleanest solution will be to download Anaconda Python, download a copy of Julia v0.6 from the official website, and do most of your work inside a Jupyter notebook. Anyways, all the best, and feel free to report back with any technical issues.
Maybe, but I think if my lecturer recommends Pro it should have the packages I need. Thanks again.
Yeah, Atom + Juno is as close to an “official” IDE as Julia is going to get (by design.) Let us know how the setup goes, if you have any questions, etc. If you’re just getting started, I also want to point out the wonders of Revise.jl, which lets you make changes to a module/package in an editor, and have them reflected in the REPL in real-time. That way you don’t need to stop, include again, etc.
You are not converting the array per say, you are only getting the pointer to the data (from `unsafe_convert`) which is, for all intents and purposes, free: ``` julia&gt; @code_native Base.unsafe_convert(Ptr{Float64}, rand(5)) .text ; ┌ @ pointer.jl:65 within `unsafe_convert' pushq %rbp movq %rsp, %rbp movq (%rdx), %rax popq %rbp retq nopl (%rax) ; └ ```
Okay, thank you both (also other comment) for the excellent answers. I was fearing that some costly operations would result. But since you are the expert ;) Can two LLVM-based languages (rust, numba, etc), call each other even quicker in theory? Via llvm-call-conventions?
Saw this package a little while ago: https://github.com/ksteimel/ParallelComprehension.jl
Come on, the python syntax is the easiest in the world, it's like english! &amp;#x200B; It'll be harder to train people to use Julia. Go with python, that's my advice. &amp;#x200B; You might also want to have a look at Octave, because it can run matlab scripts. &amp;#x200B; \&gt; Have you successsfully implemented Julia in your work environment ? &amp;#x200B; No. &amp;#x200B; \&gt; Do you think Julia is still a good idea at this scale ? I was hesistant before, since 1.0 was not out yet. &amp;#x200B; Yes, Julia is good, but you'll get things done faster in python imho. If later you need to squeeze out more speed, use Julia, but for 99% of the time, python wil be much faster to get on. &amp;#x200B; [https://www.xkcd.com/353/](https://www.xkcd.com/353/)
I think one underrated aspect of Julia that could help here is that you don’t need tricks like vectorization or Numba to get good performance. For example, in Julia, loops are meant to be “fast” in their own right. I can’t speak to the rest of it, and of course Julia has its own flaws. Perhaps you could take a weekend and gradually scale up the demands on both languages, and see how they perform.
FWIW, I used it for all my computations working on my PhD (different FEM methods) and I had a lot of success.
If this website still holds true, I don't see that Python is the easiest syntax. Especially coming from Matalb. &amp;#x200B; [https://cheatsheets.quantecon.org/](https://cheatsheets.quantecon.org/)
Besides Julia Docs, you have: + [discourse](https://discourse.julialang.org) \+ [slack group](https://slackinvite.julialang.org) \+ stackoverflow I think you'll be fine when it comes to user support. I've consulted these 3 forums and asked questions on all of them and my experience has been a 10/10. I got quick, thorough and friendly user support from a community that deals with the same kind of problems we deal with, numerical computing, so that has been a big plus for me. &amp;#x200B; Still, it's true that no other PL has better community than Python. Perhaps you could try to import some of your code into python and see if you're ok with the performance, and knowing that it won't get much faster unless you recode the bottleneck code into FORTRAN/C/C++. &amp;#x200B; Good luck!
Do you use any MATLAB toolboxes? What kind of libraries do you think you would rely on in Python?
Performance is not the main issue for us, as Matlab is currently adequate. Are there more advantages to Julia compared to Python except speed?
There is development time and there is computation time. If your computation is easy to express in NumPy, then you don't have to make this trade-off in Python. Otherwise, you do! I also disagree that it's generally faster to develop in Python (though of course I agree to some extent) Julia code seems to be more easy to compose than Python code. If you need to perform a computation in single precision, or arbitrary precision, or for your own special kind of number, then Julia will blow Python out of the water both in development time and computation time.
We don't really use toolboxes. I would use statistics, but we don't have it. From my limited understanding, we'd need at least Numpy to go around.
Heavenly metaprogramming capabilities is the other big thing Julia has going for itself. But, from what you've been saying, your workplace wouldn't benefit much from it.
Do you have any documentation on metaprogramming? I've tried understanding it from the Doc, but I didn't really see how I can use it.
migrating to julia is superior to migrating to python. the learning curve will not be any higher, but you get much more bang. i first learned julia then python, and the lack of features is painful, it is a step back. migrating from matlab to julia ... depends. you really need to consider what you win and how valuable it is to you. julia is, in my view, simply better. but of course there is a cost of adapting something new, and if you don't need the extra, why pay the price? like using a porche to by groceries on weekends. why don't you try? julia is free, pick a small test project, and see how it goes.
The administrative machine is quite hard to move at my workplace. It is already a miracle that we can change Matlab. I'll definitely try both tho.
Not really, Julia Docs + some googling was enough for me. Perhaps you don't need it and that's ok. If you're using Julia in your free time, just know what metaprogramming is good for and keep coding happily. Soon enough you'll find a use for it, and later, you'll find yourself using it again and again, until macros and `eval()` are the hammer and all you see are nails :)
The main use of metaprogramming would be to create domain specific languages that more closely map the code you write with the concept you're trying to encode. For example for circuits, Modia.jl closely approximate the syntax of circuit simulators: https://github.com/ModiaSim/Modia.jl/blob/master/examples/CauerLowPassFilter.jl It's also used in many dataframe manipulation tools to create more succinct syntax: https://github.com/JuliaData/DataFramesMeta.jl I'd say it's more intended for package/library developers to provide nicer ways of using their functionality and for avoiding boilerplate (for example, if you have to do something you have to do repetitively you can just write a code that generates the code for you).
well, there was a guy at nasa, who rewrote the entire flight control system on his own, and carried it around in his pocket on a floppy. it performed well in simulations. they hated him :)
&gt; if you don't need the extra, why pay the price? This is true, except MATLAB is a large expense, and Julia is free `:-)`. Of course, in bureaucracies this sort of thing is often reversed (you need to use your whole budget, cutting costs suddenly can make admins look stupid, etc.), but...
If you don't rely on a lot of toolboxes and functionality that you can only find in Python/Matlab, I would recommend Julia. It is closer to Matlab in syntax, in particular, linear algebra syntax is far superior to Numpy, which suffers from being 'bolted onto' the language, and is very verbose and a bit awkward. Automatic broadcasting with dots is brilliant, and means you no longer need to do special implementations of functions for arrays. This is a *big deal*, and makes code easier to read and write. Careful use of unicode identifiers can *significantly* improve readability of your code. Multiple dispatch was a revelation to me, coming from Matlab/Python, and leads to clean and beautiful code. No need to do a lot of input parsing anymore! (This should really be on top of the list) Performance is excellent, if you need it, but it's definitely not the biggest selling point in my opinion. Don't worry about metaprogramming. That is highly advanced stuff that you may never need. It's really cool, though.
I actually wrote that cheatsheet, glad to see somebody's using it "in the wild!"
I'm going to disagree here, because this seems to be written more on "gut feeling" than on empirical fact. Why will it be harder to train people to use Julia? The syntax was designed with MATLAB emigres in mind (one-based indexing is an obvious example.) Python excels at many things, but syntactic neatness isn't one of them (in my opinion, at least.) Also, I think Octave is absolutely the wrong approach. If you're deciding to ditch MATLAB for an open-source language (OP mentioned limited licenses), you should do it right. Octave is great at what it does (mimicking MATLAB), but inherits all the flaws in the original language's design, as well as any of its own. To be clear, I'm not saying "use Julia or bust." There are real drawbacks. But this post strikes me mainly as spitballing/FUD.
Thanks, it seems we have the same mindset on Julia.
I guess I'll add another vote for Octave since you don't wish to retrain people, you don't have issues with performance and doesn't use the toolboxes. Julia out of the box is certainly more intuitive for scientific computing, you don't have to vectorize everything, closer syntax to matlab (including stuff like 1-indexed arrays), you don't need to learn a language and an environment at once (the python language plus the numpy + scipy/sklearn environment) to properly start and it's also easier to extend (since you don't use blackbox libraries written in different languages). As a subjective point, I find Julia more fun to code in. On the other hand, Python is a market standard (the modern "nobody ever got fired for choosing IBM"), and in a team of 100 people there are certainly more than a few that already knows it to some extent. And although it's definitely hard to extend (using normal python tools will probably be much slower than Matlab who is optimized for that usecase), it's very likely that whatever you do is already covered in the primitives of each of those libraries (though the Python environment is really really strong in data science and machine learning, but not as good in other areas of scientific computing, including statistics, though both Julia and Python aren't as mature as R in that area).
It's free! (like you said you have limited licenses). The upshots compared to Python for Matlab users is the 1-based indexing, the "end" statements for control flow, the support for matrix math without needing imports (you do have to import the LinearAlgebra module for some of it, but that's part of the standard Julia library), and the great Juno IDE, which is designed to be similar to the way Matlab is set up. Of course Python has Spyder, but in my view that's been a bit buggier and less cooperative (it may have improved though).
Not having issues with Matlab performance does not mean you won't have it with Octave! Octave lags far behind Matlab, both in features and *especially* in performance, AFAIK. Octave is *literally* a poor man's Matlab. It's a knock-off of a language which is itself lagging behind in features and power. The best feature of octave is that you can run (some) Matlab code without a license, but moving to it to do development means you are setting the bar low, and really digging in on not learning anything new, at all cost.
I personally moved from Gauss to Julia. I am an academic, economists. I don't code that much, but every once in a while there is a project that needs numerical simulations. Beyond the reasons you cite for moving away from Matlab, there was the added reason that Gauss is definitely not widely used anymore in my profession. The learning curve was minimal. The one tricky thing about Julia is to understand the scope of local variables. Beyond that it was smooth. I did spend some time learning some Haskel as well, but that was more for my recreation. I probably become a better programmer because of that Haskel experience, since I lack any formal training in programming: I grew up in the eighties with Basic, Pascal and assembler. I did consider Python, but the speed of native Julia was a major plus. I have not learned any Python yet, but it is definitely in my plans to learn to basics at some point.
By the way. If you are mainly doing bog standard linear algebra, summing, filtering, statistics, etc, don't expect Julia to blow Matlab out of the water, performance-wise. Matlab is getting very very fast at those core operations. Faster than Python. Sometimes faster than Julia. Julia really shines when you make your own algorithms, iterative or a bit complicated, hard to vectorize, using your own types, etc. The main strengths are not performance, but the other features I already mentioned, plus several other.
Here's the thing. There are still a lot of packages missing in Julia that are available in other languages. Like, some days ago, I was trying to do ordered probit and though it's easy to implement on your own using maximum likelihood and optimization, there simply wasn't a julia 1.0 package around it. But I didn't have to worry. I simply used RCall.jl to use my familiar R function in Julia. There's also PyCall.jl and one for Matlab too. Julia has the added benefit of superior workflow imo and things like latex to unicode in the REPL made using SymPy.jl with greek symbols very clear. Overall, Julia is modern and full of features besides simply being much faster than the alternatives. But the community is only growing and it will take some time until everything with it is pure Julia as more people start contributing to the languagage. But that is not my worry thanks to the aforementioned bindings.
&gt;Matlab is currently adequate Are there really no performance issues? What if your code could run 10 times faster? What if 100 times faster? I'm not saying Julia will do this, but that "currently adequate" doesn't necessarily mean everything couldn't be better... and you don't know what awful performance you'd been living with until you're no longer living with it... Recently I rewrote a job which ran a few times a week that was taking 2 hours (in java) to 1 minute (in julia). No one thought the two hours was an issue (I didn't, I was solving something else and happened to overlap), but now it can be run trivially locally and more frequently.
Of course. I was just stating that we do not change for performance reasons.
I definitely think Julia is a good choice. Why don't you pick a specific bit of Matlab code and port it to Julia? You mentioned a limited number of licenses, so pick a chunk of code that would be useful for a wide number of people. If you get buy-in on that code, then try another chunk of widely-useful code. Rinse, lather, repeat and soon enough you'll have made the transition and will not need to buy any new Matlab licenses.
Can you explain what multiple dispatch is? I read a definition but still don't quite get it
For example in normal Object Oriented Languages, when you call obj.method(args), the compiler will check the list of methods in obj and then choose the correct one to dispatch. That's single dispatch, and obj is clearly a special argument. If you want to have two methods obj.method(obj2) and obj.method(obj3) then you either have to decide inside of obj.method (use a conditional checking arg and then dynamic dispatch into the correct version of 'method') or use some ugly patterns (https://en.wikipedia.org/wiki/Visitor_pattern) Multiple dispatch languages don't consider the first argument special in any way, every argument will be used to decide which function to call. So for example, if I want to implement complex number sum, in multiple dispatch you can +(z::Complex, w::Complex) = Complex(real(z) + real(w), imag(z) + imag(w)) +(x::Real, z::Complex) = Complex(x + real(z), imag(z)) +(z::Complex, x::Real) = x + z And it will always call directly the correct method. While in single dispatch language you'd have to define this method in the Complex object and in the Real object, and inside you'd have to check what type is the arg and choose the proper routine (adding both code size complexity and runtime complexity).
To be fair, some of the Python examples seem a bit unnecessarily verbose. For instance stuff like: def f(x): return x**2 def g(x, y): return x + 2 + y**2 x = np.arange(1, 10, 1) y = np.arange(2, 11, 1) Can just as well be written as f = lambda x: x**2 g = lambda x, y: x + 2 + y**2 x = np.arange(1, 10) y = np.arange(2, 11) etc. This is similar to blowing up its Matlab equivalent f = @(x) x.^2 g = @(x, y) x + 2 + y.^2 x = 1:10 y = 2:11 to function fout = f(x) fout = x.^2 end function gout = g(x, y) gout = x + 2 + y.^2 end x = 1:1:10 y = 2:1:11 (or something like that; sorry haven't written any Matlab code in a few years) That being said, the majority of examples seem fair on first glance.
I think Julia would be better than Octave for many reasons, but I don't think you deserved a downvote for your opinion. &amp;#x200B; I've used MATLAB, Octave, Python, and Julia, and I prefer Julia. Python would be my second choice. It's not about syntax at all, if anything, I prefer Python syntax. Julia is just far superior and getting better fast.
Thanks, though I after I read the comment I actually agreed with DNF2 in that using Octave is definitely not forward thinking and I was thinking only the short term gains. At best it should be used to keep old scripts/programs usable as reference for debugging the reimplementantion (or for using those who are not frequently used and not worth the time to reimplement).
I get what you mean. Althought, to me, syntax like np.arange(1,10) is still weird and less approchable than 1:10. I can only imagine how much a larger code will look like compared to the same thing donenin Julia.
100 % agreed, the fact that everything Matlab'y is more or less bolted on takes some getting used to. I moved from Matlab to Python when Julia was very much in its infancy and definitely had some trouble adjusting. Love and use Python for pretty much everything nowadays, but tbh if I had to make the same decision in 2019, I'd likely choose Julia instead of Python now. It's probably much easier for a Matlab user and seems to have matured a lot since back then (haven't tried it for a long time, but I'm still interested -- just don't have the time to really dive into it right now). Good luck. I don't think you can make an objectively wrong choice here.
I think it would be perfect for you, especially if you're mostly doing vector operations, statistics, and functional programming operations (map, filter, reduce). &amp;#x200B; As someone who has recently switched from python, I will say two things stand out for me. 1. Having the ability to enforce types when you need them and ignore them when you don't is oddly beautiful. I can't give a concrete example of this, but I think people who are analytically minded will be able to appreciate it when they use it. 2. Others have mentioned multiple dispatch, and I think this is a glorious thing. To me, my code feels cleaner and more logical. I can give an example of this. Suppose I have an array of structs (objects) that all have a fit function associated with them, and within these structs, there are properties that determine the type of fit &amp;#8203; function fit(arr::Array{MyObject,1}) # expect a one dimensional array of my object for obj in arr fit(obj, obj.property) end end function fit(obj::MyObject, property::Type1) #fit object with logic based on property of type1 end function fit(obj::MyObject, property::Type2) #fit object with logic based on property of type2 end function fit(obj::MyObject) #generic fall back fit, for when the property doesn't matter end Normally, you might have a single fit function with a loop, and within that loop you have an if statement or switch/case statement based on the property. Maybe you also have some specially named functions like type1fit() and type2fit(). &amp;#x200B; Here, because you're defining the fit functions based on the object and the property type, your logic stays separate and clean, AND if new properties are ever introduced, you get an error based on not defining a fit function to match the new type. &amp;#x200B; Additionally, you don't need to come up with silly names for a common action. If what you're doing can adequately be defined by fit(), there's no need to do type1fit() or genericfit() and so on. All of your fit behavior can be defined in one spot. If something goes wrong with a fit, you know exactly where it's at.
I’m basically a programing newbie, so my brain still can’t compute. Can you ELI5?
Imagine Lion and Deer, both subclass Animal. You have a method meet(animal, animal) which defines their behaviors. In a pseudo single dispatch language (typed and with method overloading, such as C++) you can define: Lion.meet(Lion) = play(Lion, Lion) Lion.meet(Deer) = eat(Lion, Deer) Deer.meet(Lion) = eat(Lion, Deer) Deer.meet(Deer) = play(Deer, Deer) animal.meet(animal2) # both are type Animal So if animal is a Lion object then it will call the Lion.meet() method, and if it's a deer it will Deer.meet() method. But to select between which Deer.meet() or Lion.meet() that language will have to ask every time (at runtime) if the argument is a Lion or a Deer, since it only knows it got an Animal (or in C++ a pointer to an Animal). Multiple Dispatch language find when compiling which of the methods you want when you call meet(animal, animal2) and for example if you call it with animal = Lion and animal2 = Deer, it will replace it at compile time and simply call eat(Lion, Deer) directly. At surface level it seems just like an optimization (and it is), but since it's so cheap in Julia to exploit it that it builds into a full programming paradigm: meet(Deer, Animal) = run(Deer) # better not take chances meet(Animal, Deer) = run(Deer) # no matter who meets who meet(Deer, Deer) = play(Deer, Deer) # but let's make an exception for other Deers
You're lucky you work in an area where Python has decent libraries. That's not true for all of Python, and actually writing and maintaining a Python library is quite difficult, whereas writing library code for Julia is straightforward.
If you're in engineering and familiar with Matlab, then Julia is a much better pick. Python is going to annoy you with the numpy wierdness and the 0-indexing. Everyone is going to have to learn a lot of new stuff and not have fun while doing so. Julia multidimensional arrays are built-in and act a lot like Matlab arrays. Along with its speed and other factors, you'll do a lot more actual work and a lot less "programming".
Lol. I searched high and low for something good after doing a project in python where I ended up using numba, and even that wasn't fast and the code looked terribly messy. After I discovered Julia, no looking back.
You can't blanket dismiss multiple dispatch until you have used it. You have basically no idea how much more useful it is than regular OOP because you have never really out any effort into learning it. Seems to me like you're stuck in a local minima and can't see over the mountains around you.
The genesis of Julia memes. I love it.
Very similar experience. Attempted to write a sparse SDP solver in Python. After the mess that was dealing with Matrices vs. Arrays in SciPy vs. NumPy along with the sparse libraries and the conversions you had to perform all the time in order to even get things to work (not to mention, the time lost chasing bugs due to the inconsistencies between Matrices and Arrays/writing C to interface with Python to make things run at reasonable speed...). Yeah, absolutely not.
You gotta start somewhere ;)
I think we can go simpler than this. Let’s say you have a function foo. It does different things based on the types of the input (that’s dispatch). But we care about all the inputs, not just the first one (that’s multiple.) Basically, I can have foo(Float, Integer) and foo(Integer, Float) and whatever else I want, and it will intelligently look for the most “specific” method I’ve defined. tl;dr dispatch chooses behaviour based on arg types, multiple means we care abt all args
For people I work with the last panel is "I'd rather use fortran", at which point you give up.
Mechanical engineers?
I want to use/like Julia, but there's very little support for it. Sure, there's documentation, but I'm looking for something a bit more incremental and user friendly. Training, not just documentation. The Julia foundation constantly e-mails me trying to sell me training, which just feels like a scam -- they want greater user adoption, but want to charge you to get up to speed with it. So I keep using Python.
I'm at the same place the op is in trying to choose between Julia and Python... but coming from IDL. All the technical arguments aside, for me it may come down to the fact that I find python syntax to be hideous. Not quite java level ugly, but still very hard on the eyes. Ymmv
Actually, the syntaxes are not equivalent. f = lambda x: x**2 creates an anonymous function, while def f(x): return x**2 creates a proper function. It's not the same. Both the Julia and the python examples show proper function definitions, Julia using the short form. The Matlab examples show anonymous functions, but that is probably because they *have to.* A severe limitation in Matlab is that functions need to be defined in a saved m-file(!!!). In fact normal functions need their own dedicated file with the same name as the function (sigh.) This is a serious annoyance, *particularly* for interactive work.
&gt; They're not the same. In what way are they different, though? Honest question. I don't know. Looks the same to me: &gt; f1 = lambda x: x**2 &gt; def f2(x): &gt; return x**2 &gt; import dis &gt; dis.dis(f1) 1 0 LOAD_FAST 0 (x) 2 LOAD_CONST 1 (2) 4 BINARY_POWER 6 RETURN_VALUE &gt; dis.dis(f2) 2 0 LOAD_FAST 0 (x) 2 LOAD_CONST 1 (2) 4 BINARY_POWER 6 RETURN_VALUE Also both type `function`. What makes it different from using Julia's short form? Re: Matlab functions: Haha, yep... I still remember how annoying that was.
There's a good amount of resources listed on the Learning webpage: https://julialang.org/learning/ These include free Youtube tutorials (https://www.youtube.com/watch?v=8h8rQyEpiZA&amp;t), Jupyter notebooks, and free books (e.g. https://benlauwens.github.io/ThinkJulia.jl/latest/book.html). Not sure what more you want...
Hey, wanna learn a secret? *LET PEOPLE USE WHATEVER LANGUAGE THEY WANT* Or am I just taking a meme too seriously, which I am, lol.
Yeah. This was very much in jest :) I still use Python, C, and C++ for a bunch of things (embedded systems, graphics stuff, etc). Though I have slowly been moving to Julia in the things that I can, there’s a place and a time for everything!
I’d have guessed physicists, but that’s when you know it’s game over... (I work in a physics lab, so I guess I can say these sort of things? ;)
Could you please post the error message
Yeah! Resolving package versions... ERROR: Unsatisfiable requirements detected for package DebuggerFramework \[67417a49\]: DebuggerFramework \[67417a49\] log: ├─possible versions are: 0.1.0-0.1.2 or uninstalled ├─restricted by compatibility requirements with Atom \[c52e3926\] to versions: 0.1.0-0.1.2 │ └─Atom \[c52e3926\] log: │ ├─possible versions are: \[0.1.0-0.1.1, 0.2.0-0.2.1, 0.3.0, 0.4.0-0.4.6, 0.5.0-0.5.10, 0.6.0-0.6.17, 0.7.0-0.7.15, 0.8.0-0.8.5\] or uninstalled │ └─restricted to versions 0.7.15 by an explicit requirement, leaving only versions 0.7.15 └─restricted by julia compatibility requirements to versions: uninstalled — no versions left
I used Java for my undergrad computational physics class, used Python for my senior research, but now I don't want to use anything other than Julia if I don't have to.
 ] rm -m DebuggerFramework
That's not "the Julia Foundation", there is no such thing. That's Julia Computing. You signed up for a company's newsletter, and yes they advertise on their newsletter.
Yes, the newsletter, but also a lot of "Hi, I'm Sanjeeb, and this isn't a newsletter; I just want to sell training to you. There is no valuable information in this e-mail whatsoever -- it is solely an advertisement."
Then unsubscribe. It's not that hard
Thank you!!
It's (personally, at least!) a little hard to answer this question without more data (pun only partially intended). What do you mean when you say "big data"? In my case, I've heard of a ton of people talk about it, but almost nobody has anything that wouldn't fit (and work) perfectly in a machine with 32-128GB of RAM and a decent 8 or 16 core processor. So, unless there is a use case in which terabytes of data are being generated and constantly queried, I think a lot of these distributed techniques are overkill for most tasks. On the other hand, Julia has joined the [Petaflop club](https://www.youtube.com/watch?v=uecdcADM3hY), so it's probably worth watching at least the first few minutes of that video to get a general idea. In particular, I'm not sure that Julia is going to be strictly better than extremely well-crafted C code, finely-tuned for the system it's being used on. But then again: when was the last time you saw even well-crafted C code? I'm being a tiny bit facetious, but I think that (a) specifying a problem correctly is important and (b) the productivity boost of Julia as compared to a well-tuned implementation on a low-level language, along with the general maintainability of the code, is far more than enough to make it worth using, even in the cases where it's not *strictly* the fastest option.
The package system is currently transitioning to a new way of registering package and managing dependencies, so these "Unsatisfiable requirements" problems might come up again, depending on what packages you install.
Looks good to me, my only advice is to add travis CI.
Thanks! That makes sense
At work at the moment our data engineer uses pyspark and I write a lot of Julia. In my experience, Spark is massive overkill for what most people are doing with it. Even with startup time/compile time, I can build stuff that will tear through datasets faster than Spark can. For me, the advantages are: - equally high performance - more elegant and flexible code - language features that the JVM or Python is simply incapable of replicating: Julia’s language-wide support for Automatic Differentiation for one. - Sure, you can write Python on Spark, but you’re not really running Python-you’re orchestrating Java. So when something throws an error, you get a big stack trace in Java. I hope you know Java. - You can only run the packages/libraries that are designed to run on Spark. Did you need to use a library for something and there isn’t an equivalent lib in Spark? Well, you are flat out of luck. You can certainly load up your own package, but it’s going to run in a single node, so you’ve pretty much lost the whole advantage of Spark already. - Notebooks, while nice to prototype on, are fantastic at encouraging all sorts of terrible programming/software engineering practices. I know people using Spark are probably more scientists and analysis, but learning and using good software engineering practices pays off 10-fold! - A guy called Frank McSherry wrote a paper called “Scalability but at what COST?” Where he introduced something called “configuration that outperforms a single thread” where for several very large graph processing benchmarks he demonstrated a better built solution running on a laptop drastically outclasses pretty much every cluster solution. People don’t realise just how much you can squeeze out of a single machine. - In my experience Spark doesn’t play nice with things outside Spark: I can’t call data jobs or results written inside Spark the same way I’d call any other service in our environment. Comparatively, my Julia code that runs in containers in our Kubernetes environments can be called and used like any other dev service. It’s also deployed like any other dev service, unlike the Spark jobs, which as far as I can tell are deployed by hand.
Just out of curiosity, what format of data (cvs, sql table, etc) do you work against in Julia?
I think that's missing (part of) the point of HDFS-based technologies. One of the ideas behind Hadoop is to move the computation to the data. This helps you avoid a lot of the costs involved with copying massive datasets around, and it also allows you to use commodity hardware. You're totally right that what you can do on a single machine keeps getting better and better, and computationally speaking there's very few applications that *need* a cluster, but there's still some sense to using Spark.
I do a lot if my work on Jupyter notebooks. Why do you think they are a bad idea? I am asking seriously, because I am new to Python (I come from a MATLAB background) and if something I am doing is not good - I can change my habits.
Looks like Julia data frames have some work to do to keep up with data.table for those types of data tasks. https://h2oai.github.io/db-benchmark/
Some CSV, some SQL tables, some raw. Most of my work is done on CSV’s, but that’s just because it didn’t live in a db in the first place (I end up sourcing a lot of the data I work on myself). I am planning to migrate more stuff to sql tables/a db soon though.
Their flexibility is a blessing and a curse: the out of order execution is handy when testing things, but can make it difficult to debug and maintain later on. Ever had to deal with notebooks where there’s notes everywhere like “don’t run any cells below here”, or “have to run these cells out of order/twice/etc” because somewhere along the way, some kind of magic occurred, or some cell put some logic/state change into effect and was then written over and now the whole thing is impossible to deal with. I’ve dealt with notebooks like that, some of which I wrote myself and take full blame, others I’ve inherited; trying to debug them and figure out what’s gone on is the worst. Other cases I’ve seen are people reimplementing logic in different cells, or doing things in the most confusing possible way. Problems that would be avoided if they used a script, broke things into small, clean functions (separation of concerns, DRY, etc). Not to mention all the normal things like global/config variables being defined and redefined in a dozen different places (makes it hard to debug), imports being used all through the program (what dependencies does this have? Who knows, you’ll just have to keep running it until it stops crashing? Were they using a now API-incompatible version? Who knows, because their dependencies weren’t listed or locked, so now you’ve got to play dependency roulette across a dozen different versions of some package). Notebooks also don’t play nice with version control systems: trying to do a `git diff` on something that’s mostly just notebook structure code and not your actual, project code is nightmarish. They’re fine to prototype with: but as soon as your done with protoyping, you should write it up into a script/project. Future you, and future co-workers will thank you.
Oh no, I completely agree! Just that it’s, I think, surprisingly rare to actually need Spark for most applications I’ve heard of (and most people that told me they use it absolutely didn’t need it).
I've been using JuliaDB to do the same thing as Spark, and it seems to be working ok. The data is spread out over a bunch of machines, and the code is pushed out to them. However.... We lose a machine, and the whole thing fails, unlike spark. However, it is FAST, like CRAZY fast.
I’m not sure that’s an entirely important comparison, dplyr, pandas, DataFrames are not really written for speed - which is why data.table is so remarkably fast compared to them. A more relevant comparison is between those other packages - it could do with some performance boost there, albeit it doesn’t lag anywhere near as much behind them. data.table is amazing, though.
Big data is no longer just size. Volume, Veracity, Velocity, Variety There's also Value and Validity Some people even add Visualisation and Virality
I have thought of using JuliaDB as a spark alternative, too. Although data manipulation is parallelized over the cluster, how about ML? Do the algos in onlinestats package scale as well as spark applications? In search of a julia spark alternative, I found the no longer developed htpat [https://github.com/IntelLabs/HPAT.jl](https://github.com/IntelLabs/HPAT.jl) (although there is a still developed python version). Still this tool is not as feature rich as spark. Is there anything else?
&gt; Do the algos in onlinestats package scale as well as spark applications? Yeah, online stats is the gold standard to be honest. It works super well.
Not a profound comment, but I'll say that this is a contender for the best package name ever... :) Better even than the power-modeling package [Joulia.jl](https://github.com/JuliaEnergy/Joulia.jl)!
We don’t have Yarn, but Kubernetes will do a fantastic job keeping a container/set of containers alive (along with numerous other advantages). Could be worth giving that a go?
Could do, I'll give it a go over the next couple of weeks, and see how far I get.
Can you give more context to these words? Some I get some I don’t
One reason performance matters, among several, would be your cost of compute time if using a cloud service. They list those numbers at the top of the image. Compound those costs for large scripts and it can add up over time.
Can you explain how a failing system occurs and how it’s a problem for you and Julia?
 The first concept is the hierarchy of data: Data -&gt; Information -&gt; Knowledge -&gt; Wisdom A date scientsist's job is to add value by climbing the hierarchy. Back to the Vs If one considers other factors rather than the static size of the dataset for instance a stream of live data where you might want to calculate the moving average - you can easily keep the live set in memory but if you're getting new records quickly then that became considered "big". On top of that - the multiplicity of data types - images, video, audio, etc. e.g. the Twitter feed So to encapsulate the concept "The three Vs of Big Data" were proposed. Volume, Velocity and Variety. Marketing already has the 4Ps (Product, Price, Place, Promotion) so this was an echo of that. Then after a while, it was expand to 4 - Veracity. The real world isn't so kind as to provide certainty. Exceptional values we can discard, known unknowns and unknown unknowns all play a part. So that became the 4Vs. https://www.ibmbigdatahub.com/infographic/four-vs-big-data Some folk then added Value as number 5 https://www.ibmbigdatahub.com/blog/why-only-one-5-vs-big-data-really-matters It's getting a bit out of hand now. Here's [the 7 Vs](https://impact.com/marketing-intelligence/7-vs-big-data/) Volume, Velocity, Variety, Variability, Veracity, Visualization, and Value. Or try [The 10Vs](https://tdwi.org/articles/2017/02/08/10-vs-of-big-data.aspx) Volume, Velocity, Variety, Variability , Veracity, Validity, Vulnerability, Volatility, Visualization, Value The Marketing people stuck on [The 7Ps](https://marketingmix.co.uk/content/uploads/marketing-mix-7p.jpg) - Product, Price, Place, Promotion, Process, People, and Physical evidence or maybe 8 - Product, Price, Place, Promotion, Process, People, Physical evidence, Performance
Thank you!
Of course, I didn’t say performance doesn’t matter. Indeed, in such cases data.table should be the go to way of doing it. My point is simply, DataFrames, dplyr etc are not really built with performance as their first priority, so it’s not really fair to highlight only that point in a comparison of any of them with data.table.
With Julia, isn't performance one of its main selling points?
That’s a bit of an oversimplification. Yes one of Julia’s selling points is the speed of the pure language. So is ease of use and style. The language is particularly aimed at data scientists, therefore the suite of packages naturally is written in a way that they find useful. Sometimes that’s not always speed. DataFrames, like data.frames, dplyr et al is doing lots of things in the background that uses up computation for the benefits of ease of use. That’s a conscious decision by the package developers for data scientists who don’t want to have to worry about too much fiddling around with type coercion themselves, or who want certain features data.table can’t do (at least not easily). Look at it this way, pandas is written in C to be as fast as it can be - so why isn’t it as fast as data.table? Because it’s doing more in the background - by design. They’ve made it as fast as they can - in the context of all the other things they want it to be doing. data.table prioritises speed over some of those things - e.g. readability, flexibility, etc etc. That’s why it’s not really right to compare them for speed - the packages have different design decisions. You need to compare the Julia equivalent of data.table to data.table for a fair comparison.
There is Julia the language, which focuses on performance with expressiveness, and then each third party library, which will be whatever their creators decide to provide. And data.table is written in C, so DataFrames.jl being in Julia won't give it any comparative advantage in performance.
There are some good reasons to still use Fortran over Julia. Julia compares better as an improvement over Python and Matlab.
Can you explain some of these behind the scene computation benefits? I'm a data scientist and I find data.table to be extremely easy to use without any shortcomings.
&gt;Suppose I overload show(), for one of my structs. How do I keep the drop down button in the workspace? Once, I overloaded show with some partial information from my struct that I wanted to be visible in the workspace, I lost the option to click the struct and see the remaining information. TreeViews.jl. It's not well-documented, but there are some examples in DiffEqBase.jl for how DifferentialEquations.jl handles it.
there's a \`tiedrank\` function in \`StatsBase\`
What do you do if it's [11, 4, 8, 8, 8]? Is it [5, 1, 3.3, 3.3, 3.3]?
If a machine goes down in Spark, since, the data is replicated by default to another node, Spark will rerun that part of the "query" on another box, and still get an answer. with JuliaDB it fails, and you will need to fire up another version of that box, with the same data as the old one on it. If I am using it to get answers, then, that is ok. If I am using it as a long running service for other systems to use, it is... less ideal.
Wouldn't it be [1,3,3,3,5]? The average of ranks 2, 3, and 4 is 3
Two obvious examples. The way type coercion is handled - or not. And then all the various things that allow a readable and flexible syntax. Some people prefer the concise data.table syntax, but there’s a reason why dplyr is so popular. The metaprogramming and lazy evaluation that allows the various functions like mutate, select etc to be used in extremely flexible ways.
The reason for dplyr popularity is likely due to the marketing efforts by Hadley compared to Dowle. With a growing number of folks looking to get into data science by learning R their likely source for information is the internet, where dplyr is blogged about much more frequently (most likely by newbies looking to gain exposure to help get them hired), which makes me think that referring to popularity is not a wise option when selecting a framework to learn.
Ok, so dplyr is purely popular because of marketing - despite all the extremely smart developers working on it - while data.table is better in every single way, and dplyr offers absolutely nothing a data scientist might find useful. They’re all wrong and you’re right. Thanks for clarifying that for me.
Nice, thanks for sharing.
I will take a look, Thanks!
I just found the answer to question number 1. Apparently messages can be suppressed using a semicolon at the end of a statement, just like in Matlab.
I think this would be a great discussion to take up on discourse ! How to improve it that is
Thanks. And that's a great presentation too.
It should contain, ideally, a step to apply PackageCompiler.jl to build a new “system images containing the packages. This is better than Julia’s base precompilation
If you're saying that in this particular example, you could use anonymous functions instead of regular ones, then I can agree. I thought at first that you were saying they are equivalent *in general,* which isn't right.
Thank you, I think I understand it a little better now. It’s probably one of those things that becomes apparent once you become a more experienced coder.
That was a little jargon-y and more like ELI(1st year CS student), but it helped. Thank you.
Yes, I was more focused in avoiding being innacurate then really explaining for someone who is truly starting. For multiple dispatch you need (Not ELI5 again, just giving a reference of the terms): 1) Types: Since you want to decide which method based on the type of the arguments 2) Method overloading: You need to be able to write functions with the same name for the compiler to decide which to dispatch/choose. 3) Polymorphism/Subtyping: If the variables can't hold different types (for example by having a supertype that can have methods implemented differently by multiple subtypes), then there is no real decision about which method to call. And the solutions are (ignoring double, triple dispatch which is the middle ground): 1) Dynamic Dispatch: The method is chosen based on the actual types the variable hold during runtime. 2) Single Dispatch: The compiler keeps track of the first argument of the call (the object) and decides at compile time (before running) which object's method to call, but any other overloaded method will need to be chosen through dynamic dispatch. 3) Multiple Dispatch: There is a type inference system that will anticipate before running all the types involved in the method call, and choose the appropriate method directly. It's definitely a concept that becomes more clear after dealing with object oriented languages (and even then, if you don't have contact with a multiple dispatch language you'll probably never realize that limitation of single dispatch). In general you can also learn Julia easily without knowing that other languages can't do what it can.
Try sortperm
Or was it permsort
It's pretty likely it wasn't recorded. The links for details of the talks are [here](https://arpa-e.energy.gov/?q=workshop/machine-learning-enhanced-energy-product-development), so it looks like it was given by Edelman in 2018. The issue is the video is nowhere.
The recipients of the Nextjournal Scholarship for Explorable Research have been announced. They span the globe and cross several scientific domains. Of particular interest to r/Julia, the first project, "Exploring and Statistically Learning an Excitable Stochastic-Dynamical Model" will be written in Julia. A fascinating group: [https://nextjournal.com/nextjournal/scholarship-winners](https://nextjournal.com/nextjournal/scholarship-winners).
It was last summer. It wasn't recorded. The video on modern software engineering referenced in the slideshow is this one: https://www.youtube.com/watch?v=X5SkW7K0e3Y
Thank you!
Actually now i have another problem.There are some Matlab functions such that colamd(),amd(),colperm(),symamd(),symrcm() and i am searching that functions equations of in Julia language. Are there any do you know ?
The [AMD.jl](https://github.com/JuliaSmoothOptimizers/AMD.jl) package might of interest to you.
For rcm see the discussion of this discourse thread: https://discourse.julialang.org/t/implementing-cuthill-mckee/9268/7
It's sortperm https://docs.julialang.org/en/v1/base/sort/index.html sortperm returns the permutation that would sort the array (equivalent to numpy's argsort).
Are the arrays in a container?
You can do it, but it's probably not the fastest way. https://docs.julialang.org/en/v1/manual/metaprogramming/index.html Expr = Meta.parse("A=1") eval(Expr)
something like?: for a in (a1, a2, a3) a .+= rand(0:5, size(a)...) end
You could probably broadcast the `push!` call over the collection for even more syntactic neatness.
So I don't think it is the best idea to statically name these arrays. I am unsure how you're creating A1 to A10, but it makes it harder to loop through when they're are all differently named. if you can elaborate on how you create these arrays that will probably help me come up with a better solution. As for now this is what I can suggest. &amp;#x200B; count = 10 #how many arrays you have A = similar(Array{Any}, count) #creates a structure similar to a cell in matlab A = A1 , A2 , A3 . . . A10 # and so on for i = 1:10 \#this last part is up to you A\[i\]\[x\] = #your random integer \# x is how you would index a specific element &amp;#x200B; if you just wanted to add a specific value to every value in the array you would replace that last line with &amp;#x200B; A\[i\] += x &amp;#x200B; again you haven't specified where you want to add this random integer or how so I cant really give you anymore specific help than this. If you elaborate more on what exactly you want to do I can give a better answer.
Metaprogramming can build the code before evaluating it: add_random_float(arr) = arr .+ rand(length(arr)) A1 = rand(10) A2 = rand(15) e = :() for i in 1:2 arr = Symbol("A$(i)") append!(e.args, [:(add_random_float($arr))]) end eval(e) You can see what's happening by running `e.args`: julia&gt; e.args 2-element Array{Any,1}: :(add_random_float(A1)) :(add_random_float(A2)) Alternatively, as others have mentioned, structuring your program differently can clean things up. The sample below would accomplish my float ambitions using broadcasting: A_arrays = [rand(10), rand(15)] add_random_float.(A_arrays)
I don't know how to answer this. There are many libraries in Julia; see if they exist for the problems you're working on. If they don't, either create them, let someone else make them, or don't use the language. There are likewise many jobs in Julia (I have one right now), but maybe they don't appeal to or target you. As for Julia being a passing fad, that's a strange question to ask. Nobody can predict the future. But it's a sophisticated and useful codebase, so I don't see it disappearing tomorrow. In general (and I can't put my finger on it exactly), I think what bugs me about this question is that it asserts three highly negative and unsubstantiated claims about a project, and basically says "prove me wrong." But Julia's viability isn't contingent on proving you wrong. The only way to answer these questions is to give it a go, see if you like it, and see if it's useful in your community for the things you do.
1. Is Julia a trend? I don't think so. It surely seems like it's growing to me, but not nearly the rate of python or javascript. Of course Julia only hit version 1.0 like ~9 months ago. So it's still a young language. Who knows what the landscape will look like in 5 years. 2. Julia is extremely readable and expressive. It's just as clear as python or ruby and in certain cases, a lot more readable. I think numpy, pandas, tensorflow and pytorch are an ugly mess compared to Flux. But only time will tell whether the community agrees with me. The bottom line is that any programmer worth their salt can get up to speed on a new language pretty quickly, and julia is not hard by any means compared to a lot of other popular languages. So the concern that your code wouldn't be communicable with other devs is a weak complaint I think. 3. Yea, you are correct that there's few jobs. Most Julia usage in the wild is in academia as far as I can tell. If you want to work with Julia professionally right now, you need to create your own position. Whether that be through consulting or doing research at universities or something more clever. Again, who knows what 5 years from now will look like. &gt; I already spend a lot of time in niche languages and the last thing I want is to invest more in something I'll never get to use in any professional context. I don't think everything has to be done with the sole purpose of getting a job. Why do you spend time in other niche languages? Because they're fun! I think Julia is fun and worthy of my time. Julia offers a simple, elegant and clean way to write math and scientific code, and IMO it is better than every single other language in this regard. Everybody praises Julia's performance, but I honestly think its killer feature is the language's semantics. Also, the Julia Pro IDE has been a really fun way to explore Julia in an easy "all batteries included" bundle.
Thoughts on each: &amp;#x200B; 1. Sure but hindsight is 20/20. There are plenty of languages that are flashes in pans. It's completely impossible to tell which flashes will turn into an open flame. Sometimes you just have to dive in and learn stuff. 2. This is patently not true in Python. Library operation is next to impossible without significant overhead in managing dependencies and input and output data formats. Also adding features to existing code to make it either more performant and easier to use is hard. Yeah there's a library for everything, but you're going to have to jurry rig the libraries to all work together and make it performant. My thing with Julia is I can actually *manipulate the code itself* with macros. With python, you're only stuck with the source code and the object API. With Julia, I can change a library at run time if I really want to. 3. Sure, but Julia just came out. In terms of lifetimes when comparing to other languages, Julia isn't on the map yet. 15 years ago no one was talking about Python as a mature language. 15 years ago, python was 15 years old. It takes a long time for a language to become prominent as a language you can know and get a job with. I don't even think Javascript is there yet. &amp;#x200B; Julia is fun for now. There's definitely a professional use case, but it's extremely small right now. Just dive in, commit, and work on some projects. You can't always learn for the sake of reward. Personally there's no professional reason for me to learn Julia, but I think the metaprogramming is state of the art and I want to learn more in that area, so I'm just doing it.
The whole question of whether Julia “will take off” relates the whole Zipf/Pareto phenomenon as illustrated by the paperclip experiment [in this VSauce video](https://youtu.be/fCn8zs912OE), relevant section starts around 12:41. The more people use it, the more popular it will get, the more even more people will use it.
My employers have never heard of Julia. But they get reports created by it every day.
Remember the times when all three points were true for Python?
It all depends on what you goal is; if your goal is to find a job tomorrow Julia is maybe not the best choice (note thought that there's also less competition in the Julia space, there's not that maybe experienced Julia devs). If you want to do develop relatively involved software then Julia is the choice. If you want just to apply already existing techniques it's less clear. If you want to learn Julia is the choice.
Same. I don't use it for everything, or even the majority of what I currently work on, but they're getting reports made in Julia that use models made in Julia. They neither know nor care.
Thank you for the in depth response! &amp;#x200B; \&gt; But it's a sophisticated and useful codebase, so I don't see it disappearing tomorrow. &amp;#x200B; I understand that Julia outperforms Python for a lot of mathy statistics stuff. For someone who doesn't care as much about performance, what kind of problems would you say Julia is particularly well suited to? &amp;#x200B; \&gt; But Julia's viability isn't contingent on proving you wrong. Of course not, I'm posting because I'm intrigued by Julia and feel like a lot of other Python devs are sitting on the fence, unsure whether to dabble in Julia or not (I am definitely going to try my next project in Julia though, I decided tonight. It's a perfect first Julia project anyway)
Yes. I was an early adopter of Python when it was the total underdog beneath Perl. I took the plunge because, having worked in Perl, I understood well why Python needed to win. I was a *believer* in Python.
As a Racket programmer I'm interested in what Julia has to offer in terms of metaprogramming. I'll look into that. Thanks!
When you hit a true speed wall in Python, you go to Julia because the libraries often have similar syntax. Python will always suffer from this and nobody in the stats space really wants to learn C. Thus, Julia will be a force for a while.
This is the most important thing - as managers find out that people are subtly replacing some things with Julia, they'll add that to the tech buzzwords they hire for. Only then will you see Julia in job descriptions.
For general purpose stuff, it can do just as well as Python can, with the added benefit of an improved type system and metaprogramming meaning you can do even more cool (and useful) things. One thing to keep in mind, is Julia is incredibly young compared to Python: Python is nearly 30 years old. Juli has less libraries, but so far that’s not been a huge issue for me, and I definitely see Julia’s popularity picking up.
\&gt; I understand that Julia outperforms Python for a lot of mathy statistics stuff. For someone who doesn't care as much about performance, what kind of problems would you say Julia is particularly well suited to &amp;#x200B; I'm in the same boat - the data I encounter at work usually fits cleanly in memory, and the tools made available to us are pretty much R and Python. But if given the opportunity I'd use Julia all the time, if not for the performance for the clean syntax. &amp;#x200B; For example, let's say `my_simulation` produces a vector of values of arbitrary length. I want to see what the capped distribution as a percent of the uncapped looks like over many realizations: using UnicodePlots realizations = [my_simulation() for i in 1:1000]; result = ( arr -&gt; sum(min.(arr, 0.25))/sum(arr) ).(realizations); histogram(result) [0.36, 0.38) ┤ 2 [0.38, 0.4 ) ┤▇ 9 [0.4 , 0.42) ┤▇▇▇ 43 [0.42, 0.44) ┤▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 547 [0.44, 0.46) ┤▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 354 [0.46, 0.48) ┤▇▇ 27 [0.48, 0.5 ) ┤▇ 9 [0.5 , 0.52) ┤ 5 [0.52, 0.54) ┤ 2 [0.54, 0.56) ┤ 1 [0.56, 0.58) ┤ 0 [0.58, 0.6 ) ┤ 1 └ ┘ Frequency I can definitely accomplish this same thing in R or Python, but Julia just feels easier and like less work.
As a racket programmer, I think you're in a better position than most to understand what Julia has to offer other than brute speed. Julia's macro system is less sophisticated than Racket's, but to be fair, so is everyone's. However, for the majority of purposes Julia's macro system is more than sufficient. Delving deeper into metaprogramming though, Julia has some capabilities that I don't think are around yet in the Racket world such as [Cassette.jl](https://github.com/jrevels/Cassette.jl). Julia also is built on multiple dispatch / generic functions as a core idiom. This allows for some amazingly expressive code and makes julia one of the only languages out there where code injection and re-use really 'just works'. I see you're concerned about julia's lack of libraries relative to Python. In my experience, this isn't actually a big issue because of Julia's impressive flexibility, composibility and speed, one will often find that you end up relying on built up infrastructure much much less. For instance, [Flux.jl](https://github.com/FluxML/Flux.jl) is a ML library that might make you scratch your head at first wondering where all the fancy stuff is that you're familiar with from the big Python ML frameworks. The difference is that after typing `using Flux`, julia is suddenly a machine learning language, instead of a language with a machine learning library. Flux integrates incredibly tightly with all the existing data-structures and functions (as well as those from packages that Flux knows nothing about!) in the language instead of reinventing a million wheels as is necessary in other languages.
Julia and Python are intended for two different purposes. Python is OOP. It is easier to write Python for many reasons and cases including not being strongly typed. Julia people will say OOP isn't all that but really it's a natural way to do many things, but not required in Python. Julia has the multiple dispatch and it's fast. It's better than C but comparably fast. It's not OOP. If you need speed among other things use Julia. python will struggle to process and display very large amounts of data in which case you will need something else probably...or it might be able to use something else rather than wrapping C plus plus or C code. Python is still very much popular than Julia which is a good reason to use it too. Collective productivity is important and when people are well-versed in a tool it's important that it is used collectively, and many libraries will probably exist in Python before they'll exist in Julia. It doesn't work for everything but it's still the most versatile tool in your toolbox. You're not going to use it to write database code that requires any type of performance or for embedded software though right?
Hot Take incoming: ML is more of a fad than Julia
As someone who loves Julia, Julia doesn't replace Fortran.
Are you talking about the term ML specifically? Because *maybe* that's true, but the techniques now classified as ML have been around for decades and will remain with us.
What I'm saying is that right now ML is a buzzword. I can't tell you how many pitches I've seen for things like "Machine Learning Structural Simulations", "Machine Learning Linear Solvers", "Machine Learning Coffee Maker", "Machine Learning Bed", etc. Sure, the mathematical foundations of ML are sound, but right now we're in one of the "bubbles" of AI/ML where [the current state of the technology is being oversold.](https://emtemp.gcom.cloud/ngw/globalassets/en/research/images/illustrations/researchmethodology-illustration-hype-cycle.jpg). I could say the same about blockchain... OP said, "I'm **getting more into** machine learning..." so obviously doesn't have a long history with ML, so he's probably one of the people getting picked up by the current bubble / wave of ML interest. Not that that's a bad thing, but it'd be disingenuous to call Julia a fad and not comment on the current ML fad.
Julia "metaprogramming" is a little different from Racket/CL (like a safer Ruby, even though it does have all the macro utilities, and it's (arguably) homoiconic, and the parser is actually a Lisp: https://github.com/JuliaLang/julia/blob/master/src/flisp/compiler.lsp which you can access using julia --lisp ). The everyday domain specific language creation in Julia is not done through macros, it's through the type system and multiple dispatch that creates the concept of specialization. Kind of a basic example of extending the language to support adding temperature as primitives: import Base.+, Base.promote_rule, Base.convert # Defining a type Temperature with 2 implementations each with one field (kinda like OOP): abstract type Temperature end struct Fahrenheit &lt;: Temperature temp::Float64 end struct Celsius &lt;: Temperature temp::Float64 end # We teach the language how to convert one into the other (just one way) convert(::Type{Celsius}, f::Fahrenheit) = Celsius((f.temp-32)/1.8) # If you have Celsius and Fahrenheit in the same operation, then we just convert it to Celsius promote_rule(::Type{Celsius}, ::Type{Fahrenheit}) = Celsius # Specialize the sum to add two temperatures (don't bother with the syntax) function +(a::T1, b::T2) where {T2&lt;:Temperature, T1&lt;:Temperature} a, b = promote(a, b) typeof(a)(a.temp + b.temp) end # Now everything in the language that builds upon the operator + automatically knows how to work with temperatures f, c = Fahrenheit(32), Celsius(10) f + f # Fahrenheit(64) sum([c, f, f, f, f]) # Celsius(10) f + 3 # MethodError: no method matching +(::Fahrenheit, ::Int64) So if you specialize a few basic functions, you can get a completely new behaviors from the entire ecosystem (like dual numbers for automatic differentiation, arrays that implements operations in parallel, or in a GPU, or in a TPU...). And that's Julia's greatest strength (and unique feature) in my opinion. By the way every type in the language (even Int and Float) are actually defined this way (just using primitive types instead of a struct type).
Let me start just by saying I appreciate your curiosity and openness! While Julia's biggest initial draw for people is usually its speed, it has a lot of other really nice featured too. Multiple dispatch is tremendously useful; it lets you easily use a single function to do multiple things depending on its arguments types. For instance, you could have one function, stats_sum() that outputs summary statistics for some list/vector/array of inputs, and define it for continuous data types, cardinal data types, and categorical data types. If you come across a type of data that needs some unique sort of summarizing, you don't need a new function, just an additional definition for that type. Idk if that's necessarily the best example of its powerfulness, but it's hard to come up with any good simple examples of it. I haven't delved into it myself, but from what I've read, the metaprogramming that's available is pretty amazing. If that's something you'd find useful, then it could be rather nice for you. I, personally, love its syntax; at least for the sort of stuff I do with it, it feels like a beautiful mix between Python and R syntax. The simple things are easy to do, like in R, and the more complex logic is as natural as writing in Python (and, of course, it's much faster than both). I mostly use R, but if there's a particular algorithm I want to use but can't find a decent package for, I've started to just write it up myself in Julia, rather than fussing around with R and trying to avoid loops. It's easy and fast to both write and execute. Really, I think the best way to figure out if it's worth it to invest in seriously learning the language is to try it out for a week or two, see how it feels, if it has the flexibility you need, or the style you like. It isn't OO, so it may feel weird coming from Python, but I think it's still worth keeping an open mind about.
Do you use Julia to create pdf reports? If so, what package(s) do you use? I've been looking for a Julia version of RMarkdown to build dynamic pdfs with
Sorry, no. Excel via XlsxWriter
https://phrb.github.io/2019-02-16-intro_parallel_julia/ https://docs.julialang.org/en/v1/manual/parallel-computing/index.html Though the compiler team seems to be currently focusing on Julia parallelism (should be much more powerful in the near future, but maybe different).
Agree. One more point. "Sure, the mathematical foundations of ML are sound, ..." - not even that =D. When you use ML - you hope that you will get correct/optimal result. There is no guarantee, yet people try to apply it to complicated nonlinear problems, because as you said - it is a buzzword. There is no math in "hope" =D
Many of us feel the same way about Julia.
Forgive me if this is a stupid question, but how would one go about using the LLVM IR directly?
Didn't distributed julia already kindda worked? I thought the shared memory parallelism part was the thing they were working on.
I found the thread I was remembering, and it seems to be about introducing nested parallelism and thread safety. https://discourse.julialang.org/t/compiler-work-priorities/17623 Distributed (as in clusters) doesn't seem to be changing (though I assumed the post was about parallelism because @everywhere).
Yeah, what I meant by that things like: "the tensor outer product **is well defined** and isn't going anywhere" and "linear regressions are well defined and aren't going anywhere." These are among the things at the mathematical heart of ML, but unfortunately they're often *misapplied* by overly enthusiastic, untrained humans.
[https://juliaastro.github.io/](https://juliaastro.github.io/) &amp;#x200B; that's probably a starter
I don't know your application, but Julia joined the petaflop club on an astronomical analysis: [https://juliacomputing.com/case-studies/celeste.html](https://juliacomputing.com/case-studies/celeste.html)
Doubt you could call it 'widely', but the use is growing. I 'made' my old university's astronomy department use it by writing all my code in Julia so now it's catching on with a few people. The best answer is that Julia could be widely used, if people (like you 😉) jumped into using it.
[JuliaObserver](https://juliaobserver.com/packages/Juniper) might help. Apart from the GitHub information, I don't think there are any stats available yet, but https://pkg.julialang.org/docs/ might do that one day. You could also use the Unix shell to find some stuff out. For example: cd ~/.julia/registries/General find . -type f -name "*.toml" -print0 | xargs -I {} -0 grep -l "Colors" "{}" ./R/RoMEPlotting/Deps.toml ./R/RoMEPlotting/Compat.toml ./R/RoME/Deps.toml ./R/RoME/Compat.toml ./R/ReinforcementLearning/Deps.toml ./R/ReinforcementLearning/Compat.toml etc might find most of the packages that reference Colors.
Thanks actually I saw that it's not that easy to get the test dependencies using the Registry. Some packages are using a solver for the test cases which is interesting I think.
Go take a look at [Nerd Fonts](https://nerdfonts.com/). They have a ton of options.
I don't know if I'd call Julia the best for either. In distributed computing there is for example erlang/elixir which has an extremely mature VM and base library focused on distributed computing and with an entire ecosystem that builds on it's greatest strength, and clojure with an equally mature VM and many of the same advantages (like pure functions and immutability making it pretty easy to work with the built-in concurrency tools). Though if we are talking about numeric processing, one Julia thread will do as much work as 50+ from erlang/elixir, and it does provide some easy to use tools for distributed computing. And remote deployment, having a single binary with no dependencies and a lightweight runtime seems like it would be the best, (or in the other side strong hot swapping abilities like erlang/elixir), or being one of the most popular languages, which would mean native support for most tools/cloud services. It shouldn't really matter on docker though, since you can have your own environment (and a long living process will help with the precompilation issues of Julia programs). What makes Julia great is the no compromise between development and deployment, in which you can have an iterative and easy to develop platform, as a state of the art dynamic language (which is very important when you need something built fast or it's something that requires more exploration like data science and scientific computing), but without risking having to rewrite later in another language because of performance (which means having readable and maintainable high level and high performance programs).
Looks very useful! Is there a story behind the name "Nextjournal" that you can share? I wondered if you had dreams of becoming the next platform for open-access journals. Looking around the site, it looks like there is no explicit support for periodic groups of articles into journals, though I guess one could do such a thing.
The distributed computing they’re talking about here is mostly distributed scientific computing, for which Julia has out-of-the-box support for. This is miles in front of the likes of Python, Scala etc. Erlang/Elixir is obviously the king of distributed languages, but that’s a whole different kettle-of-fish. Julia does play well with Docker: I’ve certainly had an easier time deploying Julia docker images to Kubernetes clusters than I have deploying Python. Julia’s dependency management is just a whole lot better in that area. There’s a bit more work to go: package compilation will make deployments even easier and startup/runtimes even faster. What it is missing, is the same level of support and tooling that a lot of other languages have. That will come with time though.
Unifont (https://en.wikipedia.org/wiki/GNU_Unifont) has got quite a few, although it's not exactly beautiful... There's also Gnu FreeMono. You can't have a font with more than 65536 characters in it, so Unicode, with its 125,000 characters, can't be put inside a single font. The STIX project aims to support every math symbol, so that would be worth installing (if your gnome terminal does font substitution...). There's a XITS project too, which is a derivative (?) of STIX. Asana Math also has good math support.
Point of etiquette - when you mention the Reddit thread where you got feedback, you should probably link to it. Thems old school blogger rules.
I wish **foreach** was included
You should probably check out [this blog](https://fluxml.ai/2019/03/05/dp-vs-rl.html). There are links there to the Flux model zoo, which has all of the cartpole examples ready to run.
This is just what i was looking for! Thanks. In need to improve my search skills
[Iosevka](https://github.com/be5invis/Iosevka) is my top choice, both for character support and programming ligatures.
It's interesting that you plot the maximum time, was it reproducible? I would expect the maximum time to be dominated by unrelated things (gc pauses, OS scheduling a different process, etc.).
I was planning on doing a box plot of the distribution of the run times, but the minimum and lower quartiles were close to the average value. It was more demonstrative and better visually to just show the average and the maximum. Yeah its reproducible, each benchmark was run 10k times so I'm confident that the noise has been averaged out.
&gt; Yeah its reproducible, each benchmark was run 10k times so I'm confident that the noise has been averaged out. Did you run `@benchmark` 10000 times or do you mean the 10000 times that `@benchmark` ran the functions? Because if it's the latter then the maximum will be the maximum of those 10000 runs, not an average of the various maximums. My point is that usually CPU-bound benchmarks will have averages that are close to the minimum and long tails, so the maximum usually doesn't tell you anything about the distribution. If you ran `@benchmark` multiple times and kept getting a big difference between the maximums of those two methods then that tells you something interesting, but if you only ran `@benchmark` once then those disparity between the maximums could easily just be noise.
Yeah thats all very true and a good point. My confidence comes from running the notebook multiple (maybe 10?) times and not seeing the actual maximum values change much.
Awesome, so it is probably something inherent in the method. I have a look later and see if I can figure out what
maybe you can run firefox.exe from the command line and give it the address as a parameter?
In your second plot, the code and the plot make it look like you're showing the minimum and the median, rather than the median and maximum as shown on the plot legends. I.e. assuming the line timeArray = mapreduce(x -&gt; [minimum(x).time, median(x).time, maximum(x).time], hcat, bmList)' then rather than bar(nms[2:4], (timeArray[2:4, 1]), seriestype=:scatter, yaxis=("Time"), label="Median") plot!(nms[2:4], (timeArray[2:4, 2]), seriestype=:scatter, label="Maximum") perhaps you should use bar(nms[2:4], (timeArray[2:4, 1]), seriestype=:scatter, yaxis=("Time"), label="Minimum") plot!(nms[2:4], (timeArray[2:4, 2]), seriestype=:scatter, label="Median")
What do you mean “if they’re a dead end?” We could probably help a bit more with some more information :) Are you trying to serve pages/data via a web URL ? (Like a webserver?) Or are you trying to get data from the web pages/URL? Or are you trying to do something else? Because if it’s the first 2, they can definitely be done! The first is web development stuff: Julia is still pretty immature in this field, but the excellent Genie framework is shaping up to be everything you would need! If you want to make requests and generally pull data from websites, the HTTP.jl package has everything you need, and plenty of examples of how to use it.
Well I'm getting links. And most of them don't exist. (End in an error) that's what I ment with dead end. HTTP.jl looks very promising. I will definitely look into that. What would be optimal, is to generate all the web links, check if they lead somewhere and then open the good ones in the Webbrowser.
Oh in that case, that’s largely pretty straightforward. linkarray = [“link 1”, “link 2, “etc etc”] httpresults = asyncmap(HTTP.get, linkarray) validurls = filter(x-&gt;x.status == 200, httpresults) That will give you an array of the valid urls with minimum waiting time. As for opening the valid ones in a web browser, I’m not sure of the best way to go about doing that off the top of my head, Firefox probably has an API or command line option that you can call/invoke that opens a website. If so, you probably don’t want to push all of the valid urls to open in your browser at once, so maybe put them in a loop that blocks on user input (`readline()` can be used to block waiting for anything) so you don’t flood their browser with potentially many tabs
Cheers for picking that up, I've corrected that now. Thankfully the conclusion doesn't change!
That final graph with the medians/maximums is actually wrong (see ChristianPeel's comment), I was plotting the minimum as the bar and the median as the point.
Hi, newb wannabe programmer here, sorry for the stupid questions and scenario about to follow. I recently got my mgmt to agree to let me experiment with Julia on my work computer (corporate windows environment). The short term goal is to come up with some analytic tools around the work that we already do. The long term goal is to try to convince senior mgmt to let us rewrite (some of) the company’s heavy computational systems with Julia. I will only have three days of admin access to install what I need to get up and running. I am already planning on installing the Juno IDE, as well as versions 1.1 and 0.7 (to be able to update any packages that are still on 0.6). My question is this: do I need Docker? I’m not sure what it even does exactly. My initial need is small scale — just a write a few scripts for analysis and visualization. My long term goal, which isn’t all that feasible, will require being able to deploy code to production. It seems that’s what Docker would be good for, but I am no where near close to anything production (and may never be). So does it make sense for me to install it? Would I get any benefit out of it even if my purposes remain small scale?
Thanks! This is interesting. When you deployed to the cluster, did you just do it using terminal? Can I ask what kind of projects you're deploying, that requires additional compute power (assuming that's why you're deploying)?
you most likely have virtualization disabled in the bios of that machine, so forget about Docker. I would actually also install Anaconda Python if you want to prototyprle and interoperate in a corporate environment.
I haven’t (yet) needed to deploy a Julia cluster, I’ve just been deploying Julia containers into our Kubernetes environment. Basically I wrote a small ML classifier and web server with it and put them into a docker container. Deployment is managed by our CI/CD pipeline: whenever there’s a new commit or pull request onto the master branch it will build and deploy stuff automatically. At the moment there’s only that one service, but as soon as our “workflow engine” thing is installed in Kubernetes I’ll be running bulk machine learning and data processing jobs(written in Julia) on there.
Thank you for your reply! &gt; I would actually also install Anaconda Python and use Jupyter in firefox to prototype and Pycharm to code. if you want to prototype and interoperate in a corporate environment. Can you explain that further? What does interoperate mean? And why isn’t Juno enough?
Docker is basically a tool to make it easy to share your development setup across any machine. You describe the exact environment you're going to run (from the operating system and installed programs up to the files you have to copy from your computer and network configuration) in a Dockefile, and anyone can recreate the exact same environment to run your code in the exact same way you did (there are limitations though since it's not a full virtual machine, just a lightweight one). For example this Dockerfile describes how to download a debian Linux, install Julia in it and open the REPL when running: https://github.com/docker-library/julia/blob/cccb8b5148d509a74cb12b64de44ad7b20328f23/1.1/stretch/Dockerfile You can build it (if you have docker installed and a terminal) and run the Julia REPL with: docker build -t "juliadocker" . # in the directory you saved the file above docker run -it juliadocker # open a terminal and interact with it That said, if you can just install Julia manually in the machine you're developing and the one you're going to deploy then it's much easier, so I would definitely recommend that instead of using docker. Installing and configuring docker is definitely not trivial.
Why not just #!/usr/local/bin/julia println(\"hello\") ??
In your script you assume the location of julia rather than petting the user's PATH variables use the correct \`julia\` and as for \`\`\` \#!julia println("hey") \`\`\` I'm not a bash expert, but I think bash requires an exact path to the script runner, this script fails because julia isn't in current directory.
Not really, for one it's simple to just do a \`which julia\` and put that into the shebang instead. For two, if you don't want to modify the script it would be easy to just symlink to wherever weird place you put julia. For three, you could make that argument about your shell script: I could put zsh in whatever weird place I want, maybe I keep it in /opt for some reason, that' doesn't mean that \`#!/bin/zsh\` is somehow generally less reusable or bad practice lol. I'm just struggling to understand why you'd reinvent the wheel to do something so standard, especially in light of the fact that you refer to it as a "tip" instead of a "strange thing that works sort of I guess"
/bin/sh is an extremely standard convention, your path to Julia or wherever you are expecting a third party user to symlink to is not. Are you even trying these ideas you are suggesting? Both of your suggestions fail
The solution for this problem other script languages such as perl or python often use should work just as well for julia: #!/usr/bin/env julia println("foo")
The solution for this problem from other script languages (such as perl or python) should work just as well for julia: #!/usr/bin/env julia println("foo")
The two methods are not the same. The exec method uses an extra fork Presumtious of you to think bash is default shell too.
I have used both but honestly I'm not surprised that itd fail when you try it
 #!/usr/bin/env julia println("hey") is the recommended way of doing this. I believe `/usr/bin/env` is a POSIX requirement, but even if it's not every single Unix system I've ever seen has it (and some, such as NixOS, only have `/usr/bin/env`).
If you want to get started with Julia you don't need docker. As it was pointed out in the comments, docker is a tool that let's you recreate the production environment so that you can be sure that if your code runs on your machine, it will also run on the production server (which is not to be underestimated). That said, the fastest way to start working with Julia, and what I would advise if you have only 3 days to install all of the components and are not that familiar with atom, is to install juliapro. It will install Julia 1.0.3 and juno for you and configure everything you need. The drawback is that you will need a Juliapro account (which is free but still...) Another solution, which is the best in my opinion, is to install the Julia binary (ie install the Julia package which you can get from the Julia website), and follow my guide to install atom and juno. When you have installed juno, instead of configuring it to work with docker, you can simply type the path to your Julia exe and it will work with your local installation of Julia. You can find more information at the official juno installation guide (which is more than enough for what you need) http://docs.junolab.org/latest/man/installation/ Last, but not least, if you want to experiment with Julia, without the need to install anything on your work pc, you can give a shot at Juliabox https://www.juliabox.com You will get a free online working environment with jupyter installed. It comes "batteries included" as it is ready to work and packed with interactive tutorials to get you started with Julia. Jupyter in my opinion is not as complete as Juno, but if you are starting to learn how to program in Julia, it is much more friendly and it has the possibility to write beautiful markdown comments to your code, making it a much better tool to showcase your work to your boss. **Bottom line: if you want a full ide without the hassle of configuring anything, go with juliapro. If you want a fully customizable environment running with Julia, go for juno + personal Julia installation. If you want to get started immediately go for Juliabox (which is the solution I recommend if you are new at programming/Julia) ** Ps: somebody suggested that you install anaconda and pycharm, but this is not exactly what you need if you want to code in julia. Anaconda is mainly a package manager for python related libraries and pycharm (although it is a wonderful software which I personally use for programming in python) is a commercial software meant for python developement and it has little support for the Julia language, so I would not recommend using it in your case.
If you want to code in Julia you don't need either anaconda or pycharm, although they are really great software if you want to code in **python** Juno is more than enough if you want to code in Julia! You may also want to take a look at juliapro and juliabox, to make installation of software much easier. Please take a look at the other answer I've written to have further informations ^^ If you need more help please feel free to ask!
Thank you so much for the detailed reply! I feel bad for hijacking your post, I should’ve made own separate post. Your tutorial is really well written, by the way. I have considered the other options you mentioned, and they are enticing. The biggest drawback with Juliabox is that the free account only gives you 90 minutes of work time at a clip (unless I misinterpreted the website), plus I feel I’m ready for “the real thing” since I’ve already played around on repl.it (and at home, see below). I should mention, I am not completely without coding experience. I use SQL on a daily basis, and occasionally tinker with VBA, PowerShell (though I suck at that), and Windows command prompt. I also took programming in high school, ages ago, where we learned qbasic :P and a bit of C++. I am far from being a real programmer though. I have to say, JuliaPro looks very enticing. I don’t mind having to create an account so much. I like that it makes the setup with Juno easy, and I believe it comes with a standard set of packages, which is a big deal for me. One of the biggest challenges I’m envisioning in those three days, is figuring out which packages to install. To me the biggest drawback though is that it doesn’t come with the latest version of Julia. Installing the binaries doesn’t bother me so much, since I’ve done that at home. I’ve installed v0.6, 0.7, and 1.0 at various points over the last few months on my home PC trying to work on a personal project (no Juno though). I am a little nervous about installing and setting up Atom/Juno properly since enterprise environments can be finicky and problematic. But it seems that’s the setup a lot of people use, so I want to be on the same plane as the rest of the community, if that makes sense. I think I might still do a separate question post asking for package (and other) advice related to my specific use case. But I want to thank you again for taking the time to write out these kind and detailed replies. I very much appreciate it :)
Thanks, i'll update the article. One interesting thing I saw looking up on scripts using `/usr/bin/env` is you can't pass in parameters to `julia` using this method (like -O -machine-file, etc).
This brought up some really interesting questions. `/usr/bin/env` works, but it prevents you from passing arguments to `julia`. Sounds like it might not be that bad ... but, in order to get SIMD working julia, you need to pass -O3 parameter to julia it appears.
The thing with Julia box is that once in 90 minutes your running kernels get killed and you have to login again and restart. The only problem with juliabox is that, as you mentioned, it uses a "copy" of the list of packages which don't get updated as often as the real packages do. It is fairly easy to install juno from scratch, just install Julia, installatom, install the plug in "Uber-juno" and type the path to your Julia binary, as easy as that. As concerning the Julia packages to install, you can manually install, for example, the packages provided with Julia pro. If you want to make atom feel even more like an ide, I suggest that you also install the packages "project manager" and "minimap"
This is really cool! I think the DE.jl library really is a gem for Julia and I try to show it off when I try to sell computational people on it.
What do “project manager” and “minimap” do? Also, is there a way to install all registered Julia packages at once? Is that crazy? Would they take up a lot of space? I will not need most of them, but I will not know which ones I will need.
You can't possibly install all the existing packages, but you can install packages from the Julia repl without admin rights (as they will be saved in your personal folder). Project manager let's you specify a folder for your project and quickly swap settings/folder between projects. Minimap is an useful tool that previews your code ad a scrollbar, so that you can move faster between chunks of your code. Just utility tools, but I find them useful
This is all very useful, thank you.
DE.jl, JuMP, JuliaDB and OnlineStats the 4 amazing libraries.
Use a plugin called `project manager` and disable the plugins which are not useful for the specific project (for example Juno if working with python). Go to package manager, save your project, then click `edit projects`. I work with both python (using hydrogen + ide-python) and Julia (uber juno and co), and my config file looks like this: ```json [ { title: "my julia project" paths: [ "D:\\path\\to\\your\\julia\\project" ] settings: core: disabledPackages: [ "atom-ide-ui" "ide-python" "hydrogen" ] } { title: "my python project" paths: [ "D:\\path\\to\\your\\python\\project" ] settings: core: disabledPackages: [ "julia-client" ] } ] ``` Once you switch project, all the appropriate plugins will be loaded/disabled automatically! Remember that you can have only one project active at time, or you will mess with your plugins if you have different enabled plugins!
Thanks Chris and others for bringing this to fruition.
Thanks, that looks very nice. I assume you are one of the authors?
Thanks, Yes, Yoni.
This is Julia at its best!
Awesome, that looks exactly like what I need. Thank you.
And **24 sec** to plot single graph code [https://i.imgur.com/vNE0YiY.png](https://i.imgur.com/vNE0YiY.png)
The `Plots.jl` package has multiple backends which can be selected at runtime. For that reason it cannot precompile itself completely. Most users just choose to keep the Julia session active for as long as they can so each unique function call only requires a single compilation step. Ps. You are also measuring the Julia startup time since you are running the script from the shell. An alternative is to start Julia as normal and use `include("path/to/script.jl")`.
No, it's not the startup time, if you run \`println("hello")\` it would take \~250ms. Thanks, I know that's the preferable way is to keep the session or Jupyter Notebook. But it's kinda should be also more or less possible to use it as a plain script too, 24sec for simple plot seems way too slow... I'm going to write multiple project files, also some boring stuff like reading/writing files etc that doesn't need interactive running in Jupyter. So I created multiple files in VSCode, you change multiple files and run it to see if it works as you expected. It's not convenient to test every single line in Notebook and then copy and paste in the files...
I would still recommend just starting a Julia REPL session and then using `include(...)` on the multiple files. There is no specific reason to have to open up a Jupyter notebook. This is what I've been doing all day as I've been generating plots for my Masters Thesis (beautiful plots if I may say so myself). Unfortunately the price to pay for the flexibility and convenient syntax of `Plots.jl` is the startup speed, the reasons therefore are already mentioned. Side Note: Your current workflow sounds very much like what I was doing when I was using Python. That workflow doesn't translate well, primarily because Julia is not really a scripting language.
Yes it is faster, thanks. Would be nice to have command line utility sort of `run-and-reuse-julia-session test.jl`
I've played around with the idea of a Julia process that listens on a socket. You could then send the desired file to the process which will then run it. Never could get it to work as I do not have much experience with that sort of thing.
It's how it works, at least for now. Julia is in an unusual place in which it's not a compiled language but it only runs compile code. A language like C++ also has a huge amount of possible functions to compile thanks to templates, but since it's static it knows beforehand what will ever be used (there is no REPL in which a user can call whatever they want and functions can't be redefined at runtime). Python on CPython on the other side does not compile anything before running, so you import everything and it only what's called will be ever evaluated (which also means it's much slower than a pre-compiled language once it's compiled). Plot libraries are probably the ones that suffer the most since they have lots of arguments and dynamic elements, leading to an exponential amount of possible paths that would all need to be compiled if you have no previous information about what will be called. That's not without solution though, and I wouldn't be surprised if in the future Julia will be fully able to run in interpreted mode (no compilation lag, best for short scripts), just Ahead-of-Time mode (full flexibility of a dynamic language, speed of a compiled language, but compilation lag) and compiled mode (somewhat restricted functionality, but good for creating fast libraries).
It seems like it works well if instead of using \`julia test.jl\` you open REPL and execute \` include("test.jl")\` every time you need to run the script as suggested above. Seems strange that such an important use case does not automated somehow or explained in the docs.
The workflow in the manual could definitely have more information https://docs.julialang.org/en/v1/manual/workflow-tips/ For example using Revise to make it easier: https://timholy.github.io/Revise.jl/stable/ And you can also just say what you'll use and precompile based on that, but that's definitely trickier at this point: https://github.com/JuliaLang/PackageCompiler.jl
Need to crush this one - been a well-known thorn for a while. Unfortunately, Julia fills a niche where one-off-plotting seems natural for the language. As others have pointed out, this is related to libs written in Julia being JIT-compiled. Note that this is neither a problem in interpreted, nor compiled languages, but shows up here.
Thanks Chris and the DE team !!!
There's a lot here. But I just want to point out briefly that reducing iterative loops is not necessarily a way to improve performance in Julia, as it is in MATLAB. Other things to note: * Don't use type annotations unless they're needed for dispatch. * Don't use structs unless they're needed for dispatch (e.g., `Parameters.jl` named tuples with `@unpack` work fine.) * If you do use structs, don't annotate the fields with abstract types. * Take a look at the performance page of the official Julia documentation for a good set of easy-to-apply tips. For example, a key property in Julia is "type stability," which means that the type of a function's output is inferable from the _types_ (not values) of the inputs. There are tools you can use to test this, like `@inferred` and `@btime`. The `need for speed` lecture of lectures.quantecon.org/jl is probably a good reference as well.
two speed tips: 1. be very careful about type stability. here is a constant i have casually written: const things = [(100, "something"), (1.28, "somerhing else")] the type is Array{Tuple{Real,String},1}, which seems cool until you realize that Real is not an actual type, you wanted Float64 or you just say a = 1 if condition a = a * 1.1 end and now you have type instability, because a is either an integer or a float64 depending on the condition. the solution is to use 1.0, or the *one* function. 2. allocations. avoid allocations and mass copying inside loops like the plague. try to use fused operations as much as you can. always try to understand what happens behind the scenes. you can go to the source code and see how a fold or a comprehension actually works.
&gt; Don't use structs unless they're needed for dispatch Why not? I find one of Julia’s advantages the presence of structs that I can use to model re-occurring structures in my code. I often use them to model response objects from Http API’s as well.
* replace iterative loops: the main one unique to Julia is the [broadcasting](https://docs.julialang.org/en/v1.1/manual/functions/#man-vectorized-1): map(==, map(+, map(sqrt, [1,4,9]), [1,2,3]), [2,4,7]) # using maps (though not in a way I would write) [2,4,7] .== [1,2,3] .+ sqrt.([1,4,9]) #using broadcasting Julia also supports list comprehensions such as [x for x in 1:10 if x % 2 == 0] and generators ("lists" that are not evaluated until required, which avoid creating arrays unnecessarily) by using () instead of []. * anonymous functions: You can use them whenever you don't intend to use the function outside of the current context and you're passing them around as arguments, for example for a filter(a -&gt; a &gt;= 7, [1,7,13]) there is no point in naming that method. * functional or procedural: adopting a functional or a procedural style is a matter of preference, and the language supports both and there is usually no cost in performance. Higher order functions, [composition](https://docs.julialang.org/en/v1.1/base/base/#Base.:|%3E), [pattern matching](https://github.com/kmsquire/Match.jl), efficient use for sum types for error handling (such as Union{MyResult, MyError}) and many libraries provide functional interfaces like Query.jl, DataFramesMeta.jl and MLStyle.jl. Support for [immutable structures](https://github.com/JuliaCollections/FunctionalCollections.jl), isn't nearly as strong as functional languages though. The strength of functional languages for parallel computing is pure functions (no side-effect and not mutating the arguments) and immutable structures. It's practice in Julia to add a ! in methods that mutate (for example push!), so if you're careful you can avoid it (though for many algorithms mutating is much more efficient). The best reference for parallel programming in Julia is the [manual](https://docs.julialang.org/en/v1/manual/parallel-computing/index.html)
I was under the impression that declaring datatypes may increase computational speed due to the information being explicitly provided to the compiler...? &amp;#x200B; Let say I wanted to create a general function that handles both Int64 and Float64. I shouldn't explicitly declare a type in the function. I should algorithmic design the function to suit ether case? &amp;#x200B; I'm still piecing bits of theory together but, is this the case were I would take advantage of multiple dispatch or is that computationally expensive?
Type annotations helps you sometimes (say, when inference fails), but it's not straightforward for a novice. For example, see this bit from the Julia docs: ``` Type annotation will not enhance (and can actually hinder) performance if the type is constructed at run-time. This is because the compiler cannot use the annotation to specialize the subsequent code ``` If you want a function that handles both, it depends on whether they do the same thing or different things. For different, you might try dispatch: ``` foo(x::Int64) = something foo(x::Float64) = something_else ``` If it does the same thing in each case, you could try something suitably generic like: ``` addone(x) = x + one(x) # this is generic to any type which implements one() ``` Edit: See this thing from the "Type Declaration" section of the Performance Tips docs. &gt; In many languages with optional type declarations, adding declarations is the principal way to make code run faster. This is _not_ the case in Julia. In Julia, the compiler generally knows the types of all function arguments, local variables, and expressions. However, there are a few specific instances where declarations are helpful. (Emphasis theirs.)
The main issue is that with structs, you're using a bomb (creating a new type) to deal with a nail (the need to bundle and exploit re-occurring structures.) If you don't manage your structs properly (e.g., with parametrized types and such), it can easily be more trouble than it's worth, in a performance sense. Named tuples are much easier for this purpose, in my opinion. And cleaner. Instead of: ``` struct MyStruct{T1, T2, T3} x::T1 y::T2 z::T3 end m = MyStruct(1, 2, 3) ``` I'd have something like: ``` m = (x = 1, y = 2, z = 3) ``` The upshot here is the annotations, e.g.: ``` julia&gt; m = (x = 1, y = 2, z = 3) (x = 1, y = 2, z = 3) julia&gt; typeof(m) NamedTuple{(:x, :y, :z),Tuple{Int64,Int64,Int64}} ``` It's all done "properly," without the need for clunky boilerplate. Edit: Also, with `Parameters.jl`, there are nice named tuple utilities for default values in constructors, `@unpack`ing (e.g., `@unpack x, y, z = m`), and others.
From Julia Docs: &gt; Adding annotations serves three primary purposes: to take advantage of Julia's powerful multiple-dispatch mechanism, to improve human readability, and to catch programmer errors. &amp;#x200B; I could argue best practice would be to never declare types unless I want to utilize dispatch as you've mentioned. And, I should try to maintain abstract and generic logic for each function. &amp;#x200B; Excellent, this is really helpful!
Julia is optimized to be a dynamic language, even though it has a type system that resembles static languages, which might confuse some people. Sure, if you write Julia code like Fortran or C and annotating everything like a static language your code should be fast, since you're basically enforcing type-stability. But you're sacrificing the language main strength, which is the automatic specialization of code. If you create two methods: mysum(a::Int64, b::Int64)::Int64 = a + b mysum2(a, b) = a + b And call them with two Int64, the compiler will create the exact same code for both since Julia will already infer the type and create a specialized function for you (you can try @code_native(mysum(1,1)) and @code_native(mysum2(1,1)) to see the instructions in assembly to see for yourself). But the second one will generate equally optimal code for all cases covered by the 163 implementations of '+' in the base library (which you can see with methods(+)). And it will play nice with libraries that have custom types that have their own unique implementation of '+', such as machine learning libraries. Of course, annotations can still be useful (outside of dispatching, in which, like the other post said, you want different behaviors for different types) when you want to deliberately restrict parts of your code to make sure the code behaves exactly as you expect (basically an assert). But it won't earn any extra performance.
What do you mean by fused operations? &amp;#x200B; I'm a bit scared of looking at the source code. I will make a point to try it though.
some of the code is pretty advanced, but boy, you learn good techniques. fusing is that .-thing. when you say a .+ sin.(b) for similar arrays
&gt;But the second one will generate equally optimal code for all cases covered by the 163 implementations of '+' in the base library (which you can see with methods(+)). This seems expensive. &amp;#x200B; I will primarily be using floats for modeling physical units. Hypothetically, lets say I'm doing thousands of executions of this one function on a data set. The function is built once by the compiler but, every execution requires the software to sift through 163 cases?
&gt;This seems expensive. No, the look-up will happen exactly once, during compile-time (before your function is even executed). During runtime (while your function is being called) it will immediately call the correct function every time without any overhead. That's part of what defines multiple dispatch (as opposed to dynamic dispatch which will look-up the correct method during runtime). &gt;I will primarily be using floats for modeling physical units. If you're modeling physical units, you might want to check unitful, to avoid using generic types: https://painterqubits.github.io/Unitful.jl/stable/
Yep, this is pretty much exactly right. Np.
Thanks for the knowledge! I will definitely look into Unitful.jl too.
Unitful is actually a nice example of the benefits of leaving your methods generic (or in this case, restricted to Number). Since DifferentialEquations don't restrict to Float64, you can use Unitful types with it and have the combined functionality of both, a kind of interop that is very particular to Julia. http://juliadiffeq.org/DiffEqTutorials.jl/html/type_handling/unitful.html
This is called broadcasting BTw.
And fusion an optimization over loops/broadcast. https://julialang.org/blog/2018/05/extensible-broadcast-fusion
So fusion looks like a way to optimize multiple chained broadcast operations. Thanks, TIL.
1. The structure of the modules is not the same as the structure of the files. And there's an unfortunately obscure difference between loading a module in the REPL/via script and from another module root, which is likely what you're running into. 2. More resources are being put into the Atom editor right now, so it's probably best to use that.
So, there are a couple of things to this, the first question being what are you trying to do? I think the quick and dirty solution for you would be to just call the symmetric constructor on your array of garbage. `Symmetric(Array{Float64,2}(undef, 3, 3))` But I don't really see a reason why you would do that
Sorry, but what are you talking about? These are very grandiose claims ("the best way to use feature X is not to use it at all," "the documentation is broken," etc.) for a beginner to be making (judging this from your post history.) I think what you need to do is figure out how modules work in Julia. I don't have time to write a play-by-play for file inclusion, but you could probably search discourse or GitHub for a tutorial.
Does this make sense? Symmetry is a property regarding the values you stored in the matrix. If you have a symmetric matrix claimed and call some function that only works on symmetric matrixes on this gibrish (in terms of values in memory), what do you expect to happen?
I guess it's more like a highlighting the problem. I'm beginner with Julia but quite experienced with other languages. There are lots of nice things in Julia, but its module system looks like [Astronaut Architecture](https://www.joelonsoftware.com/2008/05/01/architecture-astronauts-take-over/) over-engineered nonsense.
Just to add to what others have already posted, if you want to create a square matrix (nxn), then what you initially posted works just fine. A symmetric matrix is a matrix where A = A^T, and as previously mentioned it doesn't make sense to have a symmetric matrix that's uninitialized. Random, sure, but not "uninitialized". Wiki links for clarity: Square Matrix - https://en.wikipedia.org/wiki/Square_matrix Symmetric Matrix - https://en.wikipedia.org/wiki/Symmetric_matrix
Why not? Just because it's uninitialized doesn't mean it contains no values. It just means that it contains garbage values that were already in that memory location. You can make the matrix symmetric with respect to those values. Unfortunately, the `Symmetric` wrapper doesn't allow you to update the matrix, except along the diagonal, which is probably something you want to do with an uninitialized matrix.
`H = Array{Float64,2}(undef, (_n, _n))` `for i in 1:_n` `for j in i:_n` `H[i, j] = sum( (_𝒜[i]*_S_which_norm).*(_𝒜[j]*_S_which_norm)' )` `end` `end` `H = Symmetric(H, :U)` &amp;#x200B; That's the way I'm doing it now. But I would like to put the data directly into the matrix.
I want to update it and expect that it will update symmetrically.
Yes that's right. So after I made it symmetric, I can't change the matrix except on the diagonal?!
So, Symmetric is just a wrapper for a full array anyway because the LAPACK routines still use the dense array with the note that it is symmetric for multiplication and diagonalization routines. &amp;#x200B; Someone correct me if I am wrong, but you want to save the conversion till you need to use the fact that it is typed to be symmetric. So basically, you work with it as it is dense, then you tag it as you use it to save the LAPACK backend the time of determining its structure. &amp;#x200B; Calling symmetric doesn't do anything other than telling the underlaying routines its structure. julia&gt; A = rand(3,3) 3×3 Array{Float64,2}: 0.344044 0.779088 0.790496 0.951341 0.186702 0.759037 0.778591 0.949122 0.854685 julia&gt; @benchmark B = Symmetric(A, :U) BenchmarkTools.Trial: memory estimate: 32 bytes allocs estimate: 1 -------------- minimum time: 67.180 ns (0.00% GC) median time: 70.075 ns (0.00% GC) mean time: 82.696 ns (13.08% GC) maximum time: 67.892 μs (99.83% GC) -------------- samples: 10000 evals/sample: 977 &amp;#x200B; So if you were going to solve an Ax=b problem, you would form your A and then `x = Symmetric(A) \ b` &amp;#x200B; Check out the benchmarks: &amp;#x200B; # Make a big random,dense,symmetric matrix A and solution b n = 100 A = Array(Symmetric(rand(n,n), :U)) b = rand(n) # Having julia do the structure inference @benchmark A \ b BenchmarkTools.Trial: memory estimate: 79.98 KiB allocs estimate: 5 -------------- minimum time: 748.579 μs (0.00% GC) median time: 1.402 ms (0.00% GC) mean time: 1.563 ms (6.71% GC) maximum time: 190.750 ms (99.16% GC) -------------- samples: 3147 evals/sample: 1 # Explicitly calling it symmetric @benchmark Symmetric(A)\b BenchmarkTools.Trial: memory estimate: 130.22 KiB allocs estimate: 9 -------------- minimum time: 526.225 μs (0.00% GC) median time: 725.039 μs (0.00% GC) mean time: 845.671 μs (12.18% GC) maximum time: 204.283 ms (99.62% GC) -------------- samples: 5770 evals/sample: 1
why the weird error is a good question, but your code is invalid. you promised that the return value will be Vector{T}, but you returned Vector{Union{Missing, T}}. this is invalid because the very purpose of convert is to convert to the give type. it can be reproduced by a much simpler Base.convert(::Type{Int}, S::Float64) = 0x00 i suppose it throws a wrench in the works, because the conversion subsystem assumes the result unchecked.
Yes and no. It seems to be an overkill to have a vector version of the function when one can simply broadcast a base case. However, the crashing issue is still there even if I change the return type to be \`::Type{Vector{Union{Missing,T}}}\`. The following code still fails with the same error message. function Base.convert(::Type{Vector{Union{Missing,T}}}, x::Vector{Union{Missing,S}}) where {T,S} n = length(x) y = Vector{Union{Missing, T}}(undef,n) @inbounds for i = 1:n if ismissing(x[i]) y[i] = missing else y[i] = convert(T, x[i]) end end return y end It seems the abstract types T and S cause the problem. If I change T to a concrete type like Int32, the code works.
this works well Base.convert(::Type{Vector{T}}, x::Tuple{S, S}) where {T, S} = [convert(T, x[1]), convert(T, x[2])]
Yes, it's a bit odd, I don't know why this is. You can sneak around it by modifying `s.data` if `s` is your symmetric matrix. I don't recommend it, though, it's probably a bad idea. Perhaps you should ask this question on [https://discourse.julialang.org/latest](https://discourse.julialang.org/latest) You're more likely to get help there.
It’s unclear to me that you read the article you linked. A module is a simple, well-defined thing that organizes Julia code. No astronauts, live or otherwise, were harmed in the development of modules. And as for over-engineered... have you taken a look at the backend? I’d guess not, and neither have I. Who knows how “engineered” it is. If you can’t be bothered to study the feature or ask for help, then of course you’ll find it useless. But charging in guns blazing and declaring the thing DOA is not the best approach.
b = deleteat!(deep copy(a), 2)
Or if you want unique set, b=unique(a) will do
 b = a[true, false, true, true, true]
 b = a[[1;3:end]]
But why do you need to define this method? It already works automatically with `convert`, as is, without defining a special method: julia&gt; a = [1,2,3,missing,5] 5-element Array{Union{Missing, Int64},1}: 1 2 3 missing 5 julia&gt; convert(Vector{Union{Float32,Missing}}, a) 5-element Array{Union{Missing, Float32},1}: 1.0f0 2.0f0 3.0f0 missing 5.0f0
&gt;And as for over-engineered... have you taken a look at the backend? Who cares about the implementation. The design itself is wrong. Again, if you seen lots of cars, and now see one with square wheels - you don't need to "study" it to understand it's wrong.
How big is your array? If it is small enough, a normal for loop should suffice. let k be the element you want to delete. for i = 1, k-1 array_new(i) = array(i) for i = k+1, n array_new(i-1) = array(i) end array = array_new. For small arrays, this should be really quick. P.S. This is a general solution, not necessarily restrcited to Julia.
Could you elaborate on the overall goal of this piece of code? There are many ways to read input into Julia so it would be useful if you could elaborate on the context? It's hard to improve your code if the goals are unknown. Also, this probably isn't the answer you are looking for (and I am jumping to conclusions) but I get the impression that Julia isn't the best language for what you are trying to achieve here.
As I was writing a response, I came up with the answer myself. I am so sorry sometimes I am really unclear. The goal of this code is just to represent 4 directional movement and make sure I am using scoping right. My end goal is to use Julia for science &amp; math's project, but I find it enjoyable to make text adventure games when I first start learning a language. I find it helps me to learn quickly and to enjoy the process. I just started learning today and next I probably should start checking the documentation to be honest.
I realized this definition/function is unnecessary. It just puzzles me that this function crashes Julia...
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/askcomputerscience] [I am new to Julia, please help me with my code!](https://www.reddit.com/r/AskComputerScience/comments/boxos3/i_am_new_to_julia_please_help_me_with_my_code/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
The reason is that a loops behave a bit different in the global scope (e.g. if you type it in the repl) Julia has some strange scopeing behaviour, which is subject to discussion. In short, in julia it is recomennded to use functions because then this issue does not occur. If you are more interested in the background of this issue you can read [link](https://docs.julialang.org/en/v1/manual/variables-and-scoping/index.html#Local-Scope-1)
So I need to use functions as workaround, I see. Thank you so much! Would you be able to give me an example of using functions to achieve this. I just started learning today and am loving it. :) I am very interested in finding out more about this issue, it was actually really annoying when I first ran into it. To be honest it took my by surprise. One more question. Is Julia too new of a language to be investing my time in, if it is I will just switch to Haskell.
Okay, glad you sorted it. I have some general advice for you anyway. You mentioned your end goal as being a science project, which is exactly the use case Julia is intended for. Although its possible to create text adventures in Julia and doing so helps you learn the basic syntax, I would argue that this isn't a great way to properly learn the language. It is easy to just look at the basic syntax ('if' statements, 'for' loops, 'function' definitions) of a language and think that you have learnt the language whilst ignoring the features that make it unique. In Julia examples of such features could be the type hierarchy, multiple dispatch or higher level array operations. If your use case does not utilise these unique features, you may want to consider using a 'more general' or 'better suited' language. Julia is general purpose, so you could implement many features using basic constructs if you wanted. If you choose do this, you might as well use a more verbose language like C or a more widely used language like Python (both of which benefits from an insane amount of resources online compared to new languages like Julia). TLDR: writing a text adventure game may help you learn the syntax of a language, but won't help you learn the nuances of it. If the nuances of the language aren't important to you, maybe another language is more suitable.
For anyone who comes across this looking for an answer. While julia on the command line accepts '-p 5', julia in the atom (juno) ide does not accept the space. It requires '-p5', which is also valid on the command line.
This is Enjayay, by the way. I got locked out of my account on this computer and can't remember the email, or pass. ha ha! Oh I see. So you feel that doing these kinds of projects will not make use of a lot of things Julia is good for and therefore hinder my learning, right? I will look into these features and try to find out more about what Julia is good for. I see what you mean there, like if I am not using what language X is good for, when language Y would be more suitable, why use X when you can use Y. I have heard Julia is quite good for metaprogramming and the core of the language is written in itself. So I could could contribute to the language, if I continue to use it. I really liked Python, but I did not like the idea that I would have to learn C on top of it, if I wanted to change anything. (Performance and so on) Actually I have thought about learning C, or continuing C++, but I have enjoyed learning Julia a lot.
 function loop(N) i = 0 while i &lt;= N display(i) i += 1 end end N = 10 loop(N) @show i # not defined ## using global to make local variable global i = 0 while i &lt;= N global i display(i) i += 1 end @show i
&gt; So you feel that doing these kinds of projects will not make use of a lot of things Julia is good for and therefore hinder my learning, right? Its certainly not hindering your learning. Honestly, if you enjoy writing adventure games to learn a language don't stop doing it. Just make sure you are not restricting yourself. &gt;I really liked Python, but I did not like the idea that I would have to learn C on top of it, if I wanted to change anything. (Performance and so on) Python performance is decent for numerical programming if you are happy to learn [numpy](https://www.numpy.org/). Good luck with your project and I hope my comment doesn't put you off using Julia!
It's called "traits", if you Google "Julia traits" you can find more info. In short multiple dispatch works based on the type of the inputs to functions, so you can create empty structs and pass them as function parameters.
It should look something like this &amp;#x200B; abstract type Method end struct Method1 &lt;: Method end struct Method2 &lt;: Method end function algorithm(method::Method1) #implementation details for Method1 end function algorithm(method::Method2) #implementation details for Method2 end Call example &amp;#x200B; #dispatch to algorithm for method input of Method1 algorithm(Method1()) &amp;#x200B; I believe Method1 has to be instantiated because \`typeof(Method1()) = Method1\` and \`typeof(Method1) = DataType\`. &amp;#x200B; Additionally, you could do similar logic with functions using typeof(func), and I hope we will eventually be able to use string literals, so you could just dispatch on something like :method1
The design at least changed twice, motivation being you can tell the localization of a variable just reading local code. No, Haskell does not have the same performance and Julia is popular/stable enough at this point
You actually should invest your time in learning lots of different languages (with different paradigms, as is Haskell and Julia) to become a good programmer, and choosing to go deep in a language that better solves the problem you're currently trying to face. So if you're working on scientific computing or machine learning, Julia is one of the most advanced environments available (since that's where the effort has been since the start), while if you're trying to create a website then you'll suffer a little with the language being new (mostly in terms of libraries available, compared to some other languages).
You could use the \`@softscope\` macro from SoftScopes.jl to disable this inside a script. This scoping decision was very controversial inside the Julia community, and the maintainers are still mitigating the fallout. The package docs also have a human-readable entry to the discussion.
Aaahhh, Julia's scoping, never fails to surprise. I'm not saying I'd know how to fix the previous issues that these scoping rules were trying to fix, but they have to admit that the new scoping rules deserve a place in the annals of language design fuck-ups. edit: grammar
&gt; new scoping rules deserve a place in the annals of language design fuck-ups. Not only true, but very poetically said :)
The package [https://github.com/QuantEcon/Expectations.jl](https://github.com/QuantEcon/Expectations.jl) does exactly this (have abstract types be flags for dispatch, to select algorithms and stuff.) I recommend taking a look at it.
Do you feel this is a good thing and do you think it will be changed again? If Julia is stable enough, I'll keep learning it. :)
So you are saying I should learn programming in general, pick the right language for the job. I am really interested in scientific computing to be honest. I am also interested in meta-programming which is supported in Julia.
So I can use @softscope in my first example to get the expected behavior. I think I need to start to looking at the docs more.
That's what I was thinking. :D
Yeah they suck. I kind of want to punch them in the face. :)
 function loop() i = 0 while i &lt; 10 print(i) i += 1 end end loop() I made a simplified version of your first loop. It is really interesting that I don't need to use a global, if I just wrap the while loop within a function and call it.
Method1 does not have to be instantiated (this is often true with zero-field structs) - if done in the manner you write there, there is zero overhead to using traits.
I think it would be a great match. The compile edit cycle would be longer though if you had to have the julia start-up penalty every time, so you might want to think around running your program and exiting every time but instead hag ing a listener that takes a path or something like that
That would really depend on the type of language you want to implement (and requirements, though if you're doing it as a learning project some complications like large binaries, lots of dependencies, inconvenient to run, slow compilation and etc... shouldn't matter much). I'd say the best way you could do it with Julia would be to encode your language in terms of the Julia AST (using the macro system if Julia's parser is flexible enough with non standard string literals as a fast way to write your own parser), and possibly use the multi-stage programming facilities if you want to extend the Julia compiler itself (see Cassette, and Zygote and XLA.jl as reference). So in a way, you're creating a dialect of Julia that should be compatible with the rest of the language (kind of like Elixir to Erlang at most, or more precisely languages implemented with Racket). If you want to create a language completely from scratch, then I don't know if Julia would be good because there probably aren't many tools for that job.
Why do you feel Julia would be a great match? Thanks so much for you advice!! :D
toy compiler? or professional compiler? Take my word with caution (let other people with more experience correct me) - I only wrote toy interpreters for fun - quite different from compilers :) &amp;#x200B; I think, it will be significantly easier and more fun to write a compiler in julia than in C/C++ due to more expressive nature and nice repl. Especially if you want just to play and experiment. If you know what you are doing, you probably know more than me. However, C/C++ should have more mature libraries (various parsers/printers what not). &amp;#x200B; If you compare to Haskell (and other ML derivatives, e.g., ocaml), than Haskell will be much more easier to use for compilers - it has nice abstract data types + pattern matching - a joy to work with tree like structures (i.e., abstract syntax tree). Also Haskell has nice monadic parsers which is also neat and simple :) If you do not know haskell already, it will be easier to start with Ocaml - it has all you need for compiler writing. For example first version of rust was written in ocaml (if I am not mistaken). &amp;#x200B; Lisps will be also a good pick - scheme/racket or common lisp for example. Nice to work with trees and expressiveness (Julia is very lispy so it has some of lisps benefits) &amp;#x200B; Anyway, if you just want to play and learn julia - julia is a nice language and writing a compiler is a nice exercise - I bet it will be fun. If you know what you are doing... you probably do not need my advice.
My first project will be a learning project, but later projects will be more serious undertakings. I feel extending the compiler could be a fun project for me. I understand what dialects are, I have been looking into the Lisp family a lot recently, so I have been learning quite a lot about them I could not use LLVM with Julia? I know I can use LLVM with C++, or Haskell. I really appreciate your help a lot, by the way! :)
I meant Julia should have more support for the LLVM for being an LLVM based language. https://docs.julialang.org/en/v1/devdocs/llvm/ https://github.com/maleadt/LLVM.jl
Haskell is very performant (usually comparable to equivalent C). GHC is mature and can make some very aggressive optimizations. That being said, learn both - each has its own idioms from which you can learn and become a better programmer. Julia and Haskell aren’t closed systems; they have a fair amount of potential interoperability through common compilation targets and shared libraries. Use whichever one suits your needs for your current application, and don’t be afraid to leverage both if needed.
Julia has functions-as-data, so it’s probably more idiomatic to use a higher-order function to do this anyway. Something like mapping a print function over a list `[0..9]`.
Ocaml is probably a better match, but Julia would work great. Julia's own compiler is largely written in Julia.
well it sounds like you have installation problems.... Those messages will contain **some** useful information, theyre not just to waste your CPU. :) Luxor.jl is a thin wrapper for Cairo.jl. If you can install Cairo, you shoukd be OK.
The REPL is a good place to learn the syntax of individual commands. People use Juno/Atom as a development environment, and some love their Jupyter notebooks. Then there those who use Vim or Emacs or VSCode. Problems with individual packages are harder to solve, unless theyre common ones. Sometimes your best bet is to visit the packages' github page... Does shell mode work on Windows?
First of all welcome to Julia. I know the people working on PowerModels and I'm sure they will help you whenever you have a problem. First I would check which version of JuMP you have. By going into the repl and then ] status The ] brings you into the package mode where you install packages. Status gives you information about installed packages. with_optimizer works with JuMP 0.19 BTW I assume you're using Julia 1 or 1.1 To get to shell mode you can use ; instead of ] I'm not sure how the shell is functioning in Windows systems. For Linux I can use ; cd .. Hope that helps
Is there a subreddit for persuvading people to stop using Windows and Nvidia propiatary shit?
OCaml would be a better choice.
&gt;Does shell mode work on Windows? No not really. If you use it in another terminal, Git Bash or WSL it should work though.
Thanks so much. Yes, I realized that my JuMP is 0.18.5, and I tried to remove it and add it again with rm and add, but Pkg.status("JuMP") is still giving me 0.18.5. Is there any way around it so that I can use the latest JuMP?
Can you check your Julia version? By default, Pkg.add should be able to install JuMP 0.19.
I have Julia 1.1.0. I just saw that current stable release is 1.1.1. Should I update to that?
I believe 1.1 should be fine. You can try getting the latest version by providing the github link in add package.
Are you using Juliabox by any chance?
Except for the availability of LLVM and a [reasonably powerful](https://github.com/maleadt/LLVM.jl/tree/master/examples/Kaleidoscope) wrapper LLVM.jl (I'm biased, of course, but we're using it pretty successfully in CUDAnative.jl), not many of Julia's strengths map particularly well on the specific use case of compiler development. The language will give you a productivity boost due to being high-level, but there might be better high-level languages for the job (eg. maybe OCaml, which also has [an LLVM wrapper](https://llvm.org/docs/tutorial/OCamlLangImpl1.html)). Of course, if you want your language to interoperate with the rest of the Julia ecosystem (eg. an embedded language), do implement that language in Julia and reuse as much of the existing compiler/language as possible.
You can use macro in program....
If I use @benchmark fn(), the output I get on the REPL and the program aren't the same. How do I get the REPL output on the program?
I removed both JuMP and PowerModels and now I have v0.19.1 JuMP, however it is restricting my PowerModels to be v0.9.6 (its current stable release is 0.10.0). How can I add the 0.9.6 PowerModels? &amp;#x200B; And no, I'm just using Atom/Juno on Windows 10.
You should `println` or `@show` the result of `@benchmark`. If you want the output as you would have seen in the REPL (does it even differ in the case of `@benchmark`?), you can call `display`.
As mentioned normally the latest should be installed automatically. I think you can use ] add PowerModels@0.10 to get it. This might tell you why it's not installing it because of some dependency issues.
Yup, I'll try my best not to restrict myself. :) I am not opposed to learning numpy. :D Oh this resource seems very easy. Thank you so much!
I have heard a lot of people recommend OCaml and recommend using it over using Haskell even. Is this because of some of the limitations of Haskell?
Oh I see! These links are going to keep me entertained for quite some time. Thank you so much! :)
So many people here are recommend that I go with OCaml. I think I need to go and check it out throughly. So I would write a language in Julia, if I wanted it to integrate more seamlessly with Julia?
Oh right, I see. I will keep this in mind while progressing forward.
Maturity is something that is highly appealing to me, so I like the sound of that. What kind of aggressive optimizations can I make? I think sometimes I fall into the trap of thinking I have to pick one thing, when I can actually pick more. :) I love science, math's and language, so I feel I could take a lot away from both of them, definitely. Leveraging both of them sounds like a very exciting idea! \^\^
&gt;1) So far almost all the tutorials I've seen run snippets of code on the Julia REPL. Hi! I’m new to Julia as well. Did you know you can access a bunch of tutorials in Jupyter notebooks here: https://juliabox.com/ Just log in and head to the tutorials in the jupyter notebook. Hope it helps!
Try to run the PowerShell as admin
Tried it. Same result.
It's probably just the design choices. Haskell is a very strict pure functional language, you can never have functions with (implicit) side-effects (such as I/O or variables) so you have to use monads to implement any imperative/state manipulation (which is definitely complicated for beginner to design your software around it). And everything is lazy evaluated which can make it hard to debug some aspects. But since haskell is so strict it forces you to really understand it's paradigm. Ocaml is multiparadigm so you can always fall back from functional and use whatever is more appropriate for each problem.
have no clue, but i have a lot of ideas the folder C:\Users\ryan\.julia exists? the file listed exists? what if you interactively run julia, and add the package manually? is there any chance powershell runs in the name of another user? (other than ryan) from the logs, i can see that cygwin was installed as Administrator. maybe Julia was too?
C:\Users\ryan\julia exists, but there is no Registry.toml at the path provided. "ryan" is the only user. I have no idea about Cygwin. This is essentially a fresh installation of Win10 and all I've done is run the installer from the Julia site.
i would assume that the toml file was created in the users/administrator folder. there is some way to convince julia that the root is there, but then you need to give it write privileges. probably you are better off hacking the system by moving the files from the administrator/.julia over to your profile. my windows admin-fu is limited, but i think if you run a program "as administrator", the privileges are escalated but the current user remains ryan. however, if you run "as another user" and specify administrator, software will be installed to the administrator profile. i suspect this is what happened. if administrator/.julia does not exist, or the toml is not there, then i'm out of ideas.
why is windows the way it is
I know this isn't the answer to your question. But I would recommend downloading Linux subsystem on windows and use that for Julia. It works smooth on that system. Julia on windows also had trouble installing notebook when I had a previous anaconda installation on windows.
i'm gonna be real witchu... i'll probably just install manjaro or something soon
i'm more and more inclined to think that out of malice
I Googled everywhere for Cairo. I see there is a reference to Cairo.jl which must mean it is a Julia plug-in. Is there any command from the julia&gt; prompt where I can "import" the Cairo package. Many thanks !!
So OCaml would be less strict and allow me to avoid using monads and also be easier to debug? I feel you have really sold me on OCaml, I'll check it out. :)
Julia like most languages relies on packages which may also rely on binary libraries. Julia has a built in package manager and you use that to download the ones you need from GitHub (usually). Once they’re successfully downloaded they will also need to be compiled (“built”). You need to look carefully at error messages if you don’t succeed in loading packages. People can’t help if they don’t know where the problems are occurring! 😀
This is what I'm getting: (v1.1) pkg&gt; status Status `C:\Users\me\.juliapro\JuliaPro_v1.1.0.1\environments\v1.1\Project.toml` [c52e3926] Atom v0.8.5 [7073ff75] IJulia v1.18.1 [916415d5] Images v0.18.0 [4076af6c] JuMP v0.19.1 [`C:\Users\me\.julia\dev\JuMP`] [e5e0dc1b] Juno v0.7.0 [91a5bcdd] Plots v0.24.0 [438e738f] PyCall v1.91.2 [d330b81b] PyPlot v2.8.1+ [`C:\Users\me\.julia\dev\PyPlot`] (v1.1) pkg&gt; add PowerModels@0.10 Updating registry at `C:\Users\Jeffrey\.juliapro\JuliaPro_v1.1.0.1\registries\JuliaPro` Updating git-repo `https://pkg.juliacomputing.com//registry/JuliaPro` Resolving package versions... ERROR: Unsatisfiable requirements detected for package PowerModels [c36e90e8]: PowerModels [c36e90e8] log: ├─possible versions are: [0.1.0, 0.2.0-0.2.3, 0.3.0-0.3.4, 0.4.0, 0.5.0-0.5.1, 0.6.0-0.6.1, 0.7.0-0.7.2, 0.8.0-0.8.8, 0.9.0-0.9.6] or uninstalled └─restricted to versions 0.10 by an explicit requirement — no versions left (v1.1) pkg&gt; add PowerModels@0.9.6 Resolving package versions... ERROR: Unsatisfiable requirements detected for package JuMP [4076af6c]: JuMP [4076af6c] log: ├─possible versions are: 0.19.1 or uninstalled ├─JuMP [4076af6c] is fixed to version 0.19.1 └─found to have no compatible versions left with PowerModels [c36e90e8] └─PowerModels [c36e90e8] log: ├─possible versions are: [0.1.0, 0.2.0-0.2.3, 0.3.0-0.3.4, 0.4.0, 0.5.0-0.5.1, 0.6.0-0.6.1, 0.7.0-0.7.2, 0.8.0-0.8.8, 0.9.0-0.9.6] or uninstalled └─restricted to versions 0.9.6 by an explicit requirement, leaving only versions 0.9.6
Haskell is more hyped than Ocaml, but they are both beautiful languages worse learning. Each language has features the other miss (e.g., typeclasses vs modular Functors, bla bla). However, they share a lot, and if you know one, it would be relatively easier to learn another. It would be easier to start with Ocaml. In my opinion Haskell is more elegant, but Ocaml is more pragmatic (again only my opinion - some people would argue). Which is more practical? well, this is a holy-war type question :) and because Haskell community is more vocal at the moment, you will find more comments "For Haskell" than "For Ocaml" in the web. Truth - every one of them would be a fine choice. &amp;#x200B; "So OCaml would be less strict and allow me to avoid using monads and also be easier to debug?" Ocaml would me more strict (pun intended). Yes, in ocaml you do not have to use monads and it is easier to understand and debug coming from typical C-type language experience. Not only due to monads and purity, but haskell is also lazy by default which is quite different :)
Oh you're using JuliaPro? Not sure whether that changes something. Anyhow normal adding works for me: not cloning the repository. Simply ] add JuMP and add PowerModels
Wait what's the difference between JuliaPro and regular Julia? I didn't even realize that was a thing.
https://discourse.julialang.org/t/julia-pro-packages-vs-regular-packages/17232
So you are saying you feel it would be easier to learn one after, but easier still if you learn OCaml first. So you don't have to deal with monads and it's stricter. I did not realize it is easier to debug. This laziness thing in Haskell is quite fascinating to me. I don't think I have ever heard about it before.
Just look at Julia generators such as (x for x in 1:100). They are a promise to do some work eventually. In haskell everything is like that, it defers evaluation until it's needed (for example if you need to print them). If you want to defer general evaluation in Julia (outside of generators) you can either box something in a anonymous function such as () -&gt; [x for x in 1:100000] or use macros (which does not evaluates it's arguments like functions do).
Then, if the Haskell programming language were a person they would be that person who always puts things off until they have to do those things. Haskell's laziness just sounds like me lol! :D So in Haskell everything is like this, but in Julia it's optional. Julia let's you put off evaluation via anonymous functions &amp; macros. Wait, do you mean macros don't evaluate at all, or that they evaluate in a different way?
Macros only see literals and primitives. macro print_macro(a) print(a) end @print_macro(function_that_does_not_exist([array, with, some, stuff]) # output: function_that_does_not_exist([array, with, some, stuff]) function_that_does_not_exist([array, with, some, stuff]) # output: ERROR: UndefVarError: array not defined Notice that array variable is the first thing to give an error (even though everything there should throw an error), because when you call a function it's arguments are evaluated first, thus before they are even needed (which is strict evaluation).
Julia itself is written in Scheme and Julia, as I know, right? So I don't see problems. As for Haskell, most famous compiler written in Haskell is GHC itself, and it's fantastically slow and super big (deps, etc). Generated binaries are also very big, so Haskell will be a bad choice.
Julia parser is written in scheme/femtolisp, and Julia's library (every type and method) is written in Julia (plus LLVM IR). The compiler itself is written in C/C++ (and there is probably not a lot of motivation right now for bootstrapping Julia, especially since the design of the language already focuses on it being able to extend itself without changing the compiler, for example JuliaInterpreter used in the Debugger is just a Julia library).
Yes JuliaPro was the problem. Now I got it working. Thanks so much!
Maybe there's a issue on GitHub
maybe [this](https://www.reddit.com/r/fsf/) :)
Try asking this on the Slack channel, the community is much more active there and most likely you'll be able to directly chat with Juno developers.
Nevermind. I see now that NEW layers are being created each time I call the anonymous function which becomes the "model".
That's definitely a kind of unusual way of doing it. The more usual way is having a struct containing the params of your model, with which you define a function that defines the forward pass. Like the last example in the documentation: https://fluxml.ai/Flux.jl/stable/models/basics/#Building-Layers-1 https://docs.julialang.org/en/v1/manual/methods/#Function-like-objects-1
So, this is how I fixed it, just to be clear: function create_model(board_size::Int16) # Individual layers conv1 = Conv((3, 3), 11=&gt;50, relu) conv2 = Conv((3, 3), 50=&gt;50, relu) conv3 = Conv((3, 3), 50=&gt;50, relu) conv4 = Conv((3, 3), 50=&gt;50, relu) processed_board = Dense(50, 500) policy_hidden_layer = Dense(500, 500, relu) policy_output_layer = Dense(500, board_size^2) value_hidden_layer = Dense(500, 500, relu) value_output_layer = Dense(500, 1, sigmoid) # Assembled layers conv_chain = Chain( conv1, conv2, conv3, conv4, (a) -&gt; reshape(a, (50, 1)), processed_board) policy_chain = Chain( policy_hidden_layer, policy_output_layer, softmax, (a) -&gt; reshape(a, (board_size, board_size))) value_chain = Chain( value_hidden_layer, value_output_layer) function (x) conv_chain_output = conv_chain(x) return (policy_chain(conv_chain_output), value_chain(conv_chain_output)[1]) end end It's just parametrizing the model on the board size. You can generate a different model for each board size. The layers are defined outside the anonymous function which gets returned and becomes the "model".
Yes, that solves it. conv_chain, policy_chain and value_chain will only run once (when calling create_model), they'll be stored with their weights as parameters of the closure and used for every call of model() (instead of recreating them at every call) . fieldnames(typeof(model)) # (:conv_chain, :policy_chain, :value_chain) model.conv_chain # Chain(Conv((3, 3), 11=&gt;50, NNlib.relu), Conv((3, 3), 50=&gt;50, NNlib.relu)...
I guess it is a bit weird because `params(model)` doesn't work. You have to do something like `params(model.conv_chain, model.policy_chain, model.value_chain)`.
It's not weird in the sense that params can't understand arbitrary structs and figure out how to extract the data (and you can create a method that handles that), but there could be a way so you don't have to manually track the params of your custom layers/structures. Especially when the example I passed first says "Congratulations! You just built the Dense layer that comes with Flux." but the example fails to include the mechanism that exposes the params into the chain.
http://pages.stat.wisc.edu/~bates/JuliaForRProgrammers.pdf Also ask on the forums about specific issues, since there are probably quite a few who already traveled the same path.
I suggest that you make an account at Juliabox and try out the tutorials available there. It is not usually suggegested that you try to program in one language just the way you would do in another language, as every language has its quirks and conventions... You will naturally understand what is similar and what is different by studying on a "general purpose" book. If you want to buy a textbook I suggest "Julia 1.0 programming - second edition" published by packt
There is also this book for statistics: https://www.reddit.com/r/Julia/comments/bmptuy/statistics_with_julia_draft_book_update_with/ and the best guide towards the language is the manual: https://docs.julialang.org/en/v1/ If you need anything from R when you're writing Julia, you can do it through RCall (though you might avoid it during the learning phase): https://juliainterop.github.io/RCall.jl/stable/ And Julia also has an equivalent of tidyverse, though still much younger. https://www.queryverse.org/
This would be cool except I can't sign up.
The guys who developed that were on this subreddit a few weeks ago. Personally I’ve been using containers.
Cool. This is not a replacement for containers--it works with and improves upon containers. I think the drag and drop interface is great. What do you think? Does it seem like it would be an improvement for your specific projects? Where are you deploying your remote runs?
there is an official julia docker image. amazon ecs natively runs containers, and can connect them in a local network for multi node computing. of course there is quite some learning curve. but that's to be expected.
I’m generally not a fan of drag and drop, I usually prefer code that can be executed, because then everything can be automated. Currently I’m working locally and deploying packaged models into containers running on Kubernetes via a build pipeline. In the future I’ll be doing the model training flows via a workflow framework like Argo.
learning curve... exactly. This gets around that completely. It's genius in that sense. It's potentially like driving instead of walking. Even once you get over the initial learning curve with amazon, it seems that this could be faster on a day to day basis. Does that seem right?
Weird, you didn't get some kind of "welcome" message?
I just signed up and got a welcome message from one of their team members.
nope
why do i have the feeling that this post is not a genuine question, but you are advertising this tool
 R: lapply(myList, function(x) 2*x) J: map(x-&gt;2x, myArray) R: dbinom(2,100,0.5) J: pdf(Binomial(100,0.5),2) That's the extend of my knowledge on the subject.
Ah, sorry about that--I definitely work with the guys who created this, on some of their other projects, so it makes sense that I sound biased. I'm trying to gauge the degree to which there is real use for this, or this kind of tool, in the Julia community, as the developers allege. As I've been saying, it seems great to me, but I'm genuinely asking questions to learn more.
&gt; 2013 It’s not like Julia has changed since then...
Yes, written for version 0.1, so a veritable antique... ! Apart from some syntax changes (in Julia, not sure about R), most of the conceptual information is still true, I think.
You basically said all the things yourself. We can't help you without actually look at your code, Julia should be as fast as TF, notice TF is not Python but C++ speed under the hood.
Have you converted all your inputs and outputs to `Float32`? Otherwise, Flux will not hit multithreaded BLAS, and become really slow in both forward and backward passes.
It will still do multithreaded BLAS. All of Julia uses multithreaded BLAS with Float64.
From my experiments it won't if there are mismatching types. So if your data in `Float64` and your weights are `Float32` it's orders of magnitudes slower.
This seems likely, I'll double check.
&gt; Does that mean that it's using BLAS? It's totally using BLAS, the question is if it's using it multithreaded :) But, as you said, if it's using all your cores (and I mean 100% utilization with non-trivial networks), then you're good on this front, so I'm out of ideas. -- How much slower are we talking here? Also, what kind of layers are you using?
[https://github.com/DevJac/julia\_gobot](https://github.com/DevJac/julia_gobot) There's the repo. The learning stuff is in `nnbot.jl`. I know in Keras I would built up a replay memory of 500,000 moves or so (I'm trying to do something like AlphaGo), and it would burn through batches of 1000 every one or two seconds. With Julia it was taking close to 90 seconds for a batch of 1000. I know my Julia network was a bit bigger, so it's not an apples to apples comparison.
I suspect Flux.jl doesn't use MKL-DNN which is a library optimized for deep learning on Intel Processors. Some of the popular Python libraries use MKL-DNN but I'm not sure if Torch makes use if it.
[https://www.crcpress.com/Data-Science-with-Julia/McNicholas-Tait/p/book/9781138499980](https://www.crcpress.com/Data-Science-with-Julia/McNicholas-Tait/p/book/9781138499980) &amp;#x200B; good book for people with r background (as i am and turn to julia for the same reason (i.e. big data limits in R), i liked this book a lot). &amp;#x200B; also take a look at the queryverse, which allows you to use pipes as in the tidyverse for a variety of data structures. [https://www.youtube.com/watch?v=OFPNph-WxLM](https://www.youtube.com/watch?v=OFPNph-WxLM) &amp;#x200B; video - queryverse stack also includes tips on visualisation and interactive visual exploration of your data. quite powerful. Is actually more versatile as you can query a variety of data structures with it. However, if you need high performance for operations on tabular data, there is no way around either the dataframes package or juliadb and onlinestats for big data. the queryverse is easier to use and versatile, but is actually nothing more than a row iterating query interface. Which works quite well in Julia, unless you deal with large datasets. For those you need to know the apis of the original packages. there is like juliadbmeta &amp; dataframesmeta for piping. &amp;#x200B; for ml as in mlr, check out [https://github.com/alan-turing-institute/MLJ.jl](https://github.com/alan-turing-institute/MLJ.jl) &amp;#x200B; However, do not expect the maturity you are used from R for all r equivalents in julia. it is getting closer, but julia is not there yet with regard to usability for data analysis as compared to R. I expect it to catch up with R for main, butter and bread functionality in like two years or so. Not all of crans highly specialized stuff but the usual data manipulation, statistics, ml and visualisation stack. Right now, the main functionality for data science in Julia just works well enough to work with it, but not as elegant as in R, though. Only basic stuff is included right now, you may quickly have to implement something for yourself. Your productivity may suffer a bit from that, but then again, it is a really nice thing to have a language where you can see through easily till the lowest level of it. There is no hidden layer of c code as in i.e. r data.tables which you use. You can modify and extend anything, which is very handy I think.
Yes, ADAM only takes eta and beta. That other way of calling is from an older version. Doesn't it work with [callbacks](https://fluxml.ai/Flux.jl/stable/training/training/#Callbacks-1)? You can see with "? Flux.train!" in the REPL: it takes train!(loss_function, model_params, data, optimizer; cb = optional_callback). If you do not add biases, your network will not be able to approximate any function that does not cross the origin. I don't see many people not using them, though you occasionally see books/papers simplifying it and adding it to W by having one of the inputs fixed as 1. You can see that Dense(2, 1) in Flux has it, by doing params(Dense(2, 1)), Dense(2, 1).W, Dense(2, 1).b. I wouldn't mind much that CuArrays failed in a few test, that can just mean some missing functionality (for example from cuda 10.0 instead of 10.1). I feel like gpu would be exported if you did using Flux even if your GPU didn't work. You could just use cu(array) instead if that works though. You could try taking a look at the [model zoo](https://github.com/FluxML/model-zoo) as reference for Flux. There is probably not much performance difference between flux and knet as long as they are using the main cuda libraries like CuDNN, but I've not seen benchmarks. They are both good libraries.
Great. Here are a few things I believe people should look at when checking out Julia just from a business perspective: &amp;#x200B; [https://github.com/tensorflow/swift/blob/master/docs/WhySwiftForTensorFlow.md](https://github.com/tensorflow/swift/blob/master/docs/WhySwiftForTensorFlow.md) [https://www.youtube.com/watch?v=\_\_gMirBBNXY](https://www.youtube.com/watch?v=__gMirBBNXY&amp;t=1326s) &amp;#x200B; Also, there's a new Julia for Stats book that I think could turn out to be a great book for people who want to use Julia professionally. &amp;#x200B; [https://people.smp.uq.edu.au/YoniNazarathy/julia-stats/StatisticsWithJulia.pdf](https://people.smp.uq.edu.au/YoniNazarathy/julia-stats/StatisticsWithJulia.pdf) &amp;#x200B; Also: don't take it too seriously. I think people are very excited about Julia, but people need to keep in mind adoption for programming languages is actually on the decade scale. Personally, I remember up until 2013 Python was still regarded as a toy language when Java was the standard for what "real" software engineers used. Python was started in 1990. Just keep this in mind and remember it's going to take a long time before there's serious adoption in Julia.
Thanks a lot! I'll try callbacks and keep looking into the CuArrays documentation. As for bias, [model zoo mlp](https://github.com/FluxML/model-zoo/blob/master/vision/mnist/mlp.jl) here is why I asked: the two dense layers are basically the weight matrices connecting input &amp; hidden layer, and hidden layer &amp; output right? And hidden layer size is 32. But there isn't a bias, since I thought the second dense layer would be (32+1) x 10 if there was. Could you clarify that?
The nice thing about Julia libraries is that you can easily see what happens, since it's all written in a high level language: https://github.com/FluxML/Flux.jl/blob/master/src/layers/basic.jl # L82-86 struct Dense{F,S,T} W::S b::T σ::F end We can see that Dense has 3 parameters, the weight matrix, the bias and the activation function. You can see in the constructor in L90-93 that W has dimensions (out, in) and b (out) when you call Dense(in, out, activation). As for what it does: # L97-100 function (a::Dense)(x::AbstractArray) W, b, σ = a.W, a.b, a.σ σ.(W*x .+ b) end So it applies the affine transformation (including the bias) and apply the element-wise activation function in a very standard way. If you want 32 elements input and 16 output (or 16 neurons in the hidden/output layer) for example, you'd do your W (16, 32) since W(16, 32) *x(32) = (16). A sum of vectors does not change the size, so (W*x) (16) + b (16) = (16) (same for the activation function). Which means the output size of one layer is completely independent from the bias, so you could remove it or add it without any change to the topology of the network. The bias is actually just an offset learned and applied in each neuron so it can produce results different from 0 when all weights are 0 (W*0 = 0 for every W). I really like this channel, and it has a nice small series about neural networks that helps visualizing it better: https://www.youtube.com/watch?v=aircAruvnKk
You could use the case studies in julia computing website: https://juliacomputing.com/case-studies/ If I remember correctly, Julia was the first (and still only?) dynamic language to reach the petaflop scale. https://www.nextplatform.com/2017/11/28/julia-language-delivers-petascale-hpc-performance/
In my team we had some success using the package compiler. https://github.com/JuliaLang/PackageCompiler.jl
I just ran in the console `export CXXFLAGS=-D_GLIBCXX_USE_CXX11_ABI=0` and it is running so far. Waiting for the next error in the mean while.
Isn't GDPgrowth a 1D array? Then in the for loop it would be GDPgrowth[t] = ... The way it's written is as if GDPgrowth were 2-dimensional, hence the error.
Damn dude I feel like a huge dipshit right now. Thanks so much for that.
Hey, just a quick style note: `T` as a variable is normally used to refer to a type. So when I read `zeros(T)` I thought you were trying to create zeros of a particular type. There's nothing really wrong with using `T` for other things like you have, but as things get more complex it's a bit useful to be in the habit of using standard nomenclature especially in terms of collaborating with others.
Which version of Julia are you using? I was getting this error message with version 1.0.4, and it stopped after I updated to version 1.1.1.
The problem is already solved, so more of a general fyi: Writing or importing a function that returns the n-th lead or lag is something I always use all the time when working with time series. So you can just write `growth = gdp ./ lag(gdp, 1)` Where lag is a function that returns an array with lag 1 (or lead if negative) and handles the first/last n observations. It doesn't matter much for simple problems like this, but if you do macroeconomic work you'll often end up doing this a lot. &amp;#x200B; I don't know if it's the performance-wise best as you generate a new array, but it makes the code less error prone and easier to read. If you're not really a programmer but write a lot of code (like me), those two factors matters more than saving a second or two at runtime.
So would have been more appropriate to simply write zeros(288) for this simple for loop?
This specific course is macro based but I'm hoping to focus more on the micro -level when I finish up my studies in two weeks time. The challenge has been learning Julia and improving my R toolkit at the same time. I've been focusing more on Julia b/c this quantitative course has been a lot more challenging than my econometrics course.
What package do I install to use the lag function?
This is not Julia related, but in case you were interested, r/badeconomics is where the most interesting (and non-bulshitty) econ discussions happen on Reddit. Most of the people there are either grad students or full PhDs (and some are neither, lurkers like me). They tend to know their stuff. A few of them use Julia, most use either Stata or R or Python (and then making fun of each other’s software is a running gag/meme). The younger students tend to ask questions in the fiat threads related to technical work. Just thought you’d be interested to know.
If something works in 1.0, it should work in 1.1. Have you tried it?
I haven’t, the version checker is preventing me from trying it.
Assuming this is the package you're trying to use, have you looked at [this issue](https://github.com/JuliaAudio/PortAudio.jl/issues/34)?
A little bit, but not completely. When I have more time, I’ll look into it ~~now that I know it applies to 1.1 as well~~
There's a package called TimeSeries.jl Or you can make your own simple one like: ```function lag(ts, n) Lts = [] if n &gt; 0 append!(Lts, repeat([missing], n)) append!(Lts , ts[1:end-n]) elseif n &lt; 0 append!(Lts, ts[1+abs(n):end]) append!(Lts, repeat([missing], abs(n))) end return Lts end ```
Thanks so much!! I got most of it working, but I just have another question about storing training/validation loss/accuracy values in callback if you don't mind: [https://stackoverflow.com/questions/56333944/recording-accuracy-and-loss-data-in-callbacks-in-flux-jl](https://stackoverflow.com/questions/56333944/recording-accuracy-and-loss-data-in-callbacks-in-flux-jl)
Thanks so much! It's working now :)
I get "input/output overflow" message when I playback &amp; record using PortAudio.jl on Julia 1.1.1. But otherwise, I think it's working fine. (I installed PortAudio.jl with "add PortAudio#julia1")
No, certainly not :) Hardcoding numbers like that is normally bad practice. For example `N` or `L`, or maybe something more descriptive, `numquarters`??. `T` is okay, but I, too, first misread your code, thinking `T` was a type variable (it almost always is in Julia.) Better yet, where does the number 288 come from? Is it the length of `GDP`? In that case you should write ``` GDPGrowth = zeros(length(GDP)) ```
I have a new idea to use Julia as a benchmark. I can run the testsuite and time how long it takes. The testsuit uses all threads. I am still in limbo how long it is supposed to take to compile Julia from scratch. I just coned Julia from scratch (github) and Julia compiled within 30 seconds. That doesn't seem right, because last time it took several hours.
You could try [SDL2](https://github.com/jonathanBieler/SimpleDirectMediaLayer.jl) I think there's support for audio (look in the examples).
Ahhh, yes, you're exactly correct. That's so simple yet makes so much more sense. It's interesting how Julia seems less intuitive to me than R does. I think it's because Econometrics is more straightforward to me while these stochastic processes are more foreign. My linear algebra is too weak for this course, I think.
You might want to check out https://github.com/JuliaPlots/Makie.jl
My goodness, I feel I’ve been graced by Julia royalty. Any advice for an aspiring Julia coder? Thank you by the way, I’ll take a look.
Did you get to setting up Part E?
The old version of https://lectures.quantecon.org/jl/matsuyama.html used to be interactive (with a slider.) PM me and I can send you that source.
This was done with Makie https://github.com/r3tex/LossSurface
I don't know if this is the kind of thing you're looking for, but this one is really cool https://github.com/SimonDanisch/SchroedingersSmoke.jl
That’s cool! Not related to what we do, but that’s ok, it looks awesome. The point is to say, “hey look what Julia can do.”
Thanks!
Thank you for sharing this! This is fantastic. Do you understand this loss function surface plotting? I don't understand how they create it. In my mind one way to do is it to plot do a brute force iteration by changing the weight parameters one by one and create the loss landscape like that. How do they achieve it in this example?
Julia's good for making eye candy. And data visualisations. Fast and powerful, and easy to write. There might be something [here](https://cormullion.github.io) that piques your interest...
Check out VegaLite.jl if you’re in need of quick interactive data visualization
It's my repo and visualization so I think I understand it ;) Your intuition is pointing in the right direction. If you click on the ipynb file itself you'll find I've written an in-depth explanation for every step. The code is simple enough that you cant try it for yourself!
Nice! Is everything on your Flickr made with Julia?
Awesome, will do!
Yes - except the photographs of the computer screen... :)
That is awesome. Thanks!
This is fantastic. I am going to show it to my colleagues who teach this stuff.
You images are awesome! Big fan.
:)
That looks great! Stuff like this is why I have such a hard time focusing one learning the basics of Julia properly.
Usually people just write the differential equations. This is an older example of that: https://github.com/JuliaMPC/MichiganAutonomousVehicles.jl
i'm just guessing, but i think it boils down to concatenation being a completely different operation than either multiplication or addition, so the usual "just use + because logical" is not very logical. however, concatenation is a semigroup operation, and in abstract algebra, they often use * to denote the single operation of an algebraic structure. you often see people talking about "discrete *logarithm* problem", implying that the group operation is "multiplication", while in some cases it is nothing like a multiplication at all. so you can think of it as an "asterisk" and not a multiplication sign.
Solved: https://github.com/pkofod/PlusPlus.jl
Julia's an incredibly neat language, but its one weakness is its unnecessarily obscure syntax. Clearly, the problem is that the people who design it have been in academia so long that [simple concepts are thought of "most easily" in terms of ](http://www.smbc-comics.com/?id=3565#comic)fundamental mathematical properties. To me, if you've gone that far, you've already made it too complicated. If you have a box with one apple and another apple, then you have a box with two apples. That's all addition means. And if you have a string with "John " and a string with "Smith", then you have a string with "John Smith". The argument that that is confusing because addition is fundementally an associative property (which string concatenation isn't) is absurd. No. Addition is fundamentally "one apple plus one apple is two apples".
Isn't any string basically character^n? "Apples" is char^6 and "Oranges" is char^7, so "Apples"*"Oranges"="ApplesOranges" is char^13. In that case your argument with dimensionality would be actually in favour of * for concatenation.
Did you see https://unitylist.com/p/d1j/Unity-.jl (https://github.com/StreetLevel/Unity.jl)
Best README.md
What? Julia's syntax is pretty conventional. In extreme cases you can basically take Matlab/octave code and port it to Julia with zero changes.
What if I want erlang-style &lt;&gt; ?
I love Julia's choice to use \`\*\`, even if I weren't familiar with abstract algebra simply because \`a \* b = ab\` from elementary algebra. \&gt; Addition is fundamentally "one apple plus one apple is two apples". That position justifies using \`cat("John"," Smith")\` and throwing out syntactic sugar (which is consistent, but not popular).
haha the tests... [https://github.com/pkofod/PlusPlus.jl/blob/master/test/runtests.jl](https://github.com/pkofod/PlusPlus.jl/blob/master/test/runtests.jl)
A vararg ``concat(x::String...)`` would be my preference. Why? Because sometimes you have code reviews with customers who come from many backgrounds and you cannot assume one background or another. So any operators which make sense with one background would not likely make the same sense to anyone else. And if you use that ``concat()`` for many enough arguments, you can neglect the start and then the concatenation character between the arguments becomes ``,`` :)
To my knowledge PackageCompiler.jl is the only way. The Julia community is pretty small and what they do care about is pretty narrow. I’ve given up hope of Julia ever having first class support for compilation. Good luck.
Use `string()` for varargs and conversion. The source code for `*` is `(*)(s1::Union{AbstractChar, AbstractString}, ss::Union{AbstractChar, AbstractString}...) = string(s1, ss...)` so you can even call it like `*("Hi ", "there ", "world")`. `string()` however will convert other types for you: `string(1,2,3)` returns `"123"`.
There's also https://github.com/NHDaly/ApplicationBuilder.jl
https://discourse.julialang.org/t/compiler-work-priorities/17623 The priority for the main compiler team at this point (until Julia 1.3 at least) seems to be getting the multithreading framework finished (though other stuff like the Debugger situation has improved in the mean time). After that hopefully they'll be able to focus on the time to first plot issue which could be solved with better pre-compilation (though like you said, Julia cannot be compiled like static languages can, which makes it a really hard and still unsolved problem).
I wouldn't be so pessimistic. Julia has been gaining a lot of big users who are typically influential in their fields, such as NASA and many other government orgs, as well as large financial institutions (blackrock), other big companies, and major research centers. With time that should lead to a stronger developer community
I've managed to compile simple programs statically before, but that included big parts of the Julia runtime in the binary and then it has to be wrapped with a basic main function in a language like C
Hopefully, to be totally open, after coming from the Rust world, the culture around Julia just feels dismissive and unhelpful. It's weird small but also advanced in very niche areas.
I have noticed that they're very helpful in the parts they're most interested in. Everything else usually gets a "if you implement it, cool, we'll consider merging it"
I just want to be able to compile to byte code like Java. Recompilation every single time has knobs on it.
It's because Julia is actually targeting niche areas compared to Rust. Julia started as a scientific computing language and keep it that way. And this actually takes Julia to a better position compared to Rust since the focus is more... focused.
For Julia that would be the same thing as compiling to machine code since it doesn't have a virtual machine, once it compiles to LLVM IR the LLVM (which is not a VM even with that name) can just generate machine code. Though of course if it did have a VM (like most other dynamic languages) it would be able to avoid the compilation issues in the first place (since it wouldn't really compile) but it would probably be much slower.
The thing is that a lot of people find themselves in the situation where they don't have the time to do too much in terms of implementing feature requests - most of us actually do something else (i.e., not programming) for a living!
Noob question: this works with v1.0 and above? A line in the read me had a reference to v0.6.
It should. One of the releases even dropped v0.6 support.
Didn't you do a data science video? would like to see that again...
Cool. Have you built any apps with this yet?
Hehe. Guess I'll have to make another on this year with more explosions and special effects. https://youtu.be/ESCrVfQJLO8
No, I just came across it at some point and thought it was relevant.
I don't think it does much more than PackageCompiler, it just makes the build process a bit easier.
Quite a bit https://en.m.wikipedia.org/wiki/Recursion_(computer_science)
Exactly. I'm not sure if the "file-based recursion" does anything different than the function-based recursion in Julia, but since including a file is essentially the same as loading codes, I guess this isn't anything new.
Reading the description I don't think something like that is possible (at least in that form) for Julia. Julia is not an interpreted language, it's a dynamically compiled language. Any long running Julia program that doesn't involve parsing new user input (like the REPL) will eventually end up fully compiled by the combination of each "Just Ahead-of-Time" compilation of every reachable branch. An approach that might work with Julia is by injecting code during that compilation phase that profiles every function using the benchmark and keeps the data. That would be similar to how [MagneticReadHead.jl](https://github.com/oxinabox/MagneticReadHead.jl) inserts all the debug information within the compiled code using [Cassette.jl](https://github.com/jrevels/Cassette.jl). This way there would be no need to modify the code before the profiling, but you probably won't be able to intercept an already running process (since Julia will not try to recompile any code that is unchanged).
What does py-spy do that you're missing from the built-in profiler? I use that extensively, and it works well for my needs.
The ability to attach to another process is nice, but I haven't used it much. I mostly just like the top-like interface. It makes it easy to see how much time is spent in a function and its subroutines, and how much time is spent in the function excluding its subroutines. Just a single button press to switch. It shows percentages rather than raw sample counts. It can profile functions as a whole, or individual lines. Julia has all the data to calculate these things, but as far as I know I'd have to write a bunch of my own code to do those calculations.
Just-In-Time?
It's definitely a JIT compiler, just an extremely aggressive one that would try to compile the entire program at once if it can predict all types from the very first instruction (and Julia has a powerful compile time environment, which also makes it more complex than usual).
Sounds like you have the answer to your question then. As long as you're content with having a view only *after* a function completes, it's totally possible, and most of the infrastructure already exists. For a live view, I'm thinking that one is probably possible using Cassette.jl, but it would be much more difficult.
If you figure out how to compile your Julia to a native exe compiled on a linux compatible with Amazon linux (most people use docker for lambda cross compilation), you should be able to use the "native runtime" mode of lambda to use an HTTP client to request new events and process them using Julia json parsing/encoding.
I've used [this container](https://hub.docker.com/r/lambci/lambda/) for compiling native code for AWS Lambda. It works pretty well.
If you do not have the Mosek distribution binaries installed, the .jl package will try to download them from the Mosek website. However, if you have previously installed Mosek.jl and a path to the binaries exists, but the binaries are not there, this might throw an error. Try the following commands: `Pkg.checkout("Mosek","master")` [`Pkg.build`](https://Pkg.build)`("Mosek")` If this doesn't work, check the documentation for Mosek.jl [https://github.com/JuliaOpt/Mosek.jl](https://github.com/JuliaOpt/Mosek.jl)
Just a heads up, you’re doxxing yourself a bit here.
In case you are referring to the user name being visible, not really, "Bachelor Arbeit" is not a name, it just means bachelor's thesis.
I don't want to annoy anyone. I really like Julia and the community. So thank you for the feedback!
I've tried both commands and after the first command I've got this. [https://photos.app.goo.gl/8ptapSGUqkpjTTKR6](https://photos.app.goo.gl/8ptapSGUqkpjTTKR6)
`checkout` is deprecated in 1.0 - try `]add Mosek#master` instead.
Just want you to stay safe! Nothing annoying, just important to some people! It's a great community!
Thought it was a weird name but all good then haha!
Fuck u fuck u fuck u fuck the whole worldvevbshsnsvjdhxjdbnevejxybecejegsjs k svekavJvajVajsvssjbsvsvsvsjsvsbbs
Fuck u fuck u fuck u fuck your mom fuck your grandmom fuck fuck duck
Bitch fuck fuck
Fuck fuck fuck duck duck
Fuck fuck fuck Fuck fuckfuck Fuck fuck fuck Fuck fuckfuck Fuck fuck fuck Fuck fuckfuck Fuck fuck fuck Fuck fuckfuck Fuck fuck fuck Fuck fuckfuck Fuck fuck fuck Fuck fuckfuck Fuck fuck fuck Fuck fuckfuck Fuck fuck fuck Fuck fuckfuck
I think it's a windows problem. I tried it with linux and everything worked find. I tried the command above and got this: [https://photos.app.goo.gl/u9AxSYG46yJxd4f66](https://photos.app.goo.gl/u9AxSYG46yJxd4f66)
Maybe raise an issue on Github?
This is amazing progress. VectorContinuousCallback seems very useful! I'm guessing the other comment in this thread was from a MathWorks employee.
Oh fuck
Yes, they are separate packages with separate functionality and you have to add them separately (but they are compatible) https://juliadata.github.io/DataFrames.jl/stable/man/getting_started.html#Importing-and-Exporting-Data-(I/O)-1 Alternatively Queryverse is a meta package includes multiple packages including one for decoding CSV files and dataframes, if you want a single package. https://www.queryverse.org/
Thanks! Now you got me wondering if I should include that in my decision.
https://app.codesignal.com/signup/NaqMfsKPZ6h7Av5mH/main
Sort of, but in Julia's other domains which are math/science, for example [calculus](https://calculuswithjulia.github.io/) and [economics](https://lectures.quantecon.org/jl/), I'm not aware of one focused on the general programming applications of Julia's paradigm (such as multiple-dispatch).
Unfortunately there aren't a lot of resources for teaching raw Julia without any other technical goal in mind. Part of this is because it's rapid growth is due to productivity in very advanced domains, so at this point seasoned users are more likely to put together very applied resources. If you have a specific area of interest it would be easier to point you to something. The good news is that whatever you learn in one domain of Julia typically applies throughout all of Julia because it uses a very generic interface to Types. So if you just started by learning data organization, plotting, etc. you'd be able to easily pick up other domains within Julia.
Any comments on this ? Is it understandable, useful, correct ? Need a conclusion, etc. ?
How does this differ from @code_warntype?
You can think of it as an automated @code_warntype.
my 2c: if feel putting restrictions on input types is unnecessarily limiting. the example give is pretty arbitrary, but if you take real life examples, it is very hard to come up with a function that relies so heavily on its input types. maybe if it uses bit patterns and such, idk. but for most functions, i might want to instantiate them with my own types, which only in know, and i asses would work nicely with your function. but i can't call it, because you shut me out. and about the result type: i feel it should go into the testing, not the declaration. the assert seems to be better to me, but not immediately after the declaration, instead in the test code. more complex type assertations can be defined for more general inputs. like assert that the result type is the same as one of the inputs, or the bigger of the inputs, something of that nature.
Thanks for writing and posting this! This is very useful! For me personally, what would make this above and beyond useful is if there is a way to run this for all the functions I've written in my package. I don't want to manually go in and add the `@checked` to every function (&gt;200) in my package that is intended to be a library for other people to use. Maybe a simple macro that can add it once that functions are already defined in a package? I might even want to run this on a bunch of functions in someone else's package every time I run the tests in my package, and I don't want to fork their package and modify their source code just for this. Also, ideally (this is a nit pick) this could be made into a git repo that you can share somewhere online. I see the value of posting on nextjournal so that you can run it in the browser. But I'd like to download and run this locally, and copy pasting all the code from different cell blocks doesn't seem ideal, especially when this is in a draft or possibly going to be updated. What do you think?
I do agree that's not particularly useful in many cases, I did that more for fun or proof of principle. I also saw a lot of people worried about the lack of static typing in Julia. That said there's maybe some variations of it that can be useful in some context. For example if I try to add a new Number type and want to check that the promotion rules I defined works that could be handy. A version of the macro that works in tests should be possible, the only problem is that you would need to duplicate the function signature.
The issue is that you have to specify the return type, otherwise you wouldn't know what to test. You could do a version that checks that the return type is not Any, or apply the same checks on all functions in the block. I can do a gist with the code yes, that's a good idea.
What kind of subset? If you want a subset based on the internal representation, it you could simply do it in a comprehension: let local set_copy=copy(set) local n = 5 [pop!(set_copy) for i=1:n] end more likely you'd want a random subset of some predefined length *n*, in which case you'll want to use *shuffle* from the [Random Standard Library](https://docs.julialang.org/en/v1/stdlib/Random/index.html#Subsequences,-permutations-and-shuffling-1). using Random Set(getindex(shuffle(collect(set)),1:n)) I prefer using pipes and anonymous functions to make it easier to see the data flow: set |&gt; collect |&gt; shuffle |&gt; x-&gt;getindex(x,1:n) |&gt; Set Note that this approach shuffles the entire set before picking the first n elements, so it probably isn't performant with large sets
I mean a list of all subsets. The function `combinations` in the package Combinatorics does the same thing for Arrays. It's true that I could convert a Set to an Array but it costs memory. This is why I what to use Iterators.
this is called a power set
Oh fuck not this again
Hi, im newbie on Reddit. I dunno why i keep receive notifications from this box. So im trying to spam and hope moderators could banning or smt to prevent me see this again. Plz help. I dont want to do this.
Well all subsets of length k to be clear
What?
I mean that im so sorry about this spamming, but i have no choice. I sent private messages to Mods, but they didn’t help me. So i have to do this. Im so sorry.
In what way do have no choice?
Just go to the top of this post. Go to r/Julia. Disable the bell and leave this community.
Why do you think using an iterator would be faster than converting the data to an existing datatype? A lazy iterator might work nice a language like Haskell, but Julia is a different beast. Julia's Set is basically a Dictionary with no values, and dictionary keys are stored as 1D Arrays, so doesn't it make sense to convert it to make use of the underlying data in a form where the indexes are indices and not complicated hashs? Itertools has a subset function that does exactly what you need, but it needs an indexed collection: https://juliacollections.github.io/IterTools.jl/latest/#subsets(xs,-[k])-1
kind of funny how powerset does not work with Sets :)
Im sorry for this inconvenience, but I couldn’t find any bell icon anywhere. I’m using mobile version. Could u show me plz. I unjointed it for so many time, but it still sent me notifications. I truly regret of this spamming. But i have no choice
Hey, I got a PR for AbstractChars! World domination is near.
I don't know why but I assumed that converting a Set into an indexed collection would be slower. If what you said is the case then I guess I'll just simply do that. Follow up: how are Sets performance-wise compared to 1D Arrays when dealing with unordered data? If the order of the elements doesn't matter then should I use Sets? if not then what do I use Sets for?
As other have mentioned, your best bet is to convert your Set to a Vector and use Itertools.subsets: julia&gt; subsets(s::AbstractSet, k) = IterTools.subsets(collect(s), k) subsets (generic function with 1 method) julia&gt; collect(subsets(Set([4,9,2]), Val(2))) 3-element Array{Tuple{Int64,Int64},1}: (4, 9) (4, 2) (9, 2) There is an alternative way to get elements from a Set. You can take it from the underlying array \`s.dict.keys\`. However, some of these elements in the set are not used, and thus undefined. So you would need to make an array of the indices that are used in the Set. This can be found in \`s.dict.slots\`, bnd then you might as well just make an array of the objects themselves.
Performance depends on the operation. Adding/removing elements of a set/dictionary are very cheap, but they're expensive with arrays because you'll have to re-allocate a new list each time. julia&gt; l = rand(100); s = Set(l); julia&gt; @time for i in 1:100 push!(s,5.0) end 0.003736 seconds (22 allocations: 1.875 KiB) julia&gt; @time for i in 1:100 push!(s,Float64(i)) end 0.000020 seconds (100 allocations: 1.563 KiB) julia&gt; @time for i in 1:100 push!(l,5.0) end 0.006183 seconds (4.16 k allocations: 210.418 KiB) julia&gt; @time for i in 1:100 l[i]=Float64(i) end 0.006485 seconds (124 allocations: 4.328 KiB) Taking union a list is pretty cheap if you want to use that instead of a set: @time union!(l) 0.000016 seconds (16 allocations: 6.680 KiB) If for some reason you need to do something with something resembling indices of a Set, you could enumerate it, but it would still likely be more expensive than collecting it into a list. julia&gt; @time collect(s); 0.000010 seconds (5 allocations: 1.031 KiB) julia&gt; @time for x in enumerate(s) end 0.000022 seconds (201 allocations: 7.828 KiB) More likely, you'd probably want to find a data structure that fits your needs, possibly from a library such as https://juliacollections.github.io/DataStructures.jl/latest/index.html
The allocation advantage of Sets does seem very appealing since I'm looping it many times. Again I'm thinking about custom iterators to find the subsets. I haven't had much experience with DataStructures, but [OrderedSets](https://juliacollections.github.io/DataStructures.jl/latest/ordered_containers.html#OrderedDicts-and-OrderedSets-1) seem to be what I may want from a first look. How is that performance-wise?
Good suggestion, I'll try it out.
Honestly don't know, haven't used it, you can @time it yourself, but generally Standard Library implementations will likely be the most efficient performance-wise. But for performance critical work, you'll likely want to identify the bottlenecks and pick your data structure based on that.
\`\`\` using DataStructures, Combinatorics function poptest() s = Set(rand(1:10000,1000)) a = collect(s) loop = collect(s) os = OrderedSet(s) println("Sets") @time for i in loop pop!(s,i) end println("OrderedSets") @time for i in loop pop!(os,i) end println("Arrays") @time for i in loop deleteat!(a,findfirst(x-&gt;x==i,a)) end end function pushtest() s = Set(rand(1:10000,1000)) a = collect(s) loop = rand(1:10000,1000) os = OrderedSet(s) println("Sets") @time for i in loop push!(s,i) end println("OrderedSets") @time for i in loop push!(os,i) end println("Arrays") @time for i in loop push!(a,i) end end function choose2\_test() s = Set(rand(1:10000,100)) a = collect(s) os = OrderedSet(s) println("OrderedSets") @time collect(combinations(os,2)) println("Arrays") @time collect(combinations(a,2)) return end \`\`\`
``` using DataStructures, Combinatorics function poptest() s = Set(rand(1:10000,1000)) a = collect(s) loop = collect(s) os = OrderedSet(s) println("Sets") @time for i in loop pop!(s,i) end println("OrderedSets") @time for i in loop pop!(os,i) end println("Arrays") @time for i in loop deleteat!(a,findfirst(x-&gt;x==i,a)) end end function pushtest() s = Set(rand(1:10000,1000)) a = collect(s) loop = rand(1:10000,1000) os = OrderedSet(s) println("Sets") @time for i in loop push!(s,i) end println("OrderedSets") @time for i in loop push!(os,i) end println("Arrays") @time for i in loop push!(a,i) end end function choose2_test() s = Set(rand(1:10000,100)) a = collect(s) os = OrderedSet(s) println("OrderedSets") @time collect(combinations(os,2)) println("Arrays") @time collect(combinations(a,2)) return end ``` ``` julia&gt; poptest() Sets 0.000036 seconds OrderedSets 0.000077 seconds Arrays 0.000278 seconds (3.71 k allocations: 58.000 KiB) julia&gt; pushtest() Sets 0.000046 seconds OrderedSets 0.000085 seconds (2 allocations: 16.070 KiB) Arrays 0.000020 seconds (2 allocations: 44.188 KiB) julia&gt; choose2_test() OrderedSets 10.176032 seconds (2.37 M allocations: 171.569 MiB, 0.37% gc time) Arrays 0.000895 seconds (24.75 k allocations: 1.322 MiB) ``` OrderedSets seem to be slightly worse in pushing and popping than Sets but still preferable. I don't know what is going on with finding combinations of OrderSets. I'm getting a warning saying that `indexing is deprecated for OrderedSet, please rewrite your code to use iteration`. If that's the case, then there's no point in using OrderedSet, since Sets can be iterated as well.
I just give up trying to solve the problem with Iterators. This is my first time working with Iterators, and the performance of the code is getting worse. I'll stick to arrays instead of Sets to solve the pr0blem.
[removed]
well, there is the methodswith(&lt;type&gt;) function, but there is no such type as Collection. but for example methodswith(Array) and methodswith(AbstractArray) gives you a bunch. you can find all iterables with methods(iterate). this is not very practical.
apropos(Array) &amp;#x200B; Maybe?
I still think that the way the api of Java is documented is the best I've seen. Lists of everything, nice short explanatory comments, and links to the source code.
The docs is good, but I miss that part, I can't navigate as easy as I used to in Ruby or in Java or TypeScript with completion by typing \`type.&lt;cmd+space&gt;\`
And yet, you can spell *dyslexia* without problems.
I had to look it up :(
we need an extended methodswith which walks up the parents. &amp;#x200B; That could work.
in this case, a combination of "get all iterables" and then the walking up you have mentioned. and then maybe some filtering on how many times a method comes up.
I don't have dyslexia and I still struggle (mainly because I feel like there should be two "p"s for the second one), but unfortunately there's a lot of other utilities with that name so Julia probably just went with the path of least resistance.
While the main conclusions of the article are well taken (specifically, that one should use well-established libraries of best-of-breed solvers) the use of a single test problem to "prove" these conclusions is hardly convincing. Systems of ODEs vary greatly in terms of which solvers work best, and in many cases, the ODE solver is not even close to the most important part of the solver. Molecular dynamics solvers basically just solve many thousands of ODEs simultaneously, but the choice of solver must be energy conserving and extremely cheap and parallelizable. As usual, don't make generalizations to all problems based on narrow experience.
You can put Euler's method into any of the 17 ODE equations in benchmarks repository that is linked in the article ( [https://github.com/JuliaDiffEq/DiffEqBenchmarks.jl](https://github.com/JuliaDiffEq/DiffEqBenchmarks.jl) ) and you'll get the same results. Of course, this is just a blog post that cannot be exhaustive and you should definitely explore this yourself, but it gets pretty obvious that Euler never gets close. &amp;#x200B; How would you condense this into a 2-3 minute read? I just chose a random equation from the Slack and just run standard benchmarks on it. &amp;#x200B; (The more detailed answer: Euler's method is basically only better when the ODE derivative function \`f\` is Holder continuous with Holder continuity alpha &lt; 1, in which case Euler's method converges with order alpha, and so does any other ODE method. When that function has special characteristics though, like in stochastic or random ordinary differential equations, you can still construct much better methods by utilizing whatever information about regularity / rough paths you have. However, the mathematics gets hairy in any case where classical Taylor series cannot be used) &amp;#x200B; \&gt; Molecular dynamics solvers basically just solve many thousands of ODEs simultaneously, but the choice of solver must be energy conserving and extremely cheap and parallelizable &amp;#x200B; Note that the standard methods used in molecular dynamics are symplectic, not energy conserving. Those are similar but not the same. Also, if you benchmark it in the case of high accuracy, the symplectic methods don't necessarily do better, dependent on the equation as well. &amp;#x200B; [https://nbviewer.jupyter.org/github/JuliaDiffEq/DiffEqBenchmarks.jl/blob/master/DynamicalODE/Henon-Heiles\_energy\_conservation\_benchmark.ipynb](https://nbviewer.jupyter.org/github/JuliaDiffEq/DiffEqBenchmarks.jl/blob/master/DynamicalODE/Henon-Heiles_energy_conservation_benchmark.ipynb) [https://nbviewer.jupyter.org/github/JuliaDiffEq/DiffEqBenchmarks.jl/blob/master/DynamicalODE/Quadrupole\_boson\_Hamiltonian\_energy\_conservation\_benchmark.ipynb](https://nbviewer.jupyter.org/github/JuliaDiffEq/DiffEqBenchmarks.jl/blob/master/DynamicalODE/Quadrupole_boson_Hamiltonian_energy_conservation_benchmark.ipynb) &amp;#x200B; That's just something to ponder. Indeed, the choice of solver is highly dependent on the problem you are trying to solve, which is why we've implemented and optimized over 300 now :). &amp;#x200B; [http://docs.juliadiffeq.org/latest/solvers/ode\_solve.html](http://docs.juliadiffeq.org/latest/solvers/ode_solve.html)
I think the [documentation](https://docs.julialang.org/en/v1/stdlib/Sockets/index.html) is useful and I started with [this example](https://blog.leahhanson.us/post/julia/julia-sockets.html), it's old but still relevant. [This part of the doc](https://docs.julialang.org/en/v1/manual/networking-and-streams/#Networking-and-Streams-1) is also useful. Edit: +1 link
My Phd supervisor's book (Data Science with Julia) covers exactly what you are looking for, unfortunately you have to buy it. &amp;#x200B; [https://www.crcpress.com/Data-Science-with-Julia/McNicholas-Tait/p/book/9781138499980](https://www.crcpress.com/Data-Science-with-Julia/McNicholas-Tait/p/book/9781138499980)
that looks great! the specific problem I had was how to pipe the output to a specific argument of a function. Any ideas?
do you have multiple arguments in the second function?
You have encountered the well known scoping issue. [The manual](https://docs.julialang.org/en/v1/manual/variables-and-scoping/#scope-of-variables-1) explains it all.
Great! This was very helpful - thanks!
You're running into [integer overflow](https://en.wikipedia.org/wiki/Integer_overflow) since you're calculating numbers past `typemax(Int)` (`9223372036854775807`). You can use `BigInt` instead of `Int` (though it's a lot slower) or (better) reset `i` to zero when `mod(i,1000000) == 0`. You're running into this issue earlier than you should because there's a logic error. `i` is increased every line, and every `1000000`th line you multiply `i` by `1000000` and print that number. So after `1000000` lines, you evaluate `1000000 * 1000000` and print that, while it should just print `1000000`.
ah of course! Thanks!
Also, based on what you seem to be trying to do, you wouldn't multiply i by anything. You just print i. I is being incremented for each line that you read, so when your conditional statement is true, i will be a multiple of 1,000,000. &amp;#x200B; Additionally, you may want to look into the enumerate function, this way you don't have to declare an index when using a for loop. &amp;#x200B; for (i, l) in enumerate(eachline(f))
I'm not sure if I understand correctly. A Set is unordered, so there isn't a j'th element in it, but I'll assume an array. Your problem is likely that Julia is (column major)[https://en.wikipedia.org/wiki/Row-_and_column-major_order] instead of row major like python, so your list comprehension example with python should be the inverted in Julia: using DataFrames keywords = [:a, :b, :c] lines = [[:c], [:a, :b], [:a, :b, :c]] DataFrame([[if i in j 1 else 0 end for j in lines] for i in keywords], keywords)
If you want to check if a substring is in a string, you need \`occursin()\`. I haven't checked, but I think something like DataFrame(k =&gt; Int.(occursin.(k, lines)) for k in keywords) The \`Int.(occursin.(k, lines))\` is because converting a Bool to Int gives you a 1 for true and 0 for false. Alternatively, you could do \`map(line-&gt; occursin(k, line) ? 1 : 0, lines)\` for that bit. Edit: I misread that you have an array of arrays for your lines. This will mess with broadcasting (the dot-syntax in my code). Did you manually split your lines into words, or is that just how the data comes? In that case, it would be: DataFrame(k =&gt; map(line, k in line ? 1 : 0, lines) for k in keywords)
Julia is functional, so you write whatever you want to do with data flow in mind, and use struct along the way to help yourself.
Would you mind sharing some example of good practice? It's difficult to wrap my head around it without seeing it because I'm used to OOP.
use duck typing. use traits.
Not sure if this will help you exactly, but have you tried looking at the QuantEcon lectures or the DSGE.jl model on Github? Have you tried asking the Julia users on BE?
Object Oriented Programming (OOP) and Functional Programming (FP) have very long history through multiple popular languages, which is why the do or don't are very well understood. I feel there isn't such a thing for Multiple Dispatch (MD), which is only used for somewhat exotic languages like Common Lisp (Object System), Dylan and Julia. But MD is very close to the modern blend of OOP and FP that discourages inheritance and favors functions, composition and interfaces. In my view OOP is about structuring your code as actors that encapsulate their own logic and then how they interact with each others (subjects manage actions), FP is about structuring your code as mapping from inputs to outputs (actions manage subjects) and MD is about defining separate action and actors, and then linking (specializing) your action to your actor (actions and subjects are independent). As an example, let's say we have a chess game. In OOP one possible solution is to have a chess board object that manages the position of the pieces, and two players both children of the abstract concept of a player (but lacking their quirks). At each turn each player requests the position of the board, decide internally the next move and then request the board to apply that move, which it complies as long as it considers it valid and if no one won yet. In this case, what you figure out first is the boundaries (player, board), which gives you order and organization (encapsulation) to your logic. In FP the board itself is part of a world state, and each player is a function that maps the current board state to the one that includes their next move, but since one player could create an invalid state, you can have a judge functions that receives both the proposed new state and the previous state and if it's valid returns the new state and otherwise returns the world state to before the invalid choice was made (or returns the winner if any). So in this case you first defined the "world state", and the actors are the within mappings between different states, which gives you visibility and control over how the world as a whole evolves. In MD you can start with the actions (functions) such as the act of making a move (in something) and the act of evaluating the validity or winning conditions (of something) - which is a functional approach. Or you can start with the actors (types/structs or even interfaces) such as the chess board and the abstract player with two children, player 1 and player 2, before any of them are able to perform any logic yet - which is the object oriented approach. And when you have both, you can link them through the specialization logic (methods), such that the logic of player 1 is encapsulated in the "act of making a move" that links "player 1" and "chess board", and the evaluating in "evaluating the validity or winning conditions" of "chess board". So this way you decouple "who does it" from "what is done", and your game loop "someone plays a move" -&gt; "evaluates" -&gt; "someone plays a move" -&gt; "evaluates" can apply to chess board, but also checkers boards and even rock–paper–scissors, and also any kind of player. So with MD you get extremely generic code that can easily adapt to completely different contexts by swapping the actors and therefore the interpretation of the actions. Although sorry since you probably wanted a practical guide and I just posted some philosophical opinion stuff.
This [blog post](https://lwn.net/Articles/764001/) was recently posted here and might help. And as I understand it the [types](https://docs.julialang.org/en/v1/manual/types/), methods and constructor bits of the julia documentation cover the kind of things you'd do with OOP in other languages. Cause Julia simply doesn't combine the data and methods into a single entity like a class, rather it keeps them separate. The data is stored in a type (often a struct) which can do inherentence. The methods are just functions that take said type as an argument. And a constructor is simply a function that returns a new object of said type.
Thanks I get it a bit more
Thank you!
I'm not a macro guy so going through quantecon just for the sake of Julia seems a bit too much though you're right I'll probably start there. thanks for the input.
: ) That blog post really did help me out.
Just bringing it just a little more practical on the two ways you can work in MD, the action (algorithm) to subjects (data structure) approach and a subject to action approach: You can start by thinking your functions (basically your API and top level logic). In this case you write functions without types and uses the generic data structures already available (multidimensional arrays, dataframes, dictionaries, named tuples) without caring much about performance outside of the usual (trying to be type-stable for example) but trying to build helpers to manipulate the data instead of interacting with them directly (for example intelligent setters, getters and constructors). You use this step to understand your problem, and what does the specific types you need must have (for example, what kind of state it contains, what kind of specific methods you need to interface with that data - the helpers, what are the best ways to construct that data). After you get a handle of both sides of the problem (the algorithm and the data structure), then you'll reuse most of the code used in the first step (the algorithm), but you'll specialize the getters, setters and constructors to optimally use the data structures you created. And you can also start from the data structures. What is the data I'm going to work with, perhaps something like a database/dataset table, perhaps something like a json, perhaps a tree like structure or a graph. In the case above, how to store the position and owners of the pieces of the chess board, how to keep track of the score of the players, what's the hierarchy involved "abstract chess piece" -&gt; "rook", or perhaps they are just an enum type. After you decided the form of your data what kind of behavior should they have? This is the point you start writing the getters, setters and constructors (the overall interface) based on what you'll want from your data, which could also comply with already existent interfaces (like the Number interface, or Array interface or Tables.jl interface which will integrate them with actions already written by other people). Finally, when you have your "objects" with their functionality ready, you can start creating the algorithms that will finally do the task you need, including how those many "objects" will interact with each other for that purpose. You can see that division of approaches in the ecosystem, with packages focused in being generic algorithms that will work with any data structure that comply to their interface (Flux.jl, DifferentialEquations.jl) and packages that focuses on being data structures that will work with any algorithm that comply to their interface (DataFrames.jl, LighGraphs.jl, StaticArrays.jl), and peculiarly there is also one type of package that avoid both implementing a data structure and an algorithm and focus solely on coordinating and enhancing interfaces (Tables.jl, IterTools.jl). Which approach you're going to follow probably depends on your objective, and if you understand the algorithm or the data better at the start, and remember that Julia is a dynamic language, so the development is iterative (experimenting with the program you make it and evolve it with your understanding of the problem).
Thank you very much for all your explanations it's absolutely helpful!
Also a link about a long OOP to Julia discussion for even more practical. https://discourse.julialang.org/t/composition-and-inheritance-the-julian-way/11231
but why
Because we can lol
I guess it's the most powerful calculator on Android so far.
TL;DW (TOO LONG; DIDN'T WATCH) Here is summery of all the resources and commands: Download termux: https://play.google.com/store/apps/details?id=com.termux&amp;hl=en Install Fedora in termux: Website: https://github.com/nmilosev/termux-fedora Commands for termux: pkg install wget -y &amp;&amp; /data/data/com.termux/files/usr/bin/wget https://raw.githubusercontent.com/nmilosev/termux-fedora/master/termux-fedora.sh sh termux-fedora.sh f30_arm64 startfedora In Fedora (still in termux): Install wget: yum install wget confirm with yes during installation: y Go to Julia Lang website and download binaries: https://julialang.org/downloads/ https://julialang-s3.julialang.org/bin/linux/aarch64/1.1/julia-1.1.1-linux-aarch64.tar.gz extract it: tar -xzf julia-1.1.1-linux-aarch64.tar.gz Go to Julia directory: cd julia-1.1.1 cd bin start Julia and have fun! ./julia
TL;DW (TOO LONG; DIDN'T WATCH) Here is summery of all the resources and commands: Download termux: https://play.google.com/store/apps/details?id=com.termux&amp;hl=en Install Fedora in termux: Website: https://github.com/nmilosev/termux-fedora Commands for termux: pkg install wget -y &amp;&amp; /data/data/com.termux/files/usr/bin/wget https://raw.githubusercontent.com/nmilosev/termux-fedora/master/termux-fedora.sh sh termux-fedora.sh f30_arm64 startfedora In Fedora (still in termux): Install wget: yum install wget confirm with yes during installation: y Go to Julia Lang website and download binaries: https://julialang.org/downloads/ https://julialang-s3.julialang.org/bin/linux/aarch64/1.1/julia-1.1.1-linux-aarch64.tar.gz extract it: tar -xzf julia-1.1.1-linux-aarch64.tar.gz Go to Julia directory: cd julia-1.1.1 cd bin start Julia and have fun! ./julia
Just fyi you can also make a symbolic link to add it to fedora's path (I just used it's native python3 to add it into that path) and call julia from any directory
A little late to this thread, but I was wondering how you were able to get it to work. I keep on encountering this error: `ERROR: LoadError: Could not load pa_shim library, please file an issue at` [`https://github.com/JuliaAudio/RingBuffers.jl/issues`](https://github.com/JuliaAudio/RingBuffers.jl/issues) `with your \`versioninfo()\` output` &amp;#x200B; This happens after i added PortAudio (fromthe julia1 branch). I try to type `using PortAudio` and it throws the error during the precompilation. I am not sure why this is happening. Would you happen to know anything about this? Or could you maybe tell me if you had to do anything else?
I am not sure about the details of PortAudio, but missing pa\_shim library may be related to your OS or machine. I am using Julia on my macOS Mojave 10.14.5, and I found pa\_shim\_x86\_64-apple-darwin14.dylib in \~/.julia/packages/PortAudio/gsLXv/deps/usr/lib directory. There are library files in that directory for several other CPU/OS but maybe not the one that suits your environment?
Checkout https://en.m.wikipedia.org/wiki/Simpson%27s_rule. The linear combination should result in numerical errors but if will be differentiable.
Desktop link: https://en.wikipedia.org/wiki/Simpson%27s_rule. *** ^^/r/HelperBot_ ^^Downvote ^^to ^^remove. ^^Counter: ^^261579
Yes, that helps a lot. I didn't see that at the bottom of the github page. I probably should have looked. Thank You.
Have you tried working with [ApproxFun.jl](https://juliaapproximation.github.io/ApproxFun.jl/latest/)?
I second this option.
Hope it solves your problem!
I don't love the way the code is formatted in the post. Here is a link to the same code on github: [link](https://github.com/samcollie/FADs/blob/master/ez_dynaic_program_II.jl).
you're allocing 3 x 1001 (ngrid) every one of your 10,000 iterations. Zeros allocates memory, you should try fill!(harvest, 0) instead
If you indent everything by 4 extra spaces, it should format as code.
Thanks. Didn't realize you needed to be in markdown mode for that to work.
Thanks for the suggestion, it's gotta be something like this. I'm still learning how to troubleshoot. &amp;#x200B; I tried your recommendation but unfortunately no improvement. =(
Update: Adding the type declaration "::Float64" to the function "Bellman" speeds things up by 10x, and reduces memory allocation by \~4GB. Not sure exactly why this works, I'm not clear on when to use/not use type declarations. Still looking to optimize this code if there are further suggestions.
This sounds like some sort of type instability. Try calling your function with the @code_warntype macro and see if it flags anything up
Hi, First note that you have an insane number of grid points. If you're doing economics, you'll see that most published papers use way less. For example Corbae &amp; Quintin (2005) use only 20 grid points for assets, and another example Dahlquish, Vestman and Setty (2018) use 5 or 10 if I remember correctly. Now, if you use a profiler (e.g. in Atom using Juno), you can see where the code spends it time. Juno.Profile.clear() Juno.@profile valu_iteration() Juno.profiler() This reveals that there is not too much you can do short of writing a new interpolation routine that is faster or use a different optimizer, since virtually all of the time is spent inside the result = optimize(objective, 0.0, stock) line. * You probably don't need to find new policy functions every iteration. You can update the policy function say every other iteration, and just re-use them. * You can decrease the tolerance for convergence in the optim.jl pacakge. For example, set the criteria lower for the first x iterations, and then go higher.
How about getting rid of global constants? I know people say that it can be detrimental to performance. Just take the consts and have them inside as inputs to the functions or something? I would also be interested in why specifying types helped speed up your program. I also recently read on QuantEcon somewhere that keeping functions as generic as possible is most efficient, but I guess its not the case here? Man I love Julia but I'm so confused many times as well. I wish I was smarter... -end rant.
This is because when the JIT compiler knows you are using Float64 it uses faster, more appropriate machine code. This mainly has to do with the fact that on a 64-bit machine you don’t need a pointer to store f64, but a Number type will require a pointer because it may take over 64 bits to represent
You can use one of the numerical integration package: https://github.com/JuliaMath/QuadGK.jl https://github.com/stevengj/HCubature.jl And then optimize over x using Optim.jl.
You can remove the fill! too, since you overwrite the values in your loop anyway. One my computer getting rid of the allocations makes a 100x difference. Most of the time (using Profile) is then spent in Optim, which is what I would expect.
Thanks! Filling in zeros is a weird habit I picked up working in python/numba. The idea is to wipe the slate clean, in case there is some noise that doesn't get overwritten. No idea if that makes sense or not but in very long loops in python, it seemed to make a difference. But I agree, if it isn't needed and it slows things down, might as well get rid of it!
Ya I think you are right. Using @code\_warntype, there did appear to be a type instability. I have no idea how to read the output, except that you want to avoid red text. Interesting solution- I broke the code into many smaller functions, and the type instability was resolved. At that point the type declaration is no longer needed! Thanks for the advice- still learning here.
Yes, that all sounds quite typical. Avoiding red text is definitely the goal. Typically you will be able to isolate the point where the instability entered (it tends to be infectious / propagate into later parts of the function). Look out for the first variable whose type is Any or Union{Something,SomethingElse}. Look at how that variable was created: it may be that you can just correct the function it came from, or in rare cases (if it is a variable coming from the outside world) you can add a type check (e.g. myint::Int64 = 0) which will have a runtime dynamic check, but will allow the compiler to use the type information through the rest of the function body if the type is correct.
Yup.. I'm kinda overdoing it with the grid points in order to optimize the code. This is kind of a toy example for now. The actual problem I am working on has much higher dimensionality, and this optimization routine will be nested inside a larger problem (I think quant econ refers to this kind of problem as DP\^2). So the idea was to get this algorithm running super fast so it can be run over and over. I like your idea of gradually increasing Optim's convergence tolerance. If you are familiar with this kind of stuff, I wonder if most people are using Optim or writing their own optimization routine? Its sure nice to use an out-of-the box solver, but in the past (using Numba) I had to write my own.
I read about avoiding globals as well. That's why I designated them as constants, which appears to work well. I tried your suggestion of using them as function arguments and didn't see much of a difference. So I think using constants is OK for globals (at least based on that one experiment). And yes, the Julia performance tips also indicate type declarations can hinder performance. In the end, breaking this up into lots of smaller functions resolved the type instability and it ran much faster.
The biggest slowdown is from memory allocation. Preallocate your vectors before the loop begins, when possible. Doing an alloc every loop iteration will kill your performance
Thanks for the tips. I played around with allocating vectors to no avail. I analyzed the memory allocation using "--track-allocations=all" and found out the anonymous function "objective = h -&gt; - bellman(h, stock, valu\_interp)" is the reason for the bulk of the memory allocation. And yes, some allocation on defining the vecors within the loop. Not sure how this can be avoided. Thanks again!
Thanks this is super helpful. The updated code (in the post) is all Blue... which sped things up by 10x. Still a large memory allocation for the anonymous function though, not sure if this can be avoided. Thanks again!
 my_array .= 0 Would also be an easy and intuitive way to reset array values to zero without reallocating.
I agree. From looking at a little Profile run, almost all the time is spent in the brent optimiser and the interpolation code.
yes, you type stuff and programs come out
I am a relatively new user. I think it is as easy to use at the fundamental level. I am still trying to get my head around multiple dispatch - I'm a statistician by training and taught myself most of the comp. sci. stuff I know. The syntax is really good in my opinion. The only frustration I have is googling for answers. It's sometimes difficult to find answers to more specific questions currently. This will change with time.
As a language? Julia is much cleaner and therefore easier. Julia has more natural math notation built in, for starters. Second, Python’s object system is downright ugly (hello magic double-underscore functions and self parameters) and had a completely broken multiple inheritance system for years, until GvR read a paper about how a real programming language does it (https://python-history.blogspot.com/2010/06/method-resolution-order.html). Third, Python has nasty scoping semantics, called “late binding closures”. Fourth, more controversially, significant whitespace is a bad idea because it makes refactoring and pasting code a much more fraught endeavor than it has to be. As an ecosystem in which to solve problems? It depends. Julia has a more solid foundation with its package manager and installation system. But Python has had years and years more of development work and libraries put into it. Help is easy to find. It doesn’t help that Julia is still changing rapidly, so older writings about it do not necessarily hold true any more. In general, Python has enormous and growing mindshare. That can be an important advantage.
Julia was originally built for a specific domain, which is scientific programming, while Python got into domain by brute force (large groups outside of the language creators decided to make it the de facto language for those purposes). That means that for those domain you end up with multiple languages, the core Python and the language that those groups create to cover for what the core language is missing (languages that are therefore not written in Python), which is numpy, pandas, pytorch, tensorflow. In those domains Julia code is more [natural](https://cheatsheets.quantecon.org/). In general computing, their philosophies are different. Python follows the mantra that there is only one obvious way of doing something (though it doesn't go as far as Go in that it still gives you tons of power, just not easily accessible). Julia is the opposite with it's Lisp inheritance, there is not only many equally good ways to do anything but you can also easily extend the language to increase the options even further. That together with the high performance, makes, in my opinion, Python is a language that is easier to do the simple stuff, while Julia is a language that is easier to do the complex stuff.
absolutely not agreed with your critique of Python. But Julia is however very interesting language
&gt;which is scientific programming, while Python \&gt; which is scientific programming, while Python SciPy? SymPy?
I think you're agreeing with me. Python was made, way later in it's life, the de facto language in scientific computing thanks to the efforts in creating those libraries, many of those in other languages while using python merely as an interactive glue/orchestrator including SciPy. Both those libraries had their first release when Python was older than Julia is now, and by the time Julia was first released (way before it became stable) they were already also older than Julia is now.
Searching [https://discourse.julialang.org/](https://discourse.julialang.org/) instead of Google at least sidesteps the problem of Julia also being a given name, and usually results in decent quality answers from active community members.
I like Julia however I find Python significantly easier to prototype in, and write quick scripts in, and do quick little tasks in. I haven't found a use for Julia yet, but I'm sure I will. I just feel like there's more setup to Julia where I can just drop some Python down and be done with it. In the argument of "which is easier", I'd say absolutely Python. Though reading below almost every single person seems to disagree with me. However I'd say /u/fborda's comment in this thread hits the nail on the head for why. As I don't think either are hard, if you're an experienced programmer I'd say just learn both and form your own opinion.
Julia stopped changing rapidly after the release of version 1.0, so that part is incorrect.
if you want the same basic features, yes or even easier. the differences might kick in if you develop reusable modules. the type system is one of a kind. the the lack of oop features (which is actually a good thing) might surprise some people.
I think [perl](https://famicol.in/sigbovik/) beats julia there.
You type hieroglyphics and stuff happens
If you're struggling with multiple dispatch or other more advanced topics, I'd focus on the basics for now and start to play with them once you're more comfortable with the overall language
If you're struggling with multiple dispatch or other more advanced topics, I'd focus on the basics for now and start to play with them once you're more comfortable with the overall language
If you're struggling with multiple dispatch or other more advanced topics, I'd focus on the basics for now and start to play with them once you're more comfortable with the overall language
There has been a discussion regarding this on [discourse.julialang.org](https://discourse.julialang.org): [https://discourse.julialang.org/t/julia-programs-now-shown-on-benchmarks-game-website/17722/92](https://discourse.julialang.org/t/julia-programs-now-shown-on-benchmarks-game-website/17722/92)
Have the benchmarks been updated at all since the initial version ?
Julia has very few concepts I find, you have types, methods/functions, and that's about it. There's some complexity in those (parametric types, generic functions, etc.) but once you understood them you can do most things.
When I try to type perl no program comes out though 🤔
I'm pretty new to Julia but becoming increasingly convinced that it's about time we all abandoned the myth that Python is simple and easy to use. I'm a bit of a programming language nerd. but for the last year or so I've done most of the programming work for my day job in Python, having not really used it in anger since the 2.4 days. Back then, yes, I'd probably say it was fairly simple and easy - sure, OO was a mess, for example, but that's because it seemed kinda tacked on to a simple language using simple tools, so you can almost forgive it. After all, it's a simple scripting language, right? Now it really isn't, and I'd better preface this by saying that I do quite like Python for doing real work - I just don't think it that modern Python is as easy as it's historic reputation would lead you to believe. I often spend time with colleagues in non-techie roles to help them learn to automate things, and Python is definitely the go-to for that, but it's just massively apparent that Python is now a Frankenstein's monster of barely hidden nuts and bolts. Some of it is little things like why on earth it thinks that, rather than a representation of the list of numbers, when someone prints a range at the REPL, what they really want is to see that yes, it is indeed a range (seriously, explaining this to newbies is a nightmare). Other things are a bit deeper, like the umpteen different ways you can deal with filepaths (all standard!), all sorts of weird semi-enforced conventions usually using underscores or, pervasive class-based OO (that may be a problem to me more than others due to personal preference, but it stops a lot of things being simple imho). And don't even get me started on even getting the code running in the first place! Trying to juggle all the different versions and environments is such a faff that it's no wonder people are spinning up Docker containers for 500 line scripts these days - imagine explaining all that to a beginner! My system has Python 3.6, yours has 2.7 but - SURPRISE! - they're both just called 'python'. No, they're not compatible, so you probably want to upgrade. Oh, and the libraries might be all over the place so be sure to activate the venv… I'll explain it later... And you don't know any of this because you're new to it, just want to get shit done, and have heard from everyone in the world that Python is nice and easy to use! Sorry, that was a bit of a rant, but I guess I had to get it out somewhere. To answer the actual question, I am fairly new to Julia as I say, but have already used it for some fairly hairy data scraping, extraction, and analysis in my spare time. So far I've found it no less easy to use than Python... but, as I've hopefully made clear, I don't see that as a particularly high bar. I can imagine that Julia could potentially fall into some of the same traps as Python when it comes versioning (probably a risk for any implementation-defined non-hosted language), but it does look more consistent a language than Python from what I've seen.
At the beginner level you have a lot of stuff besides that. For example for a loop Python would use a for loop or a list comprehension. In Julia you can use Python's list comprehension or a for loop (though if it's a multidimensional loop you can use in julia another option, CartesianIndex, while python you can only nest), but you also can write in terms of higher order functions and Ruby's do block (hard in python because of lambda's restrictions) or you can also model it in terms of Julia's unique broadcast system. You can also instead of a list comprehension use a generator (which you can in python, but you need to use yield, while Julia you can just use a comprehension with parentheses), and you'll see people recommending that approach to avoid unnecessary allocation. And you have to be careful that during the loop you do not change a variable's type (for Python it doesn't matter since it will always perform as if it was type-unstable). And if you really like functional programming you can also pipe, compose or use a macro definition (available in many packages) to combine loop transformations. But for more complex stuff that's actually good, you can write code that is extremely concise and can be fused in a single operation with broadcast, you can write easy cache-efficient multidimensional loops with CartesianIndex, you can easily parallelize the loops (which you really can't in python at all), "automatically" add SIMD properties, higher order functions are known to make simple and easy to debug code, you can write short code that barely allocates with generators (so you can have more realtime systems not disturbed by the garbage collector) and finally you can use some fantastic DSLs for iteration such as DataFramesMeta and Query.jl thanks to the macro system.
Thanks!
It's a question that's very subjective, relying on your opinion not only of Julia but of Python, and answers might well depend on which you learn first, what you currently use, and your use cases. I think the two languages are broadly comparable, more so than say Rust or Mathematica. These one-liners for reversing the lines of a text file look broadly similar. Both show some idiomatic tricks, which experienced users would be happy with and that new users will soon get used to. python -c "import sys; print '\n'.join(reversed(sys.stdin.read().split('\n')))" &lt; .bash_profile julia -e 'println.(split(read(stdin, String), "\n") |&gt; reverse)' &lt; .bash_profile I do think Julia's syntax is cleaner than Python's, particularly for metaprogramming, functional programming, and maths notation. I personally prefer explicit syntax rather than white-space syntax, but switching between them is easy enough. I think Julia's REPL is better than Python's. The Python code that I've converted to Julia usually lends up less complicated. So I'd agree with the question - for me, it is. Take that, Betteridge!
Take a look at the code. It's pretty funny: it's hard to come up with a worse implementation of the Julia versions that's still correct.
&gt; I am pretty proficient at using R for data analysis tasks, and at using Python for similar tasks as well as some more general purposes. &gt; [...] &gt; One of the reasons I'm drawn to it is the prospect of contributing to a young language and helping it grow. If you like writing, you could write about transitioning from R to Julia, or how to accomplish common R tasks in Julia. Other than that, looking for the `good-first-patch` or `intro-issue` labels on GitHub is a good way to find simple issues to learn how to contribute to Julia &amp; open-source. If a repo uses those labels, it's likely they'll guide you a bit so the lack of experience of developing tools shouldn't be an issue. Similarly, improving documentation is also a good way to start. Helping people learn the language, e.g. by answering questions on StackOverflow or Discourse, mentoring on Exercism or is also a good way to help the language grow.
Take a look at something in a place you know. Sounds like you're good with data analysis, so maybe JuliaStats. Distributions.jl, GLM.jl, DataFrames.jl, etc.: take a look at open issues and take a crack at them. Or, just try to start using the software to do something non-trivial and fix any bugs you find.
Wow. The Julia community (or Chris?) shows its true colors.
Did you take a look at the code? \- K-nucleotide uses \` counts = Dict{AbstractString, Int}() \`. Abstractly typed container? \- Binary trees uses: \` mutable struct Node &lt;: BTree info left::BTree right::BTreeend\` : No parametric type on \`info\`? \- Fasta.jl's cumsum isn't in place but the array isn't used after that? \- \`masksAtCell \` is a \`Matrix{Any}\` in meteor\_contest.jl? &amp;#x200B; That's what I saw in the first minutes. Please defend these as good ideas. All of those errors are the things that you teach people to not do if they care about performance. So it's pretty clear that it's harder to come up with a worse implementation because what's there is already a fully allocating and type-unstable version. In fact, usually these kinds of issues are how you demonstrate bad code. Anyways, it's already been proven that the code that was benchmarked was bad because someone wrote canonical Julia code and in some cases it was about 13x faster. So this is a good repo to show how far off the mark the original benchmarks were: [https://github.com/KristofferC/BenchmarksGame.jl](https://github.com/KristofferC/BenchmarksGame.jl) . When the benchmarks get updated to be more normal Julia codes like the ones in this repo the timings will drop.
Learn Julia. Try packages and open issues on github. Most popular packages are well tested but there's plenty of smaller ones that are used just by a few people, and testing them is always a good idea.
Thank you for being more specific and for the link to that repo 👍
Could have the readline in another thread. But ive found that multithreading in julia is somewhat weird to work with. Could do the file check bodge a bit more effectively by for example only checking every some number of iterations or every second ore something to reduce the performance impact though.
You could use Julia's Channel structure to pass the readline() value from an IO thread to the main thread. You can do that by calling, firstly, the @async macro for the IO thread function and, then, the @elapsed macro for the main thread function. I have actually coded an example of this that was almost working but I don't have it right now, otherwise I would post it here. :/
I actually haven't dealt with Channels too much before. I would very much like to see that example But I'll go look it up in the main time.
Code quality aside, free tip https://www.youtube.com/watch?v=o5LgwwiesQ0#t=17m10s You're welcome!
I actually couldn't make it work until now. Channels happened not to be the best way to achieve it. Sorry. I will keep trying, though. So if I end up making it work I will post it here :)
So I eventually found an easy solution in the Networking and Streams section of the Julia's website. You can easily set an async loop to read all your inputs. Here is one example: &gt; @async begin while true in = readline() print("input = $in\n") end end &gt; yourFunctionCall()
The piece of code seem to capture input after whatever comes after finishes. I'm still trying the understand how Channels work, so I don't currently now how to modify the code or use the mechanism. So I'm guessing that `@async` makes so that the block runs constantly, along with `yourFunctionCall()` for the sake of the example. It is capturing the input, but then it prints out after `yourFunctionCall()`. Wait but the printout isn't conditional, so why does it only print when there is a input? Does `readline` "continue" the loop when there is not a input present? I'm confused. This doesn't work: ``` stop = false @async begin while true in = readline() print("input = $in\n") global stop = true end end for i = 1:10^9 #long task sin(rand()) stop &amp;&amp; (println("stopped");break) end ``` so the code after the `readline` happens after `yourFunctionCall()`, which doesn't really help with my issue. Unless I'm missing something... do I need start Julia on multiple processors?
Ok. I found a solution. There is a problem with my previous answer: in my yourFunctionCall( ), I was using a sleep(time) call, which was preempting the thread and giving the CPU to the other one. This made the input readable since it would be running its thread for most of the time. Since you didn't preempt your function at any time, it wouldn't give up the CPU by any means to the readline( ) @async call (I actually don't know why they don't use a Java like [Thread.run](https://Thread.run)( ), which would be much easiier this way and it's probably easy to implement). So the solution I found, recognizing this situation, is to add the sleep( ) command to yourFunctionCall( ) with a minimum value. This will eventually turn your processing slower but I don't think it will be very noticeable. Here goes my code: stop = false @async begin global stop while true in = readline() if in == "stop" stop = true end end end for i=1:10^9 global stop sin(rand()) sleep(0.00001) if stop print("breaking...\n") break end end
What’s great about Julia is that every Julia user can become a core developer because Julia is mainly written in Julia. For doing the same in Python, you better know C++ as well.
Another nice related thing is that it's very easy to debug code in Julia packages that I download. I have submitted bug reports for packages in Julia where I can pinpoint the exact error that I encounter and propose a solution. And I can fix that error by myself immediately and continue with my work instead of waiting for the package to be updated by its developers. I could never do that for python packages which C++ dependencies because I would just get some vague error from the compiled code instead.
Thank you it works like a charm. This so much better than my file detection system. the minimum value for `sleep` is 0.001 second, so I'll probably run `sleep` every 100 iterations or something. Real-time control? How is this not a thing? It's so useful.
I hadn't searched for key interruptions and handlers on google. Found this: https://stackoverflow.com/questions/23593986/detecting-keystrokes-in-julia which has a solution close to ours. Maybe this will fix the time issue.
As far as I know, this is the best freely-available DE solver avail. Crushes anything avail in Python, for example. I may use Julia in future projects just for this.
I struggled with understanding multiple dispatch myself. I probably still don’t fully grasp it, but then again I’m programming newbie. However, this post really helped me wrap my mind around the concept, hope it helps: https://lwn.net/Articles/764001/?utm_source=share&amp;utm_medium=ios_app
Thanks! I read that article when you posted it a few weeks back. The lingering question I have is what multiple dispatch means with regards to performance. Is the intention to improve performance at all? Does performance improve because the interpreter doesn't have to determine input types and thus what operations are applicable?
You’re probably better off getting an answer from one of the big dogs, but my understanding so far is this: Multiple dispatch does not necessarily improve speed/runtimes by itself, but rather provides a way to write more succinct code, which in itself improves developer productivity, readability, code maintenance, etc. “More succinct code” probably doesn’t even capture what it’s all about. There appears to be a certain elegance to it, but that’s just me regurgitating now. I tried reading through the Fed’s DSGE documentation and how they used types and multiple dispatch. It was a little bit too much for me, not being an economist or a programmer, but the little I could grasp kinda blew my mind. Given that you’re a statistician, you might have a better go at understanding their documentation, which should be on Github I believe. If you can’t find it, I can dig around and paste the link.
&gt; Fed’s DSGE documentation Are you referring this the documentation here: https://github.com/FRBNY-DSGE/DSGE.jl/blob/master/docs/DSGE_Model_Documentation_1002.pdf
This is [their blog](https://libertystreeteconomics.newyorkfed.org/2017/05/forecasting-with-julia.html), you might find it useful. There is a link there that leads to [this site](https://frbny-dsge.github.io/DSGE.jl/latest/julia_forecasting.html). I’m on mobile and it doesn’t load well, so I can’t verify, but I think either the *Estimating the Model* or *Forecasting* sections go into the type declarations and multiple dispatch. This might be the same as the pdf you provided (portable document format, not probability distribution function 🙃), but this html version is easier to look through.
It will not always improve performance, but it usually should. Multiple dispatch is a compile time operation, meaning it runs while reading the code, while the stuff that actually do the work are run time operations. Compile time scales (more or less) with the number of lines of code, while run time scales with the number of instructions to be executed given the control flow. So a 5 lines loop that executes 1000 times will take 5x to compile and 5000y to execute. So running the code in a JIT compiled language like Julia means it will take 5x+5000y to execute, so it's much more efficient to have the dispatch being in that 'x' (compile time) than in that 'y' (runtime/dynamic dispatch, which amounts to conditional operations in runtime every time you call the method). Though of course, if you have to compile many thousands of lines to execute just a small portion of the code in runtime, it will actually reduce performance (like when you have to compile giant graphic libraries just to run one very simple plot). And more about code design specifically, you use multiple dispatch to specialize parts of your algorithm to your specific data type (for example, having a sort function handle differently a linked list and an array so you can make the most optimal implementation for each, while the user will not really have to understand the difference and just call sort(data) for everything while always "magically" getting the best performance).
Why are there so many N/A's in the nvidia-smi output? Check if CUDA 10.0 works with your 960M GPU. Install tensor flow 1.13 (works with CUDA 10.0) and see if that works.
In the case that you misunderstood my last line there, I just wanted to show my appreciation for the algorithm :) It's honestly more than enough for me. But I'll go check out the site as well
Yeah, I did :) I just think that 10^9 × 0.0001 is a huge difference in effuciency and that there should be a better way. But, if that works fine for you, awesome! I am really happy to help :D
From [NVIDIA website](https://www.geforce.com/hardware/notebook-gpus/geforce-gtx-960m/specifications) it simply said CUDA is supported. I think it might still just be the memory size issue. As for tensorflow, I haven't used it in a while but if I recall correctly the last time I tried training a small CNN I also got an error about running out of memory. Simpler stuff like linear and logistic regression with tensorflow was okay though. For this project, as of now I can only have a single hidden layer in the MLP. As long as I use mini batches, even simplest cases with \~800 x N input runs out of memory with more than one hidden layer.
I don't have much experience with this stuff at all, but I'll wager that 99% of published econ papers use either some variant of Nelder Mead (e.g. fminsearch in matlab), bisection or golden search. Then they either use grid search or simulated annealing for the structural estimation of parameters.
I believe Gilbert Strang’s new MIT OpenCourseware class uses Julia in addition to MatLab now.
Wow, these guys wrote a whole fast compiler for static computation graphs generated in Julia. They are right across the street and I never even heard of them before haha. This is some cool stuff and I'm definitely going to have to talk with them. The computation graph itself might be a very useful tool for other applications beyond PP.
How big are the street's at the MIT?
Small enough that you don't even have to wait at a cross walk. For reference, the Julia Lab is at [32 Vassar Street](https://www.google.com/maps/place/Ray+and+Maria+Stata+Center,+32+Vassar+St,+Cambridge,+MA+02139/@42.3616095,-71.0928242,17z/data=!3m1!4b1!4m5!3m4!1s0x89e370a95d3025a9:0xb1de557289ff6bbe!8m2!3d42.3616095!4d-71.0906355) and these guys are at [43 Vassar Street](https://www.google.com/maps/place/Brain+and+Cognitive+Sciences+Complex,+43+Vassar+St,+Cambridge,+MA+02139/@42.3623024,-71.0939546,17z/data=!3m1!4b1!4m5!3m4!1s0x89e370abfd23371f:0x6de0e79d1bce9bf7!8m2!3d42.3623024!4d-71.0917659). We should get a Julia flag and claim that road.
At my uni, I’m helping transition our linear algebra courses to Julia. We use Strang’s book!
 Try running &amp;#x200B; 1:3 |&gt; x-&gt; ind\[x\] |&gt; y -&gt; M\[y\]
That did not work, same result.
you can splat it: M[ind...]
Perhaps you mean ``` M[ind...] ``` ?
yes, that works
[Gen package website.](https://probcomp.github.io/Gen/).
It's a great package, but so far at least it's not really novice-friendly, as the article claims, but that's likely since it's still under heavy development. I think its appeal is that it allows you to perform customized inference using whatever julia code you want as your model.
I wish they used Distributions interface (rand(D,N)), but maybe there's some technical reason why they don't. They use Distribution internally though.
I remember I had a 900 series GPU and had weird errors when I tried running neural network code with it. I didn't really play with it much but yeah your GPU is the most likely culprit.
I agree. would you mind opening an issue?
Uhh... Flux anyone?
Hmm... compares with Turing?
ok, answered my own question - println is not thread safe, leaving this here for other people in case they run into this problem, the correct code is: func\_1(printlock) = while true lock(printlock) println("foo") unlock(printlock) end func\_2(printlock) = while true lock(printlock) println("bar") unlock(printlock) end funclist = \[func\_1, func\_2\] printlock = Threads.SpinLock() Threads.@threads for f in funclist f() end
Awesome!