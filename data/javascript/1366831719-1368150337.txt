A DRM implementation that has source code available renders it useless. I don't know a single example of open source DRM. There has to be some "black box" on the user's computer, which is why I say that DRM turns a computer against it's user.
Yah, I don't like the idea of a black box either. I think the ultimate goal is just making it hard for people to do something, not perfect. I'd be surprised if there weren't an open source option that could made. SSH comes to mind, it uses math cryptography that in theory COULD be broken, but would just take a long time.
Yes, because as explained already many times. You can't create a perfectly secure system, you can only create one that is effective, as in "works for most users" in protecting how an author intends for his content to be used.
While I can see why people wouldn't want DRM in a W3 spec, I'd like to point out that the spec really isn't DRM. All this spec does is provide an interface between the open technologies that we wish people would use (because HTML5 video rocks for cross browser/OS/device compatibility), and the requirements the content providers have for security (Netflix, for instance, is probably contractually obligated to protect the data they stream to users). All this does is add a standard interface for "Special Sauce" on top of the HTML5 spec. It does nothing to challenge the free-ness of the internet. If you don't implement something like this, you end up with the giant mess that is Flash/Silverlight/etc. Once you implement this, you can (hopefully) have a standard DRM library or two that can do the necessary handshakes and decryption quietly in the background.
The problem is that there's competition between build systems much like there's competition between frameworks. For instance, I don't like AMD, preferring npm's CommonJS; I like grunt pretty well, but I'm not all that impressed by Bower, so I'm unlikely to use Yeoman. I mean, this project uses grunt (CommonJS) _and_ requirejs. For good or ill, boilerplate curves around this problem by providing a constant, safe blank slate. Personally, I think the rise in boilerplate projects is indicative of a push towards a new, encompassing framework, coalescing from many small pieces into a competitor for (if not destroyer of) Ember and Angular without their cumbersome monolithic mass. Then again, I would see it that way; I have my own horse in that race. 
&gt; It forces them to have a license. How? We already know that it's not too difficult to intercept the output of the decoder, and thus get an equivalent video file without DRM. If these are distributors who already operate outside the law (by distributing without a license), why wouldn't they just distribute a DRM-stripped version?
[citation needed] You sound like MS in the early 90's. \^\^
You honestly believe that a high budget TV show like Game of Thrones could be made by a network of part-time volunteers? How would that even work? That makes no sense. While we're on the topic, several of the sites you mentioned have methods for making money: Ubuntu and Mozilla have lucritic search deals, OpenOffice has corporate sponsors, Wordpress and Drupal do consulting, etc. Regardless of your stance on DRM, only a fool would argue that producing high quality media content is not expensive.
Thanks menno :)
I agree with you here. I think a lot of people are missing the point. Hollywood isn't just going to use non drm'ed video. They either use a proprietary plugin like flash or silverlight, or get something built into the html5 spec. The people who don't want drm in their browser can use a browser that wont include it. I'm sure there will be many browsers who dont support it anyways.
CTRL + U or CTRL + SHIFT + J
Those VHS tapes were hell of a lot harder to copy and distribute than just clicking a button. Also, the tapes themselves cost money.
I thought anyone can participate in the standardization process and the spec can only pass if there is a consensus among all participants. I'm sure EFF won't simply let this pass?
They're trying to get control over the whole chain. All of their attempts fail, but they've tried to build DRM into HDMI cables, and make it so monitor vendors add DRM tech to their devices. The net result for us though, is clunkier, kludier, crappier, increasingly locked-down technology. Poorer support for open platforms like Linux. I'm annoyed because this is just going to make HTML5 bigger and more impractical to implement.
&gt; you just need enough security to **feel** what you are trading to the world is protected enough to be worth your time. Emphasis mine. You are asking that, in order to make the content creators have warm and fuzzy feelings, I shall give up doing the following things with content I have purchased: * Use it on a device by any unrelated manufacturer, e.g. locking me to ereaders from the same company (if I buy books from Amazon or B&amp;N) * Have it available if the original seller goes out of business * Use it when disconnected from the net * Display it without being forced to look at menus or commercials * Display it in a context I choose, e.g. using XBMC to get a systematic overview of episodes I've watched vs not watched. * Work if I upgrade my system. * Work even if the vendor choose to upgrade their system. * For physical media, allow me to have backups (or reasonable replacement costs), so I don't lose my investment if something happens to a fragile physical copy. * Allow access without any kind of monopoly-extending region enforcement. All of these are things that I feel are completely reasonable for me to expect for material I've purchased rights to. DRM is stopping me from doing this, and DRM systems have consistently violated these points. And it doesn't stop piracy; it just makes piracy more attractive. I don't do it - I rip DVDs instead - but it is a pain. The only benefit the DRM gives is the ability to enforce licenses on player manufacturers, licenses that force them to sell crippled players - players that refuse to skip when I ask them to, that refuse to bypass menus, that can't buffer the movies on disk. And the warm, fuzzy feeling, of course.
&gt; I know DRM isn't foolproof; **all** movies and shows still end up on torrent networks. FTFY.
Keeping high-value media off the web isn't going to happen. The high value media will always be available from illegal sources; and it is just going to get easier and easier and less and less risky to get them from there. The question is whether the media companies is going to be hidden from reality with fake warm fuzzies of DRM, or are going to face up to this and do this without DRM.
seems to be 10% but it varies day by day
No it doesn't. Open source does not mean something is insecure. There isn't open source DRM because it's not something useful to the kind of people who volunteer to write open source software.
Freely opt out of? Tell me how I can legally purchase a copy of most modern movies without having DRM applied. These movies are given a temporary, partial monopoly on reproduction in order to enhance the production of culture. The production of culture is effective - but this again means that if I am to be part of culture, to have reasonable cultural references to communicate with people, I need to have access to them. I can either opt out of communicating effectively with young people, or I can accept DRM or break the law. I'm getting old - I've given up on staying up to date on young people's media. That is a much worse option for young people, and pretending that they can "freely" opt out of this is disingenuous. 
This does nothing to stop organizations that are willing to operate outside of the law. But, most organizations only operate within the law. DRM circumvention provisions in the law prevent business from circumventing DRM. In order to legally decrypt said content you will have to sign an agreement with the content owner, and the content owner can place whatever restrictions within that license they want. So, DRM gives content owners full control over legal distribution, over and above mere copyright protection. My tl;dr version is just a summary, it is better explained if you click on the link.
If they made it legal to crack DRM would finally die. Big media is not stupid enough to really think that DRM prevents copying. The proof is in the pudding. What it does do is legally prevent media players, distributors, and the like, from doing anything without first signing a license agreement. That license agreement comes with any restriction the content owner dictates. DRM's purpose is not to prevent copying. Rather, it is a legal loophole used to prevent things that copyright would ordinarily allow. [More info here.](https://plus.google.com/107429617152575897589/posts/iPmatxBYuj2)
yeah because the donation model always makes people loads of money... heaven forbid someone want to get paid for their work.. experience: I accept donations for RES. I know how miniscule the percentage of people is who will donate.
&lt;!DOCTYPE html&gt; &lt;html&gt; &lt;head&gt; &lt;meta charset="utf-8" /&gt; &lt;script&gt; function func(){ var diceArray = new Array(); diceArray[0] = '&lt;img src="one.png"&gt;'; diceArray[1] = '&lt;img src="two.png"&gt;'; diceArray[2] = '&lt;img src="three.png"&gt;'; diceArray[3] = '&lt;img src="four.png"&gt;'; diceArray[4] = '&lt;img src="five.png"&gt;'; diceArray[5] = '&lt;img src="six.png"&gt;'; var num = Math.floor(Math.random()*6) /* Hopefully this will get all the proper things in motion. */ alert(" You Rolled "+ diceArray[num]); } &lt;/script&gt; &lt;/head&gt; &lt;body&gt; &lt;p&gt; See how you do against the computer &lt;/p&gt; &lt;button onclick="func()"&gt; Click to roll the dice &lt;/button&gt; &lt;/body&gt; &lt;/html&gt;
Hey man, Here are a couple of things I can see off the bat that you are going to have problems with: * Your function needs to start with an open curly bracket eg: func(){ } * You can only alert text, so your dice alert will not display the image. * When you are dealing with strings you need to put them in quotes. eg. your img tag should be '&lt;img src="one.png" /&gt;' * A much easier way to make an array is by going like this: var diceArray = ['first thing', 'second thing', 'third thing'] etc. (you will still be able to access them by going diceArray[0] etc. * you are missing an "=" on this line: var num Math.floor(Math.random()*6) That should get you started.
Haha, someone needs to read up on W3C DRM specifications.
Thanks man, I can't believe I missed that "=" on the math random line -.- I'm getting pretty frazzled from doing this all day &lt;!DOCTYPE html&gt; &lt;html&gt; &lt;head&gt; &lt;meta charset="utf-8" /&gt; &lt;script&gt; function func() { var diceArray = new Array([num]) diceArray[0] = '&lt;img src="one.png"&gt;'; diceArray[1] = '&lt;img src="two.png"&gt;'; diceArray[2] = '&lt;img src="three.png"&gt;'; diceArray[3] = '&lt;img src="four.png"&gt;'; diceArray[4] = '&lt;img src="five.png"&gt;'; diceArray[5] = '&lt;img src="six.png"&gt;'; var num1 = Math.floor(Math.random()*6) var num2 = Math.floor(Math.random()*6) var num3 = Math.floor(Math.random()*6) var num4 = Math.floor(Math.random()*6) /* Hopefully this will get all the proper things in motion. */ { document.write("&lt;p&gt; You Rolled "+ diceArray[num1] +" "+ diceArray[num2] +" &lt;/p&gt;"); /* { document.write(&lt;/ }*/ } } &lt;/script&gt; &lt;/head&gt; &lt;body&gt; &lt;p id ="p1"&gt; See how you do against the computer &lt;/p&gt; &lt;input type="button" value="Click to roll the dice" onclick="func()"&gt; &lt;/body&gt; &lt;/html&gt; This is how far I've gotten so far with all your suggestions sans the easier array one is it supposed to go var diceArray = ['&lt;img src="one.png" /&gt;, &lt;img src="two.png" /&gt;, &lt;img src="three.png" /&gt;, etc... 
Hey thanks for making the changes on the code, I appreciate the help!
A fair point. The counter is, if your product was behind a paywall how many users would you have? Do you really think you would have made money? - Honestly curious.
Ok, I see what you're saying. But it seems pretty screwed up that an algorithm (in this case, one to circumvent DRM) can be illegal. I fully understand and accept the illegality of unauthorized content distribution, but the very same principle should argue that if I write an algorithm, I can run it, or give it away, or sell it.
you have a solid argument, but one thing caught my eye -- &gt; I don't do [piracy] - I rip DVDs instead. Just curious - are these DVDs you own which you are merely moving to a more convenient format, or are you using "piracy" as a synonym for "downloading pirated content"?
1998 called, they want their *whoosh* back.
I *honestly believe* it hasn't been done yet. I *honestly believe* that **20 years ago, people said the same thing about software that you're saying about media.**
You should go through some basics on http://www.w3schools.com/js/js_intro.asp
Software is a very different kind of thing than a TV show. Say I write a piece of software, maybe a basic implementation of pong. I can send you the source and you can make improvements, say by improving the physics. You send the code back to me and I can add an intelligent AI opponent. Keep that up and pretty soon we have developed a sweet new game. That works because software is *composable*. Sadly, an episode of Game of Thrones is not composable. If I shoot a scene of a bare throne room, you can't take my work and add the throne and tapestries. Post production aside, video has to be shot all at once. Likewise, the barrier of entry in developing software is very low: at a minimum, you need a single computer to edit, compile and test your code. Making a TV show requires cameras, sound equipment, stages, hair and makeup, lighting, etc. All the properties that make open source, free software possible simply don't hold with media as complex as a TV show.
If you don't like DRM, there is a whole world of legal music, videos and other entertainment out there without DRM. There's also a whole world outside your computer without DRM &amp; with better color, resolution, sound and interactions too.
Ok, then he should write a parser for JavaScript in JavaScript. It is not easy if you have tried it. It is far more challenging that implementing a tool that already exists.
&gt; Had look at yeoman, was on the beta for it. Totally agree with you about initial impressions of Yeoman, but I thought it worth mentioning that Yeoman has changed quite a bit since it's humble beginnings. It might be worth your time to take another look at it. If it still doesn't meet your needs, it'd be great to leave some feedback for the team to help them improve. :) Cheers!
This is a presentation by Nicholas Zakas from Fluent conference in 2012. 
If out will mean I can watch Netflix on Linux, I'm OK with it.
http://www.sprymedia.co.uk/article/visual+event+2 Give this a shot. If that fails, open the resources panel in Chrome and start ctrl+f-ing for 'mousedown' and 'click' :P
wow. that is awesome tool, if it doesn't save me, at least it will get me closer, thanks!
So Netflix is going to wait until 2021? I don't think blockbuster was around that long...
I don't know if they'll wait until 2021 but from what I understand after reading the article it was part of their decision to pursue HTML5 DRM.
I will say.. I did a small scrapping project with jsdom and ran into problems. If your project is basic scrapping, I highly suggest cheerio. I'm pretty slow and it was great and easy to work with. 
i don't think you should be calling this promises if they dont match any of the existing specs. in fact, i would probably actively dissuade people from using this if it came up. Refer to them as 'futures' or something else that isn't as loaded.
if my product is readily demonstratable through videos or a solidly designed page, plenty. this particular product has an impressive video. I get what the product does and it looks great. RES may not lend itself quite as well to that, hard to guess. What I do know is that out of roughly 2 million users, only a few hundred have donated. I didn't create RES for money, so I'm not terribly distraught about that. I'm fine with giving to the open source community. However, the whole "everything should be open source and free" line is a load of horseshit, too. Take SublimeText for example... all the people I see pissing and moaning that it's $49 (actually $70 now but when I saw most complaints it was $49!) and that's somehow a fucking crime against developers. Any Javascript developer in the US worth his salt is making anywhere from $50k-$150k or even higher. Yeah, that's a wide range, but let's look at the $50k guy - that's $25/hour he's being paid. $70 comes out to less than 3 hours of his salary for something he'll likely use for about 2,000 hours per year or more - and yet somehow it's a heinous crime to charge for the product. As a guy whose full time job it is to write code: I'm sick of seeing people whine that everything I do should be freely available. It's how I make a living and I shouldn't be made to feel guilty about that.
Yes, that was the first point of the post...and it quoted that exact piece 
W3schools is a horrible reference for JS (or anything web related), here's why: http://w3fools.com/ Although they have improved a lot, there are much better alternatives like CodeAcademy.
Myth #1. It will be illegal to crack, but it doesn't matter how crackable it is. Did you read? 
Woosh. 
It won't. And you can use wine to watch netflix just fine. 
You clearly didn't read. 
Theoretically, there's no limit. Practically, the limit depends on the RAM/CPU/NET power of your server, but is rather high compared to classic web servers.
If you don't want it copied, either encrypt it or don't put it on the web at all. Digital Restrictions Management is just an erroneous attempt to find a technical solution to a legal problem.
&gt; you can (hopefully) have a standard DRM library or two that can do the necessary handshakes and decryption quietly in the background. Nope, you'll have a proprietary one, that's binary only, and yet again it won't be available on any kind of libre linux or for that matter any new platform without someone paying licensing fees to have the code written for them.
Occasional? I've been dealing with them since flash has had video playback support.
It won't, because they still won't bother to implement the new crypto module for linux. They'll satisfy 99% of their user base with windows and mac proprietary modules, and call it a day.
They can probably watermark the stream to tell who was the license who distributed it.
Factor out the pure algorithmic parts into separate modules. Test the separated modules entirely in isolation with tons of data and edge cases. Testing is so much easier when the parts are all split apart and can stand entirely on their own.
Then we're in _exactly_ the same situation that we're in now, and that's the worst case. Given that Google is leading the push and (if I recall correctly) has started implementing their own solution directly into Chrome, I wouldn't be surprised if we saw the other major browsers following suit. Two of the 3 major players behind the push are both companies that are Open Source friendly - Google, of course, has symbiotic relationships with the Chromium and Android communities (among others), and Netflix has a whole lot of stuff up on [Github](https://github.com/netflix) under an Apache license. There's nothing inherent to DRM that requires that the algorithms be secret, there's just some handshaking and keys to exchange. All of that can be done with an open source project just as easily as it can some proprietary binary. Will that happen? Who knows, I sure hope so. Is it possible we'll end up with Flash and Silverlight 2.0? Sure, that's a possible outcome. Hell, I'll even say I'm a tad bit worried about how this situation will turn out. It's quite possible the companies will all run in opposite directions and we'll end up with more binaries than we know what to do with, all of which sorta work in some situations. Which will put us exactly where we are now, at which point we can try something else.
&gt; If you don't implement something like this, you end up with the giant mess that is Flash/Silverlight/etc. Exactly right. The choice here isn't between DRM or no DRM, it's between whether we want those companies that rely on DRM for their entire business model, like Netflix or Spotify, to continue to use Flash or Silverlight, or if we want them to be able to load their DRM code in a standard way that is built into HTML5. If this proposal is better than the status quo of Flash and Silverlight, then it is a benefit, not loss.
It's also childishly easy to lie to the tax department or to drink alcohol and drive. There are a great many things in life where doing the wrong thing is childishly easy - that doesn't mean that there shouldn't be a law against it or that having a law against it is silly. If we could use cryptography to make it literally impossible for someone to run a red traffic light and cause an accident, we would. But the fact that we can't do this, and it remains childishly easy for people to run red lights or cause accidents, doesn't mean it's a silly law.
I've ran into 'memory leaks' when using jsdom for scraping large amounts of data. I found cheerio to be very light and simple. Some jquery functions might not be implemented, but if you keep that in mind there shouldn't be much problems. If anyone's interested, I've made a scraper that will retrieve the .ISO downloads of various Linux distros. It uses cheerio too, so it might be worth looking into: http://github.com/FrozenCow/distscraper
A little bug I noticed in Right Disabled Demo http://www.screenr.com/Wi57 
Well, if you want to get picky, Adobe actually implemented flash for linux, for a while... But I'd rather say they're all as shitty as eachother.
How is this different from [Knockout.js](http://knockoutjs.com/)?
Oh god, that website brings back bad memories. I'm pretty sure their little legal copy-paste thing used to be like 10 lines long
If by "fine" you mean as long as there aren't any updates, and you don't mind glitchy performance.
Chromebook.
fighting drm at the tech level is just absolutely stupid imo. It needs to be dealt with at the legislative and economic levels. Urge your political representatives that you want to ensure your rights as a consumer. Speak directly to companies with your wallet. Trying to fight drm later in the chain is just a misguided waste of effort.
This is pretty awesome! A minor nitpick though: shelfs -&gt; shelves https://www.google.com/search?q=shelfs&amp;aq=f&amp;oq=shelfs&amp;aqs=chrome.0.57j0l3j62l2.1985j0&amp;sourceid=chrome&amp;ie=UTF-8
&gt; Want to view HBO shows on your open source/libre web browser? I do too. Refusing DRM won't make that happen. Actually, most people are already able to watch HBO shows in their open source browsers. It's just that HBO does not offer an easy way of paying them for such a service.
[Used to be 5 lines](http://web.archive.org/web/19981202192458/http://dynamicdrive.com/dynamicindex10/text4.htm).
When you rely on security through obscurity, open source seems like a bad idea.
&gt;You clearly ~~didn't read~~ don't care. FTFY
The observableCollection methods should return observables which update whenever the collection changes. For example, filter should return a new observableCollection with only the values of the original ObservableCollection which go through the filter. 
Use a static website generator like Middleman or Jekyll. That way you can use all kinds of template languages, Markdown, SCSS, code highlighting, and so forth. You can put everything into SCM. You can run it as a server (change stuff, press F5). And once you're happy with everything you can build/generate static HTML files (SCSS -&gt; CSS, CS -&gt; js, etc).
yes, the UI widgets are JS. The platform they wrap around should be inconsequential. We can't host this documentation in the platform unfortunately. So, this is API level docs what you demonstrate here. Even if they have examples, you'd still have to know what methods/classes you're going after. I'm more influenced by those demo docs that are grouped by functionality. eg: I don't knwo what class I need to use, but I know I need such-and-such effect or such-and-such functionality. That to me is how the demo docs should be organized. That said, I'm not terribly worried about WHAT to write, or how to style it. I'm more worried about HOW we are going to write it. It'd be rather time consuming to write plain HTML manually, with a collection of external tools to handle things like syntax highlighting. 
So the idea is you have a proxy 'collection' that caches a filter on a Backbone.Collection, right? That's nice and lightweight. I run into a bit of confusion when figuring out whether filters/sorting should happen at the Collection or the View. In general it's pretty clear that it's a Collection concern, but a theoretical question: If you weren't using Marionette's CollectionView, can you imagine mixing in the VirtualCollection's functions onto a View? As if that particular View _were_ the subset of the original collection? The obvious downside is that it binds the data to that particular view; on the other hand, because there is no data duplication, creating another VirtualCollectionView isn't expensive. In fact, if you do it this way, while each VirtualCollectionView might be initialized with the same references to the same models, they might diverge and find their own sorting or more selective filtering or whatever. I'm mostly thinking out loud here. Let me know what you think, or if I'm way out in impractical land. 
Maybe I'm not seeing the problem you're trying to solve, but what advantage does this have over using collection.filter and rendering to a view? Or making an object of views from the filtered collection and listening to the collection change event and re-rendering the view associated to the changed object? (Also I'm on my phone so I didn't really dig much into the library) Edit: I missed the "no data duplication" part, which in the case of making an object of views would be duplicating the data from the filtered collection models.
 b = [a, a = b][0]; is the same as a = b; b = a;
You have a recursive function, and at the end of the recursion you have the alert (which shows the correct answer), but return nothing. So, do the alert (if you want) and return anyway, but more important, replace the "gcd(a, b)" line inside gcd with "return gcd(a, b)" [Here](http://jsfiddle.net/mutU9/3/)
Please read the article. Your comment is literally the first myth debunked. If you disagree with the assessment of the article, please respond to the argument made there.
Yeah you need a temp variable anyway, unless you do some fancy arithmetic. The swapping doesn't work in OP's code, however. A becomes B and B stays B. But you don't really need to swap the variables. Just call gcd(a, b) or gcd(b, a) depending which is smaller.
I was not aware of these two things. Thank you. 
OP's code still works. b becomes a, and a becomes b. Try it yourself: http://jsfiddle.net/KrgAC/ I agree though, it's clever for clever's sake. Just add `if (a &gt; b) return gcd(b, a);` into the function, and the order won't matter.
Didn't know that, very interesting. Do they allow user modifications?
Sometimes you're not trying to render only the filtered models. Let's say you have a large collection that you grab from the server and you split these off in to several collections based on a filter function, each one a collection itself, or perhaps you've grabbed a collection somewhere in the middle and you want to pass it along, but only with a subset of models but you still want it to be 'ilve' not all collections are tied directly to a view, sometimes you're just using them to pass to other subsets of your program, which may in turn pass those along
So it does. Should've taken closer look at it. My bad.
I wrote a collection filter that takes in a collection and sets it as the prototype for the new object. Then I duplicate any functions on the instance and bind them (actually a special bind where it's bound to parent only if called with the filtered collection context) to the original collection (except a few choice functions like the underscore and event handling methods). Last thing I do is a bit of logic when the parent collection fires an event and I filter the add/remove events (or change change:blah events in to add/remove) depending on whether they now pass or don't pass the filter. I then fire the updated events. All this means I have a filtered collection that acts exactly like the original collection (we have extended collections with different functionality) and it can be passed around without anyone knowing the difference. I had it up on Github a bit ago, but had to take it down while my company is going over their open source policies but I'll open source it again as soon as I'm allowed. I went down the route of only having one functions that deals with all events rather than split up add/remove/reset and filter the models collection everytime because if the filter function is not pure then that could make your models collection stale (and our filter functions sometimes depend on application state). 
ah, thanks everyone! I'm pretty new at js so all this was really helpful. 
best thing would be to add it to package manager: http://wbond.net/sublime_packages/package_control/package_developers I can definitely see myself using this
this looks like fun: function gcd(b, a) {a%b?alert(b):gcd(a%b, b)} gcd.apply(null,[prompt("enter a"),prompt("enter b")].sort())
Hehe. Very clever ;)
Just for fun, a jsperf benchmark: http://jsperf.com/swap-cute-vs-simple Pretty much what you'd expect. 
If they don't get the option of using DRM, then they're not going to do it using DRM. The non-DRMed pirate options are going to keep improving against them, and at some point the pirate options are going to be so good that there is no convenience, availability, quality or risk advantage to using legal as opposed to pirated media. Right now, they are using DRM because pirating isn't easy enough that they want to really compete. That will change as pirating gets easier and easier. My opinion is that it is better for everybody if it change early. The earlier it change, the earlier we can get rid of the inconvenience of DRM, the earlier people can get into the habit of buying legitimate copies. If we allow them to do too much protection for now, people will get into the habit of using pirated copies for convenience. And any attempt at making DRM easier to use just stretch out this problem - it's all like a bandaid, rip it off and be done with it. In thirty years, all teenagers are going to either have a copy of all movies ever made, or their are going to have convenient, always-available access to them at a cost that is so low that they don't mind - because they *will* be able to have a copy of every movie ever made with so little effort that it is easier to just copy it all than to select what to pirate.
All the requirements you describe are a result of the present way of making shows. I am not certain that we'll ever get past it by technology - but I'm not sure we won't, either. Rendering technology is getting better and better, and there are shows made as [Machinima](http://en.wikipedia.org/wiki/Machinima) that manage to capture a reasonable audience, and interesting work made using [Source Film Maker](http://www.sourcefilmmaker.com/). It is not obvious that this type of technology cannot be extended to make most kinds of movies. It is obvious it would involve a lot of challenges - but not obvious that it won't be possible. I am not sure it will be enough to make this kind of entertainment be done open source style; having made a handful of movies using traditional techniques (cameras, actors, etc), I can say there are a number of large advantages to working in a traditional style, and working through a user interface like Source Movie Maker doesn't yet make it all that easy. And making media don't have the same incremental and repeated payoff as developing software does. I am, however, sure that predicting that "technology will never solve hurdle X" where there is no good theoretical reason to say hurdle X is non-solvable historically has tended to be a losing game.
There's probably a way to put a smart chip on the GPU and periodically update it's key in the open.
Yeah I suppose the term "promises" has taken on a bit of a sheen on modern JS culture, beyond the generic programming principle. Many might jump straight to assuming something specific like the promises/a+ spec. Not something I'm going to lose sleep about at this point :)
&gt; neat you're about 999 shy.
I like this approach. Very flexible -- you could easily swap different filters in and out. 
What exactly is Parse? Never heard of it
Parse does two cool things. First, it handles push notifications for mobile apps. Second, it eliminates the need for a backend server. They become your back end and handle the database and routes. This lets you build web/mobile apps that are pure JS/HTML/CSS
Which is a closed system, unlike whatever Linux distribution you install manually.
unfortunately, I did all that and came up empty, without even delving too deep, I commented out all the things I could find that use `.which`, no luck. then, while writing this message I decided to check google again and found this - https://github.com/twitter/bootstrap/issues/7118 I'm so happy it wasn't me :) thanks
How many billions did they buy it for? 6?
I was joking.
They've already said it's not a talent acquisition and they aren't killing it. Parse is a great service. Not sure I'm happy to see Facebook pick them up but they did say it'll live on. 
Ah, cool. I'd imagine a few people would be upset by that. I don't use it myself and don't do much coding at all for that matter (subbed out of interest). I'd look at it kind of how Instagram was acquired and the reasons behind it and the motivations of Facebook. I don't use Instagram but did it get crummier when it was acquired or just mass marketed? I read that they bought it because it was competition. If not for that reason I'd suspect they want to monetize Parse further by buying it; a you scratch my back I'll scratch yours sort of thing. I'm out of touch with all this stuff, hahaha. 
They probably know their app platform is a complete joke &amp; want to eventually revamp it.
So is the onchange not firing? Have you tried different browsers?
Case sensitivity. Sex.
Two things. Case sensitivity - your ID is "Sex" not "sex". And options is an array, so the correct syntax is select.options[selIndex], not select.options(selIndex).
Here: http://jsfiddle.net/9G7HF/ 
And please close your &lt;option&gt; tags
Thanks for the help.
People do go to great lengths to implement their anti-features.
Wow, what is this badass bash configuration this dude has running? https://raw.github.com/f/omelette/master/resources/omelette.gif
nice! thanks
No backend so someone can just send SQL to the database from the client. Doesn't sound very secure
The average user has no idea how badly they're being fucked by such restrictions.
Isn't that a bit arrogant?
Absolutely not. Do *you* realize how bad digital restrictions management is?
Let's assume for the sake of argument that we are talking about a purpose built device like a game console to keep it simple. What's the problem with DRM?
I can't find the docs anywhere apart from the repo. Are they hosted anywhere? Edit: Specifically I'd like to know the difference between underscore.function.arity.js and _.partial function currying
In the case of a video game console, such copy protection prevents backed up game discs or custom software from being run by the user. Such a restriction isn't inherent in the technology, the DRM anti-feature has to be in place to make it not work.
Make a JSFiddle, so much data is missing to help you
This could be bad news for Parse users. I liked their product and was thinking about using it in some serious project, but they could suffer the same fate as Drop.io that was killed after being acquired.
I can only give you one upvote but man, your comment is too close to home for me. A lot of my business owner friends say that cheap customers are the worst customers. I wonder if it's the same with developers who demand everything for free. Just as a food for thought, have you consider charging 1 time fee for downloading RES? Just A/B test that for 2-3 months, see where it goes.
&gt; Just as a food for thought, have you consider charging 1 time fee for downloading RES? Just A/B test that for 2-3 months, see where it goes. Nah, it's on github and I never made it to make money. I'm disappointed mostly at how many people *say* they'll donate, or even publicly post saying they ARE donating, and I see nothing in my inbox. It's fine. I'm not trying to sell it, so I can't really complain too much. It's just really disappointing to read all over the internet "why isn't this software free?!" as if no work was put into it.
... bad joke? or... really don't know what parse.com is?
they're claiming that Parse will still live on and be business as usual, but I'm not sure.
I think this is a great lead to the "correct" solution. I've never even heard of web workers...this is what happens when a physicist writes a complex web application. Thanks for the tip!
Have a look at http://yeoman.io/
Makes no sense to have &lt;br&gt; in a select, BTW.
I believe you can get media replacement for less than the actual cost of a game but certainly more than the cost of a blank CD. I chose the game console for example precisely because it is not being sold as a general purpose computer. Your automobile may very well have processing capability that you could use to run custom software as well but no one expects to be able to do that because it's sold as car not as a computer. Just because something has the hardware capable of being repurposed for something else doesn't mean the manufacturer has to support that (or can't actively take measures to prevent its use for purposes other than what it is being sold for). You know there is DRM and you can choose not to buy it if it doesn't suit your purposes. I do have problems with the laws surrounding what you can do with hardware you bought but I don't have any such problems with technical measures to prevent certain uses as long as the buyer knows upfront what they are buying. If they decided to plaster the circuit board with epoxy to keep people from modifying it I'd be OK with that too because I accept it as a black box that performs the function it was intended to. If that means the device is utterly non-modifyable it's OK because I already assume that's the case and still choose to purchase it but certainly not because I don't understand DRM.
Yeah i messed up and had that on every line... 
The docs are in the repo only at the moment. It's still in active development so rather than re-generating the docs constantly I'll put them up when the library has stabilized. For your other question, Underscore's `_.partial` function gives partial application (not currying) up to the number of args given. If I give `_.partial` a function of three parameters and 2 arguments it'll give me back a function expecting one more argument (the right-most). The `_.fix` function in underscore.function.arity.js is also giving partial application except that it let's you fill in slots for the arguments that you want to fill in later: var foo = _.fix(aFunction, 1, _, _, 4); The `foo` function will take two arguments and call `aFunction` with those arguments in the 2nd and 3rd argument positions.
If only we didn't have to target browsers other than webkit and firefox..
Good one sir. I prefer these bindings for query selectors: * `$.one = document.querySelector` * `$.all = document.querySelectorAll` Also you could mention awesome [classList](https://developer.mozilla.org/en-US/docs/DOM/element.classList): * `$.one("something").classList.add("blue");` * `$.one("something").classList.remove("blue");` * `$.one("something").classList.toggle("blue");`
You are right and it gets even worse if you have to start using events such a keyup and stuff. I mostly start each new project really motivated thinking I can use native DOM API but then two hours I just include jQuery because of all the cross-browser code I have to write.
JQuery isn't free, the 90ish KB it uses can make a difference depending on your needs (i.e. mobile sites).
Or if we didn't have to deal with Ajax. Or if promises were native. Or if everyone supported CSS transitions. Or if foreach worked sanely on objects. ... or any number of the other thousand reasons why we started using jQuery in the first place because javascript let us down. This "VanillaJS" hipster nonsense needs to stop - we have tools now, use them.
**I'll give you my jquery when you pry it from my cold, dead hands!** ... seriously though, its important to know some vanilla js for making libraries and stuff. Massive deps FTL.
I get that, but i'm not going to lie - chances are if you implement the functionality in jquery yourself you will probably do it wrong. (unless we are talking about something very simple) It's like trying to write your own data structures or re implement libc. Chances are you will do it wrong. (this is by no means a shot at your skill as a programmer, the 'you' is a general word directed at the majority of programmers out there) 
you could also try delaying the DB interactions until the current stack clears. create: function (cb) { //adds a callback parameter var pv = new Mjdb.ProgressView({"label":"History Creation Progress","nSteps":10}); pv.render(); var afterAction = function(){ pv.removeEl(); if( success ) { this.render(); } cb( success ); //this sort of replaces your return statements from before. }); _.defer(this.dbAction.bind(this), afterAction.bind(this)); //_.defer waits for the current stack to clear before running functions passed to it. //here, afterAction is passed to dbAction as a callback //if this doesn't work, could also try //_.delay(this.dbAction.bind(this), 100, afterAction.bind(this)); //this delays 100 miliseconds before calling dbAction }, var dbAction = function(cb){ var success = this.model.createHistory( $(this.el).find(".hist_type").val() ); cb(success); }; now, `create()` instead of returning `true` or `false` will call a function passed into it when the DB interactions are done with the success or failure of the operation So instead of if( create() ){ //do stuff } Do create(function(success){ if(success){ //do stuff } });
Because I am a forgetful naive optimist. I pick up new stuff each time though which is a nice and makes you appreciate jquery more.
If you think jQuery alone is good enough then you shouldn't have a job in web development, simple as that. But, if you don't see the benefits of jQuery then you lack experience to do your job well. It's not an either-or proposition, they're both important and have their place (insert your library of choice in place of jQuery BTW and you get the same answer). If all you know is a hammer then it all looks like nails... but if you don't see why a hammer is better than a stick then you have a problem too! 
Hey, I've seen Yeoman in the past, but never tried it. But with Yeoman, you need to write the bower install, in here you just do nothing. That's what I understand at least. Anyway, I'm gonna take another look at Yeoman to see how it works now :). I have a "default" Gruntfile configured with bump, uglify, replace scripts in HTML, concat and so on that I use everytime. Thanks for the suggestion! 
"i don't think you should be calling this promises if they dont match any of the existing specs." Why would you need "specs" for that? Promises have been around for decades, with various semantics in various languages (e.g., *delay* / *force* forms in Scheme, recently augmented by the *lazy* form), meaning that it's already been a "loaded term" for quite some time.
Or you could just provide a different experience to people browsing on older browsers, not everyone needs those flashy animations. If they work natively; good, if not that's not the end of the world. Object.keys() will return an array for an object by the way, shim it in older browsers.
of course it's nicer... because you're doing it wrong. document.querySelector('#mainLink').style.color = 'red' my issue with jquery isn't that it's bad or it's bloated, but because people who rely on it don't know basic vanilla javascript
I copied the example from the article. What you changed it to is still worse (subjectively, in terms of readability) than the jQuery version. I only partially agree with your second part. If all someone _needs_ is some jQuery methods, what's the problem with them not knowing vanilla javascript? Does it matter? But yes, at some point, you'll need to do something that jQuery can't handle for you, and if you don't know vanilla JS it'll come as a shock. I'm not sure we as a developer community need the attitude that unless you're already experienced in hand-rolling solutions in vanilla JS you aren't worthy enough to use jQuery. Some people know jQuery, it's all they need, and that's fine.
oh. i didn't read the article. i guess the author is retarded then and should stick to jquery. haha.
C++?
Imma let jQuery be my shim. I'd rather have 1 library than a shim for every unspupported feature I want.
Yes, or C++. You could have googled that. :P
There are much worse web-devs out there than those who do html/css with a pinch of jQuery. JS, in the context of small "brochure-like" websites at least - is just sprinkles on top of the cupcake. I'd happily recommend a friend for such sites with little/no *real* js experience as long as he can markup and style a website using web-standards. No need to be mean to folks. That said - learn JS, its good for you ;-)
Yeah, ok , fair point, the term "web dev" was perhaps too broad. I'll back my comments for anyone doing web APP development specifically, that better? :) 
The term vanilla js is as dumb as the term Web 2.0 
Classlist is amazing, but I sort of feel like the it's too recent an addition to IE to expect users to have it. On the other hand, it's really easy to shim.
Does anyone know the relationship between underscore and lodash as far as why the people at underscore didn't bother to pull what lodash was doing, since it's far more optimized?
I'm sure you can get by as a jQuery developer, not knowing how to use Javascript without it, but jQuery knowledge isn't lasting knowledge, and Javascript knowledge is. People seem to forget that Javascript is the language and jQuery is just a library. Languages and libraries both evolve, and they both die out, but this happens more slowly with languages. At least that's my perspective as a more traditional software engineer. Maybe the web will make the story of JS and jQuery an exceptional case, who knows.
&gt; document.querySelector('parent')[0] querySelector doesn't return a list... so the [0] is pointless and will probably break. querySelectorAll does however.
I completely agree with you - I was responding to the comment saying that jQuery is a problem because people rely on it. My point being that relying on it is OK if all you need to do is manipulate a few elements. I'm certainly not saying it's a replacement for knowing javascript for all applications.
Random thought: a jQuery *compiler* would be really cool.
99% of websites need only jQuery.
The problem is when you become useless without access to those tools. Say you have to write an embedding script to run on any page, whether or not jQuery is available, under a tight deadline. Suddenly, missed deadline.
Because people keep writing it with a capital V. 'Vanilla' in this case is just an adjective, not a proper noun.
Javascript native objects come with a number of free properties / methods. Array.length returns the size of an array: x = []; // length 0 x.push('o hai'); console.log(x.length); // shows 1 x[1] = 4000; // Length 2 See: https://developer.mozilla.org/en-US/docs/JavaScript/Reference/Global_Objects/Array#Array_instances Try playing with code in Chrome's developer console, or firebug in Firefox -- you'll be able to see each object's properties/methods and try code out on the fly.
OHH! God it's so obvious now. Thanks! You've saved me a lot of frustration.
That's a strawman argument - just because you can't use jQuery doesn't mean you suddenly become stupid. 
Piggybacking here. Not to mention that jQuery is being responsible and branching into 2.0 and 1.9.x, where 2.0 is a thin wrapper for modern browsers and 1.9.x provides complete legacy support. However in full disclosure, I have production code that patches Array.prototype.indexOf for IE.
But you could always load it from a popular CDN (like google) and then its usually cached no?
Not stupid, useless. You can be smart and still be useless.
**Jack Bauer**: "Chloe! I need those codes, NOW!" **Chloe**: "I'm working on it Jack. These assholes embedded the abort code information in a transposed table with colspans and the only browser on this old SCADA workstation is IE6." **Jack Bauer**: "Chloe, the president is bleeding out. He's the only one who knows the second key to abort the nuclear launch!" **Chloe**: "I said I'm working on it Jack. Maybe if I can just include jQuery from the google CDN..." **Jack Bauer**: "NO CHLOE! Nobody can EVER know we were here..." **Chloe**: "Shit, fine..." 2 minutes later... *fffwwaAAAROOOOOOOSSSSHHHoooooOOOOOOOMMMMMMKADEWGDLSKJF:SKLDJFLKJCRACKSSDFDFDSLBMA&gt;SPLAT* **Jack Bauer**: "Chloe..." **Chloe**: "GOD DAMN IT JACK, I WAS JUST GETTING DONE WRITING DOM MANIPULATION HELPERS. GOD DAMN IT." **Jack Bauer**: "Chloe... America burns because of you. Because of... you." **Chloe**: *sob* *sniff* "AMERICAAAAA!!!!!" *sob*
I wanted to downvote so bad, but I upvoted because this is a worthy discussion. Why did I want to downvote? You're wrong! Knowing "vanilla JS" is important. In my experience with complex JS UI elements, jQuery can be a leaky abstraction. For example, it doesnt matter what library you use, if you need accurate size measurements, it needs to be on the DOM. jQuery is a leaky abstraction in this regard; it can provide almost anything else for an unattached elements, except for accurate sizing. Also, this may be a sign of the times, but Ive seen too many instances where developers dont truly understand where "vanilla" ends and "jQuery" begins. 
It wasn't working (the blog). Now it's back up! Sorry
yes! it does I cant thank you enough
I like to use these, it makes my code read better: $.wait = function(ms, callBack) { return window.setTimeout(callBack, ms); } $.ignore = function(id) { return window.clearTimeout(id); }
That's one option, and it will be usually true. However, there are good reasons to choose not to do that; if you have multiple libraries it's probably a good idea to concatenate and minify all your js so you can avoid the overhead of additional HTTP requests.
Module systems and package managers give you these benefits and even more without the bloat of lumping everything into a single kitchen sink like jquery. For ajax, when you use something like browserify `require('http')` works for requests like it does in node with xhr wrappers that work all the way down to IE6. [See the bottom of my recent post about this](http://substack.net/weaning_yourself_off_jquery). If you think this example is uglier than the jquery version it's trivial to wrap it in a different api but you get streaming updates as the default. The biggest problem with jquery is that it doesn't compose well since it tries to do so much. It's really hard to publish reusable components with a jquery dependency as a result and it doesn't scale up well when you want to be using dozens of modular components.
The funny thing about this post is that in a previous post he was obviously trying to make a point about how jQuery and CoffeeScript simplify things, but (as he mentions) that post was not popular, so this post he emphasizes how easily you can duplicate some jQuery features using modern JavaScript (although of course those things won't work in IE). And this post IS popular. LOL.
But jquery does the first thing. That's how .val() works. I'm curious why it's slower?
No arguing with that! Kinda surprising though. I wonder what causes that. 
That seems kinda dumb to me. I'm sure they have their reasons. Sorry I'm kinda useless in this convo, I'm on my phone. 
it is 'weird', if you are coming at it from the perspective of someone who has learned class-based inheritance. I think the problem is that many people think OO = classes, and javascript just doesn't have them. Javascript object's don't exactly 'inherit' the way these people think about inheritance. 'Delegate' or 'cascade' are probably more appropriate words for describing how the prototype chain works.
a more fair comparison would be &gt; $('.element').css(name, value); vs &gt; document.querySelectorAll('.element').forEach(function (elem) { elem.style[name] = value; }); if you really just like using '=' for assignment, you can also use &gt; $('#element')[0].style[name] = value; in the case where you know there is only one element
for simple use cases (one or two 'private' values needed) I think this is a cleaner approach function Person (name) { var name = name; this.name = function () { return name; } } although it doesn't scale extremely well, and misses some of the features you listed. luckily some day we'll be able to use weak maps and that'll be that.
http://stackoverflow.com/questions/3823357/how-to-set-the-img-tag-with-basic-authentication TL;DR - bad idea.. anyone can view source and get the password. Set the server up to proxy the image and serve it locally.
Please checkout http://h3manth.com/new/images/icanuse.png
I totally agree, quoted this comment in my post.
Please checkout http://h3manth.com/new/images/icanuse.png
classList is mentioned is one of the old posts already :) But nice one there with $.one and $.all 
Instead of disposing the handler in `disposeInternal`, you can do `this.registerDisposable(handler)` in the constructor, and then it will get disposed automatically. Also: "Prefer goog.dom.classlist over [goog.dom.classes] since goog.dom.classlist conforms closer to the semantics of Element.classList, is faster (uses native methods rather than parsing strings on every call) and compiles to smaller code as a result." (http://closure-library.googlecode.com/svn/docs/closure_goog_dom_classes.js.html)
&gt; I'm looking to store some data via a chrome extension and then be able to read that data back from an external application on my machine. Data written by extensions is stored in a sandboxed storage. It can be read by other applications, but your application will have to understand the format Chrome writes these files. It's not an ideal solution, and the Chrome implementation may change, making your app obsolete. &gt; I would prefer to use WebSockets and the browser's WebSQL or LocalStorage, but if I can't access that with the external app, I figured a WebSocket server to host up that data in JSON would work. WebSQL is deprecated from HTML5. Don't use it. &gt; 1) Is a WebSocket server possible in a chrome extension (without NPAPI)? You can't write a WebSocket server using an extension. You can write a Chrome app using sockets API. Apps use the same platform and same extension file formats/manifests etc. &gt; 2) If no, Is it possible to read/write files (silently) in JS and still be able to read those files outside of the browser? Again, technically possible but not very good idea. &gt; 3) If no (again), I'm open to architecture suggestions :] What are you trying to achieve? Do you really need to write an extension? Can you instead write an app, which is more powerful and will let you do most of the things you are asking? 
This is now one of my favorite programming-related reddit posts. I want to follow you around like a lost puppy.
&gt;What are you trying to achieve? Do you really need to write an extension? Can you instead write an app, which is more powerful and will let you do most of the things you are asking? Yeah I feel like this is a classic [XY problem](http://www.perlmonks.org/?node_id=542341).
Ultimately, I'm scraping song info from Pandora and need to report on that data. Thanks for all the helpful info, Zebra. 
I replied to the parent if your comment, buy what I an looking to do is report on data that I gather from Pandora.
Indeed it is much cleaner and far less complex, I mention this at the top of my blog post on Privy, but it lacks prototype method support :( Oh my, I had no idea WeakMaps were in the works for Harmony. They will solve a boat load of issues!
I think part of the problem is being owned by facebook is not very cool. I know people will say that it shouldn't matter, but it does. A lot of developers are going to feel slightly resistant to using facebook as their backend.
I think a lot of webdevs are just upset seeing other webdevs resorting to jQuery for *simple* Javascript projects. So functionality that could have been implemented with ~5 lines of code now requires a 90kB Javascript file. 
If you want others to help you, always make it easy for them to help.
And here we get into the chicken and egg portion of the conversation. Why not use jQuery? Who cares if it's a big library? It does a **lot** of useful things! Here's a dismantling of the usual arguments: * **Filesize** Just leave one image off your page, or optimize your CSS, and you'll more than account for it! * **Load time** I guarantee that if you deliver jQuery via Google CDN it will be the fastest loading asset on your page * **Mobile Devices** I work fulltime for a mobile app company, and I have yet to see a single page where jQuery was the actual bottleneck - it's almost always using too many selectors that's the problem.
yes ;-)
&gt; 2.0 is a thin wrapper for modern browsers well, its supposed to be (and maybe will be in future versions), but as it stands now, 1.9 is 33k and 2.0 is 29k (minified &amp; gzipped), so not that huge of a difference yet in filesize. [source](http://mathiasbynens.be/demo/jquery-size)
So my takeaway from that was ... that assembly-like optimizations in Javascript isn't quite ready for prime time and your fractal generator wasn't optimized at all with asm.js? I'm assuming this is all a work in progress and expect an update! :D
Thanks, I'm fully aware of the compatibility of modern browsers. 
So you are scraping data and feed into your executable, as I understand. Chrome is going to have Native Messaging API which will allow your extension talk to your executable, but it's not in Stable channel yet: https://code.google.com/p/chromium/issues/detail?id=142915 You might consider that one.
* composability I write reusable libraries. jquery is really bad at this.
This is a value you'll want to store somewhere on your website, then retrieve the value when someone opens the page. A database may be overkill, though. 
I don't know how i would do that.
When was that written? it may apply to what's the beta version now current nightly does have it on.
How is it bad at this? It's got a great plugin architecture with thousands of plugins.
That sounds like exactly what I am looking for. That would eliminate the datastore middleman altogether. Thanks for pointing it out. Hopefully it matures faster than I did.
jquery plugins rely on globally mutable side-effects to expose their functionality so it's very difficult to tell where functionality was defined and it's really easy to accidentally break something because the functionality all gets lumped on the same $ wrappers.
Well, like the KB article says, it won't work in IE. Your workaround then is to do some server side coding, download the image to a spot relative to the page and just reference it from there. This is PHP but you can do it in pretty much any server-side language assuming the server is configured for such a thing... I'm not sure if 'copy' will work here with basic auth -- but you can use something like curl or wget and set parameters for basic auth. &lt;?php var $sites = ["site1.com","site2.com","site3.com"]; for( $site in $sites ) { copy("https://{$site}/webcamImage", 'local/folder/{$site}.jpg'); } ?&gt; &lt;body&gt; &lt;?php for( $site in $sites ) { echo "&lt;img src=local/folder/{$site}.jpg&gt;"; } ?&gt; &lt;/body&gt; If you make the page itself the script that does it, the only javascript you'd need is setTimeout( window.location.reload(), inverval); And even then you don't really need javascript, you can do it with a meta redirect &lt;meta http-equiv="refresh" content="5;URL=current-page.php"&gt; 
From The Fine Article: &gt; I decided to reimplement jQuery’s Deferred as an experiment for myself. The nice thing about doing something like this as an exercise is that it comes with a set of tests defining its behaviour for you.
This code can be further optimized quite easily using common fast code techniques typically applied to compiled languages like C and C++.
the trouble is that a meta refresh refreshes the entire page rather than just elements. That results in a blank page while it is refreshed, and that is very ugly. That's why I wanted to use js.
I swear I read the article twice looking for exactly that reason. Somehow I managed to gloss right over it.
If you give it a shot, let me know how I can improve its usefulness. Any component that plays well with Bower's conventions should work well with this plugin. If you notice any issues, please report! Any ideas to make it better would be great, too :)
Or use jQuery 2, which is built to be lighter y dropping suport for old versions of IE
As an aside, your sorting function in your JSFiddle wasn't doing anything. Here it is, fixed: http://jsfiddle.net/p9ksM/
Awesome! Bower is great, but it's hard to deal with all the stuff it doesn't do out of the box.
Really glad to see the focus on performance, it's something that needs more attention as larger JavaScript applications are being made.
so what's the benefit of this over jquery?, seems like the same just with longer code
I know that you're right, but I can't resist poking fun at yui any chance I get
d3 is a visualization library while jQuery is a cross-browser DOM manipulation library. Very different stuff. [Check out what d3 can do] (https://github.com/mbostock/d3/wiki/Gallery).
Fooled you, I actually do hate those sites--okay, let's say that I really don't like the UX of those I've actually used, which is about half. Also I found that list rather paltry. 
that looks amazingly, i guess i judged too fast
Chrome extensions can use real sockets so if you wanted to implement a websocket server over that you could (see the chrome.socket api). I've been writing a port of nodes http/tcp APIs with it
:-)
You can write to a sqlite database in Chrome (you're probably doing this anyway) and then with some investigation it should be possible to read that database with an external program and a sqlite client. Here are some clues: http://computer-forensics.sans.org/blog/2010/01/21/google-chrome-forensics/ I have got this working (but it was a major faff and I can't remember how I did it). The difficulty is that Chrome locks the sqlite file when it's running and appears to deliberately corrupt it when you shut Chrome down presumably to prevent interference. However, it can be done ... Reading and writing localStorage files directly from disk is probably possible (easier) too 
Can anyone explain what exactly a promise is? My view of a deferred is this: It allows you to define callbacks for a successful and an erroneous outcome of an operation or function call. But where does a Promise come in here?
Link? To any of those per-chance 
Thanks very much akwok, that's a big help. The JSFiddle was to sort of show what I've been looking at and trying to adapt. Just to make clear it wasn't mine, but I had been led there by a stackoverflow question. But yeah, this was more or less what I was looking for. To put it in more context, I have a database of around 300+ bird sightings of several different species, and I'd like to be able to provide total counts of the differing species that may occur is a users query. Thanks again!! Working perfectly!
A promise, in this case, is an object returned that has functions for subscribing to success and failure events from the deferred
But couldn't I just use the Deferred itself ad pass that to the function?
You could, but there's no reason the other function would need access to everything exposed on the deferred, like the resolve or reject methods. 
Okay, I was just asking to make sure I got the concept. Basically, it's just the Deferred object exposing only success() and fail() functions?
The promise exposes success and fail. Deferred exposes resolve and reject. 
http://www.htmlgoodies.com/beyond/javascript/read-text-files-using-the-javascript-filereader.html#fbid=nPOpXZz7jDR
Either need to Ajax it in or use a file upload prompt. Then use the html5 File API. Check out this small tutorial. Keep an eye out for how he uses the FileReader() constructor. http://www.html5rocks.com/en/tutorials/file/dndfiles/
i want to the code to automatically access the .txt file and display it's contents.
I would like to add that the latest event handling system, you don't actually even need to unlisten anymore (apart from when listening to DOM events). for obvs not recommended but will help reduce memory leaks.
ah, i missed this, this is great.
 var xhr = new XMLHttpRequest; xhr.open('GET', 'file.txt', true); xhr.onload = function (){ document.write( xhr.responseText ); }; xhr.send(null);
yeoman.io
Since 1.0, Yeoman doesn't do this. But, this task will plug right in with the Yeoman application structure.
If you are using jQuery, the .load() function is pretty handy for this: http://jqapi.com/#p=load $("#container").load("path/to/file.txt"); 
Bower is taking some heat for being *too* unopinionated lately. There is talk of subcommands: https://github.com/bower/bower/issues/403 and postinstalls: https://github.com/bower/bower/issues/249 which would happily make grunt-bower-install unnecessary. Until then, I'm going to try to make this tool work as well as possible. So if you do find any issues, please let me know! Also, JS soup is by far the best kind.
I went to this presentation and it was awesome! Thanks!
OK, well you can still do it with JavaScript -- Just put that php code, without that &lt;body&gt; and &lt;img&gt; outputs, in a file called something like 'reloadImage.php'. Set up an ajax call to 'reloadImage' &amp; follow that with updating the src for each img. You'll need to ensure you have some sort of random text in the querystring of the src so the browser won't use the cached version e.g., function reloadImages() { $.ajax({ url:'reloadImage.php', complete:function(){ $("img").each(function(i,itm){ itm.src = itm.src.split("?")[0] + "?" + Math.random()); }); setTimeout( reloadImages, 5000 ); }); } 
[Slides](https://docs.google.com/presentation/d/1bO1OlVtMUHkXS7aApQsX1hbaQ6qCJ01lrjTv0nChIK0/edit?usp=sharing)
Exactly! One implication of this is that when you'er given a Promise, you are assured that some other mechanism invisible to you will resolve or reject the promise. A deferred in and of itself needn't assure you of this.
I would not characterize JSONP as a great benefit, but a huge liability. JSONP is the equivalent of a store owner who gives the UPS driver a key to the back door so that he can deliver packages if no one is around. It is putting complete faith and trust in a third party not to wreck utter havoc on your site -- not to steal passwords, trash databases, spread malware to your users, and so on. The only thing preventing any of this is the honor system, because using JSONP is the same as allowing a third party to execute arbitrary code on your site. We trust that Twitter is not interested in ruining their reputation by trashing the store, but what about smaller companies? Or what about a company whose server is compromised? That we've arrived at solutions like JSONP should be a source of deep shame and embarrassment to the web ecosystem.
&gt; // Buildings &gt; z=y%U&gt;16&amp;&amp;x%U&gt;11?Math.abs(Math.cos(x/U|0)/Math.cos(y/U|0)*h):0; &gt; hD[i]=z&lt;U?0:x%74&gt;U&amp;&amp;y%75&gt;U?z/ca:z; I love how obfuscated the non-compressed code is. Great write up, just wish it went into more formulaic detail.
I would characterize it as both great benefit and huge liability. If you put trust in a third party's JSON service -- for sure, you could open yourself up to them running arbitrary code on your site and yea they can do some pretty nasty stuff with it and this is most definitly a liability. But, if you are in control of the code on both the web service and the calling page, I find JSONP is the easiest one. [There are other ways](http://stackoverflow.com/questions/3076414/ways-to-circumvent-the-same-origin-policy), for sure -- but JSONP doesn't require x-browser cruft &amp; changes to server configurations.
Whoa, small world!
shared mutable state between modules is dangerous
A promise is just an object that matches the [Promise/A spec](http://promises-aplus.github.io/promises-spec/), in that it provides a `then` function to attach callbacks which will be fired later on. It's an [interface definition](http://en.wikipedia.org/wiki/Interface_\(computing\)#Software_interfaces_in_object-oriented_languages), that's all. jQuery's Deferred conforms to the Promise interface but provides a bit more than that, such as the `success()` and `fail()` alternatives to `then()`, and also has the hooks for resolving or rejecting the promise. The purpose of the interface is to ensure that multiple libraries can rely on the same routines being available. Somebody else using your library may not be using jQuery Deferred, but instead using when.js or Q, which has their own additions to the deferred object. To make sure that your library is compatible with others, it's best to return just the raw promise that only has `then` and not any other extra functions.
Thanks. It ended up being more than one, but it's a good idea.
&gt; Non-JS event loops: Ruby, Python, PHP Seeing this list made me suddenly realise why there are so many nodejs fanboys. Try Lua and Tcl. Even Python with greenlet. Heck, there's even a decent C library to do coroutines. The key word is "yield". &gt; They can be implemented in the more traditional procedural languages as well. However, by having it built into the language, it’ll surely be quicker and have a nicer syntax. No idea what this is supposed to be claiming ... javascript is less "traditional" &amp; "procedural" than what? The event loop is "built in" and has syntax? Nope, event loops and callback-driven frameworks have existed for decades and Javascript is neither new nor magic in this respect.
Emacs Lisp also operates as a single-threaded event loop, though blocking I/O functions are still fairly common. 
I like your point about js being multithreaded for I/O but not for CPU. It's obvious to any js developer but a nice way of putting it.
Is there a video?
Promise/A is just approach to standardize promises in JavaScript, but mind there are many nice Promise libraries, that are not fully compliant with a spec (and some do it for valid reason). So don't feel too attached to it.
Thanks, those were some nice examples.
Thanks. Are there any plans to support this directly in JavaScript core?
Alternatively: var text="Text to be swapped in"; function textSwap() { document.getElementById("side").innerHTML=text; } setTimeout(textSwap, 1000);
I love the idea, I teach graphics at the university and I can see this being very useful. Some tips: - let the people drag to rotate the 3d scene, sometimes it is hard to see the transformation from the view point. - animate the process instead of showing the wireframe version. - avoid giving the same answer twice (it happened a lot) And some features: - do the oposite, show the transformation and ask which of several 3d object is the result - exercises about concatenate transforms - exercises about local and global transforms Great job.
I can't change the code at the other end as it is from a 3rd party API I'm trying to authenticate to
Great idea. Ideally have some kind of numbers on the axis.
Why is everyone comparing to jQuery? YES it is the most used library out there, but there are plenty of other DOM manipulation libraries and understanding vanilla JS helps you use them ALL, while a focus on jQuery can make your code a dependent mess
Closure compiler (and to some extent the library) has define at compile time constants that you can use to define code paths in if statement. Or you could have different versions of jQuery that target a browser. Though now-a-days it's all about feature detection, so people would probably want a feature detection library at the start then you could use that to request a certain version of jQuery. very possible, though a pain to create and I wonder how much size advantage you would actually get out of it
Odd, that should work...
Possibly add buffering / smart coordinate guessing so it doesn't lag as much and you don't need a stable connection to play.
Pssst [Here](http://i.imgur.com/C18jMhD.png)
I like the idea with the animations, should help a lot with most of the problems I saw in the demo. Maybe allowing people to toggle on off the wireframe and object individually as well. I know on this computer I could hardly tell a scale(2,1,1) wasn't just a translate in the x axis as the wireframe is hard to view sitting inside the object itself.
Definitely. Also do you make sure that the distance moved is possible within the number of frames between new packets? If not that would definitely help as well. I haven't looked at the source so I'm just assuming things. I'll look later :)
It's too early for a survey - Your quizzes are buggy. There are frequently multiple instances of the correct answer in the multiple choice selection options. After I finished the quiz, the last page was blank, with no elements available to navigate back to the beginning. My results weren't recorded when I refreshed the page to get back to the main menu. 
There is no server side validation or throttling, it's currently very trusting of the client. I'm going to work this kind of thing into Omni.js. EDIT: There IS server side validation for shooting, however, so you won't be able to shoot more than 10 bullets per second. Similar validation could be added for movement.
the definitive guide if you know no javascript, the good parts if you know some. Patterns is better for when you're already writing applications
Fantastic, thank you.
I would be curious to know what the performance difference is with an implementation of `setImmediate` available.
Thank you kind sir! 
I don't know, are we talking about [this](http://upload.wikimedia.org/wikipedia/commons/thumb/3/35/Incarceration_Rates_Worldwide_ZP.svg/220px-Incarceration_Rates_Worldwide_ZP.svg.png)? Because the round things that recently became popular are something truly idiotic, so yes, if by "bars" you mean something like the picture I linked—the kind of charts that sane people use—that's what I'd prefer. (There are other kinds of idiotic graphical data representations, such as 3D cone charts—some people actually use that crap, probably because Excel offers it and they think it's cute for some reason—but dials seem to be the hottest new way of obfuscating data sets. I hope the fad goes away ASAP.)
Here you go http://than.pol.as/OdIx Article updated
There's a jQuery plugin for that problem ;-)
~~On the site's front page:~~ ~~Websites, IT &amp; Software: (job listings)~~ ~~Javascript (0)~~ edit: It looks like the site is just starting out, good luck!
Nice article. Are the authors of when planning to get performance back down to the 1.81 levels? Or is resolving async in the later versions more conformant to the spec? What are your thoughts on rspv.js - can you give that a benchmark too? https://github.com/tildeio/rsvp.js
The first listing, of which there are only seven, appears to be from October of 2012. It appears that this isn't picking up any steam. http://www.sellyourskills.net/freelance-jobs-dashboard/project/listproject Edit: Also, 4 of the jobs that have been posted were posted by David Elbro who is also the creator of the website.
Is this for showing a tooltip? If it is, then you could try CSS transitions to fade the content in. (Only works on certain browsers though).
Oh, thanks, that's nice. You didn't have to do that for me, though, I've already divided the numbers for myself. (For those still wondering what I was talking about: Dials are too often misused. What serves as a quick-glance indicator for checking if you haven't crossed a fixed critical value of a variable (car speed, for example) is all too often used to present multiple random variables (MBs, seconds...) that you're supposed to compare to each other (e.g., in benchmarks). That's difficult compared to a simple bar chart because 1) the 2D nature of the circular dials wastes space, spatially separating the individual data points, and 2) most of us mortals are much better at comparing the lengths of bars (especially when they're next to each other) than at comparing the needle angles. For a meaningful use of a dial, see the Use pane at the http://www.google.com/fonts/ web page, showing you how reasonable the size of user-downloaded web fonts is going to be.)
Does this use websockets or flash for networking? 
Websockets - it uses Omni.js for networking, which uses socket.io.
Doesn't socket.io use flashsockets by default though? Edit: Nevermind apparently flashsockets are no longer the default in socket.io. 
http://socket.io/#browser-support - It starts with websockets, which most browsers will support now, and only falls back to flash if websockets aren't supported. It then falls back to other methods if flash isn't supported. EDIT: I could be wrong on the order, but I think websockets would be the first choice.
Tried mimosa? (mimosajs.com) - it uses requirejs and can compile your code down into a single optimised file, as well as offering equivalents for CSS files, templates, and a bunch of other stuff.
You don't need fancy arithmetic: a += b; b = a - b; a -= b; However, this is still slower than a temp variable swap: http://jsperf.com/basic-swap-vs-math-swap
Read them in that very order. The first one starts at zero. That's where you are. [I heard it's a good introduction.] The second one is about style and conventions. This helps with writing better code, which is easier to read and easier to maintain (and hopefully also less buggy). This book is aimed at people who already know some C-like language like Java or C#. [Knowing a bunch of other languages, I used this to learn JS.] The third one is about design patterns, which is somewhat more high-level or abstract. This book is aimed at people who've already written quite a bit of JavaScript. [I preordered it the day it was announced. It isn't as exhaustive as I wished it would be, but it was very useful nonetheless.]
I had not. It looks pretty comprehensive. However, I was more targeting projects not using requirejs or commonjs. Sometimes I just want to whip up a jquery widget or I'm writing code for a legacy ecosystem where using modular JavaScript is more trouble than it's worth. I don't want to write the thing in one giant file, but using a module system just so I can develop in multiple files seems like overkill. And, I think (but I don't know) that using a module system probably adds some overhead even after compilation.
I kept hitting "Skip" instead of "Check." Maybe move the button further away. Good stuff!
I've used datatables for a project and while it's a little cumbersome to use, it's better than the alternatives at the time. The main problem with data tables in general is the browser can't handle processing 100,000+ rows of tabular data. It eats up the memory usage and severely bogs down the machine. For my implementation I decided to load and cache the results server-side and include a page parameter in the AJAX call for datatables. The PHP component would reload the JSON response in full and strip out the section it needed to reply with. This was also implemented later on in the API for the service I was requesting the data from, including a search function.
Add vow to the comparison, please: https://github.com/dfilatov/jspromise
Include jQuery after your ad. 
This is great! I'm starting to dip my toes into WebGL, and this will be super helpful once it's a bit more polished. Keep up the great work!
I don't really consider that spaghetti code, wrapping your code in a closure is best practice to prevent your script from polluting the global namespace. You could add a build step if you wanted to keep it cleaner that does something equivalent to cat header.js &gt; build.js cat body.js &gt;&gt; build.js cat footer.js &gt;&gt; build.js Other than that you could re-assign the global $ object to jQuery manually after you add your script but that might break something and is far more messy imho
Neat idea, but has the severe draw back that it can't dynamically sort -- and even if you add that, as you stream data in, you're going to end up jumping the view all over the place and just generally make a mess of things. Once you have all the data, you're left with the same dilemma as before when trying to sort large numbers of rows in the DOM. So, sure, it can handle pre-sorted data sets much better than DataTables -- but big deal. DataTables' features around sorting, pagination, filtering, and column intelligence *all combined* are what makes it amazingly useful.
Relinquish $ at the global scope (use [jQuery.noConflict](http://api.jquery.com/jQuery.noConflict/)) and wrap all of the code that uses jQuery with a closure (as mentioned in the comment by/u/ThisIsMy12thAccount), while maintaining the window object as this via .call. It should be simple enough like this at the top/bottom of each js source file: (function($){ /* body of the file */ }).call(window, jQuery) 
What about \_\_defineSetter\_\_? https://developer.mozilla.org/en-US/docs/JavaScript/Reference/Global_Objects/Object/defineSetter window.__defineSetter__('$', function(val) { debugger; }); should show you who's messing with your `$`.
This looks pretty cool. I have a similar plugin I've developed over the past couple years. It doesn't try to do anything with data streaming, but it's much more enjoyable and easier to work with and configure than DataTables. Though it's been in production in a few different startups' apps for a while, I just haven't gotten around to fully documenting it yet. If anyone is interested: [jQuery Dynatable Github](https://github.com/JangoSteve/jquery-dynatable) [Dynatable docs (work in progress)](http://os.alfajango.com/dynatable/http://os.alfajango.com/dynatable/)
Lazy rendering is your friend: http://addepar.github.io/ember-table/ http://blog.streak.com/2013/04/search-find-in-pipeline-now-possible.html http://airbnb.github.io/infinity/ Basically the same idea minus pagination.
nice responsive site
Oh! Sorry I misread it, we've all been there before. Another option would be to take the ad-code and modify it so it's wrapped in a closure and preventing it from messing with your global scope (function($){ // the code from your ad script })(undefined) 
The prototype library uses the `$` symbol. (apparently, TIL, the jQuery developers took the idea from prototype) Could be prototype stealing the variable.
It's possible one of your javascript includes is making a reference to $ to shortcut getElementById. Probably some archaic script. Stackoverflow has a comment that suggests this so maybe someone just copied it from there: http://stackoverflow.com/a/3081894/182890
To be honest, a lot of those websites are pretty old. Anyone building a new site from scratch would most likely avoid YUI in favor of a slimmer and more modern web framework. I also love that when I click "..and more" I have to sign in to yahoo first.
Thanks, fixed the URL. As to modularised code- I think it actually loads faster once you've got a real code base with a lot of assets. RequireJs works out a dependency tree and loads modules async if your browser supports it- it's actually really fast! It's certainly easier to maintain, and having private / public bits in your modules helps keep code tidy and organised.
Yes, if you're developing a really large code base. Real modules are the way to go since downloading all of it up front might introduce a few seconds (or more on a slow connection) delay before your app becomes ready. I guess I came from a C world and I like to have one class per file. Sometimes I'll write a library (module) that has a public facing class, but a few support classes that are "internal". I wrote this so I could keep them in separate files rather than have a several thousand line single file with large blocks of code I'm not actively hacking on. Maybe I'm not up to speed on the latest design patterns, but I tend to think that a module can be greater than a single file/class, and that not every class needs to be its own module.
"* considered harmful" considered harmful.
This form is inside flash object. You can not access it from javascript.
looks nice but it's way too inaccessible. They should take a look at Twitter Bootstrap for a lesson in how to demo the features. I tried to look at what it could do and was bored/frustrated after a few mins of trying.
I'm sorry and even a bit angry every time I see someone trying to fix what isn't broken with JavaScript object orientation. Sure enough, "new" doesn't have any bells and whistles, but it allows for enough control and works in every JS environment you could think of.
To all the people who consider "new" as harmful: JavaScript is NOT a fully object-oriented language like Java, or C#, or whatever it is you are directly comparing with. It is its own language, has its own constraints and limitations, and should not be used like or compared to other languages directly. I hate to see people creating "constructors" or "object inheritance", but mainly I hate the use of "new" out of place.
can you send a pull request?
Thank you for adding your comments to the conversation and going through the awful code. Using promises in the benchmark itself really makes no difference as the same methods were used for all tests, thus eliminating any differences between the tests. What is measured is the performance differences between the packages, not the absolute values produced. Thus having a big overhead just to prep the test plays no part in the final results. I am not comparing Async per se, although admittedly it does look like it. Async only handles the parallel execution of the n loops in the test. The focus of the article are Promises and how poorly they perform in specific situations. It is these **specific** situations that i care about and the benchmark targets. The [scenario being tested cannot be cleaner](https://github.com/thanpolas/perf-promises/blob/master/lib/app.js#L5:#L42), 7 chained promises for every loop. I am happy for your benchmark, if you find any actual issues with the findings of the article please do point them out so i can amend them.
I don't grok this test. The spec says a Promise.then() returns a Promise, so why chain a bunch of then()s, which are a series of related Promises, which each return other/new Promises, effectively forking the resolution? The test should simply be Prom.promise.then().then() (repeating N times)., and then resolve the 'deferred'.
avoidwork the test represents a call stack of 7 consecutive functions. In all these functions a new Deferred object is created and fulfilled. It is a case from reality being tested here.
Please understand that Promises are not my interest, rather it's the hit i suffered that drived me. Thus the poor test setup etc. Please feel free to send a PR with Deferred and i will update the article. 
Some things author doesn't mention: * Additional overhead of mustash * Any performance comparison between StreamTable and DataTable Still good to publish and relatively well written :) 
This is ALWAYS good practice when bringing in external code :)
I'm not sure what you mean by "reality being tested here", but I'll try to break it down. Lines 7 - 11 you make deferreds (logical) Lines 15-19 you make functions to return the deferreds promises (logical) Line 22-28 you chain `def2.promise` with 6 new promises, each returning the previously generated deferred's promises; you never get past `getDef3` because that deferred promise is left in a pending state, blocking the next 6. Line 30-36 you manually resolve each promise, ignoring the chain, and you probably never see the error bubble because it's handled inside the promise logic. You're doing it wrong. demo here: https://gist.github.com/avoidwork/5488297 
Yes, but you throw wrong statements. You should rather say: "how Q or When perform poorly", and not "how Promises perform poorly", as that's not the case with well designed implementation. 
and how you expect someone to work on PR for such messy project? Clean it up, and I'll be more than happy to add Deferred :)
That's great in a small web page but some of us are building large single page apps with hundreds of classes, not using constructors, new and sometimes inheritance is simply not a choice.
jQuery IS javascript. Why would you need jQuery to do this?
From the http://jsonblob.com/about page: "Credit for the JavaScript editor and formatter goes to JSON Editor Online"
This is how I do subclassing: function Constructor(param1, param2){ this.method = function(){ //do something } init:{ extend(this, new SuperConstructor(param2)); } } function SuperConstructor(param2){ } SuperConstructor.prototype.method2 = function(){ //do something } The only special thing needed for this to work is an extend method, but that is easy to write or include from a library
Ahhh, okay. Thanks for the info.
There are so many flows to handle promises. You can actually see @bcavalier's implementation [right under the actual test](https://github.com/thanpolas/perf-promises/blob/master/lib/app.js#L45:L79). What i am testing for is simple and explained in the article. There is nothing wrong with that, it is just what i want to test. Feel free to create your own tests.
i think you are exaggerating a bit. As long as your package supports the `.defer()` method it's pretty easy to add and test. If it has it's own way of providing a Deferred Object / and or handling promises then you'd have to dig a little deeper to allow for configurability of the test case. 
Many answers repeated multiple times http://i.imgur.com/aOcMgEc.png But rad idea, otherwise!
A new-ish version of the Boiler.js site has been launched that is more simplistic and mobile viewable for those of you having trouble viewing it before.
RSVP.js does support `.defer()` actually. Their docs are just out of date. (yay...)
`__defineSetter__` is deprecated, ES5 is the standard: Object.defineProperty(window, '$', { set: function(val) { debugger; }, configurable: true });
Yes jsBind is quite similar to Knockout.js and essentially does the same thing - only better (hopefully). I found myself having problems with Knockout.js which I wanted to solve with jsBind. As far as I can tell Knockout.js works by recording what observable objects where called when a binding is first evaluated and then re-running the entire recording when a bound value changes. This can lead to a lot of processing that has no effect because it does not split things down to the smallest possible change. This can have a big effect on performance when doing something like inserting an item in to a collection - I have seen Knockout.js rebuild the entire list in the DOM. jsBind always makes the smallest possible change when an observable object updates. It avoids re-building entire lists in DOM and instead only inserts and deletes DOM nodes as neccessary etc. As a second benefit jsBind also automatically handles things like CSS prefixes which makes it easier to bind styles. I also wanted to have a go at improving the binding syntax itself making it more consistent as well as more flexible. Some more info about how jsBind works is on the [jsBind Blog](http://jsbind.blogspot.co.uk/)
(?:...) will match without storing the match in RegExp.$[1..9] or add it to arguments of a replace-callback. ()+ will match multiple times; ^|\n will match the start of the string or the new line, while [*#].* will match every string that starts with * or #. string.replace(regexp, callback function) is a very powerful pattern.
also recursion stops is through the replacer function, which will only get to run and replace if there is a match for the regex.
"transpiler"? If it takes an input language and produces an output language, it's a compiler. Don't invent useless new terminology (something the entire javascript community seems to delight in). Oh, and get off of my lawn, you dang kids. 
Hey - I took a look at the listElement function. Basically, it is what finds all of the ordered or unordered lists in the text - the lines that start with "*" or "#". [Here is a screen shot showing it do just that](http://i.imgur.com/yn1U4OE.png): After it gets all of the matches, it passes them off to replacer, which will turn it into a ul or ol, depending on whether or not "*" or "#" was used. The regular expression, for demonstration purposes at least, could be simplified to /[^|\n]?[\*#].*/g Which basically says, "Match any new line that starts with a * or #, and grab everything after that." I didn't have time to look at anything else. Good luck!
The recursion stops when str doesn't match the regex anymore, which means all lines starting with "*" or "#" have been processed into markup. replacer() is only called by str.replace() if str matches. 
Need to remove the , after Ice1. Otherwise, the editor is expecting another item for the array. Also, remove the ; at the end, it is not needed here.
Yea, was looking at it again while brushing my teeth and saw that ... also, not sure why I'd have "spellnames" in "Spells" ... I'm getting the idea I was drinking when I wrote this. Thanks
I would argue that shared state is *anti-modular*. It increases coupling and makes composition of parts brittle. jQuery, for example, is customarily a shared object between JavaScript modules (closures). They mutually add plugins and things to it. Now you have coupling between every module that interacts with jQuery. If you want to use AMD asynchronous module loading, or if two modules disagree about the semantics of some property on jQuery, you are screwed. In connect/express, the primary unit of 'module' is middleware. Each middleware mutates the the request and response object. Now ordering of middleware is very important and you have no guarantees about the behavior of so many compositions of middleware unless you look at their source code. I would argue that if you want a unit of modularity, it should have a well defined interface whos semantics are not altered by the context it is used in or what other modular peices it is composed with.
Looks like Reddit formatting screwed up your post a little bit. You can wrap stuff like `^` and `+` with backticks to prevent that: `(?: )` - is a non-capturing group `(?:^|\n)` - for start of a line or string (\m will be a line break, but looks like we're getting things after it `[*#]` - an astericks or hash `.*` - ANYTHING! `+` - one or more
Not sure if [this helps](http://i.imgur.com/cWhHp9n.jpg), but you can get visualizations of your regular expressions from [regexper.com](http://www.regexper.com/).
On a side note, why use the key names 'CharName', 'CharRace', and 'CharClass'? Why not go for the shorter, easier to read 'Name', 'Race', and 'Class' so that when you are getting values it reads like Character.Name instead of Character.CharName? Just a thought
No I never had. Is it just a JavaScript meta language?
A trick to know when your collision (or something that happens very quickly) is going wrong is to slow down all objects in your game and watch them interacting (eg. pos = pos + speed/10). By doing so, my conclusion is that your collision detection is working correctly, the problem is that two bodies can not occupy the same space at the same time (from Newton or Pauli, can't remember), which means that the way your particle spawns is incorrect. When you add one of your 'particles' and it is created inside another, they both have the same idea of going 'backwards' to avoid this collision, but they can't just go the opposite way to free themselves from eachother because they aren't just 3 or 5 pixels colliding, they are inside each other, even if they go the other way they will still be colliding next time they detect, forcing them to go back and forth again and again and again. To fix that, you could just create a new block at the position you want, check if it's colliding with anything, if it is, delete it (or move it) and try somewhere else.
http://en.wikipedia.org/wiki/Source-to-source_compiler &gt;A source-to-source compiler, transcompiler, or transpiler is a type of compiler You get off my lawn. A good number of languages (see Dart, CoffeeScript, TypeScript, SASS, LESS) are transcompiled to other languages. It's not a new thing.
Did you even look up what the term meant before you called it useless? From SO: "A transpiler is a compiler that translates the source code of one high-level programming language to the source code of another high-level programming language." Nobody is arguing that it isn't a compiler.
A promise is a deferred that someone else will resolve. It's just a deferred without the resolve methods.
If you're sharing code between the client and server side it's nice to use just one deferred implementation.
the issue is that the function signature may change, in future, also to change the implementation of a function you would have to redo the functionality to allow an arbitrary order. Also there are now a whole lot of different ways to pass in arguments, so different members on a team might write the call different ways which makes code harder to read and now you have to test them all. There is also usually a reason the arguments are passed in a certain order, so that the code reads well and you can show the importance of some variables. Also it might make it harder to curry functions. And then you have to enforce order if you do have more than one argument with a specific type, so now you've got a mixture of functions and it may not be easy to remember which is which There are some cases where you want the same function to take in different arguments (just take a look at the jQuery API), but they aren't allowing arbitrary ordering even when the types change.
Has anyone coded in ES6 enough to give an opinion on it? I ask because I'm somewhat skeptical of ES6. Something like this is yet another indicator that ES6 is on a level field with Coffeescript and Dart, regardless of the nominally official nature of the current ES6 spec. 
It's its own language that requires a compilation process before it can run in your browser--but basically, the compilation is essentially just type-checking, so as far as I can tell the generated code is roughly the same size. By this I mean it _should_ be an extremely efficient means of adding type safety to javascript. I haven't actually used it myself so I can't provide firm evidence one way or another, but it seems to me that Typescript is one of the neater languages targeting javascript at the moment. 
As canvas dev becomes more and more prevalent, this is going to become more and more important. I wouldn't be surprised if canvas inconsistencies became just as troublesome as any other annoying cross-browser behavior. 
I don't disagree with any of what you said. I apologize for any ranty-ness I am displaying also. The point I keep trying to make (because people keep telling me this is bad modular behavior), is that I didn't write a module system. _I wrote this to **build** individual modules._ One of the build targets for my last project (a JsonML template engine) was a RequireJS module! Another target was a Node module. Another was a browser &lt;script&gt; that required jQuery but _not_ RequireJS (that one also wants to be minified). Another was a browser &lt;script&gt; that did _not_ require jQuery. And finally I have a test build target. &gt; ... if you want a unit of modularity, it should have a well defined interface... I am _building_ a **single** unit of modularity. I was not breaking my module up into separate files for more modularity. The encapsulation is of the _whole_ module, which happens to be made up of several source files purely for readability and organization. I could have achieved a similar (but not as good) effect by moving source around in a single file and putting in big separating comments /* ----- HERE LIES YE OLDE FILE LOADING ABSTRACTION ----- */, but instead I decided to create several files, without need or want for those file boundaries to become boundaries of encapsulation. I think the name is causing some confusion. I named it "encapsulate" because it is a way of defining _arbitrary_ encapsulation. To intentionally break the JavaScript obsession with "source file as module", or even "class as module". As such, the "local" state object I advocated is no worse, or even really different, from a single variable defined at the top of a single file which contains the entirety of an encapsulated module.
Yeah. At first glance it definitely seems like it's worth investigating further.
I've been meaning to checkout Thorax for a while. Seems like a really clean and minimal framework built on top of Backbone. Great for simplifying view management.
I mean a mixture of functions that can take arguments in any order and those that can't. Testing is still a problem, because you'll want to test the functions that take the arguments in any order. So if you've got a function which takes 3 arguments in any order you now have 6 different tests for: fn(a,b,c);fn(a,c,b);fn(b,a,c);fn(b,c,a);fn(c,a,b);fn(c,b,a); what if the function signature changes to one that isn't compatible? say it took in different types, like a string and a number, but then it changes to take in another optional string and suddenly order is important - you then have to go through and change all the written functions. I'm not convinced that differing arguments by type would work too well, especially when you want to pass in an undefined, null or NaN.
&gt; I think why I like AMD / one module per file is that it encourages code re-use Thanks. That actually helped me solidify something I was trying to express. And that is this: I don't want to be forced to keep my source in a single file to avoid implying that part of it was designed for reuse. When I write a "module", it must have a well defined API, documentation, maintain encapsulation, validate inputs, etc so forth. I do not always want to go there simply to justify having a separate file. There are lots of ways to achieve what I use separate _non-modular_ files for. Code folding is one way. Also separation comments, closures, and/or classes contained within a single file. Having separate files means separate tabs in an IDE. It also means I can isolate environmentally specific code. I am all for reuse. This actually _helps_ me reuse code in more environments. Thanks again. You've brought up some great points and resources.
This is the kind of thing that makes me miss the heyday of Flash. It had its faults, but it was much better at pushing pixels than canvas.
This is the first I've run into it, and I'm pretty impressed, particularly with its &lt;form/&gt; handling. I wish it didn't provide a monolithic full stack, I'd like to include some part of its library instead of cloning its seed project like it recommends... 
Wonderful Quiz. Thanks!
I'm curious if anybody would use something like this and what for.
I agree that it's definitely messy to have properties on both model and view. But I've been turning around whether Form attributes should be held on the View or on a Model, and I'm becoming firmly convinced that they should be only on the View until saved into a new Model. Here's why: suppose the Form has month, day, and year attributes. But the API endpoint requires a UTC timestamp--that is to say, when the Model is saved its attribute is a timestamp, not the month, day, and year. In other words there's a process for converting from View data to Model data. So the View/Form has a different "truth" than the Model--but the Model doesn't really 'exist' until it is saved/submitted: that is, when the View converts its attributes into the Model's attributes. Now I haven't looked too closely at Thorax's form (other than running through it's serialize function), but if it's mixing model and view properties like that it sounds problematic. 
what were the faults of flash, me not understand :( Also, flash powers youtube, how weak can it be?
Yeah, I hate to say it but Flash is still much more powerful. In fact, if Flash were to be introduced now, I think we would call it a revolution. Still, I understand the idea of transitioning to open standards. But I'm afraid "standards" is still a bit of a myth in some ways. I'm sure we'll see more consistency and power in the future.
The two real 'faults' of Flash were that 1) There were, and still are, a lot of crappy, annoying things written in Flash. 2) Steve Jobs said publicly that he didn't like it. This gave steam to a backlash that the was already brewing because of number 1, and the fashionability of Flash dropped almost overnight. A shame, really. It'll be 4 years at least before mainstream browser technology is at the point Flash was at 3 years ago.
Not exactly. The only real 'fault' with Flash is that it's proprietary. If you want to support it in your platform, you need Adobe's help and permission. This is not free. As more apps were written in Flash, this became a bigger and bigger problem for companies that wanted to attract developers to their own platforms, or at least attract developers and not have to give up profit margin to Adobe. Jobs's refusal to include Flash on iOS broke this cycle. The success of iOS and resulting large potential customer base that *could not* run Flash forced developers to do something else to reach this audience: use iOS or HTML. And once you're not using Flash on one platform, it's easy to also not use it on other platforms. There are a lot of crappy annoying things written for any platform -- once the customer base is there, the crappy annoying software follows.
I don't think anyone calls Silverlight or Unity a revolution. They are good but open standards definitely have their advantages. Bonafidebob said it pretty well http://www.reddit.com/r/javascript/comments/1dfpex/your_particles_are_leaving_behind_ugly_gray/c9q1l0j
I like the idea. Trying to think if I have a good use for it, but I will definitely star it on GitHub for future reference. Thanks for sharing.
Right way to dom build is to use something like a handlebars template and jQuerify it. e.g. foo.handlebars &lt;label&gt;foo &lt;input type="checkbox"/&gt;&lt;/label&gt; fooView.js var $el = $(Handlebars.templates['templates/foo']()); $el is now a jQuery el that can be inserted into the dom.
Very important to be standards-compliant when you're running a one-off script in the console to gather information! :)
It's also been riddled with bugs and security problems in recent years, to the point where the liability:benefit ratio has grown too large for many people.
You're right, except that now a new set of clusterfucks will now rise to meet the standard of clusterfuckery that was flash.
Okay, I added some of the code to the post
Yesterday when I tried to run your suite using command provided in README, it run just one test, as others for some reason [are commented out](https://github.com/thanpolas/perf-promises/blob/master/promises.perf.js#L182-L214) When I uncommented those lines, suite crashed with "TypeError: Cannot read property '2' of undefined". It's hard to "add" to project which doesn't work on first run, also mind that `lib.defer()` isn't any standard, and you can't rely on it blindly. In deferred unresolved promise is created via `deferred()`
When a collision occurs you need to do two things... 1. Move the objects apart so they are no longer intersecting with each other. 2. Apply your desired change in velocities It looks like you're missing the first step so the two items are detecting an intersection and then constantly bouncing back and forth as the bounce isn't enough to separate them. With the lessened velocities on each bounce, they are *never* able to get away from each other. You need to separate the two objects to resolve the collision before you can apply the collision response. Hope that helps. 
Fingers crossed for MIDI I/O support!
I made some changes to what you said and it works... better, well they don't get stuck anymore, still looks a little weird but it works at least http://passion4web.co.uk/ben/collision1_2.html
No linux support :-(
In Flash, fading is best done not by painting a low alpha black rectangle over the entire screen, but rather applying a colorTransform that applies affine transformations to color values. It works really well, and it's fast. You can fade to black or white if you set it up correctly. And with a ColorMatrixFilter or a BlurFilter, you can do more interesting effects. But still, I understand the points made about why Flash should ultimately become a thing of the past. The reason I made the blog post was simply because I've been noticing quite a lot of people making animations that fade to black using this technique of painting a transparent rectangle, and I think maybe they are only testing their animations in a browser on their Mac and unaware that in other browsers things are not quite the same.
Yeah, that's what I meant. Flash can do so much more, and do it faster and more consistently. I understand all the points made about why Flash ultimately has to become a thing of the past, though. I sure opened up a can of worms here! Really I just wanted to point out a browser inconsistency that people seem unaware of. If you're going to use HTML5 for what people used to use Flash for, then you have to be careful and find some workarounds.
An other solution that is not being mentioned by the author is to cover up particle trails with more particles.
Here is something that I wrote a while back to help with that problem: https://npmjs.org/package/nullet It isn't complete, but it works for a lot of different cases. 
I'm 33 ... I know a little bit about a LOT of languages and can still pretty easily keep up with new technologies as they emerge (I actually find a lot of "new" ideas are really just repackaged paradigms and patterns that I already know, but I digress.) The real problem I have is being able to sit at a computer for longer than 4 or 5 hours at a time. If you are young and work with a computer a lot, I cannot stress the importance of ergonomics and stretching breaks. Please take care of yourself.
Very good point. This bit of json isn't being parsed or manipulated by anything yet and was the very, very beginning of a small mmo (pretty obvious, I know) that has been brewing in the back of my mind forever. A pet project, if you will, and the only bit of json I had lying around to try this checker out.
The other thing to consider is that you're best determining your collisions predictively rather that retro-actively. What I mean is look at where the velocity is *going to* position it, not where it currently is. You also want to do those checks on each axis separately rather than at the same time. I suspect you're doing all your movement and then trying to fix the collision. You actually want to step in before the collision occurs. So in my engine I do the following, each frame, before drawing... 1. Update all objects so they are setting their velocity based upon states and suchlike. Engines apply thrust, players jump etc etc. 2. Check all entities for collisions looking at where those velocity values will place them. 3. When any impending collision is detected, do the appropriate resolution (adjusting their position and velocity accordingly). 4. When all objects have been updated and checked for collision, *and only when*, move their positions based upon their velocity values. It's one of those things that you can get mostly working very simply. But to make it really solid takes a lot more extra steps and careful decomposition of the interactions. But I hope that's still of some use to you.
I really wish coffee script's existential operator was being included in ES6, so that you could (someday) write: tech = os?.install?.gentoo?.harhar[0]?.text Not sure if that'd work with the harhar[0] part, but you get the idea. In any event, I think something like nullet is your best bet. Also, if you're wondering how to handle harhar[0] with nullet, you can do: nullet.get(os, 'install.gentoo.harhar.0.text') and it will work.
Alright, thanks for the advice. As I haven't been doing this long, thinking ahead never really crossed my mind
Did you forget the part where Steve Jobs went on stage and said web apps were the future of iPhone 3rd party dev, a year before the app store? Do you think he was lying through his teeth, or do you suppose maybe webkit wasn't able to deliver on the imagined promises?
we're leaving MVC behind?
Just use the try/catch. Nullet will be many times slower than this: var foo = "default"; try { foo = complex.object.with.an[array].too; } catch (err) { } Edit: I'm wrong. The nullet method is better. See hillmanov's reply below. Edit2: I'm not wrong... An invalid path using try/catch is 25% slower than nullet method. BUT, a good path using try/catch is 50x FASTER than nullet method. See itsnotlupus below. Assuming you aren't expecting a miss the vast majority of the time. Try/Catch is the way to go
From the github repo - An open source code editor for the web, written in JavaScript, HTML and CSS. No installer support but should be able to get it to work as all of the above is platform independant.
 exist = function(obj, namespace, backup) { if (obj == null) return backup; var names = namespace.split('.').reverse(); while(names.length &amp;&amp; (obj = obj[names.pop()]) != null); return(obj == null ? backup : obj); }
A volume control would be cool (I didn't see one), also a toggle for sharps vs flats would be good (C# is always C# but A# is normally B flat). Cool deal though, I can see a lot of potential in something like this
I'm okay with if statements in template languages, the real source of complexity tends to be if statements with complex expressions, and the other source of complexity tends to be being able to pull data out of potentially complex APIs directly from the template. Just a plain if that depends on a true/false value is not a big deal. Push templates where data is only pushed into the template, in combination with no complex expressions, have some advantages: * separation of logic from template; no calling APIs or doing complex calculations in the template * templates become easier to debug as you know absolutely what goes in. In addition, if something goes wrong the person who is tweaking the templates (who may not be the original application programmer) doesn't see weird tracebacks from deep in the stack. * templates become easier to test in isolation * templates can more easily be security sandboxed * potentially better performance This all suits client-side templating very well. Obviel Template is an example that I implemented of a push-only, expression-free, client-side template language: http://www.obviel.org/en/1.0b6/template.html It directly embeds support for OOP patterns through data-render, letting you delegated rendering of sub-objects or objects in an array. It's not 'if' free but 'if' is only useful for very limited cases. 
[moving to github...](https://github.com/thanpolas/perf-promises/issues/1)
I don't mean this critique specifically against you or your writing, but because I've heard this argument before and I still don't buy it, I feel that I should say something. Maybe we both will learn something from what seems like a very big divide of viewpoints. &gt;In most cases you can compute a value and pass it in to the template. This works for things like adding on a class depending on a value. So now (if you're using MVC), your controller is responsible for determining what classes to pass into your view? BARF! Now you've just completely destroyed the separation of concerns between your C and your V by munging your V into your M. You've really just passed the buck to someplace that is less suited to handle it. The last paragraph decides that it's time to sprinkle in some talk about OOP and once again talk about the evils of the IF statement. OOP will save us by having us use sub-views! &gt; The advantages though are great, you've removed complexity from the templates and what's more you've also broken out the templates in to more logical pieces. You just moved all the complexity to the parent view. The reality of it is that using some _view-specific_ logic in your templates can really help keep things cleaner and maintain a cleaner separation between the V and C. The logic you DON'T want to include is business/application logic.
"Maybe" = "I have not tested, but my experience tells me that..." I agree; just wanted to spell it out.
Pauli Exclusion Principle
What?
You can also test and assign using the following pattern: var prop = "default"; base &amp;&amp; base.one &amp;&amp; base.one.two &amp;&amp; (prop = base.one.two); The caveat being that if base.one.two === false, it won't do the assignment. If the final property is a boolean: var prop = true; base &amp;&amp; base.one &amp;&amp; (typeof base.one.two !== "undefined") &amp;&amp; (prop = base.one.two); Edit: The second example didn't make sense as I didn't hash it out to the property.
This is great! Much faster and simpler than nullet. I think I will update nullet with this approach. 
In chrome the Try/Catch method is **25%** slower than the nullet method. And the *nullet method is more readable* in my opinion. Edit: Actually, try/catch is 50x faster for good paths. Nullet is only 25% faster for bad paths.
Check this out: http://jsperf.com/deep-attributes I'm comparing 3 approaches: ((member||{}).member||{})... vs try/catch vs a little utility method that loops over members: function deep_get(obj, path, fallback) { var fields = path.split("."); while (obj &amp;&amp; fields.length) { obj = obj[fields.shift()]; } return obj === undefined ? fallback: obj; } The results differ greatly in a fully resolved flow vs a fully failed flow. In the fully resolved case, try/catch is actually the fastest way to do it. Since it's arguably the most readable as well, it's a no brainer if you expect things to almost always resolve. The fully failed case is the exact opposite. try/catch becomes far slower than any other alternative, and should be avoided if you expect to bump into that scenario often. The (member||{}) approach is the more consistent approach, a bit slower than try/catch on success, but faster than other options on failure. The deep_get method, which I was expecting to be dead last, what with its array split and loop, performs very poorly, yet still beats the try/catch approach by an order of magnitude on failures. If there's one take-away from this, it's that try/catch is only meant for exceptional failure, not as a substitute for ordinary checks. 
you haven't moved the complexity to your parent view - at least not if you're using dependency injection. Yes you're putting in complexity *somewhere* but it's more logical to have that complexity in a place that deals with the data rather than on the display. After all an if statement in the template means you're changing the display based on data you have so the decision for what view to show should be based on the data. Also you'll see that I'm not for putting class information in the control, rather I pointed at rivets as a way to declare how a class should be placed. The problem is most templates will render a string, so that after rendering if the statement is false the class will not be put in and you have no way of knowing what that class is or if it should be put on unless you re-render the whole template. However if you have it declared using a DSL... so I guess I'm arguing more for binding with simple cases such as putting on a class, but larger cases where you actually change the dom structure should be handled by subviews.
Using a try/catch doesn't slow things down (much.) Having an exception handled from within a try/catch slows things down to a crawl. If you split failing gets from successful gets in your benchmark, you'll see a stark contrast. ( see http://jsperf.com/deep-attributes ) *edit: Seeing some numbers for IE and Opera forces me to revisit. try/catch statement are expensive on those browsers, even without exceptions being thrown. That's sad. It's all relative though, and even on those browsers, a try/catch statement around code that doesn't throw is still meaningfully faster than split/looping over fields.
Looks like it is possible to build it yourself, but not terribly easy: https://github.com/adobe/brackets/wiki/Linux-Version
&gt; but larger cases where you actually change the dom structure should be handled by subviews. okaaayyyy, so you've replaced the electric chair with lethal injection just to get away from if()s. That is not really an improvement is it? Actually it sounds worse. 
We need help with Linux. The core Brackets team came from Adobe and it's probably not surprising to hear that they don't have much/any Linux expertise. We're hoping someone will step up and help out. We think it's less than a month's worth of work to get everything ported and to setup a build process that can be easily maintained moving forward. If anyone has the skills and wants to help out, you can find us on IRC freenode/#brackets or our mailing list: https://groups.google.com/forum/?fromgroups#!forum/brackets-dev 
Companies using YUI: NFL.com, PayPal, Apple, SmugMug, Wells Fargo, Canonical, Zillow, Yahoo, and more. http://www.youtube.com/watch?v=F-krPD9fRsk
Asynchronous resolution is a part of the spec, so when has to conform to that (see below tho). We use a user-space trampoline that is able to conflate multiple "ticks" (or potential ticks). It allows handlers to run within the same tick as each other in a safe way, but still prevent hazards that occur when a handler is allowed to execute synchronously in the same execution context as the then() call that registered it. There's currently some discussion in Promises/A+ about what the actual safety invariant is that we need to provide. IOW, what does "Asynchronous resolutions" actually mean? Hopefully that will lead to more potential for optimizations.
sure is, views should already have a lifecycle applied to them that gets handled automatically, whereas DOM that is placed in or out of the page would need any setup run explicitly. It's not just to get away from if()s, it's to get away from the problems that the if()s can create. ask the inmates which they'd prefer (those that don't like pain)
I completely agree with your comments. These concerns are exactly why we created Typecast.js http://www.typecastjs.org/ Typescript and CoffeeScript both provide strong typing, but they are both custom compilers. There needs to be a way to do this in javascript. Hit us up on Gitbuh if you have any interest in merging libraries and/or continuing work on this area of JS. I really think this problem has to be solved before other problems in the JS world can be.
Oh, when you said "old", I thought you actually meant advanced in age. 33 is young bro (31 here.) Totally agree with the ergonomics comment though. 
Anyone read this? No sample chapter, no indication of what version it was written for, no page count.
what makes this the "right" way?
Yes! As far as I understand, [this](http://webaudio.github.io/web-midi-api/) hasn't been implemented yet.
I thought backbone was meant to be more of an "ease in" to the MVC way of coding rather than trying to get away from it
I seem to remember one of the Google Web Audio API people using some kind of plugin to get MIDI into the browser. I forget the guy's name, but there's a video somewhere on Youtube..
Backbone is not MVC in any fashion whatsoever. 
Knockout and Angular style frameworks tend to talk about ViewModels or Model-View-Whatever, and if you code Controllers it's not through the framework, nor does the framework particularly encourage it. Backbone does not have a Controller, and control flow is managed through event channels. Its Model and View objects aren't really related to the M and V in MVC; Backbone.Views are the set of objects mediating DOM and app state, and Backbone.Models are the set of objects mediating API and app state. 
Thanks! There's currently no volume control, but that shouldn't be difficult. At first, I was going to use some sharp/flat toggle, but I thought this interface would be a little cleaner. Although A# is normally Bb, is it technically incorrect? I understand why that note is sometimes A# or Bb in the context of particular keys, but I'm not well-versed in music theory, so any explanation would be helpful.
Dope. I'll look into it, probably later tonight, and see what's possible.
It is available at http://www.amazon.com/gp/product/B00CJLFF8K with sample chapter. It's around 100 pages. It covers AngularJS thru 1.0.5 with mention of some unstable features in the 1.1.x branch.
I'd recommend against try/catch for this, because "catch" can accidentally sweep a lot of important errors under the rug. For example in this code: try { tech = os.install.gentoo.harhar[0].text; } catch (err) { tech = "whatever some text"; } Suppose some other programmer later changes **os.install** into a complex [getter function](https://developer.mozilla.org/en-US/docs/JavaScript/Reference/Global_Objects/Object/defineProperty#Description), but they've implemented it badly so it throws a RangeError because of an infinite recursive loop. Your code, as written, will catch that error and continue as if os.install was simply undefined. That's bad: you're hiding important debugging information. 
It would be nice to see a comparison of this in terms of performance on JSPerf. In terms of function params, I've been leaning towards using config objects for a single param in any method. Then I use underscore to validate types as needed within the function prior to usage. I have gotten tired of re-ordering and names in method calls. A param-config obj solves that for me consistently in most cases.
ie what? works fine in ie10. edit: ie9 throws an error that your functions are undefined. this may not be a bug in ie, per se, but because you are running it in an iframe on a site. put it in a stand alone page and see if that works.
Exactly. I'm not sure why painting over the whole canvas with a black rectangle would be the preferred solution here, as opposed to redrawing the trails with adjusted intensity values. I would bet it wouldn't be faster to use the "cover everything with low-alpha black" solution until you had an awful lot of particles, and this trick doesn't work at all if you've got anything else drawn into the scene other than particles and the background. 
You don't need to use separate functions for run1, run2 and run3. Just maintain a single variable that keeps track of the current state and check that the state matches the number of the div the user wants to move. Here's a revised code for you. For this to work, add data-block="1" (and ="2" and ="3") attribute to each of the divs, respectively and remove the onclick attributes. The event delegation takes care of that for you. i.e. var state = 1; function run(evt){ // Cast the attribute value to a number and check it matches the state. if(state === +evt.target.dataset.block){ $(evt.target).animate({left:'+=100px'},400); $('#james').animate({top:'+=53px'},800); state++; } if (state === 4) { $('.winner').css("display","block"); } } $(document).ready(function() { $("#wrap").delegate(".block", "click", run); })
You need semicolons at the end of your functions because IE is dumb. You also don't need to define 'var james = true'. Also if you want it to be better, follow /u/lachlanhunt's solution.
$(evt.target).data('block') would be slightly less efficient than the native evt.target.dataset.block supported by the browser. First, $(evt.target) is evaluated, which has to instantiate a new JQuery wrapper object around the element and return it before it invokes the data() function that then, more or less, iteratively searches the element for data-* attributes that match. The advantage, however, is that the JQuery method should work for browsers that don't implement .dataset natively yet.
You've missed *my* point: this isn't a very good technique to fade to black because rounding error is going to mess it up one way or the other. That it appears to work in some browsers and not others is also an issue, but it doesn't mean the technique is a valid one.
How is it victimless? Not buying something is not illegal but copyright infringement is even if you wish that weren't true and rationalize about doing it. It may not be stealing in the sense that you aren't denying someone else possession but in the sense that you are taking something that does not belong to you it is unless you believe all digitally encoded works are yours for some reason.
The problem is HIS code doesn't work in IE.
Learn to use Vagrant for developing modern web applications on Windows. Use it to create a Single Page Application using Chaplin and Backbone. [Developing modern web applications on Windows with Vagrant](http://www.habdas.org/developing-modern-web-applications-on-windows-vagrant/)
Yeah...fading by painting a transparent black rectangle is not guaranteed to work because of rounding, but if you see it working in one browser because of flooring (instead of nearest integer rounding), then it would be nice if you could assume it would work the same in every browser. But the reality is that you can't make such assumptions, and consistency across browsers to that minute level of detail is unlikely to arrive any time soon. Another little browser difference I noticed some time ago: when copying images from a sprite sheet, I believe some browsers (like Chrome) would copy to the destination canvas by snapping to the nearest pixel, whereas other browsers (such as IE) would smoothly interpolate, producing a better effect (but probably more CPU intensive). Also, antialiased lines appear different in Chrome versus IE and other browsers. But again, I understand the whole idea of moving to open standards, and away from proprietary plugins. But "standards" seem to be a bit of a myth. At least the way it is now, there seems to be different interpretations of certain standards. So it can be a little messy. But I guess the key is to keep testing in multiple platforms and browsers, and making adjustments.
This is very exciting indeed. The 1.0 version of the library has already been incorporated into the Chaplin skeletons. Here's a short tutorial to help Windows users set-up Chaplin on Windows using Vagrant: [Developing modern web applications on Windows with Vagrant](http://www.habdas.org/developing-modern-web-applications-on-windows-vagrant/)
Slightly less efficient, yes. Significantly less efficient, no.
I'm not arguing that browsers shouldn't be consistent, I was pointing out that it was a questionable way of fading to black. When making an argument that browsers need to be consistent, it would be nice to have a better example of browser inconsistency than this, which is in my opinion a poor example for the aforementioned reasons.
Nice writeup. Rivets + MVVM is pretty great.
Make sure you let us know how it goes! :)
I use it a lot for for bad APIs that just give giant json files of information. I copy and paste it in this bad boy, and then it clearly shows the layout of the JSON.
What?
I know that, but other people already addressed that issue, and I don't see what's wrong with offering general advise to improve the code further.
Someone wrote a little library for this and posted it a while back: https://github.com/jclem/steeltoe
 $(evt.target).data('block') may be slower, but for the vast majority of use cases, it won't come close to affecting perceived performance.
The real problem I have with these kinds of "hard and fast" rules is that breaking the rule is often damn useful. Being able to use if()s and logic to render data in a particular way is often very useful and much simpler than having to spread view and design related code elsewhere in the code. I'm not advocating anarchy or poor design or unmaintainable code. I just want to say that there is powerful and pragmatic middle ground between the two extremes ("no if()s" vs "complexity in the templates") which often gets forgotten. Yes, people will need to use good taste and their heads about what to do in a template and what code should be split out and put elsewhere. Right now I've got some JSP code open and the author had to write it under a no-scriptlets rule (i.e. no embedded Java fragments) and to use the awful JSP XML based tag stuff. They have found a way to use a string to keep track of a (mathematical) set of IDs and to test for membership using string indexOf(). It is insane and stupid. 
Looks cool, wish I know where to start with testing JS. Problem is I maintain a legacy site, procedural code everywhere.
I think you are confused with the concept of prototype and closure I think you jumped the gun right now. Your intention is good to be organized about the functions. But just from reading the phrasing of your question I think you need to learn more and UNDERSTAND the meaning of them first Read eloquent JavaScript. 
I believe what you referred to as a closure is called the module pattern and for something like this would probably make the most sense, but I'd call Window exports inside the module so you can use it with commonjs/nods
That one I do intend to read. If one wouldn't be confused, what implementation would you suggest and why? The context will help. 
&gt; but I'd call Window exports inside the module Can you be a little more specific? .. and thanks, I thought it was more of a nomenclature misunderstanding on my part. 
First line: (function(exports) { Then last line is still })(window); Then you'd have to change a line in there somewhere, I'm on my phone so I can't find it but a guy named Tom MacWright did an article on this you can probably find via Google 
Alright, I think I know what you mean. Will take a look around google for that article. Thanks for the help!
Weirdo. Upvote.
Closure is the concept of scoping. Prototype, the best way I can explain it is *extending*. 
For a utility library you're mostly working with functions (not methods), so you'd want to use the module pattern that you showed above. If you used a prototype, need to be working with an instance: function Utils () {}; Utils.prototype = { foo: function () { console.log("hello world"); } } var utils = new Utils(); utils.foo(); // as opposed to var Utils = { foo: function () { //... } }; 
When I hear about programmings attempting OOP in JavaScript I ask why? The whole point of OOP is persistent reusable memory. This is the point of classes and it is the point of prototypes. In many compiled languages this is very important because their execution is persistent. In JavaScript this is pointless. 99.99% of the time JavaScript applications are not persistent. Persistent means you launch the application and the application does not immediately close. Nearly all JavaScript applications execute and then immediately close. Furthermore, in JavaScript you have absolutely no control over memory management and garbage collection is automatic. This means memory management in JavaScript is not available and is not necessary, which then means the benefits OOP are irrelevant.
Anything inside the try clause is absurdly slow in V8, like ie6 slow. So I try to avoid it at all costs unless I am 100% sure it's not going to run in chrome (like inside an ie shim) 
You should look into [require](http://requirejs.org/). It's purpose it to facilitate modular design, which is what you're (slowly but surely) coalescing on. It uses the module pattern which other posters have suggested, but formalizes it and brings more machinery of modular design into the picture, like being able to set up other modules as requirements (hence its name) before the construction of your own module.
When ever I hear a question about patterns all I can think about is an invitation is a tremendous amount of ass pain. Forget patterns. The problem here is that you do not understand the concepts of closure or prototypes. Learn what those are and your question will answer itself.
I made the changes that everyone suggested individually to see the actual root cause, and you nailed it. The same code copy and pasted to an external document works fine in IE9, must be the iframe on the code pen site. Thanks buddy
Use CoffeeScript but keep looking at the output of the CoffeeScript compiler. Research anything you don't understand. When you understand all CoffeeScript output, you can stop looking at it. At this point your brain will be programming in JavaScript but writing CoffeeScript. This is the correct state for your brain to be in since CoffeeScript *is* JavaScript with some of the more egregious bullets removed from the gun pointing at your feet.
Use whatever serves you best. If you take the "coffeescript first" route, have a look at the resulting JS source now and then so you don't lose your JS skills by not using them.
Updated, code in the example in the spec perfed and more! 
Learn CoffeScript before JS unless you want to have a job where you need to write JavaScirpt.
so start by doing acceptance testing, something like casper or with this selenium. 
yup. just remember to isolate the issue as much as possible, and that includes any overhead these sharing sites add. and let us know what version of IE, as there are significant differences between. and lastly, IE does have dev tools and a console if you hit the f12 key.
Good to know, thanks.
Learn the language before you start using an abbreviated (or just modified) version of it. It still compiles down into vanilla, so you should understand the pitfalls, lest you fall victim to an entrapment of the actual language (unbeknownst to the ancillary version).
Would like to see a 'JS testing roundup', I'm needing to revisit my entire testing setup and there are so many more choices today then when it was originally built. I have no idea which are applicable for my purposes or which excel at what (and conversely, which falter at what). While I love the ecosystem explosion, it's led to such a large and diverse set of offerings that it's daunting to try and figure out which is the best fit for me. edit: the Github page for this project has a [feature comparison matrix](https://github.com/theintern/intern) for some of the common testing frameworks, if anyone else is in my boat.
thanks for the tip, hadn't realized that was a bad thing. Guess I'll have to start posting other links as well then
Good point about tuts/articles in vanilla.
And sub to /r/LearnJavaScript (I'm a mod).
Stick to vanilla. Not everyone uses coffeescript, like me. If you want to contribute to an OSS project, it'll probably be in js. If there's an odd bug in the output of generated CS, you need to understand vanilla to debug it. Stuff like that. 
As everyone else is saying, definitely read up on the language, JavaScript patterns, etc. to get a solid idea of what the difference between the two concepts are. It'll stop being an either-or at that point and what you need will flow naturally, just like how you use any other languages that you're fluent in. That's the best answer to the bigger picture question that you asked. For the specific problem that you are asking about, you're on the right track. I'd just extend your example out a bit more: (function(root){ // Cache local references to frequently-used globals here. // Local references are faster than global references. // They save typing, too. // Specific values here are just pulled out of the air var slice = Array.prototype.slice, exp = Math.exp, $ = jQuery; // You can also have utility functions that are available // only to the functions that you're exporting. var helperFunction = function(someArgs) { // Stuff happens here }; // Exporting functions as part of an object literal // makes it a bit easier to read than separate // assignments root.boogots = { util: { doFoo: function() {}, evenMoreFoo: function() {} }, foo: { lessFoo(); } }; })(window); Edit: Fixed reference to Math.exp. Wanted Math.exp, which is a pointer to the function, not Math.exp(), which is an invocation of that function.
The best way to learn is to read good (and bad) source code. If it helps you to read compiled coffeescript, then I don't see the harm in learning that way. However you do it, make sure you DO learn vanilla JS. You need to understand what's going on under the hood or your Coffee isn't going to be very good anyway.
Hang on boys, before you downvote etrnloptimist. Let's clarify. (sorry, my earlier responses ITT was via my phone) Modular design is a pattern/technique. Like OO. Shit, you don't need to write c# in OO, but then it makes a mess when if the business logic is complex. OO helps you do BUSINESS DOMAIN design. RequireJS is something else. RequireJS is a library, the basic feature is that it helps you grab JS files without using script tags. And then it knows how to load the script into the scope. (and dependencies etc etc) RequireJS recommends you to WRITE modular code. You don't have to, but they explain how the PHYSICAL folder structures of your files, and why the modular pattern is organic/matches/looks natural to how JS/HTML/CSS works. The repeating pattern I see in /r/javascript and /r/webdev is that people just want libraries that works. "hey do you know this this this". Heck, try to build it first. Or else you will never fully understand what Javascript IS, what it can or cannot do. And then your system is just going to be a bunch of plugin and libraries. 
It just takes a couple hours to be fluent in CoffeScript. If you already know JavaScript, you can learn it as an afternoon project and then decide whether to stick with it or not. It does have some idiosyncrasies when it comes to indenting and passing multiple parameters to nested functions, but other than that it is really simple and easy to understand.
&gt; var slice = Array.prototype.slice, &gt; exp = Math.prototype.exp(), &gt; $ = jQuery; Oh man, thanks for the hidden gem in there. These are also the types of things I am trying to pick up as I go! 
Awesome, will check it out.
Great point. If you use CS, the compiled JavaScript should look very normal, and not alien. You should understand why the for in [array] compiles the way it does, iife wrappers, etc...
Never CoffeeScript... so that leaves you with vanilla JavaScript.
I recommend you continue with vanilla JS but also do CoffeeScript. You should then study what the CoffeeScript compiler outputs. I think that someone of your experience level will benefit from CoffeeScript too. I don't use CoffeeScript for my own coding, I think it's setting out to solve some things which can be solved by making a few simple JavaScript functions, but it's main benefit as far as I see it is in making it so you don't need as many keystrokes to write the same things (like not repeating the word 'function' a lot), and when you are fluent with it you'll be able to read code quicker because of its relative terseness. I don't think 'just skipping to CoffeeScript' is the right idea, you should continue to build up your knowledge on vanilla JS, however learning CoffeeScript as well certainly makes sense. Regardless of how long it takes for you to master JS, you should make sure you know about closures and asynchronous programming (using callbacks). The two are very related because closures mean you have access to more data in the callbacks.
How do you come to that conclusion when the OP writes the following and then immediately lists code examples? &gt; My question is pretty straight forward, I see things like closure and prototype to structure code a bit better but ... what is the ideal pattern to use in my case? When all is said and done I just want to make calls... If the OP were aware of the issues at hand then it is logical to presume his code samples would be sufficiently different as his problem is invariably altered.
The module pattern is the correct answer. I'm sorry you think design patterns are a tremendous pain in the ass. But learning all about closures and prototypes, while great general knowledge, will *not* lead OP to the module pattern which is where he needs to be.
What about a factory factory module? http://discuss.joelonsoftware.com/default.asp?joel.3.219431.12
I will :)
You might want to take a look at [TodoMVC](http://todomvc.com/). Its a project to build the same application using a different MV* framework every time and compare them.
1. A design pattern is not a framework. 2. In Javascript especially, in order to write some serious code, you need to rely on design patterns. The constructor pattern. The module pattern. [Crockford](http://javascript.crockford.com/prototypal.html) himself advocates several useful patterns, and he wrote the language. 3. Don't conflate the existence of such things with the abuse of such things. Nobody likes [architect astronauts](http://www.joelonsoftware.com/articles/fog0000000018.html) either, but that doesn't negate the importance of architecture in software design.
No problem. Regarding imperative vs functional, the corresponding wikipedia articles are a good start. Intuitively, if your program tells the computer explicitly what to do it is more imperative. If you make heavy use of properties that are updated by multiple functions and write many for loops with an explicit iteration variable, you could say your programs are more imperative. E.g., var sum = 0 for(var i=0;i&lt;n;i++) { sum += a[i] } If the result of your program is more the result of composing multiple functions, with the state being passed around through their parameters or through closures, than your program could be described as more functional. E.g., var sum = a.reduce(function(x, y) { return x + y; }) 
&gt; A design pattern is not a framework. I have seen patterns called factories that try to do very frameworkish types of nonsense. &gt; In Javascript especially, in order to write some serious code, you need to rely on design patterns. Nope. JavaScript is a functional language with functions as first class objects that have lexical scope. This means all you need are functions which are nested by either declaration or reference. All that other crap, including constructors, is extra and unnecessary. &gt; Don't conflate the existence of such things with the abuse of such things. Cheney's Law: "Technicians willing to increase productivity by way of convention contrarily become sufficiently productive increasing conventions." Good architecture focuses on the needs of the problem and not the approach by which the problem is framed.
Crockford did not write the language. That distinction goes to Brendan Eich.
You're welcome. Take another quick look at that before using it, though. I goofed on my reference to `Math.exp`. In addition to getting slightly faster local references, these local variables that point to the built-in functions can be minified, since they're just local variables. Also note that `slice()` needs an array-like object to be `this` for it, so you would call it something like this: slice.call(someArray, 2, 5) For this particular function, you're better off just using it directly off of the array: someArray.slice(2, 5) The reason why a lot of libraries tend to cache a local reference to this function is because it's extremely useful to get an array copied off of something that is like an array, but not one: var args = slice.call(arguments); In this example, `arguments` is an indexed collection of the arguments to the current function. It is not an array! Calling slice on it will return an actual array that is a copy of `arguments`. That is available within closure scope to a function that the current function would return. See https://gist.github.com/mcordingley/4680147 for an example of that in action and a good example of the use of closures. Another good array-like thing to copy with `slice` is the result of `document.querySelectorAll()`.
I stand corrected. I went to a talk by him a while ago, and I could have sworn he told the story of how he was given just 10 days to design the language and that it had to look like java, etc. But I guess I got the "he" wrong.
[Some technical details about this are in a blog post by me.](http://blog.bitops.com/blog/2013/05/01/unreal-javascript/) Really happy now that this is public!
Yeah, I just added the API and JSON storage.
Dear god that is awesome
Per the FAQ: *Chrome currently crashes, but is expected to be resolved by the Chrome team soon.*
Amazing. I wonder if this is the future of gaming and whether having games in the browser would eliminate piracy?
He could have meant JSON, but he was probably referring to his revelation with JavaScript as a useful non-compiled language.
Are modules just a classical face for javascript? Are they necessary for complex programs? 
Wow, that just beach balled my computer. Not the tab, or the browser, but the entire computer. Had to reboot. Admittedly, it's a hurtin' old iMac, but javascript shouldn't be allowed to do that, should it? edit: It seems that it was Firefox's Web Developer extension, not JS or WebGL, that caused the trouble.
It's not the js that does it, it's webgl - GPUs aren't designed around untrusted content, and so you get badness - if you file a bug they can probably add your driver (if you're lucky) or gnu (if you're unlucky) to the blacklist of devices they don't want to let run webgl.
I would disagree with your interpretation of OOP. It's my understanding that it's a means of organizing your codebase. It's a way of grouping functionality in an application. You hide or expose functions and variables on each object, and this is the means for other objects to interact with it. It allows you to create an application using small, well defined building blocks, instead of a multitude of globally scoped functions that are scattered all over the place.
The future of gaming? If you mean its going to eclipse all other gaming platforms then no. If you mean that there might be hardware accelerated games running in browsers then, well, I would probably call that closer to the present of gaming rather than the future. Still, I don't really foresee many of the major high performance game developers jumping on the bandwagon. A web browser doesn't offer much in the way of bleeding edge technology. And the available development tools are currently a little ill equipped for that kind of development.
&gt;A web browser doesn't offer much in the way of bleeding edge technology. *Yet.*
That is very impressive. The guys behind the Unreal Engine have always delivered very solid work and I hope this particular release is the start of some exciting things to come for the web. Great work!
Why do you think "[object HTMLCollection]" is an error? Isn't it just the collection of matching elements that is returned? If you want people to look at your problem in some context, it would probably be helpful to make jsfiddle with an example.
&gt; Well for example if I have something like: &lt;div id = "cat"&gt; and I select div or id I want the answer to show up as cat not as "[object HTMLCollection]" So you iterate over the collection and look at the individual elements' "id" property.
Now why would your example output anything? You aren't doing anything useful with the result in the last line. What do you expect to happen here? If you tried this in your Javascript console, the last line would give you the matched elements back, but what you want is the attributes' values from your elements and not the htmlelement-objects themselves. Have a look at this: http://jsfiddle.net/JuTcJ/1/
.
.
It isn't a "reason" - WebGL exposes your GPU to the web, that's what it is meant to do. GPUs (and notoriously their drivers) aren't very good at dealing with slightly malformed content, let alone the completely untrusted content coming off the web. WebGL tries to mitigate these, but the reality is that it is trivial to hang* the GPU itself - only the most recent GPUs actually support shader preemption. For the most part on any consumer level GPU once a shader has start executing, the won't be able to do anything else until the shader finishes its current work set. The only thing that the OS is able to do is have a system watchdog timer that eventually performs a hard reset of the video card. Even that requires a GPU that can deal with it, but thankfully that's a bit less rare having been in shipping GPUs for a while. Unfortunately once the GPU has hung your display won't be updated, because its the GPU that is responsible for doing that. There are a bunch of actual security concerns due to the historically lousy quality of GPU drivers and the complete absence of hardware memory protection, but those are theoretical: I haven't seen anyone demonstrate something that exploits the driver. WebGL implementations have worked hard to try and ensure that everything that can be validated and sanitized has been before it heads to the driver. * Hang in the older GPU sense means "make the shader + work set take long enough to appear dead"
I'm sorry, but I think you may be better off learning some js and/or programming basics before you go on hacking about stuff like that. When you call the function "getAllElementsWithAttribute" with the argument "data-foo", the function will return a collection of elements that have the attribute you wanted. But calling the function like you did does not achieve anything, because the function gives you a result that you then throw away. What you need to do is assign the return value of the function to a variable, like so: var bla = getAllElementsWithAttribute('data-foo'); This introduces a new variable bla and puts the functions result in that variable. The result is a collection of HTML-elements that you can iterate over. The attribute you are looking for is contained a property on these elements.
.
Almost certainly not webgl. The shaders in this demo are very well behaved. More likely a browser bug of some sort. (Which browser and OS was this on?) 
OOP is certainly a model of architecture, but the primary benefits of memory reuse are of little or no benefit in this language. I find functional architectures to be more expressive and more simple to understand.
.
.
&gt; I am just trying to get the element(s) which correspond to a CSS selector. Your code (and the built in querySelectorAll) already does that in principle. The selector you have chosen for your second example is not appropriate for the function you chose, but it would work with querySelectorAll. You just have to keep in mind that "getting" the elements that matched a selector isn't all there is to it. You still have to do something with these elements, like extracting an attribute's value.
A module is a pattern of expressing a function in a particular way. I tend to favor functions over more OOP styles of organizing code. In my mind you are likely going to be using functions no matter what and it is the organizations of those functions into a hierarchy that defines the structure of a large JS application. It takes a substantial amount of extra work to accomplish the same depth of organization using OOP that is then less simple to read and which becomes an exercise in tracing references rather than identifying scope.
No, not webgl per se, but most likely a GPU driver bug. Apple has had notoriously bad GL drivers until very very recently which doesn't help.
.
So simple, so awesome! Thank you op. can you also do?: nodeObj.childNodes[0].create({ whatever: 456 });
It sounds like you aren't very familiar with how javascript interacts with the DOM (document object model). Here's a jsFiddle I made quickly to illustrate a simple action -- selecting an html element by its id, then moving it to another location on the page -- with some comments thrown in. [http://jsfiddle.net/N7bPj/](http://jsfiddle.net/N7bPj/)
&gt;skip to CoffeeScript You realize CoffeeScript and ECMAScript aren't sequential right? CoffeeScript just obscures the language and sacrifices control for the sake of silly things like not writing curly braces (which you don't have to write in JS either if you just absolutely love to write extremely terse code).
Doesn't seem complicated to me. myObject.prototype.getFoo = function(){ return this.foo }; what's wrong with this?
This was updated mainstream Firefox ( I lost track of version numbers ), on OSX 10.8.3. The machine is an iMac7,1 with a Radeon HD 2400 XT. It should also be noted that the crash occurred (twice) during 'Preparing Javascript', before 'Downloading'. Developer extensions were enabled, which might have an effect.
Thanks for the in-depth reply.
Wut? Prototype is basically Javascript way of implementing OOP. Most other language does it via Classes. Closure is something you get when you nest function together. The inner most functions get access to all the local variable of the outer most functions. Because of closure it save the outer most function's local variable values. It's a neat trick. Since most languages once you call a function and supplied it with an argument that value of the variable will go away. IE. function add(num1, num2) { return num1+num2; } Once you call add(1,2) the value of num1 =1, num2 = 2 is gone forever. But with closure you can freeze these values. function add(num1) { return function (num2) { return num1 + num2; } } var add3 = add(3); add3(2); // 5 The 3 is preserve because of closure.
I acctually think you are making it more complicated..
If you think coffee is easier, stick with js for a while. When you fully understand the output of coffeescript, make the change and refactor your js code. Coffee should not be easier, only cleaner.
So true. 
It's written in C++ though, not JavaScript. It's a bit like posting Dart, Python (Pyjamas), or Java (GTW) stuff here. Is anything okay as long as it compiles to JavaScript? I thought this sub is "all about the JavaScript programming language" (see sidebar) and not JavaScript the compiler target. Well, if it's now all about JavaScript the compiler target, that would be fine with me, too. However, if that's the case, the sidebar should make this clear.
&gt; A web browser doesn't offer much in the way of bleeding edge technology But how much bleeding edge do most games really need? If you look at the current gen consoles then the graphics they offer have been enough for lot of people. I think what gamer wants depends on the device he is using. If he has a PC then he expects better graphics than someone who has only a console.
That's what I though too. I'd imagine browser games are more vulnerable to piracy since you have access to the source code.
You can generate a random RGB color with rather little code: function(){return'#'+(1e8^Math.random()*(1&lt;&lt;24)).toString(16).slice(1)} All you need to do is to use two or more random colors to create a gradient, be it with CSS3 gradients, SVG or canvas (you can even have a dithered gradient using [this](http://rectangleworld.com/blog/archives/833)).
aah, thanks everyone! I'm pretty new at JS so all this was really helpful for me. 
I think the day games run in the cloud entirely is when piracy will at least die off partially. With nvidia investing in cloud gpu solutions and Internet connections becoming faster, and tech like nvidia grid I can see cloud gaming services becoming mainstream by 2018 or something. 
You don't really get access to the source code, though, what you see has been compiled into a low-level subset of Javascript.
Still easier to understand than ASM though.
You are using jQuery, get rid of the inline event handlers! $("#one").click(run1); $("#two").click(run2); $("#three").click(run3); And you should be able to make all of those run functions into one. 
First, try setting up a prototype chain with more than 2 objects in it. For example, objectA -&gt; objectB -&gt; objectC. By default, there isn't a really clean way to do it, even if you are using Object.create (if you are not using it, there are only ugly ways). Secondly, if you are assigning properties via the prototype property, then you need to be making functions for every object in the chain, which in some cases makes sense but isn't always necessary. EDIT: I've updated the gist with another comment that (hopefully) explains what I'm talkin about.
Don't call it a monad i've been... jQuery("div").chain("hide", "slow") .addClass("done") .find("span") .addClass("done") .end() .chain("show", "slow") .removeClass("done") .end() .end(); ...for years!
https://github.com/joyent/libuv
Awesome! That is easy. That's probably why there isn't a plug/library around it. 
Well its early implementation and I feel its important to follow a long with whats happening in the javascript world regardless how long it takes to create. I know I probably wont ever be doing anything like this but I will at least know where to go and what tools are required if I had to learn it in the near future. 
Just about any software on your computer that directly interacts with your hardware is going to be written in C or C++ (or, to go further down the rabbithole, assembly), and NodeJS is no different. JS is a high-level language, and NodeJS (i.e. the v8 JS engine) is written mostly in C++ I think.
Looks like a fun learning project, but these types of posts are made every week here on this sub. It's pretty much a rite of passage for JS devs to attempt and write some kind of abstraction to make JS fit their mental image of what OO should be -- which is fine! It's a great learning experience. But if you search this sub, you'll find 100's of similar posts about people attempting to recreate classical OO within the JS runtime (and an equal number attempting to 'demystify' closures, `this`, etc.). They are all pretty much flawed in one way or another, but as long as your flavor works for your needs, that's really all that matters.
All I meant is that you're probably not going to see studios like Bethesda, Rockstar, Crytek, etc attempting to make their latest and greatest graphically intense games run inside of a Canvas element in Internet Explorer. But smaller studios and indie developers I definitely see. 
This jQuery color plugin is very powerful and could be used to create a set of really nice gradients. The problem with the Math.random solution is that you will end up with way too much green and almost never get yellow. https://github.com/jquery/jquery-color/
"Preparing" means it was compiling JS, so that's before it does any WebGL at all. Running WebGL starts with the engine startup, which is after "Downloading". edit: does it happen consistently?
JS is just a language you code in, all operations that aren't part of the language (e.g. almost everything) is written in a host language, typically the language that the JS engine is written in. Node embeds the V8 JS implementation which is written in C++, and uses the V8 C++ API to expose host APIs. Basically Node has functions that are written in C++ that call the appropriate host functions, the engine provides an API to wrap those host functions in a JS object that acts like a function. There is no reason you couldn't do this with another engine such as SpiderMonkey or JavaScriptCore - I think there's actually an effort to build Node on top of SpiderMonkey but i don't know if it's still alive.
Remember that a browser is accessing your system's internals as well: rendering images to the screen, saving files, etc. Node provides some global objects that are supported by c++ (just like a browser provides, say, the ability to draw onto a canvas that can be manipulated by JavaScript but which actually operates via c++). This is how, in node, you can spawn processes, write to streams, etc.
I've seen plenty of "well behaved" shaders that are able to take out drivers. A lot of the shader rewriting that we do is to reform "perfectly reasonable" shaders into things that aren't going to clobber the system. I know of at least one set of nvidia drivers that would optimize out one field of a uniform vector and then not stop you from writing out data of the full size of the uniform. The shader was sane, the code was semantically correct, the driver crashed. Look at the gpu and driver blacklists in webgl implementations and realize that those lists simply grow as new ways to crash/corrupt memory doing "correct" things on certain combinations.
I think most of V8 is actually written in C++
You're probably right, I should have just said C/C++ like most people do.
Good points, yeah, that's definitely possible. My guess is still a non-GPU issue though, because it looks like this happened during startup, before any WebGL was even executed. Also, even if WebGL were run, these shaders are identical to the ones in Epic Citadel, which has been tested in the past on many setups and is considered very stable (that's what I referred to before). But yes, you are correct, even well-tested shaders can have surprising results. 
From what I understand, #RandomQuestion isn't the name of the function. The function is an anonymous function that doesn't have a name, just created by saying 'function() {' $('#RandomQuestion') in jQuery is shorthand for returning a collection of matched elements either found in the DOM based on passed argument(s) or created by passing an HTML string. In this case the passed string is '#RandomQuestion' which gets all the elements that have the id ('#' means id) of RandomQuestion. So your &lt;li&gt; above. Following that collection selector should be something to operate on that collection of dom elements. So like $('RandomQuestion').someFunction()
This isn't really an attempt to mimic classic OOP in javascript. kind of the opposite, its an attempt to make the prototype accessible while still letting you construct objects with object-literal syntax. I do admit that I did not search the subreddit first to know if something similar existed here (i know, bad manners), but I do think that that this has practical potential. EDIT: for what its worth, I have gone through that 'rite of passage' before, and am generally of the mindset that classes are not entirely needed in js, although in some cases they are a convenient way to think about things. 
This happened twice yesterday on my work computer. That's about my limit for crashing my machine with links from reddit :) Today, I tried again with the Web Developer extension disabled, and it worked perfectly, so that seems to be the culprit. 
Interesting. Not sure what that extension does, but must be something significant to cause that.
it is what ever you want to do to those objects. It could be anything, like if you want to bind them all to an event like a mouse click, or a key press you would do: $('RandomQuestion').bind('click', function() { alert('User clicked on me!'); }); Here you can see another use of an anonymous function that will alert that message when ever that event that we created is fired. (In this case when a user clicks on the DOM element)
For the love of god please open the FAQ in a new window. I was waiting a while to get the game started and then clicked on the FAQ to view controls, and had to go through it all again.
It is exactly as easy to understand as ASM (assuming it has been obfuscated).
http://jsperf.com/fast-deep-checking/3
Is this bike-shedding about bike-shedding? Meta-bike-shedding?
My sentiments exactly. I've never seen something so beautiful and awesome. I can't wait until multiplayer games are made like this. **Edit** Jesus Christ it doesn't look like it's too far off! http://www.youtube.com/watch?feature=player_detailpage&amp;v=BV32Cs_CMqo#t=66s **Edit** Apparently it already exists! https://developer.mozilla.org/en-US/demos/detail/bananabread
Looks like you've got a syntax error in the provided code. There's a jQuery selector to find the #RandomQuestion element, but then you have an opening brace, like you're trying to call a method on the jQuery object passing anonymous function, but there is no method call, and no anonymous function... if needed, you can call something like the following to run a function on each element returned by the selector: $("#RandomQuestion").each(function(index, element){ alert( element ); // [object HTMLLIElement] }) However, "each" may not be the best here, because there would only ever be one element with a given ID... you might be better off to say something like this to get just the LI element : myObj = $("#RandomQuestion")[0]; // [object HTMLLIElement] .. I'm not sure where you're getting an [object HTMLCollection] from -- it isn't really an error but an object. An HTMLCollection is like an array; it has a 'length' property and you can accessed each item in the collection by it's index. Assuming you document.write(objCollection) returns [object HtmlCollection], objCollection[0] would return the first element.... crazy guess, an [object HTMLLIElement] myObj = objCollection[0]; // [object HTMLLIElement] .. Once you have an [object HTMLLIElement] you can access the data-* properties via the 'dataset' property... something like the following can be used to get at the data property ( actually, now that I re-read it, the answer is on the SPAN element and not the LI... if there were any data properties on the LI they could be returned ) : myObj.dataset.bind // "text: RandomQuestion". .. If all you're trying to do is get a random question from a list of LI elements, I would use jQuery to find ALL the elements with the data property, and pick a random one from the list, and do something with it, something like the following : var $allAnswers = $("UL.mdHoverActions LI SPAN"); var randomIndex = parseInt( Math.random() * $allAnswers.size() ); var myAnswer = $allAnswers[ randomIndex ].dataset.bind; alert( myAnswer ); 
Ohh is this using asm.js? 
"Does JS actually have..." is not the right way to think about it. If you read the language specification, JS has very little functionality. The only types it knows about are `Object`, `Function`, `Array`, `String`, `Boolean`, `Number`, `Math`, `Date`, `RegExp`, and some error types. It has a few helper functions on the global object like `parseInt()` and `parseFloat()`, but that's it. Anything that you are used to doing in JavaScript in the context of a browser, such as `document.getElementById()` is not officially part of the language. The whole Document Object Model (DOM) is a set of APIs bolted onto the side of the language. And those APIs are implemented not in JS but in native code in the browser. When you call such a function there's a transition out of JS and into the C++ that the browser was written in, which does whatever it needs to do to find a node in a tree with a given element, and then wrap that in a JS class named `Element` and return it. But you could do anything. You could expose the low level socket routines available from the operating system as a set of JavaScript objects, for example. Calling such methods transitions out of JS and into the native host language, which calls the OS function, and returns a result wrapped in some kind of appropriate wrapper. [Here's an example of a simple socket server in Node.js](http://nodejs.org/api/net.html#net_net_createserver_options_connectionlistener): var net = require('net'); var server = net.createServer(function(c) { //'connection' listener console.log('server connected'); c.on('end', function() { console.log('server disconnected'); }); c.write('hello\r\n'); c.pipe(c); }); server.listen(8124, function() { //'listening' listener console.log('server bound'); }); It's just plain vanilla JavaScript. `server.listen()` is conceptually no different than `document.getElementById()`. Neither are "part of JS", they are both libraries. It's just that in the above example the `net` module was explicitly loaded, whereas the DOM is implicitly (automatically) loaded when you're working in the context of a browser. In other words, any language worth using can be extended to do anything, simply through the use of libraries/modules. If there's an API that is useful, you write a module that exposes it. This is the case of virtually all programming languages. For example, if you read the official international standard for C++, there is absolutely nothing about graphics, sound, animation, 3D, GUIs, networking, keyboards, mice, input, etc. And yet people write programs in C++ all the time that use those features, such as video games and web browsers. C++ is just like JavaScript in that the only things "in" the core language are the most basic features, like strings and arrays. It has nothing whatsoever to say about graphics, but you can write a library that exposes the graphics APIs of your operating system to C++, which means you can write programs in C++ that use those features, despite the fact that they aren't part of the language. This is what makes a programming language useful. If there was a fixed list of things that a language could do, then it would be next to useless. Instead, you extend the language using libraries to provide whatever features you want. Newbies seem to continually ask what language to learn as if there was a list of things that one language can do that another can't, but the reality is nothing like that. With a few exceptions, any language can do anything, it's just a matter of writing the plumbing to make that possible. 
No need for closure or prototype. * If they're all static functions (i.e. need to interact with an object, effectively 'stand-alone') you don't need prototype. * If you don't do any global vars aside from you 'boogots' you dont need a closure, either. It's still good to, though, because no one likes polluting window with crap. But honestly, if you're the only author on a page (i.e. you're not building a library for others to use), it shouldn't matter. This will work : var boogots = { util : { first:function(){return 1}, second:function(){return 1}, third:function(){return 1} } } 
Yes.
I really liked this presentation... very humorous, especially : Would you buy airplane controls, the good parts?
I am webdeveloper so I don't use InDesign. That's why I'd like to script Photoshop and Illustrator
This leaves me confused and vaguely angry. Tell me who to follow! 
People who say the [bike shed](http://bikeshed.com/) should be painted yellow.
TIL everyone is doing it all wrong.
I was thoroughly confused about the style guide slide until I read the text at the bottom: &gt; NOTE for those who didn't see it live: "humble style guide" is a parody. I'm not hating on "new" and I'm certainly not advocating "always use ==="
This is the typical way you would do it, and sort of illustrates my point. Using this method is fine if you are always going to have this pattern (A-&gt;B-&gt;C), but its highly impractical for arbitrary inheritance inheritance patterns. Any time you want to create a new type of object, you need to write another constructor function. Each constructor function is essentially 'locked in' to inheriting from the same parent (ie, B will always inherit from A). One of the most beautiful things about JavaScript is its object literal syntax - its very easy to create arbitrary objects on the fly. When you have to rely on constructor functions to set prototyping, you not only lose the ability to use object literals, but you lose the ability to create arbitrary objects altogether. And to top it all that, I really think it just looks cleaner: var obj1 = NodeObject.createRoot({ test1: 'test 1' }); var obj2 = obj1.create({ test2: 'test 2' }); var obj3 = obj2.create({ test3: 'test 3' }); etc.. I will say, though, that if you are making large numbers of consistent and/or complex object, building them via a constructor function is the way to go.
When professionals promulgate absolutes such as "never use ``==``," they are well aware that it's more complicated than that. The audience they are giving this message to is not people who know the difference between ``==`` and ``===``. It's people who *don't* know the difference. And if you don't know the difference between ``==`` and ``===``, then you *should* always use ``===``! It will make life so much easier for you and for everyone else. "Never use X" is shorthand for "using X can be dangerous unless you're well aware of all the caveats and corner cases. If you aren't aware of all those caveats, you should avoid using it because you will be unpleasantly surprised by them at the least opportune moment. If you *are* aware of those caveats, then you know enough to discard the 'never use X' advice when appropriate." Nothing is ever absolute, including this sentence.
Yes! Canary yellow!
&gt;When professionals promulgate absolutes such as "never use ==," they are well aware that it's more complicated than that. When most people say "don't use == or !=", they really mean it. If you need a check for null or undefined once or twice a year, just check for null or undefined. Be explicit. Things are much easier if your code accurately reflects what it does (or what it's supposed to do). Other languages which lack type coercion are perfectly usable. Type coercion isn't required for anything. numberFive == stringFive numberFive === +stringFive The difference isn't 2 characters. The difference is that the second one clearly states that it expected a string, that it meant to convert this string to a number, and that it meant to compare it to some other number. Type coercion hides this information. It makes your intent less clear.
OP demonstrates that ``==`` has a (minuscule) performance benefit over ``===``. This makes sense, since when you aren't coercing a type, ``===`` is the same as ``==`` with an extra type comparison. I doubt there are any applications where your bottleneck is going to be the 5% slowdown introduced by using ``===`` ... but it's enough for me to agree that "never use ``==``" shouldn't be considered an absolute rule. It should just be the default unless you can make a really convincing case otherwise.
&gt;when you aren't coercing a type But when you are doing type coercion, it's slower. Anyhow. It's hundreds of millions of ops per second. It doesn't matter. This 5 year old 200€ office machine can do over 350 million of those per second. Virtually everything else you do in a program is far more expensive than that. E.g. actually doing some branching with that condition, accessing arrays, creating objects, calling functions, garbage collection, and so forth.
Well it depends on if you need to support anything lower than IE9 or not. You should fix your variable declarations/function scopes/closures though. 
I agree with you. "never use ==" is not an absolute rule, but it makes a person's life easier and simpler. For example, `if(x == 10)` matches `"10"`, `"10.0"`, `"9.9999999999999999"`, `"1e1"`, or `"0xA"`. (Notice that `parseInt("1e1", 10)` yields 1 while `+"1e1"` and `~~"1e1"` yields 10). If you don't want that, use `if(x === "10")`. Of course, `if(x == "10")` (when you know that `typeof x == "string"`) is one of valid options, and `if(x == 10)` is valid if you really wanted above results.
Source code also available on github: https://github.com/davemo/intro-to-angularjs Topics covered: * angular.module * angular.controller * angular.directive * angular.$routeProvider * angular.factory * ng-app, ng-model, ng-submit, ng-click * $scope, inheritance, and its relationship with the DOM * contrasting some of angular w/ jQuery/Backbone
ok, take this function function ObjectB() { ObjectA.call(this); this.test3 = 'test 3'; } ObjectB.prototype = Object.create(ObjectA.prototype); it will only ever make instances of ObjectB which inherit from ObjectA. They will all have the same properties. This is, of course, by design and useful in many situations. But what if, for example, you want to create another object that has a different set of properties that also inherits from ObjectA? Using this pattern, you would need to write another constructor function. Thats extra overhead for just a one-off object. Or what if the object, for some reason, needed to be completely dynamic (as in, you don't know what properties it will have up front). Or what if ObjectB needs to inherit from ObjectA sometimes, and ObjectD other times? These are some of the limitations to this pattern.
Nice AngularJS introduction. Thanks! The plugin you want at the end to avoid stringifying service names is called ngmin (https://github.com/btford/ngmin) which can be run as a pre-minifier step.
So, are we just going to ignore the ugliness of how to get around the minification problem? Great video. very informative--hadn't looked at angular before now.
&gt; Nothing is ever absolute, including this sentence. does that mean that some things are, sometimes, absolute?
It's kind of like the incompleteness theorem of relativism: any system that declares the non-existence of absolutes is inconsistent because it contains an absolute.
i mostly agree with you, but I think I'd still go with numberFive == stringFive IF stringFive could be either a string or a number. But really, i feel like if you know that +stringFive means 'coerce stringFive into a number' that you probably know the difference between == and ===. Also, x == null is just useful.
[JohannesH](http://www.reddit.com/user/JohannesH) comment above has a decent looking solution :) https://github.com/btford/ngmin
Omg I'm so sorry. I think the cheatsheet is refering to Google's Closure js library vs jQuery. https://developers.google.com/closure/library/ I thought goog.dom was a bit odd. Sorry about the confusion, google really chose a stupid name for their js library...
I see what you mean. No multi-inheritence without duck typing.
Clearly goldenrod is the only good choice.
It's interesting that Angular JS comes from Google, since Google is the reason why I avoid developing single-page web apps... the problem is then there aren't separate landing pages for Google to index in their search results, which is something you really need for most websites. (I work on shopping websites, hotel booking sites, etc.) Am I right to think that, or am I missing something?
My favourite James Bond film.
To answer my own question, I found this: https://developers.google.com/webmasters/ajax-crawling/ I had no idea Google offered a way to crawl AJAX / dynamic web pages. So much to learn... 
It would be something like: var timeout_id=null; var text="Text to be swapped in"; function textSwap() { timeout_id=setTimeout(function(){ document.getElementById("side").innerHTML = text; }, 1000); } &lt;element&gt;.onmouseout=textSwap; &lt;element&gt;.onmouseover=function() { clearTimeout(timeout_id); }; 
Watched the whole video and think you did a really solid job with this. One of the more accessible and clear Angular videos I've seen. One question, I noticed you used: $scope in some places and: scope in the showsMessageWhenHovered directive. Is there a reason for the inconsistency?
"ABOUT!!!!" - sorry had to do that. Very well done, extremely poignant for me personally. Have been going through egghead.io this weekend - this was an awesome summary course. Thanks for producing/sharing this!! Tip - you can hit ctrl-x to clear a line.
You could make life easier. Put your questions into an array, then in simple JS it's just ... function (myQues) {var nq=myQues.length;var qnum=Math.round(Math.random()*nq);return MyQues[qnum]} ... There may be a place for jQuery, but I recommend learning more JS.
So window.navigator.plugins returns Plugins, which is an array indexing the plugins the user has installed? Do I just loop over .plugins until I find "Adobe Acrobat" and then compare .version to the real version (that I will presumably have set as a variable)?
Author of ngmin here. All module systems have this same "ugliness" where you need an array of strings that correspond to function params. Take a look at Require.js for example: require(["helper/util"], function(util) { //This function is called when scripts/helper/util.js is loaded. }); versus: angular.module('whatever', []) .controller(['$scope', function ($scope) { ... }); I actually think Angular's system is nice because it gives you the option of deferring adding the strings to injectables (controllers, services, etc.) until you care about minifying. This lets you use a static analysis system like ngmin to add the strings automatically, or you can add them yourself. There are use cases for both approaches (for example, aliasing module names). But again, Angular isn't any more "ugly" than other existing solutions.
Hmm. Perhaps I'm not that familiar -- is this a common problem for most frameworks, or just ones with dependency injection?
Having watched a few AngularJS videos, this one kept me around. I found this better as a "Beginner/Intermediate", mainly because of the several use cases of controllers, factories, and directives COMBINED with some logic as to when you may an may not want to use them. Most of that is in the last 20 minutes, but I watched the last 20 ish twice... the Author does a great job being smart and accessible! 
And a side note, I also think this same scope vs $scope oddity is why minification is a pain in the ass.
Question: I'm at a cross roads here. Initially, I was viewing AngularJS as something I'd use to make "apps" or within website tools. This example, one of the cleaner ones, made me think... Would it be worth building a website on angular? I mean, is it worth all the extra .js file loads and counting on Javascript to just say "screw this, my sites being built on Angular"
off topic, but what chrome theme is that?
The sort of thing you would develop with angular.js isn't usually the sort of thing you care about getting all its pages indexed.
Great introduction. Thanks!
Yeah that was the most amusing part of the post.
What the hell is this: http://miketaylr.com/post/68403ceb.png
So is it really mostly just for form input and interactive types of apps? It wouldn't be good to use it for browsing/searching a shopping site for example?
You can type scope = $scope to attach the new var to the window object. Easier for debugging. 
Thanks for watching :)
Did you mean my editor theme? I'm using [Sublime Text 2](http://www.sublimetext.com/) with the [Soda Theme](https://github.com/buymeasoda/soda-theme/). As far as I know my Chrome theme is just stock with what comes with the release channel.
Given angular is googles' (or at least Misko Heverys) vision for how web _applications_ should be built that it probably applies more to apps than to websites. However, I think that shouldn't prevent you from building a website with it; it all comes down to [capability vs suitability](http://blog.davemo.com/posts/2011-11-21-the-scna-2011-narrative-suitability-capability-anarchy-and-propaganda.html). :)
Thank you so much for the feedback; I definitely intended to try and make some "smart" assumptions about the experience level of most people who would watch this and your comment regarding "Beginner/Intermediate" validates that :) 
Thanks for your feedback, the [egghead.io](http://www.egghead.io) screencasts that John Lindquist have produced are awesome, so I'm glad to hear that my screencast still provides value on top of that. Thanks for the editor tip, I keep trying to incorporate keyboard shortcuts that can help me be more efficient :) 
The lame answer is that I read the [docs and saw examples](http://docs.angularjs.org/guide/directive) and just kind of copypasta'd my directive link function definition without thinking about it ;) This is definitely one of those questions where the answer eludes me; I'm going to do some research into this and try and find the right answer.
I think any language that has a compile target (you could consider JavaScript minification fits that description) and uses some sort of intelligence around Dependency Injection is going to be subject to this issue; in languages like Java using Spring to inject dependencies you end up with all the configuration in an XML file. In JavaScript we have pre-minification scripts; a "pick your poison" sort of situation, but I'd much rather avoid XML :)
Bear in mind that I'm both strongly opinionated in general, and philosophically opposed to data-binding frameworks in particular, so take what I say with a grain of salt--though here it seems I'm going to vent on AMD/CommonJS instead of Angular. There's a tendency for the overhead on modularization to become excessive, particularly in the AMD family. Anyone who has used dojo might know what I mean. Think about how you import code in any other language. Python's import, php includes, C++--they all assume that when you require code you have it in a synchronous fashion. Meanwhile Node, using a CommonJS implementation of modularization, has you write var http = require("http"); You're asking the environment for a module, and you put it in your variable; and if it doesn't have it, an error is thrown. Do you expect that will cause any minification problems? And doesn't it seem like the general tried and true method for importing code? Because let's be clear about what we're doing when we write: ['$scope', 'AuthenticationService', function($scope, AuthenticationService{...}] In some very fundamental way, this is akin to the linking step of a compiler. Modularization is prone to a fantasy wherein everything is a module that can, if it chooses, include every other module. All it has to do is tell the compiler--in this case, the Angular compiler (yes, it has one)--where it would like the code, please, whenever it can get it, because javascript got in the habit of requesting libraries basically whenever the hell it wanted to. So yeah, you can code a bunch of modules like this, and there's a module manager that's keeping track of all this, generating an include tree _for_ you and probably not messing up and probably throwing good errors when a module isn't available or the asynchronous bit fails... Or... we could remember that **programs follow a linear (synchronous!) execution**, even javascript once you understand events, and we're forgetting that if a web app were in C++ we'd be required to have all the code we were ever going to use before even getting started. We could just say "Ok, include this, that, and the other" and... know that we have them. By now you're going "You're way off topic, dude, we're just talking about minification." But it's the **same thing**. Minification gets difficult when strings and variable names have to start matching up for a _very good reason_, and the reason is you've begun storing code in your strings. Quines are cute and all, but you'd never catch a sensible C program treating ['$','s','c','o','p','e','\0'] as a pointer to a variable. It's _because_ AMD-style dependency injection attempts too much meta-programming that we have to face questions like this. 
It really is a bit of a revolution in my mind; I was a huge skeptic at first but I think the lightbulb moment for me was hearing Misko Hevery describe the separation of "declarative ui, imperative app-code" as the core of what Angular gets to. After you work with it for even a little bit you start to see why that separation makes so much sense, but it's definitely not something that I would have thought a few weeks ago.
Eh, it does, but it's not perfect. It runs my site's js up to the point that I GET a Markdown file for the main content, and then halts--so all of my records in Google's search display my catch-all error text. 
Also, as a Canadian I fully appreciate the accent sounds "funny" ;) 
Not lame. I've done the same innumerable times with copy/paste. Generally I prefix javascript variables with a dollar sign when they contain a jQuery object, as a reminder of what's stored in there. Just thought maybe there was some similar representation with something provided by Angular.
I appreciate your feedback; we should be clear that there _is_ value in the principle of Dependency Injection regardless of the specific method one uses to implement it. I'm not a fan of AMD either, mostly due to the asynchronous part, but the way Angular implements DI and doesn't force async into the equation is the "right way" to do it IMHO. Edit: This [talk by Mark Trostler of Google](http://www.youtube.com/watch?v=JjqKQ8ezwKQ) is awesome at explaining the basics of DI, and sheds a lot of insight into the design decisions of AngularJS
It's not really a DI-specific thing, it just happens to be an implementation detail of Angular's DI system. [DI](http://en.wikipedia.org/wiki/Dependency_injection) is a really cool technique though, so if you're unfamiliar I'd read up on it. I wouldn't really call this a "problem," but it's something that throws people off when coming to Angular because (as far as I'm aware) Angular is the only framework using this sort of implementation.
Ah, I think I can answer that. My understanding is that variables prefixed with $ in angular are things provided by the framework using the [$provide abstraction](http://docs.angularjs.org/api/AUTO.$provide). So when you inject $scope into a function, angular makes a lookup internally to a $scopeProvider, $http lookup to $httpProvider etc.. It still doesn't answer the question why the $scopeProvider is used in the case of a controller dependency and _not_ in the case of the linking function inside of a directive. 
I think it also depends on where your interests lie. If you are truly interested in front-end development then jQuery and other libraries are great but I would recommend you really learn JavaScript spec and challenge yourself to small projects w/o the help of libraries. That will lead to you truly understanding the core of the language, the best way to implement programmatic solutions, etc. If you just start working with the aid of the library it might lead to you not learning some of the more important fundamentals of the language. 
Awesome, good to hear the value distilled down to just one or two phrases or concepts :D I hear you about the learning curve, another set of screencasts I want to do is on some of the differences between Angular and Backbone in a more direct comparison; but I think for an "Intermediate/Advanced" topic covering the Digest loop, $watch, and "Models" as POJSOs (plain old javascript objects) would be valuable too.
I think that's a bit of a red herring; if you frame it in terms of "declarative for ui, imperative for app-code" as Misko Hevery does in [this screencast](http://www.youtube.com/watch?feature=player_embedded&amp;v=cF_JsA9KsDM) on Angular I think it makes a lot more sense.
God what a horrible presentation and website. That does sit with me wrong to try to make HTML a programming language--but is that really what it's doing? how is it different from templating? (besides the data binding)
I get into this a lot and I try to avoid repetitively linking what I've said before, so I'll try to keep it brief: You can write a recursive template using Angular, and it will fail at a depth of 10 because Angular, despite having a $parse and $compile for its templates, doesn't realize as a _project_ that it is creating a brand new language. [Check out the console here (my code).](http://koglerjs.com/example/angular/#/r/AskReddit/comments/17fjyb)
I think Douglas Adams said it best, "Don't Panic!" You just used a few more words and catered it to AngularJS.
If you have a dumb template, your render is a one-way process. Fire and forget. Your render layer is the DOM, and the browser takes that layer as input to its own render layer. A dumb template is like a stamp, and you can change the color of the ink (the data given to it) but not how it prints (the _static_ elements it creates). It's clean, because your javascript works directly on the view tree. 
I've seen your posts before; I think you could abuse any framework to do things that fail, but that doesn't mean the ideas the framework presents aren't valuable :)
Frankly the separation makes sense only after you've had to write the glue code for UI elements for dozens of times. I'm guilty of writing countless lines of nothing but: $("#some-button").click(function(e){}); $("#some-other-button").click(function(e){}); $(this).parent().parent().removeClass("disabled"); // etc File after file of event handlers that are a horrendous mix of DOM manipulation, logic checking, ajax calls, custom events, and so forth. It's simply unmaintainable, especially when you have to re-use some bits of logic but with different DOM elements, different class names/IDs -- but not enough to create some semblance of structure. Making CSS queries easy (and normal) for front-end programming has been the best and worst thing to happen to web development.
I agree to some extent--that Bombermine MMO used Angular to great effect. But at some point we're going to have to move past "live and let live, every tool has its use" and make decisions about what future web dev will actually be. 
This is incorrect. You can write a recursive template with infinite recursion depth. What you cannot do is change a model more than 10 times within a digest cycle. Your code causes a model mutation at each recursion. The solutions is to use `$compile` manually, or to do the recursion asynchronously. There are several examples of this [here](https://github.com/angular/angular.js/wiki/JsFiddle-Examples). Also this has absolutely nothing to do with "creating a brand new language."
I figured out how to run Angular on the server with Node.js. It doesn't work for everything and is quite bit hackish but probably will work for a lot of scenarios. https://github.com/ithkuil/angular-on-server/wiki/Running-AngularJS-on-the-server-with-Node.js-and-jsdom Note that the last time I tried rethinkdb (after I wrote that) they changed the API and actually broke the Node package in npm I think, so skip all of the rethinkdb stuff in there.
Oh, I'm not really sure it's worth getting into this again (especially in a pro-Angular topic), but while I'm here... &gt;What you cannot do is change a model more than 10 times within a digest cycle. I don't know why my template would change the model. All I'm doing is including the template again. The data is static. &gt;The solutions is to use $compile manually In order to write a recursive template I need to invoke the Angular compiler. That doesn't seem sensible. &gt;or to do the recursion asynchronously. That doesn't seem sensible either. You've said a recursive template should be a feature of Angular before, but Angular sure doesn't make it easy. &gt;There are several examples of this [1] here. I referenced [this example](http://jsfiddle.net/IgorMinar/CHVbb/423/) previously, with the Angular template recursive call inlined into javascript. Even if it weren't inlined it'd still be treating a string as code. [This one](http://jsfiddle.net/brendanowen/uXbn6/8/) would, I suspect, encounter the same depth limit if it were given the object to render all at once instead of the 'asynchronous' recursive onclick add. And [this one](http://plnkr.co/edit/T0BgQR), the only one I hadn't encountered before, is doing what you recommend (albeit to an inline template) &gt;Also this has absolutely nothing to do with "creating a brand new language." $compile treats Angular templates as if it's a programming language. 
scope is read only access to the scope within the link function - in which you are not within angular world and you don't have automatic data binding anymore. 
you should take this and make a second one that explains how to professionally (orderly) manage and deploy apps with multiple team members. that's kinda what I'm missing. ;) when you do this let me know :D
they also crawl PDF, and any text based document type like jar files or flash files, everything that can be easy decompiled basically.
We don't use any of those but jQuery and that's only when we stole some code from someone else but will eventually rewrite to exclude jQuery. This is the third thread I've seen started on reddit with the same title over the past week asking the same question.
are you talking about Git and version control?
sorta, + deploy + managing the libraries etc.
Yes! I'd watched all the egghead.io videos back-to-back and forgot about that one; to summarize the difference: $scope is an injected dependency that is looked up using angulars $provide mechanism, and scope in the context of a directive linking function is simply a positional argument that the framework gives you automatically. Those arguments for the link function, (scope, element, attributes) are always the same.
What? jQuery really is not that slow on any modern browser as it will use native methods wherever it can and only fall back to non-native methods and regex based selectors (Sizzle engine) when it has to. And unless you're doing thousands of iterations of something, there's really not that much difference. That said, learning the foundations of core JS *is* a good thing for any serious front-end developer.
Actually, that's very cool. Good job !
Thanks man! Thinking about making scales and bpm changeable and maybe some more advanced synthesis :)
Congrats to all the winners. There were lots of great ones that didn't make the top 10. Check out [the demos page](http://js1k.com/2013-spring/demos) for the full list. Here are some of my favorite demos that were not in the top 10: (I'm sure I'm missing some wonderful ones.) * http://js1k.com/2013-spring/demo/1388 * http://js1k.com/2013-spring/demo/1458 * http://js1k.com/2013-spring/demo/1548 * http://js1k.com/2013-spring/demo/1473 * http://js1k.com/2013-spring/demo/1542 * http://js1k.com/2013-spring/demo/1401 * http://js1k.com/2013-spring/demo/1504 And my own: * http://js1k.com/2013-spring/demo/1509 (It does stuff like [this](http://i.imgur.com/S4bsaCT.jpg) and [this](http://i.imgur.com/NTK3Nat.jpg))
Ok, same answer -- right click, inspect one of the card elements and figure out a CSS selector that applies to it, and all the other ones -- run that through jquery, select a random one &amp; look at the dataset.... the code near the end of my post basically does this, but the selector might need some work. What do you plan to do with the answer returned? In your original post you passing it through document.write() .... if you do this after the page has loaded, it will completely blow everything away and replace it with whatever you put through document.write. If you're debugging to figure out what you're dealing with, I find alert works well. OR, protip : use the developer console to execute code on a loaded page.
Need to de-click your audio samples.
more customization would be cool, but the presets you have are terrific. well done.
Be-warned modules are not yet complete, so there may still be more changes to come. The next meeting in in May in London and before it there should be a more up to date design put up on the harmony wiki page.
wow! love the line of sight! You should consider pushing the button states to an array and check in the update loop for smoother keyboard controls 
Thanks. The line of sight was done using [visibility-polygon.js](https://code.google.com/p/visibility-polygon-js/). The keyboard controls can be made smoother by firing on keydown and keyup so as to keep the thing moving while the key is pressed down. But I think the mouse controls are nicer.
I thought about that. Here are some ideas I considered: 1. Collect, against the clock, points scattered around the maze that are only visible within line of sight. 2. Race to a point in the maze. 3. Evade enemies only visible within line of sight. But I can't decide if any of these would be fun enough.
nice! :D anyone else got any melodies to show off? just copy the local storage entry
[[0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0],[0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0],[0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0],[1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1],[0,0,0,0,0,0,0,0,0,1,1,1,1,0,0,0],[0,0,0,0,1,1,1,1,1,0,0,0,0,0,0,0],[1,1,1,1,0,0,0,0,0,1,1,1,1,1,1,1],[0,0,0,0,1,1,1,1,1,0,0,0,0,0,0,0],[1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0]]
the line of sight mechanic and visualization are awesome, so I would go with a gameplay mechanic based on that. patrolling enemies with line of sight would be sweet.
definitely nicer! feels like you're controlling a mouse
I love how the second one is literally a ripoff of the first one.
If you want to make it easy to share melodies, maybe store it in base 36 (0-9,a-z). For example, using the following 2 functions, your melody can be stored as `g-cn4-e8-40-5-35s-188w-5xc-ds-1e6n-ds-f`. var decode = function(in_s) { var s = in_s.split('-'); var out_int, out_array = []; var jj = parseInt(s[0],36); for(var i=1, ii = s.length; i&lt;ii; i++) { out_int = parseInt(s[i], 36); out_array[i-1] = []; for(var j=0; j&lt;jj; j++) { out_array[i-1].push((out_int&gt;&gt;j)&amp;1); } } return out_array; }; var encode = function(in_array) { var out_int, out_s = in_array[0].length.toString(36); for(var i=0, ii = in_array.length; i&lt;ii; i++) { out_int = 0; for(var j=0, k = in_array[i], jj = k.length; j&lt;jj; j++) { out_int |= k[j]&lt;&lt;j; } out_s += '-' + out_int.toString(36); } return out_s; };
That's cute, but its not really a maze if there's no entrance and no exit.
You're right, but nobody would click on a link that said, "randomized space-filling tree on a hexagonal lattice".
Nice UI! Some thoughts: * Don't use a mono-spaced font for the placeholder text * After searching, my keywords become placeholder text (sortof), and clicking to edit them results in them disappearing * Turn the play button into a pause button when playing (clicking it again already pauses the video, but it change from a &gt; icon to a || icon) * Would be nice to be able to play individual vines * Would be nice to see the "next page" of results (scroll on desktop swipe right on mobile?) * The results UI gets smashed on iPhone in portrait. Use media queries to detect portrait mode and change the results layout. * The search box does not fit on iPhone in portrait. * The Twitter bird (share half circle) on the results page is not centered on iPhone. * This deserves a favicon.ico! 
&gt; It's hundreds of millions of ops per second. It doesn't matter. I don't think that follows. People whose only interest is eliminating bottlenecks on a website may rightfully not care about it, but it is still an issue relevant to almost every piece of javascript code, and therefore one that deserves attention, from those who view the language itself as their specialty if nobody else.
You can't edit CSS in an external file, it will need to be binded to the style attribute of the element. I instead created two different classes; 1 for 'playing' and another for 'paused'. I then just toggle the classes. Check it out: http://jsfiddle.net/uc9c5/6/
jsFiddle's default setting automatically wraps your javascript in a function, so declaring variables or functions doesn't make them global, and the button's `onclick` can't find the function `spin`. You can fix this by selecting a "no wrap" option in the second select menu on the left, or by declaring `spin` as `window.spin = function...`, or by adding the listener with js instead of html.
Wow, do you have the source?
Oh no, I killed it! I was playing with it and tried to go back just now, only to find that we've used all your CPU time.
[Dropbox mirror](http://dl.dropboxusercontent.com/u/1423612/hexmaze/index.html). I have no idea how serving up a static webpage uses up all the CPU time. Maybe it's analytics.
Sure, just right click and view source.
CPU limit reached; might I suggest using Google Drive hosting? Whoops, noticed your Dropbox link.
Hmm, I guess you have a point but again, this only matters if you're doing thousands of iterations on something.
I've played games that use lights/lanterns to limit your visibility; they all seem to be agonizingly long (or maybe I don't have the patience for them...) I'd suggest keeping the maze walls visible so you at least have a feel for where you are no matter what.
Yes you can (temporarily) modify CSS in an external file. Besides, he isn't trying to edit an external file, he's setting `style`, like you said. The reason your version works is because you added the listener differently. See my reply.
i didn't know that about jsFiddle. Learn something new everyday :)
Please do the math. Calculate how many of those minor difference you have to add up in order to gain 1msec. Also, use a profiler to find actual bottlenecks. This never was a bottleneck and it never will be.
I don't think you understood what I said. I agreed that it will (most likely) never be a bottleneck. My point was that, despite that, this is not a meaningless discussion. You may have lost interest once you realized it wouldn't benefit your code, and that's fine, but we still need some people who care about implementation details, for the same reason we need people who write javascript and not just jquery.
I didn't either. Thank you chrome dev tools.
Sorry... here is the [Dropbox mirror](http://dl.dropboxusercontent.com/u/1423612/hexmaze/index.html). Enjoy!
**EDIT:** Updating to remove inaccurate guesses. As I understand it, the one you linked is the original ["Strange Crystals"](http://js1k.com/2013-spring/demo/1459) by Philippe Deschaseaux. Roman Cortes originally submitted [this demo](http://js1k.com/2013-spring/demo/1451) of Furbee. Roman Cortes [implemented his own similar tunnel effect](http://www.reddit.com/r/javascript/comments/1dqs3a/js1k_spring_competition_winners_announced/c9tcp6j) and then added the Furbee code into there as [this.](http://js1k.com/2013-spring/demo/1461) After that Philippe Deschaseaux released the modified ["Strange Crystals II"](http://js1k.com/2013-spring/demo/1555). That's my guess as to how it went, at least. Even without the ideas being shared, their original "Strange Crystals" and "Furbee" probably would have placed into the exact same positions without the idea borrowing, as they both were very impressive in their original forms. 
&gt;this is not a meaningless discussion In the context of writing JavaScript, it's completely pointless. If you need a few *nano* seconds *that* badly, you really shouldn't use JavaScript in first place. Furthermore, you need to be immortal in order to do every other optimization which gives you more bang for the buck. &gt;we still need some people who care about implementation details I can assure you that there are still people who work on those VMs. For someone who writes JavaScript, it doesn't matter. It's like specks of dust on a bowling ball. No one notices the difference and no one can measure it either. Seriously, that's the kind of scale we're talking about here.
You're getting emotional; please make sure that you actually read what you reply to carefully before replying to it, as you're replying to your emotions rather than what I said. You've also forgot the part where I said I don't pirate stuff. I've lived off producing copyrighted material for mass distribution; I'm not unaware of the problems with piracy. That was not my point. My point was this: The damage from piracy is indirect. There is no damage in the actual act of piracy; the damage comes from the acts that *are not done* as a result of piracy. If somebody that would not otherwise have bought one of my games makes a pirate copy, it makes no difference to me. In most cases, it was better for me if he pirated my game rather than pirating or buying another game; it made him familiar with the type of things my company made, and more likely to buy something from us in the future. The only thing better than pirating my stuff is buying it. In Forbes' rephrasing of Bill Gates: "Gates argued at the time that while it was terrible that people in China pirated so much software, if they were going to pirate anybody's software he'd certainly prefer it be Microsoft's." http://labnol.blogspot.com/2007/07/we-love-microsoft-software-piracy-in.html And this is very different from theft. This shows your "you are taking" metaphor is failing you; people are not taking. They are copying. Nothing is lost in the act of copying; if Chow in China makes a copy of my game, which he never could buy in the first place because it is priced out of reach for him, then it makes no difference to me. The only thing that makes a difference is if somebody that would have bought the game does not buy the game because they now have access to a pirate copy, or aggregate similar effects. *Any other copying is a net positive*. I usually list this as the four cases of piracy (as viewed from the content producer, in terms of direct revenue): 1. Pirated, did not buy, and would not have bought if didn't pirate: The pirating is revenue neutral. 2. Pirated, bought, and would have bought if didn't pirate: The pirating is revenue neutral. 3. Pirated, did not buy, and would have bought if did not pirate: The pirating is revenue negative. 4. Pirated, bought, and would not have bought if did not pirate: The pirating is revenue positive. From the point of view of me as a content producer, the question is primarily what the difference is between 3 and 4. 1 and 2 are revenue neutral in direct revenue terms, but most likely slightly positive in terms of reputation and future revenue from influenced consumers.
As I already said, twice, I'm not advocating knowledge for the sake of a few nanoseconds, I'm advocating knowledge for the sake of knowledge. That's a principle I doubt you could dissuade me of. To use your analogy: sure, a speck of dust on a bowling ball is imperceptible. But if the bowling ball is coated in dust, you can bet I'm going to wipe it off. Not because it would perceptibly affect its weight, but because I'm not going to present and use a dusty bowling ball. And that's what code that has little performance compromises lying around is: dusty.
Thank you very much! I'll be sure to give you credit if I use it for any project of mine.
Thanks, much appreciated. Give [visibility-polygon.js](https://code.google.com/p/visibility-polygon-js/)'s author [byronknoll](/u/byronknoll) credit too if you use that.
wow thats genius man, thanks! If I put it in the url hash people can easily link melodies
https://github.com/jchavannes/earth
[[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1],[0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0],[1,0,0,0,0,1,0,0,0,0,0,1,0,0,1,1],[0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0],[1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,1,0,1,0,0,0,0,0,1,0,0],[0,1,1,0,0,0,0,0,1,0,0,0,0,0,0,0],[1,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0],[0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0],[1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0]]
page views are going to eat that up especially if you set your server up in a way that does a lot of work before it gives the static file.
Why would you even need a php proxy for this, rather than just getting the XML file in your jQuery call?
I also thought that his demo was bold in a way others were not. Hiding content by making it appear randomly was a gamble because some visitors will never see it all but allowed for progression that wouldn't have been possible otherwise. He also made some clever design decisions that make it feel consistent and hide the drawbacks of his techniques instead of e.g. adding some random effects.
Small point: Michael Fogus and Jeremy Ashkenas published underscore-contrib, I (raganwald) am a contributor, and they encourage others to contribute as well.
[[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0],[0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0],[0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0],[1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0]] A little more complex: [[0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0],[0,1,0,1,0,1,0,0,0,0,0,0,0,0,0,1],[0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0],[0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,1],[0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0],[0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0],[0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0],[0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0],[1,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0],[0,0,0,1,0,0,1,0,0,0,1,0,0,0,1,0],[1,0,1,0,1,0,0,0,1,0,0,0,1,0,0,1]]
This submission has been linked to in 1 subreddit (at the time of comment generation): * /r/webdev: [iPresent, javascript mobile mockups presenter](/r/webdev/comments/1dsgr2/ipresent_javascript_mobile_mockups_presenter/) ---- [This comment was posted by a bot](/r/cecinestpasunepony)
&gt;Also, x == null is just useful. ``!x`` is as useful as ``x == null`` and even more concise, as in: if (!x) { throw new TypeError('x should have a value'); } Sure, this will cause false positives if ``x`` is ``false``, 0, or the empty string... But if you're expecting ``x`` to be a boolean, number, or string, then you shouldn't use either ``!x`` or ``x == null``, you should check for the actual type. if ('number' !== typeof x) { throw new TypeError('x should be a Number'); } if ('boolean' !== typeof x) { throw new TypeError('x should be a Boolean'); } if ('string' !== typeof x) { throw new TypeError('x should be a String'); } This check ensures not only that it isn't ``null`` or ``undefined``, but also that it's the type you expect it to be. And if you're dealing with a parameter that might be ``null``, might be ``undefined``, might be an object, and might be a number, boolean, or string, then your API is too permissive.
Cool, many thanks for going through the trouble to help me.
This is pretty cool in it's own right, but I was wondering if you could refashion this into a static site generator along the lines of [Jekyll](https://github.com/mojombo/jekyll) (written in Ruby) or [Pelican](https://github.com/getpelican/pelican/) (written in Python) 
Choose a different naming schema than "NodeObject", "childNodes", "attributeList", etc., lest it be confused at first sight for something to do with the DOM.
&gt;But if the bowling ball is coated in dust, you can bet I'm going to wipe it off. Not because it would perceptibly affect its weight [...] The analogy was about weight though. &gt;And that's what code that has little performance compromises lying around is: dusty. It isn't a compromise. There is no difference. You can't measure it. If you can't measure it, it didn't add any business value. Besides, your "optimization" might be actually slower under real-world conditions (e.g. in this case, you might accidentally coerce types, which you would have noticed otherwise). Micro benchmarks don't always measure the right thing. There can be also "hockey curve" distortions (e.g. browsers behave really weird if you have 100 times more nodes than the average website). Furthermore, the actual workload might be very different (e.g. sorting algorithms behave very differently with tiny data sets or with almost sorted data). Write clean maintainable code. Use a profiler. Optimize the hot spots (sorted by bang/buck). Huge performance improvements are only possible with better more suitable algorithms. Secondly, you *will* spend most of your time with maintenance. Clean well-organized code will safe lots of time there. This time can then be used to optimize new or remaining hot spots. If it's your own product, doing more marketing might be the most important optimization. Or tweaking the website. A/B testing. That kind of thing. Resources are always very limited and you have to spend them wisely. Micro optimizations are generally the wrong thing to do. Focus on productivity. The optimizations you do should be like strategically dropped atom bombs.
Just pushed up some adjustments * base36 encoding for melodies (thanks /u/dllu) * melodies can now be sent as links * pause sound when window looses focus * able to paint notes with click and move
i second this. I came into jQuery thinking it was a catch-all evolution of basic javascript and would handle everything I need, when really its only really used when you're manipulating the dom. Anything logic related where you aren't directly manipulating DOM elements, its pretty ill suited or even useless. 
I like this. Continue to make it better, but very good start. I really like the 'flashlight' effect.
What makes this Istanbul library better than [jscoverage](https://github.com/fishbar/jscoverage)? jscoverage doesn't require any specially-instrumented version of your code, just have your test suite require your module with jscoverage's require instead of the normal one.
You make good points about where your time is best spent. But if you haven't written the code yet, using `==` instead of `===` doesn't cost time. It's a decision that, if made beforehand, has absolutely no cost - in fact it saves you a few seconds. So it's not a micro-optimization, it's just a decision. Not one related to the performance of your code, but to its appearance. &gt; You can't measure it. Well, that's how this conversation got started - you can measure it. Maybe not in the context of a full application, but it does provably exist. &gt; Besides, your "optimization" might be actually slower under real-world conditions (e.g. in this case, you might accidentally coerce types, which you would have noticed otherwise). This kind of argument shows up all the time regarding javascript, and I hate it. Yes, of course if you make a mistake your code can end up slower - actually it's more likely to introduce a bug or break it outright. This is a fact that applies equally to every programming practice and style in existence; calling it a con of one particular method is foolish.
This is great! Thanks for open sourcing it too. One suggestion- decrease the gain node value for the synths if there are several notes triggered at the same time- this will ensure there isn't any fuzziness/clipping.
Without seeing your HTML and perhaps a more complete version of the JavaScript it is hard to help. Is it possible for you to provide a dummy setup with jsFiddle? http://jsfiddle.net/ Edit: added link
I think it would be better if the rules were defined declaratively as objects and not as strings. The string-as-everything format never goes well in the JS ecosystem
no. but I'd look into html 5 video. That'd take flash out of the equation at least, and the possible timing issues that might derive from its use. or maybe not. I just went to http://www.youtube.com/html5 to opt-in, then back to your fiddle, and it's stuck looping the first half-second of the video. still, if you can work out what's breaking the html5 version, you might get better timing. Or just different timing issues. 
I know promise implementations are different and I'm no expert but shouldn't node.js domains be used in conjunction with the async module to give a more accurate comparison since promises provide exception handling (i.e. throw) and async does not?
Oh sorry. I'm already opted into the HTML5 trial. I've been opted in for ages -- I completely forgot YouTube even renders in Flash. I suppose a fix for either would be just as interesting, though.
work through some javascript koans https://github.com/liammclennan/JavaScript-Koans
http://carlbarrdahl.se/tm/#g-6bk-sg0-8-1s-pag-3k-35s-e8-vm2-0-unb
"Optimized Vector-based Fill Patterns for Dynamic 6-gon Pathing: A Lonely White Dot Cries Alone In The Darkness. It cannot get out. It cannot get out, no matter how many times it invokes function chasemouse(){...}."
This is just a current issues with the browsers. Unfortunately its not solved yet. So the basic answer is no :( Wish it wasn't so, but alas, it is. 
I'd use external player controls.
Use chromeless player windows then have external javascript buttons &amp; slider to play/pause/seek both videos simultaneously.
Hey sounds awesome! Maybe throw a web audio compressor on that so it doesn't distort when a lot of notes are playing.
I couldn't seem to get the drag/drop on the live demo page to work with Chrome 26. Dragging over the landing area and dropping never did anything. Edit: Not sure what caused the script to crash. Hangs the tab in Chrome, but I tried first with a 21K file (250 rows, 15 columns, lots of formulas) and then again with just 5K (6 columns, 32 rows, no formulas) and it worked that time. Both only had one sheet.
Check out other projects that do similar and look at their javascript... e.g. http://www.youtubemixer.net/
yeah, I'd suggest using the chromeless player if you're trying to get two videos to play at the same time. It would be pretty easy.
check your pm, gave you a link you can test with
But what if one is already playing?
HTML5, Flash, makes no difference. The underlying problems still exist.
That may be a good idea in practice, although it was done intentionally. The reason I threw this together in the first place was out of curiosity as to what a minimal library for mocking some certain DOM behaviors using normal js objects would be - specifically event bubbling (via prototype chain) and parent/child traversal. I then realized that, in practice, it could make the prototype chain a little nicer to deal with.
Cool, keep up the good work. This is a pretty nifty tool. There are probably tons of business use cases for doing something like this in a lightweight situation.
This isn't really a browser issue, it's a Flash/plugin issue. Flash communicates w/ browser through each browser's plugin architecture. The way that Flash communicates w/ a browser's JS hooks is through the Flash/JS bridge, which is notoriously slow. So when you click the play button in the top viewer, an event is triggered in actionscript, has to be serialized to be pushed over the Flash/JS bridge, trigger an event in the browser, which then has to interface w/ the 2nd Flash/JS bridge instance, which in turn then has to trigger an event in the 2nd flash player. Add to this fact that the flash plugin is single threaded. It's barrels of fun. If you can get away w/ using HTML5 video, do it.
[http://jsfiddle.net/uc9c5/9/](http://jsfiddle.net/uc9c5/9/) recommend adding/removing classes for this sort of thing rather then changing the css properties of the dom element directly. Got it mostly working, although I'm noticing a little blinking when the animation resumes. Could just be my browser, though. EDIT: as somebody else mentioned, the spin function was not accessible to the button because jsFiddle was wrapping it in a closure. I fixed that by assigning it directly to the window object. In practice, this is not a great way to do things, and event handlers should probably be added using `addEventListener`.
This looks like it could be very useful to me! I will be checking it out soon. Thanks!!
It actually lags with the HTML5 player, too. If you opt-in to HTML5 on youtube here: http://www.youtube.com/html5 it'll still lag :)
very cool to see this redone with the WebAudio API! since i see you're taking feedback... it would be really nice if you highlighted the "active" column so we have a visual idea of when notes we place are hitting... also: [here's a melody](http://carlbarrdahl.se/tm/#g-0-368-4-1kw-sgw-1s-djc-4ya-1w-e9-16) and finally: it mutes itself when the tab is not in focus - AWESOME OF YOU TO DO THIS! edit: [a bit nicer melody](http://carlbarrdahl.se/tm/#g-1-fv4-79-1og-sv4-g0-djc-u8i-1w-e9-16) .. fun! :-)
Thanks for the feedback! That would make more sense in the way that you wouldn't have to parse the string at all, but how would you suggest doing the whole chaining/grouping thing? Via methods such as .or(object) and .and(object), maybe, with possibly something like .group() for "nested" ones?
There are a few libs that use chaining to create validation patterns - check out shouldjs or chai - just add .and and .or condition to that that build a query and you're most of the way done
Thanks man! Thats actually a great idea, will see how it turns out. Oh and I'll give your melodies a listen as soon as I get back from work
&gt; using == instead of === doesn't cost time It does, because === is more restrictive. There isn't any ambiguity whatsoever. This catches trivial issues and it makes the code easier to read, because what you see is exactly what happens. &gt;So it's not a micro-optimization, it's just a decision. Using == because it's 5% faster than === (if no type coercion occurs) is a micro optimization, because it's 5% of virtually nothing. If it would make your whole program 5% faster, that would be something. However, it's so close to zero that you won't be able to tell the difference. &gt;you can measure it You can't measure it as part of something which does some actual work. Naturally, you can't measure it in an actual application either. &gt;This kind of argument shows up all the time regarding javascript, and I hate it. The only thing which matters is how some algorithm (or whatever) behaves as part of your actual application. For example, there was a discussion about DeltaBlue (one of the Octane benchmarks, a one-way constraint solver with a focus on OOP and polymorphism) a few weeks ago. It used a seemingly complicated method to remove elements from an array. In a micro benchmark, the usual reverse-iteration + splice a bit faster. However, when plugged into the actual benchmark it was drastically slower. This isn't about mistakes or anything like that. You really have to measure the real thing. If you can't prove that you've improved anything, you've wasted your time. &gt;calling it a con of one particular method is foolish The point was that it doesn't necessarily save you a few nanoseconds. If you can measure it, it might be actually a few nanoseconds slower. You won't be able to tell.
A rule from your example test: [18, "min:5 &amp;&amp; (max:10 || between:15,20)"] With chaining var validator = pupil.min(5) .or(pupil.max(10), pupil.between(15, 20)); var isValid = validator.validate(18); You can assume every new chained condition is an AND and an OR takes * arguments - this is similar to the way a mongodb query works {$or:[array of conditions]} This style opens it up more to specifying multiple rules and combining them into new rules via chaining
True, true, although I would probably change the or's arguments to also be chained in that example (wouldn't it be more consistent?). I could add support for that kind of chained rules (maybe via a plugin?), but I'd also like to keep support for the string-based system so the same rules can be passed from the PHP-based back-end to the JS-based front-end. Thank you for the suggestion and example!
Same here, with Safari 6.0.4 on OS X. The workaround is openssl enc -a -in filename.xlsx | pbcopy ... then paste into the textarea. Then it's awesome.
Do you have any requirement where JSON.parse is not an option? If not just use that, it will handle nested JSON objects just fine.
Oh! Awesome; it's just built-in then? How modern a browser do you need for that to work?
caniuse.com says ie8 and up so you should be good, if you need ie7 I'm sure a pollyfill exists
Thanks for the detailed response! I know there's no such thing as a "JSON object," I was just typing imprecisely; instead of "JSON object," a better thing to say would be JSON representation of an object." I'm curious though; why do you say that what I'm doing is a bad idea?
[Can I use... Support tables for HTML5, CSS3, etc](http://caniuse.com) [caniuse.com]
Look into the "gapless playback" problem. http://stackoverflow.com/questions/7330023/gapless-looping-audio-html5 
I meant "JSON within JSON" is a bad idea. Suppose you have this JSON string: var myString = '{"foo": "bar"}'; You could assign it to an object member: var strageObject = {"data": "{\"foo\":\"bar\"}"}; And turn this object to JSON data: '{"data":"{\"foo\":\"bar\"}"}' *That* is JSON within JSON, and I've seen it in real live. Trust me, this is never, *ever* a good idea. What you're trying to do is just commonplace in web development. I guess you will do this many, many times from now on ;)
Not really it calls for JSON.parse, but if you do want to use recursion, use [brush tail](https://github.com/pufuwozu/brushtail)
The writer doesn't back up his premise and arrives at no conclusion, leaving me puzzled about the wall of rhetoric in between.
He seems to think JavaScript is doomed because he doesn't like it. The majority of his complaints are actually well founded but not with JavaScript itself, but the DOM.
I have to disagree there. I love Javascript. I really don't like jQuery (much). I love how Javascripts implements inheritance. It's a bit weird at first, but it's actually more flexible than classic class based OO. I love how it supports closures. And regexes. Javascript can do pretty much everything I want. The problem with Javascript, 15 years ago, was that it was so unportable. You had to do things one way on a bunch of browsers, and you had to do it a completely different way in MSIE. Events, for example. All jQuery really did, was make Javascript more portable. It's ironic that now, jQuery drops support for the old MSIE versions, which is the main reason why it got popular in the first place. Newer versions of MSIE are getting closer to how other browsers behave, anyway. So "make everything alike" libraries like jQuery are less necessary than ever before. I think Javascript may actually outlive jQuery. Some other library will probably come out and take its place, one that isn't so weird. Oh, and I like sweet potato, too.
This might as well be "Why English is doomed". Javascript is messy, cobbled together, and very hacky in a lot of ways. But it's also ubiquitous. That's why it's remained popular all these years. Everyone knows it. Hearing arguments about why javascript is going to die, to me comes off as the same as hearing why Esperanto is going to replace English/Chinese/French as the language of the future. 
 &gt; JQuery was basically designed to make JavaScript possible to work with. I thought it was to solve the cross-browser issues mostly caused by an inconsistent DOM API.
Tldr don't read it.
When the page loads have the youtube clips start buffering (but not playing), then let your custom buttons call the youtube API js code to start/pause both videos.
Are you sure steam isn't throwing an error or something? i.e. if there's malformed json it doesn't like it'll throw a ParseError. $.ajax({ url: './whatever', dataType: 'json', success: function(data, textStatus, jqHXR) { ... }, error: function(jqHXR, textState, errorThrown) { ... } }); Or are you running up against same-origin policy stuff here?
:) Thanks! I've reported the issue to Google and am looking into the Web Audio API!
Trying to sync one player with one which is already playing. The end goal is to be able to mute the 1st player, unmute the 2nd player and have the audio seamlessly transition between the two players.
I wrote a node module for interacting with the steam web API, may help as a proxy... I've never tried it with DotA2, but would love feedback on improving it to be more flexible. https://github.com/Tidwell/nodeSteam
Unfortunately Apple has not shown much interest in supporting many of the more exciting HTML5 technologies on mobile safari. Perhaps not wanting to compete with native apps on their devices. But I live in hope :-)
I was half expecting a scary girl and a loud screech when I was playing this game. Looks very good, works perfectly in Opera as well!
setup an nginx server, proxy requests to steam. simple as location /steam { proxy_pass http://api.steam.com/ ##i dont know what this is } then all calls to /steam on your server will go to them
Actually it's possible to use the camera/photo API (though not video afaik) in mobile Safari to import into canvas using iOS6 Check out https://gokercebeci.com/dev/canvasresize
Anyone else have to read the article to figure out what a "photobooth" was?
I Don't know if I'd call 15kb lightweight...
This is not a great short story about "javascript" [sic]. It is a link to the ECMA-262 Standard. What is the point of the misleading title?
I don't get it... the most interesting part about ORBX.js - aside from the spectacular performance - is that no addon is required - everything can be handled right in the JS engine of any modern browser... anyway, AFAIK, FSX means Flight Sim X from Microsoft - wasn't that discontinued?
Too bad chrome has a bug where the camera remains switched on after the tab is closed. [Bug](https://code.google.com/p/chromium/issues/detail?id=235126)
you could try manually triggering the event: https://developer.mozilla.org/en-US/docs/DOM/Creating_and_triggering_events
No demo? fuck off. This belongs in /r/coffeescript anyway. So fuck off twice.
Cool. Will have a look into it!
Excellent, this is exactly what I was looking for!
Cool. Too bad they haven't implemented polygon offsetting. The straight skeleton algorithm is notoriously painful. However for doing certain types of illustrations creating concentric paths are really helpful / essential. I've been using and extending paper.js, but their public releases are non-existent -- just nightly builds.
&gt; In the context of writing JavaScript, it's completely pointless. No. It still matter. The statement can be inside a hot loop. All these small things add up.
Can someone explain what this actually is? "event-driven architecture for developing scalable applications using reusable widgets" seems a little vague, and describes pretty much every JS lib in existence nowadays.
I have a feeling this is based on Nikolas Zakas lecture on scalable applications, google it and give it a watch :) I contributed to a framework which uses the exact same vauge jargon (scalable, sandboxed, widgets/modules, extensions) so I have a feeling this is similar. And no, its very different. Its a framework for your app rather than a library. All libraries you wanna use can't just be included. You need to write your own extension for each new library you wanna use so it can be sandboxed ....it runs on the concept of loose coupling between widgets so that they have no dependencies on eachother and can be tested and developed seperately by many different developers working together to build a large app. Its most efficient for large apps and not your typical website or anything. It doesnt depend on any single existing library out there so you can easily switch out. Meaning your app doesnt depend on the libs you choose to suppory it and if a lib becomes depreciated or needs to be replaced you can do it without having to rewrite the app, therefore making it highly scalable
Yes, those five specs of dust on the bowling ball surely add up. You'll certainly feel this added weight when you drop it on your foot. You need like 10 million of those ops to make a difference of 1msec. However, everything else you do is *way* slower than that. So, in order to get this 1 msec difference, your program needs to run for minutes. No one will ever notice the difference. You also won't be able to measure the difference, because this is way below the random fluctuations you always have. It's a matter of scale, really. It's +5% (or -15% with coercion) of virtually nothing. Okay. Here is an example. Say there is some function in your program, which takes 10% of the time. If you make this function 10 times (!) as fast, your program will only get 9% faster. If you only make that function 5% faster, your program will only get 0.5% faster. However, in this case you don't start with 10% of the total run time, you start with less than 1 millionth. Making this 1 millionth 5% faster will not change anything. Seriously, making it twice as slow won't change anything either. Feel free to prove me wrong. Write some loop which does something useful where this crap makes a difference.
Sounds like a bad reimplementation of npm.
Supercool :)
Since the name of the codec is "ORBX.js" and not "Orbx", I don't see a problem. Anyway, names can be changed easily.
node jitsu provides a free HTTP to JSONp bridge service: http://jsonp.jit.su/?callback=[callback]&amp;url=[url]
Anyone know what browsers this supports? IE8?
**Seriously**? A whole system written in node to do what `wc` or `grep` does in a single line. This right here exemplifies the problem with developers these days - they dont bother to learn their tools, so everything looks like a nail.
jQuery seemed to me to be created for 2 reasons. a) to provide a higher level interface to DOM and b) to hide differences in browsers. How much has reason b) gone away with modern browsers? 
"modern browsers" So, no.
The most immediate things I note are that the prices and payment e-mail are supplied in the HTML page. That means they're basically trivial to edit. Now, if you're a tiny store and manually checking transactions received before shipping is feasible, that's fine. However, it feels like you're rapidly going to outgrow this... 
It's basically using the module pattern to keep things private in a module, then using a "sandbox" which is the interface in to that module and allowing cross communication between modules using the mediator pattern. check out http://addyosmani.com/scalablejs/
Yeah you could easily manipulate the prices in the HTML before adding stuff to the cart... so you'd need to validate the prices again using an own checkout method: http://simplecartjs.org/documentation/sendform-checkout So it really kinda looks too easy and needs more work than you'd think at first glance.
I already use such comparator — see inducedOrdering which makes one from a collection. I also have extensive test suite which covers that. Yeah, SortedCollection is a cool idea.
Indeed, or to use a [tool that already does exactly this](http://cloc.sourceforge.net/)
Yeah that's not good, OP be sure the price is picked from the database when transferring to paypal.
&gt;Is Not: Foolproof &gt;Clever hackers can change prices of simpleCart(js) items before checkout. This is a known security flaw that exists with ALL javascript shopping carts. It is highly recommended that all orders be checked upon receiving or backend security checks be put in place 
So it seems like it's a strategy for 1) isolating client-side UI components from each other, 2) allowing them to communicate in a loosely-coupled fashion, and 3) for making libraries more swappable. Assuming I've understood it, it seems like you get a little bit of this with AMD, but this takes it a lot further, especially with the concept of the sandbox. Thanks for posting.
Thanks. I made a reply [here](http://www.reddit.com/r/javascript/comments/1dvzn2/aura_a_clean_and_scalable_architecture_for/c9uupc7).
As long as your callback checks the amount received vs the order / array of items you'll be fine - which should happen especially with automated systems (downloads etc).
This is just about the only non-open source project I get behind. It really is just so freakin stellar. Although, I would love if it went OS.
Thanks so much for the strong vote of confidence. Yes, the licensing model is a thorn for some folks, but it is absolutely essential for sustained growth and support of the platform. Anyone who is curious about the need of a proper licensing model / funding mechanism might find this helpful: http://www.greensock.com/licensing-considerations/ Again, very cool that you chimed in. Great to have you onboard. Best, Carl Geek Ambassador www.greensock.com
I get it. Pretty sure that's one of the reasons GSAP stays so far ahead of people's needs. Nothing compares to it.
I just released the first part of a Sanity Arte remake: http://redd.it/1dwpo3
Yeah, AMD doesnt mean you need to write your own sandbox code so if your app depends on jQuery for dom manipulation but tommorrow a new library for DOM manipulation takes the lead and becomes the shit, you pretty much need to rewrite your app because it depends on jquery.... But in this case your app depends on a sandbox. So you only need to rewrite your extension but you dont need to change the modules or widgets themselves. So hundreds of modules can be reused for years while underlying technolgies get better and only need to rewrite that DOM manipulation extension
The cache is used so that we don't have to hit the DOM over and over. I provided a means to clear out the cache just for a bit more control over what's saved in memory. Have I had to clear it out yet? nope... haha Yeah, moving to prototypes soon. Started out really simple and I was just trying out a different way to format my code. But it's grown a bit so going forward, I'll be moving to prototypes. I was thinking about just providing a way for a developer to just provide their own _selectElements or _compileTemplate functions. Really, you got it spot on, it's pretty much the same as querying the DOM and just storing it in a variable. I wrote this with the idea that I'd prefer to keep all of my dom access in a central object instead of spread throughout my code so I can just call it like a function. I could just as well create a function on whatever object I'm writing but that's not as fun. Thanks for the feedback!
A templating engine would only serve to pull the HTML out of your JS. You'd still have to take the output of the template and append it into the DOM. For what you're doing, you're doing it right. If you were to have an HTML snippet and use a templating system, then you'd have the template separate from your JS code, you'd call the compilation step from your JS, and then append the result to the DOM. With Underscore.js, it would look something like this. var template = _.template($('#idOfYourTemplate').html()); // Sometime later... dataObject = someJSONSource(); $('body').append(template(dataObject));
Nice! I like the minimal approach it is taking. I have been trying out lots of Backbone libs to manage collections and I have found [Backbone.CollectionView](http://rotundasoftware.github.io/backbone.collectionView/) to be a very convenient abstraction. Sometimes you don't need _all_ of the functionality these libraries provide so I can see where this lib would come in handy!
love reading security analysis from you and then seeing your username is chmod777 :)
fwiw, I think the idea is right on point. Have not looked closely at the code, but I like the direction you are taking it.
It's not a while lot different really. My goal would be to provide a bit more functionality beyond storing your cached elements in an object. Or at least making managing those cached elements a bit easier. For instance, say we have an element `foo` that is added to the DOM when someone clicks a button. Instead of adding it to the `doms` object by doing something like `doms.foo = $("#foo")`, you could use domvelope to lazy load it. When you set up domvelope, you'd say the element is lazy loaded. `var myDom = new domvelope({"name":"foo","selector":"#foo", "lazy":true});` Now, when `foo` is added to the DOM and you need to access it, you only need to call `myDom.foo()`. This will grab `foo` from the DOM, cache it, and return it. Even if `foo` is just an element that isn't needed often and you don't want to store it in memory, you can use lazy loading to ignore it until it's needed. And while yeah, adding `foo` to your `doms` object to cache it is easy, I just like the feel of `myDom.foo()` more. 
Isn't this the same thing as [groundskeeper](https://github.com/Couto/groundskeeper) ?
how about currying/partial application? usually the second one
No... P2P is for Peer to Peer. ;)
What is "the return function"? ****edit**** In case you meant the return statement, e.g.: function a () { return "lol"; } whenever you call a, i.e. you do a(); the parser will change 'a()' to whatever is returned, so in this case it will become "lol". In this case saying var c = a(); is identical to var c = "lol"
Unit tests (with QUNIT) now exist. See: http://www.boilerjs.com/test/
Now there is no more than 1 alias name per method.
I usually put your first solution into an immediately invoked function: var f = (function() { var t = function() { // do something with 'f.a'; } t.a = "initial"; return t; }()); There's an ugly intermediate private 't' in there, but otherwise I find this very readable and self-contained.
I'm guessing because the 5/10/15, etc dropdown was just a UI decision. Steam didnt actually care what amount you asked to charge, as long as the transaction went through, they go their money.
The second form is the most obvious to me, once you understand closures. That's what I use. 
Yeah, this particular issue means the cart should not be used with paypal or google checkout. It should collect the products and send them to a checkout system run by the same website that is using the cart in order to verify each item in the cart exists and that the price is correct. Otherwise, the cart is just too easy for customers to tamper with and opens the seller up to a constant stream of payments going to the payment processors that need to be canceled out, which is not something they tend to like. They want you to have a good storefront and I don't think they'll have much patience for something like this (at least not PayPal EDIT or Amazon. Amazon will likely shut you out right away for using a poorly implemented solution like this). 
so function a () { return "lol" } returns a variable called *lol* that is the result of the function and the variable passed to the function and always updates dependent on the conditions of the function? how is this different, from say function a(number) { number + 2 }
http://www.quirksmode.org/js/function.html
unless there is a good reason why you need the 'static' variable to be private, the first or last options are best in my opinion. The options using closures are really no better than the last option (the one using an object) as they introduce an extra function (the closure) which is extra markup and overhead (however minimal). I prefer the last option over the first slightly, since using `this.a` is a little better than having to reference the name of the function/property. Again, other than making your variable 'public' (is that really so bad?) the only issue I see with this method is that it makes your function call slightly uglier (i.e., `g.f()` vs `f()`) but i think being able to use 'this' is worth it. There is another option, that is a sort of hybrid approach of your first and last options: function g() { g.f(); } g.a = "initial"; g.f = function () { // do something with 'this.a'; }; basically, creating a function (`g`) that will act as the wrapper object. This way, your `f` function will be able to use `this`, and you can still call it using `f()`. Personally, I'd still go with the last option (object), but since function calls in js are relatively cheap, this is a viable option too.
it should noted that it depends where you're writing your code. if you're writing in the global scope, the function definition should be put in a closure anyways. if you're writing in commonjs modules, you're writing in a closure already. if declaring variables in the current scope has a potential of confusing you, you may be better off refactoring into smaller "closures". personally, i hate nesting and avoid then as much as possible. i don't think you should be running into any problems with variable definitions if you're naming your functions uniquely as well as modularizing your code in closures. i personally despise the last one though as it requires a reference to `g`. stick with the first three. although i prefer the first one, there's a case for each. 
Surely you don't mean the back end receives prices from the browser without checking the database ... ? -_O
&gt;returns a variable called *lol* No, it returns a string object with the value lol. There is no relation between the value of a variable and its name. &gt;that is the result of the function and the variable passed to the function and always updates dependent on the conditions of the function? Not entirely sure what you mean. The result of a function call will usually depend on the arguments it is called with, as your own example below shows. &gt;how is this different, from say function a(number) { number + 2 } This function doesn't return anything and the number variable has the same value after a() is called as it had before. The result of number + 2 disappears because it wasn't assigned to anything. In JavaScript you have to call the return statement with an argument otherwise the function would just return undefined. 
And my favourite porno
Option #2, because you're more often than not defining a module of functions collected together, and doing it from within the closure is most useful. 
I use handle bars because it gives you more flexibility in the firm of helpers, essentially allowing you add your own conditional statements which ends up making it that much more powerful. If you don't need conditionals and just want to display info in a standardised way, mustache is just fine. 
I'm on my phone so I can't test, but I'm pretty sure your smart quotes are the cause. Take them out, use single quotes for strings and double quotes inside them.
Security guy here. The fact that they would include price information from the client (js) *at all* means they don't understand security. There should be nothing to "check" server-side. The **only** source of this information should be the server. The fact that they would characterize this easy exploit as something that requires "clever hackers" means the developers are way out of their league. Do not use.
Thank you very much Pinewold! I will definitely take your advice.
Thank you jcunews. It makes sense now.
The Client can not have security issues and it can not be secure. The only problem are XSS attacks and they have most of the time to do with innerHTML, write, eval in combination with an other bugs. &lt;script type="text/javascript"&gt; alert("don't use innerHTML, write and eval"); &lt;/script&gt; Loading Javascript from sources that are not under your control like this: http://www.bugherd.com/sidebarv2.js?apikey=e17e5c93-4185-482c-ba7c-c925b546d45d is a security risk. Even if you trust them, they can be hacked. 
No worries. Switching books would probably be a good idea - that code was...kinda weird. :-) You can give the head first books a chance, they probably are more current and teach more best-practices.
In my experience, I find that when I need to have a function that has any kind of static data I usually end up wanting to do more than one thing with that data so I prefer the object oriented approach. This really helps with maintainability moving forward and helps other devs grow the codebase without screwing it all up ;). Although I've dug my way into Closure hell enough times, I will still usually use the this scope and prototyping to achieve this kind of example: var MyClass = function(config){ this.a = config.a; }; MyClass.prototype.f = function(){ return this.a; }; Then will instantiate with: var thing = new MyClass({a: "my a value"}); I like this approach because then you have a base class to inherit when you need this thing to be more robust. If I'm making a singleton, I will generally create a closured object that contains many of my classes. 
use handlebars. i answer like this because explaining it will take a lot of time for me.
 &gt; needs more work than you'd think at first glance. You can tell that by the way it says &gt; All you need to know is basic HTML. That's always a lie.
Thank You. If you don't mind... Why did you decide to learn javascript? I've always been interested in programming, just never took the plunge. I've been in the I.T. industry 3 years, and it's just not fulfilling.
https://github.com/rook2pawn/node-partial something i did a few years ago to the op, there is closure here.
closure definitely. see [partial application](http://en.wikipedia.org/wiki/Partial_application) if you are so inclined and/or check out my [module](https://github.com/rook2pawn/node-partial)
I was doing office related crap at a tech company after studying music in college. I saw all these geeks making money and loving life, so I decided that if they can do it - so can I. I started copying websites using w3schools as a reference. 2 - 3 years later I'm completely comfortable with backbone (the current popular framework), freelancing for clients like PayPal and Wells Fargo, work from home and make around $80.
&gt; I personally believe this fact validates JSHint as a successful open source project. Is it really open source now? Or does it still have that additional clause (which it inherited from JSLint) restricting how it can be used? Edit: [Apparently that clause is no longer there, and nor is Crockford's old copyright notice](http://www.jshint.com/hack/). I guess it's been rewritten from scratch? Awesome!!
Are you going on about the tongue-in-cheek clause that JSLint can only be used for Good and not Evil?
You haven't provided much in the way of context, but I would start by using console.log() on the various objects to see where the questions and answers are stored.
Well, I think the script is safe to use, but it's important to check server-side what the client is actually ordering. Instead of storing prices, store an item identifier and its quantity, then make sure it all "adds up" server side. As always: Treat everything from the client as if it were a bomb. 
It really isn't an issue. You validate on checkout like you normally would anyway.
It was like that for a while but it's been updated since, and now you can input a custom amount.
It might be possible to use it safely, but this script makes the path of least resistance the insecure route. One day you are likely to screw up, or your successor might. And it shows poor judgement on the part of the developers, which suggests there may be other problems. If one of my clients really wanted to use this, I would advise them to maintain a fork which has the stupid parts (price, maybe more) stripped out. 
I disagree. It *is* an issue because it makes mistakes more likely. Always assume the person maintaining your code will be a copy-paste monkey. Why would such a person bother with a slow and complicated database query when the data is already right there in the POST parameters?
The clause that prevented it from being an open-source-compatible license, effectively preventing its inclusion in open source projects, yes. Am I wrong about this? Did you downvote me? I was of the understanding that JSHint used to be a fork of JSLint which has/had such a clause. If JSHint 2.0 has been re-written to no longer depend on any JSLint code or be burdened by its license, that's good news for open source developers who may want to package it... 
if this isn't a type, then your first method is probably preferred. although you only get the ability to have one copy of the 'static' variable you're talking about: var f = (function () { var x = 0; return function () { // do stuff }; })(); and you could always make a 'generator' thing that produces fresh copies of this function if you needed. (i think people typically refer to this as a factory, actually.) var f_gen = function () { return (function () { var x = 0; return function () { // do stuff }; })(); }; 
By open source, you mean GPLv2/3. You could still include it with a BSD or MIT licensed project, right?
No, you couldn't. You could not include it in any project distributed under *any* free license, because it restricts [freedom 0](https://www.gnu.org/philosophy/free-sw.html). You can go the other way around, though - you could include some BSD or MIT licensed code in your project and then distribute that under the non-free DONOEVIL license. But once you have that clause in the license, it can never be incorporated back into BSD or MIT licensed code (or GPL, or any other free software): that clause would have to stay (or be distributed separately). The MIT and BSD licenses are great because they so permissive, and this is why adding that clause destroys their usefulness because it takes aware their permissiveness, and why the 4 freedoms of free software are kind of important. 
Learn the [AngularJS](http://angularjs.org) framework, which uses data-binding and direct DOM manipulation instead of templates. That's my advice. :) Or even better, use Google's new language, [Dart](http://dartlang.org).
I would do something like this, but I would have to see the rest of the code to be sure: $(function(){ $('.tab').click(function() { $('.main').each(function() {$(this).removeClass("first"); } $(this).addClass("first"); } }); 
Because innerHTML apparently doesn't convert characters specified using HTML entities back into their entities. If you make the last line `alert(body.innerHTML);` you'll see that innerHTML is returning a string consisting of a single double quote character (the character that &amp;\#34; encodes)... which obviously isn't equal to a string containing the characters "\&amp;#34;".
If you set innerHTML, the browser parses the markup and does whatever it want with it. If you get innerHTML, the browser serializes that part of the DOM. Basically, there are getters and setters for innerHTML. You don't simply change/retrieve the value of some property. E.g. if you set it to "&lt;&lt;", you'll get "&amp;amp;lt;&amp;amp;lt;" back, because the HTML parser guesses that this crap is supposed to be text. If you set it to "&lt;b&gt;&lt;i&gt;&lt;/b&gt;&lt;/i&gt;", you'll get "&lt;b&gt;&lt;i&gt;&lt;/i&gt;&lt;/b&gt;" back, because the HTML parser fixed the overlapping tags.
Any recommendation as to how I can do a comparison to innerHTML having only the \&amp;#XXXX; version of the character?
Any recommendation as to how I can do a comparison to innerHTML having only the \&amp;#XXXX; version of the character?
I recommend not to use the DOM to store data. Well, you can of course create a new element and use it to round-trip the data once: http://jsfiddle.net/W2Fjv/3/
Thanks. I was hoping for something a bit more native to JavaScript, but I guess that works. And for the record, I'm using this method to toggle between two characters of an icon font, not to store data.
This. GSAP is a masterpiece of engineering. Works flawlessly.
I have made a web app that started getting too much data to manually keep track of, and I needed to make for loops in for loops and script for what happens when people didn't have the data initially (creating the array in side and array inside the original object for example be came a pain) so I started looking into Backbone, but found that it was using templateing libraries all over the place and had heard some about them and just needed to know what one to start with so I didn't end up in some sort of sink hole of crap since I had no real idea. (eks. if I had learned Flash before HTML back in the day I might have found it phesable to make whole webpages out of flash for a few years before I would have to learn something new.)
Why Dart over AngularJS ? I just checked out both and AngularJS seems much easier to learn. What I need most is something that I can easily hook up to my PHP API to fetch and save data. auto complete and keep track of the data created. The app I'm working on (got far but got frustrated using only Jquery) gets JSON for search results and lets the user created multiple lists of the elements it found through the searches . it all kind of stopped when I started having problems remembering where I declared the lists and where the info for the lists where. I dynamically generate empty lists, but the first list when the user has nothing, would not accept that I had a associated array that didn't have anything in it so when I tried adding to it everything failed and I got lost in my data-"model"
That would be correct, however, you've missed something. You're busy talking about how the checkout is outside of the scope of the cart and you're right, it is. However, this cart has gone out of its scope, it's handling part of the checkout, that is, delivering the order details to the payment gateway. That is this scripts glaring technical security flaw, it handles delivering the order to the payment gateway. This thing can be made to have no security flaws, I've checked through the code and it has the required infrastructure to made secure. That is by correcting the scope, reporting back to a server controlled by you, adding ids to the items so that you can check them against a database (or whatever you're using to keep track of your products) and performing the initial part of the checkout, sending the order to the payment gateway on the server after building the order details from a trustworthy source, your own data, not the clients (This is of course excluding data the client *can* specify, like what they're buying and how much unless if you have restrictions on those, but that's part of validation). The second security flaw is really just an extension of the first, which is that it markets itself in a *ready-to-use* state. Like you can just whack it in and use it. However, that is not the case if you have any idea what security is. I'm talking about the social aspect of security. While it can be made secure, it comes by default as insecure, all the documentation I've read has not *mentioned* that aspect of security. Simply put, it's too easy to make this insecure. **Edit:** Accidentally a word.
How do you mean I can't use conditionals in Mustache ? in the templates? like If(!age) { echo 'no age';} ??? or is it more complex?
I'm not saying this is a good idea, as I generally agree with the above commenters about this approach, but Javascript technically does support unicode characters so you could always do it like this: http://jsfiddle.net/KJuyf/1/ 
Thanks you for this. Turns out IE7 which is what Im stuck with doesnt have debug native so Im attempting to install firebug..
Either: * Use the actual unicode characters in your JS (as other posters have sugggested, if you have an [appropriate characterset defined in your HTML](http://en.wikipedia.org/wiki/Character_encodings_in_HTML)), or * Pass the item to be compared through the DOM as well. For example [like this](http://jsfiddle.net/xcs2T/)
Well, the front page says "No databases", so I'm inferring there isn't a database. 
E commerce site without a database? I'm not even going to go and read it, I assure you there is a database or something closely resembling a database involved.
How is it possible to be stuck with IE7 in your development environment? That's the biggest WTF I've heard in a long time...
Create two classes, each of which with a :before element and content of the character you want. Then, toggle the classes.
Add a class to it and switch it when you switch the text it'll probably be easier as you can create an object with class names and characters to avoid dealing with lots of random character codes
I mostly build up my code from modules which correspond to javascript files. Each module is isolated the usual way (function(global) { ... })(this); which is why I would mostly tend to use the solution [flyingmeteor suggested](http://www.reddit.com/r/javascript/comments/1dy99r/whats_your_preferred_form_of_defining_a_function/c9v5q7e). I would generally try to minimize the amount of symbols I have to export into the global namespace while at the same time eschewing deep object hierarchies because they mostly just cost access time / require more local var aliasing while not really providing that much better structure. So our function here would have to be part of a greater context / combined with similar functions into one module, all sharing one private module state. Where custom events are possible I would often replace single functions with custom events. So it depends what kind of function we're talking about here. 
Nice! I added your library to our catalog. You can see the entry [here](http://jster.net/library/babelext). :)
You could assign it to an attribute of the element for comparison later: body.setAttribute('data-value',"&amp;amp;amp;#34;"); alert(body.getAttribute('data-value')); Edit: sorry, Alienblue isn't letting me show the ampersand correctly.
trying on chrome it looks like the transitions are good but the jerkiness comes from setting the background image on the div. Perhaps try using an image element or creating the next div offpage and adding it on rather than loading the background image directly on the element
Ah yes, that was something else that I needed to look into, thanks for reminding me! Chrome has a flicker in between frames, which I think is due to an [image caching bug](http://stackoverflow.com/questions/13316821/pause-between-div-background-switching-with-chrome) and could apparently be avoided by setting the max-age in the header (which I can't do with the lorempixel.com images). Would the above suggestions work around that? Actually, it seems that the transition is much smoother in Chrome as well, but just looks pretty choppy in Firefox for some reason.
Why the IIFE? You're not hiding anything that's not going to be exposed -- the intermediate `t` is being returned. This seems like a superfluous use of an IIFE.
Neat stuff. I wrote an implementation of the Game of Life in WPF some time back. Time to upgrade to a new concept!
I wrote something like this as well (not open sourced), so can definitely see the value in it. The idea was to remove as many strings as possible and to try to enforce a best practice of not querying the DOM over and over again for the same elements.
You can create an element and set its innerHTML to use for comparison later. var body=document.getElementById("body") , compare = document.createElement('div') compare.innerHTML = body.innerHTML = "&amp;#34;"; alert(body.innerHTML==compare.innerHTML); 
I love this thing.
I salute you! I'm studying very hard and cannot wait to land my first job doing this. I know it will be difficult to get my foot in the door, but a door will open. Did you begin playing with javascript first and then backbone.js? I just looked up backbone and it seems like a better approach to javascript. Would you recommend begin comfortable with Javascript before learning backbone? Thank you so much Krakov! Oh, was that 80K or 80hr?
Someone tell me what I'm looking at...
Sure, it's a 3D version of this: http://en.wikipedia.org/wiki/Conway's_Game_of_Life Basically, cells are born or continue to live if they are not too crowded and not too lonely. Otherwise, they die. (Conway was quite brutal to artificial life forms.)
This doesn't use a database. It does use localStorage to save what you've entered into the cart, but otherwise it's pure HTML/jQuery.
that broke my back button… maybe push a history after a slider hase not been moved for 1 or 2 seconds?
Can I ask where you get this from? The item inventory has to come from somewhere, are you saying it is just all hard coded into static HTML files? That seems very bizarre. Are you sure it's not just that the website is *only* referring to the code for the shopping cart side of things.
You should not be doing any comparisons based off data coming from text inside the DOM. DOM elements are not variables that are used to store data, or state. DOM elements are for display only, and using them to test for state and alter program function will lead to problems such as what you are seeing here. Store the state in a javascript variable, not in DOM elements. If you really insist on storing state data with a DOM element, then you should be using setAttribute and getAttribute on the DOM element, as these won't change the values you are setting. It's still not better than using a javascript variable to control program flow. 
Yes. It's hardcoded static HTML for novice programmers. You could easily pull the HTML out of a database using JSP/PHP/Ruby, and the library would play along fine. It uses class="item_*" to get cart properties, which you could dump out and send back to your database before sending to checkout.
Ok I stand corrected, but it still seems a very strange way of doing things if the intended usage is as you say. edit: Hang on, so what happens when someone buys something. Email the retailer?
Completely unrelated: is anyone else completely fed up with the hipster tld .io yet? I'm disappointed that github (which otherwise mostly does things properly and excellently) would base their hosting services on such a short sighted and meaningless thing just because it's trendy right now.
Workspace and snippets are going to save me a whole lot of time
Yeah, it's a cool little algo and simple to implement too. I wrote [a version](https://github.com/elovalo/elovalo/blob/master/src/effects/game_of_life.c) for a LED cube (8x8x8) in C. I also have a web based prototype IDE for it (uses JavaScript) but that's another story. :)
It's true .io is hipstery and expensive even. I'm curious to hear what you would prefer instead. I think the decision to use .io for GitHub Pages had something to do with security. More details at [their blog](https://github.com/blog/1452-new-github-pages-domain-github-io).
You might enjoy Michael Abrash's discussion of this topic: http://www.phatcode.net/res/224/files/html/ch17/17-01.html
Nice! Some very clever tricks there. Thanks for sharing. We were very constrained by memory on that LED cube thinger so memory based optimizations were out of order. Our CPU (AVR) ran at mere 8 MHz and it was enough to run our small voxel display. If you are interested, you can see Conway in action [here](http://youtu.be/DIwVZDG_i2M?t=52s).
Sweet! Here, I tried to replicate your Conway in mine. Edit: here's a proper link: http://wwwtyro.github.io/conway3d.js/#{"foreground":[255,0,0],"background":[0,0,0],"resolution":1,"opacity":1,"size":1,"speed":20,"lonely":4,"crowded":14,"simsize":8} You may need to reset the simulation a couple times to find a starting point that does not immediately die.
Thanks; I'll try to fit this in with my url-sharing fix.
Ooh, good call there.
Yeah. Looks about right. :) Maybe you could hook into some link shortening service to solve the link problem. Or alternatively you might need to invent some packed format which you can then encode. Probably too much work for something simple like this, though.
In this particular case, you can change it to `alert(body.innerHTML=='"'); // true` because the $#...; code gets converted to the corresponding character.
It's not recognizable as a cellular automata since it really just looks like a generated cloud. Could you add an option so that we could look at the actual voxels as bunches of cubes, in addition to the cloud rendering?
[from mdn](https://developer.mozilla.org/en-US/docs/JavaScript/Guide/Working_with_Objects?redirectlocale=en-US&amp;redirectslug=Core_JavaScript_1.5_Guide%2FWorking_with_Objects#Defining_getters_and_setters) basically, its the syntax for defining getters/setters inline in an object literal. EDIT: in case you aren't familiar with them, getters/setters are functions that are called when a property is accessed (getter) or set (setter). so foo = { _a : 5, get a() { return this._a * 2; }, set a(x) { this._a = x / 2; } } console.log(foo.a); // 10 foo.a = 100; // sets foo._a to 50 console.log(foo.a); // 100 hopefully that makes sense 
you beat me to self promotion, how awesome :-D
I wouldn't call it short-sighted, unlike say .ly which screwed some people over .io is a first world democracy controled domain without much of a population , British Indian Ocean Territory, which according to wikipedia has a population of about 3000. 
Thanks for the reply. That seems like a good way to go, and it'll involve me learning about CSS3 transitions at the same time, which is a bonus. I'd like this to be usable on IE8–9 as well though, so perhaps I'll have to use a jQuery fallback for those as well...
It's probably also worth checking out [Paul Irish's video on workflow](https://www.youtube.com/watch?v=f7AU2Ozu8eo); he touches on some Chrome dev tool features there, too.
Ah, well then it seems like his fun "do no evil" clause ruined the fun for everyone else. 
Coming from Backbone I found Angular to be quite useful, but I see why some developers see it very difficult to reason for Angular, Ember or any other framework on the frontend that simulates backend functionality. The simplest answer to your question is to create seamless, one page applications without the overhead of creating your own databinding, which you would need to do in Backbone. Angular makes this easier for you by providing these build in API's without any additional development on your end; such as creating a render method that triggers when a user saves data. Traditionally if you wanted to use Rails, Django or CakePHP to create your rendered pages you would serve up static HTML which would then be appended to the DOM. Nowadays the backend servers us data, such as JSON which we then interpret and render in the DOM. This is much easier now with API based development which allows us to send and retrieve data easily - thats where Angular comes in. So the more complex answer is that you would use Rails as your middleware which would communicate to your APIs and handle the communication between the frontend and backend. Hopefully this has helped :)
Wow, does this guy sleep? Addy is involved in just about every major JS MV* framework out there.
That's really cool. Nice work!
Yes, I understand what would the role of Rails be in that scenario. However, why would I prefer doing that instead of just using Rails views?
Backbone is a way of organizing your javascript. You need to know it before you can organize it. :) Do you already know html and css? Start messing around with the DOM. Play around with jquery and start by making some sort of interactive webpage. Here's an idea - You have an &lt;input type="text"/&gt; and a submit button (should be made out of a &lt;a&gt; for simplicity). Every time you click on the button, whatever you have in the input is added to an &lt;ol&gt;. Clicking on the **second** letter of a word in the list adds 'ing' to the end of the string. If this seems like it's too much, do something simpler. Remember - no one starts out building full-on webapps. Gotta cut your teeth on some static pages and build on top of it.
Single page web applications are not suitable for everything, but there definitely are use cases where it can make sense. For example: say you have an account area for your website/service. Instead of forcing the user to "refresh" the page to switch between say the profile information and the billing information views, the new view would simply be rendered on the client side which eliminates the need to "refresh" the page. This makes it *feel* more like a desktop application and less like a static webpage. It can also save on bandwidth usage as resources (css,js,html) do not need to be re-downloaded each and every time (which they should be cached anyways) Look at the AWS console. That is a very fat client side javascript web app and it's only really loaded once. You are not "refreshing" the page to create new instances and apply certain settings. Everything is done via get,set,post,update to the backend api endpoints.