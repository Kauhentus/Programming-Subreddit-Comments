Great job listening to feedback!
Well, we used the bootstrap-sass variant before it was official. The whole repo got nuked and there was no way to do a 'simple' checkout and recompile anymore. We spend a nice couple of hours rewriting all the includes and extends. That was fun. For another project 4 years ago (when composer was new), we had dependencies on whole repositories that went missing. We also had a few dependencies to zip packages from github since those version numbers were incompatible. Guess what happens when they remove those zip's. Made us have copy's of all our dependencies for a couple of years. Never had this problem any more in the last years (except the story above)
That database makes me cringe.
Thanks! I actually thought of that way but I wasn't sure of how to implement it. The fact is that I wanted to have a page that showed the changes made to a POI (like on github/wikipedia). At this point, is there any good explanation of how should I save just the changes? Saving a change for something like the GPS coordinates or the addition/removal of a tag is easy: I just have to change it on the main table and then record the change in the history. However, for text (I have two columns that hold something around 10K characters maximum) how should I work? More precisely: * How do I save only the changed parts? * What if I wanted to watch from the history my first version that now has been modified several times? (obviously if a new POI is added I save in the history a complete copy of the data)
Using magical integer values is no that good either, even for internal state. When looking at the data, you see a bunch of numbers with no meaning until you look at the code. There is no table to lookup a value's description. Querying database like this ad hoc, or from another application is a pain, comments come to mind to remedy this. In MySQL you can use enum type (CREATE TYPE in postgresql), still efficient but meaningful for everyone looking directly at the db. Then you define your constants as PAYSTATUS_PENDING = 'pending', much better then 1. 
[Pig latin](https://en.wikipedia.org/wiki/Pig_Latin) converter/translator.
We ask for code examples from applicants, too. So from an employer's perspective I would prefer *real* code and not something you whipped up as an example to be shown for the job application only. If you have nothing to show from previous projects I would recommend contributing to an OpenSource project. Pick something you like/use, check their issue tracker and send a pull request to fix an open issue. Respond to the repo owner's comments and fix your PR until it's good enough to be applied. As an employer I'd love to see real a real PR with interaction with the project owners. Tells me so much more about how the candidate approaches problems, codes and interacts with team members.
&gt; How do I save only the changed parts? I would always store the changed columns completely. You could diff the old and the new column value but I think that would overdo it a bit. &gt; What if I wanted to watch from the history my first version that now has been modified several times? Select the history entry ordered by ID descending (or by change date descending), then you have the first version.
From my experience, this usually means 'show us a piece of code you write that you're proud of'.
(╯ಠ_ಠ）╯︵ ┻━┻
Yep, this is a good point. Some people don't like (or are not aware of) enums and so you see a lot of "magic integers" being used in the wild, which is where I started at least "documenting" them with class constants. But for something new, the enum approach here is nicer. 
i always ask for "Show me a piece of code that you are proud of" even if they dont ask for that specifically - thats exactly what you should deliver
I missed the joke.
Welcome to the world of maintenance programming. Much like "cleaning the fryer" or "unblocking the toilet", this is where juniors come to have their optimism dashed and standards lowered. I jest. Sorta. You're just going to have to read it. Using a tool like [XDebug](http://xdebug.org) can be very helpful for actually following the spaghetti, one painful line at a time.
That's... Like an excercise in programming I made on my calculator in high-school. Why not implement some algorith, luhn, Levenstein, A*, Dijkstra or some sort of sorting algorithm?
Well it wouldn't be random companies, it would be the single user who required access. And presumably they'd be under some legal paperwork type restrictions from giving away the technology or telling anyone about it. sftp to the test server is a familiar idea. But this whole question doesn't really make sense to me.
I would find a small, but non-trivial problem/solution, that relates to the employer's industry. Make sure the problem you are solving has a good variance in solutions. Example, if they are a fulfillment company, write a really simple supply chain management tool. It depends on the position too. If you're a backend developer, work up a database design with some nicely designed API endpoints and reasonably decoupled business logic. If you're full-stack, demonstrate the whole stack. The idea is to make yourself shine with how well you can work a certain topic. General advice about demonstration projects: In my experience, employers have handed me a problem and said tear into it. Others have given me a whiteboard problem during an in-person interview. They aren't really looking for mind-blowing elegance in solving the problem with the most optimized algorithm that would even make Donald Knuth weep. Elegant code is a plus, but they're more so evaluating your thought process for development. If you want the job, **write tests**. You don't have to use PHPUnit, Behat, Codeception, or any testing framework. Just demonstrate the bare concept that you understand testing and how to test with good coverage. Show that you know the current state of the PHP development community, Composer, Symfony, Laravel, Doctrine, PHP7, PSRs, etc. Use PHPDoc commenting, and document your code well. If you're like me, I typically work within a framework, and thus I pull in an ORM/ActiveRecord implementation. Quite obviously, this abstracts the SQL implementation and actual queries. However, I like to leave comments with the SQL I intend before I actually write my calls to the ORM. This lends some good visibility to your understanding of SQL and database design.
Great, so, let's say you want to make a living by making APIs, what language would you choose?
APIs for what? For bank software or anything else where money is on the line? Where the whole system must be perfectly atomic? Erlang likely. It's a well established tool for such things. I personally dislike Java but I can't deny it has a lot going for it in that area. In the future perhaps Rust. Add in, depending on requirements, additional tools like RabbitMQ and Kafka for a stable queueing system and possibly segregated docker/jail instances for different API users behind a router if a given API user shouldn't have any perms on the data generated and consumed by other API users. Simple CRUD type APIs which can afford the occasional minor break of atomicity or what have you, but still requires massive scalability? Where security isnt overwhelmingly important? Something like IMDB? Erlang and Java, possibly Rust are still great bets for that, but without architecture complications like CQRS, segregated instances, etc. Such an API not expected to get more than maybe a few thousand requests a day? Depends on what systems I might need to integrate with or the company already has. What developers in the company know. Django, Laravel and RoR can all get a simple enough API out the door well enough. 
I was in your shoes not long ago. I sugges that you familiarize yourself with the grep command, it works wonders when navigating large amounts of code. [Blackfire](https://blackfire.io) can help you understand what happens on a per request basis Best of luck!
Right, if the job requires the production of brand new algorithms, coded with specific efficiencies and footprints in mind - great. But: a: If this was a job, you would not be doing it under test conditions. You would spend weeks planning out the memory requirements, object interactions, and testing real-world examples for speed and accuracy. You wouldn't be working quickly from memory, and you would certainly have Google to help you through the tricky stuff - which is what you're interviewing for. or b: The interviewer is lazy, and is using a pointless test to weed out people because they don't want to assess the person, just a tick box on a sheet. The 30 mins spent waiting for the applicant to re-invent the wheel could be spent have conversation about design ideas, motivations, and bouncing hypothetical situations back and forth to see if they're on the same page, hopelessly lost, or have new insights they can bring to the company/ team. Either way, this is wasted time, and doesn't provide you with information that actually interviewing the applicant wouldn't provide in a more human and real-world way. 
You're using a PHP enum-like convention instead of a table. That's perfectly fine and a common way to do things. In fact, I'd say it should be the default approach. The less tables you need to query and think about, the better. :-) I always start with PHP-only objects, and add tables only when the options need to be dynamic, persistent, or too numerous to keep in PHP at once (as in hundreds or thousands, or more). So good job on being pragmatic. I think the parent commenter didn't read your post fully before commenting.
That really wouldn't solve an in-app dependency conflict though. It'd address system package A needs C2.7 and system package B needs C3.4, but then only if they're in separate processes.
Composer update should write the lock file for you though? 
SFTP? That's an improvement from the company I used to work at. They used FTP and made all changes directly to the production server. There was no such thing as a test server.
well, it was a hands-off remark from our host that lead my attention to the staging-server. Didn't help much, except I didn't take our entire site down quite as often. No version control and typo3 is a match made i hell... Luckily I only have 3 days left at that place.
I would guess it's implicitly treating your HTML as a paragraph, thus expanding it to fill the width of the page (as it does on screen. Maybe generate the whole report as HTML that looks correct in the browser, and convert the final output to a PDF? Or maybe try to convince someone that there's no reason for this to be a PDF in the first place?
npm is indeed better but it's because of how Node behaves, not because of some npm-inherent architecture
I do this but using [this enum library] (https://github.com/marc-mabe/php-enum/blob/master/README.md). Typically the enums correspond to an actual enum in the database instead of another table. 
Turns out, it's not so much the row count but the column count, because to ORDER BY RAND() the whole result gets copied to a temporary table. Fetching just the IDs with ORDER BY RAND() is actually faster and more memory efficient than fetching all ids and using PHP to choose them randomly. But a second query for the actual data really pays off. This is the promised blog post, with benchmarks on a Magento catalog between 100 and 2,000,000 products: http://www.schmengler-se.de/en/2015/09/show-random-products-in-magento-you-are-doing-it-wrong/
I'd probably use the short version, but with a short comment above it - `// constrain $target between 0 and MAX_TARGET_VALUE`. But I'd absolutely not fault anyone for typing out the longer version. It's normally conditionals that have too many parens or `||`/`&amp;&amp;` that get really unpleasant. How many is too many? Hard to say exactly; it's usually a "know it when you see it" thing. More than two usually feels like too many, unless it's a basic coalesce-type thing (`if ($this-&gt;x || $this-&gt;y || $this-&gt;z || $this-&gt;a)`)
This particular approach may be overkill, but the important point is to put it behind a function call and don't worry about it. The function name documents what the code is doing, at which point the short or long version doesn't really matter. function clampMin($target,$min) { if($target &lt; $min) return $min; return $target; } function clampMax($target,$max) { if($target &gt; $max) return $max; return $target; } function clamp($target, $min,$max) { $target = clampMin($target,$min); return clampMax($target,$max); } $target = clamp($target, 0,self::MAX_TARGET_VALUE); 
I think the key thing here is adding the comment after it
The short version is such a common pattern that I don't think it can be called clever.
Implicit assignments in if statements are the devil. I mean, when they talk about the devil being in the details - that's the devil they mean.
I think that $target = min(max(0, $target), self::MAX_TARGET_VALUE)); is perfectly readable. $target must always be between 0 and MAX_TARGET_VALUE
I think it's fine, though here is another way that would work using a ternary: $target = $target &lt; 0 ? 0 : min($target, self::MAX_TARGET_VALUE); 
Honestly, I'm surprised PHP doesn't already included a standard library function to do this.
That level of engagement section. 10/10 m8. Haven't legitimately spat my coffee in too long.
Would I be a buzz-kill if I said both are equally readable? I'd personally write the top line in application code where I value terseness of expression over performance (because the two function calls are a touch slower than a direct comparison and assignment). I'd write the bottom line in reusable library code where I value performance over terseness. I think *in this particular example* the above line seems less understandable to you because you've not encountered it enough. Look, your co-worker even carefully laid it out to match the order in which you'd write the expression in math: min(max(0, $target), self::MAX_TARGET_VALUE)) 0 &lt;= $target &lt;= self::MAX_TARGET_VALUE He could've written it worse: min(self::MAX_TARGET_VALUE, max(0, $target)) When you see min/max combos, 90% of the cases someone is trying to cap a value within a range on top and bottom.
In short... Yes it is readable
"Clamp" is the usual name for exactly this operation. You "clamp" a value to a range. Bound and constrain would also work, but these names are a bit broader.
Your first two functions reimplement min() and max(), except with reversed names.
&gt; Meh, if you're going to go that route there's no need (imo) to have multiple functions...... That code is not analogous, clampMin and clampMax can be used on their own, your function supports no such thing. Below is the adjustment you'd be making assuming you choose to express the min/max requirement using null. function clamp($value, $minimum, $maximum) { if ($maximum!= null &amp;&amp; $value &gt; $maximum) { return $maximum; } if ($minimum!= null &amp;&amp; $value &lt; $minimum) { return $minimum; } return $value; } and the usages $target = clamp($inputTarget, 0, self::MAX_TARGET_VALUE); $target2 = clamp($inputTarget, 0, null); $target3 = clamp($inputTarget, null, self::MAX_TARGET_VALUE); vs $target = clamp($inputTarget, 0, self::MAX_TARGET_VALUE); $target2 = clampMin($inputTarget, 0); $target3 = clampMax($inputTarget, self::MAX_TARGET_VALUE); The reason I said it *may* be overkill is because the clampMin/clampMax functionality may not be needed for your project. But if they are, then you should prefer the separate functions for the reason I gave originally, it gives you context to the code.
Having the following code: private function constrainValueBetweenLimits(int $value, int $min, int $max): int { return min(max($min, $value), $max)); } Would you feel the need to add an explanation?
Sounds like we need more blogs on this subject.
Eh, I named it after Sodium Chloride crystals, since it builds atop libsodium.
The name 'clamp' has well understood semantics and is more than sufficient.
&gt; The ASP.NET stack is made to be convenient first, and well engineered a distant second (they're doing serious work to improve architecture &amp; performance in the new release, but it's a new direction for them). It's important to differentiate between ASP.Net and Web Controls. ASP.Net has always been fairly well engineered, but then they attempted to bring desktop semantics to the web via web controls...
yeah, I had someone else point that out. I wasn't aware of those functions :)
I think you mean Web Forms. Those are long obsolete. The modern stack is still full of problems.
So you're basically inflexible on a principle. Ok.
Derp, good call. lol...
The consensus seems to be that: i) Is doesn't add anything to the language, as the min/max functions can be put together to make it in userland. ii) No one wants to try and write the documentation for what happens when you do `clamp(-2, "8bananas", new StdClass());`
Write readable code, not clever code.
Personally, it takes a second or two to think about it but the short version is nice and I would leave it in. I've looked at far more confusing code, even today, and I'm not going to class this as 'difficult' to understand. 
What's with the upvotes? First of all, isGreaterThanTargetValue does not make your code more clean or readable than just using the "&gt;" operator in the setter method. Secondly, return ($target &gt; self::MAX_TARGET_VALUE ? true : false); should be written like this if you want readability: return $target &gt; self::MAX_TARGET_VALUE; 
No more or less than with the inlined version.
&gt; Please do not frequently submit links to your own content, exclusively. It's likely to get spam-filtered. 
You are right in this example. I may have went over board on being to verbose. Although, you can pretty much read it like a book and figure out exactly what is going on. 
Sometimes (not always) I will write a method using lots of if/elseif/else then write my unit tests to cover all the decision trees (Yes I know this is not TDD) then I will go back and refactor the method to not use any elseif or elses and instead just return early when possible. My unit tests help me not make any stupid mistakes that way.
my solution: ($target &gt; self::MAX_TARGET_VALUE) &amp;&amp; ($target = self::MAX_TARGET_VALUE) ($target &lt; 0) &amp;&amp; ($target = 0) 
This is the best example since it "communicates intent" to anyone reading the code. I.e. don't try to be clever, try to be clear. PS: When encapsulating it in a method, you can also write tests for it. Nice side effect.
1) It would obviously add something or we wouldn't even have this whole thread, right? 2) I'd hope someone would have enough forethought to enforce all parameters being numbers, but I also fall into the STH fan category.
Yeah, my brain went straight to 'bound' and 'constrain' works just as easily. Using 'clamp' seems less obvious, but it seems to be the common term, at least in graphics programming.
By adding the foreign key in the form you are allowing the user to influence record relationships in your database. You should book keep it on the server by some method of your choice where you have complete control. Can't you imagine what would happen if someone polluted that value? Let's say it's a user profile record and the foreign key is user id. Someone could change the user id and the profile would be associated to the wrong user. etc. etc.
What about a 3rd option: writing a separate threshold function, with `value`,`min`, and `max` as params? that's short, self-documenting, and DRY.
The answer to that is obvious to anyone who puts thought into it, and I don't feel the need to defend myself to you, sorry.
 clamp(-2, "8bananas", new StdClass()); would give undefined behaviour as per: - http://php.net/manual/en/language.types.integer.php#language.types.integer.casting However, if we assume StdClass to cast to 0, we would get clamp(-2, 8, 0) which should return 0, as per - http://php.net/manual/en/language.types.type-juggling.php - http://php.net/manual/en/types.comparisons.php
This is a joke right? Looks like Java. 
Oh, no need to convince me. I'd be perfectly happy if most of the core functions were pulled out. But, thing being what they are, I'm surprised that a function like this was never added.
[removed]
I'm just confused that you started providing solutions when the language already included faster native functions covering 2/3 of your suggested solution. Thanks for helping, but maybe at least knowing the core functions beforehand would be helpful and allow you to provide even better suggestions?
This is not a PHP question, unless you somehow know the site in question is written in PHP and you are asking how PHP handles file input elements and the implications of client browser security? So long as you didn't install anything to perform the upload then no the transfer did not open a demon portal within your local file tree, but you must assume any information within the document in question is now known to others, so any data with easily recognizable structure like emails, CC numbers, phone numbers, username/PW pairs, URLs, keys, etc are all now known by your enemy so act accordingly. Try to stay away from free services unless you know the data is benign, instead build up a test suite of tools, parsers and tests you can take with you. Good luck.
Personally I think that's more confusing. You're doing two similar things in two different ways.
"Clean Code" was all examples in Java.
I believe FPDF utilizes TCPDF, correct? I ran into difficulty with certain PNGs of varying alph-channel configurations, color spaces, etc and at one point tracked it down to a fault betwixt libpng and either GD or ImageMagick - I forget which one we were using at the time. Anyway, glad to bear you got it squared.
I agree with your point about making a function and using a DRY coding style, but then the question becomes what code do you put in the function: Option 1 or Option 2, putting us right back where we started. The question OP asked is "[is] the following easily understandable?", not "Is there a better solution or way to write this?". I think we could argue until the end of time about the "best" way to write this (and there are a lot of good solutions in this thread), but all arguments will boil down to personal opinions about what solution is the most readable and not what solution is the most clever, its this distinction that will always make me choose option 2 over option 1 if forced to choose between only these two solutions.
I agree with you in principle completely, and beside the performance and security benefits I find the concept clean and resolute though like most things personal experience has altered my views. My misjudgment involved scalability with respect to class extension and massive restructuring, something foreign keys would have been especially adept at handling. I forget which version, maybe 5.6, where class constant arrays became legal. I used the ability the first chance I got, simple, fast, done. I love rdbs and what they offer, but I agree with you that not every data point needs its own table.
The real issue in my experience is not understanding but maintaining and adapting. That block of code has to be rewritten if your goal is anything other than setting that value. If you want to, in a bit contrived instance, also log that a greater than max value was found (for some purpose) then you have to rewrite the entire block of code and that rewrite of even one line can introduce a bug in the old logic.
if you think that's too clever to understand
I also agree with this. Let SSL secure your data in transit, and validate the operation based on the logic of the authentication/authorization depending on the complexity of your privilege architecture. I'm just now learning how to NOT over complicate things in my work, so your comment resonated with me.
My suggestion: if ($target &lt; self::MIN_TARGET_VALUE) { $target = self::MIN_TARGET_VALUE; } else if (self::MAX_TARGET_VALUE &lt; $target) { $target = self::MAX_TARGET_VALUE; } * Write just one statement per line * The minimum value should also be a constant, regardless if it's a zero * When comparing values, I learned from Code Complete to write smaller values on the left side. I feel it reads more easily (we are culturally used to higher values being on the right) * As a reading microoptimization I would write the minimum guard first
I assumed the longer one since its more clear. Should have said that.
tl;dr: good concept, don't know all the details I seem to remember commenting on something along those lines a couple months back (may have been another userland lib, not sure) and had pretty similar feedback. The, uh, meta-feedback I received was that the names I suggested were too use-case specific; accurate, but missing the point. PDO is a decent (if not slightly kludgey) layer that's held up well for something close to 10 years. It could do better, but manages to do well for how inconsistent the stuff it's trying to abstract is. If PCO comes to fruition (I can't find any RFCs), I hope it manages to do a bit better off the bat. However it gets done, there absolutely should be *something* in PHP core that gets you not-completely-broken crypto with a simple API. Much like with `password_hash` and friends, the raw functionality has been there for ages, but easier to get wrong in user-land than it was to get right. The documentation will be just as important as the API, especially if it ships as part of PHP Core. Lots of "**do not use this for storing user passwords**" and the like.
&gt; $target = clamp(0,self::MAX_TARGET_VALUE); Except that this function call is missing an argument.
This. I don't think the single liner is too complicated to understand, but this is a perfect place for a simple comment.
I'm still wondering why the value needs to be constrained.
Even though it's micro-optimization, I'd like to know which is FASTER? I'd suspect the 4-line version. Individually, little slowdowns don't really matter, but do it enough times, or in oft-called code... 
I said it was readable, not correct ;)
There are tons of reasons to constrain values, but one of the more common ones might be positioning things dynamically on the viewport without going outside of it. So, for example, you might want to show a tooltip when the user clicks on an element, and you want that tooltip positioned 10 pixels above the element. Well, that's fine if the element is near the middle of the viewport, but what happens when it's at the extreme top or bottom of it? In that case, you want to position the tooltip at some dynamically calculated Y coordinate relative to the element, but you also want to clamp that calculated value to the viewport size so it's never invisible or cut off: // calculate the tooltip's Y coordinate... tooltipY = elementY - 10 - tooltipHeight; // but constrain it to the viewport, because we don't want positions outside the viewable area... tooltipY = min(viewportHeight, max(0, tooltipY)); // or, alternatively written to be more readable... tooltipY = clamp(tooltipY, 0, viewportHeight);
It should be in a function. Then, it doesn't matter as much whether you put the min value in a constant, unless it is actually used in another place in your class. Your last two points are very good - I didn't realize this at first, but I try to keep track if all cases are covered, and that's harder to do if you don't start at the lowest value.
Some recent discussion on performance of this release I found here: https://laracasts.com/discuss/channels/general-discussion/is-it-just-me-or-is-php-7-slow
Mostly backing up what others have said. Keeping your variables `private` or `protected` gives you *control* on if/when/how the app or user can make changes to the objects variables. - You can easily create read-only data - just don't have a set method. - You can validate the data before assigning the variable. This prevents the user from doing stupid things. - You can trigger other events or functions when a variable is set. - If the API changes for whatever reason, you can change the get method so it obtains the data some other way so that backwards compatibility is maintained. - You can do nifty things, like a "write-once" settings object I have, where values can be supplied in the constructor OR set later, but once set they cannot be changed. 
The ternary "?" plus "true : false" is a beginner mistake and not a matter of style or readability.
The shorter one is both clearer and less repetitive, imo. I only need to understand a few function arguments vs the conditionals and both sides of the assignments.
Also start small and communicate with the project members. 
I know it's not directly related, but here's a super cool ruby way to clamp a value. x = [a, x, b].sort[1] I've written SO many "clever" solutions to problems in ruby and python, I'd absolutely flood the comment section. I regret some things.
What you call ListView is exactly what my Loader interface is. ArrayAcces Loader is one of its implementations, and also the ORM Query Loader. As for why paginate and paginateOrm dont have a common interface that is simple: the methods have different signatures. 
TIL camp .... thank you. (I've never heard of 'clamp' before)
As apps get larger and older, the benefits become clearer. Extending and changing systems becomes increasingly difficult in procedural code. For an ecample, try looking at older php commerce and blogging platforms, and debugging and customising them. Following include paths endlessly, adding more and more params to functions, etc.
oop usually brings the benefits of testability and better maintainability. try to setup automated tests (e. g. unit tests) for your scripts. try to write a huge website with a lot of procedual scripts and come back a year later to change something. if you put your code into small and independent pieces, you have testable and maintainable code. also, your code could be reused more easily by someone else. look at all the popular packages on [packagist.org](https://packagist.org/), they're all namespaced, put into classes.
Ok, so essentially Builder is your internal dependency factory, and you expose some of it it through a public module class. That's a great architecture for complex components where one entry point is preferred. Like applications and services, nice! But I think you realize yourself that it's not a great approach for granular libraries. Maybe you can have a separate convention for those? I'd consider it. Even if you insist to have one public class, I don't think "pre-mixing" ready recipes for your users is a good idea, you can expose more granular methods to the objects: $pgMod = new \PHPPixie\PaginatorModule(); $arrayLoader = $pgMod-&gt;createArrayLoader($array); $ormLoader = $pgMod-&gt;createOrmLoader($orm, $query); $arrayPager = $pgMod-&gt;createPager($arrayLoader); $ormPager = $pgMod-&gt;createPager($ormLoader); $cachedOrmLoader = $pgMod-&gt;createCachingPager($ormPager, new FileCache($path)); $cachedFilteredOrmLoader = $pgMod-&gt;createFilteringPager($cachedOrmLoader, function ($item) { $item['fullName'] = $item['firstName'] + ' ' + $item['lastName']; return $item; }); Basically, if you want to put your classes behind a factory, that's fine, but it doesn't mean you need to need to limit the flexibility of your users to compose objects together. You can't capture *all possible pager compositions* as pre-composed builders. They grow in a geometric fashion. BTW, part of the confusion I had when exploring your library is the unclear or misleading naming of the various classes. I realize it's probably not a problem for PHPixie users, but I mean developers in general. The Builder pattern is a stateful configurator which builds *one type* of objects, like this: $ball = $builder -&gt;setColor('red') -&gt;setSize('big') -&gt;addDecal('i-heart-donuts.jpg') -&gt;buildBall(); In your case you have a Factory not a Builder, so if you have named this class something like DependencyFactory or "DependencyContainer" or something, it'd be immediately obvious what's its role is. Also I'd suggest you introduce an internal factory only when it's needed. I do have a similar structure for my modules, but there's no need to go full "enterprisey" from the very start. You can *always* introduce an internal factory later on without changing the public class.
&gt; I would actually prefer having your $pgMod-&gt;createOrmLoader($orm, $query);, but sadly I can't. the reason being such a PaginationModule would have to have ORM as its dependency, this way including it in a project where Pixie ORM is not used would result in a lot of useless dependencies installed. PHP has great support for "soft" or optional dependencies, because a class will be loaded only when it's needed at runtime (which is when you instantiate it, not when you typehint for it, not even when the method with the typehint is *called*). So I feel you should be free to create adapters and classes which rely on optional dependencies and not put them in your composer.json. Additionally, ideally, Loader classes can be their own library, because they can be used for more than pagers. Or you can choose to have OrmLoader be a part of your Orm package (which I think is the case, right?), which means that it'd be: $ormMod = new \PHPPixie\OrmModule(..., ...); $ormLoader = $ormModule-&gt;getLoader($query); $pgMod = new \PHPPixie\PaginatorModule(); $ormPager = $pgMod-&gt;createPager($ormLoader); This probably suggests the question "where is the Loader interface defined". I'd propose either in a simple one-file package as hard dependency (in Composer), or packaged with related general-purpose interfaces, like Laravel's "Contracts" namespace has. BTW, Loader also could be named a bit better IMHO. :-) I mean... there are many ways to "load" things, it's not clear at all from the name that it has list semantics for a read-only list collection. I'd propose ListView, ListReader... something like that.
How can you have a private method with a static class? Wouldn't the method only be available to an instantiation of that class, which by definition rules out static methods.
I was actully just following this: http://php-and-symfony.matthiasnoback.nl/2014/04/theres-no-such-thing-as-an-optional-dependency/ The problem is that tests will also not run if the optional deps are not met =\
It's available to other static methods as well.
&gt;How can you have a private method with a static class? Pretty much exactly how you'd expect, just swap `public` for a different access-modifier. &gt; Wouldn't the method only be available to an instantiation of that class, which by definition rules out static methods. No: It surprises a lot of people to find out that `private` does **not** mean "only the same instance can access this". It's actually less restrictive, and means "only code defined somewhere on the same *class* can access this". A practical example of this is using static "named constructor" method to call a non-public `__construct()`. Another good example is if you make two instances of the same class, one can freely see and change the "private" properties of the other, which is especially useful for "compare yourself to this other one" functionality.
Sure the object is technically destroyed when the script ends, but it comes back on the other side when you retrieve it from the database later. You can then do special things with an object you can't do with a flat array. For example, say you have a 'User' object with a first and last name, you can add a getFullName method to quickly get their full name rather than concatenating it every time or having the function located somewhere else. Your request data needs to be stored in an intermediate location at some point between the request and the database (unless you're putting post data straight in :O). So that intermediate location might as well be a nicely structured object rather than a free-for-all array.
Encapsulation leads to reusability and single responsibility, which let's you delegate and use design patterns and make composable decoupled code, then you could start unit testing new units. The "SOLID" principles really open up a whole new level. MVC pattern and an ORM help keep your code clean and readable. I never truly saw the benefit until after I had learnt it all, sounds strange i know. Now having to go back and maintain procedural code is a nightmare. This probably didn't help but.. I'll just say I wish I'd been taught this at school.
I thought about that too, but paginateOrm would need an instance of the Paginate to be built. But Paginate will also nerd an instance of PaginateOrm to wotk as desctibed. So a circular dependency is unavoidable. The only way to have it is adding a paginate::registerPlugin() method that modifues the already built instance. Which up until now I also avoided =\
For the record: Procedural programming is not functional programming.
I agree.
Any reason the first conditional is not a &lt;= so that if $target is equal to self::MIN_TARGET_VALUE then the statement is completed as soon as possible without performing the second check? Just curious
Why did you omit dollar signs? 
You won't really grasp the actual need for OOP until your code base becomes several years old and continues to grow as you've been constantly ask to "change this", "add that", "remove this". During this time you will be happily maintaining without much issue, looking at some of the more advanced development techniques and saying that you don't think you need them. You are doing fine after all. Then at some point you are going to wake up to a 300,000 line procedural, tightly coupled code base. You are gonna realize that simple additions and updates are requiring more and more time to implement because everything is spread out and duplicated in multiple places. This is the day when you come to reddit and make a post that says "PHP OOP - Why you should use it!" Edit: And yes, coming from procedural you may have a difficult time wrapping your head and around how OOP is actually implemented and makes sense. You're gonna be fighting you instincts on this one. 
I see, but then ORM has to depend on Pagination, which I also want to avoid)
It won't depend on Pagination, it'll depend on the Loader package (or Contracts package, however you want to call it).
I would agree that it helps when trying to create unit tests for code that primarily does calculations or manipulations of data. For some, that is an incredibly small portion of the code base. Genuinely asking: Any thoughts on web sites whose main purpose is just to store data in a database and later output it to users without much manipulation? User input validation aside, there really isn't much that I can think of that can be easily unit tested. How do you catch that you used firstName as a form input and first_name when reading POST data in a unit test? How do you test for display issues without loading it in your browser?
Ah, I see your point now. But then I would still have separately a Loader package, a Loader adapter for ORM and a paginator that can be plugged with these adapters.
You would have a separate package for the Loader, yeah. It can literally be one file, though (well, two, with composer.json). I think it's reasonable, especially if you compare it to the alternative (PaginatorOrm + Builder + ...).
I tested, and they didn't get chopped. 
unit tests cannot cover everything. that's not what they're for. that thing you're talking about is end to end testing, or integration testing.
I've used these, and they're not exclusive to one another (you can combine techniques into hybrid approaches), but the thing that's missing from your question is your specific use case. The reason so many algorithms exist for this is that they have different best use cases. :-) So share.
Clean and readable on some projects can be subjective. You understand the structure of MVC now, but you had to learn it at some point. You are at least partially having to learn the structure of the procedural code you are trying to maintain. I don't really think you intended it this way, but I have seen many people substitute "clean" or "easy to understand" with "how I learned to do it". For some smaller projects, adding a framework, ORM, and using MVC can easily triple the size of a code base and spread it out across dozens of files that require a lot more hunting to go through one execution of a script. That's not even counting the extra code you added from the framework that had its own set of bugs. The number of bugs in a code base increases the larger it gets. For some projects, the added structure helps - especially as code size grows. For others, you are trading simplicity for structure and getting increased complexity, code size, and bugs to go with it. Just saying that frameworks aren't the holy grail. Very interesting discussion in this thread. Too many people say use objects and frameworks because it is the "right" way without considering the merits. Too many people think it's too complicated to learn to use objects and frameworks because they don't see the point. I'm betting the answer lies in the middle, and I'm learning from both groups. 
So, what's the use case, anyway. :-)
I really like that you made comparisons of code bases so you can "see" the difference. I don't suppose you've run any software metrics on an "old" and "new" commerce platform or read someone who has? It's easy to have opinions, but you had a unique answer. Thoughts on smaller code bases?
Choosing the default approach for phpixie)
Thanks for the book link !
&gt; So that intermediate location might as well be a nicely structured object rather than a free-for-all array Almost my words exactly, but I'd change it to So that intermediate location might as well be a nicely structured object, which you can add behaviours to independent of the data itself, rather than a free-for-all array. That's the major point of object stuff - combining behaviour with grouped data. Your 'getFullname' example, while trivial, is a good one. Without a 'user' object, you'd be left with having function getFullname($firstname, $lastname) or function getFullname($nameArray) which would (necessarily?) be in global scope (unless you put it in a generic 'functions' class and called it statically) Is OOP the 'best' approach to all problems? Probably not, but there's a good amount of support for building OO code in PHP (both the language and ecosystem) that it's probably the 'best' approach to most problems if you're using PHP for anything non trivial. As PHP progresses, we may see better support for more functional paradigms, and that approach may become more entrenched in various PHP communities/projects. 
Neat packages. I'm particularly interested in the rate limiting package. Do you have any usage examples for it?
Procedural programming is for amateurs, it's good and only good for absolute newbies to learn the basics of coding since starting directly at OOP is harder, not everyone is a computer genius. OOP on the other hand is what it takes for professional programming. If you cannot code in OOP, you aint gonna find a decent job in this industry(unless you work with pure procedural languages like C). In a perfect script, everything is an object. OOP offers encapsulation, inheritance, polymorphism and composition, it helps you build much more extendable, reusable, maintainable, and organized application. The advantages are more evident in larger programs, which are pretty much impossible to work on with procedural style, especially when teamwork is required. With OOP, you usually spend longer time to design and architect your application upfront, but it's worth the efforts as future development will be a lot easier, faster and more enjoyable. For newbies or procedural coders who never got any exposure to OOP, I'd recommend you to work on a project on Github, and learn step by step on why OOP will help you create better software. As your application grows in size and complexity, you will realize how it's difficult or impossible to maintain and extend your application further if its written in procedural style. At this point, you won't ever want to code in procedural php any longer. 
I did. I was all over IRC. Talked about things, what was needed, submitted small pull requests. They got eaten. Found bugs, talked about it on IRC, submitted issues. During digging up details on what the bug was, figured out how to fix those, submitted larger pull requests. They got eaten. Needed an extra feature or two for my own use that I felt was useful for others. Talked about it on IRC, they agreed. Implemented them, submitted pull requests. They got eaten. None of the pull requests I submitted were accepted, and 90% of them were re-submitted by the author with little changes so they showed as HIS contributions. Gave up. 
You could also encapsulate the $target value and the `MAX_TARGET_VALUE` constant in a separate class. class Target { const MAX = 10; const MIN = 0; private $value = 0; public function __construct($value) { $this-&gt;value = $value; } public function value() { $this-&gt;enforceLimits(); return $this-&gt;value; } private function enforceLimits() { if ($this-&gt;value &gt; self::MAX) $this-&gt;value = self::MAX; if ($this-&gt;value &lt; self::MIN) $this-&gt;value = self::MIN; } } Since I'm not aware of the surrounding context, this example could miss the spot, but you get the idea. Naming the method enforceLimits kind of tells you what happens, which adds up to readability. (See Value Object pattern)
I see $'s where appropriate.
&gt; for example I'm creating a system to manage customers ... the end of the script Just from one this one sentence there is a problem with your scale. You want a "system" for managing customers but you mention only a single script. If you are dealing with a single script, you probably don't need OOP. But if you are dealing with a system, which presumably has many pages and many different elements that something else entirely. And creating classes allow you to organize code and data so that it can be more easily reused across the entire application. This allows you to contain all your customer logic together with your customer data in one place. 
NotORM converts records to arrays (going in and out). You might like it.
I've made the same journey as you describe - thinking it was too many files, too spread out and basically confusing, but then I wasn't really taught how to think that way. The way I've come to think of it is, with MVC, I can look at the controller and (if light controller, heavy model has been followed) read a high-level script of what is happening for a given page. - Use the SMS class to send a text - Send this email out - Update the database with the date - Render a template with the date as a variable If I want to see what each bit does you just follow the code down each path. Admittedly get a bit more complicated with Dependency Injection but it's just a case of looking up the file location from another file. With Procedural you can do the same stuff, but usually have to open a 2k line file to do it, and following the logic through can very easily become a nightmare. It's frustrating being able to *feel* the benefit but not really describe it well, becuase everything chains together, you won't be able to even understand design patterns until your classes are decoupled, and you won't understand how to decouple properly without knowing about other OOP stuff like single responsibility and Interface segregation, loosely coupled versus tightly coupled. It's this whole shift in paradigm and a massive info dump to take in, I was lucky that I got good training in almost the right order, and then one day it just clicked. You can use MVC as a pattern you make yourself without a framework, but I don't relish the idea of reinventing the wheel, personal preference though. [This wasn't all technically a reply to you mnk6, I think I just carried on my rambling, sorry! Don't feel obliged to reply :)]
I don't think such a performance microoptimization is worth breaking the logic in the `if` statement.
Weird, I'm using Sync, and it has shown dollars in my test comment. 
Nested sets are really slow for inserts and deletes as all left/right columns in all related rows need to be modified.
he is talking about putting the post_id in the form, not the user id which is perfectly acceptable. obviously the user stuff should be handled by the session but the post_id is fine in the form so long as you do some checking to make sure the user has access to comment on that post and that it exists.
In 2015, using OOP is almost a given at this point. People rarely question the validity of that approach for building modern day software. However, what most tend to forget is that we are not building software, we are building webware. The stateless nature of the web renders "stateful" concepts rather... obsolete [state is a snapshot of the application at a give time]. We manage state over the web by sending ID strings back and forward with every request to pick the state back up from previous calls ($_SESSION is an example of that in PHP). In this web-world of ours, we have borrowed a lot of concepts from stateful software platforms that we don't really need or most often don't even utilize. Take the language C for example. Coding in C is procedural by nature. Heck, PHP is coded in C and uses C-extensions/packages. However, have you asked yourself why we are not using C for web-development directly instead of using PHP that is clearly slower than its mother language while we use C for software all the time? The #1 reason as to why PHP &gt; C for web-developers is that PHP offers us a bundle of abstractions/features that are easier to implement and understand than C. That same logic applies to Procedural VS OOP. OOP just gives us so many abstractions/features that Procedural can't do (or at least is not good at doing). There is more to application development than just coding. There is maintenance, readability, automated testing, bugfixing, etc. Even if we assume that OOP is a waste of time and procedural is the way to go when it comes to coding, we can easily prove that OOP wins over procedural in all these other areas. So in terms of trade offs, OOP wins by default. Now the next question becomes: is procedural really better than OOP for coding? and the answer is, like it most often should be, it depends. It all depends on the application you are building. The size of it, the intended business use cases, and future plans. I bet you anything that fixing a bug that you found in your procedural code 11 months after you built it is gonna be a bitch compared to fixing that same bug on an OOP code base. It might not make sense to you now, but it will one day. I remember when I used to think like you do now. That is something most self-taught developers like you and I have to go through. Once you face the problems that OOP solves it becomes obvious as to why it is a better coding practice than procedural. 
Alright I'll bite. Name me 1 framework that does not utilize procedural and I will name you king of the internet!
This is an amazing answer. 
I fought the idea of using classes/objects for everything for a long time. It seemed like a silly waste of time. Then I started using Laravel and realized how nice it is to write code when everything is organized into objects. It's so nice being able to write things like Customer::havingBillsDue()-&gt;sendNoticeEmails(); I used to have tons of include files with complicated functions with a bunch of parameters that I had to look up each time I wanted to remember how to use it. Once I got used to the idea of having classes with methods so that each class is just responsible for itself, suddenly difficult problems became much simpler and everything was easier to maintain and understand. You just have to try it and you'll see.
Check out [dotenv](https://github.com/vlucas/phpdotenv) for separating your app from its environment.
The shorter version, is not clever, is concise and describe that you specified a boundary between values. Don't listen the word of this shitty PHP developers on the comments.
1 file with some environment detection? If dev, else if staging, else if production? 
Second this. Pretty simple and it's all in one place, no file switching. Do a switch statement on $_SERVER['http_host'] then depending on the environment host set different db creds.
At work we use Zend Framework 1. DB credentials are all in the same ini file separated by environment (production, staging, development). The parser looks at a constant `APPLICATION_ENV` to know which config section to load. The `APPLICATION_ENV` can be set as an environment variable in the Apache config. (We also have a policy against committing the ini files containing DB credentials to our repo, so we copy them over manually when deploying a new app.)
Any solution that involves committing credentials into version control is bad though. Sure it might seem harmless when it's just a localhost database password, but there are other strings - AWS keys, SMTP credentials, secrets for hashing passwords etc. that just shouldn't ever be committed into version control.
OOP is all about organizing the project , a single good programmer doesnt need it but when you have a group to manage it makes things easier. The dangers as with anything is over-engineering the objects, ie, an object that does everything.
Into a public one, nah. But a private server git or other vcs no reason not to. 
pay to get a job? no
Sure if you don't care, that's fine. But if you're doing anything of any importance, this is not a valid excuse. Remember, we're talking about putting production credentials into the code base - you could have all sorts of people working on your 'private' codebase who you wouldn't want to have your *production* credentials. Contractors, employees who later leave, dumb coworkers who accidentally leave their laptop on the bus, or set the repo to public, or push their local to the wrong remote, or deploy the codebase to the wrong server, or share your code with a friend for one of a million reasons, or sync your codebase via dropbox to their hacked pc, there's so many ways this info could be compromised. And once it's in git, it's hard to get it out. What if you have to later share your repo with someone, open source the project, your codebase gets purchased, your code gets audited by a 3rd party etc etc etc. Sure you can scrub these details out with some effort, but why expose yourself to all the potential leaks, for a tiny tiny use of up front effort? It's just a bad sloppy habit to have as a developer.
Irony detected: "Elimate Low Quality" It does actually sound like a worthwhile thing, as elance is a complete shit-show. But I haven't tried it. btw this submission is very likely a sock-puppet for the company, but hey, they are at least capable of putting out links without making it obvious that they are doing so. whois of HACKERLEADS.COM Updated Date 07-sep-2015 Creation Date 06-sep-2015 
This, 100x this.
Hah, thank you, I know you're right that I've done *something* stupid in all that code. There's no way I would move forward without getting the right help first. Unfortunately, I don't have any university contacts in CS. Honestly, I don't have any programming contacts at all (I come from a vastly different field of expertise). Would it be reasonable to meet with any ole web development agency for a security audit? Any advice on picking a good one? Oh, and to address your bullet points: * Passwords are salted. * Ah, funny story: I realized about 6 months ago that my site was *full* of SQL vulnerabilities, which led to a frantic weekend of learning and coding. As far as I know they're fixed, but I could be wrong (and that's why I'm here!). * The third point doesn't apply just yet, but yes, I've been escaping user input. I'll go back and make sure I didn't miss anything though.
How did you patch the SQL injection vulnerabilities? :) Did you escape on input, or use prepared statements?
I used prepared statements. Honestly, I don't remember if I escaped on input. I'll read up on this tomorrow! I'm only really familiar with escaping on output.
No, you did the right thing. * SQL Injection: Just use prepared statements, and where you can't use them, use a *very* strict white-list. * Cross-Site Scripting: Escape on output. Blog posts I've written on the subject, but if you can follow the above advice, you don't really even need to read them: * https://paragonie.com/blog/2015/05/preventing-sql-injection-in-php-applications-easy-and-definitive-guide * https://paragonie.com/blog/2015/06/preventing-xss-vulnerabilities-in-php-everything-you-need-know
ooh boy. this is bad on so many levels. See spidermonk's post above for more info!
I am currently doing something similar to B. But I will check out dotenv right now and see if its's a better option!
I've written the short version a couple of times myself. I think it's ok. The long version is a bit too much in my opinion for such a single operation.
I really strongly recommend looking at a framework like Laravel, before you continue your development. It's worth rewriting a bit of your code now for the long term gains it will bring. It will be much tidier, much more secure, and much easier and more enjoyable to expand and update. It might be overwhelming at first, but before you know it you'll be wondering how you ever worked without it.
My entry: http://pastebin.com/W5ws1EFM 164 characters
&gt;I'm self-taught and don't have much of a tech background, so this is all foreign to me. I recommend reading this great resource [phptherightway.com](http://www.phptherightway.com) and follow all the best practices stated there. There is a section about [security](http://www.phptherightway.com/#security) which basically what you need. I also suggest you start learning php frameworks and using latest technologies, trends and standards like [composer](https://getcomposer.org/), (PSR Coding Standards)[http://www.php-fig.org/psr/].
&gt; Into a public one, nah. But a private server git or other vcs no reason not to. There is a very good reason not to, aside from security. You don't want someone to clone your git repo, and try to run it, unknowingly, on the production database &amp; production PayPal API keys etc. That *someone* can even be me or you one day, if we're not careful. You don't want to be the guy who accidentally "migrated" the production database 10 versions backs and lost a ton of data in the process. The risk is very real that if you don't put strict boundaries between your environments, that make it hard to screw up... you're much more likely to screw up. I know, seems obvious when I say it like this, but here we go, discussing the merits of it. :-) The deployment settings are ultimately a concern of deployment. There is never just one context of deployment, so we should have a codebase which is relatively independent of its deployment context, so hosts, keys and passwords don't belong there. For the same reasons we pass dependencies from outside in our objects to make them flexible and decoupled, we should pass credentials into our apps from outside. Rules of good software architecture are universal rules of good organization in general. They don't just apply to how you write your classes.
I was just thinking that after seeing the OP hasn't responded further. I know there's still plenty of time for him to respond yet, maybe he's busy this weekend. But I hope he does come by and respond to these people who have taken the time to respond.
Thanks, looks good I'll definitely check it out.
Is this just bad if you put it in your repo? (I can't remember where I put anything about committing it (not saying I haven't made that mistake in the past, just unsure where the conclusion came from?)) EDIT: Also had a thought, why would you not have the switch statement on HTTP_HOST committed but instead of having the environment variables there you have them in separate files (local, dev, staging etc etc) that are loaded dependant on the host. (These files wouldn't be committed and would only exist on their separate servers, however because you'd have this general switch it makes it easier to swap these environment vars around if necessary).
I had the exact same problem with big JS files and solved it by changing the JVM heap settings. To help everyone else getting started I put together some details in this article: http://www.qualitycodeblog.de/performance-tweaks-phpstorm/ . Besides you can find a JetBrains help site with less detail there: https://www.jetbrains.com/phpstorm/help/tuning-phpstorm.html
yeah, mostly interested because it looks like it might save me some time 
Biggest advantage imho is the ability to change how each object works, by providing interfaces that are consistent. With procedural code it's quite messy to change integral parts of a system. with OOP it's easier. It's structure and interfaces also make it easier to collaborate with others. They only have to know the interface they talk to, they don't have to understand what's behind that. You shouldn't judge it based on what it does at a single page-request during runtime, you should judge it based on what you have to do to change anything and get it to work. objects are a lot easier to work with and read than gigantic procedural lists of functions. 
Don't save to your data store unless all inputs are valid. I've never seen it done any other way. If you really don't want to lose the input you could log it.
What do you want to happen? You could reject everything until the complete form validates. You could save some parts and reject other parts. You could POST the whole page, or you could use AJAX. It really depends on what your requirements are.
An agency won't do code review as in-depth as you would want unless they are specifically targeting that market. They will want to do some work quickly and get it out the door so you can be invoiced. Some agencies are just not worth the trouble as your work could even be outsourced by them.
Google: CCDC [name of nearest university or large state college here]
Reject the form unless all inputs are valid (what ever your criteria are), and prompt the user to try again. Repopulate the fields with the values they entered (except password-like fields) on the second form if they skip the client-side validation. (Always assume they'll bypass client-side validation.)
/r/phphelp
&gt; Pimple is quite simple and often people tend to use it as a service locator than a DI Container. You will set the services in the container and will pass the di container around the application to get the services in different classes. This idea is not always good for several reasons. This accurately describes how I wind up using Pimple (in the context of the Silex micro-framework). Can someone actually explain to me the situations where using it as a service locator isn't a good idea?
Unfortunately, I'm not using any sort of framework or CMS. I wish I had started with a framework, but I learned about them once I was already in pretty deep. I'm planning on researching some over the next week or so (inspired by the responses here). GitHub is also on my learning "todo". Right now I just use a local test server, and I set aside backups every morning. By stacks, I think you're asking what programming languages / technologies I use? If so: I have content and accounts stored in SQL, and they're pulled using PHP. Sorry, I'm not sure how to answer this part thoroughly! And yes, I'm using the password_hash function. I appreciate the post: The things you mentioned are all things I initially missed.
Service Locators are a bad idea generally speaking because they make your code more difficult to test and reduces flexibility, a class that has had a DI container passed in will ask for it's dependencies so if you want to pass class B instead of class A you have to do some tom foolery with the container. Rule of thumb is if you're passing you're DI container to anything but the application bootstrap you're probably doing something wrong. 
Thankfully, I am using the password_hash function. As for the other suggestions, I'll read up to make sure I've taken the appropriate precautions. Thanks for the advice!
Not everyone is able to use the "perfect" font choice (if such a thing even exists) at every opportunity.
&gt; Service Locators are a bad idea generally speaking because they make your code more difficult to test and reduces flexibility, a class that has had a DI container passed in will ask for it's dependencies so if you want to pass class B instead of class A you have to do some tom foolery with the container. Well to some extent, that's the point. A class either has a hard dependency on a specific class, or can use anything that adheres to a given interface. If it's the former (ignoring your overly-coupled design), there's really no point adding complexity with a Container unless you're in the middle of refactoring. In practice, the testing is just about as easy or difficult with DI or SL, it's just laid out differently. The stuff that's really hard to test is where the class being tested instantiates its dependencies directly, which is neither DI nor SL. The big downside of the SL approach is that your dependent class now forces your container to have a specific format. Day-to-day I think it's more annoying than actually problematic - bad practice for sure, but honestly not nearly as bad as some would have you believe. Having `$this-&gt;config-&gt;get('prop')` instead of `$this-&gt;injected_prop` doesn't magically destroy things, and honestly has pretty much the same potential for mis-configuration (though hopefully only in one place...), but certainly makes changing the config a lot more brittle. How problematic this is really depends on how stable the code using it is. All else being equal, DI is better. But let's not fool ourselves into thinking that using a service locator is on the same level of procedural spaghetti that's mixing html, js, php, and sql into a single file.
The common way is to not store anything until the form is completely valid. If the form is submitted when invalid, just display the form again with the incorrect values (bonus points for highlighting which ones are wrong) and allow the user to correct it. However you may have requirements that force you to do something different. I can think of 2 scenario's I've handled. 1) The form is big and even though it is not fully filled out yet it contains data important to the organisation. I'm thinking planning tools or risk assesment tables. Store these in the database. 2) You want to remember data for the users convenience, like a cart or order form or search form. I'd generally use JS localStorage for this, unless storing it in the database has added value of some sort. I basically never store any form data in $_SESSION. It doesn't work across devices and by default session data is lost quickly.
If UCF is the nearest, just jump on #hackucf on irc.freenode.net, I know them personally. They won't find it weird.
&gt; Pimple is quite simple and often people tend to use it as a service locator than a DI Container Is the author implying that using pimple encourages service location and that Aura.DI will fix that? Seems bizarre to me.
It's not really that. Its to actually run the installer for a given package. Ala what drush site install does for drupal
I have spent the last couple of months building my own framework ( i dont plan on releasing etc. i want to understand how to properly build an application without the magic behind frameworks - i want to learn the language not the framework ) within most of my stack overflow and reddit answers laravel was the most recommended/worshiped so i thought it was the framework that i should use to learn from. If minimizing container dependency is the goal what are some of the ways i could solve larger dependency lists within controllers? This is where my main issue lies at the moment. I have a base controller that injects the basic dependencies. Then each controller requires access to a specific service. So i have to redefine dependencies in all controllers which is becoming tedious and repetitive so i am trying to figure out how to solve this problem and alias loading would do the trick but i am trying to avoid building my app the "wrong" way.
How fucked am I if the current company which I have worked for many years uses purely procedural programming in PHP and I lose my job? My OOP skills are basically non-existent. 
Generally speaking, it will limit you to companies also using procedural code or those willing to take a risk that you can learn. If you're aiming for a company using OOP, you'll be looking at a Jr. or Mid-Level position (which may be the case anyway, depending on your experience). That being said, you can leverage books (or webinars, talks, etc) to up your game. I'd start with Brandon Savage's *Mastering Object Oriented PHP* and then move on to a design pattern book like *Head First Design Patterns* or *Agile Software Development: Principles, Patterns, and Practices*. While reading through those, you can practice and demonstrate your skills. Start with simple things, like a code kata. Or refactor some code at work into a testable class, even if you're not allowed to commit it. And then contribute to an open source project. Find projects that are doing OOP that have open issues you think you can handle. You'll get a chance to read through the code, while also setting it up in your environment to test. You can also start your own projects on GitHub. One thing that might be straight forward is to take an existing library and make the adapter needed to set it up in a framework. Something like ghislainf/zf2-whoops which sets up the Whoops library in Zend Framework 2. You'd have to read and understand the OOP concepts in both the library and framework to set something like that up.
The default php cli on Ubuntu 14.10. Had to turn on short tags, but otherwise no special configuration. ckwalsh@laptop:/tmp$ php -v PHP 5.5.9-1ubuntu4.12 (cli) (built: Aug 13 2015 22:34:42) Copyright (c) 1997-2014 The PHP Group Zend Engine v2.5.0, Copyright (c) 1998-2014 Zend Technologies with Zend OPcache v7.0.3, Copyright (c) 1999-2014, by Zend Technologies
Thanks!
What makes it different and/or better than [league/uri](https://github.com/thephpleague/uri)?
10 year old legacy app for which there is never time to rewrite things because you are constantly trying to put off some fire. Been there, done that, got the t-shirt. Not doing it again. If you can't set aside some time for your dev to do some refactoring on what they are maintaining each week you're doomed. Bonus to the hero worship. You know the guys who are here early, stay late, are always running around to try to correct some bug in prod. But who are in fact the root cause of those problems (testing? No need to). Now when interviewing there is one thing which a direct red flag: no proper QA. Nope, devs testing other devs code is not QA.
Where is the documentation?
&gt; Why use this framework instead of Code Igniter
When you put it this way, I agree.
Why not repopulate password-like fields?
I don't think there is anything different that you would have to do in 7 to gain more speed, as if the 2X speed performance is not even enough for you. I suppose you could disable unwanted extensions do some trivial stuff in the php.ini but, still this can be done for any version of PHP, and not only for PHP7
Why use trait instead of an abstract class?
Whats the difference between: echo $uri-&gt;getPort() . PHP_EOL; // outputs: 8080 echo $uri-&gt;getStandardPort() . PHP_EOL; // outputs: 80
I'd tackle this problem using a message queue. I'd probably use rabbitmq or beanstalk. So basically when the user requests a job some basic info gets added to a queue. I'd then have a number of command line php workers connecting to the queues and doing the work and sending the emails once complete. The following tutorials should get you started (beanstalk is probably the simpler of the two): http://www.sitepoint.com/use-rabbitmq-php/ http://www.lornajane.net/posts/2014/working-with-php-and-beanstalkd
I'm the author of League Url so I may be bias but here's what I found: - Kit-UrlParser v2.0.0 was released before League URL v4.0.0 What they have in common: - They ship with their own parsers (they do not rely on PHP's `parse_url`) - They provide a class implementing PSR-7 UriInterface. - URI objects normalize the input URI string Kit-UrlParser specificities: - The parser uses its own parser based on RFC3986 and is also a validator. - The parser uses mode settings so depending on your settings the following URI won't be parse: `http://fööbâr.com/bàr/bâz`; - The parser returns `null` on failure and an URI object compliant with PSR-7 UriInterface on success; - On success the parser modifies the URI characters; - The Uri class exposes more public methods than those specified by PSR-7 UriInterface to ease URI modifications; - The Uri class accepts any URI schemes but detects the standard port for HTTP, HTTPS and FTP; League\Url specificities: - The parser uses RFC3986 URI parser regex; - The parser is decoupled from the URI object; - The parser is not a validator, you are required to use a League URI object to validate your URI; - The parser throws exception on error and return a array similar to `parse_url` result on success; - League Uri objects are scheme specific and will throw exception when used with unsupported schemes; - League Uri objects uses URI modifiers to ease URI modifications; As for which parser is better than the other ? While developing league URL I've created a package https://github.com/nyamsprod/uri-parser-benchmarks with the URI parsers that I have found. While any PHP user land implementation is de-facto slower that `parse_url`, The results depend on the URI used. You'll notice that Zend URI parser and Pear URI parser are also tested.
I would assume getPort gets the current port in the specified url, while getStandardPort gets the standard port associated with the url scheme.
&gt; All else being equal, DI is better. But let's not fool ourselves into thinking that using a service locator is on the same level of procedural spaghetti that's mixing html, js, php, and sql into a single file. Service locators are in the same level as global variables. There's not much difference between `$svc['db']` and `$GLOBALS['db']`, though the former gives a false sense of good design.
I find when anyone asks the question "Is a trait the right way to do this?" the answer is just about always no. This isn't saying that there are no good use cases for traits but they are fairly rare and it should be pretty obvious that a trait is the right approach if you're considering it over inheritance or composition.
Will try it later on today give u feedback. Looks nice though
The top 5 points of [the OWASP Top 10](https://www.owasp.org/index.php/Top_10_2013-Table_of_Contents).
&gt; In PHP7, is there any tweaks I have to make to php.ini to have the best speed possible? Make sure your opcode cache is on and well tuned. You can also disable file stat checks for an extra boost, but this means you need to clear cache manually when you upload new files (check docs on opcache). Before you do that, I'd suggest you think hard if the project needs the boost, or you're doing it because it feels good to be faster. I help many people with their PHP projects and in most cases we don't even go for the easy wins (like opcache) because it's simply irrelevant when you get only at a few requests a second. BTW, for benchmarks that run in a loop in CLI, opcode cache has no effect (it only has effect when testing multiple requests on a server).
Thanks for the info about numbers. Note it is also important to compare (in this kind of discussion) hashing schemes that runs approx the same time on defenders system. The example I gave, 2^18 iterations if MD5 (and bcrypt with cost of 14) shows that the stretching is high enough to make the 8 x 290X setup go from 81549.2 MH/s to ~311084 H/s. Quite significant decrease I'd say. &gt; Fundamentally you're pushing "MD5 is okay if you roll your own crypto" which goes against he core of modern best practices. I've seen it done wrong too many times. I imagine others have too which is why you're so downvoted. I'm only commenting here on technical facts (showing that MD5 is not "broken" like it is broken for something that requires collision resistance, and not talking about any best practices). If someone else happens to read this, I emphasize I'm not advocating to use MD5 or any home made construction for password handling (in case that wasn't clear already). Just use bcrypt. My comments here are just on technical side of thing, which I think, is good to know when having this kind of conversation. It may surprise people when I said the "extreme example" (my earlier comment on this thread about raw MD5 and quality passwords). But it is just a pure technical fact under the hood and has nothing to do with anything else.
Maybe it's so you can do `if ($uri-&gt;getPort() !== $uri-&gt;getStandardPort())`? That would let you determine if you need to include the port in the URL.
https://github.com/chrisboulton/php-resque is my recommendation. used it for all sorts. 
I try to avoid instilling a checklist mentality in people. Attackers don't think in checklists. ;) The closest thing is a [list of four basic guidelines for secure development](https://paragonie.com/blog/2015/08/gentle-introduction-application-security), derived from a taxonomic perspective on how vulnerabilities occur. 1. Prevent data from corrupting the instructions that operate on it. 2. Be explicit and comprehensive with your application logic. 3. Keep your software up to date and don't rely on abandoned components. 4. Don't write your own cryptography. 
globals and singletons are frowned upon because it makes testing difficult, but there are times when I really want an instance of my container. Would writing [a class](https://gist.github.com/G4MR/bc05b925956bd0dcb95c) like this be bad practice?
People tend to promote dependency injection over a singleton. For example, you could instantiate your View object in your bootstrap.php or index.php file, and pass it to the router -- which your router will slingshot into the controller for this request. That said, I do use a singleton in one of my projects, because I do something strange and it's the best tool I've found for the problem I'm trying to solve.
I've tried drinking coffee and energy drinks at the same time. My heart wanted to leave my body.
The ZF DB Queue adapter does this as well. It actually sets a timestamp when popping items from the queue and will time out your lock on the item automatically. https://github.com/zendframework/ZendQueue/blob/master/library/ZendQueue/Adapter/Db.php
Why do we have to write common functions like - camelCaseToWhatEverTransformation() - read and write object properties through a key-path - etc. in userland PHP?
&gt;Is there a better way to go about this that I'm not thinking of? Proper PR is a checklist of many things, one of which is code. If your language of choice is PHP, you will need a qualified developer/group who is naturally fluent in PHP and values the items on the PR list. The problem is that it is not easy find other professionals with whom you have congruent ideals. If I were in your situation, I would start with asking myself to identify what needs PR and what qualities to check, ie: * security * formatting * code standards * test coverage (functional / unit / regression) * package notes * install/deploy notes * version comments/instructions * logic reviewed * requirements met * user documentation ( if applicable ) Do not take all of the above as final list. It's just some items to get you started. Next, ask whoever you interview what they think about any of the points which are important to you. Ask them if anything is missing from your list. Be comfortable with taking criticism and be prepared to validate suggestions and your own assumptions. I would 100% avoid odesk, freelancer and such for many off-topic reasons. Look in niche subs like this, make relationships with local devs via meetups (meetup.com may have a local PHP group) and feel out who to trust. If you settle for a remote dev, be prepared to get really comfortable with contractor type of communication like chat, email, skype. If you are still in need of more consultation, feel free to PM me. I run a PHP centric shop in Canada and can guide or work with you. ps. congratulations on your success! 
&gt; the essence of OOP is that wraps procedural code in neat little packages we call objects. &gt; This allows us to reason about our code at a higher level, which is helpful for bigger projects. This simply means you are namespacing functions, which is not what OOP is about.
It is more than just a named closure array, it does have factory properties which is where its usefulness comes into play. $container['session_storage'] = function ($c) { return new SessionStorage('SESSION_ID'); }; $container['session'] = function ($c) { return new Session($c['session_storage']); }; It is particularly useful when you combine it when class names. $container[MyClassName::class] = function ($c) { return new MyClassName($c[MyClassDependency::class]); } $container[MyController::class] =&gt; = function ($c) { return new MyController($c[MyClassName::class]); }
This is very interesting...
I took the freedom to refactor your example as a literal array of closures: $c[MyClassName::class] = function () use (&amp;$c) { return new MyClassName($c[MyClassDependency::class]()); } $c[MyController::class] = function () use (&amp;$c) { return new MyController($c[MyClassName::class]()); } And as a plain old PHP class (which I'd prefer if I had to pick between the two): class Container { function getMyClassName() { return new MyClassName($this-&gt;getMyClassDependency()); } function getMyController() { return new MyController($this-&gt;getMyClassName()); } } I think the examples speak by themselves.
I think we should paint it red.
This is a terrible guide. Obvious MySQL injection vulnerability. Not even basic hashing of passwords. You should also read up about RESTful APIs, as you shouldn't be creating a user following a GET request and '0' is not a valid HTTP status code.
Absolutely, but the number of suggestions to use cron in this thread kinda prompted me to explicitly mention it.
If you need more than 1 type of that object you insert an Object Factory into pimple. What I don't understand is your need to reinvent the wheel. Yes DI containers can be done in a lot of ways, but why bother to DIY when there are so many already pre-existing, and that are quite well documented and accepted in the community.
I really appreciate and very happy for your feedback. It increase my confidence because at least one user take interest what i am working on. the next thing which i will do is to revise it again and Improve it. Yes some other friends also told me about these issues. Thanks for being a good user
You *could* satisfy OP's use case with a job runner kicked off periodically by cron, but you'd have to do without a lot of nice features that come with a more fully-baked task task runner / queue system. You'd have to roll your own on a lot of things.
Here are a few differences between Opis Closure and Super Closure. * The parser used by Opis Closure is 3 to 5 times faster than the one used by Super Closure * Opis Closure does not use *eval* to reconstruct the closure * Opis Closure has a single serializer * Opis Closure can serialize a closure [multiple](https://gist.github.com/msarca/0dd444638e677d85d5ad) times. Super Closure cannot. * Opis Closure can correctly and automatically unwrap other [referenced closures](https://gist.github.com/msarca/7191732c0482042c5cd8). Super Closure cannot. * Opis Closure facilitates [debugging](https://gist.github.com/msarca/2bfa20149ab2c042abe8). Super Closure does not. * Opis Closure supports PHP 5.3. Super Closure does not. 
Yep. We currently pipe some errors via logstash to email and slack. When I have some spare time I'd like to look into https://www.elastic.co/products/watcher though.
Thanks for the awesome comparison and the benchmarks!
I can say for sure that it is not a good approach to have everything extending dbclass, or any other class. Check the Liskov substitution principle. The author is using inheritance where they should have used composition. As for the trait, this seems acceptable, but only if the db classes in question strictly implement general purpose SQL drivers, and not any application specific logic.
As with all things I would say it depends. The 'best' way to build something is always the 'simplest' to implement for the specific requirements you have. The important question for me is the message ingress rate vs. processing time. How many submissions are you expecting initially? Have you built the report generation component yet and profiled the average processing time? You may only need one process/worker to generate your reports as opposed to a pool of them. Many of the technologies suggested so far are perfectly, perfectly valid, but whatever you use I would highly recommend building something simple first to fully understand your requirements. In the past I've used a combination of Redis to host a queue, a long running process/worker using Cilex (https://github.com/Cilex/Cilex) calling 'blpop' (http://redis.io/commands/BLPOP) on the queue periodically, and managing the Cilex command using Supervisord. It works very well. You can use Supervisord to spawn several instances of your Cilex tool. 
&gt; I don't know what framework you use, but chances are it's quite guilty of this. Even if not to the same extent. Not really. I listed out a bunch of them that do not fall into the tight-coupling category in nearly the same way that this does. Silex (via Pimple), Slim ([to a lesser extent](http://www.slimframework.com/docs/concepts/di.html)), and even Lumen (Service Providers) provide much more in the way of allowing the developer to implement their own tools and components. &gt; What if you use Laravel and you don't want to use an Eloquent model with an SQL database? :-) That would be a fine question, had I listed Laravel among the frameworks that I had pointed out. Laravel is a poor example to use in this case, because it's a full-stack framework and is much more heavily opinionated in how it expects you to leverage what it has to offer - Eloquent being the closest thing it has to a God class, and therefore much harder to decouple from the library itself. Had you asked about - say - Laravel and decoupling Blade in favor of Twig, well then something like that [is certainly possible](https://github.com/rcrowe/TwigBridge), since Blade isn't exactly Laravel's centerpiece. **EDIT**: As it turns out, dropping in a MongoDB implementation [also doesn't seem to be terribly difficult](https://github.com/jenssegers/laravel-mongodb) (untested) But this isn't Laravel, and we're not talking about a totally separate library that this is attempting to implement. This is hardwiring a lower-level extension like MySQLi to its core, with no direct way to choose anything else. Any modern framework nowadays would at least attempt to type-hint some kind of interface with a default class passed in via dependency injection, which would be a FAR better option than what this offers. That's why I had asked about dependency injection, as this would at least help alleviate some of the code smell.
What is the use-case for wanting to serialize/deserialize closures? And are they really so important to warrant the security risks that come with running code from another data source? 
We use NewRelic for APM monitoring, and use the built in error reporting.
We use https://papertrailapp.com/ which puts them in a nice searchable central location (and it ships them off to S3 after a while, too). Very happy with it.
&gt; Opis Closure does not use eval to reconstruct the closure Yes it does. Ok, it's not calling the eval function, but binding a stream handler and calling `include()` is semantically identical: https://github.com/opis/closure/blob/master/lib/SerializableClosure.php#L367 It's still executing code that's constructed dynamically. So saying "does not use eval" is totally misleading, because it is doing **exactly the same thing**.
Isn't that what Composer "scripts" are for?
&gt; Not really. I listed out a bunch of them that do not fall into the tight-coupling category in nearly the same way that this does. Silex (via Pimple), Slim (to a lesser extent), and even Lumen (Service Providers) provide much more in the way of allowing the developer to implement their own tools and components. All right. If I get your intent right, you hold Silex in highest regard regarding low coupling and replaceable components (from the listed here: Silex, Slim, Lumen). Let's explore the Application class of Silex. The application class takes on a ton of responsibilities: - composition root - event bus implementation - request routing - request/response filter pipeline - controller dispatching - view factory - html escaping (?) - view handling Holy cow, that's a lot to put in one class, isn't it? And it's very hard to argue that all of this is *cohesive*. Sure, you can say it's cohesive to the responsibilities of the *entire application*, but by that logic we might as well stuff our entire application codebase in one class. A good chunk of the functionality is a facade for components which app has composed within itself, which is good. But this means the facade expects *certain components* to be present, which we can't replace. Technically we can replace them. Maybe. Let's say I want to replace the router with my own. There is this barely documented option $values in the Application constructor, it says just this: @param array $values The parameters or objects. Well that isn't helpful. What parameters? What objects? Reading the source we see that this array will be merged onto $this via array access (i.e. this goes into Pimple), so it seems that I can *technically* replace the router. But should I rely on this functionality, might it change in the future? It's not documented that it's *ok* for me to replace the router, or any of the other default components, so I'm taking a big risk here. What if they get declared as frozen in Pimple in the next release? Additionally, if you look closer at how the facade for the various kitchen sink of responsibilities above is implemented in the Application class, you'll notice you can replace the router... only with another instance of a Symfony router. Wanna use Nikita Popov's FastRoute? Tough luck, the facade can't work with it, so you can't either. Likewise for all the other components. There aren't minimal, easily implementable Service Provider Interfaces (SPI) created by the framework to allow components to be replaced with non-Symphony components. Oh and if you want to replace the container, you can't. It's statically coupled to the application class by inheritance (Application extends Pimple). If you have a modular app, and every module requires its own container, tough luck again. So as you see, a lot left to be desired here.
 &gt; EDIT: As it turns out, dropping in a MongoDB implementation also doesn't seem to be terribly difficult (untested) Did you actually check how large the integration code is? All the code in the MongoDb namespace isn't a driver for MongoDb. It's adapters to the various components of Eloquent. How does that not fit perfectly in my description of "requires significant integration work"?
Good idea. I have some thoughts about your implementation, though: I would advise you to implement some kind of dependency container, where you could configure concrete implementations of your abstract installer. Do not define your methods static. Instead, define an interface (or use your abstract class as that, but it would be cleaner to do it in a pure interface). Then you can implement a collector (or something similar, it is up to you), that can provide your command with instances of specific installers. Every installer can be configured separately and given to the collector including a name. Like $installerProvider-&gt;addInstaller('joomla', new JoomlaInstaller()) This way you can ask your provider afterwards for a specific installer by name and know that you'll get an implementation of your installer interface. You can then call the appropriate methods on that. Moreover, your provider can throw an exception, if it hasn't got an installer associated with that name and you can handle this case as well. Just some thoughts about how your architecture would become way cleaner and more maintainable, extensible and testable. :)
Json format is for data, not for code or something like it. How would you write error handlers in json? It creates temp file in system temp folder, so it will be cleaned at some time. But I'll add additional checks for failure to clean everything up.
&gt; but are evaluated using different operands and the behavior is different, especialy when we talk about error handling The behavior difference is simply the difference between bailout on parse error (terminate script execution) vs returning null. But the same remote code injection problems exist on both. You're still executing dynamic code. And seeing as this library doesn't have support for cryptographic signatures on the serialized code block, this is **WORSE** than SuperClosure. Yes, SuperClosure may use "eval", but yours is an unrestricted RCE. The avoidance of "eval" over this sort of method is only useful if you want to get around static analyzers that look for "eval" in the codebase. There's zero semantic difference here (other than a parse error is a bailout with include). But that parse error should never happen anyway, so what's the point?
I have written a couple of systems at work that does background processing. I have simply set up cron jobs to execute them as often as needed
We use [Sentry](https://getsentry.com). It's awesome. Though their website makes it look like Sentry is only a subscription-based hosted service, they actually do have an [open source version](https://github.com/getsentry/sentry) that you can host yourself for free. One of the cool things about Sentry is that it supports a wide array of languages, including Javascript, so you can use it to capture errors on your front-end as well. 
Why all the hate? I can definetly see this as useful for learning purposes. Its easier for the beginners to just write the code on their own computer no?
Generating efficient code based on an AST-like description via the Builder pattern, where you can plug-in customizable logic via closures (because not everything can be expressed declaratively via built-in classes and options). Example, building a validator: $val = (new DictionaryValidator()) -&gt;addField('foo', new NumberValidator) -&gt;addField('bar', new BooleanValidator) -&gt;addField('baz', function ($value) { // Custom validator logic here if (...) { ... return true; } else { if (...) return true; } return false; }); file_put_contents(CACHE_DIR . '/validator.php', $val-&gt;buildPhpSource()); Other examples: 1. Parser generators. 2. Rule engine code generators. 3. Workflow engine code generators. 4. Translation string DSL code generators. And so on. I've not used Opis Closure and I'll play with it, but to allow closures to be passed to my code generation routines for better extensibility, I've had to resort to tokenizing the PHP source and looking for closure on the line where the closure is defined (which I obtain via reflection). From what I understand Opis Closure does something similar. It's not a very elegant approach, and the fact PHP doesn't report character position, but only line is making it a bit more awkward (hence Opis Closure notes on their home page that you can't put two closures on the same line).
I don't know what are you trying to prove. Actually, I do know, but I'm just pretending I don't :) You are wrong, "eval" do behaves different than "include". Just try the following code: &lt;?php require 'vendor/autoload.php'; $whoops = new \Whoops\Run; $whoops-&gt;pushHandler(new \Whoops\Handler\PrettyPageHandler); $whoops-&gt;register(); eval('test.php'); test(); The *test* function is located in a file named *test.php* &lt;?php function test() { //some code here throw new Exception(); } Just run the code above and tell me on which line, inside the *test.php* file, the exception was thrown. 
Is there anybody here willing to collab on building something very easy on github?
If someone would like to add new script or improve current behavior, just fork it, commit changes and create pr.
Sorry if I'm being stupid, that happens ... but why isn't the baz validator an object like the other two ?
An important use case is passing code to a queue system. This usually requires the code to be converted to a string. Hence the serialisation. 
Maybe the *addFiled* method is defined like below public function addField(string $name, callable $callback); A closure would be then a valid argument.
This is not my work! But I wanted to share it.
I feel alone in thinking this. Does anyone else dislike the use of frameworks (Laravel, Symfony, Yii) and prefers components (Symfony Components, Zend, and more single-item components)? I work mostly on the backend side of things, and most of the big frameworks are geared towards creating a frontend. But even when working on the frontend, I prefer components.
Another usage scenario is to make a DI container fully serializable. I've done that and works great. For example, by making the container fully serializable you are not forced anymore to load all classes that provides bindings for the container, at every request. You could use a cache mechanism to store and retrieve the state of the container. class Foo implements ServiceProviderInterface { public function register(DIContainer $container) { $container-&gt;bind('Foo', 'Bar'); $container-&gt;singleton('Baz', function(){ new Baz('arg1', 'arg2'); }); } } $container = null; // Load from cache $container = $cache-&gt;load('service', function() use(&amp;$container){ $container = new DIContainer(); //find all providers foreach(Services::providers() as $provider) { $provider-&gt;register($container); } }); $container-&gt;make('Baz')-&gt;foo(); 
We follow a standard so it makes reading other peoples code easer called [PSR-1](http://www.php-fig.org/psr/psr-1/). When everyone's following similar coding style it's less work trying to learn others coding conventions. - Classes are StudlyCaps or UpperCamelCase - methods are lowerCamelCase - constants are UPPER_CASE with underline separators - Properties don't matter as long as they are consistent throughout the project and should be easily picked up by other contributing developers by just reading the source. 
Rollbar is amazing for this. The exponential increase alerts are amazingly helpful and the JIRA/GitHub integration is a ton of help. I pay them every month and am thrilled to do so. 
This will not handle machine failures though, which is my main reason of wanting a rescheduling system.
Hi, i'd suggest you to have a look and use micro-framework for this, like: http://silex.sensiolabs.org/ (DRY)
Use native string functions if they cover all your use cases in a simple, easily readable manner. I would use regular expressions for anything more complex - some people hate regular expressions, but, maybe because I came from Perl, I consider them to be an awesome tool that allows you to quickly write concise code. fnmatch is specifically for filename matching. I'd only use it for matching filenames. I don't think I've ever seen this function used in the wild, so a lot of developers likely won't be familiar with the way this works, which could lead to them making assumptions, which could lead to bugs. Unless this code is being run many, many times in a single request, I doubt performance is going to be an issue whichever method you choose.
It can, but it might result in a degraded experience for your users depending on the SAPI. Turn on output buffering at the start, calculate the content length and send the appropriate header along with the content itself. That will tell the browser that nothing else is following, and it *might not* sit and spin waiting for more from the server. Some SAPIs have [ways to formally end output](http://us2.php.net/manual/en/function.fastcgi-finish-request.php). If you can, though, you should just use an actual work queue. [I like Gearman for this](http://gearman.org/), but any message queue solution can be (ab)used similarly. 
With this particular implementation though, it looks like he would need methods to pick up the table name and other details which would fit as abstract methods or interfaces better. If he needs multiple inheritance then you could probably get away with using the other base classes as traits.
Garbage clickbait title Poorly compiled list of barely maintained PHP extension-land code Blogspam 2/10 
Sometimes there's more talk about framework than components, but you're definitely not alone. Just look at packagist. So much to choose from with some gems in there. I'd love for people to share their combinations or picks of components more often. Great way to find new projects.
sorry, i didn't know i was replying to a special little snowflake.
&gt; Actually, I do know, but I'm just pretending I don't :) You are wrong, "eval" do behaves different than "include". I'm talking about the security concerns around it. And they are identical to both. You're running code in both that you don't own. And since you don't have any line of defense, you're still an unrestricted remote code execution vulnerability. SuperClosure at least takes steps to try to fix that. The example you cite works great for the difference between `eval(file_get_contents(...))` and `include ...`, but here, for a non-trivial closure your debug experience isn't significantly better (since the URL is just a giant blob of code, not all of which is identical to the source). If you look at the first thing I said, it was that `include` and `eval` are **semantically identical**. Meaning the relationship between the caller and the called are identical in both. The precise error message doesn't change the semantics at all. And no matter how much you want to argue that, the fact remains that it's semantically calling `eval()` (executing dynamic code).
&gt; Generating efficient code based on an AST-like description via the Builder pattern You can do that without serializing closures. In fact, it has nothing to do with serializing closures. Unless I'm missing something...
That is nice app, doesn't see through the code tho, but it would be great so see it built as a library instead of app, and integrate to some templating engine like twig or blade
Can we stop with all the new stuff? Thanks JK this is probably awesome. 
As long as we're sharing query builders, also really enjoy DBAL's Query Builder: http://doctrine-dbal.readthedocs.org/en/latest/reference/query-builder.html
I have a php script that automatically collects and calculates data from elsewhere and updates my SQL table based on what it sees every 60 seconds. My question is... how do I keep it running on the server side with my web host? It's password protected with sensitive data so no outsider should access it. Obviously I could open it with my home computer and auto-refresh it, but it's not feasible for me to keep my computer running 24/7. I don't suppose there is any php command that could self-sustain running a script indefinitely without outside input? Or is there a specific tool I should use for it? Will it work on a shared server or do I need a dedicated one?
It'd be interesting to have vagrant plus terraform combined. Shame I *just* got it set up.. Always a new toy to learn.
Important to note that this is from HashiCorp, the same folks who make Vagrant. &gt; after working on Vagrant for over six years, we've learned a lot and we believe Otto is a superior tool for development plus so much more. Vagrant will still fill an important role for some users, but for the majority of developers, Otto will replace Vagrant over time.
Maybe you could read the documentation.
Ah, i looked it up after you mentioned graphics. That is probably why I was so thrown off by the word, I've done very little in terms of Graphics programming and more in mathematics. So that might explain why I was thrown off. Thanks.
I mean I got clamp as a verb that fits, but it just sounded so foreign to me. Not used to seeing that word outside of physical clamps. Also /u/nashkara mentioned it came up in the context of graphics programming, which may explain why I didn't think of it as readily.
Maybe I did read the documentation. https://ottoproject.io/docs/apps/php/dev.html Which doesn't even mention ZTS or custom repositories.
There are bugs in some versions of PHP, and also subtle changes in behaviour that are not considered bugs. Even if you can detect the minor version, detecting which bugfix version the code is designed to run on is 'non-trivial'. 
I want to second this. Regular expressions are hard to learn but extremely useful for pattern matching. If you don't take advantage of them, you're handicapping yourself. There are things that would take a hundred lines of code to do with built-in string functions that can be done in one line with regex. That being said, if the thing you're doing can be done easily with built-in string functions, by all means, use them.
+1 for Regex101, it's fantastic.
You have valid points, but you are missing the point. Its kinda a simple start up for "defacto" builds. which as anyone can quickly tell you is 100% bullshit. If you run a small simple stack, ya this may be fucking awesome, but people stuck in some legacy system, or weird work related build it wont be to useful. Starting from scratch I think is the optimum. 
Just about all projects maintained under the league (https://thephpleague.com) follow what's considered modern standards: * composer based * follows solid principles * adheres to the fig where applicable * well tested and documented The Doctrine projects DBAL and ORM are, for the most part, great examples of modern and good php development. 
One is where wolves live! Which one should I use?
Does this help at all? https://ottoproject.io/docs/appfile/dep-sources.html As far as PHP versions, pretty sure that is to come as they let you specify for other languages.
Shamelessly yanked from HN: http://alexking.org/blog/2015/08/24/rememberances &gt; One of the things my wife and I are trying to do is put together some information about my career that will hopefully give my 6 year-old daughter a better sense of who I was as an adult. She knows me as “dad”, but when she gets older she’ll be curious about who I was to my peers and colleagues. &gt; I’ve spent more than a decade in the WordPress community and I’d like to request that you to share a few thoughts or remembrances about me that we can compile and share with her when the time is right. &gt; If we have crossed paths or if I have managed to do something that you found helpful, I’d love it if you would take a few minutes to write it down and send it to me or my wife: heatherkingcom@gmail.com. If you’re willing to have the story shared publicly, please indicate that accordingly. By default, we will keep everything confidential.
Yup, I host a sentry instance on a 512mb linode box that captures errors for 7 different servers. Runs just fine and I haven't overloaded it yet (even with 25,000 errors per minute when a big background job fell over).
I'm just now figuring out Vagrant, dammit!
Use this library that I wrote a while back. I have it running on my job server. You can use it to collect error info with timestamps and stuff. https://github.com/WesamMikhail/ScriptCheck I just noticed however that I should also add a shutdown_handler in case of runtime code error. I will add that when I get home so feel free to play with it and see if it suits your use case!
Vagrant's not going away any time soon. As Hashicorp themselves put it, it's battle tested.
As you mentioned smaller ones . Checkout a library from https://github.com/auraphp/ . Feel free to look at Symfony, Zend etc.
I don't understand your reaction. Can you refactor my example and take away closure serialisation, and be left with the same feature I demonstrate?
&gt; All the frameworks have to recreate very common functionality I think that this is problem of framework authors, not PHP itself. This problem is also called [NIH](https://en.wikipedia.org/wiki/Not_invented_here). &gt; Access properties of objects by key or resolve them by key-path For example Symfony already provides pretty [solid component](http://symfony.com/doc/current/components/property_access/introduction.html) for properties. You can use it. There is no any standards to describe property paths, every one invents their own implementation of property pointer, xpath for php object or whatever. I'll repeat: if you can implement something in the userland, you pretty much should implement it in userland. C code hard to maintain, it easier to migrate userland PHP code to new PHP version. &gt; JIT would be great, but will we get it? I do believe that we'll get JIT in PHP8. Or earlier as [part of opcache](https://github.com/zendtech/php-src/tree/zend-jit/ext/opcache/jit#future-directions). 
I just wasted 2 minutes of my life. Give them back to me.
Global variables are bad because there is no control on 1) state (everyone can silently change everything) and 2) dependencies (everyone can access and silently depends on everything). If you got a few constant parameters, then you mitigate 1), but only if you use const ie. primitive, scalar types. You still do not control who access these parameters, so you can have hidden dependencies. It also makes unit testing difficult or impossible. These problems are not 'awful' until some of assumptions are broken, eg. you got to connect to several databases, some module must be split and relocated to another server, or new team members jump in the project and don't use your code as you intended to.
The existing classes are the DSL. And as I said, it covers the common cases, but it can't cover for everything you can do in PHP and its standard library. I refer again to parser generators already doing that en masse. You can insert C snippets in the grammar of a parser generator written in C. Regarding bike shedding about what serialization means, read the source of Opis. It can't access op codes, naturally, as a user land library, so it takes the code, then includes it from a custom stream. For both my purposes and theirs, where the code comes from, and is it op codes or source code, is irrelevant. What is relevant is that it runs in a place other than where it was originally defined.
Hello. &gt; SuperClosure allows you to sign a serialized closure and check for tampering before unserialization Indeed, SuperClosure provides a signing mechanism, which is a plus, but I intentionally avoided this. Implementing a signing mechanism, similar to the SuperClosure's one, should be a breeze for every developer. &gt; SuperClosure preserves the static-ness of static closures and its analyzers can also tell you if it is static or not. That's something that Opis Closure needs to implement. It was left aside due to its obscurity and low usage. &gt; That is an unfair comparison unless you state which serializer/analyzer you are comparing to in SuperClosure. I can confirm that the speed comparision was made taking into consideration the default SuperClosure's analyzer, which is AstAnalyzer, but after running some simple tests, it turns out that the speed difference is actually much more higher. &gt; SuperClosure has two for a reason. One uses an AST and provides coverage of more edge cases at the expense of speed Since the Opis Closure's serializer can handle by default all those edge cases that AstAnalyzer does, I would also say that comparing TokenAnaylzer with Opis Closure's serializer is a bit unfair. &gt; Thanks for the bug report. :-) It can now I'm glad that something good came out from this thread :) Oh, and I don't see Opis as a [competitor](https://github.com/jeremeamia/super_closure/blob/opis-comparison-and-bugfix/tests/Integ/SerializationTest.php#L189), but rather as an alternative. 
&gt; Regarding bike shedding about what serialization means Are you freakin serious !? You didn't hear me trying to impart knowledge, you heard me *bikeshedding* .... Not all conversations on reddit go anywhere, this is one of those ... I'm out ...
BTW when I "imparted knowledge" with a detailed example above, you shrugged it off and said you don't get it. I didn't shout back. What you're doing here, down voting the people you talk with and trying to step over them with the pretense you know better by default is quite pathetic. Maybe it's possible others can teach you a thing or two, as well. Consider that next time.
You are extremely bad at listening. If you just stop for a moment and consider that you could be wrong about something, like most normal people do a hundred times a day, you might get to understand something new ... I don't care what opis, or super closure do, I tried to explain to you what serializing a closure would *actually* mean, note that I opened by offering to implement *actual* serialization if someone could present a use case ... you haven't ... because you don't seem to understand what serializing a closure would mean, even though I explained it, very clearly. &gt; You can insert C snippets in the grammar of a parser generator written in C. You seem to have a frightening lack of understanding here; Yes, you can include C in generated output, but you can't include ruddy machine code. You have the equivalent of machine code when you are passed a closure in PHP, not a source string, but the executable representation, machine code for the Zend VM. Serializing and unserializing that representation does not produce a string that is useful for code generation.
Every login is going through because you're verifying the user inputted password ($_POST) vs. hashing the user inputted password (from $_POST) so everytime you are calling password_verify it is returning as a match (true). You're not retrieving the password hash stored in your database table. password_verify works by comparing the plain-text inputted password and the hash that you have stored. You'll need to retrieve the hash from the database by looking up the user (make sure your usernames are unique in your table). A good reference: [http://www.phptherightway.com/#password_hashing](http://www.phptherightway.com/#password_hashing) I'd recommend reading through all of phptherightway, it's a great resource :) [https://www.reddit.com/r/phphelp](https://www.reddit.com/r/phphelp)
&gt; I don't care what opis, or super closure do, I tried to explain to you what serializing a closure would actually mean, note that I opened by offering to implement actual serialization if someone could present a use case ... you haven't ... because you don't seem to understand what serializing a closure would mean, even though I explained it, very clearly. **If you don't care what Opis does**, and you don't care what the use cases for what Opis Closure are, why are you even contributing (in a very loose sense of the word) to this thread? You just really wanted us to know that you know that... &gt; [serialization] would be a region of memory containing containing a bitwise copy of the function and it's opcodes and whatever, Is PHP's *own* serialization format a "region of memory containing a bitwise copy of the ZVALs and their internal structs and whatever"? No. Is JsonSerializable in PHP a bitwise copy of something? No (even the strings are converted to UTF8 first and escaped for some special characters). Here's a list of common serialization formats in use by the industry: https://en.wikipedia.org/wiki/Comparison_of_data_serialization_formats A *small fraction* of them are a "bitwise copy" of an in-memory format (like Cap'n Proto). *Most are not*. Such serialization formats are quick &amp; easy in languages like C and C++ because you have access to the raw pointer of an object. The counter point is, of course, they're very fragile, because the internal format may change (unless you lock it down, like Cap'n Proto), while their semantics may not. Case in point ZVALs in PHP5 vs ZVALs in PHP7. Thank god, PHP didn't follow your definition of serialization for their serialization, so PHP7 apps can read PHP5 serialized objects. So, is this enough bikeshedding for today, or should we do more? &gt; You seem to have a frightening lack of understanding here; Yes, you can include C in generated output, but you can't include ruddy machine code. You have the equivalent of machine code when you are passed a closure in PHP, not a source string, but the executable representation, machine code for the Zend VM. Serializing and unserializing that representation does not produce a string that is useful for code generation. If PHP had a mechanism for generating opcodes from a closure, and a userland API for turning those opcodes into a closure, I'd simply *generate the whole validator as opcodes*, and then I'd be able to inline the opcodes from another serialized closure, as well. So yes, if we had that, I'd be able to use it, and I'd use it. The whole point of generating code is to go faster. **EDIT**: And here'd another way to use it (generated code sample): $closure1 = unserialize('...inlined closure stream...'); $closure2 = unserialize('...inlined closure stream...'); return function ($value) use ($closure1, $closure2) { ..inlined code for checks from declarative rules... ..inlined code for checks from declarative rules... ..inlined code for checks from declarative rules... if (!$closure1($value)) return false; ..inlined code for checks from declarative rules... ..inlined code for checks from declarative rules... if (!$closure2($value)) return false; return true; }; From PHP7 tests it was demonstrated that even if you read opcodes from a file, and not from RAM, it provides a 4x boost for common apps. So why not. Of course, if you choose to implement serialization of closures as *opcodes*, then you run into the same PHP5 -&gt; PHP7 problem - if PHP8 changes its opcodes, but preserves source code compatibility (which is very likely seeing how things are going towards JIT and the best design for opcodes there differs from interpreters), all those serialized closures would no longer work (without some ugly convertor, at least). I'm sure you didn't know all that, having been working on PHP for years, but here I am, "imparting knowledge". /s So, *now* did we have enough bikeshedding for today? We wasted a lot of time &amp; ended up with **nothing valuable**. Congrats. You know, if you make an offer like "if someone presents a use case for serializing closures I'll make it happen" and then you get lazy and change your mind and don't want to do it, just say "ok I don't want to really do it". Don't try to make the other party look like an idiot just so you can weasel out of it.
I like this, I had a quick read and starred it. I will have a better look at it later, mind if people contribute? :)
&gt; PLEASE do not do this. Please send job descriptions, don't send the code itself. Yes, it may seem easy to send a serialized closure. You're only shooting yourself in the foot for the long run. There isn't always a "long run". In map reduce tasks, you can send a task to be executed on another machine, code once and many input instances for this task type, and it comes back nearly instantly. So "the long run" takes about a second *at most*. It's just a way of spreading the CPU/RAM load of executing the task, nothing more. This allows you to quickly add more machines by simply installing a generic "task runner" client on them, and putting no other code. If the task runner crashes or has a bug (which... for some reason you didn't catch locally, which is rare), it reports the error back to the coordinator node, and you fix the code in the coordinator, and that gets fixed next time this type of task is sent to a task runner node. Not that I run map reduce on PHP... But you know. Keep an open mind.
Can you please link to an example of them sending code? Curious.
Just some additional notes from the PHP Manual * http://php.net/manual/en/function.password-verify.php * http://php.net/manual/en/function.password-hash.php * http://php.net/manual/en/function.password-needs-rehash.php The work-flow is simple, when registering, but it looks like you could benefit from * Learning &amp; using a framework * Separating logic from presentation (framework will help) * Knowing what you are checking * Continuous integration, great write the tests to check your code &amp; db, then verify each build against them. In great news, it's not the worst code ever, and you are not trying to do too much, so thats awesome, I left a tonne of comments on the repo ;)
if youre annotating arrays with keys youre using the dataprovider to group tests, so doesnt sound like a healthy way to test. better to have more testXyz's. something about coverage
&gt; I have a feeling that Otto is more geared towards It could also be more geared towards Node and Rails apps where apart from the specific version, there's relatively little stuff that need to be configured/installed by compilation.
The people over there are far more cynical about it than even I am.
Thank you for the comparison and including my library in that benchmark! The benchmark actually highlighted an issue with my library I hadn't fully thought out. While it is not fully intended for the encodings to change in the `userinfo` component the way they do in the benchmark, it is actually a feature consistent with the PSR-7 API (since it doesn't provide a way to provide the username and password in an encoded form). I thought about changing the behaviour of handling the username and password, but as I said it would create a weird inconsistency between the parser and the Uri API (plus the implementation would probably be an ugly and reflection based). I should probably just document this in the normalisations section of the README.
&gt; As for your security concerns, I don't believe that people are a bunch of idiots that run remote code from unknown sources. And even if they are running code from known sources, I think they are smart enough to take some precautions. You haven't been paying attention. Watch the full disclosure mailing list. Unserialize attacks occur often enough to be scary. You would think that wouldn't be the case, but that's the situation we find ourselves in. And note, it doesn't even need to be a serialized closure, if you have this library installed, *any* unserialize call can be made more vulnerable. Signatures, and indeed any security prevention mechanism, work best when closest to the thing they are guarding. In this particular case, the thing bring guarded is eval. Therefore, it would be best to put the guard as close to it as possible. Hence why having it in-library is best...
Not at all. The point I was making is that since you are doing code generation anyway, there is no reason to serialize the closure. You can just code generate the function as is. And that's why I said it has nothing to do with serializing closures. Because there is another method that is demonstrably better with minimal extra work (since you're doing code generation anyway)...
[removed]
I, as in the lib maker, generate the code. Part of that code comes from user supplied closures from lib users, who I don't want to burden with error prone concatenation of pieces of string manually. Do you get it now? And please don't say "DSLs", I addressed this argument in another comment here.
What doesn't see through the code? It is meant to be used right before production, near minification. All the same, integrating templating engines could be helpful! 
Perhaps, but I would argue that your custom solution is precisely that, custom. Odds are that in a team, most people would be familiar and comfortable with dataProviders if PHPUnit is part of the job description =) 
I mean I only read the readme part. if we need to run it each time html is updated, I think it will only useful for static site, with integration to template engine, it can be useful for dynamic site too. sorry for bad english :D
What kind of a question is that? Should I be forced into a black and white kind of position? Should I also maybe stop using the words in English that he uses in his replies? Can we grow up a little? I didn't say he's Satan in PHP's clothing, who thrives on the flesh of newborns and pees fire. I said he's a troll. He has personality issues and is often not constructive in a debate, because his focus is on being right, and the other side being wrong. Password_hash is a very, very thin wrapper of crypt. It does nothing except guide newbies to good crypt settings, which *is useful* and I recommend it to people who seem not to understand crypto well. I personally don't use it, not that I mind, but my projects have been using crypt before there was password_hash, and I prefer control over my crypt settings rather than using default settings. Does that answer your question?
I occasionally try to poke holes in League packages, which in turn leads to the discovery and rapid resolution of security vulnerabilities. Keep your dependencies up to date as rapidly as you can, and you can consider this a bonus.
&gt; I do believe that we'll get JIT in PHP8. Or earlier as part of opcache. I'm not sure why, seems like a wild guess ... that document doesn't say anything positive about the future of a JIT, in fact, it's rather negative. &gt; Also LLVM compilation time is not suatable for run-time code generation (it make take few minutes). JIT compilation is not magic sauce, but is really rather complicated ... be careful what you wish for ...
So, I just posted this RFC. It is still at the draft stage as there is no implementation done yet, and I suspect there may be some stuff in the SPL which might need small further changes in the RFC to allow 'stuff' to work. What I wanted to ask /r/php is this; are there any other things in PHP similar to this that are complete bullshit that need to be cleaned up? Most of the time people who are complaining about PHP being crap are complaining about stuff that just doesn't matter (e.g. parameter order, some functions having underscores), which drowns out legitimate complaints about one of the built-in types being fundamentally broken. So I'm asking are you aware of anything that needs tidying up? i.e. not new features, not new functions, just stuff at a language level that is like it is because it just wasn't planned properly. Er, excluding the type-juggling rules which I'm not going to touch with a barge-pole. 
What a goddamn shame. With a young daughter too. It's just not right.
&gt; (RE: signing mechanism) should be a breeze for every developer Most security-related things are not a breeze for every developer. They are often overlooked or implemented incorrectly. I made sure to have my signature code reviewed, and it took me a couple of iterations to get right. &gt; (RE: static closures) That's something that Opis Closure needs to implement. Yeah, and it could could be easily added on top of what you already have in your ReflectionClosure. &gt; the Opis Closure's serializer can handle by default all those edge cases that AstAnalyzer does I'm not convinced it does (especially with the magic constants), but it does seem pretty dang close. :-) &gt; Oh, and I don't see Opis as a competitor, but rather as an alternative. Yeah, bad word choice on that line. I'm calling it out better on the README: https://github.com/jeremeamia/super_closure/pull/63/files#diff-04c6e90faac2675aa89e2176d2eec7d8R309 Maybe someday, after we converge on the handling of some edge cases, we can still merge projects. opis/closure is a better brand, but super_closure has many more installations at this point.
&gt; Do you get it now? No, I don't. I'm not saying to do "concatenation of pieces of string manually". I'm saying to do code generation. Which if you're already doing, what's the difference if the input comes in via a builder pattern (like you're doing) or a callback. You can convert both into an AST internally. The point is that while you *can* implement this with a serialized closure, you don't have to. And it doesn't even make it significantly easier to use a serialized closure. It just changes what you rely on. And that was my overall point. Unless I'm missing something, what you've proposed has nothing to do with a serialized closure. That's not an essential component, hence it's not really a fair comparison.
&gt; what's the difference if the input comes in via a builder pattern (like you're doing) or a callback. You can convert both into an AST internally. **Both?** PHP does not provide a way to fetch AST from a callback. I know there is an effort to get an extension for it in 7.1 but that is not real yet. Until then, any way to grab a closure and copy it elsewhere is a better solution than nothing. User land libraries that build AST would be an overengineered, slower and more fragile solution than what Opis does.
&gt; However making the 'ClassName::method' syntax no longer a valid callable is a terrible idea. You don't actually say why it's a terrible idea. Both `'ClassName::method'` and `['ClassName', 'method']` have the same meaning currently. From the fine RFC: &gt; It is easier (in the sense of fewer CPU operations) to validate ['className', 'methodName'] as a valid callable than it is to validate 'className::methodName'. Currently each place inside the engine that requires to validate something like 'className::methodName' as a valid callable needs to i) Search for '::', ii) Check that the '::' isn't at the start of a string iii) allocate a string each to hold the classname and method name. By holding the className and methodName separately, those steps can be skipped, as the className and methodName can be used directly. Why should we leave this duplication in, when it is a pain in the butt for the engine to deal with? &gt; That sounds like a huge BC break. Additionally, moving from `'ClassName::method'` to `['ClassName', 'method']` is a very minor change. As the RFC plans to add a deprecation notice in the last minor release of PHP 7, this would allow people to be sure that they had changed all uses of the colon separated string to the array syntax, before even starting to upgrade to PHP 8. So, yes it's a BC break, but it's a minor one, that will have deprecation notices.
You blame cancer, it kills and it's not right. It took him away from his family and being able to raise and see his daughter grow into a woman.
I'm glad you managed to find the important problem in this thread...
In addition to the other suggestions given, you should also consider giving each application its own session store - this can be set using the session.save_path setting (obviously this has to be set before session_start() is called, so can't be used if sessions are autostarted). Once this is done, other session garbage collection settings will not be applied to other sites session files. Session handling in PHP isn't rocket science as long as you don't try to do anything stupid with it, and shouldn't involved the writing of any custom code for the vast majority of use cases. While disabling session garbage collection on user requests can be a valid resolution to specific performance issues, session garbage collection should never be disabled completely without a cron already set up to trigger garbage collection manually (which is done by simply changing the session garbage collection settings for the cron, then opening a new session).
That's the type of answer I was looking for. Thank you.
This is terrible to hear; my condolences to his family, and may he rest in peace. But for the rest of us, this can be a call-to-action: you don't know when your time will come, or how. Prepare for that time and **get some TERM life insurance.** It's cheap. Get 10x your annual income. I can't tell you how many developers I've seen pass away, leaving their families behind with bills and other burdens. Having the money doesn't replace you, but it makes your family's life easier when you're gone.
It's more fragile not because it's *poorly written*, which is something you're trying to say I implied, but because it analyzes the code fully, so the next minor version of PHP might add something little and it breaks. Which means you need to block users from upgrading until the AST library can handle the new features that might appear in user source. This is why the AST API should best come with the language itself. It should be built-in so they are always in sync. If there is no native solution, the next best thing is not full AST, but a partial analyzer which can skip reliably over what it doesn't understand in most cases. Those are smaller, faster, easier to update. As for speed and LOC, unless you imply I have one validator in my project, it's not 10ms vs. 50ms. It may be the difference between a 10 sec build and a 1 minute build. Given a full AST gives me nothing in terms of value in this case, no, I won't use more code &amp; slower code, for the same feature. Would you? Awkward.
[removed]
Cool. Yeah.....this RFC will be usable in like 2 years time minimum. Maybe more! I am reminded of this proverb: &gt; The best time to plant a tree was 20 years ago. The second best time is now. 
&gt; but because it analyzes the code fully, so the next minor version of PHP might add something little and it breaks. You do realize that a core developer maintains this library. And it's typically updated for new rules even before the next version hits beta. Not to mention that it can parse code that's written for future versions (PHP 5.6 syntax can be parsed on 5.4). That's something a built-in could never do. So I'd argue the built-in would be *harder* to work with, since you'd get different representations on different versions, whereas something like PHP-Parser would give you the same representation irrespective of version. &gt; If there is no native solution, the next best thing is not full AST, but a partial analyzer which can skip reliably over what it doesn't understand in most cases. Those are smaller, faster, easier to update. Yet there's no need for that. PHP-Parser is an active project that's maintained by people closely involved with the project. It's constantly updated to be consistent across versions. &gt; Given a full AST gives me nothing in terms of value in this case, no, I won't use more code &amp; slower code, for the same feature. Would you? Awkward. You say it gives no value. I say the consistency and the structure gives massive value. There's a reason it's used by compilers (which is really what you're building) instead of just a stream of tokens.
Someone who uses xampp/MAMP/WAMP downvoted me. Possibly OP.
To be clear, you wouldn't need to find them in searches. The plan is for the last version of PHP 7 to issue deprecation notices where they are used: &gt;Deprecate with notices colon separated string callables 7.last - Any usage of a colon separated string callable will generate a E_DEPRECATED notice in the place that they are used, i.e. either as a callable typehint for a param, call_user_func, or is_callable. To be clear 7.last means the last planned minor point release before PHP 8. i.e. code that used "className::methodName" would still work in PHP 7.last, but it would give an E_DEPRECATED wherever you still had one in your code. And it shouldn't be too hard to figure out where those values are being generated.
Let's not get too OT. It's Blowfish with appropriate cost.
+1. Nice work! I'd vote for this.
&gt; You do realize that a core developer maintains this library. "A" developer. So, bus factor of 1. &gt; You say it gives no value. I say the consistency and the structure gives massive value. There's a reason it's used by compilers (which is really what you're building) instead of just a stream of tokens. Can you please not put words in my mouth. My own generator has a domain specific AST. What I see no value in, is depending on an involved, slow, third party AST for the whole PHP, when all I want is to to reproduce function from point A into point B. I'd **love** for PHP to expose its AST in PHP7, as I already said. Context matters. Read what I say more carefully.
I know. Your posts are not too hard to interpret. You always want to point out the security vulnerabilities. :-)
The legitimate use cases (passing closures between threads / processes, maybe other things) are kind of outweighed by the severe security consequences. Anthony has strong opinions in this, as he already voiced. As much as I would *love* to have a thread pool that could be used like this: $promise = Pool::enqueue(function() { // do stuff... }); That would require serialization in PHP's current state. Other languages do this, but without serialization. Serialization is not the way forward. *BTW, I appreciate you volunteering yourself to write an extension though!*
&gt; BTW, I appreciate you volunteering yourself to write an extension though! More like, he volunteered to belittle anyone who responded with use cases.
You can check out https://github.com/letsdrink/ouzo-goodies
A better solution would be to support a shared, global heap for all threads in ZTS and then be able to reference functions (or objects) on the heap; then just pass a function pointer essentially, to various threads. [Joe](https://www.reddit.com/user/krakjoe), as an aside, do you think this would be possible with the current PHP architecture?
It's most likely just changing a php.ini setting. It's not going to be complicated, he's just lazy. Automatic deletion is actually the default, and very simple to manipulate.
Where are those no more tears shampoo when we need them.
I guess it depends if there is any other business logic to adhere to when deleting sessions.
http://php.net/manual/en/function.fgetcsv.php
That is sad
&gt; The legitimate use cases (passing closures between threads / processes, maybe other things) are kind of outweighed by the severe security consequences. Anthony has strong opinions in this, as he already voiced. As much as I would love to have a thread pool that could be used like this: Note that, you can actually "pass" a closure between threads, but it's not useful to expose that same logic to userland ... unless it is ... nobody proved it is yet .. there's no security concern for this particular use case (threads), opcache does near as makes no difference the same thing ... Passing between processes is more complex, once the process has already been started ... but doable ... This is working PHP7 code: &lt;?php class ClosureTask extends Collectable { public function __construct(Closure $closure, ... $args) { $this-&gt;closure = $closure; $this-&gt;args = $args; } public function run() { try { ($this-&gt;closure)(... $this-&gt;args); } catch(Throwable $t) { $this-&gt;error = [ "message" =&gt; $t-&gt;getMessage(), "line" =&gt; $t-&gt;getLine() ]; } } private $closure; private $error; } class ClosurePool extends Pool { public function enqueue(Closure $closure, ...$args) { $this-&gt;submit(new ClosureTask($closure, ...$args)); } } $pool = new ClosurePool(4); # I'm going to assume you meant for this to be an object ;) $pool-&gt;enqueue(function($message, ...$args) { vprintf($message, ...$args); }, "Hello %s\n", ["World"]); $pool-&gt;shutdown(); ?&gt; So you don't need to wait for that to be possible, it already is ...
The changes regarding private and protected methods could be explained a bit better, but they seem a bit backwards. The behavior of is_callable() is currently "From this/given scope, can I call the given callable?" rather than "is this variable considered the psedotype callable." e.g, this method should always return true: function thisIsReallyAMethod(callable $callable) { return is_callable($callable); // the function should be consistent with the parameter type } The current behavior should be implemented as its own function, (e.g:`scope_callable($callable, $scope = current_scope)`). Implementing such a function in user-space (at least one that grabs the called scope) would be complicated-- and it still has its use cases. The RFC proposes excluding private and protected methods as part of the callable type. `is_callable()` would always return false for those methods. I have at least concerns with this: 1. What a call is, within PHP, becomes inconsistent. `$this-&gt;protectedMethod()` is a method call. Its callable from certain places, but it isn't a callable. 2. Compatibility breaks silently, e.g: This code used to work, now it silently skips over the method call without notifying the user. $callable = array($this, 'protectedMethod') if (is_callable($callable) { $callable(); } IMHO: 1. The callable type should include private and protected methods, they are callable-- even if its from a restricted set of areas. method_exists() doesn't change based on visibility, property_exists() doesn't changed based on visibility, is_callable() should move to be consistent with that 2. A private object should be allowed to pass around private callables privately. Passing around private/protected callables publicly should be discouraged by practice, not as top-down solution. Public methods should not need to call scope_callable()/is_callable() 3. `is_callable()` should be changed to return true for protected/private methods (likely in PHP 8.x or 9.x). While it may seem like a bigger backwards compatibility break, it breaks in a much more predictable way. (Giving an error, rather than skipping over code that used to execute). P.S. One of the examples does not work (*Private / protected methods report as callable when they are not*). The `privMethod()` method does not exist, so there is no fatal error. The method that does exist is `privateMethod()` tl;dr -- We shouldn't discriminate against protected and private methods. #privateMethodsMatter #protectedMethodsMatter
&gt; A better solution would be to support a shared, global heap for all threads in ZTS and then be able to reference functions (or objects) on the heap; then just pass a function pointer essentially, to various threads. This is just too scary to think about, everything from the ground up, the executor, all extensions, down to the macros/functions that manipulate zvals, absolutely everything (outside of pthreads) assumes there is one thread of execution. Sharing (violating share nothing), would be extremely dangerous, and for not much gain, copying a function into another context is *very very fast*, and as you can see from above, it appears as if the closure is being passed around. The programmer shouldn't really care how it works.
PUT for an already existing file. POST for a new file
Ok, thanks to all for suggestions, I'll implement some of them. But now I want to note that builder already can install 5 scripts: wordpress, joomla, drupal, typo3 and magento. Just command `bin/builder build joomla`and it's done!
RIP Alex my man.
I looked through the *password_hashing* section and also looked through the examples provided [here](http://php.net/manual/en/function.password-hash.php#refsect1-function.password-hash-examples) and get how to get a hashed password as it echo's correctly using &gt;echo password_hash("testing", PASSWORD_DEFAULT)."\n"; but I do not know how to get it so it hashes the password that users input and it checks for it in the database with the hash? Do I use **$_POST['password']** to hash the password that they put into the password field and checks if its stored in the database? Also, Do I need to add another field in my database to stored hash passwords or is that not needed because of the *password_hash* function?
could try: `strtotime('Jan, 1st, 2015 +'.($days-1).' days');`
Coming from the demo scene I like those kind of challenges. BUT, aAnything can be written in "one line". For all it's worth we could condense wordpress to a single line in a single file. When doing any such challenge you should either: 1. Do one line = one statement, therefore ; terminates a line. Your code is WAY more than one line. 2. Just set an arbitrary code length limit, eg 120 characters, not line count. That's usually how challenges work. 
You are scum. __EDIT:__ after looking through his history, this guy is a homophobic, /r/iamverysmart-worthy troll. Upvote him to zero, he just wanted to be 'edgy'. 
&gt; What about the other tasks? All the frameworks have to recreate very common functionality that could be handled by PHP itself. This is being handled more and more with dependencies by Composer.
[removed]
[removed]
No matter how you feel about a person's work, suggesting that they should die of cancer, leaving a young girl fatherless, is just disgusting. 
[removed]
Not a fan of Wordpress in the slightest, but it has it's place. You however need to learn to respect others before you will have one.
Wish we were all as enlightened as you. 
It is possible that the site uses some custom code for tracking sessions instead of using PHP's built-in session management. All other comments so far assume only built-in sessions. But even for a custom session solution, writing a garbage collector should not take long. A few hours to a day of uninterrupted work, tops.
&gt; Can you give me an example of what I would want to pass as configuration to an object like this besides, maybe, my database type, in this example? In this example, you may want to pass in the table names you want users to be stored in, the address of a memcached server to cache and respond faster to queries, various other business logic options, such as should the system send a registration email after creation, which email template it should use etc. etc. What you want to configure depends on how reusable and flexible your code is. Too flexible and generic also isn't great, but once you get the hang of the technique, use your gut, only the sky is the limit. &gt; If so, isn't this just the same as having a funciton with a switch inside my class in the end? If userType =1, do something. If userType=2, do something else, etc.... Maybe I'm too hard headed... Read the rest of my comment, I do mention switch as an alternative and I explain the drawbacks of doing it this way. In a nutshell, unless you expect to encounter randomly any or all of the options in your switch in every request, there is no point in loading the code for all options you won't use. Instead you can have an interface, decide once early in the app startup which option you need, and load the implementation only for that option. You'd probably agree, it's unlikely you'll switch your server, say, from MySQL to Oracle in the middle of a request. So why load the code for its "case" in the switch then?
&gt; I don't, and I also proposed an RFC today to make them easier to use by creating closures from them: https://wiki.php.net/rfc/closurefromcallable I suppose that makes it a little less annoying, but its still a bit odd that `[$object, 'somePublicMethod']` is callable, but `[$object, 'someProtectedMethod']` isn't-- at least different enough that they require different code to call, because PHP decided that some methods are not "callable". &gt; A callable is a type Yep, the `is_callable()` naming logically fits more with the type than the ability to call from a certain scope. &gt; Types should have the same meaning everywhere. If they don't, and the meaning changes based on where the type is being used, then you get stupid bugs or unexpected errors or just generally angry at the language for being a bit crap. I certainly see the appeal, but I also don't think that disallowing private/protected methods *everywhere* is the right answer for something like that. Would it be much different if the failure was scope-specific? (e.g: they're only valid if the function/method they're given to can call them)
Totally agreed. Thats why this turned into a count of characters.
Your comment made me think about the scoring system, which I knew wasn't perfect. So I *rewamped* it and now the score is a percentage relative to the time cost of the best (minimal) piece of code. ***What that means:*** the best code has 100 % score with 50 ms and the second best has a cost of 100 ms, the second score would be 50 %, even if it's the last one. The `call_user_func` call was a leftover from the times when all callables - not only closures - were permitted to be added as a single benchmark case. That unfortunately lead to more problems (with validation, etc.) than it had benefits, so I decided to focus *phpcb* to do only closure benchmarking. The `call_user_func` was left behind; now it's fixed. Thank you for the (most appreciated) feedback. :)
Sure, anything can fit in a single line... that is why this turned into a character count golf game.
You most certainly gave me some things to think about! Thanks :D
&gt; at least different enough that they require different code to call I probably ought to clarify in the RFC, that it's only the callable type-checking which is modified. This code would still work: class foo { function getPrivateCallable() { return [$this, 'privateMethod']; } private function privateMethod() { } function bar() { $fn = $this-&gt;getPrivateCallable(); $fn(); //Or call_user_func($fn); } } It's just that the return type of getPrivateCallable couldn't be `callable` and if you wanted to check whether the function is callable locally (rather than globally) you would need to use either `scope_callable($fn)` or `is_callable($fb, ..., $currentScope = true);` 
OOP doesn't ask for getters and setters for each object variable and it's mostly just wrong to provide them. There should be as few methods as necessary available outside of the class. You wouldn't provide something like getPassword. The class that has passwords instead should handle that logic. This might be something like $class-&gt;authenticate($password) Now the things outside of the class don't care how you represent passwords or how you verify them, you've only committed to returning true or false (possibly) to a single action. Everything else in the class can now be changed without having to update things that call it. Clear as mud? 
&gt; DateTime::createFromFormat('z', 237-1) This does [something very funky when the second parameter is 365](https://3v4l.org/KlrL4).
He made an offer to help, with the clear intent, in hindsight, to do nothing, but mock the people who reply for being so foolish to find use in closure serialisation. This is is not about him being entitled to his opinion, it's just disrespectful.
?
There's no reason not to use them. I do not think going 100% annotations is the right thing to do though. A good balance of yml, xml and annotations can make a project easy to figure out.
Personally I feel that comments should never affect the functionality of an application. It's an extra layer of "magic" that feels hack-ish and potentially harder to debug. I'd opt for storing configuration separately, though if you're fully invested into Symfony and/or Doctrine then I suppose you should adopt their standard conventions.
Yeah, I kind of figured that it wouldn't work in PHP. We can just work with what we have.
That's my feeling as well. It would be different if the language actually had annotation support baked in but docblock annotations feel like an abuse of docblocks. The argument always seems to be "Because dockblock comments are treated differently they are NOT comments therefore they can be used for meta programming".. and that to me is a bit of a cop out. They are clearly comments. Their treatment by the parser for the purpose of allowing documentation generation shouldn't be abused for meta programming. Furthermore, if we accept these userland implementations of annotations we reduce the chances that 'real' annotation support will find it's way into the language in the future. There is little or no motivation for core devs to run an RFC up if a userland solution is broadly accepted. That all said, as a tool in general annotations/attributes/metadata is a useful and powerful concept, so it's little wonder that hacks such as doctrine-annotations exist. 
Oh, my bad! I gladly stand corrected.
Just to over-simplify things, at a very high level, in OOP, there are two types of classes; classes that do stuff, and classes that *are* stuff. An entity is stuff. class Gremlin { public $id; public $name; } Pretty simple. Entities should be POPOs (plain old PHP objects) or POCOs if you're using C# or POJOs if you're using Java, or as close to a plain old -whatever- object as possible. They just hold data, they don't actually do anything, and if you're really super-modern they might even be immutable and contain value objects or something like that. Getters and setters aside (debatable in PHP), an Entity really shouldn't have any methods. A repository, a service, or a factory or a builder or an adapter or any other you-name-it pattern, is a class that *does* stuff. In the case of a repository, the job that thing does is persistence, or at least provides a layer of abstraction between your entities and where they are ultimately stored. class GremlinRepository { public function persist(Gremlin $gremlin) { $this-&gt;db-&gt;insertOrUpdate($gremlin); } } 
??????? Entities should have business logics in them. If you have only getters and setters, that is an anemic domain model.
I don't like mixing anything and xml, but I'm OK with mixing YML and Annotations in certain scenarios. For example, I prefer defining doctrine entities with annotations, but everything else with YML, in Symfony 2 applications. (My post below about adding a fourth configuration type is just a joke, of course)
Regarding the following line in `-&gt;get()`: return (isset($data) &amp;&amp; $data) ? $data : $default; What if the stored value is literally false, a 0 integer or string, empty string, or anything else that is a false equivalent? I'd just use: return isset( $data ) ? $data : $default;
According to? Entities are simply data objects, just like codenamegary said.
That’s because there’s only 365 days in the year.
I would expect it to error out, return false, throw an exception, move on to the next year, or start back over at 0. This? This was a very odd result.
Certain IDE's typehint/autocomplete method names for you. Some can also autogenerate getters/setters for you as well.
Symfony already really does it. Yaml for the the config.yml, parameter.yml, etc. XML for the service configuration. You can do annotations for routes and Doctrine2 typically does annotations for entity configuration.
You could also start with public methods and write `__get` and `__set` magic methods later if you need to. This is more idiomatically PHP. The typical Python way of dealing with getters and setters is to make the properties settable by default and then add a decorator to "methodize" them if you need. This makes for much more elegant and readable code than PHP, IMHO.
The objections to annotations always felt arbitrary to me. If they're parsed by native PHP, that's ok; if they're parsed by userland, that's bad. If we use string-based configuration and it's Yaml, that's ok; if we use string-based configuration and it's @Annotations, that's bad.
Putting all the text inside a textbox is not "an extremely limited fashion". Make boxes for each key/value they can change. Validate each and add them to the file.
I know this is a tangent from the op but i have been trying to figure this out as well. So the project (personal) i am converting involves user teams/groups. If the user model contains a $teams var that is populated by the teamsRepo with the team name, url, and minor stats of the teams is the user model reaching outside to much? When a user logs in, there is various data similar to this that is displayed on all pages in various parts of the pages, most of this data is set/defined by outside repo's. At the moment i just define it all when authenticating user and set it within the user model/entity. 
&gt; If they're parsed by native PHP, that's ok; if they're parsed by userland, that's bad I don't think that's arbitrary, I think that's consistent. Code execution by the language interpreter is universally better than a userland interpreter. &gt; If we use string-based configuration and it's Yaml, that's ok; if we use string-based configuration and it's @Annotations A configuration language baked into a string is indeed bad. If we're just talking key/value structure, that's not bad. So if we had Yaml key/value, and Annotation key/value, which would be better? Depends. Yaml key/value implies a separate, centralized configuration that can be independently modified. It also implies the configuration values are extracted via native PHP syntax in some way `Config::get('key')`. Annotated key/value configuration on the other hand, couple the configuration to the implementation. But that's not necessarily a bad thing: Consider this hyper-local configuration. It would make no sense to define a bunch of immutable properties for classes in a centralized configuration file, Yaml or otherwise. This type of configuration is inherently local in nature: /** * @immutable */ public function setWeight($weight) { $this-&gt;weight = $weight; } So that in itself doesn't make annotations bad. But something like `@route` *does*, as routing is, and should be, a centralized configuration. Your route definitions should not be broken up across dozens or hundreds of classes. Not to mention that a route should decide what its handler is, and not the other way around........ But looking at that example above, what do you think is going to be safer, more stable, more understandable, and more reliable: the above, or this? public function setWeight($weight) { if (!is_null($this-&gt;weight)) { throw new Exception('Cannot mutate the $weight property after setting it'); } $this-&gt;weight = $weight; } Sure, the annotated version is more terse and readable, but HOW does it enforce the immutability? It's totally unclear from the code. Some disconnected magic is happening, and the code is not transparent. Further, it's easy to mistakenly remove since it looks like a comment telling the developer the method should be immutable and not to call it twice, rather than something actually executing code.
 /** * @Route("/{id}") * @Cache(smaxage="15", lastModified="post.getUpdatedAt()", ETag="'Post' ~ post.getId() ~ post.getUpdatedAt()") * @Security("has_role('ROLE_ADMIN') and is_granted('POST_SHOW', post)") */ Look at this masterpiece I just took from Symfony's website. So readable, so debugable, so solid. For fuck sake, this damned thing uses `@Security` annotation to check if user has admin roles, and does it in a comment! Remove this and your application will still work while giving access for other roles. It also violates solid principles, adds external package dependency, slows your parsing considerably, makes you learn a new comment based programming language that isn't similar to PHP, and your IDE won't even properly do syntax highlighting until you find one "Symfony Annotations Syntax Highlighter" extension. This annotation thing isn't even included in the Symfony core, but you can use it only if you install SensioFrameworkExtraBundle and configure Symfony to load this bundle. Oh wait, the motto was `convention over configuration` after all, right. 
This is of course a matter of opinion/style, so I'm not saying you're wrong. But in my experience I've found it better to build things as simple as they need to be RIGHT NOW, and then in the future if I need more complication I'll add it in. So I'd think about if it's really worth all the extra overhead (both in terms of the complexity of your code and the additional runtime performance) for something you aren't even sure you'll need. And then in the future if you do need it, you can just change the property to private and write some accessor methods. Unless you're doing a lot of meta-programming, it should be easy to find/replace `-&gt;my_property_name` with `-&gt;myAccessor()` throughout your codebase.
&gt; Your Github account is less than 3 years old. Seems like a very silly metric.
Just use Rockstar. ;)
So far that's what I've found. Some of the other comments link to and mention testing and other issues. I might give both a try and form an opinion of my own.
I sure do. Saves time when you don't have to remember an extra language just to write an occasional script.
Thanks I'm starting to agree with you. Can you point me to some resources or explain a bit more why YAML or XML could be powerful options?
I use PHP for front and backend, it just makes life easier.
sure, laravel makes it easy to write commands... and in the end it's no difference to what the web server does.
If I need to use my own PHP models or classes, then yes I would write it in PHP. If it's a one off script, I usually use node.
I can't disagree. However, since we're talking about an entity in DDD context, I can't agree either. If your entities have only setters and getters, why bother implementing them in DDD? That sounds like a good fit for CRUD. I agree that an entity should not have all the logics. It can have just a few logics if those logics fit into an entity. That's the reason why we have domain and application services. You only need services if the logics in your service classes do not fit into an entity. 
&gt; knowledge of common software engineering patterns expert experience &gt; proper selection of indices Git workflow &gt; Experience with Continuous Integration (Circle CI preferred) maturity, responsibility It sounds like somebody with recruiter tourretes wrote this job description. The mixed use of capitalized/non-capitalized bullet-points also bothers me. Best of luck though!
I can see that being a bonus but you're basically saying you don't want qualified developers who write high-quality code for companies with non-open-sourced code-bases to apply. You're really eliminating a lot of highly-qualified applicants.
node + chalk + nomnom is pretty decent. For PHP, I either use symfony console component or [c9s/GetOptionKit](https://github.com/c9s/GetOptionKit).
Yes, like the other commenters here. But big surprise given you're on /r/php ;) Depends on the task, though. Really small stuff is usually straight bash, anything moderately complicated is PHP just because I can bang it out faster than a comparable language most of the time. My server automation stuff uses Ansible, which is Python-based (although 99% of the work is YAML config files). I'd strongly suggest avoiding trying to cleverly invent your own.
Heh, good luck with that stuff. Internals would have a field day! I disagree on the naming collision stuff anyways - it's not at all uncommon to have a getter just be the property name (commonly seen in Objective-C). Personally I've never had shared names become a technical problem, although it may be an indication of bad design. Case-sensitive everything will happen about ten years after the standard library gets cleaned up, which is to say never. I've casually suggested it though, as it makes searching codebases drastically easier.
I like where this is going, a lot. For the most part I haven't encountered the issues, but I'm sure it's a matter of time. Anything that's passed to a `callable` typehint should work as expected (`$callable()`), full stop. It's probably out of scope, but I'd like better control over callable signatures; basically something to use as a lightweight interface implementation. E.g.: function doesWork(callable&lt;int, string, MyClass:MyOtherClass&gt; $callable):MyOtherClass { return $callable(3, 'three', new MyClass()); } Ignore the actual syntax as it's not a suggestion, but hopefully gets my point across - there's a huge amount of stuff that will technically make it through (be cast to?) a `callable` typehint, but all that currently means is that it can be invoked. It will still throw up all over the place at runtime if the signature is incompatible, and it would be fantastic to be able to get that earlier in the process (static analysis FTW) On a related note, I'd love to see array typehints work more as expected when it comes to other array-like structures; that is to say if an object implements the pseudo-array interfaces (`ArrayAccess`, `Countable`, and `Iterator`), it should work anywhere that an array is accepted. Or maybe vice-versa; create `ArrayObject extends ArrayAccess, Countable, Iterator` and any true arrays will be accepted. There's an underlying issue with the pass-by-ref/pass-by-val semantics difference between arrays and objects, so by-ref `ArrayObject` may be a bit safer. Not quite sure what the answer is to this one, but it's a huge frustration for me and is very much on the same train of thought as cleaning up the `callable` typehint.
I use Laravel Artisan commands which are basically a neat wrapper around Symfony's code. It is quite competent, but I find they can be quite slow.. a BASH script to rename 500,000 folder names to lowercase, vs a PHP script iterating each one.. is much faster.
I use Laravel Artisan commands frequently to perform business tasks. They are a wrapper around Symfony Console which is great. We do things such as updating database records, importing CSV and Excel files, processing large data sets in chunks, and generally fixing any bad data that our bad codebase has created. (sucks) I wish I knew more Bash or Python or similar because I think they can be faster at some tasks than PHP. Bash scripting is just quite difficult to get your head around. (I think)
But what if I've had a loaded BitBucket account for 5+ years? The posting would disregard much of my experience from proprietary projects. Seriously, who in the hell in their right mind would write a posting like this? I hope this was not a developer. Keep in mind, this group is an agency, with an incredibly narrow job posting. My advice to would-be applicants: run away, you can do better.
I use annotations for both routing and doctrine nowadays and I absolutely love it. The code is way more clear to me this way and it's considered a good practice by the [Official Symfony Best Practices](http://symfony.com/doc/current/best_practices/introduction.html). As a prior Java dev I don't get this whole "comments should not affect an application" thing since to me annotations shouldn't be considered as pure comments in the first place anyway, but I guess it's different here
You got part of it right, and part of it wrong. A Repository serves only as an abstraction of your storage. It should ideally have only two functions: interface Repository { /* note the missing typehint.when implementing this interface, we want to specify one. */ public function persist($entity); public function getById($id); } An Entity - and its cousin, the Aggregate - serves as the representation of your domain logic. It contains the information necessary to perform it's purpose. class Employee { private $id; private $salary; private $balance; public function __construct($id, Money $salary) { $this-&gt;id = $id; $this-&gt;salary = $salary; $this-&gt;balance = new Money(0); } public function work($hours) { $this-&gt;balance-&gt;add($this-&gt;salary-&gt;times($hours)); } } In a DDD application, this would be used to perform domain logic: $employee = $employeeRepository-&gt;getById(1); $employee-&gt;work(8); $employeeRepository-&gt;persist($employee);
Yeah I know "do you use php" in /r/php is redundant but was more wondering about adoption for just straight scripting vs front end web design 
Sorry this did somehow go into the wrong tab. I'll delete it from here.
[removed]
Personally I barely touch the server at all for front-end work these days. React won me over. But I know what you mean either way :)
I agree bash sucks but i don't find file operations w/ php a pain at all; well unless they're several gigs or more.
That's likely, and really it should be OP's job to do this at all. But the simplicity of the sulotion (and others) show how much bullshit that developer is spitting.
Used to be 110%, but I've been melting down lately. 
Maybe you're right in that the most performance sensitive usecases are covered. I like the idea of Cocoa's [Key-Value-Coding](https://developer.apple.com/library/mac/documentation/General/Conceptual/DevPedia-CocoaCore/KeyValueCoding.html#//apple_ref/doc/uid/TP40008195-CH25). If you have got a property name, or a dot-separated path of names you always know how you can fetch the actual value.
Especially considering they don't actually specify that you need anything in the account.
I get the native vs user land argument, though I think it's a bit silly as-is. It's already awkward enough because the only ways to get at that information are reflection (ugh) and actually re-tokenizing the file (insanity) Technically the tokenizer does treat docblocks and regular comments differently, producing `T_COMMENT` and `T_DOC_COMMENT`, but by any traditional view, the source code should be able to be stripped of both and still function identically. Your opinion may differ, of course. Also, YAML isn't string-based, unless you consider your entire application to be string-based. It's not binary, but it's most certainly for structured data. Just a less-ugly array, really. 
My opinion on shell scripting is it doesn't fit on on screen or has any complicated logic other than just sequential tasks I'm doing it wrong in pure shell script
Built a forking extension. Useful for running console commands. https://github.com/jayesbe/php-process-executive
I generally like annotations for the simple reason that they make it very hard to make a mistake where you refactor a part of your code but forget to change the metadata part that lives in a separate file. When reading code, it also gives you the information you need, at the time and place you're most likely to need it (although IDEs can help with this even when using separate files, by showing those little icons next to lines that allow you to click-through to the connected parts). Depends what they are used for though. I do not consider them "configuration" in the strict sense of the word. They're meant for metadata. Annotations in docblocks are definitely a bad solution, but I sort of see them like browser shims and polyfills - they're here *now*, and the alternative is to wait for years and years to be able to use them the "correct" way. The good outweighs the bad in this case.
Ok... I think Magento uses it a lot under the hood, TYPO3's Extbase maps database column names to properties that way (e.g. `booking_uid` to `bookingUid`, or `start_date`to `startDate`) and Doctrine does the same if I'm right (http://doctrine-orm.readthedocs.org/en/latest/reference/basic-mapping.html#property-mapping).
&gt; find /tmp -type f -name sess_* +atime 7 | xargs rm -f I would really quote that `sess_*`.
Your @immutable example is impossible in PHP, at least if it's meant to do the same thing as the block of code below it. The annotation only serves to tell some *other* userland code something about that method. So for example, some code could go like if ($reflectionMethod-&gt;hasAnnotation('immutable')) { throw new ImmutableSomethingException('I refuse to call this method because...'); }else{ $object-&gt;$method(); } Therefore, this is really an example where annotations are *not* a good way of achieving the goal. A good example would be ORM mappings, because then the object still makes perfect sense even without any of the persistence-related annotations. Any code can create those objects, execute some business logic, etc. and not give a shit about any annotations - everything will work like it would expect. Only doctrine will explicitly look at them and say "ok, I am going to write the value of this property into this database column" when asked to persist the object. The meaning of any annotation should of course be documented, just like your methods, classes, properties, constants, etc. should be documented. With [doctrine/annotations](https://packagist.org/packages/doctrine/annotations), every annotation is a class, so you would just read the docblock above that class.
It's just a wrapper around Vagrant for those who didn't figure that out, yet.
&gt; and your IDE won't even properly do syntax highlighting until you find one "Symfony Annotations Syntax Highlighter" extension. Which exists for phpstorm. I have full syntax highlighting in my annotations.
Shouldn't your Repository also have other methods based on your Domain usecases? Such as "getPersonByGroup" or something similar?
Ah, that makes sense, thanks
Technically annotations aren't comments, they're docblocks which are different tokens (respectively T_COMMENT and T_DOC_COMMENT).
I've never written a utility script in PHP. My go-to shell languages are bash, because it's native and always there and so simple for simple things, and then Python for more complex stuff. But really a language is just a language. Use the tool best suited for the task. PHP may not be the best choice for long-running tasks but could be the easiest for web-related things.
This!
use the system and libs / programs written in C for 55GB files, or split the file into chunks. There is never any point loading huge wedges of data into any managed language as there is an overhead to managed data, and RAM has a speed, + operations are taken up "managing" the data
We all seem to be missing that its sessions being saved to disk for a number of websites on one server... Yikes! I'm going to say OP's problem should not be fixed by talking to the developer, or hosting. This business sounds like it has really bad practices and needs help from a professional that is not their developer, like a consultant, with experience in working in multi-application environments. Get someone in to be dedicated for X hours to Ops. Raise concerns with them, and a consultant who will assist you in hiring them; include one guy said yikes about disk-based sessions, what are the alternatives? It sounds to me like typical micro-business mentality, get developer, ask them to work with the worst work-flow possible, probably little to no budget, make them work solo, expect them to be developers and ops, all neatly bundled, cry yourself to sleep because nothing works... This is why you need the stakeholders to review digital, and online; be honest and realistic, and take pro-active action to ensure that everything is in place before the problems happen, but also during, and after, so you can iterate and revise business processes to ensure trouble free IT
Yes. Well that's the problem, isn't it? :-) Let's fix PHP, but no, no, no... Let's not fix it.
I've spoken about this before, but I'd suggest it's far more common in PHP for the right decision to be to have read and write models separated, and the business logic to be in the service layer that returns and accepts those models, because we are much more reliant on external state than DDD solutions with a persistent process. In a persistent process the canonical state is typically in one process'es objects, and the database acts as a glorified transaction log that is used in the event if is crash, reboot, power loss, etc. With PHP, we have concurrent processes accessing a shared data store, and we "crash", losing our local state at the end of every request. So local PHP state can't be considered canonical, and there is much less logic one can put on the models themselves that will directly transform state. We instead need to cooperate with the external storage layer more, because it's changing under our nose as other requests access it. Just personal observation, I do DDD both ways. When I have a persistent process and I can use pure local objects for all things in the domain, it is naturally much easier. But it scales less (only works in the boundaries of the process). Separated read and write models, which I believe PHP forces us into, scale better. I see many people use DDD in PHP as if they run in a persistent process. Unfortunately many popular ORM projects for PHP encourage that leaky abstraction as well. It easily leads to poor performance, race conditions that corrupt state under high concurrency. Remember folks. Just because it works in a serial set of unit tests, doesn't mean you have no race conditions.
Well, he did the math in PowerShell.
Sometimes, though recently Go has grown on me as I can just pass around a single binary as needed.
I've used it for basic scripting... but looking at Symfony's Console component /u/talisto posted... I might start looking at it more. &gt;Curious how many people here are backend vs frontend PHP is a backend language rather it's used for web development or CLI scripts. The server is the one compiling and processing the code... not a client. Frontend would be more HTML, JS, CSS...
Nobody else in this thread has mentioned it, but Ruby is a good option too. That's my fallback if straight bash is going to be too complex.
Neat but as improvement it could really use a size-limited pool/allocation, using his example you wouldn't really want to fork 500k process in a matter of seconds
I am very surprised by your comment. Why do you say this?
I really dislike bash's syntax ... 
The documentation says it can be null. I guess it is null if the GraphUser isnt valid? You don't do any checks after your $user = $me-&gt;getGraphUser(); line. So you don't even know what the $user is. You should make some checks to be sure, it is what you wanted it to be (a valid GraphUser). Ninjaedit: Documentation https://developers.facebook.com/docs/php/GraphNode/5.0.0#user-instance-methods
The parent poster is not wrong. They describe one approach to DDD, you - another. DDD has strategies, and tactics. The strategies are universal, the tactics are not. Case in point, your example has a *race condition*, as you used a model design that is only suitable if your domain state is operated on from one process, which is not the case in PHP. The entity you read from a repo is outdated the moment you read it in PHP due to concurrent requests, which have a *separate copy of all your objects*. So you need to materialize this cross-process sync somehow, one way is split read-write models with logic focused in the service layer, or you get inconsistent data.
&gt; Your @immutable example is impossible in PHP No, I don't think it is. You'd use the pointcut/interceptor pattern like you do here: http://sf.khepin.com/2012/06/validate-your-user-before-any-method-call/ and here: http://jmsyst.com/bundles/JMSAopBundle#usage I don't know *exactly* how this works, but I imagine it does something super gross and nasty like intercepting the class call at the autoloader level, and substituting it for a proxy that reflects over the original class to scrape annotated information, which then acts as a gate between you and the original class. &gt; Only doctrine will explicitly look at them and say "ok, I am going to write the value of this property into this database column" when asked to persist the object But that still means that Doctrine is using a string-based proprietary DSL bolted on top of a class which now has the added responsibility of making sure some *other* class can understand how to perform operations on it, in this proprietary way.
PHP is a backend language, just like Python, and Ruby, etc. I really dont understand how many people can use PHP and not know the difference between backend and frontend. Is Chrome rendering your PHP?
Don't confuse "entity" with "model". All your business logic should be in the model, yes, but the entirety of you model is not necessarily *just your entities*, but also any "managers" for those entities and other APIs which expose queries and commands to be performed on your domain. How much logic you put in an entity is a contextual decision. In his example, those are not the entities per se, as much as they represent a possible snapshot of an entity, i.e. it is a DTO. You wouldn't put key business logic on a DTO.
For me, the vast majority of scripts are bash (plus sed, awk, etc) or python. They just integrate a lot better with the system than php. For example, you can basically always count on bash and an older version of python being installed. For server automation, I've used chef and ansible. Chef requires a bit of ruby knowledge. Ansible is mostly YAML for simple things, but more complicated things may take some bash and/or python. I've made a few CLI utils using c for raw speed or reduced memory usage, but I don't know if that counts as scripting.
&gt; Case in point, your example has a race condition, as you used a model design that is only suitable if your domain state is operated on from one process, which is not the case in PHP. The entity you read from a repo is outdated the moment you read it in PHP due to concurrent requests, which have a separate copy of all your objects. So you need to materialize this cross-process sync somehow, one way is split read-write models with logic focused in the service layer, or you get inconsistent data. i am well aware of the concept of eventual consistency. What you seem to be missing is that there are several strategies to prevent a *race condition*, that can -and should - be implemented inside of the storage layer (Repository). Like database transactions or version locking. Completly hidden from the domain user. Actually, when using logic focused in the service layer (which DDD does not know), you are surely doing something. Some tactics or strategy or whatever. But no DDD. Sorry. That is no "approach to DDD". Just as homeopathy is no approach to healing cancer.
What if I use Vim or Sublime Text? Does your framework also make you tightly coupled to a certain IDE too? Why a PHP Framework would require different extensions just to highlight syntax? Seriously, you're trying to copy annotations from other languages such as Java which is already supported at the core of the language. @Override public function something() This is an annotation. It makes sense. It also helps the `compiler` to be able to warn certain issues. /** * @Override */ public function something() This is a comment. If you're relying on a magical parser which can parse comments to develop your application, then you're doing something really really wrong. Is relying on comments becoming a hip lately? AngularJS also has a support to rely on HTML comments to place directives but having a proper qa/build workflow should get rid of them and minify files accordingly. Nobody will change their workflows just because a random guy in the company wants to write directives as comments and not rely on HTML attributes as any sane person would do. Not to mention, you're literally breaking most of the software design principles.
Why? Granted *sh has some fairly weird syntax, but if you do much non-PHP you probably find plenty of PHP syntax weird. *sh languages also have the huge benefit of portability and generally needing less boilerplate. The stream redirection and ease of paralleling up many small tasks in particular are really nice.
I do my application support scripts in the same language they are written in which is PHP. If I was hired to do sysadmin background tasks stuff I'd use Python.
What kind of configuration file are you talking about? YAML, PHP, XML, JSON, or does Symfony has a special component to rely on comments for configuration files too?
What i am **actually** doing is using a event-based insert only database. if concurrent transactions occur, and one tries to do something that has become invalidated, it is rejected. Stuff being out of sync is something that actually happens in many architectures, not just the one i described. Point is, resolving these issues has to happen in the storage layer. The domain logic has to be decoupled from infrastructure logic. That is the point of DDD. Business stuff is done in business models. preventing data loss is done in storage. IDGAF if you store all your stuff in one big table (like reddit does), or serialize it to CLOBs and fax it to your bank as storage. Thats not my domain's issue.
You are describing optimistic locking (which, BTW, is distinct from "eventual consistency"). If this is the case, your code example above is quite misleading, because it's missing one big fat **try...catch** block around everything, to catch VersionConflictException or SerializationException, or whatever you decided to call it, because you need to repeat your entire controller if a conflict is detected. And you better hope nothing in that try block has side effects, because there be dragons when you repeat! So much for isolating infrastructure, huh?
All the time.
Yeah, I'll sometimes start writing a PHP script to perform some task and realize that I'd be easier to do in bash.
It's really hard to take your current package seriously because it has no unit tests and: - doesn't use [namespaces](http://php.net/namespaces) - doesn't use [composer](http://getcomposer.org/) - uses a GPLv3 license instead of business-friendly MIT - reinvents oauth1 instead of using [league/oauth1-client](https://packagist.org/packages/league/oauth1-client) as a base (Also worth noting that you can also use [built-in functionality of oauth1-client to make arbitrary requests](https://github.com/thephpleague/oauth1-client/issues/20) though it is not fully documented.) **EDIT**: and btw, there are already [two other smugmug packages](https://packagist.org/search/?q=smugmug) already on packagist.
If the code is rendered server-side it's backend. If the browser is renders it, then it's frontend. Yes, you can break down backend into different layers but regardless it's all still server-side code.
You should look at Cronjobs all the features you need.
Yes, in postgres 9.5 and newer there is built in syntax which looks like: ... ON CONFLICT UPDATE ... I'm guessing you agree with me that I said that it does support it?
&gt; If the code is rendered server-side it's backend. If the browser is renders it, then it's frontend. No, the front-end is in front, the back-end is in the back. 'Front' and 'back' rely on context to have meaning. Google "ffmpeg front-end" for example; despite the fact ffmpeg is mainly a library for other programs people consider it to have a 'front' and 'back'. Why is that? Because an ffmpeg front-end doesn't actually encode videos or any of that, all it does is give a nice interface to the actual encoding logic in the ffmpeg library. It's 'front' because it faces the customer and relies on something in the 'back' to do the real work. So in the web-dev sense you mean the front-end is what goes on in the browser because the browser is doing everything for user-interface, while the server-side code is handling actually processing the data and doing whatever it is the site does. From the perspective of a server-side programmer with no involvement in the client-side interface the code that processes incoming requests and communicates that to the business logic is the front-end. If you're writing an ORM the exposed interfaces on your functions/objects are a front-end to your SQL generator and serializers and all that. Etc.
The primary purpose of this library would be to store a key:value store of the closure and the date it needs to be ran. Not to do the scheduling itself, but to resolve the task at hand on the date.
I use [WordPress CLI](http://wp-cli.org/) all of the time... 
Yep, for anything more complex than a 3 line shell script, I use PHP. I've been programming in PHP for about 16 years, so I can usually write pretty large scripts off the top of my head without needing to look anything up. Super fast to get shit done, and I'm yet to come across any problems using PHP instead of something more common to CLI like Python or complex shell scripts. It also means that moving some code from a website to CLI or vice-versa is effortless.
yes. I do.
&gt; it would still be great if there was a way to pass references to functions around without treating strings as function names. Say for example, as a closure? Someone should totally write an RFC for that. Oh wait, what have we here: https://wiki.php.net/rfc/closurefromcallable You then don't need to test against callable (which can have unexpected results) you can test against closure, which is not confusable. &gt; support was added for the values to be able to be closures in order to gain a bit more flexibility like this: &gt; ... &gt; Yeah - the train has left the station long ago, but one might still dream. Right? I can't put this nicely; your problem is your config sucks, and not being able to tell the difference between a function and param name is the real problem you have. Writing up how I do config nicely with Auryn in on my list of tasks to do, but I don't have time to write it it now. But basically what I do is: * [Setup the list of functions that need to be called, as just strings of function names.](https://github.com/Danack/Imagick-demos/blob/master/src/injectionParams.php#L60-L61) * [The defination of the delegate functions are here](https://github.com/Danack/Imagick-demos/blob/master/src/appFunctions.php#L739-L755) * All of the injection params are [passed to the injector in one go ](https://github.com/Danack/Tier/blob/master/src/Tier/InjectionParams.php#L140). It's has all the power of being able to delegate config setup to functions without any confusion about whether something is a function or a param, no over-head for setting up closures, and nothing, except the actual app runner needs to know about the DIC. 
I get what you mean. You should be using Cache-Control set to the publish time, instead of trying to manually control the cache flushing. If you need any more finer degree of control than that, then you shouldn't be using full HTML caching in the first place. (Static-resource only caching would be more appropriate) Or alternatively, make your CMS ping / push to the cache server to reset the scheduled flush time whenever the scheduled publish time is changed.
I have no objections to using annotations provided there's native language support for them. Annotations can be great. What sucks is using annotations in a comment/docblock which is clearly a structure not meant to *define* application logic/behavior, but to *describe* it.
There is not front end access. why are you quoting this? Please just consider for a moment that your view on front vs back, may no be correct. We clearly know how to code these languages out, but don't get the implementation of the front and back wrong. They define very clear understandings on each languages role. The browser CANNOT read python. Python renders data from the server and gives it to the browser as 'html' or 'js'. If you typed out a whole page in python and tried to run it locally without defining a 'server' there wont be any logic for python to run. But you can do this with html because THEY ARE ONLY FRONT END.
I've created this library for the specific needs of one project : - I need to be able to use a datastore at a time and migrate easily when the project grow without model modification - We use at beginning a NoSQL store (Elasticsearch) but we can move to any datastore which match our needs The goal of this library is to detach all the data manipulation from any query language or specific database system. I've used the ActiveRecord model because it's just what I need. What are you thinking about ? I'm currently working on some issues defined here: https://gitlab.com/bee4/activerecord/issues I'll be glad of some of you can help ;) Next steps are : - MongoDB integration ; - Transaction upgrade to be fully operationnal ; - Stability, tests and documentation. I've made the repo public really soon so it's only my point of view for the moment! Thanks for your help :)
Sorry but it's unclear to me what you point is.
Actually i still have no idea what you are actually trying to tell me. Data is out of sync all the time. Web clients are out of sync with their backend. Backends are out of sync with their databases. Databases are out of sync with each other. Every time something happens, stuff is out of sync. And then shit happens. Yes i do have a try...catch block. Not a fat one. A very small one. Because i allow only atomic operations. If they fail, it is the frontend's job to take care of resolving it. "The profile you were editing has changed." 409. And my storage will have to rewind the changes to the database. thats what you can rollback transactions for. try { $entity = $repository-&gt;getById($id); $entity-&gt;mutate(); $repository-&gt;persist($entity); return new SuccessResponse($entity); } catch (StorageConflictException $e) { return new ErrorResponse($entity, $e); } There are no controllers that do multiple things. Make them one atomic step. With their own aggregates and repositories. If that atomic thing fails, retry or leave it be. Why do that? Easy: Business use cases are translated into code. You can actually read and understand these with ease. You dont have to worry, what mechanisms and strategies are employed on storage side. Single table? Document storage? Filesytem? That code does not get in my way. Business changes, needs code changed accordingly? I can do that. I can test my business cases. I can run 5k test cases in 5 seconds. If it depends on your storage, you can not. You set up fixtures for each test, you get cross-test-sideeffects. Tests run slow. Coders start to despise to run them. You implement CI. Checkins start to take hours away from productivity. If you have found a way of doing things so they never fail, please show me how. And they best be concise and easy to maintain. And maybe agnostic to their database? So what is this argument about? 
Pretty sure your downvotes were people people misinterpreting what you meant by front and back end. There is a thread higher up that has an argument. Some seem to thing it should be relative (I agree) and some think it should be synonymous with view and controller. 
Ah true true, in the DDD context, agreed. I was referring to in the general case.
That's pretty much my exact setup! Pretty nice: https://github.com/titon/toolkit/blob/3.0/titon
You're going off on a tangent and contradicting yourself. Let me explain: - I never said "you should expose your storage medium". I said "you have a race condition". - Then you said you use optimistic locking. So I said "then you need to handle conflict and redo". - Your original example didn't show handling conflict, it was misleading. And you just confirmed that it was. - Saying "I let the front end deal with it" doesn't mean you encapsulated the problem, it means the opposite - you pushed the problem up the chain. The controller that will repeat may be PHP on the server, or JS on the client, but in both cases someone will have to safely repeat their logic, or propagate the error even *further* up the chain, until it blows up in user's face, which is bad UX. So "I let the front end deal with it" while it is an option, is a very poor example of hiding storage mechanics, which is your main focus, because optimistic locking is one of *many* options, and now you exposed your technical choice to the front end. Only optimistic locking results in errors that require action repeat. Here is a question. Think of public APIs like those from Google, Facebook. How often do they say "we might just occasionally tell you to repeat yourself despite connectivity is fine and all. So repeat yourself when you get error code RepeatYourself". Optimistic locking is a great technique, put it is typically an example of a really bad design to let it bubble all the way up to the front end. 
Look what happened to Python. There's a totally valid reason for that attitude. Not saying I agree, but a near-rewrite of the API for the most widely-deployed language on the planet isn't exactly a small task. 
I wouldn't call what I suggest a near rewrite. 
* I am **not** against actual configuration in YAML/XML/JSON/PHP/ini. * I am against configuration in a DocBlock if it affects execution of the application's code. I don't even like using `@expectedException` or `@test` in PHPUnit * I am ***fervently*** against DocBlocks with *string expressions* that contain logic or special syntax which affects the execution of the application's code. * I am **not** against DocBlocks used for external meta tools such as API documenters or static analyers * I am mostly against DSL *string expressions* of any kind, regardless of where it lives or what its purpose is, but the simpler the expression, the better. Laravel's `Controller@method` is fine. 
&gt; not the entity itself (which is in the database) Well, I'd disagree that the entity is necessarily "is in the database". The database usually stores some sort of serialized version of the entity. Its possible that someone can program the entity in the database itself, but that appears to be uncommon (as opposed to using C#, Java, or PHP). &gt; In DDD you need one process where canonical domain state is captured as a graph of objects. &gt; &gt; And pretending that there is one PHP request happening at a time means race conditions and data inconsistencies. No you don't, DDD can be done in multi-process, multi-server, and multi-datacenter configurations. Having one process is not a requirement. (The DDD portion doesn't really care that much about the database/storage). &gt; So DDD has to be implemented slightly differently that you would in, say, a persistent Java process. If you want to follow DDD verbatim, I am afraid PHP is a bad choice. But with some small adjustments, it works great. So, pick your boat, as they say. Not much differently than implementing a multi-server DDD solution. The example given didn't include versioning or conflict checking, but really, that's better left for the repository to take care of (unless the Entity is event sourced, or similar). The entity doesn't really *need* to know how its serialized, or that a serialization attempt failed. Short lived processes have some advantages and disadvantages, long lived processed have their own advantages and disadvantages. Needing to think about multi-process up-front doesn't make PHP a bad choice for DDD. The PHP/DDD community is growing, and it handles DDD pretty well. 
I hate to be _that guy_, but I wrote a somewhat simple package that has a pretty modern approach and was rated highly by the assessments of "Scrutinizer-CI" and "Symfony Labs Insight": https://github.com/Rican7/incoming
&gt; I am not against actual configuration in YAML/XML/JSON/PHP/ini. ... I am against DSL string expressions in config of any kind, regardless of where it lives. These statements are contradictory. It seems like you're making an arbitrary distinction between "actual configuration" and "string expressions". Would `path: /blog` be "actual configuration"? After all, it's just a simple key-value, right? What about `path: /blog/{slug}`? Still "actual configuration"? Or has this crossed your line into "string expressions"? The braces in the path don't have any special meaning to the configuration format; it's passed verbatim to the router. What about `requirements: { slug: \d+ }`? It's still just key-value pairs, and the regular expression again has no special meaning to the configuration format. Yet this is something you called out as "unacceptable shit".
Sorry - no, of course not. I meant most of the community's requests around what's "wrong" with PHP as a whole. Individually they are (usually) not too bad.
Indeed you are right :) I think that inferior is not the important thing. Maybe the ActiveRecord is not the right pattern but the idea behind the lib is a real new approach to data manipulation. I mean that I'm not coupled to ActiveRecord itself if it's the problem ;)
Is there a way to use the console component globally? Probably just install via composer w/ the global switch/argument passed?
Well you are just playing the word game, I aint interested in this. Docblock comments are comments, and more precisely a subset of all forms of comments. A docblock comment is a comment, but a comment is not necessarily a docblock comment. Ford is a car, while a car may not be Ford, see this analogy? But either way, it doesnt change the fact that all docblock comments are comments. Just because you call your docblock annotations, doesnt mean they are not comments. They are still being ignored by the language parser/interpreter, and treated by PHP as comments. And as I said earlier, comments should never affect the code, nor stop your application from running if modified or removed. My point stands, and userland implementation of docblock annotations are abomination. 
&gt; Well you are just playing the word game, I aint interested in this. lol. I'm trying to get you to think *beyond* mere labels. If there's a language construct that doesn't behave at all like a comment, then we shouldn't treat it like one.
But it is not a language construct, PHP does not support annotations natively. Symfony and Doctrine's annotations are bad workaround for PHP's lack of annotation implementation. This approach is more detrimental than beneficial, because it breaks the very fundamental rule of programming - Comments are not supposed to affect code. And there are serious consequence to this violation, as your code become tightly coupled to comments. If you want annotations that much, you should try using a different programming language. Or you can use Hack/HHVM, I think it has annotation support(just checked, its called 'attributes', but may suit your need). But anyway, docblock annotations are clearly abomination, period. 
A docblock is meant to do something specific. An annotation is meant to do another, completely unrelated specific thing. Using docblocks to house annotations is like taking a screwdriver to a screw but using a hammer on the screwdriver to nail the screw in. Yeah, sure it works, but you should probably just use the screwdriver until you can get your hands on a drill.
Well if XML or YAML are written in docblock comments and they start to affect code, they will be abomination just like PHP annotations, and I will be against them as well. But they dont, XML by itself is a markup language independent of PHP, so is with YAML. If you create an annotation markup language, save annotations in separate files rather than docblock comments, Id have no problem at all. But who will do it this way? I can see past the labels, you are the one who cannot. Again, its not a language construct. Comments in PHP(and any programming languages) are language construct in PHP, annotations in PHP are not, they are treated by PHP as comments. And in fact, they do behave as comments, according to PHP parsers/interpreters. If you copy/paste the docblocks to another framework which does not parse annotations, they will behave just like comments. Just because Symfony and Doctrine introduced their annotations parsers to turn comments into black magic, doesnt mean they are not comments.
&gt;submitting to /r/PHP &gt;Please note that /r/PHP is not a support subreddit. Please direct all support-related posts to /r/phphelp, or connect to ##php on Freenode IRC. This is directly under, and to the right of the submission box.
yeah, unless your password hashing algorithm is extremely complex and involves other entities, you should leave this logic inside your domain models. Stripping out all business logic from your models are bad practices, its called Anemic Domain Model. Read more at: http://www.martinfowler.com/bliki/AnemicDomainModel.html
You can, the problem is that composer's "global" install is actually just for the current user, it's not really global system-wide. So it won't allow any other users other than yourself to use it. You're much better off making a self-contained "[phar](http://php.net/manual/en/intro.phar.php)" executable for each of your CLI programs, which will contain the required dependencies inside the phar. You can see an example of how I've set up one here: https://github.com/talisto/cpvc
I don't have time right now to tell you what you may be missing out on just because you don't like the syntax. I program 6 languages professionally, and that's not special but I can tell you the syntax should not be that foreign to you as a php dev. Are you on Windows or something? Do you not know anything about Unix? I am seriously trying to figure this out.
Not 100% related but as I was reading through Part 1 of this post, I noticed this: &gt; I also set up an artisan command If you're using Laravel you don't need to make an artisan command to do this. Rather, making a job that imports the CSV, then whenever a CSV is uploaded, dispatching the job into the queue, seems like it would be more optimal. Since queued jobs run separately from the web request processing, you don't have to worry about the PHP timeout issue.
No, I do all my dev on linux. I really just hate BASH... i really don't know why myself... but there is just something about it ... I use php, perl, bash everyday... I just try to minimize bash as much as possible haha.
Sorry mate, but all i did was present a simple example to OPs original question: the difference between Entity and Repository. Not how to master any pitfall you can come up with. Maybe i got you in a bad mood by calling you out on a service oriented architecture being something else than DDD? I dont know, but all i see is you trying to find flaws and me offering possible solutions. 
I don't feel called out? Requiring that every interaction with the repository is wrapped in a try block and repeatable is a very important thing to miss in your example, because without that **you can't model your entities in this manner** in PHP and preserve consistency (short of locking the entire domain every time you interact with it, like PHP sessions work, for example; not very practical as a domain is not segmented strictly by user). When you make a design choice that **fundamentally changes the way the domain is used**, hell yes I'll ask that you mention that. You can still implement an entity and a repository, which doesn't produce serialization errors and is consistent. But not in the way your example had mutation methods on the entity. Do you get it now? I frankly doubt you will, because your ego depends on going off on tangents, and ignoring my point the entire time.
Can we stop with the automatic downvoting of anything CodeIgniter here in /r/PHP? Not only are there still lots of people using it, but it's being actively maintained by a group of people who are very interested in bringing in more best practices, modern technologies, etc. This post could very well be relevant and interesting to a number of /r/PHP readers.
I don't know Cocoa, so I can't really tell how that works out in practice. Does it retain proper type information? Because that's the only thing that bothers me about associative arrays being used everywhere: you lose type-hinting in IDEs.
Oh 3) annotations have no parsing slowdown over configuration files when in production - both are cached.
&gt; neither comes equipped with the business logic required to repeat the action without involving their caller. When that condition is encountered, the repository should fail (e.g: throw an exception). What happens after the failure depends on the application. In a system with commands, the command can just be attempted again-- or the error can be reported back to the user. There is no one-size fits all. DDD doesn't change that. The retry/error logic is better living outside of the code that operates on the aggregate (e.g: in a decorator or controller). &gt; The only way to handle it internally in the storage medium is via explicit locks, i.e. pessimistic locking, which enforces one-by-one serial execution, which means there is one process at a time accessing and modifying the domain. Which brings us back to "one process". Only one process can change an aggregate at a time-- but again, that's not different than a non-DDD system. DDD-encourages setting designing consistency boundries up front (with aggregates). As long as more than one aggregate exists, two or more aggregates can be changed at the same time. &gt; To get back to optimistic locking, another problem I see a lot is "optimistic underlocking". People only verify the version of the rows they're writing to, but not the ones they're reading from which directly affected the decision about what you ended up writing elsewhere. The aggregate is supposed to ensure its own consistency, it usually doesn't have to read any other aggregates or entities. Operations that require more than one aggregate are not a responsibility for the aggregate (more on that later). &gt; Here's an example of this: http://guterfluss.blogspot.bg/2013/04/about-how-to-use-optimistic-locking.html[1] - the article demonstrates treating optimistic locking like a black box may end up biting you in the behind. Its looks like an aggregate design problem. From the looks of it, they were treating the BackOrderItem as an aggregate root, operating on the BackOrderItem aggregate root, but trying to save cherry-pick the changes made to the Task. That's a design problem, but not a DDD problem. &gt; However, even that article misses a problem: the situations where you read from one aggregate root and that might affect the writing decision for another. So even locking an entire aggregate root isn't enough. In fact, short of locking the entire domain, nothing can protect you against race conditions, if your business logic does enough of cross-aggregate interaction. This is a well discussed/debated problem in DDD. Aggregates are only responsible to be consistent within themselves. An operation on multiple aggregates can not be guaranteed to be consistent. (Yes, you can get around this by using a single process or a single database, but that isn't a good solution, really). Still, there are several solutions available. In general, I prefer using Domain Messages/Events to communicate across multiple aggregates. For the example with the BackOrderItem, each task would (after saving), dispatch an event, say `TaskHoursSet`. Another aggregate root/saga/manager would listen for that message and then act appropriately. It would not use the same object that was acted on earlier, it would ask the repository for one. You no longer have multiple aggregates changing at once. The manager does not need to be in the same process, but it can be. It can be running in a seperate process (communication via IPC). It may not even be running at all, e.g: queue the event and process it later. This is why in most DDD communities, aggregate design is emphisized. It doesn't make sense to prevent Task B from being updated because Task A was updated-- that's why they're different aggregates. The third, unrelated, aggregate doesn't change that fact. Communicating between aggregates may or may not be important, there are different ways to solve it (but it doesn't require everything in "one process"). Most resources seem to encourage smaller specific aggregates that only perform small tasks-- that reduces the risk of conflict more. That being said, using DDD doesn't create these problems. Unless the database was configured very specifically, these problems exist in other applications. The information retrieved from the database is out-of-date the moment it is retrieved. A user can request an update, only to have outdated information. The general solutions tend to work with and without DDD: locking, making changesets smaller, allowing changes that are compatible with one another. Still, we're far away from requiring a single process.
How else are you going to configure a non-standard ORM? YML and XML may be standards, but structure of the configuration (the mappings, types, attributes are still non-standard. Yet, even being "non-standard", there are already several open tools that work with annotation data and link it with in-PHP classes. The very same objects created in the annotations can be created without annotations (with more code). Despite PHPdoc blocks being non-standard, they're used throughout many tools, and even have an API within PHP. Should we stop using phpdoc attributes as well? (No more @return, @param, @var etc). They're tools, they're designed in a way that works, and in a way that allows the flexibility to move to other tools later. Isn't this a much better solution that the ActiveRecord ORMs that require you to extend their base class and use their methods?
Ask this on /r/programming and the answers will be very different.
If I'm making tools that coincide with an existing PHP web app, then yes. Otherwise, ehhhhhh probably not. I certainly *have*, and Symfony2's console stuff is pretty nice, but it's usually not my go-to.
&gt; In a system with commands, the command can just be attempted again-- or the error can be reported back to the user. There is no one-size fits all. DDD doesn't change that. If we had a system of commands, this means the entities don't have mutators on themselves (that mutate them locally in memory) and then get persisted. Instead we have queries producing *read-only* models of an entity, and commands to mutate them. In such a system effectively the read model is a DTO, nothing more. And all our argument about where the "entity is" becomes pointless. The big point of contention is whether to put mutators on the entity, which mutate it in memory (oblivious to the current state of the domain, which is away from the process, in the db), as that's where all the problems in concurrency come from. And optimistic locking as I discussed before is not automatic, or free. It requires careful thinking how to materialize all the conflicts with regards to the business domain (not just storage), and then requires pure controllers that are repeatable. But if we have command-query separation... the problem goes away. Poof. Because the command encodes intent, not just data delta, which I can see you do understand clearly. &gt; Its looks like an aggregate design problem. From the looks of it, they were treating the BackOrderItem as an aggregate root, operating on the BackOrderItem aggregate root, but trying to save cherry-pick the changes made to the Task. No matter how it's designed, you'll have cross-entity interactions and cross-aggregate interactions in a domain. I mean, after all, if you had no interactions between them... they might as well be separate domains, no? Which brings me to the next quote from your reply... &gt; Aggregates are only responsible to be consistent within themselves. An operation on multiple aggregates can not be guaranteed to be consistent. That's right, but it's a bit like saying "it's nice to have nice things". It's similar to something we see in microservice architectures: "oh your microservices should be isolated and not have shared state etc.; each is only responsible for its own consistency". And it's indeed the case, but what is the result? The result is microservices which start implementing various aspects of transactions at the *application level*, because while you may avoid microservices that need to work in concert, say 8 times out of 10, there are those 2 times where neither ignoring consistency problems is a good idea, nor is it a good idea to merge them into one, uhmm... "macroservice" :-) ... and it's the same with DDD aggregates. But that's not something PHP developers understand at large, this is why I'm very careful what I recommend in this subreddit when it comes to how to model entities and so on. They don't see the problem of consistency, they read &amp; implement the examples from DDD verbatim, thus landing smack in the middle of a solution where aggregates are explicitly independent, but implicitly dependent, they access and modify them concurrently through outdated in-memory models using a codebase that considers no concurrency, and are completely ignorant about the problems this may cause. Because DDD said so on a blog overview somewhere, and because their ORM has some magic that promises really hard that it'll all be ok. &gt; In general, I prefer using Domain Messages/Events to communicate across multiple aggregates. For the example with the BackOrderItem, each task would (after saving), dispatch an event, say TaskHoursSet. Another aggregate root/saga/manager would listen for that message and then act appropriately. It would not use the same object that was acted on earlier, it would ask the repository for one. I'm nodding my head in full agreement, but to come back to our argument, if we use commands, queries and events, where are the "domain entities"? They're no longer materialized in-full as objects shared throughout the app and full of business logic, they're a concept somewhere between the database and the service layer. Instead, what gets shared throughout the app are entity-like read models = DTOs. No? &gt; That being said, using DDD doesn't create these problems. Of course. I never thought the problem *is* DDD. DDD is a great book, that takes eternal principles of system design, and combines them in an accessible mental model for modeling business domains. The problem is reading DDD too literally. Say, mention "commands, queries and events" to a DDD "literalist" and they'll reject the notion that this has any place in a DDD system. It happened to me in this very thread (and many other times). &gt; The information retrieved from the database is out-of-date the moment it is retrieved. A user can request an update, only to have outdated information. The general solutions tend to work with and without DDD: locking, making changesets smaller, allowing changes that are compatible with one another. Still, we're far away from requiring a single process. I fully agree, but within your description here is hidden the need for some forethought that isn't happening with DDD mainstream fans. If you ask them, it's "simple": $thing = $repo-&gt;getThing(1); $thing-&gt;setSomething($thing-&gt;getSomething() + 123); $repo-&gt;persist($thing); Of course, we can have *granular* locks, we can have explicit &amp; optimistic locking, we can have change commands that encode *intent*, and so on, but we need to have people acknowledge first, that these concerns don't exclusively exist within the repository, but also involve *at least a bit* of consideration in the layer above. Because full locking (or single process) is the only thing that works *entirely within the repository*. That's my point. Everything else requires *some* cooperation from above, and alignment with business logic.
Yeah i know "lol php sucks" was only interested in people that use it in some context or another
I'm not sure about libraries, but I've used SendGrid's inbound API to do this easily in the past. If you go the API route, check SendGrid, MailJet, and MailGun to name a few.
Right, one tiny little detail: Why a page where I'll send a sample of my work to potentially become a writer for you asks for "payment method" or "billing address"?
I know the Data Mapper pattern is better in theory but Active Record isn't as bad as Reddit makes it out to be. Everyone here acts like Active Record is the end of the world. Active Record trades off separation of concerns and a small amount of performance for faster development. I don't believe it's the end of the world if my models are responsible for persisting themselves, and the performance difference is negligible until you start returning large numbers of raw records (which is rare--generally you want to use pagination and let your database handle aggregation and sorting). Both Active Record and Data Mapper are viable ORM patterns. Otherwise, I doubt Rails, Laravel, etc. would be as widely used as they are.
Can you provide a bit more context of the problem you're trying to solve? The analyst/query thing isn't too helpful as stated.
I have a config file (sort of...the file doesn't impact on the website itself, its used for some scripts to do other things on the server side). I need to show the contents of the file in my website. Then allow users to edit the content of the file in the UI, save and the change made to the file are saved. I'm using angular on the front end and have the edit and save part working. Now I'm trying to figure out how to do the post request 
Err... yes, I gathered that much from the original post. Why do the users need to edit the file from a web UI (especially if it doesn't impact the website)? There's probably a better way to solve the problem, I just don't understand what you're actually trying to solve :)
The controller is the boss, his job is to satisfy the customer requests. The boss has two workers called model and view. Model's job is to return data the boss asks for without bothering the boss with stupid questions. Views job is to arrange the data the boss gives him in the way the boss tells him. The boss then hands the result back to the customer. So the controller doesn't really 'render' the view, it's delegating that to it's employee Mr View and then returning the output of View to the customer. In coding you'd probably write this like this, but it's two separate steps. Get the results from the view object and then the controller is responsible to return that to the requestor. return $this-&gt;view('template', $data)
&gt; You propably can. Question is, can you do it in a DDD-way, or more like in a "tuned for high concurrency, not easy maintenance and understandability"-way. Feel invited to deliver an proper example maybe? I'll acknowledge that there is a threshold in design between "some concurrency" and "very high concurrency". But this shouldn't be taken as putting a line between "some data corruption" and "lots of data corruption" ... due to concurrency. Low or high concurrency, concurrency should be acknowledged, and race conditions eliminated. We both agree on that, I believe. But, you know, I can actually stop pulling the rope towards a command-query style design, and go **completely in the opposite direction**, where you want me to be: some, but low concurrency, kind-of-ok performance, and *very easy to understand and maintain*. How'd that look, probably: $employee = $employeeRepository-&gt;getById(1); $employee-&gt;work(8); I removed the persist command. Instead, every change is automatically persisted, and it **never** will throw a SerializationException. What's simpler than that, right? What's happening, is $employeeRepository isn't the real repository. It's a simplified facade for a more elaborate version, which entities can cooperate with to produce consistent transforms in the domain: class Employee { private $id; private $repo; public function __construct($id, InternalEmployeeRepository $repo) { $this-&gt;id = $id; $this-&gt;repo = $repo; } public function work($hours) { $r = $this-&gt;repo; $salary = $r-&gt;salaryFor($this-&gt;id); $balance = $r-&gt;balanceFor($this-&gt;id); $r-&gt;begin(); $balance-&gt;add($hours * $salary-&gt;rate()); $r-&gt;commit(); } } Notice that the entity still encapsulates its business logic, but it simply doesn't retain its state. It delegates that to the repository. The entity is *still not tied to a specific storage layer*. The work() method looks like SQL a bit, but it's not SQL. You can implement the same repository system on NoSQL, plain files, Redis, whatever. This is not what you'd see in an off-the-shelf example for DDD, but it achieves the stated goal. Maintainable, easy to understand (*especially* to those using the entity from outside), and data remains consistent. No need to remember to persist(), no need to remember to try...catch. Admit, it's kind of fun to think about doing it *this* way, no? Now. Is this what I'd do in PHP... not quite, because no one likes N+1 interactions with their persistence storage. That would barely deliver kind-of-ok performance... I don't like **kind-of-ok** performance. We need a fast way to work on many employees, yo! So, instead, I use collections semantics. My model objects represent an *entire collection of employees*. So, instead of this: $employee = $employeeRepository-&gt;getById(1); $employee-&gt;work(8); I have this: $employeeCollection = $employeeRepository-&gt;selectOne(1); $employeeCollection-&gt;work(8); It's not a big leap in terms of understanding, wouldn't you say? It's kind of like jQuery. However, now I can actually operate on many employees in one transaction: $employeeCollection = $employeeRepository-&gt;selectAll([1, 12, 15, 19]); $employeeCollection-&gt;work(8); Or I can select them by arbitrary criteria and apply a change in one transaction: $employeeCollection = $employeeRepository-&gt;selectPresent(); $employeeCollection-&gt;work(8); Reading from multiple objects is also easy: $employeeCollection = $employeeRepository-&gt;selectPresent(); $namesOfPresentEmployees = $employeeCollection-&gt;name(); Note that the selectFoo() methods don't actually *call the database*. They just specify an empty selector. This allows me to fetch data at the very last moment, when it's needed, including within a transaction, so I know it's fresh and up to date. It *looks* kind of similar to what you did, but it's also *fundamentally different*. You deferred the write and the transaction, but didn't defer the read. This allows for race conditions. I, instead, deferred the read until the moment I need to write, so I can wrap all in a transaction, and avoid a whole lot of trouble. So, I give up saying what's DDD and not anymore. It's been a long thread, you be the jury. Is this *totally not DDD* despite I achieve the same conceptual separation? :-) Oh, and BTW... the examples with the $employeeCollection are a command and query design in disguise. :-) Did I fool you? Do you like it? Is it easy?
You seem to almost be talking about CQS and [CQRS](http://martinfowler.com/bliki/CQRS.html). The separation of commands and queries doesn't exactly say *where* the domain entities live. You *could* implement all of your business and consistency logic in the database, and then we could confidently say they live in the database, but that does not seem common. In a DDD-context, an entity is still an object with identity. Very often, an entity is also an aggregate that ensures its own consistency. That DTO, while it may qualify as a ReadModel or projection, is not a part of the [domain model/layer](https://en.wikipedia.org/wiki/Domain_layer). &gt; But that's not something PHP developers understand at large, this is why I'm very careful what I recommend in this subreddit when it comes to how to model entities and so on. You could say that about a lot of developers, not just PHP ones. (Maybe you should find some cooler PHP developers to hang with) &gt; The problem is reading DDD too literally. Say, mention "commands, queries and events" to a DDD "literalist" and they'll reject the notion that this has any place in a DDD system. I've had the opposite experience. When trying to grasp the more complicated DDD concepts, a lot of the best solutions leaned more and more towards a separation of commands and queries, and perhaps even event-sourcing (rather than just event/domain messaging that I mentioned before). &gt; I fully agree, but within your description here is hidden the need for some forethought that isn't happening with DDD mainstream fans. If you ask them, it's "simple": &gt; &gt; $thing = $repo-&gt;getThing(1); &gt; $thing-&gt;setSomething($thing-&gt;getSomething() + 123); &gt; $repo-&gt;persist($thing); Whenever I go near the php ddd communities, they seem quick to point out how anemic a domain model is. How setters and getters suggest anemic domain models. The literalistic you speak of, seem to be off in some separate world: DDD usually encourages moving away from CRUD, separating concerns, exploring the domain, designing it properly, and working with business experts to share ubiquitous language. Maybe they need to do some more reading?
I agree. private scripts are private. I don't know how to code in python. I've Just had experience working with more focused devs, while explicitly doing front-end myself. But I'm not calling you stupid hear, just clarifying the rules I learnt. Im sure your knowledge on programming is legit.
&gt; You seem to almost be talking about CQS and CQRS. I'm very definitely talking about CQS-like separation, but not CQRS, this is a quite more involved architecture, and I wouldn't like to demote it to just "the API has commands and queries". :-) BTW, I'll use c&amp;q in this comment to denote "commands and queries". &gt; The separation of commands and queries doesn't exactly say where the domain entities live. You could implement all of your business and consistency logic in the database, and then we could confidently say they live in the database, but that does not seem common. Yes, I kind of corrected myself mid-flight, a more accurate way to say it is that the entities exist somewhere between the service layer and the persistence layer. A service layer which accepts domain-specific c&amp;q, and through business logic transforms them into repository c&amp;q, and the repository transforms them into SQL (or Redis, or file I/O or whatever) c&amp;q. &gt; In a DDD-context, an entity is still an object with identity. Very often, an entity is also an aggregate that ensures its own consistency. That DTO, while it may qualify as a ReadModel or projection, is not a part of the domain model/layer. Well, I'm curious what you think in this case about the links I sent you in my previous comment. It's an example of entity modeled as a materialized entity collection, which *is* an aggregate, and which *does* ensure its own consistency, but also... doesn't hold any significant domain state in itself (it accesses it only through repository). &gt; You could say that about a lot of developers, not just PHP ones. (Maybe you should find some cooler PHP developers to hang with) Well, that's fair. My experience with Java and .NET devs isn't significantly better, but they also tend to have a more favorable platform paradigm to work in (Java and .NET apps are single process persistent processes by default) which doesn't immediately invoke the edge cases that PHP developers have to deal with, and are ignorant of. So non-PHP devs are sort of privileged when it comes to butchering DDD designs without causing trouble, I guess. &gt; I've had the opposite experience. When trying to grasp the more complicated DDD concepts, a lot of the best solutions leaned more and more towards a separation of commands and queries, and perhaps even event-sourcing (rather than just event/domain messaging that I mentioned before). Can I hang out with your crowd, please? :-) &gt; Whenever I go near the php ddd communities, they seem quick to point out how anemic a domain model is. How setters and getters suggest anemic domain models. I can take my example: $thing = $repo-&gt;getThing(1); $thing-&gt;setSomething($thing-&gt;getSomething() + 123); $repo-&gt;persist($thing); And refactor it to this: $thing = $repo-&gt;getThing(1); $thing-&gt;incrementSomething(123); $repo-&gt;persist($thing); But if the method inside does this: $this-&gt;something += 123; Then we gained nothing. You need to understand that people who parrot about "anemic models" often don't understand the context of this advice. They think it's about treating your DTOs as entities and stuffing them with dangerously race condition-prone logic. You need to explore in depth what exactly person X suggests in order to avoid "anemic model" before you can tell apart the folks who know what they're talking about, from the folks who simply listen to them and repeat cargo cult-style. This subreddit, for example, is unfortunately *full* of people who give advice like that without understanding its implication in depth.
I laughed out loud when I saw that field. Why would that ever be provided? 
I believe it's the successor to nomnom.
This is all you need.
Your use of the terms "frontend" and "backend" threw me in a loop for a good 20 seconds, trying to figure out what you mean. What you mean, I figured, is you're talking about your "public site" and your "admin panel site". Phew. :-) So the short answer is: you need to move all your non-UI logic from the controllers to services: ProductService. Then you will still have two controllers, but they'll be just a couple of lines, calling into the service. Controllers + views = UI. They're not for business logic, this is why they return specific templates. 
Built a help desk at my previous job - not aware of any such package that does this. We had to code it by hand and pulled some logic from our other product HelpSpot (helpspot.com). It's all closed source though.
Have you looked at RT? https://www.bestpractical.com/rt/ It plus some of its extensions might be up your alley, or it is pretty easy to integrate your own code with it. RT itself is perl unfortunately.
&gt; If the method inside does this: &gt; &gt; $this-&gt;something += 123; &gt; &gt; Then we gained nothing. Actually, you've gained a lot: flexibility. In the original case, the domain may be updated infrequently enough that conflicts are rare-- so a simpler method is quicker to write, simpler, and meets the business needs (as long as something actually detects conflicts or other concurrency issues, that is) However, if the use-cases change and this method becomes a major source of contention, it can be changed to be more scalable. The other method, setSomething($something + $i) has a lot less flexibility. We don't know the intent, maybe you could extract it indirectly, but for all we know, the user of `setSomething()` didn't even use something. They wanted to reset the value and then add a *new* amount. If the code is ever updated to be better at concurrency, you'd have far more flexibility and less refactoring with specific commands, rather than generic ones. (Refactoring is supposed to be one of those DDD advantages/goals) e can't directly communicate that intent to the repository, e.g: by storing a log describing the changes to $something.
**Part 1/2** (Holy shit, I reached Reddit's comment limit) &gt; Its not a conventional pattern, but it looks like it could be considered Domain-Driven-Designtm (although work() may not the best method name). Hehe, the name didn't come from me, but the original example I was modifying. :-) &gt; 1. Complicated repositories (if we can still call it that) I have some practice with that, and they're not really more complicated in terms of LOC - they're broken into more objects, however (of which, the branches don't touch persistence, only the leaves do). This is done to make the API more granular for the entities to use. &gt; 2. Fixed transactional boundaries. That's... a drawback? Note there's a lot I can't cover in a simple example, but if you mean several actions that should be executed in one transaction, there's this: $employees-&gt;begin(); $employees-&gt;thisIsRunning(); $employees-&gt;inATransaction(); $employees-&gt;commit(); My DB layer supports nested transactions (like Doctrine, but savepoints are also supported), so the logic behind this is basically nothing. It's the same like SQL: if you don't open a transaction, every command is one. But if you BEGIN it's one transaction until you COMMIT (or roll back). Easy peasy. &gt; 3. More complicated consistency logic Sorry, not sure what makes it more complicated...? It's like a super-basic version of SQL, but modeled after the domain. Can you clarify, please? &gt; 4. O(N) performance (unless the repository is much more complicated than we already assume, still the $hours * $rate calculation happens in the php). You're looking at the implementation that I said "I wouldn't do in PHP". This was before I revealed I use collection semantics. So it's O(1) really. Here's how that method would look with collection semantics: class Employee { private $selector; private $repo; public function __construct(EmployeeSelector $selector, InternalEmployeeRepository $repo) { $this-&gt;selector = $selector; $this-&gt;repo = $repo; } public function work($hours) { $r = $this-&gt;repo; $salary = $r-&gt;salaryFor($this-&gt;selector); $balance = $r-&gt;balanceFor($this-&gt;selector); $r-&gt;begin(); $balance-&gt;addByRate($hours, $salary-&gt;rate()); $r-&gt;commit(); } } I'm not quite happy with that, and the "work()" example, but you know, that's the example that was given up the thread. Still, notice, no loop. Of course, when it comes to UPDATE queries, *sometimes* they may unroll in the repository to N updates due to SQL limitations (can't avoid that), however if all the salaries are the same, it'll be one query. I can easily see how I can avoid running 2 queries and do this with a join and put that in the repository but... let's stick to the example. It's still better than what the trivial DDD solution would achieve here. &gt; 5. Unclear conflict resolution. I can clear it up, if it's not clear. What's unclear. :-) From outside, it "just works". From the inside, you run one query to select rates and you run one (or more) queries to increment the balance for every employee. This is not an operation that can end in some sort of conflict I can think of, especially in a transaction. Of course, there's always a bit of leeway to come up with a "conflict" in a hypothetical example I don't know the details of (again, not my example)... but nope, I can't think of any. &gt; The repository used to just have two or so methods-- now it requires an accessors for every sort of property the entity needs to track. Not every individual property, but for this example that's how I'd do it. I typically model this less granularly. Say, I wouldn't set 4 profile properties on an employee one by one, I'd: $employee-&gt;setProfile($profile); // $profile instanceof EmployeeProfile DTO Do we count complexity by method count, BTW? There's a *whole lot of complexity* behind those two methods in the original repository, you know. I mean, persist() might as well be called "make pigs fly". Look at what it has to do in the original example. It needs to determine the employee hasn't changed, then dig into their properties, find the balance has changed and update it. And that same method may do something completely different, depending on the change set it finds. But worst of all, the intent is lost. When you persist() and a change set is extracted, you can't infer the intent was "add 8 hours * rate". So when the repository is that dumb, your only option is aggressive locks (optimistic or pessimistic) and repeating logic when it fails. Because nothing else could work with a dumb repository. We went over this a couple of times. In my case, the actions are modeled after the domain. A *balance* is never *set*. No bank account, or online payment site, or whatever works like this. You debit and credit a balance. So the methods would be add() and subtract() or what have you, and that's what my repository has. &gt; How much do you need to add to the repository and the entity when there's a new requirement? Say, the first 8 hours are paid automatically, but more hours require manager approval and are paid at a different rate. How many round-trips does that add to process? In my example above there are 2 roundtrips: 1. Fetch rates for N employees. 2. Increment balance for N employees (if salaries are the same; otherwise one UPDATE per employee). For that logic you mentioned, it'd be 2 more roundtrips. The repository doesn't have to change. It's kind of weird to talk about counting roundtrips for a task that has to literally run once a day, but let's assume we're in the 60s and one roundtrip costs them $1000 + first child. :-) &gt; I recently implemented a command that would execute two methods on an aggregate. In the very unlikely event that it happens to fail, because I never called persist(), I can assume the database is still in a consistent state. That isn't possible with an entity that automatically tracks its own state. I'm sorry this is a bit too abstract and I can't understand the scenario presented. The methods of the entities are literally commands, that get executed immediately (unless wrapped in a transaction as pointed out above). I can't see the difference you see. &gt; Really, there isn't much reason for the entity to care about transactions. The entity can't *not care about transactions* when its state is in a remote service that's accessed concurrently. I felt we had an understanding that mutating domain state can't happen on an old snapshot in memory and just cross our fingers and hope it works out. I guess not. This is like saying an entity doesn't care about Java's memory model when it's accessed concurrently from different threads. It's not serious. &gt; The main advantages of the code you proposed is really the way it transforms data and how it pulls the information needed at the last possible moment. Starting and finishing the transaction from outside the entity doesn't change that. It does change it because these transactions sometimes *return results*, first. Second, you can't implement all business logic in vacuum sometimes. If you have a constraint "every employee must have a unique email", you can't enforce this without the repository at the moment the email is set, and your transaction failing *somewhere in a remote corner of your codebase* is a really bad way to (fail at) encapsulating of the handling of that error. When you request that an action happen, it should happen, so you know it happened. If you'll be calling persist() right after every action, it might as well persist automatically. And if you don't want it to persist automatically, you can open a transaction as demonstrated above. You slipped up above that a command may fail because "you may forget to persist". Well, here's one class of errors that can't happen in this setup. It's nearly impossible to forget to commit, I actually have a closure syntax for this, to make it *literally* impossible: $repo-&gt;transactional(function () { ... }); &gt; My domain model entities are unit tested. These entities are very testable. If you don't want to test against the DB... just don't use a DB-based repository... &gt; They do not depend on a repository and will ensure at consistent state even if the repository implementation is buggy. That is, when I add a business rule that automatically approves up to 8 hours and marks the rest to be approved. It'll do that even if the repository is buggy or otherwise fails. What is this cosmic magic where your repository fails to persist your entity properly, but your in-memory entity somehow "ensures a consistent state". That "consistent state" would last about 0.1 milliseconds before PHP's environment goes poof? Is this the big win? &gt; The performance doesn't really seem that different. Arguable, but I have no basis to argue as the example is too basic. I find performance a whole lot better than ORM solutions, I'll just say that. 
I was dealing with the same types of questions some years back when I was trying to compare various architectures for various applications. The simple reasoning is because it's just how the initial framework architects designed their systems, but I'll still expand the rationale. Most php frameworks are marketed as MV* systems, but since php isn't a persistent application there can't be a direct comparison to what php applications call MV* and what traditional desktop applications use as MV*. This causes some confusion because with a persistent application, the entry point for user interaction for a MVC application is via the controller and it returns the view whereas a php MVC application goes through the entire application stack and eventually reaches the controller which parallels the design so naturally returns the view. The various MV* designs include MVC (model-view-controller), MVVM (model-view-viewmodel), MVP (model-view-presenter), etc. The naming can be confusing if taken literally, but when taken in a more general sense it's more easily understandable. The view is the rendering layer, the template section. The model is not just a single object representation of something, but is instead the entire business logic layer of the application and includes databases and such, the guts to be manipulated. The controller, viewmodel, presenter, etc is the data manipulation layer. How the actual manipulation is done is nuanced which is why there are multiple MV* architectures described. Specifically with regards to MVC, it's the controller layer that returns the rendered view, but since many php applications are restricted to single controllers due to architectural reasons the result is your single controller object returning the view. When thinking about the render, what controls it depends on the design. If the view object was responsible for rendering everything, then it might be better described as a presenter of the MVP design. If the render was responsible for rendering a single object or model and then multiple renderings were pieced together, then it might be MVVM or HMVC. If the controller triggers and returns the render, then it's a typical MVC design. These are all MV* patterns where the model layer is the business logic, the view layer is the rendering, and the *-layer is how the two are pieced together. How the layers are pieced together is what defines which type it actually is. Expanding on the reusability concern, you're actually correct and it's long been a concern of mine. The specific controllers in many frameworks and applications are responsible for too many things. Unfortunately, you're very much limited in options to whatever you can accomplish with your framework of choice, or develop your own like I wound up doing. This concern is why I decided to design my own architecture specifically for the web and not adhering to the classic architectural designs I was taught or from my books. I'll eventually release it, but I'm a long way from formally doing so at the moment. The design I went with was a dispatcher that cycled through a series of pluggable dispatchables instead of hard-coding a controller or presenter. Each is passed application request and response objects (not PSR-7 HTTP ones) as well as arbitrary parameters and they each act as decorators on the request/response if data needs to be passed. By default, I have a controller, presenter, and responder and each is completely reusable as desired. The controller is responsible for only manipulating data and returning resulting objects. These objects could be views if you wanted, or even viewmodels, but are usually data objects for what I do. The presenter does not manipulate the data and only returns the renders. The responder takes the output and generates the actual application response be it http, ftp, spdy, gopher, etc. Each item is segmented into finite roles so it would allow for the same controller to process something and plug in different presenters depending on if you want html, xml, json, etc as your output and finally the responder would do things like return the proper content-type headers for whatever you're doing. That is how I avoided the problem in my world, but if you want to get pedantic about it, then it could still be classified as an MV* architecture where the dispatchables would be analogous to a controller or presenter layer.
&gt; as long as something actually detects conflicts or other concurrency issues, that is "Something" ಠ_ಠ I'm noticing a trend to talk about consistency as if it's someone else's problem, brushing it away as if it's a minor concern we can solve later. I've actually not heard what your strategy for achieving consistency is. I was left with the impression you see the repository almost as a key-value store for dumping serialized objects in. So apparently, it's not there. My point was there isn't something detecting conflicts. If one codes a method like this hoping that "something" detects conflicts, chances are it doesn't detect all of them as the programmer doesn't fully understand what the something does in specific. There's no automatic magic trick that leads to conflict detection. You design the domain, then analyze where the conflicts come from and how to make sure they don't result in problems. Also, I have to admit I find it odd that you say this... &gt; if the use-cases change and this method becomes a major source of contention, it can be changed to be more scalable. The other method, setSomething($something + $i) has a lot less flexibility. We don't know the intent ... and yes you argued in favor of -&gt;persist($bagOfStuff) versus -&gt;balance-&gt;add($amount); 
Hmm. Thats a good question! Objective-C (the language of the cocoa framework) is a "superset of C". The C data types are statically typed, but Objc instances carry their class around. I guess they are structs internally... I'm not sure how well Xcode provides type hints for results of valueForKeyPath(). But once it is assigned to a typed variable it should work at least. I agree with you. I more and more like the ability to provide type information. I think my IDE prevented me from a lot of headache when it checks argument types. 
It looks like your best option might be writing a wrapper to "exiv2" or "exiftool", which are shell commands for updating exif data. I suspect wrapper libraries for exiftool might already exist, given the search results on [packagist.org](http://packagist.org), but I didn't see any hits for an exiv2 wrapper.
How is the new CodeIgniter going?
You can always start with micro-framework like slim or silex, or even start with a DIC and then: * start add routing component, it can be symfony routing, fastroute etc * add templating engine, like twig or plate, or even blade * add database layer, we can use doctrine dbal, full doctrine orm, or eloquent as you wish * add more functionality via service provider if necessary * ta-da..., your mvc framework is ready cmiiw
QFT. My world ended a couple times when i had to migrate an entire backoffice and provisioning systems because 2 hosting providers were merging, everything was build on ZF1/Doc1. Good luck with that.
What would be the correct way to structure this then? Ideally we want a minimal number of queries being made to the database, with the cache instead being primed and invalidated at the correct moments to allow repriming with latest data. Also, the example of the Article / Blog system is not our actual use case, our use case is much more complex (it regards an ecommerce product, so invalidation would be on products, with many joins etc, rather than just a single table with article information).
Yeah we don't set it more than 4. It really should be limited to the number of cores available. 
I don't tell that SR is only for purist... I tell that if the pattern used is the problem to reach my goals it's always possible to change it. I know about SR and I know that AR break it... But over architecturing is just too much complexity at the beginning. AR is the simplest pattern to implement here so I go with it. It's also the pattern which fit my needs... I do not invented a pattern I used a software development proven pattern... Another time, I do not posted my link here to discuss about the pattern war. If Active Record is not the right choice for me, just be more constructive from the current code version ;) The idea here is just to provide a simple DAL abstraction... Behind it all can be done !
You should have some kind of database or similar that stores the status of a transaction/payment and only performs an action (approve, refund, etc.) when the status changes within an allowed range. IPNs can be sent an undetermined amount of times but this does not mean you should re-perform the action. There are also verification methods within PayPal you should be calling to ensure that the notification is not being spoofed by the user, as to procure goods or services for free, i.e. stealing/fraud...
Not a package or library, but I built this functionality. We use Mailgun.com You could probably use Mandrill.com but I noticed companies like Intercom.io are using Mailgun.com for the email converstaions (tracked parsed emails, write about this line functionality). 
Yip I wrote one recently. But I made mine all HTTP. So I only ever pass my class a URL and a time that I want this executed after. * This queue then has a cron task that runs it every minute. * Queue finds any tasks that have a time less than current date. * Passes each task to a Guzzle to execute in parallel (Guzzle Pools) * Deletes task if it succeeds, if it fails, it ups the fail count ++ * If a task fails more than x times it is then ignored and an email is sent to me with details of this failed task. Wrote it in few hours, worked flawlessly since. It's a fantastic bit of simple code that solved a real PITA problem where I wanted to something to run after X minutes. 
The fact that you had to ask for any help in regards to that IPN script is a clear indicator that you should stay the fuck away from any code handling money. 
once it works, its good :P, until it works, im just using www.sandbox.paypal.com :3
So you'll just throw mud at the wall and when it sticks you'll put it in production. There is no way this could backfire, please proceed.
Haha... yeah and please post your IPN url here so we can help you "debug". Not so we can create our own free servers or anything. Just debug.
Personally I am a bit skeptical such a library can be written (that is nice to use) as a one fits all ORM as each storage type is so very different but good luck :) One thing I noticed when having a quick look at the code is that the RDMS is rather simple for what is actually a very complicated problem. One example is just limiting a recordset different databases works quite differently (limit offset is non standard). https://en.wikipedia.org/wiki/Select_%28SQL%29#Result_limits https://gitlab.com/bee4/activerecord/blob/develop/src/Connections/PdoConnection.php#L53 Doctrine has lots of code to abstract out these differences in its DBAL project and as you can see its non trivial (this is SQL Server count code): https://github.com/doctrine/dbal/blob/master/lib/Doctrine/DBAL/Platforms/SQLServerPlatform.php#L1177 Make the queries more complicated, adding joins etc and it gets even more complicated. Anyway hope that helps
If you think you're being helpful then I think you should call your doctor and ask him how deep you need to stick the needle in if you want to perform a lumbar puncture on yourself. See if he tells you how to do it, or advises you to get your ass to a doctor. 
at least in slim v3, the route action is responsible for writing data to the response object. now there are a few different ways to do that , one being a View where it renders (and inserts) the data into the response object... return $this-&gt;htmlView-&gt;render($response, 'template', [data]) ... or $this-&gt;jsonView-&gt;render($response, 'template', [data])
Yea, perhaps at all was a bit strong, but certainly less so than a python, perl or php script. Cross compiling has been the only hoop I've had to jump through to date. 
&gt; following the SR principle is something for purists I have no idea how you got that from what I said. But go ahead, rant on.
I was about to say that I have worked with various mvc's so much that when I start writing vanilla php I end up making a mini mvc anyway. 
That's just plain ridiculous. Written in 2015, filled with flagrant SQL injection issues and posted here? The only explanation is that it's copy-pasted from 2002. What have we become...
SQL Injection, passwords in plaintext. It has the whole "please hack me" package. This is looks like something from the 90s indeed. Do **not** use any of the presented code as if it were good. It is not. It's plain awful.
If I had to say anything positive about that, it would be that it uses mysqli.
Also known as MVC, despite author's insistence to the contrary. :-)
I think CodeIgniter version 4 is still under develop and it will be a major upgrade that will break backwards compatibility.
Hello ! Indeed you are right... I'm currently trying to create a QueryBuilder which is usable globally and it's really hard ;) I think I'm gonna use dependencies like Doctrine DBAL as reference (they do the job really nicely). I think that the lib must be a global canvas then we can imagine add DBAL layers as we need. I've an issue for that : https://gitlab.com/bee4/activerecord/issues/6 The relations are not fully implemented for the moment too (only Parent &gt; Child which is ElasticSearch specific). But I've also an issue for that : https://gitlab.com/bee4/activerecord/issues/3 I'll try to move forward on that code, maybe I'll post later a new link when some parts are ready ;) Thanks!
Memcache is not a database. It is a cache. Databases store persistent data over extended periods of time and are intended to retain the data regardless of whether or not the server reboots. Memcache loses all data upon a server reboot and will occasionally discard data that hasn't been used in a while. It is absolutely unreliable as a persistent data storage medium and is only useful and reliable for transitory operations.
truth.
In the case of: $fn1 = closure('foo'); $fn2 = closure('foo'); vs. $fn1 = new closure('foo'); $fn2 = new closure('foo'); ~~It is actually not a good idea for the same object to be returned from either instance because there are situations in which you would wish to rebind the closure to a different `$this` variable, and if they are the same object then both would be rebound to the the new `$this` in an unintuitive manner.~~ EDIT: Never mind, didn't realize `-&gt;bindTo()` was an immutable operation.
Honest question as I have not experimented in this area at all: Why use reflection over bind/bindTo?
&gt; if they are the same object then both would be rebound to the the new $this in an unintuitive manner. Hmm? Closure rebinding gives a new closure, it doesn't alter the current Closure [Closure::bindTo](http://php.net/manual/en/closure.bindto.php) - Create and return a new anonymous function with the same body and bound variables as this one, but possibly with a different bound object and a new class scope. Though that does need editing.
Make one then. Seriously, you must be a great developer. When clients ask for X, you must give them Y and when they are pissed that it's not what you want you say 'ohhh well you should have specified you wanted Z.' I bet they love that. I'd love to see what you come up with. You're defending memcache in a situation that is not suited for it. There is no reasonable way an enterprise application could use memcache for storage that is critical to the operations of the enterprise. You could make something that looks like a booking system, and has some of the functionality of a booking system, but it would be like selling someone a golf cart while implying it's as safe as a car. 
The importance of this is that T_COMMENT's are not cached by opcode where as T_DOC_COMMENT (i.e., where annotations live) are indeed cached by opcode, and are accessible via Reflection.
Why use: if (function_exists($item) == false) { throw new \LogicException("Invalid callable - '$item' is not a function"); } $reflFunction = new \ReflectionFunction($item); return $reflFunction-&gt;getClosure(); instead of: if (function_exists($item) == false) { throw new \LogicException("Invalid callable - '$item' is not a function"); } return function() { return call_user_func_array($item, func_get_args()); }; Is reflection faster/safer/better? Same with binding method calls, why use reflection? Why not call Closure::bind()?
Well, they have the right to get their solution out there. :-) I'm sure many people would be perfectly happy with a custom tailored Symfony framework.
&gt; return function() { return call_user_func_array($item, func_get_args()); }; The difference is this loses all of the parameter information. I use [Auryn](https://github.com/rdlowrey/Auryn) a lot, including using it as a dispatcher in applications. If all of the parameter information is lost, it is impossible for the required parameters to be determined at run time. The closure generated from `closure('foo')` retains all the parameter, and return type information. &gt; Same with binding method calls, why use reflection? Why not call Closure::bind()? Sorry, again I'm not sure what you're asking here.
Worked for Microsoft, Google and AOL. It's the same everywhere. 
[Here's a presentation on the failed PHP6 project](http://www.slideshare.net/andreizm/the-good-the-bad-and-the-ugly-what-happened-to-unicode-and-php-6). The postmortem itself starts on slide 60, but the entire thing is worth it.
Ahh, did not catch the immutability.
Some people are perfectly happy with an all-in-the-box Symfony framework And speaking of Laravel, where does a php noob find good literature on MVC? I feel like I'm doing it wrong
RC means "is this good enough?". That's why it is "release candidate". A candidate for release. I don't know why you're pretending it means something else. Maybe you just didn't know? Multiple RCs mean low quality and poor project planning. That's because it was a release candidate but deemed unsuitable. If it's good enough it would be released. 
Well you wont 'blatantly' remove your docblocks for no reason, but the docblocks may still be changed or entirely replaced in future. Because they are comments, comments can be changed if the underlying code changes. Maybe the method signature changes, maybe the method now does different things, there can be many possibilities. No matter what, changing comments should not affect the your application code in any possible ways. Comments should depend on code, but code should never depend on comments. Yes that one is not an argument 'not to use annotation', but an argument that 'you dont have to use annotations (since there are alternatives)', see the difference? Sometimes you shouldnt be doing something, but you may have to do it since you have no alternatives, not in this case of course. The arguments for 'not to use annotation' have been provided in the two earlier paragraphs, I have no need to repeat myself. I did say that annotations have their use cases, in programming languages such as Java, C# and Python in which annotations are supported at language level, its perfectly fine to use them. However, in PHP there is no annotation support, the userland implementation from Symfony and Doctrine offers a terrible workaround that turns comments into code, which is more detrimental than beneficial. If you need annotations that much, you should consider using another programming language, or to edit the C internal files for PHP to add native annotations support, if you are that skillful. Annotations inside comments are abomination, and my point stands. 
I'd just like to be able to typehint the parameters and return types of a callable. callable (int, string, MyValueType):ReturnType $test 
&gt; So what do you think of making it less about Closure, but more about a callable construct/function that enable strict callables? It sounds good! I have no idea how to do it, but it sounds good! &gt; After re-reading what I wrote,.., except the method is named callable() and the fact that it returns a Closure is an implementation detail. This is why I introduced this RFC now, rather than waiting to figure out how to do anything else 'properly' even if the other thing could be slightly better semantically. Although closures are allegedly an implementation detail in PHP, I don't think there would be any pragmatic difference between what would be possible with this RFC, and what would be possible if functions could be referred as first class citizens in the language. e.g. imagine that the ability to define a strict callable looked like this: function foo() {} $x = foo::function; Yay - $x is now a reference to the function 'foo' that can be called. Compare that to this RFC function foo() {} $x = closure('foo'); Yay - $x is now a ~~reference~~ closure to the function 'foo' that can be called . Although the first one is better from a language design point of view, and doesn't expose the icky implementation details about closures, by itself it doesn't seem to offer any tangible benefits to end-users*. So, the choice was between either introducing this RFC for PHP 7.1 and having all of it's benefits sooner rather than later, or to go for a much larger RFC that could only probably be introduced at PHP8 at the earliest, at might not even pass. *I can imagine that there could be other things done that would still make it worth doing that, but which would need lots of thought e.g. being able to composite functions together like [memoize](http://docs.hhvm.com/manual/en/hack.attributes.memoize.php) combined with any function to produce a cached version of that function....but again, that would have to be part of a much larger RFC. 
&gt; Make one then. That does what? &gt; You're defending memcache in a situation that is not suited for it. No situation was described. There is an IMAGINED scenario by people who are just injecting their own biases. &gt; You could make something that looks like a booking system, and has some of the functionality of a booking system, but it would be like selling someone a golf cart while implying it's as safe as a car. You're under the impression they don't want a golf cart? I lost you when you started making stuff up again.
Care to explain? I've read the pull request, but... I haven't understood it :S
What SMS platform are you using, and is it premium rate SMS you're billing with?
And now I know you're arguing just for the sake of arguing, because as was pointed out to you several times, this entire discussion is in the context of using Memcache in a system that needs to be durable across reboots.
&gt; And now I know you're arguing just for the sake of arguing ... &gt; this entire discussion is in the context of using Memcache in a system that needs to be durable across reboots Making up requirements or making technical errors does not change the context. It's sometimes referred to as "staying on topic". Asserting the discussion is now about some other topic is a derailment and is not compelling. I will respond to steer toward the original topic (at hand) while attempting to be technically correct. This whole wharglebargle is a classic illustration of how projects are polluted by assumptions and technical bloat is introduced, if nothing else.
Are you talking about Doctrine1 or Doctrine2? Because D1 is Active Record, D2 is Entity Manager - it's performance is far better than any AR based library I've ever seen for data manipulation, reading, and hydration. &gt; Do you also know that most of the projects use simple data model ? This is completely subjective. All of my projects have sophisticated enough data models that warrant using a performant ORM. Hey - if ActiveRecord meets your needs, go wild. I've just been burned too many times by it to really ever consider it as an option. I'd sooner use no ORM than an AR based ORM. 
Do we really need to prop up this legacy of inefficient stringly typed callables? Because adding more types that allow in legacy callables will do that. I don't think we need another type for this either. Phasing it all out in favor of closure references is IMHO the best solution. I'm *already* doing this in my code, and I'm not looking back.
&gt; I wasn't going to post this as i've criticised people recently for linking to their own stuff too much, and wouldn't want to act hypocritically. But your stuff is *great*. PHP needs more of this kind of work. Thank you for doing this and I'll pray you find the folks at internals in a good mood, for the good of all of us.
I'm sorry :(
&gt; Everyone here acts like Active Record is the end of the world. No, it's just the end of your project. Once you get deep into Active Record, getting back out again is impossibly difficult.
Remember: on the server side, [the template is not the view.](http://paul-m-jones.com/archives/5993)
Hi Tanks!
Good post, but $250K? Where the hell do you live? I feel like I have hit the "glass ceiling" at around the $130K mark as a full stack developer with ~10 years of experience. (Boulder, CO)
I think they are contracting, it's a lot more lucrative, especially if you deliberately leave 30% of time for "emergencies" you can often later re-sell it as emergency / priority service and make 200-300% of general $/hr
You're not alone. I much prefer ruby for command line scripts over PHP.
Thanks, as someone that bought a book and was eagerly awaiting it for like 12 months after the initial buzz, I am a bit gutted at what I see as a cover-up culture, rather than a learning culture around PHP6 and in some small part Python3, and PERL, all of which were planned to take a part of my work-flow many years ago but either never materialized (PHP6), or needed years more work than originally imagined. Still the PHP community is very different now, and is I would say more professional than I would have ever imagined as a self-taught dev. 
Also - New England, in a "city" where housing is ~150k-300k average.
looks like it : * adds support for linux platform random advances * removes `/dev/arandom` *(something to do with RC4?)* * three more checks, with errors * adds more tests See at https://github.com/php/php-src/commit/2bb7e65a5d73a949390da778975856b581571e75 (there are more patches, but this looks like the bulk of it)
I can't really answer that question without being a little too revealing, but suffice it to say it's not holding me back career wise as there are many other projects that I work on using modern tech stacks (though none of them are PHP).
| Date | Release | |-------------|---------| | Jun 11 2015 | Alpha 1 | | Jun 25 2015 | Alpha 2 | | Jul 09 2015 | Beta 1 | | Jul 23 2015 | Beta 2 | | Aug 06 2015 | Beta 3 | | Aug 20 2015 | RC 1 | | Sep 03 2015 | RC 2 | | Sep 17 2015 | RC 3 | | **Oct 01 2015** | **RC 4** | | Oct 15 2015 | RC 5 | | Oct 29 2015 | RC 6 | | Nov 12 2015 | Final |
PHP is for web apps, and PHP desktop is probably not going to work on a surface. You could write a web app and store data in local storage if no internet connection is available.
Well, there's no magic answer here. You need to start with profiling your application. Many people (formerly myself included) start with caching because it's kind of just what you do, but that creates this exact type of problem. For that reason, I always leave caching out of my v1 concepts these days. I've seen very complex applications run great with no caching at all because they're well-designed. Try logging your queries with the cache off and see what happens. Look for slow ones. See if you're creating any [n+1 problems](https://secure.phabricator.com/book/phabcontrib/article/n_plus_one/). If you are, fix them. Anything unavoidable run `EXPLAIN` on and see what you get. Maybe you're missing indexes? JOINs aren't inherently slow if your database is configured correctly (this mostly comes down to indexes) If your DB falls over with cache disabled, it's not a caching problem but a design problem - and in fact, you're setting yourself up for a catastrophic failure. Something will inevitably be evicted from cache at a bad time (most certainly under high load) and you'll suffer from an awful thundering herd problem. I'm sure that's not the answer you were hoping for, but of all the approaches I've tried in the past, it's certainly been the most effective.
I'm just saying you are really going on the road less traveled here, not a lot of people have used PHP Desktop and probably you would be the first to try it on a Surface (some of which are very different from a normal windows desktop).
call it from another script in the background. e.g., shell_exec("myscript.php 2&gt;&amp;1 &amp;"); edit: use nohup for extra safety (I'm assuming you're on some *nix): shell_exec("nohup myscript.php 2&gt;&amp;1 &amp;");
The *second* I see imagick and pgsql support, I'm dropping everything to test, deploy, and never looking back.
So, I do a lot of PHP development (server-side, obviously) and toyed with the idea of giving PHP-GTK a whirl. In the end, I settled on [Electron](https://github.com/atom/electron) for desktop and [Cordova](https://cordova.apache.org/) for mobile.
This is certainly the *easiest*, but using a proper message queue of any sort is better. If it's only a couple things I wouldn't bother setting up a queue though; only when it's actually a real part of the infrastructure.
I'm surprised [Robo](http://robo.li/) hasn't been mentioned yet. Define a `RoboFile` class and any public function will be available as a command on the CLI. Great for setting up either common global or project specific commands. For example, I use the [VCS tasks](http://robo.li/tasks/Vcs/) to prepare a release by generating API documentation and committing it to gh-pages.
So is the cost &amp; risk involved in refactoring to support newer PHP going to return enough value to justify? As a digital manager in a large enterprise I would love to use the latest and greatest for everything but sometimes, that old app that was written 5 years ago and needs minimal maintenance and is working fine is best left alone and your time spend on other things. It all comes down to delivering value for the business and making sure you are spending your time on the most important goals.
I would recommend you to make the client in either a language like C# or just simply use something like AppJS. Then simply use REST for interfacing with the backend
sometimes we don't get the CVE assigned in time for the announcement, other times we fix issues which isn't considered cve worthy. and it also happens that the RMs simply forgot to add the CVEs to the initial release announcements.
What extra safety does `nohup` give you?
`pcntl_fork()` would be perfect, but it's not available in the `mod_php`, only at the CLI.
The case for non-native external declarative formats is integration with third party tools, like IDEs deployment tooling, etc. Also settings editable by users who don't know the code, although this is best done via an admin panel or an interactive command line tool. Unfortunately, people have forgotten this in the last decade or so, and have gone nuts with it regarding internal app config that no tool or user ever reads or edits. One good example is composer.json - this one has IDE integration and is very simple, so having config in JSON is worth it. Annotations... They're obviously a hack in PHP, because PHP does not support annotations, but let's put that aside. That is not the important thing about annotations. The problem is annotation encourage statically coupled concerns and implicit hard to debug and discern behaviors. They encourage you to sprinkle your DI config around throughout your classes in a way that promotes poor visibility of app's wiring and reduced code reuse. Once you configure your DI by putting annotation on class X, like Inject and Qualifier, it's obvious that the source code of this class may have to change if you use it for another project with a different configuration. Ew. Another example is ORMs. Doctrine rewrote itself in version 2 to decouple entities from their persistence, only to then promote coupling the entity class with a specific type of persistence mapping by adding annotations to it. Annotations are powerful, but with power comes You know what, and unfortunately framework designed don't have Peter Parker's discipline when it comes to using annotation with discipline. Instead it's like watching a kid run around with a loaded and cocked gun. The sane default is to configure PHP with PHP, because what is code, but configuration? What is configuration, but code? If one wants to separate their, say, deployment settings from their projects, fine. Put them in another file. In another directory. Put them outside the repository. But you don't have to change the syntax to achieve that. Separate **concerns**, not syntaxes. :-) 
I'd strongly advise trying to make that work even if it were usable in web requests - you'd totally screw up the web request lifecycle, and in all likelyhood end up crashing the machine by overloading the process table (the POSIX/PCNTL APIs are awful, and *way* easier to get wrong than right) Seriously, a queue (rabbitmq, gearman, etc) is objectively the right tool for the job here. But `nohup` and `&amp;` can get it done easily with almost no work.
Are you using any sort of framework?
Right on, thanks for the reply. I'll keep an eye out! 
It's a slightly loaded question since yes they do, but there a ton of operations that can change it, so if I'm not mistaken that's why the docs extensively state its not guarenteed
I noticed how most vagrant tutorials leave users halfway explaining just the basics and leving them to figure out the rest. None of the tutorials explained how to connect any kind of GUI to the MySQL server (which is rather tricky if you are a fresh developer with no knowledge of tunneling). A lot of them omit basic things like modifying your PATH to include vagrant bin folder, etc. So I created this tutorial to fix these issues. It assumes you start from a clean slate and leaves you with actually working environment and explains how to modify it later on. I hope this can be one of those tutorials that can help people with little prior tech knowledge, and we can finally get rid of WAMP, XAMP, MAMP etc.
Same here. Work doesn't want to take the time to bring it up to date :(
Thank you for your response. Do you know the specific path to find that section? AWS seems to have a ton of components, I would guess the EC2 instance? But even then I am not quite sure what to look for, please advise. Thanks!
You're getting down voted because this same question is asked every time a thread on PHP 7 is mentioned. 
Do you have your access key set up and permissions set on the EC2 instance?
Yes I do. It is not amazon which is blocking me, it is houzz.com. So its as if houzz.com specifically is blocking whatever AWS IP I am using. But I launched an elastic beanstalk as well and it works fine, so I am not sure why my instance that I can SSH into is not working.
TL;DR for the lazy folks: PHP 5.6 == PHP 6. Our overlords can't ever seem to agree on anything, and after a whole bunch of in-fighting and stalling they end up making a bizarre compromise at the last minute…liiiike creating an abomination by just skipping the next major version number and shoving half of the proposed shit into the final minor (6) of the previous major (5) and deferring the rest to the next major (7).
The RFC process isn't perfect, but man...compared to a decade ago before the restructuring? Internals looks like the inside of a Swiss watch these days, by comparison.
"The community"
I'm almost tearing up. Honestly. *MY BODY IS READY.*
You could probably look at creating a RESTful API in PHP/MySQL then create a desktop application with [Qt](http://www.qt.io). Added bonus, you can compile it to run on any OS.
Well its pretty good for a beginer. Last week I had experiene with puphpet and vagrant and I had to give up. I have heavy website build on symfony2. And it was loading in 160000 ms on vagrant vs. 20000 on xampp. So i have to stay with xampp for now. I also did see that on your computer simple adding &lt;h1&gt; to site took considerably long time to load.
Have you tried using a backend othwr than virtualbox? There are faster obes like vmware. Although personally I prefer docker for virtualization because of far less overhead
class DesktopFormatter implements AbstractFormatter ... class MobileFormatter implements AbstractFormatter ... formatText($text, AbstractFormatter formatter) ... formatText($text, detectClientAndSelectFormatter()); * If formatting don't need server-side data I recommend to leave this custom tag as-is and use AngularJS directive to convert it on client. * It's better to use DOM interface to manipulate HTML document, not text substitution 
Move your cache dir outside of shared folder, and if you have xdebug enabled make sure you have a listener (your IDE).
Internal classes aren't the same as classes you write yourself - due to magic :-) 
oh the nightmares docker ftw
It's important to have a [well configured base box](http://www.sitepoint.com/quick-tip-get-homestead-vagrant-vm-running/).
The only way to have docker on Windows and MacOS still requires you to have a virtual Linux OS. So its not at all easier. The benefit of using docker is that if you need multile containers they will all run inside the same virtual machine. But you will still need to set virtualization up.
In fact, I got this 'code' from PhpStorm (ctrl+click). But, I still wonder how they managed to put many constructors, and why we can't reproduce it.
Thank you, but I'm not looking for an alternative solution. I'm just curious why it's forbidden in PHP, but not in the PHP libraries..
So you recommend using the Data Mapper pattern here ? I think I'm gonna take a look to handle my models in a better way. For the moment I've the static part which is able to query and the non static part which is able to fill data and trigger create / save / delete on connection. Adding a new layer to introduce the Mapper and rely to SR will not be really difficult (I think...).
Hello, I mean that Doctrine is not only Data Mapper it's a really huge engine. Doctrine also is not able to perform ORM / ODM switch without pain (maybe it's just crazy to imagine that...?). I don't know if AR fit all my needs, it fitted my previous needs. I'm going to take a deeper look at the Data Mapper pattern. It's really important for me to know that you'll prefer use no ORM than an AR based one... Maybe the project need to take a new direction...
It is forbidden; To be clear, no class can have more than one constructor, whether it is internal or not. The way internal classes accept parameters is very similar to the manual example quoted above; they parse their parameters in the body of the internal functions code, allowing them to support more than one permutation, or different parameters entirely.
It's not three constructors, it's one with four parameters and the constructor checks the type of the parameters and does different things based on that. Only the documentation looks like there are three different constructors. 
They're written in C. The code for DatePeriod's constructor can be found [here](https://github.com/php/php-src/blob/0787cd60ed3d0c8c8c8ff7e49b9bb3587bf33b64/ext/date/php_date.c#L4316).
Looks like you're after an ajax request, allows you to submit an action without refreshing the page. 
Yes, they are similar in behaviour to Java's LinkedHashMap. 
How can i actually do that? I mean what would be the code for it then?
We don't know your solution so it will be hard to provide an accurate answer however here's a good introduction, which will give you some pointers on where to start. http://blog.teamtreehouse.com/beginners-guide-to-ajax-development-with-php
And/or switch to nfs instead of shared folders. Shared folders are slow as they can be. http://mitchellh.com/comparing-filesystem-performance-in-virtual-machines
you mean if i put ajax then the cart would function in the same page without refreshing to a new page?
*[My body is ready](https://tlcdn.net/i/1/11443790644.jpg)!*
That code never actually executes. It only is there as a reference to help out php storm developers while writing their application.
It's like the PHP4-&gt;PHP5 transition all over again. 
You don't become a senior programmer by learning a new programming language. Senior programmer becomes who has more experience of solving programming problems. Senior programmers tend to be more lazy than others, but lazy != efficient. 
&gt; Every single person responding to OP (except you) has suggested database systems that would persist across reboots, That's not true. Even if it was, only the OP's actual stated requirements matters. Consensus on some forum is not a methodology for how you plan your projects or determine features. I'm not saying anyone's wrong. I'm saying we don't know. If you want to pose "this is what we might want to build and following that, we would need these things", that's an unequivocal plan of need and solution. The problem remains, there is no requirement so there's not a lot of value (which is why few people start listing out a bunch of theoretical requirements). It self-evident, as you're making up work and have to ask yourself...why?
Totally agreed with that comment ! You don't became a senior by learning tools... It's more about team, architecture, experience...
Active Record is easy to start with but has a very short useful lifespan. Data Mapper is harder to get started with, but lasts much longer. You may wish to read http://www.mehdi-khalili.com/orm-anti-patterns-part-4-persistence-domain-model/ for more; I found it enlightening.
I think you meant lazy != inefficient but your definitly on point. I think it's the ability to actually solve problems more unexperienced programmers would have problems with. And of course you might come up with applicable ideas faster due to experience. Which in the end means more efficiency, of course.
Here's another one, specifically about Active Record: http://www.mehdi-khalili.com/orm-anti-patterns-part-1-active-record/
I will reference Zend, even though I would not always tread in their path, but sessions are a critical path for every web app. So here goes: Step 1. * Check for Id and if Id is valid. If id is not valid, proceed to set the session id with a temporary, valid id and mark the state so that the temporary id is regenerated. Set exception variable. //instance variable, etc, could be also an error bitset $throwStartupExceptions = true; if (session_id() &amp;&amp; !isIdValid(session_id())) { session_id(md5(session_id())); $regenerateLater = true; } Step 2. * Return if session is already started. if ($sessionStarted === true) return; Step 3 * Create a session exception error handler, instead of allowing it to throw PHP errors $errorLevel = (is_int($throwStartupExceptions)) ? $throwStartupExceptions : E_ALL; if ($throwStartupExceptions) { //capture the error message, code, log it, mark error instance variable in it that error happened set_error_handler(array('My_Exception_Object', 'handleSessionStartError'), $errorLevel); } Step 4 * Start session and restore error handler, close session on error $startedCleanly = session_start(); if ($throwStartupExceptions) { restore_error_handler(); } if (!$startedCleanly || My_Exception_Object::errorHappened != null) { if ($throwStartupExceptions) { restore_error_handler(); } session_write_close(); if ($throwStartupExceptions) { restore_error_handler(); throw new My_Exception_Object();} } Step 5 * mark session as readable, writable and started, regenerate the temporary id set in step 1 $readable = true; $writeable = true; $sessionStarted = true; if ($regenerateLater === true) { if ($sessionStarted) { session_regenerate_id(true); } $regenerateLater = false; } function isIdValid($id) { //custom logic if save_handler is custom, omitted $hashBitsPerChar = ini_get('session.hash_bits_per_character'); if (!$hashBitsPerChar) { $hashBitsPerChar = 5; // the default value } switch($hashBitsPerChar) { case 4: $pattern = '^[0-9a-f]*$'; break; case 5: $pattern = '^[0-9a-v]*$'; break; case 6: $pattern = '^[0-9a-zA-Z-,]*$'; break; } return preg_match('#'.$pattern.'#', $id); } I've replaced a lot of instance and static variable access with regular method variables, but the logic is here mostly. I would reference a framework in the end, and if a plugin you are using is not obiding by the framework or wordpress quality then fix it and submit a patch, or dump the crappy plugin !
There is a strict guarantee about the order. If the docs state otherwise, the docs are wrong.
The process would go like this: * User clicks "Add to Cart" button * JS is listening for the click event on the "Add to Cart" button, and proceeds to gather required info from DOM * AJAX request to your server passing all required data (e.g., product id, quantity, the data gathered from the DOM) * Server handles request (whatever that means in your application) * Server responds with success and maybe some data if that is appropriate * Client receives success response along with whatever else was sent from the server * Client updates DOM to reflect changes applied on the server side (e.g., add item row to the cart table) 
I personally don't recall reading in the PHP docs that the order isn't guaranteed. I've written and read more code that relies on PHP arrays retaining their order, than I could ever recall, and I've never seen a single bug that was a result of PHP arrays not being in order. Some functions may mess up the order, and/or break the key-to-value mapping, but AFAIK the docs always state it, if this may be the case.
It's a bit hidden, in the EC2 management console you need to go into Security Groups, then select the group for your instance and you should see tabs load beneath showing inbound and outbound firewall rules. The default should be to allow all outbound and to limit inbound to SSH, HTTP, and possibly HTTPS.
Can you point us to a page in the docs where this is stated?
What happens when you try and access the site with something like this: $resp = exec('lynx -dump http://www.houzz.com');
Hey, sorry about the slow response. Just wanted to say thanks for the advice. After getting a few things squared away, I'm going to look into some local PHP shops. If I don't find anything I like, I'll look into your business.
dumb, just use static factory methods, along the lines of ... DatePeriod::interval(..) DatePeriod::intervalExcludeStartDate(..) DatePeriod::iso(..) DatePeriod::isoExcludeStartDate(..)
I like Go too, but thought the "idiomatic" way of running commands was awkward, until I discovered you could exec.Command("sh", "-c", 'string of commands'). How do you jump outside to templating/running shell scripts?
Hey, Happy Cake Day!!! 
What people are trying to tell you is that the libraries don't have multiple constructors. What you see is pseudo code from an IDE that captures the supported formats by a constructor with optional or alternative argument signatures. You can do the same in PHP with techniques like: - "Rest" arguments aka splat operator foo(...$bar) - Optional arguments. - Untyped arguments whose type you check in the function. - func_get_args()
Annotations in Java, C# and Python are not comments, but in PHP they are comments. Since PHP's userland implementation uses docblock comments to write annotations, and docblock comments are comments. To summarize, annotations in general aint comments, but PHP's annotations are. Its fine if the annotations are used simply as tools to generate documentation like PHPDoc. However, Symfony and Doctrine's annotations are terribly detrimental since these are annotations that affect code, putting such annotations in comments means that your code will depend on comments. If the comments are modified or removed, your application will stop running properly, which is ludicrous. And nope, that video is not good at all, PHP annotations are abomination and you should not use it. 
Senior developer is a team role, not a title. My tip is, to be a senior programmer, show more initiative to solve problems before someone else breaks them down into a neat spec for you. Identify issues in the enterprise and propose solutions. If those above you see what you propose as valuable, they will assign you more responsibility, and congrats, you're now a senior developer, even maybe a team leader, if you can get more people under your projects. And then then the only thing remaining is to remind your boss about your increased responsibilities and ask for a raise. They won't say no, because you've become more valuable to the company.
PuPHPet has a checkbox for enabling it
&gt; Senior developer is a team role, not a title. My tip is, to be a senior programmer, show more initiative to solve problems before someone else breaks them down into a neat spec for you. You have to be careful with that too. Refactoring the code is one thing but you gotta be careful not to take the initiative and add new features that you think would be cool. You're a developer - not a project or product manager. I mean, at small companies, that may fly, but at large companies there's generally the expectation of an audit trail and no new feature can be implemented without the proper sign off, etc.
You are so right. I ditched Vagrant multple times because it was easy to get started, hard to get it working the way you want. I'll definitely take a look.
Why do you prefer docker (myself I have never used it, that's why I'm asking)?
I won't comment on how bad of an idea this is but... this should solve your problem. ``` function query($query, $error){ global $database; //... } ```
Since when did phpmyadmin become a database?
As others have said, that example is special-case internal thing to PHP. For your own code, the best alternative is to use named constructors, e.g.: class Foo{ private $a; private $b; protected function __construct(){ } public static function CreateA($a){ $f = new Foo(); $f-&gt;a = $a; return $f; } public static function CreateB($b){ $f = new Foo(); $f-&gt;b = $b; return $f; } } $typeA = Foo::CreateA("hello"); $typeB = Foo::CreateB("world"); Essentially, you're making your `__construct()` method dumb/incomplete, but also making it private or protected so that nobody else can call it by accident. Then you provide your own static methods that know how to do the right thing and return a "proper" Foo for either configuration.
Likely when phpMyAdmin is preinstalled and you use it to exclusively interact with MySQL. Not quite as bad as the Excel database though.
Good point! "interoperability" to throw a business buzzword in the mix. You need to understand the jobs the rest of the team does. You do not need to be able to do their job, but you have to understand how they work and what their goal is. I´ve seen some seniors that seem to be working against a large part of the team because the were too focused on one view. 
phpmyadmin is not a database. its jst tool a to interact with MySql database.
&gt; Every single person responding to OP (except you) has suggested database systems that would persist across reboots, It's funny how you're wrong and can't admit to it, despite direct [evidence](https://www.reddit.com/r/PHP/comments/3lrret/can_an_offline_hotel_management_system_be_done_in/cvc97c4) to the contrary. You can't even pay attention to detail. Full on conversations must be really maddening everywhere you visit. Maybe it's the draw of the red envelope and feeling like you need the attention. Those are my best guesses, based on your behaviors. You failed at making a salient point worth discussing after MULTIPLE opportunities to say more than "nuh uh, see they agree with me" like some kind of idiot. Ignored.
I believe you haven't read the article in detail. The list is built on a tool known as List.ly which can be embeded on a blog. Second, they have clearly mention that its not a ranking. The list based on personal liking. Everything is mentioned there. And I believe there are all good people there. Stop complaining please.
I have a custom class running on one of my sites that integrates Google Drive, Calendars, and the Admin SDK. You just need to use Composer to install the library and all the dependencies, then as they show in the demo code you include 'vendor/autoload.php' where you need to access the library. From there is really depends on what you're trying to do. Are you using this to access your own personal Google account or are you the admin for a corporate Google Apps account? If you're doing this to access Google Apps I'd recommend creating a service account. Then you'll have to setup some permissions to allow you to access drives in your organization and list their contents. 
A reply to your two parter, hope it isn't too late... 1) Complexity -- The more I think about it, the more it seems like its just a really powerful data-access layer, rather than a domain-layer. That's not a bad thing, because domain-models really come into place when theres [complex business logic](http://udidahan.com/2006/10/07/ddd-why-bother/). Adding a few numbers together isn't complex business logic. If we added rules regarding vacation, overtime, sick days, holidays, and manager approval-- we could probably classify it as a complex domain model. With all of those rules added, it becomes more apparent where the business logic is actually checked: the database. e.g UPDATE ... SET value = IF(someCondition, someValue, someotherValue) or worse: value = IF(someCondition, IF(someOtherCondition, IF(someThirdCondition, value1, value2), IF(someFourthCondition, value3, value2)), IF(sometFifthCondition, value4, value5)) From a functional perspective, it looks fairly attractive. From a business perspective, it looks complex. The benefit of having "simple" domain objects with clearer logic appears lost as more business logic (conditions, rules, invariants) are added. You may want to consult some of the other ddd groups online about the idea-- in particular: the disadvantages of tying your business layer to a persistence library. When I think of complexity, I think of cognitive complexity, things like: * How many things does a developer have to think of when working with a component? Before, an aggregate needed no further information outside the aggregate. Now the developer has to keep in mind the repository and its capabilities. * How easy is the system to test (automated: unit, spec, etc)? How many methods/classes need to be mocked/stubbed. How much behavior are we testing on a method? * How easy it is to refactor/change? It seems like the database/repository needs to support every big of logic needed-- which seems limited. Especially if you consider the cost of the simplest possible implementation-- that's the *minimum* cost for changing a persistence layer-- and that seems *much* higher. Its also a cost that grows as more logic is needed. 2) Fixed transactional boundaries -- Nested transactions do help resolve a big chunk of issues I had regarding how fixed they were. Still, I wonder what their point is. Isn't it safe to assume that at least one database transaction is open? Does the behavior/code change at all if we remove the start() and end() lines from the facade/entity? 3) More complicated consistency logic. -- I touched a bit on above (in #1). I'm also addressing [your other comment](https://www.reddit.com/r/PHP/comments/3mwl2z/in_ddd_for_php_what_is_the_difference_between_a/cvk64pu) here. Consistency is not something that's magically solved. There's no one-size-fits-all solution. There are cases where an aggregate/entity may need to know more to keep the entire system consistent. That does not mean that it needs to be aware of the persistent transactions. If a business has its own transactions, they may not line up with a databases concept of them (like one spanning several weeks). There is a difference in what a domain cares about. The business model, logic, and changes are (often) better described and expanded. The repository is more simple because its usually less important to the business. You don't talk about properties, tables, ifs, ands, adds, and such as much because those are technical details. The business cares about the hours being recorded-- they don't care so much about the rows tracking the hours being inserted/updated. I don't think I can stress how important understanding [aggregates](https://vaughnvernon.co/?p=879)) are. They heavily influence domain design, how entities interact with one another, and what methods/actions are in the entity. When the aggregates are well designed, consistency isn't really the same problem that it was. You seem to keep talking about *finding* consistency problems. That's just not something I've really heard of DDD projects needing to do (I may be spoiled by well designed-aggregates). The consistency problems tend to be more along the lines of: * We have an action that doesn't belong to either aggregate, where does it go? (possible solution: domain messaging). * We stopped an action we shouldn't have (Aggregate may be too large, or there may be more business logic to add) * We're bogged down by conflict notices (Same as the above, but there is the chance the aggregate really is updated too often for the simplistic locking to work) None of these problems can be solved by some magical one-size-fits-all situation. A medical records domain, invoicing domain, and servicing domain are likely going to have different solutions to the same problems. Banking, for example, wants to ensure an account is debited and an account is credited. Your simple example works to show how a transaction can ensure that business rule is maintained, but its not the only way. Other methods become more apparent as you add more business rules: * A debit more than $3000 is not allowed unless its done in a bank with 3 days notice. With notice, its shown as pending until picked up. * Credits more than $500 takes an additional business day, but they show up as pending as soon as they are received. * An account shouldn't be allowed to debit more than it has available, unless they have overdraft protection. * When overdrafting, the account is charged an overdraft fee which allows them to overdraft up to a different amount. This fee should be refunded if the transfer fails. * All of these actions should be logged in the account history, even if they fail. * If there's any unexpected error, the transaction should be flagged for attention. So, the original process was just like this, in one transaction: * AccountDebbited &gt; AccountCreddited With the additional business rules, it looks a bit more like this: * TransferStarted &gt; DebitPending &gt; CreditPending &gt; Debited &gt; Credited &gt; TransferComplete * TransferStarted &gt; DebitPending &gt; AccountCreditPending &gt; TransferCanceled &gt; AccountCreditCanceled &gt; AccountDebitCanceled * TransferStarted &gt; TransferFailedDueToInsufficentFunds * TransferStarted &gt; * &gt; TransferErrored Each action is independent, each action can fail and be visible. Neither account will enter a state that's unexpected (if another account updated is persisted before we can save, we reload it and try again). Yes, its a lot more work to work than having a single database transaction, but the smaller non-conflicting actions are more flexible, auditable, and can span longer than one would sanely run a database transaction for. For most aggregates, writes are only occasional. Simple checks (like a version) work to *detect* a conflict. Actions that can be retried safely are, in my experience, better handled as a case-by-case basis. Account being credited? Retry permitted. Account being closed? Maybe we should notify the user of the change first. 4. Performance. -- I'll admit that doing the math in the database does allow for a fairly significant operation. It still only works if you can fit *all* of the logic inside the repository. As soon as there is logic that can't exist in the repository, you're back to the same old O(1) performance. In a lot of cases, especially when just performing writes, that's acceptable. Hardware tends to be cheaper than developers. Be wary of premature optimization. When it comes to scalability (horizontal and vertical), #1 and #2 tend to be more important. 5. Unclear Conflict Resolution -- You addressed the concern that I saw, everything else I wanted to say fit under #1 and #2. tl;dr == * BlahBlah Complexity * Consistency is important at the business level, not the persistence level. The design should reflect as much. * BlahBlah Performance
What are some people that you would include?
Off the top of my head, in alphabetical order: * @AndreaFaulds * @dd32 - Pretty much the only person who gets anything done in Wordpress land, when I report any issue * @dshafik * @elstamey * @enygma * @htimoh * @ircmaxell * @narfbg * @nikita_ppv * @padraicb This doesn't include organizations, mailing lists, etc. or people already listed in the article.
This is awesome list. Thanks a million for sharing.
I've used Google Drive and OAuth2 - the google documentation was awful, out dated, confusing and didn't seem to really teach as much as obliterate my mind, ( this was about a year ago ), so there was a lot of search and several frustrating 14hr days so the project would finish on time.
I believe that is what is happening. Do you recommend a web host for PHP apps? I just tried siteground but non of my code seemed to work on their server. Would you recommend anything for a custom build app such as mine? Preferably something with a 30 day trial. 
Twitter is too full of drama and non-relevant tweets and opinions for me. Does anyone else happen feel the same? 
What you *think* you're looking at is called "Constructor Overloading". This is not supported in PHP, but rather other strongly typed languages like C# and Java. Even in these other languages, Constructor overloads does not mean that you can have multiple constructors. Only one constructor ever gets called on a new class instance: The one that matches the arguments provided. [Here is a StackOverflow question](http://stackoverflow.com/questions/829870/calling-constructor-from-other-constructor-in-same-class) that shows how overloaded constructors work, and how C# specifically can support constructor chaining. However, this is getting off topic. The reason why I'm saying that you *think* you're looking at Constructor Overloading is because of the way PHPStorm's documentation works on PHP Internals. These are not true representations of native PHP classes, because PHP classes themselves are written in C. What this is showing you is an example of how the DatePeriod class may be called, and the various combinations of arguments its ***single*** constructor supports. It's purely for reference only, and is intended to provide a more comprehensive way for PHP developers to know how PHP internals work without having to stare at C, or scour the PHP website's documentation.
&gt; A reply to your two parter, hope it isn't too late... Nope! I'm still alive. :-) &gt; The more I think about it, the more it seems like its just a really powerful data-access layer, rather than a domain-layer. Not sure if you mean my repos, or my entities. &gt; You may want to consult some of the other ddd groups online about the idea-- in particular: the disadvantages of tying your business layer to a persistence library. None of my examples demonstrate tying the business layer to a persistence library. I went out of my way to explain you can implement a repo with just about anything. So... ? &gt; How many things does a developer have to think of when working with a component? Before, an aggregate needed no further information outside the aggregate. Now the developer has to keep in mind the repository and its capabilities. The fact the storage is *remote* &amp; *accessed concurrently* are two things *you can't ignore*. Ignore them... you get broken domain state. I'm not a bad guy telling you can't have your all-in-memory domain. I'm the *messenger* who's telling you that it's impossible to interact with a *remote concurrent* server as if it's a *local non-concurrent* one. Not sure the message is understood. &gt; How easy is the system to test Normal. - Unit-test the entities with in-memory repo. - Test a repo with its storage mechanism (makes no sense otherwise). &gt; How many methods/classes need to be mocked/stubbed. Can't be answered out of context. Depends on the project. Let's say one or more. :-) &gt; How easy it is to refactor/change? It seems like the database/repository needs to support every big of logic needed Again a relative question with no point of reference. But you should know I value refactoring very highly. Re. "every bit of logic" that's obviously not my example, because otherwise I wouldn't have entities with logic. So your conclusion has a faulty premise. A repository concerns itself with *persistence &amp; transforms of the domain state*. It doesn't validate data, it doesn't enforce business constraints, it doesn't decide or know what happens when there's a business logic error, it doesn't even *know* what that is. Think of it as LEGO blocks made out of comp-sci data structures. Tuples, maps, scalars, trees, graphs, logs, lists etc. I take those building blocks, and I give them intuitive names to the domain. Event log = "balance". Append debit event = "add". Those are one-line abstractions intended to put the data model in context. That's all it is. The transaction mechanics are simple: everything you read &amp; everything you write (and everything those reads/writes depend on) in a transaction should be temporarily consistent with each other. &gt; Especially if you consider the cost of the simplest possible implementation-- that's the minimum cost for changing a persistence layer-- and that seems much higher. That simplest possible implementation you keep implying is this: $entity = $repo-&gt;get($id); $repo-&gt;persist($entity); It's "key value store", a "dictionary" or a "map": $object = $keyval-&gt;get($key); $repo-&gt;set($key, $object); It's simple and I use it. *When it applies*. Do you know why I don't **always** use it? Because this data model doesn't capture the fact there may be *any* sort of connection between the data at one key and another. You know what happens when your data model doesn't capture a relationship like that? You lose.... Consistency ------------- If we'll continue to keep brushing the topic of consistency under the carpet, we'll never agree. I keep stressing how important consistency is, and you ignore why a naive key-val approach may violate it. &gt; Still, I wonder what their point is. Isn't it safe to assume that at least one database transaction is open? Does the behavior/code change at all if we remove the start() and end() lines from the facade/entity? It's not safe to assume. It makes no sense to just open one and keep it open the whole time. So if you don't open one, one won't be open. &gt; Does the behavior/code change at all if we remove the start() and end() lines from the facade/entity? ... Of course! It changes in these ways: 1. No locks on the data you read and write, means no atomicity of the transforms. Say hello to inconsistent data. 2. If you don't specify where the *start* of a transaction is, you *can't roll back* to that start if you encounter a business logic problem in your entity or in the caller using it. I think this question reveals a big misunderstanding about what I'm doing. I thought the fact my API is at least superficially similar to an SQL transaction helps drive the point across. **Those are actual transactions that take locks, and can be rolled back.** &gt; If a business has its own transactions, they may not line up with a databases concept of them (like one spanning several weeks). I think you confuse "database" with my repository in a few places, despite my attempts to differentiate both. A repository is an abstraction. It may choose to implement a transaction as an SQL transaction or not. Obviously you won't be holding an SQL transaction for weeks. This doesn't mean a repository can't expose a transaction you can hold for weeks. A repository can use SQL transactions (if it's implemented via SQL connection) without you opening a repo transaction, and may not open an SQL transaction when you open a repo transaction. Both concepts, as you say, are not related. The only thing that matters is the repository has a clear public contract and enforces it however it chooses to. &gt; I don't think I can stress how important understanding aggregates are. You seem to keep talking about finding consistency problems. That's just not something I've really heard of DDD projects needing to do (I may be spoiled by well designed-aggregates). Our entire discussion is implicitly about *how* to design an aggregate, basically. So saying "I may be spoiled by well designed-aggregates" does in essence mean "I don't have this problem because someone else solved it for me". I'm sure you don't mean it this way, but it for sure doesn't give an example of me misunderstanding aggregates. If you think I'm missing something, share it. &gt; Same as the above, but there is the chance the aggregate really is updated too often for the simplistic locking to work. Can we focus on what "the simplistic locking" *means* here? I wonder if this entire time, you've been arguing in favor of coarse locking on low load systems, and me on granular locking and high load systems? I guess that would explain *some things*. But I'd need an example. &gt; Banking, for example, wants to ensure an account is debited and an account is credited. Your simple example works to show how a transaction can ensure that business rule is maintained, but its not the only way. Other methods become more apparent as you add more business rules: After this you're listing a bunch of business concerns, and then giving an example of a workflow process to implement it. I'm fine with that, I implement business process automation software for a living. So I do it a lot, too (I also have a reusable repository interface for it!). But I also see no connection at all between the example and what we're talking about. Just like you wouldn't use SQL for a week-long process workflow, you wouldn't use a workflow API to model a transaction that lasts 0.001ms. *Correct?* But honestly, why are you even giving this example. You were arguing how simple repositories should be one moment, and how simplicity is important and next moment you give an example that ends with... &gt; Yes, its a lot more work to work than having a single database transaction Yes, it's a possible way of modeling things, but we don't have an example here where this is a suitable approach. Maybe, to suggest again, you should give a better example? A scenario of some sort I can respond to? Although if it takes weeks, it'll be a workflow API, that's a given. &gt; For most aggregates, writes are only occasional. Simple checks (like a version) work to detect a conflict. Actions that can be retried safely are, in my experience, better handled as a case-by-case basis. Account being credited? Retry permitted. Account being closed? Maybe we should notify the user of the change first. Occasional writes... It's a big assumption to make for the general case. I have situations where it's correct and ones where I have literally *write-only models*. Let me focus like an eagle on something else you said: "simple checks like a version work to detect a conflict". 3 important questions: 1. **When you read something, do you analyze to determine if your writes are based on your reads?** 2. **Do you have reads that might change (the resource being read is mutable, a repeat read might return a different value)**. 3. **If your writes are deemed dependent on your reads (which may change), do you version check your READS as well AFTER THE WRITE?** I think answering those would give me a lot of insight about whether your approach works, or not. So I suggest you may ignore the entire rest of this comment, but I'd *highly appreciate* if you answer those three questions. &gt; I'll admit that doing the math in the database does allow for a fairly significant operation. It still only works if you can fit all of the logic inside the repository. As soon as there is logic that can't exist in the repository, you're back to the same old O(1) performance. It's fine if we have a middle-ground, let's not be absolutists. I start with the simplest repository that *properly reflects the domain*, and then, if need be *I can add specialized transform methods on it* that handle bottlenecks more efficiently. It's possible to have best of both worlds.
I like it. Has enough brevity without being too unfamiliar and ambiguous.
Without creating your own DNS server, you could always modify the [hosts](http://www.howtogeek.com/howto/27350/beginner-geek-how-to-edit-your-hosts-file/) file.
I like the direction PHP is going lately. I've been using PHP for close to 15 years now and the evolution is amazing to see. I mean I use Ruby, Python, etc, as well but it's the first language I've seen personally to go through so such a nice change.
Was there ever a discussion about why we can't just try for the Hack syntax? These are valid in Hack: array_map($x ==&gt; $x * 2, [1, 2, 3]); array_map(($x) ==&gt; $x * 2, [1, 2, 3]); array_map(($x) ==&gt; { return $x * 2; }, [1, 2, 3]); array_map(($x) ==&gt; { return $x * 2; }, [1, 2, 3]); http://docs.hhvm.com/manual/en/hack.lambda.examples.php EDIT: Just to clarify, totally support the RFC, just curious if that syntax has been considered and rejected for any reason.
&gt; Bob's proposal dropped the function prefix because of brevity. This is also done in EcmaScript 2015 (ES6) and C#. However, in PHP we can provide type information on parameters and can also pass variables by reference which together cause an ambiguity. In the definition (Type &amp;$x) =&gt; expr the part (Type &amp;$var) can parse as “take constant Type and variable $var and do a bitwise and &amp; operation.” After that the =&gt; will be an unexpected token. Even though the rule would be invalid the parser doesn't know that far ahead there will be an error and it doesn't know which rule to pick. This can be resolved by parser backtracking or a painstaking rewrite of our parsing rules. This author is of the opinion that it is better to avoid the issue than to resolve it in this case. I appreciate the pragmatic approach of the RFC author, but it's awkward to discuss how hard it is for the parser to figure out it's not parsing a bitwise expression, or that we can't have types in the short version because the parser so and so. C# has types, TypeScript which implements ES arrow function has types, and Java, which has arrow functions has types. And they don't need the "function" prefix. I'd rather not have arrow functions until we can get it right, and not get a half-assed version in 7.1. We should be moving in a direction where typing "function" in front of both methods and closures is not necessary, instead of introducing a minor syntax variation that adds little value to closure heavy code. And another issue, this is yet another RFC which ignores the uses of modifying captured variables from a closure. JavaScript doesn't have PHP's references, but modifying captured variables **just works**. The function semantically inherits its parent's symbol table and nothing gets copied. This RFC copies.
You are astonishingly dumb.
&gt; Constructor overloads does not mean that you can have multiple constructors. If you define multiple constructors in the definition of the class, that's simplest way to meet the qualification of "having multiple constructors". Being able to call them is even stronger (since they survive compilation). Constructor overloading specifically means you _can_ have multiple constructors. That's the point of the feature. Nothing about order of execution is implied from that. PHP does not support constructor overloading, this is true.
The OPs description: &gt; What is the best PHP online viewer available like how codepen works for js,css,html? Maybe they would. I would *shrug*
I feel this is a much better proposal than the short_closures rfc. I like the idea of a using the shorter fn prefix, but I would honestly be happy with either. This is something I've been hoping for in php for a long time. 
QNAP NAS does this without installing any software on your local machine. Not sure how it works. Once you refresh "My Network" or "Network" It shows up as both a shared drive and another item shows up as a website to get to the admin panel. I guess technically it shows up as a "network device" and even right-click has an option to view device web page which is the local admin panel.
I'm a big fan of fat arrows in ES2015 - I'd love to use them in PHP. Definitely makes what should be simple map-and-reduces less jarring.
Yes, thanks for the correction!
It allows you to split your environment in containers instead of having a complete separate virtual system for each project. So you might have a simple website with 2 containers, 1 for nginx+php, and 1 for mysql. its extremely easy to setup, has zero noticable overhead, no bottlenecking anywhere. I dont even stop containers when im working on other projects, because you dont need to. Deploying and developing becomes 10x easier. I rather scoop poop in the zoo than go back to virtual machines. The beauty of it is that there is a very actively maintained registry so there is very little chance to you need to build your own environment, and maintain them. (https://hub.docker.com/) And if u need to something to suit your needs, you can just extend and adjust. Only downside is that it runs on top of the linux kernel, so you need a virtual machine to run the engine if you develop on osx/windows.
And changing *function call syntax,* and changing *string indexing syntax*... twice... was less of a change? - What about changing object pass semantics? - Or changing the syntax of declaring constructors? - Or changing the syntax of declaring public properties? - Or changing implicit static methods to explicit ones? - What about *adding short array syntax declaration in 5.4*... ? I can keep going. All stuff that happened in PHP 5. All those comments that said 5.4 short arrays would ruin PHP sure *do look silly now*, don't they? Maybe you wrote one of *those* comments now. ;-)
func
just function, fn or func would just increase inconsistency (for example people wanting to use fn or func in normal function declarations as well)
Note and fixed. Thank you.
&gt; We should be moving in a direction where typing "function" in front of both methods and closures is not necessary[…] Actually, if you look at modern languages such as Swift, Rust and more they keep a leading token. It's always shorter than `function` (`fn`, `func`, `fun`, `def`) but the point is they keep the token. I get the feeling you don't like it because certain languages you are familiar with don't have it, which demonstrate *it's possible*. However, it makes things more difficult to write tools for. I expect you don't realize how much more difficult.
&gt; Python is not a programming language for WEB The website you are currently looking at is written in Python. 
Bob's Short Closures RFC failed and a lot of people expressed reasons other than the symbol choice (`~&gt;`). They want type declarations, they don't like omitting parenthesis for single parameters, etc. It's uncertain if changing the symbol from `~&gt;` to `=&gt;` would get enough votes to pass but I think it would be insufficient.
I wasn't commenting on what it has, just making a note on how much good change is coming to PHP lately.
You misunderstood. You said closures and methods. These languages keep it for methods and functions, which is what I was addressing (not closures). Sorry for not stating that more clearly.
Fine, keep it for methods. I think it's entirely pointless, but I don't care. But don't put it in expressions.
&gt; The same thing now. We have more and more languages support nearly the same arrow syntax and semantics for closures, and PHP internals is always castrating RFCs because people have "expressed reasons". The difference is that we still have to get RFCs to pass. Short closures did not pass. There has to be something more substantive than changing the symbol from `~&gt;` to `=&gt;` to get it to pass.
Please be more polite. You have been quite disparaging. You have no idea how much work or discussion has gone on – how can you say they are just lazy and whiny? In case you are not aware I am one of the RFC authors. I'm not complaining about the parser. I simply provided reasons for the choice so that people who aren't familiar with that aspect are aware. I clearly stated facts and then said it was my opinion that it's better to work around these issue. I also can say I've put in a lot of effort. I have not been lazy. Please be more respectful. If you disagree with decisions and philosophies that's fine, but your attitude here has just been aggressive and arrogant.
There's probably no stable MySQL module for Python3 because everyone uses Postgres because it is far superior. You're welcome to have your opinion, but there are a huge number of large and small websites you likely look at daily written in Python. Saying Python is not for web development is like saying a socket wrench set is not for working on cars.
[removed]
&gt; There's probably no stable MySQL module for Python3 because everyone uses Postgres because it is far superior. You're welcome to have your opinion, but there are a huge number of large and small websites you likely look at daily written in Python. Saying Python is not for web development is like saying a socket wrench set is not for working on cars. You are true. I told missing , In my opinion Py3 isn't for web. I code PHP over 3 years and I can't give up habits. But I have to say Mysql(MariaDB fork) is still useful database. It's simple and It has all features of a RDBMS needs. 
To be fair, every standard install of Apache and Nginx handles PHP like it was meant for it. Getting a stable Python process running that is as error tolerant and robust is not quite as easy. Following the WSGI tutorial for Django, for example, didn't actually work the last time I tried it. I had to search around try a few SO suggestions to get Django to run via a web server the way any PHP framework does, and I was not about to dive into learning how to configure supervisors to keep a Python script running. You also cannot easily echo Python data into HTML without a separate template engine, since Python is not a template language the way PHP is. Not that any sizable PHP applications are built without a template layer these days, it's still pretty simple to build a PHP HTML page that echos dynamic data via PHP. So to OP's point, Python does not quite have the same web DNA that PHP and its ecosystem does. Of course, that doesn't mean you can't build websites with it, but I would argue that PHP is still a more appropriate tool for most web applications.
http://php.net/manual/en/features.commandline.webserver.php solution is embedded web server. false flag this isn't solution
The embedded webserver is only for debugging purposes. It is single threaded and not for production The first thing on the page you linked said to not use it for what you're using it for.
&gt; So the object model is revamped since PHP 5.4? I thought PHP 5.4 is a minor release(not a major one like PHP 7). A major release is only required when the *public* api (e.g. they way you program in the language) is changed. Only the way objects are internally represented (i.e. the private api) was changed. &gt; Does it mean that sometimes objects are still represented like associative arrays? PHP is a Dynamic language, you can create properties on an object without predefining them (Though this is not often advisable as it makes an class/object harder to understand) - in this case you must use a hash-table (Or associative array as you call it) to store the properties. &gt;Also do you know how objects are represented in other programming languages? It varies a lot, in C++ all the properties are predefined and have a set offset from the base object pointer. In Python all properties (Including the methods) exist in a Dictionary (Basically an associative array) that you can directly access via the meta property `__dict__`, e.g. `cat.__dict__` is the dictionary of properties for the Object called cat. 
Let me guess... safe mode, magic quotes, or register globals. http://php.net/manual/en/migration54.incompatible.php
Dynamic languages typically need a quick way to look up object members by name so hash map is a common solution. Ruby, Python, JS, etc. Separately from this these languages may have an optimized in-memory model for their instances, to save on RAM, optimize performance. PHP also does. This doesn't affect how you program, however, so it shouldn't be important as a user.
Wouldn't it be fair to say they're still hash maps, but with a shared table. Also, a per object hashmap is created for ad-hoc instance properties, right?
 The accepted answer has it right, you can just do ""composer dump-autoload -o"" and get the speed benefit of classmap. Looking at a non-psr-4 PHP codebase these days really takes you back to what 'legacy' feels like. I've had much more pleasant coding experience integrating with PSR-4 libraries. It seems like such a small detail (how files are organized), but it really makes a huge difference when you're working with 5+ different libraries. It's one of the things I miss when working with JavaScript (looking at you, node.js)...
The pointer may change, but the order is maintained - unless of course you run a method that explicitly modifies the order.
So in dynamically typed languages such as Python and Ruby, objects are implemented like associative arrays too, similar to PHP? 
You can likely reproduce this yourself by visiting your site to get a proper session cookie, then edit the cookie value by erasing the session id so it's blank, and then visit your site. You'll likely see the error you describe. Frequently bots will come by your site and keep your cookies but lose the cookie values. So they'll pass the cookie to your server so it's technically set, but the value is invalid when passed to session start. The fix is to validate the session id value before session_start().
Yes. You are true , I tried and I understood :/ I am still search a solution
I would also add Sarah Golemon @SaraMG
If I understand correctly you should the folder where php.exe is located to the Windows Path. I hope you know what is Path. Then when you type php ind the command line php.exe will be executed. Something like that.
Coming from a Tech background in radio communication repair It's MIC. Mike will back me on this. Hu man.
In some of them, sure. Others may do different things. To the end-users, it's absolutely irrelevant.
They didn't. The *actual* constructor looks probably something like the following (except in C): public function __construct(...$args) { if (is_string($args[0])) { // option 3 } elseif ($args[0] instanceof DateTimeInterface) { if ($args[2] instanceof DateTimeInterface) { // option 2 } elseif (is_int($args[2])) { // option 1 } else { // error } } else { // error } }
Digital Ocean is another popular cloud service you might try.
I haven't spoken danish in years, let me try this .. Jeg havde sadan et problem før, prøve det her: $html = file_get_contents('http://websiden.dk'); $dom = new \DOMDocument(); libxml_use_internal_errors(true); $dom-&gt;loadHTML('&lt;meta http-equiv="content-type" content="text/html; charset=utf-8"&gt;' . $html); libxml_use_internal_errors(false); Det gør at inholdet bliver utf8 encoderet selvom documentet ikke er erklaret som utf8. 
As a data-access/transformation tool, the pattern (I'm unsure if Repository is accurate) you describe has a simpler, more powerful API. When I think of it in that context, it and the pattern it follows sound very useful. For a domain model, it seems like a lot of coupling to add-- particularly for limited gains. I haven't seen how exactly it handles updating values conditionally or disallowing certain operations, but it sounds as though the transformation performance benefit goes away when those are introduced. Its not just the complexity-- I mean, writing a Domain Model is more complicated than not writing one, so we have to accept some complexity. Its the complexity I see is seems hard to justify it when what appears to be the only main advantage of the tight coupling disappears. (I'll get to consistency, again, shortly) There are reusable repositories that don't couple with the domain code in the same way. ValueObjects and entities can encapsulate the business logic just as well. In the same way that you repository started captured the business value of $balance, the same can be done using value objects and/or entites. We covered testability a bit, and you likely disagree, but the repository seems to be something too complicated to mock-- so it depends on the in-memory implementation. In a lot of domains with tests, there is no in-memory repository implementation (they're not needed for testing). The domain logic (yes, with consistency) can be tested without it. The consistency isn't magical-- it exists because the repository only operates on [aggregate root](http://dddcommunity.org/wp-content/uploads/files/pdf_articles/Vernon_2011_1.pdf) objects-- and the aggregates don't reference outside entities/value objects. The repository only stores the aggregate root if the stored one hasn't changed (implementations vary, but optimistic locking seems common). So if any of the data the aggregate depends on changes, the save fails. This includes any sub-entites and such (those are only editable by the aggregate root, because its what uses consistency). These aren't new patterns (AR was described in the "Blue Book" in 2004, but was in use before that), its not mine (I simply use it), and they grow pretty well-- even when the database is sharded or otherwise implemented using multiple databases. Vernon's essay (linked above) probably provides better examples while going advantages, disadvantages, and common pitfalls. &gt; 1. When you read something, do you analyze to determine if your writes are based on your reads? &gt; 2. Do you have reads that might change (the resource being read is mutable, a repeat read might return a different value). &gt; 3. If your writes are deemed dependent on your reads (which may change), do you version check your READS as well AFTER THE WRITE? 1: Yes (as a part of aggregate design), 2: Yes In the context of an aggregate root, it seems like you're asking this: BEGIN -- transaction started by repository SELECT a.field1, a.field2, a.field3, a.version FROM employees WHERE id = :id -- load() UPDATE SET a.field2 = :value, a.field2 = :field2 WHERE id = :id AND version = :version -- save() --checks for affected rows COMMIT -- save() SELECT a.field1, a.field2, a.field3, a.version FROM employees WHERE id = :id -- are you asking if save() does this? What good is the second select if the we've already has already discarded the entity? Is it because your less coupled repositories use to reusing the object returned? My services use a separate repository to fetch and manipulate the domain-- so it doesn't get a reused object. If I call a service twice, yes, it'll run an additional query. Usually my service has a methods to avoid those cases. Side-topics: &gt; Just like you wouldn't use SQL for a week-long process workflow, you wouldn't use a workflow API to model a transaction that lasts 0.001ms. Correct? The time it takes doesn't matter as much as the business logic. If the business wants to actually apply business rules to it, a workflow API may be a better fit. &gt; It's simple and I use it. When it applies. Do you know why I don't always use it? Because this data model doesn't capture the fact there may be any sort of connection between the data at one key and another. In the situation you describe, the object depends on data outside of itself, so it can't be an aggregate root. Yet, even though repositories should only deal with aggregate roots, we're loading and saving the non-aggregate root anyways.. There's nothing in the domain working to ensure its consistency, so its no surprise consistency can be lost. &gt; After this you're listing a bunch of business concerns, and then giving an example of a workflow process to implement it. I'm fine with that, I implement business process automation software for a living. So I do it a lot, too (I also have a reusable repository interface for it!). But I also see no connection at all between the example and what we're talking about. Well, its more than just business concerns, they're business invariants/rules. I gave a workflow that described the calls a service would make on the aggregate roots. I'm a bit curious if, with rules like that, if ther would be much advantage to using the pattern you proposed. Remember, any time any part of the aggregate changes, including its subentities, its version changes. (It'd be nice if there was a good DDD sample repository to help point to things, but I've not found a good one yet). If I've missed an important question or something I missed needs follow-up-- let me know. I'm not purposely trying to brush anything under the rug or declare it as someone else's problem. 
&gt; C# has types, TypeScript which implements ES arrow function has types, and Java, which has arrow functions has types. And they don't need the "function" prefix. Again, those languages are designed differently than PHP. If PHP does it differently, why is that "wrong"?
Yeah, Python has proven to be a programming language for almost anything. Kudos to them.
You're allowed to have that opinion about Python. Though I can think of some Pythonistas that would argue that point... My personal opinion is this: any complete language can be used for pretty much anything, but some offer advantages in certain situations.
&gt; For a domain model, it seems like a lot of coupling to add-- particularly for limited gains. The repository is an interface (or a set of interfaces) which is modeled after the business layer that is using it. I don't think a SPI can be properly classified as coupling. It's like saying a component is coupled to itself. I suppose you mean the surface of the repo interface. Which, is again, not as much if you consider a data-level model tends to resemble one of the core comp-sci data structures as I already noted and listed. If you think I'm doing it for limited gains, let's see what I gain through *examples*. Say, my task is to model a giant tree of data. The user can move/delete/insert subtrees under another node, sort and re-order children under a parent, easily fetch ancestors and descendants. I have the building block, as an interface and several implementations, and I only have to either directly expose it as a repository, or compose it into a domain-specific facade with several 2-3 line methods. I'm curious how would the repository for *this* look in your case? Just don't forget: you can't load the entire tree in the PHP process, it's too big. You've said several times that consistency is about splitting things in small aggregates and operating on them atomically. It's proposed as a silver bullet solution, so I'm curious what the small aggregate is *here* that would fix this problem. &gt; There are reusable repositories that don't couple with the domain code in the same way. ValueObjects and entities can encapsulate the business logic just as well. In the same way that you repository started captured the business value of $balance, the same can be done using value objects and/or entites. Well I'd be curious to see you tackle the tree example with that strategy. The way I see it, we're not even on the same page, because the problem I'm trying to describe is capturing *relationships between entities and values* and the examples you give can only capture isolated logic in isolated objects. You mentioned messaging in at some point. Messaging also requires protocol guarantees like guaranteed delivery and idempotency, or you end up up with inconsistent state. And even with those guarantees, you still need to implement coordination at the application level, which as you said is way more complicated, than using local transactions when they suffice. I like the simplest solution that works. With you... I'm on the fence. You keep switching between "repositories should be simple" and "lets reinvent the wheel by reimplementing transaction mechanics in the business layer through workflow engines". There's a place for custom coordination through messaging and inherently local updates is not them. &gt; We covered testability a bit, and you likely disagree, but the repository seems to be something too complicated to mock. Often I don't even have to rewrite the repository as a mock, I just need to inject the existing repository with a mock data structure repository. This is because the repository is often a simple facade for a basic comp-sci data structure or a pattern, you know, the way "models" in MVC and so on were originally intended to be designed. So if I do new BalanceRepo($mysqlEventSourceLog) I have a "real" repository. But if I do new BalanceRepo($mockEventSourceLog) I have a "mock" repository. It took me 0 lines of code. &gt; In a lot of domains with tests, there is no in-memory repository implementation (they're not needed for testing). The domain logic (yes, with consistency) can be tested without it. All, right, let's put this to the test with a basic 101 examples, shall we? Here's a business logic invariant (I already mentioned this one): - No two users on the system can have the same email address. Now show me how an entity enforces this and how you unit test for it without a repository (mock or not). I'm honest when I say, if you can show me how to do this, it'd be opening a new universe of possibilities for me. So looking forward to your answer. &gt; 1: Yes (as a part of aggregate design) You're saying that when you write to an aggregate, you base this only on choices that you've read from the aggregate and nowhere else. I honestly seriously doubt you'd go far with this approach. So "aggregate design" isn't really answering my question. &gt; In the context of an aggregate root, it seems like you're asking this [... SQL example...] What good is the second select if the we've already has already discarded the entity? I'm sorry but the SQL example and your question are unclear. Can you restate the question. We've *discarded* the entity? Which entity and when did we discard it? &gt; Is it because your less coupled repositories use to reusing the object returned? Maybe I'm tired but I really can't parse this sentence. Editing mix-up? And no this is not about reused objects. It's about atomic updates (which ensure consistency). I'm afraid we're not on the same page. Also your SQL example, I don't know quite how to read it, but it's not checking the version of the read data at all, so you're not demonstrating optimistic locking on the data you read. Which would again resulting in inconsistent data when writes depend on reads (which is most cases). &gt; The time it takes doesn't matter as much as the business logic. If the business wants to actually apply business rules to it, a workflow API may be a better fit. So you're saying, let's not use storage backend transactions, let's reimplement them manually in the application layer. And who's paying for maintaining all the extra code to implement this again? This is you switching again from "simpler is better" to "complex is better". Saving 500 lines of repository code doesn't mean it's ok to add 5,000 lines of manual coordination logic in the business layer, does it? &gt; In the situation you describe, the object depends on data outside of itself, so it can't be an aggregate root. Yet, even though repositories should only deal with aggregate roots, we're loading and saving the non-aggregate root anyways.. There's nothing in the domain working to ensure its consistency, so its no surprise consistency can be lost. So what are you saying here, I don't get it? You always go back to aggregate roots as the solution to all problems of consistency, now it turns out of objects interact, aggregate roots aren't solving anything. Back to square 1? &gt; I gave a workflow that described the calls a service would make on the aggregate roots. I'm a bit curious if, with rules like that, if ther would be much advantage to using the pattern you proposed. It's like asking "if I have a car, do I need wheels". Interactions between larger systems rely on smaller interactions in smaller systems. Just because A and B use messaging to coordinate through a state-machine protocol, doesn't mean that, individually, A and B can just forget about all consistency and make a mess of their own state. You need local transactions to ensure local consistency, and you need manual coordinated transactions (or eventual consistency) between larger services to ensure global consistency. Without local consistency there's also no global consistency, the latter is built upon the former. So my patterns don't just lose their qualities all of a sudden. &gt; If I've missed an important question or something I missed needs follow-up-- let me know. I'm not purposely trying to brush anything under the rug or declare it as someone else's problem. I think if you can tell me how'd you model the tree example above it'd go a long way towards us getting on the same page. The issue, I think, is that you are thinking about problems that can be solved purely through 100% isolation between pieces of state (aggregate roots). But the kind of problems I solve, and I think most PHP devs and devs in general, require pieces of state that interacts intelligently with one another. There isn't just "aggregate roots". There are aggregate roots, made of aggregate roots, made of aggregate roots, and so on, in a recursion, for as long as you're willing to go on. And at every level you need to ensure proper consistency. Imagining it all as a flat set of aggregates that don't talk to each other and can be persisted in a dumb keyval store, is not even starting to capture the complexity of real app object interactions.
I'm not familiar with its use so others might fill in the gaps for me here, but then you might be interested in [PHP-FPM (FastCGI Process Manager)](http://php-fpm.org/).
&gt; Again, those languages are designed differently than PHP. If PHP does it differently, why is that "wrong"? Because the explanation given is not "this is better for PHP" (it's really not), instead, the explanation given is "it's too hard to fix the parser rules to do it that way, so here's an easier way". It's not "wrong", it's just *lazy*. It's poor craftsmanship. 
&gt; But... why? You established that you think the current proposal stems from laziness, but that there are other reasons why it is not better for PHP. Why is it not better for PHP? You're asking me to prove a negative statement. That's not how logic works, but see the examples I added to my other comment to you.
OK, will do. &gt; You're asking me to prove a negative statement. That's not how logic works Don't assume that I'm not educated in the field of abstract mathematics. Let P = The given syntax is better for PHP. Then ¬P = The given syntax is not better for PHP. Then let Q = ¬P. Are you suggesting it is impossible to prove Q? A little off-topic. :)
Well, that was 60sec of my life wasted.
PHP Frameworks? For *free*? Well gosh, sign me up! Also, how is it possible to have 10 Best frameworks?
&gt; This seems like more of a documentation issue than anything else Why should have to spend time documenting something that should be standardized? It's hard enough creating documentation regarding your architecture, why throw in the need to document the mechanics of importing your classes as well? That makes no sense. &gt; the way the source code is organized shouldn't matter at all to you If you're talking about the importability of that code (the actual mechanics of it), then it doesn't matter when PSR-4 is used, which is why that convention is so important.
&gt; Also, how is it possible to have 10 Best frameworks? When they're all free, of course!
&gt; There is only one lambda calculus, and only one math. And PHP is neither of those things, though it implements a subset of arithmetic. &gt; In 2015, it's no longer acceptable to be ignorant about lambdas. Whose ignorant about lambdas? That seems pretty harsh, don't you think? P.S. that's called an *ad hominem*.
While he may kind of right be blunt case, Graham can be a little too brusque at times in how he handles issues with the projects he helps with.
Because at some point you have to be able to look at the source code - you said it yourself, there's really no avoiding it. If I use a library where the developer decided to put 60 classes in a single file, and classmap them all in a virtual namespace structure, then they've made everyone's lives harder. I know where to look for files when classes are PSR-4. I don't when you take it upon yourself to abuse class maps to fit whatever quirky source code structure you've invented for yourself.
Lumen by Laravel 
You won't find a reliable PHP framework for *just* writing a console script. Bring in composer's autoloader, set up a simple DI container like Pimple, and set up a PDO instance or an ORM yourself.
You can always use Laravel and just unregister the service providers for the components you don't want to use. Alternatively, the Lumen framework from Laravel is probably more what you're looking for. Curious what the concern is with this "bloat" though, especially if it's not web/user-facing. Sounds to me like some unnecessary optimization to me :P 
What sort of DB support, beyond doctrine do you need?
Maybe it's still more than you want, but take a look at Yii2. There is a tutorial specifically for [setting up a console app](https://github.com/yiisoft/yii2/blob/master/docs/guide/tutorial-console.md), and also [setting up a cron console app](http://www.yiiframework.com/wiki/646/how-to-implement-cron-in-yii-2/). Yii2 happily separates your frontend from backend from common from console. You can write a strictly console app, or you can tie it all together with a backend site and/or a frontend site and/or whatever else you want. I suggest using one of the advanced application templates; I personally like [nenad's advanced template](https://github.com/nenad-zivkovic/yii2-advanced-template), although the [standard one](https://github.com/yiisoft/yii2-app-advanced) works well too (I just prefer the directory structure in nenad). In regards to your other comment about "stuff you don't need", just don't use the stuff you don't need. You'll find Yii2 quite responsive. 
Well even to the end users, it may be important. We all know that PHP's objects instantiation has overhead, so perhaps another implementation of object in internal C source code could make this better. In programming languages such as Python and Ruby, everything is an object, even numbers and booleans. I am actually trying to create a scalar class extension in C using Nikita's scalar objects extension as dependency. I wonder, how do Python and Ruby optimize them internally in a way that creating a boolean or number will be fast enough. 'cause I know, if you wrap primitives in PHP classes, they will be much slower. 
Wow where to begin? Terribly outdated and full of security holes! 
I'd probably still suggest Laravel as one of the better PHP frameworks. You don't need to use any of the view processors if you don't want to, and they're there if you do ever want to (maybe such as viewing reports or making an admin section for yourself to manage things). Laravel 5 works really well with handling cron scripts. You'd only have to create one cron, and you'd define all your individual tasks right inside of Laravel's \App\Console\Kernel.php file. http://laravel.com/docs/5.1/scheduling
&gt; I am self taught ( online and book resources ) so i am pretty sure i am experiencing some form of imposter syndrome and its starting to bug the shit out of me. There is absolutely nothing wrong with being self-taught and anyone who tries to convince you different is either selling you something or trying to assure themselves that their money is well spent. What would you say if I told you that a person who found security-related defects (not necessarily vulnerabilities, but in many cases they were) in WordPress, Joomla, CodeIgniter, Kohana, Laravel, Symfony, CakePHP, the Facebook SDK, Yii 2.0, and AnchorCMS in the past two years had no formal PHP education? And despite being self-taught, they end up being roped into security discussions by industry professionals when no easy answer is available (i.e. it involves cryptography)? There's absolutely nothing special about this person, by the way. There's no reason to believe that everyone reading this comment couldn't do what he did, or even surpass him, should you decide to do so. And he encourages you to try, in your own way and on your own terms. In sum: Being self-taught is in no way a detriment, it just means you took a different path to arrive here than someone formally educated. Take the chance to exchange notes and learn about each other's perspectives. (If you do, the entire community becomes richer.)
Oh ok. In the service providers I wouldn't worry about that too much. It's not like you can take a service provider and throw it in another framework anyway. The argument "if you switch frameworks" is, IMO, in practise just a tool for writing better decoupled code. The actual switch rarely happens.
Relevant: https://youtu.be/TdEVaOjL20s?t=49m26s
My goto for putting together console apps quickly was always [Cilex](https://github.com/Cilex/Cilex). However, I also don't really see anything wrong with starting with full-stack symfony and just not using / removing the web stuff. It'll still be useful to have the DI container and bundle structure even if you only have console commands to run. As far as db stuff goes, I guess it depends on what kind of work you're doing with it, and whether using an ORM (Doctrine) makes sense or not. Either way, your db adapter choice shouldn't (in theory at least) tie you to a framework.
I personally find Assetic a pain in the arse to work with. I use it because its the defacto standard but to be honest, most of the time I have no clue how it wants me to operate it and the caching is just driving me nuts. What are you guys using instead of Assetic for Symfony2/Twig projects? And how is your typical work flow? 
You might want to start with something like https://puphpet.com/ or http://phansible.com/ as they will setup both local and production environments to be the same. Having consistency will be a major improvement from the sounds of your workplace As you gain experience you can customize or DIY with Ansible/Puppet/Chef/Salt whatever you prefer. I like Ansible myself There is no modern server oriented linux out that comes with a bunch of services enabled that are going to impact performance, especially for some small time stuff. I wouldn't worry. As far as fine tuning... this is always app dependent. Also you cannot fine tune unless you have the ability to measure your adjustments. Monitor/log and focus on what is slowest. You can start with something easy like New Relic http://newrelic.com/ https://www.nginx.com/blog/tuning-nginx/ I'd leave your workers to auto as suggested PHP settings you can start with the production settings as suggested by PHP https://github.com/php/php-src/blob/master/php.ini-production https://www.percona.com/blog/2014/01/28/10-mysql-settings-to-tune-after-installation/ while it is hard to go wrong with Percona information, don't just blindly follow before understanding what and why you are doing something. Measure before and after under both expected load and above what you expect for load As far as coding practices... strive for this http://12factor.net/
It's almost like he's a kid with a massive ego and an obvious inability to work in a team? Oh wait... Laravel's "open source face" is poisonous. There's very little motivation to contribute - you'll be treated like a pleb.
BTW I've read some of that Vernon paper you linked. Some quotes are included. Your thesis is that all consistency issues can be solved by combining the objects needed for a business rule in one aggregate. Vernon: &gt; As a result [of these consistency rules], Product was first modeled as a very large aggregate. The root object, Product, held all Backlog Item, all Release, and all Sprint instances associated with it. The interface design protected all parts from inadvertent client removal. However, he says... &gt; The big aggregate looked attractive, but it wasn't truly practical. Once the application was running in its intended multi-user environment it began to regularly experience transactional failures. So he then recommends breaking the objects into several smaller aggregates, despite the business rules depend on *several aggregates being changed in sync*. What about the business rules? Says Vernon: &gt; These redesigned methods have a [CQS] query contract, and act as factories. That is, they each respectively create a new aggregate instance and return a reference to it. Now when a client wants to plan a backlog item, the transactional application service must do the following And he then provides an example of a service which interacts with the *repository directly* in order to instantiate entities *across aggregates* in a *transactional method*. It acts as a factory, and this is then delegated to the repository as the data layer factory. This is starting to look less and less like your recommendations of sticking to one aggregate. It also involves the *repository* in the enforcement of a business invariant, because the repositories now run in a shared transaction. Says Vernon about large aggregates: &gt; What additional cost would there be in keeping the large cluster aggregate? The problem is that it could actually grow out of control. [...] Even if we guarantee that every transaction would succeed, we still limit performance and scalability. [...] Performance and scalability are nonfunctional requirements that cannot be ignored. &gt; This large cluster aggregate will never perform or scale well. It is more likely to become a nightmare leading only to failure. It was deficient from the start because the false invariants and a desire for compositional convenience drove the design, to the detriment of transactional success, performance, and scalability I guess putting everything into a few big aggregate roots can't be our golden hammer for solving consistency. Part 2 of the paper goes directly into eventual consistency and transactional consistency in the layers above and it quickly starts looking like my approach. It says this curious bit too: &gt; And experienced DDD practitioner may at times decide to persist changes to multiple aggregate instances in a single transaction, but only with good reason. What might some reasons be? [...] &gt; Reason One: User Interface Convenience &gt; Reason Two: Lack of Technical Mechanisms &gt; Reason Three: Global Transactions &gt; Reason Four: Query Performance Reading through those I can see the author had to list those because he's in tune with real-world apps where naive DDD is not applicable. But this is where we start to diverge, because while like him I prefer small aggregates, I don't necessarily see multi-aggregate transactions as "breaking the rules". Here's what I'd posit: **Any set of rules for system design which don't include a recursive definition is incomplete.** You can't just have a flat list of aggregates. You have aggregates made of aggregates. This allows you to have your small atomic ones, and the bigger ones which have granular cross-aggregate transactions. This means: - Every aggregate also can be seen as a domain, and can have its own smaller aggregates. - Every domain can be seen as an aggregate of a larger domain. Without knowing how DDD defines itself recursively, we can't discuss what it means to break the rules here. From the smallest processor unit in your CPU, to cross-datacenter interactions, system interactions largely form the same patterns and exhibit the same phenomenons over and over, but simply at a different physical and time scale. It's senseless &amp; short-sighted to take one level out of this fractal and say "this is the place for DDD" and claim the layer below it and the layer above it aren't.
What are you referring to? I haven't heard that claim. 
Some great tips, thanks! I've known some of these but it's nice when someone puts them in one picture.
I avoid Assetic entirely and use a gulp to do my asset management. I'm using SystemJS to load ES6 classes / modules and I then have one entry point in the head of the HTML. The only thing that changes per environment is where and how my JS is loaded, for example. 1. In development I load the un-minified files the standard way. 2. In production I load the filename from a manifest.json file ``` { 'main.js': 'main-123456.js' } ``` This is because when I run the production gulp task it combines all the files and generates a unique file name. The mapping is so I can reference it within my templates without knowing the revision. The advantage of this is that I can build and upload my assets to a CDN before I do a site deploy. Then if I want to bump my assets I just commit the new manifest file. 
&gt; The repository is an interface (or a set of interfaces) which is modeled after the business layer that is using it. I don't think a SPI can be properly classified as coupling. It's like saying a component is coupled to itself. Its coupled to an interface that's expected to act like itself. The __construct() hints towards it a bit, the domain methods only seem to emphasize it. And its not just coupling-- its fairly common to couple with the behavior of ValueObjects and other Entities, but that's usually worth the simpler, reusable logic embedded within those value objects-- so coupling is there, just justified. &gt; &gt; If you think I'm doing it for limited gains, let's see what I gain through *examples*. Say, my task is to model a giant tree of data. The user can move/delete/insert subtrees under another node, sort and re-order children under a parent, easily fetch ancestors and descendants. &gt; &gt; I'm curious how would the repository for *this* look in your case? Just don't forget: you can't load the entire tree in the PHP process, it's too big. &gt; &gt; You've said several times that consistency is about splitting things in small aggregates and operating on them atomically. It's proposed as a silver bullet solution, so I'm curious what the small aggregate is *here* that would fix this problem. &gt; &gt; Well I'd be curious to see you tackle the tree example with that strategy. The way I see it, we're not even on the same page, because the problem I'm trying to describe is capturing *relationships between entities and values* and the examples you give can only capture isolated logic in isolated objects. Depends on the domain. It doesn't sound like there are any rules that make one entity depend on another. It'd probably be a lot like the [model described here](http://programmers.stackexchange.com/questions/250073/how-to-work-with-large-aggregate-roots). With some additional mechanics for changing parents. startParentChange, completeParentChange, cancelParentChange-- although its very possible that can be skipped if we assume the aggregates are on the same system. &gt; You mentioned messaging in at some point. Messaging also requires protocol guarantees like guaranteed delivery and idempotency, or you end up up with inconsistent state. And even with those guarantees, you still need to implement coordination at the application level, which as you said is way more complicated, than using local transactions when they suffice. Messaging can't guarantee consistency, only eventual consistency. &gt; I like the simplest solution that works. With you... I'm on the fence. You keep switching between "repositories should be simple" and "lets reinvent the wheel by reimplementing transaction mechanics in the business layer through workflow engines". &gt; &gt; . . . &gt; &gt; So you're saying, let's not use storage backend transactions, let's reimplement them manually in the application layer. &gt; &gt; And who's paying for maintaining all the extra code to implement this again? &gt; &gt; This is you switching again from "simpler is better" to "complex is better". Saving 500 lines of repository code doesn't mean it's ok to add 5,000 lines of manual coordination logic in the business layer, does it? The transaction mechanics weren't simply reimplemented to reinvent the wheel. They're there to resolve business rules-- some of which may require a human present, and some of which span more than a single database transaction. (There's also an assumption you're making on the database: that all of the aggregates are on one system, the possibility of multiple systems interacting is why aggregates don't directly operate on one another in one transaction). &gt; There's a place for custom coordination through messaging and inherently local updates is not them. &gt; &gt; Often I don't even have to rewrite the repository as a mock, I just need to inject the existing repository with a mock data structure repository. &gt; &gt; This is because the repository is often a simple facade for a basic comp-sci data structure or a pattern, you know, the way "models" in MVC and so on were originally intended to be designed. &gt; &gt; So if I do new BalanceRepo($mysqlEventSourceLog) I have a "real" repository. &gt; &gt; But if I do new BalanceRepo($mockEventSourceLog) I have a "mock" repository. &gt; &gt; It took me 0 lines of code. You mean, asides from writing your MockEventStore? Conceptially, having an EventSourcedAggregateRoot interface to define how the repository can expect the aggregate to behave seems much more clear (and enforced by the type system) while also being reusable. (Event store's seem like an easy example, as the reusability of repositories is one of the very nice advantages). &gt; &gt; All, right, let's put this to the test with a basic 101 examples, shall we? Here's a business logic invariant (I already mentioned this one): &gt; &gt; - No two users on the system can have the same email address. &gt; &gt; Now show me how an entity enforces this and how you unit test for it without a repository (mock or not). I'm honest when I say, if you can show me how to do this, it'd be opening a new universe of possibilities for me. So looking forward to your answer. An entity enforcing a constraint outside of itself? That's not the job for an entity, but this is a fairly common subject. [Greg Young](http://codebetter.com/gregyoung/2010/08/12/eventual-consistency-and-set-validation/) covers it a bit. The gist is: Usually, its a false invariant that can be resolved well enough using eventual consistency-- and the few cases where it does happen can be dealt with using other business logic. There are cases where the uniqueness is not a false invariant. The solution depends on the exact circumstances. It may use the repository in a service (The read-what-was-written check makes more sense in that circumstance). It may use an additional aggregate that uses a generic collection interface (which may be replaced by an optimize one optimized for the repository). &gt; You're saying that when you write to an aggregate, you base this only on choices that you've read from the aggregate and nowhere else. &gt; &gt; I honestly seriously doubt you'd go far with this approach. So "aggregate design" isn't really answering my question. &gt; &gt; In the context of an aggregate root, it seems like you're asking this [... SQL example...] What good is the second select if the we've already has already discarded the entity? &gt; &gt; I'm sorry but the SQL example and your question are unclear. Can you restate the question. We've *discarded* the entity? Which entity and when did we discard it? &gt; &gt; Is it because your less coupled repositories use to reusing the object returned? &gt; &gt; Maybe I'm tired but I really can't parse this sentence. Editing mix-up? &gt; &gt; And no this is not about reused objects. It's about atomic updates (which ensure consistency). I'm afraid we're not on the same page. &gt; &gt; Also your SQL example, I don't know quite how to read it, but it's not checking the version of the read data at all, so you're not demonstrating optimistic locking on the data you read. &gt; &gt; Which would again resulting in inconsistent data when writes depend on reads (which is most cases). The SQL was just a summarization of the SQL sent to the databaes from the repository. The `UPDATE SET ..., version = :newVersion WHERE ... version = :version` checks to ensure we only update the same version of the entity (although repository should probably roll-back if the affects rows isn't what it expected). I still have a bit more to add, but your second reply seems relevant to all of this. &gt; So what are you saying here, I don't get it? You always go back to aggregate roots as the solution to all problems of consistency, now it turns out of objects interact, aggregate roots aren't solving anything. &gt; Back to square 1? &gt; It's like asking "if I have a car, do I need wheels". Interactions between larger systems rely on smaller interactions in smaller systems. &gt; &gt; Just because A and B use messaging to coordinate through a state-machine protocol, doesn't mean that, individually, A and B can just forget about all consistency and make a mess of their own state. &gt; &gt; You need local transactions to ensure local consistency, and you need manual coordinated transactions (or eventual consistency) between larger services to ensure global consistency. &gt; &gt; Without local consistency there's also no global consistency, the latter is built upon the former. So my patterns don't just lose their qualities all of a sudden. Which qualities? The one I was talking about was the selector-based operations. Its powerful, optimized, and useful: but that one seems to go away when transforms are conditional. The ability to reuse repositories? Making the entity implement an interface, say an EventSourcedAggregateRoot interface, seems just (if not more) reusable and more easily tested. &gt; The issue, I think, is that you are thinking about problems that can be solved purely through 100% isolation between pieces of state (aggregate roots). But the kind of problems I solve, and I think most PHP devs and devs in general, require pieces of state that interacts intelligently with one another. &gt; &gt; There isn't just "aggregate roots". There are aggregate roots, made of aggregate roots, made of aggregate roots, and so on, in a recursion, for as long as you're willing to go on. &gt; &gt; And at every level you need to ensure proper consistency. Imagining it all as a flat set of aggregates that don't talk to each other and can be persisted in a dumb keyval store, is not even starting to capture the complexity of real app object interactions. &gt; I'm starting to suspect what you're calling entities are what I call services-- which may a few things. I plan on replying to your second reply to expand on that a bit.
[सिम्फोनी](https://symfony.com/) का भी प्रयोग करके देखिये । ----------- Do try [Symfony](https://symfony.com/) as well.
Personally I'd still use Laravel or Lumen. Especially with the new version of PHP being faster and the fact that the speed of your PHP will probably not be your limiting factor. 
&gt; Depends on the domain. It doesn't sound like there are any rules that make one entity depend on another. It'd probably be a lot like the model described here. With some additional mechanics for changing parents. startParentChange, completeParentChange, cancelParentChange-- although its very possible that can be skipped if we assume the aggregates are on the same system. This doesn't really answer my question though - how would that tree work. I gave you the domain, give me one possible rendition. &gt; startParentChange, completeParentChange, cancelParentChange Well congrats you just turned one domain mutation event into three, and probably quintupled the amount of code needed to implement it. I'd just have a tree repository and not implement this in my business layer. A tree is a data structure, and the repository is the one that should take care of how that is implemented. A business layer should only care about additional rules and logic added on top of a generic tree. &gt; although its very possible that can be skipped if we assume the aggregates are on the same system. Not according to the rules you yourself noted for aggregates - same system or not same system, consistency between them shouldn't be enforced, right? So is it on the same system is entirely irrelevant here. You're suggesting we treat the entire tree as one aggregate, but you can't do this, because the problem definition states the tree is too big to load in RAM at once. So... how would you design it? &gt; Messaging can't guarantee consistency, only eventual consistency. Eventual consistency is a subset of consistency, you mean to differentiate "strict" from "eventual" consistency. But that's also not correct to say, because messaging can model both kinds of consistency. The communication between your CPU, CPU caches, memory controller and RAM is done through messaging. The communication with I/O deviced like a hard drive is done through messaging. Everything is done through messaging if you look close enough. &gt; You mean, asides from writing your MockEventStore? EventLog is a reusable store interface with many implementations. I don't rewrite it for every repository, I reuse it across projects. Basic object reuse. &gt; Conceptially, having an EventSourcedAggregateRoot interface to define how the repository can expect the aggregate to behave seems much more clear (and enforced by the type system) while also being reusable. (Event store's seem like an easy example, as the reusability of repositories is one of the very nice advantages). This is too vague for me to discern what you added here. Technically to an event log, the event payload is entirely irrelevant. So what would that interface even have on it? &gt; An entity enforcing a constraint outside of itself? That's not the job for an entity, but this is a fairly common subject. Greg Young covers it a bit. The gist is: Usually, its a false invariant that can be resolved well enough using eventual consistency-- and the few cases where it does happen can be dealt with using other business logic. This is at least the fourth time where you answer my questions with the following attitude: - *"Well I can't solve your problem with what I know, so therefore your problem is not a problem."* I'm afraid going to your boss and saying "this is a false invariant" won't get you a pat on the back. Regarding eventual consistency. Telling them "this will take several days and several thousand lines of code to model this as a state engine and complicating our business process by having to involve an admin to resolve conflicts" as Greg proposes also won't get you a pat on the back. I'm afraid DDD is failing you quite hard here. We added code to maintain, we complicated process and we added untold amount of edge cases to the system that we need to handle. Edge cases like: - If the repository has "getUserByEmail" and there are two users with the same email, *which users should it return now* if the interface specifies 0 or 1 results? - If I reset my password by email, *which account did I just reset* if I have a conflict? Screw consistency and API contracts, huh? YOLO! &gt; There are cases where the uniqueness is not a false invariant. The solution depends on the exact circumstances. It may use the repository in a service (The read-what-was-written check makes more sense in that circumstance). It may use an additional aggregate that uses a generic collection interface (which may be replaced by an optimize one optimized for the repository). I'm asking you to give me one specific rendition which is basic and satisfies the problem. Using a service is a cop-out: it means your entities can't enforce this invariant. What if I don't know I have to go through a service for this? - You'll have to enforce that I use the service. - And if you enforce the service, it becomes a factory. - And if it's an entity factory, it's in fact acting as your repository. So technically your answer is "enforce this particular invariant in the repository". The very thing I started with. Do you see what I mean? &gt; The SQL was just a summarization of the SQL sent to the databaes from the repository. I prefer we stick to PHP because when you mix SQL in this is gets too hard to discern if you rely on a specific transaction isolation level, what you read back PHP and so on. For example your SQL made no sense up there, you did a select after you committed. It's too late to revert after you commit, and just reading the data wouldn't accomplish anything, anyway, you need to version the read data as well and check THAT, if you want your optimistic locking scheme to work. Please stick to PHP and say what it does and why. &gt; I'm starting to suspect what you're calling entities are what I call services-- which may a few things. I plan on replying to your second reply to expand on that a bit. I have orchestration services on top of these for specific use cases, but I'm curious if we call it all "services" what would change. I'm simply materializing entities as a collection, rather than as separate objects carrying business state. Nothing else is different on the outside.
I can't really answer your question about aliases, as I don't know enough about them to be able to comment. Regards annotations, which I see have been raised in your previous threads, I am completely against them, as amongst other things for the fact that they are a hack applied on top of the language. I'd avoid them like the plague, even though some people far cleverer than me have made a lot of decent PHP software with them. In answer to your side tangent: be anal and take your time and try to TDD as much as you can. This way you do it properly, you can refactor easily to good design as you discover your design and you can speed up your development as your project gets larger. I'm like you, in being self-taught, but having done it badly for 15 years, mainly stuck maintaining terribly coded systems, I decided to learn how to do it properly. It's been a few years learning and now I am only really starting to reap the rewards of this. I have found that without TDD, I am not able to do good design. Doing TDD right is hard. Knowing what to write is difficult. Making sure you use it to discover and drive out your design feels counter-intuitive at first, but the rewards are great. I now write quite good code, not the rubbish which I did before (that I thought was good back then). I still hack code around on my old systems that have no test suites and guess what, it comes back to bite me somewhere down the line. I just wish I could easily apply tests after the fact, but the reality is on legacy systems, this is prohibitively expensive, so make sure that the code you write today has tests built in from the beginning, as in a few years this will be legacy code for you to maintain.
Why not change your hosting platform to something which will accommodate a framework. You can find decent hosting from about $10 a month.
&gt; What should happen when a user attempts to register with an email that's already in the system? Send an email notification letting them know? That's a really bad way to handle it, as the email is a key asset in a user account that's used for authorization purposes (like password reset). More telling than arguing about the abstract merit of all this is that *I can't think of one system that does this*. Everything, from the smallest web forum up to iCloud, Amazon accounts and what not, they all ensure this invariant strongly. So if you'll push this as an accepted industry solution, I'm afraid you fail the empirical test. &gt; Really? I don't think the Product aggregate actually had changes that needed persisting, so while two aggregates were used, only one was persisted. Service code from part 1 included below: The code clearly shows the service interacting directly with two repositories, which is *two more* than you claimed is necessary for enforcing all business invariants in the system (zero). The amount of persisted entities is irrelevant, BTW, because both reads and writes are critical when ensuring consistency. &gt; that'd probably a good place to add eventual consistency and domain messaging. I honestly don't understand why you skip between two extremes. It's either one-bit optimistic locking, or full-blown eventual consistency. When you choose where to have a dinner out, are the only two choices McDonald's and flying to Japan to try Jiro's sushi? Why do you fail to acknowledge all the viable choices in-between that 90% of the industry consists of? &gt; If you can assume that everything is in one database, that's fine. But the general rule is to assume that different aggregates may be on completely different systems. The number of databases and systems is *entirely irrelevant* about my point here. The repository chooses how many machines it's spread on, the only thing the repository user has to do is use it according to the repository contract. &gt; Well, one of the things I like about DDD is that even if I may not like your particular implementation/design, it may still be DDD. For all I know it'll catch on and become a standard practice. I'm only really seeing a fraction of a bigger picture. Aspects of my approach has a name (in the rough sense), it's called Data-Oriented Design (DOD). One letter, so much difference, I guess. &gt; For some reason, this reply highlighted what should've been obvious to me. I use repositories in services, the services are the ones that are dependent on the repository. A lot of my concerns simply evaporate when I apply it to services instead. Rather than it be an Employee, its an EmployeeService of EmployeeServiceFacade. Repositories are already injected into services to test them. Entities/aggregate roots only act on themselves, but services have an understanding that there's multiple aggregates. They're often the way to interact with different aggregates at once, and its also common for them to have an understanding of transactions/unit of works. Yup. &gt; There wlll be some who have the concerns with assuming all of the aggregates are in one database (or can otherwise all be acted on in a transactional manner)-- so you may want to consider how to handle those failures. There's no such assumption. I feel the number of databases, or even if there's a database is an entirely separate concern here. &gt; I'd still suggest you work to keep aggregates small, and consider using eventual consistency where it makes sense. (It almost seems to make more sense to be used to update a read model in response to an event) The whole point of using collection semantics for exposing the entities is to allow for small aggregates (without incurring N+1 type costs for interacting with the system). A collection of entities is... a collection of aggregate instances of the same type. That's all they are. Another collection of entities in the same domain has a completely independent storage.
I quit using Eclipse and Netbeans years ago for tools from JetBrains. Is there anyone who has used modern versions of all three (Eclipse, Netbeans, PhpStorm) that can give a recap for what the scene is like for PHP specifically?
&gt; This doesn't really answer my question though - how would that tree work. I gave you the domain, give me one possible rendition. It does, Replace Directory and File with Tree, you have a `Tree` aggregate root with a reference to `TreeId`s. Or are you implying that one tree's ids can't possibly fit its direct descendants into memory as well (your tree has over 16 million direct children?) Then we're back to looking at a domain-specific collection interface, or, if you prefer, a domain-specific tree interface. &gt; Well congrats you just turned one domain mutation event into three, and probably quintupled the amount of code needed to implement it. One property and three one-line (four if you include the declaration and the opening/closing braces) methods has quintupled the amount of code? &gt; I'd just have a tree repository and not implement this in my business layer. A tree is a data structure, and the repository is the one that should take care of how that is implemented. Almost like a reusable tree interface, rather than a repository? &gt; Not according to the rules you yourself noted for aggregates - same system or not same system, consistency between them shouldn't be enforced, right? Consistency between them can not be guaranteeds, you can only realistically operate on one repository at a time. If you ever tried to implement multi-database repositories, you realize that its difficult (if not impossible) to ensure multi-database consistency. Commit 1 succeeds, Commit 2 succeeds, now Commit 3 has failed-- but those other two database systems could have performed operations assuming that your commit was valid-- so it can't reasonably roll them back-- if it tries, you likely have an inconsistent system. &gt; So is it on the same system is entirely irrelevant here. You're suggesting we treat the entire tree as one aggregate, but you can't do this, because the problem definition states the tree is too big to load in RAM at once. &gt; &gt; So... how would you design it? I did not suggest the entire tree as one aggregate. I only suggested each parent aggregate know the ids and their position. I didn't even suggest that they were loaded all at the same time, the closest I did was suggest that there was a Collection interface to abstract it (similar to the one that Doctrine provides-- where it can be an ArrayCollection or a LazyLoaded collection). The CollectionInterface, though, would have the methods the aggregate desired to use for operations that can't be loaded all at once. &gt; This is at least the fourth time where you answer my questions with the following attitude: I linked a resource that explained it far better than I could-- including the fact that some domains require it. I also went a little further to elaborate on something I think the resource brushed over: what to do when uniqueness truely is important to the domain. (It only covered it one paragraph). &gt; &gt; There are cases where the uniqueness is not a false invariant. The solution depends on the exact circumstances. It may use the repository in a service (The read-what-was-written check makes more sense in that circumstance). It may use an additional aggregate that uses a generic collection interface (which may be replaced by an optimize one optimized for the repository). Its not just the constraint, not what the business rules regarding the constraint are. Some missing answers: * What should happen when a user tries to change their email to one that doesn't exist? Do we just tell them to that its impossible to use an email they can verify is theirs? * If the system is under high load, what's better: disallow any registration or accidentally register the same email twice and apologize? &gt; EventLog is a reusable store interface with many implementations. I don't rewrite it for every repository, I reuse it across projects. Basic object reuse. &gt; his is too vague for me to discern what you added here. Technically to an event log, the event payload is entirely irrelevant. So what would that interface even have on it? [GitHub](https://github.com/search?utf8=%E2%9C%93&amp;q=EventSourced+Interface&amp;type=Code&amp;ref=searchresults) may answer that question better than I can. If you want a more specific implementation, Broadway has one, as does prooph-- I'm not the biggest fan of either. &gt; This is at least the fourth time where you answer my questions with the following attitude: &gt; &gt; "Well I can't solve your problem with what I know, so therefore your problem is not a problem." &gt; &gt; I'm afraid going to your boss and saying "this is a false invariant" won't get you a pat on the back. You don't go back and say, "this is a false invariant.". You go back and ask more questions (see above). &gt; Regarding eventual consistency. Telling them "this will take several days and several thousand lines of code to model this as a state engine and complicating our business process by having to involve an admin to resolve conflicts" as Greg proposes also won't get you a pat on the back. Some companies have support agents that would handle that, rather than an admin. They can also be handled in an automated fashion. If there's no concept of eventual consistency in the system at this point-- might it be worth it to start thinking about it. You don't *have* to go to eventually consistent, but when you define this as a hard rule-- there are associated costs with that. Whenever any email is added to the system, any operations that add/change an email in the system have to assume they are no longer valid and begin again (or possibly put the system into an inconsistent state). Enforcing it in the repository simplifies it a bit, but... &gt; Edge cases like: &gt; If the repository has "getUserByEmail" and there are two users with the same email, which users should it return now if the interface specifies 0 or 1 results? &gt; If I reset my password by email, which account did I just reset if I have a conflict? I'll get more to this later, either in this reply or the next. &gt; I'm asking you to give me one specific rendition which is basic and satisfies the problem. Using a service is a cop-out: it means your entities can't enforce this invariant. What if I don't know I have to go through a service for this? I think I addressed this a bit in my other reply-- where your entities seem to be more like what I consider services than entities. To me it doesn't make sense for an individual entity to try to enforce a constraint that exceeds its scope.That being said, you could have an entity who's scope is the *everything*, theoretically. And maybe that's what your implementation leads towards. &gt; I'm asking you to give me one specific rendition which is basic and satisfies the problem. Using a service is a cop-out: it means your entities can't enforce this invariant. What if I don't know I have to go through a service for this? * You'll have to enforce that I use the service. * And if you enforce the service, it becomes a factory. * And if it's an entity factory, it's in fact acting as your repository. The first one, sure-- but using services in this way is fairly common. Regarding #2 and #3) Repositories aren't factories. There's various conflicting views on where the creation of the entity should happen, but I've usually seen it either a service or another aggregate root. &gt; For example your SQL made no sense up there, you did a select after you committed. It's too late to revert after you commit, and just reading the data wouldn't accomplish anything, anyway, you need to version the read data as well and check THAT, if you want your optimistic locking scheme to work. I know it didn't make sense, but that seems to be what the question was (emphasis mine). &gt; If your writes are deemed dependent on your reads (which may change), do you version check your READS as well **AFTER THE WRITE**? What read data? The aggregate only operated on values it received from itself and from parameters passed into the method (which are coming from the user). 
&gt; That's a really bad way to handle it, as the email is a key asset in a user account that's used for authorization purposes (like password reset). Really? You probably shoulndn't be using the email for authorization until its verfied. If two accounts have verified the same email, for some odd reason, is there any reason to assume that one shouldn't be able to reset the password for the other? &gt; More telling than arguing about the abstract merit of all this is that I can't think of one system that does this. &gt; &gt; Everything, from the smallest web forum up to iCloud, Amazon accounts and what not, they all ensure this invariant strongly. &gt; &gt; So if you'll push this as an accepted industry solution, I'm afraid you fail the empirical test. Its actually not as so clear-cut as it appears. For example, Amazon allows multiple emails to exist within their system. Someone pointed it out years ago: they were trying to merge their accounts and were complaining it wouldn't work. I thought that their system would disallow them from changing them to the same email, but surely enough, [Amazon allows multiple accounts to have the same email address](http://www.experimentgarden.com/2009/11/why-does-amazoncom-allows-multiple.html). They changed their account to the same email, and it continued to allow both accounts to log in. I'm assuming in most cases, the second account can simply be ignored-- and their support can handle the few cases its confusing or prevents a user from changing a password. One of the edge cases you mentioned in your other reply is relevant here, one isn't: * If there is even a concept of a repository in their code, it probably doesn't restrict the findByEmail() to return one account; rather, the login service uses a getAccountForLogin(email, password) and a isEmailRegistered(email). * Amazon does allow for password reset, so if they have anything like a getAccountForReset(email), it must choose one of the accounts. I can only assume they chose the most-recently-active one-- and let their support handle the cases in which the user notices and complains. &gt; There's no such assumption. I feel the number of databases, or even if there's a database is an entirely separate concern here. I mentioned this a bit in the last post. Its very hard to implement consistency between multiple disconnected databases. (Say, where two commits succeed, and one commit fails). Still, be aware that assuming the repository can update all of the aggregate roots in a transaction manner is one that one with known scalability/flexibility issues. &gt; The whole point of using collection semantics for exposing the entities is to allow for small aggregates (without incurring N+1 type costs for interacting with the system). &gt; Another collection of entities in the same domain has a completely independent storage. Yep. I think I mentioned it before, the pattern you proposed does seem to be something that's missing. That is, something that can be useful, but doesn't really have a library nor is something taught. There may be glimpses of it in various spots, but no where near the same level of API simplicity and power. I'm still not entirely sure any of the names we've used accurately describe it. It definitely has a good place to be used, perhaps even in the domain layer. As you mentioned, its very much like a collection (which is also a sort of aggregate root that the repository ensures can be kept consistent). I could definitely see a service requesting this sort of object from the repository. I'm almost tempted to call to try calling it a Collection, but I'd be more certain if I found a good place to try using it. At this point, the remaining conflicts seem to be small preferences. (system-wide consistencies handled in Services vs Entities, Transactions declaration in the domain vs in the infrastructure, and auto vs explicitly committed entities). Does seem about right, to you? 
Laravel 5.1's authentication and authorization support is incredibly easy but it might take you a little while to wrap your head around Laravel as a whole.
If you have zero control over the production server, you can use Codeigniter+Ion Auth,... and also other options.
Note this is the *third* time OP has posted the same general question about Laravel aliases. Previous questions: * [Dependency Injection Container Using Aliases And Caching](https://www.reddit.com/r/PHP/comments/3l4x1d/dependency_injection_container_using_aliases_and/) * [Understanding The Correct And Incorrect Way To Use Aliases](https://www.reddit.com/r/PHP/comments/3mmkta/understanding_the_correct_and_incorrect_way_to/) No doubt OP will be back *again* in a few days to ask it again, hoping someone will give them the answer they want instead of actual, useful feedback. OP, you've been given tons of advice over the past two weeks. Instead of trolling /r/PHP, learn from it. Or if just fuck off to /r/phphelp if you can't be bothered.
Sorry, why am I being downvoted? He doesn't _have_ to use Laravel, I was just letting him know how to use it for his specific needs. I didn't realize my comment didn't _contribute to any discussion_.
&gt; &gt; If your writes are deemed dependent on your reads (which may change), do you version check your READS as well AFTER THE WRITE? &gt; What read data? The aggregate only operated on values it received from itself and from parameters passed into the method (which are coming from the user). I'll use an example from the Vernon paper. If we have A, B, C and we need to ensure C = A + B, then, if you rely entirely on optimistic locking you need to do something like this (pseudo code, don't mind the syntax): BEGIN SELECT value, ver FROM things WHERE id IN ('a', 'b'); // We store rows in $a and $b UPDATE things SET value = $a['value'] + $b['value'] WHERE id = 'c' SELECT COUNT(*) FROM things WHERE (id = 'a' AND ver = $a['ver']) OR (id = 'b' AND ver = $b['ver']) if ($count != 2) ROLL BACK; else COMMIT; This is an example where you need to version check the *reads* to enforce the invariant. In many real-world cases you need to version check *both the reads and the writes* to ensure consistency. Checking just the writes doesn't solve the problem, as you can still get inconsistent C.
&gt; I think I addressed this a bit in my other reply-- where your entities seem to be more like what I consider services than entities. To me it doesn't make sense for an individual entity to try to enforce a constraint that exceeds its scope. My entities as exposed as a collection. So it's never an "individual entity". So that conclusion seems disconnected from the premise. The scope is a collection of same-typed entities (users, orders, checkouts, etc.). Nothing more, nothing less. &gt; That being said, you could have an entity who's scope is the everything, theoretically. And maybe that's what your implementation leads towards. Nope, there is no such danger. It's just a collection of same-typed entities. That's all.
So? Using old mysql functions, no sound, etc. Useless.
&gt; I mentioned this a bit in the last post. Its very hard to implement consistency between multiple disconnected databases. (Say, where two commits succeed, and one commit fails). Still, be aware that assuming the repository can update all of the aggregate roots in a transaction manner is one that one with known scalability/flexibility issues. We talk about workflow state machines and eventual consistency and yet say it's very hard to implement consistency across databases... something doesn't compute to be honest. Is application-level coordination the best go-to solution we should jump to, or not, because it's too hard? Can't be both ways. In any case, yes, it is hard, this is why it's best not to start with it in a place that'll never need it. But then you also need to ensure your business domain is stable, right? You need to model the domain in a way that can scale to bigger more complex architecture over time. How about... we centralize all I/O logic in a class and we call it "the repository". Then the business layer can do I/O with it through a generic interface which manages I/O in a consistent way to the domain, but also abstract way, so the repository can go from single-thread SQLite to MVCC PgSQL to a distributed, sharded event-sourced set of read/write models. And the business layer remains the same! Fancy idea, right. :-)
get this shit off of here before people think this is the best way to connect to a DB. 
I didn't realize DDD recommends value objects so big, they literally can't fit in memory. :-)
 &gt; I'll use an example from the Vernon paper. If we have A, B, C and we need to ensure C = A + B, then, if you rely entirely on optimistic locking you need to do something like this (pseudo code, don't mind the syntax): &gt; BEGIN &gt; SELECT value, ver FROM things WHERE id IN ('a', 'b'); // We store rows in $a and $b &gt; UPDATE things SET value = $a['value'] + $b['value'] WHERE id = 'c' &gt; SELECT COUNT(*) FROM things WHERE (id = 'a' AND ver = $a['ver']) OR (id = 'b' AND ver = $b['ver']) &gt; if ($count != 2) ROLL BACK; else COMMIT; &gt; &gt; This is an example where you need to version check the *reads* to enforce the invariant. In many real-world cases you need to version check *both the reads and the writes* to ensure consistency. The values may still have changed between your second read and your commit-- and the things that changed those values may have operated without knowing about your uncommitted data. When operating on multiple rows like that, only the database can really guarantee the data is consistent. One way is to use some sort of read lock (e.g SELECT ... IN SHARED MODE or a setting the transaction mode to * serializable*). That'll ensure the values aren't updated before you commit. That locking comes with its own advantages and disadvantages. 
&gt; The problem as stated wasn't how we'll call the poor human that will have to deal with it, but the fact we saddled a human to do a computer's job. &gt; &gt; Never make a computer do a human's job and never make a human do a computer's job. They have different strengths which are entirely complementary. &gt; &gt; If one's domain modeling technique doesn't allow you to do that, it suggests a flaw in the modeling technique or author's understanding of it. Its not that the domain model doesn't allow for it, its just that its a common situation where its not really what the business wants. Usually a business would rather accept the rare non-unique registration under high load than having having all of the registrations locked because of an unexpected spike in traffic.
&gt; So the problem remains: how do you ensure that two accounts with the same email never have the same password. They tell you that you can't use that password, but they don't tell you why. A compenent user can then log out, then log into the other account and change *that* password. They already own the email, so its doesn't seem much different than sending a password reset email. The email owner may have changed, but that's what grants account access.
&gt; I didn't realize DDD recommends value objects so big, they literally can't fit in memory. :-) Is it much different from a lazy-loaded collection?
https://github.com/brandonsavage/Upload Should be all you need.
&gt; Is application-level coordination the best go-to solution we should jump to, or not, because it's too hard? Can't be both ways. Maybe, maybe not. The big blue book suggests working with things at the aggregate level and assuming that changes to two aggregates may not be committed at the same time. Is it worth the effort? Maybe, maybe not. I'd think that the question becomes: how likely is there to be an unusual spike that creates high-contention. If its not likely, then it may not be worth the effort. If its likely-- you can choose not to, it may just be a good idea to have a plan in place. I'm reminded of what Udi Dahan said in one his [presentations](https://vimeo.com/131199089) "Contention is a really big problem, that you don't really know that you have it until it blows up in your face . . . The problem is, this occurs at the worse possible moment." (I enjoyed the presentation, he goes over how CQRS is complicated, simple ways to implement some of the concepts, and how just adding a complicated CQRS domain doesn't always solve performance issues).
&gt; A value object is either immutable, or passed by value (by copy) and everywhere you go you'll see it described as an object * smaller than an entity*, a building block for one entity. I.e. An entity contains value objects, and not what you propose - value objects containing entities. Are two trees with the same values identical? I suppose you could go either way, but I'm leaning towards yes, two trees with the same values are equal to each other, even if they have different identities.
&gt; They tell you that you can't use that password, but they don't tell you why. &gt; A compenent user can then log out, then log into the other account and change that password. They already own the email, so its doesn't seem much different than sending a password reset email. The email owner may have changed, but that's what grants account access. I don't understand your answer.
&gt;&gt; They tell you that you can't use that password, but they don't tell you why. &gt;&gt; &gt;&gt; A compenent user can then log out, then log into the other account and change that password. They already own the email, so its doesn't seem much different than sending a password reset email. The email owner may have changed, but that's what grants account access. &gt; I don't understand your answer. In regard to what prevents you from having two emails with the same password. They won't let you change your password if it new one matches the password of the old account-- or at least that's the last I heard. (Maybe they let you change the password and make the other account inaccessible).
&gt; You don't have to load them in RAM. You're only really screwed, performance-wise, in the unfortunate even that they have different selectors, the same row count, and every row but the last is identical. I don't put comparison logic on my repositories for comparing their content. That's nonsense. The conversation went into full-blown nonsense here. :-) &gt; Agreed that value objects have no meanful identities, but collections and similar concepts may be a special case. It shouldn't really be surprising that a domain would consider these equal: C Well, collections are used quite a bit, but there's usually no need to compare them. They're mutable, yes-- so they don't fit the strict definition of a value object, but I'm starting to wonder if they're a sort of middle ollection::of(["a"]) == Collection::of(["a"]). Nope collections are not special case of a VO, it's simply that what you're talking about is not a VO. A collection can be a VO, if it has pass-by-value semantics (like PHP arrays) or is immutable. Because if it's passed by identity (reference) and is mutable, you need to *care* who else has this reference and what it means for you to modify it. Here's an example of a value, a number assigned as an entity field: $entity = new stdClass(); $entity-&gt;foo = 5; Now elsewhere you have this: $number = $entity-&gt;foo; Now, what is in $number? Is it $entity's number? Will doing $number++ mutate the entity? No. A value is yours the moment you grab it out of somewhere. That's what a value is. It has no identity, it doesn't belong to anyone. It just is. &gt; &gt; They represent domain state at some point in the domain. They have an identity. &gt; That sounds an awfully lot like an entity. It doesn't need to have its own row to be an entity. At first you were like "that repository is a VO" now you're like "that repository is an entity". What's next? I think you're messing with me at this point. How about: "that repository is a repository". &gt; However, if the case comes up where they are actually compared-- its now an entity that be compared by value. I suppose that works, but it sounds a bit more in the middle than the usual entity. There is no case for comparing them. Literally there is no case. It's nonsense. I don't compare my repositories by value.
&gt; Maybe reddit can learn a thing or two... What, you can't register fast enough? ಠ_ಠ &gt; Register... isn't that more like queue? No I mean register a user, because the example was "user registration". LMAX can handle millions of *material transactions* in a second. This means facts, put on disk. Queueing has no limits as to scaling as it's fully horizontally scalable.
&gt; I'm reminded of what Udi Dahan said in one his presentations "Contention is a really big problem, that you don't really know that you have it until it blows up in your face . . . The problem is, this occurs at the worse possible moment." (I enjoyed the presentation, he goes over how CQRS is complicated, simple ways to implement some of the concepts, and how just adding a complicated CQRS domain doesn't always solve performance issues). I saw some of this. I wasn't quite impressed. It felt like the kind of loud-mouth presentation that's very short on facts and very rich on inapplicable advice like "connect the client directly to the database", or "have less horizontal layers by splitting in vertical bounded contexts" (completely independent concepts). This example of contention was artificially constructed to use optimistic locks, which by design are, you know... "optimistic" - they depend on the resource being very unpopular by default. Would the same query be slightly changed to use pessimistic locks, the scenario of "transactions" failing would disappear, and the site would see orders of magnitude better performance. Of course even pessimistic locks have a limit, so to go into his example of problems in CQRS, few puzzling things he said: *"Even companies that apply CQRS to the letter, that when they trip over a contention scenario, they have the bottleneck arriving on the query side. That the background update logic from all of the commands and events cannot process against the query model fast enough, because the query model keeps deadlocking on them."* This makes no sense in two big honking ways. - Why is there a *contention scenario* in a linear stream of commands? The point of accepting async commands is that you can process them linearly (if you wish), thus eliminating the need for locks, and eliminating contention. There won't be *any contention*. I'll assume when he says "contention" he simply means "load", although that is certainly not the same as contention. - He claims the query model keeps "deadlocking on them" because of a lot of events coming from the write model. *What?* Why would the query model be deadlocking. Did he just invent a new meaning for this term, or am I missing something? Events are coming, the query model applies them. There's no *conflict* that would cause *deadlocking*. This and the rest he said and the overly simplistic solutions he's offering that favor crude use of SQL in examples that talk about "high scale" make me think he doesn't quite know what he's talking about. "Voila," he says, "CQRS but sitting on a shared data model." Well, I guess he kinda missed the one important thing about CQRS, which is that there is no shared read/write data model.
Using aliases in the service provider is fine, since the service provider is extending an Illuminate class anyway. If you are using aliases in other places, the problem isn't so much that you are using aliases but that you are passing the container in to your class. Injecting the entire container instead of the individual classes you need means you are using the container as a [service locator](https://en.wikipedia.org/wiki/Service_locator_pattern). As a side note, using an IDE with good autocompletion like PHPStorm and PHP 5.5+ makes using the class name incredibly easy - just start typing the name of the class, press enter when it autocompletes, and append [`::class`](http://php.net/oop5.basic#language.oop5.basic.class.class) &gt; Where do you draw the line between being productive and being Anal about the "write way" to develop an app? I've never regretted time spent understanding a concept or design pattern. Still, you probably have more fun coding then agonizing over design decisions right? I think constantly looking back at your code and refactoring is a sign of healthy development practices, so I wouldn't worry about it.
Lumen has a lot of the Laravel bloat removed yet still has [queues](http://lumen.laravel.com/docs/queues) and the Laravel [console](http://laravel.com/docs/5.1/artisan). If you really don't want the HTTP stuff you could just pull in the [Illuminate packages](https://github.com/illuminate) you need and wire them together. The console package and database package can both be used pretty easily without the rest of Laravel and the bulk of the steps in Laravel tutorials would transfer over.
Can you be more specific? If you're talking about a message queue I would suggest looking into [beanstalkd](http://kr.github.io/beanstalkd/). It's dead simple to install, run, and use, and there are a bunch of PHP client libraries for adding to and reading from the queue.
Gee, I dunno, maybe the bajillions of dollars in revenues it's made companies over the last 15 years or so?
because some developers aren't delinquents
Right, but your CI server needs those testing frameworks installed so the way I see it you can: 1. composer install, build, composer install --no-dev, deploy - The code is never tested without dev dependencies 2. composer global require, composer install --no-dev, build, deploy - The packages could conflict with other build tasks on the same server 
Or deploy it to stage with no dev dependencies and run your integration tests (webdriver) against that. If you don't have a staging environment that mirrors live you're doing it wrong ...
You're missing my point. Let's say a developer accidentally requires a dev dependency such as throwing an exception which is part of phpunit. On his machine phpunit is installed so everything will work well. On the CI server the tests are run with phpunit installed so everything will work well. Then you deploy the code without phpunit and crash the production servers. 
Probably best to use an existing library
Didn't PHP start out as something to just render simple HTML pages with a *little* bit of dynamic code. Why in this day and age, an HTML templating library is used to make an entire server software like Wordpress? PHP was made for simplicity, does no one see the irony? Is this Poe's law in action where someone started making big things with PHP just as a joke but then actual devs started jumping in and now the whole thing's gotten too far?
&gt; But can you ever be sure? Of course not. But your code with --dev is constantly tested on workstations which gives you a higher chance of finding errors before deploying. When using --no-dev you give that up and I'm questioning whether it's worth the little benefit you get in return. 
Isn't that what code review is for? Automation gets you so far. It's usually good at picking up dumb repetitive things. But people inappropriately using classes and the like is probably a job better suited to a human casting an eye over things.
If you happen to speak French, the [NoSQL / SQL with Postgres](https://youtu.be/CH8AJEsVMmI) talk Pomm's creator gave during the PHP Tour 2015 is quite interesting. 
This, let the session storage specifics to http://php.net/manual/en/session.configuration.php#ini.session.save-handler
how about /api/v1/items/{channel id}/{parent channel id} i really dont know what routing system youre using but you need to have parameters for both id's regardless. one however can be implied. again depends on your routing system i think. 
This introduces another drawback because we wouldn't be able to use behat's symfony2 driver anymore on tests which don't require javascript. We'd have to run everything in selenium which would make it significantly slower. It's possible, just not worth it. 
nah people started making small things then they turned into big things when suddenly they had a multi-million pound business running off it
A routing system would manage this for you. You can have two routes that go to the same controller. I would go with separate routes and have your controller handle it. 
Seriously, how can you post such a dumb question in a serious thread? 
This just seems like poor design
As a Dane, I myself have spent a lot of time working with the danish character set and the issues they have. In my experience, using mb_detect_encoding() is somewhat tedious and error prone, when trying to work with multiple types of encoding. Don't rely on the encoding stated in the HTML; lots of danish web agencies uses third party hosting with whom they have no technical contact with. Both Apache and Nginx can return a different charset than what the HTML is written in and the agencies just mess up the encoding till it works. The result is somewhat confusing when trying to decipher it. You should just assume that what you are receiving is UTF-8 if not the server response is stating otherwise. 
The context of my reply wasn't about collections, but about you needing a service using a repository, to ensure an invariant you can't ensure in the entity. You claimed earlier your entities ensure all business invariants without a repository. So this is a direct contradiction.
Twitch uses IRC. [See here on what clients they support.](http://help.twitch.tv/customer/portal/articles/1302780-twitch-irc) You can: * use on of the clients and follow instructions to connect, then configure the client to log messages to a file. You can work with the log later to get your text. * use a [PHP package to programmatically connect](https://github.com/phergie/phergie-irc-parser), interact and log your messages. * use a plugin for one of the desktop/command line clients to script additional features if needed. * scrape the DOM as rendered on Twitch channel. They use EmberJS and the HTML is very clean. 
How do I use PHPStorm about my JavaScript module paths (webpack) so I can navigate through module-relative requires/imports?
Which is a better way to dockerize a PHP web application? Take the nginx container as a base and install php there, or take the php container as a base and install nginx there?
&gt; &gt; Yep. Just because most interfaces treat them as mutable doesn't mean they shouldn't be ValueObjects. For another example, see PHP mutable and immutable Datetimes or Java's mutable Date and Time objects &gt; No, actually the mutable datetimes on both platforms have widely been recognized as mistakes. So we shouldn't point out mistakes as examples of doing it right. &gt; &gt; PHP rectified their mistake with DateTimeImmutable, and Java has recognized the issue at the platform level and they're adding native Value Types soon (and then expect a lot of these objects like date time, becoming native VT so they won't even technically be objects anymore). That's precisely why I pointed to both the mutable and immutable versions. Just because something is designed as used as a mutable object, doesn't mean that's the right or only implementation. &gt; Which means you can't use them for a repository that's, say 1TB large. &gt; You're talking about pointing to a remote collection, exposing it as an immutable locally, then allowing mutated copies by storing changesets and then committing them to the remote collection, basically. &gt; Which means you mutated the remote collection, so your interface is lying if it claims the collection is immutable. Only the changesets were immutable, not the collection. Not the same thing at all. Maybe not technically a collection, but it wht I was thinking (or trying to describe) wouldn't be much different from how git works, just needs a proper backend, but it doesn't mean its suitable for all cases, as its a more complex design. &gt; I don't know why you keep "enhancing" my architecture until it has more, rather than less problems. :-) I never said using immutable repositories would be simpler, its just a possibility. &gt; I don't know why you keep "enhancing" my architecture until it has more, rather than less problems. :-) &gt; &gt; Here's a simpler idea: a tree is a repository of entities, it's mutable, it's stored remotely, it has commands &amp; queries apt for manipulating and reading parts of a tree, and it ensures its own consistency under the format it represents. Commands and queries. Simple. I already said yours work better as entities, which are mutable-- and never had a requirement to be stored locally. Commands and queries-- sounds a lot like entities. 
So, you want to look at a collection of items, filtered by a channel. It looks like you want to look at items in a channel with a specific ID, regardless of whether this is the channel an items is in, or the parent channel of the channel it's in. This suggestion may not be precisely what you need, but it sounds like it depends on how you define channels, and how the hierarchy between them works. It sounds like since you're wanting to look at a collection within two different, potentially distinct parent collections (i.e. channel, or channel parent). That says to me you're wanting an items endpoint, at the top level, with some query parameters to filter the collection. If you want any item within a channel with a given ID at any depth, i.e. if your items are in a channel which has a parent which has a parent with ID 1, or an item that's just in a channel with ID 1, then something like the below URL could work and your controller could do the heavy lifting, instead of making complex URLs. A more real world example of what I mean here is for example like saying when you're looking at a category in a shop, the items in the subcategories are still part of the category as a whole, or maybe an even higher up collection. GET /api/v1/items?channel=1 # Or the shop example, laptops, in computers store, which would contain # all laptops, like gaming laptops, ultrabooks, whatever: GET /api/v1/products?store=computers&amp;category=laptops If however you want it to only be the parent or a given channel specifically, I still think you want an items collection with a filter, just a different structured query string: GET /api/v1/items?channelOrParentChannel=1
What is considered better practise out of curiosity? if (isSomething === true) { //logic } OR if (isSomething) { //logic } I was reading that the former can some times give a bad impression, but it is something I have done previously since I just thought it reads better and is more explicit. Although that's just an assumption I came to on my own so thought I'd see if really I should be using the latter method! Thanks in advance :)
&gt; When you drop the idea repositories can't see beyond aggregates, you realize the uniqueness constraint isn't about aggregates, it's about one specific piece of data in them.. An aggregate is there to ensure buisness rules are followed. An aggregate to ensure uniqueness in a list of emails is a different from an aggregate that has one email. I was talking about the former. Yes, the aggregate may cover an unconstrained scope. Yes, a repository may also do the same thing. The difference is where the domain logic is. You implied that the repository couldn't be trusted to maintain the unique aggregate (with findByEmail()). So if the repository is limited in that way, it make sense to move the enforcing of that invariant into the domain. &gt;... and you can verify uniqueness for by storing the hash index in RAM (which is much smaller than all emails together), and distributing the buckets on several servers, to handle somewhere between 83 (32 chars emails) and 830 GB (320 char emails, upper boundary for emails by RFC) of in emails for the entire connected population of people on Earth. So, it keeps it all in-memory. &gt; So you know, it's actually quite doable, and the above is if we want to register the entire population of Earth in the span of an hour. Its a cool system, but it doesn't sound like a practical solution to our use cases-- at least to justify the cost. "We can register the entire population of the hour" is just going to be met with "When are we ever going to need that?"
&gt; The context of my reply wasn't about collections, but about you needing a service using a repository, to ensure an invariant you can't ensure in the entity. &gt; You claimed earlier your entities ensure all business invariants without a repository. So this is a direct contradiction. I don't think I did, but in the odd case that I did: I meant that aggregate ensure all buisness invariants within themselves. A business invariant's spans multiple aggregates isn't handle within those individual aggreagtes, but in the scope of another aggregate (which may be encapsulated by a repository)
It really depends on the coding style. I personally prefer the latter. However in dynamic language like PHP, it's usually considered bad to rely on some strange casting behaviour when your expression isn't strictly a boolean but instead something like a number or string.
CRQS, but different [feedback reply](https://www.reddit.com/r/PHP/comments/3mwl2z/in_ddd_for_php_what_is_the_difference_between_a/cvodssz) 2/5 == &gt; Would the same query be slightly changed to use pessimistic locks, the scenario of "transactions" failing would disappear, and the site would see orders of magnitude better performance. &gt; &gt; This example of contention was artificially constructed to use optimistic locks, which by design are, you know... "optimistic" - they depend on the resource being very unpopular by default-- of course. You'd think it'd be so simple, but it wouldn't be surprising the optimistic locking was running more predictably and with more throughput than a pessimistic lock would. Pessimistic locking with that many threads tend to be expensive-- like order or two of magnitude difference. The problem isn't just updating the database, but the checking that the inventory &gt; 0 before doing that update. The pessimistic locking would block all of those threads trying to do the same thing on the same row. You can either just have them wait and block-- but because there are so many threads waiting: either the same connection pool problem arrives or the query times out and you're back to the same situation as the optimistic locking. The retry logic used helped resolve the supposively occasional conflict, but still: the situation was never designed for that That being said, I wonder if the lack of upper bounds on the retry logic escalated the problem. Splitting this up to keep with our smaller threads.
CRQS, but different [feedback reply](https://www.reddit.com/r/PHP/comments/3mwl2z/in_ddd_for_php_what_is_the_difference_between_a/cvodssz) 3/5 == &gt; - Why is there a contention scenario in a linear stream of commands? The point of accepting async commands is that you can process them linearly (if you wish), thus eliminating the need for locks, and eliminating contention. There won't be any contention. I'll assume when he says "contention" he simply means "load", although that is certainly not the same as contention. Its a bit limiting to try to do a high volume of commands linerally-- especially when *most* of them don't interact with each other. The point isn't just to process them linerally, its the ability to process them *later*-- linerally or not.
&gt; So, it keeps it all in-memory. Just the email index. You can store the buckets on SSD or even HDD for the other nodes, but of course then you have a lot less throughput per node. This is about: do you want to do this with 10 machines and lots of RAM, or hundreds of machines with less RAM. I'd take the solution in RAM. &gt; Its a cool system, but it doesn't sound like a practical solution to our use cases-- at least to justify the cost. "We can register the entire population of the hour" is just going to be met with "When are we ever going to need that?" Uhmm. Maybe this is why I said "An actual operation would have far more modest throughput and overall volume requirements overall so we can go back to the queue solution, a throughput around 10k user registrations a second and save some coin on hardware too." The system I described covers the worst possible case scenario. It only gets cheaper than that. My point was to shoot down "we can't do this if we need to suddenly scale", by describing what it takes to register **Earth** in an hour. I think it's not too shabby. Now, of course, we have to scale up to the entire Galactic Federation, I'd probably go for another solution, light speed limits and all.
CRQS, but different [feedback reply](https://www.reddit.com/r/PHP/comments/3mwl2z/in_ddd_for_php_what_is_the_difference_between_a/cvodssz) 4/5 -- &gt; This and the rest he said and the overly simplistic solutions he's offering that favor crude use of SQL in examples that talk about "high scale" make me think he doesn't quite know what he's talking about. &gt; "Voila," he says, "CQRS but sitting on a shared data model." Well, I guess he kinda missed the one important thing about CQRS, which is that there is no shared read/write data model. CQRS can use the same underlying database, but it still requires the two (code) models to be CQRS, the rest of the quote was specifically having the command in a seperate object from the query. "Fundamentally, you could say that the that the select query is on a separate object *this [update] code*" The separate objects is what CQRS distinguishes CQRS from CQS-- e.g: CQS encourages different methods for queries and commands, not for both. CQRS encourages different objects for queries and commands, not one for both.
I've been interested in Pomm for a while, but have never got round to actually using it. Has anyone here used it for a serious project, or is anyone aware of an OSS project that uses it I can look at? I love the idea, but am wondering what the real life pros and cons are.
When there's high contention, pessimistic locking is always better. Running out of connections in a pool of connections is a separate problem. At least those that *managed to get a connection* would get a result, even if they have to wait a few seconds about it. Optimistic locking in a high contention scenario means that from everyone fighting to read and mutate a row, one wins, the others lose. You tell me what takes more resources: - 1 thread doing work and getting results, 99 threads waiting. - 100 threads doing work, 1 getting results, the other's work wasted, and must repeat. It's kind of obvious, isn't it? As for the connection pool and timeout issues: queues. Which I mentioned. BTW, a couple of other creative solutions to this "contended in-stock counter" problem: 1. Split availability counters in groups of, say, 100 (or whatever). Have each user pick a book randomly from one of the piles. If we have 10k books in 100 piles of 100 books, we just made this resource 100x less contended, but *without losing strict consistency* (which his event sourced solution without checking total counts *would*). 2. When a good is in high demand, it means additional supply is guaranteed. Flip a switch on such goods and turn off the counter logic, just process sales and order more. This was mentioned somewhat in the talk, but what wasn't mentioned was that *you only need to do this for some items, not all*. If you have exactly 3 "Rick and Morty" cups and they've been discontinued, it's best not to allow people to buy 4 cups.
We had a bug due to this the other day. My idea is to create a new vendor directory that the unit test tooling can use (ie "vendor-test/"). I've been looking into doing a PoC soon. Our immediate solution was to move everything to "require".
&gt; CQRS can use the same underlying database, but it still requires the two (code) models to be CQRS, the rest of the quote was specifically having the command in a seperate object from the query. That's a big problem some have in understanding CQRS. There's no such thing as "code models". You can't say "those models are independent, but oh well, changing one magically affects the other". No, if they're joined at the hip via a data store, it's one model with two interfaces exposed via two objects, it's not two models. **Completely different thing**. A model has its own storage. Look at any CQRS diagram and you see the read models have their own "cylinder" denoting data storage. The write model itself either has no storage (sometimes that's fine), or just an event store, or it has its own *independent* storage (like an private read model that no one else can read). http://www.codeproject.com/KB/architecture/555855/CQRS.jpg This is *very, very* important part of CQRS. It's the one constraint we can't play with, because without it, all the promises for scalability and so on go out the window. If you have a shared model with commands and queries on top, this is like a single SQL database, it's an example of CQS, but not an example of CQRS. &gt; The separate objects is what CQRS distinguishes CQRS from CQS-- e.g: CQS encourages different methods for queries and commands, not for both. CQRS encourages different objects for queries and commands, not one for both. I'm afraid this is not about methods or objects. It's about strict boundaries between read and write models in the system *as a whole*. If you have these two: $model-&gt;read() $model-&gt;write() And these two: $readModel-&gt;read() $writeModel-&gt;write() These two are both CQS, but looking at the number of objects is not enough to determine if they're actually independent models, and thus CQRS. If the write model directly affects the read model storage (instead of producing events and sending them to the read model), it's not CQRS. **The whole point of CQRS** is that the read model updates asynchronously from the command execution, and that you can have *multiple read models with different structure* for one write model. This is not a trivial high-brow point I want to have so I can argue semantics. This separation is literally the thing that creates all the properties of the CQRS architecture, that CQS doesn't have. This is why when I see some people in the PHP community implement "CQRS" toys (that aren't CQRS) I shake my head. We like very much to use Big Words without understanding what they mean.
&gt; If the remote repository can tell you the differences between the reference you have now, and the reference it has now. Yes, it can be treated immutability. I honestly don't get your point. - What on Earth will I do with an immutable repository? - What on Earth will I do with comparing two immutable repositories? Do you have a point here, or just brainstorming? &gt; [In Git] You can literally compare equality on two different trees or commits in repositories. Git is not immutable. Otherwise you'd be unable to "push" to a repository or "commit" to it. As for why you compare codebases in Git, it's because a Git repository is effectively just files of bytes. Git has no higher understanding of the domain, so it can't automatically merge data. Operating at that level of model design in the enterprise means to drop everything and go back to Excel spreadsheets for the entire operation. Developers deal with the fact they aren't as scalable ("adding more people to a late project makes it later") and their work is very prone to human errors (breaking the build, various other kinds of bugs) and involves a lot of manual human work (writing the code and operating on the repository directly), so that the *enterprise* won't have to suffer that. If you model a domain for an enterprise to a point it's no better than what a developer experiences day to day at work working with raw code, then it means you failed to add value to the company and they're better off with Excel.
&gt; Return all items where item.channel = 1 GET /channel/1/items &gt; Return all items where item.channel parent.id = 1 GET /channel/1/children/_all/items &gt; Return all items where item.channel = 1 AND item.channel parent.id = 1 Why would a channels have itself as parent? I'll assume, you mean parent channel 1 and channel 2 ? Still for any item in channel 2, it would have parent_id == 1... So lets assume, what you really want, is get all items of channel 1 OR its descendant channels? GET /channel/1/descendants/_all/items 
You could easily say the same about Javascript. "Didn't JS start out as something to just add a little interactivity to HTML? Why in this day an age, is an animation library used to make a entire dynamic web apps like React?"
I suspect that there might be tight coupling or lack of dependency inversion... /authenticate would have the following dependencies. UserRepository, SecurityService. Where you pull a User from the repository then pass it to a SecurityService object for validation
&gt; I can't say the same for Python or Java. PHP is very similar to Java in a lot of ways. A lot of PHP's OOP features have been Java inspired.
Excellent theme! Btw what is the font from this screenshot https://camo.githubusercontent.com/7926d90e17568bbce53760698e603c3b7bc2c2d2/687474703a2f2f63646e2e6869666976652e6e6f2f6d6174657269616c2d75692f6d742e706e67 ?
Maybe Source Code Pro?
I'm not quite sure, though it looks similar to [Hack](http://sourcefoundry.org/hack/)
There is no reason you should be using code from PHPUnit outside of your test suite. If you are, you are failing to apply separation of concerns. Your application should not know anything about your test suite. Furthermore, if you know you're using a library in production, you shouldn't put it in require-dev. The purpose of require-dev isn't to keep your production apps running. It's for use in your **dev** environment, hence the name **require-dev**.
The @ symbol is very unusual
Yeah PHP's OOP actually is more similar to java's than to C++'s, especially since PHP 5 the objects are passed by references(so no need for ampersand anymore, like you do in C++). In fact, PHP borrows many features from other languages. The class constants and namespaces are borrowed from C#, generator and magic methods are borrowed from Python, anonymous methods and short array syntax are borrowed from javascript, and traits are borrowed from scala. In a way, PHP is like jack of all trades, at least that how I perceive it. 
Nice, it looks great !
That's just what we need. [Web Development in Assembly](https://1.bp.blogspot.com/-3wHCYHd_yf4/Uwk0bpUPqII/AAAAAAAAEeE/BTCJlN_HGXI/s1600/image-773266.jpeg)!
My bad for the `AND` statement, that should've been an `OR`!
It's Fira Mono/Fira Code. The @ symbol is a dead giveaway. I used Fira Code for awhile before going to Hack. But I do agree that the @ symbol alone is almost a reason to use Fira Mono/Code.
Well, It's been a few years since I looked it. Last time I looked at it I was only a front-end guy who did mostly procedural type work (and mostly for Wordpress). I just remember I tried to learn some of the Android SDK, but since I didn't know Java I was lost. Never really looked at it again. Things might be different now that I a few years of "real" programing experience under my belt. 
The main usecase here is that we have a main channel, f.e. `Radiostation X`. This has subchannels such as `Radiostation X: Rock` and `Radiostation X: Dance`. A song for example can be played on either one of those channels. Now if I want to request the songs played by `Radiostation X` including all of it's subchannels, what would my route look like?
Depends what you want to use C++ for. I wouldn't use C++ for a smaller website or non-enterprise type applications that need to run on the backend. However, if you have something like a website that has 1,000,000 plus products that need to be queried and logic performed on the results. Things can get a bit harry real fast with PHP. Sure you can throw more memory and CPU at it, but it's starts getting expensive real fast. That's where something like C++ comes in. It will run the model on those apps, piping the output to your view which will likely still be outputted with PHP. Things you run with C++ are typically apps that seldom change in functionality, but require higher performance and scalability. 
I've heard about using node JS as some sort of proxy with PHP, something to do with templates? What kind of jse case or app would you use something like this?
&gt; I suspect that there might be tight coupling or lack of dependency inversion... &gt; /authenticate would have the following dependencies. UserRepository, SecurityService. Where you pull a User from the repository then pass it to a SecurityService object for validation You say there might be tight coupling and then propose two services share a repository? :-) That's not a way to have less coupling, you know.
For most people: [`password_hash()`](https://secure.php.net/password_hash) + [`password_verify()`](https://secure.php.net/password_verify) + [`password_needs_rehash()`](https://secure.php.net/password_needs_rehash). For people with a separate web server and database server who want to go the extra mile, a Hash-then-Encrypt scheme (e.g. what [Halite](https://github.com/paragonie/halite#secure-password-storage-hash-then-encrypt) does) is preferable to "peppering".
&gt; That's a big problem some have in understanding CQRS. There's no such thing as "code models". You can't say "those models are independent, but oh well, changing one magically affects the other". No, if they're joined at the hip via a data store, it's one model with two interfaces exposed via two objects, it's not two models. **Completely different thing**. &gt; &gt; A model has its own storage. Look at any CQRS diagram and you see the read models have their own "cylinder" denoting data storage. The write model itself either has no storage (sometimes that's fine), or just an event store, or it has its own *independent* storage (like an private read model that no one else can read). I usually see the opposite problem, where its hard to understand you *don't* need a command bus, event stores, event sourcing, eventual consistency or other things to get CQRS. The difference between CQS is that "R": Responsibility. As in "Single Responsibility" from [SRP](https://en.wikipedia.org/wiki/Single_responsibility_principle): "every class should have responsibility over a single part of the functionality provided by the software, and that responsibility should be entirely encapsulated by the class. All its services should be narrowly aligned with that responsibility." That would be consistent with [Microsoft's](https://msdn.microsoft.com/en-us/library/dn568103.aspx) stance and several others (e.g: [1](http://codebetter.com/gregyoung/2010/02/16/cqrs-task-based-uis-event-sourcing-agh/), [2](http://martinfowler.com/bliki/CQRS.html), [3](http://cqrs.nu/Faq/command-query-responsibility-segregation). That quite clearly say the objects are separate, but the separating the database is not required. (I think I may have referred to executing commands whenever as an advantage to CQRS-- but it isn't strictly required; it something I usually do when using CQRS rather than CQS that I naturally think of it in that way) &gt; This is why when I see some people in the PHP community implement "CQRS" toys (that aren't CQRS) I shake my head. We may be shaking our heads for different reasons, but there's certainly a long way to go.
I'm sorry I read material theme and I was waiting Material design ™ but this https://camo.githubusercontent.com/7926d90e17568bbce53760698e603c3b7bc2c2d2/687474703a2f2f63646e2e6869666976652e6e6f2f6d6174657269616c2d75692f6d742e706e67 is not material design™. Colors that you have used are not in the palette from Material design. https://www.google.com/design/spec/style/color.html#color-color-palette The font that you used is some serif font and material design does not use serif, use sans based Like Roboto. https://www.google.com/design/spec/style/typography.html Actually there is nothing from material design in that theme. 
The question was "the best way" not "the simplest way". Kafka isn't that hard to use BTW.
&gt; Let's say a developer **accidentally** requires a dev dependency 
&gt; Do you have a point here, or just brainstorming? Probably just brainstorming-- but these are concepts that can extend to efficiently comparing and updating two remote collections. &gt; Git is not immutable. Otherwise you'd be unable to "push" to a repository or "commit" to it. I didn't say a git repository was immutable, but git commits are. Even if you rewrite the entire history, the original commit still exists in its original form until it is deleted. The new history has a different commit. &gt; Operating at that level of model design in the enterprise means to drop everything and go back to Excel spreadsheets for the entire operation. I'm still not seeing your point here. I'm not talking about making a data-unaware collection here. I'm just talking about how a collection (which I think we both agree can be very useful in a domain) can be made immutable, even if its stored remotely.
I guess that's a little misleading, though the theme is actually called "Material Theme"
&gt; &gt; I'm still not seeing your point here. I'm not talking about making a data-unaware collection here. I'm just talking about how a collection (which I think we both agree can be very useful in a domain) can be made immutable, even if its stored remotely. &gt; It can be made immutable by simply blocking access to the methods to mutate it, so that's simple :P Good job! You've done it.
&gt; I usually see the opposite problem, where its hard to understand you don't need a command bus, event stores, event sourcing, eventual consistency or other things to get CQRS. The difference between CQS is that "R": Responsibility. So which responsibility do you think CQRS seggregates? It's right there in the name, but I don't think we have a clear definition about your side of it. To me it's clear... separation of the models that handle commands, from the models that handle queries. &gt; That would be consistent with Microsoft's stance and several others (e.g: 1, 2, 3. That quite clearly say the objects are separate, but the separating the database is not required. That makes the distinction of CQS from CQRS pointless. If CQRS doesn't require separate read/write models and storage, **what makes it distinct from CQS**? I never spoke about command buses, and I said event stores are optional. Eventual consistency is also optional, you can have your read models be strictly consistent, but this obviously reduces your throughput and increases latency, so it should be done with care. But with all that said, I'd like to know how CQS and CQRS differ to you.
Anyone tried it with WebStorm? It installs but isn't available as a theme for me. Works in PHPStorm. I'm surprised the behavior is different between the two.
The reasons given for the migration are painful to listen to. "1. namespaces 2. single points of entry." This was recorded in June 2015, but sounds like they haven't used PHP in over a decade. Probably the most cringeworthy part of the whole interview was this gem: &gt; [The] ecosystem of PHP we found much weaker than that in Python, I think a lot of the main consumers of PHP are mostly revolved around Facebook and a little bit around like Wikimedia foundation, but besides that, not a lot of support. Whereas Python has what feels like a more active developer community and more active ecosystem. He sounds like a smart guy and I have nothing against Python. But this doesn't sound like a "best tool for the job" sort of scenario. 
I recently had the material theme on my work computer through sublime and loved it. At home I was using php storm and started working on a material theme for personal use. So glad you made this now I can save myself some time. 
A lot of people use version 2 and when they need a feature from 3 they just import it. From the limited Python work I've done, this seems to be the most popular approach. from __future__ import print_function, division, absolute_import, unicode_literals
Well java is not 100% OOP. Its primitive types are not objects, so not everything is an object. Also it doesnt support first class class, and class methods(static methods) are just namespaced functions(which is why languages like Java, C# and PHP need to avoid static methods at any cost). 
If so, how will they communicate?
That's how I stumbled across this podcast. After seeing their codebase is littered with references to a "PHP codebase" without a single .php file, I did a Google search and found this interview. To be fair, the "hack" was due to a publicly accessible staging server. I doubt it had anything to do with Python specifically.
Oh yeah, he mentions PHP mysql_ functions and magic quotes as security issues. Again, issues resolved a decade ago.
Whoosh.gif It wasn't just immutable collections, it was objects that represent large remotely-stored collections, that can be treated as value objects, while also being able to operate on all of its members in an efficient way.
You can write your code to be dependent on one specific framework, or you can write your code to be portable between frameworks. Just like when idiot CTOs decided that tying their business sites to IE6 was a brilliant idea, why not give yourself the option by writing more generic code in the first place? I can see how DI could make this possible; all you would need to do is change how stuff is configured in the DIC, if your interfaces are up to snuff.
Because it is not free. If you spent 30% of the development time to add abstractions every time you could make a framework call, that is 30% less features built. And if you never come to that point when you need to swap the framework, that is 30% of work wasted. Also there is a huge possibility that you spend 30% time decoupling, but don't do it well, and when time to swap the framework comes you realize you still can't really do it without rewriting a lot. CTOs that tied their infrastructure to IE6 did that because when they were designing the infrastructure IE6 was the real deal, and then the cost of upgrades piled up.
I haven't messed around in python since my job asked me to move into PHP. My Job is the very reason I ever even dabbled in python. Just got my hands on a RaspberryPi, so may get back into it... 
Yeah, I was going to ask how someone gets things done with such a large line height. maybe I just like to see as much code on the screen as I can at once. I use a 10px font at 1920 x 1080
I feel like it adds a lot of cost in the long run, as one would have to rewrite the application just to upgrade to a newer (with no BC) version. Imagine I write my apps without framework abstraction, and I've been doing so for the past years. Now, my current framework version has just been vastly upgraded (symfony 1.4 to symfony 2.0, imagine), and there aren't many things that I can port to the newer version. My app has just been downgraded to a legacy app, as there are no means to switch to a newer version now, nor in the future. It will require a full rewrite. Switching from Symfony to Zend is one thing, but upgrading to a newer symfony version, even with lots of major compatibility breaks, should not be as much as a pain in the ass. And frameworks upgrade much faster then the language itself. One should always be prepared.
Honestly I cannot imagine any level of abstraction that would allow you to move from Symfony 1.4 to 2.x, since the entire paradign of the framework shifted dramatically. It has to be said that any kind of decently written code is already somewhat decoupled form the framework and upgradable. What Im saying is that the actual goal of "make the framework swappable" is really really costly. 
http://stackoverflow.com/questions/29905953/how-to-correctly-link-php-fpm-and-nginx-docker-containers-together
You forgot 'return' in your getByName method :p By the way, i am in a similar situation right now, but my Repositories abstract an AbstractRepository, so in the getByName method i would call parent::findByNameValue(array('username' =&gt; 'fabien potencier')); So the AbstractRepository hides the implementation, and all my Repositories aren't coupled to one specific implementation.
Using arrays for conditions works only if you use AND logic. How would you query by either username or email? Or find items that have price lower than 10% OR are currently on sale?
Your fonts look pretty good, how did u get them to look so smooth?
You have 2 options in the case of very specific queries, **A)** You make methods for each possible query that you need in the AbstractRepository (while trying to make them as reusable as you can), which would mean that you potentially have a large amount of methods in your AbstractRepository, but it would still be fairly easy to switch your Database implementation because you only need to change 1 class **B)** Again, you try to write as much reusable methods in the AbstractRepository as possible, and then use them in your Repositories if possible, and if there are too specific queries, then you can directly use the Database implementation in your Repositories. This still makes it fairly easy to switch your implementation..
I definitely agree with you (if you mean the `.../channels/{id}/channels/` part). IMO URL design of a tree hierarchy is not easy if you don't name your things properly. One could improve readability by using `/api/v1/channels/1/subchannels/items`. Another thought of mine is to describe the requested dataset better: what dataset has the id `1` and as itself as a parent? Depending on your structure you mean a `root`-ish node of a tree (however, `null` would be far more common) or a recursive node in general, which would lead to the possibility of a `/api/v1/channels/1/items?recursive=1` or something like `/api/v1/channels/1/items?depth=1` &gt; There are only two hard things in Computer Science: cache invalidation, naming things, and off-by-one errors. 
So instead of using a nice query builder you spam your repository class with methods. And the you end up with a repository class that has 30+ getStuffByOtherStuffs methods and that already looks like a code smell
Ah, makes sense! Thank you.
"Decouple everything" and "decouple nothing" both don't quite paint the right picture. Most frameworks that the PHP community calls "frameworks" are frontend frameworks. As in, while they dabble with ORMs and what not, they primarily deal with web UI components like HTTP abstraction, routers, controller, templates. So then it makes sense to build your frontend in them ("coupled"), but to build your backend (service layer) completely independently ("decoupled"). Is this a position OP is willing to accept? :)
I agree. I had very similar thoughts, but I couldn't be arsed to articulate them, thank you for writing this out.
Flask is one of the most minimal frameworks in Python, why they chose that instead of something full-featured like Django is also a big WTF. So yeah, lets ditch everything we know and write this payments system from scratch - because that will most definitely result in better performance. Oh and since nothing has been unit tested or proven in the wild, lets just leave the debug system on in the production environment so we can publicly see what breaks in excruciating detail. Idiots.
Are there any opensource projects that demonstrates thin controllers + fat models (or entity/repository/services) well? Trying to break the habit of having fat controllers, and refactoring more to make things a bit cleaner. I'd rather try comparing what I've tried vs something that's correct/better (lack of a better word) than what I'm currently doing.
One benefit of having reuse across frameworks is moving those large chunks. For example, when moving from CodeIgniter to Laravel, we used https://github.com/rcrowe/TwigBridge to not have to rewrite our TWIG templates. 
Is there a solution to [this question](http://stackoverflow.com/questions/32942297/what-is-the-most-effecient-way-to-check-if-a-variable-isset-and-to-assign-its-va) on stackoverflow or is it pure OCD?
PHP 7.0 `$source = $request['controller']['options']['data']['source'] ?? null;` Which probably does what you don't want to do behind the scenes. &gt; or is it pure OCD? Probs
Where are you getting "Material Design" from? Maybe it's just me, but I'm not seeing the words "Material Design" appearing together anywhere in any of the documentation or readable codebase. Even in previous iterations of his readme, the words "Material Design" don't appear anywhere. So apart from the word "Material" being the only similarity you could conjure up, why are you trying to compare "Material Design" to a project clearly labeled "Material Theme"?
It looks really good! I will try it.
I'd like to hear from developers who actually ported a serious app from one large framework to another. We do seem to be programming defensively to mitigate a situation that will never happen, and let's be honest: developers are _great_ at solving problems that will almost certainly never exist. Sometimes I like to listen to meetings objectively instead of being part of the discussion, and I think to myself, "Jesus, we're fucking nuts. We're bikeshedding over an issue that has a 0.001% probability of ever happening." Which isn't to say you shouldn't break your code up into components. Most of my code is broken up into composer packages so it's easier to maintain and test, and so I can use it in multiple projects (and so I can open source it), but I'm not thinking about abstraction when I write controller actions. When I get to the point of wiring the components together I'm not adding any more abstraction. I'm going to use the tools provided by the framework, and that's that.
Huge fan of this. Thanks!
Depending on what kind of app it is, rewriting that bridge could be enormously costly, obviously not quite as costly as rewriting parts of the app that might have been otherwise tightly coupled to BC changes, but sometimes adapters are as costly as the underlying service layer. So again, depending on the app, a change from 2.3 LTS to 3 may not be any cheaper than 1.4 to 2.0, no matter the quality of abstraction.
There's no "silver bullet" here, honestly. You're not missing out on that one great thing. There's so many different tools out there that can piece together to do what you're looking for but there's not one unified solution in the PHP space that I've seen. There's been some solutions in the past that have tried to unify a lot of the things you're looking for and the ones you listed are decent options but they're also all in various states of support. The truth is user authentication and authorization is a pretty tricky thing to get right. Throw in various concerns about your own systems and pre-existing tools and things get even more tricky. Honestly, there's not going to be one system that's going to have all the exact pieces you need. The best you can hope for is either: 1) something you've built yourself or 2) a patchwork of components wired together to make a more complete system. I've put together a tool that addresses some of your needs, [Gatekeeper](https://github.com/psecio/gatekeeper), but it doesn't meet all of them. There's also [Invoke](https://github.com/psecio/invoke) that does endpoint-based security but that's just another piece to the puzzle. So, don't feel like you're missing out on that One Great Thing in the world of PHP authentication and authorization. Much like any kind of development task out there, there's always more than one way to do something and that "more than one way" leads to a multitude of tools.
&gt; Not use prepared statements? I use them sometimes, actually, but most times I don't. Let's not pretend as if prepared statements are the only secure way to call a query, please? Unless you can demonstrate injection on a value escaped against a UTF8 connection.
The use case was rendering React components on the server for SEO purpose (initial render - server, subsequent updates - client). You need Node in this case because React and any React components are JS. And it's also a horrible idea, and you should look at it more as an experiment than a technique that a sane person would use on a production app.
For PHP7 I'd say go for the ?? operator like another user here recommended. For PHP5 there is... *technically* a solution. But it's the kind of solution that'll cause people to make fun of you: $source = @$request['controller']['options']['data']['source']; Now, in reality, this specific use of the silence operator is harmless, as you don't silence functions (that may call other functions and so on, as a result silencing errors for half your program, which would be really a bad idea). It's also not slow, it's literally the fastest method in PHP5 to do this. But there's the social stigma. So use it only if you don't care what people say about your code. :)
/u/CQRS wrote: &gt; &gt; Not use prepared statements? &gt; &gt; I use them sometimes, actually, but most times I don't. Okay, how do you determine when you should use them and when you shouldn't? &gt; Let's not pretend as if prepared statements are the only secure way to call a query, please? It's not the only way, it's the *best* way. Prepared statements achieve [data/instruction separation](https://paragonie.com/blog/2015/08/gentle-introduction-application-security), where escaping does not. With a couple of caveats, of course: 1. Sadly, because PHP shit the bed so hard, they're not enabled by default; instead, they're emulated. 2. They don't stop second-order, third-order, etc. injection attacks. You can achieve a similar level of safety by hex-encoding all input parameters then passing them to `UNHEX()`, if you'd prefer. (I cannot imagine any input ever violating `bin2hex()` and leading to command injection.) Can we say the same about more Unicode hacks? I can't say for absolute certain, but new ones are unlikely. Nonetheless, there's a rule in computer security: Attacks only get better. &gt; Unless you can demonstrate injection on a value escaped against a UTF8 connection. Can you demonstrate that everyone uses UTF8 connections 100% of the time without exception? This is a non-starter. ----- Also, the overwhelming majority of PHP users in the world aren't security experts; why burden them with additional responsibilities (remembering to escape) when we can just teach them to cultivate the habits to write `("string", [$param[0], $param[1], ... ])`? Better security, less cognitive load. If we make it habitual, developers are less likely to write vulnerable code. That's why I advocate the adoption of prepared statements so strongly. If you don't agree with my reasoning, feel free to not follow it. Disagreeing with me isn't necessarily a bad security decision.
Yay! Finally!!! \ :D /
I've worked on several systems each meeting unique needs in unique ways and for different clients. For something that's so integral and with many levels of complication I feel like there should be a common standard. I'm starting a new project, part hobby, part research project. It's my opportunity to get this right, and I'm left with the question, roll my own, or test and sort through various existing options.
Despite the name, it can also catch things that aren't errors (such as E_WARNING and E_NOTICE)
Yeah, pretty straight forward. 
Don't be an ass. Are you telling me that you can't understand why a post to a web development sub, about applying a GUI theme for a web dev application, that uses the word "Material" to describe itself, is causing confusion?
Please start using a spellchecker (your browser has one built in, just switch the language to English). Other than that, good post. Being paranoid about a change that has next to no likelihood of happening is ludicrous.
&gt; we knew that like Python is a really easy language, just pick up over a weekend And that's how you get yourself hacked.
Which micro-frameworks would you recommend?
Thanks) I actually typed all that on my phone. )))
I'm looking forward to PHP7, though when I'll get to use it in anger is up to hosting providers I suppose.
you can suggest another ways to connect to DB. I do want to know easier method.
This is one reason to do your own hosting. We keep sites in house and our customers pay us about the same as they would pay any other hosting provider. We can run more than one site on one Linode VPS. The downside is that we need to do all the chores of a hosting provider, such as setup, backups, monitoring etc. The upside is that we know the environment and do not get subpar support answers. I believe we even make some money from this in the end but the real reason is that we don't want to play by someone else's rules. Almost all our customers choose us when they hear our reasons for suggesting ourselves (not least because it is the same money in the end). Some clients wants e-mail too, which we provide through [GleSYS](http://www.glesys.com/). We do not want to deal with all the hassles of e-mail hosting and their service has been working great so far.
Unfortunately I'm using Sentry. Don't use it if you actually want to keep a working system between updates.
The change in behaviour to set_error_handler() is a pretty big BC break for Magento. I can only imagine there will be hundreds of thousands of Magento sites that will never run properly on PHP7 due to the tons of cruft being spammed into the system.log file that will now just cause error reports.
No, that was in 5.5
I'm using Fira Code in the screenshot above, as that's the recommended font by the original author of Material Theme for sublime.
Thanks Paul for your links and your feedback, it means a lot for me ;) I think I need to think a little deeper to see where I want to lead this project ... Maybe create different lib (one for the Model, one for the Data Mapper which is also responsible of Connection and query abstraction...).
This is interesting but not revolutionary. It's already used in other industry. PHP is late on this one.
I'd be interested to see what you think of [Gatekeeper](https://github.com/psecio/gatekeeper). It's roughly modeled on the same idea as Sentry, even a similar structure but has a few different features.
I haven't read the link, but I sure hope it lies about the results.
It depends on your disaster recovery scenario. Backups are a good start, but what happens if one of your servers drops off the face of the earth, can you provision a new server quickly with all the little config tweaks and set-up accumulated over time? For us, losing a day or more to rebuilding servers is unnacceptable purely for the case that client's sites may be unavailable for an unpredictable amount of time. The benefits of automation in that case, even for just a couple of servers, is being able to spin up and recover from a provider's worst nightmare complete data loss situation. This is especially important if you guarantee your hosting in some way. I agree with you it's not always necessary, but it's definitely handy to be able to express your infrastructure in version controlled code. I would argue that it is essential in order to scale past more than a small handful of servers and have seen the mammoth effort that goes in to manually syncing config changes between 60+ nodes (madness that way lies)
This is a joke. It's called VW, paying homage to Volkswagens emission test hack that cheated emission tests by making them pass even when they shouldn't have.
Check out https://github.com/paragonie/easydb &lt;- that is what I meant, not `pg_query_params()`.
How does this address any of what I said?
Yeah, this looks very valid! I might take this route indeed, thanks!