BTW: what would your favourite way of communication between loops? I know local(global) variables + queues.
I'm a huge fan of using the CVT protocol for just about anything I can find to use it for: http://www.ni.com/example/30326/en/ Natively, queues or notifiers are probably the safest bet, and I don't use locals unless I absolutey can't think of another way to do something.
Are...are they giant frickin' lasers?
The ones I work on, no. Some of the ones we sell are in the kilowatt range, cost six figures, and are about the size of a washing machine.
I'm in bed on my tablet. Tomorrow i'll write up a vi for you. Since you know how to use a do while loop in another language, you'll be happy to know that while loops in labview are do while loops. Drop one of them down and stop the loop when output equals 5. Use shift registers to store output between loop iterations (google shift registers until I can get to a computer to explain it better). Edit: What version of LabVIEW you using?
Awesome, thank you very much! I was playing around with while loops, but wasn't sure if I needed another loop in the loop I already have. I'll look more into shift registers as well. I am using 2013. 
http://imgur.com/gmt0M8F Check this out. Hopefully it will explain shift registers a bit better. From here you need to replace -1 with .6 and drop a 5 second wait timer in the loop. Add .2 to your shift register value and store that new sum back in the shift register. Compare your shift register to 5, if they're greater than or equal, stop the while loop.
Excellent, this is very helpful. I'll be in the lab tomorrow and give this a try. Thank you! 
What do you mean "rebuild DAQ" and "it gets assigned a new port"? This all depends on how your program/MAX are addressing the Physical Channel. You could be using Global Virtual Channels, You could have Pre-configured tasks, you could have a project channel, etc. Tell me more about your setup.
I've built Golden Search algorithms in Matlab before, never done one in Labview. I will say that the way you built this doesn't really resemble the Golden Search algorithms I did in Matlab. I see that it's a search algorithm but I'm not seeing where the golden part comes in. Not saying it doesn't work, especially because I'm not going through it with a fine tooth comb. Why the use of a single frame flat sequence to introduce the function? Also interesting to use percentage of error instead of a tolerance as your convergence criteria.
That is *a* search function but I'm not sure it's a golden search. I think the definition of a golden search is that it maintains the golden ratio between successful function evaluations which allows it to converge quickly while only using one function evaluation per iteration. That being said, I've only used golden search to find the minimum of a function for a line search, not a specific value of a function. But I would assume that it would be the same except solving for some number instead of solving for 0. My basic understanding of the golden search is it divides the search range into three sections sized by the golden ratio, evaluates at the section boundaries, compares them, then lops one off. So if you take a look at [this image](https://wiki.ece.cmu.edu/ddl/images/Goldensection.png), the top row of variables on the x axis is the first iteration and the second row is the second iteration. Initially the function evaluates the boundary conditions at a and b (these are the extent of the search). Then it evaluates points x1 and x2, which correspond to the golden ratio. I think that would be (a + 0.618 * interval) and (a + 1.618 * interval) but don't quote me on that. This separates the function into 3 segments. If x1 &gt; x2 then the minimum lies in the second two segments, if x1 &lt; x2 then the minimum lies in the first two segments. Based on this, the search can lop off one of the sections, either segment 1 or 3. In the image I linked, x2 &gt; x1. So you can see that the second iteration variables ignore the upper portion of the function. Now this is where it gets efficient: it reuses 3 of the 4 function evaluations from iteration 1. a (lower bound) becomes a' (lower bound). x2 becomes b' (the new upper bound). And x1 becomes x2'. The only function eval to be done is a new x1', once again based on the golden ratio. Because of this the function is converging very quickly (exponentially? I'm not sure) but using only one function eval per iteration. Edit: Looking at your problem some more I'm not sure if this kind of golden search would even be ideal. See golden searches are generally used to find a minimum from optimization. Your method is also converging exponentially with one function eval per iteration. And I'm having a hard time thinking of an easy way to solve a golden search for an f(x) value, not an f'(x) value/minimum. I have faint recollections that Newton-Raphson method is more what you're looking for. But anyway, I'm not sure why your customer requested a golden search or what it's doing so these are all stabs in the dark.
The customer didn't request it, I just thought it was a good fit. Good thing I hacked together such a piss poor version of it or it sounds like it wouldn't have been such a good fit after all. From a high level, the customer's device has a specific output for a given 'N' input. During a test the customer needs to set 'N' to obtain a given output. The relationship between N and the output can be vaguely described by a third order polynomial close to what I have in my .vi and is monotonic. It's essentially flat at low values of N, linearly sloped in the middle and than flat again up top (saturation). That equation will be different between devices under test but the target output will always have an N value somewhere along the linear portion of the graph. Since the equation changes slightly though between devices, it's a guess, measure, and refine problem to set N. Seems like the Newton-Raphson method uses the derivative to try and closely approximate zero. I looked into implementing it, but fell short while attempting it. The route I took seemed much more straightforward given the simplicity of the problem. However, the method I implemented does now seem better than the golden search. The golden method eliminates a third of the set with each iteration while this method eliminates half. Since the relationship between N and the output is monotonic, there's no need to maintain the golden ratio. The hope is to have an initial guess vaguely near the sloped portion of the curve, later iterations refine the search to the sloped only portion. 
Most of the 1D optimization algorithms I'm aware of are all for finding minimums of the function so I'm not surprised that they don't quite translate well to what you're doing. The finer differences and possible pros and cons of your method are well beyond my rudimentary knowledge of optimization and it works well anyway. So it sounds like all this was just a theoretical interlude, though an enjoyable one at that.
This sounds more like a binary search algorithm, where you discard half your potential values each iteration.
Can you share that? I do a lot of embedded development using RIO or PXI-RT and the 785x line. Its always fun to see other people's FPGA code since it can be so wildly different.
Yes my main point was that .. I want to create a program from the scratch without using their blocks for eg creating a VI for Convolution of signal without using the Convolution VI given in signal processing
well then i'll give you the completely useless answer of "the same way you'd do it in any other language". As it is, I'm not aware of any specific practical machine vision literature directed at LV.
Double click on the first or last number in the range to manually enter it. Also, you can right click and select auto fit
Hmmm This didn't seem to do it. It ignores my attempts to update it, also autofit is already selected. Thanks for the reply though
Were you able to figure this out?
I know I'm mad late here, but I'm quite curious given my current situation. At what point did you 'breakway' to form your own thing? Exactly how experienced of a LV dev were you? Did you come from a large/small company with many senior LV (or other focus) experts? Thx.
I'm not exactly sure what your issue is, but I suspect you can solve it with the "DevNames" property node in [this NI Forums thread](http://forums.ni.com/t5/LabVIEW/Command-or-Query-to-list-all-connected-DAQ-devices/td-p/705930).
Google man: http://www.ni.com/white-paper/8224/en/#toc2
First: call NI for these type of issues. I'm 100% sure the AE on the phone deals with these sort of things fairly regularly. Your local NI sales rep can probably answer all the license problem too. Second: I think you need two licenses. One on the dev machine, and one on the deployed machine. You cannot share the licenses. I wouldn't worry about the code not working until you got the license stuff worked out. I bet the two are related.
Pretty much going to agree with PewPew here. I've worked on and deployed many vision apps in the past (currently working on one right now) and the target machine only needs the run time licence to work. You are deploying it as an installer aren't you? And your installer is including the vision components right? Even if your code is flawless, if something is going wrong with the licences it may not run correctly. I would take PewPew's advice and contact NI for assistance. 
I would really suggest you try and get hold of/go on a national instruments labview beginners course. They have excellent training and this starts you off properly. With labview the basics are key as it is too easy to get into habits that Make further development very difficult. 
I second what Hairyned says. Get yourself to the classroom courses run by NI as you get a certificate for each one. I've been through about $30k worth of LabVIEW training and have the paperwork to prove it, which just lets me roll over anyone else in a job interview. In the meantime though the LabVIEW Skills guide is a pretty good place to go: http://www.ni.com/labview/skills-guide/en/ It will quickly direct you toward the LabVIEW Getting Started: http://www.ni.com/gettingstarted/labviewbasics/ Good luck and have fun!
If you haven't found it yet, make sure you check out the tools network for LIFA. (LabVIEW interface for arduino, iirc) there's another I haven't tried yet there too. Also, I highly recommend the classes if you can do them. They help a lot. Also, the forums on NI.com are great, lavag.org is also awesome, but pretty abrasive at times. I recommend lurking first on lavag.
Which university do you go to? Many large schools have on-campus labview classes taught by a labview student ambassador which is a free way to learn the LabVIEW Core 1 and Core 2 material and take the CLAD exam for free. You can also attend the local or online Labview training courses (core 1, 2, 3) for only like 150 bucks a piece, about 10% of industry price. Awesome way to build a great foundation in the material. And if you want to do some super awesome stuff, go grab a myRIO. Its like an arduino on super steroids. It has a ton of inputs and outputs, a high speed processor, and an FPGA onboard. I find theyre even easier to start using than an arduino, and way, way, way more powerful. Not to mention an ME coming out of school who can program an embedded system with an FPGA would be super impressive. 
Looking at the manual for the 4x4 kepad from RS: http://docs-europe.electrocomponents.com/webdocs/106a/0900766b8106a511.pdf It looks like you have a number of pins detailing X1:4 and Y1:4 which corresponds to a table of keypad values (2nd page of manual). I would suggest you perform a DI task on the pins of the keypad and place some logic in your code such that the combination of 2 pins going high will tell you which key is pressed.
It looks like there are 2 varieties of keypad, one with "matrix" layout. For this logic, i would poll all DI simultaneously as a binary array and convert into an integer. Wire this integer into a case structure. If you have the other variety "PC" each contact represents a keypress so you can just wire the DI directly into the case structure or use an event structure to grab the keypress.
Have you ever needed to do a set of changes on multiple files at once? This is the most common use but there are many others. Often a company will take each VI and apply their logo to the icon. Or change all VIs to password protected. Or add a disclaimer comment to the block diagram. Or replace a bunch of controls at once. Quick drop uses lots of scripting too and makes coding faster and easier.
LabVIEW scripting is using LabVIEW to write LabVIEW. Imagine you had something that you needed to do over and over again. For this example, we'll say you need to add an arbitrary number controls to the front panel. The type of each control would be dictated by a string that I input. So if I input "String, String, I32", i would get a VI with two string and one I32 inputs. Scripting allows me to write some labview code to parse the string, then add the controls to the front panel. Now when i type "I32, double, string, double" and run it through my scripting VI, I'd get something with 4 controls on it. This is obviously a contrived example. It gets more complicated very quickly. Project templates, for example, depend heavily on scripting. After the new project is copied, there's usually a bunch of scripting code that renames classes/vis, changes the data type of constants, etc. Quickdrop also relies on it heavily. There are a ton of IDE tools that all require scripting. I'd describe anything but the most trivial scripting task to be advanced labview code. I'm not saying don't look into it, but be prepared to scratch your head a lot. Also, it can get messy super fast. I found this article to be pretty helpful when for scripting style points: http://mooregoodideas.com/vi-scripting-pattern/
here's one example. i recently started a side project that included some experimentation with polymorphic VIs. once i had a few examples made up, i didn't really want to spend the tedious time and effort required to create all the instances of my polymorphic VI. so i created a template VI, then scripted all the difference instances (where basically the differences are in the type of the controls and indicators). this is nice, because if i need to make a design change to my VIs, i make it in my scripting VI. this change then propagates itself since i use this VI to script all the other VIs. that's just one example. basically anytime you can think of some tedious task that would need to happen often or to lots of files, VI scripting should be considered.
I would recommend [Sixclear's YouTube videos](http://www.youtube.com/user/Sixclear/videos?flow=grid&amp;view=0&amp;sort=da) as a good place to start. 50%+ of my day is LabVIEW programming and watching those videos got me started.
Why not use the Modbus library? I don't see NI breaking it anytime soon.
Maybe the register addresses are bad? http://digital.ni.com/public.nsf/allkb/64D9902E8B9AE4AF8625712D0076B463 
Hey guys thanks for the help! I was never able to get it to work with an ASCII message but after some futzing around I have an RTU configuration working. From searching the NI board it seems like I'm not the only one having trouble with ASCII configs.
Thanks! I had heard of the LIFA and I actually just ordered the combined arduino + student LabVIEW kit. thanks for the link to the forum!
That's actually my thought process as well, I figure an ME coming out of school with some programming would be a pretty versatile candidate for many things. It's funny you mention the myRIO, on one of the design teams I'm on we just started using a cRIO which I am trying to learn how to use. I will look into the my myRIO. Thanks!
You cannot load labview RTOS on any platform you want. The OS needs to be compiled to support the architecture, which is really only certain x86 hardware configurations. Both the setups you mentioned are ARM based (this is a different language than x86). There is the labview embedded module for ARM, but it looks like the processor in either of them are not supported. Basically you're out of luck.
You are totally right. And about the LabVIEW for ARM Module - it has been obsoleted and works only with LabVIEW up to 2012 version.
Currently you cannot, but you should just go get a myrio and blow your beaglebone or raspberry pi out of the water :)
Yeah, it seemed pretty pokey. Wouldn't be something I'd recommend looking into it
1) there are online trainings and seminars held by NI AEs and a bunch of prerecorded stuff: http://us.ni.com/academic/training http://www.ni.com/seminars/usa.htm http://sine.ni.com/tacs/app/fp/p/ap/ov/lang/en/pg/1/sn/n5:selfpacedonline/ 2) there are some other "raw" materials online, provided by NI: http://www.ni.com/white-paper/5247/en/ http://www.ni.com/white-paper/5241/en/ 3) there are A LOT of youtube videos about LabVIEW programming **If you can, go for the instructor led training - it really pays off**
If you can swing a myRIO, the plastic comes off and it's not much bigger than a raspi.
theres also a board only version as well. 
Wow these are great! Thanks!!
Wow I didn't realize there was so much out there. Definitely gonna get started on those and take an instructor led training when it's available. Thanks so much!
There's no way to convert C to LabVIEW to my knowledge but I'm pretty sure you can convert LabVIEW to C. As to how to do that you might have to ask Google. Maybe by looking at the converted C code and comparing it to your own C code you can get an idea of why you're having trouble.
I tried converting a known solution, but I can't identify two objects: http://imgur.com/VyZpaL0 and http://imgur.com/0FPzNXU.
Software timed means each update is sent from the pc. There is no buffer. It also means your update rate is not guaranteed on a non-deterministic OS like Windows. Therefore it is mostly a moot point, because if you can't update at a consistent rate, then you can't reliably produce a frequency. BTW if it wasn't software timed, nyquist theorem says it can't be more than half your generation rate. In practice you'd want many more samples, though.
I see, so it updates the output at 150Hz. This means that I can output a sinusoid of maximum 75Hz?
it wouldn't look pretty (it may resemble a square wave more than a sine wave). if you need a nice sine wave at that freq try to get your hands on a usb-621x. if you have the funds and you need a nice arb get an Agilent 33500B (control with USB, GPIB, or LAN). we use those at work and they work great. 
I see, a 75Hz square wave is the best I can get without aliasing. I will have to use the sound card for this project.
I'd just like to add that in my experience Nyquist minimum is pretty much never enough. At around 10x you start to get something usable with interpolation. Mo' points no' betta'.
http://digital.ni.com/public.nsf/allkb/3E3D74E26B8A5B83862575CA0053E4B5?OpenDocument although about acquisition, same ideas apply. You'd be able to create a 75Hz square wave, one high , one low. Half the rate is the minimum to recognize a frequency component. More realistically as mentioned above, this would be more like a 15Hz sine wave. But remember the update rate **is not guaranteed** with software timing, so it is mostly a moot point. I'd recommend a device with hardware timed generation if you need to generate sine waves at certain frequencies.
Yes there are lots of bundled vi's that will be helpful for most of what you are describing. In the Block Diagram under **Programming&gt;&gt;File I/O** you will find a ton of vi's for reading/writing spreadsheet files and text files. Also check out the **Data Communication&gt;&gt;Protocols** here you have ready vi's for ftp, udp, and tcp. It sounds like you might need to manipulate a lot of strings so get familiar with those vi's as well. Good luck!
You don't have a link or whatever to the exact product you have and want to adapt, but a quick google search shows [this](http://www.neoptix.com/t1-sensor.asp) as the most likely sensor you're talking about. At the bottom of that page, it says &gt; Neoptix T1™ optical temperature sensor is designed to perfectly mate with our complete line of signal conditioners including the Nomad handheld, the Reflex™ and the Omniflex™ Both the [Reflex](http://www.neoptix.com/reflex.asp) and the [Omniflex](http://www.neoptix.com/omniflex.asp) claim to have LabVIEW drivers on their respective pages and there's links to them at the bottom of [this page](http://www.neoptix.com/download.asp), though for reasons that I cannot fathom they won't let you get them without a username and password, so you may have to e-mail them. Did you e-mail them and they never got back to you, or what? Also, they have links to user manuals on those pages, even if they don't have premade VIs for your exact product the user manuals may have remote commands you can implement yourself using standard VISA serial communications.
Why switch to Agilent? NI has a lot of hardware, that can generate signals, 6008/9 are the cheapest devices you can get, hence they don't have the greatest parameters. You can find both ARBs: http://sine.ni.com/nips/cds/view/p/lang/en/nid/14838 and DAQ cards that have higher sampling rates: http://sine.ni.com/nips/cds/view/p/lang/en/nid/209147
To get the file: easiest thing is to set up a network folder, if that doesn't work you can use the FTP or http interface with the built in VIs. To open a file use the Read Measurement file VI (as said before) or use the low level file I/O VIs. At this point you have access to the content of the file. To do Excel stuff: from LV 2013 the Write to measurement file VI can natively create an excel file. Or use the Report Generation toolkit, it is an extra add-on.
Any more pictures of the actual encoder? I would try to narrow down who manufactured the unit. The manufacturer probably has a propriety instruction set that they use. I have never used a rotary stage but are you absolutely sure that it is using serial communication? How did you figure the pinout with out the manual?
Yeah, if you can find the manufacturer you may want to ask them and they may be able to help you out. My gut says that you will be issuing serial commands to this thing. 
Thank you for all of your input. Here are some images. Controller http://oi62.tinypic.com/2uo6fjl.jpg | http://oi61.tinypic.com/win3gn.jpg | http://oi58.tinypic.com/11rt995.jpg | http://oi59.tinypic.com/fk4i1h.jpg Rotary Stage: http://oi61.tinypic.com/35mic5h.jpg | http://oi62.tinypic.com/2vv6sua.jpg | http://oi60.tinypic.com/2heefpw.jpg The scanner that this stage came with was from 3d3 which was bought by lmi3d. I'm in the process of seeing if they have drivers or anything else that would make this more manageable. I'll look into your other suggestions and post more as I figure things out. Thank you again
Upon the picture you posted it is a 4 axis WRL TOXOT with USB instead of the parallel port. The manual was not useful regarding the SW. I did some googling at that was not helpful either. The panel looks like that the parallel port was just replaced with an FTDI serial chip and the USB port so the communication is simple serial, as you already figured out. From what you wrote I understand that you have some working software. Do you have a dll or such in the installation folder that you can share? (apart from the ftdi driver) 
That looks like a stepper motor. The four pins are for four (two-by-two) opposing coils that together can be used to rotate the motor in any direction. You can probably drive them with any normal stepper motor driver. See https://en.wikipedia.org/wiki/Stepper_motor for more details.
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**Stepper motor**](https://en.wikipedia.org/wiki/Stepper%20motor): [](#sfw) --- &gt; &gt;A __stepper motor__ (or __step motor__) is a [brushless DC electric motor](https://en.wikipedia.org/wiki/Brushless_DC_electric_motor) that divides a full rotation into a number of equal steps. The motor's position can then be commanded to move and hold at one of these steps without any feedback sensor (an [open-loop controller](https://en.wikipedia.org/wiki/Open-loop_controller)), as long as the motor is carefully sized to the application. &gt;[Switched reluctance motors](https://en.wikipedia.org/wiki/Switched_reluctance_motor) are very large stepping motors with a reduced pole count, and generally are closed-loop commutated. &gt;==== &gt;[**Image**](https://i.imgur.com/lRrfSgU.gif) [^(i)](https://commons.wikimedia.org/wiki/File:StepperMotor.gif) - *Animation of a simplified stepper motor \(unipolar\) __Frame 1:__ The top electromagnet \(1\) is turned on, attracting the nearest teeth of the gear-shaped iron rotor. With the teeth aligned to electromagnet 1, they will be slightly offset from right electromagnet \(2\). __Frame 2:__ The top electromagnet \(1\) is turned off, and the right electromagnet \(2\) is energized, pulling the teeth into alignment with it. This results in a rotation of 3.6° in this example. __Frame 3:__ The bottom electromagnet \(3\) is energized; another 3.6° rotation occurs. __Frame 4:__ The left electromagnet \(4\) is energized, rotating again by 3.6°. When the top electromagnet \(1\) is again enabled, the rotor will have rotated by one tooth position; since there are 25 teeth, it will take 100 steps to make a full rotation in this example.* --- ^Interesting: [^Canon ^EF ^lens ^mount](https://en.wikipedia.org/wiki/Canon_EF_lens_mount) ^| [^Electric ^motor](https://en.wikipedia.org/wiki/Electric_motor) ^| [^Brushless ^DC ^electric ^motor](https://en.wikipedia.org/wiki/Brushless_DC_electric_motor) ^| [^Switched ^reluctance ^motor](https://en.wikipedia.org/wiki/Switched_reluctance_motor) ^Parent ^commenter ^can [^toggle ^NSFW](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+chh6dme) ^or[](#or) [^delete](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+chh6dme)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
For one, your random numbers will be fractional between 0 and 1, so the limit test will always return true, and then the velocity will always be positive. Also there is some fuzziness on your array dimensions and indexing (the Y values going into the graph are 0)
So then that makes my negate function useless? And why are the Y values going into the graph 0? I really appreciate you taking time out to help me.
Im an academic field engineer for NI, and I believe the academic site license of labview should include both IMAQ and Vision development module. Which school are you with (feel free to message me if you dont want to publicly say)? 
Not sure. Is it a compiled vs interpreted issue? It could take Origin some time to read in the numeric values from your array where as LabVIEW would do that before you ran your function.
I'm still unsure as to what you're trying to accomplish, so I'll just say this: put down your controls and diagram down first. Then once that works and gets you what you want, put down indicators. You shouldn't be forced into indicator types (save the graph of course). For example. I can put a indicator for a double down and tie the random number into it, no array needed.
There is something like this: http://www.ni.com/example/30030/en/ It is not supported, but should work. Still, just as kmoz mentioned, you should be able to use VAS under ASL (http://www.ni.com/white-paper/8115/en/).
How do you meet girls?
The 4 ports are for 4 motors, which makes perfect sense in case of CNC control. If I had to design a serial communication protocol a message would contain the port number, direction and number of steps. There might be additional commands to enable/disable the port. To be honest I didn't read through the manual but if can make sure you won't damage the equipment ( legal warning :) ) you can try using the existing SW and the port sniffer to see what changes when using different ports/directions/manoveurs. Just send some logs with explanation of what happened there and I'm sure we will try to help
TLDR the comments: by a quick by glance of your code it looks quite OK. I guess the problem is that the step you add to x and y is so big that it will be coerced all the time. Try using very small values for "a" so it will take some iterations to go out of +/- 10
I would have thought a VDM license includes the VAS license (though the opposite is not true).
We fired up the old software (flexscan3d) but the connection to the rotary table failed. It's looking like a driver/software mismatch, but we can't update flexscan without buying support from them. We're still going back and forth, but it looks like it'll cost quite a bit of money so that path is looking bleak. We tried to get mach3 to drive it, but you need a special card for it to connect to USB. Western-robotics, the people who made the stepper motor controller, was very helpful. They said they'd send us a list of commands (sounds like C# or VBasic). They said that as long as we use a virtual serial port we'd be good with labview. That's the current state of things, still plugging away. Thanks agian
Yeah, you're all right. I had mistyped two numbers in the serial number while activating and assumed it just wasn't included. The IMAQdx drivers' description in the License Manager specify that a separate license is required, reaffirming my assumption. All is well now.
it is almost always down to this. LabVIEW isn't always the easiest to code for mathematical algorithms but they usually run faster as it is compiled to machine code not interpreted. I tested it with a prime number generator. it took only 7 lines in python, but the LabVIEW code blitzed it in execution time.
I guess it all depends on what you need. If you configure a PXI system, you can have other modules along with the ARB in one chassis, with easy synchronization options and a much smaller form factor than a box instrument. Anyway for the application described by the OP, using an ARB is an overkill. Some basic multifunction DAQ with hardware sampled AO would do (like the 621X). And it could be used for many more (simple) applications. 
(q refs are typed.. derp derp a derp)
a queue reference is created using a data type (string/cluster/etc) to define what data type is enqueued and dequeued from the queue. if you wire along queue refs with mismatched datatypes you will end up withis error... 
I use labVIEW with lasers too! In the UK. I'm sure most of us will understand your pain with hardware communications. I found the CLAD courses pretty useful and the dev days I've been to. I'm about to look into formalising my OO knowledge, which I suspect I'll start by going to the CLD intro talk at the next round in autumn. Most of my knowledge has come from on the job too, but classes have really boosted and focused my skill. What is TestStand? 
Learn OO. There are only benefits.
Both labview OOP and TestStand are great skills to pick up. TestStand is pretty much the industry standard for what it does, and its super cool the more you get into it. Would recommend. If youre just looking to build a random skill for fun, Labview FPGA and Vision are pretty sweet. 
Did your company give you the goal of "professional development"? Are they giving you any budget to take courses? I highly recommend two courses: 1) LabVIEW Performance 2) Advanced Architectures in LabVIEW I'd never taken them, only taught them and I learned a lot even after developing for 40 hrs/wk for 1.5 years. LabVIEW performance teaches all the little tricks to eek out the best performance possible. It even ends with a 3-hour exercise where you attempt to speed up a tetris game. By far the most fun course for students. Advanced Architectures well teach you a good portion of all the different architectures out there. You'll laugh at how basic state machines and producer/consumer architectures are compared to some of the ones you learn in that class. Please consider #2 if your company will pay for a class. Also, be aware, National Instruments offers a training package. Your company pays X amount per year and you can take any class they offer as well as take the certification exams for "free". This would be a good option if the other LabVIEW developers you work with need to work on "professional development".
I would recommend taking certifications from NI. The Certified LabVIEW Developer exam is a 4-hour practical test that would really set you apart as knowing your stuff in LabVIEW.
I did ask about taking certification exams. They were of the opinion that they were mostly taken by people who want to do consulting work or get hired, not by people who already have a full-time job. I could do it on my own time and out of my own pocket, or not at all.
They might pay for a class, but they'd be far more likely to want to pay for those self-paced no-instructor classes that are far cheaper and can be done any time during a 6-month period. I might look into the package though, especially if the other people at my company could share it if they wanted. With what I do now we generally don't have any issues with performance... we might be making or testing between 3 and 50 parts a day tops, so shaving a few seconds on a data crunching operation isn't going to help much with what I do now. The advanced architectures one might help though, I didn't consider that one.
I'd have to prove whatever I was spending my time on was worth it to the company. We don't have any FPGAs and almost certainly never will.
As I understand it, TestStand is a ready-made framework that you can use to make test sequences out of any number of modules of code. I think that in addition to using LabVIEW VIs you can also use things like dll calls and EXEs, but I am not sure. It basically simplifies and standardizes a lot of the stuff that goes into testing software... but I have no idea beyond that.
I'm partially concerned that learning OO stuff in LabVIEW will just confuse any other developer in my company who tries to use it, since as far as I know nobody else does...
Some oddball ideas that are not LVOOP or TestStand: 1. Consolidate all your commonly used tools into a VIPM package, and set it up so everyone can use it 2. Set up project and VI templates for your most common tasks: UI's, service VI's, whatever. Include everything you've built for talking to devices. 3. Figure out LV's integration with its built in web server, and build a web service with it 4. Run a class of things you've learned about LV for your other developers 5. Read all of AQ's posts on LAVA 6. Read a good book about programming, but not necessarily about LV (Code Complete, SICP, Intro to Algorithms, K&amp;R, etc) 7. Go to NI week. 8. Dig into scripting. Write something that builds unit/integration/functional tests from documentation. 9. Try to understand your licensing agreement... (okay, this is actually impossible) 10. Set up integration with an Arduino/RasPi/BeagleBone with an eye to using it on another project 11. Learn about websockets and build an LV back end/web front end. Then port it to a mobile device. 12. Figure out how to build a 6, or 8, or 12 headed tester using template instantiation. 13. Write some open-source code. 
Tell them to step up their game. It's not complicated. It takes a couple hours to get the basics
I've been told that I can only spend about 2 hours a week on purely intellectual efforts and that the remainder ought to at least build up towards something more productive. 1. Not a bad idea, one of the other guys did float the idea of trying to make some of the stuff we do more often more into share-able libraries instead of app-specific code 2. I have yet to know what will be most common, there's almost nothing I've had to do more than once at this point 3. Not sure what I'd use it for, but it's an interesting idea. 4. Highly unlikely, the IT staff is slammed with other projects and they have a different boss than I do who hasn't asked them to do some personal development. 5. I have no idea what this is. 6. No idea what book I might read, plus I tend to fall asleep when reading boring textbooks (one reason I stopped CS classes after year 1...) 7. Taking a week off (unless it's vacation time approved in advance) isn't going to happen with my current responsibilities and schedule, let alone them paying to send me to it... not that I don't want to go. 8. I have heard of it but again, not sure what I'd use it for. 9. Heh. 10. We don't have any of those and would need a pretty good reason to get one. Plus we tend to buy enterprise-level hardware and as I understand it those are pretty much strictly consumer-level. 11. This would be a pretty cool idea if I had a reason to do it... not sure what I'd do with it. 12. I'm not sure what this is either... is there something special about those numbers? 13. I don't know that there's much demand for open-source LabVIEW code, nor any idea what I could make that would also be helpful to my company and that they'd give me permission to release. So, 3 things that sound interesting and a few more that just confuse me...
Number 5: LAVA is lavag.org, the forum where a lot of professional LV programmers post. AQ is the user Aristos Queue, aka Stephen Mercer, a member of the LV dev team. He's worth reading. Number 12: Nothing special about the numbers. A test station with one set of hardware that can simultaneously test many parts ("multi-headed") is an interesting challenge, but probably not what you would need in a low volume shop. 
Labview is not the right tool for this. It is fairly hard to inject inputs (it requires calling a lot of windows dlls). This sounds like a common problem. I'd do some googling and see if someone has done this already. I wouldn't be surprised if the mfg had some driver settings to support this. 
Ok, I was just wondering since it's fairly easy to call inputs. I've done a bit of googling but since I'm new to programming for a windows os and haven't done much scripting, it's somewhat difficult. Do you know where I could start from here? Also the manufacturer is a bit unhelpful in this dept. Hence the query.
In LabVIEW? I'd argue :)
I've seen a lot of people who also thought that Core 1 and Core 2 were too easy for them and then were surprised how much stuff it could offer them. My suggestion is to contact your local NI office and tell them exactly what you wrote here. Sending code for review might also be a good thing to do. People that learned LabVIEW themselves often make basic mistakes, like overusing local variables.
Yeah. Also, there is an instrument driver for 2400 to use in LabVIEW: http://sine.ni.com/apps/utf8/niid_web_display.model_page?p_model_id=1583
Before you get into OO within LV, i would really recommend reading up on the benefits and drawbacks. I think its a good thing to learn, but there are a lot of issues with classes in LV and it can be a turn off in a lot of situtations.
I'm familiar enough with softmotion to know that I hate it. That being said, I'm pretty sure this wont require any FPGA code with softmotion. You should be able to setup the CRIO in scan mode, then configure your axis is softmotion, then setup a coordinate space that uses the two axis. Now you're be able to issue high level commands like "move to position X,Y" and softmotion will figure out how each motor needs to move to get there. I've never actually done this though, so I may be simplifying it. Not much info about this on the interwebs, so if I ran into trouble doing this I'd call NI for support.
Okay, I'll look into this path. Thanks for your input!
The simplest way would be have a (or even two separate) parallel while loop(s) with a copy of that VI in Stepper1.VI and Stepper2.VI. Make sure any of the panel controls you were using before are wired through to front panel elements on the main VI. So now your VI has twice as many front panel elements, one for each controller. NOTE: This is terrible practice. FPGAs use more resources the more front panel elements you have. In principle you should create have a separate VI on the host computer which sends data to the FPGA via a FIFO. * Host VI interleaves X &amp; Y data -&gt; Writes to FIFO * FPGA constantly reads FIFO, when it doesn't time out it takes the data and splits it into X &amp; Y data. * The FPGA performs the steps (can then also communicate back to host that it is finished). It doesn't sound like you're especially competent in Labview so I would work on the basics. Make some programs which use SubVIs, and get used to wiring the terminals up, there should be plenty of tutorials for this sort of stuff.
For anyone who is interested: http://vi-lib.com/
Go and click [this button](http://imgur.com/s1GDidE) here in your block diagram. It will clean your code so we can properly see what wires are going where. Also on your main VI can you find out what nodes the 0(int) and two 20's(dbl) are feeding in to? I'm going to assume that Seconds to Wait (spin.vi) is the 0 int from main.vi, acceleration and deceleration are both the 20 dbls from main.vi. Main.vi shows us that: Distance = Time(min) * Speed(rpm) * 360 Spin.vi shows us that: ((Distance/(speed*360/60)) + (speed/accel) + (speed/deccel))*1000 + ((SecondsToWait*1000)+200) **note that this wait time is in milliseconds.* Going with my assumptions: ((Distance/(speed*360/60)) + (speed/20) + (speed/20))*1000 + (200) So the equation above should be what you need to determine how long spin.vi will run for. Based on the way that the math is being done I am assuming that the acceleration and deceleration is 20ms. 
I've never done that personally so I have no idea what your problem could be. I would recommend calling NI support for that one.
I believe the message is asking for the location of the driver disks (or files) for the install, not the location you are trying to install them too.
The problem with source control in LabVIEW is VIs are binary files, making merging and differencing tough. NI offers LVMerge and LVDiff. I recommend finding a source control that allows you to use different tools for different file extensions. Perforce is what we use at work and offers this functionality. I haved not done the research on others though.
I'm using SVN at work, but there isn't much merging in what I do. I usually create a branch and just work with it. Since I need separate code for each release I use there are limits to merging anyway. Another department at work uses got, and they swear by it. I think almost anything will work as long as you use it. The biggest thing is to separate compiled code.
NI uses perforce internally, but I also use git sometimes for personal LabVIEW projects. Source: I work on the LabVIEW team. 
Congratulation!
Just click decline support unless you have drivers to install
High-5 buddy!
That's what I did. Just haven't deleted the post yet.
Thank you everyone for your responses. Seems like I'm going to take a look at perforce then.
First off, why not do the analysis in LabVIEW? Second, have you tried using the mathscript node to save the whole data saving/formatting steps and bring the processing in line? Often can save lots a programming and execution time that way.
Right, this is what I would do. However, the intended userbase is not for it. It is a research institution and even allowing me to re-work their spaghetti was apparently a difficult decision. The code I am replacing was written in 1993... you get the point. Additionally, the issue of pulling the processing in-line is that it changes quite often with each new experiment (while the data output does not). This, paired with the use of MATLAB toolboxes, makes a MATLAB Node necessary. As far as I can see, the MATLAB Node just eats errors and is a huge pain to debug, so if they will be developing data processing code in MATLAB, using MATLAB toolboxes (which are not available in the MathScript node), it seems to me easier for it to all be done in MATLAB. I understand a simple MATLAB import script for a binary file would work, but I have been asked to try to make this as easy as possible to share with colleagues, etc. So a file that is MATLAB-importable while being structured and named correctly is necessary. If there is no easy way to do this, however, the MATLAB import script may be the best way to go.
Tdms files have a places (multiple levels at that) for metadata and I know there's a MATLAB plugin for it. Might be an easy way to get it working. And you can use diadem when you want to play with data which is super awesome :)
Im an NI employee, so I dont actually process that much data myself, but Ive taught the DIAdem class a few times and used to support the product. I love DIAdem, and pretty much everyone that uses it seems to as well. Its one of those tools that few people pick up because they "already know excel" but once you learn it a bit (and the learning curve is very reasonable), its AMAZING. So killer at doing lots of analysis, visualizing, and reporting it super fast. 
I see. Well, thanks a ton for the information and advice! If TDMS ends up working out and being approved by the clients, I'll drop back here to thank you again :)
You could always write metadata to a separate plain text file and parse this into a struct in MATLAB. Writing it in an easily readable format like JSON might make things quicker. The bonus here is that you can read the metadata with any text editor. 
Yeah, with out knowing what the data actually looks like. The most versatile solution would be to write the data to a .dat file using the JSON vi's and then use a toolbox like this (http://www.mathworks.com/matlabcentral/fileexchange/33381-jsonlab--a-toolbox-to-encode-decode-json-files-in-matlab-octave) to parse the data in the MATLAB UI.
A little late in replying to this, but Amazon Cloud Services offers Perforce servers if you don't have the means to host your own.
Read the following document and then the tutorial linked under 'Related Links' and you should be all set: http://digital.ni.com/public.nsf/allkb/75CF478A58545DFC86256FCC006E25A2
Congrats!
Check out: http://sine.ni.com/nips/cds/view/p/lang/en/nid/211524 It provides LabVIEW project integration for TSVN features and is relatively easy to use. [Edit] TSVN adds a right-click TSVN menu from the LabVIEW project with icon overlays to indicate status, and ties into the existing LabVIEW Merge and Diff tools for revision control. Perforce, as mentioned above, also integrates with LabVIEW natively and there are others: http://digital.ni.com/public.nsf/websearch/26EC5904169430CE8625706E00743997?OpenDocument
RDR350Z already mentioned the white paper describing simulating instruments in MAX. If you find you can't simulate the instrument for whatever reason, or it's simulated value isn't really what you're looking for, you can always wrap a Conditional Disable block around your DAQmx code, create a project property called 'SIMULATED' and make the simulated case have a waveform generator .vi in it. That's typically what we use when we're prototyping code, especially if we're gonna put it in front of a customer and they want to see how things are going to work.
Or you can create your own class to use dynamic dispatch VIs so you can 1. use multiple típe of HW 2. do simulation with data you can determine http://zone.ni.com/reference/en-XX/help/371361J-01/lvconcepts/creating_classes/
You can just use the normal greater than and less than primitives. Wire the array of numbers into the top, and the mean value in the bottom. You will get an array of booleans out. Now you can use the "bool to (0-1)" primitive in the Boolean pallet. Finally, you can use the "add array elements" in the numeric pallet
Thanks! I was really over complicating things. What I had before did not involve "bool to (0-1)" but instead an initialized variable set to 0, and an increment +1 if &gt;. Looks like I'm up and running! Thanks!
I'd just chop the first 549 points off the array using the delete from array function
Could you post a screen shot of your code. 
Soundalike you're new to DAQmx. Checkout the express VIs in the DAQmx pallet, they should get you started. Once you figure out how to read from the hardware you should be able to replace the sim code fairly easily
This could be as simple as changing the value of the signal channel constant to the channel you want to use. I concur with the posters below in that you will probably need post a screenshot of your code.
You have a couple of options, but the first thing I'd do is note the parameters of the simulated signals. That way you can set the sampling parameters on either the DAQmx Express or low-level VIs, whichever you end up using. Open up Example Finder and search for examples on analog acquisition, there are tons of them. You can just copy and paste code from one of them into your code. Do you have the scaling set up in MAX already? If it's part of the simulated signals then you'll need to create the scale(s) and apply it/them to your load cell channels. 
These would be the steps you would want to follow for doing this: 1. http://imgur.com/9nZwBzg 2. Drop that VI into your block diagram 3. http://imgur.com/jzHx7g4 - select the type of input you wish to acquire, under analog input there is even an option for strain which should work for your load cell. 5. Select sampling rates, excitation voltage, etc 6. Done!
No problem. Any way you can get a student version?
Here is a student version of labview plus an Arduino for $50. https://www.sparkfun.com/products/11225 You can use the arduino for real data acquisition and output. You can also learn labview and the arduino idk, which is C or something similar. I have also found that this is a good reference book even though it is for version 8. http://www.amazon.com/LabVIEW-Everyone-Graphical-Programming-Edition/dp/0131856723/ref=sr_1_2?ie=UTF8&amp;qid=1405129568&amp;sr=8-2&amp;keywords=jim+kring Let me know if you need any ideas for projects!
Actually the student version evaluation is six months.
Thanks! I can't wait to get started.
I'm not enrolled in any school so it's doubtful. But if I'm lucky I can convince my work to invest in the software.
Awesome find! LV for Everyone is a great book, forgot to recommend that myself.
Thank you, I had no idea NI offer these seminars. Seems quite pricey but I am definitely interested, I will try to convince my work. Thanks again.
virtualbox is free
I'm not sure if I fully understand your problem, but... You have 300 functions, all output a Boolean right? Array all of them together. Now you have a Boolean array of all the results, one of which will be T. Now use the search 1D array function (in the arrays pallet) with a T wired into the Element input. This function will output the array index that lines up with the element (this is a number between 0-299) That answers your question I think. Now my question: what are these 300 functions and how are you using them? There has to be a better way of doing this
It's a controller for a biological system. The researcher I'm working with swears by it. Are there ways to make the code look cleaner, like put "black boxes" around the logic rules?
Btw, this is much simpler, for which I thank you! Didn't consider using an array; too much in a VHDL mindset at the moment.
On the pallets on the front panel there are decorations. You can drop these on the block diagram as well.
Vhdl is a data flow language just like labview. Conceptually there are a lot of similarities
Fortunately so! I was taught (as an EE, I guess) that using arrays wasn't something you should always rely on, due to memory constraints. But it's nice to have that available for sure.
You should pretty much always rely on arrays. If you need to store 300 or something, it's going to take up 300 chunks of memory whether you store it an array or 300 local variables. I guarantee your code will be way more readable AND memory efficient. High level languages like labview you definitely trade memory efficiency and a bit of performance for coding ease. With modern hardware though it's definitely worth the trade off. As you get closer to FPGA it starts to matter more and more (obviously this is getting into the realm VHDL). I'd still be willing to bet in all but the most specialized cases, a well thought out array based solution will be better (both in performance and maintainability) than a non-array based one.
Can you give an example, maybe just one of your 300 cases? It's difficult to understand what exactly you are trying to do but I'm sure there is a simple way to implement your code. 
If you really want to stick to your original idea I would build an array of the boolean values, convert them to x bit long integers and put the case structure on the integer value. If that doesn't work then I would create a VI that accepts boolean values and can dynamically call VIs so you could create a list of VIs and call ones upon some rules. But to be honest I would just scrap the original idea and use the Fuzzy Designer in LabVIEW because at the end of the day that's what you are trying to do! :)
Trust me, I've had the same conversation with the guy about simplifying the code. Firstly, it seems that each block is actually mutually exclusive of all the others. Secondly, because its applications are medical, using shortcuts and simplifications may not be the best idea. I'm definitely trying to optimize the code, because it's probably going to live on an FPGA in the near future, but for now, pewpew's solution seems like the best one available. The cases work something like this: First, we read the current and two previous values from a sensor. You compute a differential between pairs of them, and if the value of the differential is below some threshold value, and the current flow of the four controller substances (enzymes and the like) matches certain criteria, we then adjust the flow rates for the next time step. Time steps are going to be between 5 and 10 minutes (based on a tertiary controller). The researcher I'm working with has been observing the same system for over 25 years, and has basically come up with these events based on his knowledge of the system. Based on other simulations, and comparisons to the controllers used by other groups in the field, his definitely has the best time controlling it. Pretty amazing for a fairly simple fuzzy logic system.
That sounds like exactly what we'd need to simplify this! Does it let you code in a C-like fashion?
https://drive.google.com/file/d/0B3prfgo8KEmYUDJKdmMyN3E4WVk/edit?usp=sharing some help on how-to start. I know that if someone has been programming in C then the first thing is to try to do the same stuff in LabVIEW. But keep in mind that LabVIEW has been specificly designed to do these measurements/computing tasks so with some paradigm-shift there is always a better way to do in LV! :)
Agreed. Using a case structure that has 300 different options seems like the best way to implement this. 
http://sine.ni.com/nips/cds/view/p/lang/hu/nid/209054 this is the fuzzy toolkit, you can try it as an evaluation version. In the Fuzzy System Designer you can create your own membership functions, rules and there is a test view to pick any 2 inputs and 1 output to see what the output mesh is
Similarly to your top two responses, you can build a Boolean array and then do a "Search 1D Array" directly after. You would search for a true value. If there is a possibility that all values in the array can be False (at any time), just make the nth element T and use that case as the Default case.
Thanks for the advice, the final product will have a constant video stream so running out of frames won't be an issue (it also won't be auto-indexed for that). It is comparing the current and previous through the use of shift registers. The one I setup follows the logic of the original as closely as possible. Since all of the images being fed in are the exact same dimensions the suggestion you had about setting image size shouldn't be applicable but I'll check just to make sure. The one semi-useful tip I found online says this error could be caused by images of different bit depth. I don't know why that would be an issue since I'm casting the images to the same bit depth. I'll let you know what I find out later when I can get back to this. 
The image size is your problem. Your image labeled previous is size [0,0] in the code you linked.
Thanks for the suggestions! I have already implemented suggestions 1-3 and if I have time I will try to get 4 and 5 changed as well. Like I said, I only have a couple of moths of LabVIEW under my belt so thanks!
Feel free to post more pictures as you go. I'd be happy to keep helping. :)
Wow. I could have. Sorry, new to labView and running a test completely slipped my mind. Thank you!
It's normal to not think of testing something. That's why I suggested it :)
If the word cluster comes up ever. Make it a type def. You can drop the type def constant and use a bundle to initialize. Plus when you need to add to the cluster, just modify the type def and you don't have to search for all the broken wires. Type defs will save you in the future. 
Ah so this is the struct/record corollary for labview?
Struct, yes. Record, I guess so... Haven't come across those yet. 
Great idea btw. Making changes in the future is going to be a breeze.
Awesome project! Where are you set to launch from? I have to second all of u/SillySnake , some good points there. Definite candidate for http://www.ni.com/student-design/
Sorry for the delay, I get assigned to multiple projects at once a lot. [A screen shot of the load cell portion of the code has been provided here](http://imgur.com/3WanIG4). I hope this helps. Thank you for taking the time to reply.
I agree with our good friend PewPew, not the best case for a while loop. However, if you want all shift registers initialized to first reading then make your first read outside the loop, wire that to all 3 shift registers and then make all subsequent reads within the loop. This is how it's done (albeit using a for loop) in the weather station project in the LabVIEW Core 1 class when you add functionality to use a running average of temperature. EDIT: It's fine to replicate "the sensor" by which you mean the code block which reads from the DAQ card. If you are using the "DAQ Assistant" express VI just copy and paste the one you have so it's outside the loop.
Can you do that using the same sensor?
Also, is there any way to prevent the rollover?
Ah, I think I got it: http://imgur.com/NpxZ2vT Hopefully this'll do the trick. edit: works just like I want it to now! Thanks for the advice.
Ah, that's the one! I didn't see this until after I responded. As long as that DBL control gets updated by the sensor input before the loop runs, you should be fine. If not, watch your race conditions.
Never even considered the race conditions! Hopefully the slow clock will prevent those.
There is actually a way easier way to do this while not worrying about loops rolling over. Simply initialize the 3 shift registers to the same value like here: http://imgur.com/cYhM6EF
What OS? The windows snippet tool captures an image by dragging a box to select and area.
Have you ever played around with a Teensy? It's pretty impressive, but is only suitable for select applications.
Just to clarify: the BNC 2120 is just a terminal block, the DAQ card's I/O can be accessed by other type of terminal blocks as well. The BNC 2120 offers a simplified connection with the BNC connectors, however you can reach the DIO lines in the bottom right corner (black connector) via screw terminals. The User-Defined Signal connector can drive the signal from one of the screw terminals to a BNC connector, in case it is easier connect like that. As for the programming the DAQmx driver handles the DAQ card so you can: 1. go to help/find examples/hardware input and output/DAQmx/Digital Output/Digital SW timed output. It shows you how to use the DAQmx VIs. 2. You can use the DAQ Assistant (Ctrl+Space to search for it) and it will be straightforward what to do. Plus there are some tutorials: http://www.ni.com/tutorial/52006/en/ In case you are not familiar with LabVIEW I suggest you have a look at http://www.ni.com/gettingstarted/labviewbasics/ And btw NI has framegrabber cards with triggering. I don't know how deterministic your app should be and what will be the trigger condition but in case you use SW timing the trigger will have a jitter and a significant propagation time 
If you open a reference to the VI you want the screenshot of then you can use the Block Diagram/Get Image Scaled invoke node to get the image info. After that you can save it to a jpeg or anything you need. This method is very efficient in case you have many VIs and the screenshots have to be extracted for your thesis or any documentation
You want easy? :) CTRL+A CTRL+C on block diagram CTRL+V on Gimp, or even paint. And resize to your heart content. Gimp is better for resizing of course. 
Many types you automatically generate a trigger when some conditions are met. For example you take a highspeed data acquisition and when some edge/pattern is found you generate a pulse to take a picture or start an other acquisition. NI has many technologies Tclk, RTSI, RIO, etc.. to do that in a very fast and deterministic way. In your case -as you manually press the button - the effect of the lag is most probably not even noticable. Handling the UI, communicating with the DAQ card, signal propagation can take up to approx. something in the x1 ms range depending on what is happening in your computer at the moment you press the button. In worst case it might go up to 10-100ms so if the phenomena you are trying to catch with your cam is shorter then that you might miss it.
I'll check it out, thanks. 
Don't use hopefully. Use dataflow to guarantee that it'll happen in the right order.
Followup showing all the examples: http://imgur.com/25QDJe8 Rollover is almost never something that you actually have to worry about. If your loop was running once every 5 min, it would roll over every 40k years. If it was running every second, it would roll over every 132 years.
CTRL + Shift + N And just a note to everyone reading: You should be able to keep all over your code on 1 screen. If you need to overflow, go in one direction (horizontal or vertical). When you starting thinking "man, i wish i could zoom out" it means you need more subVIs and modularization in your code.
The way youre phrasing everything makes me think youre trying to add in something way more complicated than necessary for this functionality. Just take a data point outside the loop to initialize, like this: http://imgur.com/3ytlJQS
wow, are you researching some heart surgery? There are two ways to deal with m code in LabVIEW. One way is to interface with matlab. The other way is - if you have the LV matlab RT module is to natively run m code in LV. This way you don't even need a matlab on the PC so no matlab license, just LV. As for converting your m code to LV code: IMAQ (or IMAQdx recently) offers a platform to grab data from imaging devices and it also has some basic picture control. The full vision capability is in the Vision module. You can modify the picture, run filters, measure distances (I have some customers who are measuring the diameter of veins with this module). However the bad news is that the Vision module is not doing very well in regards of transparency. There are some examples online for that: https://decibel.ni.com/content/docs/DOC-24002 On the other hand I checked your m code and fortunately it is not very complicated, so can be done in LV in no time with basic functions (any labview can open a png file) So to sum up: If you want to further develop your application in the future for machine vision and advanced signal processing then download the vision module and do the stuff with it. If your goal is just to do the same in LV what you did in matlab then you do not need the vision module. Just use basic functions (working on 2D arrays) Here is a little hint how to start: http://digital.ni.com/public.nsf/allkb/02971A30F5D8FC6986256A0A004F11A0
Yea that's wayyy simpler than my configuration. Thanks for the tip. Also, how do you add a shift register that has no shift-in on the right side of the loop?
Hi, Certified LabVIEW Developer here giving you my two cents: * **What are the advantages of LabView and what are its limitations?** The benefits of LabVIEW is its ease of use. Without much experience or training you can easily create a graphical measurement and control system. I can whip up a simple graphical program to measure, log and display voltage from a National Instruments device in a few minutes. The drawbacks tend to be the cost and the over head. As you may already know the software is licensed, which may not be an issue for a company because you will make it back in labor saved but it can scare off students or hobbyists who have to work within tight budgets. Overhead is another problem, you need to have the run time engine installed on the computer where the code will be deployed. * **Can a C++ code be executed in conjunction with LabView programs?** LabVIEW has VIs (similar to C subroutines) that can access DLLs. So as long as you can generate your code into DLLs you should be fine. You also mentioned that you know Matlab, LabVIEW has nodes (control sections of code) for executing Matlab scripts. * **What equipment or hardware do I need to purchase to use LabView with this set up?** Without knowing all the details of your project, I don't think you need any, at least no additional hardware to interface to the MCS. According to the MCS website you get a simple graphical tool, DLL and a LabVIEW driver (which is probably just a wrapper for the DLL). Feel free to give me more details of your project and I may be able to better answer your questions on what hardware you may need.
This is one of those things that you have to get stuck on at least once while you're learning LabVIEW before it sinks in.
The controller you've linked to has a [LabVIEW wrapper for its DLLs](http://www.smaract.de/index.php/products/controlsystems/mcs/mcsoftware). Programming a basic control panel for the controller should be pretty quick since you don't have to write the LabVIEW wrapper. But, once you get into something more complex, like experiment automation, it really depends on what you are comfortable with. The controller has DLLs so if you are more comfortable with C++ as your language of choice it'll work just as well as LabVIEW. LabVIEW might be worthwhile if you are using other National Instruments products in conjunction with this company's products; also, the development time for a useful GUI is almost zero.
You can take the bottom of a shift register and drag it down to initialize more iterations back :)
Thank you for your help! I am lucky that the university has the license for LabVIEW so no personal cost to me. Am I correct in thinking the run time engine is a free download off NI (or included with license)? Not sure what you meant by overhead. How limiting is LabVIEW's library of functions/flowchart blocks? I think my adviser's concern is that LabVIEW will limit the frequencies supplied to the actuators or the speed at which the program runs. Can I write user code as in some flowchart-style programs? I need to operate three actuators independently for XYZ motion and then two simultaneously to achieve rotation of an object. Feeling like LabVIEW is a no-brainer compared to learning C/C++ for a thesis due in November '14! Also, LabVIEW is offering a free workshop for us next month-- score. Going to bed but thank you for the response :)
Cool thanks!
Nice!
It's important to point out that the hardware linked in the original post is designed to have a PID loop that is controlled by sensors purchased as an option with the equipment. It's very likely the PID mechanism is built into the hardware and is faster than a software PID implementation.
To be honest I didn't go through every function but you can find the supported functions here: http://zone.ni.com/reference/en-XX/help/373123C-01/
To put a few of these comments into more plain language: Labview is a very powerful graphical programming language which is great at system design and integrating things together, particularly for engineering applications. Its awesome at interfacing to hardware of nearly every variety, its got huge analysis libraries, you can make really complex control systems, and its a relatively fast language to boot. Sure, LabVIEW is great for setting up a closed loop linear actuator system, but where it really shines is when you need to integrate that into a much larger automated system with data acquisition, user interfaces, lots of instruments, etc. Which university are you at? We have academic field engineers across the US (like myself) which might be able to help point you in the right direction on your project. Feel free to PM me it if you dont want it public. 
I'm creating a controller for a fairly complex biological system. It's controlled by two variables, generated within the main VI. Two controllers are multiplicative (Xnew = N`*`Xold, etc), and the third is additive. Controls are based on the current and two previous sensor values taken. The controllers are really glorified case structures (one has just over 200 cases, the second has around 30, and the third has about 40). The way I'm implementing it, a cluster of pertinent data is fed into a subVI, which then has multiple other subVIs that get the cluster and output booleans. The booleans are arrayed, searched for a TRUE, and the index is sent "up" to the main VI. This index serves as the selector for the case structure. The relevant values are sent into the case structure, modified, sent out, and then reclustered and used in the next controller. Once all three controllers have modified the values, they are sent out to the device used to control the system. Because of the sheer number of rules in the first controller, I've taken to using subVI's just to make an array of the booleans, which will then be concatenated in the penultimate VI before being searched. Here's a picture of what I mean: http://imgur.com/a/cfLhL Each subVI has an icon according to its rule number. I did the offset in the second picture at the request of the researcher I'm working with, to avoid confusion with rule numbers. edit: One more detail, which is pretty key, is that the clock speed changes based on another block of code that also uses the same cluster.
Global variables don't make things more modular. They make things more coupled (the opposite of what you want). I always discourage people from using global variables. They lead to bad design.
Look into object oriented programming. Your many rule VIs could be made into various implementations of a Rule interface (a base class). Instead of having a giant diagram with every rule VI on it you could instead put an instance of each rule into an array and run through each rule in a loop. You can even dynamically populate the rule array by reading the list of rules from a file or by just loading every class in a given directory (I.e., plugins!). OOP will take your software design to the next level. 
Some useful links: [Rules to Wire By](http://www.ni.com/white-paper/5560/en/) [Top 5 LabVIEw Rookie Mistakes](http://www.ni.com/newsletter/51735/en/#toc3)
LabVIEW can do that?!
Yes! Since 8.2, which was released in 2006. 
Wow. That almost feels like cheating.
I personally stopped using the clean up button. You can muck with teh algorithm via the LabVIEW options as to how is places and spaces but I never got into that. The LabVIEW Style Guide is pretty good. It's also a big part of CLD exam. There's a [checklist](http://http://zone.ni.com/reference/en-XX/help/371361K-01/lvdevconcepts/checklist/) in the help file but it's pretty long. VI Analyser checks the same things automatically for you. It's under *Tools &gt;&gt; VI Analyser* if you have it installed. Other top-of-mind tips: No block diagram bigger than 1280x768 and keep to the standard 4-2-2-4 connector pane - if you have more wires than that there's too much functionality in one subVI or start using clusters. Also, use strings and variants to pass data between subVIs and in ques etc. (a LabVIEW Core 3 tip that one)
Certified LabVIEW Developer and academic researcher here. LabVIEW looks like your fastest option, but your adviser's concern about longevity is completely justified. I am endlessly fighting with different operating systems, bit widths, and LV versions. Not fun. Also not your problem once you've gone, though, so I wouldn't worry about it. Still, if you want to make a habit out of programming, learn C++ (hardcore) or python (easy). 
Even in that limited usage you are still introducing coupling between your initialization of the globals and their usage. The result is that you can't do something like run that same code twice in parallel. Suppose this code is for initializing a UI for some top level window in your app. Maybe for now you only need to have one of those windows, but what if in the future you need to have two windows open at once doing the same thing? Maybe your requirements change and now a user wants to manage multiple processes, each with their own UI. Your globals are stopping you from doing that. A better approach is to use a cluster (or, better, a class) representing the configuration and pass that to the VI as an input. If you find that you have too many inputs then maybe what you need is to have a class with some state, and those settings are all stored in the state of the class (I.e., the private data control). 
Sigh, that's the endless question, isn't it? Do I make it easy for me now and hard for future me, or hard now and easy for the next guy. If you want the easy now, do what you said: build a cluster and save everything as a XML or something. If you want easy later, make a config file (ini file). On the block diagram &gt; File IO &gt; Config File VIs. EDIT: If you want a very quick and dirty and very wrong way to simply restart something, you can just Edit&gt;Make Current Values Default. Then save. You can do this programmatically too, but it's very bad and you shouldn't.
I'm pretty sure you can't do the "Default Values" thing programmatically, you can only reinitialize to the default, but not change from what the build has.
Clusters are great and probably what you want. To counteract the problem with compatablity, just make your cluster of data type-defined and save it. If you update the data type in the future, it will be updated everywhere. I've XML multiple applications where I write a cluster to a config file using the XML VI's and I find it very easy to alter and scale. Look up the OpenG cluster config file VI's as well because they make it very easy. If you want more detailed help, you can include a code snippet for us to look at.
you're both right; *Reinitialize to default value* is available at both Control and VI level through invoke nodes (methods). *Make current value default* is only available at VI level (as far as I'm aware), so you can only do it for everything on the front panel at once.
You can, by running a second VI. The second VI quits the first one, makes current values default and then restarts the first. http://i.imgur.com/KbGIDx7.png This is a very stupid thing to do, and won't work in a build. Please, nobody do this.
Congrats! Diving in is the best way to learn LV.
That could be any number of things. Offhand, it sounds like you have the wrong COM port selected. &gt;Labview recognizes the instrument, and seems to work How do you figure that? Can you communicate with the instrument with hyperterminal?
Use the VISA Interactive Control to see if you can send a basic *IDN?\n command to your device and get a response. This eliminates the LabVIEW API, etc. If you can't get it to respond, then you have a basic communication error.
It returns the right identity in the programs. I can make it beep as well, so it is not a communication error 
Can you successfully send the same *IDN?\n through the Basic serial write and read example as well? Can you use the examples in the Keithley 2400 driver successfully?
Sounds like a read timeout. Can you open the Initialize function? It may be that you've got some sort of configuration error that's causing the Keithley to respond with a number of bytes that's lower than the Init function is trying to read so it's timing out.
use NI MAX and open up a session with it- make sure you select the enable termination char on reads, and make sure the term char defined is the same one the keithly is using.... 
&gt; One issue I've considered with the Cluster method is losing compatibility if I add more stored variables in future revisions. If you take the time to make your config data cluster into a class, LabVIEW automatically stores the version of the class in any save operation (I'm assuming you're going to save it as XML or somesuch). If you add a new variable to the class data later, and load an old file, it'll load with any new variable set to the default for that data type. In the coding I've done so far, it's the main reason to use a class that I've found.
I am able to write to the instrument, but am unable to read 
I am able to write to the instrument in MAX, but am unable to query the instrument, I get a Read operation error 
I opened MAX and am able to write a command but am unable to read the instrument 
OK, things are a bit muddy here. You say you can 'make it beep', how exactly are you doing that? You say you can 'write to the instrument in MAX', are you talking about just sending the *IDN? command? I assume this is the case, and you say it returns the correct identity, so can you confirm that sending the *IDN? command through the VISA Interactive Control panel returns the SMU's info? Also, can you tell us what LV version you're using and provide a link to the Keithley driver you're using?
You're not giving a whole lot of information in your responses, just repeating the same thing. I think everyone gets that you can write, but hear nothing in response. Have you been able to query in Hyperterminal? Are you sure the device is even sending anything? Have you tried the driver [HERE](http://sine.ni.com/apps/utf8/niid_web_display.download_page?p_id_guid=25B255F3AA83660EE0440003BA7CCD71)? MAX is usually the key to figuring stuff like this out. Use it. Embrace it. Edit: formatting
I have been using max, Check the post, I figured it out 
According to his edit, this was the problem, right? Some people, man, some people...
Can you supply a snippet of your code currently? If you can split it up in the two different processes that you want to be running separately, you probably want to just put them in parallel while loops. Otherwise you may want to look in to asynchronous calling. Most likely parallel while loops will do what you want.
Do you mean my Arduino code or the Labview VI?
first thought is you could make the vi reentrant and launch it via reference with an asyncronous call... I dont know what your subVI looks like but if I'm interpreting what you're talking about correctly, these two options might help... or they might create more issues for you haha
LabVIEW. Even a picture would help. Do you have a read from serial port with no time out? If so it will wait until the other board sends it something.
Yup, what SillySnake said. His timeout option could allow you not to change anything in the code and just keep checking for new information from the Arduino every iteration, but not wait around.
I've never even tried that till now... That's hilarious... it would make my fucking week to find some memes like that in someone elses code haha... have an upboat!
you might want to provide a lil more info- Is all you need labview to do is provide a 5v ttl output at the push of a button on screen? or does it need to manage the image from the camera as well? 
Yes... but you probably shouldn't... (we got a negative Nancy over here!) Though I do find it useful for putting the equation I'm reproducing in situ.
Yeah, I deleted it after taking the screenshot, no way was I committing that to source control. It also more than doubled the VI size on disk. On the other hand, if I ever did LabVIEW for purely personal use... it could happen.
Tricky stuff. I'm guessing the blocking comes from your query architecture. I'm guessing you send a command, wait for it to come back, and then go about your day. The problem with this is that A) Your thread will be hung waiting for a response B) You want to make sure that query 1 doesn't throw out the data for a query 2 that returned faster. The simplest architecture I can think of is just a while loop with an event structure inside. You initiate commands by manipulating front panel controls. The commands add a new element to an array of "commands" (represented by a cluster or an object) stored on a shift register on you while loop. In the timeout case (5 ms timeout), you do a short 100 ms serial port read. If you see a response message show up, search your array for the command ID to find where your command was initiated from and delete the element from your array.
I use this all the time. If I'm implementing a part of a communication protocol, I'll screenshot the document and paste it on the side. It instantly clarifies what the VI is doing and where the constants came from.
I was working on-site with a client. One of the technicians was in the room watching me program. At one point he made the comment that he was enjoying watching me move wires around but wished there was more to look at. 2 minutes later I had a picture of [Megan Fox](https://somethg.files.wordpress.com/2010/05/megan-fox-bikini-gq-03.jpg) in a bikini sitting in my block diagram.
Its a dexcom platinum g4 glucose sensor
It looks like they don't have a publicly available API for their sensor. You should contact [their tech support](http://www.dexcom.com/customer-care/contact-us) and see if they have any options. As far as I am aware, there is no way to do what you are asking. If worst-comes-to-worse, you could see if there are any programs that snoop usb ports and see what strings are sent and received when you connect the device.
... which is why I suggested contacting dexcom tech support.
Sorry, I meant for my answer to be more of confirming what you said than correcting. Guess it didnt come off quite right.
Does the device create a virtual comm port? If not you will probably won't get far trying to read a serial port; best to try and find a DLL to interface with. 
It does! What do I do from here? I've got USBPcap and Wireshark if it helps.
Oh god. If I understand this correctly, the higher the integer is the longer it takes for the integer to arrive at the enqueue (as a factor of 10ms). Therefore, roughly 90ms later the user has a sorted array.
Not sure if the factor of 10 is necessary just put it in to pre-empt any timing issues with low value integer, but yea, this sort is nothing more than a "hey, that's ingenius but useless". I would never ever use this sort for anything which is why the code is such an undocumented mess. Edit: If you want to wait forever, try an extremely large integer value. :P
For small lists just use a for loop. http://imgur.com/cJCqk9a
http://i.imgur.com/EEkSQjr.png Got it down to 1 VI. Only works when the input array is less than 64
Ah. I'll just have to go the OCR route then. Tech support basically said no to an API. 
Awesome! I know the random VI only generates positive values without an offset, but heads up if you try to incorporate user input with an I32 that you may end up with negative wait times.
Looks like it has a high potential for a race condition if you have too many of the same number in the unsorted array
I'm not talking about efficiency or items going in out of order. I'm saying what you put in the parallel for loop isn't thread safe. There's a potential for two parallel executions of the "Sorted" array of being read at the same time. This causes the build array to happen on two different memory locations which means that when they get copied back into Sorted, one appended copy will get written instead of the other and the element that was appended to the other array will be dropped.
I know ... I'm saying 64 additions within 1 ms is very doable and has a low chance for crashes. I'll try it ... later ... and let you know. 
I know this isnt good code .... its a joke.
http://www.reddit.com/r/LabVIEW/comments/2dbyla/working_with_async_hardware_need_some_help/cjuex0u here I paste the snippet. Hope it helps
Yeah, I already looked at these example, but I got no Ideal. 
[Use this for setting up the physical hardware.](http://www.ni.com/pdf/manuals/373784f.pdf) [Here are specs for your specific module](http://zone.ni.com/reference/en-XX/help/370984R-01/criodevicehelp/crio-9233/) ~~Use this to learn how to work with the FPGA in your module~~ Good luck. 
Ignore the 3rd link, you won't be using it with an FPGA.
I guess I'll try and break the ice. I'm a Physicist, currently in my last year/getting ready to write and defend my Ph.D. thesis. I have about 8 years of experience with LabVIEW, primarily for data collection and equipment automation (no analysis, we use other stuff for that). I have many more years of experience programming in C-style languages. Also, I am trying to convince my boss to pay for some certifications so I have something to add to my resume. So far, the only thing I have come across that I haven't figured out yet is the Actor Framework implementation in LabVIEW; mostly because I haven't found a good tutorial or walk-through that goes through the process step-by-step. I've only found complete examples. Maybe someone here can point me to a good one?
i believe in labview 2012 and later, you can create an actor framework project from a template. it will auto-create some classses, VIs, and documentation that has the best walkthrough i have been through. other than that, it truly is poorly documented. it calls into question its robustness and stability in the use of a serious project. also, in general, you really only need to consider it if you have the need for concurrent programming, as that is where the actor model shines. i have personally used labview classes on multiple projects with great success without the use of (and actually need) of the actor framework. i am considering it for a portion of my next project.
Hey there! Im an Academic Field Engineer for NI. I work with students, professors, and researchers on teaching and research projects of all shapes and sizes using NI tools. Get to work on some really cool stuff, from terawatt class lasers to giant shake tables, as well as work with a lot of students on neat projects.
I'm a physics grad student, about to start year 4 of my Ph. D. My lab uses LabVIEW for data acquisition, but the program we use was written years ago by a student long gone. I'm working on learning LabVIEW, using the version we have (8.20) and have only a small amount of experience in coding.
I'm a twice-over NI intern in Systems Engineering - Embedded, and am about to graduate in December with my B.S. in Computer Engineering. I use LabVIEW in my research for anything from vision, to motion, to remote monitoring. I currently have my CLD.
I'm also an Academic Engineer working for NI from the UK. This work is amazing, I've just spent my day teaching LabVIEW and judging case studies for our Engineering Impact Awards - There's everything from [Cyborg Plants](http://sine.ni.com/cs/app/doc/p/id/cs-16257), [Preventing HIV transfer](http://sine.ni.com/cs/app/doc/p/id/cs-16122) between breast feeding mother and child, attaching [live feed cameras to the International Space Station](http://sine.ni.com/cs/app/doc/p/id/cs-16236) - I'm like a kid in a sweet shop! My background is a Masters in Communications Engineering with a big focus on power amplifier test for 3G and LTE. I'm also trying to spend as much time as possible on this sub helping people out and generally adding to the community. If I see anything cool I'll post it, but in the meantime.... [SpaceX!](http://static.squarespace.com/static/5123c236e4b02be7ede3128a/t/51c0733ce4b018bf440a80e6/1371566910275/SpaceX%20LabVIEW.png)
Try dropping the DAQ assistant then. It will walk you through a wizard that generates the DAQ code for you.
EE undergrad, second year. I'm doing work with a professor in glucose control and needed to figure out LabVIEW to advance the project. I've got a background in VHDL and Verilog, so most of what I did came from practices I learned while doing that. My current project involves nabbing some data from a sensor, as well as starting to build a codebase to port this project over to a microprocessor.
Yeah, I've used the wizard to make an Actor Framework template. My problem is that I would like to understand the framework before I use it. It freaks me out to just start adding stuff to the template without knowing what I could be affecting somewhere else in the framework. Also, without knowing how it works (in a practical way), I am not sure what I can modify/extend within the framework to better suit my application.
I also work for a small company (4 people) using LabVIEW primarily for building custom measurement and automation systems as well as hyperspectral imaging systems. I completed my CLD-R about two years ago and have been using LabVIEW since I started about 6 years ago.
Can you use NI-MAX to create a task for that device and then just verify the mic is working? NI-MAX should also allow you to use a "test panel" to verify the device is working. Verify the device is working in NI-MAX and then move onto the DAQ assistant in labview to start. I have never used cRio but why are you using an accelerometer module (NI 9233) for a mic? Is the mic a piezoelectric device?
"GPIB Wait" pauses the labview program until something happens from the instrument. What "something" is depends on what's wired to the "wait state vector", but it's probably the signal that the measurement is complete. Clear sends a standard command to the instrument that should stop it from talking. Exactly what it does depends on the instrument.
Gpib clear is used to free up the instrument (on the hardware level) for later use. Try removing it and see what happens (hint, youll have to restart your computer)
Hardware communication is usually asynchronous, meaning that you can send out a command and then execution of your code continues without waiting for a response. So, if your code depends on a response from the device you have to figure out a way to make sure you get that response. One way is to guess how long it takes for your device to collect what ever data it needs to collect and send it to the output buffer. Then tell the program to wait that long and ask for a response. That is what "GPIB Wait" is for. The other way, is to serial poll the status byte register (and/or setup a service request). So, on most GPIB (IEEE 488.2 specifically) enabled devices there is a register in the device that indicates the status of the device, called the status byte register. One of the bits in this register is a flag that indicates that a response is ready to be sent. So, you can have your program repeatedly poll this register in a while loop and then continue execution when the device indicates a response is ready rather than using "GPIB Wait". You can read about it on [Page 14 of this PDF](http://g2pc1.bu.edu/~qzpeng/gpib/manual/GpibProgTut.pdf). Here is the [NI white paper](http://www.ni.com/white-paper/4056/en/) on doing this in LabVIEW.
I started using Labview for a university research project on a diesel engine (ran all of my acquisition with it). After a year and a half of self teaching myself I got my CLAD cert, which did a nice job of rounding out my knowledge. Now I work for an experimental combustion engine company as a test engineer and I just [talked my company into dropping some money on Labview](http://www.reddit.com/r/LabVIEW/comments/2ea0xo/graduated_in_may_with_a_clad_persuaded_my_company/)! I have a NI USB-6216 BNC on the way as well. Most of my LV experience is in data acquisition. I do some pretty solid Matlab as well and I have a lot of CAD experience.
Congrats. Welcome to the fold. 
Have you tried CVI? It's a way to use the LabVIEW Run-time Environment while programming in C. They also offer a certification for it.
A lot of people try to understand AF by trying to figure out HOW it works. This leads them into a lot of complicated code that gets them very confused. When learning the AF, you only need to learn WHAT it does and how to use it. It doesn't matter how Message X got to the actor core of Y, all you need to to know is that when you enqueue it here, it shows up there. Stuff like that. AF is written in a way that it's pretty much impossible to break the framework. All you do is extend the classes (actor.lvclass and Message.lvclass) and you get all this asynchronous stuff for free.
I understand why documentation gets you paranoid about robustness, but AF is probably one of the most robust offerings NI has. I've used it on a bunch of projects (from tiny to 10+ developer multiyear projects) and the framework has always worked beautify. Any problems you have while developing actors come from: 1. Mistakes you're making 2. LabVIEW IDE having trouble with lots of classes (this has gotten MUCH better since 2012+)
Just development suite? Or did you also get some sweet packages?
Just the full developer package right now. There are some packages I'm interested in but we'll see how far I get. I'm used to my university's tricked out copies so we'll see how sparse I find it with no add ons.
I dread the day my VPN no longer goes through. RIP Vision and Motion.
One of my coworker's favourite tasks is calling NI to tell them we have enough copies of their latest and greatest.
You have no idea how happy I was when they stopped giving out their development suite quarterly. The last time I was that happy was when they switched from CD to DVD.
Why wouldn't you want free software!?
Do you by chance have a link to a good tutorial on the subject? It would be nice to go through some sort of walk through to learn how to program a basic hardware controller (for example).
Nope, never tried CVI. For the most part any C development I do is not related to anything I would use LabVIEW for. But, I'll take a look at it.
https://decibel.ni.com/content/docs/DOC-17193 this is your best bet, but I'm not a huge fan of most of the examples. Give me some specs for an example you'd like to see and if it's not much work I'll try to put it together for you.
When you run out of places to put it, it becomes a problem. And you can't get rid of it. The second you do you get a perspective client calling up asking to revive an ancient bit of code. 
That would be awesome. Basically, the type of AF tutorial that would be most relevant to me would be one that goes through the design of a program used to set instrument parameters and collect data from a piece of hardware (for which labVIEW drivers are provided). The thing is, the feedback cooler example would be a great tutorial project. The problem is that it's already complete. I have no idea where someone would start building that application from scratch if they were only given instrument drivers and the design spec for the program.
It's still free (to those enrolled in ssp). You just have to download it from their site now when the SP releases
How much is enrollment in SSP? I can imagine it would be ungodly expensive.
Is it a contract?
Sort of. http://www.ni.com/services/software_benefits.htm
&gt;LabVIEW IDE having trouble with lots of classes (this has gotten MUCH better since 2012+) We had a nightmare class library corruption problem a few months back. Has made me really wary of how LabVIEW handles classes. The problem was not obvious and didn't throw up any errors when running in the IDE. It simply caused a catastrophic crash (no error window) of the environment when trying to run the exe build. Two weeks of trying to fix the problem was fruitless (even tracking it down was difficult) and we eventually had to roll back our entire source control repository, then painstakingly re-implement code. I saw [this debugging breakdown](http://lavag.org/topic/18364-bug-class-indicator-not-transferring-contents-to-parent-block-diagram/?p=110081) on LAVA, which is (1) nice to know that others have had mysterious class problems and (2) frustrating because it's such an involved debugging process for which the actual IDE provides no useful feedback.
A little late to the punch, I am a lurker. Computer science type who is still trying to decide if labview is great or the biggest pain in the ass with which I have ever worked. I am a mentor on a First Robotics team and have 25 years experience with C/C++, but have tried very hard to keep an open mind. The kids seem to pickup labview much easier than more traditional languages and I am impressed at how naturally multi-threaded systems can be modeled in labview. 
Can you give us a screenshot or post the code? I'm not quite sure I understand what you're getting at.
[Well, you could do this...](http://i.imgur.com/HP2hHQm) Variants will let you pass in whatever type you want and cast the variant to that type. You have to be careful though and handle errors, in case someone passes in an unsupported type (i.e., if it's not an array of something, this will throw an error. In essence, you're hiding the typecast. [Here is how it would be called.](http://i.imgur.com/TLyO09T) Edit: And also, in essence, you're still duplicating the code, just within the case structure. Someone else might have a way to do it without so much duplication, I don't, it's 6AM.
What you are wanting is a polymorphic VI. You can make a VI for each data type you want your sub VI to handle and combine then in to a single VI. 
This. Another comment below mentions using a polymorphic VI. That would work, but you're not duplicating the code in that case. You would have to go through and edit each different subVI if a change were to be made. Since your application isn't actually doing anything with the data in the array, just getting the array size and indexing, an array of variants should do the trick. ~~That all being said, I don't think iYogurt's snippet would do the job. You need the array input as an array of variants, not just a variant. I think what you need is [THIS](http://i.imgur.com/5zyAgkM.png), where the boxed portion would be created in to a subVI to be used with any array of variants.~~ Edit: Do it the way iYogurt says
This is the proper way to get what you want, and yes, it's a pain to make. I usually just make a library and add a new VI for each datatype as I need it. Or you could make a "get index" subVI, like so: http://i.imgur.com/hdfwaJm.png Also, what is the point of the floor function? That means that you will only return the max index if the input is exactly the max range; 99.99% of the max range will return index n-1.
This is what I was thinking I will have to do. The downside is that it takes extra development on top of the UIs I already have and if I change something with the first 3 UI, I have to go in and change the 4th manually. [I would also have to do some fancy global event triggering like like this in order to communicate between two different VIs.](https://decibel.ni.com/content/docs/DOC-21762)
You don't have to do anything like that at all to communicate between the VIs. Give me a bit to write up an example, but that link is way overkill.
That's the most common use-case for me: dropping in equations. I've occasionally also dropped a diagram in or a copy/paste out of an Excel table.
Try mine. It works just fine. Why wouldn't you just put the whole array in as a variant? The first .vi I call there "VariantType.lvlib:Get Array Info.vi" gets the array info from out of a variant. You can get the number of dimensions, the datatype of the array, and the dimension lengths.
I don't see how it's that much of a pain... You just create a new polymorphic VI and add all of the instances to it. What you're describing doesn't get around having to have a different sub VI for each data type.
Just a thought, but could you break up your UI so that you have multiple sub-panels that load running instances of a VI in multiple panels? That way you could update values in one place and display them in several UI panels. Another option is to use your messaging system to generate UI update events from a controller VI that does not have a display itself, but it's job is to handle getting data to and from the UI panels and communicate with the rest of the application.
What's an SMU?
Its a source measurement unit. From my google research I thought it was common language when working with labview but I guess that was just for my machine, sorry about that. A SMU measures a certain variable using the equipment and then labview can access the data that it returns and perform computations with it
True dat.
The problem is that we test 1 solar cell at a time. We are a research lab so we test them for experimental data, not to asses the quality of an entire batch. Because of that we test solar cells individually. The machine runs voltage through the cell and measures the current that comes off of it. We are only measuring one variable, on one device. Because of that I do not think it is possible to hook up a second SMU to the device, it would have nothing to measure. All that would do is make it easier to set up an experiment, which is all my professor wants. And its not quite an ammeter. The SMU can measure a whole bunch of stuff, we just currently use it to measure current for solar cell testing
No, the program works. We can measure current correctly. I need to know how to hook up two SMUs, not change our entire experimental setup
My professor wants to attach two data acquisition modules to one device to measure 1 variable. That is why I do not think it is possible. Why would you need two devices to measure the one variable? We only test one unit at a time
I'm a PhD student working on microfluidic chips, my background is in Mechanical Engineering. I was introduced to LabVIEW during my undergraduate degree but it wasn't really taught properly (only a one day "workshop"). When I finished my degree I went to work for National Instruments in the UK for a year, that's where I really learned LabVIEW! Since then I've got my CLAD and CLD, working towards doing CLED next :D Most of the equipment in our lab is controlled using LabVIEW, this includes everything from high frequency pulsed laser systems to surface profilometers! Big shout out to /u/CptDerpDerp who I used to work with in National Instruments!! Legend!
You guys, this is an SMU: http://sine.ni.com/nips/cds/view/p/lang/en/nid/204239 It's just a fancy power supply. It can source, or in this case, it can sync. That is, the solar cells want to put out 4 volts, but the SMU acts like a programmable load where you can suck all the current down to 0 volts. The SMU also has very fancy current measurement like a DMM but with the shunt resistance calculated out of the picture. I don't quite understand why hooking the cell up to one SMU is a problem. I don't know why you'd want to hook it up to two. Ideally you'd just get an SMU with the current rating you're looking for. You could try to hook it up to two and put all the connections in parallel (all the plusses together including the sense). Just sum the current measurements and make sure they don't fight each other I guess.
A simple diagram of the measurement would help too.
Just hook everything up in parallel and sum the currents.
I put together this example that shows some really basic usage of actors. You should download it, open it and go through the top level VIs (they're named in the order you should look at them) Check it out, run it, play with it. ask me all of your questions
I put together a simple example and posted it here http://www.reddit.com/r/LabVIEW/comments/2eyjbz/basic_actor_example/
Eeek. Thank you! I'll go through it on Monday when I have access to my work computer.
You're some sort of mind-reading Jedi. I was just going through the stuff in your link [HERE](http://www.reddit.com/r/LabVIEW/comments/2e1u6p/whos_out_there/cjxvyuh) while I wait for LV2014 to install, contemplating doing my next big project in AF... and then you post this! Thanks!
You can disable the clean-up function in your individual loops and cases. just right-click on the border. that helps a lot
I can't run any of the vi's. It says it's missing Create Directory Recursive.vi
Thanks everyone for the help. Extracted the index lookups (with the bug fix) from the array lookup. Now they are going to try out the polymorphic approach to wrap it up.
If the usb-to-serial registers as a COM port, then it's very simple. Labview can communicate over the serial port easily, you just need to make small VI's with the commands. e.g. a "move.vi" for for the move function that sends the move command. I think most companies just ask their customers, since with any large customer base it's almost certain someone has made the labview interface. 
You're dead on. The sequence structure is used to enforce data flow, for example to make sure that GPIB Write finishes before GPIB Wait begins. Your program accomplishes this by using error handling, which is definitely preferrable. All this to say: They are functionally the same, but error handling is a much better way of enforcing dataflow.
Thanks a lot, it seemed to do the same thing, and was WAY easier to glance at and understand. All the current program is in sequence form, clicking through each one to see what is happening is such a hassle and seemed unnecessary. As a followup, at the final step, is there are reason to make an indicator for the error out other than when I'm troubleshooting? 
Yes. If you want to use that VI as a sub-VI with error handling, you need a control to link to the error out terminal of the sub-VI. There are others, but this is the biggest one that I encounter regularly.
So anything I plan to use as a sub-vi I should create an error indicator and make it a terminal in case I need it later right?
Minor point here, those are not stacked sequence structures, it's a flat sequence with multiple frames. Yes, as others have mentioned, it's a poor means of enforcing data flow. Just wait until you get to the ol' nested case structure driven by an integer! That's what they used to do in Ye Olden Dayes prior to the Event structure. When I say 'they', I mean 'lousy LV programmers'. Another thing I'd like to point out here is that while tenets of graphical programming may not be intuitive to text-based coders, don't take a really crappy example of LV code as proof of LV being a junk language. Text-based language users have crappy coding habits, too. I literally can't count the number of times I wanted to beat my head on my desk after being handed someone else's electric spaghetti written in LV 0.5 pre-alpha that I had to migrate to LV2010 or whatever. Seeing the ways the Old Ones used in The Before Time was pretty cool (try placing a function on a block diagram in an LV5.1 VI. Go ahead, I'll wait), but it was the way they did it. The...the horror... The horror...
Yes. Use both an error in and error out. Then wrap your entire subvi in a case with the error in acting as the selector. This is known as standard error in functionality. It makes debugging much easier because you can find the source of the error instead of fixing a side effect of the error. 
What NakedOldGuy said.
Yeah I quickly converted the stacked to flat sequences before posting. I'm using LabVIEW 8.20 because thats what we have, will learning on this version hurt me, or is that modern enough to transition to a new version someday?
8.2 is definitely old, but it's got the Event structure in it, IIRC. You should be fine, I don't remember any other major items it was lacking. Please note that I wasn't dogging your code specifically, but rather the approach in general. Stay away from sequences by learning to use basic state machines instead. Sequences have their place but once your code enters one, you're stuck and you can't do anything but wait for the frame to finish. It's very restrictive and poor programming practice to use them to do the bulk of the heavy lifting in executing your application. Case structure driven by a typedef enum with all your necessary states inside a while loop with a shift register (for passing states) and you've got a state machine. No more sequences, flat or stacked.
Are you just trying to draw an envelope or are you trying to detect an envelope. I ask because the former is pretty simple and the latter is not, but can be done in a jiffy if you have the Order Analysis Toolkit. I believe it's now the Sound and Vibration Measurement Suite, now that I'm thinking about it. What LV version are you using?
I'm not sure I follow. I created an indicator on the error out of the last element in my vi, and a control for the initial error in. Now, do I put a case around it in that vi? Or do you mean when I use that subvi in my larger program, put it in a case then? And I don't seem to be able to wire the error in to the case selector, I get an error about types (Boolean versus cluster)
Ahh ok, makes sense. So, like this: [Updated](http://imgur.com/DBXGANC)
http://www.ni.com/white-paper/3770/en/ Would any of these vis help you to figure out a function to draw over the top of your data? My idea is that you use one of those peak finders to figure out where the interesting parts are and draw some function at them (Gauss/Lorentz/Voigt, whatever), added to a baseline function: E(x) = 0.2 + f(39710) + f(39950) Might be a function you could use to just generate a line to plot on the same canvas and it might be accurate enough for your purposes.
You can try using the Hilbert Transform to calculate an envelope for a discrete signal. Here is a VI Snippet and some reading material. http://i.imgur.com/xMWFeVz.png http://www.ece.iit.edu/~biitcomm/research/references/Other/Tutorials%20in%20Communications%20Engineering/Tutorial%207%20-%20Hilbert%20Transform%20and%20the%20Complex%20Envelope.pdf
second the equations. It can be very useful to have a well formatted equation next to a bunch of numeric primitives or even a function node.
so a couple routes : check out http://www.quanser.com/ for ideas on projects. Their stuff may be out of budget for self-learning though. But great ideas none the less. next check out the mrRIO http://www.ni.com/myrio/ This is a full fledged RT FPGA board ideal for students. That said, it's still a bit pricey, and you'll need to grab all your gear for projects off sparkfun, etc still. This board is badass though and you can do a legit amount of high-end control with it. On the bargain basement side checkout http://sine.ni.com/nips/cds/view/p/lang/en/nid/209835 It gives you an IO board with slow update rates and no RT for pretty cheap and will let you get a feel for using LabVIEW with the world around you. 
What directory are you trying to extract to?
C:\National Instruments Downloads The default directory. 
This seems like a strange, rare problem. The NI guys will probably be able to help you better than any of us users can. Maybe you could try extracting to another directly like a folder on your desktop (somewhere you know you have full user access)? Edit: FYI, I just installed 32-bit and 64-bit 2014 on my 64-bit Windows 8 laptop along with the Vision, Realtime, and Analyzer modules with no problem. (Realtime doesn't work with 64bit LV though btw)
Can't make it out, but wanted to wish you the best! Is your NI sales rep going to stop by? NI presence tends to draw attendance to these meetings. Have fun!
I could've sworn I posted my example already, maybe it was a private message to you. https://www.dropbox.com/s/pk0nxh6dyxkhy4y/UDE%20Example.zip?dl=0 It's written in 2013, if I need to save it for a previous version for you, let me know! I tried to keep it basic while demonstrating updating multiple UIs and handling UI events from multiple UIs. In this example, the exit button on either screen will stop the software, while both screens will be updating DAQ data.
You can get the myRIO cheaper if you request an evaluation kit. You may need your professor to request it, however. http://www.ni.com/myrio/evaluate/
Any idea on cost? I've been wondering what it's going to cost the uni kids.
Yes, at least one NI rep, /u/muman, will be there. He's supposed to be picking up the food and drinks, so he better show up!
Last I heard it was $1000 off the shelf, $500 for alliance partners, and $250 for students. That may have changed, but it's still a good deal at $500. 
Have you tried using the NI Update Service tool? 
Muun, thank you so much for going in to so much detail for me. Honestly, I should have already thought of doing it this way since that's how I handle the stopping criteria for all of the modules in my application. 
Hey, thanks for your advice
Hi insteresting question!, Im in Argentina, the company where i work manufactures refrigerating appliances and washing machines, I use Labview in different laboratories where we test prototypes and most of the applications i develop are for electrical/mechanical safety certification. We acquire lots of data from thermocouples, energy meters, accelerometers, strain gauges, laser micrometers, etc. The only sad thing about my job is that im the only person in the whole company that work with Labview, and whenever I finished some important coding there is no one to share my excitement!..there is no one who undestads/cares what ive done! they only want to see the results, not the whole process to achieve it! haha
It took me months to convince my boss to get Labview. I remeber the day i finally got it, i felt so happy like it was chirstmas!...Now, they are in love of it, and we haven't stopped buying NI hardware since then!
congrats! last week i also finished a project i worked for 2 months and it feels great when you contemplate your creation and everything runs smootly! 
I think the only way you're going to be able to solve that particular DE via Runge-Kutta is to make a custom version of the Runge-Kutta Syntax VI and pass the result of the max function into it. You'll have to do some custom parsing to make sure the value gets stuck into the F(X,t) string array in the right place. Not the most efficient means of solving this, to be honest. Do you have to use Runge-Kutta for all four of your DEs?
Take a look at [this](http://zone.ni.com/reference/en-XX/help/371361J-01/TOC116.htm) and also check the Example Finder. What LV version and level (Full, Professional, etc.) are you using?
Academic Site License, so I believe Professional.
This. Trivial.
Gahdang I'm retarded. Thanks. It's been a long week. haha
I love one of the pictures in the comments: http://lavag.org/uploads/monthly_09_2014/post-50361-0-89008500-1410334695_thumb.gif
In order: 1) Queue is generated. 2) A new queue is generated with a queue reference datatype. 3) A new queue is generated with a queue reference for a queue with a queue datatype. 4) A new queue is generated. The datatype is a queue reference for a queue with a datatype of a queue reference for a queue with a datatype of a queue. 5) An array of queues with datatype of queue reference for a queue with a datatype of a queue reference for a queue with a datatype of a queue reference for a queue with a datatype of queue reference is intialized. 6) A queue is generated with the datatype of an array of... fuck don't wanna type that again. You get the picture. You're also seeing a "failure" of LabVIEW's wire depiction algorithm. To represent multi-dimensional datatypes, LabVIEW increases the wire width/diameter for each new dimension. By the time we finally output to an indicator, the wire width is gigantic and now overlaps all the VIs/indicators. I put failure in quotes because I don't see any use case for a datatype like this and National Instruments shouldn't have to adjust their wire generation algorithms to accounts for something this ridiculous.
Did you see my version?
Muman . AKA Song Mu will be bringing that sweet sweet beer. I am curious how many redditors we got
Haha, with the animated wires? Yea, got a good laugh out of it.
I scanned through it as well. It looks pretty good, but I fear that the state machine architecture might not be modular enough for their preferences. I think your general coding habits are probably good enough to pass, though. Do you have the ATM Machine practice exam as well? If so, you should look at their solution for that exam. They use the queued message handler architecture for each module and launched each module in parallel with FGVs holding the queue references. This is what it sounds like /u/freethinker did and is what I did as well. I took the exam a couple of months ago with almost identical architecture to the ATM Machine solution and I got 90%. The exam was also incredibly similar (in requirements breakdown) to the practice exams, FYI. As far as your question regarding "some guy's personal opinion", the exam is graded by two people in order to get an agreeing result. The requirements tags are also supposed to streamline the grading process and remove opinionated grades. If the requirement is met at the same location as the tag, you get a point, plain and simple. As /u/freethinker said, you really are on the right track. The most important piece of advice I can give is to just practice over and over and over again until building the architecture you want comes as second nature. If you can blast through your initial architecture, you get all the more time to work through the requirements. Check out this post [HERE](http://www.reddit.com/r/LabVIEW/comments/1hby6w/suggestionstips_for_the_cla/). It has a TON of helpful information. Good luck!
[I may have gotten a little out of hand testing this out...](http://i.imgur.com/WLM5NiU.png)
Thanks for the advice guys, I actually took the test this morning. It was my second attempt. The first time I only got about half of the requirements tags on the diagrams and about half of those were just thrown someplace. So, yeah, that was a fail, but still a 63. This morning felt a lot better. I spent the last few days just practicing getting the basic framework and the three usual modules (Error Handler/Database Handler/Abort Handler) written as quick as possible. Went in this morning and before I even opened the packet I got to work on that stuff. In the first hour I had the framework done and before hour two was up I had all my modules built and the Main vi had all the appropriate cases in it. I used a queued state machine architecture, like in my sample test, but with half an hour left I had every requirements tag on a diagram with either accompanying code or a comment on how to implement, I had a #TODO tag at every not implemented or broken piece of code, and even a couple of fully functional modules. I took the last half hour to comment every control and VI about how they're supposed to work. The first time I took the test, when I handed it in I felt pretty 'whatever' about the whole thing. I'm feeling pretty good about today's test, especially considering your guys' comments. I just did what I did before, but got more done.
Nice! Best of luck to you! My results took over 8 weeks to get back to me, so learn from me and don't obsess over your email at the 6-week mark.
Can anyone swing by? I'm an engineering student and I've never used LabView but I'm interested in learning.
Everyone's welcome! It might be a little challenging to follow the presentations with zero LabVIEW experience, but everyone has to start from somewhere.
I'm interested! 
The [call by reference](http://zone.ni.com/reference/en-XX/help/371361L-01/glang/call_by_reference_node/) node might be useful to you. It will certainly be easier to pass in inputs.
It would. I need to re-tool stuff slight. I remember seeing a better way of handing variants in the past. Ideally I need to allow a funtion to accept both arrays and functions AND normal single values as inputs and treat them all equally. Hopefully I'll figure it out.
Use a pattern with List Folder (File IO &gt; Adv File Funcs &gt; List Folder) to get an array of all the image file names. Then you can load the images into an array. http://i.imgur.com/kzrcuWZ.png
Didn't know that list folder vi existed, works like a charm. Thanks!
This looks promising: https://decibel.ni.com/content/groups/interactive-internet-interface-json-toolkit-for-labview?view=discussions First hit after searching "json labview" edit: further poking around reveals this LV primitive: http://zone.ni.com/reference/en-XX/help/371361K-01/glang/unflatten_from_json/ Requires LV2013 or newer, and the flatten/unflatten strings toolkit. edit2: and this guy, which was released last year. Not from NI, so be careful using it (seems pretty good though): https://code.google.com/p/i3-labview/downloads/detail?name=i3_json-2013-06-17.vipc&amp;can=2&amp;q=
First thing is, you need something that can communicate with the server. Since you are passing JSON formatted data, I'm guessing you are communicating via HTTP requests. LabVIEW has VI's for that: http://zone.ni.com/reference/en-XX/help/371361H-01/lvcomm/http_client/ Once you've setup communication with the server, passing JSON formatted data is pretty easy. You can use one of the toolkits posted by the other posters, or roll your own. It's a pretty simple structure. http://en.wikipedia.org/wiki/JSON I'd probably do it with the variant to data vi and some text processing. Also, here's a [tutorial about communicating with SQL databases with labVIEW (pdf)](http://home.hit.no/~hansha/documents/labview/training/Database%20Communication%20in%20LabVIEW/Database%20Communication%20in%20LabVIEW.pdf) that might be helpful if that is what you are doing.
What speeds are you looking to do these things at, what USB DAQ are you using, and what is keeping you from just doing a very simple read-analyze-write all in one loop?
Speed: Maybe 15kHz-20kHz sample rate on read/write each, some delay (as expected with USBDAQ) is ok (&lt;50ms) DAQ: USB-6251 (http://sine.ni.com/nips/cds/view/p/lang/en/nid/209149) When I've tried to do it all in one loop, the read/write/VI slows down/became unresponsive. (or I get buffer issues, which likely has to do with dataflow and my lack of understanding of it - the examples/tutorials I've read have made sense and I can implement them as they are - but when I go to change something... I get issues) 
Running this (just straight acquire and write), works for about 5 seconds: http://i.imgur.com/as5ItdO.png then I get these two errors, even though the sample rates and # of samples are matched to run at 20 kS/s and 500 samples 1. http://i.imgur.com/EfB6dCf.png 2. http://i.imgur.com/Ka2pAZQ.png I'm guessing the StartTask vi is in the wrong place... but... I don't know the correct place. Outside the loop or just before the write vi, it didn't even write at all.
I think the start should be on the outside to the left of the while loop like the example you posted at the start of this thread. Try making both loops read the same number of samples. You have a constant 1000 to the bottom loop but the top loop is getting an undetermined number. Maybe try lowing your sample rate. The second error says you're not reading fast enough.
Whoops. I tried 1000 first (took screenshot). then 500. Both had the same error. I did exactly what you made in that image, and it works (can see it on the oscilloscope) for about 10 seconds, then gives this error (with 20kS,15kS,10kS, all failed): http://i.imgur.com/WoMTFS3.png When it was working, there's about a 4 second delay between reading and writing. These are the kinds of issues I've been repeatedly running into with all these different methods.
Thank you. I tried this - same sample rate and number of samples (tried 10kS, 500 samples, 20 kS, 500 samples, and even lower on both). Putting the start task outside the loop to the left results in: http://i.imgur.com/yhyryPY.png
OK, that's odd. What happens if you put the start task in the loop like above but you put it in a case structure with a [first call](http://zone.ni.com/reference/en-XX/help/371361L-01/glang/first_call/) wired to the case structure so it only starts once. Oh, also, can you time how long the read and write take. You'll have to use [flat sequence structures and the ms tick timer](http://digital.ni.com/public.nsf/allkb/6F6B9F4E149C80578625652800784764). Try 1kHz and 500 samples for both buffer sizes. That way you know how long your read and write should take.
With that sample info, and set to what another commenter suggested (no start task for the writing), it ran for 10 or 15 seconds, and the timer said about 1s (999ms, 1001ms, changed every now and then) for read and write. http://i.imgur.com/SGFsX50.png
"operation can't be performed while the task is running" is probably occurring on the second iteration of the loop because you already started the task in the first iteration. You can use the [highlight execution](https://decibel.ni.com/content/docs/DOC-7574) tool to verify this. If you use the first call, it shouldn't happen. Though, I wonder what happens if you leave out the start all together.
Leaving out start for both (as someone else suggested), results in: http://i.imgur.com/WoMTFS3.png
How many iterations occurred (put an indicator on the i in the lower left)? Hmm, one second seems too long. Can you time the read and write separately. Try increasing the write sample rate to twice that of the read (2kHz) Your ultimate solution may be splitting the read and the write into separate loops and using a queue to pass info in between. 
Ran it twice, same result each time. 34 iterations (0 to 33) error 1: &gt; Warning 200015 occurred at reddit timing.vi &gt; While writing to the buffer during a regeneration, the actual data generated might have alternated between old data and new data. That is, while the driver was replacing the old pattern in the buffer with the new pattern, the device might have generated a portion of new data, then a portion of old data, and then a portion of new data again. &gt; Reduce the sample rate, use a larger buffer, or refer to documentation about DAQmx Write for information about other ways to avoid this warning. error 2: &gt; Error -200279 occurred at reddit timing.vi &gt; Attempted to read samples that are no longer available. The requested sample was previously available, but has since been overwritten. &gt; Increasing the buffer size, reading the data more frequently, or specifying a fixed number of samples to read instead of reading all available samples might correct the problem.
Very interesting. I looks like you are almost there since you got so many iterations out of it. You just have to have some smart handling of the buffer. Might mean two loops
Ok. I'll look into the queue. Thank you. http://digital.ni.com/public.nsf/allkb/DD7DBD9B10E3E537862565BC006CC2E4 It's at 560 iterations now. Only problem is massive delay - about 7 seconds.
Feel free to send some this way. I am still using 8.5, which is a bit difficult when trying to find compatible hardware for setting up a new compact DAQ system.
Look at this example which shows you how to write a non-repeating continuous waveform to an AO http://www.ni.com/example/25370/en/ or {LabVIEW}\examples\DAQmx\Analog Output\Voltage (non-regeneration) - Continuous Output.vi The important parts are the nonregeneration property node and the DAQ start inside the case structure and to the right of the write.
this is interesting! what is also interesting is that i have started a library that does very similar things. similar to what you have here, i started off with the idea of implementing some of the functions in haskell's data.list module for labview arrays. i ended up with a similar solution of "hacking" the labview VI type references. for example, the filter function becomes a VI that takes in an array of some type and static VI reference to a predicate, taking the type and giving a boolean. another thing i learned is that i used a polymorphic VI for development time ease of use so that the filter VI could adapt to the array type or predicate reference given. all of this required caused me to go down the path of generating the entire library through VI scripting, which was essentially to save time creating all the polymorphic versions.
The problem I'm running into. Is a language like Haskell, Rust, or Java with a *generic* polymorphic function. Will set its type on compile with the fairly standard (at least Rust and Java) def doStuff&lt;T&gt;(&lt;T&gt; a) -&gt; &lt;T&gt; { And this is just awesome, the compiler will build 1 function per data type, or use a generic function for all types (I.E.: 32, 64 bit int, etc.). With Labview I'm doing more (in java) public static Object doStuff(Object a) { if(a isinstanceof Integer) { The problem is I have to do a (no so expensive, but not free) type check, branch to a case, cast to type, process, cast to generic, and return. And its kinda killing my performance. Because there are something like 36 different types of refnums and 40 different normal data types. Then arrays on top of that, which requires an additional call. So i think I have to make an average of 4 function calls per actual call. 
&gt; all the files are writable are you extra sure of this? i often will close labview, explicitly make all VIs read-only, and then make them writable again. then i will open back up labview and start fixing errors. doing a force compile and/or a mass compile can help out as well. labview and windows can sometimes get into a state where you think the VIs and classes are writable, but they are not. i have had this happen many times. what type of errors are you getting? the "this VI claims to be part of a library but the library doesn't claim to own the VI" error? or is it something else? also, i highly recommend moving to a source code control method. you can set up a perforce server for free for up to twenty users.
if i am understanding you correctly, that is exactly why i used a polymorphic VI. this is so that my filter VI is a polymorphic VI that has a version for every type you might want to use it on. there is still the general labview class and variant versions that allow you the genericy but they still require type conversions inside the predicate on a practical level, but the polymorphic VI gives you the flexibility of not doing type conversions for every use of the VI. i have been meaning to write up my findings and my library, and you've helped push me in that direction. time to start a blog!
I look forward to your results. I've stayed away from polymorphic VI's, hopefully give me a second wind. They'll likely be useful. 
Another tid-bit that helped me when working with Vision is the ability to make a VI "inline". This a is a property of a VI that allows the VI to ignore its Front Panel and only operate the code. I used this for subVIs that were doing pixel operations over and over again, thousands of times. You might already know about this, but just a heads up in case you don't. 
ahh, sorry. i knew that but i got mistaken and thought you were also getting errors in the project error window. outside of the revert dialog, are all of the VIs and classes error free? one thing i have had to do before is remove offending VIs from classes, save both the VI and class, and then add them back. this has helped get VIs and classes back in sync. sorry nothing is working. i honestly wouldn't mind trying to fix it for you, but that would require you to post your code. i would highly recommend contacting NI about this. they really need to understand this issue and the difficulty with classes. i have had similar issues on nearly every project containing classes.
No problem! I personally keep each LabVIEW version on a separate VM just to prevent any issues between the versions.
I had this problem with large applications that used multiple classes and found that working on the code locally as opposed to from a network drive resolved the issue. I've since moved to subversion so I'm always local now and don't see this problem anymore.
I tried all of the steps it suggested, and there were no errors, my example programs (given to me by Agilent) are still not detecting the devices.
Do you have agilent's VISA or NI's VISA drivers installed? I know they dont tend to play very nicely together. Also of note, this is why you buy NI's GPIB devices. Theyre the bomb and work with the 8000 or so drivers on ni.com/idnet. 
Kmoz is right. I'd return the agilent adaptor and go get an NI model. 
Does NIMax see it? I useally start there for connection issues 
Yes, NI MAX sees it. Edit: Just edit'd post with status report.
This sounds closest to my issue. I will make sure to do this in the future, even if this doesn't solve my problem. Edit: Didn't help :/
I have both of them installed, and working. Just updated. I wanted it, but this is the one we had available :/ and trying to get your boss to buy a new $500 cable when you have one is really difficult unless the first cable is unusable.
Unfortunately, it didn't fix the issue.
Sorry to hear that :( A string can be converted to VISA resource type. Have you tried manually creating the GPIB0:13:INSTR reference?
How is that done?
Just simply create a string constant and connect it instead of the VISA resource constant. If that doesn't help you may try adding a VISA alias manually. In the NI Measurement and Automation Explorer go to Tools/NI-VISA/VISA Options -&gt; Aliases and enter the GPIB resource name + a custom alias. Then you might be able to find it in the VISA resource control. If it doesn't help either you can try Tools/Reset configuration data. Be careful with that because it is going to delete tasks, scales, etc defined in MAX. Should you have active subscription to NI technical support I suggest you call in to your local branch and ask Application Engineering about your issue. They will be able to help you much faster :) 
I have my CLAD so try me. I'm using state machines elsewhere in my code but my tab control isn't a state machine right now. It's just the tab control terminal wired to a case structure. Are you saying I should implement a state machine instead?
Keithley provides VIs that do most of that work, if not all of it. [this I guess](http://www.keithley.com/products/dcac/dmm/broadpurpose/?path=2000/Downloads#2). 
I thing the most important question is why your application hangs. I can tell you that it shouldn't just because you use tabs. I suggest you debug your code first and then deal with tabs
The code runs fine with no errors or bugs until I try to implement the tab switching. It hangs when I try to switch tabs with a running program inside the case structure that's hooked up to the tab control.
ni.com/idnet has about 7000 drivers for instruments on it, including the keithley 2000 [here](http://sine.ni.com/apps/utf8/niid_web_display.download_page?p_id_guid=014E6EF883B9743DE0440003BA7CCD71)
Ok so that makes things easier. I'm working currently so will have to reply later. I. Will try and describe an architecture that will suit. One thing to think about in advance is that a tab control only changes what is displayed to the operator. All the code behind this still needs to be organised as properly. And a case structure won't transition between states until everything inside the case has finished executing. 
&gt;And a case structure won't transition between states until everything inside the case has finished executing. Ah this right here might be an issue. This is what I was getting at with my last question there, I was unsure if the case structure would force the change or not.
Pm'd you
I love industry support for LV.
How synchronous does it have to be? Have you looked at a producer-consumer architecture?
The way to think about a tab control is just a numeric control. Picture changing the value on a numeric control on the front panel and then reading that value in somewhere, like your case structure. Most of the time when I use a tab control, I don't use the value. I just use the container for cosmetic reasons. So I think the key to what you're doing has to do with what's in your case structure. Is it a problem to run through all the cases of you case structure each loop? I assume, maybe you only want to update the indicators in the current tab or something similar.
Are you finally using an event structure? If you create an event case but don't actually read the object's value your frontpanel will be locked and the problem you described sounds like this. Make sure that the terminal of your tab control is in the event where it is handled
I'm a current employee and would be happy to answer any questions potential applicants might have.
Sorry if I confused you! I'll try to rephrase it a little better. So I have a LabVIEW program where I can send large chunks of data from a PC into a PXIe chassis with FlexRIO cards in it. You don't have to know what they do, except that I need to send this data at high rates (which again is no problem with a LabVIEW program interfaced to a PXIe chassis full of FlexRIO cards. I have no issue with streaming bandwidth with this setup. There is now a need for this system to be able to interface with other software, so let's say I want to use Python to send data to this system, or C#, or MATLAB, or anything. What would be a good way to send data to this LabVIEW program? I know I can do TCP/IP, but I don't know the limits and before I try anything I'd like to see who might. There could be a better way to stream data to different applications other than TCP/IP. (ActiveX for example)
Is all of the software running on the same system, or are you sending it from one computer to another? 100 MB/s between computers would be much more challenging. There are quite a few different ways to have programs exchange data in a single system, some of which are talked about here: http://www.ni.com/white-paper/5719/en/ I dont know how activeX stands up in terms of data rates, but it might be doable. You can either call the labview code from another language, or, instead wrap the functions you need in the other language in Labview (which I think will be much easier). For instance, if youre doing a bunch of stuff in matlab, throw down the mathscript node and use that instead of the 2 separate interfaces. 
Gigabit ethernet should be able to do 100MB/s. I'm curious if you'd have lower overhead with UDP. You could always use two ports if you need more throughput. If you need a even more, I think NI has a [fiber card](http://sine.ni.com/ds/app/doc/p/id/ds-521/lang/en) you can use for COM.
Thanks for the insight on this. All of the software will be running on the same computer. Unfortunately I can't wrap the functions from the other programs, as they have already been created and are quite massive codewise. The software is just calling this LabVIEW and FlexRIO solution to do a small portion of what is going on in the whole system.
Right-click on the path control, select 'Data Operations' then 'Make Current Value Default'. After you've entered/browsed to your desired folder, of course.
Thanks! I don't need to interface to an external piece of hardware; I'm just sending data in the same PC from one application to a LabVIEW program. Would TCP/IP or UDP be advantageous here?
I did some quick research and apparently TCP where the host and server are on the same computer is [very high performance](http://stackoverflow.com/questions/2365400/localhost-vs-lan-speed-difference). I think TCP/IP is going to be easier but if you want to accept some potential loss for lower latency, UDP might be a better fit.
If you're using a path control, right click on it and click browse options. One of the fields in the dialog is "start path". You can also set this using a property node. If you're using the browse for file express vi all you have to do is wire a path into the start path input
Use a sequence structure and build one with three sections. The sequence structure executes each section (left to right) one at a time. In the first section you put boolean indicator 1, the second one gets boolean indicator 2 and the third gets boolean indicator 3. Now put a wait control (from timing pallette) in the second and third sections. Wire an integer to each delay amount in milliseconds. 
nvm maybe i understood all the things, just one i need to do. After all the three led is turned on and after 3 sec all the led need to turn off. How can i do it? http://i.imgur.com/6OoC89Z.png
Add a fourth section to the end of sequence structure. In that section wire FALSE to local variables of your three LED indicators. 
I actually had an event structure implemented before I even made this post. I had an event structure in each case that detected a change in the Tab Control value and then stopped the loops in the case. I was concerned that was creating a race condition so I set those events to stop the loops on 'mouse down' then created another event structure outside of the case structure that changed values on tab control 'mouse up'. And yeah the tab control was in that event structure. Neither seemed to work but perhaps I was still hitting a race condition so maybe I'll try again with some delays. Any tips on how you would implement an event structure?
While that would certainly work, if OP hasn't covered sequence structures in class yet I would assume that they're expected to do it with for or while loops. I just feel like SSS is a wee bit more advanced, at least it wasn't introduced immediately hen I took my CLAD class.
Sorry, this is a cost-cutting hire. We're actually already utilizing a contractors service and need to get away from it for cost reasons.
Like most software development tasks, the problem can be solved many different ways. And while you *can* use for loops that iterate once or while loops with true wired to the stop condition, both cases do not follow best case design practices. If sequence structures were to be avoided, one could use property nodes and the express time delay VI and have the flow of the program controlled by error clusters. I would suggest that route instead of for loops and while loops cause that shit is back-asswards. 
Not to mention, the most un-LabVIEW-like answer. I'm sorry, maybe I'm over-thinking this, but I really hope that OP's class isn't teaching dataflow concepts and order of operations by enforcing sequence structures. This would be like learning Python by enforcing C style pointers. Perhaps a more LabVIEW oriented solution would be to create a for loop that executes 'n' number of times containing the three LEDs and use a shift register to set the state each time. This uses a basic for loop, some front panel indicators and an expanded shift register. The most advanced concept here is the shift register, but one that I know for sure is on a CLAD test. EDIT: is was an isn't
I've actually rarely had cause to use a SSS and from my reading of the NI forums it seems that there are some people who think they should be dropped from LV altogether. That's why I thought it would be peculiar for an instructor to assign an obviously very basic task to be implemented with a rather tricky and niche structure. Edit: Also the shift register is on the CLAD test and it was actually taught pretty early in the CLAD course I had.
If I'm reading this correctly, the external application (in C#, Python, MATLAB, Cobol, ArnoldC) communicates with an existing LabVIEW program that pushes data onto a FlexRIO card at a rate of 100mB/s. Your goal is to extend the LabVIEW program and expose an API that the external application can interface with in order to push data onto the FlexRIO card. If that's correct, there's a number of good ways to do it. TCP/IP is good and straightforward, but you'll need a parser on your LabVIEW-side to receive, handle, and respond to API calls. That portion of your code would be the bottleneck and need to be optimized to fit your specific need, i.e. can the data be buffered, what's the expected outcome from the perspective of the external application, does it expect it's commands to be answered immediately within the realm of network latency, does it need a response, does it's data need to get to the card ASAP, lossless, without exception, what happens if two separate instances of the external app (maybe a python and C++ instance) call the API at the same time, priorities, etc...? Lot's of questions there to consider, likely not in your immediate scope, just throwing them out there. So, to answer your original question, TCP/IP is a good solution and probably a good first route if the API you have for your LabVIEW program works and is robust enough to extend. 
That's cool. What type of experience are you looking for? Can you talk about the types of programs the position will be working on?
Your comment about the wait function made me think about wait vs wait until next ms multiple. So I googled it to brush up, and lo and behold [I found a good use of sequencing.](http://www.ni.com/white-paper/4120/en/) Interesting little read, for me at least.
http://i.imgur.com/Rrcge5J.png Where am i failing?
CLD, with the thinking of moving to a CLA function. Capable of working on object-oriented modules utilizing an actor framework-ish framework. 99% of the framework is there and complete, so it's a lot of building modules that interface with the physical world. Most everything is RT-based using 9068's on a path towards migrating to the SOM. . Some fairly cutting edge software as it pertains to LabVIEW software engineering. It's been an awesome learning experience for me, and there's still a ton of work to do.
While a sequence structure would get the job done, I think it might not be the direction that your professor is intending you to go with this. Did the professor specify a for loop? Think about how a for loop works, and how you could possibly use the index to turn on and off different lights. What if instead of treating them like individual lights, you treated them as an array? As a general rule, you should avoid sequence structures. I'll default to the discussion below as to why, but if the goal is to make code that is readable and modular you should try a different approach.
Thanks for this input. These are all things we are talking about actually. We plan on making a standard command set and to help the speed of sending large chunks of data, the user, in their commands, will specify the length of the data stream they are sending. Very similar to the "Raw Binary Data" style commands seen in this white paper [here](http://www.ni.com/white-paper/4727/en/)
I assumed, by the username. Nice, btw.
Thanks! That's what I meant in terms of finding a LabVIEW alternative for the intermediate layer. LabVIEW is great, and what a simple way to program FPGA's, but there could be a better way to utilize the intermediate layer in this regard. Regarding P2P, that's what we're using once the data gets to the chassis filled with FPGA's, they're talking to each other through that high speed backplane. I was looking at using this C API, but my only concern is how well it would play with other languages other than C (ad how easy I can interface this with other laguages), as I know almost every language can do TCP/IP following a basic command style.
Thanks! By the way, in my research for a common command set (SCPI esque), I came across [this guide](http://www.keysight.com/upload/cmc_upload/All/SCPITrainingSlides.pdf) which I found very informative, especially if you ever have to use SCPI (or go deep into SCPI filled instrument drivers).
This is great stuff! Thanks for sharing,
A few things actually. First off, sequence structures execute left to right and all the code in the section must execute before we can enter the next section. So you can't turn off your LEDs by wiring a false from the fourth section to the first section. The compiler sees you trying to wire a constant (input) and have it go backwards through the sequence and throws an error. The second thing is that every indicator can only have one wire leading into it. So since your led on the first section is getting an additional input from that fourth section it throws an error. What you will want to do is to create a local variable for your LEDs and wire FALSE to those. You do this by right clicking on the indicator in block diagram and select create local variable. You will probably have to right click the local variable and change its access type to write. Then drag the local variable to your fourth section and wire FALSE to it. 
Have you seen NI's STM reference TCP communication API? http://www.ni.com/example/27739/en/ It's pretty good. You can strip away the XML stuff if you work really closely with the other communicating party. Also, out of curiosity, what region of the country are you in?
The teacher haven't said how to do it, but we have only studied the structure for loop, so i don't know how to do it with it. Til how to make it with the sequential structure. If you know another way to make it, write it in the comments
Well, it's your homework so I'm not going to do it, but if you have 3 lights that you can index and a structure that counts up from 0 each time...
That sounds cool. We have a modular framework we've built around communication, so every module can talk with every other module in the system the same way, whether it's in the same application, the same computer, or anywhere on the network. We built a system from that which lets us create applications for pretty much any of our labs by plugging together the needed modules and setting up configuration files. The same application can get us 1 channel on one computer or 400 channels with 8 user stations. We don't use RT, but we are starting to get a little into FPGA. I've been going through the RT 1 &amp; 2 classes online. LabVIEW can be really useful for these types of systems.
My first choice would be to use an array of control references with VI Server, but that may be too advanced for this. For a simpler approach I would use a while loop with a case structure. The case structure would have 4 cases (one for each LED being on and one for turning them all off again and stopping). The case structure would also have 4 outputs: one for each LED's value and one for whether to stop. The first three cases would output true for the LED to turn on and false for the other LEDs and the stop value. The last case would be false for each LED and true for stop. Wire the i terminal to the case selector. Place the LED control terminals after the case structure (inside the loop still), with each wired to a different one of the Boolean wires coming out of the case structure. Then wire the stop Boolean wire to the stop terminal. Lastly, drop a wait (ms) function inside the loop but outside the case structure to make each case take the right amount of time. For bonus points use a second case structure to skip the wait when stopping. 
Honestly I don't fully understand what you programmed previously. Usually most applications can be done with a single event structure so more than one event structure is a sign of being unefficient. (there are exceptions where you need more event structures of course) In the case of handling tab events I suggest you create a single event case regarding the tab control for the value change event and inside the event case, from the event data node you can read what is the new value of the tab. I guess in your case a queue driven state machine (QDSM) can be a great architecture: https://decibel.ni.com/content/docs/DOC-32964
I don't know if you really care to hear all about my program but I'm just going to list it all out, mostly for my own purposes of explaining it. It'll probably help me see it clearer. Feel free to ignore it if you'd like. But if you stop reading here, thanks for the help and suggestions and I'll check out queued state machines. *Edit: On reading about the QDSM, that's essentially what I built for the playback part of the program. I love it when I take an uneducated shot in the dark and hit upon a previously accepted solution.* The program reads in prerecorded engine data and can either play it back at various speeds, with pausing and stepping, or it can plot it all at once using filters based on run time, engine speed, etc. The plot all data side is actually pretty simple and is just a single event structure. The playback part of the program is the more complex state machine that also has an event structure for controlling playback speed and control. So the state machine handles the mode (play, pause, step forward/back) and the event structure handles waiting for button input and changes the state (as well as a few other things). There are some good reasons for this and some probably not so good reasons for this. But this isn't the culprit of my issue right now so I'm not worried about it. I'm putting this into Application Builder and distributing it around the company so I'm trying to make it as user friendly as possible, hence trying to have everything usable without stopping and starting the program. Anyway, I'm at a point with it where I think I know the problem. What I want to do (switching between playback and plot all data without stopping and starting the program) is possible in one direction, not the other. I can pause playback, switch over to the plot all data tab, and that functions. Going in the opposite direction doesn't work. I think this is because the plot all data side of the program loops once every time I push the 'plot' or 'clear' button while the playback side sits there continually looping. So if I switch the tab control, the playback program detects that and stops itself (I've implemented something like [this example](https://decibel.ni.com/content/docs/DOC-3522)). But since the plot all data side isn't continually looping it doesn't detect this until I push plot all data and I can't push plot all data if I've switched tabs away (that button is on one of the panes of the tab control). Catch-22. This issue is probably so specific to my program that I don't know if I can get general help anymore, at least without putting up my code. It just refuses to be intuitive. The loops in my plot all program don't seem to be running yet the program refuses to let me switch away from them back to playback. Possibly because it's waiting for input? I don't know.
I created a very simple piece of code that can plot/replay/pause/step back and forth. I was in a rush so it is not nice and has errors (for example opening the file ref all the time). So please take it as a pseudo code to show the architecture, however it can be run and works well. When using the QDSM I could have also used an enqueue inside the the Play case, to enqueue a Play to provide looping. However in that case you have to make sure about the race conditions (inserting a Play after having a Pause pressed for example). In this case the state machine is a bit more complex. https://drive.google.com/file/d/0B3prfgo8KEmYc3M1ZHYwZFQ0Z3M/view?usp=sharing I'd call your attention that NI DIAdem is a turnkey solution to your task.
A toggle switch configured as switch when pressed will change state (off-&gt; on or on-&gt; off) when pressed and will maintain that state indefinitely. A toggle switch configured as "latch when pressed" will change from off to on when pressed and then automatically change back to off as soon as the control is read. Which you want to use depends on the situation. If the switch's purpose is a "verb" (e.g. start, stop) you usually want a latching action so that the "verb" happens exactly once for each button press. If the switch's purpose is an "adjective" (e.g. on, off, fast, slow) you usually want a switching action so that the "adjective" continues to apply until the user presses the button again. 
Latching actions guarantee that the control's value will be read by LV at least once. Switching has no such guarantee. Open Example Finder and search for Mechanical Action. There's an awesome example in there that shows all 6 action types and explains the differences.
Look up 'Mechanical Action' shipping example. Demonstrates each one clearly.
The simplest way to get familiar with this is to right click on your boolean control, bring up Properties, go to Operation, and use the little control and indicator in the bottom right hand corner to play around with it. When I set a boolean control I usually use this to make sure I'm getting exactly what I want.
FYI: 95% of the time, use Latch when released. That way, it operates like a windows button and generates only 1 event call, state transition etc. The only time you need anything else is switch when pressed when you need the button to stay down and indicate it's state or for a radio button.
I would start by probing all of the power and signal wires. Make sure nothing's wired backwards.
After I got CLAD I actually looked in this sub for a flair cert because I really recall there was one. Maybe that was over in r/solidworks.
That's what I would do/like as well. Just self identify a certain level and/or add a certification (CLAD, CLD, CLA) if you have acquired it, even without providing proof "officially". I'll look into that. Furthermore, I would greatly appreciate any info/help from people, that have set up flairs before.
Working with /u/seedle on it. We both like the idea. Though would rather not require certification verification. 
I tried checking those, and switched them round and it didn't work. I ended up switching the channels in the program. Works like a charm now
Oh wow that's expensive! If you look up your controller, there is probably a download for some labview VIs to get you started.
A motion system consists of a motor+mechanical structure (the linear stage in your case) + a motion drive + a motion controller. First we should clarify if you really only have the stage or you have the drive as well. I checked the link at it says that the there is a suggested drive for the stage that is an inteligent drive. (it has a built in controller too) So if you have the drive as well, then please tell us the exact type and we can tell you what communication method to use to control it. (in this case usually you only have to send setpoints and it will execute, communication goes via, for example on serial port) If you don't have a drive then we can suggest you an appropriate drive+controller from NI (http://www.ni.com/white-paper/12127/en/) So to sum up: -do you have a drive for the stage? Please also note that in case of motion systems there is always a great temptation to create a DIY controller from an arduino or such and however it looks much cheaper it will be a pain in the *ss, compared to off the shelf solutions.
I have to agree with you, it's definitely not serious enough to require proof and a bunch of extra effort for the mods. The posers will be weeded out anyway haha.
Did you get the servo or stepper version? NI sells a simple motion controller that can generate the stepper signals for you, as well as letting you set motion parameters including velocity and acceleration. Without knowing how to write an effective motion control loop, this will be your best bet to use the built in NI-motion VIs. It's as simple as setting a control mode(position, velocity, torque) and commanding a relative or absolute position. I am well versed in this sort of automation so PM me if you need any assistance. Stepper Controller http://sine.ni.com/nips/cds/view/p/lang/en/nid/14338 Servo Controller http://sine.ni.com/nips/cds/view/p/lang/en/nid/13806
Awesome you did it! Is there a way to select 2, like 'CLAD/Intermediate'?
Give me two minutes and there will be.
Thanks, Munn!
I'm not at my computer at the moment, but I know OpenG has a [Large File palette](http://forums.ni.com/t5/LabVIEW/New-OpenG-Library-Large-File-I-O/td-p/102867). Have you looked in to that one? I don't know how it works with ZIPs...
OpenG does have a zip palette. It was available before the native one. The OpenG one does have more features but still can't make a zip that large. I'd use the command line version of 7-zip, and system exec. I posted on LAVA a while ago on how to do this but can't seem to find the link at the moment. EDIT: Looks like I found the [link](http://lavag.org/topic/16513-can-we-prevent-zlib-compress-dir-from-replacing-accented-characters/).
Make sure that your peak memory usage for both zipping and unzipping is well below 2GB. I'd keep the bundles to less than 500MB a piece. 
What makes someone an 'expert' or is it just your comfort level with LabVIEW?
Are you looking for local to the bay area or is remote location an option?
If you have the 64 bit version of LabVIEW there should be no problem in handling large data sets. I have over 10 GB in memory sometimes... If you have 32 bit labview I would recommend calling an external program (preferably 64bit) to do the heavy lifting.
Thanks. This looks to be the way to go.
Relocation assistance is available for the right person. If you're interested and want to send me your resume directly so I can get it in the right peoples hands, shoot me a PM.
I'll definitely check it out. There are a few differences between your code and the QDSM examples and documentation but I actually liked some of the things you did. Funny coincidence that you had previously written the exact program I was working on. I'll have to look into the changes you made. I've used classes very little and that seems to be a large addition to your program.
Oh wow, I didn't even think of looking for a SolidWorks subreddit until this comment. I'm certified in that too.
Sorry, I've got nothing to add as far as helping diagnose the BSOD issue you're having. But I will say that is the most convoluted code I've seen in a while. My head hurts a bit after looking at it.
I don't have an answer for you. I can say I had the same issue in 2011 with serial drivers BSODing my Windows 7. It even went so far as to corrupt my video card drivers. Had to re-download the video card drivers and now my video card won't let me use 3 monitors (2 external plus laptop) anymore.
What are the possible answers to "is x greater than y"?
This comes down to effort on your part. You can figure this one out without any help. I have faith.
Never thought about it that way. Basically just answered my own question. Thank you.
http://en.wikipedia.org/wiki/Serial_port#Settings
I've worked on an application that used 14x14 Data Matrices, but never worked with 11x11 Micro QR Codes. Are you testing with the Vision Assistant, or the Vision Development Module directly? [This article](https://decibel.ni.com/content/docs/DOC-19525), which NI put out, points to [this Wikipedia page](http://en.wikipedia.org/wiki/QR_code). As far as I can tell, that wiki page would suggest 11x11 are an option for you. Have you tested on any 11x11 codes that someone else has made? This would rule out the option that the codes you're making have an error. Edit: I re-read and saw that you said you couldn't find any valid examples of the 11x11. In this case, I would suggest you look through both the wikipedia page I linked and the onbarcode.com link to confirm matching rules.
VISA Write and Read write and read an array of unsigned 8-bit integers. These can be stop bits, start bits or whatever you want. [See this article for some more basic info.](http://digital.ni.com/public.nsf/allkb/6C24F2F07BC23BB78625722800710865) There are no start bits or stop bits included unless you add them in.
so here is what I don't understand: the VISA block takes in the string that you want to send. But when I try to read that string I'm unable to do so. how is that string packaged by labview?
By "read that string", what do you mean? Trying to read it on the Arduino side? Or trying to read it in LabVIEW after you sent it? You could try having the Arduino echo everything back that it reads to see exactly what is being sent and received. There's no packaging that I know of. Make sure that if your Arduino is expecting a certain start and stop byte criteria, that it looks for those and that you're sending those.
That's correct, I'm trying to read the 'command' that I send via serial on the arduino side. I'll try echoing. That being said, whenever I send a string it's almost like the arduino is resetting, the rx, tx, and the on board led on pin 13 flickers a bit.
Can you take a snippet of the LV code you're using? Maybe we can pinpoint something strange there.
Sure, [here it is](http://i.imgur.com/JSz7oEK.jpg).
Great! I was writing a comment telling you to give this a try when I refreshed and saw this. "310D0A" converts to "1" according to a hex&gt;text converter I found. You may want to look in to your string command input as well to make sure it is set to Hex display (via right click menu) if the Arduino isn't receiving what you expect.
Thanks for the help. Seeing as I have to finish this code + building my jig by wednesday, I think I'll have to do with just sending single digits from 0-9 and time it right so that I don't mess things up. If I had more time I would look into sending multiple digits ending with a termination character (/) in order to signal the end of the string. But I'll leave that for another project... Thanks again!
Glad I could help. If you want some input on how to do the actual communication read from my limited Arduino/LV experience below: -- I worked on an application previously that had an interface between LV and Arduino. All the communication was done in the form of unsigned 8bit integers. The Arduino would expect a start byte of 255, the next was a command byte, then a value for the number of data bytes to follow. Example: I want to send the command corresponding with the value "30". The Arduino knows what to do if it sees the command byte 30. I also have three values to include with this command. Say 15, 112, and 201 (corresponding to some kind of inputs with the command). This is what the array of U8 would look like that I want to send: [255, 30, 3, 15, 112, 201] Translating to: [start, command, numdatabytes, databyte1, databyte2, databyte3] The way to send this in LV was to create the above array of U8 integers, use the Byte Array to String conversion, then write it. Now this is just the way the communication was handled for this specific project and there are a million different ways to get your communication across. In this case, both the LV and Arduino code were checking over and over for the 255 start byte. Once the 255 showed up, it used the command byte and numdatapoints byte to figure out how many more bytes to wait for and what to do next.
You can try this 3rd party library http://sourceforge.net/projects/labpython/ which will give you access to a script node VI like Labview has for MATLAB already. Otherwise, could use something like this http://ironpython.net/ to interact with your scripts through Labview's built in .NET VIs 
Have you checked out [LINX](https://www.labviewhacker.com/doku.php?id=libraries:linx:linx)? It's an offshot of the LIFA (Labview Interface for Arduino). It's designed to run LabVIEW on Arduino as a non-deployable control. If LINX itself can't help you then I imagine the developer could. His name's Sam Kristoff, IIRC, he was either an NI employee or intern when he developed LIFA, and now he's doing the LINX thing as a standalone project. He's really active over on the forums at LV Hacker so if you put this question up over there he might be able to help. Can't think of many folks who have as much experience talking to Arduino with LV.
The certification eKit on [this]( http://sine.ni.com/nips/cds/view/p/lang/en/nid/10647) page has exercises that build up to a full CLD exam, as well as 4 full sample exams. If you can work through all the exercises, you should have no trouble with the exam.
You can set up TCP/IP communications and send everything JSON encoded if you want to go the network comm route. In the LabVIEW side, use "Unflatten from JSON" to convert the data to a cluster typedef.
Hmm I have not heard of LINX actually. I'm having a hard time understanding the difference between it and LIFA though. How do you know so much about it? It sounds really cool.
LINX is basically the new LIFA. The developer made LIFA while he was working for NI then I guess he finished up there and wanted to keep developing it so he started LINX. LIFA still has a few features that aren't ported over to LINX yet (built in accelerometer functions and such) but LIFA won't get any more development and LINX will. So unless you need a feature in LIFA just go with LINX. You can install them side by side if you want. I just play around with it. I'm spoiled by nice NI hardware at work (currently a USB-6216 BNC is my go to work horse) so I try to emulate that for my personal projects at home. I actually think I'll pick up an MCC DAQ or one of NI's 600x series DAQs though, the Arduino just isn't built for it.
Luckily, you can do all the exams with Simple State Machine, producer-consumer or events may make your code more efficient but it takes extra time. Make sure you are able to work with your choice of timing VIs or know how to develop a wide variety of them(countdown, countup, reset, pause, elapsed time) without thinking. File I/O should come second nature as well. You should be able read from configration files and/or log back to them. You can be guaranteed these concepts will be part of your problem. The other 62% of the exam is style and documentation. You have to study the way they grade the exams and adjust your habits as such by looking at web resources as mentioned.
I have tried to do that, the problem is that since both loops run on paralel, and execute as fast as possible, loop 1 wait for one minute, but this doesn't affect at all to loop 2, because every pump operation takes more than that. So if I want to actuate 1, actuate 2, wait 1 minute. The producer takes almost 0 time to produce "actuate 1 and 2" and then waits, but during the wait, consumer is actuating pump 1, and once the producer goes again to produce, it's so fast that it doesn't make any difference to the consumer
Are you sending serial commands? You need some sort of status signal to let you know when a pump has completed actuation. You need to send the status command and parse the response at a regular interval in order to determine when the actuation has completed.
yeah I do that. That's done in the consumer.
Send a wait command to the consumer loop along with the number of seconds to wait. Calculate the time to stop waiting and put that in a shift register in the consumer loop. Each time the loop executes compare the current time to the time in the register. If the current time is newer (greater than) the wait time, then execute. Use a case structure. Your dequeue should be inside the case structure so that it open executes if the wait time has expired. Add some small delay to the case that executes when the time hasn't expired.
Just use an event case. Set the timeout to 1000 ms to update the display every second. edit: for example: http://i.imgur.com/4GINcHq.png
Use elapsed time for determining when a set time has passed. Otherwise you have to create your own elapsed time functions. Using time delay will hault execution for a specified period of time (meaning other tasks can't be performed).
If you don't already have an event structure (like the above comment), use the Elapsed Time VI. The trick, then, is to implement it somewhere that it will get updated regularly and not look choppy to the user. One of my current clients wanted a simple stopwatch on his front panel, just as a utility during use. None of my original loops ran fast enough (nor did I want them) to update the indicator at a rate that looked nice, so I added another small while loop just to handle the stopwatch. You could do something similar if your application doesn't have a regularly updating loop.
Do you need pause function? Check this link: https://decibel.ni.com/content/docs/DOC-8175 (it is also a good hint if you are practicing for a CLD, however there are better structures)
Thanks everyone! For now, what I went with is using the Elapsed Time VI inside of a while loop. It seems to update the elapsed time good enough for our use, it doesn't appear choppy, and by wiring the while loop condition to the Elapsed Time output, it still works as a timer.
I take it you are getting a 2d array from the camera and not an imaq image as in your title. I actually wrote some code for this last week but I'm currently out of the office so I can't give it to you until Monday. The basic idea is to index the rows or columns of your array, reversing the order the the array as needed and build a new array. Sorry I can't be more help, but that should get you going. For the most part just tried it until I got it working 😛
What is the camera you are using? What is the data format?
Open the "Mechanical Action" example and check for yourself :) It is the easiest way to understand it.
Contact NI directly, they will most probably give you the answer over the phone.
Thanks, I actually got a certain version of this working with the suggestion from /u/Glorypants . The problem is that its way too slow. I think we might just have to buy the VDM.
Yeah, VDM is pretty dang awesome and makes all of the image manipulation stuff really easy. If you can find a way to unload the cost on to a client, or other department, you should give that a try.
onbarcode.com leads to [this page] (http://generator.onbarcode.com/), where I then clicked on the QR code link, leading to [this page](http://generator.onbarcode.com/online-qr-code-barcode-generator.aspx). However, even though you can change the versions, the higher the version, the middle the QR code is, and Version 1 is already too big (it doesn't seem to have micro qr codes) The wiki article references [this specification] (http://webstore.iec.ch/corrigenda/iso/isoiec18004-cor1%7Bed1.0%7Den.pdf), but I was not able to find anything helpful. Any further ideas? Edit: failed at formatting
I should note that this same question has been posted to both the [Labview forums](http://forums.ni.com/t5/Machine-Vision/Reading-M1-micro-qr-codes/td-p/3027039) and [Stack Overflow](http://stackoverflow.com/questions/26455429/what-kind-of-11x11-qr-code-does-labviews-qr-code-recognizer-take) to no avail.
You should be able to do a repair operation on the install to fix this
The labview installer is a curious thing that somehow remembers how it left the installation and assumes noone has touched it since. It cannot repair the installation as it can't see it's broken. That is one of the essential problems i'm having
I'm not sure how you're accessing the installer, but when I go into control panel -&gt; programs and features -&gt; ni products then click change/remove I get a list of all installed Ni products. Then I can highlight all of the ones I want and always click repair.
Because of me accidentally deleting the ni folder in program files I cannot access the installer in the configurations menu
Here is a walkthrough on how to force reinstall LabVIEW: http://digital.ni.com/public.nsf/allkb/ADD22E807D5A12AD862579EC00760F79
* Remove the whole National Instruments folder from Program Files. * Restart the PC. * Run regedit and remove those trees: HKEY_CURRENT_USER\Software\National Instruments HKEY_LOCAL_MACHINE\Software\National Instruments (make a snaphot of the registry before, just in case) * Restart the PC. * Try to install LabVIEW again. If it doesn't work, contact NI. They might give you a tool called MSIBlast to clean the system from NI software.
You're going to want to check out a [queued state machine](https://decibel.ni.com/content/docs/DOC-32964). Essentially you have a producer and consumer loop connected by a queue but within the consumer loop you have a state machine. Within those states you'll be able to either control the pumps or do other things, not pump related. If you haven't already, get familiar with state machines in general before jumping into the queued state machine. The QSM was suggested to me on this sub and I'm really glad I rewrote my program for it, it's a really tidy and powerful structure.
Do this before the registry edits above. 
Thank you! That cleared up some misunderstandings I had. It seems to me I should be having one TDMS file per instrument, and merge the files when I'm finished with acquisition. That way the different acquisition loops can do their own thing at their own pace. After merging into one TDMS file I can have the option to export a tab-separated file to work with in Excel. The acquisition itself works reasonably well. 
[Have you tried all of these locations?](http://digital.ni.com/public.nsf/allkb/6077DBEDA4F9FEE3862571F600449501)
In your build specifications add the custom-errors.txt as a support file (in the Source Files tab). It will be automatically included in the correct place. http://www.ni.com/white-paper/3209/en/
Hmm, the only ideas I have left: -is it the Shared folder in "Program Files" or "Program Files (x86)"? -are the error codes inside the file in ascending order? 
The 6221 has a +/- 200mV range so it should be ok. What are you trying to display? The raw waveform or frequency content? Another questions would be what configuration of the input did you choose - RSE, NRSE or DIFF?
I set up the daq with RSE and continuous I think. I'm displaying raw waveform, then doing the FFT on it. it works fine for when I plug up an mp3 to the jack and play certain tones. But when I plug up the guitar... nothing. 
I set up the daq with RSE and continuous I think. I'm displaying raw waveform, then doing the FFT on it. it works fine for when I plug up an mp3 to the jack and play certain tones. But when I plug up the guitar... nothing. 
Try with the DIFF. The RSE probably works fine with the computer jack because the sound card and 6221 are connected to the same system ground. Look here for details: http://www.ni.com/white-paper/3394/en/
Okay, I will try this when I get a chance! Thanks! 
What VI are you using to show the error. I found that error files were too much of a PITA so if you use the custom (not user) section of the error ring, you won't need the file. http://zone.ni.com/reference/en-XX/help/371361L-01/lvhowto/cust_errors_from_error_ring/ The error cluster from code VI will also work http://zone.ni.com/reference/en-XX/help/371361L-01/glang/err_cluster_from_code/
&gt; When I highlight execution, it runs as expected That usually means timing issue. Without highlighted execution, subvis run at the same time. When you highlight, they run one after the other. For example, in [this image](http://www.ni.com/cms/images/devzone/tut/MultithreadedLabVIEW.JPG) all subvi's labeled 'thread' run simultaneously. Try putting your code in a sequence structure to force the order you want. This is my first thought, I'd need to see your code to give a better answer.
Execution order is enforced via the error cluster, however I think I just figured out the problem. Added a new piece to the controller that was activated when in shouldn't be. Thanks for the help though!
Maybe you can write a script to replace them all. If you select "custom" error from the ring, it'll retain the code (but not the text)
The word simultaneous isn't really the correct word that you want. LabVIEW is inherently multi-threaded, and when possible will make use of a multi-core machine to distribute that load. However, in your attached image there is no mechanism in place to guarantee that any of those express VIs will run simultaneous. In fact, due to the nature of the express VI architecture, I can pretty much guarantee you that they will not run simultaneously. They will run in parallel, and possibly asynchronous. But the order of operations of those three VIs will be completely up the LabVIEW and when run three times could be three different orders. The only way to enforce an order of operations is dataflow (usually the best choice) or sequence structures (usually the worst choice, although not a bad idea per-say for troubleshooting.) There are some methods of enforcing simultaneous operation across multiple cores of a machine, however, just putting them in a line and connecting them to the same inputs and outputs isn't one of them.
All true. I far oversimplified because I made the assumption OP was a beginner and had missed the class on execution order. It turned out to be a bad assumption. 
Have you posted on the LabVIEW Hacker forums? They'll be much more helpful over there. Also, you know that LV on Arduino can't deploy, right? So you'll have to have the Arduino plugged in and click 'run' on your PC, as opposed to uploading an Arduino sketch and letting it just run.
When I first tried to flash Lifa_base.ino onto my arduino uno I got an error, and found out that I needed to use version 1.0.5 of the arduino IDE. Possibly try that?
Yeah, I'm running 1.0.5, I actually deleted the environment and re-downloaded it. I also upgraded my MAC OSX to Yosemite. My friend helped me out last night and where 98% there. We flashed it on to the board but now have to check with some arbitrary Vi to see if it actually works. Are you on on a MAC? 
Sorry I dont really understand, "LV on Arduino can't deploy". Do you mean that Ill have to have the uno plugged in, go to the arduino software, press run and THEN go to labview?
You're getting the right idea. Deploying code to the device (in this case, Arduino) means that the code gets stored locally on the Arduino and runs (that's the compile and upload button in the Arduino IDE). Generally it uses a while loop so it's just running continually, waiting for input. What LabVIEW on Arduino does is essentially just uses the Arduino as an input/output peripheral. So just like DAQmx tasks, it queries the pins on the Arduino to read in your signals. But the LabVIEW code isn't running on the Arduino, it's running on your PC. So you can't unplug the Arduino and have it keep running whatever your LV code does. The sketch you upload to the Arduino (lifa_base) just waits for LabVIEW communication and that's all that gets stored locally on the Arduino. I only point this out because in your original post you made it sound a bit like you expected LV on Arduino to be equivalent to writing out an Arduino sketch and uploading it. But you won't have the same capability and I thought that's what you were looking for. Trying to save some disappointment down the line.
I don't think I've ever seen "cheapest" and "NI" in the same sentence. As I am of the cheap persuasion, I stay away from NI, and recommend [LabJack](http://labjack.com/). Depending on the speed you want, you can be cheaper still with a multiplexer (small electronics skills needed).
I meant it in the relative sense :) I do have to use NI hardware. 
Really depends on what youre trying to do with the signals. How high speed is high speed, what kind of signal conditioning do you need, do you need hardware or software timing on the DIOs, how accurate of thermocouples, etc. You can definitely get all of this in a single PXI chassis, and depending on the specifics of the measurements, as few as 3 cards. 1073 chassis+5x 6363 could get you that list of features (TC channels arent ideal on there, but doable) with some room left over. A better way would be with one of the high channel count AI cards (some just released, up to 208 channels on one card), then 2 PXIe 4353s for the thermocouples, then the DIO through an FPGA card if you can benefit from that functionality. You could also do this whole system in CDAQ or MXI-RIO, although it may be 2 small chassis instead of 1. Feel free to reach out to me via PM if youd like, Im a field engineer for NI and spec out these kinds of systems every day. 
Talk to your local sales rep. It's literally their job to help with questions like this
I know, I left him a message. Just thought id get an initial opinion from somewhere else so I have something to work from. 
Labjack doesn't scale to multi card systems well, and often ends up costing a lot more in system integration time. 
Just be aware there are about 5000 ways to configure this same system and dozens of different cards possible , sometimes even for the same function. Feel free to reach out to me if you'd like to discuss in more depth, or give your local field sales guy a call.
£18 on Google Play UK right now, nicely written book so far
Didn't even need rubber ducking ;)
I'm doing some image manipulation without VDM and it kills the performance of the VI. Manipulating that 2D array is a pain. I made the program usable by dropping down to 8 bit though, that drastically reduces the size of the image array.
What is your project? The best way to suggest you a solution is to understand your goal :)
I'm on a Macbook Air, using two Arduino Unos with Labview. I found that Lifa_base.ino wouldn't upload with v1.0.6, then after some Internet searching I found that I needed to use v1.0.5, and then it uploaded properly. Not tried a redboard, and unfortunately I can't tell you what version of mac os I'm using as I don't know off the top of my head and a friend has the machine until Monday.
Understood :) So I'm just guessing then but your project (as for I/O count, speed) sounds like this http://sine.ni.com/nips/cds/view/p/lang/hu/nid/212234 with the CAS option
Similar capabilities yes. I'll look into that package a bit, thanks!
LVOOP! :)
Spotted that, huh? Good eye.
You would be surprised how many times I have to explain students that error message != bug :)
Aha! Thanks a million. Definitely need these buggers in the cluster :)
Things like this I learned the hard way
Can you post your code? Sounds like something's wrong in your code because there's no way it should take as long as you 're describing to execute basic commands like that.
Check your timeouts on all of your VISA Reads and VISA Writes. Posting a Snippet of your code here could help us figure out the problem as well.
Not very familiar with SCPI. You can try RS-232. Is the transfer speed too slow, or are the responses from the power supply coming back late? You may want to increase your baud rate if possible. Like others have said, it sounds more like an issue with software rather than communication speed.
That's not normal at all, something can definitely be done about it
GPIB will be the fastest and most reliable.
Drop the random number node. Right click -&gt; create indicator. Run the code
so it's impossible right? then i guess i had to create 3 random number... p.s. thanks
I have to create a program that continuously generates random numbers between 1 and 3. (OK this one is not a problem) When three consecutive random numbers are between 1 and 1.25 it should light a red LED for two seconds. The only problem is the 3 random numbers 
Here are some overall guidelines: - 1) Organize code in a LabVIEW Project and mirror the project hierarchy in the file hierarchy within one directory 2) Use relative directories in all File I/O code (see File Constants palette) 3)LabVIEW can do a Source Distribution build, which will grab all the code needed by the Project. (This has never worked all that well for me because you either get one folder with a ton of VIs in it, or a huge sub-directory hierarchy.) 4)You can do a Save As... of your project and create a duplicate project with contents. This isn't a bad option if your code hasn't been saved neatly throughout the development process.
Just use [this](http://www.ni.com/cms/images/devzone/pub/nrjsxmfm912163998723206173.jpg). I have no idea, but I imagine the easiest thing is going to be to replace the color in the screenshot with a photo editor. [Before](http://i.imgur.com/PPDTOqK.png) and [after](http://i.imgur.com/UUs9j5M.png). Color select tool in gimp &gt; colorize. Literally took 30 seconds.
Haha there's no way I can use that without cringing every time I walk by. I'm making something just as large (no subVIs), but much cleaner and with a few hidden messages ;) Thanks! I was leaning towards Photoshop being the only option, but didn't know GIMP was a thing.
I don't even know what is [this](http://i.imgur.com/EF2Fjr5.png) and what are [those](http://i.imgur.com/V80mbuY.png). I started using labview 1 month ago.
Build Array (builds an array...) In Range and Coerce (checks if middle value is within range of other two) And Array Elements (performs an AND operation on the array elements, if all True then outputs True) - - That's the last of the LabVIEW tutorials from me today.
Lemme know if you need detailed instructions. (GIMP, I mean, I know nothing about photoshop). Alternatively you could write a labview program to load an image and replace a certain color with another one :).
Fortunately everything, save for some drivers, is under one directory. Also, last time I made a duplicate under Save As, it literally duplicated the entire file structure of the previous one under the new folder, instead of putting the files and project into the new folder. Instead of %PROJECT%\project copy\files, it was %PROJECT%\C:\Users\&lt;user&gt;\Project\project copy\files
How are the libraries installed in the first place? Packages? Installers?
Erm, I think I used an installer from NI's driver site. They got placed in C:\Program Files\National Instruments\LabVIEW 2013\instr.lib 
Lol, use Ctrl-H :)
How do you communicate with the amplifier?
RS-232 
If I were to write it, I would probably start with a generic amplifier object, that way when you or someone on your team decides to replace the Copley with something else later you have some reuse and not a complete rewrite. The public API for this object would expose a settings object to set amplitude and frequency. I would also expose some type of State object which contains feedback of the waveform going to the Copley. I flipped through the Copley site and supposedly there are some LabVIEW examples in the CMO installed files. However, I couldn't get the downloaded installer to run on my machine, so I can't be much more help.
I am not using VISA directly, but pre-made drivers from the manufacturer for write/read to the supply. I will have to learn more about labview and VISA to make sense of their source code...
Yeah, changing the COM port baud rate had no effect.
Look at mr. top secret over here. Must be working on some cool stuff eh?
sshhh
Definitely possible, I've been setting up a similar system over the past few weeks with a CDAQ and a 9862 module. The built in functions and examples are fairly straightforward to set up.
Yes, many can work with the CAN protocol. There are CAN cards for PCI, PXI, USB, C-series, PCMCIA, myRIO, as well as built on to single board RIO. You may also be able to communicate to the motor controller through a variety of other means. http://www.ni.com/can/ I highly recommend calling either your local sales guy, or one of the inside sales guys at NI, they can help match exactly the right one to your needs. 
It looks like you're configuring your output every time you read. Is this necessary? This might be the source of your delay. To test this, can you configure once at the beginning then read as fast as possible to check the speed? The key to solving a problem like this is narrowing it down to a specific spot that takes a long time. Try using the Tick Count VI in different places, then subtract the outputs to figure out where the delay is happening.
I've always found that studying for exams helped me learn the most applicable information. If you already have some background in LabVIEW and need a refresher, I would check out [study material for the CLD exam](http://www.ni.com/tutorial/14554/en/) If you want a refresher on the basic stuff, there's always [the CLAD](http://www.ni.com/example/30225/en/).
Thankfully I've learned Saber through my work which is formatted the same way, except it's for circuit design, not programming. So it shouldn't be too hard to pick up since I know some formatting as well as programming logic. And the little bit of work I did with it in high school still sticks in my head, so I remember how to use it somewhat. I definitely couldn't use it to it's full capability right now though. So thanks for those links.
ni.com/lv101 has a good set of basic tutorials and excersizes, and there are several great textbooks available online if you like learning that way.
Still being in college, books are my current nemesis. If I can get away with not spending money, I'd prefer it, since it wouldn't be something I could get reimbursed through my company. Thanks for the link though!
No problem, feel free to come back to this sub if any more questions down the line.
I second this, I haven't watched their YouTube videos, but I learned LabVIEW at the beginning from zero using their Core 1 and Core 2 videos (not free). I watched so many of those videos, that guy's voice haunted my dreams. [Link to the YouTube videos](https://www.youtube.com/user/Sixclear/videos) [Link to their site](https://www.sixclear.com/labview-training/)
1) I think what you're looking for is the "Available Samples" property. Configure your task for the 1kHz and some large-ish buffer size before entering the While loop. Wire the DAQmx task that you created for the 1kHz sample rate in to a Read Properties node and pull the Available Samples Per Channel. This will give you an integer that you can immediately wire in to the DAQmx Read node. If you're DAQmx task has been configured to sample at 1kHz, the rest depends on your loop rate. Usually, the slower the better to save on processor utilization. This can be 4Hz for all DAQmx cares as long as your task has enough space in its buffer for that rate. If you're just updating a front panel, or just logging to file, a slow loop rate is fine. 2) [Sometimes it's easier to google things when you know key words to look for.](http://digital.ni.com/public.nsf/allkb/DC67507C500AE8138625743B0074FEF8) I think this is what you need.
1) The "number of samples" parameter in the DAQ assistant will tell you the fetch size, so Number of samples/Sampling rate is the loop time. a good rule of thumb is to have it around 1/10th of a second. Its easier to push faster on higher speed/lower latency busses like PXIe (you can hit 10+ kHz there), but fetching 1-100 times a second usually wont get you in too much trouble on any of the common interfaces Glorypants answered the second question perfectly. 
Thank you, both answers are very helpful
Wait, can't the number from the error cluster be compared to your file, and the details field filled similarly out?
There's a few different VI's that spit out the date and time, as well as let you format it. Just concatenated it to your file name and you're all set. 
How can I typecast the string as a file path? I hate those little red triangles and would like to avoid if possible.
Concatenate the time/date string with "log". Should be a function for it. Sorry I'm on my phone.. Can't test it at the moment. 
Okay. Thanks!
[Imgur](http://i.imgur.com/HteD3qL.png) Here is a code snippet that shows you one way to do it.
Go ahead and post the code. We love code here.
[Imgur](http://i.imgur.com/6JEwfoT.jpg) Thats the snippet of code thats giving me the most problems. [Imgur](http://i.imgur.com/dkkjaZg.jpg) And that's just highlighting where the error comes up.
I posted the code in another comment because I'm bad at reddit, so you can take a look there
That is a TCP read. Error 56 means it's timing out. This means that the node is waiting for the buffer to fill up, and it's not getting filled up in the time specified (since it's unwired in you code, it's waiting 25 seconds). Why isn't it getting filled up? my guess is that the hardware is never sending anything. You'll have to look at the protocol to determine what's supposed to be happening. Looking at the code you posted, my guess is this was written by a lot of guessing and checking, which is why you have such inconsistent results.
String to Path VI on the conversion sub-palette 
This code was actually written by the same people that made the hardware that im using (the physical EMG sensors). I recently got ahold of the software and hardware and was originally trying to optimize it, but now I can't even get the base program to run. There should be information sent through the port, because if I run a program to do the same thing in a different language (like matlab) it works fine, so Im not sure why its not receiving all the data it should
I think I understand most of it, but what's the shift register used for in the while loops? 
I've tried dialing back the read, but Wireshark is actually a really good idea. Thanks
Sounds like a problem with your USB port. I'm pretty sure those 6008/6009 devices blink the LED just from power being supplied through USB. That being said, the NI 6008/6009 are very cheap... I've had one spontaneously stop working when I didn't change anything. Had to be replaced.
you may have an issue with your usb port, or might have a USB port which doesnt provide enough power (rare, but it happens). Try another port if its working on another computer. 
As long as it works in another computer, then your USB port is probably to blame. If you're using a laptop, does it work differently when you're plugged into the wall vs. running on battery? The easy solution is to get a separately powered USB hub and use that to run the device. I've had to do that in the past just to make the 6008/9 more reliable.
Update: The daq shows up on Max with a yellow exclamation point. It says I do not have a driver for it. I have daqmx 14.1. Does anyone know if that is the correct driver? Thanks for your help guys.
That should be all you need. There is a [version 14.2](http://www.ni.com/download/ni-daqmx-14.2/5046/en/) out, but that shouldn't be needed. Sometimes a simple driver update can fix the problem, though. Could have been a bad install of 14.1, too.
I usually set up an error case on a TCP listener to ignore error 56. You can use this combined with an event timeout or the built in timeout input on "TCP wait on listener" to poll it and ignore the timeouts but catch any real errors. EDIT: Just saw your code snippet, you can do the same thing i just described to the TCP read VI since this is a client program.
My MAX install was faulty the first time around. Might wanna try a clean install, OP
You're going to want an event structure with a speed control and a distance control on the front panel. You'll use a driver to communicate between your UI and the hardware. The driver is hardware dependent so we'll have to know a bit more before I can be more specific.
What's the name of the stage? How does it connect to your computer? Is the controller embedded, or just run off a laptop?
I don't understand - it takes 5-10 minutes for one iteration? Is there some particular reason it behaves like this?
Did you try different USB ports? Are you working on administrator account? Did you check if your firewall/antivirus logged any warnings?
I'm assuming you're using a wait function to pause your loop iteration for a period of minutes. That is a bad idea. The much better option is to have a two state case structure: one case a time counter and one that contains the actual code you want to run. The time counter counts the time since you last entered the functional case, when the time is &gt;= the wait time you want it moves to the functional case for one single iteration and then returns to the time counter case. This way you can have a reactive loop instead of one that will lockup for long periods of time.
It's a controller for a biological system, whose natural frequencies range between 3 and 12 minutes. Any faster speeds yield redundancies.
So the time controller (which determines iteration length) is dependent upon the current and past sensor values. The sensor values, along with the other controllers are inside the while loop with the time controller. With your construction, I'd have to pass information between cases... I think. What are the disadvantages to using the timing primitive in a while loop?
Sorry I'm not 100% sure on your setup here. Could you post a screenshot of the block diagram? I'm not on my work computer so don't have LabVIEW here to post an example of what I'm getting at. What are you doing in the 5/10 minute iterations? Is the loop mostly sitting and waiting for the time period to finish or is it performing actions throughout the iteration time?
I like to use occurrences for easy and efficient interruptible sleeps. In one loop you handle the UI, and when the stop button is pressed you set the occurrence. In the control loop when you want to sleep then instead of using something like Wait (ms) you use Wait on Occurrence with a timeout. If the timeout expired then you keep going. Otherwise you stop the loop. It's important not to ignore previous because that would lead to a race condition. For more complex use cases I would suggest using user events and an event structure with a variable timeout. 
You da man, this makes things more clear. Thanks!
You got it right. I like to tell students "refnums are essentially pointers". Works for everyone with previous software experience. Might have to use your street address example for those students who've never developed software before.
I think what you want is something like this. [Code](http://i.imgur.com/zTebih0.png) The loop is iterating with a 50ms wait, however, it's usually only executing the False case and whatever other code you have in the loop. Once the current timestamp minus the original timestamp is equal to or greater than 600 (the number of seconds in five minutes) than execute your code in the True case. In the True case, make sure you pass the current timestamp into the shift register. 
Take the classes through ni. Core 1, 2 and 3
I had a lot of experience using VHDL before I decided to pick up labview. Programmatically, it's very similar to those sorts of languages. LV also has tons of built-in OOP goodies, so experience with Java, C++, or Python will also help you to program. As far as learning syntax goes, the NI courses, and Sixclear's free Youtube videos were a great help for getting me started.
thats good to know, i am quite familiar with Java, not a pro, but I can make some fairly useful programs. Did it take you long to pick up? 
Not at all! Once you find out where stuff is, LV has pretty much all the nice things you come to expect from a modern language. You just gotta learn to use dataflow to enforce proper execution order (make use of the error cluster). A bit of googling, posting here and on the official NI forums, and you'll be fine!
gotcha, thanks! 
forums + reddit and ill be a pro, got it!
Pick a project. A simple task, I recommend something involving basic file I/O. Establish exactly what you want to do before you open LabVIEW, or else you 're likely to get overwhelmed. If you get stuck, open up the example finder and search for something in the category of your project. There are TONS of usable examples. Just make sure to save a copy of the example under a new name if you want use it as a base and modify it. Post questions here, lots of folks with lots of experience willing to help. Like me!
This sounds like a perfect application for a state machine with a 'Check Time' state. There's of course a bit more overhead involved here, but once you get that in place, it's a snap to change your loop iteration timings. The Elapsed Time express VI is perfect for this, you just pass it a TRUE for the Reset? input along with your desired Target Time value when you want to kick off a new loop timing, then wire the Time Elapsed? Boolean output to a case downstream that contains your state transition logic. Please don't hesitate to ask if you don't understand anything I've just said.
Doubleclick the VI and examine the block diagram. You will notice that it is pretty much just a property node with multiple properties. Property #8 is the Arg 8, that throws the error. Thus it means it has something to do with flow control.
Set up an architecture such as a producer consumer(data) that will listen for commands in the producer loop by polling or use an event timeout. You can then queue these commands into messages for any number of consumer loops. That way you can either have one thread that handles the messages while another handles the state machine, or have one state in your main state machine that dequeues a command when ready. http://www.ni.com/white-paper/3023/en/ On a side note, whenever you end up concatenating more than two strings together, use the "Format into String" function so all you have to do is change the format string whenever a change is needed. http://zone.ni.com/reference/en-XX/help/371361J-01/glang/format_into_string/ Good luck!
From what you wrote my impression is that you are not 100% sure what you would like to achieve as you didn't write down your clear goal (asynchronous programming is a tool not a method). Don't take it as a negative critics but rather as a suggestion to be able to move forward. If your goal is to execute a certain group of commands while keeping a certain but not very strict order then I would create a scheduler part (the QueueDrivenStateMachine-QDSM) is great for that with an Event Structure. And I'd also create a part that continuously listens on the port, parses the message and throw an event to the event case when a program finished so the scheduler will now (like an interrupt in a uP). Search for examples on Registering dynamic events related to event structures, there are some really great ones!
*wanted to write a tool not a goal
Most of these answers are already pretty good starts. But is your company asking you to self-learn without investing in formal training? If so, I hope you don't have an immediate project that you'll need to deliver on right away. If the answer is no, then you should be ok. But it also surprises me how many companies will throw their customer into the proverbial fire without investing accordingly (ie: paid training). I guess it depends if LabVIEW is central to your role or more a partial focus. Self-paced is fine, but it'll take a lot longer.
It might be helpful if you drew a communication diagram sort of like this: http://www.rfwireless-world.com/Articles/SMS-MO-MT-call-flow.html That will lead you to towards an architecture that can work. Off the top of my head, you want a while loop that owns the receive part of the serial port. That loop then acts on the messages it receives. If some of your senders need data from this loop it should create a queue and pass that queue to the receive loop either using a functional global variable or the receive loop's queue. An event structure in a separate loop is a good way to launch your senders.
Try this:http://digital.ni.com/public.nsf/websearch/FCCCBC6B519CEC2E86256C95007E8FA9?OpenDocument
Thanks. It seems those instructions aren't relevant to any recent version of labview. I don't seem to have any option to load a build template, nor are there any .bld files anywhere in my labview directory.
I had this same problem once. My resolution was to open up ODBC as an administrator (windows icon -&gt; search for odbc -&gt; right-click &gt; run as adminstrator) and created a new System DSN for the server. Once I did that, I had no issues.
In addition, they operate similar to pointers in C -- providing a way to reference something in memory that is not normally accessible in the scope you are operating in.
This is a really stupid question, but: Could it have anything to do with the absolute path name?
I started rebuilding my code from scratch in a new project and everything is working, even after I build it. If I take the dsn file from the working project and try it in the bad project, no go. I've given up trying to figure out what's wrong. Almost have my program back to the state the original one was in and everything is still working. I appreciate the suggestion, though. I'm pretty convinced something got corrupted in my old project. I tried so many things to fix the issue I lost track of all the changes I made. 
I think what you want to do is wire a value for the peak to the case selector (question mark thingy). Then, in the cases, type in values or value ranges. For example, if a value of 10kHz gets sent to the selector, you should have a case "9000.. 11000" that would handle this case. Using a case structure like this allows you to code for a bunch of different possibilities. If your code needs to run reeeeaaaally fast, you might want to look in to nested case structures. This is because using a case structure like above takes ever so slightly longer for the code to run through because it goes from the first case to the last in order until it finds the right one.
OK. I think I see what you're getting at. &gt;I think what you want to do is wire a value for the peak to the case selector (question mark thingy). Would this be something like a numeric constant? &gt;Then, in the cases, type in values or value ranges. How would I do this? I understand the principle, but I'm not sure what commands I need to do it (that's my issue with LabView in general). Thanks a lot.
Alright, so I guess I'll try to relate it to the picture you shared. You should be able to get a numeric value out of your Spectra data somehow (maybe using some of the mathematics VIs. I don't know what your data is going to look like, but I'm assuming you're going to get one peak at a certain frequency that is greater than other peaks. Once you have that peak location (10kHz, 12kHz, whatever) you should have it in the form of a numeric (10000, 12000, ... ). Now that you have a numeric value that you want to use to get you coordinate output, wire that numeric to the case selector. You have a blank case selector in your image, so just wire to that question mark input on it. Once it's wired there, the "True" will change to a number. You can then click on the number and change it to whatever you want, like "10000". This will then activate that specific case if the input wired in is exactly 10000. You can right click this bar at the top, too, to edit the other cases, add cases, etc. In general, you can't think of LabVIEW in terms of "commands." This is where a lot of people who come from other programming language backgrounds get frustrated. It's all graphical, so you use your mouse a lot more than your keyboard (unless you get good with the quickdrop, but that's another topic). You need to think of programming in labview in terms of data flow (wires) and functions (VIs). The palette has almost any VI that you could want in order to perform operations on your data. 
It's all pretty well documented in the help menu. Turn on context help and hover over the structures to learn more about how to use them http://www.ni.com/getting-started/labview-basics/product-help If you wire in a number into the case structure selector, you can type numbers and ranges into the selector at the top http://zone.ni.com/reference/en-XX/help/371361J-01/glang/case_structure/ You're going to convert your dynamic data type to a numeric wire so that you can use the normal numeric functions on it http://zone.ni.com/reference/en-XX/help/371361K-01/lvexpress/convert_from_dynamic_data/
Thank you very much. This should be a great help! 
&gt;I don't know what your data is going to look like, but I'm assuming you're going to get one peak at a certain frequency that is greater than other peaks. This is correct. &gt;Once you have that peak location (10kHz, 12kHz, whatever) you should have it in the form of a numeric (10000, 12000, ... ). Is this something the FFT can accomplish, or is there another step that I need to add? &gt;Now that you have a numeric value that you want to use to get you coordinate output, wire that numeric to the case selector. You have a blank case selector in your image, so just wire to that question mark input on it. Once it's wired there, the "True" will change to a number. You can then click on the number and change it to whatever you want, like "10000". This will then activate that specific case if the input wired in is exactly 10000. You can right click this bar at the top, too, to edit the other cases, add cases, etc. OK, that makes sense. &gt;In general, you can't think of LabVIEW in terms of "commands." This is where a lot of people who come from other programming language backgrounds get frustrated. It's all graphical, so you use your mouse a lot more than your keyboard (unless you get good with the quickdrop, but that's another topic). You need to think of programming in labview in terms of data flow (wires) and functions (VIs). The palette has almost any VI that you could want in order to perform operations on your data. I see what you're saying. I have no programming experience at all. I guess I use the term "command" more generally because I'm telling the software to do something, in this case, but placing icons, values, etc. LabView is certainly a different way of thinking. Thanks again! 
The "amplitude/time" representation is referred to a "time-domain representation" and the "intensity/frequency representation" is called "frequency domain representation". I would look up the help document for the "case structure". It will inform you that most data types can be directly fed into the case selector. From the looks of the red dot on the boolean case structure, you are having a type mismatch between whatever datatype the blue wire is (variant? waveform? I can never tell) and the case selector. From the sounds of it, you need to extract a numeric value from the output of the "FFT - Peak" output from the express VI you are using that represents the strongest frequency component in the data you captured from the DAQ Assistant express VI. That numeric data will then be fed into the case structure and you will need to define certain numeric cases. As /u/Glorypants indicated, to trigger on a 10KHz wave, your case selector might look something like "9000..11000" (anything from 9KHz to 11KHz) The easiest thing is just to place a probe on the output blue wire from the FFT analysis VI and see exactly what datatype you are dealing with (hovering over the wire with context help on "ctrl+h" may accomplish the same thing.) Also, if any words we use dont make sense, just search for them in the labview help document -- its surprisingly detailed and easy to use.
You may want to look up basic labview tutorials, which will guide you through the basic structures in labview (wires, terminals, loops, case structures, data types).
I'm not too familiar with FFT, but just playing around with a spectrum output for a minute, you probably want to [do something like this](http://i.imgur.com/4We58N0.png). Like others have said here, you should look through the help documentation for FFT, for case structures, and just basic labview tutorials in general. It's hard to just dive in the LabVIEW with no previous experience, especially not programming experience. LabVIEW is a very capable program that takes a lot of experience to learn it's full potential. I still learn new features every day. Edit: If you have LabVIEW 2013, the image I linked above can be downloaded then dragged on to your LabVIEW block diagram and LabVIEW will convert it to actual code right there.
Thanks! I'm away from my computer right now. I'll try what you're suggesting. You are right about pulling numeric values from the peak info. 
Probably not a bad idea. I just wish this wasn't so close to the end of the semester. 
Thanks! I have the student LabView on my laptop. We have the 2013 version at school. 
Yeah. I probably won't be using it that much in the long run. I'm a chemistry major and I'm just taking this physics class as an elective. 
The short answer to both questions: it depends. What do you intend to do when an error is encountered and where are you checking for errors? I typically check for errors at the end of every iteration of a loop and decide what to do at that point without using a shift register, which involves assessing the severity of the error, logging it to disk, and possibly notifying the user. If severe enough, I may decide to reset hardware to a safe state and halt execution of the application by enqueuing a shutdown procedure. See the Queued Message Handler project template for an example/model of this architecture. A shift register is useful for error handling when you're checking for errors at the beginning of the loop instead of at the end as you would need to check the status of the previous iteration prior to execution. This is usually a model I see when users are initializing resources prior to entry into the loop and is common to find in example code where the model is typically init-&gt;do something in a loop-&gt;close reference(s). You could also check out the free structured error handler here: http://www.ni.com/example/31253/en/ As for reference wires, I typically end up passing these in a shift register or storing the reference in a functional global or other data structure when I plan on ever closing the reference/connection and creating a new one inside of a loop. For example VIs, you're going to get a mix of implementations as some developers are more accustomed than others with architectures like the QMH and likely used shift registers more out of habit than anything else. 
I realize it is generic but I dont have much more information, it is pretty open ended. I will attempt to explain more in depth. I described the physical system but I will be coding this to virtually simulate the physical system. There will be a virtual system vi with inputs and outputs of pwm, duration or gathering samples, and directional input. I need to take this virtual system of a lens on a slidepot focusing a laser onto a photodiode. I need to do this two different ways, 1). By using the voltage of the photodiode or light received. This is done by using arrays and output values to find gradients and determining where the 'slidepot' is and move it to the peak or where the light is focused. 2). Use the voltage from the encoder on the slidepot and use its actual position to move to find the focus point. With this we need both voltages to determine. I hope this clarifies. Its difficult to explain because the physical system does not actually exist. I just need a direction to go with the coding. 
I attempted at explaining again in my other response, but as for the physical system yes you sound exactly correct. The thing is physically I am not doing anything. I am virtually simulating the physical system with code and am not sure where to go with it. I am having trouble coding the system to automatically focus and 're position' itself to the peak of the graph where the light is focused through a 'pinhole' and as you state achieve nominal voltage for this perfect light. I also will post a few images of an outline of how to code it. Hope this helps, I apologize for the terrible explanations 
I think I get right you're angling at. [This should sort of work, or at least get you started.](http://i.imgur.com/YpHJmTn.png) You can't see it in the screenshot, but the false case is exactly the reverse of the true case. On true, we decrement, on false we increment. And make sure you're AND'ing the array, not OR'ing it. Obviously the while loop in this never stops, so you'd need to define some sort of stop condition. One way I would do it would be by removing one element of the historical array each time you reverse directions. Once you're down to one element left, you know you're at the peak. For your data you'd obviously want to play with the size of the historical array for tuning purposes. Once you go from simulation to implementation than you'd replace the array and index array with an actual call to make the photo diode measurement, and the increment/decrement would be used to adjust the position of your light.
For some of the not so obvious ones, I'd say a good rule of thumb is "am I modifying this data between iterations?". If yes, use a shift register, otherwise a simple node is fine. So, unless you are destroying a queue reference and opening a new one somewhere in your loop, you don't really need a shift register. For errors, like /u/RDR350Z said, if you're handling errors in a specific state in your loop, use a shift register. If you're handling errors every iteration of the loop it's not necessary. I use user-defined events (UDEs) a lot. I will dynamically register and deregister for events from time to time and will use a shift register to store the "registered event refnum" because of that. If I wasn't, I wouldn't bother with a shift register.
&gt; For some of the not so obvious ones, I'd say a good rule of thumb is "am I modifying this data between iterations?". If yes, use a shift register, otherwise a simple node is fine. this is great advice
Thanks, another way to look at it is "will this data change?". For the error example /u/RDR350Z mentioned, he's logging errors in every iteration and presumably clearing them. Thus, the result after the end of the loop is always no error. Edit: He didn't mention anything about logging them, but point still stands.
Thank you I shall use your advice and see if it gets me anywhere. 
The advice from Muun is perfect and i would follow that as a good practice. However, as you use labview more and more one thing you may notice is that not all wires are the same. I'm probably not going to explain this very well, but a queue wire is slightly different from a "error wire" or "data wire". It basically holds a "reference" to the queue and not any actual queue data within the queue. Essentially for a queue wire, the shift register is built in. Anywhere you take that wire, you have access to everything about the queue and data in the queue. If you destroy the queue in one loop, the other loop will see that b/c the reference to queue is now gone. This is different from other wires. If you modify an "error wire" in one loop, you will not see that in another loop, b/c the error wire actually holds the data, and that data is carried over between iterations inside of a while loop. The other loop that you have branched that wire too has no idea whats going on. This behavior of the queue wire is one reason they are used to pass data between different parts of the program. There are lots of other wires like this, Notifiers, DVR, References..... With that said there are still best practices to handling these wire types in order to avoid race conditions but its something to keep in mind and probably one reason you are seeing shift registers in some examples and not in others. 
I would like to mention however that if you are using a for loop and passing something through, like a reference, you should always use a shift register. Even when you are not modifying the data in the for loop. If the for loop get into a case with no iterations and you just have terminals, the loop will not run to get data to the exiting terminal. With a shift register the data will make it to the other side and will not be lost.
That's cool, just a simple "if slope is negative, go back." How did you do the slider graphic?
That part was provided to me in the class I'm taking, so unfortunately I don't know how it works. I'd imagine it uses a formula node or something?
have a look at http://www.ni.com/tutorials/ And don't worry you will quickly be able to get things done in Labview.
Thank you! I'll take a look at those tonight. How much of it is learning how the program works vs learning how to program?
In my opinion, programming in LabVIEW is mostly learning the program. That being said, it's easy to build a working application using examples and other people's code that they post online. The code might not be the best when you're learning, but you can pretty easily make it work.
Throw away obviously: Ask around. Do you have any CPI's at the company? (certified professional instructor). If so, they'll have access to all the training material... Skim core 1 and core 2. Do core 3. Do advanced architectures.Do DAQ. Do OO. and start adding on extras and you're competent in 3 weeks. *edit - added in extra courses
http://www.ni.com/getting-started/ is also a great resource!
Def the way to go
About 2 months ago I got a promotion into a job that is 80% LabVIEW based on the success I had with a previous project. A large part of the success I had on said previous project was from watching all of the SixClear VI High episodes: (https://www.youtube.com/user/Sixclear) 
do you know how to implement user events within the event structure? theyre pretty nice for async comms between processes...
It looks like its probably a Picture Control/Indicator with several iterations of the animation in it. Then they just set the frame based on the output of the formula. 
Hey! cool demo You can check the NI alliance directory at http://www.ni.com/alliance/ to see different companies in each region that are partnered with NI and see if anything catches your interest. There's also a lot of LabVIEW used in aerospace test so it might be worth it to just start reaching out to different companies. If you're interested in southern ontario with a diverse project range but nothing in particularly aerospace specific check us out at prolucid.ca . We haven't done anything aerospace specific yet, but we've been making bigger pushes into regulated industries the last few years and with nuclear/medical covered I can't imagine aerospace is too far off. 
I'd be happy to take a look at your simulator. I've been dabbling with creating one for a quadcopter, based on work I did in my senior year in college. A guaranteed way to get a LabVIEW-focused job is to work for an NI systems integrator. These can be found, as /u/willisbueller pointed out, on NI's list of Alliance Partners. Be advised though, you end up working on whatever project is put in front of you. This can be anything from creating an automated test system for a high-volume manufacturing application (think some sort of PCB functional test) to high-force, high-speed, real-time motion control applications. Companies like Averna, Wineman Technology, and Bloomy Controls are a few of the big ones. Know also that LabVIEW is being used more and more by a larger and larger spectrum of companies. Even NASA is using it a lot. I have no idea what your interests are, other than aircraft simulators, obviously, but keep your eyes open and get familiar with the body of work a given company has. What geographical location are you looking for, if any?
Thanks! I haven't done any work with quadcopters yet, but certainly ping me if you ever push any interesting sim code. An NI systems integrator sounds like a potentially good fit. I'll look more into this. Basically, get contracted to a project, complete it, move on to the next project? Thank you for the company names, I'll take a look at them. In terms of geographical location, I don't really care too much. The ideal job would let me travel as much as possible since I'm still young and not tied down yet. Are there contracting jobs that would move me from location to location depending on where the project was? If not, then a Los Angeles location would be my top pick. I worked in Hawthorne for a year and really enjoyed it :) 
SpaceX is big into labview and based in LA FYI
Thank you for the wealth of information! Systems integrator sounds exactly like the niche I'm looking for - design, wire, and program to completion. I will look into and start cranking out applications to these companies. In terms of the SIMULINK code, that was written by a Purdue professor a decade ago. Most of the math is based off of Roskam's aircraft dynamics textbooks from the 1930's. I simply wrapped the code and exported the resulting state space. The LabVIEW code was all written by me though. MATLAB was just not cutting it in terms of getting a "feel" for the aircraft that I was designing.
haha, yeah, that's where I was first introduced to LabVIEW - I was an intern there for about a year. Was only an internship though, I'm hoping to get back there someday
You can do that? That's why those error wires looked so strange.
I did it in Gimp
There's no super easy way to do this. At first glance it seems like it should be easy though. So for problems like this there's a good chance some nerd figured out a good way to do this back in the day. So I googled it and found this http://en.m.wikipedia.org/wiki/Fisher%E2%80%93Yates_shuffle Looks pretty easy to implement in labview
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**Fisher–Yates shuffle**](https://en.wikipedia.org/wiki/Fisher%E2%80%93Yates%20shuffle): [](#sfw) --- &gt;The __Fisher–Yates shuffle__ (named after [Ronald Fisher](https://en.wikipedia.org/wiki/Ronald_Fisher) and [Frank Yates](https://en.wikipedia.org/wiki/Frank_Yates)), also known as the __Knuth shuffle__ (after [Donald Knuth](https://en.wikipedia.org/wiki/Donald_Knuth)), is an [algorithm](https://en.wikipedia.org/wiki/Algorithm) for generating a [random permutation](https://en.wikipedia.org/wiki/Random_permutation) of a [finite set](https://en.wikipedia.org/wiki/Finite_set)—in plain terms, for randomly [shuffling](https://en.wikipedia.org/wiki/Shuffling) the set. A variant of the Fisher–Yates shuffle, known as __Sattolo's algorithm__, may be used to generate random [cyclic permutations](https://en.wikipedia.org/wiki/Cyclic_permutation) of length *n* instead. The Fisher–Yates shuffle is [unbiased](https://en.wikipedia.org/wiki/Biased_sample), so that every permutation is equally likely. The modern version of the algorithm is also rather efficient, requiring only time proportional to the number of items being shuffled and no additional storage space. &gt;==== &gt;[**Image**](https://i.imgur.com/mUN58yD.png) [^(i)](https://commons.wikimedia.org/wiki/File:Probabilities7.svg) --- ^Interesting: [^List ^of ^permutation ^topics](https://en.wikipedia.org/wiki/List_of_permutation_topics) ^| [^Frank ^Yates](https://en.wikipedia.org/wiki/Frank_Yates) ^Parent ^commenter ^can [^toggle ^NSFW](/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+cn7k3o4) ^or[](#or) [^delete](/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+cn7k3o4)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
I am sure there are plenty of examples [using the NIDAQ.](http://digital.ni.com/public.nsf/allkb/E3BAF6FC4017960B8625755A00525D37)
Always use shift register with references, especially in For Loops. Whereas a File Reference or Queue technically doesn't get modified during loop execution, if you pass the reference into a For Loop as a non-indexed tunnel, and pass it out the same way, it will work in most cases until you cause the Loop to execute 0 times at which point, the output tunnel of the reference is Null. Then you spend hours troubleshooting the error message of "Invalid Reference" only to realize a problem upstream of the code caused the loop to execute 0 times and invalidate the reference.
These questions are much easier to answer if we have some idea of your current experience level or software background. Have you ever used LabVIEW? Have you ever written any software in any language? Where and what have you searched for? What kind of NI acquisition device is it? What kind of sensors? How fast are things sampling? Etc...??? Sorry if this comes off as callous, but I'd like to see you get the assistance you need. However, writing a question like this will really just get you downvoted into oblivion. 
I have moderate experience with LabView. I come from a C++ background, in which I am proficient. I have looked around the NI website and Google a lot, as well as trying to diagnose the problems with the old program. I'm not sure of the model of the DAQ, but it's a USB device. One sensor is a load cell and the other is a pressure transducer. I have no problem getting a signal to the computer, but I cannot get data onto a plot in LabView, or into a text file. End goal is to get data versus time onto a plot and input into a spreadsheet. 
Awesome. [Here's a forum thread](http://forums.ni.com/t5/Academic-Hardware-Products-ELVIS/usb-6009-high-speed-continuous-acquisition-for-slow-control-loop/td-p/1296584) that should help you get started. The example shown in it deals with output as well as input, but the input onto the graph is fairly basic. Writing to a .csv file is fairly straightfoward, look at the examples in the Help &gt;&gt; Find Examples by searching for csv, the first result gives a good look at how to do it.
Look no further. [This book here](http://www.amazon.com/LabVIEW-Everyone-Graphical-Programming-Edition/dp/0131856723) is all you need. Comes with a CD and everything. I found it very useful.
You want to get data over http? Just use the [HTTP client VIs](http://i.imgur.com/CKX2yjQ.png). Getting data is [very easy](http://i.imgur.com/UOsmbnn.png). Sending data is slightly more complicated, but can easily be done with PUT.
Thank you
The Write to spreadsheet vi has a bool input for append or replace.
I think this should be easy, if I understand your question. There are a few different tunnels out of a for loop. You want the default tunnel, so it should be easy. Just wire the results out of your for loop and they should automatically become a 1D array. You can right click (or double click?) on the tunnel to change it but the default (it looks like an open square) is probably what you are looking for. 
the array needs to be outside the For loop :'( mannnn that was my whole issue!!!
now that I have that figured out I have one last question. I currently have a Tab Control leading into a case structure (3 options: Linear function 2nd degree polynomial and a Cubic polynomial) What I want to do is to implement the function into each case structure. Is the X that I was going to be the iterations + 0.1? 
 &gt;What I want to do is to implement the function into each case structure. &gt;Is the X that I was going to be the iterations + 0.1? I think you missed a word in the last sentence and I'm not quite understanding what X is or where you're using it 
I basically want to make f=x I'm not sure how to explain that so ill say it this way. I want the x to be in the case structure. For this X do you take it from the iteration line or do you have to do something different?
I'm still not understanding, can you write out the equation you want to implement for one of your cases, e.g. linear function?
yeah I have that all worked out. I'm just having trouble with trying to implement the actual function. I am wondering if i need to take the incremental (+.1) INTO the case structure for each case and manipulate that with various math symbols to feed it into a waveform graph
If I understand you correctly, yes, you would need to take the (.1 * N) into the case structure, do some different math in each case, and then bring the result out of the case structure to your graph.
would you mind taking a look at my block diagram and relevant graph as it pertains to my second degree polynomial (it looks wrong). I just need another set of eyes to see it and tell me if im doing anything wrong http://imgur.com/a/xjdRc there is the link to both the pictures of my function. Now I have to figure out how to do the derivative function of these
Error: your number generator is making (0.1, 1.1, 2.1, 3.1, ...., 90.1) which is not what you want. Secondly, I would move the chart outside the for loop. This way, you can create an array of your results, and plot that array, instead of the way you have it now, which is plotting one result at a time and adding it to the end of your chart. Look up "chart vs graph LabVIEW"
so to correct the issue should I have the 91 become a 10? I now transferred my waveform graphs outside the For Loop and I now get a good looking function. And I keep forgetting that the For Loop boundary changes the type of number you get :p Last question. How should I go about performing a derivative using the derivative VI. (im not sure what I should be linking into the inputs of the derivative) 
 &gt;so to correct the issue should I have the 91 become a 10? No, 91 is the right amount of loops, there are 91 values between 1.0 and 10.0 if you count by 0.1 increments. But the math you are doing on N in order to get X is not correct. &gt;I now transferred my waveform graphs outside the For Loop and I now get a good looking function. &gt;And I keep forgetting that the For Loop boundary changes the type of number you get :p Have you tried using "show execution"? Click the lightbulb on the block diagram and then run your VI. It can help you get an idea of how this works. &gt;Last question. How should I go about performing a derivative using the derivative VI. (im not sure what I should be linking into the inputs of the derivative) If you use the context help (control+h) you can see what the required inputs are. If you plan to use LabVIEW, I *highly* recommend you watch NI's videos called LabVIEW Core 1 and Core 2. I guarantee you'll spend less time total in the semester if you do Core 1 &amp; 2 and then do these problems than if you try to just figure it out 
Perfect thanks a lot man, you've been a lot of help! I'll watch those videos tomorrow
I am working on a research project where I will be taking temperature readings and collecting the data in excel. My professor told me to look at old programs to see examples of how he wants the program set up. In the section shown they take many sets of dynamic data convert it into arrays and then bundle the arrays just to unbundle them, change them back to dynamic data and then combine the data to plot it. To me this seems unnecessary to convert to arrays and bundle just to undo everything that was just done. If someone could explain the reasoning behind it I would greatly appreciate it.
how about some code comments or something...
Because code comments are ugly for something on the wall. It's a decoration.
Hi I'm a CPI and would like to weigh in briefly here. As mentioned in another comment, there appears to be a lot of express VIs. You'll benefit greatly by removing them. There are many examples out there on how to use the DAQmx pallet and I promise is very easy to use. If you need an example, pm me and I'll put something together to share with everyone. Also, I'd recommend using waveform as your data type. You'll get all of the data channels, and a timestamp for the start of each sample set. There is also a waveform pallet for working with waveforms easily. Lastly, take a look at the TDMI pallet under file oi. They use waveforms to write excel like data sheets with a cover page included. If you want, I can give a more detailed description on anything mentioned, just ask. I'll be happy to help. 
might i recommend right clicking on the loops and adding a sub diagram label for each one (unless that case in the case statement does a good job explaining functionality). Otherwise exceedingly neat. (OCD level)
First off I love this. Cool application and very tight block diagram. Minimal wire bends. I love it. The only thing I don't love: the way the error wires enter your subvis. If I were reviewing this code, I would mention it, but it wouldn't gate my approval. Overall, probably the best example of good code I've seen on this subreddit! Nice work!
My OCD problem with loop labels is that they're invisible to block diagram clean up. So I often wind up with them over wires, or messing up spacing.
Dunno. just experimented, could not replicate issue. It wont even let me move anything into the label, it will resize the forloop accordingly. Cleanup seems to work fine for my [quick test](http://imgur.com/pwaVhIh)
Huh, I'll muck about with it more later on then. I may have been missing something.
The cases are pretty descriptive and the names of the queues leading into each loop help name the loops.
The Example Finder is a great place to start, then there's always the NI forums and the LAVA forums.
You've done a lot of things right with your code from a cleanliness perspective, and it would be easy to hand this code off to someone else and they could probably figure out what is doing what with minimal effort. However, from an architecture standpoint, there's a lot that could be done to improve efficiency, re-use, and test-ability. Currently, you can't test any of the loops independent of one another, so your functions are tightly coupled to one another. It also seems like all of your loops are tightly coupled to front panel controls. So changing something on the front panel intrinsically means changing something in one of your loops. A perhaps better design might be to implement each of those loops as a standalone 'module' that may use data passed into it from the front panel controller, but isn't reliant on it. Than you can simulate the data into each module so you can unit test the module without having to have a front panel or camera or whatever. Than, each time your engineering team decides to change something you can implement the changes internally, unit test, than deploy it without having to take down the test system. Also, if you ever change hardware, as long as your API of your module never changes your top-level system code never needs to change. You just build a new module, drop it into place. You are at a great place to begin considering expanding your LabVIEW toolbox. Start looking into to how classes work. Maybe begin exploring how some OOP frameworks are implemented like the Actor Framework. Like I said at the top, this is good, and it works, which makes it even better. But, it's complicated, and tightly coupled everywhere. You've built a very nice looking house of cards, but unfortunately it's still a house of cards. It's time to begin to explore how to build a strong foundation for your house, and how to make it's rooms more modular so you can remove or change one without the whole thing coming down.
I don't know either but I would assume taking an output from a microcontroller?
Definitely need more information here, an embedded signal could be any output or input to an embedded system. Microcontroller/FPGA? 5V, 0V, 3.3, PWM, Sine etc etc
While the others are right I would add that you can embed one signal into an other. Having in mind that you have to create two 2D arrays I suppose this is your case. It is difficult to define in general what an embedded signal is but here are some examples to give you some hints: http://en.wikipedia.org/wiki/Self-clocking_signal http://en.wikipedia.org/wiki/Digital_watermarking I think the latter one represents what you're really meant to do. 
I emailed him and he told me that it is building an object inside a matrix. I think he misspoke maybe 
Hah sounds like someone sucks at labview
Hah got me there
What LabVIEW version are you using?
I would check the font selection for each item. Highlight the text and hit ctrl-0 to bring up the text settings and make sure you are consistent with the font types. 
Glad I re-read your post before responding, I was going to read you the riot act for telling OP to make sure the 'Scale all objects...' option was CHECKED. Clearly, you did the opposite, so have an upvote.
Are you seeing this on your development machine or after you install it on another machine? I've had the same problem with the latter scenario, and you MUST make sure that the screen resolutions match. In other words, if your target machine's monitor is 1920 x 1080, set yours to that as well. edit: Just to be clear, since you didn't mention specifically: Are you building an executable or just running the VI?
I'm seeing this on the development machine after I build the project into an executable and run that version of the program.
It looks like changing the font changes the size of the boxes but they're anchored at the center. I see three options: * I would think that setting the size of the font through the front panel editor and saving those changes would preserve the spacing. Then you can remove the code where you programmatically change the font size. * Change your justification on the left hand side of your block diagram, though I imagine that would only fix one side or the other. * Use yet more property nodes to programmatically set the location of the front panel objects after you change the text size.
sweet baby jesus
Check out TDMS files. It's a binary format designed by NI to literally address the exact problem you're looking at.
So it looks like the problem was I messed up the default font size on the front panel editor at some point. I deleted labview.ini and got everything set back to default and when I opened my front panel again, it looked the way it looks after I built the program. So I moved things back to normal, rebuilt the program, and now everything looks as it should. Thanks for your help. :)
So it looks like the problem was I messed up the default font size on the front panel editor at some point. I deleted labview.ini and got everything set back to default and when I opened my front panel again, it looked the way it looks after I built the program. So I moved things back to normal, rebuilt the program, and now everything looks as it should. Thanks for your help. :)
Perfect, glad it worked out!
Much appreciated! That makes a lot more sense.
Therefore the emphasis is on minimalism. Basically the anti [this](http://www.ni.com/cms/images/devzone/pub/nrjsxmfm912163998723206173.jpg) 
Ah okay I think I get your question. Each data point is just a number. I think it's an 8 bit string if I remember correctly. And yea that's fine. I can't see us collecting data forever lol. 
I just checked it's actually single (32-bit) real (~6 digit precision) 2D array that I'm taking in as my data. The data is coming in through sensors which collect data every 1/14th of a second. 
Nope, I totally understand now. I think I got confused somewhere along the way. I mocked up a quick application to see how large your queue would get. That 1/14th of a second (~75ms) should be slow enough that your queue will only ever contain 1 data point; you will be able to write the data to file almost as fast as it comes in. That being said the system I am suggesting may be a little overkill. In my test queue based system it took 7.5s to process 100 8-bit data bytes at 14Hz. I modified the application to remove the queue and just wrote the data to file as fast as I got it and I was still able to process the same amount of data (100 bytes) in 7.5s. In this case the queue system doesn't do much for you because your data acquisition is so slow. I still prefer to implement the queue system as it does ensure that the ability to write to the hard drive doesn't interfere with data collection, but if you are struggling with getting it to work simply writing the data to file after you collect it (one byte at a time) would probably work fine in your case.
What are the dimensions of the 2D array?
Are you sure you're not exceeding your timeout in the loop with the FIFO write? Try monitoring Timed Out? maybe.
All the range for single-ended analog input: raw data between 0 to 4095.
So I do just have one quick question. My data comes in as an array, so when I try to write the data to disk it only saves one instance of the array. How can that it saves all the data instead of rewriting the data every second?
This is very strange... it even looks like you mimicked NI's sample code for reading from an FPGA FIFO. Have you tried narrowing down the specific item that is getting missed? (i.e. confirm that it's the first element not being written instead of it maybe reading 5 elements at some point later) You could do this by simply writing the iteration index in to the FIFO, so you would end up reading [0,0,0,0],[1,1,1,1][2,2,...][...]. This would help you narrow down the issue to make sure you're chasing the right problem.
Short answer is no. I use binary in my examples because that's how I need to store my data. You can easily switch from Write Binary to Write Text so it can be opened and read with something like notepad. It can be opened with Excel too but you my need some additional formatting to make it come put the way you want it. 
Sorry, the point isn't decimal, sample rate is 100k Hz, which the depth is set as 5000, fairly enough for 4 channels in 10 ms loop (1000 samples per channel).
I removed the FIFO.Configure and the problem persist.
I work full time 9-5 doing LabVIEW. I have a degree in Mechanical Engineering and picked up LabVIEW once I started at my company. Over the span of a year I went from designing in SolidWorks 40hrs/wk to developing in LabVIEW. LabVIEW is really the only language I am proficient in at this point, though I learned MatLab and Python in school. I hope to teach myself to program iOS soon, but so far this New Year's resolution is failing... My company is small (15 employees total, 1 CLAD, 2 CLD, 1 CLA) and does a very wide variety of projects. This makes my day-to-day always different. I am currently working on three different projects in three very different fields. As the CLA I am consulted on new projects that might come in and do the initial architecture when needed. Also, since I still have my mechanical abilities, I jump in to help out sometimes with any ME projects that are running behind.
I've been primarily a Labview programmer for about 20 years now. Started as an EE with a software emphasis. Used VB, C++, C#, Assembly etc. Now I'm a 9-5 Labview programmer, still haven't gone for my certification yet but that's a goal for this year. 
I do. I work from home for an Alliance Partner that has an office in another state. My days are great: get the wife and kids out the door, design and develop in my pajamas (I do have a strict pants policy), and find time to run at some point during the day. I've had to work in a few languages (mostly for reverse engineering or supporting third party systems) over the years, but LabVIEW is definitely our tool of choice. I've worked in Agilent VEE and am spending a good deal of time with C++ these days. Since I'm working from home, all of the meetings I attend are on Lync, so I spend a fair amount of time on that. And reddit, obviously. 
*this*, is my dream. I make a great living doing a 9-5, but on days i get to stay home and code, those are the best (+ i have a 4k monitor on my pc! great for LV). Would love to be a consultant...So hard to take the plunge from a steady paycheck though.
Can you explain how you ended up in this gig? It sounds like something I would love to do. 
I do. Usually working on projects that are 3-4 months in length. Come in, sit down, do code, go home, repeat. Occasional meetings, and a product integration at the client site at the end of the project.
(ง ͠° ͟ل͜ ͡°)ง
Well I'll be
Dual 24" monitors at 1920 x 1080. Don't let extra real estate make you lazy! Remember to code with modularity in mind and don't stop make subVIs where appropriate.
I have a 28" 4k monitor at home and it sucks to use LabVIEW (2011) on it. The problem is that nothing scales well with the resolution, so everything is half as large as on a 1920x1200 monitor of the same size. Either windows or my monitor can scale that to 1920x1200 but neither will scale without blurring, which results in headache inducing eye-strain foe me. I'd stay away from a 4k monitor for LV unless NI implements native support for different DPI scaling. That being said, programs which can handle scaling for different DPI look gorgeous (especially CAD).
Yes, this. Keep in mind that whatever you're developing is probably going to need to be integrated at some point and that's probably going to happen from a laptop. If it doesn't fit on a laptop screen without scrolling it's too damn big.
I am a LabVIEW Consultant/NI Alliance Partner. I work almost exclusively in LabVIEW with some occasional TestStand, or reverse engineering of other languages to re-implement in LabVIEW. My day can vary widely based on the project(s). Some projects require onsite development/testing, others can be done from home. Today I had a conference call with a customer several states away to talk about the implementation of a feature, then had to review some quotes to build a second system for a different customer. I will spend part of the afternoon coding, and will also run over to a local customer who just had a personnel change on the project to make sure they order the correct parts for a second test system we have been developing. And I need to run my dogs over to the vet for their annual shots.
I'm not quite sure I follow what you're attempting to do, but to answer the question here. The easiest and most straightforward way to read/write a cluster from/to file is to use the MGI add-on "Read/Write Anything" which you can find in VIPM. This gives you a human readable .ini file format. Although, I myself prefer JSON or XML, but that's not the question here. Back to the topic though. Let me re-state the problem to see if I understand. Your team's robot has a number of values stored in an .ini file. You need to read these values from the file on startup and than use those values in approximately 45 places all over the code. Is that the gist of it?
You're looking for an "event structure." Sixclear has a nice tutorial on them [here](http://blog.sixclear.com/post/2927225020/labview-events)
I've seen the tutorial and I'm not sure how this will do what I need, could you elaborate? Specifically, what will trigger the event?
So you put down an event structure, put your code inside, and right click the border, do add event case. LV will automatically detect what things within the structure can trigger an event, and what types of events they trigger. In your case, youd select the averaging code/variable/wire and add a case for a value change. Click okay and the event structure will have a new pane in its dropdown that will let you write code to execute when that event happens.
Thanks! I'll go experiment.
Don't slow down. If you get stuck on one part start working on the other parts. Try to do as much as possible on fast autopilot before accidentally wasting too much time solving problems.
I was hoping this would be a response.
Are you using VI Analyzer to check your work? If not, you should be.
I would hope that you were enforcing the time limit when taking your practice exams. If you have, you should understand the need to prioritize. Understand the specifications and requirements fully, get all your code infrastructure in place, then make the thing work. Save putting the documentation in the tip strips, etc., for last. The one exception to this last bit of advice is if you can't get a certain function implemented. It's not ideal, but putting a comment outlining what your plan was and saying you ran out of time or something is FAR better than not doing anything and leaving a feature out. Best piece of advice I got from my former boss: "Make it work, then make it pretty." BTW, you WILL lose a point or two for "Unnecessary wire bends". edit: GOOD LUCK!
how would that help? the problem seems to be that the reading is happening too fast after the previous one. how would an event structure help this?
Correct me if I'm wrong, but arent event structure used exclusively for asynchronous code execution?
interesting. you might provide that feedback to the manufacturer. they might have another VI that provides the wait i was talking about, or they might have had a bug in their VI. either way, they might appreciate it.
Run the VI analyzer! Most of the points you can lose for style (wire bends, etc) are generated from the grader running the VI analyzer on your code. 
I see. What solution would you propose? 
There's still a DASYLab forum at NI.com, [here](http://forums.ni.com/t5/DASYLab/bd-p/50) Not sure if you knew, but NI bought up DASYLab a few years ago. 
That's your problem. System fonts. The scourge of designers everywhere. What is a system font on your computer is different on another computer and likely even different on your own computer in and out of the development environment. It's because the system font goes and asks Windows what to use for a font and sizing. Windows gives back the font that's been selected, along with the size that being used. Don't use system font. Specify a font, like Ariel, etc... Choose something that's system agnostic and you'll never have problems again, regardless of your .ini settings.
[Does this help?](https://decibel.ni.com/content/docs/DOC-23382)
This is definitely the best advice. My application at the end was functional in the sense that it ran, but many features were not functional. I documented the hell out of everything before I started coding, so when I got stuck, the documentation was still there so the grader knew what I was trying to do.
You need to right-click on the project name, go to new... then select Targets and Devices. That will get your myRIO added to the project, then you move the VIs to under that target and run them.
http://www.ni.com/academic/students/learn-rio/embedded-programming/
There are quite a bit of vis but they don't really cover processing the wait I would need, I'd have to use the usual math/programming array vis 
Parallelism is only beneficial with multiple cores, unfortunately. Single core computers still exist?? Did you set the subVI to execute inline? If you need to iterate through it, that's really the only performance boost you can do. 
Yeah, I got a Dinosaur laptop. My friend might have a dual core though. I'm curious, it seems to me that (according to the night tutorial) improve performance more than twice on at least dual core. But doesn't each core have a predetermined thread count? So then wouldn't a single core also benefit? What does the inline execution option do? I don't have a subvi yet. I just have a outer while loop that on every iteration grabs data from the kinect, use the nested for loops to replace array values, and outputs via IMAP image window.
Inlining would only help if the For loop is outside the subVI. Front panels have a bit of overhead that gets loaded every time unless executed inline. Maybe good way to think of it is when you inline a subVI, you essentially make it act like any of the native LabVIEW operators. It takes inputs and spits out outputs and nothing else.
This is pretty interesting and a great way to familiarize yourself with array operations. A few pointers: * As /u/meeva said, use more subVIs to fit it all on to one screen and modularize the code. * As you continue with LabVIEW, use more documentation in your code. Add wire labels and text boxes, so we know what's happening without having to trace out the code.
I appreciate the input. Notation is the thing that kills me the most because "I know what it means so why do I need to notate". This is something that I have to get used to. One question I do have as I can see you are much more proficient that I am in LabVIEW. If I use local variables in the way that I did, basically only as a read from, will I make race conditions and cause unwanted outcomes in the VI?
Yikes, just looked at the forecast and Looks like we're in for some rain. Personally, I'm from the desert, so I can't wait!
Hey, FRC mentor here. How is this method working out for your team?
Well, you can view DBCs using the Database Editor that installs with NI-XNET driver ( http://www.ni.com/white-paper/9715/en/ ). Are you using NI-CAN or NI-XNET devices? If you don't know what I am talking about you can have a look here: http://www.ni.com/white-paper/9727/en/ and http://www.ni.com/tutorial/9717/en/
XNET PCI-8512/2. I can get a read off of raw CAN just fine, and even modded one of the examples to try and read values using the data base, but am getting wild values like 67 average volts from a 12 v capacity battery, so I dunno if its a bad DBC or a bad translation. Same thing I suppose.
thanks, I will try this.
Thanks. Turns out that the Can messages themselves are bad from the battery sending them, since Canalyzer and LabVIEW are giving me the same data. Plot thickens, but it's not my issue alone anymore. 
There's a few SQLite toolkits out there. TDMS is worth looking into if your data fits.
What is the problem, exactly? What happens when you try to run the VI? Have you tried the C calling convention?
From the screenshot it seems that LV is using the correct arguments to call the functions inside the dll. In your example OI_API type is not a generic one thus LV uses the corresponding int32_t type as a return value, which is most probably correct. First I would make sure that the card is correctly recognized in the Windows device manager and is operating well with the turn-key software. Also in case of using a driver it is important to follow the internal state-machine to make sure the card is in its correct state to accept the new command. If you have a look at the example codes first you set some HW mode, then Init, then Set other stuff. 
That's not a problem, I would look elsewhere. Is there an initialization function you're supposed to call, and if so does it's return value indicate success?
You seriously overlooked rs422/485 in the serial section which does offer multi-drop serial. The other thing that you don't really touch on is that USB and Ethernet involve big layers of abstraction. With serial and gpib, you clock data directly onto the bus and your slave clocks it off. Interfacing this bus with an embedded system is stupid easy and can be preformed by the simplest controllers. Ethernet and USB require a whole specialized stack to operate, which besides increasing the complexity and horsepower needed in your equipment, also decreases the 'real time'-ness of the communication. When I clock a byte into a serial/parallel bus, I know exactly when it gets clocked off. With USB and Ethernet I only know I sent it and the other equipment should see it soon.
I appreciate your feedback, you are right a lot of instrumentation including temperature chambers use RS422/485. I didn't really cover that well in the write up. The product lines of thermal testing systems that have worked with most didn't usually support these interfaces but they definitely deserve note for instrumentation systems. My understanding is that USB is-what-it-is and a low cost option but things with critical timing can be dealt with appropriately yet maybe slightly differently using the timing features in the IEEE1588 spec. making timing as deterministic as need be.
Does it have to be outside of labview? Launching an asynchronous vi and giving it references to a string control for logging and having the parent periodically check the state of the running vi might be what you want. Having "monitor" vis to aggregate state and error data from other non interactive vis is a useful trick. Named queues and references are your friend here.
You could update the VI to write to a custom log file, and when it's manually stopped have it write a different string, so the monitor script knows the VI was purposely stopped
You might look into pid files and how they are used by linux daemons, I think that could help you out.
The event that I'm trying to monitor is when a vi freezes and is no longer responding, so any booleans on the front panel would be a moot point. 
Thanks, I'll look down that path 
Does Windows use PID files as well?
Ah, that's a totally different animal. But, if you take my approach, you can at least know the most recent state or action prior to the freeze.
Guitar at 1:12 :)
Yeah, the cost of licencing and the hardware is definitely a downside to Labview. If you are looking into some kind of home automation checkout arduino or raspberry pi. Much cheaper options. 
That can be done?
I'm mobile at the moment, but will link when I get back to my computer.
I found the LINX package for interfacing with Arduino which is cool, but what I really want to know is if I can run LabVIEW RTOS on a Beagleboard.
I've used the compiler and it is pretty slick!
Where did you get it? The link /u/iYogurt provided says "coming soon". I'd like to play around with it if I could get my hands on it.
I was just looking at this today. There are 3 examples in VISION 2010, search for 'calibration'. There is a IMAQ Learn Calibration Template vi example (I think nonlinear distortion or something) that does the circle thing.
Thanks for the reply. Unfortunately the examples I had looked at in 2013 don't produce a single calibration image from multiple sources and having spoken to someone from NI today, I'm not sure if it can be achieved outside of Vision Assistant
Calibration information is a metadata thus by creating an array of pixels as an output you will definitely lose the calibration information. However you can apply the calibration onto the image and then export it. Of course it significantly differs from having the calibration information. Try this VI: IMAQ Correct Calibrated Image
Ha. Looks like one of my professors slides
That's when you put a block in the VI saying, "Warning: Customer wants it ugly".
I thought for sure the comments on that thread would be full of labview hate. It was not however. 
If theres some solace that its likely super legacy considering its using a nice 20 year old PCI-MIO-16XE. Or extra cringe that someone has had to use it for 20 years. 
Yeah, every time I open legacy code, it looks like this. The pangs of sadness are even worse when it's my legacy code.
Yeesh, visual vomit. I've had to deal with far more of those than I'd care to admit. At my last job I seemed to always be the unlucky SOB that got the legacy code migration jobs. The horror...the horror...
DeVry: Teaching the latest the 1980's have to offer.
I started learning LabVIEW by watching instructional videos made my SixClear. That's how I learned the general functionalities of LabVIEW that is. As far as getting comfortable programming actual, functional things, it's all practice. I started with one small project here and there, made many mistakes, and redid them if needed. Nobody's first project is any good. Unfortunate for me I didn't have a whole lot of guidance above me, so a lot of my learning was self-taught. Hopefully you have someone above you who can help you with best practices and architectures. Edit: If you ever have a question about anything or how to do something, Google is your best friend and so are the LabVIEW examples. There's an example for anything you could come up with.
Use a class based architecture instead, placing appropriate pieces of your cluster into sub-classes' private data. By compartmentalizing the private data, you will have smaller data sizes on your wires in general. By just breaking up the cluster into smaller pieces, you would have to keep track of a bunch of new type definitions, which OOP does much more cleanly. On a side note, you also seem to be a fan of stacked (or even flat?? *shudder*) sequences. This sort of architecture inevitably leads to huge block diagrams and memory allocation problems as you seem to be experiencing.
Could you not use the diagram disable structure to prevent unintentional latching/unlatching until a process is done? Or if it's a state machine, could you wire the current step to a string indicator so you know where it's at? Last thing, why don't you just put the unwired things in a diagram disable structure?
If they're in a disable structure they're not going to be read and subsequently unlatched... I guess I'm confused why you have it this way. If they're in an event structure, they will all be read once the event executes. Can you not just have all the buttons as latched but only have the event structure run once a "execute" button is pressed? Have you looked into the producer/consumer queue structures? I feel like this is something similar to a program I made where I fill out the entire front panel with values then hit "send" and the program starts running with those parameters in the order I want it to due to the queue. 
The buttons are not in a diagram disable structure. Sorry, I was talking about two different scenarios (one for buttons and one for other things that are not needed to be ran in the block diagram). Queue structures are great, but I am not convinced that I need them for the type of program I am currently making. 
yea...there's a template for this already in labview.
I think you meant to reply to /u/nikofeyn 's comment. You need to click "reply" below the comment you want to reply to. 
Doh! My apologies.
Meh, good on him for taking the time to do it. Some people learn a lot better watching a video like that than trying to use a static example. LV beginners especially will benefit from that, given the fact that one of the more challenging things for them tends to be knowing where functions and controls are on palettes. One potential improvement I'd point out (from the short bit I watched) is to use a typedef enum and a variant constant instead of a string &amp; variant like that guy did. Yes, I know it's a bit of a religious argument in some circles, pros and cons can be listed for each. But in the uncounted number of these I've built, I've never had any difficulties with state selection. I always create a typedef enum and then put that and the variant constant in a typedef constant.
what are the deficiencies?
I've done uncounted string versions (12 years worth, anyway). It really doesn't matter.
no, it could very easily be a runtime error. if you have an event that fires off the typo action string, then that event could very easily be missed at development time and then occur during runtime. sure, you might catch it at runtime, but now you have a bug that requires a rev of your build for something that would be prevented at development time by a typedef.
Key difference here being that my approach frees up the default state for whatever purpose you see fit. I usually make it 'Idle' and use it for low-priority periodic tasks. Plus I never have to add state transition logic to my code unless I want to, which is nice.
Do you mean something like [this?](http://i.imgur.com/Kb43Hh5.png) On error, your case goes true, and than you send an error?
Suddenly, I feel like I've never developed anything cool in LabVIEW.
Update, that was extremely helpful! Thank you. I managed to make it work the way I had hoped. I so far have only tested it [outside my VI](http://i.imgur.com/QAjYFkl.png), using a T constant. I will figure out if I can get it to work with my idea in my VI in a little bit. &amp;nbsp; I am currently trying to figure out how to stop it from sending repeatedly while running, though the timer works perfectly in the sense that it will allow the initial message to send immediately, then delay, say, 100000ms (100s) to send the next one; when I start my VI it takes a few cycles to run normally and would cause an error message to send on start up which is needless. This issue is hardly one at all, because the nature of ECG, it would only technically have to "boot" once in practical application, just curious if you had any ideas. 
Check out a ton of programming tutorials [here](http://www.ni.com/getting-started/labview-basics/). How do you connect to the motors?
Yea unfortunately it has to be with the launchpad. However, I've spent MANY hours trying to figure it out. I've learned how to program it and how to work labview effectively. It was oddly satisfying
Putting this in a while loop is the correct thing to do. Just make sure to have a [Wait (ms)](http://www.ni.com/white-paper/4120/en/) execute after the second case structure. If you need your GUI elements to work during the execution of the Wait, you should refactor your code using the [producer/consumer design pattern.](http://www.ni.com/white-paper/3023/en/)
My idea was to run the random numbers generator once and see if they are odd or even and if you guessed. Then the program doesn't stop, instead, waits for you to guess with the button once more and generates new numbers only then. I actualy did the Wait(ms) thing and if no one comes with a solution I will leave it that way, but it was not the general idea. And I am kinda afraid to use the produced/consumer design pattern becouse I would have to explain how everything works on it and it would be hard :S Thanks for the answer!
If you need an idler, replace the Wait (ms) with a while loop with a button labeled "Try Again" or something like that wired to the stop. If you want to get fancy with it, you can even use property nodes to grey out or hide the "Try Again" button until it is needed. 
Anytime you need to interact with a user, you should consider event structures (sorry no link, I'm on mobile, but googling it should yield good results). Event structures are not always the right choice, but many times can be used to produce efficient UI interactions. In this case, pairing an event structure with a while loop, and creating an event case to handle you user's guess should do the trick.
I'm sure you should use an Event structure. It's the only way to make an efficient UI interaction. Check this out: [Imgur](http://i.imgur.com/km13e3N.png?1)
I would be down, studying for my CLA this year
This article is about how to run VIs programmatically: http://digital.ni.com/public.nsf/allkb/9282BA6C907DF5B2862572480069E570 You can also use "Call By Reference" and "Start Asynchronous Call". And here is an example of how you can make a menu using a listbox: http://i.imgur.com/Z0rfZJg.png
Just got a new Arduino ProMicro in the mail today to try out LINX. excited to see what all it can do! 
I played around with it a bit and it seems good. I like that I can use my old Arduino next time I need to do some really rudimentary DAQ stuff.
Oh, cool! I didn't realize that LINX had officially replaced LIFA. I will have to check this out.
Try out the CLD practice exam(s): http://www.ni.com/gate/gb/GB_EKITCLDEXMPRP/US If you find that too easy, check out the CLA exam: http://www.ni.com/gate/gb/GB_INFOCLAEXMPRP/US
This is very cool. I like what you guys are doing. Now I need to dream up a reason to get a Kinect and do something fun with it!
I wrote some 'programming challenges' for the high school robotics team I mentor at. I uploaded a couple of them here: http://williamtoth.com/data/uploads/labview-challenges.zip They range in difficulty from simple to 'rather hard for a student'. Each has a problem description and a password protected solution, which allows access to the front panel (to show expected output), but not the code. We structured it so after students completed the challenge I gave them the password to see another solution.
I did use an Event structure and it works fine now. Thank you !
digital filter design toolbox http://zone.ni.com/reference/en-XX/help/371325F-01/lvdigfiltdestk/dfd_build_from_tf/
What exactly do you mean by 'real-time filter'? If it's critical for your application to perform this type of signal processing in as close to real-time as possible, then you need to be using an FPGA to acquire the signals. Is this an option? How fast do you really need to process and display your data?
Try to create a VI that processes a single channel. Wire the waveform array into a for loop, enable auto indexing on the tunnel if that doesn't happen by default with your settings. Enable parallel execution on the for loop. Now, insted of placing the processing subVI into the loop open a reference to it outside the loop, with the appropriate options (http://zone.ni.com/reference/en-XX/help/371361H-01/glang/open_vi_reference/). I believe it should be (0x40 AND 0x80) but doublecheck it because I'm not sure if I remember correctly. Now send the waveform information into the referenced VI (there are multiple ways depending on your LV version) and then do the closing and other stuff. (Init and Close should happen outside the Producer loop of course. By doing this you can make sure you did a real effort to make things happen parallel :) 
Seems interesting, which modules are included?
This is an interesting move. Does anyone one know what hardware is supported, raspberry Pi, Arduino, etc. ? If you call your rep in the are you can always get an extension on the trial period because you're using their hardware. Now they probably will make you buy this version.
I'm half-tempted to buy it, just to see what's included and whether or not it can open my gargantuan AF project.
The LabVIEW Home Bundle includes: * LabVIEW Full Development System * LabVIEW Control Design and Simulation Module * LabVIEW MathScript RT Module All licensed for non-commercial use. The software does not expire and includes a home edition watermark. NI drivers like DAQmx and VISA are fully compatible with this version of LabVIEW so you can use DAQ and serial hardware. We also have a bunch of free, open source add-ons for maker friendly devices like Kinect, Arduino, Nest, and more at [LabVIEW MakerHub](https://www.labviewmakerhub.com/) -Sam K
Really exciting! This will only grow their market share and I look forward to recommending this as a home solution more. A lot of projects that I work on in my personal life would greatly benefit from LV and now it's viable to suggest it to other people as well.
Hi Sam, this is the first I've heard of the transition to your new site and name. Really exciting, congratulations!
It looks like it's identical to Labview Full but with a watermark and a different license. So unless your project uses libraries or modules that aren't in Full then I bet it'll be fine.
The [JKI Software support page](http://jkisoft.com/vipm/docs/2014/index.html?turl=appendixbhowtomanuallyconfigureyourlabviewversiontoallowvipmcommunication.htm) says that if VIPM isn't able to establish a connection to LabVIEW on your computer, then it will show the [Labview version dialougue](http://jkisoft.com/vipm/docs/2014/index.html?turl=definelabviewversiondialog.htm), but this doesn't happen. I've also tried configuring the port in LabVIEW, but haven't been able to pull up the menu to do this on the VIPM end. (Note: I'm using Mac OS X) 
I checked my VI Server specifications in LabVIEW and they check out. But when I go to Tools &gt;&gt; Options &gt;&gt; LabVIEW in the VIPM, nothing comes up for the full allowed connection time, and no dialogue opens to allow me to input one myself. 
It depends a little on the DAQ system you're using. If you are using x series, you could just retrigger on the falling edge of the digital signal you're interested in. It's a little more complicated if you're using cdaq or something. I think you could probably just use a pulse output task for your output and trigger on the signal of interest. Depending on your timing requirements, you might be better off with rio and labview fpga, which I'm not really versed in. 
Hmm. I'm not sure what your data is and where you want your output (daq? Boolean values? Some kind of array?) So let's say your data coming in is a 1D array of 0s and 1s, or even 0 to 255. You also initialize another array , array B of the same size. As you iterate through the data, you compare with the previous value to detect if the level has changed and from high to low or low to high. If a falling edge detected was true, then you can write a boolean True or a value of 255 say, in array B for that current iteration and for x numbers of iterations later, where x is your number of samples you want high. Then, write a zero or false in array B otherwise
Since the second signals is fixed size, use a counter (if you have one). Then you can set a number of ticks up and down (from a source clock) and retrigger it on the falling edge of your first signal. This will be quite accurate timing-wise as well.
"DFD build form TF" is not working although I combined all 4 TFs to one in matlab and converted it with c2d to a discrete TF. I entered them als numerator and denumerator but the signal output is just totally wrong.... 
The first link describes *exactly* what I was trying to say but much detailed and with an example. The second link....Oh how I wish I could spend 5k on every problem I had. Before the days of RTSI and on-board signal routing though, this was the easiest way to synchronize. Expensive and have to set it up for every different configuration but for prototyping it's so much easier.
First, size. http://sine.ni.com/np/app/main/p/bot/no/ap/daq/lang/en/pg/1/sn/n17:daq,n24:USB,n1:11604/. This is just the sub form factor., but they do make them pretty big. Talk to your local NI sales guy to figure out different hardware. Daq devices typically measure a voltage, so you'll have to do a little bit of hardware to make it measure resistance. Depending what you're exactly measuring and the hardware you choose, there may be some NI hardware to just get the measurement you want. Again, talk to your local NI sales guy. As far as software goes, checkout the DAQ examples. You can probably find an example that does 90% of what you want. If you go with non NI hardware you have to find out what the interface is (it'll probably be serial) then send it the right messages
I've had good luck with National Instrument's customer service. Call your local rep and they will help let you know what you need. They don't pressure you to buy either.
i believe that JKI updated the package manager between LabVIEW 2013 and 2014. you might want to upgrade your package manager manually from their website to the latest version, which has backwards LabVIEW version support, and hopefully that will help out. also, the suggestion of making sure your VI Server settings between LabVIEW and the VIPM match up is a helpful one, and not just the machine access. sometimes i have had issues with the port numbers being off and even issues with the port numbers being the same. sometimes resetting them and restarting everything helps.
There are some things you can try to get it work: http://digital.ni.com/public.nsf/allkb/81CBF2F7F0BAEDBD86257330000514DF but the workaround might not be compatible with your device and can even cause damage to it so read the manuals before trying anything! I'm using a latitude e6430 (many of my colleagues are using similar laptops from similar series) and never had any issue with them. Btw, what is your problem exactly? your laptop halts/windows freezes during bootup/red LEDs/etc? 
For similar projects a DMM + Switch card is commonly used. Because of how the switch/dmm driver works the option for synchronization is built in be default, so can be done even with an Express VI. http://www.ni.com/white-paper/8321/en/ For selecting the proper DMM + Switch card just call your local sales office as others already mentioned.
I ended up reformatting my PC and tried a new PXI chassis, and it worked... Must have been some weird driver conflict or something. It would detect the PCI bus and bridge, but wouldnt detect the chassis and modules. I had installed the PXI platform services and tried all there recommendations. Even had NI apps support stumped. Regardless, its working now, so im happy! Thanks for the reply!
If you are referring to the green boxes in Figure 11 with the white T with green background, that is the older form of the True/False constant set to True. Newer versions of LabVIEW simply show either T or F, not both with one highlighted. Figure 11 has two true constants, one false constant (F in white text with green background), and one empty Boolean array constant.
Thanks couldnt find any more modern notes and i have no idea what the old symbols mean
I know I'm a little late to the party (just started a new job in a new city), but i think this is a great idea! I may not have the time to watch someone code for 4 hours now, but I definitely would have made the time when I was in prep mode for the CLA exam.
I don't think you are going to get LabVIEW working quite the same way you have it in Java. What I typically do is make a library project and a main project. I build my library into an LLB and copy it over to my main project. Then I have my main code access my library LLB. If you want to get fancy I think you can build your library so the LLB automatically goes into your user.llb so you access your libraries from the pallet and you don't have to manually copy them into your project.
I just gave it a go with my webcam, worked fine :)
Hey iYogurt... thanks for your comments. As "novel" suggested, all that is required is a webcam and a desktop/laptop. Using IMAQdx, you can use any direct show compliant webcam (I just used the cam integrated into my laptop). As "Novel" said, the code is surprisingly simple. The only clever(ish) bit... having the software combine the actual resolution of the camera with the user's requested "scan resolution"... the code calculates how many images are required to be held in memory at any one time, and preallocates the memory space. Everything else was super simple. Read more and DOWNLOAD THE CODE here... https://decibel.ni.com/content/docs/DOC-29064 Thanks again for your interest
You are 100% correct on all fronts. Although the result is pretty spectacular... the code is super simple (albeit processor/memory intensive). Grab the code here... https://decibel.ni.com/content/docs/DOC-29064 
It can work with OOP fine, you just make your class and it's methods the contents of your shared library. 
&gt; If you want to get fancy I think you can build your library so the LLB automatically goes into your user.llb so you access your libraries from the pallet and you don't have to manually copy them into your project. I had something like this happening when using VIPM, when I was trying out an alternative to my [normal approach](http://www.reddit.com/r/LabVIEW/comments/3583s6/how_do_you_reuse_libraries_effectively_with/cr2mxey). I tried out producing a VIPM build spec and then getting it to install both packed and non-packed library versions of my project under "vi.lib" and then including their paths within new projects that depended on them. The problem with this approach is that 1) I can't keep a project and all of its dependencies as one self-contained thing that's entirely stored under the project root directory and 2) I encountered problems producing distributions of projects that depended on other projects this way. To expand on 2), I found that VIPM threw up errors when I tried to compile a project that referenced a packed library for a dependent project kept under "vi.lib". I don't think VIPM is happy working with VIs that don't have a block diagram available, I guess. When I tried to reference normal libraries rather than packed libraries kept under "vi.lib", I could build VIPM packages of the top-level projects fine (if I'm recalling correctly) but it wasn't happy letting me produce packed library versions of those projects. Which is kind of weird because I've created a few projects that directly reference the actor framework library kept under "vi.lib" and I can create packed library distributions of those projects just fine. In any case, I wouldn't want to make my projects depend on non-packed libraries under "vi.lib" because you can freely modify them and potentially break all projects that depend on them. What would be ideal would be a cross between VIPM and my normal approach, where I can create something like "Command-1.5.4.lvlibp"*, upload it to a repository, and then declare dependencies on that within other projects where it will download copies of those packed libraries into a local cache. LabVIEW could learn a lot from Java. *And yeah, another thing about LabVIEW is that you can't really include a version suffix in the filename since your projects would then be full of references like "Command-1.5.4.lvlibp:Command.lvclass:Execute.vi" which would break the moment you upgraded to v1.5.5. :/ So many limitations...
Really? I find that very odd. Our company only has a four seat licence (full development but still) and I've never been limited by how many service requests I can make. 
Our situation is that we have three members of the development team: my boss, a senior colleague and me. I don't really have much of an understanding about how licensing works since I'm not the one who deals with it but apparently only our boss is technically allowed to make support requests since our version of LabVIEW is licensed under him. Perhaps he has the wrong license and the two support requests I got handled before were freebies or done by mistake.
I'm very interested!
You might want to explain WHY stacked sequences should be avoided. The reason for this is that once a given frame in a sequence (stacked or flat) begins, there's no way to cause it to exit. You are forced to wait until it completes, and this can lead to unwanted behavior (like long delays or application hang) at best, and damage to components or people at worst. Sequences aren't fundamentally evil, but they have the potential to cause serious problems and, just like global variables, their use should be kept to an absolute minimum if not avoided altogether. To OP's question: You need to find documentation for the steppers you're using, specifically a programming reference manual. That will contain the command set and likely some example code for controlling the steppers via serial comms. Also, don't forget that you can change how the values in string constants and received data appear by right-clicking on them and selecting 'Slash codes' or 'Hex', etc. This helps to show things like tabs and spaces if you're not sure what's being sent or returned.
sweet. that's awesome man. i'll call up the manufacturer (MDrive, not McAllister (they did the rest of the machine)) and get a user manual. this user manual thing sounds like exactly what I need lol. edit: I found this http://motion.schneider-electric.com/downloads/manuals/MDO17_23_Plus.pdf after typing the produce numbers/names into google. On 2-23 and 2-24, this manual gives a variety of write / read commands to the ports and gives their interpretations. I guess this is what I'm looking for, but it doesn't seem to have the same format as before (even though they give an example later where the xxx=##### 0A format is used again). So , looking at that, VI is initially set to 1000. could I write in "VI=" + "100" + "0A" and it would slow down the initial velocity. right?
Yeah, but make sure you have your strings set up the right way. If you sent that command as is, it wouldn't work because "Vl=1000A" is wrong. It would either be all hex or all alphanumeric. If your end-of-command character is always going to be "0A", you could just make a subVI that concatenates all of your strings with "0A" (hex display) before sending. When you concatenate strings, the way you were displaying them beforehand doesn't matter, they get put together the right way. Edit: like someone said below, to test out some commands, create string constants to send out and manipulate how they appear by right clicking and selecting "slash codes" or "hex"
I've done lots of Arduino based projects that utilize LabVIEW interfaces. There is a free Arduino driver "sdk" that I downloaded through the Package Manager (I think that comes with LabVIEW now). With the code out-of-the-box, you can use an Arduino as a basic Multifunctional DAQ device. Dig into the source code of that and it is easy to add your own custom commands and adapt the sdk code to your Arduino project. 
http://www.ni.com/example/30225/en/ Just do a google search for ni clad exam
Care to provide an example?
their emphasis on using implicit property nodes instead of explicit, for example (my opinion is explicit emphasizes flow). Another thing I disagree with is their emphasis on wiring directly to controls where possible, my opinion is this leads to a more disorganized and uglier program. Some other things that I can't remember right now. 
you shouldn't be; CLAD is pretty easy to pass.
Counter points: Implicit property nodes means that the reference is known at compile time instead of run time making it more efficient. Wires are the fastest, most direct, and guarantee that data flow is enforced. Every other mechanism of data transfer also requires additional copies of the data to be created in memory. Also, I'll take a few ugly crossed wires over the time it takes to troubleshoot a race condition any day of the week
Well, my opinion is that efficiency and footprint only matter under specific circumstances and I'll gladly exchange a little of that for a *prettier* (read: more maintainable) program 9 times out of 10. As for race conditions, plenty of ways to avoid them w/o the use of direct wiring (functional globals, for example). 
I hope so! Any study tips? 😆
To each their own, but practices like declaring constants and not declaring unnecessary variables aren't "The NI Way", they're best practices for all programming. I don't mean to be confrontational, and I agree that the reasoning behind these could be explained better in Core 1 and 2, but I stand by them as good practices whether you're writing in LabVIEW, C, Java, or INTERCAL. 
Do every practice test you can, and see if you can solve some of the codingbat problems in LabVIEW to get a good handle on boolean logic, loops, and arrays. 
I don't think I was arguing against declaring constants and for creating unnecessary variables. LabVIEW can turn into an unmaintainable spaghetti feast rather quickly, is the problem. If you're going to be writing via reference anywhere, may as well be everywhere - assuming you do it right.
Hey thanks for sharing, I would love to learn more about your approach. Any recommended reading?
Give me a couple of days and I can probably throw an example on GitHub. I picked up the architecture from a co-worker. Not sure where he read about it.
That is such a relief! I have it Friday morning at 8:30 AM. 
Ok so I added a delay, and I tried to implement what you are talking about and got this https://scontent-lga1-1.xx.fbcdn.net/hphotos-xpt1/v/t34.0-12/11355401_10152889581331381_841356196_n.jpg?oh=a73bc75d9ea2bef02af0b9a2ccc06d9e&amp;oe=55612336 , however now I am getting an error that says : Shift register unwired from inside the loop
Right so what im trying to do is calculate the total distance the knob travels, using the formula abs(current pos - prev pos) and store that number into an array which i will later sum up, there is where im currently at https://scontent.xx.fbcdn.net/hphotos-xpt1/v/t34.0-12/11349105_10152889755291381_1463187422_n.jpg?oh=87f3f7834c852dec37ef1a066cf4b059&amp;oe=55615376 , i feel like this should work but when I run it, nothing gets pushed into my array
Or put the array indicator inside the loop. 
Awesome thank you Dr Oops!
yeah that's definitely a much more efficient way! Thanks
Actually, FGVs are THE way to prevent race conditoins when accessing data on a global scale. They read-modify-write in a single call and are non-reentrant, so you literally can't produce a race condition using one.
[I'll just leave this here](http://forums.ni.com/t5/LabVIEW/Race-condition-using-Data-Value-Reference/m-p/2001011#M658628)
I'm sorry. The way I'm wording things is coming off like I actually think DVRs are race-condition proof. What I'm really trying to say is I prefer using DVRs over FGVs. Not actively trying to imply anything else. I think FGVs are messy and not in the true spirit of the dataflow paradigm. They also make code a serious pain in the ass to debug. I won't denounce anyone for using them, but I never teach their use to developers in training.
Hey, appreciate the input of a CLA. &gt; In my office, we take advantage of VIPM Pro for the repository feature. $500 for what you essentially get for free with Java, but could be worse I suppose. &gt; We build our libraries into VI packages and throw them up on AeroFS (basically DropBox). AeroFS is free for up to 30 users? That's brilliant; we only have 3. &gt; We then have VIPM point to our AeroFS folders as an additional repository so anytime one of us updates a .vip, VIPM will push the updates out. That seems like a good idea. &gt; Very effective. However, sometimes code is just not suited well for a palette install. In those cases, we have a separate Git repository we call "User Lib". Each of us clones the repository into our user.lib folder and we can push and pull changes that way. I'm unclear about this part. What exactly gets pushed to this Git repo? Does it just contain any code from any project that ought to be reused and kept under "user.lib"? &gt; Can treat .lvlib files like .jar files. If you want to import your libraries, try a virtual folder instead of copying them to some auto-populating folder. Can name the virtual folder "dependencies" if it helps get the point across. I've tried this already. When I was trying out VIPM, I got it to install my projects as .lvlib's or .lvlibp's under "vi.lib" (which probably ought to have been "user.lib" - but either way...), and then imported them into my project with the standard right click -&gt; Add file to project. What I found was that this caused problems with either producing .lvlipb of the "higher-tier" project (e.g. Banking) or producing VIP builds. In the latter case, I think VIPM has trouble building projects if they depend upon packed libraries. Perhaps VIPM needs access to the block diagram of all VIs or something. What would really help if you have the time (and I'd appreciate a lot) would be to walk me through step-by-step how you'd go about applying your approach to the example scenario I included in the original post: creating a "Command" project, packaging it, distributing it, producing a "Banking" project that depended upon it and then packaging that up in turn (in a way that can be easily used without having to worry about its dependencies). That said I don't have any urgent need to know this stuff right now so I'm happy if you don't want to do that.
You need to use a method node on the FPGA http://zone.ni.com/reference/en-XX/help/371599K-01/lvfpga/io_method_node/
&gt; my recommendation would be to distribute your library as a VI package using the VIPM. this would allow you to place the polymorphic VIs on the palette, leaving off all of the instance VIs. this means you would need to distribute your VIs as a source distribution, but if they are intended to be consumed by other developers, that is probably best. I suppose that's the best way if there's no option I can specify within the packed library itself, but I'd have to redesign the way I go about distributing everything in LabVIEW and using VIPM may be problematic for the reasons given [here](http://www.reddit.com/r/LabVIEW/comments/3583s6/how_do_you_reuse_libraries_effectively_with/). /u/Muun offers a possible solution using VIPM but I'm not 100% clear on it yet, and it would involve spending $500. VIPM also seems to throw errors if you try to build a package that includes packed libraries.
&gt; I suppose that's the best way if there's no option I can specify within the packed library itself yea, that's probably your best bet, particularly if you intend on developers consuming these in their own development. there is no option in the packed project library itself other than arranging things nicely in virtual folders. packed project libraries will load up with the virtual folders that are defined in the respective .lvlib. &gt; using VIPM may be problematic for the reasons given here i suspect those reasons are due to the pro version. i have used the free version to good effect for small-ish reusable libraries. i'll follow up on your link tomorrow, as i just glossed over the previous thread. i am somewhat experienced in reusable libraries and components. &gt; VIPM also seems to throw errors if you try to build a package that includes packed libraries. i guess that isn't surprising to me. VIPM uses their own labview build code, as they do not use labview's AppBuilder.
If I can get VIPM to install packed libraries under "user.lib" and have the PPL VI's show up in the palette menus, I suppose it could work. Otherwise I'm going to have to go through every single project I've made bottom-up and change thousands of VI's from something like "Project.lvlibp:Something.vi" to "Project.lvlib:Something.vi". :\
Never worked on a PXI system, but with cDAQs and cRIOs you can go into MAX and create a simulated device. Then you can go and write your code with your simulated hardware. Your software wont know the difference as long as you are using the same modules in the same slots and if your device as the same name as your simulated device. I'd also recommend using tasks if you are not already, they do a great job of abstracting the hardware and makes your coding so much easier to manage and change in the future.
If you design your software in a modular manner, it should be easy to swap "simulated instrument" for "real instrument". For example, you could have VI with inputs for the channel to be read, measurement type, scale, etc and an output with the measure value. The "real instrument" version would set the multiplexer, wait the required setting time, and then take and return the measurement. The "simulated measurement" version could either simply return 0, or provide simulated. This would let you develop the rest of your software while waiting for the equipment. You could then use a conditional diagram disable structure to switch between versions. For more complex systems, you can also use labview classes for this. Define the interface in a base class and then create a child class with the simulated implementation. Once you have your hardware, implement a second child class with the hardware implementation. 
If you use the IVI DMM and IVI Switch interfaces, IVI supports simulation mode with no code change. It is done at the configuration level setup in MAX. However, the IVI Switch programming interface does not allow all multiplex configuration. If you have a 2:1 multiplexer (Com, A, and B), the IVI interface only allows COM to A or COM to B even if the native driver and hardware allows COM to A AND B.
Never worked with an MSP430. I assume you communicate with it via USB? If so, did the USB drivers come with any DLLs? You may be able to call the functions for flashing the chip in LabVIEW using the "Call Library Function Node" VI. If the API is in .NET then you can just use normal property/invoke nodes to call the functions as well. 
You want to reprogram the device every time? I've used several tools to reprogram the MSP430, but that seems like a lot of overhead for some calibration, and Flash is only good for a certain amount of write cycles. Two tools come to mind that you can call from LabVIEW which I have done before. MSPdebug is an open source project. It works OK. Then TI has "MSP Flasher" which you can call from LabVIEW. Both call MSP430.dll, which you can probably call directly from LabVIEW. All depends on how you want to approach it. If you want to see how I did it with MSPdebug you can see it on my github here (you'll have to dig, its part of an older LINX fork): https://github.com/willtoth/LVH-LINX 
It really depends on what you want the application to do other than just send commands and read values. It's easy to set up an event structure loop that sends out commands when you press a button. Do you want it to continuously read some values over the RS232 port every few milliseconds? Add a parallel loop that does that or a timeout on the event structure. Do you want specific actions to occur based on values or user input? Create a simple event based state machine. If you can think it up, usually LabVIEW can do it.
Oh man, you're going to have to do some research about programming practices in general. You have While loops and For loops. You shouldn't worry about parallelization of For loops for now, it's used to speed up For loops processing by utilizing multi-core processors. When someone talks about parallel loops, they mean two or more loops running separate from each other and therefore running in parallel, not sequential. These can be parallel For loops or While loops. The purpose of parallel loops is so that they can each run at their own pace. You could have one loop that's just sitting waiting for a command, and another parallel loop running continuously to pull in new data. You should look in to Core 1 and Core 2 LabVIEW classes or videos. Also, the NI forums are a great place to get information. They're incredibly active and they're the main reason this subreddit is pretty inactive, cause it's not needed.
Looks like yes. http://www.ni.com/sdr/usrp/ 2 seconds on Google.
Yes, there's plenty of examples out there or doing so. if you're looking at more advanced stuff, the usrp rios+lv comms+app frameworks are an awesome 802.11 or LTE phy layer stack to work off of. The data stream they use is typically video data. I work with a ton of USRP+labview stuff, feel free to msg me if you have specific questions. 
To save yourself a ton of work, be sure to always take a look on ni.com/idnet which houses thousands and thousands of premade labview instrument drivers. Most common Instruments already have nice interfaces, often with examples and more. Will save you a ton of time and make your code far far cleaner.
You can also look in LabVIEW and go to Help --&gt; Find Instrument Drivers
Does this necessarily yield the same results?
IVI stands for Interchangeable Virtual Instrument. It is a cross company approach to provide simplified instrument drivers that could work at a generic level regardless of the make/model of the instrument. For example, there is the IVI DMM class. Theoretically, any DMM the provides an IVI driver can be programmed via the IVI DMM interface regardless of whether the actual instrument is an NI PXI based DMM, an Agilent DMM, etc... To use the IVI base classes, you install the IVI Compliance Package. For some basic NI information on IVI see: http://www.ni.com/ivi/ A tutorial for the IVI DMM driver can be found here: http://www.ni.com/tutorial/3377/en/ Some additional documentation that may help: http://www.ni.com/tutorial/4556/en/ A big key to getting IVI working properly is the configuration of the devices in MAX. See the following document: http://www.ni.com/pdf/manuals/370402a.pdf Page 5 shows a figure which includes the "Simulate" checkbox which determine whether the driver actually tries to talk to the instrument or returns fake data. Also, be aware that not all functionality of a given instrument may be exposed via the generic IVI class drivers (see my example regarding switch multiplexers above). 
Yes, that sounds like what I'm looking for! It would pop up when the exe is run, and have the user set some personal info, like weight, height, eye color, along with the location of some VISA resources. All of this data would get output to a cluster, which is used in the execution of the rest of the application. I think Cancel should close the program, and OK should allow continued execution.
I see. I think I've got that much. I've got another discussion going here: http://forums.ni.com/t5/LabVIEW/How-do-I-create-an-event-driven-dialog-box-that-allows-the-user/m-p/3142450#M904311 from which maybe you could help on this: &gt; What about the case where they don't change a value, and it's not in range? An example would be that a default weight is 0kg, which I'd want them to change. Also, I like your idea to disable and grey out the OK button, I'll definitely be using that. Is it possible to have another button that the user could click to "show errors" (which would pop up a window wist a list of errors) found in their inputs, so they could go back and know what to fix? Pardon my noobiness, thanks for the help! &gt; edit: about that "show errors" button, I'd like to be able to create some sort of array of errors that gets added to in each event that finds one, and gets called from the "show errors" event as a new VI, which would just be another dialog (one button) that shows the array of errors. How would I do that? Would I need to use global variables to pass the array between events, or is there a better way? 
Way late but I think he is referring to the file path constants. In figure 11. 
That's ideal,I'd rather not spend my time rooting through the directories of these gov't computers. Thanks!
Fantastic idea, I've been waiting for someone to do something like this since I've discovered livecoding.tv - will be watching intently Muun.
Right so I did that, but now I'm getting a "The type of the source is 1D array of double [64-bit real (~15 digit precision)]. The type of the sink is double [64-bit real (~15 digit precision)]." VI:https://scontent.xx.fbcdn.net/hphotos-xfa1/v/t34.0-12/11350362_10152917069551381_1894022282_n.jpg?oh=d8a3260279494a1b42bb152367e29917&amp;oe=5570DABE 
The source wire is an array. The sink is a single value (called a scalar). Since these data types are not the same you get the broken run arrow. To fix, change the data type of the shared variable to a "array of double" instead of "double"
Honestly 600 VIs isn't that much. I regularly work with larger projects and I can still build stuff. You've got something else going on
I've never made a packed library in LabVIEW ever. I think they're a pain to deal with and a mistake on NI's part. But besides that, I'm currently working on a project right now with over 3000 vi's spanning three target deployments using LVOOP and I have no problem with creating distributions, installers, etc... If I were you, I'd start by removing classes a bit at a time and retrying the build to see if it works. My guess is you've either got a corrupted project, which I've seen happen or a corrupted library somewhere. You are keeping your classes in library's, right?