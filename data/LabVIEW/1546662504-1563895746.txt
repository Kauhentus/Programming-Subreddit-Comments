Thanks :) If you can help me with this i would be gratefull. The minesweeper cells are created in labview in a cluster. How do i create a random "map"? and the 1st click can't be a loss.
Not sure why you would use a cluster, a 2D array of Boolean matching the size of the play space would make more sense to me where T=mine F=safe. Set the number of mines you want then pass the data into [Shuffle 2D Array](http://zone.ni.com/reference/en-XX/help/371361P-01/glang/shuffle_2d_array/). If the first click was a mine, either reshuffle the field really quick or switch the mine with a random safe space. Now if I were asked to implement it I would probably have a 2D array of Tile objects. There would be a Safe Space, and Mine class which inherit from Tile. Tile would have Clicked (Boolean) and Flagged (Boolean) as part of its class private data and probably have methods like Get Number of Adjacent Mines and Get Picture. I could then use a 2D array of Picture Rings and ask each Tile what picture I should display and update accordingly. Hopefully this will be enough to get you started and thinking. 
Its a school project, that is why i have to do with a cluster
2D Cluster with type (mine or open - boolean) and status (uncovered or covered - boolean)
Can i change the 2D Cluster box image throw c# ? If not, how do i indicate a wide variaty of pictures? Thanks in advance to all
Wow, thanks amazing, which country teaches LabVIEW and C# in school? 
Can you post a link for the code?
See if you can go into MAX&gt;File&gt;Import and point it to the file that was located at C:\Users\&lt;username&gt;\Documents\configData.nce on the old system.
There doesn't seem to be any .nce files that are custom to the settings. I think the person that created these settings just created them in NI MAX but never exported them to a file. I thought NI would automatically store those somewhere. Maybe they changed that later, this is a machine that ran on LabVIEW 2014.
Yeah I don't go on the labview forums, any search results with that URL I just ignore and look elsewhere. 
I think most of us here agree with this, but this also happens in stack overflow. I decided to delete my account there after a user came to my answer only to say that it wasn't anywhere as good as his, but the question was 6 (six!) months old already. Sometimes we can see a smartass in here too, but overall we are safe here :) &amp;#x200B;
Maybe try https://lavag.org/. It's not run by NI so it's more likely to be actively managed.
Former NI employee here. I totally know what you mean about the toxicity of the forums... The people who you have encountered are most likely 'knights of NI' - people that are not employed by NI but post a lot in their free time. They are there for the intellectual challenge of 'interesting' questions and often laugh at newbies rather than help them. There is a policy that if a new thread doesn't get a reply within 2 days, it will get answered by an NI engineer who will be a lot more polite and understanding. I think some of the problem stems from it being more difficult to 'copy and paste'/recreate LabVIEW code than in text based languages, and the inherent messiness of wiring done by beginners. You can avoid some of the latter with block diagram cleanup, Ctrl+U, although sometimes it still looks ugly. But yeah, if you have an active license you are actually entitled to telephone/email support from NI - worth knowing. Don't let those guys put you off!
I completely agree. AltenBach and Michael Porter come to mind. Those two are ruthless and unnecessarily rude. Steve Mercer can also be a jackass occasionally, but he always backs up his posts with solid computer science that I can't help but respect him and/or forgive him. The other two just seem to be there to inflate their ego. Granted, inheriting a messy block diagram sends my blood pressure sky high, but I can ignore that to actually answer someone's question on a forum.
I agree. LavaG is awesome. The overall tone is friendly, but stay away from opinion topics. Those can get heated. Don't remember usernames or exactly what was being discussed, but there was some intense (that's as nice as I can describe it) discussion on the merits of QMHs about 5-10 years ago on there.
He may be snarky, but must admit that some of AltenBach's code is beautiful! Did you know that NI paid for some of the most prolific forum contributers to go on a free cruise a few years ago, to say thank you?! That's what I heard.
I agree with you, it's the worst on NI forums. I do see some of this on Stack, and in other places. "Hey, I need help with this, but I can't use XYZ for various reasons." "You should use XYZ. What's wrong with XYZ? WHAT KIND OF MONSTER DOESN'T USE XYZ?!!!!" (Especially the Node forums) But yeah, I have noticed that my fellow LV programmers take shit too far.
&gt; Granted, inheriting a messy block diagram sends my blood pressure sky high Pfffft. Nothing CTRL-U can't fix.
If you are able to, the easiest way to go about this might be to use the disk on a similar computer to the one where the drive comes from. The closer to the original computer you can get, the better (due to differences in drivers and what not). If you manage to boot-up from the disk with a different computer, you can then generate the .nce file by following these instructions: [https://knowledge.ni.com/KnowledgeArticleDetails?id=kA00Z000000PA7DSAW&amp;l=en-US](https://knowledge.ni.com/KnowledgeArticleDetails?id=kA00Z000000PA7DSAW&amp;l=en-US)
Whether you can change an image dynamically at run-time depends on the type of control you use. I think the easiest way for you to have access to "a variety of pictures" is to use a "Picture Ring", which is kind of like an enumerator control where each element is an image. These ones don't allow you to dynamically change the picture elements, but it sounds like you have a finite number of states, with each state corresponding to an image, so I don't think you need to dynamically define them.
Lots to take apart here and your opinion is your own and based on your experience. I'm not here to tell you, you are wrong, just a few reasons why you may have had the experience you did, again not that this is excuses just a reason why it might be like it is. So the forums themselves are polluted with people who despise LabVIEW for one reason or another and themselves come off as hostile, or flippant about a thing that others spent their careers working with and getting good at. There is a chance for others to feel threatened by this and fly off the handle. If you are in the top 1% of your field but still find yourself daily explaining it is a real programming language and can do amazing things, only to get others disregarding your expert opinion, you may get salty. If a new user on the forums isn't someone who hates LabVIEW and inherited some program they don't want, they might be a student, who also has little interest in anything other than graduating. Asking for help without providing any useful information, and expecting people to do your work for you also leads to replies that are basically "you should take this training" because without a base level of understanding you may spend hours coming up with a solution only for OP to say that isn't what they meant and to start over. Throwing away hours of hard work over and over again, only for someone to say it's wrong is frustrating and makes you want to give up or tell them they need to come back after they've learned a bit more, rather than wasting everyone's time. Compound this with non native English speakers and communication can be a pain. The laziness you see with just pasting links, is a result of the laziness of others who don't want "help" they want their work done for them. There are people on the forums who are passionate and intelligent, but who often feel jaded by the community that they want to contribute to. LabVIEW is a smaller community than most programming languages. There are less people willing to make good tutorials, and less super stars. It sounds like you have a decent amount of experience and maybe it would be useful if you were able to make a tutorial aimed at the demographic you feel isn't being helped well. People are always welcome over on [LAVAG](https://lavag.org) where we see less lazy new people to LabVIEW, and so I think there are less people burned out by trying to help them. Oh and the [LabVIEW Wiki](https://labviewwiki.org/wiki/Home) is working again with lots of good content. But most of the [getting started](https://labviewwiki.org/wiki/Getting_Started) page is the same training links you've probably seen. And lastly, we're all human and have bad days. I come across as "abrasive and unapproachable" too often.
There are certainly more prolific contributors on the forums than me, but I have never heard of NI ever paying anyone for helping, and I've never heard of any free cruise. There are non-monetary perks to having certifications, and being a LabVIEW Champion, but they aren't the reason I do this stuff. Some of these things are premium NI support (which my job has anyway), early access to beta test software (which anyone can apply for anyway), closer seating at NI Week Keynotes, usually a free dinner at NI Week, usually a free shirt once every year or two, and a few promotional items like mouse or USB drive with NI branding.
Who pays for dinner at NI Week? That's what NI reps are for.
Well Sunday night is usually the Gingerman where I pay for myself, Monday I sometimes get into an event or just snack at the opening of the expo floor, Tuesday LAVA BBQ, Wednesday conference party, Thursday night I usually buy dinner. But yeah I get your point there is usually more food and drink than I need.
Oh, I agree his code is beautiful! In fact, I don't think any of the jerks on the forums are bad coders, I think they just need to relax and remember they're talking to fellow humans still learning the ropes.
Should have fact-checked what I said... Distinctly remember an older colleague telling me this, but then again, it's the sort of place where they tell the new people that long wires transmit data slower, haha. Yeah, no reference to it anywhere on the internet, must be folklore! Sorry about that. You guys do deserve a thank you! 
This seems like an incredible mistake to me... I get great solutions from ni.com in my Google searches all the time
You hit it on the head regarding people wanting their work done for them. It's incredibly common on any forum and ni.com is no exceptio. Contributers like Altenbach and crossrulz are incredibly active but also have been jaded by this and don't have time to write solutions ever time instead of suggesting training.
Are you being sarcastic? XD Portugal
Thanks, i will try to do like that just to know how. What i did was return a string with the patch of the image. And in labview, created a case for the pictures. Connected the path, used the png reader, then draw the picture into the cluster
FWIW, Gingerman has closed :(
If you are a student , then go to your library and get LabVIEW core 1 and Core 2 book and follow the material. Also check out the NI and Sixclear YT Channels. 
thanks. I am not a student but I will check out the books and channels
I would highly recommend watching any and all YouTube material about Lab View before starting to program. I hit my head on my desk for a few hours in Core one (mentioned above) because I was so tied up in menus, terminology, and formatting. After watching and taking notes, core 1 and core 2 became a great deal easier. 
The LabVIEW wiki has lots of topics. The getting started page has lots of training and tutorials, with one from NI specifically for students. If you have an active SSP NI also has a pretty good self-paced training. https://labviewwiki.org/wiki/Training
Thank you! However this is one of those situations where that’s not possible. Update, tinkering around with it for hours I got the right settings. There where roughly 5 tasks reading analog input, so it was a matter of what inputs go to what task and setting the right sampling rate and total samples and also digital input triggers. Pain in the butt.... but you live and you learn!
Highly recommend getting hold of Core 1 and Core 2 notes if you can, that will cover all the essentials, as well as practise questions, examples etc. Instead of trying to do something novel, just stick to what works and you’ll be guaranteed to get on well.
The forum is your friend because no matter what problems you encounter someone else has already had it and probably solved it. 
great, thanks a lot man.
I sure will.
I'd like to avoid writing a wall of text, so I'll try to keep it to the point. First, some disclosure - I have posted quite a bit on a bunch of LV places over the last ~15 years (~12.5k posts on NI forums, a couple of thousand on LAVA and fewer on Info-LV and Stack Overflow), so I know some of the people mentioned here, but I haven't been monitoring the LV board on the NI forums over the last few years, so things may have changed. Over that time, there have been multiple complaints of people being rude, and I have generally maintained that people shouldn't be rude. I have tried to always be polite myself, although not with full success. As to the actual rant - Some people are rude. Some people have off days. Some people are not rude, but come off as rude to others due to different viewpoints. As someone who knows the people mentioned in this thread and has been in a similar position, I don't believe they're rude, certainly not intentionally. They are trying to help, but it can be frustrating to interact with people year after year, so you may sometimes appear rude even if it's not your intention. I'm assuming the post that triggered this thread was this one - https://forums.ni.com/t5/LabVIEW/How-to-pass-variables-arrays-boolean-etc-from-main-VI-to-subVI/m-p/3881260#M1100304 I happen to agree with you that this style can come off as very rude, but would recommend that for specific people, just tell them. If they're not aware, they certainly can't think about whether they need to change or not.
I should probably also mention that the LV board gets on the order of 200-300 posts every day. That's considerably more than you get on LAVA, and it can have an effect on how people who post regularly react.
The only cruise I'm aware of was that a few years ago, I believe one of the courses (I believe it was an NI course, I think an OOP or architectures course) was held on a cruise ship at least once. I'm assuming the people who participated in the course also paid for the cruise, but I don't know the details. As someone who is both a LabVIEW Champion (members of the community who "champion LV" and are usually self-nominated and then approved by a black box process inside NI - roughly 100 people by now) and a Knight of NI (people who have &gt;10K posts on the NI forums - probably around 10-20 people now), I can say that there is no real compensation. There are some knickknacks, and the Champions do get NIWeek discounts and in theory personal LV licenses, but these things are generally ad-hoc and you don't get actual monetary compensation.
Ooh wow, never heard the one about the OOP course cruise - that would be awesome! Now, how can I convince my current employer to send me on that? ... NI also definitely send their top performing sales people from around the world on a cruise, as a reward for excellent work - some people from my office went. That's a shame - it's so kind of people to give up their time to help out and educate others. When you say 'personal LabVIEW licence', would that be Software Platform Bundle?! Guessing some people have the licences from their employer, but I've always thought that LabVIEW with add-ons/toolkits was prohibitively expensive for self-employed people. 
&gt; That's a shame - it's so kind of people to give up their time to help out and educate others. Broadly speaking, certainly for the people who did this before the Champions program existed, I can say that people probably preferred that there is no monetary compensation. People helped for their own reasons. That said, certain perks (such as more access to people at NI or events at NIWeek) are certainly a nice after-the-fact reward. &gt; When you say 'personal LabVIEW licence', would that be Software Platform Bundle?! No idea. Like I said, these things are pretty ad-hoc and depend on the people inside NI who run these programs and what kind of authorization they can get for different things they ask. I believe this was originally offered only to Champions who came to NIWeek (and was signed by NI brass), but may have later been offered to the others as well. I never got mine, but that might have been because I didn't feel the need, but I don't remember whether I actually had the option. In any case, it probably came with the understanding that this was for personal use only. I'm not sure if any of the Champions who got it ever actually installed it and I don't think this offer still exists today.
I’m just guessing but austin is probably a good bet considering NI is there. 
That would be great, and I hope that's the case! I love Austin but I thought maybe NI being there would mean that the market is saturated.
Here's the list of NI Alliance Partners: http://www.ni.com/en-us/alliance.html
OMG, how I understand you! I'm not a technical person or a programmer, but I had to gain some LabVIEW knowledge in order to do my work (I'm working as a graphic designer at a company who make their software with LabVIEW). Actually, I've learned how to make buttons quite fast, but when I have some issues with the vi not working right or some missing components because of what the vi is broken - there's no one to help me. The tutorials are simply awful! I couldn't get past further from the 2nd lesson of core 1! I just dropped the idea, and was searching for separate topics where the issues that concern me were discussed. This last time I was searching for the way to recolor the selected rows in a table. As you know, it can't be done other way than programmatically. I've lost almost 2 hours while searching a normal tutorial of what to do in steps. 2 hours, people! And I've noticed one more thing about LabVIEW programmers - when you're asking them something, they're not always giving you the answer. They think you should dig into the programme and solve your problem yourself. Ok, programmer fellow, I'm a graphic designer! You didn't provide me with normal tutorials to learn everything myself, what else can I do? The thing that's driving me crazy is that I'm normally good at learning something new. I've learned Adobe Photoshop and Illustrator by myself because there were perfect tutorials all over the Internet. But how do the LabVIEW tutorial makers think we can master LabVIEW with those poor instructions? And as I've seen many rush comments while searching for solutions on ni forum I event can't ask something there...
I know of a few people in Boston, Detroit, and Chicago. I've never done the contract thing so I don't know from experience.
I'd have to disagree with you completely. I'm not an expert when it comes to labview, I prefer text based programming, but I'm not new at it either. Usually when I have an error it is from having a block being used incorrectly, or having the structure not exactly right. Whenever I've had to ask about it, I'll show the part that I'm having issues with, and usually within a couple hours I'll have a reply saying what part is slightly wrong and how to fix it. The worst place for programming help today is stack overflow. I'd rather go anywhere else other than there anymore. 
Mass Compile and Save All should resolve it.
You can also press Ctrl+Shift+Run Arrow on your top level VI to mark all VIs it calls as needing to be resaved then do a save all.
Do you actually have the relevant VIs (the OpenG sort 2D DBL array VI in this case) installed in the correct location? If you don't, that would be the reason why LV can't find it and you should install it (in this case, use VIPM to install the OpenG VIs).
I don't have the answer to your question, but this may help: https://knowledge.ni.com/KnowledgeArticleDetails?id=kA00Z000000kJ7DSAU&amp;l=en-US
Your PXIe-8133 is likely damaged, and needs RMA. &amp;#x200B; One thing you could check is the Flex cable PN (there was an issue with the initial cable, and a comment here indicates the new part number): [http://www.ni.com/tutorial/13408/en/](http://www.ni.com/tutorial/13408/en/)
https://knowledge.ni.com/KnowledgeArticleDetails?id=kA00Z000000PASaSAO&amp;l=en-US http://www.ni.com/pdf/manuals/372870d.pdf • Green BLINKING—One of the onboard power supplies is operating outside of the normal limits or the system power supply is out of regulation. 
I do. But it just searches for them everytime. I am trying the mass compile suggestion later today to see if that helps. Thanks!
The [LabVIEW Biomedical Toolkit](http://sine.ni.com/nips/cds/view/p/lang/en/nid/211023) may have some useful algorithms and examples. [LabVIEW for ECG Signal Processing](http://www.ni.com/tutorial/6349/en/) example shows using the LabVIEW Biomedical toolkit with ECG signals.
Step 1. Improve your skills of asking for help. 
How would you suggest I alter my post?
&gt; I do. But it just searches for them everytime That seems wrong. If they exist in the specified location, they should just be loaded from there. The searching should only happen if they weren't found where specified. I can't say that I see a good reason why a mass compile would affect that, unless something has gone very wrong. The only thing that comes to mind right now is that you have more than one LV version installed and that the VIs are only installed for one of them. The user.lib folder is a logical folder with a separate copy for each LV version, so it's possible to have the VIs installed in one LV version and not another. VIPM deals with each installed LV version separately (there's a version dropdown in the VIPM toolbar). You also look at where the VIs are actually loaded from when the searching is done. You get a dialog at the end of the loading sequence telling you the details of the dependency changes and you can look at the files tab in the project window. Also, keep in mind that the searching stops only once the calling entity (usually a VI) is saved, and then it also saves where the VIs it uses are stored, so even if found, you need to save all of the calling VIs. **But make sure you're not loading them from the incorrect location**. You don't want cross linking issues. Those can be not fun to untangle.
I don't think this unpleasant comments purpose was to ask for more details regarding your question, which could be placed in the body of the thread that you created. Still... The choice of words was not really adequate.
You need to demonstrate a little bit about how much you know, show what you're getting stuck on, and explain the problem a bit more. The first one is really important; when people ask "How do I do ___?" to strangers, we have no way to know what your background is. 
Any potential for remote? I live and work in Asheville, so not too terrible of a commute if it could be biweekly
Is "build from template" a normal thing? I learned LabVIEW in an environment with little to no stipulations on how we develop. Over time, we've created some standards but we're still free to choose the best approach architecturally for each project. We have a small team, and we each have our own coding style. We just play off of each other's strengths and weaknesses. I'd love to move to Raleigh, but I'm not currently in a position to do so. Good luck finding the right candidate! 
Industry is semiconductor and lighting production. And yeah, most places I've worked that do production test tend to do the same thing over and over, typically the products they test are the same except for mechanical differences, or possibly new features. Thus templates make it easy since most test applications are going to be similar. Faster test development yields less NPI time for products and faster time to market.
Sent you a PM.
Ah, that makes sense. We do DV test validation, regression testing, and FCC precompliance testing. I can see why production testing would require more stringent standards and benefit from templates. I don't know that I would enjoy that line of work as much, but I'm certainly open-minded about my next career move.
If you really want it just dm me your email and I’ll send it to you
Basler Dart? What's cheap to you?
There are honestly so many cameras that could fit your need. Heck, if you got a simple USB webcam lying around, try plugging that in and see if MAX can see it off the bat.
Cheap is preferable but I think I can get up to $1000 if need be
This is a big problem with LabVIEW. You have to use their vision acquisition add-on with most cameras, and that is $400 per seat if you're not at an institution with a site license or want to distribute a built application. 
Sounds more suited to a raspberry pi. $4 camera on eBay. 
Not quite. If you only want to do basic image stuff like aquire images, save, display, and get image data then you don't need to pack for any licensing for development or deployment. https://lavag.org/topic/17114-image-roi/?do=findComment&amp;comment=105496 If you want vision things like object detection, and OCR then you need to pay for a development license, and an extra cost for each deployment in EXEs.
Hes a university student so his costs are dramatically different than industry pricing. Often free if his school has a site license. 
Hey! The PXIe-5171 is actually an oscilloscope and so you’ll need the NI-SCOPE drivers l, here the link to the support page to the 5171: http://search.ni.com/nisearch/app/main/p/ap/tech/lang/en/pg/1/sn/ssnav:sup/aq/or(%22PXIe-5171%22,prodcatpmdm:123644)/ I’d advise you have a read through the getting started guide there and then download the latest NI-SCOPE driver. You’ll then find that you’ll have A new palette available to you for the SCOPE driver VI’s. I’d definitely recommend using example finder to get started with some simple scope functions, here’s how to find and use that functionality: http://www.ni.com/getting-started/labview-basics/examples Best of luck! 
Thanks for the fast response! Does that mean I can't use the DAQ Assistant and have to use other programming blocks/tools?
Yup! There are a lot of different types of cards that you can put in a PXI for different functions, like switches, DMMs, SCOPEs and DAQ cards (among many others). In this case as you have a scope card it won’t be compatible with the functions in the DAQ pallets. Best practice if you want to know what functionality the cards have is by googling the product that you have in the chassis and (if it’s and NI card), going to the support page: http://www.ni.com/en-gb/support/model.pxie-5171.html See the “support library” link down the bottom. Separate to the Scope card stuff, I can see you’ve also got a GPIB card, if you’re looking to connect that up to some external devices in the future I’d recommend going here and seeing if there are drivers already for what you want to connect to: http://www.ni.com/downloads/instrument-drivers/ 
That's sad, the DAQ Assistant was such a great tool. Well I guess I have to start over and see what SCOPE can do. Thanks for the help!
If you are talking about the DAQ express VIS which are large light blue boxes with a wizard then my advi e is to not use them. They are great for getting something noddy set up fairly quickly but fall short when trying to do anything intermediate. NI scope is your best starting point and look for examples in the example finder. Once familiar I also suggest setting up a task using NI MAX and using them on your block diagram.
Try out InstrumentStudio for interactive measurements with scopes :) http://www.ni.com/en-us/shop/electronic-test-instrumentation/application-software-for-electronic-test-and-instrumentation-category/instrumentstudio.html
There is a SCOPE express VI similar to the DAQ assistant, so if you enjoy using the easy to use configuration dialog box then you can use that. However as others pointed out it is better to not use the express VIs because you have better control over your application via low-level VIs. By the way: the “R” at the end of the name of your product means it has a user programmable FPGA. If you have the LabVIEW FPGA Module you can add the FPGA as a target in your project. If you are not interested in it you can continue to use your card as a regular scope card. FPGA enables you to do fantastic stuff: RealTime signal processing, triggering on arbitrary waveforms, spectrum calculation, triggering on spectrum, whatever you can imagine!
So for starters, if you were to just use color boxes instead of images it would probably simplify some of the work for you because you are going to need to compare one cell value to another. If you need to use images you are better off doing all your logic with a numeric value and then translating that into images to display to the user. Also keep in mind that you are going to need to keep track of a 'blank' cell. In CandyCrush or similar games once a match is made the matching gems fall down and are replaced with new gems. You will need a way to keep track of that 'bank' state. I would also suggest that you implement some kind of state machine logic into your program. You need states for a minimum the following: * Check if there is a match and clear the matched cells * Populate the table with new gems * Allow the player to make a move or exit the game
Thanks for your response, so if I do use coloured blocks (like in the [frozen game](https://forums.ni.com/t5/LabVIEW/Frozen-game/m-p/3128823/highlight/true#M899061) example on the NI forums) how exactly can I use the values to in states. I take it that using the coloured blocks means that each colour automatically has a value assigned to it so I can use the state machine to scan through the array and when it sees three values in a row or column it will take action? So I guess my next question would be is there a "delete value" function for an array and what would be an easy way of scanning for the values?
&gt; so if I do use coloured blocks how exactly can I use the values to in states? A ColorBox is a numeric that represents the RGB values of a color. This means that you can compare if one value equals another value. On the front panel, it appears as a box displaying it's RGB color. There is a delete from array subVI but think, do you want to delete elements from the array or replace them with a 'blank' value? Is there a difference and which is best for your application? What would deleting value from a 2 dimensional array actually do? 
&gt; There is a delete from array subVI but think, do you want to delete elements from the array or replace them with a 'blank' value? Is there a difference and which is best for your application? What would deleting value from a 2 dimensional array actually do You’re right on this one actually, I do want to replace them with something not just fully delete it, so would I be able to use that subVI and then get it to randomly generate replacements like I did before? &gt; I'm not going to give it away completely and there are a few ways you could do it. Just remember you are working with two dimensions of data here, you need to find matches in rows and columns. Maybe start figuring out how you would find if a set of three (or more) identical adjacent values are located in a one dimensional array, how do you identify the start of that match and how long is the match? Thanks for the pointer, I think I’ve got an idea on how to do this. Thank you!
I'm feeling really, really dumb right now. I'm running LV 2017 32-bit and when testing a subVI, with a Hex String to Number conversion, and I keep getting zero as the result. In the attached image (snippet) I have the string constant set to hex mode and entered 006B (the string I'm getting back from external equipment). I keep getting 0 as the conversion output but it should be 107. What am I doing wrong here? I've tried setting the default type to I32, U32, I16, and U16 and they all return 0. I know it has to be something simple but, for the life of me, I can't figure it out.
I'm on it! Give me like 5 minutes and i'll probably be as confused as you lol.
I got 107
Right click on that string contstant and choose the formatting to show / codes
Attached is an image with an added string indicator set to slash codes mode and probes on both wires. [Screenshot](https://i.imgur.com/B7V6bSi.png)
Attached is an image with an added string indicator set to slash codes mode and probes on both wires. [Screenshot](https://i.imgur.com/B7V6bSi.png)
You have your string constant set to HEX display. That means you're inputing the HEX values of a string of ASCII characters. In ASCII, the HEX 00 6B is " k" That is equal to the number "0" because that's not a HEX string. Instead, right click your string constant and set it to normal and type the hex code "006B" in it. The result is 107.
See my answer above. Don't set your sting to HEX mode. That means you're inputting the HEX values of the ASCII characters in a string, e.g. 006B is " k".
Oh crud, you're absolutely right! I knew it was something stupidly simple that I was forgetting! Thank you so much.
The equipment we're interfacing with returned a string (over serial) with the non-ASCII values and all of my diagnostic indicators are set to display in hex and I just never made the connection. I'll correct the subVI tomorrow to handle the conversion properly. Thank you so much again.
To be fair, I think your way makes more sense. Don't feel bad, it happens to everyone.
No worries, we all get stuck banging our heads against the walls sometimes.
[https://forums.ni.com/t5/Version-Conversion/bd-p/VersionConversion](https://forums.ni.com/t5/Version-Conversion/bd-p/VersionConversion) There's a whole forum for this over at the NI forums. I know people hate on them for their toxicity, but it looks like they're pretty responsive over there. I don't have 2014 installed anymore outside of a specific 2014 development VM or I'd give you a hand.
If u/iYogurt's forum link doesn't end up working for you, PM me and I will convert it. 
Did he lose his because it's now assigned to someone else, or did he reformat and just doesn't know where the license is? I'll convert it if you need. 
I have been working in labview for a couple of years and have never seen this, if anyone could help I'd appreciate it 
That is a breakpoint that has been disabled, I think 
You are correct. You can go to view&gt;&gt;breakpoint manager to see if there are any more in your code.
Hi, Thanks a lot that's very kind of you. The 2018 program was made in the trial version, turns out our university only gets access to 2014...! The .vi is in my dropbox: [https://www.dropbox.com/s/ogix4jwbs59yjqs/16valves\_sequencing.vi?dl=0](https://www.dropbox.com/s/ogix4jwbs59yjqs/16valves_sequencing.vi?dl=0) &amp;#x200B; I'm not sure how to send a pm, I hope it's okay for me to share it here ? Thanks!
yup, sent :) 
Nope, when I was in school there was a bit of programming in BASIC but mostly useless. IT's really interesting to hear you're learning LabVIEW in school.
Thanks, will do!
try built in example. Help -&gt; Find examples -&gt; search smtp
Same deal with the example, blank email.
I use those subs regularly and exactly the same way with no issue. My hunch it is one of your SMTP clients - sending or receiving - that are cutting the message content out. I typically use Mailgun as my outbound server and have never had any issues with clients receiving messages.
That's what I was worried about, I was using an internal smtp server at work but I may have to try something outside.
Arduino is better, you can download digilent linx from VI package manager and you can easily write the program. Linx also supports raspberry pie
Why not just use a 2nd graphics card, even if it's an external one, and just setup another screen output?
We have a huge TV that we have attached to the cycling setup facing the rider, so when he drives the cycle he will see the track 
So why not just connect that huge TV to an extra output from your PC?
That's what we are doing at the moment , but we don't want it to be a second monitor. We just want to give out image data as the output from the pc to the TV display 
Why? You can probably have labview make a borderless fullscreen window, that’d be just as good. 
We also have programs that controls the motor and other aspects of the project and we get to see various outputs. We don't want to display the other data in the TV , and the vision program is integrated with the motor's program. 
That's not relevant to my suggestion :) 
NI does not currently make a c-series module (for your cDAQ) that does I2C natively. You could purchase a DIO card that supports per-cycle tri-state and implement the code yourself, per this article on [ni.com](https://ni.com) ([https://knowledge.ni.com/KnowledgeArticleDetails?id=kA00Z000000P6UhSAK&amp;l=en-US](https://knowledge.ni.com/KnowledgeArticleDetails?id=kA00Z000000P6UhSAK&amp;l=en-US)). However, my suggestion would be to purchase an I2C device that has LabVIEW support. If you don't have too tight of a budget the NI 8451/8452 is a great product that we use in our production test systems and don't have any issues with them. 
okay, my budget is pretty tight I'm trying to work with what my lab already has. Do you think an I2C to analog converter could work?
or would I lose information that way? would I end up having to calibrate it myself? Sorry I know next to nothing about sensors and data acquisition. 
Can you change the sensor to one of the many Analog sensors that Honeywell offers? It looks like they offer that HAF sensor in a 0-10V version.
Unfortunately the Analogue ones are not within my flow range :/ I'm looking at around 50 SLPM
While not the cheapest alternative, the Alicat flow measurement stuff is good quality. Also there is a full library of Labview drivers plus you can run analogue or rs232. Just plug a USB converter in and preserve a daq slot for something else [https://www.alicat.com/products/flow/#g\_models8878](https://www.alicat.com/products/flow/#g_models8878) 
If you want to go cheap you can use an Arduino's i2c interface and output the data over UART back to your PC. I think you may need to use a mega instead of the uno so you can use both i2c and uart at the same time. Someone correct me if that's wrong. Alternately, you can get a cheap i2c reader on eBay and see what happens. There's tons of janky ways to solve this. I would recommend asking for some more money and getting the right component that's already in the NI product ecosystem.
Lots of different things can be causing something like this. Are you calling the subVI statically or dynamically? If dynamically, are you maintaining a reference to the subVI, is it getting a reference to itself? Does the subVI have error handling and is an error being reported? Any references being passed in or out? Is there an upstream error? Are you using sub panels?
do you know if I can run one labView script that will record data from both a cDAQ and a NI 8453/8452 at the same time?
I believe that I am calling the subVI statically. I have error handling and reporting for the subVI. There are references into and out of the subVI - I'm using digital references...could I lose these references when the subVI is closed? I don't think that there is an upstream error from what I've seen from highlight execution. Thanks for your help! &amp;#x200B; Part of VI:
Having a second UI running full-screen on the TV you have is definitely the easiest way to go. You could use the NI Vision External Display tools to easily create a pop-up that you can send images to. Connect the TV and your Monitor to the PC, set up your display to extend your desktop onto the TV rather than duplicate your display. Use the External Display tool to display the image in a popup. Move the External Display to the TV and full-screen the External Display (you might need to set Zoom to Fit so the image expands with the UI)
Yes, you could use the example finder to find relevant sample code and then copy paste them together, but you'd be much better off trying to learn the language a bit and making something that does what you need now and is easy to work on next time. Look into producer consumer patterns or the data acquisition template code. Both of those would be beneficial for this type of application and both are pretty easy to learn.
Are you using the subVI in two places? Does the subVI get stuck, and if so where does it get stuck? Or does it just only run part of the code if closed?
If you wire an array to a For loop, it will auto-index the array. With a 2D array, it will auto-index by row, you can then use an[add array elements](http://zone.ni.com/reference/en-XX/help/371361N-01/glang/add_array_elements/) function to sum the elements column-wise. Wiring the result out of the array will build a new 1D array with the sum of elements. I'm on mobile, so here's a better [picture](https://knowledge.ni.com/KnowledgeArticleDetails?id=kA00Z0000019M3dSAE&amp;l=en-US) explaining everything.
Why do you turn the 2D array into a 3D array before operating? You have a lot of extra code in there that doesn't help you. You can do what you want using Auto-Indexing like you do on the exit of your first loops. The problem you're seeing is that your exit from your While loop is just using the last value it's given. Right-click that terminal and turn on Auto-Indexing. Why are you using a While loop at all? The whole point of a For loop is you can do everything you're doing without all of the Array index operations that you're doing.
oh my god the solution is so simple thank you q.q
i deleted the first post because i thought i solved it myself, sorry. but thank you for the link, the solutions is perfect. i'll delete this post.
It really depends on the bandwidth and how noisy your environment is. The more data you're trying to move, the more likely it is that an error will cause a buffer overflow. The concepts in this article are relevant even though its for wireless: http://www.ni.com/white-paper/52233/en/ . Your bandwidth is your number of channels x the acquisition rate x the bits in the measurement. The noisier your environment, the more likely you are to error. That being said, the USB-6001 isn't that channel dense and doesn't acquire that fast, so it would likely work. Only way to really know is to try it out in your environment.
Well, crap. Please don't belittle me for this, I'd just like to note that I'm not an electrical engineer and my team has been making decisions without me. This is my fault though, I did not realize that the 6001 does not use a pure USB but rather a USB-C or something similar. Oops.
Why does the DAQ need to be so far from a computer? Just use a second computer and remote access? 
The general experience with various extenders, converters and such is that they might work or might not.. Instruments usually use special features of interfaces that are usually not used by consumer products like USB audio interfaces, other accessories. So extenders that work with the aforementioned devices might not work with these instruments. It is quite impossible to try without testing or knowing how the extender works (it’s an Ethernet cable but the actual communication can be whatever). As others pointed out one option is to put a second computer closer to place of measurement or setup a trigger so no user interaction is needed to start the measurement. Or you can purchase a myRIO (similar price range) which has built in WiFi 
USB is the worst protocol if you need that distance. You might want to put a cheap headless computer at the other end and retransmit over ethernet, wifi, etc. Another option for USB is to daisy chain a number of active repeaters. Something like this: https://www.amazon.com/BlueRigger-Female-Active-Extension-Repeater/dp/B005LJKEXS
We're doing a rocket engine for our senior project! Remote access is something we've looked at, just concerned about losing signal.
And what all does the Labview program do? This is very likely ["the X-Y problem."](https://en.wikipedia.org/wiki/XY_problem) Are you just controlling relays with the digital I/O? Or are you actually reading some analog data? Do you need realtime/quick user control? 
Yeah at the moment we are just extending the monitor. I made this post as I made a dumb mistake of using the main VI in which I had all the program as the VI in which the image was displayed. Now I am using queue to have a separate VI that just displays the output and I have extended that window to the second monitor. Also I will check out the NI Vision External display. Thanks for your reply 
I think I didn't effectively communicate to you what my problem was , but still thank you for your reply. I guess I have figured out a way :)
Great! Learning how to ask questions effectively is the #1 way to improve your programming abilities in any language. 
We are using a USB 62xx DAQ with a USB extender at work to do a bunch of timing and analog voltage i/o with a gige USB extender. So it can work. Part of it depends on on you write the data to it, how you buffer it. They are cheap so just buy one and try it out. Might have to tweak some code but you should be okay. 
Cum
just as a clarification, whenever you see these usb extenders using a cat5 or cat6 cable, it isn't usb over ethernet. when you say something like &lt;blank&gt; over ethernet, the implication is that &lt;blank&gt; is converted to an ethernet-based protocol and transmitted over an ethernet network. in the case of these usb extenders, they are simply using a cat5 or cat6 cable as just a cable. in general, you can find usb over cat5/cat6 or fiber (that uses a fiber optic cable) extenders. i have used products from gefen. so something like this: https://www.gefen.com/product/gefen-usb-20-sr-extender-over-one-cat-5-cable-EXT-USB2.0-SR however, these can be expensive. the cat5 cable one is around $300 or so. the fiber ones are even more expensive. however, these are very reliable. the little usb to cat5 extenders, like the one you linked, are pretty unreliable, at least in my experience. the only time i have used them is to transmit usb for use of a keyboard, mouse, and external storage media, and so temporary dropouts were okay with a mouse and keyboard. although, i quickly moved to the gefen devices and have had zero issues so far. i have not used either solution for instrumentation, and i probably wouldn't recommend that. though i think the gefens would probably work well enough for instrumentation. however, this is a student project, so you obviously have strict budgets and must unfortunately cut corners on robustness. there are some options. have you considered using the myRIO? this would allow you to have daq and your control system installed on the site of your system. the myRIO can be connected to a wireless network, and then communicated with a remote computer. there may be solutions to attach the myRIO to a wired network. the myRIO has the benefit of having an FPGA as well, so if you need stuff to happen reliably fast, then it's a good option. the academic pricing is nice on it as well.
&gt; I did not realize that the 6001 does not use a pure USB but rather a USB-C or something similar. i don't know what you mean by this. "pure usb" doesn't mean anything. usb-c describes a connector. it is usually associated with usb 3.x devices, but that doesn't have to be the case. when i looked up the usb-6001 on NI's site, it showed it is a usb 2.0 device with a micro usb b connector (but comes with a micro usb b to usb a cable). &gt; I'd just like to note that I'm not an electrical engineer and my team has been making decisions without me what is your background then? when designing systems, it is important to not get hung up on backgrounds. it is important the system is designed systematically and as simple as possible and that everyone is aware of what everyone else is doing. it is also helpful to be as explicit as possible, especially when designing and communicating interfaces (software, electrical, mechanical, etc.).
My background is in mechanical but I've spent time around labView and know a bit of arduino. Hence why I My confusion with the "pure USB" comment was me thinking that the USB 3 to ethernet to USB 3 connection wouldn't work. Sorry for not being explicit, I'll try my best to go into detail: -We are running a few solenoids (3, I believe, but my team keeps changing things on me and if you can't tell I'm frustrated with them). Those should work by using the digital output to a 5 volt relay, with that relay being connected to the solenoids to complete the circuit. -There are three pressure sensors we'd like to use, from omega (the PX309). -1 force sensor (LC304, for 100 lbf) -1 flow meter (which, I just saw, is not digital which means we'll probably just put a gopro or something close by it and watch it afterwards). Overall, it seems like 4 analog instruments should work with the 6001. I'm probably just being overly cautious. If need be, we may just have the solenoids run on their own circuit, giving us control of the process. Thank you and sorry for the confusion. It's been a rough week.
Sorry for not going into detail. I replied to another person going into more detail about what we're using. It looks like 4 analog sensors (3 pressure, 1 load cell) and then control of solenoid valves using relays. The solenoid valves run at a higher voltage so we're using 5 volt relays to turn those on and off. Quick response time is ideal but not necessary, as we have burst disks built into the system for emergencies. 
What accuracy do you need the pressures to be read? This sounds like something an arduino could do for a lot cheaper. 
this is kind of a confusing comment, and it is even more confusing people upvoted it. the usb over cat5 adapters the op linked have nothing to do with an ethernet network. i have no idea why you are talking about network bandwidth issues here. it is entirely irrelevant here.
Bandwidth is the amount of data being transferred over a connection, whether it is USB or ethernet. The more bandwidth being used the less room for error you have, as it will not be able to recover from the error before the buffer onboard the daq device overflows. It's absolutely relevant. 
there isn’t anything to do with ethernet, networks, or bandwidth in the op’s question. they are simply asking about extending a usb connection. if they weren’t having bandwidth issues without the extender, then they won’t with it. although i am not deeply familiar with the usb-6001, with such a simple usb device, there is almost no chance of usb bandwidth issues. it was designed for usb. even if there were usb bandwidth issues, the article you linked is still irrelevant and confusing, especially for someone already confused.
I disagree. He's adding complexity to the system by going USB&gt;Ethernet&gt;USB, and doing it with an Amazon converter. He's doing it over at least 100 feet, which, if you've tried using ethernet for a run this long in a less than ideal environment, can absolutely open you up to problems. He's going to ethernet not just because of the long run, but because they need a long run due to the danger of the environment, which increases the likelihood of interference. The lower his bandwidth, the less likely he'll run into problems, there's no way around that. No one can say, including you, that this will work without knowing more about: his actual cable lengths, the number of channels, his sample rate and what other things in his test environment might interfere. He should be aware of those considerations before he makes a purchasing decision, because it sounds like funds, and maybe time, are tight.
100.0 feet ≈ 30.5 metres ^(1 foot ≈ 0.3m) ^(I'm a bot. Downvote to remove.) _____ ^| ^[Info](https://www.reddit.com/user/Bot_Metric/comments/8lt7af/i_am_a_bot/) ^| ^[PM](https://www.reddit.com/message/compose?to=Ttime5) ^| ^[Stats](http://botmetric.pythonanywhere.com) ^| ^[Opt-out](https://www.reddit.com/message/compose?to=Bot_Metric&amp;subject=Don't%20reply%20to%20me&amp;message=If%20you%20send%20this%20message,%20I%20will%20no%20longer%20reply%20to%20your%20comments%20and%20posts.) ^| ^[v.4.4.7](https://www.reddit.com/user/Bot_Metric/comments/8o9vgz/updates/) ^|
thanks for the detail. it is always hard to know the details and when asking engineers questions, you’ll always get people trying to engineer things for you. my suggestion is to read a lot of reviews for the usb over cat5/cat6 adapters to try and find the most reliable one. if you can find an externally powered one, that may be better. in my experience they work but are finicky. another suggestion is to see if you can find a used gefen adapter (the one i linked in my other comment). those are quite reliable and industrial. lastly, you could look into the myRIO to replace you’re current control setup. it has similar i/o as the 6001, but it has an onboard fpga and arm chip, both of which can be programmed with labview. you can ignore the fpga if you want. so you could program your control system on the linux side and connect to it wirelessly with another computer. however, that increases your programming work. something to think about though. i just searched “gefen usb” on ebay. looks like there are actually some one for sell on there. make sure they come as a pair. you need one to go from usb to cat5/cat6 then another to go from cat5/cat6 to usb. make sure it is the correct usb version! sometimes those extenders aren’t compatible like usb. for example, i have seen one where it was usb 3 but it didn’t work for usb 2 devices (said so in the details).
you may be unfamiliar with these adapters. as i mentioned in my other comment, they aren’t usb over ethernet adapters. they are usb over cat5/cat6 cables. they don’t have anything to do with ethernet other than using the same cables. their maximum length have nothing to do with ethernet lengths, but are specified by the device. for example, the gefen adapters i linked in my other comment (https://www.gefen.com/product/gefen-usb-20-sr-extender-over-one-cat-5-cable-EXT-USB2.0-SR) go up to 165ft. i have used those exact ones with 100ft cables with zero issues. however, the are powered and more industrial than these simple adapters on amazon.
I know the pic shows zero for all values, but in case you want them: Freq: 4 rad/s V1_mag: 10V V1_angle: 0 deg V2_mag_20V V2_angle: -30 deg R1: 1 Ohm R2: 1 Ohm R3: 1 Ohm L1: 1H L2: 1 H C: 1 F 
I'm most interested in the construction of the block diagram / how to simulate the equations acquired from Mesh analysis there. Any help or advice is appreciated. 
This looks an awful lot like a homework problem. Use the Numeric Palette to get all of your basic math functions, then plug your controls into them. Wire the outputs of your functions into your indicators to display the results. Stick the whole thing in a loop if you feel like being able to update the values and see results in real time. LabVIEW isn't about to solve a system of equations for you so solve your system symbolically and work from there.
Definitely homework. Also file this deep in the "Yes you CAN do this in labview, but why would you WANT to?" folder...
You'll need to work out the equations on paper, and then implement them through LabVIEW's basic math functions.
I could see using it for showing a simulated time response since there are the integration functions, but for steady state it's definitely a waste.
Yes, that would work nicely. You will need to write code on the Arduino to accept commands from LabVIEW (usually via the serial port) and make the desired movements with the motors using the shield. It's more expensive (about $250 each), but if you want to avoid having to program the Arduino, there is a board called a 1240i from Applied Motion Products that accepts serial commands and then controls a motor. It has memory and can be programmed with a series of movements, and it has input from limit switches and the like. You'll need a USB-to-Serial adapter as well. 
Can you perhaps provide a link that can help me write such a code? I am farely new to this
You can download diligent Linx from VI package manager to write the Arduino code in LabVIEW itself 
There are several steps to doing this project. Do you have a deadline for a class? Is this a hobby project that you can do at leisure? A commercial project? Have you worked with LabVIEW before? With Arduino? First you have to learn to communicate with the Arduino via serial. You can use VISA to do this because the Arduino looks like a standard COM port when it's plugged in via USB. It's pretty straightforward, but like any programming, somewhat fussy work to get all the details working. Then you have to write code on the Arduino to receive the data from the serial port. I would start with something simple, like sending a 1-character command from LV, detecting it, and sending something back in response. On the Arduino side, you would put Serial.Available() in a loop, and then read with Serial.read() when Serial.Available returns something non-zero. Then write back with Serial.Write depending on what was sent. Have you done any work with Arduino before? To get it working, I would work in the Arduino IDE and simulate commands by typing into the Serial Monitor window, before you try to get the connection with LabVIEW. I would not use the the LabVIEW package for Arduino for this, but use the Arduino IDE. You have more control, will understand much more about what the Arduino is doing, be able to debug more easily, and end up with better code. Once you have that part working, you'll want to get the stepper motors working. Again, start with Arduino IDE with the serial monitor to get to the point where you can send simple commands from there and have the motors move. Depending on how your shield works, you might have to sequence the leads on the motor explicitly in your code, sending things like 1001 0001 0011 0010 0110 0100 etc to a set of GPIO pins (with appropriate delays between each write). If you have a smarter stepper controller chip, then you might be able to send it things like steps and speed, but then you will likely need to communicate with it over I2C or SPI, which is it own thing. The good thing about Arduino is that there are lots of examples out there (the Arduino forums, StackExchange, youtube videos, etc.), and the makers of shields usually have drivers and example programs available to get you started. 
You will need to either use a shift register or a producer consumer loop. As for where to put the loop it depends on the application I guess.
But I don't need the measurement from the previous iteration, I just need a way to store every new record in a new column. Auto indexing would be nice. I'll look after the producer consumer loop you mentioned, thanks! 
Updating the window settings in the SubVI solved it the issue: File -&gt; VI Properties -&gt; For Category choose Window Appearance -&gt; Click Customize button -&gt; Check "Show front panel when called" and "Close afterwards if originally closed" File -&gt; VI Properties -&gt; For Category choose Window Run-Time Position -&gt; For Position choose "Minimized" Found solution at [https://forums.ni.com/t5/LabVIEW/My-sub-VI-doesnt-work-unless-it-is-open/td-p/2841984](https://forums.ni.com/t5/LabVIEW/My-sub-VI-doesnt-work-unless-it-is-open/td-p/2841984)
Thank you for the advice! I have a lot to learn about efficiently coding in LabVIEW. 
The value wired to N will be the number of points in your graph. So 1.25 will give you 1 point.
Well what you programmed doesn’t match your equation. You also need to keep in mind that the math on paper is a continuous function. Your program is going to be discrete, meaning that it will be composed of small steps. It will be up to you to determine what that step size is and account for it. Among the issues you currently have, you have the iterations count wired multiplied by 2pi. That makes your step size effectively 2pi, which means you’re basically generating the same point every iteration. 
My process for writing code is to start with a clear goal, then break that goal into individual steps that computer can perform. I don't think you've done this because your code doesn't have a clear goal, it's sort-of contradicting itself. (Sorry, this is the most polite way I could think of phrasing this) Here's what your code does right now: It creates an array with "T" (your variable name, not mine) numbers in it, and passes that array to your output once the array has been calculated. This means that (assuming your math is correct, which I haven't even looked into) you will output a picture of a sine wave matching the input variables you chose. This is probably what you want to do if you're working on geometry homework and you're being asked to draw a sine wave with some specific attributes. But, you have this wait function in your loop. What that means is that your program will calculate one value of your array, and then wait for some number of milliseconds, then calculate the next value and wait again. Finally, after waiting for T times your wait time, it will output your graph. If you want a static image, there is absolutely no reason for your wait function, because you're only updating your graph when you leave the array, and you only leave the array once. If you turn on execution highlighting (the light bulb icon by the run button), you'll see this happening live. If you are trying to create a "live" moving output of your sine wave, you need to use a Waveform Chart instead of a graph, and put it inside of your for loop. This way, it will update and display the new value every time it's calculated, instead of just once at the end. 
Well, the Analytics and Machine Learning toolkit is an obvious place to start, although if you're already familiar with machine learning I'd guess you've done it in Python, in which case you might want to look into the Python integration built into more recent releases.
Can't really speculate based on nothing. Are you able to share your code? One thing that sticks out, although it isn't likely to be the cause of your problem - why the 50ms delay?
Not at the moment. But I am putting it at 50 ms because i am using periodic tasks. I feel like it should be 20 ms, because that equals 50 htz or whatever the unit is, but my mentor told me to put it at 50 ms which I think is off.
The units could be htz instead of ms 
Try probing it, see if the motor controller indicator lights change color, make sure the pwm cables are in the right rio ports, make sure you’re calling and writing to the drive train and arcade drive, and most importantly try to post this on Chief Delphi with the code included. Also if you get the code don’t hesitate to dm me, I’m the head student programmer on my team. Good luck.
Also why periodic tasks? Just call arcade drive and put it in teleop, which runs in its own loop.
We are currently not using teleop. I don’t know why we aren’t using it be our mentor specifically wants us to use periodic tasks I have the picture of my code right here 
Also how do I send a photo lol
Nevermind I resolved the issue it was the ms delay
Thank you
Implement a single P controller - actually a subtract and a multiply - to get to the set point defined by the input
I would definitely combine those two loops. What is the total time you want it to take to ramp between some set points say 0-1? Basically you need to set a time step. Then you will add the exponential amount according to that curve based on the time that has passed. 
Here is an honest bit of advice: you aren't doing a good job of asking for help. The thing to is to say "Hey, I'm trying to do X but when I run my code all I get is Y, what's going wrong?". No one knows what your application is trying to do or what it's for. Showing us the block diagram of the entire application doesn't explain anything. Also if you are going to show us code, upload the .vi file somewhere or at least post screenshots that include the entire block diagram. &amp;#x200B;
u/L0ngp1nk is correct. You've provided too little information for anybody to help. It looks like you have the same subVI in that loop twice, so my guess is you need to wire the output of the top VI to the "0.1" add/subtract functions like the bottom subVI.
A lead lag might work
You can use this [Protocol Analyzer](http://sine.ni.com/nips/cds/view/p/lang/en/nid/213379) to analyze the I2C frames and send/receive data to your device. There's a free trial If your budget is tight. 
Also posting on Chief Delphi may grab some of the more FIRST specific LabVIEW people. I agree with others I just don’t fully understand what you want!
do you know how long the evaluation period is?
1 month if I'm not mistaken. 
Can you share a piece of code, describe what you'd like to do and tell us what happening? Because i guess the obvious answer to your question wont help you: Connect the potentiometer to the Arduinos Ain. Then, in a loop: read the voltage in Labview and process it with the PID controller to get the signal for the actuator output. 
Is this what you had in mind? [https://www.ebay.com/itm/113612805598?item=113612805598&amp;autorefresh=true](https://www.ebay.com/itm/113612805598?item=113612805598&amp;autorefresh=true) 
There are a few ways to do this but the easiest is a transitor. There are NPN and PNP transistors but getting a low current 5V signal to switch in a higher current supply is something pretty basic and is common in most small electronics. Here is something Google found. https://roboindia.com/tutorials/arduino-transistor-relay And here is something from NI showing a low current digital from a myRIO triggering a relay with a transistor. https://learn.ni.com/teach/resources/15/relay
It should be that's what's strange. Do you know how any settings that could be changed to make it work?
Your data sheet says &gt;each channel needs a 15-20mA driver current so i'm not sure where you're getting 90ma from. It's really weird that your relay would be working as NO vs NC. Can you hear it clicking. There is a trick if you need a little more current. You can connect four DIO lines together and switch them all at the same time. Their currents are additive (5ma+5ma+...)
Oh, now I see that now. The 90mA comes from the power needed to run it (that wiki links to a datasheet which seemingly omits the channel requirements). I honestly feel like a bozo for only paying attention to the datasheet. On the plus side I would have never thought of that. It should work as have 3 relays we need to use are there are 13 I/O digital pins. I'll let you know if that works! Thank you!
Comment so I can stay notified (I know this is a Facebook style thing but not sure of reddit etiquette) I potentially have a similar task of collecting accelerometer data with myRIO
Can you help us by sharing a screenshot of your code so far? You said that Tick Count doesn't reset, but that doesn't make sense if you use the timing properly. The Tick Count function returns the current clock tick count in milliseconds every time it runs.
Use a waveform chart instead of waveform graph. The x axis will be in "data points".
https://ibb.co/Cz1bkSx Huh... I haven't saved my last VI. Tried to recreate it by memory, and this time Tick Count skyrocketed up to 10^8 instantly... Oh, well Aside from that, the problem remains the same. No graph whatsoever. If I raise or lower the slide, Y axis scrolls up and down, but that's pretty much it. I tried to put all this into a while loop with boolean button as stop condition. Haven't done me any good really
Tick Count doesn't work the way you think it works. It's a large number that corresponds to your computer's clock, and doesn't start at zero. What you want is the Time in Seconds function. [See example here](https://forums.ni.com/t5/LabVIEW-Idea-Exchange/Simple-chart-with-time-as-X-Axis/idc-p/1255536/highlight/true#M7868)
So that's why it skyrocketed. I see.. Also, there's waveform chart Rock It Scientist mentioned I should give it a shot. Hopefully this will work. Thanks a lot
Do you have any plans/timescale for a LabVIEW API? At the moment is reads as if the data has to be uploaded manually.
Those express vis make it hard to see what’s going on. Depending on your hardware you may be able to sample at 2 different rates. You just set them up like two different DAQ tasks (aka drop two of those DAQ assistant vis). You’ll want to put them in two separate loops. if your device dies t support two separate tasks then you’ll sample at the faster rate and decimate the extra data for the slower channels. As for your error. The DAQ is acquiring data faster than your software is dealing with the data. It’s probably because of that wait in the bottom right corner. You typically don’t need a wait in a DAQ loop because the DAQ read will limit the loop. Finally, i would log the data to file as it came in (meaning each loop iteration). This helps with a few things. 1. You won’t lose data if your application crashes. 2. If (in the future) you had to speed up your data rates you won’t be at risk of running out of memory during longer acquisitions. That said it’s hard to run out of memory acquiring 1 channel at 320hz.
https://knowledge.ni.com/KnowledgeArticleDetails?id=kA00Z0000019KTeSAM&amp;l=en-US
okay I tried two daq assistants in different loops and it is giving me Error 50103 saying "The specified resource is reserved. The operation could not be completed as specified." so I'm guessing labView wont work with 2 of these. I'm looking into trying without the express VI but i'm not very experienced in this program so i'm having trouble.
That error means your hardware does not support two simultaneous DAQ tasks. You'll have to sample at the higher rate and then decimate the data.
Thanks for the tip, getting rid of the wait did solve the error!
Have you tried this? &amp;#x200B; [Example program](https://forums.ni.com/t5/Example-Programs/LabVIEW-Android-comm-via-bluetooth-serial-using-LabVIEW/ta-p/3534964) &amp;#x200B; What I thought first was creating a simple app in android and then configured it to automatically send to the bluetooth whatever is on the clipboard. &amp;#x200B; I have use [appypie](https://www.appypie.com/android-app-builder) before, maybe you will find it useful.
What's encoded in your barcode? If it's just ascii text, you can probably find a really cheap USB barcode scanner. They act just like a keyboard so you can use the keydown/up/value change events on a string control. Otherwise, I like the bluetooth idea from /u/Emilior94 
That will be better, the only problem is the cost, but I'm surprised there are some cheap barcode scanners, I thought they were over $75 usd
[removed]
Hi Sam - thanks for the question / sorry for the delay I thought I had my notifications on. That's correct currently TDMS files have to be uploaded to the website. We're gathering feedback to see what next steps make sense for our users. What kind of integration did you have in mind ? A component directly on your block diagram ? Or some other sort of API ? &amp;#x200B; &amp;#x200B;
I was thinking something on the block diagram, yes. Ideally with the same functionality as the matlab plugin demo on your website.
Understood. Yes we definitely want to add that although no exact timeline yet. This will also enable data streaming to a dashboard in real-time. Do you have a specific project you would like to use curve with ? If so I would love to figure out if there’s a quick win for fitting into your workflow. We can discuss in more detail over email if you’d like: paul@getcurve.io. 
The barcode readers are looking a lot more appealing than the smartphone idea due to ease of implementation, even though they do cost a bit. I really appreciate the feedback, guys.
There is a wait function in the timing pallet that will wait a specified number of milliseconds. That would be a very simple yet open loop way to do it. A better way would be to move to a position then wait until the loop controlling the sampling has finished. The sampling loop can send a signal back to the control loop that it has finished and the control loop must wait until it receives the signal before waiting. A screenshot of your code would help me diagnose and give a better suggestion.
Yes, sorry. I don't have access to it at them moment or else I would have provided one. I can provide it later, if still necessary. I will test your suggestion. Thank you, really appreciate the help.
Is the sensor hooked up properly? Does the sensor even work? Have you been able to communicate at all with the arduino via any interface? LabVIEW? Do you have any code to post?
Hi. Yes I’ve been able to use the sensor using the Arduino interface and it all seems to function properly. However I am wondering how I can display the read voltages in labview. I’ve been attempting to use the Linx tool kit to do so but I’m sort of stuck right now. My set up is essentially this. https://cdn.sparkfun.com/assets/learn_tutorials/5/4/6/HX711_Fritzing.JPG
Can you share any code? I'm assuming you have already scoured [this](https://www.labviewmakerhub.com/doku.php?id=libraries:linx:start) for the tutorials and examples. Do you get LabVIEW error messages? Pictures and detailed descriptions always help helpers
I like it. I'll look into it.
:) Glad you do!
Oh actually, perhaps a better idea, the increment/decrement buttons from the default Modern-style numeric control
\+1 could be Increment from LabVIEW 20xx, -1 could be Decrement from NXG...
1++
I think the Increment and Decrement nodes themselves would be ideal rather than incr and decr controls on a numeric.
My CLD recertification was issued in 2017 and expires in 2020. ¯\\\_(ツ)\_/¯ 
You have to certify every 3 years. Every couple years LV introduces new features. For example malleable VIs. The thinking was that the certification implies that you're "fresh" and know how to use the latest features.
You can also [recertify by points](ftp://ftp.ni.com/evaluation/certification/recertification_by_points/recertification_by_points.pdf), which some people prefer to taking the recert exams.
I don't recognise the component after the index array, but you're doing /something/ to the 0th index and... the 0th index again. If that's meant to be the first index you need to manually wire it in. As for debugging, I'd put a probe at the bytes out and see what comes out. 
I took that from another I2C example, so im not even sure if that's right. The data sheet for the sensor gives this Transfer function equation: flow applied= Full Scale Flow \* \[(Digital Output Code/16384) - 0.1\]/0.8. I'm thinking I need to do something with the bytes read and this equation, but I'm unsure because when I did this it still wasn't giving me a reading. 
Yep, bi-yearly trips to NI Week and CLA Summit take care of my re-certs.
Assume you're looking to hire someone for a LabVIEW project. Do you want the guy who knew how it worked 15 years ago and hasn't touched it since? Or the one who can readily prove up to date knowledge and practice?
I think you're just reading the data too fast. I would add a 10ms delay per iteration-- the data sheet says to wait that long between reads. If that doesn't work, have you checked what's actually happening on the bus? If you look with the scope, do you see all 0's on SDA during the reads? Is ACK being generated?
Because NI makes more money if you have to keep doing it...
Do you leave those on? I always turn off the the visibility of those buttons so I can make my UIs look flat. Example: https://i.imgur.com/AwfWxwG.png Would look terrible with those buttons present.
Index array auto-increments unwired indices starting from the last wired index. The first 0 index wired up isn't even necessary. &amp;#x200B; OP, what prompted the change in slave address? In the first example you use 100100, the second was 49. Was the first example, 100100, supposed to be in binary? Right click on the constant and select Show Radix Before worrying about the transfer function, just see if you get anything out of the I2C read.
It might be useful if you sent us a [snippet](http://www.ni.com/tutorial/9330/en/) instead of a picture of your code. Also, I have no idea what you're trying to do with that equal sign. It seems to be saying if an image equals an array of colors? What? That makes no sense and that's what the broken wire is telling you.
The output of the Boolean node is an array, you are trying to connect it to a scalar indicator. 
This right here. 
Huh, TIL. I'll still always wire my indexes but good to know!
This is right. You are going to need to build that array and use "And Array Elements" for the logic instead of just "and".
The first one was in binary, I went ahead and changed it to hexadecimal. The sensor data sheet says the address is 0x49. How do I go about seeing if I can get anything out of the read? whenever I run the program the numeric constant only shows 0.
I think flat UI's are overrated myself. And I generally leave them on; I don't use LabVIEW professionally. I just use it for fun.
Fair enough. I half agree with you on the overrated part but I do use LabVIEW professionally, and I've found that flat UIs are the only ones I can really generate with the limitations of LabVIEW UI design. Resizing and UI customization is just plain hard in LabVIEW. I've often considered just running webservers on my cRIOs and designing a nice React GUI in an electron application. However, have to consider the 'hit by a bus' factor and make sure my co-workers can edit my code.
Not sure as I’ve never used the rio thing, But a tip I can give is use the error in and error out on each subvi instead of the sequence structure. 
I think you need to run the task out from the DAQ Assistant express VI to the task in portion of the DAQmx Timing node. Since the error indicates the task doesn't exist and you didn't supply the timing node with a task.
Because the RIO has no UI it doesn't support Controls. As an embedded device the usual use case is to have your code running continuously and if you need some user intera option then you have to setup some communication mechanism between the RIO and a host computer. TCP IP is a common method but shared variables and others are also available.
Try to get in the practice if tidying up your diagram as you go and don't make wires go backwards. It will help in the long run and it takes less squinting and brain hurt our end when we're reading it :)
You have not given the task reference to the Daqmx timing property node. Connect the task out from the express VI to the property node and you wouldn't get the error. 
XY graphs display what you wire into them, to "reset" it you must wire in nothing. Reinitialise your array to have zero elements and pass it that.
But then when my array doesn't get reset, so when I do an new measurement I get all the values back
sounds like you aren't clearing wherever you're storing the measurement data when you want to do this "reset". Showing your code would give us a much better idea of where the bug is.
I'm only a casual but is it possible to reset it with a property node? Right click on graph, create, property node, data history, place the icon, right click that, set to write, create constant If you manually want to decide when to reset create a case structure first and have a boolean switch to trigger it 
that only works with charts... not with graphs... it's very annoying, but thank you
will do, thank yo :)
My bad way; when you first make the graph, copy and paste it and then right click on one and change to control. Hide the control. Instead of “clearing” it, write the blank one to it. 
You are resetting the graph to default but at the same time the data still exists on the XY wire so every iteration it'll rewrite all the data on the wire to the graph. My suggestion would be to wire the output of the build array (where you add new data to existing data) through the case structure. If Clear is True, wire an empty array to the output. If Clear is False, pass data through.
This is the answer you're looking for
You should set the bottom daq assistant up to be continuous (not hardware timed single point) which will give it a hardware clock. currently youre just sampling whenever that loop comes around, which is slow, inconsistent, and wont really give you meaningful data. At worst, put a wait timer in the bottom loop so you can get some level of timing. 
I'm not very familiar with the daq tool, but does it not have a trigger function to get the timing right? 
it does, but youre telling it not to use it. youre setting it up for one sample on demand, which means when you call the VI, it gets 1 sample as fast as it can. If you put it in continuous mode you can instead have the 9215 get clocked by the onboard hardware clock, take samples continuously, and then just return a chunk of them (number of samples per fetch) each time you call the VI. A good ballpark number is 1/10th the sampling rate, so if youre sampling at 500hz, set the number of samples to grab to 50. 
Do you have an example? Is it just an intensity plot? https://forums.ni.com/t5/LabVIEW/Using-negative-values-in-an-intensity-plot/td-p/2548129
yeah sort of like that, should look like this: [https://www.mathworks.com/help/examples/phased/win64/RangeSpeedResponsePatternOfTargetExample\_01.png](https://www.mathworks.com/help/examples/phased/win64/RangeSpeedResponsePatternOfTargetExample_01.png) I used the intensity plot but it only gives me frequency over time.
Did you see the code in the post I linked to? That is not vs time. Are you confusing intensity graphs with intensity charts?
Graph or Chart doesn't matter right? One saves previous data the other not. And the "code" in the link is only an intensity graph with frequency over time and not range over speed.
Frequency and time are just labels. You can type in whatever you want. In the example, it's just random data being plotted.
and what about the values of the axes? Are they set automatically or do I have to set min. max. values myself?
Yes and yes.
For some reason when ever i try it, labVIEW deploys my cDAQ program to the myRIO as well
You have your choice of either manual or automatic but 99% of the time, you want to display all the data so you use automatic. You can also include scaling so that the value at index [10,10] appears at [100,100]
LabVIEW is just executing the instructions you gave it. 
Programs run in their targeted application instance. You can see where that is at the bottom left side of the front panel or block diagram. By default, it's set by where the VI is located in the project hierarchy http://zone.ni.com/reference/en-XX/help/371361P-01/lvconcepts/application_instances/ 
okay i just had to open my cDAQ labview first so that it wouldn't open inside of the myRIO project. I still don't see how it would work to combine the VIs if it can't handle the cDAQ program being in the myRIO project.
You have to get the data over the network to the host. Many beginners like using shared variables for this purpose. http://www.ni.com/product-documentation/4679/en/ If you want to get a bit more professional, you can look at this example project: http://www.ni.com/example/14137/en/ which uses network streams
That seems to give me the samples in huge chunks instead of updating in real time though, is that the best I can hope for?
What about an array of clusters? The user presses the add button and that adds a new element to the array that the user can then populate?
Yeah, that was my fallback option. I was just hoping there was a more dynamic way to do it (mostly because I hate the appearance of the array control). Thanks though!
it will give you samples in the size of chunks you tell it to give you. if you want to sample 500 times a second, you dont want to have to poll the device 500 times a second, its a lot easier to just ask for it maybe 10 times a second with 50 sample packets. Even if you set the packet size to 1, it will at least hardware time the samples instead of software time them. 
You can make both the array and cluster completely transparent. It just requires the right order of operations: 1. Drop an array, change to the color tool and color the BG transparent. 2. Right click its border with the color tool and with the color dialog open, press the space bar to toggle FG/BG selection (note the line of text at the bottom of the color dialog). This will allow you to color the border transparent. 3. Do the same with the cluster and then you can insert it into the array. Note that the array element and the cluster still have the border, so you will possibly get more distance than you want between the elements. There are some 1 pixel border clusters floating around the web (specifically, on LAVA). More dynamic creation is possible, but will be more of a hassle.
There is a calibration function within NI Vision designee exactly for compensating for lens distortion. Basically you put a paper with a grid of dots with known spacing in the FOV of the camera, and the function can do the adjustment itself. I can’t remember what it’s called off the top of my head though. 
thanks for your reply! I don't mind if the image is shown as a spherical image or not, all I want to do is display the 'true' x/y coorindates. Will this function help me achieve this?
Check [this](https://knowledge.ni.com/KnowledgeArticleDetails?id=kA00Z0000019QsaSAE&amp;l=en-US) out.
Thanks!
Thank you! I’ll definitely try this then. I had no idea that there were 1px border arrays available, but that sounds like it will be perfect.
How about something like this? https://github.com/unipsycho/LabVIEWdotNetDataGrid
I don't see a coercion dot on the "greater than" which means to me it's trying to compare dynamic data with a single scalar. Try inserting a "Convert from Dynamic Data" configured as a single scalar output (assuming your DAQ assistant is set to 1 sample on demand) then compare that output to your 2.5.
I will try it tomorrow and let you know how it goes. Thanks!
Hey, I did the convert from dynamic data as a "float" instead of a "boolean" and still didn't get a coercion dot: https://gyazo.com/323dabc203249935af7bfd836893c307
Yup, that's a good thing. If you configured a single scalar as a floating point (double) then the output is the same as your 2.5 constant (also a double). Hence no coercion dot. Does it work now? If not, double click on your DAQ assistant and show us the settings.
So you're creating an array of 13 elements that are multiples of 25. Running the for loop 25 iterations and sending some command to a pump? 25 iterations with a 500ms wait time = 12.5s. So I assume your real aim here is to send a command to a pump every 12.5 seconds? Am I on the right track? If I'm on the right track then all you need is a while loop. Plug a one into a shift register. Run an elapsed timer of 12.5 seconds in the loop (there's an elapsed timer under the timing palette). When time has elapsed, send your command and increment the shift register. When the shift register = 13, stop your loop.
Yes I am creating an array with 13 elements and then multiplying the elements by 25. What I was trying to do was to send a signal to the pump every 25 iterations or another value. I put the wait time to be 500 to make it visible when the pump got the signal for 25-75 etc to troubleshoot the VI. The time to wait isn't really important. I want to make the Boolean be dependent on iterations because my Main VI will be based on iteration values and not time. 
I see. Well you can simplify your logic a bit. You don't really need a for loop. A while loop would be better. Instead of creating an array, use the divide and remainder function to divide (i + 1) by 25. If remainder comes out zero, do whacha gotta do.
How are you reading the file? I've never done anything like this but you could try building a new file from the contents of the original starting 7 bytes in using set file position.
Wow! Your divide and remainder idea is great. Thank you. I'm wondering if I could remove the array components and simply have a shift register for my (i+1) value. By using the divide and remainder feature I could make sure that if my iteration number is a multiple of my C with no remainder. If this is true it would send a True statement to my pump and thus activating it. 
Ive tried reading it as a text file and reading it as a binary. And have tried different ways to minipulate the string with truncation or converting it to a U8 array and deleting the first 7 elements. Tried deleting the first 7 string characters... Tried saving it as a text file, tried saving it as a binary. But theres always nul and mistakes in front of the next bytes. Ive grabbed a screen shot from some test equipment and the SCPI response adds a hash data block in front of the PNG data. I just need to delete the first 7 bytes to make the image usable.
I tried this for like 4 hours at work one day (had no better tasks...). Could not figure it out. LV’s utilities for reading and writing binaries are really shit. Like you said, I was able to do this in another language in like 5 lines. 
Make sure prepend array / string size is false when writing to binary file (this is the NUL garbage you're seeing). Also after writing the data back to disk, make sure to set the file size to the old file size - 7, so there's no garbage at the end. [VI snippet](https://imgur.com/a/h74aYrm)
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/F7S703S.png** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) ^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20eihwem6) 
First check if you start LV (first instance) as administrator. Second check LabVIEW.ini: allowmultipleinstances=TRUE
As reference the INI file is located here (assuming default install directory): 32bit: C:\\Program Files (x86)\\National Instruments\\LabVIEW 2017\\LabVIEW.ini 64bit: C:\\Program Files\\National Instruments\\LabVIEW 2017\\LabVIEW.ini
That did it! That parameter was not present at all, so I added it and set it to False. I also set labview.exe to start as administrator as you said. Thanks a lot fantasmag00ria!
No no, if you set starting LV as administrator vi's might get opened in another instance. If you don't need LV to start as admin don't do it ;)
Come again? What do you want? And why can't you contact the creator?
Good point! I removed Admin. The ini-parameter was enough. Thanks again.
Thanks!
Switching the string size to false and then resizing it size-7 did it!!! Thanks so much for the help! I'll have to ask HR if we have a budget for giving reddit gold to kind strangers that help! Here's my solution which is fairly similar done in labview 2014 https://m.imgur.com/a/Ji9yx6B
I don't totally understand your question either. I'm guessing you're wondering about how much control you have over the styling of the controls? It looks like this may offer several pre-made styles with a tool to customize the coloring of those styles. If you need control over the styling, you can design your own styles yourself. Right-click on a control and select advanced &gt; customize. You can switch to the part editor and modify all the parts of a control. You can even override the display with your own images if you wanted.
Muun, no, I didn't mean that. Maybe I explained the issue wrongly. There is [a link](https://rafasolutions.com/products/labview-experience/labview-ui-kits/31/custom-labview-ui-kit?number=SW10031) in the article, which redirects to a page called "LabVIEW UI kit creator". And there is a form that lets us choose the colors of the kit and the icon styles. What I meant is will the form itself be enough to make an order from that company? I mean, anyway, as a customer I will need to contact them with email and clarify some aspects that are important for me. So, what do you think, is that tool helpful?
Why, shurely, I can contact the creator. I mean, If I should contact the company anyway, would that tool be helpful for a customer? 
Oh I see. I mean, you're a paying customer, I'm sure they'd work with you. To me the form looks like it's enough. They have a inquiry form you can fill out for additional questions: https://rafasolutions.com/support/index/sFid/16/sInquiry/detail/sOrdernumber/SW10031 RAFA contributes a lot of UI kits to the community. They're a good company and will take care of you.
What I'm wondering is, why do they make it look like it's a tool that automatically generates a UI kit from customizable options, when it's really just an order form for a custom-made one costing upwards of ~$200?
Have you tried the keyboard shortcut "Ctrl+?". In a default installation that should open the labview help window
What version of LabVIEW?
2018
No, but I already know how to open help through the context help window. Thanks though!
So you're not crazy. Opened 2018 and the menu selection isn't there. However, the equivalent is the Ctrl+? shortcut that /u/tigershadowclaw mentioned. Then switch to the "Search" tab. https://i.imgur.com/Peh7m9V.png 
[Here's something](https://forums.ni.com/t5/LabVIEW/Two-Threshold-Analog-to-Digital/m-p/2738970#M809660) I used a while ago that works well. It is a two level threshold, so the counter goes up above a limit, and then only goes up again if it goes below a limit, and then above it again. If you count the times you are above some level does each sample increment it by one? I wouldn't think so which is why I though this would work well.
What do you mean gather the information from labview? Have you attempted to use the DB toolkit to connect to a database and store/read data? If you have succeeded then do you have a further question?
Most barcode scanners act as if they were keyboards. When you scan something, it will type letters for you and then a carriage return line feed.
Can't really understand that mate, how would you do it with a DC input from an input daq, won't let me use it with that dynamic data
Are you giving them the development version? Why don't you give them a run time version so that it can't be moved? 
Yes because the may want to further add something 
Alright. Check out this thread if you haven't, looks like they've got answers: https://forums.ni.com/t5/LabVIEW/How-to-lock-the-controls-in-front-panel-of-LabVIEW/td-p/136320
Thanks! 
Then convert from dynamic data, or don't use dynamic data. Perform a read with normal DAQmx functions like the "Read" function. Then have it read what you want which for you will either be 1 Channel N Samples, or N Channels N Samples. Using dynamic data hides lots of those various settings and can make shooting yourself in the foot a non obvious problem. If you go to Help &gt;&gt; Find Examples and search for Voltage - Continuous Input.vi you'll see the basics of how to use DAQmx without using express VIs. But to be fair in your case it is likely easier to just use the Convert From Dynamic Data, selecting the right conversion for your application.
In most situations what you do is have your source code, and commit it to a source code control system. Then build that source into an application. This is what the users of your software interact with, they don't have to have the full IDE with all the everything to run it. If a developer ever wants to make changes to your software they will checkout the source code, make changes, and make a new build for the users to use. I know it seems convenient to just make a system that both developers and users of your system use, but these processes help keep you from making mistakes that cause lots more trouble down the road.
Have your mouse down event create a timestamp. Store that however you like. Shift register would be better. Have your mouse up event create a second timestamp. Subtract the first timestamp from the second and you have your elapsed time.
I implemented as close as I could but its inconsistent. Did I do this right? https://i.imgur.com/omb9Aum.jpg
Not quite. Ditch the lower event structure and add a mouse up event case to the top structure. Is there a timeout case in those? If there's no timeout case, the event structure will sit until it gets an event and keep your loop from executing. It may be why you're getting inconsistent results. Ditch the bottom event structure, add a case for mouse up in the top one and see where that gets you. Shouldn't matter if you have a timeout case in the top one or not.
I'd rather implement it somehow in this way: https://i.imgur.com/O2lvRds.jpg
Nevermind I figured it out, thank you.
Thanks for the link, haven't seen that! 
What's your load? The 9264 can only source +±16 mA all channels maximum and ±4 mA per channel typical. Assuming your top graph is the output signal from the basic function generator, that waveform looks like what you're expecting. Also make sure you're sampling fast enough on the read. 
That is indeed the output from the function generator, though I was expecting a continuous sine wave instead of a sort of rectified wave every second
I meant if the top graph is what you're sending to the 9264, then you're using the function correctly. I suspect a load or sampling issue or possibly a grounding issue due to the differences in your input/output module.
[Turns out you're right](https://i.imgur.com/Ufgul1n.png), the top graph was all that I was sending - only a second's worth of signal repeated ad infinitum Regarding the grounding issue though, I've got no load (the DAC is connected directly to the ADC (NI 9215) I do think it's an issue with the Function Generator block, though I don't know how to go about remedying it.
Does it has to be a TDMS file? This is super easy if you can use a delimited spreadsheet file.
I'll have to ask my supervisor. How would it work with your method?
tdms data is one dimensional only. you can do what you want, but you need to keep track of the columns yourself. or, you can create separate channels for each column.
If you just split out the Y values of the waveform and wire that into write spreadsheet string without a format it will give you the values in a single row. I don't know what you do with your waveform though... [https://i.imgur.com/WV6t1Cs.png](https://i.imgur.com/WV6t1Cs.png)
Looks like you are storing all your data in 1 channel. Try using a incrementing value for the channel group. I.e first time you write to channel group 1, then next time channel group 2 etc... This can be achieved via an increment with either a feedback node or shift register.
And this doesn't create new files with each increment but new columns? 
Is this your homework?
Yes, it is possible, and custom robotics is something that labview/LV RT/LVFPGA are used for fairly often. The Control Design and Simulation module of labview is typically used for this, it kind of mirrors how youd build out the models in simulink. Its also technically possible to compile your matlab model and hook it into labview directly via Veristand, although ive never used a myRIO in veristand. 
It was in a last course but we never got the right answers. I didnt underst the followings: · Text file has the time and date in the beginning of file followed by temperature data. · Front panel has a separate start button which is activated only when text file has been successfully created or opened. · Temperature values are displayed on a waveform chart while measurements are taken. &amp;#x200B;
You don't need to use veristand actually. You can just use the veristand toolkit that adds the compilation properties to Simulink. It allows you to easily build a LabVIEW compatible .dll that has a step function for the model at a fixed time-step.
It sounds like you might be missing some of the basics on how TDMS files work. Data is referenced by name, not by column or row or index. In a TDMS file there aren't rows or columns. When you import it into something like Excel they represent parts of the TDMS file into a spreadsheet making it easier to view but the file itself isn't defined by rows or columns. The file and it's data is organized by the naming of the Groups and Channels. When imported into Excel a Group becomes a worksheet, and a Channel becomes a column. But things like the order of channels in the TDMS file isn't really required or defined in any way. It just so happens that the order that Excel imports the channels into columns, is in the same order that the Channels are first written, but that order doesn't have to be followed. I could make my own importer that imports Channels in alphabetical order because order doesn't matter to the file, and it is just a way to view a TDMS file in another way. All that matters is you reference your data by the Group Name, by the Channel name, and then by the sample, which in Excel will be the row. So when you write data to the same Group, and same Channel that is telling the TDMS file you want to append this data to the data already there. If you want a new Group or Channel you'll need to specify a new name. I presented on TDMS at NI Week a couple [years ago here](https://forums.ni.com/t5/Past-NIWeek-Sessions/TS9046-TDMS-File-Format-Usage-In-LabVIEW/ta-p/3315943) if you are interested in more.
I used the tool in the edited post, now it works like I want it to (well, just in the simulation and not on the real setup)
Thank you very much! I like your presentation! Yeah you're right, I'm just a student helping out as a side job and noone at my institute knows LabVIEW. I don't even know how my supervisors are handling the TDMS data so I just checked with Excel. I found a tool to display the data in Excel like I wanted (see the edited post). What do you think about that?
The Write to spreadsheet express VI can write to XLSX files. There is also the Report Generation Toolkit which is included with several versions of LabVIEW and is basically an ActiveX wrapper around Excel and Word functions. For these to work you need Excel or Word installed, while the express VI doesn't rely on that. If this works for you great, but if an Express VI doesn't meet all of your needs it is mostly useless. Express VIs are meant to handle the most common use cases, but when they don't meet your need there isn't much in terms of editing them. Sure you can convert them into a normal subVI but the code behind that one in particular is quite a large code set which is hard to understand.
Thanks, I think the express vi is fine in my case. Is it possible to save data after a measurement has run? Right know it only saves the data if I enable it prior to running the VI. Would be cool if I had the chance to save whenever I want. 
Probably with a case structure and a toggle enabling or disabling logging while the loop is running. You may want to checkout some [LabVIEW basics](https://labviewwiki.org/wiki/Training) if you want to learn more.
Take a look at the file i/o library under programming VIs in the block diagram. Without telling you how to do your school work, pay particular attention to open/create file VI followed by the write delimited spreadsheet VI. In general, to keep from holding onto a ton of data until the end of the program, it would be best to write the data on each press and use the close file VI upon exit. It would probably also be a good idea to explore event structures in the help because they are great for taking UI elements like buttons and performing actions without constantly polling them which results in increased VI performance. 
Does this method require using a shift register inside of the case structure to add to the array only on button press? 
The only shift register would be for the file reference. The data would go directly to the file and go no where else. 
Also worth it to go to Help-&gt;Find Examples. You might find one that does exactly what you're looking for already. But yeah event structures for gui things.
The way references work means that you never need to put them in a shift register. I think a simple tunnel will work the same way.
I don't believe System Exec.vi exposes the standard output or standard error until after it has finished executing. I don't believe you'll get the information you want that way. Can the python scripts set up a udp broadcast that LabVIEW can listen to? Or maybe they log to a file that LabVIEW can continuously read?
The symptoms you are describing sound like a driver issue. Remember, there is a specific [order of operations](https://knowledge.ni.com/KnowledgeArticleDetails?id=kA00Z000000P8aNSAS&amp;l=en-US) for intsalling LabVIEW software components.
hmm, that's really odd. when you search your computer for "NI", does NI-VISA Interactive Control pop up as a program installed? hopefully downloading and reinstalling [NI-VISA 18.5](https://www.ni.com/en-us/support/downloads/drivers/download.ni-visa.html) will help. 
https://knowledge.ni.com/KnowledgeArticleDetails?id=kA00Z0000019SCgSAM&amp;l=en-US https://knowledge.ni.com/KnowledgeArticleDetails?id=kA00Z0000019QOfSAM&amp;l=en-US https://knowledge.ni.com/KnowledgeArticleDetails?id=kA00Z000000PATTSA4&amp;l=en-US
I agree with previous comments on checking your 488.2 and VISA driver. Try the latest (18.5) version for both, assuming your on a version of LabVIEW thats compatible. Also, just to check, I what happens when you click "scan for instruments"? 
Thanks for the suggestions - it was an issue with the VISA and 488.2 projects (accidentally installed wrong)
this was (one of) the issues - I think I have it working now though&gt; thanks!
What kind of accuracy are you after with your time intervals? ms? second?
probably second, I was thinking for waiting a new measurement every few minutes ish and the timer intervals don't have to be 100% exact to the second but I want the VIs activated every few min at least (if this makes sense). I was then planning on outputting the runtime of experiment into excel file as well.
In that case I'd wrap the measurement vi in an event case inside a while loop. Wire the delay you want to the timeout value (i.e your desired wait time in ms) and make sure that the measurement subvi is in the timeout case. This way is less cpu intensive than a while loop and you can easily instantly interrupt it with a user action (which you can't do with the wait(ms) function). This method doesn't take into account how long the subvi takes to run, in case that's an issue for you. 
While I'm not very experienced with Labview I ran into this problem a few weeks back. My solution was to use the array functions to create a rolling array of fixed size (insert and delete etc) . In my case 2 mins worth of data was continually over written . Every few hours I sent this data into a spreadsheet using time and date to name the spreadsheet (csv). Maybe an idea for those of us less experienced
lol thanks, I actually used labview a couple yrs back as well briefly and I got relatively not shitty at it and did something similar as well. My only thing is I wouldn't data to be continuously being taken if it doesn't need to be (ex. I just need to measure resistance every 2 minutes not continuously and added into a spreadsheet every two min). I think the array ish can def be helpful tho
Did you put the servo demo VI under the MyRIO device in the project explorer hierarchy?
Call the Search 1D array function multiple times, once for each element chosen by the user.
I see, so I need to work with the basic function, thought maybe there is another one, isn't there a way to link the search function result as a boolean value, and from that I can check that ?
With the search function, you are given the index of the first match, if not found, index is -1, so you can easily use a greater or equal to 0 to get a Boolean of found.
exactly, thanks a lot!
There’s an openg function that does this.
From LabVIEW 2018 there is a Python integration node that does exactly what you are looking for
What are you using to interface with labview? An NI DAQ with digital outputs, an arduino over serial, what? I suggest starting by showing the pinouts on your boards and how you have them connected.
Here you can find the program I am using: [https://forums.ni.com/t5/LabVIEW-Interface-for-Arduino/Controlling-2-Stepper-Motors-CNC-Shield-with-Labview/m-p/3913407](https://forums.ni.com/t5/LabVIEW-Interface-for-Arduino/Controlling-2-Stepper-Motors-CNC-Shield-with-Labview/m-p/3913407)
*OwO, what's this? * It's your **1st Cakeday** tregdor3! ^(hug)
First off, because you uploaded just the VI and not a snippet I can’t see it from mobile. It’s better to do both. Second, that’s just the software. I was asking about hardware.
[https://imgur.com/3u0zzaW](https://imgur.com/3u0zzaW) [https://imgur.com/XUmHngJ](https://imgur.com/XUmHngJ) [https://imgur.com/9ni8qBa](https://imgur.com/9ni8qBa) This should include everything you asked for
Don't have an fpga installed atm so can't help directly, but what are the options when you click on the property node?
No, because FPGA code is ultimately compiled to firmware, and your FPGA hardware actually *becomes* your code. You would have to recompile the code every time you added an input. You could achieve something like what you are after by setting up your FPGA to read all inputs, and then filtering the data in your RT or Windows code.
Yeah that was what I was afraid would happen. So I would need to have all the possible channel already in the node, and after detect if I'm receiving data from each channel, to filter does who do not have data?
As u/Aviator07 points out, the FPGA VI is compiled and loaded into the FPGA so you can not do a dynamic change like you are wanting. However, many (but not all) modules can be accessed via SCAN mode so if you have lower speed signals (digital or analog) on some modules that do not require access in the FPGA code you can configure them for scan mode access instead of FPGA mode access and do dynamic access to channels that way. In general scan engine will be slower and not deterministic (meaning you can lose data) so it is not appropriate for waveform type measurements. Caveats; any module may be in scan mode or FPGA mode, it cannot be in both. Having some modules in SCAN mode while at least one is in FPGA mode is known as hybrid mode. When you compile the FPGA code it will take more time and more space than you might expect since it will be automatically compiling the SCAN Engine interface into the FPGA code. For a module to be considered in SCAN Engine mode in the Project, it will be located under the RT Target. If you want to access it in the FPGA you place the module under the FPGA folder instead.
Sadly, waveform is exactly what I'm measuring. But this scan mode sounds interesting, I'll remember that for future project. Thank you very much!
Just have your FPGA code read everything, and reject the stuff you don't care about in your RT code.
https://knowledge.ni.com/KnowledgeArticleDetails?id=kA00Z0000019PUNSA2&amp;l=en-US May be worth looking into. Life is a lot easier when you don’t have to write FPGA code
Labview is just the software, and much of your question depends on your hardware. The hardware probably has an example for the channel you connected to the led. On any VI, the menu will have a Help&gt;Find LabVIEW Examples... Or something like that. Then search for your device. If you can't find it, what is your hardware?
I'm using the myDAQ hardware, but I really want to know how to properly make a sound go off when a condition is met, cuz right now it's just a shot in the dark. I just need a beep.
I'm using myDAQ software. My biggest question is how to get a beep working when a condition is met on labview, the led also needs to be turned on using labview (which I know how to do).
can you share the VI ?
https://knowledge.ni.com/KnowledgeArticleDetails?id=kA00Z0000019PdFSAU&amp;l=en-US https://knowledge.ni.com/KnowledgeArticleDetails?id=kA00Z0000004A6MSAU&amp;l=en-US It would help to know the numeric error code you're getting.
hello [imsoupercereal](https://www.reddit.com/user/imsoupercereal), thank you for your time and effort but i am not getting any numerical errors i am trying to reset the myRIO to default setting by downloading the recovery file on the chip by using a flash drive like the steps in the link that i have mentioned before.
Sounds like you need a comparitor that evaluates if the point is higher then the threshold. The output of a comparitor is a Boolean (on or off)
Call NI's support
It depends on how you learn, but if it were me I would just start playing with it and go check the forums on the NI website when you encounter something you don't know how to do. The forums are awesome. There are plenty of free resources on the NI website, but personally I like learning by doing. Maybe do a combination of both! Skim through some of the documentation to see what's possible and then play with it. If you get to the point where you really like it and want to get pretty competent, I think the CLD is a great certification and I certainly got better at LabVIEW through my pursuit of it. CLAD is the bottom tier and not particularly meaningful IMO. Good luck! Have fun!
Even just solving the example CLD exams is a good way to learn LabVIEW.
What area do you live in? Getting involved in your local user group can be a huge springboard forward. Also, get onto lavag.org and don't be shy about asking questions and posting your thoughts on other questions. Also, please consider watching this presentation regarding coding principals https://youtu.be/vXAwcwJVM94
I find the best way to learn how to use something is to use it to solve problems. Find a job at work that you think you can solve with labview and convince your manager to give you time to do it. Also the NI forums are an invaluable resource where basically every problem you will encounter someone else already has and solved.
Find a project! LabVIEW's WAY too much to dive into without a specific goal to guide your efforts. I'd recommend (as I always do in these threads) picking up a copy of LabVIEW for Everyone ( [https://www.amazon.com/LabVIEW-Everyone-Graphical-Programming-Made/dp/0131856723](https://www.amazon.com/LabVIEW-Everyone-Graphical-Programming-Made/dp/0131856723) ) for a readily accessible reference. Do yourself a favor and avoid posting on the NI forums (they can be a bit on the snarky side), but definitely use the site as a reference.
Take Core I and Core II
Doing programming/lab exercises you've done with other tools in LabVIEW is good. It doesn't really matter if you use NXG or LabVIEW 201x, both have the language features you'd need as a beginner, but you'll likely get better support using 201x.
thank you i will
*bump not hump!
Depends on what sort of hardware you have. Simplest is to connect the switch to a digital or analog input to a general purpose board (USB-6001 is very inexpensive) and read the inputs once every millisecond in a loop. If you have no hardware yet, then I'd suggest an Arduino. A little extra step to program the arduino to read the switches and send the result to the computer on the USB. But an Arduino will run to about $30 tops.
Is there a way to just set it up with direct connection to myDAQ?
Absolutely. Any of the digital or analog inputs would work. Check to see what is the state of an input without the switch connected. If that is high, then connect the switch to ground and look for a transition to 0. If the resting state is off (0 volts), then connect the switch to +5 and check for a transition high. Pretty simple in principle, and all you need to do is check out a LV example where you sample the inputs.
Labview is the software; you will need some hardware device to connect the switches up to. Do you have something? &amp;#x200B; NI, the company that makes labview, has some very nice hardware that can do this. But it will be on the expensive side. Other companies also make DAQ (Data AcQuisition hardware), but that hardware tends to take longer to set up, and many times doesn't have all the features of the NI stuff. The two Big Questions I think you need to answer about hardware are: * How long are the distances to your sensors? This is a track, I expect you are talking on the order 100s or 1000s of feet. * How accurate do you want to be? &amp;#x200B; It's a little out of my depth to tell you exactly what you would need to interface a switch that far away. Most of the lower-cost DAQ hardware uses "5V TTL" signals, with the computer providing the power for them. I can't say for sure, but if you had several of these on 500-1000 foot wires, I think you would get a bad signal. In this case, you probably want to use 24VDC signals with an external power supply; and either different hardware that can directly interface those signals, or some interface that turns the 24VDC into 5V TTL for the hardware. &amp;#x200B; As for accuracy, there are two ways to collect this data. The first is a Software method, where you write an application that constantly loops and checks the sensors, reading the data into the main computer memory and then processing it (that is, looking for when the switches are hit). The problem here is that your car is moving very fast, right? If the tire is only in contact with 8 inches of ground at a time, a back of my hand calculation says that the switch will only be pressed for around 1/150 of a second. To tell you the truth, it's unreliable for the software to loop that fast and get you accurate results. Most likely you would a) miss some hits and b) what timing data you get could be off by a lot... like tenths of seconds. And if you buy the least expensive hardware, that's all you'll be able to do. &amp;#x200B; So instead, you get hardware that can do "buffered digital input". In this case, the DAQ system reads the inputs incredibly fast, and dumps the data back to the software "in buckets". The data is exactly spaced/timed, and as long as you read faster than the shortest possibly press, you will always see the triggers. &amp;#x200B; Here is the least expensive setup I can find for you on NI's site: &amp;#x200B; Here is an example of the 24V hardware you could use from NI: * NI-9421 (8 channel, 24V digital input module) \~$115 * CDAQ-9171 (USB single module chassis) \~$350 You would also need a power supply for this, but it probably doesn't have to be a big one. &amp;#x200B; Another possibility: Have you considered using photo-switches? You would set them up at points on the track, possibly with reflector targets on the other side of the track. As the car passes, it will interrupt the reflection and you can read that signal. Probably both easier to "hit" (you won't have to aim for the bump switches) and also probably less likely to be moved/damaged. The photo switches could work with exactly the same hardware I mentioned above. &amp;#x200B; Good luck! &amp;#x200B; NOTE: IF you are talking 1000s of feet, event he 24V stuff might not work. At that point, you should probably engage the sales guys at NI or find a forum for industrial I/O (/r/PLC might be a decent place to start) and just ask them if someone can help solve that long-distance issue.
8 inches is 20.32 cm
Wire the array of user defined integers into a FOR loop with the Search 1D array inside the loop. Take the element number found, use that to index the array of integers and then output that to the right tunnel of the for loop. Use the conditional terminal with an equal function to check for -1. Invert the output of the equal so that when a -1 is found, that data is filtered and then the output of your array is the integers that were found.
There is no way we can help without knowing which DAQ you are using, although I doubt many people on here will want to make a wiring diagram for you, even if enough information was provided. &amp;#x200B; As for LabVIEW side, examples are your friend. There is an example for virtually anything you may need from DAQ to doing FFTs. I'd say try to understand what you truly need then do some googling or looking through the shipping examples that come with LabVIEW
This may get you started: [https://imgur.com/a/hYCOiWC](https://imgur.com/a/hYCOiWC) Note: This is running at 1KHz. It will probably work reliably on a modern windows machine up to around 5KHz, maybe 10KHz. But if you are dealing with sound (44KHz) I suspect you will occasionally have intermittent, additive delays in the signal. Any UI actions or screensaver or windows Voodoo will cause blips. So I don't recommend this for a non-fault tolerant application. Also, this has a delay of around 200ms between input and output. You could lower it, but there will be some. If you need it to have no faults and little to no delay, you will have to look at LabVIEW RT or cRIO system hardware.
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/bZJH8hx.png** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme)
You can convert the signal to frequency domain with this: http://zone.ni.com/reference/en-XX/help/371361R-01/lvwave/fft_power_spec_psd/
What part of the project do you need help with? Break up the project into smaller pieces and work on each piece one at a time. I like UI stuff so I'd probably start with the wheel. You could do this several ways. You could have an image for each position of the wheel in a folder and then cycle through loading them one at a time into a picture box. Or you could load them all into a picture ring and select them one at a time by writing to it as a decimal. Or you could draw the wheel from scratch using the image and graphics functions for drawing all the components. Or you could load one image and then find code to rotate it. You'll need code to convert from an angle or image number, into the selected number on the wheel. Then it is a random number generator on how far to spin it from the current location. Is there betting? Is there odds? If so the angle, and corresponding numbers need to be kept track of to see what comes up and who won.
Normally listboxes are just for selecting the row. So you might have a listbox that lists the channels and when they click each row, you can populate a nearby numeric control where they can input the settings for that channel.
http://sine.ni.com/nips/cds/view/p/lang/en/nid/213896 Check out the datagrid, it's .net which some people hate, and it's a little complex initially but I think it's worth it for some cases
I have this application frequently when I have a combination of program-populated data (device names, physical channels, etc) and user entered data (channel names, calibration factors, etc.). For a list box, I usually allow only a row selection and popup a data entry subVI when the user double-clicks using an event structure. I have used a table control before so that the user can type the data in directly and just allowed the program to write over any cells that aren't supposed to be user-editable but that is a bit less elegant IMO.
This sub-reddit is not for cheating.
Hahaha. Classic. You have obviously not looked through any of the posts in this sub, nor viewed any public forums online, ever. Let me enlighten you a little bit. In most cases, public programming forums are generally against doing people's homework, even for money. Normally the problems they assign for homework, or tests, can usually be solved with some simple Googling and little messing around. That is mostly how we all do our work anyway so the last thing we want to do as professional software engineers is allow someone to skate by into the industry who is incapable of even the most rudimentary form of research. That is true for 'most' languages and 'most' forums. LabVIEW, since it is a such a small world, tends to be a little more not accepting of the practice. This forum, even less so. My guess is this post will probably go nowhere, at best. But considering you made an account specifically for this post (probably because classmates or perhaps even your teacher might find this post on your main) my guess is it'll just turn into you getting insults thrown your way. In the end you would have been better off just posting a job on a freelance job website and letting someone do it for $50 and hope for the best. Again, something you would have learned if you had Googled a little first. Good luck on your final, I hope for your sake, that you don't approach your future life or career with the enthusiasm you've approached your schooling. You're not Lori Laughlin's kid, are you?
This response is great and why I left this open in a tab. Exactly the kind of truth telling I'd hope to get to read.
I too hunger for this knowledge
On the sidebar, next to your username: https://imgur.com/Grs3gU9
Thanks! That was too easy
Grand! Thanks very much!
https://www.dmcinfo.com/latest-thinking/blog/id/9339/dmc-labview-ui-suite The easiest thing you can do to make your UIs look modern. https://www.mooregoodideas.com/products/panel-manager/index.html A framework to help compose modular, complex UIs
The dcm UI suite is pretty good. But this subsequently taught me that there are loads of UI suites that can just be installed from VI package manager. Now I have loads to choose from. Thanks!!!!
Dmc is the most complete. It sucks to commit to a UI toolkit, then realize half way through a project that it’s missing a gauge. Dmc has everything that you need, and they’re flat style, and colorable. You should be able to make a really nice UI with waaaayyy less effort than any other toolkit.
Take a look at [this one](https://rafasolutions.com/products/labview-experience/labview-ui-kits/10/classic-flat-ui-controls-kit), it looks pretty cool on dark background. Actually, I'm happy with the UI suits I've found here: [https://rafasolutions.com/products/labview-experience/labview-ui-kits/](https://rafasolutions.com/products/labview-experience/labview-ui-kits/)
The only thing I don't like about the custom UI suites, are none of them use vectorized graphics so you get really bad resizing behaviour. To be fair to them, LabVIEW barely supports vectorized graphics. There is no support for SVGs at all. You have to convert them to some bizarre windows format (can't even remember the file extension) and then bring them in. I typically just go with a flat look that doesn't require custom graphics in order to support resizing.
They changed the format in 2018 I believe. Made it like 100x harder than the old example tests. You can find the new example tests on NIs website.
Just so you know as well, I believe they removed the requirement to take the CLAD before the CLD. So you may honestly be better off skipping it and going straight for the CLD. It’s a better indication of knowledge than the CLAD anyways!
Yes, all FPGA code should go into FPGA Main. If you put it into RT Main, it will run on the RT processor.
Sigh, what a terrible luck I've got.. I went on the NI website and was only able to find the latest prep guide: https://download.ni.com/evaluation/certification/clad/CLAD_Exam_Preparation_Guide_using_LabVIEW_2017.pdf The guide has some questions however I was not able to find sample test from 2019. Do you happen to know where they are on the NI website? Thank you a lot for help!
Thanks for the feedback. Since I've already spent time preparing for the CLAD, I really wanna get it over with. However, I definitely want to prep for CLD right after. Have they changed the prep material recently for the CLD as well? If you happen to have the latest prep material or latest sample papers for the CLD, I'd really appreciate it if you share it with me. Thank you!
As far as I know there are no other prep guides. I've been putting off taking the test since I definitely struggled finishing that prep in the time allowed despite programming in labview for a few years.
I honestly have no idea! I’m about to start preparing to take it myself, I just thought I would let you know that the CLAD is no longer required in case you didn’t want to spend the money or time on it. I think the most up to date resources for the CLD exam are at this address: http://sine.ni.com/nips/cds/view/p/lang/en/nid/10647
You can tell where LV is "compiling" the code for by where the VI is in your project. That is, check if the VI is under the host, the RT, or the FPGA.
Post a picture of your block diagram.
To be honest, I can't figure out what you are asking. So here is an example of how Build Array works with and without Concatenate Inputs turned on or off. &amp;#x200B; Cheers
 [https://prnt.sc/nl9xdm](https://prnt.sc/nl9xdm)
You should wire both the row and column index inputs on the index array function rather than using two index arrays. Can you give us an example of what you're trying to do maybe with constants and what you want out? Just a random guess but maybe you can use autoindexing to get what you want out? [https://zone.ni.com/reference/en-XX/help/371361N-01/lvhowto/condacc\_valuesnloops/](https://zone.ni.com/reference/en-XX/help/371361N-01/lvhowto/condacc_valuesnloops/)
You shouldn't have any problems using the newer card. You'll have to go into the code and re-map the channels from their assignments on the old 6052E to the new ones on the 6321. How much work that's going to be is impossible to tell without seeing the source code. If the channel assignments were done in NI MAX, you'll have to duplicate them in MAX on the new PC. If they were all done in the code, your job is a bit easier as you'll just have to update things there after you install the new board.
NI has really good simulation capabilities. You should be able to create a simulated 6321 and test out most of your code before you buy. http://www.ni.com/tutorial/3698/en/
There are two problems you are likely to encounter; first the version of the LabVIEW drivers you are using may not include the newer card. If this is the case you will need to upgrade the version of your LabVIEW drivers. However each version of LabVIEW drivers is only compatible with certain versions of LabVIEW, see this [compatibility chart](http://www.ni.com/product-documentation/53326/en/). The other problem you may encounter is that the DAQ read functions used are the old legacy DAQ functions, if this is the case they may need to be changed to DAQmx equivalent function calls to be compatible with the newer card. It's not hard to do but requires some familiarity with LabVIEW programming.
Also, you can call NI Application Engineering about this and they can answer all of these questions on the phone (as well as follow ups). This is one of the types of questions they’re best with helping at. 888.280.7645
Just FYI, the Support number is 866.275.6964 (866.ASK.MYNI). And, after many years as "Applications Engineering," the Support team recently rebranded to "Technical Support Engineering." But you are correct, the Support team is great at compatibility questions.
Although more expensive I'd would replace the hardware for its CompaqDaq equivalent, it would be better as a long term solution, and you\`ll forget about looking for an specific PC with PCi or PCIe boards, you'll have a "portable" solution valid for any desktop or laptop. But you as you've been told, someone has to "re-link" the signals to the new hardware in the new LabVIEW version, not a really difficult task if you're used to.
Thank you very much for your comment!
I did call the service and they told me that it was likely to work... thanks for your comment
Thank you very much for your comment! I think the main problem I do have is the DAQ conversion... I am not that familiar to change the labVIEW code...
Yeah... this is a good option and I am chechink your link, thanks man!
Thanks, your comment is encouraging... :)
cDAQ www.ni.com
Are you sure you need the device to have wireless capabilities? Or just the ability to be hooked up to a common network?
The tablet will be very mobile and the place it will be used doesn't have an existing network and would be pretty difficult to establish one. I see where your head's at though, I appreciate that! The DAQ needs to be able to transmit data over wireless to the tablet that will be nearby. A USB cable or Ethernet cable is kind of out of the question.
I'm sure your sales rep would be very happy to talk to you. This describes the system: [http://www.ni.com/product-documentation/7375/en/](http://www.ni.com/product-documentation/7375/en/) &amp;#x200B; Analog input module: [http://www.ni.com/en-us/support/model.ni-9205.html](http://www.ni.com/en-us/support/model.ni-9205.html) Current input module: [http://www.ni.com/en-us/support/model.ni-9203.html](http://www.ni.com/en-us/support/model.ni-9203.html)
So I agree with everyone before in saying this is a great problem to call support. That being said two things stand out: 1. This is a kick the can down the road. Ultimately, support for this version, this device, these drivers... will run out and someone will have to deal with the exact same issue. I'd tell support that and they can work with you to find a new board and even help recreate functionality depending on code complexity (you'd be shocked at how good LV DAQ examples can be) 2. I'd also consider making the code into an installer or executable then you wouldn't even need the code installed and it could work on several computers. Long term I know but it'll save everyone a headache in the long run.
The CIN doesn't actually contain any particular executable code - it just points to the file. So if you don't have the file that it is pointing to, I'm afraid you're out of luck.
I understand this. I believe the file with the code is somewhere on the old computer, as I can run the VI that includes the CIN on that computer. However, the computer (and the VI) belonged to the previous researcher, and there are a ton of files and very little organization. As such, I can't tell what file the CIN actually points to. Is there a way in Labview to see what code file is loaded in a CIN? Checking the properties of the CIN doesn't tell me, and choosing "reload code" just opens a file prompt to my documents.
 [https://knowledge.ni.com/KnowledgeArticleDetails?id=kA00Z0000019Ls1SAE&amp;l=en-US](https://knowledge.ni.com/KnowledgeArticleDetails?id=kA00Z0000019Ls1SAE&amp;l=en-US)
Double-click and it should open.
I've been twice, a while ago. The ribbons are abundant, one for everything. While I don't know what the All-star ribbon is, in my opinion they don't mean too much. The event is very sale-oriented so there's a lot of extra pat's on the back to promote happy customers.
Congrats on whatever it means. I'm going for the first time this year, I'm pumped.
I went a couple of years ago and was content with my ribbon until i saw a Knight of NI. His ribbon combo would make a North Korean general jealous.
Ribbons are passe' now, the real hackers have zero ribbons.
About half way through this video you can see me with some ribbons a couple years ago. https://www.facebook.com/NationalInstruments/videos/10154653540549021/?app=fbl It is partially a status symbol and NI knows it. Never heard of an allstar, but I wasn't at NI Week last year maybe it is new. They used to have ribbons for forum activity, certifications, presenters, VLA, MGI had some, Bloomy, LAVA has some and hands them out to members and people at the BBQ (which you should totally go to) Lavag.org/bbq
As far as I know CLD hasn't changed in a while. I took it in December so as long as they haven't changed it since then you should be good.
If we're real lucky we might even get LabVIEW limerick or two by one of NIs senior LabVIEW nerds
Follow these steps: https://zone.ni.com/reference/en-XX/help/371361R-01/lvhowto/reading_using_expressvi/
Thank you vey much for your help sir, I have a graph of combined signals right now, how can I seperate them to different graphs? And how can I get the mean, mac and min values? Thanks in advance?
Thank you vey much for your help sir, I have a graph of combined signals right now, how can I seperate them to different graphs? And how can I get the mean, mac and min values? Thanks in advance?
If you are looking for a function that you know the name of, use the search function: http://www.ni.com/tutorial/7423/en/ The express VI gives a 'dynamic data'. Use the 'convert from dynamic data' to do data manipulation. There is also a mean.vi and min Max of array values that you can use from LabVIEWs statistics VIs. Also, if you haven't already, consider going through the LabVIEW in 3 hours crash course. http://www.ni.com/tutorial/5247/en/
Thank you so much!! And my last question is: Build a VI that captures the sound and plots it with respect to time. Moreover, the VI should plot the spectrum of the sound data. You should use the spectral block that performs FFT analysis for the sound spectrum. Using the developed VI, plot the sound data with respect to time and the sound spectrum with respect to frequency for a 1000Hz sinus signal and for a (mixed) signal composed of a 1000Hz sinus and 1800Hz sinus. Moreover use the Filter Function to remove noises with lower frequencies than How can I do that? Thanks for your helping.
These sound like homework assignments.... Like, directly copied from an assignment.
Where &amp; what time?
The link says Tuesday 7:30-10 at Uncle Billy's Brewery and Smokehouse 1530 Barton Springs Rd, Austin, TX 78704. And to be clear this event and not included with the price of the conference.
It hasn’t, my certification ran out and i had to redo it beginning of this year. I had a different excercise but it was one they already had before (Microwave) As long as you’re able to do the example Tests from their website and understand them you should be good for CLD.
Google shows 3 results and one is this thread. So going to go ahead and say nothing exists. I just moved into a new home with an IQPanel and all z-wave devices so you've piqued my interest. I'm going to look into the Open Z-Wave api tonight and see how hard it would be to write a LabVIEW wrapper. I'll let you know. Sorry I didn't see this post sooner.
Took a look at the API. I started writing a wrapper. Got hung up trying to figure out how to pass an Int32 pointer to some functions. It's not too bad overall but not something I can get done quickly. I'll work on it every now and then, but I wouldn't expect a wrapper out of me for quite awhile.
We use NXG now?
http://www.ni.com/en-gb/shop/labview/compare-labview-nxg-and-labview.html Should be there. But might be in the"nxg web module" reading between the lines the http api is supported natively...
Yeah that bothered me too, and it's weird I get the HTML nodes with version 2.1 but not 3.1.
No. No we do not.
Nothing opens when I double-click. Does this mean the file isn't on the computer? The VI is able to run and work normally, so I assumed that meant the file was on the computer somewhere.
I don’t know the issue itself now, but normally you don’t need to have any service membership to create a service request. for getting an older version for download i’d suggest just trying to contact them
Thank you for your reply! This is what i get when trying to open up a service request: [image](https://i.imgur.com/EbVq4i7.png). Using the links on the bottom only cycles me back asking for my id, or gives me the option to manually download drivers (17.6 and up). I guess i could send them a general email, outside of the support channel. Thank you for the suggestion.
Cool. Glad somebody read this post :) I was also looking at Silicon Labs as they look to have some options as well. Take a peek there if you get a chance.
Support services at NI do cost money, but sometimes there is a service account for a university. Check with your professor or whoever gave you the device.
I did install si labs PC Controller but haven't made it much further. This looks to have promise with their libraries.
In the project, have you found the myrio device? What happens when you right-click on it and select 'connect'?
Also yes, you can use Labview 2018. You will have to open NI MAX (measurement and automation explorer) and install labview 2018 on the myrio first. This may also require a firmware upgrade, which is also achieved through MAX.
That code is telling the MyRIO to try and read + process the measurements 1 million times a second, it can't possibly hope to do that. Since it's a timed loop it's run at a high priority, higher then the network communication thread, so running this code could cause the target to go unresponsive. &amp;#x200B; Rebooting the MyRIO should allow you to communicate again. &amp;#x200B; Change the timed loop to a while loop, and it should work better.
You can set up a simulated DAQmx device in MAX and acquire data from it using the same functions you would normally use.
How?
http://www.ni.com/tutorial/3698/en/
Thanks
Open LabVIEW and create those pieces of code, you have plenty of time to work this out in 3 days.
thats the problem i dont have pc or laptop on me
I'm familiar enough with Z-Wave to know that this will be extremely hard to implement z-wave in Labview. And I doubt anyone has done it. The problem is that the USB z-wave devices are pretty low-level, they expect you to implement all of the encryption and other things, and pass around very low-level details to the RF stack, and oh by the way, you have to come up with the database of features / protocols for each end every device because that's not anything you can get from the USB stick. What I would do is communicate back and forth with python, to python-openzwave: https://github.com/OpenZWave/python-openzwave .. It has all of those things I listed above implemented and working with any off-the-shelf USB z-wave stick. I recommend the Aeotec Z-Stick USB. Because it's Linux only you probably want to run it on a Raspberry Pi and connect Labview to it via wifi or Ethernet, communicating to it using a socket, or ideally, something higher-level like HTTP. Use something like HomeAssistant or IFTT to make it reachable via a REST API client. Then, you can use some custom REST API client endpoints, written in Labview, that access the server side endpoints. This keeps the Labview code to an absolute minimum. While this would work, I wouldn't say any of this is trivial. It's probably going to be an extremely frustrating project. If there does happen to be a .NET framework out there, that would be much easier, but I'm not sure if such a thing exists or not. I've only got experience with python-openzwave. It works extremely well.
We're not going to do your final for you. Find a computer somewhere. Ask the instructor, they should be able to set you up on one.
Sure, no problem I will help you with this. A, B, A, C, C, D, A, D, B, B, A, C, D, B, A, C, D, A, A, C
do this using visa talking to which ever com port your pc puts the usb device
I have NI-VISA 18 installed. Would you elaborate more on how to “use” visa?
A copy of the Core 1 course manual would be very useful. You may find an old version online for free.
In LabVIEW go to Help &gt;&gt; Find Examples then search for simple serial or VISA to see some basics.
It looks like there is an instrument driver for it: http://sine.ni.com/apps/utf8/niid_web_display.model_page?p_model_id=2780 Will be easier to use that raw VISA commands.
Thanks !!! Like really big thanks for helping
I dont have any access to computer , that's why I'm posting it here , I'm not trying to make others do my work because I'm lazy or something , i just can't do it at the moment
Your best bet is Labview Core 1 and 2 at least to give you a feel for the environment. I would also suggest the object oriented module because LVOOP is best practices these days. Beyond that, if you have good programming skills already, you'll find LV is just a different language, albeit very different in some ways (data flow). If you have an active SSP with NI, I believe you can access the training materials online, though I could be wrong. I feel like they change that stuff often.
You're gonna have to post your subvis if you want help. Yes it sounds like labview is messing with your Bluetooth. Is the Bluetooth being reset upon initialization?
Here is some free training information online. https://labviewwiki.org/wiki/Training But looking around that wiki in general is a good way to learn new things.
YouTube, also for everything else
https://www.youtube.com/playlist?list=PLKPIOR44GFIvSvujzXafTRC3fvS4vjOqI
Good lord that sounds like my username. I do have an aeotec z stick. And am currently using home assistant on my rpi. But was hoping to do something completely homegrown so I bought another one. Interestingly when I installed that software I mentioned (pc controller from si labs) it immediately identified my zstick. On my windows pc. And it worked like a charm to control the devices I added. But that means its off to .net land if I chose this route. I do like the idea of python openzwave. Using labview to interface with python scripts could be pretty good as well. If I was smart I would probably just learn enough python and do it all there but I'm not smart. Thanks for the comments. Gave me some cool ideas. Appreciate it!
Turn the problem around. Take your problem timestamp and convert it to a row count. Parse out how many seconds from midnight the problem occurs from the timestamp. Divide that by 100 and that's how many rows into the sensor data file you need to read to find the data you're looking for. Read the text file by line. The number of lines you'd need to see the whole problem event is just the end time line count minus the start time line count. Make sense? Or am I not understanding the problem correctly?
I would take the problem timestamp and look for it in the sensor files. Forget about the seconds on the timestamp at first (or forever if it doesn't matter). Then, using a for loop, look for that timestamp in the sensor file and have it make note of the index of the array. Then do the same for the end time. Now you have the index (row essentially) for the start and stop times of the issue. Take the sensor csv files and extract the array subset based on the indexes you found earlier. Obviously there is some logic to write if the problem runs overnight and whatnot...
Typically, I would read the whole spreadsheet using read delimited spreadsheet. That will give a 2D array then you can manipulate that array however you need. I am unclear on what the calculation you are trying to do so I don’t have any advice for that.
To extract the csv data, you can use "Open/Create/Replace File" -&gt; "Read Text File" -&gt; "Close File". Wire the "text" output terminal from "Read Text File" into the "spreadsheet string" input terminal of "Spreadsheet String To Array". Set the following constants for the "Spreadsheet String To Array" function: - delimiter -&gt; A string constant of "," - format string -&gt; A string constant of "%d" - array type -&gt; A 2D array constant containing an I32 numeric constant The output from this function will be an integer array of your CSV data. I'm not entirely sure what your calculation entails. However, assuming you are applying the same operation to each row sequentially, you can wire the array into a For Loop, this will automatically index your array row by row and perform you can perform your calculations within the Loop.
how to get the 2d array i can only see the values with all rows indicator
All rows will give you a 2D array of everything. You can select the type of data in the selection box underneath the vi
To expand on this slightly, to get an individual row from your 2d array you can use *Index Array*, or you can wire it into a for loop and it will automatically index through one row at a time.
What are your questions?
How can I register all the info of multiple users like the money won + name on each “play”. How can I relate the input on interface and the register of the users on a data sheet for example on excel.
There are multiple ways to do this. You can run each user's winning/losses into a loop that keeps track of the amount in a shift register. You can export each user's transactions in a separate data file then compile the data from each file at the start and end of each play" I really depends on the requirements of the project. Is there a set number of users or a variable number of users? Will the users change throughout the game or is a just like "these people sit down and play the game until they are all done" type of thing? I'm not being difficult I just don't have any context on what features this software needs to have.
My immediate go to would be a CSV file - you can just read it from the spreadsheet and dump it back in as required, they're very easy to work with. Give it a go, and if you're struggling post the .vi for us to have a look at the specific issue.
in for loop which control we have to use
Quick and dirty animation can be done with a picture ring. Load all the pics into the picture ring. Write some logic in a for loop to increment the value of the picture ring and you're in business. You can play games with the speed of the "wheel" by varying the input to a wait node in the for loop.
Sequencing issues like this are almost impossible to diagnose without looking at the code. You probably want to do some type of event based producer consumer architecture with separate loops for sequencing and control. [http://www.ni.com/tutorial/3023/en/](http://www.ni.com/tutorial/3023/en/) There is an example here: [https://forums.ni.com/t5/LabVIEW/How-do-I-build-a-producer-consumer-VI-with-multiple-consumers/td-p/2417984?profile.language=en](https://forums.ni.com/t5/LabVIEW/How-do-I-build-a-producer-consumer-VI-with-multiple-consumers/td-p/2417984?profile.language=en) Try getting those working and I will answer any questions you have.
Thank you, I'll read through that when I get some time.
Really, the only suggestion that can be given is to study that practice material. If you use LabVIEW as often as you say, you should have all the base work laid to understand the important stuff and pass it. Are you struggling with specific parts of the exam each time? Which sorts of questions have you been missing?
Spend time studying the menus and being familiar with the different options. Familiarize yourself with different integer representations and their quirks. But I'd mostly focus on the practice exam. Most of the questions you can get are pretty similar. So if you understand all the questions on the practice exams then you're probably fine.
Yeah man. CLAD is dumb. It’s about knowing the material, not knowing Labview It’s like drivers ed exam(if you had one). It has very little basis on real life knowledge. It just tests your ability to study.
My LabVIEW professor took it for fun along with us and he failed. If you want to pass it do the sample papers , it will give you a general idea about the questions and way of preparation. All I did was solve 6 sample question papers and I cleared my exam.
You can go straight to the CLD now - and for my money you should. The CLAD tests some properly daft things, whereas the CLD is a very functional test. You get a lot of leeway to solve the problem the way you want. God knows the architecture I used wasn't standard, but it worked neatly. Half of the CLAD problems, if you came across them in reality it'd be the work of 30 seconds to throw open a blank VI and confirm how the functionality works, so there isn't *that* much value in knowing off the top of your head.
While the CLD exam is a practical (build this) exam, the CLD-R (recertification) is multiple choice and is very familiar to the CLAD.
Eugh. Hoping to push for my CLA before I need to do that...
I struggle with tracing exactly what the code is doing like when they ask is the output 5, 6, or 7. I get what the code is doing back if i miss one iteration by accident I get it wrong...
Then I suggest spending some time making sure you fully understand how loops and indexing work iteration by iteration. That's actually a very important part of any programming language and you should be able to spot an error based on loop iteration mistakes. I can't tell you how many times I've found a bug in my code that was caused by one too many iterations or one too few.
Man, try the CLA-R (or try not to take the CLA-R using the recert point system). Been using LabVIEW professionally for 6 years now and I just barely got the 70 I needed to pass.
You’ll be happy to know that also the CLA Recertification is multiple choice 😄
At least it kicks the can a few years down the road. I'll just have to do a few more talks at events, go the recertification by points route.
https://knowledge.ni.com/KnowledgeArticleDetails?id=kA00Z0000019KbYSAU&amp;l=en-US http://www.ni.com/pdf/manuals/372092c.pdf
oh yep, forgot that’s also possible. good idea
Thanks. I found the DAQmx functions after I posted. Tried a self cal, but failed. These must not have an internal reference. Instead of writing a program and finding a 6.5 digit meter, I just swapped out the module for now. It seems like there should be a prepackaged utility for calibration, but maybe they want these sent back to the factory or a 3rd party cal house.
Hi all, I could really use some help. I've attached a snippet in the post. I have 32 channels reading from DAQmx on my NI 9209 and writing to a file. It's just some analog voltage inputs. With 1 channel only, it is quite fast and exactly what I need (5-10 datapoints per second). With 32 channels however, it is extremely slow and I only get 1 datapoint every 5 seconds. I don't understand why this is so slow. Can anyone help me?
You use NI-VISA for serial communications, not NI-DAQmx. Start with a VISA Configure Serial Port, then you can use VISA Read or VISA Write, and when you are done, use VISA Close.
You can always check if there is a driver for the linear actuator.
Okay so I've gotten to the point where I have a visa serial configuration block and visa write, read, and close. I've got numeric constants set for the baud rate and the timeout on the configuration block but everytime I run the code, I am receiving a timeout error where it tells me the timeout expired before the operation completed. I have my timeout set to 10 sec and the program will only display data on the front panel after this timeout error occurs. Any ideas on how to get rid of this error? It is error 1073807339 for reference. Sorry about the wall of text!
Error -1073807339 is the generic serial timeout error. It means the device has not sent data to you. This may be due to a lot of reasons: * The command you sent was not recognized by the device (bad formatting) * The command you sent was not recognized by the device (bad addressing) * The command you sent was not recognized by the device (bad termination) * The port was configured with the wrong baud rate, parity, stop bits, start bits, etc... * The serial cable is not connected (not likely due to your success with putty) * The serial cable is mis-wired (DTE to DTE for example) (not likely due to your success with putty) * The device is not powered on (not likely due to your success with putty) &amp;#x200B; Since you are successful with Putty, I would install a serial port spy program on your computer and run a capture using Putty, then do the same with your LabVIEW program, looking for differences.
Okay this gives me a good place to start again tomorrow, thanks.
Use a property node and write the desired value maybe? Honestly, it isnt particularly clear to me what you are trying to do.
Property node can definatley beused to disable/enable the control. I think an invoke(?) node can be used to initialize to default values.
I think you want to disable an input when the other is used. The code below will do what I think you want. It is crude but shows you the basic principle. [https://i.imgur.com/gOPWtjh.png](https://i.imgur.com/gOPWtjh.png)
Trust the hardware. Figure 2 is what you want. The millisecond tick count is a windows thing. Execution is not deterministic. When you take data from a piece of hardware, your basically grabbing the next full group from the pool. You don't miss points (or at least you shouldn't if the collection is setup right), so you can use dt reliably to figure out your time at each point.
Yup, just use the timestamps that the DAQmx read is returning, they'll be good enough: [https://knowledge.ni.com/KnowledgeArticleDetails?id=kA00Z000000P9Q9SAK](https://knowledge.ni.com/KnowledgeArticleDetails?id=kA00Z000000P9Q9SAK)
Ok, thank you!
Thanks for the link!
Yes, this is how I have it set up but even when numeric 2 is grayed out and disabled it still performs the calculation. This would be fine but I need my VI to totally ignore numeric 2 when only 1 is active.
Put the calculations in the same case structure. Or same type of case structure. The Disabled property node is for the front panel control.
Thank you, that worked! My skills are pretty rusty.
no problem
I also want to point out that figure 1 doesn't necessarily take the time directly before sampling. Since the first tick count and the DAQmx read are in the same frame, they will happen in "parallel". Of course computers don't actually do things in parallel, so what this actually means is that there is no way to know when one happens in relation to the other. One iteration of your loop can even be different than the next. Tick count is used in places to measure elapsed time, but the first one should be in its own frame.
Thanks!
You are likely to receive a better response with a specific question and more detail. An example would be: "I have a project where I am required to implement a low-pass filter. I have already implemented the DAQ part of my project, but I am unsure about how to proceed. I have already tried XYZ, but it failed because reasons. Here is a link to my code" Nobody here will create the entire application for you.
I dabbled in it a couple of times, but every time I had a serious application it would always need some unsupported feature (vision, real-time, etc.) that pushed me right back to the ol' standby. I like the UI though so I use the NXG style controls heavily now. I'm interested to see how it grows in the community though.
Same story here. Also the muscle memory that I have with LabVIEW is completely useless when trying out NXG. A lot of frustration there. And it is slowww! Hopefully we'll end up getting the best out of 2 worlds...
I'm excited for LabVIEW 2019, but no reason to upgrade.
Yes this is possible I do something very similar just using an external injector driver. Look at the documentation for the camera it should tell you what pin(s) are trigger inputs. You have to configure the samara thru NI MAX or the camera's software interface to accept a trigger on the pin you choose. You may not even need the FPGA, while I do use them on some stands for triggering the camera and injector it's also possible to do with a high speed DIO module like a 9401. But since you have the cRIO that's one way to time the triggers.
I'm excited about what it could be, but it isn't there yet. I still use traditional LV and probably will be for the next few years.
Don’t ever let anyone dull your sparkle :)
Had it on my computer for 2 years now and have never opened it. I've 15 years invested in conventional LabVIEW. It makes no sense for me to change at the moment. So yeah' my excitement levels are zero. I kinda wish they'd just given regular LabVIEW vector graphics instead of a ground up redesign.
It’s still lacking some features that I need, so I haven’t started using it for real. But I’m definitely excited for its future. It should fix some of the problems that annoy me most about labview. For example, I hate how many separate windows I end up with when I’m developing. The tabs are a big improvement there. And I don’t like that merge errors will potentially make you lose errors. NXG treats the error wire more like an array of errors, so you can have more than one at a time. In general I think the user interface is a lot better.
same. RT is a requirement for the majority of our work.
I presented on "three reasons to not use NXG and three reasons to use it" at our local LabVIEW user group. https://forums.ni.com/t5/North-Oakland-County-LabVIEW/LabVIEW-2017-amp-NXG-Presentation-8-2-2017/gpm-p/3669665?profile.language=en WebVIs are a cool technology and being able to publish normal LabVIEW programs to websockets and have an NXG webVI read and display gives a non g developer like me lots of capabilities. I'm not a web developer but being able to publish to a web server with dynamic page sizes for different devices is relatively easy. As for full blown applications with DAQ, FPGA, Real-time, database, data logging, and instrumentation? Well it isn't feature parity and until it is closer, web stuff is all I'll be using it for.
the diagram zoom is a welcome improvement
Diagram zoom is awesome like another person said. I just need actor framework before I get too serious with it, they said it's supposed to come towards the end of this year so might do some more serious work then.
It is not ready yet, but the VIs are no longer binary shite, so source control might work as it should once in a while...
Really? Never had the need for that. State machine layout makes sure that the diagram is fullscreen, but never need to scroll.
On second thought : Sometimes when you drop or copy/paste something on the front panel, its code gets placed on the diagram in another zip-code.
Starting to use it for Web VI's but a lot of my projects are cRIO based so no reason to migrate from classic LabVIEW for a couple years yet beyond the use of WebVI's.
Yeah, basically the exe that ships with the hardware is owning the port, so any other application can't access it until the exe is done. I think you are going to have to build your own application to do the control and data logging you need.
As above, it is likely that you will have to write your own software from scratch. It is likely that the serial communication won't be too hard, and the necessary serial commands should be well documented in a datasheet. You may also be able to get hold of editable source code LabVIEW drivers from NI's LabVIEW Driver Network (IDNet), where you can search for your hardware. If you get mega stuck, feel free to PM me.
I've never tried this, but you may be able to turn the VI Server on for the EXE via its INI file and use VI Server to read the controls in the EXE for logging. Again, never tried this and I don't know if you can simply enable VI Server like that. It also may be difficult if you're not super experienced with LabVIEW.
Alright, that's basically what I expected.
The instruments I'm working with aren't unusual, so it does appear that there are Labview drivers available. I think I'll be able to cobble together something that will do the job. Thanks.
Interesting idea. I'll take a look at what it's possible to do through the ini file.
Also, just brain-storming... I wonder if it's possible to get a serial splitter. Open up a second port and sniff the serial traffic coming from the device for the values you need? Again, never tried. Just spit-balling here.
Oh snap, check this out: https://www.fabulatech.com/serial-port-splitter.html
Oh, very interesting--that might be an easy solution. I'll give the trial a shot, see how it works.
As someone else stated VI Server would be your best bet to take the executable given to you and somehow work with it (if it's even possible). If the company made an executable, they should be able to give you their LabVIEW code (at least the driver layer), most companies have been willing I've talked to, if it's not already on their site for free download.
Hello, Well what i understand is that you are using a c-RIO 9014 controller in your chassis with NI-9403 digital module for triggering of signal to NI\_9751 injector driver.... The issue you will face is NI-9403 digital module is that it transfers data serially and if you want to simultaneously give trigger to injector and camera it wont be possible. For further confirmation read the datasheet of NI-9403 digital module [NI 9403](http://www.ni.com/pdf/manuals/374069a_02.pdf). There is another solution and it is very easy that you use USB DAQ device NI-USB-6009 and you dont have to write anything in FPGA. It is plug and play using DAQ subvis. Regards,
Hi, Thank you for your answer. I am trying to explain our system better. So yes, we have the c-RIO 9014 as a controller in our chassis but NI-9403 DIO module is only for triggering of signal to camera. According to our Labview code, FPGA target of the controller would trigger to NI-9751 driver. We only need to trigger camera with NI-9403 hence we don't need to give simultaneously trigger to injector and camera with NI-9403. We only need to synchronise the processes of triggering with the FPGA to take photos of injection. Would it be possible to do that? I hope that it does make sense. Regards,
Okay many thanks, I will look at the documentation for the camera. Currently we have a push button which is connected to the camera's trigger input so should we trigger camera by sending out external trigger or receiving trigger signals with the camera? In your case how did you trigger the camera? Regards,
Ive worked with tons of people using Labview on servers like the poweredge with no issue. Labview scales pretty nicely to multi-processor machines.
Hey, I would ditch the event structure. Instead do a check on the FIFO size on the RT side and only actually pull data when it surpasses the threshold. Using case structures and while loops. It's been a while since I've dealt with FPGAs so you'll have to excuse my quite general explanation.
I gave this a try, and while I was able to split the port and have both the vi and the exe connect to it, I ended up with different errors. The two programs aren't synchronized in how they read and write and from the port, so they end up getting mangled data in return. I tried different ways of splitting/sharing the port, but it looks like it won't work. Oh well. I did get the vi from the cryostat company, so for now I'm just gonna mash that and my vi together.
Can you not just listen on the new port? Does the response from the device not appear on both ports? If you can see the serial stream on both ports, I wouldn't send any commands on your new port, I'd just parse packets that are relevant to you. However, if you need to acquire at a specific interval, that won't work.
&gt; However, when I hit "Highlight Execution" and run the code, the data takes a significant amount of time to go from the DAQ assistant to Write to Measurement File which isn't convenient for my application. Turn that off. Highlight Execution is for debugging. It's not representative of the actual computation time. It purposefully slows down the execution of your code so you can watch the data flow. You may want to look into the lower level file i/o VIs. The express VIs can only take you so far and limit how much control you have over file naming and when to write to files.
Everything said here is correct. Just want to add: if you go to Help &gt;&gt; Example Finder you will be able to find some good quality code to start off with when using lower level VIs
You should try buffering your data and saving in a seperate module from your DAQ acquisition loop. That way you wont throttle your DAQ with file write VIs.
Yes it is possible to synchronize the processes if a common trigger is used to drive both the devices. I hope that helps you in your project. Regards,
thank you that was helpful!
I think I understand the general idea of what you mean but I got no clue on how I can apply this in Labview
http://www.ni.com/tutorial/3023/en/ Check out the producer/consumer design pattern. Have the DAQ loop be your producer and the logging loop your consumer.
i use lv daily but not a pro. what i seen so far, the new nxg seems being pita.
I'm new to Labview, so in my data logging program, I was using a instrument driver I got from the labview driver network to read data from the instrument. In this subvi, I see that that it builds a sensor data query string and sends it to the instrument, then reads the response from the instrument. As such, when this ran, both my vi and the exe were sending commands, and both were recieving mangled data and throwing errors. Then I tried modifying the diver by simply disabling the part where it sends the command so that it just reads from the port, so only the exe ever sends commands. Now, when I ran both at once, the exe would work fine and read data correctly, but my vi wouldn't read the responses correctly and would occasionally throw up an overrun error "A character was not read from the hardware before the next character arrived". So, while it might be possible to just listen on this port, and decode what it receives, it's going to be tough for a beginner like me to write something that can parse what the device is sending.
you can selfstudy, but that takes longer. but definitely is possible with trial and error. 2 years later company sent us to the labview 1+2 course. hmm.. while i was familiar with everything, it changed my opinions about few things which i avoided like plague until the courses.
Sounds like exactly what I'm looking for! I will be trying applying this. Thanks
It's been a few years for me, but the fundamentals are far more important than a new feature when it comes to LabVIEW. Good luck!
Yes these will be enough to learn it for the certifications. The CLAD is a multiple choice exam testing you on an understanding of the programming environment. The CLD exam requires you to actually program and is largely based on the state machine process flow. There are project examples in LabVIEW that start you off with the state machine structures if you’d like to start there. I would recommend checking out some of the practice exams available online. Source: NI employee and have the CLAD and CLD certs
You don't need the software to get the CLAD. It's multiple choice so if you can read LabVIEW that is all. I suggest reading through "The Daily CLAD" that's on the NI Forums. It helps a lot. As for the CLD the normal home version should be fine. The tests don't require anything special as long as you perfect an architecture (producer consumer, event structure, et ...) You'll be fine.
Thank you, all, I will start with the Home bundle then and see where it takes me.
hey did you ever figure this out? im trying to send data from an arduino to the myrio via the i2c (different one)
I didn’t actually do so (our team abandoned using the MyRio with the IMU and switched to arduino). If you’re transmitting data from the arduino to the MyRio, you might wanna look into using UART, there’s some stuff videos and info about it online.
thanks, ill check it out
They really just want to see a fairly simple architecture. Don’t get fancy, just make sure it works. Functionality is. Large part of the grade. You are free to use any of the shipping templates on the exam, so do that. Don’t build the Producer Consumer framework from scratch - create it from the template. Take a little time to plan out your strategy, but then get going. Time is really the hardest part. Few people actually completely finish. Practice the sample tests. Those are decommissioned real CLD tests. Time yourself and treat it like a real exam. Don’t waste time on things that don’t matter - make sure it works! You’d be amazed at how many people submitted gorgeous front panels that did absolutely nothing. Source: used to grade CLD exams at NI.
Thanks. I'm going to time myself. Do you think that the Producer Consumer it's the best choice for most of the exams, or should i keep it simple with a State Machine?
If you are comfortable with a state machine and can solve the problem with it, do that. If I were solving these challenges for real world problems, I would probably use some variety of queued message handler which is essentially a statemachine/producer-consumer hybrid. But the point is, don't get over complicated for the test.
I did a queued message handler using an array of strings in a single loop. Nothing too fancy. Here is a good thread with other information and tips. https://forums.ni.com/t5/Certification/Certification-Nugget-CLD-Certified-LabVIEW-Developer/td-p/3161799
I understand that about the QMH. I have my own template actually. But i think it would cost me a lot of time implementing it from scratch, or even if I use the one provided by NI. That's why i want to keep it in a Event Driven State Machine, or in a Producer Consumer (just if i have to). I'm gonna try to keep it simple.
I used a weird one, I built a state machine in the timeout case of an event structure. It worked, and I passed with flying colours. Functionality is king!
I used a QMH. The template takes care of most of the overhead work for it, so I don’t think it takes much more time than a state machine. And I think it’s easier to incrementally add functionality, without risking adding a bug that will screw up your existing functionality. Practice making timers that you can pause and reset. A lot of the exams will require one of those. Focus your time on maximizing the points you’ll get, not on actually making a fully functional application.
You should probably post a picture of your code. Hard to say if you are missing anything without it.
Make it easier on yourself, get/use car models that have good contrast to your "road" color. If you're using a color camera, extract the green color plane. Mask out areas of the image that don't matter. Threshold the entire image. Count the blobs. Use blob analysis to determine each blobs location.
https://www.youtube.com/playlist?list=PLB968815D7BB78F9C I literally searched LabVIEW on YouTube. My tip, focus on data flow. "nodes don't execute until all data is available, nodes don't supply an output until it's done executing"
Thanks for the response pal. I saw this playlist but I was unsure if it would be enough by itself. Do you think this playlist is comprehensive enough for my purposes?
Might also wanna have a look at the DAXmx driver functions and queues. But yeah, data flow, arrays and structures are the most important.
Thanks for the quality advice pal.
Have you checked out the [CLAD Exam prep materials.](https://download.ni.com/evaluation/certification/clad/CLAD_Exam_Preparation_Guide_using_LabVIEW_2017.pdf)?
The LabVIEW wiki has lots of resources. If start with the online training section. https://labviewwiki.org/wiki/Training
I'm from Brazil, and I'm also studying to CLD. A good tip is: know how to build a FGV and how to use it to implement timers. Almost all exams require some time measurement and the first exercises of the CLD preparation can help with that. Do not forgot documentation and tip strips.
I'm calculating 137.5 kbps... I'd argue that's absolutely feasible on modern connections. What are you communicating with? You'll probably want to handle the parsing, storing, and plotting outside your network loop. But otherwise, I don't see a problem. I do have to chuckle a bit at going with LabVIEW to have a nice UI. I don't really consider LabVIEW UIs to be nice without intense customization. Does Python not have UI toolkits? I'd hate for you to have to spend time recreating your code in LabVIEW just for a UI.
Thanks for replying. I'm communicating with a custom hardware platform and my packets are always 44 bytes. When I said "nice" interface I should have said easy to program. In fact, I could use Python for the UI but I never used any UI design with these kinds of software so I guess LabView requires less learning time and could be useful in future projects. Do you have any getting started with TCP protocol? Besides the one that NI has on their website. Thanks again!
Haven't looked at the NI TCP whitepaper in a long time, but it's probably all you need. NI simplifies all their data communication VIs so that it's all just "Open", "Read/Write", "Close". So you really just have to say "Open TCP Connection", specify the IP, Port, and Timeout. Then use the Read VI in a loop with the byte count set to 44. When the loop ends, call the "Close TCP Connection" vi to kill the connection cleanly. A few other tips, error 56 is the timeout error code. Also, if you want your software to auto-recover from connection errors, you'll probably want to have a state machine in a loop with a connect, read, and shutdown state. If you see errors in your read state, go back to the connect state.
That sounds great. I'll give it a try. Thank you a lot.
Error 56 is the bane of my life!
Use this matrix to verify compatibility. [http://www.ni.com/product-documentation/53494/en/](http://www.ni.com/product-documentation/53494/en/)
Ohh ok, I'll look again, it seems that I have 8452 but labview 2014
This is a typical arm and trigger pattern. You can actually program most of this on the FPGA. You'll want to grab samples, do your threshold comparison, and in a case structure, grab a tick count from the FPGA clock, grab from your analog input in a loop and stuff the measurement into a FIFO until NowTime-StartTime = 1 second. You'll also want a couple status indicators on your FPGA (Waiting, acquiring), and a count of how many times you've acquired. From the RT, you can monitor the status and if you see it start acquiring, you can sleep for 1 second, ask the fifo for how many elements it has, and then grab them all. In theory, the number of elements it grabs should be exactly the same each time and if so, then you can just grab that many elements instead of checking the size first.
Also an amateur, but recently have been playing around with the SPI functionality of the 8451. I'd recommend taking a look at the canned VI's that come with the 845x driver, perhaps with an oscilloscope. Once you get an idea of whats going on with the example VI's you can manipulate them into what you need.
Okay, I Will try and take a look at it and see what I can get, thank you. Is there a why pull from one pin from the usb to the interface?
Checkout the examples in Help &gt;&gt; Find Examples. Basically every NI toolkit comes with some kind of help showing how the thing works.
Sounds like your measurements are a bit noisy. There is a good guide to dealing with that here: https://knowledge.ni.com/KnowledgeArticleDetails?id=kA00Z000000P9K1SAK&amp;l=en-GB
You need to pick the correct mode and wire for it correctly: http://www.ni.com/product-documentation/3344/en/
Yup. I'm pretty sure this article would solve it.
On your string constant, right click and choose '/ Codes Display'. There can be hidden characters that aren't displaying to you that are being sent.
You haven't given any time between VISA Write and VISA Read. What is your VISA component? Serial communication? If so, depending on the baud rate, allot around 100ms between read and write and retry your VI.
None. I have worked on multi-processor system without any issues. 2016 through 2019 version. All versions work perfectly.
Could *CLS? be the problem? Shouldn't it be *CLS without the question mark? Do you expect *CLS to return something? If so, try to split those commands into their own writes and do a read between them.
Semicolon is the standard delimiter for GPIB commands (through VISA). I see the '\\n' (newline) appearing to be the delimiter between commands - not sure if this is right unless being sent to a serial device. 'CLS' is a command to clear the status register of the instrument, you do not 'query' (?) this command. You would send this command to initialize on the first run (to clear errors from prior program execution) or to recover from detection of an error.
Welcome! I took a (very) brief scan through the manual. If I was doing this project, I would use the RS485 or the CANbus protocol on the drive, let LabVIEW set the necessary parameters on the drive for the particular motor I was testing, and send a series of test setpoints to the drive, reading back values for performance from the drive itself (speed, electrical loading, etc.). There are lots of examples for CANbus and VISA (serial) functions within LabVIEW to get you started. The drive is going to do all the heavy lifting as far as controlling the motor, you would just need LabVIEW to tell it what to do, read values back (including any additional sensors you might have that are not read by the drive), and write files. You should think about safety while you are setting this up. Avoid the temptation to throw all your setpoints into a single loop with wait functions and file IO or if you have to cancel the test or stop the motor suddenly your program will be unresponsive. A simple producer-consumer loop would be very appropriate here. Again, tons of examples within LabVIEW.
Let's say, I connected the hardware to my laptop with rs485( with a usb adapter), can I communicate with the drive directly by this connection without any other intermediary software other then labview?
you will most probably need a driver to recognize the usb to rs485 adapter, i only know the ones produced by prolific. you can easily find that one on google by searching for „prolific usb rs485 driver“. that has to be installed in the windows device manager on the com-port representing your adapter. after that it should be recognized, (you can test that out in NI Max) and visible from the NI Visa driver
I have a mac and prolific apparently has the drivers for rs485-usb also, does the adapter(actual hardware) have to be "Prolific" brand or can I buy any other one(random brand, probs chinese) and just install the Mac adapter from Prolifics website?
that was just an example, if you use another one i’m sure they‘ll also have drivers on their website ;-) just make sure they support mac os before you buy one
Saw comments above and see you are trying to do this with a Mac. I can't comment on how to go about development on a Mac but I can tell you what I would do with a Windows PC. First get a USB to RS485 adaptor that you know to be compatible with your hardware (on windows this is almost all). Then wire it all up to a test motor that you understand well. Program the drive for that motor using the keypad. Then using Putty (or other similar terminal program) get a feel for the serial interface manually. Try and read the motor parameters you programmed manually, try and read the current RPM and other feedback parameters, then try to spin it. Once you understand the commands you need to program, control, and read the motor start building up a library of labview functions to send the specific serial commands and properly parse the feedback. As the other guy said, because this is a high power motion system, always be thinking about safety. Wire a physical e-stop to the motor completely disconnected from the computer so you can kill it at any time. Keep that e-stop near to you while testing. Design your code to handle errors properly, stopping the motor when they arise.
Prolific (aptly named) and FTDI are the two big names in USB to RSXXX interface. Whichever one you get, as long as your mac recognizes it as a COM port VISA will too. Make sure you have VISA as part of your drivers. Configuration support and NI MAX will be tremendously helpful. If you see the RS485 adapter in NI MAX as a COM port life is easy. Go through that novella-I mean manual -and highlight all the commands you are going to use, write a subVI that has all the commands linked to an enum so you can use plain English (or whatever you like) on the block diagram, and Bob's your Auntie. Suddenly your motor driver is a simple block on the diagram like everything else.
Ideally I woudn't use Mac but I don't want to buy a new Windows PC for this purpose unless I have to. For sure first thing I'll find a way to stop the motor. I don't understand why I need "Putty" like program can't I just read the data via LabVIEW?
I'm trying to transmit 1.5 GHz sine wave using USRP 2901 and so far i've made that progress. kindly point out any mistakes or please tell me a better way to do this task.
LabVIEW is not great as acting as a general purpose serial terminal. Serial comms can be kind of finicky and sometimes it's hard to know if the problem is with the command you are sending or the connection. For this reason you can sort out the exact details of the command and response structure with a general purpose terminal first. Then you know if you send a command that you know gives a certain action or response that the problem is in the communication.
Is it working and you just want advice? Or it's not working is why you're asking?
Hi. It is working. I was just wondering if there was another way to do the same thing.