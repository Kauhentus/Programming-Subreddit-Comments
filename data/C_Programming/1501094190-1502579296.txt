Arm is little endian but fread() doesn't care. Whatever format your file has been stored in is what you read. 
I don't understand. fread() doesn't care about the type it scans to either, right? So it just stores sizeof(c) bytes sequentially at &amp;c in the example. Like ((char *) &amp;c)[0] = byte[0] and so on. So if the file has 64 bit integer in same endianness as the machine, it's alright, but if it's in big endian in file and little endian in machine, or little endian in file and big endian in machine, problems arise - you have to reverse the byte order. Yes, the file is scanned the same on all machines, but the value scanned into multiple-byte types can be interpreted differently after the scan.
Correct. If you want a specific endianness, you have to handle that conversion yourself. the "htons" and "htonl" and "ntohs" and "ntohl" functions might be helpful for you. These convert between 'network' and 'host' byte order.
Hey sorry this is a little off topic. I'm a few months in to my C course. I was wondering what kind of theory work you guys do? My course is heavily practical so I just wanted to see how I could supplement it. 
Coldfire (68000 descendant) is also big-endian.
Right, and you can use functions like htons() and ntohs() to convert.
you could try out this too: https://github.com/antirez/linenoise
IIRC K&amp;R tends to put the semicolon on the following line to indicate an empty statement, as such: while (x++ != y) ; So if you see a semicolon at the end of a line for a loop then it's probably wrong. Unless the translator fucked that up too.
Well you could implement the script using C if you want, there are syscall for ls and echo.
&gt; What do you think should be done with OpenSSL in order to fix this legal issues? Nothing can be done without the written consent of every single author and contributor in all of OpenSSL's history. &gt; And what about the messy code and API? What can be done with that? Other than switching to a different library or starting from scratch: what the OpenBSD developers are doing with LibreSSL, but it's going to take a very long time.
most ARM CPUs have configureable endianness, I believe
Don't just blindly write structures to disk. Come up with some sort of format (or reuse an existing one like yaml or json), and write functions to convert back and forth.
You can do this, but it's nearly impossible to portably estimate how much space a struct field takes up, so if you do that, it's not really going to be portable.
This's what I'm doing for my everyday coding, but only in a embedded system with a tiny flash memory, so every byte counts, usually I'll leave 10-20 bytes at the end so that my structure sized at multiples of 16B. For PC application, I will serialize them as the others mentioned. 
Why not just use JSON? [Jansson](http://www.digip.org/jansson/) is really nice to work with.
A mix between YAML and JSON xD
There are more ARM CPUs in use than x86/x64.
I like the fact that it can be included in a project by copying 2 files. Very important if you need zero-dependency code. I'm probably the only one who uses Lua(-embedded) as a configuration language these days and was thinking about a switch for a long time. Might try this instead. 
[this may be of use](http://www.catb.org/esr/structure-packing/)
&gt; Does gcc supports the graphics.h library which was in borland turbo C? It's just a library, so yes. There are ports ([e.g.](http://libxbgi.sourceforge.net/)) of it to both Windows and X11. Not that you should use them; try [SDL](https://en.wikipedia.org/wiki/Simple_DirectMedia_Layer) or something instead. &gt; I didn't know that Stanford standard libraries also included the graphics.h library. [It doesn't](https://stanford.edu/~stepp/cppdoc/). You're looking at an old archive page. &gt; Dead or alive It's been dead for 20 years, but Indian universities still insist on parading its corpse around for some indiscernible reason.
Well if you use a compiler flag (-Werror for clang and gcc) to treat all warnings as errors (which is what you probably should be doing anyway) then it will be a hard error.
I haven't used Linux as much as older versions of Unix, but if you look in /bin or /usr/bin, some binaries are symbolically linked together, and they use argv [0] to determine how they function. For example, you might have rm and mv linked as the same binary, and based on argv[0] decide whether it moves or removes files. You may also find that larger programs are linked for ease of maintenance. For example, ex and vi are linked. When you use ex, it is running vi in "ex" mode (vi -ex) and vi runs in visual mode. "Back in the day" disk space was not as plentiful as it is now so commands that were just basically command line interfaces over system calls were like this.
I also wanted to know if ncurses are an alternative to this
Not really. ncurses is alive and well, but it deals only with *text graphics* - letters and symbols on a grid of letter-sized spaces, not actual pixels. (Although, to be fair, you can [get](http://aa-project.sourceforge.net/gallery/) [rather](http://mewbies.com/geek_fun_files/caca/cacaview_blueeyed.png) [close](https://www.youtube.com/watch?v=impMxxNGESQ).)
This, alone, justifies it. Anyone who has ever written a command line utility is familiar with `argv[0]` (and various ways to strip away leading path components if desired).
I do that all the time. To keep compatibility, you'll need the first field to be a version number or identifier. struct my_struct { uint8_t struct_version; uint8_t my_data; uint8_t my_other_data; uint8_t letskeepthisforfutureuse[32]; }; Then you know struct_version=1 only has my_data and my_other_data. One year from now, you decide to add some more data: struct my_struct { uint8_t struct_version; uint8_t my_data; uint8_t my_other_data; uint8_t more_data; uint8_t letskeepthisforfutureuse[31]; }; So then you'll set struct_version=2 and when you read back one structure from some file, you'll now which version it is. Note that both examples pack tightly. If you were to do something like this: struct my_struct { uint8_t struct_version; uint8_t my_data; uint8_t my_other_data; float some_number; uint8_t more_data; uint8_t letskeepthisforfutureuse[27]; }; Then you'll be falling onto alignment problems, and this structure won't need letskeepthisforfutureuse[27] to keep the same size as previous structures, because there will be some empty space. In this case, you should force packing: struct __attribute__((__packed__)) my_struct { uint8_t struct_version; uint8_t my_data; uint8_t my_other_data; float some_number; uint8_t more_data; uint8_t letskeepthisforfutureuse[27]; }; Take a look at something like this https://stackoverflow.com/questions/4306186/structure-padding-and-packing
`gcc` is a compiler. It supports whatever library is availably for your system and generally, it doesn't matter what compiler you use when using a library. The more interesting question is, is the library available for your operating system? And which operating system do you use?
First, fix your wrong formatting specifiers. And there is really no use to use `unsigned short` for loop indices. Then, try to instrument `root` with some `printf` calls to see what values `lower`, `upper`, `medium`, and `tmp_power` take up. I think you hit an integer overflow when trying to compute the power of `medium`.
Your code tries to store 14^9 in a `unsigned long` (could be 4 bytes) inside the function `power`, that number is quite a bit bigger than 2^32, so you might already overflow there. You also call the `root` function with `base == 14^9` and `r_root == 9` then basically call `power((base+1)/2, 9)` which will try to calculate (14^9 )^9 - that's not going to fit in a 128-bit integer, nevermind a `unsigned long`. edit: By the way, `clang` can detect integer overflows (among other undefined behaviors): $ clang root.c -o root -fsanitize=unsigned-integer-overflow &gt;/dev/null $ ./root &gt; /dev/null root.c:9:50: runtime error: unsigned integer overflow: 72057594037927936 * 256 cannot be represented in type 'unsigned long' 
Hm. It looks like a text file. It looks like a text file like we've used in *nix for decades. I'm glad they invented it.
Offsetof()?
How does that help you find out how large a field is on other platforms?
Doesn't. You have to rely on using the same compiler, architecture if you're gonna save like that. 
So then, how does your comment add anything to the comment I made originally?
It certainly doesn't!
I just want to say good job on recognizing and correcting the bug. I'm assuming you're still rather new since you're going through this book. For a newish developer, you showed some "Next level" understanding. Keep it up! 
I had C programming class in high school but it was really really basic. I'm writing C for fun for 1-2 years but never actually studied it properly untill now. So far I was learning by trial and error and by looking up on internet how to do things.
Why not, use classic INI file
Because JSON is far more expressive.
What you want is a [serialization](https://en.wikipedia.org/wiki/Serialization) library. There are many. If you want something lightweight, try [XDR](https://en.wikipedia.org/wiki/External_Data_Representation) or NVPair (which is XDR but with named values).
**Serialization** In computer science, in the context of data storage, serialization is the process of translating data structures or object state into a format that can be stored (for example, in a file or memory buffer) or transmitted (for example, across a network connection link) and reconstructed later (possibly in a different computer environment). When the resulting series of bits is reread according to the serialization format, it can be used to create a semantically identical clone of the original object. For many complex objects, such as those that make extensive use of references, this process is not straightforward. Serialization of object-oriented objects does not include any of their associated methods with which they were previously linked. *** **External Data Representation** External Data Representation (XDR) is a standard data serialization format, for uses such as computer network protocols. It allows data to be transferred between different kinds of computer systems. Converting from the local representation to XDR is called encoding. Converting from XDR to the local representation is called decoding. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/C_Programming/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.24
For a simple basic configuration file, you not to be "more expressive". You only need to write a simple map of string_id -&gt; value
I'd just like to add a little nuance to what /u/raevnos said. It's generally good advice, and certainly "modern" best practice to do what he suggested. But it's not one-size-fits-all. The correct way to store and access your data depends on your use case, including your performance needs. Since you said this is a toy project, I expect you don't have much in the way of pressing performance constraints. There are times though, when the difference of an extra 5 ms in processing is the difference between success and failure. If you find yourself in one of those situations, being able to "seek" directly to the exact record and data field you need rather than having to slurp in the whole file and run an expensive lexer/parser can be a *huge* performance win. Basically, what he said is good advice for most situations. It's certainly easy to troubleshoot data issues in a json file than a binary blob. But don't think it's the only way to do it, or that binary records don't have their place, because they do.
Assuming availability of &lt;stdint.h&gt;, does not (consistent) use of the size_t types (or typedefs thereof, for readability) basically ensure this? Granted, somewhat awkward at times, but not what I would characterize as "nearly impossible".
Can you elaborate on that? `size_t` has a different size on each platform so you can't use it to make structures whose size are the same on all platforms.
Yes. The elaboration is that I had a total mental fart when I wrote that. I referenced stdint.h, but I didn't mean size\_t. I meant the [u]int_&lt;size&gt;_t types, such as uint_32_t.
Even these do not do the trick as you can't predict how much padding another architecture is going to insert.
OP's code is typed and supports arrays.
I obviously need more caffeine. Wasn't thinking about platform-dependent padding. Brainstorming here, what about wrapping the struct in a union where the other member is a struct comprised of a bitfield member for the intended length to defeat padding? Granted, now we are moving into esoteric tricks.
That doesn't fix alignment-related padding either.
Bleh. I guess at that point you have to start resorting to alignas() values based on the largest alignment/padding value of any target platform you care about, which is kind of a messy hack. I retract my original statement; it counts as "nearly impossible" to do portably with anything like a clean approach.
`alignas` doesn't help either because there is no language in the standard that says that the padding would only go to the next sufficiently aligned address. For example, some experimental compilers align structure members to cache lines even if they have lower alignment requirements.
That's when you turn on -fdont-be-stupid. /s I applaud attempts to be *completely* portable, but for anything like a "production" codebase (which is the only place I'm likely to have sufficient performance constraints to be using binary record data), I expect to have control over the target compiler for a given platform. If I could verify that gcc and/or clang produced usable code, in practice, I would consider that a win. Your point is certainly valid though, from a pure portability standpoint.
Then I stand by what I said: Good Job! You've shown an understanding of misplaced semicolons (they can be hard to notice at times) and bit-shift operators. It shows that you've been working in the area for a bit. I hope your more disciplined studies continue to help focus your skills. It looks like you have a great future ahead of you. 
So, if you can assume to have a “nice” compiler, then go by the rule that every fixed-size integer type will be aligned at most to the next multiple of its alignment. Then you just build your structure such that each member begins at an aligned address and insert padding manually using dummy members. This works on all nice platforms modulo endianess issues.
Thanks! This really motivates me to continue learning programming. 
Think hard before writing binary data to a file; that's the death of compatibility. Port your software to a different architecture and it's almost a certainty that the files written by one will be unreadable by the other. That said, if you really want to do it, then every data structure written to file should have the same standard header. The header should include the size of the data structure and the type of data. Write your software so that it reads exactly the amount of data specified in the header no matter what your software *thinks* the size should be. This will allow you to at least skip over data structures that are unrecognized or malformed.
If you are looking for a simple way to play with graphics in C, you might wanna try [raylib](http://www.raylib.com/).
Let's kill this obsolete thing once and for all... I use allegro for basic 2D applications and SDL for 3D (but I don't really know to code 3D well, I'm still learning).
We used to have an old source control program that was distributed as a single executable. We symlinked the different source code operations to it, and the source control would execute the various source control operations based on argv[0]. Basically used argv[0] as a shortcut for the first command line argument. 
Thanks. You were right. `root` isn't written correctly. How could I fix it? *Please*.
I don't get this JSON hype for configuration files. IMO it's just wrong for such things. It does not even support proper commenting, which is something you most likely want in config files. [TOML](https://github.com/toml-lang/toml) is a great alternative between plain INI-style files and JSON. It provides flexibility, it's not as convoluted as YAML, reads better than JSON.
But it is suitable only for open projects as it is released with GPLv3 licence.
You're pushing the same pointer 'word' onto the stack on every iteration. The fscanf is merely changing the data that 'word' points to, which is why every entry points to the last word you read. After reading the word in you should allocate a new block of memory and copy the word into it to push onto the stack.
You're overwriting the same heap buffer with every new read. You need to allocate a new buffer for each new read from the file. 
the problem is in your read_file() function. you're reading into the same memory address, overwriting the data you previously had there. Then, in the stack, you point all of the nodes to the same memory address. you need to allocate new memory each time you fscanf.
Please put four spaces before each line of code to make it readable.
Sorry. One moment
I'm not a huge fan of JSON either, but it's flexible enough, well known, easy to learn, and almost universally portable. TOML has sections. I hate sections. They make it more difficult to parse and more difficult to model. But more importantly, it doesn't seem to be nestable. If you want something that “reads better than JSON”, try [UCL](https://github.com/vstakhov/libucl).
I mean I'm writing [BitIO](https://github.com/bumblebritches57/BitIO), but it's not complete, and I change the API pretty often. Right now I'm actually experimenting with making the ReadBits function a generic, endian dependent group of functions for easier inlining, but as I said, it's not complete.
ARM ***CAN*** be little endian, or it can be big endian. Assuming it's either is a disaster waiting to happen.
ARM Is NOT little endian, nor is it big endian. it's configurable endian, and you need to test it to know for sure.
Honestly, amen. I have heard of a single good use case for it tho, and that's in the binutil library used for lots of embedded devices, using that to call into a single program and it decides which functions to use based on that. I forget what the library is called tho :/ Edit: BusyBox, that's it.
Done.
Do the same thing with the program output. Reddit eats newlines if you don't do that. And put an empty line between sections of code and sections of text.
Did that as well. Sorry, I'm new and I was desperate for answers
Let's start at the beginning. Do you understand how number bases work? All the program is really doing is printing an integer in base 10 and then the same integer in base 2, repeated for a handful of integer values. It's demonstrating that each symbolic constant has only a single "1" digit in base 2, so that when the values are combined with the bitwise-or, the result is a composite value with a "1" digit from each of the combined values. The example is not really great because it uses multiplication and division for performing bit shifting, which is really not ideal. There is a pair of dedicated operators for that, `&lt;&lt;` and `&gt;&gt;`. You should understand that multiplying or dividing by a power of the base shifts digits in that base. For example in base 10, if you multiply by 10 you shift all digits to the left, and if you divide by 10 you shift all digits to the right, and similarly for 100 or 1000 or any other power of ten. 256 is 2^(8) so dividing by that shifts digits eight places to the right in base 2, but that would be better expressed as `foo &gt;&gt; 8`. 
Holy crap, that cleared a lot of it. Yes, I understand base 2 but take note that I have trouble interpreting verbal explanations, and I learned that with number pictures. Can you walk me through the code? All of these function prototypes I told myself I understand but don't understand completely because the book never went through them in detail, it basically used a sentence to explain the whole concept. So, in here; void display_flags(char *, unsigned int); void binary_print(unsigned int); int main(int argc, char *argv[]) { What exactly is happening? My conception has been that the function prototype for display_flags has been declared, which expects a character pointer along with unsigned integer values, which is the same for binary_print barring the character pointer. Now, in int main, I can recognize the arguments as Command Line arguments, however, I don't understand how the other function is used within this main one, as shown here: display_flags("O_RDONLY\t\t", O_RDONLY); display_flags("O_WRONLY\t\t", O_WRONLY); display_flags("O_RDWR\t\t\t", O_RDWR); printf("\n"); display_flags("O_APPEND\t\t", O_APPEND); display_flags("O_TRUNC\t\t\t", O_TRUNC); display_flags("O_CREAT\t\t\t", O_CREAT); printf("\n"); display_flags("O_WRONLY|O_APPEND|O_CREAT", O_WRONLY|O_APPEND|O_CREAT); } I need clarification on what's happening here.
I didn't really understand this one thoroughly either: #include &lt;stdio.h&gt; int main() { int i, bit_a, bit_b; printf("bitwise OR operator |\n"); for(i=0; i &lt; 4; i++) { bit_a = (i &amp; 2) / 2; // Get the second bit. bit_b = (i &amp; 1); // Get the first bit. printf("%d | %d = %d\n", bit_a, bit_b, bit_a | bit_b); } printf("\nbitwise AND operator &amp;\n"); for(i=0; i &lt; 4; i++) { bit_a = (i &amp; 2) / 2; // Get the second bit. bit_b = (i &amp; 1); // Get the first bit. printf("%d &amp; %d = %d\n", bit_a, bit_b, bit_a &amp; bit_b); } } Compiled code: reader@hacking:~/booksrc $ gcc bitwise.c reader@hacking:~/booksrc $ ./a.out bitwise OR operator | 0 | 0 = 0 0 | 1 = 1 1 | 0 = 1 1 | 1 = 1 bitwise AND operator &amp; 0 &amp; 0 = 0 0 &amp; 1 = 0 1 &amp; 0 = 0 1 &amp; 1 = 1 reader@hacking:~/booksrc $ this one is probably a better place to start. 
Thank you for formatting your code! Many people don't do that, it's always annoying.
I acknowledge that you're a non-verbal learner. However, [this video](https://youtu.be/d0AwjSpNXR0) is an excellent resource. 
Okay, thanks
The function display_flags just prints the char * as a label, then the int as a decimal, then the int again as binary. Look at the first call, and the first line of output: display_flags("O_RDONLY\t\t", O_RDONLY); O_RDONLY : 0 : 00000000 00000000 00000000 00000000 The char * argument is just "O_RDONLY" with two tabs after it. If you look at the printf call in display_flags: printf("%s\t: %d\t:", label, value); You see that it prints: - %s, a string. The second argument to printf will be printed as a string. - \t, a tab - :, just a colon character - %d, a decimal. The third argument to printf will be printed as decimal. - \t, another tab. - :, another colon. After it prints these, it calls binary_print to print value as binary, then calls printf again with "\n", a newline.
Do you understand how [printf](https://en.wikipedia.org/wiki/Printf_format_string) works? Since it takes a variable number of arguments, the very first argument is a string that tells it how to interpret the remaining arguments. printf("%d | %d = %d\n", bit_a, bit_b, bit_a | bit_b); Says to print bit_a as %d (decimal), then print a vertical bar, then print bit_b as %d, then print an equals sign, then print (bit_a | bit_b) as %d. The vertical bar is the "bitwise OR" operator: bit_a | bit_b The way it works is, it looks at each bit position for the two operands, and makes the corresponding bit in the result equal to the "or" of the operand bits. The way "or" works is like this: 0 or 0 = 0 0 or 1 = 1 1 or 0 = 1 1 or 1 = 1 So you can see that if the first operand "or" the second operand is 1, the result will be 1 as well. The important thing to get here is that in C, the "|" operator does this for EVERY bit of the operands and the result. So, if I have two binary numbers, 01 and 10: 01 | 10 == 11 Because this is in base-2, this is just another way of saying (decimal this time): 1 | 2 == 3 Bitwise "and" (&amp;) works the same way, except that the "and" operation on two bits works like this: 0 and 0 = 0 0 and 1 = 0 1 and 0 = 0 1 and 1 = 1 So the result bit will only be 1 if the first operand bit "and" the second operand bit are also 1. Because these operators work on every bit, they can be used to extract or set a particular bit for some number. If I do: 3 &amp; 1 That's the same (in binary) as: 11 &amp; 01 If you look at each bit in turn, you'll see that the result will be 01. We've effectively extracted the 1-s place bit from the number 3. In this example, the author loops through 0-3, takes the 2-s bit and puts it in bit_a, takes the 1-s bit and puts it in bit_b, and then prints each bit, and the bitwise or of each bit. 
**Printf format string** Printf format string (of which "printf" stands for "print formatted") refers to a control parameter used by a class of functions in the string-processing libraries of various programming languages. The format string is written in a simple template language, and specifies a method for rendering an arbitrary number of varied data type parameters into a string. This string is then by default printed on the standard output stream, but variants exist that perform other tasks with the result, such as returning it as the value of the function. Characters in the format string are usually copied literally into the function's output, as is usual for templates, with the other parameters being rendered into the resulting text in place of certain placeholders – points marked by format specifiers, which are typically introduced by a % character, though syntax varies. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/C_Programming/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.24
 I do understand printf.
This looks so awesome, I can't believe I haven't heard of it before now.
The functions are declared with prototypes so that they may be called before the definition has been seen. `O_RDONLY` and the others are integer constants that are defined by the standard header `&lt;fcntl.h&gt;` and are being passed as the second parameter. The first parameter is just a string literal, which is an array of const char that decays to a pointer to char. 
I'm impressed by some of these other responses, though.
&gt;It's been dead for 20 years, but Indian universities still insist on parading its corpse around for some indiscernible reason. I've heard it's because they use ancient textbooks as a cost saving measure.
Okay, that clears up the bitwise.c program. 
That clears up a lot of it. Now what about this snippet? int main(int argc, char* argv[]); I know it's a Command Line Argument, but... how does this tie into it all? Also, with the display_flags(char *label, unsigned int value) { Where does "value" come in? Perhaps it's DECLARING an unsigned int that is named "value"? Then, when it prints it using format parameters here printf("%s\t: %d\t:", label, value) I can tell it has a specific value, and well....I just don't get it. It then uses it again here void binary_print(unsigned int value); Which, it probably again is just declaring an unsigned int by the name of "value", but how can you use such a variable and know it's gonna print the values you want it to? I need to know how it comes in. I'm probably overthinking or not catching something, enlighten me please. 
Sounds like someone wanted to disable the `N_TTY_TRACE` logic but didn't want to remove the code. Commenting it out would have been easier though.
That makes sense, thanks for the reply.
 int main(int argc, char* argv[]) This is just the main function signature. Your main function is where your entire program begins. You're right about them being command-line arguments; argc is the argument count, or the number of arguments passed to the program when it is run from the command line. It will be at least 1, since the command used to invoke the program is passed as argument 0. The other argument to main, char \*argv[], is an array of char \*, which you could call an array of strings. When you call your program, after your shell has finished doing any needed substitutions, the command-line arguments are split into this array for you. You use the argument count to figure out how big this array is, so you can iterate over it if you need to work with the arguments. Another valid way to declare main is like this: int main() In my opinion, this would have been a better way for the book to do it, since it makes it clear that this example program does not actually use any command-line arguments. display_flags(char *label, unsigned int value) { This is another function signature. In it, we define what variables we'd like to be passed into the function body when the function is called. In the function body, you'll get a local variable "label", and another "value", and the starting values of these will be whatever you pass in, in the same order, when you call the function. For example, display_flags("O_RDONLY\t\t", O_RDONLY); Passes the string literal "O_RDONLY\t\t", then the value of O_RDONLY. These are then copied into the local variables "label" and "value". The two arguments, in this way, take on new names within the function body.
It looks real nice! Tiny tidbit: Pointers that are equal to NULL evaluate to false in a boolean expression so that you can write `if(!p)` instead of `if(p == NULL)`.
p == NULL is more readable.
While it is mostly a matter of personal preference, I'd argue that `!p` is better because it is faster to write and reduces code verbosity.
You're right, it's just a preference.
C is a good first programming language if you want to learn programming by understanding how a computer works. C allows you to write programs in exactly the way computers are going to execute them with very little abstraction inbetween.
I think C is not a bad choice but you need to be patient. It takes time and practice to master such a complicated discipline as software development. For the first year or so, don't expect to make anything more complex than a simple address book. But when you will be fluent in programming, you will see the benefits. It's not strictly language dependent, it's just the normal way things works, even with python.
I agree its preference. I do use p == NULL because when its late and i'm reading some code and see that, I know your most likely testing a pointer. !p could be testing an integer or a pointer. For me I don't feel it really takes that long to write and I'm ok with that level of verbosity.
Or even better, NULL == p to avoid making = or == mistakes.
This way it can be enabled or disabled with a quick `//`
messagepack's C implementation is also very light
&gt; Goodness - that actually worked! Goodness - you're a noob
It can already be enabled or disabled with a quick `#define N_TTY_TRACE` somewhere. This just breaks that mechanism.
You're actually pretty good at writing in English and I understood your blog post just fine. Someday I'll compare this to my Tetris clone that I'll eventually make when I stop being lazy.
If you run git blame and find the thread where this patch was sent the author says he added this as a way to debug flow control problems. He doesn't want to use `dev_dbg` because printk is slower than ftrace. What's weird is GKH says he didn't apply the patch that contains this (he didn't like the custom debug code) but it made it in. So he must have had a change of mind. 
&gt; most of my time is spent on things like debugging and trying to figure out why my code won’t work rather than actually programming That *is* "actual programming". Jokes aside, I personally don't think C is any good for beginners. C is good for really, really hardcore stuff (like OSes, other programming languages and their compilers, drivers, firmware, etc.) and unless you really know what hardcore thing you want to do, C is not a very rewarding learning tool. It's ruthlessly unforgiving because it assumes you know *exactly* what you're doing. It's not going to hold your hand and throw all kinds of fancy exceptions with handy-dandy stack traces for you to peruse at leisure when you gone done wrong. I'd suggest an *friendlier* language like Python...heck, even Javascript...because you get your kicks sooner. However, once you've become proficient in one or two languages, it's really quite easy to pick up others. **edit:** expanding on the first point about "actual programming": A well-written piece of software is as much a work of art as is a novel, or a painting, or a sculpture. It goes through much the same processes: conceptualisation, prototyping, recursive/reiterative refinement, optimisation and finally, beautification. **edit 2:** Knowing *exactly* what hardcore thing to do isn't something one learns, it's something one earns. I'd actually go so far as to say that unless C is where you start because you *must* (Uni, for example) one should only really get serious about it a good few years into one's career. By that time you'll have tons of experience and C will make *intuitive* sense.
Depends whether you prefer to learn top-down or bottom-up. For the latter, it's an extremely good choice.
Also, learning C allows you to learn other higher level languages faster. There are plenty of tutorials online that explains C basics thoroughly, and the best way to learn more is by practicing. Later you'll learn about more abstract concepts that can be applied in different ways and in different languages. Some languages are better for certain tasks than others. Keep an open mind and read on.
Same here. And the seconds you might save typing can be wiped out by a single bug, or even just having to take a little longer to understand what's going on. It's also just a useful habit to keep since there are other languages where NULL and 0 aren't the same thing.
if you want to feel the "true programming", yes if you want to be "productive", no it was a good start for me... then go to Go
Good job! You even make good use of `const`, that's better than most of my interns already. You might want to add a `-std=c99` to your [`CFLAGS`](https://github.com/MarcoLucidi01/tetris_clone/blob/6072f2ad97c39fc44353c28d5909d5c721ce9cb0/Makefile#L7), it was the only thing for me between a `git clone` and a `make` to get a working program.
I'd say stick with it. No sense in going to Python if your end goals are all C oriented anyway. I personally dislike Python anyway, but I honestly feel it would be counterproductive for you. A month may feel like a long time to keep banging your head against a wall, but it really isn't. Write something every day, and you will surprise yourself. Nobody said this would be easy, but the payoff for putting in the time and effort to be effective in C will be far far greater than by puttering around in Python and then coming back to C and needing to learn low-level concepts all over again anyway. You might even develop habits in Python that simple won't translate. You're somewhat unique in that you already have a sense of the types of things you want to build - choose the right tool for the job. C demands much more attention to detail and deeper knowledge of how your code runs, but that extra knowledge you gain will only help you if you decide to switch later. Also, the "debugging" process is part of it no matter what language you choose. No way around it. When I code, I'd say it's about 50% planning before I write a single line, 10-20% writing code, and the remaining 30-40% debugging and tweaking. It's a big cycle.
Also, an excerpt from the intro: &gt; For now, we mainly see the library as a target for library writers or compilers. For example, the P language [10] is a language for describing verifiable asynchronous state machines, and used for example to implement and verify the core of the USB device driver stack that ships with Microsoft Windows 8. Compiling to C involves a complex CPS-style transformation [19, 26] to enable async/await style programming [5] with a receive statement – using the effects library this transformation is no longer necessary and we can generate straightforward C code instead. Similarly, we hope to integrate this library with libuv [29] (the asynchronous C library underlying Node [43]) and improve programming with libuv directly from C or C++ using async/await style abstractions [12, 27].
Thank you! Actually I prefer `if (p == NULL)` because it makes (at least to me) more "explicit" and understandable at first glance what I'm testing, but as you already stated, it's just a personal preference.
I'm not op but I'll weigh in as I first learned python -&gt; C -&gt; other langs. It's dependent on your learning style. If you prefer a bottom up approach, starting with the most basic blocks to build up the higher abstraction then C is great. If you prefer to learn the fundamentals without worrying about things like manual memory management, pointer stuff, etc, then python or any other language is a better fit. Alternatively, there are the functional languages. There are good reasons many of the top universities in the West teach racket /scheme as a first language. It provides a great environment to focus purely (pun intended) on the basics of programming and cs, like data structures, recursion, etc. It also introduces you into a very different style of programming early on so you can understand the benefits of functional vs imperative programming. If you hate that python "does the work for you" you may as well just code in assembly. Languages higher up the abstraction chainare meant to make your life easier, and hence you more productive. I personally would recommend learning C first because I am a bottom-uo style learner, then possibly move into a functional language like scheme once the basics are covered. This is opposite what my university taught their cs students (scheme -&gt; c). In the end I doubt it would matter much as languages are simply a tool used by the programmer, and you will inevitably need to learn many tools.
As a person who was in your shoes, unless you are really going to need C in practice, I would rather suggest you pick C++. Being a language just as capable, but implementing some abstraction and OOP, it would be a great place to start. You will encounter a lot of struggles that C offers along with getting knowledge in OOP. 
Just want to tack on that Racket/Scheme are definitely not only for beginners. I think there's something to be said for a programmer of any level, with any background, taking a look - it forces you to break out of the C world of doing things that many mainstream languages lock you in to and will make you a more well-rounded programmer. To a lesser extent I think the same thing about, say Prolog or J or what have you, anything that forces you to take a new approach to computation....but Racket is a hell of a tool no matter who you are.
C is an amazing first language and I wish more schools taught it first. Learn to compile on the command line and maybe start with a simple text editor for programming. 
Thank you! I'm glad you understood smoothly! All the best for your clone, feel free to take good ideas (if any) from my code ;)
Well, yes. That's why I expressed uncertainty in my first post about whether this was the right forum for my beginner's questions.
From my experience, starting with C is much better than any other programming language. I started with C myself, and after I learned a lot (you cannot master C language, those who say you can are gravely wrong, because you need more then one lifespan to learn it, and still it won't be enough) I swapped to Java and after Python. Now I'm considering to learn C++. Anyway, C made my path easier. Any language after it looks easy. 
So then, the value of O_RDONLY is now value? So this has been done? value = *O_RDONLY Not in the code, but in that sense...uh....I couldn't explain it...I guess it puts that value into "value" indirectly...I guess you could say..? But...that can't be, because there's many of these display_flags((the literal string of the flags you mentioned) , (the unsigned int value of those flags)) Which would mean....the void display_flags (char *label, unsigned int value) { would consider all those values and all those strings from all the other signatures, not just: display_flags("0_WRONLY|O_APPEND|O_CREAT", 0_WRONLY|0_APPEND|0_CREAT); ...I'm confused.
Thank you! I really appreciate your words! Did it complained about stuff like `for (int i = 0; ...`? Anyway I added `-std=c99` to the flags ;)
In my opinion, C is the best language any beginner can start with. Being the lingua franca of modern programming languages, it makes getting into high level languages pretty easy when you need to use them. Do be patient and persistent though, it's well worth it.
Might as well make it -std=c11 C11 has been around for 6 years now. Why do people still cling so much to the 1999 standard?
I think you would learn more about how computers work by learning C. I think you would be able to complete more interesting projects with Python. Both have their place. Maybe just learn both? Think up a project with each, complete it and move on. C is very low level and Python is very high level. About as low and high level as most people ever need to get into themselves. It's good to understand them both.
This is what has been done: value = O_RDONLY There's nothing indirect about it. "value" takes on the exact bit pattern of O_RDONLY. When you call display_flags, the local variables "label" and "value" are created and take on the value of the arguments to display_flags. When display_flags exits, those variables go out of scope, and cease to exist. The next time that display_flags is called, they are created again, with the new values. I think you might be getting a bit confused between function definition and function invocation. When I define a function: int add(int a, int b) { return a + b; } I describe what arguments I would like to be passed into the function, what I would like those arguments to be called from within the function, and the type of the return value. When I call a function, I don't change the function signature, I just pass values that match the types I defined for the arguments earlier: int result1 = add(1, 2); //result1 is now 3 In this example, I pass 1 and 2. The value 1 is copied into a temporary local variable "a", and the value 2 is copied into a temporary local variable "b". The sum is computed, and returned. I take that return value and store it in "result1". I can call the function again with different arguments: result1 = add(2, 3); //result1 is now 5 In this case, a is created as 2, b is created as 3, and I assign the return value to result1, overwriting it. I can even use result1 and overwrite it in the same statement: result1 = add(result1, 5); //result1 is now 10 In this case, the value of result1, 5, is copied into a. Then the value 5 is placed into b. Then the result of a + b, 5 + 5 = 10, is returned, and overwrites result1. I can call "add" as many times as I want with different values for the arguments, as long as they match the type. I can't, for instance, do this: int result2 = add(3, "hello"); In this case, the type of "hello", char *, does not match the type that add expects for its second argument, int. This would not compile.
I'm a big fan of both Python and C (and try to use both based on what I'm trying to do, and they go very well together too). I would probably advocate learning Python as a first programming language, since you can learn better about the concept of programming, i.e., at a higher level of abstraction, while you're fairly isolated from the low-level implementation issues. However I usually don't advocate that for the simple fact that once people get comfortable with Python they're scared to death to take the plunge into low-level programming, and this psychological barrier prevents them from learning something low-level like C or assembly later. So in the end, you have to ask yourself what kind of person you are. Are you the type who would have difficulty taking the plunge into C, and having to learn a lot more about computer hardware etc, after having learnt Python? if yes, then you should learn C first. If not, you should learn Python first.
This might have the downside of having to learn OOP at the same time though. To an experienced programmer, OOP is a very simple and intuitive concept, but I see many students struggling to grasp it. It might make things harder in the beginning. 
Dunno why you're getting downvoted for a light-hearted joke about the truth
I'm running into the opposite problem. I'm frustratingly trudging my way through learning python after being good with embedded C for the last few years. I can't keep track of all the assumptions Python makes about the way you move data. Everything seems so ambiguous compared to memory model that C uses.
If you're getting into low-level stuff, you're going to have to learn it sooner or later anyway. I think it's a reasonable place to start. Even if you're never going to code in C, it's like learning Latin. It's the progenitor of many languages and it's the one that everything is compared to.
For a long time, and maybe current, there was a sense that not all major compilers supported C11 fully; in particular MSVC was considered a holdout. I can't recall whether they've changed that stance. Compilers for more niche systems, like embedded micro-controllers, have always had a reputation of being quite a bit behind in support. 
So is void display_flags(char *label, unsigned int value) a function signature/a defining function? It's confusing because it's being used the same way as void display_flags(char *, unsigned int); And if it is...how is it that label and value how values of their own? They're printed in the format parameters here printf("%s\t: %d\t:", label, value); As if they had a value already set...which tells me it corresponds to all the display_flags function calls which are the lines of code from here display_flags("0_RDONLY\t\t", O_RDONLY); to here display_flags("0_WRONLY|0_APPEMD|0_CREAT", 0_WRONLY|0_APPEND|0_CREAT); I'm having trouble linking that all together. Btw, you've been of immense help.
Learn C++ or C# first. Otherwise you will learn a bunch of bad habits then have to unlearn them. EDIT: My bad, I thought this was /r/programming. Now I realize how this just looks like bait.
Yep - one of the many reasons I strongly dislike Python. I also started with C and felt the same. Not to be one of "those" people, but Rust is pretty dang great if you want the high-level abstractions (especially pattern matching) without forfeiting control :)
I agree. It's pretty clear and structured and you don't have to write crazy stuff if you don't want to. Great for starters. 
I'm sorry, I didn't notice something, and may have confused you. At the top of the file, this: display_flags(char *, unsigned int); Is called a forward declaration, it's not a call to the function. It tells the compiler that there's a function with that name, and arguments of those types, so that you can make calls to it. The arguments don't need to be named for a forward declaration, just in the matching definition. In this case, it's just a convenience to avoid having to write the definition of display_flags earlier up in the file. 
If you stick with C and don't move on you are going to miss the amazing utility that is object oriented programming. That being said, I do think you should stick with C until you at least start exploring data structures and algorithms. I started in BASIC then Java then C and now Assembly. Even though I started with 2 easy languages, I wish I would have started with C. I found that learning C helped my existing programming experience, much in the same way that assembly is helping me now. The lower you go the higher you understand
It's all good, let me go over what you said EDIT: Ok, so, isn't definition the same as declaration? So this other one void_flags (char* label, unsigned int value) { ISN'T a forward declaration because the arguments have names, right? So it makes it a definition, aka declaration, correct(just not forward)?
Really cool! How does this compare to csmith? I tried to modify csmith in the past to make it generate Frama-C function contracts, but it never worked out due to the (ehem) mediocre codebase of csmith and my own ineptitude. Perhaps this project will be able to yield better results.
&gt; Experience of Programming in C, C++ or C# One of these is not like the others...
Correct. A declaration is often incomplete. A function has to be declared somewhere higher up in the file before it can be used, and it must be completely defined *somewhere* for the program to link. Sometimes people will just write the function higher up, but if you want to put it in any order, you can use a forward declaration, either in the file itself, or in a header file you include. Header files usually just contain a lot of declarations for functions and types, because that's all you need to compile a given .c file. When you want to compile the final full program, the linker searches for the definitions of these and links them in.
The key difference is that CSmith generates some unreachable, dead code - all code generated by ldrgen should be reachable. Not sure if ldrgen would work better than CSmith with Frama-C.
I agree, i learned C first and had trouble getting logic down (loops, Boolean) because C is so syntax heavy. I'd recommend python or hell even HTML/CSS as a way to get used to typing commands into a computer before C
Now I'm confused as to what a function definition, and declaration each are specifically. Is there any way I can contact you besides reddit? I could really use your help.
It was my first real programming language, but it doesn't really matter what your first is because the language is just a way of telling the computer to do what you want the real skill is in actually knowing what you want the computer to do. 
You're not wrong. Personally I'd do a comparison like this pretty much every time, for readability's sake. It might be faster to type (!p) but when your program breaks in 6 months time you're going to lose more time figuring out precisely what that means in the context than you ever gained from typing less characters. 
I think the Latin analogy is a good one. I think a main feature of C is understanding pointers... understand them in C and it is useful in programming everywhere.
This could probably explain it better than I could: http://www.cprogramming.com/declare_vs_define.html I work full time, so I can't afford to answer your questions except on my own schedule, sorry. I highly recommend you read [The C Programming Language](https://www.amazon.com/gp/product/0131103628), it's very approachable for beginners.
That's way too slow compared to changing 1 or 2 characters when you are debugging :)
I think you mean assembly language. Sure you can inline ASM code in C, buy you're not 'closer' to the hardware.. its still a compiled language. Also the 'memory' you access is all virtualized on modern PCs. So its not like the old days when you had to manipulate memory bank offsets etc. C is quick, and can be compact.. but once you start talking about modern graphical OSs most of your time is spend messing with system API calls. C does have its place, but unless you're dead set on learning C most places will use whatever dev environment fits their needs... be it C, C++, C#, etc etc etc. 
C provides you with all the tools and control structures that directly map to the hardware. Higher languages just translate their features to the structures C provides, so learning the tool set C provides is essential for understanding programming. Yes, there is virtual memory, but that doesn't really hinder the understanding. Yes, C allows you to write code in a more symbolic fashion, but at the end of the day there is a clear correspondence between a C program and the assembly generated by the compiler with each operation you perform clearly corresponding to some instructions that do exactly the same thing without any magic or complexity. This is just not the case with other languages. Yes, you could also learn assembly, but assembly for a modern amd64 CPU is difficult. Even though I'm good at it, I don't recommend beginners to start out with learning assembly as many of the things you do in an assembly program (e.g. establishing a stack frame, preparing function calls) make more sense if you understood what they represent in C.
&gt; a bunch of bad habits I disagree with this. Any bad habit in C will also work in C++. What bad habits are you thinking of?
&gt; C provides you with all the tools and control structures that directly map to the hardware. It really depends on the OS you're writing for. Under something like DOS there is nothing hiding the hardware.. feel free to peek/poke (anyone remember those??) memory all day long. Once you step up to Windows or IOS then everything (well, almost) is virtualized and accessed via API. You *can* get to the hardware, but it usually breaks things. As for 'tools and control structures' well that's just header files to explain what you need.. there is nothing magical about C's ability to 'see' hardware... its up to the OS as to how 'easily' you can 'see' hardware. Free Pascal allows you to get to hardware via the proper lib calls. 
I am not talking about libraries or system calls. I am talking about the basic building blocks that make up C, like: * integers and the ability to add, subtract, multiply, divide, and, or, xor, not, or bitshift them * pointers, the ability to take addresses, to dereference pointers and to do arithmetic with them * the different kinds of loops and the various kinds of jump statements * the different kinds of storage classes (static, automatic, allocated) and how to allocate and release data * the way code is organized (in functions organized in translation units) * the ways we can structure our data (using structures, arrays, and pointers) * the way we can make our code modular (by using functions) All these building blocks map directly to functionality your platform provides. For example, storage classes map directly to whether you allocate memory in the data segment at compile time, on the stack, or in the heap at runtime. The three kinds of loops map directly to the three idioms used for writing loops in assembly, etc. etc. Once you have understood these building blocks, talking to the operating system or messing with the memory is just an exercise in applying this knowledge. Similarly, other languages are easy to learn once you understood these concepts.
You may as well do C++ I think. Stroustrup, creator of C++, has a very good intro to programming book. Then once you have a good grasp of modern programming you can learn what's behind it. C is good, I just prefer using C++ and thinking with Classes and OOP. I started with Java, so it was a better transition, I like C++ much more than Java, and find C's syntax quite verbose.
None of these are exclusive to C. 
Can I ask why? I'd argue there are better alternatives to both, and I can recommend jumping off points, depending on the end goal, though Ruby is far more worthwhile than Python.
* overuse of malloc * global variables * no concepts of public/protected/private members (new programmers coming from C tend to make everything public) * confusingly named functions to work around the lack of function overloading Those are just a few of the ones that I thought of off the top of my head. 
Of course not! But few other languages have all of these without also having magic that doesn't cleanly map. That's why C is such a good beginner's language.
I think that's what C++, C# programmers do. Not *actual* C programmers.
Same case happened to me. I must say to stick with C language. In the beginning you will face some complexity but after getting into it. Just try to code as much as you can and you will land in a very good position and afterwards you can easily learn some other programming languages very easily e.g-Python, etc. 
&gt; * overuse of malloc Really not sure what this means. I can't possibly see how the no-safety-net approach to memory allocation in C (i.e. `malloc` &amp; `free`) leads to a *bad* habit. The opposite is true IME ... C programmers are required to explicitly allocate and free dynamic allocations without help from the language or a runtime. This discipline is great for beginners. &gt; * global variables Globals work just as well in C++ as in C and there's nothing inherently "C" about being taught to use globals. As we all know now, global state is best avoided and that requires programmer discipline in both C++ or C. &gt; * no concepts of public/protected/private members (new programmers coming from C tend to make everything public) +1. Solid point. When a C programmer moves to a language that allows struct field visibility, they will need to learn that new feature and syntax. &gt; * confusingly named functions to work around the lack of function overloading Nothing about C encourages confusingly named functions. Put the type name into the function name. Problem solved, without confusion. Or do you mean that common C libraries are confusingly named? At any rate, the beauty of a strongly typed language is the function signature says what you need to know about the types it operates on. Lacking function overloading is not a problem IMO. Consider this: Serious new contenders for systems-level languages (e.g. Go and Rust) completely lack function overloading. This is by design. C++ taught the programming community and language designers a *lot* about OOP and esoteric language features. Some of it is being backed away from. (And this is by language designers likely much smarter and more experienced than us!)
Woah there buddy, down off your high horse.
Overuse of malloc instead of doing what? I think this comment needs some context because every language needs memory management, the difference with C is that you need to be explicit instead of having the details hidden. Do C++ programmers not use new? I think every C programmer will agree that global variables are a bad idea. Globals exist just as much in OOP languages as well, but they are wrapped in a class so it hides the fact that it is global, which can be more dangerous. There is a concept of 'private' and you can make variables and functions static which confines them to a file, it is a different style of design but still a very valid form of information hiding. public/protected/private relates to classes, so obviously C doesn't have them. the idea of function overloading creates it's own problems, for example having several functions the same name (and with different parameters) in different classes and not knowing which one will be called when tracing through code can become a nightmare when things don't work. Naming is a fine art, it's true and it's not easy, but I can't see how someone that will create 'interesting' names in C will suddenly be able to create magically readable names in C++. 
C is a very simple language in the way that there are not many concepts to learn, but some things are quite hard to get right (like pointers). Everything is exposed in C so you learn how to program at a very basic level, it's not that easy but it is programming at it's rawest (unless you want to use assembler). For example if you want to create a list in a higher level language there is probably already a library that does that and all you need to know are the operations that you can do on a list. Using C you have to know how to create list and do all the operations yourself. This is harder for a beginner and it can take a long time to get anything done, but I feel that if you have the mindset that you want to learn and push through it, you will become a better programmer because of it. I like C simply because it allows control at a very precise level and everything I make is faster/smaller and more interesting/fun. The main advantages though are that C is a default syntax for most languages now and once you learn C you can easily move to other languages. Also the standard libraries for C are very simple and commonly used so you spend more time actually learning programming rather than how to use a library (I'm looking at you Java!) and for some reason most examples of algorithms on the web are written in C (does anyone else find this?). Good Luck! 
Which is why it is good for beginners, most other languages are a lot more complicated than C. Some might argue that they also remove complications but from a simplicity point of view there is a cost in doing that. Rust for example has no memory management but it almost impossible to compile with all the rules. Java is simple to program but you need to know a bunch of API's to do anything. A lot of simple C programs don't use much more then scanf, printf. 
Everyone is this thread says C is a good first language because this is a C programming subreddit. Ask other subreddits, or you are just reinforcing a previously held belief. You can use asm and Python without going through C. We all learn multiple languages to various degrees. I would not advocate for becoming highly proficient in C. Reading it definitely, writing a small amount of it, sure. Fluency, not so much. High level languages have their place for lots of reasons. Many would argue that Python might not be high enough. Look at Guile scheme, or any HLL compiler/runtime, there is a ton to learn there. Don't overwhelm yourself.
that #ifdef looks like a type for #ifndef, but I'm not all that knowledgeable on the preprocessor so maybe I'm wrong?
My first language was C, but I also had quite a bit of support from professors and TA's when I was learning. If you plan to learn a language on your own, I would suggest Python as a first language. And don't let people discourage you by saying it's a toy language! I write Python for a living.
C is a fine language. It can take you a long way. Other languages are more abstract and let you do big things with a small amount of code. But you have said that you feel like you have spent most of your beginner time debugged Nguyen. Well, good. If you end up programming for a living, and end up old and farty, you will find that a lot of your ti,e is spent debugging, figuring out why some code is written in some style and just plain figuring out how to bend some old code to support a new feature. Persistence and debugging skills are important. I see questions here where the answer should be "if you put two or three prints in your code you could skip Reddit". Learning in the school of hard knocks should not be underestimated.
Why even use VLAs? why not just use a pointer?
&gt; a month or so LOL I didn't even start getting C until I was like 6 months in.
C++ is complicated as hell, learn C first, at least the basics, then move on to C++ if that's what you're trying to do.
What the hell is unl?
Xcode almost exclusively. Sometimes I'll use nano if I need to fix tabs in my makefile
Python isn't a simpler language than C, but it is much easier to use. C is very simple but has several sticking points that are terrible for beginners. The complete lack of abstraction is probably not a terrible thing for beginners. Undefined behavior, manual memory management, a standard library with multiple versions of cryptically named functions some of which bite, lexical macros, and pointer arithmetic are terrible for beginners but also things that will need to be learned to work on a systems level. You should probably try something like Go with the understanding that it is over opinionated or learn a simple assembly language like MIPS along side C.
It was just more of a discussion rather than actually considering the use of VLAs. We were curious about their implementation. 
Rust is also difficult to read due to all the terseness and symbols. I like the language, but it's difficult to get the hang of
Did you dive in pre-1.0? They've done away with many of the sigils in favor of more descriptive words. I find it no more difficult to read than C++ - easier in fact. 
Both. I've had multiple false starts with it starting about 3 years ago. Docs were always lacking, examples where hard to find, libraries were constantly changing and not stable. (looking at you gfx-rs) I do like it. The ability to include bytes into the binary is fantastic for shaders that you don't want people mucking with. One of my biggest hang ups right now is the module system and how stuff is organized. Take for instance I have a thing called `thing::renderer::Renderer` It's really annoying to have it declared under `renderer`, and it is doubly annoying to `pub use self::`, just so I can say `thing::Renderer::new()` What I miss is similar to java or c++ for that matter, where you declare the namespace of the file. I know this decision was done so that you can find the code easier, but it's still really annoying when you don't want to mash all the concerns for something in a single file.
I would say C is a better language to start if programming is what you want to do. The language itself is very simple, but also very expressive, as you use it you can also better understand how, machine works on lower level. Another benefit is that majority of current languages drive from it. Python on the other hand might be easier to start, but once you stay using its vast library, will get complex. Also switching from Python to C will be much more challenging, because things that you took for granted are no longer there. You also will try to apply way of Python thinking into C which won't work which will be frustrating. IMO Python might be good language to start when one has no interest to go into programming, but just want to learn programming to automate things.
&gt; Do C++ programmers not use new? You can get a long way without using `new`, by sticking to `make_unique`, `make_shared`, and the like. Probably recommend beginners stick to those. 
For anyone considering this, it's worth noting that BAE has a [terrible record on ethics and human rights](https://en.wikipedia.org/wiki/BAE_Systems#Criticism) (cluster bombs, torture equipment, etc), is a major supplier of arms to Saudi Arabia and is heavily implicated in the current genocide in Yemen. See also [this article](http://ethicalinvestmentedinburgh.blogspot.co.uk/2007/04/why-are-bae-systems-bad-company.html). 
NULL ist not explicitly stated to be a pointer to address 0, if I have remembered it correctly. Thus, checking for equality with NULL might be better. Edit: Ok, the standard says that it compares to 0, but may not be composed of only 0 bits. See this SO question: https://stackoverflow.com/questions/9894013/is-null-always-zero-in-c
Some contexts require you to only use stack memory. As an example, NASA engineers couldn't use dynamic allocation when programming the various components of the space shuttle. This allows the devices to run faster, as `malloc` takes a lot of cycles and radiation-hardened computers are quite slow, but also prevents memory leaks, and makes the system safer. You don't want your space shuttle's flight computer to crash because you forgot to free a pointer.
C (and x86 assembly after that) was my first proper language and it has given me a very solid base to learn higher lever languages, so I'd say yes. Plus its' fun. 
What is its name? 
Best answer :D But, you know... Actually, nobody wants to know. People prefer just to use RPG Maker or Unity without knowing what is a "pointer". This world is dead :D
 typedef unsigned long unl; It's just a shorthand. My eyes roll.
Oh, jesus christ it was literally 1 line up smh.
If you don't have a solution, get off my question.
&gt; C a good low-level language to create applications You can create desktop applications with C, sure. Take a look at [GTK+](https://en.wikipedia.org/wiki/GTK%2B) for an example of a well-supported GUI toolkit. If you want to make super-lean software for windows, using the win32 API directly is also pretty fun. &gt; should I use C++ for its more advanced features even if it costs me resource efficiency? The performance difference will be rather minor; compared to the [abominations](https://en.wikipedia.org/wiki/Electron_(software_framework\)) used nowadays the language you use doesn't really matter. C++ does give you access to a pretty good number of toolkits (e.g QT) though. Use whatever you prefer.
And those programs don't do much, either. Java is object oriented as it Rust, so you cannot compare them to each other. A lot of complex and simple programs used Perl. So what?
On a side note why are you trying to move away from OOP? You probably have a biased (and wrong) notion of OOP. As you'll probably find out, GTK+ even has it's own OOP implementation in pure C (gobject). If you really want to move away from OOP in GUIs, you should look for something called "immediate mode GUI" (or imgui). But really, Java is by no means a good example of OOP.
Totally agree with the abomination part.
It looks correct to me unless I'm seriously overlooking something. Here is a simple example that is correct. char* CreateBuffer(int size) { char* buffer = malloc(size); if (!buffer) /* error check for NULL return of malloc */ { fprintf(stderr, "failed to malloc\n"); return NULL; } return buffer; } int main(void) { char* buffer = CreateBuffer(9); free(buffer); return 0; } On a side note, for function where you want no parameters you should use "void"; [leaving it empty means something else](https://stackoverflow.com/questions/5929711/c-function-with-no-parameters-behavior). [Also sizeof(char) is always 1](https://stackoverflow.com/questions/2215445/are-there-machines-where-sizeofchar-1-or-at-least-char-bit-8), but it's not incorrect. // function that does not take any parameters int Function(void) { return 0; }
It looked correct to me too. :-( Thank you for your additional notes, I'll update my code accordingly.
Did you name a global variable "bar"? 
Not really. This variable only appears inside that function.
There's something screwy going on alright. There must be something in that "&lt;Do something with bar here&gt;" that makes clang give this warning. Something like bar = &amp;bar; or equivalent might do it. But of course that would give a compiler warning about incompatible pointer types. You are compiling with all warnings enabled, right?
I'm actually `analyze`ing right now because I want to find that error. - Other than that, I usually compile with `/Wall`. `bar` is used three times: The initial declaration, the `free()` and one additional `char* quux = bar` for some tokenizing between that. `bar` is never changed.
No weird \#defines involving `free` or `bar`? Are you sure it's this `free()` that clang is complaining about?
No `#define` containing either `free` or `bar`. Well, Clang gives me the line number with the warning which happens to be the line with the `free()`, so - at least - I hope that it is correct. :-(
Try to narrow down the problem by //commenting everything out except the malloc and free related stuff so that it looks something like: char* give_me_the_memory(void) { char* foo = malloc(9); //snprintf(foo, chunk.size, "%s", chunk.memory); //curl_easy_cleanup(curl_handle); //free(chunk.memory); //curl_global_cleanup(); return foo; } void some_other_func(void) { char *bar = give_me_the_memory(); free(bar); } -which hopefully works. Then work your way down by slowly un-commenting and compiling.
Can you provide all the code for this? Something we can compile and run?
Hmm ... OK, this is strange. char* give_me_the_memory(void) { char *foo = malloc(9); return foo; } That removes the warning. Thank you! So obviously something is fishy with `chunk.size` here - reinserting it (minus the four lines commented out in your suggestion) yells "this is not memory allocated by malloc()". `chunk.size` is defined as a `size_t` and actually increased by `+=` in the original (and my) code. Shouldn't all numbers just work?
The complete application code consists of a number of C files and uses external libraries. I provided a minimal example for good reasons. (I would, of course!)
Try printing the value of chunk.size. It might be uninitialized or something. All number should work though, negative numbers are casted to size_t which will wrap around and malloc(0) should return NULL. Also you cant initialize a variable with +=, it has to be defined with number such as zero first which could be the problem.
`chunk.size` is 5712 (reproducably) when this method is called. Weird. edit: the standard value is set to 0 because a not yet filled `curl` buffer is basically empty...? Would a NULL check work?
I'm willing to bet the problem is caused by something else in your code that you haven't posted. At the very least, give us some_other_func()'s source.
Does the malloc return a non-NULL pointer tho?
 char* ret = malloc(chunk.size); if (ret == NULL) { printf("ret = NULL!\n"); } // ... return ret; --&gt; nothing printed. It seems to...
That's not how it works around here bucko. You ask a question, the community decides to do whatever the fuck it wants with it, and you sit there like a helpless little bitch asking your parents for help.
You show us 2 functions - which line in any of these 2 functions matches the line number that your compiler warning shows you ? Ofcourse the first one that sticks out is. free(chunk.memory); and as you show no code that assigns anything to chunk.memory, so that could certainly result in a compiler warning. Other than that - please post the actual unedited code (don't change or remove **ANYTHING**) in your 2 functions - it is much easier to find errors in the actual code as opposed to not-really-the-code, where you unwittingly removed the error when you edited the code for posting. Also, make sure your some_other_func() sees the declaration of your give_me_the_memory() function - and also if there are any other warnings, they might be the actual cause of the warning in question here. 
Sorry - I did not think about that. I thought just barfing two lengthy functions with some vaguely unrelated third-party stuff into this sub won't make it easier to help me. The `chunk.memory` code is from a *third* function which can be found behind the URL provided in the initial posting. static size_t write_memory_callback(void *contents, size_t size, size_t nmemb, void *userp) { /* cURL -&gt; Memory Callback. Shamelessly borrowed from the cURL web page: https://curl.haxx.se/libcurl/c/getinmemory.html */ size_t realsize = size * nmemb; struct memstruct *mem = (struct memstruct *)userp; mem-&gt;memory = realloc(mem-&gt;memory, mem-&gt;size + realsize + 1); if (mem-&gt;memory == NULL) { /* out of memory! */ debug_print("%s\n", "not enough memory (realloc returned NULL)"); return 0; } memcpy(&amp;(mem-&gt;memory[mem-&gt;size]), contents, realsize); mem-&gt;size += realsize; mem-&gt;memory[mem-&gt;size] = 0; return realsize; } `debug_print` is a debugging macro in yet another file. Here's where the problems are: In order to help you understand what does what and why, I'd have to paste the *complete* application code here. :-/ (Sorry if this sounds rude - not intended!) The analyzer complains about the line where `free(bar);` is the only code, the compiler just ignores the memory thing (but I'd guess the issue is still there).
The primary benefit of C is that it's small enough that you can fit the entire language and its standard library in your head at once. This is also it's primary drawback: there are a lot of things that aren't built in, and you either have to build them yourself or use a third party library. 
I once ran away from C++ back to C and stayed there for years. I switched back when I realized I'd crafted a pretty feature-complete OOP system in C which was ugly as hell because all the machinery was exposed and had to be manually called, paving the way for all sorts of horrible bugs. C++ had all that functionality baked right into the language. Turns out, it wasn't C++ that was bad, it was that horrible job I had wherein they abused C++ like you wouldn't believe. Unlike Java, C++ has deterministic object destruction, which lets you safely craft some pretty complicated yet easy to maintain systems. I'd recommend C++ over C for desktop application dev work.
Ok, I'm just saying that when you do not show the code where the error might be, we cannot find it :-( I was just mentioning free(chunk.memory) as it was not clear from your question where the compiler warning you got came from (where the warning points to is very obvious to you - after all you see it on your screen - but I cannot see that screen , so it was not clear that free(bar) was where the warning pointed to.) If all your application code is inside the some_other_func() and the give_me_the_memory() , at least try to remove/comment out as much code as you can, until you are left with a minimal amount of code that still produces the error - that is easier for you to debug, and it's hopefully a small enough amount of code that you can post it here. You mention analyzer a few times also, are you showing us a normal compiler warning, or are you showing us a diagnostic from one of the clang static analyzers ? 
You can go with libiup. It's great and portable: http://webserver2.tecgraf.puc-rio.br/iup/
I posted a minimal minimal :-) example [here](https://www.reddit.com/r/C_Programming/comments/6qdln4/probably_misunderstanding_malloc_i_cant_free_a/dkwijvk/), identifying (but not finding nor fixing) the culprit. Looks like `free()` cannot handle large numbers? Hmm. &gt; You mention analyzer a few times also, are you showing us a normal compiler warning, or are you showing us a diagnostic from one of the clang static analyzers ? Sorry, I should probably have mentioned that in the OP (I'll add this information after this reply): `clang --analyze` informs me of the misuse of `free()`, `clang-cl /Wall` totally ignores it. I still decided to try and understand why.
free() works fine and it absolutely handles large numbers fine - so something else is going on - it might even be that the clang analyzer is just wrong. Firstly ofcourse, ensure that there are no warnings when you compile your code normally (without the analzyer) with the -Wall and -Wextra flags. Unfortunately, knowing that the warning is from clang --analyze changes the game - debugging it is a lot harder, and often requires one to trace through all the code to track down all the roots of the variables involved in the warning, combing over all the code that touches these variables for any potential wrongdoings... 
Enabling -Wall and -Wextra gives me a couple of warnings regarding unused variables/functions (all of which are known to me), nothing about memory problems. Does Clang have a flag to enable `malloc()`/`free()` check during compilation so I can verify that?
There are more toolkits available for C++ than for C, and even some well-respected lightweight toolkits. (FLTK springs to mind) The performance difference between C and C++ is very tiny to begin with, and absolutely negligible for a gui on modern hardware. But if you do end up deciding to go with C, I've started writing up a document about the various toolkits available. [Here's a link!](https://orangehattech.com/cgui) Let me know if it's useful at all. Oh, and don't run from OOP. It fits some classes of problems really well, to the point that I've created major aspects of it in C. It's just not a silver bullet to be thrown at every problem.
Overall, this was very insightful. What do you mean by "toolkit" in this context? Are you referring to IDE's and libraries?
I agree with what you said about it being compared to what I can only assume are higher-level languages in terms of efficiency as a low-level.
Let me be clear, then. Java for the longest time was my main language. I greatly appreciated the abstraction and OOP features it had and conceptually I was fully on board and familiar with using objects. I have come to learn that conceptualizing development into objects may prove to be more productive but less efficient at runtime. I'm not running but I'm trying to get out of the headspace that OOP is inherently a superior paradigm. I am fine with working with lower-level components which is why I thought I would try my hand at using C only.
Noted!
How do i fit the entire language and its standard library in my head?
Mostly gui libraries. There are a few general drawing libraries at the end. I'm not aware of any full IDE that provides integrated visual editing of GUI interfaces using C. There are a few items on my list that don't quite fit the term 'library', though. "XVT", for example, provides some sort of visual-editing tool in addition to its library. And "TK" provides an API to a secondary programming language, which you have to use to specify your GUI.
&gt;We need fancy GUIs that are easy to code. So they made Javascript. &gt;Javascript is great, let's use it for servers! So they made Node.js. &gt;Node.js is great, let's use it for fancy GUIs that are easy to code! ~~So they switched back to Javascript.~~ So they made Electron! I... My brain hurts and I need to go have a little lie down now.
With years of experience.
Oh, one may swear here? If not, I apologize. I proceed assuming I can. I'm here to discuss C and not to bitch with a jackass like you. I didn't intend to be frikin confrontational. I don't know what the hell is going on in your life that's makes you ramble like a petty motherfucker about a damn abbreviation. Nor do I give a fuck. I really don't. You're right "bucko". I'm opening myself to a community. That apparently includes provocative pricks, like you. Others have offered plausible answers to my frikin question and I thank them. If you're so mediocre that you say "TL;DR," well, fuck you mofo.
Yep, with decades of experience, I cannot fit all of C++ in my head at once. Constantly coming across little nuggets that I had previously forgotten.
Can confirm. Have to run three web browsers on my computer simultaneously now.
You have to use the micro sd card slot and side load it. But youll have to jailbreak your head.
Well, if you want to try a different paradigm I'd suggest you some lisp variant, for desktop applications I particularly suggest Racket. But be advised that for GUIs you'll probably won't find much outside of OOP because OOP suits very well the GUI programming needs. That said, you're welcome to try C too, but I believe you'll find that it's very cumbersome (again talking about GUIs) and not much better than what you'd get in C++ in terms of performance, without the benefits of C++. For an example of what I mean, even GTK+ heavy users like Gnome devs have invented a language called Vala that transpiles to C with Gobject so they can be a bit more productive.
In that case have you considered using C++ and a gui framework for the GUI and writing the actual meat of the program as a C library that you could then even use in a C based command line interface for the application as well,
If your platform is windows use : c# (windows forms) + pinvoke (to call a C dll) I never liked the look of QT, java or other framework's on windows. The end application just doesn't feel windows-y. C# windows forms has that default windows look and snappy feel. And with Pinvoke you can have a great looking desktop app. in few days. 
It's probably a bug in the static analyzer relating to the conversion of `0xff` to `uint8_t`. Maybe you could experiment with other initializers and structs to find out exactly what it does and doesn't like here.
Among the others, an use case would be to change the process name.
This example was actually taken from a webinar for a static analysis tool. I stared at the slide for a while since I didn't find anything about the first message and thought if anyone here might see something. Maybe the presenter forgot to change the code before the presentation. 
How Can Mirrors Be Real If Our Eyes Aren't Real?
Why bother when you can't help?
I'd be happy to help if you asked an actual question.
I'd be glad if you would bother to read.
I would if you would bother to write. See, I can keep this nonsense going all day. But wouldn't you rather just rephrase your question so people have a chance to understand what you're talking about?
Yeah I think there's some missing information. 8-bits to 7-bits could imply `uint8_t` is a signed value which won't store the value `0xff` as the unsigned value it's presented as. Maybe they changed their stdint.h typedef from `unsigned char` to `signed char` and forgot to tell everyone, or changed their example from `char` to `uint8_t` and didn't run it through the static analysis tool again.
UIs are better suited to higher level languages. Perhaps you can create a DSL in C and just use C for the primitives.
I'm developing exclusively for Linux.
I need the power of a low-level language.
Which functions are you thinking of?
Yes, but the logic part is 0.01% of the running time. What takes time is doing things like building a window and all its components. The logic is hard to do in a verbose low level language. Small DSLs are super fast. If you need more speed you can compile them to a bytecode. See how SQLite does it with it's VM. But for desktop apps you don't need one, just a good API you call from the yacc or whatever grammar. The compiler generates all the boring C code for you.
I think he's talking about putchar
Yep I would definitely play around with the `uint8_t` assignment and try to see exactly what's happening there. Also, this unaligned structure makes me uncomfortable.
Unrelated to your question, but I noticed this: fgets(networkStatus, 3, fptr); /* Somehow 'up' is three characters. */ In C, strings need to be terminated with the null terminator, or the character `\0`. This tells the string functions that the end of the string has been reached. Since C programs have access to direct memory, without the null terminator, string functions could go into memory outside of the scope of the variable. This is mostly necessary because there is no "string" type in C, only arrays of characters. So if your string contains "up", it looks like this: networkStatus[0] = 'u'; networkStatus[1] = 'p'; networkStatus[2] = '\0'; Note that single characters are declared with single quotes. If you use double quotes in C, the \0 is automatically appended to the end; e.g. the declaration: char networkStatus[3] = "up"; will automatically append the null terminator to the array. This is allowed in declaration, but you need to use `strcpy` to do this kind of statement after declaration. Here's an "Engineer Man" video that covers characters and character arrays pretty well for beginners: https://www.youtube.com/watch?v=90gFFdzuZMw 
We don't need those (could use `fread()` and `fwrite()` instead), but they are just so damn useful. On the other hand, `getwc()` and `fgetwc()` are rather difficult to implement manually.
I'll second this. There are good reasons for some people to prefer either C or C++ over the other, but performance really isn't one of them. And if you're looking for a different programming style, C is probably not going to offer anything substantially different than Java. Racket will be different, but depending on your requirements, may not be performant enough (but give it a shot!) You might also check out Rust, which some people might call OOP but is probably OOP in a way you wouldn't recognize if you are coming from Java. If it had a better UI story, I would recommend Haskell just for being *totally out there*. If you're looking for a different paradigm, it's hard to be any more different.
That is very helpful. Thank you. I've bookmarked your comment. EDIT: Your username does *not* checkout!
That would actually be the best thing to do from a software engineering standpoint. You are forced to decouple the "business logic" from the UI.
Try calling wine with the full path of the wine binary (e.g. /usr/local/bin/wine)
&gt;even if I give the full path of wine executable, Eclipse can't find it
For when you want to handle single characters instead of strings. I once hand wrote a SLR parser and my lexical analyiser was entirely based on `getc()` and `putc()` as a state machine.
getch() getchar() getche()
What the hell are you talking about dude?
Yeah, keep spewing garbage from your mouth all day instead of helping others, you condescending motherfucker.
Are you talking about getchar(), friend?
So, is getchar() useless when compared to string handling functions?
getchar() getche() getch() Are they useless when compared to string handling functions? Do they hinder execution speed in any way; e.g. by bottlenecking the input as buffer memory of the keyboard can usually hold a single character , or at least they initially could in the past?
&gt; So, is getchar() useless when compared to string handling functions? It isn't a matter of being useless or not it is a matter of if you want to read in a single `char` or a null terminated array of `char`. Incidentally, I tend not to use the standard library string handling functions for reading in user input - buffer over flows and all that.
Only when you replace your eyes with mirrors will you be able to see.
That has been very helpful, thanks. What do you think would make the functions I listed, getchar() particularly, obsolete?
Well, `getch()` and `getche()` are not standard library functions, they come from an MS-DOS library. `getchar()` on the other hand is part of the standard library, therefore a reliable function. I would not classify `getchar()` as obsolete.
This just might be the answer I was looking for. Thank you!
Nope, I don't get it, but then I don't routinely work on systems where read() is used to access hardware devices. That assignment to buffer[0] looks to be useless, unless DEVICE_STATUS_REGISTER_ADDRESS is a pointer and the read does something. Beyond that, it looks like it returns the second word of whatever read() returns.
Sounds like premature optimization but ok
Your question doesn't make much sense with respect to string handling functions. `getchar()` (and company) deal with I/O and have nothing to do with functions like `strcmp()` or `strtok()`. As for efficiency, they don't effect it too much, but that really depends upon what you are attempting to do. If you are using `getchar()` in a loop to read a line of text, well, a function for that already exists (`fgets()`), but if you are filtering textual input, say, changing all control characters in a textfile to spaces, then using `getchar()` for that is fine. Also note that the C I/O model is buffered by default. Typically (but not always---check your system documentation) true files get full buffering (BUFSIZ number of characters are read in at once, then doled out depending upon the call; for output, BUFSIZ number of characters are buffered before being written out), character I/O devices (like the keyboard) get line buffering (similar to full buffering, but output is written when a newline character is seen). You can change this using `setvbuf()` but generally, if you have to ask if you should, you probably shouldn't, as in general, this does speed up the operation of the code (fewer calls into the operating system mainly). 
MSVC will add the few remaining C11 features when they're done with C++ support (which considering the number of standards recently, will be never but whateve)
But this is much harder to read, and unlike this subreddit's circlejerk, is frowned upon.
in Clang anyway, NULL is defined as being equal to a void pointer to 0, not just 0.
Or just use clang lol.
Ick, are they calling copy_from_user in the character driver read to save a write syscall for the address?
I would imagine that someone without a clue of how things should work wrote a kernel module for their obscure hardware, and instead of doing things normally they implemented a harebrained scheme where the read() syscall peeks at the first word of the buffer to get the desired register. 
Exactly correct.
We have a winnah! Device driver has access to the user's address space, and so is reading *from* the user's buffer to get the address, before writing the results *to* the buffer. This works for a kernel-level driver if the operating system supports it. It does *not* work for a user-space driver, which is what I'm writing.
Many have tried to come up with a replacement for make that wasn't even worse. None have succeeded. imake, xmkmf, build, ant, msbuild, config, autoconf, many others whose names I've forgotten. Many many years ago, I attended a lecture by the guy who invented make. it started out as an exercise learning how to use lex and yacc. After he got it working, he shared it with his co-workers. By the time he realized the syntax was a mess and it needed re-writing it was too late because he already had a user base of over a dozen users.
TIL. Never seen anyone do this. Thanks for posting it.
&gt;they implemented a harebrained scheme Grrrrr. &gt; this is working production code that's been in use for a decade. Well, that's actually a longer successful track record than a lot of hacks I have to live with! 
Among others, yes.
Then something else is wrong. Is your script marked executable?
ninja exists, but they even suggest letting cmake handle building the files
i love shit like this
C has no concept of a keyboard, and there is no `getch()` or `getche()` in the standard C library. The `getchar()`, `getc()`, `fgetc()`, `gets()`, `fgets()`, `fread()`, etc. functions operate on streams, which are usually buffered. The character functions pull one character at a time from the buffer. When the buffer is empty, they invoke some sort of behind-the-scenes magic to refill it; the precise nature of that magic depends on the C library, the operating system, and what the stream is connected to. In some cases, the function call will block until more data is available or an error occurs. If the buffer could not be filled, the function returns `EOF` and either `feof()` or `ferror()` will return true, depending on the reason why the buffer could not be filled.
Yes.
Very weird. What error message do you get exactly? What does the script look like?
Why is LGPL licensed software taken as a negative?
Can be defensible on systems where every byte counts. 
Look up the RAII pattern. It's something hard to do in garbage-collected languages, but with deterministic object destruction in C++ you can eliminate entire categories of bugs by making each of your classes as const and possible and set up *everything* in the constructor. If you follow this rule of thumb, you ensure that anytime you have an instance of a class, it's guaranteed to be completely initialized and ready to go. Things you would normally have to unit test for are now a non-issue.
Not really relevant to this subreddit, but it's a sentence using every letter of an alphabet at least once.
True. Pretty much anything is defensible in those conditions.
Just to be clear, I'm not endorsing this madness. There's a saying: if it's a dumb idea, but it works, then it wasn't a dumb idea. I no longer believe that saying. This was a dumb idea that worked until we had to port to an O/S that doesn't support it. Now, not just the device driver has to be re-written.
Well, it does make a good story to tell around the campfire, to frighten the junior engineers.
Sadly, that's true. I've worked on systems where the entire operating system didn't live as long as this hack.
You can find the English version for about $15 - $20 if you search on AbeBooks (usually cheap reprinted copies of the original).
This is a cool one, but I'm getting errors when compiling it: gcc -Wall snake.c snake.c:1:10: fatal error: graphics.h: No such file or directory #include &lt;graphics.h&gt; ^~~~~~~~~~~~ compilation terminated. 
&gt; set up everything in the constructor Just be aware that if your constructor throws an exception, the destructor won't be called so you can leak memory unless you are careful. Obviously, if your program crashes (when you don't catch the exception) a modern OS will clean up after you.
Which is why it is handy to do dynamic allocations into `std::unique_ptr` and `std::shared_ptr` so you don't have to crawl through the horrors involved in making that sort of thing exception-safe on your own. Resource handling (files, DB connects, etc...) can also all be wrapped in their own little classes that do proper cleanup for exceptions during construction in the same way, keeping the insanity to a minimum.
&gt;#include &lt;graphics.h&gt; What kind of arcane does this program rely on?
Windows only OP, come on. 
No blog spam please.
No blog spam please.
Because if someone *cares* if software is LGPL, it's usually a negative in their eyes - No static compilation with closed source software. MIT/BSD/public domain/etc licensed code can be used in places that LGPL/GPL'd code can't.
Probably that it enters the loop. If it were `0`, it would never enter the while-loop, and thus never executing `print_nodes`. However, as far as I can see, you could just have done `while(print_notes(...));`. `print_notes` is executes at least once and is getting executed (while determining if it should run the empty code), until it returns 0.
Snip and updated with more whitespace: int printing = 1; while(printing) printing = print_notes(fd, userid, searchstring); The `while` loop continually evaluates its arguments and, if they evaluate to 'true', runs the statement inside the loop block until the arguments are 'false' In C, any non-zero integer evaluates to 'true' and a value of 0 evaluates to 'false' Your initial definition is setting the `printing` variable so that the loop will be run at least one time. A similar effect could be achieved with int printing; do { printing = print_notes(fd, userid, searchstring); } while(printing) Within the loop, the `printing` variable is updated with the return value from the function call. Once the function returns 0 (we don't know why this would happen without seeing the code or documentation for that function), the loop will end and execution will continue with the next `printf` statement. 
Thanks man!!! Makes sense now.
Thank you, and this was source code from a book, lol. 
https://msdn.microsoft.com/en-us/library/e2h7fzkw.aspx Visual Studio is a good choice and has a ton of documentation supporting it!
/r/cpp_questions would probably be a good place to start. &gt; maybe also C#? Don't. Just... don't.
I haven't done much driver work, but I'm curious - so what exactly is the descriptor `devFd` pointing to?
AFAIK, GCC 7 defaults to -std=c11, maybe this is the reason your original Makefile worked for you initially without specifying the standard ?
So the read call seeks buffer[0] bytes forward into the opened file descriptor? I'm sorry for the stupid question but I had to read up on half the words in that code and kind of want to be sure.
If you have a student ID, JetBrains offers for free the amazing CLion IDE which works for C and C++ (even more with plugins) I once picked up a C# book to see if it really was as trash as people mentioned. If you're forced to program for Windows targets only, then its the usual Microsoft crap. But if you want your programs to be portable, avoid it like the rotten corpse of a parasite-riddled neo-nazi summoned from hell by Satan himself. To sum up: **C:** Clion, Code::Blocks, Visual Studio **C++:** Clion, Code::Blocks, Visual Studio **C#:** Visual Studio Note that should you follow the arcane path and choose C#, JetBrains offers lots of tools you might be interested in (like ReSharper) but I never used them myself. 
Visual Studio is Windows only.
The while gets true with 1 and that's why it works. I recommend you to read some information about it, which you can receive from the web. Knowing more will let prevent your code from errors. Of course, you can always use some programs like checkamrx or others to help you detect errors within your code but it is recommended to it also on your own. Good luck.
My bad! It wasn't specified so I assumed he was windows.
As a new to c++ I would recommend you to learn as much as you can about it as it can sometimes be a bit complex. This complexity might lead your code to suffer from and those errors should be fixed quickly. There are programs who helps detecting those errors, like checkamrx or others, but I recommend to learn how to find them on your own as it helps you get better at programming. Good luck!
I've gone through all of Solo Learns C++ tutorial, so I have an idea
It's simply the integer file descriptor value that was returned from the open() syscall. In Unix (which includes Linux, BSD, and MacOS), you access a device like this: int devFd = open("/dev/mydevicename", O_RDWR); // send data to device write(devFd, buffer, len); // receive data from device read(devFd, buffer, len); // anything trickier than that: ioctl(devFd, IOCTL_MYDEVICE_COMMAND, data); So for example, if I needed to read the status register, and the device driver supported that operation, I might do: uint32_t status; ioctl(devFd, IOCTL_MYDEVICE_READ_STATUS, &amp;status); or struct DeviceCommand cmd; cmd.request = MYDEVICE_GET_STATUS; ioctl(devFd, IOCTL_MYDEVICE_READ_REGISTER, &amp;cmd); status = cmd.value; 
EOF submits the current line to `getchar` without sending a newline (`\n`) as would happen when you hit Return. If the current line is empty, then `getchar` receives an ~~actual EOF character~~ **EDIT:** There is no actual EOF character. I meant `EOT` (End Of Transmission, ASCII code 4), which gets translated to the EOF macro inside `stdio` by `read`. [Source](https://unix.stackexchange.com/questions/110240/why-does-ctrl-d-eof-exit-the-shell)
Oh, it literally was a device in `/dev/`. I feel a bit ashamed - I love *nix, and I run Fedora when I get the opportunity, and I totally blanked on this. That said, though, I haven't actually come across sending commands/receiving data from devices using `/dev/*` in C/C++ and this looks pretty interesting. If I wanted to try a hand at digging through some driver code, what would be a good place to poke around to see more of this kind of stuff? I'm comfortable with C/C++, but I just haven't done a whole lot of low-level IO of this sort.
To be totally fair, it's a great way to put together apps and have a great level of accessibility for developers who would like to contribute. And layout with HTML (at least I've found) in the context of Electron feels so much easier. That said, though... there's still a clear performance problem. It keeps getting better, but it's still there. It's a far cry from writing an application in C/C++ or Rust.
I found the first half of [this article](http://freesoftwaremagazine.com/articles/drivers_linux/) and [also this series](http://opensourceforu.com/tag/linux-device-drivers-series/page/2/) pretty useful for getting started with simple character drivers. Also, there are usually a number of loadable kernel modules that you can unload, modify, and load back on the fly without rebooting. Take [input_leds](http://elixir.free-electrons.com/linux/latest/source/drivers/input/input-leds.c), for example. This places a number of controllable LEDs, like capslock and numlock, under `/sys/class/leds`. Just remember that there's no one to catch your segfaults or memory leaks... unless you start using QEMU or [some wrapper](https://github.com/vincentbernat/eudyptula-boot) so that you can safely escape. There are lots of other [resources](https://kernelnewbies.org/Drivers) and [challenges](http://eudyptula-challenge.org/) out there to dabble in and explore.
Which character is the EOF character exactly? This is something that's been gnawing at me for a little bit too long.
I believe it changes based on the system. Glibc says that its -1 for them though. [Source] (https://en.m.wikipedia.org/wiki/End-of-file)
**End-of-file** In computing, end-of-file (commonly abbreviated EOF) is a condition in a computer operating system where no more data can be read from a data source. The data source is usually called a file or stream. In the C Standard Library, the character reading functions such as getchar return a value equal to the symbolic value (macro) EOF to indicate that an end-of-file condition has occurred. The actual value of EOF is system-dependent (but is commonly -1, such as in glibc) and is distinct from all valid character codes. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/C_Programming/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.24
Non-Mobile link: https://en.wikipedia.org/wiki/End-of-file *** ^HelperBot ^v1.1 ^/r/HelperBot_ ^I ^am ^a ^bot. ^Please ^message ^/u/swim1929 ^with ^any ^feedback ^and/or ^hate. ^Counter: ^96612
It isn't a character. Note that `getchar` returns an int, not a char, and int is a wider type than char. EOF is an integer guaranteed to be outside the range of char, so there's never an ambiguity as to whether getchar returned EOF because of an end-of-file condition or because there was an "EOF character" (whatever that would hypothetically be) in the stream being read. (Note that this is distinct from the concept of an actual EOF marker used in some ancient or rare filesystems.) When you press Ctrl-D on a terminal, at least on Unixy systems, the terminal driver intercepts that character/keystroke and simulates a "file closed" on the terminal, causing the next `read` to return EOF. This processing can be disabled and the character used to invoke this behavior can be modified as well using the various `termios` functions. 
See my edit, I was incorrect originally.
The read() system call winds up calling a handler in the device driver which obtains data from the device, and writes it to the user-provided buffer. Each device driver implements its own handler for the read() system call. This particular driver, having access to the user application's address space, pulled one word of data *from* the user provided buffer, used it to decide what to do with the device, then read a word of data from the device, and then wrote the data *to* the user provided buffer.
It has to do with your environment, not with EOF. Everything is a [FILE](https://en.wikipedia.org/wiki/C_file_input/output) stream. Unless you make it otherwise (terminal voodoo), they're all line buffered. That means they wait until you hit enter for anything to happen. If you haven't typed anything at all, there's nothing to wait for.
**C file input/output** The C programming language provides many standard library functions for file input and output. These functions make up the bulk of the C standard library header &lt;stdio.h&gt;. The functionality descends from a "portable I/O package" written by Mike Lesk at Bell Labs in the early 1970s, and officially became part of the Unix operating system in Version 7. The I/O functionality of C is fairly low-level by modern standards; C abstracts all file operations into operations on streams of bytes, which may be "input streams" or "output streams". *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/C_Programming/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.24
This is not the reason.
The control character \^D instructs the terminal driver to send of anything it has in its internal buffer (this is independent from stdio) to the user. If its buffer is empty, this causes 0 bytes to be sent, interpreted as EOF. By default, the buffer is sent automatically on a newline, which is why you see the effect that you have to send \^D once when at the beginning of the line, twice otherwise. 
Yes it is. Input streams are line buffered. If you haven't typed anything, the stream is empty.
Yes, stdio streams can be line buffered, but this is not tge reason why you have to type \^D twice. See my answer.
Your answer is my answer. I say it has to do with your environment, not with C (EOF): &gt; It has to do with your environment, not with EOF. You say it has to do with your terminal (environment): &gt;The control character ^D instructs the terminal driver to send of anything it has in its internal buffer (this is independent from stdio) to the user. I say if you haven't typed anything, you have nothing to wait for: &gt; If you haven't typed anything at all, there's nothing to wait for. You say the same thing: &gt; If its buffer is empty, this causes 0 bytes to be sent, interpreted as EOF. I say it's line buffered: &gt; Unless you make it otherwise (terminal voodoo), they're all line buffered. You say it's line buffered: &gt; By default, the buffer is sent automatically on a newline, You need to actually understand what it is you're saying when you say something.
I am talking about the buffering inside the terminal driver which is different from stdio (FILE) buffering. Try turning off stdio buffering with setbuf, you'll see that the behaviour remains. Your answer tries to say that this behavior is related to stdio streams being line buffered which is just plain wrong.
Yes, something like that: my compiler is $ gcc --version gcc (Debian 6.3.0-18) 6.3.0 20170516 and its default standard is `-std=gnu11` which should be "C11 with GNU extensions" if I understood [the docs](https://gcc.gnu.org/onlinedocs/gcc-6.3.0/gcc/Standards.html#Standards) correctly.
How/where did you learn this stuff, man? It is very impressive.
https://latedev.wordpress.com/2012/12/04/all-about-eof/
Your post has been removed as it is off topic. This is a c subreddit.
You'd have to define what better is. If a smaller binary size is desired, you could run tests. If speed matters you could run tests. But in any case that would just tell you about that application. A few reasons some binaries can be bigger... inlining code, unrolling loops, unrolling recursion, and these are all done to make the code run faster.
What happens when you send gcc -OS, the size optimization flag?
That should be -Os ...lower case S, in case you have problems trying it. You also need to make sure you haven't passed -g as a large proportion of the binary could just be debug symbols. To get the actual size of the resulting code you can run `size path/to/binary` in a command prompt on linux. But ultimately, no, a smaller binary size does not mean the compiler is better.
&gt; But ultimately, no, a smaller binary size does not mean the compiler is better. I think that's debatable, *if* it's a situation where "all other factors being equal"... one compiler produces smaller output than another. That is to say, both run in equal time and use equal resources (e.g. memory footprint) and produce equally correct results. In that situation, I believe it's fair to say that producing smaller code is objectively better than consuming more of a finite resource in the form of disk space. However, as I'm sure you were intending to say, all other factors are never equal, and there is often a tradeoff between size and speed that means that size alone is insufficient to judge comparative quality.
Yes, exactly what you said! The reason I was lazy and just said no was that I realised it's not even just a code size / speed / memory question. The size of a binary includes debug symbols, initialization data, code etc. The compiler I'm using now puts checksums, timestamps and other meta data in the binary that makes no difference to how the code runs but does make the binary bigger. So measuring the disk usage of two binaries is unlikely to be what you really want to compare. 
Perhaps a better question would be what compilers do and how they might achieve different sizes and runtimes all together. It took me 4 months in a computer architecture course for ot to click.
&gt; I am talking about the buffering inside the terminal driver Try turning it off by using something like *ncurses*. My point was correct: It's the environment, it's not EOF that's at issue.
You said something about line buffered (stdio, FILE) streams which is completely misleading. Your answer is wrong, don't try to retcon it.
Yes, you can, it is part of the [SDL library](https://www.libsdl.org/), which is AFAIK also written in C. This means you need to have their header files (to get the declaration of `SDL_PollEvent`) and their library file you can link against.
Yes, you should be able to. I have some code [on github ](https://github.com/jsburke/Tetris)that uses it. It's somewhere around line 140 in TETRIS.c that I call it. That said, I wrote this code a while ago when I was first playing around with SDL, so I wouldn't use my code a good reference on how to use it. Several things are really hacky, like loading multiple images rather than loading one and clipping it. Also, I fixed this flying on a plane the other day cause I needed to kill some time, getting this current project to run correctly on Unix systems needs a few fixes. Possibly the make file, and also changing my close() function to another name because it overlapped with some unix or posix thing (sorry, I forget the exact details). But it is, technically, working SDL code in C. Just don't let this be a secure reference thing. 
It looks like you're trying to format a word into a link. Try this instead: &gt; \[on github](https://github.com/jsburke/Tetris) Result: [on github](https://github.com/jsburke/Tetris) *** ^^Please ^^note: ^^Edits ^^won't ^^appear ^^visible ^^if ^^made ^^soon ^^after ^^posting. ^^| ^^I'm ^^a ^^bot, ^^beep ^^boop ^^| ^^Downvote ^^to ^^DELETE. ^^| [^^Contact ^^me](http://www.reddit.com/message/compose/?to=LinkFixBot&amp;subject=Feedback) ^^| ^^[Opt-out](http://www.reddit.com/message/compose/?to=LinkFixBot&amp;subject=Opt+Out&amp;message=name+of+Subreddit+or+user+Opting+out:) ^^|
Why would you not? It's like asking if you can change your car's wheels
Because, i've seen someone using it in C++
C libs are often compatible with C++.
It depends on the optimizations level, you can optimize for size or for performance, for example if you optimize for performance the compiler could unroll some loops and that increases the binary size. A bigger binary doesn't mean that the program is not well optimized, but often the contrary. 
I second this. IUP is great. I was about to give up searching for a GUI library for plain C until I found it. I tried GTK but. . . 
If its just a few allocations then clean up using free() is not an issue. If you require a lot of allocations where keeping track of them is troublesome then you are better off just requesting a big piece of memory and handing it out yourself. I don't think the middle ground of using malloc for lots of small allocations and keeping track of it all is ever a good idea. If you are at the point where keeping track of it all becomes problematic then you are at the point where your custom allocator is a better solution.
gcc 6.3.0 is giving me a smaller result than clang 3.8.1, though only barely: $ clang -Os -s Feynman.c -lm -lglut -lGL -pthread $ ls -l a.out -rwxr-xr-x 1 skeeto skeeto 18928 Aug 1 23:10 a.out $ gcc -Os -s Feynman.c -lm -lglut -lGL -pthread $ ls -l a.out -rwxr-xr-x 1 skeeto skeeto 18880 Aug 1 23:10 a.out That size difference isn't going to be relevant.
Can you compare "-O2 -march=native" for both?
It is buffered. Your answer is incomplete, as was mine. ~~However, mine is actually technically correct, because~~ the first thing I said was that it was the environment, not EOF that was the issue. There's a reason you can't shouldn't ncurses IO with standard library IO. For those who don't know, 'ncurses' is a library for making the terminal work how you want it to -- one of those things is to make it stop buffering everything and to make it register individual keystrokes, rather than buffering it. However, you will still find out that using getchar in the middle of ncurses IO probably isn't going to give you the behavior you expect. edit - I'll stop trying to be right here. It wasn't accurate enough, but it wasn't wrong. It is because it's buffered. Who is doing the buffering isn't really important. The description was correct: If you haven't typed anything, there's nothing in the buffer, so it works immediately. 
my gcc is version 7.1.1 and clang 4.0.1
The difference was so small that the ELFs were exactly the same size regardless of the different compiler switches, so here's the text section measured directly: $ clang -Os -s -march=native Feynman.c -lm -lglut -lGL -pthread $ objcopy -O binary --only-section=.text a.out /dev/stdout | wc -c 7074 $ clang -O2 -s -march=native Feynman.c -lm -lglut -lGL -pthread $ objcopy -O binary --only-section=.text a.out /dev/stdout | wc -c 9106 $ gcc -Os -s -march=native Feynman.c -lm -lglut -lGL -pthread $ objcopy -O binary --only-section=.text a.out /dev/stdout | wc -c 6706 $ gcc -O2 -s -march=native Feynman.c -lm -lglut -lGL -pthread $ objcopy -O binary --only-section=.text a.out /dev/stdout | wc -c 8258 
Using void \* is generally a good idea to avoid, as you lose your type checking. I simply name my functions with new, if it returns an alloc'ed object, and include a free... Name_new Name_free, but generally prefer an init. Name_init(type\* self) so the caller may manage the memory as heap or stack. These may require dynamic memory as well, and have a free function.
You shouldn't read the entire file into memory. It limits the input size that this program can process and is an inefficient use of resources. `wc` can trivially do its job in a single pass. In fact, you can get away with nothing more than `fgetc()` (or, better, `fgetc_unlocked()`) and process an arbitrarily large file with a constant amount of memory. You should support reading from standard input like any good unix filter. Your `check_file()` is pointless since the situation could change by the time you try to open the file. Just try to open it and handle the error if it doesn't work. Use `isspace()` rather than checking specific whitespace characters (except newline when counting lines). You depend on buf being zero-terminated, but that's just dumb luck that `malloc()` is doing this for you (uninitialized memory). It also means you'll miscount a file containing a zero byte. Prefer getopt to argp. The former is standardized and the latter isn't. Overall you've over-engineered this thing. There's too much dancing around for how little it does. That does serve as reasonable practice for a larger program that requires more complexity, but it's overkill for this little program. 
Why are you trying to fit a square peg in a round hole? You've already said you don't want to solve your problem like that. My advice: go learn CMake. It's incredibly well documented. Something as simple as your template will be a breeze for you to figure out.
Thanks for the feedback. I'll look through this and try to fix things when I get a chance.
The compiler is assuming that the value will be non-zero and is optimizing the code. It should be giving you a warning that it's doing so. Try this: compile your code as such: gcc main.c -O0 -g -o main.o -c Then look at the output with objdump -dS -M intel ./main.o After that make a new file defining the functions and compile it just as before gcc ext.c -O0 -g -o ext.o -c Then link it all together gcc main.o ext.o -g -o main If you run the same objdump command as above on ./main you'll see the references to the function change from 0 to a real address after the linking phase. But the conditionals to the function will be hard-coded to the 'true' path since that was already done at compile time in the first command.
thank you for the excellent response / guide. are there any flags who disable this optimization?
What is a x-executable?
The .x file (or a file with no extension) which runs like an executable on Ubuntu. If I add GUI, it will run when I click it and if it has only CLI, I like run it in the terminal: ./application.x
What's a BitBuffer? (C is about as minimalist as you can get; structs, functions, terse syntax and the stdlib. There's no such thing as BitBuffer. It's not Java.) What are you trying to do?
Are you coding on a Raspberry Pi or similar single card computer? Your question might be better suited for the /r/embedded subreddit 
It's a struct type it's really simple that i've made i'm not sure why this matters? It should be fairly obvious what it is from the name. Anyway. The question is how the shit do i write code to automatically fread/fwrite a buffer without having to manually check the amount of space left/used? Do i just need to poll it each time it's accessed manually?
&gt; Who is doing the buffering isn't really important. Who is doing the buffering is the most important thing thing about all of this. If this was dependent on stdio buffering (which it isn't), then behaviour would differ if you used `read` and `write` directly. But that's not the case because the relevant buffering (and \^D interpretation) happens in the terminal driver!
&gt; Do i just need to poll it each time it's accessed manually? Basically, yes. Interrupts (resp. signals) are really slow for a user programs, I can't recommend to use them.
User programs can't use interrupts. The only thing you can use are *signals* which you can send manually or have the operating system send you in case of certain events, like accessing an unmapped page. Note that signals mean a roundtrip through the kernel space, so firing a signal takes at least 1000 cycles on normal hardware. The only thing you could really use signals for in this case is catching page faults, i.e. when a byte is accessed that is on an unmapped page or on a page with insufficient permissions. You could potentially use this to check if the buffer is empty by carefully aligning it around a page border, but I can't tell you how without knowing more about your data structure and it's in general a bit tricky to get this right. I recommend you to just check every time. The check is almost free.
I have literally never heard about the extension `.x`. The file format is called ELF, binaries have no file extension on UNIX.
Can you show us your source code and how you compiled it? What do you try to output? What is the architecture of the two machines?
It is undefined when the compiler increments `j`. The only thing guaranteed by the standard is that `j` is incremented at some point before `x[++j]` is computed. Thus, in `x[j] &amp; 0x3` is is undefined if `j` has sthe old or the new value, rendering the whole construct undefined.
Yes, `-O0`. Generally, don't do this kind of thing.
The compiler is allowed to assume that functions do not resolve to `NULL` pointers. If you use the linker to break this assumption, weird things happen as you saw in your code. If you want to find out if a symbol can be resolved, better use `dlsym`: void (*func0_ptr)(void) = dlsym(NULL, "func0"); printf("func0_ptr = %p\n", func0_ptr); printf("func0_ptr -&gt; %s\n\n", func0_ptr ? "true" : "false" );
I just checked using "readelf". It says it's an ELF64. So here is the issue: how can I create an executable "application" in my machine, by compiling from a .c file, such that executable "application" can run on machines other than my computer (basically like the Telegram app for Ubuntu; you click and it runs; or you do ./application in terminal - and it runs). P.S. I am not a programmer, so I am not that experienced.
I compiled the source code using "gcc -m64 -o application sourcecode.c". ("application" is the name of the executable) I have a little problem revealing the source code here. Tried on 3 machines by interchanging the compiling locations: Ubuntu 16.04.2 (on a 64 bit); Xubuntu 16.04.2 (on a 64 bit); WSL for Windows 10 (on a 64 bit). Compiles on one, runs on it, and fails to run on others. 
In that last expression, you're both using and modifying `j` without a sequence point in between. The order of evaluation between sequence points is unspecified, so the expression is ambiguous. As for your encoder, there are many ways to do it; yours is different from but no worse than others I've seen (or written). It would be more readable if you shifted your input into a temporary variable, and you'd save a line by consistently using `i++` instead of `++i`. Also, there is no need for `t` or `xmod` since by repeatedly subtracting 3 from the size you are effectively computing the remainder of its division by 3. Finally, your `code` array should be static.
Usually, a binary you compile on one Linux computer should work on another one with a similar configuration. What are you trying to output? Does your output contain any non-ASCII characters (e.g. Chinese characters)? This might cause the issues you describe. If you don't want to give me the source code, it is very hard to give further help. You could try to run the program under strace (type `strace program` instead of `program`) both on the working and on the failing system and give me both outputs. You could also run `ldd` on the program and tell me what it says. Best would be if you tried to reduce the program to as little code as possible as needed to reproduce the problem and then post that.
Really weird. Normally, this should work. See my other comment for some suggestions.
Ubuntu and Windows are two completely different operating system. A program compiled on one of them won't run on the other without some sort of emulator. For practical purposes, Ubuntu and Xubuntu are the same thing. If a program compiled on one doesn't run on the other, something must have gone wrong when you transferred the file.
Your comment got caught in our spam filter. I apologize for the inconvenience.
Linux programs can run on Windows without recompilation using WSL (Windows Services for Linux).
Yes. The binary contains characters similar to one you see in AES encrypted output files. I couldn't give you the source code for the original one, but we can consider a simpler executable. I just wrote a script for factorial; it works. #!/bin/bash seq -s "*" 1 $1 |bc So I make a .C of the script with shc; and I compile the resultant "factorial.sh.x.c" to "factorial". gcc -m64 -o factorial factorial.sh.x.c It runs on the machine I prepared it. And on the other it shows the AES output-like characters. Please excuse me if my terminology is not right. Also the original program is a bash script, converted to .c with shc, like this one. 
Why do you use `shc`? That's exceptionally weird.
I know Ubuntu and Windows are different. It's on WSL - Windows Subsystem for Linux, which is basically The Ubuntu 16.04.2 minus the GUI. So everything for Ubuntu, pretty much runs here (except ones which require GUI). The error part: I have three nachines A, B and C. I compile on A, send to B and C. And compile on B, C and do the same. Results also stay the same: runs at the origin machine, fails on the others. 
So basically, `shc` compiled scripts do not work on other computers unless you compile them with `-r`. Please always read the manual of tools you use. I highly recommend you to ditch `shc` asap. It doesn't “protect” anything, the original shell script can be reconstructed easily. It doesn't make your shell script any better. It causes issues like the ones you had above.
I don't know c. And the script is pretty big, and I have to protect the script as well. What do you suggest? 
You can't “protect” shell scripts. That's not possible. Just ship the shell script as you wrote it. Everybody who tells you that they can “protect” your shell scripts is a phony liar. Programs like `shc` work by decrypting your script and then passing it to the shell. This is very easy to intercept, everybody and their mom can find out what the script contains and modify it.
Thank you so much. I'll make changes. Also, how are scripts intercepted? It would help to know, I'm new in the arena.
Ah, thanks. I hadn't considered that the first j was the one causing the problems. There go my dreams of cramming even more operations into a single line, I suppose. 
Basically, you run the script in a modified environment so that the program the “protected” script tries to execute is actually a program that dumps the script it receives to a file. Or you stop the “protected” script at just the right time and then read the decrypted script from its RAM. Or you stop the shell invoced by the “protector” and read the script off its argument vector. Many attacks are possible, this is nothing but snake oil. For shc, there is [this program](https://github.com/yanncam/UnSHc) doing all of this for you automatically.
Well, you could do something like basestr[++i]=code[((x[j] &amp; 0x3)&lt;&lt;4)|((x[j] &amp; 0xf0)&gt;&gt;4)], j++;
Thank you. I am all about saving lines. Oh! I get it. Yes, nixing `t` and `xmod` is easy. I was clearly over thinking it. I was trying to come up with an elegant way of handling the end cases, but in the end, this seemed as good as anything else I thought of. No more `xmod` will put me a bit more at ease.
Some times, gcc decides to unroll loops clang doesn't want to unroll. This can explain such size discrepancies.
Excellent. Looks like it needs to be: basestr[++i]=code[((x[j] &amp; 0x3)&lt;&lt;4)|((x[j+1] &amp; 0xf0)&gt;&gt;4)],++j; If they ever have a competition for least readable yet most compact base 64 encoder, I'm put your name alongside mine on the entry.
I prefer adding to the include path in order to avoid relative/absolute includes. Depending on the project, I keep headers in a separate but structured directory (on the search path, of course). I like to use `_` or `$` to namespace, and I always name type related functions as if they were namespaced below the type they operate on.
BTW, protection is pointless. If this is your own project, don't worry. If you have to because someone told you to, try to explain to them first. As long as the software so much as runs on somebody else's computer, they can modify it, protection just makes it take a little longer for them. Protection for "intellectual" property retention, in my opinion, is petty and juvenile. Almost every time I have encountered and reverse engineered something in such a situation, the protected content was either bad or unoriginal.
I would recommend using CMake. It's a meta build tool which means it is creating your project structure. If you are working on linux it creates the makefile for the compiler. If you are developing under windows lilr me, it can create your VisualStudio Project with all necessary configurations. Cmake allows you to structure your source code like you desire it to. You just have to add your files to the CMakeLists.txt and rebuild your project. This adds the source and include files to you project an you can simply say "#include "header.h"" and don't care about its actual place on your hdd. Furthermore CMake allows to add libraries to your project, so u can include them the same way. You can find it at: https://cmake.org/ Hope this is what u are looking for
C++, IMO, is a terrifying hot mess. If you want to try C++, just try D. It can call C++, C (and Fortran) procedures, among others. Or check out Crystal.
In that case, please don't be the tool that develops a .NET application for Linux. Seriously, they are painfully unpleasant to use. Thanks, The whole planet.
It is not an optimization, it is a completely reasonable interpretation of what your program means. How come every time I see some variation of the phrase "how do I disable this optimization" it always actually means "I dug a hole by trying to be clever and breaking the rules, how do I dig deeper"? You wrote a non-standard program in a language that is no longer C. You had to use the hammer of non-standard linker flags to force it through the compilation. A reasonable person might ask "how do I do X in a standard way" instead of asking for a bigger sledgehammer to beat the compilation with just to force it to work. 
Thank for the recommendation. Actually I use CLion which requires at least a minimal CMake file for code completion, but when I wanted to migrate from Makefile I just couldn't compile my project, and it looked a bit complicated to get it work, so I just sticked with Makefile. My question is more about how to keep a larger project clean, and easy to find stuff in it.
I also like to 'namespace' my functions, it's really helpful. What do you mean by 'I like to use `_` or `$` to namespace'?
&gt; runs at the origin machine, fails on the others What does “fails” mean? And how do you transfer the program from one machine to the others?
&gt; an elegant way of handling the end cases A `switch` with fallthrough.
[Here](https://github.com/DxBorks/DxBorks/blob/master/kernel/arch/i386/idt.c) is an IRQ handler of mine I wrote for my operating system. I heavily commented it so people can understand it in details.
I tried to turn your GitHub links into [permanent links](https://help.github.com/articles/getting-permanent-links-to-files/) ([press **"y"**](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) to do this yourself): * [DxBorks/DxBorks/.../**idt.c** (master → c51d2dd)](https://github.com/DxBorks/DxBorks/blob/c51d2ddcb31c5176161e43cde3707409a95b917a/kernel/arch/i386/idt.c) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dl1yeq9.)^.
just dump it in a folder called src nothing wrong with that in-fact it's the most pragmatic thing you can do everything else is subjective and your decisions of "Arranging" source will only come from passion and convictions which have no place in coding...you might as well just code in c++ and jack off to the syntactic sugar Edit:- for the retards downvoting this, huge projects written in C: http://www.fossil-scm.org/index.html/dir?ci=2a615bed1184d6a1&amp;name=src http://www.sqlite.org/cgi/src/dir?ci=c45078c062f41f43&amp;name=src https://github.com/git/git All of these have just dumped .c files in a src folder. Hell for git don't even have src folder. Going C is being pragmatic. You have got to hate the time we (as humans) waste configuring java, c++, visual studio, android studio etc. If you really hate that you'll not waste time asking retarded questions like OP. Basically if you agree with https://handmade.network/manifesto then you should C
Like Duff's device? Then loop through the unencoded character set backwards to account for how the end cases work? *That* would be pretty elegant. 
&gt; It should be fairly obvious what it is from the name. it isn't but if what you're trying to do is get a callback if a certain memory location accessed (i.e. load/store) then you cannot do it in usermode and looking at your level I don't think explaining you how to do it by writing a kernel module which interacts with the usermode to achieve the said functionality is worth it. just write getter/setter methods for your DS (BitBuffer), keep counters and raise events from the getter/setters yes it will not be sexy like it's in java(or whatever) but it'll get the job done so you can move on with your life
No, that would be awful. I meant *after* the loop, for the last few bytes of input.
Sweet! Thanks for all the resources; you sure didn't have to be so helpful, but you were!
They're spamming me with knowledge!
It looks like your source file naming conventions obviate the need for the file folder structure. It's redundant.
&gt; It is not an optimization, it is a completely reasonable interpretation isn't an optimization exactly that, "a reasonable interpretation"? &gt; You wrote a non-standard program in a language that is no longer C. right, that's why i use gcc, which is filled with nonstandard extensions. &gt; A reasonable person might ask i'm not reasonable.
sadly, -O0 doesn't help.
brilliant, works perfect. thank you!
Can I add onto this question and ask, what is the best practice regarding header files? Should one always put function prototypes in a .h and definitions in a .c? Or can your functions be defined with just a .h? I've seen it done both ways, but i've never been sure which way is the more accepted way to do so. 
I don't plan on ever touching .net lol. I can't imagine why someone would want to port it to Linux anyway.
Better yet, follow Arduino's lead and put *everything* in one source file. No pesky headers, no searching for the module you want - it's all there in front of you!
I use those characters as separators in symbol names, e.g. `namespace_symbol` or `namespace$symbol`.
`localtime()` takes a unix epoch time and converts it to local time in broken-down form. Unix epoch time is represented by the type `time_t` (note: not `long int`) as the number of seconds since the epoch, without regard for timezones, and broken-down time is represented by the type `struct tm`, which represents time as a series of fields (day, month, year, etc.) `localtime()` takes a pointer to a `time_t` value and returns a pointer to a `struct tm` value. That's problematic because it requires the use of a static variable, which makes it non-reentrant. To remedy that issue, a reentrant version was standardized, which uses the `_r` suffix. That version takes a pointer to a `time_t` value and a pointer to a `struct tm` value, and fills in the latter based on the information contained in the former, thereby eliminating the need for a static variable by moving the responsibility into the caller. (In this example, the caller is passing a pointer to a variable with automatic lifetime rather than static lifetime, which is fine because the lifetime of the variable is tied to the scope of the caller.) If that does not answer your question then you should elaborate on the specific aspects that you don't understand. BTW, this example is garbage for several reasons and it probably means the book is terrible and should be used as a doorstop.
In general, I like to be explicit as possible. For an struct/type I would declare a _new, _init, and _free routine. _new allocates on the heap, _init accepts a user-allocated pointer (heap or stack). And any instantiation of my_type_t * can be passed to _free. Something like this: typedef struct { uint8_t owner; /* more fields */ } my_type_t; my_type_t my_type_new() { my_type_t *type = malloc(sizeof(my_type_t)); type-&gt;owner = 1; /* init other fields */ return type; } int my_type_init(my_type_t *type) { type-&gt;owner = 0; /* other init code */ } void my_type_free(my_type_t *type) { /* cleanup fields */ if (type-&gt;owner) { free(type); } }
There are a number of factors when deciding a good layout structure, but in general, I separate each binary/library into is own folder. If it was a large project, player/cache would contain all source, headers, make files for libplayer_cache.a/so and then level would contain all source/headers for liblevel_lighting.a/so. Then if any piece depended on the other, I'd either statically/dynamically link the necessary library and used a well-defined API for interacting with that subset of code via .h file. I've found that as projects grow in size it is important to have small well-defined APIs and code separated into small and manageable libraries. It helps with testing, code management, facilitating multiple developers in the same project and to some extent, it can help control feature creep in certain components.
Oh, I see.
I've done it. In a project of mine I've a file (among others) with 20k LOC because compilers optimize better inside a single compilation unit. And what matters isn't that my manager is happy or that the probably brainwashed guy after me will cringe when he looks at that but that the project is solving problems of others (so they can move on with their life) and earning the company some money. Of-course there are compilers that optimize among different compilation unit...but why depend on the compiler writers skill to handle all cases? Modern compilers have been failing at that even now (in 2017). It's just that when you actually understand things you realize that compilers aren't magic or compilers don't owe you to optimize properly and you should do all you can to reduce another layer of abstractions because 99% of problems are due to abstractions not working in production.
You should add your prototypes which you want to add to the interface to the headers, and keep the actual implementation in .c files. You shouldn't define functions in header files, except maybe some 'one-liners' that you declare `static inline` and don't want to create a macro from. Sometimes there are function prototypes in .c files also, it can be useful if you want to use a function before it's actual declaration, so you could maybe put the prototype at at top of the file, however, I don't like this approach. 
A separate _new and _free function is a bit cumbersome for methods like my example (basically a .toString() analogue from Java) though but I guess it's "pick your poison" either way.
&gt; BTW, this example is garbage for several reasons and it probably means the book is terrible and should be used as a doorstop. The code seems to come from [Hacking: The Art of Exploitation](https://www.amazon.com/dp/1593271441).
We need a 'bot that reformats code. #include &lt;stdio.h&gt; #include &lt;time.h&gt; int main() { long int seconds_since_epoch; struct tm current_time, *time_ptr; int hour, minute, second, day, month, year; //Pass time a null pointer as argument. seconds_since_epoch = time(0); printf("time() - seconds since epoch: %ld\n", seconds_since_epoch); //Set time_ptr to the address of // the current_time struct. time_ptr = &amp;current_time; localtime_r(&amp;seconds_since_epoch, time_ptr); //Three different ways to access struct elements: hour = current_time.tm_hour; //Direct access minute = time_ptr-&gt;tm_min; //Access via pointer second = *((int *)time_ptr); //Hacky pointer access printf("Current time is: %02d:%02d:%02d\n", hour, minute, second); } 
Now that it's formatted, I can read it. This is pretty awful code. It's like they're trying to combine a lesson about the localtime() function with a lesson about pointers. Excuse me, I meant a lesson on how to misuse pointers.
You're right, 2nd edition.
Lol. Now I'm worried about the second example, time_example2.c
Yes. Warning seems correct. Foo contains address of allocated memory, but you try to free chunk.memory, which is assumably global struct member. sprinf usage is not necessary, it is even wrong. You probably want to to following: struct { size_t size; char *pointer; } struct chunk; chunk.size = 10; chunk.pointer = malloc(chunk.size); if (chunk.pointer == NULL) { // handle out of memory error here } // free allocated mem in heap free(chunk.pointer); PS: Sorry for syntax errors. Can not write C in IPad :(
Thank you. Indeed, chunk has no built-in pointer right now. I'll change that. But I'll still have to copy the chunk into a `char*`, right? 
The per-thread malloc cache looks interesting. Does anyone have any information on how this compares with jemalloc or tcmalloc? Edit: I found [this](https://sourceware.org/ml/libc-alpha/2017-01/msg00452.html) in the libc mailing list. &gt; Executive summary: A per-thread cache reduces malloc call time to &gt; about 83% of pristine, with a cost of about 1% more RSS[*]. TCache &gt; doesn't replace Fastbins; combined performance is better than either &gt; separately. Performance testing was done on an x86_64 system, as most &gt; of my workloads won't fit on a 32-bit system :-)
You need to copy some data to allocated memory, otherwise no need, because chunk.pointer contains first address of allocated memory. I would suggest memcpy instead. BE AWARE of possible buffer overflows!!! You need to know how many bytes you can copy and may copy. It would be wise to know it before allocating memory. struct { size_t size; char *pointer; }; struct chunk; char *tmp_str = "Hello World"; // Some limited scope data what you want to store more permanent way. chunk.size = strlen(tmp_str) + 1; // You probably want to copy null terminator also, don't you? chunk.pointer = malloc(chunk.size); if (chunk.pointer == NULL) { // handle out of memory error here } memcpy(chunk.pointer, tmp_str, chunk.size); // free allocated mem in heap free(chunk.pointer);
Thank you! :-)
The `_Float128` support doesn't seem new... am I missing something?
The term “top down tree” is ambiguous. Could you provide us with some context?
This isn't really a C question so much as a language agnostic homework question. Have you tried the wikipedia article on top down parsing to start with? Is it specifically C your problem is with? Your question is essentially "write a short article for me explaining what this is". There are lots of answers out there available on the internet. If you go through a few articles from a search engine, and still don't understand it, try a post in r/learnprogramming/ where you explain what you do and don't understand about it, and ask specific question. If you already understand it, but you have a problem understanding some C specific issue with the implementation, then perhaps you could ask here - assuming you've made a good faith attempt and are able to explain your difficulty.
Don't spam please.
I think I got it now. What I didn't understand is how the struct tm variables and the actual structure (located in time.h) are related. Can you elaborate on that please? I understand that it's the data type that links them in some way, but I don't know how precisely.The book doesn't go that in-depth.
This is a question asking about trees in a general aspect of programming. I'm asked in one of my exercise problems that I need to place a set of numbers in a "top down tree of order 5". The title of the exercise is named "Understanding Multiway Search Trees" . I don't know if this helps. 
I'm trying to learn the general concept of a top down tree, so I can get some idea of how to implement it C into code later on down the rode in my class. I have already tried to look for good videos and articles on the subject, but I have not found one that does the job. I've looked up vids on YouTube and example on Geeksforgeeks. I definitly check out the learnprogramming sub to see if I can get some good info there. Thank you my friend. 
The header defines the members that the structure contains and its layout, but that's it. You then create an instance of the structure by writing `struct tm current_time`. 
it sounds like you're talking about k-ary trees. Is that correct?
No &lt;threads.h&gt; ?
As a C noob I think its cool seeing it updated and maintained. 
Maybe you can explain (top level comment) what you think you understand? That will help us know the direction from which you're coming. 
In addition to the logic advice you're receiving here, consider this: Your delclaration: char code[64]={'A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y','Z','a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z','0','1','2','3','4','5','6','7','8','9','+','/'}; May be replaced with an exactly equivalent: char *code = "ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789+/"; Or, exactly equivalent, and maintaining the expected buffer-size confirmation: char code[64] = "ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789+/"; Just a small FYI if you prefer the readability of the latter forms.
Well they are trees to the K power. I really don't know if that is K array trees
Thanks for pulling the performance info!
Each source file can be compiled in its own process. Compiling a project with one source file uses a single process. Also make -j n has no effect.
You don't per chance do the malloc() in one DLL, and the free() in another, or free in an executable that got the memory allocated from a DLL? There are caveats with that.
No, it's all one static executable (yet). But just for curiosity: What would be the caveats?
I've had problems with the heap checker (in debug mode) contained in the runtime bitterly complained that I am releasing memory that was not within that (DLL's) heap manager's list of blocks. Since each DLL gets its own heap, that'll happen. Now, I'm not sure that it would actually *crash* in Release mode. But I also don't know what sort of other heap management of the runtime is done, and if that would crash.
Thank you, I'll try to remember that. :-)
The address overflows and it will point to memory address 0x0000. What happens then is platform dependant some hardfault and block, some crash but reboot, there is one for every taste 
You have two issues: the math, and the pointer. The math: If it's signed, the behavior is undefined as to what happens. If it's unsigned, it gets reset to 0. The pointer: Now, there's another set of defined/undefined rules to worry about, and that's with accessing memory outside a defined boundary. Lets say you have an array of 1 element. You trying to check `array[ 1 ]` for data is undefined behavior. It's outside the scope of permitted access. You can check to see if you are at that point, but you can't check that point to see what it is. 
**Virtual address space** In computing, a virtual address space (VAS) or address space is the set of ranges of virtual addresses that an operating system makes available to a process. The range of virtual addresses usually starts at a low address and can extend to the highest address allowed by the computer's instruction set architecture and supported by the operating system's pointer size implementation, which can be 4 bytes for 32-bit or 8 bytes for 64-bit OS versions. This provides several benefits, one of which is, if each process is given a separate address space, security through process isolation. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/C_Programming/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.24
&gt; I dont think it has to do with the standard. It does. Signed overflow has undefined behavior.
&gt; since pointers are unsigned. I don't believe C defines how pointers are represented as far as their sign goes.
&gt; If it's signed, the behavior is undefined as to what happens. If it's unsigned, it gets reset to 0. Isn't `char *` unsigned by definition? Or am I misremembering? 
No. The standard doesn't defines 'char' either signed or unsigned but as implementation defined based on the character set it is supposed to represent. &gt; An object declared as type char is large enough to store any member of the basic execution character set. If a member of the required source character set enumerated in 5.2.1 is stored in a char object, its value is guaranteed to be positive. If any other character is stored in a char object, the resulting value is implementation-defined but shall be within the range of values that can be represented in that type. But 'char \*' is a different type entirely. Its type is "pointer to char", which isn't based on the sign of 'char', but rather on whatever it uses to represent pointer types. For example: 'unsigned char \*' and 'signed char \*' are the same signed-ness.* ^ *as far as I know. I don't believe pointers are ever represented in different signed-ness.
If your system actually gave you an array where the last element is at that address then you will get a pointer value which when decremented will give you the last element of that array. If the system didn't give you an array where the last element is at that address then the behavior is undefined. I suspect that the pointer you get can not be NULL (haven't verified, but it seems reasonable). From that we can conclude that if the increment would make the pointer value 0 which also happens to be NULL on your system, then it is impossible for the system to have ever allocated an array that uses that address. Which means that the answer is: it can never happen, in other words - undefined. Edit: In fact, ignore everything above (even though it's true), this is explicitly spelled out in the standard (emphasis mine): &gt; If both the pointer operand and the result point to elements of the same array object, or one past the last element of the array object, *the evaluation shall not produce an overflow*; otherwise, the behavior is undefined.
Couldn't this become seen as NULL?
I thought 0x0000 was always supposed to be empty and represents NULL
Pointers do not have to be numeric types, though traditionally, they are always unsigned. Early versions of C didn't have the `unsigned` specifier, if you needed an unsigned datum, you used a pointer.
According to the C standard, it depends. Pointer arithmetic is only defined within objects, you are not allowed to add a number to a pointer to get to a different object. So if there is an object that goes around the end of the address space back to the beginning, this is fine. Otherwise, behaviour is undefined. Some platforms (e.g. x86) have hardware that detects stack overflows: If you use `pop` to increase the stack pointer from `0xfffffffc` to `0x00000000`, an interrupt is executed. x86 is also the only platform I know where overflowing a pointer doesn't necessarily cause wrap around as you'd expect. The explanation is a bit long, but it's so weird that they introduced a special hardware feature to deal with it. Read [A20 line](https://en.wikipedia.org/wiki/A20_line) for details.
**A20 line** The A20, or addressing line 20, is one of the electrical lines that make up the system bus of an x86-based computer system. The A20 line in particular is used to transmit the 21st bit on the address bus. A microprocessor typically has a number of addressing lines equal to the base-two logarithm of its physical addressing space. For example, a processor with 4 GB of physical addressing space requires 32 lines, which are named A0 through A31. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/C_Programming/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.24
This only affects “signed integers.” A pointer is a pointer, not an integer.
C doesn't even define that pointers are numbers. On some weird platforms, pointers are other things, like pairs of numbers.
I think that too. 
Can you explain why it only adds 4 bits instead of 1 byte?
This is necessarily an implementation-specific question. The standard specifies nothing about the representation of pointers. Saying the pointer "has value 0xFFFFFFFF" is already implying a lot of implementation details. The guarantee provided by the standard is: &gt;If your pre-existing pointer points to a byte of a valid object, then incrementing it is well-defined. The resulting pointer must not compare equal to null. The implementation must behave in such a way that that guarantee is met. Some common implementations would meet this condition by making it so that the last byte in the address space cannot be allocated as part of a valid object. 
Signed *integer* overflow is undefined, but this question is about pointers. Pointers aren't integers.
Your code is a constraint violation. An integer may not be assigned to a pointer without a cast (except for the special case of a constant expression equal to `0`).
Pointers don't have a sign. They hold addresses.
On most sane systems it is safe to assume 0x0 is NULL.
No. NULL doesn't have to be a specific spot in memory, and it doesn't even have to be zero. It just has to hold a value which is evaluated as NULL. [C FAQ](http://c-faq.com/null/machexamp.html)
And yet, you can still do math to them.
Seems that the compiler just translates 0 into whatever it needs.
You can't add two pointers 
Most platforms won't crash just because you add 1. Maybe after dereferencing. Although it's still undefined behavior to just add some addresses at free will that have no real meaning.
On the platforms where you tested it. This doesn't mean anything from the point of view of the standard.
It doesn't "depend". It's spelled out in the standard. It's undefined. See my answer.
https://stackoverflow.com/a/2759872/6840486 also, https://softwareengineering.stackexchange.com/a/147720
Why are people downvoting a question?
http://www.open-std.org/jtc1/sc22/wg14/www/docs/n1570.pdf
You just send me a link to the standard which is absolutely useless without contextual reference. Thanks douchebag. "a null pointer constant (prvalue of integer type that evaluates to zero) is convertible to a null pointer value. (§4.10 C++11)" This can also be found in the C standard with different language. We can additionally see this by reading the link the guy posted above... http://c-faq.com/null/machexamp.html This is evidence that there are multiple machines with translations of 0 to non 0f000000 addresses that operate as null ptr values, or as I said before "seems that the compiler translates 0 to whatever it needs." 
&gt; You just send me a link to the standard which is absolutely useless without contextual reference. Thanks douchebag. You gave me two links to stackoverflow answers. I provided a link to the standard. When the question is if a behavior is implementation defined or undefined an answer that contains "Seems that the compiler ..." is actively harmful because how a behavior is defined is a question of what the standard is, not what your particular compiler happens to do this week. 
Undefined = depends on implementation. If it's undefined a implementation can also do something that's well-defined on that specific platform. EDIT: A bit unclear, just by saying "undefined = depends on implementation" I did not mean it's "implementation-defined" but that it's up to the implementation what happens and *not necessarily* documented/guaranteed by the implementation. Ie.: If C says when A happens you trigger undefined behavior, the *actual* behavior depends on the implementation (possibly it's not the only factor, but it's one). So say on implementation I X happens sometimes, sometimes Y happens. This is undefined but depending on the implementation since on platform J always Z happens. It can even happen that J guarantees Z to happen. If the standard would say "implementation-defined" it would *demand* I and J to specify what happens.
out of curiosity: can you name such a platform?
No, it does not mean that. "undefined behavior" and "implementation defined behavior" are two different terms in many programming language standards. They are very important to keep apart, especially when it comes to C. Implementation defined behavior generally means "read the ABI document and/or compiler documentation for the platforms you intend to run on and you'll be fine until you move to a different platform". Undefined behavior means "a different thing can happen next time you update the compiler for a minor bug fix or even next time you compile". Until 10-15 years ago or so the difference was mostly academic. It was a curiosity but you could mix up those two without too much danger. Around 10-15 years ago gcc got serious competition from clang and others and at the same time ran out of low hanging fruit optimizations. Today almost every release of compilers contains new and creative ways of interpreting undefined behavior because it allows for good optimizations. 
"a different thing *can* happen" – the "can" is key here: It also *allows for* the same thing to happen. While "implementation-defined" means that the implementation *has to define* what should happen, "undefined" doesn't require this. But of course the implementation can define a specific behavior and guarantee this. But even if not: "It depends" is still a valid answer because it says that there are the factors defining what happen lie outside the standard (which is exactly right). It does not say that these factors have to be written down or not totally random. (also see my edit to my former post)
I sent you links not just to answers, but to discussions specifically about the relevant subject. If you read them, there are various (also relevant) citations in the surrounding discussion. You on the other hand sent me a link to a 700 page document with no context whatsoever. Downvoting and providing a link to a 700 page reference doesn't do anything other than make you look like a jerk. Just say what you want to say and have a conversation like a decent human.
Yes, computers are deterministic, yes, compilers have a specific implementation that you can read and understand, blah, blah. Everything depends and can be reasoned about. It's technically true. It's also completely useless as an argument because you do not have the time, money and attention span to narrow down every single construct in every single situation. Even assuming a static environment which it almost never is. When discussing programming in C when you run into something that is UB the answer is always: "this is wrong" or "don't do that". It's not "it depends". It's one of those things that doesn't depend. It's always wrong/bad/incorrect. Trust me, I spent the first 10-15 years of my career thinking that "it depends and I'm getting away with it, go for it", then the past 10 years of my career thinking "what the hell was I thinking, better be strict about it". There was a push to make compilers more forgiving. There was a push to narrow down things that are undefined to be unspecified or implementation defined. Those battles have been lost. Compilers will screw you over so hard that today any reaction to UB other than immediate fixing of the code is irresponsible.
Nobody said it'd be right to rely on that behavior, it was asked "what happens" – and this the OP did answer correctly. The question was not "can I do this?".
Okay, your answer is a good one. That sentence is very convincing.
For example, there is C for some Lisp machines. On Lisp machines, everything is a pair, including pointers.
&gt; djb2 returns a long unsigned, and my table is only 1024 elements, is it ok to mod 1024 the returned hash? Or is there a better way to adapt the number to my needs, like using only the first/last 10 bits? Make sure to use the last few bits. &gt; In case of collisions, should i go with quadratic probing (with both constants being 1/2 since my table size is 2m) or double hashing (i would use k&amp;r as the second hash function in that case)? Both approaches are possible, I prefer having linked lists in each bucket. Don't use the K&amp;R hash function, it's really shitty. Instead, consider using the PJW hash function which is well-tested and known to be fast.
1. After you get the djb2 hash in unsigned long format there's plenty ways to convert it to an index that fits your array size. Using a few bits is one way but there's other ways, like the division method you mentioned with `mod`. However the division method might not work as well for array sizes that are powers of two, like in your example, since `x mod 2^10` will just return the 10 least significant bits of x. You can use the [multiplication method](http://lcm.csa.iisc.ernet.in/dsa/node44.html) in those cases. [Here](https://github.com/Gikoskos/EduDS/blob/master/lib/src/HashFunctions.c#L32) is my implementation of it if you want to use it, just replace size_t with unsigned long in your case. 2. Quadratic or linear probing are optimal methods in resolving collisions if the load factor of the table is less than 0.5, but using chained hashing with linked lists like FUZxxl mentioned is also a good idea. If you go with probing after all, I recommend giving rehashing a go, where you create a new table with double your old table's size, if load factor &gt; 0.5, and rehash every key from the old table into your newly created table. On the other hand, double hashing's performance depends mostly on the hash functions that you use, so if you think that your hash functions are very fast then there's no issue in using that method too. I think that you should go with quadratic probing and rehashing, but that's mostly my preference.
This makes sense in that you may have a pointer to the end of memory but to get there means the previous byte was the last byte of a valid object. It was valid to have incremented the pointer to the end of memory, but incrementing it any further will invoke implementation-specific behavior. Thanks!
It's undefined behaviour if you increment a pointer to outside of any valid object. 
Thanks, looks like the Additive Operators section of the standard is what I was looking for. 
Sorry, I meant that in the context of an array object where it is defined to increment one past the last element. Dereferencing the resultant pointer is undefined though. Otherwise yes, incrementing outside of a valid object is undefined.
“Linked lists in each bucket” = open hashing. OP specifically asked about closed hashing.
ah, thanks!
It looks like they finished rounding out all the features and fully support the 2015 specification now. Likely, not much has really changed.
Thanks for your reply. As /u/a4qbfb noticed, i'm bound to use closed hashing. Also, i know K&amp;R hashing function is shitty, but i thought the second version, which i linked, is a bit better. Which one are you referring to?
[PJW hash function](https://en.wikipedia.org/wiki/PJW_hash_function) (have you tried to Google it?). The function you linked is very similar to the djb function. If you know that both are shitty functions, why don't you just use a good function instead of these historical oddities?
**PJW hash function** PJW hash function is a non-cryptographic hash function created by Peter J. Weinberger of AT&amp;T Bell Labs. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/C_Programming/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.24
Wouldn't the probe type not matter if you have a uniform hash?
Quadratic probing is easier to probe, after a collision, than linear probing due to something called secondary clustering (Cormen also makes mention of this but doesn't analyze it as well as this [stackoverflow post](https://stackoverflow.com/a/36526945)).
Oh I sort of get it now. Quadratic probing would be less cache friendly since it's not being read linearly though, wouldn't it? Not at a machine to test at the moment. 
I would say that whether it's cache friendly or not depends mostly on the implementation. But you could be right on that. Besides, the point of an open addressing probing implementation is to make it so that collisions are avoided as much as possible in the first place. That can be done with a combination of a great hash function that can guarantee both uniform distribution and speed, and of rehashing if load factor &gt; 0.5. I implemented my quadratic hash tables using [xxHash](http://cyan4973.github.io/xxHash/) and rehashing, and by running some tests collisions happen very very rarely especially if the hashtable size is big.
Thanks, i did not know about this function. I initially chose djb2 and k&amp;r (again, not the loselose one) because they're simple, portable and yield decent results, as i'm working on a college assignment that needs to be a single file of ~600 lines of code. I'm gonna try this one out!
Thanks for all the information! I read something about the multiplication method and i'm gonna expand on it. For the time being i'll stick with quadratic probing, because to implement rehashing i would need to change a big part of the code. I'll try rehashing if i'll see significant performace degradation.
If you looked at the polar opposite, you could make all variables global, so they exist at file scope. But having lots of variables at that scope is problematic as it makes it hard to read what code is having what effect on what variable, and therefore difficult to maintain. Block scopes help tie together the variable to how that variable is used and also limits it's life time so as to reduce clutter and massively increases readability.
Block scopes can be used to isolate naming. This comes handy, for example, when you develop complex macros and don't want to have variable name clashes.
Why not call another function implemented with the code that would be in a block scope (including the variable declared in it)? 
Well, you'd have to carry the whole context with you which can be quite many variables or even structs, pointers to these, etc. I use it when in other languages I'd use an anonymous function/lambda. Also for structuring your code, think of an example like this: if (/*...*/) { for (/*...*/) { /* Code A */ } else { { /* Code similar to A but small edge-case not in loop, but still indented the same way for structure. */ } } EDIT: weird, code-tags seem to be broken here. EDIT2: duh, using 4 spaces now. ugly
https://stackoverflow.com/a/3419392
Preference, readability. It's up to you. Make one large function of a hundred lines or make several functions with ten lines each. There's really is no right answer and it's entirely subjective, but I would say readability is the biggest factor in finding a balance between the two. One massive function tends to be unwieldy, difficult to read and error prone. Many functions with little content are equally difficult to read, but may split up difficult to understand concepts.
Standalone blocks (i.e. that aren't the body of a function or branching/looping statement) aren't commonly used, but they are an option. They can be used to circumvent the C90 "no mixed declarations and code" rule, since the rule is applied on a per-block, not per-function level. For example, { int a, b; scanf("%d, %d", &amp;a, &amp;b); int c = a + b; } won't compile in C90 mode, but { int a, b; scanf("%d, %d", &amp;a, &amp;b); { int c = a + b; } } should. Also, this has the advantage over a function call in that the inner block can access the local variables of the outer block, which a function can't do unless those variables are passed via pointers. But overall, it's not something that will come up often. Useful to understand, but not something that you will likely use on a regular basis. 
Thanks for that! Exactly what I needed :) 
I gave a shot at this and what I get is a segfault because t never gets updated and there are some other messes in here : usual include stdio.h and string.h etc etc 20 int main( int argc, char*argv[]) 21 { 22 char *x, *basestr, xmod; 23 24 char code[64] = { 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 25 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 26 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 27 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 28 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 29 't', 'u', 'v', 'w', 'x', 'y', 'z', '0', '1', 30 '2', '3', '4', '5', '6', '7', '8', '9', 31 '+', '/' }; 32 33 int xsize, i, j, t; 34 35 basestr = calloc( (size_t) 256, (size_t) sizeof( unsigned char ) ); 36 x = calloc( (size_t) 256, (size_t) sizeof( unsigned char ) ); 37 38 strcpy ( x, "Hello World." ); 39 40 xsize = strlen(x); 41 xmod = xsize % 3; /* what is going on here? why is xmod a char? */ 42 t = xsize - xmod; /* these should be int types like int32_t */ 43 /* To temporarily remove hanging end chars */ 44 i = j = 0; 45 46 while (t &gt;= 3) { /* loops forever and just segfaults with a bad pointer */ 47 basestr[i] = code[(x[j] &amp; 0xfc)&gt;&gt;2]; 48 basestr[++i] = code[((x[j] &amp; 0x3)&lt;&lt;4)|((x[j+1] &amp; 0xf0)&gt;&gt;4)]; 49 j++; 50 basestr[++i] = code[((x[j] &amp; 0xf)&lt;&lt;2)|((x[j+1] &amp; 0xc0)&gt;&gt;6)]; 51 basestr[++i] = code[x[++j] &amp; 0x3f]; 52 i++; 53 j++; 54 } 55 56 if (xmod == 1) { 57 basestr[i] = code[(x[j] &amp; 0xfc)&gt;&gt;2]; 58 basestr[++i] = code[((x[j] &amp; 0x3)&lt;&lt;4)]; 59 basestr[++i] = '='; 60 basestr[++i] = '='; 61 } else if (xmod == 2) { 62 basestr[i] = code[(x[j] &amp; 0xfc)&gt;&gt;2]; 63 basestr[++i] = code[((x[j] &amp; 0x3)&lt;&lt;4)|((x[j+1] &amp; 0xf0)&gt;&gt;4)]; 64 j++; 65 basestr[++i] = code[((x[j] &amp; 0xf)&lt;&lt;2)]; 66 basestr[++i] = '='; 67 } 68 69 return ( EXIT_SUCCESS ); 70 71 } 72 So that is what I have .. I'll read the above comments and see what went wrong.
I've used them to avoid multiple temporary variables in my outer scope. Or if I want to do something twice in a row and use the same variable names, but another function would be inconvenient because I'd have to pass too many variables.
Hi, Thanks! I left out a bunch of extraneous code because I was just curious about the pattern for the encoding. That's definitely the source of any errors. In full, it compiles and runs fine. I ended up using the other comments and streamlined the process quite a bit. I won't lie, I really want to Duff's device it all, because I like compact things, but ended up just using the fall through as suggested. To answer your question, `xmod` was intended to handle the end cases, but as someone else said, because I decrement `t`(not sure why that's not in the code you have?), I don't need `xmod` at all. I ended up just replacing it in the final `if` structures with the remainder of `t`.
Thanks. You're right. I have no idea why I did it that way.
I carefully walked over this line by line and seem to have missed something somewhere. I started with a direct copy and paste of your original code. Personally I love the bit twiddling shifts you are doing to get a rfc-4648 base64 encoding done. Sure it is supposed to be lines that are 64 chars wide and stuff like that but your method really looks cool.
Thanks! It was a good learning experience. That's a good point. I would probably be benefited by making it a bit more compatible with the rfc, more error checking, etc. I'm using the exercises on [this site](https://cryptopals.com/) to get familiar with C, and going from hex code to base 64 is their first challenge. 
Why are you putting underscores in the folder and file names? weird. To actually answer your question tho, my projects use a simple structure. I have a simple /include folder for the internal API, and a /src folder for the source code. I may have an Encode/Decode subfolder in addition to those 2, but that's basically it. As for the external API, that's in the root of my project's library folder, and it's just a single header named libProjectName.h I'm not entirely happy with the naming of the files, but I've got more important shit to take care of rn. Anyway, here's a [screenshot](http://imgur.com/Q8FFmJu) to explain it better Also, I should probably move the generic, non-public headers to a subfolder too... [Well, I took care of that.](http://imgur.com/QtKKQqL)
Inside long switch statements is a great use for block scopes. All the performance benefits of the jump table with most of the readability of a (function) lookup table. switch(thing) { case 1: //do something break; case 100: { int foo; //do something with foo break; } } 
Where would reddit be without Stackoverflow? Stackoverflow saves reddit and gives it a reason to exist.
It segfaults.
Indent your text four spaces to get it to format as fixed-width
or use triple-backquotes. i did that but it doesn't work (anymore)
If you create and call another function, you cannot access local variables in the original scope. I've used block scope when I've had a short, once-off piece of code within a function that needs some local variables from the function it's in as well as a couple of new temporary variables. I could have declared the new temps at the top of the function but I think that doing so clutters the meaning of the code and obfuscates the valid lifetime of the variables. If it was a piece of code that was used more than once, I'd make a macro. If it was a piece of code that didn't need context from the original function, I'd make a new function. If C allowed closures which inherited the parent scope, I'd use that. 
``` doesn't it? ```
"(...)use triple-backquotes (...) it doesn't work (...)" is a rather strange advise. What is the advantage of using something that does not work?
It was no advise (because I usually do nkt advise myself ;)) but stating an alternative (I know about the 4 space thing) that used to work ...
It didn't work for me ^^
nullptr is C++, not C. Why do you think that has a memory leak?
You sure you're compiling a C program, not a C++ program?
Fail comment
Compiling with both C and C++ compilers I get the same problem
Sorry, i had a problem with reddit xD C++ and C, both are failing. I think it's a memory leak because it's increasing the memory usage until it crashes with 0xC0000005 code. strtod works fine
How is nullptr defined? I think that's a c++ constant. Is this all code, including any `#include`s? Other than that, nothing looks obviously wrong with the code, assuming nullptr is really NULL. 
I am posting this comment: it fails with both C++ and C compilers. Changing nullptr with NULL or with a pointer address, always the same problem. I don't think my code is wrong, I've revised the information about strtod and strtold functions, and it's fine. strtod is fine, but strtold is leaking memory.
&gt; If you create and call another function, you cannot access local variables in the original scope. Why not pass them as parameters?
I just made a comment explainign some things :D But yeah, NULL instead of nullptr. I also include stdlib.h, nothing else
I first realized their need when trying to declare an array whose size is determined at runtime. int i; sscanf ("%d", &amp; i) ; { char s[i]; } 
Very interesting! This shouldn't happen.
Have you tried it? Do you have the same behaviour? If nobody say anything, I think I'll open a bug in the MinGW issue tracker. But first I'll test it a bit and wait for people comments xD
You should be able to edit your original post to include the entire program, including NULL since this is not a c++ subreddit. 
Ok, sorry. Already edited with a full C compiling code!
The sidebar has many good links.
I'm using iPhone app ,so how can I get there ?
I tried on my FreeBSD machine. No allocations at all.
Open reddit in your browser perhaps?
I'll try ths 
In my testing on my Windows 10 machine: Subsystem | GCC version | Leaking memory ---------|----------|---------- Cygwin | (GCC) 5.4.0 | No MinGW | (MinGW.org GCC-6.3.0-1) 6.3.0 | No MSYS2 | (GCC) 6.3.0 | No MSYS2 MINGW32 | (Rev1, Built by MSYS2 project) 7.1.0 | Yes MSYS2 MINGW32 | (Rev2, Built by MSYS2 project) 7.1.0 | Yes MSYS2 MINGW64 | (Rev1, Built by MSYS2 project) 7.1.0 | Yes MSYS2 MINGW64 | (Rev2, Built by MSYS2 project) 7.1.0 | Yes Either it is a MSYS2 MINGW* problem or a GCC 7 problem. Couldn't reproduce on Ubuntu 16.04 with the toolset-test-ppa and gcc-7.
I'm seeing the leak, too, compiling with Mingw-w64 6.3.0 and running the program with Wine 1.8.7 on Debian 9. Edit: Looks like it happens in [`__asctoe64()`](https://github.com/Alexpux/mingw-w64/blob/master/mingw-w64-crt/misc/strtold.c#L69). There's a call to malloc(), but no call to free() and the pointer is lost.
Nice catch! So, how to fix it? I mean, how to tell the devs to fix it? I don't know how MinGW devs work :o
There's always the option of writing your own!
Note that [MinGW-w64 is a fork of MinGW](https://en.wikipedia.org/wiki/MinGW#MinGW-w64), so make sure you [report your bug to the right place](http://mingw-w64.org/doku.php/support). Most people are using MinGW-w64 nowadays rather than MinGW, and I haven't checked if this bug is also in MinGW (probably not). You haven't said which one you're using either.
Yeah, I'm using MinGW64
Haha of course but, there are a lot of cases (infinite, NaN, scientific notation...) I can also copy the MinGW code and fix it, but I don't want to fill my library with that kind of trash-code (I would like to rely on strtold, but I can't until it's fixed and released)
Note that getting strtod() right is [enormously complicated](http://www.exploringbinary.com/how-strtod-works-and-sometimes-doesnt/). IMHO, don't bother with `long double` and just stick with `double`. The extra precision is marginal, it's only longer than a `double` on some architectures, and it's going to be a lot slower. On x86 it will use the old x87 ISA and can't be vectorized. 
&gt; I particularly suggest Racket. But be advised that for GUIs you'll probably won't find much outside of OOP because OOP suits very well the GUI programming needs. I would also recommend Racket as a lisp variant to learn - and indeed it's gui library is object oriented.
&gt; I particularly suggest Racket. But be advised that for GUIs you'll probably won't find much outside of OOP because OOP suits very well the GUI programming needs. I would also recommend Racket as a lisp variant to learn - and indeed its gui library is object oriented.
`scanf` supports `%n`: (from the [C89 HTML draft](http://port70.net/~nsz/c/c89/c89-draft.html#4.9.6.2)) &gt; n No input is consumed. The corresponding argument shall be a pointer to integer into which is to be written the number of characters read from the input stream so far by this call to the fscanf function. Execution of a %n directive does not increment the assignment count returned at the completion of execution of the fscanf function. So to get the "pointer", you'd do something like `sscanf(s, "%Lf%n", &amp;longdouble, &amp;charsread); str = s + charsread;`. The following function seems to work, without any memory leaks when using MSYS MINGW64/32: long double my_strtold(char *s, char **last) { int n; long double num; int res = sscanf(s, "%Lf%n", &amp;num, &amp;n); /* TODO: Set errno correctly */ if (res != 1) return 0.0L; if (last) *last = s + n; return num; } However, it is a bug in MinGW/GCC that only exists in some versions. I wouldn't just make a workaround for these versions, they are using buggy libraries, just wait till they're fixed. Or don't use `long double`, `double` is likely the best option.
and sscanf may well call the lower level function that's leaking
Just build a string using the path name...
In my testing, on the affected version, it didn't happened. &gt; without any memory leaks when using MSYS MINGW64/32
Oh, didn't know %n. Thanks! Yeah, I think it will work for me. Well, it only exist in some versions but, I need a workaround now for this. I'll change it later when strtold has been fixed. I was using long double because I'm working on a JSON library. Well, I thought long double was 128bit, but no, it's usually 64bits... And the standard of JSON tells to use preferably 64bits double, so... I was mistaken about it :D! I'll use strtod haha Thanks! :D
I have discovered some things right now, so yeah, I'll use double and strtod xD
Lenovo. T X or P series depending on what you like. 
This subreddit is about programming in C, not picking laptops. I have removed your post as it is off topic. Please ask elsewhere.
Dude, just take in the name as a function parameter.
Of course it segfaults. That fgets() tries to store bytes in a random part of memory because buf is never initialized to point to anything meaningful. Fix that, and you still have a memory leak with line and likely other issues too.
What prevents you from passing the content of that variable to `mkdir`?
The code posted works. What I'd like to know is why I can't declare and initialize the 3 ints within main. Moving the declaration into main causes a segfault.
Oh I'm well aware, I never said it would be easy ;)
So, free line and initialize buf? I'll give it a shot after dinner.
&gt; Just use the first or last few bits, shouldn't matter. It does matter. Try computing the top 10 bits of the PJW hash for the keys “this”, “is”, “a”, “really”, “bad”, and “idea”.
Indeed. You are right, sorry for giving wrong advice.
You were correct, changing `buf` fixed things. Can you tell me why the code worked in the above configuration? Why wouldn't it just segfault all the time?
No, the code doesn't work. Using uninitialized values, and writing to memory you don't control are undefined behavior. A segfault is actually best case. Worst case could include things like overwriting your program with malicious user supplied one. Or anything between. 
Thank you for the help. I tried freeing line, but valgrind still reports a leak from the strdup call. I'm guessing it's due to the pointer moving with each call to strsep and not having a reference to the original memory location?
What the hell is an x-executable?
You have to free the address returned by strdup(), yes.
Doesn't matter as long as everyone in the project is using the same.
 if (cond) foo(); Is just wrong. if (cond) { foo(); } Is ok but is a waste. This: static inline struct complicated function_name(int really, int long, uint64_t parameter, void (*list)(int *of, int *arguments), struct complicated const *ranoutofwords) {} Is hard to read. static inline struct complicated function_name(...) { } Is better. - More than 2 spaces for indentation is a waste - Don't care for tabs, my editor automatically replaces them with spaces - `arr[i++]` is asking for trouble - `i++;` is ok - `arr + i` is better to read than `&amp;arr[i]` - If the operator precedence is anything but obvious, use parenthesis No strong feelings for anything else I guess.
I just find out about clang-format, I'm using the llvm style, seems the most comfortable to read for me.
I kinda like different people using different styles (in a small team), as then you can immediately tell who wrote a piece of code and therefore who to ask about what the fk they were doing in that piece of code 
* tabs for indentation, spaces for alignment * the `*` goes to the variable name * space after keywords * use pointer-arithmetic if it's a pointer, array-notation if it's an array. Ie. get the `i`-th element of an array and it's address using `a[i]` and `&amp;a[i]` respectively. But use `p+o` for other stuff. * function opening brace on separate line, others on the same. * always put braces, if you have just one small thing in the scope, just use `if (P) { Q; }` * 80 cpl or so
I follow KNF as it's very sensible and pleasant to the eyes. Read [here](https://www.freebsd.org/cgi/man.cgi?query=style&amp;sektion=9&amp;manpath=OpenBSD+6.1) for documentation. As a special rule, I break comments at 72 columns and code at 80 columns. Those missing eight columns make the text a bit easier to read.
Have you tried `git blame`?
Yeah true, I guess my point was more relevant in pre-git days
Since MinGW is a GCC build, you should have libquadmath and `__float128`, so you should be able to do something like #include &lt;stdlib.h&gt; #include &lt;quadmath.h&gt; #undef strtold #define strtold(str, end) ((long double)strtoflt128((str), (end))) or use an `inline` or proper function if you need to make direct reference to it for some reason. You can do #if defined(__MINGW32__) || defined(__MINGW64__) # include &lt;quadmath.h&gt; ... #endif to do that only on the one compiler.
http://astyle.sourceforge.net/ with --style=allman 
Doesn't matter as long as it's consistent, but that said I'd love a version control plugin that quietly strips all whitespace/formatting in a commit, then applies your own style on a checkout.
If you're going to use Python, Kivy is a pretty easy to use GUI framework. If you're just going to be plotting graphs, you're much better off just using something like Matplotlib with Python, rather than writing an entire interface. IPython is the usual route people take when plotting data with Python. Also, there's a lot of fast matrix libraries for Python (numpy), and it's nearly always significantly faster than iterating, so you might want to look into those too.
If you need a result(not learning) - use Python. Life is too short to program in C++.
+1 for IPython
Agreed, I dislike programming in C++ more than programming in C... by an order of magnitude.
Because undefined behavior is well, undefined. One of the possible outcomes is that it may work for you, which is bad. You may also experience nasal demons, time travel and dragons.
Of course you can do that but sometimes it feels a bit wrong to pass a bunch of parameters for a tiny one-off function when it can be avoided (and maybe be clearer) by using block scope. It's down to context, style and clarity. Usually, I prefer smaller functions and lots of them. Sometimes, I'd need to use a macro or inline function to avoid repeated code while also avoiding the cost of a function call. Even more occasionally, block scope is the right solution. C gives you a bunch of tools. The job of the programmer is choosing the right one in the right context.
If you ever decide to do GUI work in C++: From trying both Qt and GTKmm out, Qt is much better as it just has so many more resources behind it for documentation and troubleshooting. In reality: Use python for the love of god. There is a good chance (if my wild guess is correct) you could get away with matplotlib, from the scipy family of packages for plotting. NumPy is also useful, and is exactly what you are suggesting; a python frontend to some C backend with the optimization screws turned to 10. The official Python documentation tells you how to actually create data structures with C backends [here](https://wiki.python.org/moin/IntegratingPythonWithOtherLanguages), [here](https://docs.python.org/2/extending/extending.html) **with worked examples!**, and there exists a FFI library for C code [here](https://cffi.readthedocs.io/en/latest/)
C++ can become very alphabet-soupy... Especially with the introduction of lambdas and their very terse syntax, along with literally everyone online heralding them as the second coming making them so damn popular.
I don't dislike C, but it has serious limitations in some things. But considering I do hobby programming in it occasionally, it's not so bad. But C++, ugh. I start to think of what to program and then I get mentally bogged down by all the structure. Since its needed if I want to write "proper c++" where as C has none of that. Edit: has none of, not have any of.
If you can find an up to date Qt reference/tutorial then you've hit the jackpot. Qt docs just plain suck and are so out of date that the learning curve is just a vertical line. I'd see about putting the data into a file and then use python (or whatever gui you chose) it into the result. People trying the learn Qt.. I needed a good laugh.
You aren't going to like hearing this, but most IDE's that do C also do C++ because of their interoperability. One of the better ones I've found is actually Qt Creator, which you can install sans the whole Qt SDK. Visual Studio Code is actually pretty good and does all those things as well. [Qt Creator](https://www.qt.io/download-open-source/?hsCtaTracking=f977210e-de67-475f-a32b-65cec207fd03%7Cd62710cd-e1db-46aa-8d4d-2f1c1ffdacea#section-2) [VS Code](https://code.visualstudio.com/download) Good Luck.
I am personally using Vim. Works remarkably well in my opinion. Eclipse for C is also an alternative (I don't personally like it though). Atom/Visual Studio Code also works I guess What is it you don't like about Emacs/Clion?
Cheap way to build rapid guis although ugly as hell use SDL and C although python is superb when rapid prototyping is involved, the GTK and QT interfaces in my opinion are absolutely atrocious and among the worst interfaces I've ever attempted to do anything with they suffer greatly because of their abstractions and there are a great number of them GTK has GObject and QT has their preprocessor garbage which in a professional setting isn't too bad when you are working with the developer of those frameworks however independent developers tend to not care about things like portability in personal projects and are about getting shit done asap and those frameworks are terrible at that because it takes a large amount of time invested in learning the framework to use it effectively, so use QT or GTK but only if you are serious about investing time into learning the intracies otherwise build a shitty gui with SDL and if you want it done quick use python if you dont mind a modest amount of effort use C.
&gt;CLion I don't have the money. &gt;Emacs, Vim. I am accustomed to Microsoft key bindings and shortcuts, also GUI.
gVim is vim in a gui, so you can click menus to do a lot of things. You would still need to learn vim keybinds though. I still recommend it, I use Vim at my day job on Win 10, at home on BSDs. I write everything from PowerShell and Batch to C and Python using it.
Even CVS had blame (annotate), so no.. never been relevant.
Then you are probably better off with Visual Studio Code/Atom + Plugins or Eclipse. You could also try GNOME Builder, which is a C IDE built for especially for Linux (it is still in heavy development though)
Eclipse supports C, has stuff like refactoring, works with GDB.
This is my suggestion as well. It just isn't as lightweight as Code::Blocks.
I really like codelite.
Maybe unpopular opinion but you could try SFML w/ C++
Then don't write "proper C++"? That sounds terrible. I don't mind C but C++ has a few little niceties that I reach for from time to time. Write something like [Orthodox C++](https://gist.github.com/bkaradzic/2e39896bc7d8c34e042b) instead.
[This](https://www.amazon.com/All-One-Desk-Reference-Dummies/dp/0764570692) really helped me get started a few years ago. It's not bland, and it's slightly entertaining. 
I'm using Atom, which I love better than CLion ~~even though it uses a lot of memory (IMHO).~~ *(edit: this is file-size dependent, see @ssexbucket's reply and [link](https://www.reddit.com/r/programming/comments/6s3tcu/atom_needs_a_whopping_845_megabytes_to_open_a_6mb/))* You'll need to install some packages to make it work. I have `linter-clang` and a few others that support C in a very friendly way, providing autocomplete and auto-styling. However, as an ex-Sublime user (not for C) I think Sublime was faster and more responsive. I has the similar packages and should support C nicely. However, something in Sublime's design bothered me. I might switch back.
That's true, but if I want to then share the project with others, meaning other people may contribute. And having it C++ means either accepting the oddball C++ness or rejecting commits because they don't conform. If I use C they can do whatever they like and there are no problems (short of them using whitesmith styling or something).
&gt; am accustomed to Microsoft key bindings and shortcuts, also GUI. VS Code then.
CLion by Jetbrains. Works pretty damn well too. 
CLion heavily encourages CMake for build system, for what it's worth
I use SDL/C++, so right up there with you 
Not a direct answer to your question, but can you configure Code::BLocks to limit its code completion, highlighting, etc. to C? As you look at other IDEs, that same question might be useful. For instance, vim does syntax highlighting so you might be able to change the rules; the code completion plugins could also be tweaked if needed. 
+1 for Qt Creator. If you're comfortable in MSVC, you'll feel mostly at home once you get used to the weird sidebar thing.
You could use Sublime then.
~~hey so maybe get visual studio community edition, I believe it's free and comes with what you would need to compile and run C programs~~ damn didn't see you wanted something for linux, my bad
Yea, But you can change it. Honestly I just use it for the auto formatting and the syntax help. I typically compile in a bash shell because I need to use Bison and Flex. But if you aren't using those things, it seems to be easy enough to set up. I am not sure I trust anything to compile my C code correctly until I've tested it a million times though ...
Continue using gtk with C. Object oriented programming is a circlejerk of overemphasizing abstraction compared to working code.
I feel like there is but I have not really looked into it, but are there tools that allow you to change the formatting in your editor?
&gt; I am accustomed to Microsoft key bindings and shortcuts, also GUI. Start learning or stop complaining.
Why "or"? Languages are just tools. They have strengths and weaknesses. They have use cases that they were designed for and use cases where they wouldn't be a great idea. You know the old cliche about "If all you have is a hammer, everything looks like a nail?" Keep more than a hammer in your toolbox -- learn multiple languages.
Start with C. Once you know the fundamental concepts, start with all the complicated C++ features.
I'm not sure it matters that much, the difference being more in the paradigm than in the languages themselves: you could do OOP with C and non OOP with C++.
That's what I kinda figured. Thanks man!
Don't thank him, he's wrong. Modern C and C++ are superficially similar but in truth completely different languages. Once you move past the basic syntax, learning C will not help you learn C++, nor vice versa.
I can see some code whichh says `// FIXME: Handle case of strtold` a few lines down, so I guess it's that.
C++ is easier because it can deal with a lot of the low level details like memory management for you, if written right. Unfortunately most basic learning material doesn't teach it that way.
If your on OSX, I have found XCode to do everything I need it to in a reasonable fashion. Although for the most part I just use vim. 
Would it be good practice to learn the low level skills such as memory management? If it's not necessary to become a skilled programmer, then I'd skip if possible
You have to learn about things like pointers and allocation in both, but they're more intermediate topics in C++ and basic essentials in C.
The problem with Eclipse is that then you have to use Eclipse.
&gt; IMHO no, it does
It might be (and probably was) just the way my projects are organized and the way my workflow is running, but Atom used about twice as much memory as CLion (when I tested Lion as an alternative). Sublime was even kinder to my computer resources. But that was a while back and Atom definitely improved a lot since I compared their resource consumption metrics. Thanks for correcting my impression, @sexbucket (do people avoid writing your handle in their responses, I wonder...?) 🙏🏻👍🏻
&gt; for the most part I just use vim. I have a feeling you might be going back to `vim` because Xcode is *heavy* and complex. I do optimizations with Xcode "Instruments" (and I would prefer valgrind if it weren't broken on macOS), but that's about as much as I can swallow from a full blown IDE. On the other hand, I found `vim` to be a little light for me. I know it can be extended, but there are middle ground options out there that make life easier without adding excessive complexity.
I was just basing the correction on this post I saw today from the programming subreddit: https://www.reddit.com/r/programming/comments/6s3tcu/atom_needs_a_whopping_845_megabytes_to_open_a_6mb/
I disagree. There are plenty of features in C++ that have nothing to do with C, but that doesn't mean that knowing C has no value when learning C++. As far as the standard library goes, at least with libc++, a good deal of standard C is leveraged. `string` uses `cstring`, `cstdio`, `cwchar`, `cstdint`, and `cassert`. `std::thread` and friends (`mutex` etc) are basically just wrappers around pthreads. Have you needed to debug any code related to the non-STL portions of the C++ standard library? Knowing C is practically required. Even `new` is a wrapper around `malloc`. Aside from `std`, a good deal of third-party C++ packages are nothing more than wrappers around a C library. In fact, even if your main application is C++, you might write certain portions in C and then create a C++ wrapper for your own use. Why? Because C libraries are immensely more portable than C++ libraries between machines, compilers, and languages. I contend that things like RAII can only be properly implemented and utilized if the programmer understands the memory model. A lot of C++ was created to improve upon perceived shortcomings with C, while still maintaining reasonable compatibility with C. I don't think C++ is really at the point where you can just ignore C. People like to say that C++11 is a "brand new language" but it's not quite there yet. It's a lot closer than it was pre C++11, though.
&gt; C++ is easier because it can deal with a lot of the low level details like memory management for you Herein lies the rub: C++ offers useful abstractions but you still need to understand how they are implemented. Knowing C well is a great start to appreciating what C++ has to offer *and* using it properly.
Absolutely! Learn about the stack and heap and how memory is managed in both. Understanding memory management is critical when working with C and C++. C++ offers a lot of convenience when it comes to memory management, but it's not as simple as "allocate and forget" like a managed language (Java, C#, Go, etc).
I am not too sure how big your value of n is but let's assume somewhere between 0 and 63 just to make life easy. Then just bit shift and check the lowest bit with a trivial "AND". I have not really tested this but for fun : #include &lt;stddef.h&gt; #include &lt;stdint.h&gt; #include &lt;stdio.h&gt; #include &lt;stdlib.h&gt; #include &lt;string.h&gt; #include &lt;inttypes.h&gt; #include &lt;unistd.h&gt; /* see https://stackoverflow.com/questions/111928/ is-there-a-printf-converter-to-print-in-binary-format */ const char *to_binary( uint64_t x) { static char b[64]; b[0] = '\0'; uint64_t z; for ( z = 0x8000000000000000ull; z &gt; 0; z &gt;&gt;= 1) { strcat(b, ((x &amp; z) == z) ? "1" : "0"); } return b; } int main (int argc, char *argv[]) { uint64_t somebigint; uint8_t bitnum; if ( argc &lt; 3 ) { fprintf ( stderr, "I need to know two numbers.\n" ); fprintf ( stderr, "Some integer and some bit number.\n\n" ); fprintf ( stderr, "usage : %s k n\n", argv[0] ); fprintf ( stderr, " k is some positive number.\n" ); fprintf ( stderr, " n is a bit number from 0 to 63.\n" ); return ( EXIT_FAILURE ); } /* borks up at 2^62 + 1 which is weird */ somebigint = (uint64_t)strtoull( argv[1], (char **)NULL, 10); fprintf ( stdout, "\nI have an integer value %lld\n", somebigint ); fprintf( stdout, "%s binary.\n", to_binary( somebigint ) ); bitnum = (uint8_t)atoi(argv[2]); fprintf ( stdout, "Also a bit number %i\n", bitnum ); fprintf ( stdout, "Where bit number %i", bitnum ); if ( ( somebigint&gt;&gt;bitnum ) &amp; 0x01ull ) { /* note bitwise AND here */ fprintf ( stdout, " is set to one.\n" ); } else { fprintf ( stdout, " is set to zero.\n" ); } return ( EXIT_SUCCESS ); } running that with some numbers I see : $ ./nth 1234567890 0 I have an integer value 1234567890 0000000000000000000000000000000001001001100101100000001011010010 binary. Also a bit number 0 Where bit number 0 is set to zero. $ ./nth I need to know two numbers. Some integer and some bit number. usage : ./nth k n k is some positive number. n is a bit number from 0 to 63. $ $ ./nth 1234 I need to know two numbers. Some integer and some bit number. usage : ./nth k n k is some positive number. n is a bit number from 0 to 63. $ $ ./nth 1234 3 I have an integer value 1234 0000000000000000000000000000000000000000000000000000010011010010 binary. Also a bit number 3 Where bit number 3 is set to zero. $ $ ./nth 24019198012642645 0 I have an integer value 24019198012642645 0000000001010101010101010101010101010101010101010101010101010101 binary. Also a bit number 0 Where bit number 0 is set to one. $ $ $ ./nth 24019198012642645 1 I have an integer value 24019198012642645 0000000001010101010101010101010101010101010101010101010101010101 binary. Also a bit number 1 Where bit number 1 is set to zero. $ ./nth 24019198012642645 2 I have an integer value 24019198012642645 0000000001010101010101010101010101010101010101010101010101010101 binary. Also a bit number 2 Where bit number 2 is set to one. $ ./nth 24019198012642645 3 I have an integer value 24019198012642645 0000000001010101010101010101010101010101010101010101010101010101 binary. Also a bit number 3 Where bit number 3 is set to zero. $ ./nth 24019198012642645 4 I have an integer value 24019198012642645 0000000001010101010101010101010101010101010101010101010101010101 binary. Also a bit number 4 Where bit number 4 is set to one. $ ./nth 24019198012642645 5 I have an integer value 24019198012642645 0000000001010101010101010101010101010101010101010101010101010101 binary. Also a bit number 5 Where bit number 5 is set to zero. $ ./nth 24019198012642645 6 I have an integer value 24019198012642645 0000000001010101010101010101010101010101010101010101010101010101 binary. Also a bit number 6 Where bit number 6 is set to one. $ This totally goes to hell with a really large numbers over 2^63 and then minus 1. 
Thanks!!! and... F#@k, that's 🥜!
To be fair, cmake isn't exactly worse or better than straight make or scons. Not to mention other projects that I use and compile with my applications use cmake as well which makes adding it to my project easier. I don't love cmake, but god our tools are lacking so bad.
I found XCode relatively simple as far as IDE's go, but not particularly intuitive. But I only resort to it on big (for me) projects where I want the completion of a function arguments. 
Shoutout from the RAII master race.
Spent way too long testing cross-platform support for mah Makefiles. Long story short, nothing is what it seems in the dozens of possible shell environments one could potentially invoke "make" from :/ Ended up publishing maymay as a reference for others, enjoy https://github.com/mcandre/meme
I use Xcode, and haven't really had any problems. Tbh I don't get why so many people use java IDEs and other weird shit.
lldb &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; valgrind
why not use lldb?
Fuck style guides, I have my own style.
python is confusing as shit.
&gt; I found XCode relatively simple as far as IDE's go... I agree, it's simpler than other IDEs, but I guess I'm a text editor kinda' guy 😉
VS code
Have you actually used it for C or are you just participating in a circlejerk?
&gt; I don't have the money. Do you have a student email? Then using that should suffice. If you are working on a larger open source project, then that should suffice too. https://www.jetbrains.com/clion/buy/#edition=discounts
The landscape for build tools in C and/or C++ is a bit frustrating. I like CMake and the tools that offer support for it, but I'm also interested in things like Buck, which provides incremental compilation. My desire to play around with different build tools led me to writing a yaml build file in my projects and running a script to generate various build scripts. Now I can just change the yaml file and the other files get updated. I feel something like that should be completely unnecessary though...
[Netbeans](http://netbeans.org) is a pretty decent FOSS IDE which I use for C, C++, Java, Javascript (and HTML5) and Python. It's not lightweight, but it has excellent features and is extremely customizable. 
You're describing C++ the way it was 20 years ago, not the way it is now. And even then, there were enough subtle but significant differences in the semantics of the common subset of C and C++ (especially the type system) that a beginner or intermediate knowledge of C only makes it harder, not easier, to learn C++.
There are many differences, yet I doubt that knowing C would make it any harder at all.
C++ is one of the most complicated languages there are, difficult to understand and especially to know what's going on behind the scenes. And while on other languages you don't need that knowledge, in C++ you do. IMHO a horrible language anyway.
&gt;You're describing C++ the way it was 20 years ago, not the way it is now. That's not true at all. Every example I gave from libc++ is in their git master right now. Just because C++ has range-based for and auto doesn't mean it doesn't still share a great deal of implementation in very C-like code. You're conflating idiomatic modern C++ with some hypothetical new language. What makes it idiomatic is the avoidance of huge swaths of the language. Parts that still exist in terms of billions of lines of existing code. &gt; And even then, there were enough subtle but significant differences in the semantics of the common subset of C and C++ (especially the type system) that a beginner or intermediate knowledge of C only makes it harder, not easier, to learn C++. This is conjecture. I argue that C++ is so unbelievably complex that it's hard to learn, period. Knowing and understanding C can in no way hinder one's understanding of C++. On the contrary, it can only enhance the appreciation and understanding of things like templates and RAII. 
That's not the operation I want. Read the question carefully, it explains what I need.
This is difficult with C as you can do things in C that absolutely confuse syntax highlighters.
Indeed I am wrong. You should totally not learn about expressions, statements, variables, loops, arrays, pointers, storage classes, functions, the preprocessor, and memory allocation. You should just start using templates in a cargo-cult like manner because you never understood what is actually happening.
Memory management is among the most important things when programming in C or C++. You should definitely have a firm grasp on this concept before attempting to write nontrivial programs.
Visual Studio (if you use Windows) is the gold standard. On Linux, good debugger integration is usually where things fall apart for me, to the point where I've given up and just use vim+make+gdb in a terminal now. I have not looked too closely at [Geany](https://www.geany.org) yet, but would like to hear if somebody else has, and what their impressions are. 
Sutter/Alexandrescu: [C++ Coding Standards](http://www.gotw.ca/publications/c++cs.htm). As for coding style, choose something that the IDE that most people on the team use can enforce. If you cannot automate your chosen code style, it won't be followed. Check out [EditorConfig](http://editorconfig.org) and find a plugin for your editor(s). 
Xcode, I use it (through the UI) damn near daily, and I'm like 95% sure I remember hearing it was compatible with gdb, but I've never really gotten into direct CLI debugging so idk.
I've done the basic version of this operation, if you can branch, I'd just check the int's value to narrow down where to start looking from.
Could you elaborate?
Run vimtutor in your terminal.
Two I've found to be reasonable are NetBeans and VS Code. I used Eclipse CDT in the past, but other Eclipse issues made me stop. That said, it worked ok, and had a plugin for Valgrind support, so it might be worth a try. I've done much of my professional Linux C work using just Vim+ctags+cscope+shell, though.
well gee, you're no fun. Damn. The n'th bit is not the n'th bit in the bitstring but the b'th bit that is set to one. Well then ... I see a loop of shifts needed.
the basic version is just looping over the type, typesize times. (so for a 32 bit int, you'd loop 32 times) the branching one is where you check to see if the value is say, over half of the type's max (so for 32 bits if it's over 2^16 - 1, or under that), you can make as many if/else statements to check as many times as you want. this would reduce your search window by half in this example. The problem with this method, is that there's a branching penalty because you've introduced a "fork in the road" but of code, if this is in SIMD data or something, it'd be a bad idea, because they take a long time to branch, and the whole point is to operate on multiple pieces of data at once. Of course, this all assumes it's an unsigned value, if you're checking 2's complement signed integers, you'll have to rejig the algorithm entirely. (namely by checking for the highest unset bit)
I suggest you to use some compilation flags that can sometimes warn you when you triggers some potentially dangerous and unwanted behaviours https://stackoverflow.com/questions/3375697/useful-gcc-flags-for-c/3376483#3376483
&gt;maymay you fucking heretic
My college roommate called them "mee mees". I guess we'd have to ask Dawkins for the original pronunciation.
Better yet, look at the etymology. Which would confirm that it is supposed to be pronounced like "meem".
Agreed. But maymay is more fun!
true, since then you inevitably get conversations like this
comparing to the language, where `[](){}();` is a valid expression? :P
The reason why you were downvoted was it was a google-able question. Pretty much any question you could possibly ask has already been asked before. I'm always willing to help, but I would rather see a fellow programmer help themselves and get better at searching out answers themselves.
Even though I cannot disprove your claim, really especially those related to GTK3+, there is not much material out there. Especially entry level tutorials or any good help for beginners. And the stuff there is, 50% of the time is relevant to GTK2+ (to make things a little bit more complicated...). Having said this, if you are a beginner, you need help with silly questions that have been sometimes answered maybe 100 times, but still, for some reason you cannot find the info you need on that 1 website (stackoverflow). Google is awesome, but it is not God.
What question did you ask on SO? Do you have a link?
I have deleted my account. Too frustrating.
Google will have what you are looking for. Maybe not on the first or second page, but it will. It's ok to ask questions. I'm not saying not to, but actually searching for your answers will make you a better programmer and problem solver by understanding! Edit: letter
that is true. My point is that here you are not 'afraid' of asking something that might be deemed 'silly' and totally worth of downvoting. Let us do an experiment: search for newest 'C' and 'python' posts on stackoverflow, add the votes for the first three pages and check for yourself if what i am saying is actually true. Maybe the python community is made up of only geniuses that only ask clever and 'non-googleable' questions and the C community has got loads of donkeys asking stupid stuff!
Well, StackOverflow is less of a question site and more of a reference. If your question isn't general enough to help others then it probably doesn't belong there.
I don't like this idea of downvoting and not answering duplicate questions; when you ask something, isn't it better to find multiple instances of the question so you can look at different nuances of it and their many answers? I think asking and answering as much as possible is a good thing. Hell, if enough of the same questions were asked in different ways we could even use the answers to train algorithms...
It's first tab is called "Questions" and there are over 14 million of them...
that's literally 4 things to remember, and you can much more easily tell when blocks of code start and end.
Years ago, when I first started on SO, it was composed mostly of professionals. There are rules and directions on what questions can be asked and how to ask them. There are some days when I spend most of my time just downvoting and closing questions which violate most of those rules which are easy to follow. Today, for example, I voted to close five questions which asked for help with a problem with their code but none of those questions showed the code they had a problem with. I vote to close that sort several times almost every day. A lot of questions also come in where it shows the questioner made no effort to find the answer on their own. Unfortunately, there are far more amateurs coming in looking for quick answers like that and there aren't enough of us high rep/mods to stay on top of it. That's why you can still find "loads of donkeys asking stupid stuff". We just haven't gotten to them yet.
Just went through a lot of those GTK2 to GTK3 learning pains myself as I moved a project from windows to raspberry pi. There is definitely not nearly as much information as one would hope for GTK beginners or at least it wasn't written in a way that was easily digestible. All of us in the C programming class were talking about how disdainful and toxic the stack overflow community is. I can't tell you how many times I search google for answer. Top results are stack overflow. Go to stack overflow. A whole bunch of comments insulting the op and telling him to google the answer. This is why I'm sad the stack overflow documentation's beta failed. It looked like a place where repeated and beginner questions would actually just get answers instead of annoyance.
I grew up on the cboard.cprogramming.com community. I am forever indebted to them for making me the programmer I am now. I mean, I tried to research my questions as much as possible but I was like 14 years old and they were my primary tech support... I totally agree that tiny(er) communities can be way more welcoming than stackoverflow and I worry for the next generation of programmers
I disagree. I'm a top 1% guy on StackOverflow, and I'll get down voted for well researched, ungoogleable questions. It's a real problem
All the stacks sites have a different audience and a different public than Reddit. Here we discuss at a personal level, there it's more a run, a competition for the perfect answer. Unlucky, often, it's just a dick competition run by dicks, but in general the stack community doesn't want obvious and noob questions. On the other side you can face the same issues here on Reddit, so ask always to yourself if you've asked the right ~~answer~~ question ~~on~~ in the right way and place. BTW, thank you for your post ;-) Edit: typos Edit2: Thanks /u/icantthinkofone for the patch ;-)
I normally don't downvote. If I find something useful, I upvote.
I've never had a problem with them when I've asked questions, and it's their site, so they can do as they like with it, but it's not really a place I go to hang out. It's like the Wikipedia of programming, except that they're also insulting each other along the way. Curt might describe the tone best. The meta threads are such a mess there. It's a blast to read if schadenfreude is your pastime. I feel like there's a conflict between who they envision themselves as, and who they've become. They might have benefited from being even *more* elitist to keep the new kids out. It's stated pretty clearly that it's intended for 'serious' programmers, not as a friendly place to learn, but the number of posts I've seen that really blew me away are probably one for every ten asking what a variable is. That's to say nothing about the other sites they host. I'm pretty careful to try and not be too inane on there, so I stick with web stuff. For C, I stay here, where my idiocy is tolerated. All that said, they *are* consistent with their tone, and I've picked up more over the years from Stack than probably any other site, ever. If you're willing to put in the time to try working out the answer yourself, and ask a specific question in line with their rules, you will get a useful answer basically 100% of the time. It's not an exaggeration to say that I obtained a job purely from an education off that site and the PHP manual.
I agree with you. Google is not always the best tool and if the answer is available it may not always be clearly understood at your level
I haven't just SO, except as an occasional reference. Would it help to ask questions with an included "I tried Google, with these search terms and the results weren't helpful because x. A useful link or search phrase would be a useful answer."? 
I downvote questions and flag answers all the time. Downvoting answers is trickier cause I lose rep points with each one so I only downvote the real crud answers. Unfortunately, there are a lot of real crud answers lately but I find someone will eventually downvote it anyway.
I turned to IRC, the #archlinux channel on freenode is usually pretty critical but #archlinux-newbie is great for systems programmers 
Why do you dedicate so much of your time downvoting? Why do you think it is so important and what is the benefit?
I'll never use anything that can't digest a compile_commands.json
I'm seeing two topics on here: "I'm new to C programming" and "I'm new to use of the GTK APIs". Most of the conversation seems to be centered around just how unwelcoming Stackoverflow has become, and how Google doesn't have everything you need. When it comes to C programming, might I make a radical suggestion? Read a book. C became popular long before the web was invented, and though it has evolved over the years it hasn't changed as radically as, say, C++. A good book on C basics from the 90's would likely be more helpful than Google scraping; just ignore anything platform-specific ("Learn C programming for Windows 95!") and focus on the language alone and/or command-line POSIX functionality, which is sort of a native environment for C. Once you're more comfortable with the language, figuring out GTK will be a lot easier.
'How many links should I need to go through?' maybe that could be a big point of duplicated questions... to make you click more so it generates more $...
Hum... is Geany good for this?
Like me, I learned C++ a few years ago and now I am having trouble learning C
The more style guides I discover and read, the more I realize that most of them are a complete waste of time. The main jobs that I do are *reading* code, *thinking* about code, and *writing* code. Aspects of a style guide that help me do those things are useful. Everything else is just fluff, or serves some other purpose. There are some pitfalls of C syntax in general that any experienced programmer knows about, and a good style guide should discourage or ban those. Multiple statements on one line is forbidden in most styles, for example. Whitespace. Ugh! Most style guides spend huge amounts of energy talking about whitespace. You know, I have almost never been reading a block of C code and thought "This is just unreadable, if only they had put the braces on a new line I would be able to understand it perfectly." It's such a minor issue, and it's usually a very religious and dogmatic thing. As long as whitespace is used *sensibly*, I will simply follow along. Speaking of which, **"Be consistent"** is a rule that should be in more style guides, preferably near the top. First, be consistent with existing code. If there is no consistency on a particular point, then be consistent with yourself. What style guides *should* spend more time talking about are code patterns. How do you name things? The Linux style guide only has 1 meaningful sentence to say about this: "[Functions] need to have descriptive names.... If you have a function that counts the number of active users, you should call that `count_active_users()` or similar, you should not call it `cntusr()`." That's it? Nothing to say about whether it should be `count_active_users()` or `active_users_count()` or `get_active_users_count()` or `users_active_count()`? A professor of mine, writing about the [history of T](http://www.paulgraham.com/thist.html), gave this useful bit of feedback: "[T]hey chose a standard set of lexemes and a regular way of assembling them into the names of the standard procedures, so that you could easily remember or reconstruct names when you were coding. (I have followed this example in the development of the SRFIs I've done for the Scheme community. It is not an easy task.)" If you look at the standard Java libraries, you see a similar consistency: you know that methods will *always* be named `getFoo()` and `setFoo()`, rather than `fooGet()` and `fooSet()`. Not just naming, but other kinds of consistency are invaluable. Some days before I've had my coffee, I'm thankful that the C standard library function *usually* take arguments in a predictable order. If it modifies memory, it's `dest, src, size` (see `memcpy()`, `memset()`, `strncpy()`, etc.). File, stream, and network APIs always take the file or stream first. Berkeley sockets APIs always take `struct sockaddr* addr, socklen_t len` as a pair in that order. Can you imagine how unbelievably frustrating life would be without that consistency? If the string functions couldn't agree whether the `n` goes in the middle (`strncpy`) or the end (`strcpyn`)? If some functions had parameters `dest, size, src` rather than `dest, src, size`? Yet there are hundreds of projects with just such inconsistencies! That's the kind of thing style guides should work to prevent, but precious few of them do.
Stack overflow is a knowledge base , not a help forum. It sounds like you were looking for a help forum. BTW there is /r/stackoverflow if you want to make further complains.
&gt; I worry for the next generation of programmers They have it easier than ever, precisely *because* of Stack Exchange. The vast majority of times you get stuck as a beginning programmer today, you can google it and find the answer on SO etc. Imagine how it was like programming for a job before the WWW was created, it's no comparison.
You get karma only from upvoted questions and answers. You don't get karma from comments. You lose karma by downvoting; people who downvote are sacrificing their own karma to do so.
&gt; If a student asks a teacher a question, does it facilitate learning to tell the student their question "is a duplicate of another"? Absolutely. Imagine the teacher hands the student a piece of paper that has their question and the answer on it. That's what is happening here.
I have to say, reddit is very nice about helping with programming questions. I never used it much while in school, because I like to learn things the hard way, but it's been very helpful for work (even when my Q were completely ignorant).
you don't lose karma if you downvote questions, i think
Downvoting on SO means something, unlike reddit where it means nothing. Downvoted answers and questions are less likely to be shown when searched but, to be clear, I only downvote flat out wrong answers or questions that don't belong like the ones I mentioned earlier. Sometimes I will vote to close a question but not downvote and vice versa. A question may be off topic but not a bad question so it gets voted to close cause it's not within the scope of what SO is for. Other times it's on topic but a lousy question so I downvote. I don't spend a lot of time "downvoting". I spend a lot of time reviewing. Either going through new questions that popup or through the review queue which I don't remember what rep level you have to have to get access to that. I feel it is my contribution to SO to help clean up the multitude of questions that are submitted for closing or flagged for other things. The last I looked, the review queue is over 9000. I haven't looked to see how many of us have access to that but not everyone goes in there either. So lots of work needs to be done and you can see why I say it takes a lot of time. Many of those questions are, to me, just fine and I can vote to leave them open or even reopen them. Easily half, in my opinion, need to go away.
You're right ; it's -1 for downvoting an answer but no change for voting on a question. So in fact those who downvote questions aren't sacrificing karma. But I also doubt it's a conspiracy to prevent new users overtaking them!
emacs, etags, gcc or clang, gdb, valgrind, gprof... Edit: make, autoconf (I really should look into how suitable cmake is for replacing this), git... 
My claim is that I think a sense of community is important. Simply bashing on the keyboard until you find the right code to copy-and-paste can only get you so far. Also, I contributed back to cboard a lot by solving others questions. They were newbie questions very often but it was very instructive for me to help. If I browse the stackoverflow feed everything is increasingly esoteric, I barely know what half the things are asking even in cases where it overlaps my interests. A young person might not know where they fit in. I think learner's helping learner's is a good model
**I have a legit question: Why does stackoverflow exist?** The community is ridiculously hypocritical. Granted, some communities won't tolerate stupid questions, but [at least be consistent!](http://imgshare.free.fr/uploads/47c21e5712.png) I'm ready to hear your butthurt justification, the kind of butthurt you get when you resort to [censorship](http://imgshare.free.fr/uploads/e31694cc58.png).
make, bash, coreutils, findutils, docker, curl, ...
&gt; it was a google-able question What question cannot be answered by Google? Honestly? Google answers 100% of the questions ever asked in StackOverflow. Furthermore, easily google-able questions are asked in StackOverflow countless times and they don't seem to get down voted. So my question remains -- what's the point of stackoverflow if google exists? Are people expected to google if they run into a problem? Pretty sure he'll find his answer upon a little bit of research. So should the person come to stackoverflow to ask "opinion based" or "non-constructive" questions? Even those aren't allowed afaik since stackoverflow is supposed to be some sort of holy grail that can't have any "stupid" posts.
I've been using Eclipse+CDT with great success. Usually you can take it with you, JRE included, in a pen drive. I doubt any company would prevent you from using it. And the best part is that you have version control and even project management integrated. Also you can use many languages like Fortran, Python, shell script, all in the same IDE.
The only one of those I'd add to my list is make. I either don't use the others (curl, docker, bash as an interactive shell) or they're not super relevant to my C development toolchain. 
&gt; I think learner's helping learner's is a good model Well, it's *something*. It can be the blind leading the blind though. There certainly are communities where this happens and you end up with a whole community believing something that's just wrong . 
On which BSD is Valgrind working properly? FreeBSD?
You've done yourself a favour, stackoverflow is source of many wrong infos and bad advices. I've seen people "selecting" a wrong and mediocre solution to a problem over better ones. 
I know it doesn't work on NetBSD. No clue on Free or Open or Dragonfly; I don't use those. 
Vim or emacs editors. Autotools for a make project that will build, test, and deploy.
It's not what you say then, it's how you say it. It's not a real problem.
&gt; so ask always to yourself if you've asked the right answer on the right way 
&gt; the number of posts I've seen that really blew me away are probably one for every ten asking what a variable is. Yep. It's hard to keep up as I said in another post. The last couple of years have gotten worse since it's gotten mentioned on reddit more often and I can tell we draw this sort of crowd just from the style of question they ask. 
No. We don't need your back story and I will edit that out when I see it. This, too, seems to be a trend lately that's annoying and immediately makes me question what's wrong with your question.
Yes. 1. It means the code is difficult or impossible to understand. You don't know what a function needs anymore and what can impact it. It's much like making everything a global. Why bother even passing it to functions anymore. 2. The true hard problem of programming cache locality. Dram is dead slow. Like crazy slow compared to L1. If you have a structure that can't fit into L1 you may start thrashing cache. 3. Const is a suggestion not a requirememt. While it's always good to do, it doesn't prevent functions from actually writing to structures. It's best to pass values if it can be done, plus it makes thing much more clean. 4. LIMIT. SCOPE. 
bash, vim, curl, gcc, gdb, and of course man-pages
Thanks for replying. &gt; It's best to pass values if it can be done, plus it makes thing much more clean. What about the argument that passing structs as pointers is "faster"? I see some APIs that most of the time only accept pointers as paramenters, and return values to pointer params.
CMake is the bane of my existence
And the hardware?
Im pretty sure scolding an inquisitive student isn't part of the learning process
I didn't know that. Is there an alternative for BSD? In case I ever think to try one. 
What does faster even mean in this case? If it's an api you must do what it says if you are to use it. Passing a structure target than a pointer to a structure is very slow, as it must copy the structure to the stack. However if you only need a few items from a structure then it makes sense to just pass those values.
Interesting. Nonintuitive to me, but maybe I just don't understand the vibe. 
Look at the man page for man. It shows how to get to the various sections: man man Is this what you are looking for? man -3 
The equivalent of spaghetti code on Q&amp;A sites.
Alternately you can install man2html on your local webserver (Linux) and view man pages with a browser here: http://localhost/cgi-bin/man/man2html
It will have information something like this http://man.he.net/man3
It's not being inquisitive that's the problem, it's what you are asking and how.
I have a legit answer: Because there's demand for it. Different story: There are two stackexchanges that are dual to each other, quant and money, the former for professionals, the latter for personal/non-professional matters. In my opinion those roles are clearly advertised and exercised. And people who get it wrong occasionally are genuinely apologetic. What I'm trying to say is that beginners *possibly* suffer from the illusion that only "experts" can help them, and a forum full of experts is therefore the right choice.
This is the correct answer!
You're overthinking it. Just logically group your variables into a struct, and group those structs into a parent struct when it makes sense.
Using array syntax, but if you use pointer syntax it's easy as pie, and supported back to K&amp;R C (I'm pretty sure I haven't actually checked)
valgrind works fine on FreeBSD.
&gt; Implying the megalomaniac autists over there even can tell when they're being insulted
Right? I will say, another good community is com.lang.c on google groups. they can be fucking assholes, but they generally know their shit. and by that I mean they'll start out like stack overflow, but unlike over there they won't just abandon you, they'll be elitist, but still listen and try to understand your problem and what you're trying to do. 
Did you give the people who forgot to include their code any time to edit it in? I know I've done that a few times.
I've heard of #2 a lot on Quora, but mostly in relation to Cheeto worship
Nope, you typedef Token and TokenList in your header, while keeping the implementation details (the actual variables in those structs) in the source file and inaccessible to everything outside of the .c file where they're actually implemented. No, you don't need a header for each source file, I personally have a main header that contains all the public functions called libLibraryName.h in the include folder, then all my other headers are in /include/Private/*.h Now this setup only works for direct libraries, not a library for other libraries. if you're making a library to be used for libraries, you want to expose basically all of your functions publicly so they can be used. for example, I have a library for building libraries and programs called libBitIO, so I have a BitIO.h, and a CommandLineIO.h file to separate out the separate functions from each other, and they're both just in the /include folder.
I mean if you get asked to answer "What's wrong with my code \*Horribly formated code, that makes it hard to read, without any semicolons\*. I tried everything and it didn't work" 10 times a day, you'd be an angry person as well.
I get that but you really can't tell where a student is and it's always a safe assumption that someone is genuinely not capable of figuring it out for themselves. Someone's Google-fu may be weak. Maybe they don't know how to ask about what they don't know. There's just too many variables to assume someone is not trying to honestly learn.
emacs, etags, gcc, make
Emacs, gcc, gdb, scons. My team and I cross compile for ARM Cortex M parts, mostly.
I don't get it, sorry, can you explain? Is there anything wrong in what I've said or are you just highlighting it?
gmake and a bunch of custom bash/perl/python scripts.
This always blow my mind https://www.ossblog.org/learn-c-programming-with-9-excellent-open-source-books/
There is no “one size fits all” setup. Every sufficiently complicated project has special demands not met by the “universal” setup you currently use.
If you're coming from Python, you may find C++ less frustrating as it abstracts away some of the low level details that C exposes you to and comes out of the box (assuming a decent modern compiler) with a lot of useful data structures such as linked lists. You can get most of that in C as well, but you have to start pulling in libraries such as glib or rolling your own. On the other hand, C is such a small language that you may find it easier to pick up as there's less syntax to learn. It also depends on what your end goal is - do you want to write new code in 'modern' C/C++ or contribute to existing projects? For the former you might want to learn C++ first, for the latter I think C is a better starting point. Personally I enjoy writing both C and C++ - I like the simplicity of C and the fact that it forces me to think about the implications of what I'm doing (e.g. creating a huge linked list means allocating memory, whereas in Python it's all done behind the scenes) but I also like the fact that C++ often allows me to write code in a more elegant and compact way.
I tried using it to debug ffmpeg back when I was literally just starting to learn how to program. It didn't work for shit on OS X, so I had to start a ubuntu VM, and it was basically impossible to figure out how the IDE worked, so I said fuck it made an Xcode project and debugged there.
So why not use Visual Studio community edition? that's how I debug my cross platform libraries... it's completely free, and aside from being more difficult to use than Xcode and not having code completion, it gets the job done well enough.
I disagree. I'm a [top 0.45% user](https://stackoverflow.com/users/417501/fuz) on Stack Overflow and I'm almost never down voted for any of my questions. Perhaps it's related to how you ask questions.
When I see a poorly-asked question, I usually proceed like this: 1. Leave a comment indicating what information is missing. Repeat when OP adds partial information. If all information is present, upvote and attempt to answer the question. 2. If OP refuses to provide the missing information (e.g. because its confidental), tell OP to make an [MCVE](https://stackoverflow.com/help/mcve) or similar. 3. If OP doesn't edit his question or answer to comments asking for clarification for half an hour or so, possibly downvote and vote to close. 4. If the information appears after all and I'm still looking at the question, upvote and remove my vote-to-close (or vote to reopen). At each point, I give OP the chance to fix his question. If the question has formatting issues, I fix them. Everybody who cooperates with the community gets my best effort at an answer (if I can answer the question) or as much help as I can give right now. However, an astonishing amount of users just never respond to my comments or show that they have to little knowledge about the subject to even understand an answer, e.g. when someone asks an assembly question but doesn't even know what an architecture is. It is very difficult to help in these cases.
I dunno. Almost none of [my questions](https://stackoverflow.com/users/417501/fuz?tab=questions&amp;sort=newest) could have been answered in any way using Google. Try for yourself!
SO - ah that site which I haven't touched with a 20 foot pole for the past half-decade, and never missed it. That site is filled with self-aggrandizing passive-aggressive cunts.
Is it just me or the last 5 questions you asked are "opinion based" or "RTFM questions"?
[removed]
Do you have any good resources relating to memory management? Coming from Ruby and Javascript, concepts like that are pretty foreign to me. 
Which questions do you mean? The latest few I answered? Or rather the first few?
&gt; How to efficiently ... &gt; Is there a better way ... &gt; How can I effectively Seems pretty opinion based to me
**Asked the** right **answer**? ...on the right way?
That's not the problem. People come in there and are told how questions need to be asked yet ignore the framework of how things work, then get upset when they get slapped for it. If you ask a question that's already a duplicate, it shows you never searched on SO or Google but you'll just get closed for it. If you ask a question like, "What is an int?", it shows far more than you didn't search for it and that's will raise someone's ire. If you ask, as someone did a few days ago, "What's wrong with my code?" and not supply your code and don't respond for a few days, now you're just an idiot and should stay on reddit.
Performance is an objective fact. Therefore, these questions are not “primarily opinion based.” If I would have asked “How to elegantly...,” then my questions would have been closed pretty quickly as “primarily opinion based.”
Yep. You get it.
OT: Help!!! :-P I think you misunderstood, sorry. I didn't have any issues on stackoverflow, I'm a quite well reputed user over there. Here (in general, on reddit) I don't have problems when I ask questions but when I reply to some comments because sometimes I don't care about the reaction and I say what I've to say (without trolling neither). My comment was meant for OP. If he asked the wrong question in the wrong way... Should I write this in plain C, maybe it'll be easier than in my old school English :-D
If performance was really an objective fact there would be one and only one answer to all of those questions. Performance is not objective, it's purely relative. 
If `stdlib.h` is not `#include`d then you get an implicit `int` result from `malloc` instead of a (`void *`) pointer. So on a platform where `int`s are only 32 bits and pointers are 64 bits you typically end up with a truncated pointer - the top 32 bits have been discarded, so your pointer is e.g. `0x0000000076543210` instead of `0xfedcba9876543210` (although note that other outcomes are possible). Writing to `0x0000000076543210` will result in undefined behaviour of some form, which may or may not manifest itself as a crash, "stack smashing", or pretty much anything else you can think of, including your program *appearing* to work correctly.
This is C++ code, not C. You would likely find more assistance at /r/cpp_questions, especially since this seems to deal specifically with C++-unique standard data types.
Done: https://sourceforge.net/p/mingw-w64/mingw-w64/ci/450309b97b2e839ea02887dfaf0f1d10fb5d40cc/ I'll rebuild the 7.1.0 builds this weekend.
im retarded
&gt; So on a platform where ints are only 32 bits and pointers are 64 bits you end up with a truncated pointer Will this actually happen? `malloc` still returns a 64 bit pointer, which gets saved into some register, according to whatever the calling convention is on that platform. The caller will expect a 32bit value, but may read the whole 64bit register instead. In this case, things might just work out. I think the answer to what will happen in this case highly depends on the function calling convention on the platform and the actual compiler used.
Thank you! (That's such a clear and concise explanation. Thanks for posting the hex examples.)
I wish there was a single best solution. Would save me a lot of headache.
It's either that or you get whatever the contents of a register happens to be if int returns in a different register from void *.
Surprised by how many emacs users there are here. I use vim, cscope, etags, clang, gdb / lldb, valgrind, and portable Makefiles (sometimes I'll begrudgingly use autotools if there is a lot of things that need to be managed)
You can't really assume anything of the sort. For example, maybe there is a lot of register pressure and the compiler needs to spill the contents of that register to memory temporarily. It will only save and restore the lower half of the register, so by the time you get around to accessing the value the upper half could be zeros, or random garbage. 
I was teasing you. One doesn't "ask" an answer. One asks a question. And *in* the right way. :)
It depends a lot on the platform architecture and the ABI - it's conceivable you might get lucky in certain cases, but at the end of the day it's still Undefined Behaviour.
"Things might just work out"? That's your thought process? You're getting a nice warning about a cast in your source code, someone explains how it can produce problems, and your response to that is "things might just work out" This explains so much about how many vulnerabilities we find in code, and the awful ones we find in C code in particular. FYI, if you are going to program in C, you need to be much more paranoid, and you should be grateful to have tools that warn you about every little thing. The only person advantaged by your overconfidence is the enemy.
AH! :-D thanks!
On x86-64 specifically, the ABI doesn't specify the contents of the upper half of 64-bit registers when returning 32-bit values. So movq $0xFFFFFFFF00000000, %rax ret is a valid compilation of `int foo() { return 0; }`. As such, something like `(void*)foo()` is required to only look at `%eax`, i.e. the lower 32 bits.
Thanks. I didn't know that. 
visual studio code + gcc
I appreciate the reply, this is helpful advice and the perspective you give is enlightening
You can start with [Head First C](https://www.amazon.com/Head-First-C-Brain-Friendly-Guide/dp/1449399916). I think, this book is good for starting.
To be more clear, I mean a free function for complex types that the caller isn't meant to know implementation details. I would not have a _free method for a char\*, or a new_tostring. Rather a void Type_tostring (Type *self, char *str, size_t str_len); but if I wanted an allocated char\* for calling simplicity, I probably would have both Type_tostring and a char* Type_tostring_new (Type *self); to signify to the caller they have received a new heap pointer The _free would pertain to the Type.
He says he passes *a pointer* to his huge struct, not the struct itself, so your point about memory doesn't hold.
I asked because it doesn't work on OpenBSD either. There is a package but it segfaults when you try to run Valgrind or something like that. And I find it pretty annoying: I know no equivalent (MALLOC_OPTIONS or ElectricFence won't detect as many problems), so I need to move to a Linux box whenever I want to run Valgrind.
Only if you never access the structure. A large structure may not fit in Cache, which means if a function accesses different parts of it it will be terribly slow. If you only have a function have access to small structures it is much much faster. 
It works ok on FreeBSD. There are a few missing syscalls, though, which affects the usefulness of memcheck mode for programs that use those syscalls.
I got my valgrind working again on OSX, btw. I had to compile it from source but I'm not having any issues with it anymore, so you should try again.
I think you're confusing valgrind with gdb.
I will - thanks!
You're best off just adding flags like `-Wmissing-prototypes -Wmissing-declarations -Werror` when you compile rather than worrying about the effects of the compiler trying to guess function return types.
cscope
Yeah, seems like a bit of a Sisyphean task to maintain a standard against the hordes of social media, but I respect the resolution. It's actually been a few years since I actively used Stack, but I remember there being dust-ups over this issue then, too. I can appreciate both sides, I suppose. Of course, if people would just read the rules printed in bold on every page, they would be a lot less likely to be shut down. 
OK. Thank you. I think the article said it was C89 and that later versions of C don't have that problem. I'm really just studying how C works.
This sub is for C, not C#...
Any allocations (malloc, etc.) need to happen once and only once, and the corresponding deallocations (free) need to happen once and only once, no matter what. Failure to adhere strictly to this used to be some of the most common bugs in C and C++ programs, and any other language with manual memory management. For a while now we've had tools like valgrind that *greatly* assist in finding these bugs and keeping them out of a codebase. Coming from languages without memory management, one of the more subtle things to learn is what needs explicitly-coded dynamic allocation (malloc, etc) and what can be or is allocated statically (many arrays) or automatically (many variables).
What you want to do is usually called splitting a string. Also, you want /r/csharp
Definitely possible.
https://www.jetbrains.com/student/ It's free for students!
I mean, you could *at last* spell the names correctly
It's been a few years since I used COM but if I remember correctly all interface functions are also defined as free functions of the form `&lt;interface&gt;_&lt;method&gt;` and taking an instance of the interface as the first parameter. So you would use the `IMMDeviceEnumerator_EnumAudioEndpoints` function.
For any COM object, you can call the member function through the lpVtbl member. But use C++, it's much tidier for COM
vi, make, cc, ctags, cscope, git, lldb
(Under Windows, VS2015 has its place, but I only work with Windows if I absolutely have to. And even then, I mount my working directory on my Mac and edit all my files from there.)
word your looking for is 'least' but cheers.
thankyou will post there now cheers.
&gt; you need to be much more paranoid Very true. I am sick of seeing hacky code that gets published in so many open source projects where it seems to work once on one platform and then it fails everywhere else. However this is the common practice out there in the user world. Code gets hacked, compiled once with some half assed compiler and half assed switches to turn off any real strict compliance and bingo bango we get crap code. When it gets brought over to other servers ( like IBM Power running Red Hat or AIX or think industrial embedded solutions ) then it blows up. Badly. If we are very very lucky it blows up during the compile by "real world" compilers that are very strict. If we are unlucky then it compiles with warnings and blows up during execution. Paranoid or at least "strict compliance mode" should be the standard and not the damn exception. 
Ths 
So, the [other variant](https://stackoverflow.com/a/45484680/417501) I implemented uses such a branching algorithm. However, it turns out that this just doesn't cut it. The branches are so complicated that the “bad” algorithm I posted in the question seems to outperform it by about 5%.
Yes, I think C99 means the compiler has to warn about missing function declarations, and C11 means the compiler has to give an error. But even if you're using C89/C90, tightening up your compiler warning strictness will save you.
Check out [this list of resources for learning C](http://stackoverflow.com/a/562377/253056). 
OK. Thanks. I need to tighten up my c coding approaches.
Unless you're unhappy with autoconf or need a tool that makes your projects more easily portable you probably don't need CMake. I only use it when I'm writing something that will eventually make its way onto a Windows machine.
that's a RIDICULOUSLY large struct my dude... I mean shit even shitty CPUs have like 8KB of L1 cache these days, and that's including embedded?
Although I haven't used his C tutorial, a lot of [The New Boston](https://thenewboston.com/videos.php?cat=14)'s video tutorials are pretty good.
I use Sublime Text 3 for everything. It is just a text editor, but my biggest program has 6 source files, 5 header files, and about 10k lines, so I don't need any fancy IDE features. I've actually been looking for a FOSS alternative, and Gedit is okay.
Indeed they are completely different. But think about it. They share a syntax,they share keywords, they even share libraries, and parts of their names too. Knowing one won't make you know the other, but when you think about it, don't you think that for a beginner, C++ is just a C abstraction layer? Hiding the messy `malloc` and pointers with references and new keywords. Hiding the string management with dedicated classes. Making you skip learning how to make your own data structures 'cause the STL already has everything you need. Yes, they're different languages, but for beginners, it is important to learn what's behind the machine. C++ does a great job at abstracting the low level stuff, but isn't it better for a beginner to learn them? Of course you could start with C++. But before dwelling into polymorphism, templates, iterators, functors, and other abstract concepts, isn't it better to start with the simple statements, functions, loops, memory management, and other fundamental aspects? There's a reason all computer science courses start by teaching you C.
you should take C. Because it's very simple language and you will have good fundamental for learn some higher programing language like C++, Java, Python.. If you totally understand about C. Many programing language inspire from C.
in data structure
&gt; There's a reason all computer science courses start by teaching you C. Define “all”? Because in my experience they start with Java, C#, Python or some other modern GCed language, not C or C++.
That's not a CS class, but an OOP one. You don't learn programming but OOP languages. That's retarded.
You should try both and benchmark. I suspect malloc would be faster just because of no copy. The malloc case is also easier to satisfy for the OS, since allocating one large contiguous chunk of memory is hard. If you made the buffer length small enough, then yes that would matter. There is storage overhead for each malloc. A fix would be realloc into a size with low overhead relative to the buffer size.
?
Thanks for the reply. Is the storage overhead the only downside to having the data finely fragmented? Can it cause also some problems with other routines allocating memory? And do you believe the cpu time overhead when later accessing the fragmented data can also be significant?
Tell that to MIT and Stanford. BTW, MIT used to use Scheme (a Lisp dialect).
&gt;And do you believe the cpu time overhead when later accessing the fragmented data can also be significant? Benchmarking it is pretty much the only way to find out.
As others have mentioned, profiling and benchmarking are the best way to determine where the bottlenecks are in your code. However, it sounds like you are counting chickens before they hatch. Get the code running first, and *if* it is not fast enough, then start worrying about the micro-optimizations. Chances are, the code will be fine as is.
Likewise for Manchester (where the Baby originated). They teach Java in the first year, you only learn a bit of C in the second year for the Data Structures/Algorithms class.
Memory fragmentation is only a problem if the fragments are small. I recommend you to allocate large fragments (e.g. 1 MB) and don't `realloc`. This is because `realloc` is only fast when it finds free memory just behind the memory you already allocated, otherwise it needs to slowly copy the whole data.
You might also consider the target platform. Almost all the computers (vs. embedded systems) use a virtual memory addressing scheme ("Page Tables"), where the memory address used by the process isn't the actual memory address. The actual memory address is resolved by the hardware using a memory map controlled by the kernel. The underlying hardware unit of memory is a "Page" (usually ~4096Kb). Reallocating by the page (`realloc`) means that virtual addressing never needs to copy data. Even if you get a new memory address, the old page simply changes it's virtual address and you get a "new" address from the updated Page Table, even though it's the same page. Large data should (usually) use `realloc` because no data copying will occur. Embedded systems and kernel routines are different since they have access to the actual memory addressing and either manage the memory maps or directly address the memory without a memory map.
Eclipse cdt, make, cmake, git, valgrind, coverity, gcov, doxygen, and an in house linter. The static analysis tools and doxygen are all tied into git hooks and we use the egits eclipse git plugin. Set it up right and you never have to leave eclipse.
What you're looking for is called [multithreading](https://en.wikipedia.org/wiki/Thread_\(computing\)#Multithreading) and is possible in most common programming languages. 
To help filter out the content that is not useful and help direct users to useful content. If you feel your question was useful and you got downvoted, then repost your question with different wording or post it here and ask how you can improve it. I'm not saying its a perfect system, but without seeing your question, I am going to assume it was a question of poor quality and deserved to be filtered out.
Yeah, I've noticed it crashes on OpenBSD. It's really unfortunate. It has something to do with per-process memory limits, right?
As for why globals are, in general, a "bad thing", multi-threading and certain other forms of parallelism are just one reason - it goes a lot further than that (I started to write a list, but realised it was going to take a lot more time and effort than I have available right now). 
thanks! And is it possible in C for each thread to call a different function?
Name an efficient alternative? No one is saying it is perfect. Yes there are false positives and false negatives in regards to question removal based on quality. However, it is leaps and bounds above any alternative.
Yes, but writing multi threaded programs is tricky, and can get a bit mindbending.
I think I understand why now... aside from the multithreading issue, is it not a problem if you have 1000 functions in your program with 'void' input parameter that operate on global variables? It would be difficult to predict the state of a global variable at any point of the program. If, instead you use local variables, it is easier to know which functions use this local variable and how it is manipulated in the program. Does this make sense?
Eclipse cdt does
Let the flame wars commence...
Issues that often occur with concurrent functions is Race Conditions for example: function a{ x = x*x } function b { x++ } a; b; depending on the timing of the functions, there is a variaty of outputs: [x^2 +1, (x+1)^2 , x^2, x+1, etc] this is because the threads will access, process, and push the values back to the stack at different times.
Globals are bad because the idea is you can refer to them anywhere in your code. They maintain what is called "state," that is you end up with behavior that depends on the value of a global and further you can change that state from anywhere. It becomes increasingly hard to reason about how your code behaves and when. Especially when you are or even many people are working in just one part of your code base at a time. It might affect other behaviors in your code but without generating any errors. And this is just in the single-thread case. Throw in parallelism and now it's even harder to reason about what state your in, but then you also have issues with synchronizing the behavior across multiple threads. You run into what are known as data races, where if you don't have properly synchronized threads (can be hard even when you know what to do,) then the state of shared data depends on the arbitrary manner in which that shared data was used at run-time. Eg. A global changes value after you read it, then you write to it without taking into consideration it had changed on you. Your calculation doesn't take into consideration the right state, then you undo the state that the other thread set. This is a problem for both threads, but an even bigger problem if that data was a global.
I know this subreddit doesn't have too much love for oop but I find that c only devs (and lets face it, learning two languages is very time consuming so op will only end up learning 1) tend to never fully comprehend how useful modularization, abstraction, and encapsulation are in concerns to code maintenance. Every c dev I work with that never learned an OOP language has gnarly 5000+ line plus files with 500+ line functions with far too many concerns. Trying to test/verify/maintain code like this without being the original writer is impossible.
Yes, software like this (thousands of globals) does exist ([Toyota, I'm looking at you](http://www.safetyresearch.net/blog/articles/toyota-unintended-acceleration-and-big-bowl-“spaghetti”-code)), and it's a nightmare to work on, as well as potentially having reliability issues. 
I didn't downvote (nor do understand why someone would on such a subjective thread). However, 80 columns per line is an archaic satanic rule and in my experience leads to obnoxiously undescriptive abbreviations and acronyms in function/variable names
Exactly. Using local variables you can see very easily where they are used, because their scope isn't the whole program. Also why I think it's bad to declare all variables at the start of all functions.
Can be hard to debug too!
You seem to have mainly met bad programmers. C code can be incredibly well-structured and easy to understand. It all depends on the programmer. And note that I have just as often seen OOP programmers who encapsulate everything in 10 layers of abstraction until even the simplest things need megabytes of machine code and seconds to execute because all these layers of abstraction kill all remnants of performance. The same people typically defend their lack of decent data structures and common sense in writing algorithms by citing Knuth's words “premature optimization is the root of all evil,” pretending they actually understood what was meant.
Do you need your solutions in one big contiguous space? Otherwise I would suggest allocating them one by one and maybe storing them in a linked list.
I really despise that kind of style as well. I get that it might be needed in some cases where you need to carefully manage the size of the stack, and it's easier to do that when you see all the vars up front. I haven't worked on anything like that, though.
How do you debug them the correct way? I have only did a little bit of multi threading for fun and I admit i did cave man debugging.
Yes it can, if you have two functions running on different threads. When they try to read/write to the same place at the same time, bad things happen. This is a [talk](https://www.youtube.com/watch?v=oV9rvDllKEg) about concurrency and parallelism which is related to what you're asking (in Go, not C, but the concepts are the same, just think of "goroutines" as threads in C)
Good point, hadn't thought about that.
Or might be the case you register system event handlers, so at least you'd have two threads: kernel and your app. Google for "context switching", you'd see that even in single core processors is possible to have multiple parallel running tasks or threads.
Breakpoints can cause unexpected behavior because you can halt one thread while others run. What you can do is print debug statements. It's helpful to use __FILE__ and __LINE__ macros with gcc. You can also have a thread management struct that you can use to print something that identifies the thread. The struct can have pointers to the mutex, conditions, and cleanup functions.
Yes! Multiple ways. The way youre likely looking for is multithreading, which i suggest pthreads for you. Note you can get race conditions and other multithreaded bullshit. The other way is co-routines. Cooroutines swap between parts of code, or in this case functions. You do this with the setjmp and the longjmp methods, this is how you implement exception handling. 
Another couple counter-global arguments unrelated to parallelism: - If your state is global, you make it much harder to break off a useful library component for your core code, which makes it much harder for anyone else to use it, or for you to use it in more than one project. For example, if you make a compiler, it’s useful to be able to call the compiler both as a standalone program and as a component within another program. Non-`static` variables pollute the global namespace, and static state is visible at startup with no (non-extension) ability to use non-compile-time-constant initializers. When other people’s code is mixed in, shit can break easily. If you package your otherwise-global state into a `struct` type or three, you can support multiple instances of your state in the same process, and you make it easier to properly guard and control initialization/deinitialization. Proper encapsulation also makes reasoning about your code easier because you can strictly control what sees what, whereas anything can potentially see/fuck with globals at any time. - If anything in your code uses callbacks or recursion, global or statically declared state can lead to problems with reentrance. (This is closely related to the parallelism problem in practice.) Essentially, if you use something like `strtok` within one routine, and that routine calls another routine that uses `strtok`, the first routine’s state will be wiped. If it was expecting that state to stay there, then it’ll be sorely disappointed when control returns to it. Again, it’s better to package your state up and manage it properly when possible, with each function maintaining its own state. - In addition to reentrance, keeping packaged state makes it much easier for a function to cut and run, but still be able to resume operation where it left off. This is handy for error recovery, iteration, all sorts of stuff. Another quick note: It’s also possible to multiplex function execution *within* a single thread, which lends the illusion of parallelism if the multiplexing is responsive enough. There are a few ways to do this, but the most common general-purpose approach is by using fibers, which are basically program stacks + a little context, typically with lower OS overhead than threads. The green-thread/fiber approach is usually less well-supported by the OS than proper threads, however, so if you’re not careful you’ll run into problems like blocking, signal handling, or TLS sharing. For special-purpose cases, you can use the above cut-and-run approach and have a “scheduler” bounce between functions, calling each with its respective state.
It's probably C89 compliant code.
There are various trade offs. One you've identified already is realloc needing to copy if it can't grow the buffer in place. But one benefit of that is realloc keeps all the memory consecutive. Depending on your memory access pattern, that can be a major benefit. For example, if you need to walk the memory in order, some CPUs can recognize that and start pulling the page you need from main memory into cache before you actually access it. But if you use malloc, when your code needs to move to a new buffer, your more likely to stall your CPU while it pulls the page. Then again, how often will this code be used? If the code will not be used that much, spending multiple development hours to save a few seconds of execution time can be a net loss of time.
I don't know either :) that's why I wrote "or so" -- in small programs it's definitely a reasonable rule. But also in bigger programs I've usually not found it so difficult to stay below 80 cpl actually which comes naturally to me, since I often code on a laptop and in my preferred font size 160 chars, ie. 2 windows next to each other, is the width of my screen. It's really useful to have that limit. It probably depends on what you're coding though. I'm doing much of systems stuff, many functions that are simple and not much user interaction. In my experience this makes it easier to have simpler (in terms of cpl) code.
Printf and __FILE__, __LINE__ macros are your friend (or any other equivalent of your favorite compiler). It sometimes feels like making a fire with two sticks, but yeah.. :) one of the many joys of multithreaded programming. 
It's not that, it's that C-programmers who become C++ programmers carry the C idioms with them which makes their C++ code suboptimal. My advice would be pick the language that aligns most closely with the way you think. C is obviously more linear in terms of design while C++ emphasizes layers of abstraction. 
No idea. I stopped at the "it's really unfortunate" stage :-) You mean stuff like W\^X?
If having the data contiguously stored is irrelevant (no sequential access over every element for instance, or not writing/reading the entire buffer at once) then there is no reason for realloc. You could also start with a conservative buffer size and double it every time the cap is reached, or a similar multiplicative (instead of additive) scheme. This should reduce the number of reallocations while at the same time not wasting space that will not be needed, assuming a good starting guess. If this is going to be better or not really depends on how the problem scales. 
To be fair, the only definition of 'global variable' that article gives is: &gt; A variable is a location in memory that has a number in it. A global variable is any piece of software anywhere in the system can get to that number and read it or write it Which is pretty vague. That can make lots of things qualify as 'global variables' even when they're not really global in the software. I imagine that given definition is not the exact definition they used (Possibly just something the author made-up), but by that definition basically any byte of memory you have that's read/write could qualify as a "global variable" - because if you happened to know the address, any part of your software could read/write it.
fork while fork
Learn C# or stick with Python if you want to do that. Maybe Java for cross platform.
Printing can also mess with timing and "fix" or "break" things.
Even then you can declare them at the beginning of each block, though.
There is no harm in learning c programming no matter which version because the preceding versions gives you knowledge about how things are working under the hood. From my knowledge whatever the programming language and no matter which version gives fundamental concepts to learn.. so you can still learn basic version and then move to lastest version
The C standard does not specify that printf is thread-safe. Some implementations (GNU) have taken the extra step.
Using the latest version is good
Just learn with whatever version is used for the platform/toolchain you're interested in working with. More often than not your hand will be forced on this matter when you move between projects, so just roll with it, the exposure to different iterations of a language and its offshoots will make you a better programmer as you'll start to recognize patterns and will adapt to new languages faster in the future.
For most practical purposes as a beginner, the difference between C89 and C11 is not a huge concern. Other than encountering variable-length arrays, which is in the latest versions of C but not C89. C89 is widely used. You may want to refer to this page which explains the history and difference among versions. https://en.wikipedia.org/wiki/ANSI_C
In addition to multithreading, which others have mentioned, there are other ways that globals can be bad. You've written an application. There's a global variable called `currentFile`. Now you've updated your application so that it's a GUI application. You're editing `currentFile` in a window. Now you've updated your application again so that the user can have multiple windows open at once. Oh, bugger, each window wants to have its own current file. What do you do now? Oh, and `currentFile` was one of several dozen global variables. You've written a USB device driver for a portable device. There's a global variable called `pipeStatus`. Your driver works great. Now the company is building a new project with two USB ports. You've left the company and your successor can't figure out why everything comes crashing down when they use both USB ports at the same time. Your predecessor wrote a graphics library. It has a global variable named `x`. (Yes, I really saw this happen once.) A customer writes an application that links to this library and their application also has a variable named `x`. Application is now failing in bizarre and subtle ways and the customer can't figure out what the hell is going wrong. Oh, and it also fails in subtle ways if someone writes an app that renders in more than one window.
&gt; I am very new to C programming Wow, talk about jumping into the deep end. I recently stumbled across a bit of code in someone else's application that traversed a USB tree non-recursively. The algorithm went something like this: (pseudo-pseudo code) 1. Background: tree is formed by every node having "first child", "next sibling", and "parent" pointers. 2. Start at top of tree. Make that the current node 3. Process the current node 4. If there is a first child node, make that the current node and goto step 3 5. Else, if there is a next sibling node, make that the current node and goto step 3 6. Else, if there is a parent node and that parent has a next sibling, make that the current node and goto 3 7. Else, if there is a grandparent node, and it has a next sibling, go with that 8. Continue following parent nodes up until you find one with a next sibling, or you're back at the top of the tree, in which case you stop In more real code: current = topNode; while (current != NULL) { processNode(current); if (current-&gt;firstChild != NULL) { current = current-&gt;firstChild; continue; } while (current != NULL &amp;&amp; current-&gt;nextSibling == NULL) { current = current-&gt;parent; } if (current != NULL) { current = current-&gt;nextSibling; } } But really, recursive is so much easier.
Well, and declaring variables as you and perhaps compound initializers etc. C99 introduced much stuff that made it easier to write more nifty code.
You'll probably find a book on C99 – it's a standard you can expect on most systems (at least the ones you might face, except good ol' Windows/MSVC perhaps). I think the difference between C89 and C99 is more important than between C99 and C11 (C11 introduced localization support and "native" threads, also made features optional). C99 is syntactically quite similar to C89 with some additions to make your life easier (you can declare the iteration variable inside the loop, for example). Also VLAs and compound initializers. But, while some languages totally changed, C roughly is always been the same. So, with C99 is a good choice, I think.
Another way to look at the first point you make is that global variables are bound at link time before the program is run. This ties different parts of the code together in a very inflexible manner because you cannot change it at run time. In contrast, function parameters are bound at run time by the caller. The same parameter variable can refer to different things at different times and you have control over that. To make a global name refer to something else you need to re-link the program.
I would learn whichever one the resource you prefer is using. That is more key. If you can learn the base of one well, then the syntax differences will be fairly easy to pick up. In general learning a new language is mainly learning all the features and when and why to implement them, the actual structure and words shouldn't be too challenging. 
There are differences but for a beginner they aren't important enough to be noticeable. 
Start with C89 or C99. C99 is a bit more polished and brings a few reasonably common features. C11 doesn't bring much. There's no need to chase a C11 book.
I'm not sure if there is a portable solution for this particular application.
I kinda hoped this would not be the case. Is it possible with GTK+? If so I might just use that
It's worth using C99 just for the C++-style comments.
In my opinion, c99 is the way to go. You'll know if you need c89, if you don't know you need it, you probably won't. Meaning you'd probably already be a pretty experienced c programmer if you find you need it. c11 and c99 are pretty much the same thing, with extra libraries in c11.
C99, because C89 only allows /* */ comments, and // comments are invaluable.
Not unless the thing you want to capture is a gtk+ (or gdk specifically, I think) application and you have a handle for the GdkWindow. On X11 you can do it through libX11 using XGetImage(). Last I heard it was not an option on Wayland because of the security issues it could bring, but things may have changed.
Many libraries are C99 compatible for cross platform compatibility.
You could use a bitmap: enum interval { UNISON = 1 &lt;&lt; 0, ROOT = 1 &lt;&lt; 0, MINOR_SECOND = 1 &lt;&lt; 1, DIMINISHED_THIRD = 1 &lt;&lt; 1, /* ... */ MAJOR_TONIC = ROOT | MAJOR_THIRD | MAJOR_FIFTH, MINOR_TONIC = ROOT | MINOR_THIRD | MAJOR_FIFTH, /* ... */ }; Though, I'm not exactly sure what you want. Note that this subreddit is about C only. If you want to program in C++ (which seems to be the case) please ask elsewhere, e.g. in /r/cpp_questions.
Assuming all chords have exactly three intervals, you could use that to your advantage. Something like: typedef enum { MAJOR = 0, MINOR = 1, ... } CHORDS; INTERVALS chord_data[] = { ROOT, MAJOR_THIRD, MAJOR_FIFTH, ROOT, MINOR_THIRD, MAJOR_FIFTH, ... }; Now you can get intervals for a specific chord by doing: chord_data[chord * 3]; // First interval chord_data[chord * 3 + 1]; chord_data[chord * 3 + 2]; Totally manual and easy to mess up (you need to keep CHORDS and chord_data in sync) but also quite simple.
Thanks for the response. I don't require a C++ answer as I'm only use C to do this. In music theory a **chord** is made up of multiple **intervals**. If you've ever played a piano a chord is when you press 3 keys at once. There's a large list of chord types, that are all made up of intervals. In my example **major** and **minor** are just 2 chords, there's lots more. Currently in my project I have the chords defined as `ints`, like this: Chord chords[2] = { { "major": { 0, 4, 7 } }, { "minor": { 0, 3, 7 } }, ... } But I have no way of accessing these chords by their name, i.e. `major` or `minor`. **edit** I see you used the word tonic. You already know theory ha ha. Ignore my first bit of this comment.
Yes, I know what chords and intervals are. However, I do not know what you want to achieve. Perhaps if I knew what sort of operations you want to perform on intervals, I could help you find a better solution.