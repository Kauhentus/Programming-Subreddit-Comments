Not sure why the thread wouldn't terminate, although you have some more uninitialized rows and cols variables in findVariance() and worker_func3(). Hit CTRL-C when the program is hung in the debugger. Then you can run the "bt" command to see where it is hanging.
Thanks again, it's getting stuck when I'm calling pthread_join() in the main function. 
That's where the main thread is waiting, but you need the other thread. If you do info th It will list the threads, and you can switch to another by doing t x Where x is the thread number 
If `c` is a char and you write `putchar(c);` or `printf("%c", c);` then the character value of&amp;nbsp;`c` will be written to `stdout`, even if `c == '\0'`. So you can loop through the character array yourself and output each character individually this way. Of course, you'll need a different termination condition than testing for the null characterâ€”you will need to know the length of the array.
There is no way the compiler should be optimizing that out. I just tested the following code, and it writes ten null characters to `stdout`, as expected. #include &lt;stdio.h&gt; int main(void) { int i; for (i = 0; i &lt; 10; ++i) putchar('\0'); return 0; }
Seems to be in the findMean(), findSTD() and findVariance() functions. The third thread is getting stuck in one of the three each time the program is ran. Unfortunately I've ran out of time, so I'm going to have to submit what I have as of now. Thanks again, wouldn't have gotten nearly as far as I did without your help.
Remember that C functions have NUL bytes for termination, so if you were using a standard library function on your string, it almost definitely stopped at the first NUL byte it reached. That's how C strings work. Be conscious of that when you absolutely need to deal with NUL bytes. Pro-tip: don't just store the length in the first byte of the array, which is error-prone and type-unsafe. Use a struct (maybe class for C++).
this while loop: while(current != NULL){ previous = current; current = current-&gt;next; } finishes when current == NULL and the while loop after: while((current != NULL) &amp;&amp; (current-&gt;vehicleID &gt; searchID)){ current = previous; previous = previous-&gt;previous; } only is entered when current != NULL so it never enters the while loop. 
If you were just writing to a file then you could just make your ofstream binary and that would ignore the null terminators. I'm not sure if this will help you though.
The first loop should be: while(current-&gt;next != NULL) That will set you on the last node of the list instead of overstepping into NULL like you're doing now. When you're doing removal of a node from a doubly linked list, remember that you need to adjust the next pointer of the previous node and the previous pointer of the next node with respect to the node you're removing. Like: current-&gt;previous-&gt;next = current-&gt;next; current-&gt;next-&gt;previous = current-&gt;previous;
I was trying to be vague because I thought you could learn by trying to do it but I think I confused you more than helped. My apologies. For diagonal use 1 loop to find it. Example. int 2Darray [10][10]; /* assume it has numbers etc */ for (int i = 0; i &lt; 10; i++) { printf("Element at [%d][%d] = %d\n", i, i, 2Darray[i][i]); } so instead of having a nested for-loop for row/col and only printing out when Row == col -- just have a loop and use it for the index of col and row -- it gets you the diagonal values by just doing 1 for loop which is quicker than using the nested method you were using. That is all I was trying to hint to there. 
When posting code on reddit -- you have to indent the code by an extra 4 spaces. Reddit will then format it so it is easier to read. You can also post the code on gist (Gist) [https://gist.github.com/] then post a link to your public code on there to read. It makes it easier. I formatted the above for all to look at. #include &lt;stdio.h&gt; #define SIZE 50; int main(void) { int A[SIZE][SIZE]; int i=0, j=0; int num_occ, value=0; for(i=0; i &lt; SIZE; i++) { for(j=0; j &lt; SIZE; j++) { A[i][j] = rand() % 2; } } for(i=0; i &lt; SIZE; i++) { for(j=0; j &lt; SIZE; j++) { if (i == j){ printf("%2d", A[i][j]); } } } }
Thanks for the reddit posting code rule..i didnt know that. Anyway heres the link to my code...which i changed with your suggestions https://gist.github.com/anonymous/ff0c226202a0fd37b35e I still get the value 0, 22 times and the value 1, 28 times...you think this is correct ?
Your function seems wierd. I think it has to take int A[][] and not just A[] as it has 2 dimensions and your if check should check A[i][i] for the diagonal. Possible. Couple of things you can do to "test" your code. Or verify that you are getting results is by putting in extra print statements or counts to double check your results. Feel feel to comment in/out extra code (if it is an assignment you can delete you test cases below once you confirm you like what you see, etc) On the line you set the random value A[i][j] = rand() % 2; comment it out with a /* */ and change it to A[i][j] = 1; Run the program - should be 50 1s and 0 0s. Then do it again but change it A[i][j] = 0; Run the program should be 50 0s and 0 1s. Then if you want to check the rand to make sure it matches up with your code later on. you could make your code count as you initialize with this example. int num; int oneCount = 0; int zeroCount = 0; for(i=0; i &lt; SIZE; i++) { for(j=0; j &lt; SIZE; j++) { num = rand() % 2; if (i == j) { // diagonal if (num == 0) { zeroCount++; } else { oneCount++; } } A[i][j] = num; } } printf("Diagonal 0s = %d &amp; 1s = %d\n", zeroCount, oneCount); So I am using 2 extra ints to count the 1s and 0s. I used a 3rd int to store the random number and before I assign it in I check to see if I am on a diagonal and then increment my counters. I then display the check. Then later on when you call your function to count the zero or ones you can compare it to the above and it should match. Testing your program using the 3 above will cover the cases where you know all the data is 0 or all your data is 1. The third case is a second way to verify the count. Your final program wouldn't have this count in it as you wish to count the diagonals using your function. This check thou double checks it. 
Could it be possible....I think....IT WORKED? Here's my new code: #include &lt;stdio.h&gt; #define SIZE 50 int main(void) { int A[SIZE][SIZE]; int i=0, j=0; int num_occ, value=0; int num; int oneCount =0; int zeroCount =0; /* Initializing*/ for(i=0; i &lt; SIZE; i++) { for(j=0; j &lt; SIZE; j++) { A[i][j] =1; } } for(i=0; i &lt; SIZE; i++) { { printf("Element at [%d][%d] = %d\n", i, i, A[i][i]); printf("Element at [%d][%d] = %d\n", i, i, A[j][j]); } } for(i=0; i &lt; SIZE; i++) { for(j=0; j &lt; SIZE; j++) { num = rand() % 2; if (i == j) { // diagonal if (num == 0) { zeroCount++; } else { oneCount++; } } A[i][j] = num; } } printf("Diagonal 0s = %d &amp; 1s = %d\n", zeroCount, oneCount); } *and this is my output* Element at [0][0] = 1 Element at [0][0] = 0 Element at [1][1] = 1 Element at [1][1] = 0 Element at [2][2] = 1 Element at [2][2] = 0 Element at [3][3] = 1 Element at [3][3] = 0 Element at [4][4] = 1 Element at [4][4] = 0 Element at [5][5] = 1 Element at [5][5] = 0 Element at [6][6] = 1 Element at [6][6] = 0 . . . . Element at [48][48] = 1 Element at [48][48] = 0 Element at [49][49] = 1 Element at [49][49] = 0 Diagonal 0s = 25 &amp; 1s = 25 
drop the 2nd printf in the diagonal display int value is not used -- can drop it. int num_occ is not used -- can drop it Something I just thought. The only values that can be stored in the array is a 0 or a 1. So we don't have to have 2 variables to count the 0s and the 1s. Just need 1 variable to keep a running total of the total value. Since the value is just 1 or 0 the final total will be the number of 1s in the diagonal. And to get the number of 0s we just subtract that total from SIZE since whatever is NOT a 1 will be a 0 and so the total of 1s + 0s should equal SIZE. My solution to the problem. #include &lt;stdio.h&gt; #define SIZE 50 int main(void) { int A[SIZE][SIZE]; int i, j; int oneCount = 0; /* *Initialize with random number between 0 and 1 *Display the Values at the Diagonal *Count the 1s and 0s and display at end */ for(i=0; i &lt; SIZE; i++) { for(j=0; j &lt; SIZE; j++) { A[i][j] = rand() % 2; if (i == j) { oneCount = oneCount + A[i][j]; printf("Value at A[%2d][%2d] = %d\n", i, i, A[i][j]); } } } printf("\nCount of Diagonal Values\nNumber of 0s = %2d\nNumber of 1s = %2d\n", (SIZE - oneCount), oneCount); return 0; } Do you understand what I did or why it works? 
Thank you yes I understood...when I erased the value=o; I was getting a value of 267890 instead of 0s... So, pretty much u went to counting 2 variables to counting just 1..in this case 0...After that, you printed the counts of 1s, and after that you printed the counts of zeros..to achieve that you subtracted SIZE minus the counted the Os , so u can get the counts of 1s...GENIUS Thanks so much for this...I would've never thought of it like that
I know, this.function is to find the previous node. This function takes in the headpointer and returns the previous node.
No -- it isn't.
What isn't? You mean this? &gt; &gt; Operands should be promoted to the largest int-like type in that expression. &gt; &gt; Oh, but the largest int-like type is unsigned int. 
This is a somewhat frightening proposal, since people's wellbeing could potentially be *greatly* impacted by any problems with this software, including problems that you would not be able to foresee. This sounds like a common requirement in an ER setting, and I'd be surprised if existing, extensively tested solutions don't already exist. I'd look into that before attempting anything on your own.
Don't use C for this. C has many strengths, but when it comes to dealing with UIs and tons of strings, you'd have a much better time using something like C# or Python.
Care to explain? I'm not an expert but it seems more efficient (imo) to use a smaller set of variables with bit-wise operations to change "game state" than to use fifty million boolean variables/flags.
It's good news to hear that you know of the limitations of the existing solution(s). Look for gaps in existing software and fill in the gaps!
As patient's come into the ER, their info is put in the computer and once they get into a room a doc can sign up to take care of them. lab values and x-ray's, CT results are viewed in this program. It's not really for rx tracking, just patient tracking. It doesn't store long term patient data, the program is basically needed to display medical history from what the patient tells the triage nurse. And once the pt is admitted/discharged they just disappear from the tracking board. The big problem is that the software is terribly inefficient, and the doc/nurses have to click way too many times to get a simple task done. For example, a way to make it better is if a pt is complaining of chest pain, a default list of common blood tests that are usually tested with chest pains should be used, instead of having the doc click the same 10 blood labs. And the software crashes all the time, leaving us without it for hours at a time. 
Step 1. Liability insurance.
You dont want bugs in there
Wat? Why?
It may be helpful, but it's not entirely accurate. * "Most C compilers operate only one step above assembly" is really pushing it hard. Yes, C is a close-to-the-metal system programming language. However, most C compilers are very good at optimizing, and in order to optimize they use one or more intermediate forms---if they really dealt only in assembly language, they'd suck at their jobs. Clang, for example, has an intermediate representation that is assembly-ish but definitely not, and that can be used independently of the program source and binary. Furthermore, just abouy any compiler can generate assembly code but some (e.g., ICC) skip the assembly-language step entirely and just dump machine code. (Hell, ICC will even try to parse inline assembly language into a higher-level representation so it doesn't have to use the assembler at all.) * A single lexical variable may have more than one address over the course of a program's execution. E.g., a local variable sits relative to the stack pointer (recursion-&gt;multiple copies of a variable), a thread-local variable sits relative to the TLS area (spawning threads-&gt;multiple copies), and there's technically nothing preventing the compiler or runtime from rearranging memory dynamically as it sees fit as long as the compiler follows the pointer rules in the standards. * The compiler does not use headers like limits.h to figure out information about data types---that's completely backwards (and a Very Bad Idea besides). What generally happens is that the compiler feeds the information exposed by limits.h/float.h into the preprocessor---if you have GCC or something vaguely similar, you can do gcc -dM -E - &lt;/dev/null to get a fairly complete dump of preprocessor information fed in; e.g., mine has "#define __CHAR_BIT__ 8", which then shows up from limits.h aliased as "CHAR_BIT". The sizes, alignments, and formats of compiler-recognized data types are typically defined by the platform's ABI (application-binary interface), and are pretty much hard-coded into the compiler to ensure that system components don't break when they try to interact. * limits.h and float.h (*not* "floats.h" as per the article) do not define the data types in question, they define constants that describe features of those data types for the programmer's benefit. The built-in types are built-in, and can't be defined or redefined by the programmer. (The closest you can get is to break the Rules and #define a built-in type, but that doesn't really count because the compiler proper never sees anything but the macro expansion. There may be compilers that allow data types to be set with #pragma/_Pragma directives, but I've never encountered such a thing.) This is very different from typedefs, structs, unions, and enums, which *can* be defined within headers and *do* actually describe types to the compiler. 
True, but sometimes the warning needs to be enabled explicitly (-Wall and the like) so he might not have seen it.
In my experience, compilers aren't super-great at making decisions about whether to treat bitfields as independent entities or pieces of a larger number. (I.e., the compiler has to view the bitfield as union { some_unsigned_intish_t value; struct { int a : 1, bit : 4, field : 8; }; }; and it's not as well-informed as the programmer when it comes to choosing between accessing 'value' as a whole or 'bit' specifically, knowing what the starting and result value ranges are, or dropping unnecessary access instructions.) When reading and writing, there's no good way around having to OR, AND, and shift things around; some ISAs have special bit-string insertion/extraction instructions to help, though, in which case you're a little better off. Bitfields are rarely used in practice, so the compiler doesn't typically try all that hard to optimize around them. They're not very standardized and not usually covered in much detail by the ABI, which means that two compilers may have very different ideas about field layout and packing (hell, the same compiler can change its mind whenever it wants), which means that such data structures can't really be used in public APIs very extensively. Compilers also differ about what types (other than unsigned/int) they allow bitfields to possess; _Bool and enum types are nonstandard, although they're allowed by C++ and describe the two most reasonable uses for bitfields. That being said, bitfields are okay for private data that you don't access frequently, if the data savings actually helps (e.g., keeps the structure within a cache line). Per most ISAs, using a single-bit field as a Boolean value (i.e., typically predicating a branch) is no slower than using a non-bitfield value, although RISC architectures may still require an extra instruction if they only have a branch-if-(non-)zero and not a branch-if-bit-set.
Ah, so setting flags is slow, but reading them is fast, and using ints/bools only uses more RAM? Thanks for the clarification.
Almost. Reading a bit from a bitfield may or may not be fast, depending on the CPU arch, and only if you define your bitfields like /u/nerd4code did.
I'm not sure what the problem is precisely but there is a fundamental problem with trying to think that macros can allow for dynamic programming. Macros are a programmatic way to essentially do a find-replace in the source before it is read and compiled. Macros do *not* end up in the object file or final executable at all. If you are trying to do anything other than a find-replace for a macro it is probably a bad idea in my opinion. Even constants are better served as a static global or some form of enum.
hm okay, I could try to make a function "module_call" which works with function pointers. 
I think you have some misunderstanding. What exactly do you wish to do? If it is to call a function dynamically from some library - this is easy. You can use dtsym or GetProcAddress depending your system. See [here](http://en.wikipedia.org/wiki/Dynamic_loading) for more information. If you are wanting to call a function dynamically but all you have is the argument data in some form you need to use [libffi](http://sourceware.org/libffi/) or you can also use a macro trick I explain the basis of in [this](http://theorangeduck.com/page/autoc-tools) blog post. The macro is fairly complicated though so requires some understanding. Finally if you are trying to call some string with some C code inside then you are more or less out of luck.
My plan is be able to add new function without having anything to change in other source files. Also I was playing with macros and wanted to find out if it's possible to do such with macros, since it can combine things (##) and call functions. Appearantly it doesn't work like I wanted it to do
What is it?
Your example doesn't compile. I assume you want to assign to `u.i` and read from `u.s.*`.
Endianness is more platform dependent than compiler dependent, though the compiler has leeway on bi-endian systems. But I agree with the rest of what you said. 
My mistake, I wasn't clear enough. "It" was supposed to mean bitfields storage order.
Everyone who looks at the code example should also recognize this. Yes, a union puts a few objects in the same block of memory -- that's just the definition of a union. It cannot be used to reliably "transform" binary data from one type to another as the OP suggests.
This use of unions invokes undefined behavior, and can actually break when strict aliasing is involved (when the compiler can assume that no 2 pointers of different types are actually the same). When using a union, you are only allowed to use the union member that was last assigned to. In other words, if you last assigned a value to u.a, you are not allowed to access a union member other than u.a until you first assign it something. The only exception to this rule is if the currently active member of the union shares a "common initial sequence" with others, then you can access the common portion of any of the applicable union members. To quote the standard: &gt; Two structures share a "common initial sequence" if corresponding members have compatible types (and, for bit-fields, the same widths) for a sequence of one or more initial members. The following would be a valid use of unions: struct S { char a; char b; }; struct T { char c; }; union U { struct S s; struct T t; }; int main(void) { union U u; u.s.a = 'a'; printf("%d, ", u.t.c); } Because the members of the union, s and t, share a common initial sequence of a single 'char' (a and c), you can access this char of either at any time (as long as one of them is the active union member). 
Wow, I can't believe I got a reply to a 3-month-old comment. http://unixjunkie.blogspot.com/2006/02/char-apple-argument-vector.html
Nifty! (I just discovered /r/C_Programming and am reading through all the stuff posted here, hence the comment on a super old post...)
0x01020304
 u.i = 0x01020304; FTFY.
lol thanks...embarrassing
&gt; This use of unions invokes undefined behavior It's unspecified rather than undefined, and only in C standards older than '99. Otherwise it's one of only two legal ways to type pun (the other being memcpy).
I read "Unicorn can be used...", and was left a little disappointed when I saw your actual question didn't involve any.
I think casting is a bit slower thanks to it creating a new variable in memory... But I'm not sure at all... Same case as preincrement vs postincrement, I guess :-) Edit: was meant as a response to sdrawkcabsmurd, derp
Seg fault means you are accessing memory not allocated or exceeding some boundary. There is a way in the debugger to show the stack. Check for any null data on pointers being sent to insert from addline.. If you are accessing a pointer that is null -- boom Think you can compile with gcc with a flag turned on. Then when you seg fault it will core dump. You can load the core into gdb and do a stack trace to check for the pointer access causing the seg fault.
1. No space is allocated for `input`. It has to be big enough to hold the largest input string you expect, including the null terminator. If you don't know how long the input will be, don't use scanf. 2. You're allocating 82 chars for `new_item-&gt;name`. If `name` is longer, `strcpy()` will barf. Why not allocate `strlen(name)`? 3. I'm not sure if `sizeof(*new_item)` will work, especially since `new_item` is `NULL`. I would use `sizeof(Node)`.
Or you could use Valgrind, passing -g to gcc should be enough.
Of course not. gcc is a compiler. OP wants to debug his own executable, not the implementation of libc, from what I understand. To debug the libc implementation, OS' usually also ship executables with debug symbols, for example, on debian based distros one might do: apt-get install libc6-dbg
I did not. You misunderstood. I suggested `gcc -g` for his own code. Edit: Also, there's rarely any need to debug code written by other people.
&gt; &amp;#51;. I'm not sure if `sizeof(*new_item)` will work, especially since `new_item` is `NULL`. I would use `sizeof(Node)`. This is not a problem; `sizeof(*new_item)` will work. The `sizeof` operator is evaluated at compile time, not at run time, so it doesn't matter what the *value* of `new_item` is; the pointer isn't actually dereferenced. Instead, the compiler merely looks at the *type* of the expression `*new_item` to determine the size. I would use `sizeof(*new_item)` instead of `sizeof(Node)` because it's a bit safer. If you use `sizeof(*new_item)` and later you change the type of `new_item`, you don't have to remember to change the `malloc` line too. I have run into several errors that are very strange and difficult to debug because I forgot to change a `malloc` when I changed the type of a variable, so now I always use something like `malloc(sizeof(*new_item))` instead of `malloc(sizeof(Node))` to prevent that kind of error.
input it pointing to where? So, boooooom, you are using scanf on pointer that is pointing to some random memory address. Either use malloc() to allocate memory for input or if you want to use stack do something like char input[1024] but then you have problem of possible buffer overflow (ok you could check for return value of scanf and see if it's bigger then input size).
Wouldn't it be nicer to just print an error message and exit? Assert is needed for debugging purposes but is useless to call when memory ends.
Maybe malloc() failling was actually your fault, because you had a serious leak somewhere or so. Then the coredump may still be useful to track down the issue.
Yes, but atexit will no more work.
That is typically the case in my experience. Unless doing some embedded programming on a very small memory system, it is difficult to exhaust all of the memory available to you (either all of the wired and virtual memory or what ever is allotted to you via per-user and per-process limits). If I run out of memory it usually points to an error in the program. Now if you know there is a chance you may legitimately need more memory than there is available to you then I guess it is reasonable to print an error message.
Interesting fact: on Linux with default kernel config malloc virtually never fails. http://opsmonkey.blogspot.com/2007/01/linux-memory-overcommit.html
Usually, at least on Linux, malloc doesn't fail because you are already using too much memory. It fails because you requested an insane amount of memory in a single call to malloc, which is usually a bug on your part.
First build with -Wall, the warnings should give you a lot of hints here I didn't check many things, but sometimes you use head=(struct node*)malloc(sizeof(struct node)); and sometimes head=(struct node)malloc(sizeof(struct node)); Also struct node ptr1, *ptr2 is probably not how you want to declare ptr1 Now for small programs like that, just use gdb or anyother step by step debug program and check the pointers. Your pointers will fail, you will always do some kind of mistake when using pointers, this is what will be wrong in your code 40% of the time (other 40% are table boundaries... yeah these stats are made of but your pointers will be the reason why your code crash really often) 
Re-formatted for clarity: #include &lt;stdio.h&gt; #include &lt;conio.h&gt; #include &lt;stdlib.h&gt; #include &lt;string.h&gt; struct node { int data; struct node *next; } *head = NULL; void insert_node(); void insert_sorted(); void display(); int main(int argc, char *argvp[]) { int x = 1; do { printf ("\n\nPress 1 for insert\nPress 2 for insert_sorted\nPress 3 for Display\nPress 4 to exit\nEntry="); scanf("%d", &amp;x); if (x == 1) insert_node(); if (x == 2) insert_sorted(); if (x == 3) display(); if (x == 4) printf("\n\nThankyou Bye!"); } while (x != 0); getch(); } //----------------------------------------------------------------------// void insert_node() { int item; printf("Enter the number you wish to insert="); scanf("%d", &amp;item); if (head == NULL) { head = (struct node *) malloc(sizeof (struct node)); head-&gt;data = item; head-&gt;next = NULL; } struct node *ptr; ptr = head; while (ptr-&gt;next != NULL) { ptr = ptr-&gt;next; } ptr-&gt;next = (struct node *) malloc(sizeof (struct node)); ptr-&gt;next-&gt;data = item; ptr-&gt;next-&gt;next = NULL; } //----------------------------------------------------------------------// void insert_sorted() { struct node ptr1, *ptr2; int item; printf("Enter the number you wish to insert="); scanf("%d", &amp;item); if (head-&gt;data &gt; item) { ptr1 = head; head = (struct node) malloc(sizeof (struct node)); head-&gt;data = item; head-&gt;next = ptr1; } else { ptr1 = head; while (ptr1-&gt;next != NULL) { if (ptr1-&gt;next-&gt;data &gt; item) break; ptr1 = ptr1-&gt;next; } ptr2 = ptr1-&gt;next; ptr1-&gt;next = (struct node *) malloc(sizeof (struct node)); ptr1-&gt;next-&gt;data = item; ptr1-&gt;next-&gt;next = ptr2; } } //----------------------------------------------------------------------// void display() { struct node *ptr2; ptr2 = head; if (ptr2 == NULL) { printf("Linked List empty"); } while (ptr2 != NULL) { ptr2 = ptr2-&gt;next; printf("\n%d", ptr2-&gt;data); } system("pause"); } 
Here are some notes: * There is a bug in display() that had it immediately skip the first node. This causes a segfault when the list size is 1. * I would probably add a new_node() function. * You can remove prototypes if you rearrange your function definitions. * There is an incompatible pointer assignment around line 61 (ptr1 = head). * You don't need to cast the result of malloc(). * Separate insert_node and insert_sorted into a interactive versions that prompt the user and functions that actually do the insertion. eta: * You may be able to use a pointer to a pointer (struct node **) to make insert_sorted() and insert_node() much simpler. * You should use better variable names. Instead of ptr1, ptr2, etc. Try current or even cur, previous or prev, etc. It would make reading the code easier. * Your display() main loop could be simplified quite a bit. Try using a for loop. Also look into C99 style for(;;) loop variable declarations. * Check the return value of your scanf() calls. * Consider parameterizing all of your functions and getting rid of the global head variable. * Why not use switch() instead of a bunch of *if* statements in main()? 
I have sorted that out, and I yes, it did look like scrambled eggs.
Thanks for your input. Can you please specify, what is wrong with the display() function? I tried to use a temporary pointer to make sure I can traverse properly, but that came to no use.
Thanks for your input, I will fix that. As I said, this is my first linked list program, and first program for anything aside arrays and strings. Thanks for the corrections.
And if it does, the "OOM killer" starts killing random processes, so trying to do anything when malloc fails is futile. If malloc fails, your process is no longer executing. 
Remember to always build with -Wall and don't let any warning appears during compilation. There are only a few cases where you might let some, and they are for highly optimized code. If you leave any warning you must be able to justify them. And if you just start C, there are no good reason to leave any. Insert_sorted will fail if you call it on an empty list. If the list is empty you should call insert_node. Also insert_node creates two elements in the list when the list is empty. And you should do the printf before the ptr2=ptr2-&gt;next in display. This would remove the seg fault. You check if ptr2 != NULL then you set ptr2 to ptr2-&gt;next and try to get the data in it. But what if ptr2-&gt;next == NULL?
Much though I love Posix/Linux... I hate this misfeature. When malloc returns null... it's way way too late to do anything. The emphasis should be on providing a facility that allows you to say to the system.... I need at least minBytes, but could productively use up to maxBytes of ram. On the basis of the current load, the available RAM on _this_ machine, and some warm fuzzy heuristics, reserve and guarantee to this process an appropriate amount between minBytes and maxBytes, else return error. Allow me to query my current ram allocation, and just segfault hard if I ever malloc beyond my reserved amount.
You need to print before moving ptr2 = ptr2 -&gt; next; or you skip the head. You may not have noticed this since your head is duped as I wrote in the other comment.
I was really hoping this would be an insightful article that challenged the way I thought about error handling. I was disappointed.
Here is a copy of your linked list program with most of my above suggestions implemented. There is a small matter than I think the order that your sorted insert is using is opposite of what you expect but I left that part alone in case it was intentional. It will probably not get you credit if this is an assignment. Don't look at it if you hate spoilers and want to get it right yourself. http://pastebin.com/sYmHvt5t
Well, if we're out of memory we won't be able to do much (i. e. a proper cleanup) anyway, so...
Not entirely true. It won't fail because of running out of memory, but it can fail because of running out of address space. This won't happen very easily on a 64-bit platform, but it can easily happen on a 32-bit platform.
Does `perror` generally allocate heap memory?
All you need to do is put the other code after the null check on head in an else block. You could also just add return; after the first block that allocates the head, but some people frown on that. You'll probably find many here that think it's perfectly fine and in fact helps make code more readable.
The glibc developers are pretty sharp, so my guess is perror probably won't (for glibc). But this is why the whole notion is stupid. No, the man page doesn't say one way or the other. The only way to be sure is to inspect the implementation and everything the implementation calls. You have no way of knowing whether this is true for all posix compliant implementations.
Want insightful? Google ["crash only software"](https://www.google.co.nz/search?q=crash+only+software)
&gt; No, the man page doesn't say one way or the other. &gt; &gt; The only way to be sure is to inspect the implementation and everything the implementation calls. Yeah, when I didn't see it in there, I was afraid that might be the case. Thanks for the info.
"successor C++"? What the hell? It evolved out of C, but it is not - and is not meant to be - Cs successor. Apart from that, it's a decent article for those who didn't know yet.
I never knew you could do that! Thanks!
Originally written for Unix. I too found it hard to use, but when our company started using Xenix (SCO Unix) suddenly all worked! And made sense for what we were doing then
Seems to work fine to me. #include &lt;stdio.h&gt; #include &lt;stdint.h&gt; #include &lt;inttypes.h&gt; int main ( int argc, char *argv[] ) { uint64_t t = 200; uint64_t z = 100; uint64_t y = t/z; printf( "%" PRIu64 "\n", y ); return 0; } 
What if they produce a floating point value? Is there an easy way to handle that?
That would just be 0 because of integer division. The division of two integers never "produces a floating point value".
say for example I have 2 uint64_t values: 4164 and 2171905, and I wanted to divide those, and print the answer... what is the best way to go about that then?
exactly... so how can I use those numbers, and divide them to either get the closest whole number, or a decimal value?
I tried that, and got values that dont make sense (to me haha). eg. 1935485 / 2171905 = 1263.371410
Found a bug else where in the code. Thanks a lot for your help. I really appreciate it :)
 uint64_t i = 4164; uint64_t j = 2171905; printf("%lf\n", (double)i/(double)j);
Some of these are conceivably useful, and some of these are downright silly. MIN and MAX are always nice to have, though I usually write them as inline functions to avoid any weird side-effects as a result of incrementing (e.g. MAX(i++,j++)). The purported benefit of the ALLOC_STRUCT macro is that "there is no need to cast the returned pointer to the structure type", but of course there was never any need to do that since void* is silently converted to other pointer types in variable assignment anyway. Meanwhile, I see no conceivable excuse to use BEGIN_BLOCK and END_BLOCK over curly braces. That's downright ridiculous. I can't say that I would take any C programmer's code seriously if they actually defined and used those macros.
The `GET_FIELD_OFFSET` macro is entirely unnecessary, because C already has the `offsetof` macro defined in `stddef.h`. In fact, the macro `GET_FIELD_OFFSET` defined on that page has undefined behavior according to the C&amp;nbsp;standard, because it dereferences a null pointer. See the Wikipedia article on [`offsetof`](https://en.wikipedia.org/wiki/Offsetof) for more information.
If you're willing to go with GNU extensions then there's a nice solution to the MAX/MIN side effects issue: #define MAX(a, b) \ ( { \ __typeof__(a) __a = (a); \ __typeof__(b) __b = (b); \ __a &gt; __b ? __a : __b; \ } ) This does have the annoying side effect of triggering "shadow" warnings when nested, I can't think of any way around this. However it does squash the side effects issue nicely.
I'm a bit confused by the seeming overcomplexiy of the unused param macro -- I've always used (void) param; to great success. Is this dependent on the version of C?
GET_ARRAY_LEN() always makes me nervous because if you feed it a pointer, *or an array parameter that appears to be sized in its formal parameter declaration*, it will give you a very wrong number. void function(int array_arg[100]) { int *parray = malloc(100*sizeof(int)); int size_a = GET_ARRAY_LEN(array_arg); // WRONG int size_p = GET_ARRAY_LEN(parray); // WRONG }
Man, that's a lot of work to write a bunch of ugly code. Why not just write a one-line function? int max(int a, int b) { return (a &gt; b ? a : b); } You can declare it `inline` if you're really super worried about the small overhead of the function call. I understand that you'd have to write a different function for each data typeâ€”you'd need to write additional `lmin`&amp;nbsp;and&amp;nbsp;`lmax` functions to compare `long`s, for example. But `fmin`&amp;nbsp;and&amp;nbsp;`fmax` (to compare `double`s), `fminf`&amp;nbsp;and&amp;nbsp;`fmaxf` (to compare `float`s), and `fminl`&amp;nbsp;and&amp;nbsp;`fmaxl` (to compare `long double`s) are already declared in `&lt;math.h&gt;`, so you don't need to write those.
Oh my God! Why have I never heard of `__COUNTER__`? That is a life saver (if extremely horrible to write). I'll have to experiment to discover why on earth `MAKE_UNIQUE` is so complicated! Thank you. I have a large suite of macros for error handling which need this technique.
Not sure about the endianness macros. Dereferencing a short pointer on a potentially unaligned string address can cause bus error on some architectures, or at least invoke a very expensive unaligned access handler in the OS kernel. 
A function is the best way to do it. Any decent compiler will be able to recognise what you're trying to do and probably produce the same output as the macro anyway, but you get the benefit of well-defined behaviour.
Also, if you should never have to worry about endianness in your code...
Pretty much every macro has something really wrong with it. Any code which needs GET_ARRAY_LEN is broken -- either it already knows the length of the array, or the macro won't work. MIN/MAX have side effects. "The macro is avoiding the expensive modulo operator and accomplishing the same using bitwise operator." lol. Ridiculous and unnecessary casting. malloc in a macro, are you insane? Creating macros for { and } is just trolling. Programs which depend on the endianness of the hardware are pretty much always Doing It Wrong. "CONVERT_NUM"? WTF? Seriously. Horrible article written by a complete amateur lying about his experience.
An excellent reference book for&amp;nbsp;C is [*C in a Nutshell*](http://shop.oreilly.com/product/9780596006976.do). This probably isn't a good book to use to *learn* C in the first place, but once you've learned the language this is a great book to use as a reference. The classic book to learn C is [*The C Programming Language*](http://www.amazon.com/Programming-Language-2nd-Brian-Kernighan/dp/0131103628) by Kernighan and Ritchie.
Yeah, this is amazingly bad. GET\_ARRAY\_LEN is normally called "countof", which is named as/functions as a variant of sizeof (it is actually useful, I promise, and very common). GET\_FIELD\_OFFSET is already in freaking ANSI C as offsetof. ALIGN\_SIZE has overflow issues for large sizes. ALLOC\_ and INIT\_STRUCT gain one almost nothing, and "INIT" is pushing it a tad name-wise---zeroing is not necessarily initializing. UNUSED will get you warnings in many compilers. It's also a bad idea most of the time, since programmers expect code they enter to run unless warned otherwise. IS\_LITTLE\_ENDIAN may exhibit undefined behavior per the C standard. CONVERT\_FLOAT *definitely* exhibits undefined behavior, and could throw a fault on some (e.g., 64-bit) systems by accessing a float as a long. Just ... wow. If I saw this person's code in/before a job interview, I would make damn well sure they didn't get hired.
Whatever happened to [termcap](http://en.wikipedia.org/wiki/Termcap) and its [ilk](http://en.wikipedia.org/wiki/Terminfo)?
Particularly, The Catch section of [this page](http://www.linuxgazette.net/issue65/padala.html): &gt; Then what's the catch? If producing color is so easy, why do people waste their time writing huge programs in curses, which in turn query terminfo in a complex way? As we know, there are many terminals with very few capabilities and terminals which don't recognize these escape codes or need different codes to achieve the same effect. So if you want a portable program which would run on any terminal with the same (or reduced) functionality, you should use curses. Curses uses terminfo to find the correct codes to accomplish the task in style. Terminfo is a big database which contains information about the various functionalities of different terminals. &gt; But if you just want to write a simple program which produces color on a Linux console or xterm window, you can just use the escape sequences above to do it easily. The Linux console mostly emulates vt100, so it recognizes these escape sequences. Implementing color support with the preprocessor allows you to redefine it to just va_args if color isn't supported, but you'd still probably need terminfo to reliably figure out whether color is supported. 
Of course, it's true that these days it's hard to find a console or terminal emulator which doesn't implement vt100 codes.
Whenever I see code like this, doing backflips and twisting itself into knots just to make the compiler do something simple... it makes me think of the old saying: "when all you have is a hammer, everything looks like a nail." How about, you code your ISR in assembly? That way you know *exactly* what it's going to do. None of this "invoke obscure compiler magic" stuff. Which, frankly, is actually worse than assembly - at least IMO. I have never heard of uint_fast8_t and with some luck I will never hear of it again. (Those GCC guys sure love to make their compiler overly complicated...) Okay, yes, if you code the ISR in assembly, then the code won't be portable. But since the author seems to be saying that *every single cycle* matters, isn't that exactly the kind of code that makes assembly worth the hassle? Also, where exactly are you going to port this? To another AVR? Where the same assembly code will work? Or are you going to respin your board and use a PIC, in which case big chunks of your C code is going to have to change anyway, since (for example) the timers on the PIC don't work anywhere near the same way as the timers on an AVR? Some of these compiler tricks just strike me as "too clever by half". They don't actually enhance the maintainability of the code. They actually make it harder to understand than straight up assembly code would be.
uint_fast8_t is C99, not GCC. And choosing an processor-appropriate variable width is hardly an obscure optimization.
A processor-appropriate variable width would have been uint8_t. When you're getting to the point of *different types of* unsigned 8 bit storage, the people who write your language spec and/or compiler have clearly got too much free time on their hands. Also, this is like the "register" keyword all over again. Human beings really bad about predicting which variables really NEED to be access optimized. Let the compiler do it, or else write in assembly. Also, *every* variable should be a "fast" variable. We shouldn't have to tell the compiler to make our code as fast as possible. Finally, guess what that supposedly superior "fast" type [might be implemented as under the covers...](http://stackoverflow.com/questions/1953796/performance-benefit-when-using-uint-fast8-t) 
The "register" keyword is still useful. It means "this variable doesn't have any pointers to it", which allows for some optimizations. Also, trying to get the pointer of such a variable results in an error at compile-time. Basically, it's a way to be stricter, which can be a very good thing.
I think you're confusing `register` and `restrict`.
Nope, "restrict" means "this is the only pointer to this object". However, the compiler doesn't enforce this like it does with "register". It does allow for more optimizations though. 
&gt; &gt; The Linux console mostly emulates vt100, so it recognizes these escape sequences. Real vt100's (not even vt220's) do not recognize these colors, they were strictly black-and-white machines circa 1978. These colors are actually ISO-6429 / ECMA-48 / ANSI X3.64 extensions circa 1984ish - 1991ish. 
Surely the compile time error is its only possible utility now.
Your point is valid, but it doesn't counter my argument. I said humans are bad at guessing which variables need to be optimized. You said the register keyword is useful to prevent us from creating pointers to things that shouldn't have pointers pointed at them. Both of the above are true. But one of them addresses the wisdom of using uint_fast8_t, and the other doesn't.
I hope not. C++ has the feature as some types have really long or annoying names. Because of nested namespaces and templtes. I don't know why but some programmers just don't want to type^1: std::chrono::time_point&lt;std::chrono::system_clock&gt; *Added*: That may sound lazy but it looks horrible sometimes: std::vector&lt;std::chrono::time_point:&lt;std::chrono::system_clock&gt;&gt;&gt; timePoints; // Do something for(std::chrono::time_point&lt;std::chrono::system_clock&gt;&gt; &amp;t: timePoints) { // Loop stuff. } This doesn't really happen in C as most functions return very simple types. Even if they return structs it's only something like `struct sockaddr_in*`. `auto` is also used in conjunction with `decltype` due to scope issues like this: template &lt;class A, class B&gt; auto add(A a, B b) -&gt; decltype(a + b) { return a + b; } This just doesn't happen in C. 
[The article](http://www.hudku.com/blog/essential-macros-for-c-programming/#my-reply) has been updated to address most of the points made here. Do you agree with it?
correct. there is no advantage at all. its just makes debugging a lot harder
A lot of IDEs (embedded IDEs at least, which is what I typically use) have many of these features already. That said, it looks like a fantastic teaching tool and would help a lot of people wrap their head around pointers.
That guy is a wizard, he can write C code backwards on glass!
free version: only allows 1 header, 1 source, 50 lines of code. lite version ($20): only allows 5000 lines of code And of course it's windows only. So essentially, they want people to pay $80 to program as many lines of code as they want. Forget that fact that there other C IDE's that are free, open source, and have no limitations. Yeah, the GUI isn't worth that much. You're better off using GCC and GDB.
Yikes. That's too rich for my blood. In respect to GNU extensions are you referring to inline asm?
It's the same \_\_asm\_\_ keyword, but a different use. You can declare variables that sit in particular registers, sort of. It's the only way I know of to get around GCC's outright prohibition of EBX in an asm statement with -fPIC enabled; e.g.: unsigned a, c, d; register unsigned b __asm__("ebx"); __asm__("cpuid" : "=a"(a), "=c"(c), "=d"(d), "=r"(b) : "0"(leafNr)); Theoretically you can also affect the ABI by doing this statically and marking off registers that are always mapped to a particular variable, although I can't imagine that being a good idea except in very small, specific programs.
Its not a book that is particularly good at teaching C to someone new to the language, but it is pretty good at providing information on various tools for more modern C development. I haven't run across anything similar, so if learning more about a modern C development environment sounds interesting then I'd say 21st Century C is a good choice.
I thought the book was atrocious. Your odds of picking up something useful are roughly the same as the odds of you accidentally taking to heart some awful advice offered in the book: * The "switch" statement is hard, so don't use it. Use if-else blocks. * If your program leaks less than 100kB of memory, who cares? Computers have lots of memory. * Just #include everything; compilers are fast. * Checking return codes is like, so mid-90's. There are other gems, but those are just a few that I recall. This is one of the very few C books that I consider rubbish. If you want an excellent C reference, might I recommend the "GNU C Library Reference Manual"? www.gnu.org/s/libc/manual/pdf/libc.pdfâ€Ž, also available in your terminal as "info libc". Or "The Linux Programming Interface" by Michael Kerrisk. Those aren't necessarily about C the language, but more about how to correctly use it, and I highly recommned them both. 
Not really looking for a reference. More like for some resource that would teach me to write code that sucks less:)
Seriously, give those a try. They'll help. :)
A balanced search tree of some variety is a pretty weird omission.
That'll be the next addition, I've added it to a todo section of the page. I reached a point where the code I have is working well and wasn't sure what to pursue next, so I decided to share the library and see if I could get some input. Thanks!
Nice!
The null *character*, written&amp;nbsp;`'\0'`, is the `char` value that has numerical value&amp;nbsp;0. When you use a `char` in a numerical expression, the `char` becomes an `int`, so `'\0'` becomes the `int` value&amp;nbsp;`0`. Therefore, when you compare `'\0'`&amp;nbsp;to&amp;nbsp;`0`, the result is true. The technique you are trying to use here won't work. You are taking a fact you know about strings and trying to apply it to all arrays. In&amp;nbsp;C, a string is an array of `char` values that ends with the null character&amp;nbsp;`'\0'`. But other arrays (including `char` arrays that are not intended to be used as strings) are not terminated this way. In particular, if you read past the value&amp;nbsp;`9` in your `int` array, you will start reading uninitialized garbage values from memory; there is no `'\0'`&amp;nbsp;character at the end of that array. If you want to iterate through an array, you need to know the length of the array so you know when to stop, or you need to explicitly place a sentinel value at the end of the array that is different from all "real" values in the array, so that when you reach the sentinel value you know to stop. In your case, if the `int` array will never contain negative values, then perhaps you want to use&amp;nbsp;`-1` as your sentinel value at the end of the array; but you'll have to put it there yourself.
Well, actually, according to the C standards, the null pointer is nowhere guaranteed to be represented as a 0. The actual representation is implementation and/or architecture dependent; I believe it has to be constant, however. (For most practical purposes, it is 0, though). In your example, you're not checking any pointers anywhere. The only test that is happening is that you're testing that values[0] (== *valuesPtr) type int, does not equal '\0' type char. I forget which way this is promoted, but it is always going to fail. Remember the symbol, values, acts like &amp;values[0]. 
&gt; Well, actually, according to the C standards, the null pointer is nowhere guaranteed to be represented as a 0. The actual representation is implementation and/or architecture dependent; I believe it has to be constant, however. (For most practical purposes, it is 0, though). This is sort of true; the C&amp;nbsp;standard does not guarantee that the null pointer is all-bits-0. But the C&amp;nbsp;standard guarantees that the `int` literal&amp;nbsp;`0` in code, when it's in a pointer context, means the null pointer, even if the null pointer is not represented by all-bits-0. Also, the null pointer is entirely different from the null *character*, written&amp;nbsp;`'\0'`, which is guaranteed to be all-bits-0.
Duh, I was so concentrated on the pointer part of it, I wasn't really thinking. Thanks! But okay, so how come changing just the lead 0 in the int array to a 1 causes it to print and run just fine? Also, when I try to add something else to the array, it says there is no room, like there is a null terminator anyway (since as it is only positions [0] through [9] are filled)?
&gt; (since as it is only positions [0] through [9] are filled) Ah, perhaps I understand what you are asking. The declaration `int values[10]` declares an `int` array of *length*&amp;nbsp;10, so the valid offsets are `values[0]` through `values[9]` (that's 10&amp;nbsp;entries). There is no entry in the array called `values[10]`. An array declaration specifies the *length* of the array, not the last index.
That is true. He also doesn't ask about the null pointer.
I changed it by adding 10 to the end of the int values[10] initialization and the error is: ex11.c:5:54: error: excess elements in array initializer [-Werror]. So not exactly "there is no room", but the same meaning, no?
Right. See [my next comment](http://www.reddit.com/r/C_Programming/comments/1g4xiz/quick_newbie_question_on_null_zero/cagscty).
Right, exactly. So, if there isn't automatically a null character added as in a char array, why does it terminate properly? It can't just be that the memory has a 0 there, what are the chances of that?
By checking pointers I meant that I was playing around with how they work to understand them better. So in this small program, I was just working with the fact that int *valuesPtr = values; is the same as int *valuesPtr = &amp;values[0]; and also that ++valuesPtr would make the pointer point to the next element in the array. Simple stuff, but just playing around with it. 
Uninitialized memory is not *random*; it's just that you can't depend on it being any particular value. And depending on how things are implemented, the chances are pretty good that the memory there will be all-bits-0, actually. A `char` array of length&amp;nbsp;10 consists of 10&amp;nbsp;sequential bytes in memory. The most efficient way for a compiler to initialize those 10&amp;nbsp;bytes to the values you specify is likely to initialize them 4&amp;nbsp;bytes at a time (because computers often read and write memory in 32-bit chunks). So the compiler produces code that initializes the first 4&amp;nbsp;`char`s in the array with one instruction, and then initializes the next 4&amp;nbsp;`char`s in the array with the next instruction, and then initializes the remainder of the array with the next instruction. That last instruction just needs to initialize 2&amp;nbsp;`char`s in the array, but it's most efficient to write 4&amp;nbsp;bytes at a time, so the compiler probably just goes ahead and initializes the next two bytes to all-bits-0, because why not? But the point is that you should not *count* on this behavior. The C&amp;nbsp;standard does not guarantee that this will happen. *Your* compiler might happen to do initialization this way under these particular circumstances, but other compilers may do something different, and your compiler might do something different under other circumstances. Arrays in general are not terminated with&amp;nbsp;`'\0'`. That applies only to *strings*. That's what you need to take away from this.
Okay, I'm definitely with you now. That makes a WHOLE lot of sense, thank you so much. I'm still extremely new to this - really only been learning C and programming in general by myself for about 2-3 weeks now, and even that is a stretch since I don't have an abundance of free time right now. So, I appreciate the help!
I've checked through the comments, but is there something relevant for C++? I'd like to see how many of these quirks are retained.
Also related, a really good but non-free book: http://www.amazon.co.uk/Hackers-Delight-Henry-S-Warren/dp/0321842685 
It's really a bad idea to use "const" like he's using it. "const" represents a loose, easily sidestepped prohibition on the programmer writing a particular value\*, but it also represents a promise to the compiler and users of the API that the value won't be overwritten by the function taking the const pointer. If link-time optimization is supported and used, strange behavior could be elicited from this; but at the very least it's a lie to anybody reading the code. \*(For the programmer to write to the pointer, they have to cast it anyway, so the const-ness really doesn't represent an imposition anyway. There is seriously no good reason to do what he recommends.) Really, these are the only techniques for data hiding that I've found to be portable and a good idea at all: 1. Struct body omission. Covered in the article. 2. Presenting the head or some sub-structure of the overall struct body in the public API; behind the scenes, you can tack data onto the end of the struct. If you want the user to have quick access to public or semi-public struct members, make inline functions or macros for accessors. Note that this technique doesn't violate aliasing rules if you have to cast between pointers to the external head struct and the internal complete struct. This technique also allows much faster access to simple data members (e.g., asking the size of a hash table) than would be attained with function calls. 3. Use the static modifier for non-public globals, and whenever possible define things privately within the compilation unit that uses them. This has the added benefit of improving the compiler's ability to optimize the static-declared things and anything that uses them. 4. Prefix names of private things that have to be shared between compilation units. E.g., libname\_PRIVATE\_identifier; #defines can be used to rename these in the C code. Such identifiers can be linked against after the library has been compiled unless you explicitly strip them. 5. When possible, place tables, enumerations, type definitions, etc. inside the innermost possible function/statement scope. This will hide them even from other things in the same compilation unit. Extern definitions for private things can also go inside inline functions or GNU ({statement expressions;}), which is a good way to prevent things from smearing into the formal API. 6. If you have to make your data visible but want it to be somewhat private, put it in a struct {} libname_PRIVATE substructure field; if the user fucks with that field, any problems that result are their own fault. Inside code that uses the private members, you can use #defines to shorten field names. 
It's considered good practice to expose the minimum possible surface of any component implementation. The headers for the component should represent the API somewhat directly, and should only describe things that need to be described. There are a few reasons for this. First of all, omitting extra information and minimizing variables' visibility reduces the extra work the compiler has to do and lets the compiler optimize better. Second, it makes it harder for the user to fuck with internal data structures and then complain when things break. Third, it makes it easier for the user to replace one implementation of a library with another, without having to rewrite or recompile a bunch of stuff. It really doesn't affect debugging at all, because all symbols (private-ish ones included) are exported in the final binary, as long as all the components have debugging symbols included. Any hidden structures will be fully exposed in the debugger, as will functions and variables declared static. (Edit: This isn't something that one generally thinks about or needs to think about as a newbie; this is something that you have to consider once you're a halfway seasoned developer writing actual real-world applications.)
Some of these answers are architecture- and compiler-specific. He's generally correct in that those are the ways those oddities are handled in practice, but he makes some statements that are technically a little wonky per the standards.
Ah, thanks for clearing that up!
Mike's a well known Mac developer, so quirks are presented with that in mind.
I don't really see the problem. Most libraries available on linux for C will be also available on windows. And there is also quite the selection of compilers. Amongst them gcc. The real advantage of linux is that you can very easily install new libraries. Example: $ sudo emerge dev-libs/boost to install the boost libraries for C++ on gentoo. On most systems one command will be enough. Maybe the issue is with using IDEs like Eclipse that hide most of the process of building C/C++ projects behind a set of simple wizards and config windows rather then teach you how binaries are actually assembled. Which you would learn if you used tools like make, automake and so on. I don't really see a problem with learning C on Windows. I would prefer linux for convenience but that's just my preference.
i've never actually touched linux, maybe it's time to give that a shot ! thanks for the help
I suspect the issue is not so much Windows, though that will be holding you back (brain damaged shell, command line unfriendly build system, etc) as the Microsoft C compiler ... which is stuck in the early 90s! Microsoft appear to have no plans for C99 support. This is *very* restrictive when writing good C (named initialisers, for a start). Here's a simple litmus test. Download a simple C application, eg [nano](http://www.nano-editor.org/download.php), and see if `./configure &amp;&amp; make` works. If not, you're in the wrong environment to develop that kind of software.
so should i be using linux or ubuntu for this kind of stuff ? 
&gt;linux or ubuntu What? Should you be using footware or shoes when you walk outside?
lol they are exactly the same thing ? just from looking around i was under the impression that linux was the base and ubuntu was a more user-friendly version
Might be an overkill to try out a completely new environment with completely new tools and philosophy of creating and using these tools just to learn a programming language. But then again what do I know. Maybe you thrive in situations when you need to absorb huge amounts of information to get things done. And Unix philosophy is THE philosophy when you are talking about writing pure C. Really, the history of C is the history of Unix. Oh, and I definitely don't think you should start with gentoo. It was just an example.
I have a feeling you could probably use the short version. Linux can be thought of as the base. When people talk about Linux they mostly mean OS's that share the same kernel (core part of an OS) and the design ideas that go along with it. Ubuntu is usually called a distribution of Linux, basically it's the core Linux kernel packaged up with a ton of programs, user interface stuff and everything else you'd need to run a PC. There's other distributions out there with different sets that function and look very different.
If you really are just learning how to code, I don't think linux is required. If you *really* want to learn C the hard way, then I do suggest installing Ubuntu and playing around there. Just to warn you if you've never touched any kind of Linux, it will be a while before you start learning C due to the fact that you need to learn some linux basics(unless you use an IDE under linux which kills the whole point). I'd say you're perfectly fine learning without Linux. If you plan to make a career out of it, you'd be pretty much required to play with Linux. Have fun! PS: I just myself started the LCTHW as a refresher, and you really do need to spend a good amount of time on each chapter if you want to learn it. Don't skip over anything! 
Depends on what kind of stuff you want to write, but personally I'd recommend it ... but then I'm biased.
C is a very portable language, which probably partially explains Microsoft's hostility toward it. I would suggest either a different O/S or installing a full cygwin suite or even a Linux VM. 
the end goal is to make games, which i'm assuming would be done with C++. but if taking the time to learn C would allow me to explore Linux for the first time and do things like making a custom desktop GUI, that would be quite interesting. does knowing C at all help with learning/using c++ ?
Why not mingw? If you can fly with that, you can fly everywhere. Couple of months ago I went through LCTHW to freshen up my rusty C (it's still rusty and much unused). I coded in Sublime on OSX laptop, and compiled on OSX, Win7 (mingw) PC, Ubuntu in VM... it was a nice exercise. Now if I could find a good UTF lightweight(!) library to go with C89/90 or C99 I'd be happy!
Both, using Linux and learning C teach you tons of things about how computers work, so that's definitely not the worst of ideas. It will be a huge amount of learning for OP though.
Yeah, that's my only objections. Information overload. But different people learn in different ways. Might be just what OP needs.
I would say that learning C is a good foundation for learning C++. They are now very different languages, but a good understanding of C will help you understand what is going on in the machine. Personally I don't have a high opinion of C++ as a language, whereas C is pretty clean and self contained. Learning your way around Linux certainly won't hurt!
Ah sorry, I understood you already knew some languages and were looking for more. Learning C usually teaches you a little bit more than just a new language. To fully understand C, you will be exposed to memory allocation and how the computer functions in general(but mostly just memory). Learning C under Linux will help you understand that better because not as many things are sugar coated for you like if you were to use Visual Studio under Windows(or any other IDE(ex. Eclipse) for that matter, even under Linux). I have no experience developing with C under Windows without an IDE, but I imagine similar tools are available for the command line there. But, as everyone here pointed out, those tools suck quite hard apparently compared to the linux command line(terminal). PS: You're never too old to make a career out of programming!! I'd say its one of the easier ones judging by how much resources are available online(www.coursera.com, www.udacity.com, and many many more). At 23, I'd say you're still in a perfect position to find a job at it. 
Don't make it a career decision before you know what you're getting yourself into, start out doing it as a hobby instead and see how you like it. If you find out you do like it, then go right ahead and get a degree. The consensus, at least around /r/programming, seems to be that coding will not get you a job as a programmer. A degree and inter-personal relationship skills is what does land you one. The good news is that both of those can be picked up at any age, so don't worry about being late to the party! :)
I agree, I used clang on osx and mingw on windows and gcc on linux. All in parallel, that's why it was a nice exercise.
`'\0'` has type `int`.
`42 || 0` yields `true` in C++.
Pelles C is a good choice for Windows. It has full C11 support.
Rather than installing a full-blow Linux, cygwin is a good compromise if you want to learn C in a *nix-like environment with gcc et al on Windows. http://cygwin.com
Cygwin isn't a bad idea here.
http://mingw.org/ is nice as well, less of a full-blown system, but you can produce native Windows binaries free of strange dependencies which is useful. My usual porting approach is to get things building/working on GCC in MinGW, then (if I'm unfortunate enough to have to care about MS compilers) port it to VC++ also in MinGW.
Learning the basics of C is not a problem on Windows. Microsoft's C/C++ compiler does have poor support for ISO C99 (and that's a shame), but support for ANSI C89 is complete. If you really need C99, you can use alternate compilers and environments like clang or gcc. I personally develop all my stuff on Visual Studio 2010 and I'm happy with it. TL;DR: Windows is fine, Microsoft's C/C++ toolchain is a little behind.
You're right. Had a mistake in my test code. (stream &lt;&lt; 42 || 0 // The operator &lt;&lt; has precedence over ||, thus why it yielded 42.). I'll fix my comment to reflect that.
it doesn't really matter what OS you learn your code in. It does matter if you are able to write common codes such that it works in BOTH unix and windows.In linux there are software such as vilgrind that helps you find memory leaks where in windows there probably are the same thing somewhere but takes a while to use it and is not always accurate. You will love vilgrind once you get to melloc and free for c , especially in group projects.
&gt; Most libraries available on linux for C will be also available on windows. #include "unistd.h" /* what happens here? */ 
Lack of a good compiler is the #1 issue. I would suggest using MSVC and writing "C code in C++" (basically, limit yourself to the features C++ inherited from C). This way you can write 99% C-compatible code that's easy to port to real C in case you need to... but why would you? (unless you're building for an embedded platform or some other obscure target that does not have a C++ compiler). ...hell, even the mighty [GCC compiler targets C++ now](http://www.h-online.com/open/news/item/GCC-shifts-internal-focus-to-C-1668552.html) Edit: just realized I'm in /r/C_Programming and telling people to write c++ probably gets me downvoted to hell :)
Remember that first time you opened up linux or any terminal and someone told you compile that program? Regardless of distribution, its daunting for people that have never used any type of linux to do stuff without help.
Implementation details, you might say...
Nice try, RMS.
Like he'd use a site that requires javascript to run.
Dude, just use the Windows Disk Management to reduce the windows partition and create a new partition for installing linux. Or, even easier, just use VirtualBox or other VirtualMachine software and run a linux os on Windows; this works quite well. 
No it isn't. If I install the GNU userland on Solaris, it doesn't become GNU/Solaris... That would be retarded. When GNU has a complete distribution, including a decent kernel, let me know. Until then, GNU doesn't deserve a place in the OS namespace...
&gt;C is a very portable language, which probably partially explains Microsoft's hostility toward it. It couldn't have anything to do with the fact that they've thrown their weight behind C++...
http://www.debian.org/ports/kfreebsd-gnu/ QED.
&gt;I would say that learning C is a good foundation for learning C++. I would say this is absolutely wrong. I'm not saying that you can't learn C and then learn C++, but learning C **specifically** as a foundation for C++ is the wrong approach. C++ is not "*C with classes*". It's a **very** different way of solving and thinking about problems than C. Classes, inheritance, generics, lambdas, RAII are all language features or idioms that give you ways of solving problems that are vastly different than the way you solve those same problems in C. Sure, C++ is mostly, kind of, effectively, for most reasons and purposes a superset of C. And sure a **lot** of C code can be run through a 100% C++-compliant compiler and come out the other end with no errors and do exactly what it did when you shot it through a C compiler. But that doesn't mean that C's techniques, practices, et cetera are **at all** applicable to C++. That just means that interop with C APIs, libraries, et cetera in C++ is **really easy and convenient**, and that it's **really easy and convenient** to wrap those APIs, libraries, et cetera up in a **really thin and efficient** abstraction that, depending on how you compile it, may not even exist at all once it goes through a good compiler with optimizations turned on. TL;DR: Learn C, and then learn C++. Or just learn C. Or just learn C++. But **do not learn C to learn C++** that will make you write **bad** C++ code.
Writing code in a portable language doesn't necessarily make the *program* portable (you also need to not use platform-specific APIs or make any platform assumptions in your code). By that logic, MS should also hate C++ because it's just as portable a language as C is. You could also argue that C# is even more portable than both, because of Mono, so that reasoning is silly. The reason why MSVC's C99 support isn't great is because they focus their efforts on C++ and C# instead. That said, it supports C89 just fine, and that should really be good enough for almost anyone. If you *really* need 100% of C99, just use gcc or Clang instead (Cygwin/Mingw). It's not the OS, it's the toolchain.
&gt; see if ./configure &amp;&amp; make works. If not, you're in the wrong environment to develop that kind of software. Implying automake is *the one true build system*? Is that a joke?
No. Just that if it doesn't work you've got a problem.
Hand me the 'bold' button son.
that's awesome, thanks for the explanation what do you prefer using for c++ development ?
And debian is a bunch of hippies anyway, who side with Stallman. That doesn't make them correct. However, given that it is "their" operating system, they can call it whatever they want. I'm still going to scoff at them for their nitpicking over the semantics of the name.
&gt;I'm wrong, but I'll make fun of everyone pointing it out.
Your opinion that I am wrong is no more valid than my opinion that you are wrong. Let me know when the Hurd is viable, then we can revisit this discussion. 
&gt;Let me know when the Hurd is viable, then we can revisit this discussion. About three years ago. 
Yeah, it does work (in Cygwin, anyways), but I believe the implication was using Microsoft's compliers. In the case of Cygwin or MinGW you are bringing the entire posix environment with you. At that point you are no longer targeting Windows and you might as well just install a real OS and be done with it. 
./configure doesn't imply automake.
I know almost ten C compilers for Windows 7. Of those, two come with an IDE; one is the MSVC++, which supports C90 + everything that C99 and C++ share, and another one is Pelles C, whose authors claim full C11 support, which is a bald claim but may be true. The rest of the compilers can be used with MSVC++.
Seconding VirtualBox. Install Ubuntu on a virtual machine and call it a day. Doing anything else would be overkill at this point.
I've written a bit of C on Windows, and I really recommend looking into mingw. Go ahead, learn C on Windows, there's no reason not to. Linux offers a better development environment, but it would take you a long time to learn, and it sounds like two separate projects.
Ubuntu is a GNU/Linux distribution. Perhaps you meant GNU/Linux or Windows?
I'm not sure if this will help, but alot of library have 2 allocs/free. One that is global, only called once in the program and one that is for every instance of the "program/thread". To free everything, you have to call both free functions.
&gt;So can somebody PLEASE explain to me the logic behind this strange design practice. It's quite easy, pick any or all: * Programmers are lazy * Most libraries were designed to last for the duration of the program, and don't waste time cleaning up might be just as valid a choice as anything else. * Valgrind weren't around when the library was born. As for readline, you are hopefully at least free()'ing the returned value from readline() 
Sounds like you're doing something wrong (not freeing the memory yourself even though you are expected to) or using valgrind/reading the output wrong. I have an application that uses readline, and it has been running for 28 weeks 7 days 16 hours 44 minutes and 52 seconds straight (and probably longer before the last restart; I've had it in production since 2009, and restart it about once a year due to code/kernel upgrades), and it hasn't leaked any memory so far.
Almost everything that uses custom memory allocators (e.g. GLib) shows as leaking in valgrind, while it actually isn't. For GLib it helps to set GSLICE=always_malloc when running under valgrind.
I've noticed this before (forget the specific library), but a certain function was leaking (according to valgrind) a few bytes per call. When I investigated further, it turned out I was running a somewhat old version and the leak was fixed in the current version. So the point is, it's entirely possible you can resolve your problem by updating to newer versions of the libraries in question.
Do you have a link with more info here? Could we consider this a bug or missing feature in valgrind? I guess my point is, how does the user/developer know if a library is using the custom allocator? The "try it with GSLICE set, and then again without to see if that helps" approach is not very satisfying
Raymond Chen covered something similar to this, which can explain a lot of why this happens. [Allocating and freeing memory across module boundaries](http://blogs.msdn.com/b/oldnewthing/archive/2006/09/15/755966.aspx) This is specific to libraries that are separate modules (loaded at loadtime or runtime like .dll/.so, not compiled together), but it's a very good article about something few people probably think about.
Did you run it in valgrind the entire time? Just because it hasn't crashed your system doesn't mean it doesn't leak memory.
I've run valgrind on a few projects here using readline, there are zero memory leaks. There are however memory still in use at application exit time (maybe you call those leaks as well ?) 
Doesn't matter if you use Linux, the concept is the same. An .so loaded into the process may have statically linked with another version of glibc than the main program (or other modules) or linked with another libc library entirely, in which case that module's idea of what "free()" is is different from the others'.
Well, I call them unfreed (not leaking) memory, but they still are (in my opinion) very important. Ok, maybe Readline was a bad example, but there really are several examples of libraries I have used which would LEAK memory.
No, I don't run valgrind on it, but I continuously measure its heap size.
It's easy to set up with virtualbox. I use it at home and at work, and have run vbox on low end(atom procs) without much of an issue. It's worth at least playing with and if anything, you'll get your feet wet on another OS. No real need to buy a second comp unless you have some cash burning a hole in your pocket and the extra equipment to set it up.
Nope, no link. I found this solution wandering through the internet, on stackoverflow somewhere. I don't know why valgrind behaves that way; I'd presume that's a bug; if that's a feature, it's not really useful imo. The approach isn't the best, but it's the best I've got :)
Right, yes, I misread the article.
&gt;Like he'd use a site fixed it for ya
In C++ that's a violation of the one-definition-rule and results in undefined behavior anyway. I assume C has some analogue of the ODR. IME as a practical matter, non-Windows platforms seem to be more concerned about maintaining ABI compatibility and so libraries generally ensure that it's safe to do things like allocate memory in one module and deallocate it in another. For example libc++ has this sort of compatibility with libstdc++.
&gt; It seems like it would be worth while to understand the "backbone" of C++. C and C++ are largely compatible in that you can write code that is both a legal C and a legal C++ program and they both do the same thing. However the styles native to each language differ greatly. That is, if you write the best C program for some task it will not look anything like the best C++ program for that task. The primary difference is that C++ has more tools for creating abstractions. Often the right tool for building a particular abstraction will be completely different depending on what mechanisms the language provides; to build a particular abstraction in C you might combine macros, and functions in a particular way, while in C++ you might build the same thing using a template class. Additionally, C++ has some features that, in modern C++, entirely replace usage of certain C features. If you learn the proper C way to do certain things, you'll be learning things that you should not do in C++; even if the C way works in both, the best practices are different. My advice would be; if you want to learn C, just learn C. If you want to learn C++, learn C++ as its own unique language.
meh. I get the feeling the author hasn't really looked around at alternatives. Specifically, no mention of cdash or ctest, which is now widely used in cmake based projects (http://www.cdash.org/, http://cmake.org/Wiki/CMake/Testing_With_CTest). Obviously these are only useful for people using cmake, but as you can see from this article, there are a bunch of other things around (http://stackoverflow.com/questions/242926/comparison-of-c-unit-test-frameworks). Specifically Google Test is quite popular. You could argue this is just a simple intro to unit testing with some example frameworks, but the author *also* trivially glosses over some of the *most important* features of unit testing in C, for example: - mocking via linking to libraries with known ABI - continuous test integration ('I set up my development environment to automatically make all whenever a file is saved.' &lt;--- HOW? That'd be interesting to talk about). - stack state safety; should you or should you not run a binary per test to ensure that bashing the stack in one test is not generating falsely passing tests in another? - How do you integrate automated tools like valgrind with you tests to catch memory leaks? Soooo much meh. From this article. 
Some people program in C. Not in C++.
Why would you use a C++ test framework for C code ? If you're happy with the boost unit test framework, that's fine - but most people would try test their C code with a C++ framework for just as often as they'd try to test their C++ code with a C framework. 
C and C++ are different languages. They are incompatible -- [C++ is not backwards-compatible with C](http://en.wikipedia.org/wiki/Compatibility_of_C_and_C%2B%2B), as is commonly believed. Using a C++ unit testing library requires using a C++ compiler. This, thanks to the backwards-incompatibility, implies that you aren't testing a C program, you're testing a C++ program. So if you are writing a C program, don't use a C++ unit testing framework. Why use a more complicated tool (a C++ compiler) when a simpler tool is enough?
if you vote down, please tell me why!
getopt has always worked fine for me. What benefits does your library have over that?
Indeed. getopt is mature and exists pretty much on every platform. It also is a bit of a pain to set up so if this is a better alternative, I'd be happy about that. Subby: is your code portable to all the various flavours of Unix/Windows and how do you differentiate it from getopt?
un ejemplo por favor
I believe that glibc supports option grouping.
Here are some things to consider: 1. You seem to be over-using `const`, and it's causing the need for explicit casts where they shouldn't be needed. The cases in the test file are especially bad because that's happening right at the interface to the library, where the users are going to be touching it. 2. `const char*` is an immutable string, but `const char**` isn't an immutable array of immutable strings. It's an immutable array of mutable strings. 3. You do a lot of variable declaration at the top of the scope (presumably to be compatible with C89?) but aren't strict with it. You occasionally do something before a variable declaration, which will break in C89. If you don't care about C89, then the code would be cleaner if you declared variables where you use them. (In my opinion... It's a style issue, really.) 4. While we're on style, I would like to humbly request that you don't leave large white spaces in order to align bits of the code. It makes it harder for humans to parse. 5. When dealing with array sizes or array indexes, it's preferable to use `size_t` instead of `unsigned int`. 6. You should aim to have only what the users of your library need in the main header file. Move `struct option` and the defines into the source file, since the user doesn't need them. 7. Any identifiers starting with an underscore are reserved. It's best not to use them at all, regardless of how unlikely you think a conflict might be. 8. There are a few places where you access `options` without checking if it's been allocated. Yes, the user should be calling `op_init()` first, but if they forget, they're going to be getting mysterious segfaults. (Since `options` is static it will be initialized for you, but you should initialize it to NULL at the declaration in order to be explicit about what its value is supposed to be before init.) Heck, since you're dealing with a global variable, you could just do the initialization without bothering the user to do it. 9. By the way, maybe you shouldn't use global variables. ;) Okay, it's unlikely that you'll want two different options parsers at the same time, but it's still a bad habit to get into. 10. On line 240 of op.c, should there be parentheses around `o-&gt;argument_count+1`?
1. By "grouping options", do you mean to group "-a -b -c" to "-abc"? Getopt() supports that. 2. Does your library support multiple occurrences like "-x 10 -x 20 -x 30"? I guess not? With the classical unix getopt(), you get "-x" three times in order. 3. Personally, I don't like an option taking two or more arguments like "-x 10 20". This is not the unix way. I always fear I may do something wrong when a CLI program has such an option. Maybe that is just me. 4. If you want more to use your library, don't use GPL. Few bother to include GPL'd source code for simple things like option parsing. Adopt LGPL at least, or better a permissive license like MIT , BSD and others. 5. On the bright side, I like a two-file library for option parsing.
&gt;Does not care if malloc() fails, as we do not allocate this much memory in here. If it fails, the whole stuff will crash! I think this is a poor design decision.
I personally write my unit tests for my C programs in python. I make a simple script-able interface and call it from python, because C was not designed for scripting, which is all a series of unit tests are.
Thanks for your comment! I willl add these features! Grouping and multiple options are not supported yet. The source is under lgpl... I think that's fine, isn't it?
I'm about to fix these issues, thanks a lot for your comment!
Yes, indeed. I'm not that happy with it. I think I will redesign the library, so you have to pass the memory which is required internally. So the lib will not require any malloc() calls, or even the stdlib header.
*tl;dr*: The government would like you to take advantage of [this](https://github.com/SEI-CERT/scvs) code on github to analyze your code and make you a better programmer so you can one day work for them. Edit: lol, the network graph for this one is so black box.
I'm a GNU/Linux nut but I have to agree. If your goal is just to learn C, then stick to Windows for now. There are ways to learn it even with Windows' limited environment capabilities. Once you gain more skill with C, you may be naturally drawn to GNU/Linux or BSD since their internals are built in it. :)
Linux is a kernel written in C. It provides mechanisms that enable a Unix-like operating system (like GNU -- GNU's Not Unix) to interface with the hardware. There are other *nix platforms; the most popular non-GNU/Linux system being BSD (Berkeley Software Distribution). Ubuntu is a *distribution* of GNU/Linux, which ships a curated set of software preinstalled to give users a base to build their system from, much like Windows ships a basic set of software to operate a computer. In Ubuntu's case, the software you use is free (as in freedom) and you can read the source any time you want, and even modify it to fit your needs. **tldr version: GNU/Linux is the base system, Ubuntu is a distribution, or *flavor*.**
Hah, you're right. Doesn't he wget pages and/or send them to his e-mail? Though his approach may be antiquated I'm honestly interested in seeing what he actually does with his computer. There might be a few clever things in his workflow. EDIT: I'm a workflow nerd, always looking for new, clever, or fun things to add to my workflow.
To be fair, both deserve credit. Neither would have reached a considerable audience without each other.
To be fair, we still wouldn't be running GNU if Linux hadn't come along. It would have been either Minix (which still sucks shit) or one of the BSD's for 386. Personally, I'm glad Linux came along. It's more than just an OS, it opened the door for a lot of people to have a decent OS to experiment with other than DOS/Windows. Really, Unix was a good design, and GNU wanted to replicate it. They did so poorly, until Linux appeared. Sure, the GNU tools were used on plenty of other OS's, but no one tried to confuse that with SunOS, or AIX, or anything like that. Stallman is a fairly smart guy, but he's very full of himself, and assuming credit for an OS is just rude. I guess maybe he doesn't know any better, what with his social skills or lack there of. I'm just saying, GNU by it self is not useful. Userland is userland, sure it makes a big part of the OS, but so fucking what? It's not a complete OS, and the Hurd is a joke. Linux. That is all. Now go jam those info pages in a pipe and smoke it. 
A kernel is worthless without a userland to run on top of it...
http://stallman.org/stallman-computing.html
Jiffy! 1:03-1:10 https://www.youtube.com/watch?v=xP1-oquwoL8
Here's your GIF! http://i.imgur.com/MGXBHcS.gif _____ ^(Hey I'm JiffyBot, I make GIFs out of YouTube links. Find out more) [^here.](http://www.reddit.com/r/JiffyBot/comments/1fvyi5/rjiffybot_information_directory/)
Never post screenshots of code. 
I did something along the lines of this: int wins[8][3] = {{0,1,2},{3,4,5},{6,7,8},{0,3,6},{1,4,7},{2,5,8},{0,4,8},{2,4,6}}; EDIT: Code came out horrible here is a link to my GIT https://github.com/Sav10rx/cprogramming/blob/master/tictactoe essentially i created an array to store all win instances and cycled through them comparing the characters within them to X or O
But will GCC blend?
This would be interesting if it was a more recent gcc.
Hi. I added the feature of option grouping as you described in 1.! I think I will not support multible occurrences, but you are free to write a pull request!
He compared 4.2.1 GCC to clang...
clang isn't mentioned, and the article is not a comparison. In fact, back in 2010 when the article was written Clang was at version 2.7 and had just started to be capable of self-hosing.
Hmm, I see. I had searched the page but it looks like search doesn't look at hidden text. Still, an ancient GCC is compared to an ancient clang in an off-hand comment. It more like he's making a practical remark about the tools he's got, not concluding which compiler is superior. That is after all why he used GCC 4.2.1.
Microsoft has said in the past that they only support C++ (their support for C++11 is fairly complete) and the C subset of C++. So they technically don't even have a C compiler. (To be fair, the product *is* called Visual C++)
Embedded systems, operating systems, stuff that has to do low level interactions with hardware or kernels, stuff that needs to be stupid fast etc etc etc. Knowing C will help you grasp core concepts in C++ and ObjC (pointers, memory management etc), however I know plenty of C++ programmers who couldn't write C to save themselves. Not knowing C when becoming custom to ObjC is like being able to understand English and not knowing the alphabet.
You need to learn Objective C, which, as ikilledkojack pointed out, is a superset of C. However, knowing C will only give you a small head start in understanding Objective C. ObjC is a language within a language and has its own specialized syntax and idioms. If you want to learn iOS programming, don't bother with ANY C books, just jump straight into ObjC. Here are books I used: * [Programming Objective C](http://www.amazon.com/Programming-Objective-C-Edition-Developers-Library/dp/032188728X/ref=sr_1_1?ie=UTF8&amp;qid=1371821790&amp;sr=8-1&amp;keywords=programming+objective+c) - start here. It gives a good overview of the language, and makes everything that follows much easier. * [Beginning iPhone Development](http://www.amazon.com/Beginning-iOS-Development-Exploring-SDK/dp/1430245123/ref=sr_1_1?ie=UTF8&amp;qid=1371821752&amp;sr=8-1&amp;keywords=beginning+iphone+development) - understand XCode &amp; frameworks 
What? No, that's not what I'm trying to say. Learn C. It's a small and very simple language - I'd recommend [this](http://c.learncodethehardway.org/book/) over K&amp;R. You'll inevitably have to learn things like pointer maths and memory management that the other two use. Even if you only learn a bit it will you give you greater understanding of how ObjC works lower level and dealing the pure C based APIs will be a lot less painful.
C is used in anything that C *can* be used in.
You could of course go down the [Xamarin](http://xamarin.com) route (It used to be called MonoTouch) which le's you program iOS Apps in C#. It supports the entire iOS 6 (and iOS 7 if you have the beta). There's a free version which will let you deploy Apps in the App Store, but limited to 32K in size (which is nothing). To deploy any size Apps built with Xamarin will cost you $299 as an Indie developer and you will of course need a Mac and be a registered iOS developer which costs $99 a year. You need that anyway to write Apps and deploy them via the App store or on your iPhone/iPad etc.
&gt;C is a low level functional language. C is not a functional language.
&gt;Knowing C will help you grasp core concepts in C++ Or not since C and C++ are -- in practice -- totally different languages. The only time you write code that even looks like C code in C++ is when you're doing C interop, and even then you can glue C++ constructs in. Don't fall into the "*I should learn C to help me learn C++*" trap. It's very much a double-edged sword.
C++ code can also strongly resemble C if the program is trying to maintain comparability with many different compilers and be usable in embedded settings. 
For books, there is only one place to start: [The C Programming Language](http://www.amazon.com/C-Programming-Language-2nd-Edition/dp/0131103628) by Kernighan and Ritchie. Will knowing C# make learning C easier? Possibly, but it may also make it difficult, like trying to unlearn a bad habit. C is much less forgiving of errors, and it doesn't hide the [von neumann architecture](http://en.wikipedia.org/wiki/Von_Neumann_architecture) underneath like C# does. 
I don't get how, given that each of these examples are of undefined behavior in ISO C, there really is an issue of 'need.' Good read, and associated reads, though.
The need is to find programs that unknowingly invoke these forms of undefined behavior, resulting in strange and hard to diagnose bugs. 
The hardest part about using a slightly older book to learn from is the screenshots and steps might be slightly off. But it's nothing that can't be easily overcome. I say don't wait. Jump in. 
You don't see a need for tools to distinguish between correct programs and incorrect programs? Incorrect programs which may be pretending to operate correctly just long enough so they can finish wiping your hard drive and frying ram chips? I think anything that helps identify incorrect programs more quickly is good.
Ah, that makes sense, especially since UB is generally a pita to track down in pre-existing code. My opinion at first was more to do with teaching people proper coding standards, or proper functional knowledge of the standards, as to avoid UB in general, but if the errors are already there it makes sense to have a code diagnostic tool like the one mentioned in the article.
I'd add Java to that. There's plenty of indie game developers using Java and things like LWJGL. C is also good for games, although you loose the benefits of OOP. But there are some good game libraries that use C (SDL, Allegro, for instance)
C then C++. Conversely, C++ then C.
I'd recommend C++ then C, if you're going to be learning both of them. The issue with learning C before C++ is that it's just too easy to do things in C++ "*the C way*".
I agree, but for a game developer, "the C way" often yields the best performance.
Java is a good language and platform but its use is definitely on the way out. The installer is now bundled with malware and tricks end-users into installing the [Ask Toolbar](http://www.java.com/en/download/faq/ask_toolbar.xml). 
C is a wonderful language, but /u/ambethia/ is right: it's probably not what you really need. I'd actually suggest you start with Python. The game libraries are strong, it's easy to get started, and some of the stickiest stuff is managed for you. Once you're comfortable with that, then look at moving to something like C++.
Porque no los tres? C++ is probably the "de facto" for games. A lot of people end up reinventing the wheel with C (with regards to container objects, hash tables, string management etc) and although C++ can look ugly at times, it does help *a lot*. Compilers are getting better and better at giving more descriptive and less cryptic errors/warnings. C# on the other hand probably can't compete with C++ or C in terms of performance, but it would still be suitable for games that aren't CPU or graphics intensive, such as point and click adventures, simple side scrollers, Tetris clones, etc.
First, that installer bullshit is only on Windows and it can be run in silent mode. Second, if you users doesn't want to install JRE, you can bundle it with your program.
You should know how C works no matter which programming language you're developing in.
Your C coding habits will work in C++ but not the other way around, so this kind of make sense to learn C++ first so you don't need to unlearn anything.
Should be noted that learning Java is a good idea if you're planning on developing for Android.
 Yes. IMO the C language family (C,C++,C#,Objective-C) should pretty much be studied as a group. Not all at once to begin with: I'd say getting a pretty good grounding in C, then understanding that each of C++,Objective-C, and C# layer on top of the C-core. ---- As an aside, isn't lua a pretty common language in gaming programming? (I am not a games programmer).
Related question: Is C still marketable? I'm a relatively young guy (31) but recently I've been told I'm old-school because I program in C and use vi and make instead of using an IDE. Made me feel really old :(
As a former game programmer, I would say that lots of C++ code is very C-like. I have yet to see wide spread C++11 constructs. Because of this, I would recommend getting your feet wet with C. Make sure you understand it, and then when you're ready, you can move on to C++.
Yeah, definitely C++ is nice as it has been there for a long time and allows you to a reuse quite some libraries written before you, without having to go too low level with a language like C. For those considering C# they should know it is able to compete in terms of performance; its algorithms are heavily optimized, you are able to profile your code and you can generate a native image with Ngen.exe.
**Note:** *This post is made to help you understand where you get your downvotes from; I personally don't root for a particular side here, but want to make clear that the amount of statements that aren't backed up in your post can upset anyone enough to downvote you.* Yes, he is seriously asking this. Try to write World of Warcraft in assembler to prove your point; I assume, you won't get very far that way. Assembler is cool, right? It's fun to experiment with. Why is C++ not a pleasure to use? A lot of people use it, so I don't think it is not a pleasure. Why is it unreadable? You could increase font size. Why it it a pain to debug? Just because you don't master a debugger? Why is multi platform harder? You could use multi platform libraries, it's not a problem with the language; similarly, I could use a library in Java or C# that doesn't work on a particular platform. You say Java has automatic memory management, but C++ (automatic pointers, etc...; or use C++/CLR) and C# have that too so that isn't really a selling point. Why is it slower? Do you have unaffiliated benchmarks on this? What makes it more maintainable and readable? A lot of people think the verbose naming gives them the opposite effect. What makes it easier to debug? Debuggers aren't any more remarkable than they are for other languages; besides that, JVM crashes are fun. Why is LibGDX the best option? Best for what? "C language family", what does that even mean? Was Java part of it then, or not, why didn't you state that there? Attractive name, whatever; Java reminds me of coffee, coffee is attractive too, right? Or C++ maybe, which has that nice "plus plus" attached to it. Java clone, dunno, CLI / CLR / .NET is still somewhat different; also http://www.barrycornelius.com/papers/java5/onefile/ is a neat read, not just 2 or 3 better features. What does better mean in this context anyway? Better for game development? Is that relevant to OP's question? You don't need to use those frameworks you mentioned, that's like saying you need to use OGRE in C++ / Java. Slower than Java; I doubt so, you haven't heard of Native Code Generation then which makes arguments like these moot. Please come with proper unaffiliated benchmarks when you make statements like these. Mono implementation; well, it's not as if Java took much less time going cross platform. Debugging pain? Eh, why? "You won't learn much if you start in the middle?" Oh, a lot of people learned from examples though. "First learn programming, ..." Isn't that why OP asked this question in the first place? "Unity3D is really trying [... SNIP ...] you must work hard." Does that mean to not use any framework at all? That's not likely to be productive. Please stay on the OP's topic and back up your statements; well, unless you prefer not to convince people.
Learning C (I mean *really* learning it, not just doing beginner tutorials to learn the syntax) will make you better at any other language you already know. If you don't know any other programming languages, I'd suggest starting with something that makes it really easy to show something on the screen. Don't be as concerned about speed at this point.
You can't imagine how much i laughed at how said "lowest of levels"... Oh how the times have changed.
I'm also an advocate of learning assembly, but you can't pretend that you're doing it to be productive. It's a strictly academic language at this point, C at least retains some semblance of practicality.
What happens if I have a syntax error in my program? Do I get a bunch of preprocessor gobbly-gook or is there a cello preprocessor of some sort that does additional syntax verification? It seems like error messages would be extremely esoteric. Looks interesting, though.
just an ugly way of doing what you could easily do in another language
I'm quite surprised that its a modern trend to use signed types everywhere. First time I heard of it.
It surprises me mostly because it's so ass backwards. 
I think [Vala](https://wiki.gnome.org/Vala) is a more mature solution. Several Gnome projects are written in it. I, personally, have not used Vala but I do find it interesting. 
The operations macros I think I knew about. Regardless, that's awesome, and I'll probably use them in my projects from now on (if I can). Code readability goes a long way, after all! My question, however, is what exactly is going on in your two Macro examples?
The reason behind the alternative operators was that some old systems didn't have those symbols on their keyboards. The preprocessor stuff is fairly common. Some other lesser known features are [bit fields](https://en.wikipedia.org/wiki/Bit_field), [old style function declaration](http://stackoverflow.com/questions/4581586/old-style-c-function-declaration) and [compound literals](http://gcc.gnu.org/onlinedocs/gcc/Compound-Literals.html). edit: I forgot [designated initializers](http://gcc.gnu.org/onlinedocs/gcc/Designated-Inits.html) [X macros](https://en.wikipedia.org/wiki/X_Macro) are not really obscure language feature but still rare. With better optimizing compilers [Duff's device](https://en.wikipedia.org/wiki/Duffs_device) is slowly going out of fashion, but I still find it fascinating use of syntax.
I was looking for something exactly like compound literals a few days ago, thanks for that. X macros and Duff's device are incredibly clever and the reason why I keep loving C more. You can do so much with so little using these clever tricks!
The first macro example is an implementation of the C idiom of prefixing exported symbols from a module with a "namespace", which is generally the module name followed by an underscore. In the example, ns_a and ns_b can be referred to as _(a) and _(b). If, instead of "ns," the namespace was "somereallylongmodulename", then the space savings could be considerable. 
The first will concatenate the value of "x" with the prefix "ns_" The second will convert the variable name (eg: a) into a printable character string (eg: "a")
I love using ## and # for basic higher-order functions in C, to eliminate having to rewrite essentially the same function for things like memory assignment, socket opening, etc. Because constantly writing the same code to do error handling is lame. At work, though, I have to avoid the preprocessor hacks I love (no matter how well commented), because my boss thinks it's something that will confuse future programmers. Which I can understand, but am still frustrated by it. Edit: grammar
I think I'll post an example later
I don't see one iota of documentation. I'm sorry, but "just look at the headers/code" is not documentation. Do you really expect anyone to use a completely undocumented library? 
Programmers HATE this one simple trick -- SHOCKING results!!
Forgive my newbness: What's the difference between the functions you define in a `.c` file and higher-order functions?
Seems like this should have been in there sometime around the early 2000s, and definitely not after the new C11 standard was approved. A little late is an understatement, but then again when did Microsoft care about standards and actually follow them?
I think it's pretty well established that Microsoft doesn't care about supporting C, only C++.
The only reason Visual Studio 2013 is getting an improvement in C99 compliance is because C++11 requires it. See how they left out the parts of C99 that are not required by C++11? It's a pity mingw-w64 does not get more momentum, community and commercial support. 
This is only library support. They will be including designated initializers and compound literals which are not part of C++11. [Source](http://www.infoq.com/news/2013/07/vs2013_CPP_compliance)
X macros are horrible. Use arrays and loops. 
That's a common misconception. Come back to me when you get `qsort` running as fast as `std::sort`, bro.
Nobody cares.
Shouldn't that be #include &lt;unistd.h&gt; ?
&gt;Remember that first time you opened up linux or any terminal and someone told you compile that program? About as well as I remember being born. 
&gt;...hell, even the mighty GCC compiler targets C++ now GCC is now *written* in C++ if I'm not mistaken, but it compiles C. It's still distinct from G++, which compiles C++.
Of course not. Don't take it so seriously. 
&gt;Stallman is a fairly smart guy, but he's very full of himself, and assuming credit for an OS is just rude. I guess maybe he doesn't know any better, what with his social skills or lack there of. Stallman? Full of himself? Have you never heard of Linus Torvalds? Now *there's* an ego. He assumes credit for the OS because he wrote the fucking OS. An OS is SO much more than a kernel. So so so much more.
Yeah, it's a pretty bad idea. C is not a Windows language. C is a UNIX language. The design of C and UNIX are pretty heavily integrated with one another. To fully appreciate how C works, and where its power lays, you probably want to use a UNIX-like operating system. I'd suggest Ubuntu 12.04 for beginners - it's a long term support release. Oh, and when a job description asks for proficiency in C/C++, you know they probably have no idea what they're talking about.
Vala is completely different. It's a completely different language, compiled to C.
a++ and ++a should be equally as efficient, I believe.
I'd say C, if only because I can't stand the idea that 'everything should be a class!' attitude in a lot C++ code I see, I'm not a huge fan of OOP to begin with, but where it makes sense, it is nice (example: entity system for a game) but other than that, ugh, it is just too much. Question: what are you feelings about using a 3rd party engine to start off? If you're leaning that way (instead of trying to do this from the ground up), which language does said engine use? If you're planning on using something like Unity3D, learn C#, am pretty sure the Source SDK stuff uses C++, Quake (eek) and Quake 2 use C, Quake 3 and Doom 3 C++, etc, etc.
&gt;claimed that the unsignedness of size_t is merely a historical accident What? How would a negative size make any sense? In fact it really annoys me when I see code that basically replaces every instance of `ptrdiff_t` and `size_t` with `int`. I barely ever use `int` nowadays though, I normally use a `&lt;stdint.h&gt;` integer like `uint32_t`, or the two mentioned above.
Yeah, I guess if you consider EMACS an OS. Too bad it comes with a shitty editor. 
It would help if you posted a [short, self-contained, compilable example](http://sscce.org/).
I can't tell if the array you are sorting is an array of `SingleChar` objects or an array of `SingleCharPtr` objects. I think it's an array of `SingleCharPtr` objects. In that case, in your `CompareWeights` function, the arguments `a`&amp;nbsp;and&amp;nbsp;`b` are going to be pointers to elements of the array, so the type of `a`&amp;nbsp;and&amp;nbsp;`b` will be `SingleCharPtr *`, not `SingleCharPtr`.
Is something like this what you're looking for? #include &lt;stdio.h&gt; #include &lt;stdlib.h&gt; #include &lt;assert.h&gt; typedef struct SingleChar *SingleCharPtr; struct SingleChar { unsigned char Character; unsigned int Weight; }; int CompareWeights(const void *a, const void *b) { const struct SingleChar *p1 = *(struct SingleChar **)a; const struct SingleChar *p2 = *(struct SingleChar **)b; return (p1-&gt;Weight - p2-&gt;Weight); } #include &lt;assert.h&gt; int main(void) { struct SingleChar sc1 = {'A', 1}, sc2 = {'B', 2}, sc3 = {'C', 3}, sc4 = {'D', 4}; struct SingleChar *CharWeightMap[256]; CharWeightMap[0] = &amp;sc4; CharWeightMap[1] = &amp;sc3; CharWeightMap[2] = &amp;sc2; CharWeightMap[3] = &amp;sc1; printf("%c\n", CharWeightMap[0]-&gt;Character); printf("%c\n", CharWeightMap[1]-&gt;Character); printf("%c\n", CharWeightMap[2]-&gt;Character); printf("%c\n", CharWeightMap[3]-&gt;Character); qsort(&amp;CharWeightMap, 4, sizeof(SingleCharPtr), &amp;CompareWeights); printf("\nSorted:\n"); printf("%c\n", CharWeightMap[0]-&gt;Character); printf("%c\n", CharWeightMap[1]-&gt;Character); printf("%c\n", CharWeightMap[2]-&gt;Character); printf("%c\n", CharWeightMap[3]-&gt;Character); /* Check that it was sorted correctly */ assert(CharWeightMap[0]-&gt;Character == 'A'); assert(CharWeightMap[1]-&gt;Character == 'B'); assert(CharWeightMap[2]-&gt;Character == 'C'); assert(CharWeightMap[3]-&gt;Character == 'D'); return 0; }
The first argument of `qsort` should be a pointer to the first element of the array. In OP's code, the array to be sorted is apparently an array of `SingleCharPtr`; in other words, the items in the array are themselves pointers, having type `SingleCharPtr`, which is a `typedef` for `SingleChar *`. The variable `CharWeightMap` is declared to be of type `SingleCharPtr *`, so `CharWeightMap` is a pointer to the first element of the array. Note that `CharWeightMap` is declared like this: SingleCharPtr *CharWeightMap = /* stuff */; and not like this: SingleCharPtr *CharWeightMap[256]; Therefore the first argument to `qsort` should be `CharWeightMap`, not `&amp;CharWeightMap`. If you pass `&amp;CharWeightMap` to `qsort`, then you are passing a pointer to a pointer to the first element of the array; that isn't going to work. The last argument of `qsort` should be a pointer to the comparison function. In a situation like this, when the name of a function is used in isolation, it decays into a pointer to the function. So using `CompareWeights` alone is fine here; that means the same as `&amp;CompareWeights`.
He wrote GNU you incompetent idiot. Stop repeating jokes that got old before you were born.
Stallman wrote GNU in the same way that Torvalds wrote Linux i.e. the biggest contributor by far.
I would wager that neither one of them is the "Biggest Contributer By Far" anymore. 
Good point. OP's code isn't consistent. * The `CompareWeights` function treats the array as an array of `SingleChar` objects. * The array declaration in `main` seems to declare the array as an array of `SingleCharPtr` objects. This seems to match the description. This inconsistency is where OP's problem lies, I think.
&gt; Is "struct SingleChar *CharWeightMap[256]" not an array of pointer to struct? Yes, it is. But that isn't how `CharWeightMap` is declared in OP's code. In OP's code, the array is apparently allocated dynamically, so `CharWeightMap` is declared to be a `SingleCharPtr *`, which is a `struct SingleChar **`. So `&amp;CharWeightMap`, in OP's code, would be a `struct SingleChar ***`, which would be a pointer to a pointer to the first element of an array of pointer to struct. If the array is an array of pointer to struct, then the first argument to `qsort` should be a pointer to a pointer to struct, i.e., a `struct SingleChar **`, not a `struct SingleChar ***`.
I understand for a while now he's just been managing lots of merges. He audits more code than he writes. Also, what has Stallman contributed to recently? 
Linus does indeed audit more code that he writes. He does however write some code still, mostly for the core kernel stuff, not drivers for example. Stallman hasn't done much recently, but that doesn't stop him for being the author of those programs. 
Right. I was just making that comment because I didn't want OP to think that the error in the original code was `qsort(CharWeightMap, ...)` instead of `qsort(&amp;CharWeightMap, ...)`.
You don't inline your functions. The compiler knows better than you do and inlining a function can mean a cache miss, which makes your code *slower* than it would be if uninlined.
Will this do it? https://gist.github.com/hypercoder/6052118
Code uploaded to gist: https://gist.github.com/hypercoder/6052118
The problem, as I have mentioned elsewhere in this thread, is that `chars` is an array of `SingleCharPtr` objects, but your `CompareWeights` function is treating it as an array of `SingleChar` objects (because it is casting `a`&amp;nbsp;and&amp;nbsp;`b`, which are *pointers* to elements in the array, to `SingleCharPtr` instead of `SingleCharPtr *`). Your `CompareWeights` function should look like this instead: int CompareWeights(const void *a, const void *b) { const SingleCharPtr *p1 = (SingleCharPtr *)a; const SingleCharPtr *p2 = (SingleCharPtr *)b; if ((*p1)-&gt;Weight &lt; (*p2)-&gt;Weight) return -1; else if ((*p1)-&gt;Weight &gt; (*p2)-&gt;Weight) return 1; else return 0; }
&gt;The compiler knows better than you do and inlining a function can mean a cache miss, which makes your code slower than it would be if uninlined. I know, which is why all my inline functions are either invoked only once or consist of a single statement. It's the first case I'm worried about: does the optimizer count the number of times a function is called?
Thank you very, very much. This solution worked. I will edit the question as solved.
http://jbremer.org/abusing-forced-inline-in-c/ &gt; However, it should be noted that inlining relatively bigger functions (those with more than a few lines of code) can be fairly expensive. This is because the CPU can cache functions (or, actually, pages that the functions are located on) that are commonly called. But if such function is inlined multiple times, then the CPU will not be able to recognize that, and it will not be able to cache the particular function (e.g. if max is inlined several times, the CPU will not recognize one from another.) So, as usual, the techniques discussed here will drain some performance. He also gives this macro: #ifdef __MSVC__ #define FORCEDINLINE __forceinline #else #define FORCEDINLINE __attribute__((always_inline)) #endif 
Yes, the weights are always going to be &gt;= 0. I will make a note of what you said. I definitely will keep in mind about the potential integer overflow.
Lua's fairly popular as a scripting language on top of games, and even emulators (like FCE Ultra)
Thanks for the tip. Maybe I can game *and* code on Windows now instead of bouncing between Gentoo and 7.
Will both of you stop this flirting and just kiss already!
Oh Morley, go fondle your M&amp;P. 
Oh, sorry. I thought you were disagreeing with my statement. My bad. Yes, I'm in complete agreement with what you wrote here.
Agreed, a horrid frankenstein monster, but what to do? OP wants to develop iOS apps. 
OK, this is a great relief as I am a relatively ***old*** guy (51) and am trying to become knowledgeable enough with C to be marketable (and I use Geany).
Anyone have a diff showing what's been updated for this year? Edit: nvm, they included a mark for changes in the documents themselves that I somehow missed... 
Thank you so much, I would have missed it this year
Congratulations, Windows now supports a 14-year old standard.
While this seems useful, it makes me worry about the growing size of the C language. If you need polymorphism, you probably should use a language that has *actual* polymorphism, although `_Generic` does sound quite useful.
This only adds to the C preprocessor. This does not affect the language directly. C has suffered with macro hell for years and the lack of type information in the macros. Being able to detect and even reject types in macros is a huge improvement in my opinion. 
strtod is accurate. But printf by default rounds to 6 digits after the decimal point. Try something like `printf("%.17lf\n", cf[i]);` Printing floating numbers accurately is a difficult problem that was unsolved when the C library was being designed. See [How to Print Floating-Point Numbers Accurately](http://grouper.ieee.org/groups/754/email/pdfq3pavhBfih.pdf) (Steele 1990) and [Printing Floating-Point Numbers Quickly and Accurately](http://citeseer.ist.psu.edu/viewdoc/download?doi=10.1.1.52.2247&amp;rep=rep1&amp;type=pdf) (Burger 1996).
You could work around this problem by using hexadecimal float represendation. (see %a for printf).
Thanks for the quick and precise answer, as well as for the links. I feel dumb now.
Maybe you referred to C++, not C? Because C is not growing that much, they are not making mistakes as other languages -&gt; adding bunch of stuff and changes to language. _Generic is feature from 2011, i guess someone finally realized kind of generics can be useful with C, plus same goes for _Static_assert and others. For, so called, actual polymorphism or any other feature -&gt; if you want/need C, _Generic and co will be good addition. If you want/need BASIC, go write in BASIC. Not big deal. 
Plus, maybe typeof would also be good solution, but think what would that do to everyday macros -&gt; become even more unreadable and crippled version of templates? So, maybe _Generic is not such a bad choice. Only time can tell.
It's still a gross layering violation, as previously the C preprocessor dealt strictly with tokens, and now suddenly it has to care about types. EDIT: I was wrong. _Generic has nothing to do with the preprocessor. It's a language feature, but because it only introduces a new kind of expression, it doesn't really complicate anything.
I went a little crazy trying to debug something similar to case 1. Non-null terminated string placed at the end of a struct. Worked fine in 64 bit because the struct was padded and the area in memory where padding occurred just *happened* to be zeroed. On 32 bit however, non-null terminated string caused it to crash. Gotta love C right.
Most, if not all of these errors seem to be due to poor programming though, and are easily avoided using both a good compiler and something like valgrind.
Time to add a metaprepreprocessor.
No. Compiler and valgrind ill help. See also [The forgotten problems of 64-bit programs development](http://www.viva64.com/en/a/0010/) and [The third myth - dynamic analysis is better than static analysis](http://www.viva64.com/en/b/0117/).
For all my bit twiddling needs, I usually start [here](http://graphics.stanford.edu/~seander/bithacks.html) (which actually has what the author found in the iOS library).
I guess that's kinda cool. But why do I need that? It looks like it would just lead to problems.
You are correct, while a bit late :) . Thanks anyway though.
Hm, by reference... You can have the classic pointer just like void function_name(int *array, int *size) and in the second parameter you pass the size of the array. it doesn't matter that the function only knows (by reference) the address of the first element of the array. it's a pointer, it will point the other elements of the array too :P
I don't understand your question. Given that : f(int * a) { int i; for (i = 0; i &lt; 100; ++i) { a[i] = something(); } } int main() { int a[100] = {0}; f(a); return 0; } Then the pointer **a** of type (int \*) is passed by copy to f. Its value reference the same address, and thus the original array **a** in main will be affected by **f**. What is the problem then ? Do you want to do something different ?
+1 for pointing out that C has only pass-by-value. Pedantic point, but important to proper understanding of C, just as understanding that everything in Java is by-reference is important to proper understanding of Java.
[Actually...](http://javadude.com/articles/passbyvalue.htm) 
You are correct - Java parameter semantics are similar to C++. I was really referring to the lack of user defined value types, but was in to big a hurry at the time to express myself well. [edited to remove "&lt;sigh&gt;" that made it sound as if I were annoyed at the correction. I was annoyed only at myself for communicating poorly.]
This is ultimately the correct answer here. I tried to express it but I lacked concision.
Yeah, that wouldn't help.
Yes it would. The problem is that not all float or doulbe values can be represented in the decimal system. The workaround is that you add enough digits when printing those values to make the rounding error smaller then the float precision. Hex representation on the other hand can perfectly represent any float value. Another bonus is that they are shorter than decimal floats. which makes it prefect for filter tables.
Slides (PDF): http://www.pvv.org/~oma/UnspecifiedAndUndefined_ACCU_Apr2013.pdf
His problem was that printf wasn't printing enough decimal places, because he didn't specify them. BTW, hexadecimal has the same limitations in representing floats as decimal. All of the limitations come from being unable to represent fractions in binary perfectly, and hexadecimal does nothing to fix it.
His problem is lack of precision. Using %a will fix this. &gt; All of the limitations come from being unable to represent fractions in binary perfectly, and hexadecimal does nothing to fix it. This is not correct, see documentation for %a: &gt; The default precision is sufficient for exact representation of the value. You can also do some testing if you don't believe me. #include &lt;stdio.h&gt; #include &lt;math.h&gt; int main(void) { char str[512]; double pi; snprintf(str, sizeof str, "%la", M_PI); sscanf(str, "%la", &amp;pi); printf("%s\n%la\n%la\n", str, pi, pi - M_PI); return 0; }
for some reason that page eats insane amounts of memory on my machine
More accurately: knowing "the C way," whatever specifically that means, generally gives you a better understanding about what the computer's actually doing, without the varying layers of abstraction (the assembly devs are laughing right now). It often enables you evaluate what constitutes best performance, and make decisions accordingly. 
There are a few style guides, nothing official like PEP8. Check out K&amp;R (Kernighan and Ritchie), ANSI, GNU, etc.
Installing xcode installs gcc. After installing xcode, you can use any text editor and gcc from the command line.
Thanks for your response. I know xCode has gcc built inside of it, but isn't this IDE really large in size? That is, I keep seeing people on the internet complaining about how much space xCode takes away from their computer. I was wondering if there was a less memory-intensive IDE or compiler. Put more simply, are there alternatives to xCode?
You don't need to run it so it being memory intensive doesn't factor into it. Installing it takes up space, yes.
Xcode 4.6 is less than 2GB installed. Alternatively you could install gcc through homebrew or the like.
Sorry, but what is homebrew? I have never heard of this. Thanks for the help. 
A package manager for os x. Think of it as the equivalent of the App Store for free software. [homepage](http://brew.sh/)
Apple stopped shipping gcc. Their gcc is actually llvm-gcc. But that is deprecated/end-of-life. You should use clang which is the default compiler and is pretty efficient compared to gcc. Clang also tends to give much more human friendly compiler warnings and errors than gcc. And Clang is also currently farther ahead in its C11 support than gcc if you want to use the latest standard features of C. You don't have to use Xcode and can just invoke clang through the command line (noting what others said about installing the Command Line Tools). However, I think you should try using Xcode, especially if you are learning. Xcode integrates clang deeply into Xcode so the IDE can tell you as you are typing whether the line of code you are writing is correct or not. And when you have a mistake or do something that is a compiler warning, Xcode will show you that too and can even present a list of fixes that you can click that will make your code correct. And of course there is autocomplete, but clang based autocomplete is much better than most text editors because clang is using real semantic knowledge of the language and not guessing at previously typed variable names or keywords. 
clang is fine and ever-improving, but gcc still gives better and more warnings, is customizable/configurable far beyond what clang currently can do, and the binaries it produces have sometimes been up to 12x faster than what clang does in my benchmarks (these are old numbers, though). So having both compilers (and a recent GCC version, not whichever ancient version apple ships/used to ship!) is pretty much always a boon, for learning because of GCCs superior diagnostics, and for serious programming because of GCCs faster output. Personally I compile with both (as well as a few others, like keil, icc, ... depending on the situation), and use clang for the pretty error messages, and gcc for producing the final binaries.
Don't you need Xcode in order to have a compiler for which you can use homebrew?
&gt; gcc still gives better and more warnings This is one complaint I have never heard about Clang, or experienced myself. Do you have some examples of that happening?
If you already are downloading XCode, you should consider using it as the IDE too. It's rather cumbersome compared to some of the other stuff I used, but it sure beats trying to debug using gdb on the terminal.
^- This guy.
I think you can just go sign up at developer.apple.com and download the command line tools. You can avoid installing the big xcode package in this way.
I can dig out some examples for you when I get back home. In general, clang formats errors prettier, but GCC can give you a lot of warnings that clang can't, and allows you much finer control about what warnings you want.
Yeah I have seen the command line tools being mentioned before, but when I click the link you provided it says that Apple is in the process of updating the developer page. I guess once that update is done, I'll try the stand alone command line tools. Thanks! 
It worked for me just now.
Really, maybe I am doing something wrong? Went I go to the Apple developer page I click on Open Source and on the right side of this page I see Command Line Tools. But when I click on that, it leads me to a page talking about the updates Apple is working on.
Yes. Same is true for Mac Ports.
you could try installing virtual box, a few different linux variants inside that and then creating a dev environment in there, see which you like best. I prefer this as it keeps the Mac OS nice and clean and I can just copy the VM elsewhere and preserve my whole dev environment, able to run it on any machine/ OS that VirtualBox can run on. https://www.virtualbox.org/
Thank you very much for such a thorough response. I also appreciate the advice. See when I write *gcc* in my Terminal, it says that such a command is not found. Doesn't this mean I don't have gcc on my computer? If so, how can allow my computer to recognize and compile C programs?
I agree. In the general case, gcc is the one that gets all the complaints and this was one of the very reasons clang was invented. But you can go straight to the horse's mouth. Watch Chandler Carruth's (from Google's clang team) presentation: http://channel9.msdn.com/Events/GoingNative/GoingNative-2012/Clang-Defending-C-from-Murphy-s-Million-Monkeys He intros all the problems of gcc in the just the first 5 minutes of the video. 
He is talking about something different though. Yes, when gcc and clang both give a warning/error for the same thing, I first look at clangs output. Where clang gives error messages, they are usually superior in readability, and it often makes more sensible suggestions on how to fix the issue. (although gcc has improved a lot here since 4.8) But what I'm talking about, is gcc being able to warn you about things that clang can't warn you about at all. Sometimes I get warnings from gcc about things that turn out to be real issues, and clang was completely silent about it. Sometimes these warnings are because I enabled a warning flag in gcc explicitly (and gcc has about a bajillion of those, and I usually have as many enabled as I can), but clang has no corresponding option that allows you to turn a similar warning on. As per usual, to be taken with a grain of salt. I haven't seriously used either for the past two months, and clang does develop pretty rapidly. But my bottom line is to compile with both, using clang for the pretty error message, and gcc for producing binaries and getting some extra analysis. I also sometimes use icc in addition to get some more extra remarks for the same reason.
&gt; First, run which gcc to find out if gcc ins't just in a strange directory `which` only checks the user's path, so `which gcc` will not work if `gcc` on its own does not.
if you have zero experience with programming, C is going to be VERY difficult to wrap your mind around. Why did you choose C over something more suited to a complete novice?
http://www.computerscienceforeveryone.com/ Maybe?
Don't buy books. Everything a beginner could ever want to know is online, for free.
Looks good, will give it a test run asap. After a long night shift. Thanks.
Free sounds good. Hard earn money is not easy to be spent on something that might not help at all.
I'd recommend [C Primer Plus](http://www.amazon.com/C-Primer-Plus-5th-Edition/dp/0672326965), though to be honest it would be a much better investment to study C++ with [C++ Primer Plus](http://www.amazon.com/Primer-Plus-5th-Stephen-Prata/dp/0672326973) from the same author. Do you have to go with C? C++ is better for beginners while still maintaining all the C things. If you absolutely have to go with C (which I'm assuming you are studying for coding systems - most likely embedded, or to maintain legacy code), the book I pointed out should start you out nicely. If it is programming you want to learn, I'd recommend trying your hands at Java or Python or any interpreted OOP-focused language first. Java has very nice IDEs (Eclipse, NetBeans, etc.); it has its roots in C/C++ so it should help making the transition back and forth. C/C++ can be a mess to get the build environment set up correctly if you are not using an IDE like Visual Studio. If you are in college, a lot of these technical books are free through your online library.
The idea to go with Java did cross my mind, but reading about how c is bit harder and more rewarding at the end did convinced me to go with it. On the other hand i might rethink it since i can be easily scared of by it. I will have to find that out myself. 
Whatever you do, if you want to learn C, you really got to understand addresses and pointers. This is the most crucial differentiation between C and other languages, with exception of C++ and ObjC to some degree. Learn and practice pointers. It is not optional. 
C Primer Plus was a great supplement with my education. Not sure why you were down voted; you offered great advice. Reddit can be retarded.
Go to the library, and look for "C/C++ in 21 days" or "C/C++ in 24 hours". Perhaps the "C/C++ Primer" books. I don't know if these books will say "C/C++" on the cover, or just "C" or just "C++". These are two different languages, C++ being a popular derivative. My first programming books were the "C for Dummies" books when I was 9-ish. I liked the 21 days books better. They're terse, so you're going to have to supplement your reading with [Wikipedia](http://en.wikipedia.org/wiki/C_(programming_language)) and [other sources](http://en.cppreference.com/w/c), which I think would actually help you develop some insight. Bear in mind these books tend to be terse, and none of them teach you good practices, they just demonstrate the principles. It's up to you to find suplimental materials to learn how to write code that doesn't do things the hard way, that is readable, and stable. It bears mentioning here, since I have nowhere else to say it, but there are libraries of code that already do things for you. For now, I recommend focusing on the "standard library". Your code will be better for it. --- Whatever you do, **DON'T** read K&amp;R. It was first written in the 70s, and represents the now mostly irrelevant beginnings of the C language. The examples are incomplete fragments that don't follow the current C standards, so they won't compile. The book also doesn't reflect 40 years of knowledge learned about C and programming in general. ANYTHING published in the last decade or so will contain examples that compile. K&amp;R is also full of things now considered bad habits. The only thing of note is the religious part of the Linux kernel dev community both worships K&amp;R as an infalliable tome and more relevantly they use it as a coding and formatting standard. If you were to learn anything from it, it would be that, so if you ever dared submit a patch, the flaming you will receive will be minimized. --- You'll also want to research coding standards, and find a way of writing that makes your code consistent and easy to read. And take a look at the [Secure C](https://www.securecoding.cert.org/confluence/display/seccode/CERT+C+Secure+Coding+Standard) coding standard, to learn how not to write code. --- As for an environment, if you're on Windows, Visual Studio Express is free, and will get you by. I'm not sure which C standards it supports. The MSDN library is where the compiler and language documentation lives, it would behoove you to learn how to navigate and read that documentation. I otherwise suggest Linux and GCC or CLANG as your compiler. Eclipse CDT is a Visual Studio like environment. If you're not already running Linux, I recommend installing Virtual Box and using Linux Mint or Ubuntu. The nice thing about the Linux route is you have an environment that is more conformant to the language standards, and the tools are much more modular; you can learn the parts, add in cool bits like CCache or lint, and overall, it's an environment you can grow in. Windows, by contrast, is a package deal.
I use http://www.cplusplus.com/ all the time, especially http://www.cplusplus.com/reference/. Helped me lots
Also, if C is what the OP wants to learn, please don't suggest learning it from http://en.cppreference.com/w/c as it is a C++ reference. Wikipedia articles used to be incorrect about C in the past, I don't know how it is now.
Don't just look at the link, half their site is dedicated to pure C.
I wasn't aware of a second edition, I've only ever seen the first edition. With that, 1988 is probably relevant enough to learn the basics, but we have C11 now, and Wikipedia says the most recient revision of the 2nd edition does not cover it. While it would behoove OP to understand older versions of the language still in active use, I personally recommend learning based on the most recent versions of the language. We should be moving forward, not propagating relics.
That site is good. I completed the course. But Harvards CS50 uses and recommends Absolute Beginners Guide To C Programming. I've learned so much from it. It well written an entertaining. The author also shows you where to go from there, when you finish the book. 
Check out /r/carlhprogramming and www.computerscienceforeveryone.com. His regime is where I got my start with C more than a year ago as a complete beginner (no other programming experience). I feel pretty comfortable with the C language now, and I've learned a bit of Python too.
http://www.iso-9899.info/wiki/Books
I just don't trust sites, "tuts" and things like that. You want a C reference - get yourself a Harbison &amp; Steele, or at least the freely-available draft of the standard.
&gt; Whatever you do, DON'T read K&amp;R. It was first written in the 70s, and represents the now mostly irrelevant beginnings of the C language. The examples are incomplete fragments that don't follow the current C standards, so they won't compile. The book also doesn't reflect 40 years of knowledge learned about C and programming in general. ANYTHING published in the last decade or so will contain examples that compile. K&amp;R is also full of things now considered bad habits. I've been going through the K&amp;R 2nd edition and each code snippet I've had to use to solve exercises compiles just fine with GCC. I can't speak for other compilers. Given that they created the language, I believe it's a great way to *start* learning the language. The more recent C11 standard can then be read to add onto the existing foundation that the K&amp;R lays down.
Absolute Beginner's Guide to C: http://www.amazon.com/dp/0672305100/ It's old, but it will definitely get you started. Of course the other recommendations are good too, but this might be worth a look. 
I had problems with the K&amp;R until I started actually doing the exercises. The exercises forced me to apply what I was reading and proved to be very effective. YMMV of course, but it's worked great for me. I'm in chapter 5 so far and look forward to mastering pointers.
I'd recommend [Learn Code the Hard Way](http://learncodethehardway.com), starting with Python and then moving on to C. Zed Shaw has some great explanations and does a great job of teaching you to find information on syntax and common coding mistakes.
Yeah, I use the 2nd edition of it.
k&amp;R
It wasn't until I learned about stack frames and what the heap was that C started making complete sense. Specifically how memory is organized, what a pointer is, and what the data a pointer holds means. This is all after the idea of learning variable types, loops, functions, etc.
You can start with Python, but I think Zed does a good job ushering people through C with [his C course](http://c.learncodethehardway.org/book/).
This is the book I learned C with, really good stuff.
Thanks. Where did you go from there? 
As far as books? Nowhere, really. I haven't found a book that touches on intermediate or advanced C quite as well as Absolute Beginners' Guide to C touched on the basics. I learned a lot of intermediate stuff from tutorials, reading source, and various university pages on the internet. Now that a wealth of OpenCourseWare is available, I have been getting into learning more advanced, general CS stuff, and applying those concepts in C.
Everything is back online. If you're still seeing an "Updates" page, try reloading (the old page could be stuck in your cache).
This guy is giving some terrible advice that should not be followed on the Mac. First: gcc is not installed by default on the Mac and even when you install Apple's Command Line Tools, the gcc version installed is old and should only be used for legacy reasons. You should always compile with clang (part of Apple's Command Line Tools). Second: the IDE does not make you a worse programmer. C/C++/Objective-C on the Mac is canonically done in Xcode and this expectation at all levels mean that Xcode is actually the best way to do things. All the advice you'll see online will reference Xcode. All the tools are made to work better with Xcode than on the command-line. All the OS API documentation is integrated into Xcode. You need Xcode to run the iOS simulator or run on iOS devices. Xcode will encourage you to set up correctly with version control, project archives and snapshots and makes creating testing targets simpler. In short: the Xcode tools are the best way to edit, debug and profile your programs and doing things on the command-line should only be done if you have no other option (e.g. you're stuck with an autoconf makefile project from elsewhere). If you absolutely *need* gcc (you probably don't), install Homebrew on your computer: http://brew.sh and once you've installed run: brew install gcc
Perhaps not overall, but it still stands within this context. It is not reasonable to ask a complete beginner to figure out how to configure a toolchain. OP should start with the simplest environment they can, get a program compiled and working first, learn other skills first, like debugging, and then worry about the nuances of each program in turn. I would recommend learning different standards of the C language last, and really, only if OP is going to be supporting an existing legacy program.
Can you post the code you have so far?
 uintptr_t UTIL_searchuptr(uintptr_t *voidv, uintptr_t voidc, uintptr_t target){ if (voidv == NULL){ herr(-1, "!UTIL_searchuptr("ptrx", %"PRIuPTR", "ptrx"): Recieved NULL argument!", voidv, voidc, (c2ptr) target); return UINTPTR_MAX;} if (voidc == UINTPTR_MAX){ herr(-1, "!UTIL_searchuptr("ptrx", %"PRIuPTR", "ptrx"): voidc is too large!", voidv, voidc, (c2ptr) target); return UINTPTR_MAX;} if (voidc == 0){ return 0;} voidc = voidc-1; register uintptr_t l = 0; register uintptr_t h = voidc; register uintptr_t c = 0; do { c = l+((h-l)/2); if (voidv[c] &lt; target){//look upwards first because division (and right shifting) rounds down, which means that the target is more likely to be in the higher half //printf("looking up (voidv[c=%"PRIuPTR"] = %"PRIuPTR")\n", c, ((uintptr_t *) voidv)[c]); l = c+1;} else { //printf("Shrek, I'm lookin' down! (voidv[c=%"PRIuPTR"] = %"PRIuPTR")\n", c, ((uintptr_t *) voidv)[c]); //printf("c=%"PRIuPTR"\n", c); h = c-1;} } while ((voidv)[c] != target &amp;&amp; l &lt;= h &amp;&amp; c != 0); return c;} This is the best I have so far, but I've only tested it once and have a sneaking suspicion that if the proper result is odd it'll return the index before it, and I'm trying to do the same thing for strings (using `strcmp()` and storing the result in a variable) and I just can't get that to work at all.
C++, but http://en.cppreference.com/w/cpp/algorithm/lower_bound does this.
What toolchain configuration do you mean? The default GCC, installed by my operating system, will compile original K&amp;R code. No configuration required. 
You might want to get a simple, non-performance-tweaked for() loop version working first. It won't take long at all, and once you have a grasp on the basics, then you can go back and implement the binary search version.
A couple of issues I've found: * Your while loop stops if c == 0. This is a problem if you're looking for element 1. It searches until it gets to a situation where l == 0 and h == 1, and then checks 0 next. That's less than target, so it sets l to 1. It would find it next time except that c == 0, so it short circuits there. I haven't tested this, but I think you might be fine if you just remove this condition, because the l &lt;= h should catch your end case. * If an element not in the array is given, it returns the next lower index. You said something about inserting it, which appears not to be implemented yet. The issue as it is now is that from outside of the function you can't tell if it was there or not. You may have plans to address this already with your future work. Other than that, it seems to be working well, and looks accurate to me.
The first one is a relic from when it was structured differently, and the second one is what I'm currently having problems with. Note that this function won't be inserting anything, whatever calls it will (should) check for equality and insert it in there (if it wants to), which is why I want it to return the index of the next higher element (so the calling function can just do a `memmove()` and call it good).
Well, let's see if I can offer some help on the second one then. The first issue is that you'll need to resize the array. You'll probably want to get a uintptr_t** instead of a uintptr_t into the function that calls this, so that you can return a different array. Then use [realloc](http://www.cplusplus.com/reference/cstdlib/realloc/) to create an array one bigger (if necessary) You could use this function as is, and then do something like: //C-like pseudocode warning uintptr_t index = UTIL_searchuptr(*voidv,voidc,target); if ((*voidv)[index] != target) { //expand array with realloc //copy everything above index up one //insert target at index+1 } Does that address the problem?
It currently returns the index to the next lower element. Why do you need the next higher instead? You could just add one if it doesn't match. The cleanest way to get the next higher element would be to round up instead of down in your function for finding the middle. There are a [variety of ways](http://stackoverflow.com/questions/2745074/fast-ceiling-of-an-integer-division-in-c-c) to accomplish this.
I figure that after you `memmove()` all the farther elements you already have the index to insert it and then read it... it just seems nicer and I'd only have to fix it once instead of every time a function needs to insert something. EDIT: shoved `if (voidv[c] &lt; target){++c;}` at the end of it and it seems to be working now, but I'll have to fix that some time.
I'm using this book right now. It's been really great.
You can code object oriented C, just not conveniently. Though doing it gives you a tremendous insight as to how things like classes, object instances, and inheritance work in other languages. You also realise the convenience in those other languages is a lot of smoke and mirrors.
I always thought the short answer was simply to never use bit fields (they're just too weakly defined to be worth using).
Who typedefs bool to be an int? either char or uint8
8bits accesses on some processors are slower than 32bits accesses. It might be better sometimes to define it as a 16 or 32bit unsigned int.
I think I'd generally rely on the compiler for that detail, but fair enough.
Bitfields are slow except in the case where you're branching based on the zero-/nonzero-ness of a bitfield (e.g., using a single-bit field as a read-only-ish Boolean). Most CPUs can do that in one instruction.
I'm hardly defending the practice, but lots of people write code that is supposed to be consumed as both C *and* C++. If you're in that situation (which is what the article seems to be addressing), then it makes complete sense to switch between the two when discussing approaches to booleans.
C and C++ cannot both be compiled from the same full source code in any real project. Just for starters you can look at http://en.wikipedia.org/wiki/Compatibility_of_C_and_C%2B%2B C++ changes how a fair number of C mechanics work.
Sometimes you just need a bool. In the embedded C processor we use typedef unsigned char bool; works just fine, 1 byte. We know it is not necessarily portable, but sometimes you just need a bool. Is this switch on or off? Making a bool makes it more explicit than 0 / 1 in my opinion. 
I think this article was highlighting one of them.
Note how extracting the value of the byte requires additional operations besides the access (shifting and masking).
But the compiler won't generate an unaligned access---you'd have to do that yourself. The very act of shifting/masking prevents an unaligned access.
Actually that depends on the situation. If you're reading a single byte at an arbitrary address, it'll be slower by a few clocks. If you're iterating over bytes, the first access might be slower, but after that you're not going to lose any time because you're just replacing increment+load by a shift. But doesn't ARM have LDRB/STRB anyway? I can't think of a halfway recent processor that can't do an 8-bit load/store. (Although I worked on one very recently that could only do 32- or 64-bit atomics, which necessitated the shiftymasky approach.)
Too bad there's no support for C99 and later, and it doesn't appear to have been updated since '07.
Can you post the whole thing?
Agreed. Without the actual code that's compiled by gcc, it's hard to tell where the warning comes from. As an aside to OP: the "l" in `%lf` does nothing. `%f` is fine for both floats and doubles. I think `printf` actually promotes floats to double before printing them anyway.
Edited post, stuck pastebin up there.
You're trying to pass the array ferr as a double. Maybe you'd like to use an index? ferr[k]?
Hmm. Explain please? Most of my knowledge of c is effectively "I learned it as I did it" and so there are gaps.
Line 43: fprintf(Output, "%lf %lf %lf %lf %lf %lf %lf", t, mag, merr, fmag, ferr, smag, smerr); ferr is declared as: double ferr[300]; A single double is expected, but you're passing a pointer to a whole array of doubles. Without trying to understand your code further, I think your line should be: fprintf(Output, "%lf %lf %lf %lf %lf %lf %lf", t, mag, merr, fmag, ferr[k], smag, smerr); 
&gt;I think printf actually promotes floats to double before printing them anyway. [That's just variadic calls in general.](http://www.gnu.org/software/libc/manual/html_node/Calling-Variadics.html)
Oh. Oh. That's supposed to fmerr. Let me see if that fixes it. Fixed it. Hooray! Almost all of my errors are typos!
One more question to anyone reading this. Do I have the correct setup for my fopen options? I seem to not be printing any output.
Please check [my reply to Nimbal](http://www.reddit.com/r/C_Programming/comments/1kq3e8/having_issues_with_gcc_making_a_variable_into_a/cbrihnl) for some important corrections.
To be honest, your "correction" probably causes more confusion than it avoids, because you don't give one actual, visible difference between pointers and arrays besides their type which is rarely, if ever, an issue in practice. The only differences I could find is the behaviour of the `sizeof` and unary `&amp;` operators. The `sizeof` an array will be the total size of its occupied memory (that is, the number of elements multiplied by the `sizeof` one element). Note that this only works when the compiler can see the number of elements in an array, so if you pass it as a parameter into a function, `sizeof` won't work on that array. The unary `&amp;` operator for arrays is best explained [here](http://publications.gbdirect.co.uk/c_book/chapter5/arrays_and_address_of.html) where it says that, as you said, the expression `&amp;array` will have a visibly different type that `&amp;array[0]`, although they will probably point to the same address. So, there are some differences between pointers and arrays, but they are similar enough that it's practical to treat them the same.
No need to put the word 'correction' in scare quotes; sftrabbit is correct. I've seen plenty of mistakes made due to people confusing arrays for pointers. `sizeof` and address-of also aren't the only differences. For example: typedef int T[10]; T arr[10]; // array of pointers or array of arrays? T foo(); // nothing wrong with returning a pointer, right? And in C++ where you can do more with types the differences become even more pronounced.
Also, I think in all of your nested loops, you should follow the convention of fftw where the y is the fastest moving index, and x is the slowest. I think, for example, your generate2DSin follows the convention properly, and the very next loop in the main function that copies this data into the larger buffer does not. (notice how in generate2DSin you have on the innermost : index = ii*leny + jj; and in the main function: index = ii * xSize + jj; Also, after that you do some zeroing, but these values should already be zero because they were calloc'd, and you do not zero all the values you would need to anyway (you zero coordinates where x &gt; xlen AND y &gt; ylen, rather than OR). 
I have done similar work but in somewhat more detail (my implementation has been in use for quite a few years now). Unfortunately the sources aren't browsable online, you have to [clone the git](http://controls.diamond.ac.uk/downloads/python/cothread/cothread.git) or [download the sources](https://controls.diamond.ac.uk/downloads/python/cothread/). I really ought to create a blog for this sort of stuff, but in case anyone is interested, here's what I've done in [cothread](https://controls.diamond.ac.uk/downloads/python/cothread/). Firstly, the whole thing is packaged as a Python library with a very small C core and a much smaller assembler kernel. * The Python library implements the notion of "cothreads" which is are user space scheduled cooperative threads implemented as coroutines. The primitives here are creating cothreads and waitable event objects. * The C part consists of two files: `cocore.c` which provides the core coroutine switching and is probably of the most interest here, and `_coroutine.c` which provides a wafer thin Python binding to `cocore.c`. * The assembler part is `switch.c` which conditionally includes one of `switch-{x86,x86_64,arm,ppc_osx}.c` and provides two functions: `create_frame` to initialise a new coroutine, and `switch_frame` to transfer control to a coroutine. There are a lot of tricks to make this all work smoothly. Perhaps the most surprising, and one that I lifted from [greenlets](https://pypi.python.org/pypi/greenlet), which was the basis of the original implementation, is the option of recycling the stack frame when switching coroutines. This insane trick allows me to quickly create tens of thousands of cothreads on a 32-bit system, whereas if each stack frame was allocated its own block of memory this would take longer and run out of memory.
Good work! I'm trying to integrate it into one of my projects (for purposes of testing), I wonder how do you get away with all these void* arithmetics? I know of no compiler that wouldn't actually give you a lot of stick about that.
Instead of writing assembly and storing the stack, I'd just store away all the information that is needed for the *logical* function to continue in the `coroutine` structure. The result is portable and the code might be somewhat more readily understandable. However, the coroutine function would have to be designed in a very specific way, and you couldn't just create a generic `yield` function. I guess it depends on how many coroutines you need. If you require more than one or two, the solution presented in the article might prove better.
I rely on [gcc pointer arithmetic](http://gcc.gnu.org/onlinedocs/gcc-4.4.6/gcc/Pointer-Arith.html), as I consider this a helpful extension and only find myself targeting gcc (or clang). In other code I make quite aggressive use of gcc extensions such as value = ({ statements; expression; }); (in macros only, of course, ugly as hell in real code!)
I'm pretty sure this is exactly what [lua](http://www.lua.org/) does -- this has coroutines as a native part of the language, but the entire interpreter (written in C) must use a single stack. (Some speculation here, it's been a while since I looked at the language closely.)
Lua hides the implementation from the programmer, though. In C, not only your functions have to be specifically designed to work as coroutines, the logic that enables coroutines needs to be programmed explicitly.
But the whole point of coroutines is the "magic" flow of control, hence the whole stack switching business, and you just can't hide that if you're going to have coroutine-like flow of control among C subroutines.
It's worth pointing out that on Windows and POSIX you **explicitly can** convert back-and-forth between regular pointers and function pointers.
&gt;If the C standard says that converting a function pointer into a regular pointer is undefined behaviour, then you shouldn't do that at all in C, regardless of what platform you're running on. [dlsym (POSIX)](http://pubs.opengroup.org/onlinepubs/009695399/functions/dlsym.html) [GetProcAddress (WinAPI)](http://msdn.microsoft.com/en-us/library/ms683212(v=vs.85\).aspx) Have fun.
I just looked at the C99 spec, and it says in section J.5.7 that function pointer -&gt; regular pointer is a common extension to the language. So while it is recognized as something that can be done in particular cases, it's not generally portable. I stand by what I said though, in a more general sense. If something is undefined in C, don't do it just because it works on your machine.
I'm using 'Head First C' and happy with it so far, it's fun to use this one, the K&amp;R book I found a bit too much for absolute beginners.
If you're just learning C now, honestly it won't make much of a difference. C11/C99 aren't very different from each other, C11 adds a few nice things like multithreading support and (thank god) gets rid of gets() and replaces it with gets_s(), along with a bunch of other things you probably won't need to worry about for a while.
Don't worry about it too much. The core of the language is very similar across the most recent C standards and once you get familiar with C you'll recognise certain features as belonging to certain standards and you'll develop your own intuition about which standard you prefer to use for your projects. Most established code bases you encounter will still be based on older standards than C11. Many don't even specify a standard in their build configurations so they just use the compiler default (which tends to be switch-all-features-on by default but there aren't any guarantees).
If you learn C99, you won't learn anything from not learning C11 C11 *generally* simply adds new features to the C language, which you won't lose until you're more than a novice in C. At this moment, C99 vs C11 isn't an issue, just go out and learn C A couple of good projects would be things like simple Ai, (eg, go out and implement the [simulated annealing](https://en.wikipedia.org/wiki/Simulated_annealing) algorithm), some simple text processing, and a compiler. Altogether these should teach you everything you should need to know.
If you know C99 then picking up C11 is not a problem. It is just a different version of C.
People still learn C with reading K&amp;R C 2nd edition *from* 1988. The thing is that the concept is the same which is most important after all. Anyway you will learn differences by programming or reading others code, but new standards are, like someone here said, mostly additions to language. Also, unlike C++, additions to C are with real reason, for example if you know about C level programming you can guess why they, in C11, added things like _Static_assert, alignof, _Noreturn, ... Also new standard doesn't mean that your compiler implemented it or that lot of people use it, and when you talk about C people they are rather conservative :-)
I like it. You use **goto** sparsely and in a way that clarify the code. The code seems clean. Could use a little more comments, though. I don't know how **nftw** perform the actual walk, but I guess it must be one of the quickest way to do it.
nftw is a standard library function. It's an implementation detail how it works, I assume it's implemented in the most efficient manner on each possible platform. Here is some [documentation](http://linux.die.net/man/3/nftw).
I still didn't have time to look through the code, but maybe you could use some folder structure to organize it better? Something like bin/ src/ obj/ would help, IMHO. But it sure looks interesting! I comment back when I test it :-)
Haven't looked at the code, but I like the name, simply because I think it should be pronounced f'd up.
I could argue about code style but you seem to know what you are doing.
Code wise, everything looks good. Any complaints would only be stylistic. Maybe use #pragma once instead of #ifndef FILENAME_H, or TYPENAME_new() instead of new_TYPENAME(). Also, look into using a build system like cmake or autotools. You definitely dont want to be rolling your own makefiles once your codebase passes a certain size.
I'm just wondering if C was even necessary. Seems a bit masochistic for what a script in a high-level language such as python, ruby, perl, etc. could do. I'm not trying to sound discouraging and if you want to write it in C, by all means. Just that it would be a more labour intensive task.
It's a OS tool that could be tasked with traversal of huge directory trees. It should be both fast and memory efficient. Nice code OP, BTW.
 Tests
&gt;It's a OS tool that could be tasked with traversal of huge directory trees. It should be both fast and memory efficient. &gt; Nice code OP, BTW. Thank you very much. 
I don't recall #pragma once being standardized or even supported by all compilers. I like to write code in a way that it compiles on as many platforms as possible with any compiler. The other advice is pretty sound though. Maybe I'm going to change this.
C11 is (almost*) a superset of C99. This is, C11 only adds certain additional features to the C standard and everything still works as before. Learning C11 is just a matter of learning these additional features which aren't in particular groundbreaking. Examples include: * the _Thread_local storage class specifier * the _Generic generic function selector * the _Alignof operator **Footnotes** \* If I recall correctly, two features have been removed, one being the "implicit int" and the other being the gets function which you shouldn't really ever use.
&gt; Seems a bit masochistic for what a script in a high-level language such as python, ruby, perl, etc. could do. Right, call us when you get your next hip web application to run in a browser written in Perl, running on an operating system written in Ruby with all drivers written in Python.
&gt; You use goto sparsely and in a way that clarify the code. [relevant xkcd](http://xkcd.com/292/) 
Its not standard, but every major compiler supports it, plus it avoids name clashes which can happen with include guards, and some compilers optimize it better.
It's funny how I wrote this program exactly for the reason that the Python script, the guy I am writing this for used originally, was slow as fuck and needed gigabytes of RAM to go through his ~ one million files even though it used a fancy hashtable and all the shit. My program uses 200 bytes + length of filename per file it analyses and is much faster than said Python script. That amount of space does not even have to be in the RAM at all times as I use a temporary file to store file metadata in, causing it to not eat up my memory.
That's not the point. My program should work on any platform that is POSIX 2008 compliant. POSIX 2008 specifies that there shall be a C99 compliant compiler called "c99" available. C99 does not specify #pragma once, so I'm not gonna use it. The only reason to use nonstandard things in my code should be if I want to do platform dependent things, such as in the file btrfs.c. But even there nothing happens if I am compiling under a platform different from Linux. And even there I try to be as general as possible as to allow my code to be compiled with as many compilers as possible.
Its not like I'm saying you have to use it; I was merely clarifying a point. It sounds like you already have a good grasp of what your requirements are.
Yepp. This is true. The reasoning was apparently that these features are rarely used and they slowed down compiler adoption. So gcc is C11 compliant even though they do not provide \_Thread\_local, as they define \_STDC\_NO\_THREADS_
&gt; I use a temporary file to store file metadata in, causing it to not eat up my memory. Does your OS not support virtual memory?
Extensions often involve doing things that the language spec leaves as undefined behavior or which produce ill-formed programs. Other examples are threads (up until C11), and calling C functions from another language. However there's a difference between code with undefined behavior that works by accident on a particular implementation, and an implementation which says "such and such is undefined behavior according to the spec, however this implementation guarantees the following behavior..." --- Also, C++11 adds conversion between function and data pointers as a conditionally supported feature. I'm not sure if C11 adds similar language.
My OS indeed does. Regardless of this, a system that is running my program might not have enough RAM + swap for this. Therefore I am using a temporary file. (which is latter mmap'ed so it's kinda cheating).
Okay. Done. There is a new version 2 that cleans stuff up a little bit and also implements a piece of functionality to install fdup.
I'm not an expert but that to code was fun to read. Simple and elegant.
Perhaps it was implemented using the wrong algorithms/data structure. Doesn't mean a poor choice in language. Remember, python or C, you still need to eat the cost of directory tree traversal which is going to be limited to the performance of in-kernel data structures. But fine, I will concede that you were having performance issues, but sounds like you ignored what the bottleneck was and just decided to shoot ahead and go with C. It just sounds like the python program was poorly designed, not a language choice problem.
Honestly, I'd just stick all the files in the root directory. No need to mess things up with folders, there are only a few files here.
Good points. I agree with most of them. I should really rework my main function and possibly split it into multiple small parts. I disagree about the header files though. Documenting dependencies is necessary but I hugely dislike the idea that one header file includes another one. I won't do that because it might obfuscate dependencies. What if a header pulls in another causing a certain function to be defined that should not be defined? When I change the code so that said header does no longer pull in the other one, code may (and will) break. I think about averting this with a header types.h that specifies all common types, thus avoiding dependencies to other header files. The point about "standard library" is, I wouldn't get far if I'd use only C99 standard functions. The C99 standard does, for instance, not specify any functions that allow you to get the contents of a directory. I decided to go with "stick to POSIX" as there is an almost* POSIX environment available on almost any operating system. (On Windows there should be two of them, the Unix services for Windows and Cygwin if I recall correctly). \* Almost in the sense that it's probably not 100% conforming but close enough for any reasonable purpose.
I understand where youâ€™re coming from on the header dependencies point, but FWIW I donâ€™t think your current view will survive contact with your first 1M lines/10K files project. The maintenance overheads of having your header files only working in certain include orders will probably become prohibitive long before you get that far.
Typically, when you're programming stuff like robots, Arduinos, etc., you can write a program in C or almost any other languages, and it is then compiled to work with the minimal processing and storage hardware onboard.
Awesome. Now, I've got to learn the C language!
I can't condone programming in Python if you're only just learning programming. I mean, on one hand, it's immensely practical, since it's got a huge user base, and modules for basically anything you could possibly need, as well as many high-level functions already defined. It's great for when you need a dealine. On the other hand â€“ it's got everything you need already built-in or readily available via `pip`. You don't work on any of the lower-level stuff. You're less likely to work with pointers, since you'll basically never need them. My advice: start off with Arduino. You can attach motors and servos to basically create a robot, and the basic compiler uses C (although the syntax is somewhat different than what you'd normally learn at school or use outside of Arduinos). If you want to go very far, you can attach a shield (physical addon with expansions and improvements) with a memory card slot, and work on huge programs that can do AI, AR, and all sorts of interesting stuff, since you aren't bound by the 32K limit.
Make sure you pay attention to pointers. They're something that you won't understand until you suddenly "get" them but they're essential to the language.
If you're writing something for an Atmega / Attiny system (ie: most arduino and teensy boards), Python probably isn't a good option. Those system on chip packages typically have very little memory, processing power, and storage. I could see Python working well on an Arm based system, such as raspberry pi.
C is an old programming language and THE programming language to use for hardware-near programming. C is the default, but it's possible to use a number of other languages, from Assembly to C++ to Nimrod, Java, Rust or Go. They may be portable or not, higher or lower level, native or not, garbage collected or not. They may be a lot better than C, but only C is guaranteed to be available for all platforms. 
I'm starting out in building a robot, plan is quadcopter drone. Teensy3 with C. Been playing around with different sensor and component and its really fun.
CPython definitely doesn't run at all on those systems. It's not impossible that some sort of minimal, stripped-down python interpreter exists that can run on atmegas, but it's not really a serious competitor in the space.
C is without a doubt *the* language to learn for embedded (i.e. resource limited) everything, including robotics.
You're right about the Atmel type systems. I work with Atmega's from time to time, and I doubt Python would run on them at all... You just can't do that much with an 8-bit CPU and &lt;= 8kb of memory. I could see maybe the 32-bit AVRs being able to run Python, but I doubt anyone has taken the effort to port to it. I'd be surprised if CPython couldn't run on Raspberry Pi hardware, though.
The "Arduino programming language" is pretty much just wrappers of C functions in C++. As an example... say you created a variable called myPin and assigned it to digital pin 2. In "Arduino programming language" to set it high you would call digitalWrite(myPin, HIGH); which is actually a function call in C++ that does this PORTD |= (1&lt;&lt;PORTD2); Arduino pin 2 is assigned to PORTD bit 2 on the ATMEGA328p (the Arduino Uno micro-controller). The line of code above just sets that particular bit to 1. I would suggest reading the data sheet for the MEGA328p to find out which pins do what. Also, check out avrfreaks. They have some good information to help you get started. 
Seconded. At the risk of being branded as a shameless O'Reilly plugger, I recommend [this](http://www.amazon.ca/Understanding-Using-Pointers-Richard-Reese/dp/1449344186/ref=sr_1_1?ie=UTF8&amp;qid=1377906755&amp;sr=8-1&amp;keywords=understanding+and+using+pointers) book for OP. It was immensely helpful when I was learning C; I found that K&amp;R's description of them was complete, but lacked the tutorial aspect.
Arguably, what you work with on Arduino could be just be called a framework in C. Most of the functions you use are already defined, and ready to use. That being said, you're still given the option and capability to define your own functions, and, given C's pointer interface, the ability to work on a ridiculously low level.
[Yes sir.](http://programmers.stackexchange.com/questions/159637/what-is-the-mars-curiosity-rovers-software-built-in)
Ahh yeah. That glorious moment when you get it. Just like an epiphany.
Yeah - try it with an Arduino! They're like legos!
I have to say, I personally found OO programming / C++ templating a lot more confusing.
Just to ensure I get them: a pointer points to a memory address, and any "copies" of the pointer point to the same address, whereas a copy of a variable will be in a new address, right?
Oh hell yeah. Probably the best first language to learn for robotics.
Using C and Assembly you will have no issues with robotics. The two work like a dream together. You can create functions in C that point to your Assembly program, and that Assembly program can perform a very precise operation on your hardware. I used this in school for an IEEE micro mouse competition(maze solving robots). I've done this type of programming only on Motorola chips, the HC11 and the HCS12. I've never really worked with arduinos, not entirely sure about their ASM language. 
I need to know Assembly?
Almost certainly not. It depends upon the platform you're working with, but anything tageteted for beginning or intermediate users will have C libraries for all the hardware interfaces. Eventually you may wish to learn asm, though, for one or more of: efficiency, custom hw access, and/or advanced debugging. Don't worry about it for now. There are plenty if people who have made a career of embedded programming that have little to no asm chops. Not the best ones, if course. 
Online Programming Language Homework Help | Programming Language Homework Help | Programming Language Homework Help | JAVA | Visual Basic | Programming Language | Programming Language Homework Help | C++ | Programming Language Tutor Help | Live Programming Language Homework Help | Programming Language | FOX PRO | MATLAB | PHP | Programming Language Homework Help Services |
----------
That was a really clever retort on something you had no business commenting on. 10/10, Bet you learned that playing DotA.
-------------
If League's so casual, and DotA's so hardcore, why don't they have the viewing numbers hmm???? Also, it is hella easy to write portable C code, if you don't want to do anything. printf( "Hello World\n" ); FK THAT'S SO GOOD. GLAD I CAN RUN IT ON EVERYTHING. Oh shit, no I can't, cause I have to recompile it. Talk to me after you finish CS101.
I'm not getting involved here, just saying that the model of League is to pour money into it. It's not sustainable.
you have to use compilars though which not all oses might have installed by default
I may need to make a new topic later, but look at the edit! I'm wondering if anyone can help me or point me in the right direction to program Vex robotics in C. I can't find anything useful because everyone uses easyC or RobotC.
Javas only purpose is to act as a GUI. EElse Java is just horrible. Yes java is protable because it always runs in the virtual machine which also makes it realy slow comparet to c. But if you are a good c programmer your code works on every os too. Yeah you have to compile you code for every os with a different compiler but that is nothing compared to the advanteg you get. C is much faster. And when you work with microcontroler you want to use the power you have in the most efficient way possible. 
On 64-bit operating system size of int is 64 bits or 8 bytes. So you are trying to allocate 8 GB of space which fails, because you don't have enough memory. If the operating system is 32 bits, you are hitting the address space limit of 4 GB.
Each compiler is free to choose appropriate sizes for its own hardware, subject only to the restriction that `shorts` and `ints` are at least 16 bits, `longs` are at least 32 bits, and `short` is no longer than `int`, which is no longer than `long`. OP: Can you compile and run this: #include &lt;stdio.h&gt; #include &lt;limits.h&gt; /*values of char, short, int and long variables, singned and unsigned */ int main(void) { printf("Ranges of char %d %d\n", CHAR_MIN, CHAR_MAX); printf("Ranges of short %d %d\n", SHRT_MIN, SHRT_MAX); printf("Ranges of int %d %d\n", INT_MIN, INT_MAX); printf("Ranges of long %d %d\n", LONG_MIN, LONG_MAX); printf("Range of unsigned char %u\n", UCHAR_MAX); printf("Range of unsigned short %u\n", USHRT_MAX); printf("Range of unsigned int %u\n", UINT_MAX); printf("Range of unsigend long %u\n", ULONG_MAX); return 0; } Overall, I don't think you should be allocating 8GB of heap space. It's bad practice. 
The question begs, what do you need an array that big for? Any sort of activity on something large, aside from the malloc failing, would take an enormous amount of time. 
Yeah i can run it. And yeah i know i would never make a programm which allocates 8 GB of memory...And i encountered the problem. I changed my code a littlebit and now i can allocate as mujch as i want but i was curious about malloc in generall.
That could be right. Im on a 64 bits os. But i cant even allocate the half of it so 500.000.000. And i defenently have more then 5 GB Free Space. So that is not the matter. I think its more of another kind of problem.
If you're coding anything of value in C, 95% of the time you WILL NOT be able to compile it on multiple platforms. That's the point I'm trying to make. Sure, you can recompile data structure libraries, and maybe some math libraries, but outside of that, you're going to have to rewrite anything that touches the OS.
Easiest is to just switch to a 64-bit OS and compiler. If you have 8GB RAM, it makes no sense to run a 32-bit OS. And if you don't need compatibility with 32-bit machines, you can just generate 64-bit code and enjoy the benefits. If that's not an option, you'll have to use a cleverer data structure. If that array will be mostly empty (seems pretty likely), you can switch to sparse arrays, e.g. using the Judy array library). An alternative option, if you have fairly predictable access patterns, is to create a disk file to back your array and map the appropriate chunks in memory when needed. 
No, the problem is the lack of address space. Win32 apps, by default, have a 3GB address space. Eventually, your process will be paged out to the disk, or fail to allocate. Remember, you're addressing virtual memory, not physical. 
You're probably not using a 64 bit compiler.
On most 64 bit operating systems sizeof(int) is 4. Edit: Also, he could use int32_t that is 32 bits guaranteed. 
&gt;Choose dice with x sides &gt;roll the dice until number y shows up &gt;write the ammount of rolls you needed in an array &gt;repeat 1 billion times &gt;lets see ho the probability is to get number y at a x sided dice Can you guess where the 1 Billion comes from? :D And yeah i know thats something you dont do. There are much smarter ways. But i just wantet to write the programm in 2 minutes with no brain...and this is definetly the easiest way :D
That is interisting to know.I didnt know this. How is it in linux? Should i be able to run this unter ubuntu if i use kate and gcc?
Yeah, there is a 32-bit version, but the keys work for both and you get discs for x86 and x64 in the box.
That's a weird-ass way to calculate probability. Just store an int array[x] initialized to 0, and with each roll, increment the appropriate cell.
Its a brute force methode. Nobody said ist a good one.
Assuming this is some kind of school assignment, I would seriously suggest rethinking your strategy. I mean, sure, this *could* work, but you're dedicating way too much time to doing a very nonsensical and incredibly wasteful approach to a problem that could be solved with a for loop and a (small) array. 
Well, given that it's a fair dice one could just do the math. It's a Laplace experiment and therefore P(X == y) = 1/x, y âˆˆ [1..x]. The exercise itself is pretty pointless from a mathematical point of view.
That's also true... I assumed it isn't a fair die, because otherwise god help us all.
Does [this](http://arduino.cc/en/Tutorial/Dimmer) link help? Not sure what specific issues you're having.
You can also use \`backticks\` (grave accent, U+60) around code to escape it and make it `monospace`.
FFR, if you can use `mmap` or something along those lines, that's less portable but generally better for large allocations. (If you're going to be pounding away on the array, `mmap` will also let you do big-page mapping, which can save the CPU lots of TLB work.)
Thanks a lot for responding but I've already resolved the issue. Cheers
Partial agreement here. The code examples are poo on a stick but the discussion chapters are top notch. The well-thumbed copy in our lab is not used for the programs within it. The advantage of the code being rubbish is that it doesn't matter what the language your copy of NR claims to be in, you can just ignore the code unless you're writing FORTRAN.
This explanation is what I understood and agree with.
"Analyze the problem" does not mean stare at the source for a while and pronounce it fine, it means actually analyze what's going on. Use a debugger. Use a profiler. Use instrumentation. Look, here is what a compiler does when you try to turn a simple `if` statement into a switch. Testcase: int puts(const char *s); void example(int a, int b) { switch((a &gt; b) - (a &lt; b)) { case 1: puts("a is greater than b"); break; case 0: puts("a is equal to b"); break; case -1: puts("a is less than b"); break; } } Here's what gcc 4.8.0 -O3 on x86_64 linux produces: example: xor edx, edx cmp edi, esi setl al setg dl movzx eax, al sub edx, eax je .L3 cmp edx, 1 jne .L5 mov edi, OFFSET FLAT:.LC0 jmp puts .L5: mov edi, OFFSET FLAT:.LC2 jmp puts .L3: mov edi, OFFSET FLAT:.LC1 jmp puts Here's what clang 3.4 -O3 on the same platform produces: example: cmp EDI, ESI setl AL movzx ECX, AL setg AL movzx EAX, AL sub EAX, ECX je .LBB0_4 cmp EAX, -1 jne .LBB0_2 mov EDI, .L.str2 jmp puts .LBB0_4: mov EDI, .L.str1 jmp puts .LBB0_2: cmp EAX, 1 jne .LBB0_6 mov EDI, .L.str jmp puts .LBB0_6: ret There are no jump tables here. This is implemented exactly as a series of "if" tests, with `cmp` followed by `je` and `jne`. Except there's actually more overhead than a real `if` statement, because it has to actually do two subtractions, first to compare `a` to `b` and then to generate the -1/0/1 value. Compare that to a plain `if` statement: int puts(const char *s); void example2(int a, int b) { if(a &gt; b) { puts("a is greater than b"); } else if(a &lt; b) { puts("a is less than b"); } else { puts("a is equal to b"); } } gcc -O3: example2: cmp edi, esi jg .L5 jl .L6 mov edi, OFFSET FLAT:.LC2 jmp puts .L6: mov edi, OFFSET FLAT:.LC1 jmp puts .L5: mov edi, OFFSET FLAT:.LC0 jmp puts clang -O3: example2: cmp EDI, ESI jle .LBB0_1 mov EDI, .L.str jmp puts .LBB0_1: jge .LBB0_2 mov EDI, .L.str1 jmp puts .LBB0_2: mov EDI, .L.str2 jmp puts Again, simple and straightforward, and better than the switch version. So whatever the problem is, it is most certainly not going to be fixed by throwing a switch statement at it.
&gt; Alternate HTML content should be placed here. This content requires the Adobe Flash Player. Thanks but no thanks.
Care to share some links to such critique? Amazon review seems to indicate that the algorithms are useful (and from a quick skims of the table of content it seems that I could have found some solution quickly with it), but the code sucks both by itself and for its licensing.
You have a comment indicating that some vars need to be global because the nftw function definition does not provide any other params to pass them through. In my experience I will create a struct "args" that contains the arguments you need, plus one of the arguments the function definition defines - you can cast the struct to be the type specified in the function definition and unpack the arguments within the function. I do this with libpcap's callback function definition. libpcap declares the callback as: void handle_packet(u_char *user, const struct pcap_pkthdr *hdr, const u_char *packet) user is given to allow you to pass some args in. I create a struct struct args { int arg1; int arg2; }; Then I just pass the struct as the user argument cast to a u_char*, and from within my callback function definition, I unpack the arguments from the struct as needed. I couldnt quickly find your the definition - so I am not sure if this advice is relevant, or if the globals are for a scoping issue of some sort ... just thought i'd pass it along. 
I wonder how putting the book online relates to their ridiculous licensing terms (second page after the title page). Since you don't own the book, you may not have a license to any of the code in it. Though since there's no way to copy/paste from the document, you might covered. Gah, I really, really hate the licensing on this book.
Unlike C++11, C11 doesn't feel a complete [rewrite](http://en.wikipedia.org/wiki/C%2B%2B11). The only thing that I would see useful is better Unicode support and Multithreading. As everyone has said, you're not missing much.
I find it easier to think: a pointer *is* an address -- that address maps to some other part of memory you wish to read. You can print out a pointer printf("%p", xxx) -- you will see a memory address. *Deep C Secrets* is a good book if you want to understand this precisely (and it's very helpful to do so!), as well as many other interesting things about C. 
Yeah, that was probably a better way of describing it. It's been a while since I've used any sort of C subset, and pointers aren't common in scripting languages.
Why is temp defined as a pointer and not simply a char?
Oh, thanks. I knew it was something simple that I wasn't seeing.
Ah the wonders of undefined behaviour; the compiler is allowed to do literally anything. For instance the compiler could attempt to start a game, and indeed some compilers actually did this. The related implementation defined behaviour at least makes the compiler pick a behaviour, document it, and stick to it.
As a side note, don't write `strlen(str)+str-1`. Use `strchr()` or `memchr()` for that, i.e. `endptr = strchr(str, 0) - 1`. 
I suggest you make an array of the possible directions the word could go. Something like: #define NUM_DIRECTIONS 8 int directions[NUM_DIRECTIONS][2] = { {-1, 0 }, // left { 1, 0 }, // right { 0, -1 }, // up { 0, 1 }, // down {-1, -1 }, // left-up {-1, 1 }, // left-down { 1, -1 }, // right-up { 1, 1 } // right-down }; Once you find a starting letter match search in each of the possible directions until either you run off the grid or there isn't a match or you've matched the word. If you need more of a hint let me know.
I think I need more of a prod. I have just spent some time looking up #define but I'm not 100% sure what it is. Do I put it before my int main(){} and it will stay a constant whatever I do to it? I have been trying to do something like this: for (row=0; row&lt;26; row++) { for (column=0; column&lt;26; column++) { if (word[0]==wordsearcharray[column][row]) { for(i=1; i&lt;wordlength; i++) { if (word[i]==wordsearcharray[row][column++]) { printf("Word found at (%d,%d) to the right\n", row, column); return 1; } } } } } Is that a similar effect to using an array of directions? Im not sure how I would go about using that array
Are you referring to this? void prefix(char c1,char c2) { roman_Number[i++] = c1; roman_Number[i++] = c2; } void suffix(char c,int n) { int j; for(j=0;j&lt;n;j++) roman_Number[i++] = c; } These are function definitions. The first function is called `prefix` and takes two arguments of type `char` and returns nothing (that's what `void` means). The second function is called `suffix` and takes two arguments, one a `char` and the other an `int`, and also returns nothing. The `prefix` function just places the value of `c1` at position `i` in `roman_Number`, places the values of `c2` at the next position, then leaves `i` denoting the position after that. The `suffix` function places the character `c` into `roman_Number` at `n` consecutive positions starting from position `i`, leaving `i` denoting the position after those characters.
oh, Thank You! You've just save me from tremendous problems. It was really clear now.
Hmmmm. Multiple global variable declarations and a function definition on the same line, plus use of conio.h. At least it didn't look like: char roman_Number[10000];int i=0;void prefix(c1,c2) const char c1; const char c2; { roman_Number[i++] = c1;roman_Number[i++] = c2; } void suffix(c,n) char c; int n; { int j; for(j=0;j&lt;n;j++) roman_Number[i++] = c; }
I used the gettimeofday() function like you recomended but it is outputting strange values. The call to primes_func() should have lasted only a fraction of a second. the output: -1608513920.000 seconds the relevant code: #include &lt;sys/time.h&gt; int main(void) { float start; float stop; struct timeval tim; float runtime; gettimeofday(&amp;tim, NULL); start = tim.tv_sec + (tim.tv_usec / 1000000.0); primes_func(); gettimeofday(&amp;tim, NULL); stop = tim.tv_sec + (tim.tv_usec / 1000000.0); runtime = (stop - start); printf( "%.3f%s\n", runtime, " seconds"); } Any idea why it isn't working?
Compile with the ``-pg`` flag and run under [gprof](http://www.cs.utah.edu/dept/old/texinfo/as/gprof_toc.html). You can then get timing information at function resolution. If the section of the program you want to time isn't a function, make it one. **Edit:** Be sure to note [the section](http://www.cs.utah.edu/dept/old/texinfo/as/gprof.html#SEC12) in the manual on statistical inaccuracy. As with all benchmarking, one gets more accurate results running the to-be-tested function repeatedly in a loop.
Urk.
gprof can help you find slow sections of code but since it introduces quite a lot of overhead it's pretty much unusable to get actual timing information in absolute numbers.
You can't use `float` here, you need to use `double`. In general, always use `double` and avoid `float`. The reason in this specific case is that a `float` is not capable of representing a unix epoch timestamp with sufficient precision. I'm not sure how or why you're getting large negative numbers; I get simply 0.0, which is expected. Ignore the other person who brought up the signedness of floats, that's completely irrelevant. The problem is the limited mantissa. 
I disagree, but YMMV and of course it depends on what you're benchmarking.
I found a stack overflow on how to do this a whole ago, you need to include time.h and use clock() and a diff. I can't remember exactly 
1) Hook logic analyzer to output pin. 2) Set logic analyzer trigger on rising edge. 3) In code, set output pin HIGH 4) In code, run Algorithm 5) In code, set output pin LOW 6) Read width of pulse on logic analyzer in sub-microsecond accuracy 
I get exactly the same problem even with double instead of float
Is that really the whole program you're compiling? No `#include &lt;stdio.h&gt;` et cetera? What compiler and options are you using? Do you get any warnings? I compiled your code with `gcc -Wall -Wextra -pedantic -O2 -std=c99` on x86_64 Linux and it worked after switching `float` to `double`. (And writing a dummy function that sleeps for a few seconds.) 
This sounds useful, could you give more specifics on how to do it? I don't know what a 'logic analyzer' is or how to set the output pin, etc.
Try this testcase: #include &lt;stdio.h&gt; #include &lt;unistd.h&gt; #include &lt;sys/time.h&gt; int main(void) { struct timeval start, stop; double dstart, dstop; gettimeofday(&amp;start, NULL); sleep(3); gettimeofday(&amp;stop, NULL); dstart = (start.tv_usec / 1e6) + start.tv_sec; dstop = (stop.tv_usec / 1e6) + stop.tv_sec; printf("Start: %.3f\nEnd: %.3f\nElapsed: %.3f\n", dstart, dstop, dstop - dstart); } When I run this, I get: $ ./a.out Start: 1378828514.084 End: 1378828517.084 Elapsed: 3.001 You should get something vaguely similar. Turning on all warnings and strictness is generally a good idea and you should get in the habit of doing it. 
I got it working, thanks for your help and advice.
This is what I use: #include &lt;stdio.h&gt; #include &lt;time.h&gt; int main(int argc, char *argv[]) { clock_t begin, end; double time_spent; /* clock start */ begin = clock(); /* code goes here */ end = clock(); /* clock end */ time_spent = (double)(end - begin) / CLOCKS_PER_SEC; printf("ELAPSED TIME: %f\n", time_spent); return 0; } 
In general, do not use floating-point for timing things. Floating-point gets taught early and overused from then on. E.g., you can do unsigned long start, startUS, stop, stopUS; ... gettimeofday(&amp;tim, NULL); start = tim.tv_sec; startUS = tim.tv_usec; primes_func(); stop = tim.tv_sec; stopUS = tim.tv_usec; if(startUS &gt; stopUS) {--stop; stopUS += 1000000UL;} stopUS -= startUS; stop -= start; printf("%lu.%03u\n", stop, (unsigned)(stopUS / 1000)); and that'll be safer and more precise than what you'd do with a float or double (especially since you're not guaranteed much in the way of precision). Also, for future reference, if you're on x86 and timing things in GCC on a halfway recent CPU, you can get cycle counts from the timestamp counter via union { uint64_t tsc; struct {uint32_t lo, hi;} dwords; } v; __asm__ __volatile__("rdtsc" : "=a"(v.dwords.lo), "=d"(v.dwords.hi)); // Cycle count is now in v.tsc This generally shouldn't be used in production code because there's any number of things that can go wrong, but it works well for personal development.
If you don't know the terms, then you should pass on this solution. Look up "Logic Analyzer" on google. This type of solution is best for embedded computers and singled-board computers instead of PCs and MACs, still you can do it with parallel-port PCI cards. 
`clock()` measures CPU time, not wall clock time. That may be the source of the inaccuracy you're observing. C does not have any standard timing functions with better than one second resolution. POSIX provides `gettimeofday()` which provides up to microsecond resolution. If you're okay with C++ (or you're willing to write a C wrapper library around some C++ code) you can use the `&lt;chrono&gt;` library which provides `std::steady_clock` and `std::high_resolution_clock`, which should provide the highest possible resolution supported by the hardware, which is often nanosecond resolution.
When I try to compile this with -Wall, it throws the following: world.c:23:5: warning: format â€˜%fâ€™ expects argument of type â€˜float *â€™, but argument 2 has type â€˜double *â€™ [-Wformat] world.c:27:5: warning: format â€˜%dâ€™ expects argument of type â€˜intâ€™, but argument 5 has type â€˜doubleâ€™ [-Wformat] world.c:38:1: warning: implicit declaration of function â€˜systemâ€™ [-Wimplicit-function-declaration] world.c:39:1: warning: control reaches end of non-void function [-Wreturn-type] I'm pretty sure the third one is just from me using linux, you can probably ignore that, and the fourth is pretty immaterial as well. Try fixing the first two and see how that goes.
That got the Growth rate to display correctly, but what about the fact that `pop` is supposed to hold `7177838000`, not `2882870704`.
7,177,838,000 is too big to fit into a 32 bit int, unsigned or not. 32 bits can hold 4,294,967,296 different numbers, so unsigned would be 0-4,294,967,295 and signed is -2,147,483,648 to 2,147,483,647
I thought so, and when I tried to do `unsigned long long int` instead of `unsigned long int`, Visual Studio gives me errors, see the main post for the exact error.
That fixes it. So simple, I almost face-palmed. Thank you very much.
&gt;world.c:39:1: warning: control reaches end of non-void function [-Wreturn-type] &gt;I'm pretty sure the third one is just from me using linux, you can probably ignore that, and the fourth is pretty immaterial as well. OP isn't returning a value at the end of main. Both the compiler and the OS are expecting an int to be passed back. 
on a side note, you don't need the %s: `printf("Would you like to enter your own values? (1 = yes/0 = no): ");` 
It can be a good habit because then you don't take the chance of forgetting to escape percent signs that you put into your string. Better is: `puts("my string")` if you don't mind having the terminating newline, or `fputs("my string", STDOUT)` if you do.
The return value of the last function called will be returned.
 // File stopwatch.hpp #ifndef STOPWATCH_HPP #define STOPWATCH_HPP #include &lt;iostream&gt; #include &lt;ctime&gt; class StopWatch { public : StopWatch (); void reset (); void start (); void stop (); double value () const; private : clock_t val; bool running; }; std::ostream&amp; operator&lt;&lt; (std::ostream&amp;, StopWatch const&amp;); #endif // stopwatch.cpp -- implentation of stopwatch.hpp #include "stopwatch.hpp" StopWatch::StopWatch() : running(false), val(0) { } double StopWatch::value() const { return double(running ? clock()-val : val) / CLOCKS_PER_SEC; } void StopWatch::reset() { val = running ? clock() : 0; } void StopWatch::stop() { if(running) { val=clock()-val; running=false; } } void StopWatch::start() { if(!running) { val=clock() - val; running=true; } } std::ostream&amp; operator&lt;&lt; (std::ostream&amp; os, StopWatch const&amp; sw) { std::streamsize p(os.precision(2)); // centisecond accuracy std::ios::fmtflags f(os.setf(std::ios::fixed, std::ios::floatfield)); os &lt;&lt; sw.value(); os.flags(f); os.precision(p); return os; } 
Just want to note that there are several layers (last I looked into this detail was Arduino 0019) which slow down digitalWrite() and digitalRead() significantly compared to manipulating the port register directly. There is a library out there somewhere which provides digitalFastWrite() and digitalFastRead() as macros that basically distill down to direct port manipulation. Arduino is a great platform to learn on. Can't recommend picking one up enough. 
I agree.
I'm at pretty much the same stage as you are, the best example i've found so far is this guys... http://cboard.cprogramming.com/c-programming/32230-wordsearch-solver-need-help.html but i dunno if it's really necessary to have a completely separate piece of code for searching in each direction like he's done. are you at bristol uni by any chance? 
Ha yeah I am, heading back up soon
Thank you for your help, I have been busy doing a small temping job so I haven't had time to code, but I have been thinking about it, and I think I just made a breakthrough in understanding what you are trying to say. Of course that might not translate into my actual code but we'll see. Also thanks for the coding conventions so to speak, any more would be much appreciated to make my code more readable and less shite
What's wrong with VS2012? It's a really great IDE.
While I'm all for helping others out, you're basically asking us to do your homework for you. If you really don't get it, your teacher is there to help you - she/he is PAID to help you learn. You should probably start there. 
I think you didn't understand... I said "All I need" so it wasn't the one thing I have to do... I made too many things myself but I stuck here... A teacher won't give you a program, he gives you a pseudocode and this is eligible for my teacher too... It was the only one thing I had at the first, yeah. Anyway, anyone help?
You are almost there. I found a couple issues. * You are assigning the temp variables to the outer loop values. You need to swap the assignments around. * Your 'i' loop should start at 1 because you've already checked the zero element. * Your check for the end of the word needs to be one character earlier because you are including the terminating null (zero) in the search. [Here is a corrected version with a couple small enhancements](http://pastebin.com/5H1V5L9f)
Thankyou so much again! In the short time it took you to reply I worked out your last two bullet points, but it still wasn't working. I see I had my directions all wrong, just didn't get my head round it properly but I see that now. Not sure if that's what you meant by temp variables being assigned to the outer loops. I have never seen struct before so I'll see if I can get my head round it, otherwise might have to keep it as slightly less swish and readable code. I had a small thought for the if (temp_row &gt; SIDE_LENGTH) bit, I think it should be temp_row &gt;= SIDE_LENGTH or SIDE_LENGTH-1, as the array only goes from 0-25 but side length is 26, would I be right in that? Thanks again though, I'll try and get my file reader working so everything looks nice. Cheers!
Here's what I meant by my first bullet: for (row=0; row&lt;SIDELENGTH; row++) { for (column=0; column&lt;SIDELENGTH; column++) { if (word[0]==wordsearcharray[row][column]) { for (direction=0; direction&lt;NUM_DIRECTIONS; direction++) { row=temp_row; column=temp_column;//This keeps row and column the same as where the first letter was found so the co-ords can be printed out You'll notice in the last two lines you were overwriting your loop variables with uninitiallized temporary variable values.
On the contrary, it is a *bad* one. `malloc`'s refusal to give you a one-billion integer array should have been a hint that you should try other solutions.
Going back and rereading the original mailing list post, while it was spelled out it seemed to me that it was the C++ aspect of things that was the real problem. 
Looks pretty straightforward to me. The instructions already give you the pseudocode you need, just like /u/nandryshak pointed out. 
Just crazy. Yes, C is hard--and one of my favorite languages--but at the same time, it just doesn't make sense given the opportunity cost (given the alternatives) and the lack of ecosystem around web. Even then, there are things like HipHop that probably negate any value C would have in terms of performance (while still yielding the productivity of a higher-level language.) I don't know, maybe if you're a masochist you'd consider this. 
My problem with C as a language for web programming at this date and time is simply that Unicode isn't for free or part of the language. That and string handling. I'm not doing discrete math or something that's fun in C, I'm doing the part that's an absolute pain, and that's **all I will be doing**. Also http://cherokee-project.com/ is something worth mentioning.
K&amp;R states that it is a book for people with experience, not beginners.
well at least try to write something. What you are saying is , I done all the inputs. I have no idea how to do the main point of the assignment( in this case the dynamic arrays). If you don't understand this life will get very hard very quickly in that class
Why do you need Unicode at all when there is the better alternative like UTF-8 ? Well, I do not process text as part of my job. But for i18n, UTF-8 is great...
UTF-8 _is_ unicode. UTF stands for Unicode Transformation Format.
He meant that the standard library's support for Unicode is lackluster, which is true. (By the way, Unicode does NOT simply mean 16-bit characters. In fact, 16 bits isn't enough to store all Unicode code points.)
um, [libicu](http://site.icu-project.org/)?
I have in the past (while learning C) written C processes to do the heavy lifting of a php application where we simply needed to squeeze every ounce of performance from our servers. Not so much web development in C, but being able to offload some of the more demanding tasks from PHP to C processes such as stats compiling for milti-billion row databases, which would be output to another database table for the PHP web interface to display.
This is interesting. There's also G-WAN, which is pretty awesome. Also, the author is pretty brilliant too.
C doesn't know what unicode is. Or the dozens of other character sets and encodings. C knows roughly what ASCII is, and what bytes are. It'll know how many bits to a byte, and what endianess they are for a given environment, and that's about it. C compilers will let you use string literals in certain places because they are nice and written and used by other humans who don't want to deal with having to represent them in another more painful fashion. We have libraries like ICU to handle string encoding, but ultimately "strings" are nothing more than char[], one way or another.
Embedded systems.
C Programming: A Modern Approach 2rd edition is strongly recommended 
It's for a high school robotics club. And, yes, it is for the First Robotics Competition. Apparently, you can only use the languages they say you can. So, basically it's Easy C or RobotC. We've decided on RobotC. We are using Tetrix robots with NXT controllers.
&gt; C knows roughly what ASCII is Nah, it doesn't even know that. C's standard library is completely independent of the character set of the machine.
Are we talking about the same competition? Anyway, I'm assuming they would probably be okay with it if we used C or C++. But, I think for this year we are sticking to RobotC.
Cool. Yeah, for this year I'm going to try and learn RobotC. I'll probably be following some youtube tutorials or something.
Nice wheel you have there. :) Have some, hopefully, constructive feedback. I generally tell people that headers are for the interface, and implementation details don't belong there. There's some implications to this. Structs should be defined in the implementation file, with them declared in the header file. Because you're defining your structs in the implementation file, you can't put macros in the header that touch the internals of your data structures. But why do you need a macro there anyway? Any decent compiler will optimise a short function to inline anyway, especially if it consists of a one line return. Also, _t is reserved by posix. I know it's pretty commonly used out there, but I avoid it, for that reason. The other issue is that any other library that provides an "array_t" will conflict with your code. What will happen if POSIX decides to add an array type to the POSIX standard? So, I tend to pick a prefix for my types, in your case maybe lc_? You've obviously spent a lot of time on the library, and it looks like it has a lot of the kinds of features I wanted when I was writing my own collections library 6-7 years ago. I think most C programmers end up writing things like this, and I think it's a great exercise doing it because it gives you an appreciation of exactly how hard it is to write something like this well. But these days I tend to reach for something established, like libapr, simply because it's had more time to mature than my wheel. Keep at it though, sometimes using a wheel you understand really well is orders of magnitude better than using someone else's wheel that you don't understand.
xcode is a fine IDE. What else are you using the space for? it's only a couple of gig for everything.
This was true for older C versions. C11's specification makes direct reference to Unicode in a few places, and they have tried (not particularly hard, mind you) to ease a few specifically-Unicode things into a few corners of the syntax (\u and \U, u8 suffix). The C11 standard library stuff also includes `uchar.h`, which handles some woefully basic conversion stuff. I wouldn't call it well-supported by any stretch of the imagination, but at least they're not still pretending ASCII has all that much competition worth worrying about. (DAE EBCDIC guyz)
That's fine and all, but it's hard enough getting people and toolchains to use C99 as it stands. I imagine unless there is a push C11 won't get anything resembling "adoption" for a very long time.
Thanks for the info on the _t. I did not know that it was reserved by POSIX standard. I emulated what I saw a lot of APIs do and I suppose they are doing it wrong too.
Visual Studio, until recently, did not support C99. This is why I have #define's for __restrict and __inline as well. Now, that Microsoft finally implemented C99 in there compiler, I will refactor the code to get rid of some of these preprocessor defines. What specific error are you getting on Mac OS X? I am able to compile fine on Mac OS X 10.8.5, and Fedora Core 18.
A related project is http://ccodearchive.net/
Looks like you are missing some macros. What version of automake, autoconf, and autoreconf do you have? 
I'll get back to you with that info. Seems possible on Mac. Seems unless on linux though.
Oh certainly. The big compilers drag behind the standards pretty far (especially Microsoft's Visual Shitheap, which hasn't even made it all the way to C89 yet, bless its heart). But the language does at least offer grudging support for Unicode, so in 10-15 years we'll see a complete C11 implementation in Clang, and GCC will trickle for another 10 years after that until it reaches a magical "mostly supported" stopping point, and *then* we'll see who has to import libraries to use Unicode in C. *(cackle)*
Kore author here. Interesting article and a good read. Although saying is Kore seems like a replacement for PHP is not quite correct. Kore does not force templating onto a developer, but rather provides a simple set of API functions the developer can use. How they use it is up to them completely. Also worth noting Kore definitely supports forms and multipart forms. Websocket support is planned to be put in soon. :-) Edit: phone grammar
What if you used the opcodes as an index into an array of function pointers?
&gt; What do you think would give the best performance? The usual answer when discussing optimization: don't ask us, ask a benchmark. It may be tedious to implement all alternatives you can think of just to find out that some (or all) are slower or give no improvement, but if it's a performance critical part of your program (you did ascertain that with a profiler, did you?), it should be worth it.
I appreciate all of the answers so far, but am particularly interested in this approach. I was not aware that GCC could hold a pointer to a label - very interesting indeed.
A compiler typically implements a larger switch statement as a jump table. This is almost as good as it can get, because there is basically only one memory-lookup involved which is much faster than traversing a cascade of if-statements. One trick to optimize a switch statement is to look for recurring patterns that may make the size of the jump table smaller; remember: smaller jump table -&gt; more space left in your L1 cache. For instance, especially with opcodes it often occurs that certain bits of the opcode are almost always operand bits and thus not important for parsing the opcode. You could, for instance right shift the opcode byte to remove these bits and then switch on the remainder. The jump table will become much smaller, for the few opcodes that interpretate these bits differently, you have to do a switch again of course. Another optimization is also possible. Recently, I had code like this: /* apply kilo, mega, giga etc. suffix */ static off_t adjust_suffix(off_t n, char suffix) { switch (suffix) { case 'K': return n &lt;&lt; 10; case 'M': return n &lt;&lt; 20; case 'G': return n &lt;&lt; 30; case 'T': return n &lt;&lt; 40; case 'P': return n &lt;&lt; 50; case 'E': return n &lt;&lt; 60; default: return n; } } Using some simple logic, it was possible to turn it into this: /* apply kilo, mega, giga etc. suffix */ static off_t adjust_suffix(off_t n, char suffix) { static const char shifttab[] = { ['K' - 'E'] = 10, ['M' - 'E'] = 20, ['G' - 'E'] = 30, ['T' - 'E'] = 40, ['P' - 'E'] = 50, ['E' - 'E'] = 60, }; if (suffix &lt; 'E' || suffix &gt; 'T') return n; return n &lt;&lt; shifttab[suffix - 'E']; } This change reduced the size of the jump table dramatically and (as a plus) the only jump in the code is the check for the suffix range now. Beware that this change introduces a portability issue; C11 does not guarantee a certain character encoding (it is only guaranteed that a certain set of characters exist, that 0 to 9 are encoded in adjacent, increasing order and that \0 maps to 0 iirc) so this code might yield unexpected results on platform where A to Z are not adjacent and in increasing order. (I don't know such a platform).
I wouldn't try to hand optimize the switch statement. I would also consider replacing the entire construct with an array of function pointers.
If speed is what you want, you cant beat threaded code (unless you jit that is) https://en.wikipedia.org/wiki/Threaded_code
This is an extension of gcc; be careful as it is not portable. Better use pointers to (static) functions and hope that the compiler will do tail-call elimination (better compilers usually do). The result will be the same but you wont sacrifice portability.
That's essentially what a switch-statement is translated into. No need to do it manually.
&gt; This is an extension of gcc; be careful as it is not portable. It also sounds dangerous even *with* gcc... I mean, for example, the docs say: &gt; You may not use this mechanism to jump to code in a different function. If you do that, **totally unpredictable things happen**. The best way to avoid this is to store the label address only in automatic variables and never pass it as an argument. 
I asked [a question](http://stackoverflow.com/questions/18570427/how-to-optimize-the-size-of-jump-tables) about the optimization problem of arranging enumeration values for optimal switch table density before. Turns out it's an NP-complete problem. From what I know, the density has to be 40% at least on clang to emit jump tables, which is typically met in case of such opcode jump tables. I don't know however, whether compilers will emit the O(n) or O(log n) implementation for non-jump table switches. In my code I was sometimes able to isolate cases that cause a sparse jump table to optimize the code.
That's exactly how a compiler implements a switch statement, given that it is dense enough. Replacing the statement with an array of function pointer would at best make no difference and more likely (because of function-call overhead) slow down the code.
Normal goto labels have function-local scope anyway, so there is no real difference. The only reason you can even do what is stated is because it can be quite difficult (impossible?) to determine at compile-time if a variable contains a local or remote label address, so it lets it happen.
Don't forget to bench mark. These days cache effects can dominate of tweaks like that.... ie. Sometimes it is not how smart your tweak is... it is whether everything still fits in L1 or L2 or Disk Buffer or Disk. The cache hierarchy is painfully steep. So inspect, measure, tweak, measure. ps: kcachegrind is a lovely tool.
To add to what FUZxxl wrote, if this is a relatively small piece of code and the gcc extension is particularly effective, you might consider including it conditionally, and providing a portable alternative in case of another compiler.
Hello, took a look through your code and offered some advice in the form of a pull request on github for you. Here is a quick summary: void functions can have return statements without a value after. In void turn(char input) you have return NULL;, the NULL is not needed. Variable initialization for direction[0] and [1]. Initialize these values to a sane value (I will recommend 0). You use a float iter in your sqrt function. Though from a previous comment you will be moving to the math.h function, for future reference you should use an int and a for loop, tends to be cleaner looking than a while loop. I would also recommend checking out switch statements instead of if/else if statements. Most of the ones i saw while quickly going through would work very nicely with a switch statement.
I'd recommend comments. I see that you've put some in already, I guess they're pointing to where you found specific solutions. But what about the rest of the code? In three weeks time, will you be able to look at it and know exactly what's going on? Anecdote time: at work we have a system called the 'bus safety rating'. Code that one person has written, with little or no documentation has a bus safety rating of 1. This means that, should that person get hit by a bus, the rest of the team would have to decipher the code as it stands. With comments, the code would have a buys safety rating of 3 (because there are three of us in the software dev team).
Can you offer some _why_'s? I'm especially confused about your assertion regarding system(). But even rand(), sure maybe it's not good for... cryptography? But this is a snake game...
Your init_snake function looks quite a lot redundant. How about something like this: void init_snake() { int x, y, i; snake[0][0] = rand_lim(10) + 5; snake[0][1] = rand_lim(10) + 5; if (snake[0][0] &gt;= 10) { if (snake[0][1] &gt;= 10) { x = -1; y = 0; snake_direction = 2; } else { x = 0; y = 1; snake_direction = 1; } } else { if (snake[0][1] &gt;= 10) { x = 0; y = -1; snake_direction = 3; } else { x = 1; y = 0; snake_direction = 4; } } for (i = 1; i &lt; 5; i++) { snake[i][0] = snake[0][0] + x*i; snake[i][1] = snake[0][1] + y*i; } } Also, if you have a bigger program, declare functions as static that are only used in one translation unit. This allows the compiler to throw out more code and usually to optimize your code more aggressivly.
And, for the sake of safety, always watch the warnings your compiler throws at you. This is what gcc says on your code after I manually enabled -Wextra: cc -c -Wall -Wextra -O3 src/maggot.c In file included from src/maggot.c:21:0: src/maggot.h:61:8: warning: conflicting types for built-in function 'sqrt' [enabled by default] src/maggot.c: In function 'turn': src/maggot.c:299:3: warning: 'return' with a value, in function returning void [enabled by default] src/maggot.c:335:3: warning: 'return' with a value, in function returning void [enabled by default] src/maggot.c: In function 'end': src/maggot.c:430:8: warning: ignoring return value of 'system', declared with attribute warn_unused_result [-Wunused-result] src/maggot.c: In function 'info': src/maggot.c:408:8: warning: ignoring return value of 'system', declared with attribute warn_unused_result [-Wunused-result] src/maggot.c: In function 'main': src/maggot.c:51:10: warning: ignoring return value of 'system', declared with attribute warn_unused_result [-Wunused-result] src/maggot.c: In function 'move': src/maggot.c:377:27: warning: 'direction[1]' may be used uninitialized in this function [-Wmaybe-uninitialized] src/maggot.c:376:27: warning: 'direction[0]' may be used uninitialized in this function [-Wmaybe-uninitialized] src/maggot.c: In function 'main': src/maggot.c:451:7: warning: 'iter' may be used uninitialized in this function [-Wmaybe-uninitialized] src/maggot.c:448:8: note: 'iter' was declared here src/maggot.c: In function 'sqrt': src/maggot.c:451:7: warning: 'iter' may be used uninitialized in this function [-Wmaybe-uninitialized] cc maggot.o -o maggot And now, this is what happens when you use clang instead: clang -c -Wall -Wextra -O3 src/maggot.c In file included from src/maggot.c:21: src/maggot.h:61:8: warning: incompatible redeclaration of library function 'sqrt' double sqrt(float n); ^ src/maggot.h:61:8: note: 'sqrt' is a builtin with type 'double (double)' src/maggot.c:299:3: error: void function 'turn' should not return a value [-Wreturn-type] return NULL; ^ ~~~~ src/maggot.c:335:3: error: void function 'turn' should not return a value [-Wreturn-type] return NULL; ^ ~~~~ 1 warning and 2 errors generated. make: *** [maggot.o] Error 1 Your code isn't even valid C! Go fix your code!
Not need is not quite right; actually, it's forbidden to return any value when the return type is void. You *have* to write return; instead. gcc only accepts this for compatibility reasons.
 if (something % 2) { // odd do stuff } Can be a nice alternative to: if (something == 2 || something == 4) { } else { // for 1 or 3 do stuff } When you know that you only have four possible values for "something" ([1,4]) Or even better, use enums for clarity. Edit: typo, "even" not "event"
Yeah, rand is fine in this particular case and on some platforms it's fine regardless, but it's best to get into the habit of not using explicitly broken and deprecated calls. As for system(), it's just plain bad practice. It causes a fork &amp; exec every step in his case, and there's no real reason to when you have libraries available to do better.
One thing I noticed is your use of fflush. "fflush(stdin)" has undefined behavior. If you want to make sure your input buffer is empty use another method, such as reading all available data and discarding it.
Seems like everybody has already pointed out possible code issues that you might want to look into, so I'm going to point out something about your implementation choice... Linked Lists, man. You don't need to hold the snake coordinates in an array, and iterate over every element in said array to make your snake "move". You just need to store the Coordinates in a linked list, and when the snake "moves", you just pop off the tail, change the coordinates of that node so it moves one space beyond the current head, and then push it back on to the list as the new head: // This: HEAD -&gt; ELM -&gt; ELM -&gt; ELM -&gt; ELM -&gt; TAIL -&gt; NULL // Becomes this after popping off the tail: HEAD -&gt; ELM -&gt; ELM -&gt; ELM -&gt; ELM -&gt; NULL // Change tail coordinates so its one space ahead of the old head: TAIL-&gt;x = HEAD-&gt;x + 1 (or whatever direction its "moving") TAIL-&gt;y = HEAD-&gt;y (or whatever direction its "moving") // Push it on as the new head, becomes: HEAD (old tail / new head) -&gt; ELM (old head) -&gt; ELM -&gt; ELM -&gt; ELM -&gt; TAIL -&gt; NULL This takes constant time if you store pointers to all relevant nodes, as opposed to an array, which is O(N) to make your snake move. Yeah I know, at the scale of your program it probably doesn't matter, but it's always good to think about efficiency (plus single linked lists are really easy to implement in C). Just a thought.
There are always other ways to get a compiler.
Learning a bit of Computer Science will only help improve everything you do. Linked lists are fairly simple, and will really help you understand pointers better :) [Also, I made an illustration if it helps.](http://i.imgur.com/oBWhk2z.png)
hahahah Thank you very much man, I'm actually studying compsci at university, but I'm in my first year, just learning math until now. 
Thank you for posting this. I'm a relatively recent convert to Mac/OSX (long history w/ Unix &amp; Linux), first thing I tried to do in a terminal was cc/gcc/g++ etc. - nothing found, big frown. Figured it couldn't that hard to install a good command-line compiler but I didn't know where to turn. Again, thanks. Another nice thing about this is that I've heard a lot about clang's merits relative to gcc (particularly when it comes to C++) but I haven't used it yet. Looking forward to it.
Glad I could be of help. Happy hacking! :-)
Is this what you're after? https://github.com/dcorbe/timer Unfortunately it relies on posix conventions like gettimeofday() so it isn't portable to windows (yet, anyways). 
Not all compilers implement even a good portion of C11. clang and gcc are the closest so far. There are still compilers in active use today (MSVC) which don't even implement C99 in its entirety. tl;dr: Depending on the toolchain you're using, your mileage may vary.
I saw this critique at http://c.learncodethehardway.org/book/krcritique.html and wondered it it is better than the classic K&amp;R. Has anyone been through it, or know which is better?
It's been a while since I read it (I spend more time flitting through the online glibc docs) but I believe the errors were intentional, a way to force you into learning how to debug. Like you said, the 2nd edition has been around for over two decades; if there was much to add or fix it would have been already.
I have been through the entire thing and read K&amp;R as well. It is hard to compare my experience between the two as I was at different stages of my programming ability when I did them. I started programming with K&amp;R several years ago and it opened me up to several concepts. I did Learn C the hard way a year ago to do two things 1) To refresh my C programming, which I had not done for a while. 2) To force myself to work through and complete an arduous task. That being said, learn C the hard way is way more than sufficient if you want to be proficient in it as a secondary programming language or as a starting point. 
Errata for K&amp;R 2nd edition is available [here](http://cm.bell-labs.com/cm/cs/cbook/2ediffs.html) in case you're interested.
This guy has an article about mastering the C language. I think it is the best resource out there. [To become a good C programmer](http://fabiensanglard.net/c/index.php)
I personally love K&amp;R, but would say once you've gone through it, find a way to update yourself to the latest standard. There are a lot of newer features in C like fixed size ints (stdint.h) and namespaces, as well as probably a whole lot more. Th strings criticism is bad, but newer C libraries carry functions for limiting strin processing, they mostly carry the same names with an 'n' added, and an argument for the max length. 
I would stick with K&amp;R C (ANSI) 2nd Edition. Its the best all round C book.
That coelocanth book is great, should be everyone's second book on C. I would also advise that once you've understood this book, you should return to the K&amp;R chapter where they show you a simple malloc implementation, it'll be much easier now to comprehend. (Most else of K&amp;R doesn't have much reread value though, when you've got man pages, I'd say)
You're welcome.
His complaints are spot on. While K &amp; R isn't *wrong*, per se, you don't see code like that in the wild these days (or shouldn't). Learning to code with all the warnings treated as errors, using tools like `valgrind`, etc., will teach you how to write C that is acceptable in modern applications.
&gt; Long story short, just make sure you get the 2nd edition and take your second-guesses seriously; Zed's right that the copy() function and string handling in general is risky without a string library. I'm not sure Zed is really onto anything at all with his criticisms of C strings. The crux of his critique is that it'll fuck up your program if you don't properly terminate your strings, which is hardly a novel observation. If you call a function with shitty data as an argument, the results are going to be catastrophic, no matter what your data structure looks like. Sure, you can use a string library, but that's not a magical solution to the Bad Data problem. He recommends the [bstrlib](http://c.learncodethehardway.org/book/ex36.html) library, which I'm not familiar with personally, but I *guarantee* you I can still make the library crash or do invalid reads/writes by reaching into the `bstring` data structure and fucking with the elements. For example, I can tell the library there's more data allocated in a buffer than there actually is, or I can modify the data pointer so that it points to an invalid address (`(char *)0xace703`). Maybe I'm confused about how the library works, and I think that I can convert a C-string to a `bstring` by just casting it: `(bstring)"Hello world"` would probably not raise a compiler warning, despite the fact that it'll trigger undefined behavior if you try to access the `bstring` elements. Specialized libraries don't make the "invalid data" problem go away. The best you can do is specify *very clearly* what sort of data it is valid to send to your functions and hope the programmers using your library don't hurt themselves too badly. Of course, string libraries are good for other reasons (mainly because they are generally more performant than C-strings). But I'm not sure "C strings make it possible for programmers to make memory mistakes" is one of those reasons.
I feel like these are the top two resources.
&gt; While K &amp; R isn't wrong, per se, you don't see code like that in the wild these days It's important to remember K &amp; R wrote their book long before there was any type of standard, and the ANSI rewrite was only to make it compatible with the standard, not really to adjust the style.
&gt; I think learning the C language from the creators of the language itself would be the best possible way to learn. I would agree with this if you already know how to program. If you have never programmed before in your life, whether that be Java or Python or BASIC or whatever, K&amp;R is going to be a tough way to start out.
Thanks. I am considering writing something similar about C++ and STL...
Good question about the performance of string libraries. When I say "performant", I mean performant in time, rather than in space. Time-space tradeoffs are a really common trope in programming. Here's a canonical example: storing the length of a string as an integer takes a few more bytes than just using NUL-termination, but it allows you to access the length of a string instantly. If you only have the NUL byte to go on, you have to traverse the string manually to compute its length. Having the length of a string cached makes a lot of operations super easy. Here's an example: how do you concatenate two strings in C (and put the result on the heap)? If you know the strings' lengths, it's easy, fast, and memory-efficient: just `malloc` a buffer just big enough to hold the concatenation, and then copy both strings over to the buffer. Bam, instant concatenation. One allocation and one pass through the strings: the function could literally *not be any faster*. Things are much more complicated if the strings are NUL-terminated. You can traverse both strings, add their lengths, allocate the buffer, and *then* go back to the same strings you just traversed to copy them over to the heap. There might be other, better solutions to the problem. Maybe just allocating a buffer and hoping it's big enough to hold the concatenation (and resizing the allocated buffer if you run out of room) is more performant since it'll only require you to loop through the strings once, but `malloc` (and associated functions like `realloc`) are generally pretty slow too. Let's say you allocate an initial buffer of 128 bytes and then `realloc` as you go along. Even when you *do* guess right and this buffer ends up big enough to hold the concatenation, you probably didn't guess *exactly* right, and you're probably not using all 128 bytes (and are therefore wasting a little bit of space). This is a harder problem that gets much, much easier when we know the length of the strings beforehand. And this is just concatenation -- many common string operations get much easier if we have the lengths pre-computed already. There's a wonderful [Joel Spolsky article](http://www.joelonsoftware.com/articles/fog0000000319.html) on this topic that you should check out if you haven't already.
The syntax in that second block of code confused me. I did some investigation and ended up finding this: http://gcc.gnu.org/onlinedocs/gcc-4.1.2/gcc/Designated-Inits.html HOW DID I NEVER KNOW ABOUT THIS SYNTAX BEFORE?!?
It's a feature of C99. You can initialize an array like this: int foo[] = { [0] = 12, [1] = 13 }; and any struct like this: struct point bar = { .x = 12, .y = 15 }
Yes, please do! That should be very interesting to read.
You describe the exact reason a combination of tabs and spaces should be used but then pick a different option for less mistakes. I know the tabs/spaces argument is done to death, but tabs for indentation and spaces for alignment is absolutely the Right Wayâ„¢ (after consistency of course). To aid this, I highly recommend having tabs visible in your text editor. In my vim configuration, I have: " Show tabs as light gray arrows hi SpecialKey ctermfg=LightGray set listchars=tab:â†’ set list (for a dark background, you might want `DarkGray` instead)
I know it's attractive, but people get it wrong all the time. I don't think this will change. Can't you tabs people just give up, so we can all have consistency!? :P I promise this will be the last comment I make on the tabs-vs-spaces topic :) Edit: sorry that this comment wasn't very constructive. [There was a better discussion on the tabs point on /r/programming](http://www.reddit.com/r/programming/comments/1ni30m/c_style_my_favorite_c_programming_practices/ccitkue).
I prefer tabs, but I've heard a lot of support for spaces around the web. One thing I don't get is why you don't like the ++ and -- operators. To me, count++ is much simpler and easier to read than count += 1.
I don't think consistency between projects matters *too* much. At least there aren't a hundred different ways to do it. I think ideally the source code and the view of the code should be independent, but text editors aren't going anywhere any time soon, so we have to make sure it looks nice when you just view the source as-is. Anyway, I agree with most of your suggestions. I've not seen comments after code before (except on the same line), but I think it reads pretty nicely. I think it's okay to use `float` most of the time until you need `double`, but sure, this isn't much of an issue for a lot of programmers these days. I also think `if (bool_var)` is fair enough - if the variable is named well it should read nicely. Also, I don't like avoiding programming constructs just because they can result in mistakes (like `switch`) - just be careful! 
I was sad python didn't have ++, but I got over it pretty quickly. 
It's far from a necessity, but I think it's useful to have for readability reasons.
They're both as simple as each other. They both do the same thing. The differences are: - one is useful for other, similar, situations, and the other isn't - one is more readable to people with no programming experience, and the other isn't (if you say C isn't for new programmers - why can't it be?) - one has complicated evaluation rules, and a twin brother which has the opposite evaluation rules - one encourages state changes in expressions, like `xs[ i++ ]`, and that way lies madness That's why I don't like `++` and `--`, but I knew this would be a hard sell when the language is founded on the examples in K&amp;R :-)
&gt; Space isn't an issue anymore, ... someone isn't writing c for tiny embedded applications.
Why would you use postincrement when you don't want postincrement. That's poor style imo. 
State changes in expressions are not "madness" they are very well established C idioms. 
Idiomatic or not, they make code hard to follow and hard to read. Rather than reading from top to bottom, your eyes have to jump around to work out what's going to happen when. Besides, i += 1; if ( xs[ i ] == something ) { ... mightn't be "idiomatic", but it sure isn't hard to understand. It's not as if it's something you have to learn to be able to read.
This is as much a project to learn as to teach; I'd be very curious to hear your opinions (other than on trivial differences like on tabs or `++`).
Only if they codified that ++x and x++ are the same thing (or different things as they should be). I was under the impression that that was still at the whim of the compiler.
They are very different things according to the language, the compiler doesn't get to pick. The compiler can make an optimization of "**x++**" or "**x--**" ONLY when the return value of the expression is not used (i.e., it's the only thing in a statement) AND if the type of x is an intrinsic type. Both of these restrictions guarantee that there are NO side effects from the expression, and the code can be generated as if it were "**++x**" or "**--x**". Again, note that this only happens when the compiler is sure there are no side effects, and as such, the code still behaves exactly as you wrote it.
First of all, C isn't a functional programming language so applying principles used in functional programming languages is not the best way to use the language I think. Stuff like passing pointers to functions or having global and static variables are perfectly fine I think. Then there is stuff like zeroing variables, which isn't necessary since the compiler will usually warn you when you use an uninitialized variable, explicit comparing, which is harder to read in some cases (especially x == true is just unnecessary) and avoiding unsigned values, which are fine IMO. Also minor stuff like you mentioned (I'd *never* use spaces for indenting C, or i += 1, which is just ugly for my eyes). I like C code in the style of research UNIX or Plan 9.
Amen!
Perhaps I misheard something along the way. I thought K&amp;R specifically said that it was compiler specific but checking the book I don't see it now.
&gt; First of all, C isn't a functional programming language so applying principles used in functional programming languages is not the best way to use the language I think. Stuff like passing pointers to functions or having global and static variables are perfectly fine I think. So, you like global variables. To each their own, I suppose. But can you make an argument in support of global variables, or fault my argument against them? Also, the document says nothing against passing pointers to functions. &gt; Then there is stuff like zeroing variables, which isn't necessary since the compiler will usually warn you when you use an uninitialized variable You're right - I hadn't considered that. I've just removed that rule. &gt; explicit comparing, which is harder to read in some cases `NULL` is very different from `false`, which is very different from `0`. I think it's important to inform the reader of what we're expecting, because it's very rarely obvious, unless you use repetitive names like `num_widgets` a la [Hungarian notation](https://en.wikipedia.org/wiki/Hungarian_notation). I don't see how it's harder to read. What do you read when you see `if ( !something )`? Do you just say to yourself, "if it's falsy"? Really? I know I always try to find the declaration of `something` to work out what its falsy value actually is. I really wish the programmer had just told me in the comparison. &gt; avoiding unsigned values, which are fine IMO. Can you elaborate why you think so? How confident are you about the rules for sign conversions? Why would you want to use unsigned values? For a larger value, why not just use a larger type?
You're right, the examples I used in my two replies were inconsistent. That's not important. If you would use the post-increment operator, increment the variable after the statement. If you would use the pre-increment operator, increment the variable before the statement. Easy - and we don't have side-effects hiding in calls to functions or array indices. Readable programs flow from top to bottom. I know this is a concept that's hard to grasp for many C programmers. Please try.
&gt;So, you like global variables. To each their own, I suppose. But can you make an argument in support of global variables, or fault my argument against them? It's sometimes easier to read a function when you aren't passing the same variable over and over again as you require less parameters. I'm not saying you should use global variables all over the place, but they're not inherently bad. &gt; I don't see how it's harder to read. What do you read when you see if ( !something )? Do you just say to yourself, "if it's falsy"? Really? I know I always try to find the declaration of something to work out what its falsy value actually is. I really wish the programmer had just told me in the comparison. I wouldn't compare explicitly if the value is boolean. Now a pointer is different. I usually write if(pointer) and if(pointer == NULL). &gt; Why would you want to use unsigned values? For a larger value, why not just use a larger type? Because why use a larger than needed type when an unsigned is ok. I don't think you should be paranoid only because there are some cases which are somewhat unxepected. Under this premise you shouldn't *ever* use javascript. If you're unsure about a specific unusual conversion, be more explicit or make a comment.
&gt; I know this is a concept that's hard to grasp for many C programmers. Please try. You won't win people over by being condescending and insulting them.
Arrogance of the author attracts the downvotes. I for one haven't seen one submission here or on r/programming which would make my blood boil so much in years. Some nuggets: -memory isn't an issue anymore ("just use next size up" about integer types) -CPU speed isn't an issue anymore (use doubles instead of floats) -don't use any of traditional C features (++, passing pointers, malloc, omitting redundant == true in if statement, switch statement, unsigned types) -use reverse of what was established through decades of tradition (comments after not before lines, /* */ comments etc. It's like a freshman coming to some math conference and saying the notation is all wrong and people should get over it - fast. 
&gt;Who knew C programmers cared so much about that extra nano-second spent for a double to save them from floating-point drift errors. Take that nanosecond and multiply it out a bit. That's why people -- not just "*C programmers*" -- care to use the right precision. Besides which there's memory consumption issues with double- vs. single-precision -- doubles are twice as big. Sure, you scale it all the way down to one number and it appears to not matter, but what if I have an array of a million? Suddenly that's 4mb vs. 8mb -- starting to look like an important difference.
&gt; I usually write if(pointer) and if(pointer == NULL). Technically, `NULL` doesn't have to be `0`, it just pretty much typically universally is.
Irrelevant. NULL must compare equal to zero regardless of the actual NULL bit pattern. When a null pointer participates in a boolean expression it will always yield false. http://c-faq.com/null/ptrtest.html
Well, to be fair, you had some very good point laid out, but you clearly didn't get it. I'm currently reading the /r/programming thread. At the point I am reading for example, you don't understand that to lay out a boolean to test again a value with an operator in between is both prone to error and less clear. Your answer to the criticism is "fine, if you don't want to save your reader the hassle, don't do it". They're all saying to you that **they're saving their reader the hassle**. Because your notation is less clear, simply put. (for those who don't want to navigate to the thread, here is the nugget: if ( on_fire ) if ( !on_fire ) vs if (on_fire == true) if (on_fire == false) or if (on_fire != false) if (on_fire != true) This is not mundane criticism! But you still answer with an arrogant tone that they are simply not getting it... Yes, this will sure as hell attract downvotes and harsh reaction.
&gt; I'm still pretty new to C but I'm falling in love with the low-levelness and relative simplicity of the language. It gives me greater appreciation for the high level stuff in languages like Lua and Python. It's really a fantastic thing. C doesn't get all the credit it deserves nowadays. I love Ruby, but whenever I use it I can't help but be a little comfortable that I don't really know exactly what the machine is doing. C gives you a lot of control, which is really invaluable. &gt; If you were to work extensively with strings and make your own string library or set of functions, would you use a struct or a typedef? Remember that typedef's are for giving new names to types that already exist, and structs are for packaging types together to make new types. If you want to store the length of a string (as an integer) along with the string itself, you obviously need to do the second thing, and therefore need a struct: struct String { int len; char *str; }; If I were designing a string library, I would also store the length of the allocated buffer along with the string length and the pointer to the string itself.
To the OP's credit, I think a lot of the (IMHO bad) advice originates from "21st Century C" by Ben Klemens. Use doubles instead of floats, memory is plentiful and the CPU is fast, 'switch' is hard, so don't use it, etc... That was about the only programming book that I ever didn't finish, since it annoyed me so much. 
Oh, awesome. Thanks for the info!
&gt; Who knew C programmers cared so much about that extra nano-second spent for a double to save them from floating-point drift errors. Not to mention it doesn't save them from the drift issue. It reduces the issue, but does not eliminate it.
// comments are C++ comments. Some older C compilers will choke on them.
You are what I like to call "aggressively ignorant." Instead of learning the rationale, you make blanket statements that are so incorrect it's hard to even believe you're not trolling us. What happens when my data never have precision outside of 12 bits? Oh, just use a double, it's no-biggie. Even though a single multiplication in that inner loop is 4 times slower than using a float and close to 16 times slower than a [half](http://en.wikipedia.org/wiki/Half-precision_floating-point_format), *without* accounting for vectorization gains. Oh but the easily predictable and correctable drift errors apparently are more important than the pile of money our CPUs are burning when they're running jobs. Instead of designing algorithms that take this into account, just munge everything by making it all doubles and hoping it's somehow "more correct." Lemme go in and tell my team we should switch to doubles stat. I hope I never run into code you write.
We disagree. I think `== true` is more readable and informative. Deal with it. It's really not that important.
Quite interesting, don't agree with everything of course. * I write to `gnu99`, but I work in a fairly enclosed environment where this isn't a problem, and some of the GNU extensions are very valuable ... and we don't have any C11 compilers yet. * Tabs are bad ... but I'd go further and *mandate* 80 character columns. Imagine your code is printed out on punched cards! * `//` comments? Meh. * American English? Bah! I'm a brit, no chance! * `double` vs `float`. Well, the argument isn't quite as cut and dried as you say, can depend on application. My work is embedded, so size matters. On that, you seem to have forgotten `stdint.h` and the sized types it gives you: use these for binary interchange (registers, stored data, packet formats, etc). * Declare at point of use: yes! * `if (flag == true)` -- wtf? No, no, no. This makes me cross. * On the whole I agree with not changing state in expressions. There is one case where it's valuable; note the use of `,` to sweeten the pill: some_type result; while (result = action(), test(result)) { do_stuff(result); } * Avoid unsigned types? Well ... you'd better freshen up your understanding of how C handles overflow, in particular, signed types don't support modulo arithmetic. To be precise, the result of integer overflow on a signed integer type is entirely at the discretion of the compiler. Oops. * Don't use `switch`? Please. This is not sensible. I've skipped a bunch of stuff, have written enough here. * I'd replace your style on structs with the following: Almost never use `typedef`. Also, I've moved from `CamelCase` to `lower_case` naming. * Don't pretend C is object oriented: sure. But it's pretty good at function dispatch tables. * Your dependency generating rule looks simpler than the one I'm using, I'll have to give it a try. * Warnings: you seem to have missed out the most important flag of all, namely `-Werror`. I'd also throw `-Wshadow` into the list: although it's (very) annoying at first, it's caught enough hits to earn its keep for me. I'm always looking for new compiler warnings to throw into the pot! Quite an interesting list.
I don't want to think of how much boilerplate code I would have been saved from writing had I known of this sooner.
In the embedded/DSP world not all of those rules are practical. There are still compilers that need ANSI C so if you want your lib to be portable you better don't use // comments, and declare your variables at the beginning of blocks. Those compilers also have shitty optimizers, so duff's devices and manual loop unrolling are still good things to know. And I had a good laugh at the "always use double" rule. In DSP code you use the minimal precision required for the job. Speed is everything. But most of these rules seem sensible.
Thanks for the feedback! - GNU extensions are valuable, but I'm just trying to encourage people to do without them if they can. At your work, I bet it will be someone's job someday to get your thing compiling on Clang or ICC. If you can prevent this from even being a job, why not? - Agreed about 80 characters. Just pushed a rule for that. - Why not `//` comments? Lots of these rules are just about cutting complexity from the language. `/* ... */` comments are something we don't need anymore. - On American English: I just think we should all be programming using the same language. Having consistent spelling and vocabulary in your project is quite important, IMO. - I agree that using `double` vs `float` depends on the application. In retrospect, that rule was probably worded too strictly. I've just changed the title to say "Use double rather than float, unless you have a specific reason otherwise", and mentioned in the justification that you shouldn't choose `float` only for performance, without doing benchmarks. - I'm missing how `stdint.h` is relevant here? Could you elaborate? - The `if ( something )` vs `if ( something == true )` thing has been done to death. It's really trivial. I think that `== true` is more readable and informative than not. Without it, you don't know what the `if` is testing for, unless you keep the relevant types in your working memory, or stick to weird type-naming conventions, and trust that your developers do too. If you disagree, that's fine. I'm sorry for my preferences making you cross. - Thanks for pointing out that usage of assignments within expressions with `,` - I've just added a similar example to that state-change rule. - Yeah, *avoid* unsigned types. You shouldn't use an unsigned type because your value shouldn't be negative, or because you need a larger maximum. I hadn't really considered how unsigned types are useful for bit-wise operations and modular arithmetic, though, so I've added a note about that. - Can you provide a defense of `switch`, or fault my criticisms of it? - Yeah, I realize that my criticisms about `typedef`s in general apply equally to using them for structs. By typedefing the `struct` away, you're still requiring your readers be familiar with that construct. I'm still of the opinion that it's worth it for visual clarity, but I'm leaning on the fence. - Nothing against function members. I use them, too. It doesn't mean you're writing object-oriented code. - Thanks for `-Wshadow`. I've added that. `-Werror` is cool, but I don't use it myself, because it stops [Syntastic](https://github.com/scrooloose/syntastic/) from color-coding errors and warnings differently.
There are so, so many things that C is useful for, that I have no experience with whatsoever. I'm sorry if these rules came across as universal admonitions about quality, but that wasn't my intention. Personally, backwards compatibility to ANSI-C-only platforms isn't important for me. I get that it's important for other people. Also, I've just reworded the double rule's title to say: "Use double rather than float, unless you have a specific reason otherwise". I've added a few sentences to the body: "Don't use floats "because they will be faster", because without benchmarks, you can't know if it actually makes any discernible difference. Finish development, then perform benchmarks to identify the choke-points, then use floats in those areas, and see if it actually helps. Don't prematurely optimize." Do you disagree? I'd like to hear your thoughts.
Posts like this makes me cross! Oh look a no named saying what he thinks is the 'correct' way to do something in a purely abstract sense with no real claim that he actually knows what he is talking about! People like Linus et'al can do statements like this because they have proved they are definetly correct when it comes to talking about software. Posts like this should always be DISCLAIMER that its your personal opinion. Posting in such a way that oh look at this reddit this is how you should program because i think everyone else is stupid and i am right to my group of friends so listen to me. Ahh makes me so cross.
Oh and by the way, gcc supports two styles of designated initializers, one is C99 and one is a GNU extension. Try not to use the secons style for the sake of portability.
Read both. K&amp;R is legendary. LCTHW is still worthwhile and you'll probably pick up a thing or two. That said, it's a skill, and needs practice, so writing as much as possible is key.
http://www.vexforum.com/showthread.php?t=77281 It's the only C option for VEX.
As a general rule I think its now better. The reason for my initial reaction was that I work with audio processing where float is used exclusively. I ve never seen double, it would double ram for buffers, effectively half fpu registers etc ... For audio float has enough resolution. I've written a float sine generator and calculated that with float precision it would take about 7 month worth of generated data for it to be off by one sample. And you are right, nothing beats testing. One should never make assumptions about speed, especially when running float code.
Seriously? Come on dude. If you're going to ask for help at least format it for readability.
&gt; Initialize strings as arrays, and use sizeof for byte size This is going to prevent certain optimizations, like keeping string literals in read-only memory sections. You're essentially creating mutable strings, and then telling the compiler to prevent modifications with the const keyword, which can be defeated with a cast. Some things I'd add: 1. Declare functions as static as much as possible. All of those little helper functions that are only useful in the context of the current file don't need to be exposed globally. 2. Take the size of the object rather than the size of the type when allocating memory. If you decide to change the type later, then you only have to change it in one place, and you'll always have the correct size. // Good int* a = malloc(sizeof(*a)); 3. Never cast a void*, the compiler will figure out what to do itself. // Bad struct foo* a = (struct foo*)malloc(sizeof(*a)); 4. Don't use identifiers beginning with _ or ending with _t. They're reserved by the standards organizations. 5. Prefer indexing into an array over pointer arithmetic. 6. Don't use the comma operator. I can never remember which side of the comma gets returned. Can you say with 100% certainty that you always will? 7. Your header files should only include the details needed to use your library. The macro you wrote to save a bunch of typing and the struct that's only used internally don't belong here.
I have never seen anyone who thinks (x == true) is more readable than (x). I mean why stop there? Let's check ((x == true) == true) to make *reeeeally* sure our comparison returned the value we expected. Similarly post and pre increment are two of the most readable and widely used features of the language. K&amp;amp;amp;R like it, Linus likes it, Wolfgang (u-boot) likes - sorry OP but you lost this battle by a large margin. Most importantly these rule should be written "for the project" not as a general guideline. In u-boot we absolutely value space. We absolutely value speed. We also don't give a shit about clang (so far) and gnu extensions are extremely useful.
... but a table of function pointers will also, on the other hand, make the code much clearer and to-the-point. Gigantic switch statements are an eye sore.
IMHO this: switch (x) { case a: foo(); break; case b: bar(); break; ... } is much clearer than this: const void (*table)(void)[] = { [a] = foo, [b] = bar, [c] = baz, ... }; ... table[x](); Also, the first approach gives the compiler a better chance to perform various optimizations.
This is the best answer, so far. I have used this approach myself, without relying on the GCC specific extension. Create a macro something akin to this: #define DISPATCH \ instruction = code[ip++]; \ COUNT_INSTRUCTION(instruction); \ switch (instruction) \ { \ case OP_FUNC_DECL: goto _FUNC_DECL; \ case OP_CREATE_OBJECT: goto _CREATE_OBJECT; \ case OP_CREATE_DATABLOCK: goto _CREATE_DATABLOCK; \ case OP_NAME_OBJECT: goto _NAME_OBJECT; \ ... Now, in the "switch" which handles opcode dispatch: _START: DISPATCH; _FUNC_DECL: { // implementation } DISPATCH; _CREATE_OBJECT: { // implementation } DISPATCH; ... Note there is no more top-level dispatch (no top-level "switch")... you just start executing at "_START". On hardware that makes use of a BTB, this is more performant that having only that one "switch" encoded, because the branch prediction fails miserably and you're constantly flushing the instruction pipeline. Note that you *will* have to compile this as a separate module with different optimization settings to prevent the compiler from just re-optimizing all those repeated "DISPATCH;" code blocks back into one table. 
I always try to learn from well-organized software projects which I personally like. A good style guide can be found [in the man pages of the FreeBSD project](http://www.freebsd.org/cgi/man.cgi?query=style&amp;section=9).
How are you going to create the insert_into_table macro? We're talking about C, not C++ by the way.
Hm, I may be a bit late to the party... Being someone who had recently started working with C, and coming from a background of writing mainly Python server applications, I appreciate this listing of your opinions. Though I don't agree with all your statements (mainly the positioning of const in variables), I'm happy to see you list your own reasons for them. I do agree with many of your statements on pre-optimization and usage of floats. Coming from an interpreted language like Python (the horror!) I am firmly of the opinion that readability and consistency trump optimization. Usually, you don't need a program to be *fastest*, just *fast* *enough* . In short, thank you for the thought provoking read, and know that, despite what many in this thread might think, you have worth.
As someone with a solid background in JavaScript, a lot of what you say seems appropriate to meâ€”then again, this isn't JavaScript :) I didn't realise until I read these comments, but it seems pretty clear you're treading on sacred ground around here. I'm learning C currently; it's somewhat disheartening to see the vitriol surrounding a post like this. Obviously, not every new idea is better than the current well established principle, but being able to make suggestions without having the book thrown at you is part of what makes communities such as this able to thrive, adapt and improve. Or not, as the case may be.
It can either create a function marked _____attribute_____((constructor)) to do it at runtime or do linker magic by inserting it into a special reserved area during compilation and linking. Both ways work, but are suitable for different scenarios. There's probably other magic that can be used as well.
No need to be rude. That's your decision, but most people don't code ISO C, because it's not particularly useful without compiler specific extensions. Your code will be a lot more convoluted and complex without them (what I just showed you is one example), and some things are simply not possible at all (try implementing a sane protocol stack without using structs without padding, for example, or converting between representations without using unions). The gcc ones are probably the most portable, considering they are also supported in clang (and some other compilers as well). Most things in C99 and C11 started as compiler specific extensions because they were simply necessary to be able to code in a sane way. I can't see any reason why you would use _only_ ISO C, unless you thought it was a good challenge in itself. Unless you are only doing very simple things, your code is either incredibly convoluted or you are using non-standard things without realizing it. There's simply no way of coding and maintaining complex C projects without using compiler extensions.
I should have rephrased that: It can't be done in a sane way without sacrificing readability and maintainability. Of course it can be done, but the fact that plan 9 has done so doesn't mean it's a good thing. If your end goal is super-portability, then by all means do this but most people and projects, in my experience, should not. It's a bad idea. As a counter example, have a look at the linux or *bsd kernel code bases. They're incredibly simple, readable and to-the-point, precisely because they use compiler specific extensions (the gcc ones in this case, which clang has also implemented).
That's a pretty sweet set up!
Eclipse is a good multi-platform IDE, and works well for C/C++ and Java. It can also handle a whole host of other languages with various plugins.
I personally just prefer using Sublime Text 3 and my terminal.
If you're on Windows it's crazy not to try the free Visual Studio Express editions! They have a great debugger and you can use CMake or something if you want to keep your build scripts portable.
Run Linux in a VM?
As most people, I would recommend using a simple text editor + a command-line compiler(for example MinGW or gcc from cygwin). IDEs are quite confusing, especially for beginners, since they're bloated with features that you probably won't use until you are more experienced(if you will use them at all). Especially C is perfect to begin programming without an IDE. Just use any text-editor you like(if you don't have a preferred one yet, just use notepad++, it's the least confusing). Type your code in it, save it, and compile it using the command-line. An example of this can be found here: http://www.mingw.org/wiki/MinGW_for_First_Time_Users_HOWTO Although you will have to use .c file-extension and gcc instead of g++(since you're using C instead of C++, it is backwards compatible though, but it is still nicer to use pure c files and compiler if you only use c)
Visual Studio is great but you're limited to C89. C99 library support is getting better with each release but some compiler features will probably never make it. I think their goal is to have ffmpeg build in C++ mode.
Qt Creator.
I second Notepad++, if you ever mature up to Linux I'd recommend gedit or pluma (MATE's version of gedit).
Additionally the *IDE* part doesn't recognize C at all. If you write something like sometype* foo = malloc(sizeof(sometype)); the IDE will insert red squiggles below malloc indicating an error (invalid type) because it thinks the code is C++. Having said that, i ignore the editor for the awesome debugger.
Personally i really like [OpenWatcom](http://openwatcom.org/index.php/Main_Page). It provides a full development environment (compiler, IDE, tools, etc) for Windows (and other OSes, but i assume that you'll mostly be interested in Windows) with documentation about everything (tools, libraries, its C language implementation, etc). It even comes with a C tutorial :-P. The IDE has an interesting approach: instead of being a single monolithic application, it is just a small program that keeps track of your project files and provides a nice GUI over the compiler and linker settings. Editing files, debugging and profiling is handled by external programs (watcom code editor, watcom debugger and watcom profiler). Additionally you can use your favourite editor instead of OpenWatcom's (f.e. Vim works fine). On the negative side, the IDE feels a bit old... which is because it is old. Its UI hasn't been updated much since the 90s. But at least it isn't a resource hog (everything starts instantly, which is why i like it). Additionally [C99 support isn't fully there](http://www.openwatcom.org/index.php/C99_Compliance). There is an undocumented flag to enable partial C99 support (`-za99`). However it is much better than Visual Studio in that department and most of the C99 stuff i've tried work fine even without the `-za99` flag. There is also an [unofficial fork/build](http://open-watcom.github.io/open-watcom/) that contains fixes made since the last release (from 2010). AFAIK some people use Code::Blocks as an alternative IDE for OpenWatcom, although personally i never used that.
Visual Studio.
Hey, thanks very much for your comment. I'm sorry it's taken me so long to reply. &gt; This is going to prevent certain optimizations, like keeping string literals in read-only memory sections. You're essentially creating mutable strings, and then telling the compiler to prevent modifications with the const keyword, which can be defeated with a cast. This is a really interesting point I hadn't considered. With `char const *`, you have potential performance improvements via assured immutability, and simplicity from always using `strlen( str ) + 1` to get the byte size of a string without having to care if it's been defined as an array or a pointer. On the other hand, with arrays, using `sizeof` for byte size is less prone to bugs (no need to remember the `+ 1`). Also, it saves you from having to switch between pointer definitions and array definitions, depending on if you need mutability or not. You can just always use array definitions, lowering conceptual complexity. Furthermore, pointer definitions are only as safe as array definitions if you compile with `-Wwrite-strings` to ensure they're initialized as `char const *`. Otherwise, there are no warnings if you try to assign to a literal `char *` - but your program will seg-fault if you change its elements. Unfortunately, not many people work with `-Wwrite-strings`, because it isn't included in `-Wall` or `-Wextra`. Thus, I consider pointer initializations less safe *in general*. I'm not persuaded by the benefits of declaring string literals as pointers. I'd be surprised to see any real performance benefits from it, and even if there are, I don't think you should prematurely optimize like that. Also, I know I've been bitten by forgetting the `+ 1` on a `strlen` expression before. I've included these points in [that rule](https://github.com/mcinglis/c-style#initialize-strings-as-arrays-and-use-sizeof-for-byte-size): thanks for pointing it out. &gt; Declare functions as static as much as possible. All of those little helper functions that are only useful in the context of the current file don't need to be exposed globally. I agree. I [added this as a rule](https://github.com/mcinglis/c-style#minimize-what-you-expose-declare-functions-static-where-you-can). &gt; Take the size of the object rather than the size of the type when allocating memory. Agreed; [added this as a rule](https://github.com/mcinglis/c-style#where-possible-use-sizeof-on-the-variable-not-the-type), with a note about compound literals. &gt; Never cast a void*, the compiler will figure out what to do itself. Agreed. This is covered by the rule "[don't typecast unless you have to](https://github.com/mcinglis/c-style#dont-typecast-unless-you-have-to-you-probably-dont)", but I added a code example similar to yours. &gt; Don't use identifiers beginning with _ or ending with _t. They're reserved by the standards organizations. Totally agree! This annoys me more than it should. I'm amazed I forgot it. I added it [here](https://github.com/mcinglis/c-style#never-end-your-names-with-_-or-_t-theyre-reserved-for-standards). Thanks! &gt; Prefer indexing into an array over pointer arithmetic. I agree. I added [this rule](https://github.com/mcinglis/c-style#always-prefer-array-indexing-over-pointer-arithmetic). &gt; Don't use the comma operator. I can never remember which side of the comma gets returned. Can you say with 100% certainty that you always will? Yeah, I know it's not very nice, but in some situations it's the only way without repeating an expression. I've since expanded (a bit) on the mention of the comma operator [here](https://github.com/mcinglis/c-style#never-change-state-within-an-expression-eg-with-assignments-or-). Do you think using the comma operator for that `while` example is reasonable? &gt; Your header files should only include the details needed to use your library. The macro you wrote to save a bunch of typing and the struct that's only used internally don't belong here. Agreed. I added a paragraph about this to the rule I added at your suggestion about declaring functions `static` where you can. Thanks a ton for your very useful and spot-on suggestions. I'm very curious to hear your opinion of the rest of the document, if only in general.
gedit is excellent, though without the auto indentation plugin installed and enabled (think it's installed by default now but not enabled by default) I go crazy. Excellent little editor though.
Thanks for that. I bookmarked it. Although it seems to be mostly about formatting, I noticed it suggests something similar to my (controversial) "don't rely on truthiness" rule: &gt; Do not use ! for tests unless it is a boolean, e.g. use: `if (*p == '\0')` not `if (!*p)` That guide got me to write two new rules: [one](https://github.com/mcinglis/c-style#always-comment-endifs-of-large-conditional-sections) on commenting `#endif`s (as they recommend), and [another](https://github.com/mcinglis/c-style#always-use-brackets-even-for-single-statement-blocks) on always using brackets (unlike that guide recommends).
Well, you caught me while I'm bored, so here goes: &gt; Write to the most modern standard you can I think it's better to use C99 until the major compilers all support C11. &gt; We can't get tabs right, so use spaces everywhere Not touching this one! &gt; 80 characters is a hard limit Blech. &gt; Use `//` comments everywhere, never `/* ... */` Whatever. &gt; Write comments in full sentences, without abbreviations Assume as much domain knowledge as is reasonable. No need to expand all TLAs every time. &gt; Don't comment what the code says, or what it could say Don't write rules that everyone already knows. &gt; Consider writing comments after the line referred to This does not jive with the way I read code at all. &gt; Program in American English Sure, whatever. &gt; Comment all `#include`s to say what symbols you use from them Can I mow your lawn and get you a coffee while I'm at it? If you *really* want this, get a tool to do it. I can't be bothered to do computer work. &gt; `#include` the definition of everything you use Agreed. &gt; Avoid unified headers Agreed. &gt; No global or static variables if you can help it (you probably can) I agree in principle, but know the requirements of your system. Any global variables should be static unless absolutely necessary. (ie: errno) &gt; Minimize what you expose; declare functions `static` where you can &gt; Immutability saves lives: use `const` everywhere you can Warning: some const types don't convert nicely in C. Ie: `char const * const *` doesn't convert to `char**`. (Although, it does in C++.) Also, there's no point in consting basic variables in function signatures. Even if you do change them, they're just copies of the original values. Making local variables const isn't usually useful either. &gt; Always put `const` on the right and read types right-to-left I agree, but I'm usually too lazy to do this for basic types. &gt; Don't write parameter names in function prototypes if they just repeat the type Meh. I'd rather have an exact copy of the signature. &gt; Use `double` rather than `float`, unless you have a specific reason otherwise Agreed, but know your system. &gt; Declare variables when they're needed You already mentioned writing to a modern standard, but it's worth mentioning that GCC (and maybe others?) default to C89, where this won't work. &gt; Use one line per variable definition; don't bunch same types together Agreed. &gt; Don't be afraid of short variable names * i, j, k for loop variables * x, y, z for Cartesian coordinates * n for counts Other than that, you should be *very* sure that it's obvious. I find two-letter variables are often much less memorable than single-letter variables. &gt; Be consistent in your variable names across functions Sure. &gt; Use `bool` from `stdbool.h` whenever you have a binary value Agreed. &gt; Explicitly compare values; don't rely on truthiness I don't think you need to do `bool_val == true`, but I agree otherwise. &gt; Never change state within an expression (e.g. with assignments or `++`) This is one of those rules I agree with, but break more than I'd like to admit. Sometimes, it can be elegant. &gt; Avoid non-pure or non-trivial function calls in expressions Sure &gt; Avoid unsigned types because the integer conversion rules are complicated Needs an accompanying section called "Avoid signed types because signed overflow is undefined." ;) &gt; Use `+= 1` and `-= 1` over `++` and `--` Except in for loops. &gt; Use parentheses for expressions where the [operator precedence](https://en.wikipedia.org/wiki/Operators_in_C_and_C%2B%2B#Operator_precedence) isn't obvious Your sockaddr_in example would be more compelling without the cast and extravagent use of whitespace. Also, in the hungry example, the || should go at the end of the first line to indicate that the expression is continued on another line. &gt; Use `if`s instead of `switch` Sure. &gt; Separate functions and struct definitions with two lines Meh. &gt; Minimize the scope of variables I've tried to read this a few times, and have a hard time parsing what you're saying. But I think I agree? &gt; Simple constant expressions can be easier to read than variables The compiler will probably optimize this anyway. &gt; Prefer compound literals to superfluous variables Bleh. I would agree if the casts weren't required. If you're worried about limiting the scope of a variable, throw braces around it and the expression it's used in. &gt; Never use or provide macros that wrap control structures like `for` This is just repeating FUD about macros. Document what the macro does, use terminology that makes sense, and don't try to do too much magic all at once. &gt; Only upper-case a macro if will act differently than a function call Agreed. &gt; If a macro will only be used in a function, `#define` and `#undef` it in the body Agreed. &gt; Initialize strings as arrays, and use `sizeof` for byte size &gt;&gt; The benefit of initializing string literals as pointers is that those pointers will point to read-only memory, potentially preventing some optimizations. `s/preventing/allowing/` &gt; Where possible, use `sizeof` on the variable; not the type &gt; Never use array syntax for function arguments definitions Yeah... :( &gt; Always prefer array indexing over pointer arithmetic &gt; Document your struct invariants, and provide invariant checkers I felt a mix of "That's a lot of extra work", and "Well, duh." &gt; Use `assert` everywhere your program would fail otherwise Don't rely on asserts for control flow. &gt; Repeat `assert` calls; don't `&amp;&amp;` them together Agreed. &gt; Don't use variable-length arrays They also won't exist after the function returns. &gt; Avoid `void *` because it harms type safety Agreed. &gt; If you have a `void *`, assign it to a typed variable as soon as possible Agreed. &gt; Don't typecast unless you have to (you probably don't) You shouldn't be converting size_t to int anyway! Also, it's undefined behaviour to cast between types that aren't compatible, except to/from char*. &gt; Give structs TitleCase names, and typedef them Agree on the typedef, iffy on the titlecasing. &gt; Only typedef structs; never basic types or pointers Typedeffing function pointers is immensely useful. Using a typedef might also be required for some macro stuff. (I can't quite remember what...) &gt; Give enums `UPPERCASE_SNAKE_NAMES`, and lowercase their values Sure. Also, define a final enum value (NUM_JSON_TYPES, or something) to be used as an array size to hold them. &gt; Never end your names with `_` or `_t`: they're reserved for standards Should be 'begin with _' and 'end with _t'. &gt; Only use pointers in structs for nullity, dynamic arrays or incomplete types Agree, except if struct A contains struct B, and struct C contains a pointer to struct B. If struct B has an associated cleanup function that expects to free itself, you'll run into trouble. &gt; Only take pointer arguments for modifications, or for nullity Agree, but know your data. &gt; Always prefer to return a value rather than modifying pointers Prefer it, yes, but don't force it. &gt; Use structs to name functions' optional arguments Looks nice, but also very fragile. I would definitely not make it a rule. &gt; Always use designated initializers in struct literals Agree. &gt; Prefer `_new()` functions with named arguments to struct literals I'm not really sure what you're saying here. It should say, "Always fully initialize your structs." &gt; If you're providing new and free functions only for a struct member, allocate memory for the whole struct Yeah. &gt; Avoid getters and setters Eh. &gt; C isn't object-oriented, and you shouldn't pretend it is Bleh, don't preach about Haskell. &gt; Use GCC's and Clang's `-M` to automatically generate object file dependencies Totally agree. &gt; Always develop and compile with all warnings (and more) on This should be rule #1.
What you suggest will always cause a flame war. Many programmers don't need strict rules and one should accept it. I am using the goto statement which most people don't like. But I am confident enough to know *when* its use is improving style and readability. I am always trying to keep stuff readable because source code is written for humans. And things which increase verbosity like your suggestions of always using braces are actually harder to read for me, because they announce a block of following statements instead of a single one, which is immediately clear when you *don't * use braces. But like I say. Everyone has his favorite style and it's like handwriting. I tolerate all kinds of weirdnesses here, but it is a good idea to follow people who have experience and have proven that their development works good.
Please add a blankline between the text and the start of the code â˜º
Wow, thanks! &gt; I think it's better to use C99 until the major compilers all support C11. Fair point. I guess the operative word in that rule is "can". C11 is certainly usable for me in GCC and Clang. I don't know about other compilers. I added a note about this to that rule. &gt; Assume as much domain knowledge as is reasonable. No need to expand all TLAs every time. Abbreviations aren't acronyms. I think it's fair to assume reasonable acronyms for the situation, but I don't support trunc. of words in your cmnts. This is probably obvious, though, so I removed this rule. &gt; Don't write rules that everyone already knows. Too right. Removed. &gt; This does not jive with the way I read code at all. I've actually been meaning to specify that rule to only apply to comments in header files, because that's the only place I tend to be doing that at the moment. Still, I appreciate that it won't work for everyone. To each their own, I guess! &gt; Can I mow your lawn and get you a coffee while I'm at it? If you really want this, get a tool to do it. I can't be bothered to do computer work. Yeah. I know :/ I'm going to keep that rule there for as long as I keep doing it in my projects. &gt; I agree in principle, but know the requirements of your system. Any global variables should be static unless absolutely necessary. (ie: errno) Thanks; I forgot about the linkage level of global variables in that "minimize what you expose" rule you suggested. I added a mention of that. &gt; Warning: some const types don't convert nicely in C. Ie: char const * const * doesn't convert to char** By "nicely", I assume you mean it will throw warnings? I think that's a great thing: a large part of why I use `const`, in fact. &gt; Also, there's no point in consting basic variables in function signatures. Even if you do change them, they're just copies of the original values. Making local variables const isn't usually useful either. I mentioned that `const` for non-pointees in function prototypes is useless. But, for the actual argument lists and local variables, I think `const` helps your reader reason about what's going on, because it tells them what will change and what won't. For me, reading a piece of code that doesn't use `const`, working this out - what's changing where - is the main challenge. Also, `const` tends to encourage good practices, like array indexing over pointer arithmetic, or interfaces that support immutability. &gt; Needs an accompanying section called "Avoid signed types because signed overflow is undefined." ;) Aye - uses of unsigned values are mentioned at the end of that rule. &gt; Your sockaddr_in example would be more compelling without the cast and extravagent use of whitespace. Agreed - the cast wasn't necessary for the point I was making. The "extravagant whitespace" is my style, thankyou very much. &gt; Also, in the hungry example, the || should go at the end of the first line to indicate that the expression is continued on another line. I actually prefer to indent the boolean operator to a column of the preceding line. Leaving the operators at the end means that there's no consistency of which column they're placed in, making it harder to keep track of what's happening. See, for example, how I replaced [this switch](http://www.reddit.com/r/C_Programming/comments/1ni8oq/c_style_my_favorite_c_programming_practices/ccjgg1v) with a multi-line boolean expression. &gt; I've tried to read this a few times, and have a hard time parsing what you're saying. But I think I agree? If you declare a bunch of variables in a part of your code, do things with them, and after that part you only ever use one of those variables, then you should extract the part where you declare those variables to generate the one you use to a function to minimize the scope of those otherwise-unused variables. &gt; This is just repeating FUD about macros. Document what the macro does, use terminology that makes sense, and don't try to do too much magic all at once. That rule was actually saying the exact opposite until about 48 hours ago. I was convinced against it when I came back to a use of that `Trie_EACH` macro, and I realized I forgot what it was doing. I don't think a name would've helped. I stand by my opinion that control-structure-macros are harmful. &gt; Don't rely on asserts for control flow. A segmentation fault is a control flow? &gt; Typedeffing function pointers is immensely useful. I once took this as gospel, but now that I actually understand how to read and write function pointers, I'm not so sure. I didn't declare a function pointer type for my [trie mapping function](https://github.com/mcinglis/trie.c/blob/master/trie.c#L151), because I considered that it would only mask what was happening. It repeats the type twice in that source file, but that wasn't a huge trade-off for clarity. When would function pointer typedefs be useful, other than for avoiding the function pointer syntax? &gt; Sure. Also, define a final enum value (NUM_JSON_TYPES, or something) to be used as an array size to hold them. How would you define an array of those enum values without explicitly listing each, though? Then you could just use an incomplete type to infer the array size. &gt; Agree, except if struct A contains struct B, and struct C contains a pointer to struct B. If struct B has an associated cleanup function that expects to free itself, you'll run into trouble. I'd consider that the user's responsibility to ensure they're only freeing manual memory. The library should try to prevent this from happening (e.g. by copying), too. &gt; Looks nice, but also very fragile. I would definitely not make it a rule. What's fragile about it? &gt; I'm not really sure what you're saying here. It should say, "Always fully initialize your structs." This doesn't work well for structs with lots of optional fields, or with padding fields. Although it's harder on the library developer, I think it's simpler for users to have a `_new` function that takes the required parameters, and will zero whatever fields you don't mention. &gt; This should be rule #1. Yeah. I've tried to order it by how you'd approach writing a program. It probably would be best to order the rules by importance. Thanks for all your comments and fixes.
&gt;&gt; Warning: some const types don't convert nicely in C. Ie: char const * const * doesn't convert to char** &gt; By "nicely", I assume you mean it will throw warnings? I think that's a great thing: a large part of why I use const, in fact. Sorry, I meant the conversion to be in the other direction. `char**` should convert to `char const * const *`, but doesn't. (It does in C++ though...) &gt;&gt; This is just repeating FUD about macros. Document what the macro does, use terminology that makes sense, and don't try to do too much magic all at once. &gt; That rule was actually saying the exact opposite until about 48 hours ago. I was convinced against it when I came back to a use of that Trie_EACH macro, and I realized I forgot what it was doing. I don't think a name would've helped. I stand by my opinion that control-structure-macros are harmful. Well, it certainly doesn't work everywhere, but that doesn't mean it can't work anywhere. If it matters how you're iterating over your structure, then a one-size-fits-all macro probably isn't a good idea. &gt;&gt; Don't rely on asserts for control flow. &gt; A segmentation fault is a control flow? Sorry, this was meant to be an additional comment to add. Don't rely on asserts for the normal operation of your code. They should strictly be used to catch fatal situations so that you can fail fast. &gt;&gt; Typedeffing function pointers is immensely useful. &gt; I once took this as gospel, but now that I actually understand how to read and write function pointers, I'm not so sure. I didn't declare a function pointer type for my trie mapping function, because I considered that it would only mask what was happening. It repeats the type twice in that source file, but that wasn't a huge trade-off for clarity. I think it can be useful for indicating the purpose of the function pointer. ie: This is an event handler, rather than an IO callback. &gt;&gt; Sure. Also, define a final enum value (NUM_JSON_TYPES, or something) to be used as an array size to hold them. &gt; How would you define an array of those enum values without explicitly listing each, though? Then you could just use an incomplete type to infer the array size. `int foo[NUM_JSON_TYPES];` It might be part of a struct definition. It's also useful for doing a for loop over all of the values. The point is that it protects you from internal changes to the enum, like adding new values or reordering them. Obviously, it's not useful if you don't have consecutive values. &gt;&gt; Looks nice, but also very fragile. I would definitely not make it a rule. &gt; What's fragile about it? There's no warning if you accidentally use the same argument twice. There's no protection against using unnamed arguments. There's a hidden cast, which means that you're losing type checking. The errors will be weird in the context of a function call. 
&gt; Sorry, I meant the conversion to be in the other direction. char** should convert to char const * const *, but doesn't. (It does in C++ though...) Ah! Right. I added a note about that to the `const` rule. Thanks. &gt; Sorry, this was meant to be an additional comment to add. Don't rely on asserts for the normal operation of your code. They should strictly be used to catch fatal situations so that you can fail fast. Okay. I've extended the "don't use assertions for error reporting" part of that rule. &gt; It might be part of a struct definition. It's also useful for doing a for loop over all of the values. The point is that it protects you from internal changes to the enum, like adding new values or reordering them. Obviously, it's not useful if you don't have consecutive values. Okay, I'm convinced. [I added a rule](https://github.com/mcinglis/c-style#define-a-constant-for-the-size-of-every-enum) about this. To me, `#define`ing the size is more natural. My (subjective) reasons for that are mentioned in that rule. &gt; There's no protection against using unnamed arguments. Users could, in theory, use unnamed arguments. But, if you don't document their placement, and in your header files, mention that the field order should not be relied on, then the onus is on them, I feel. &gt; There's a hidden cast, which means that you're losing type checking. It's a compile-time error to specify a named argument that isn't a field of the options struct for that function (or a compatible type). I don't see how you lose type-checking? I concede that the named arguments construct isn't as hardened as it could be. But I think its benefits far outweigh its risks. When the alternative is [variadic functions](https://www.securecoding.cert.org/confluence/display/seccode/DCL11-C.+Understand+the+type+issues+associated+with+variadic+functions), or requiring callers to write arguments that are only useful in a minority of use cases, named arguments as suggested are really, really nice.
Yeah. I don't know how representative /r/c_programming is of C programmers in general, but it's been quite, um, enlightening to see the majority's reaction to something like this. "Having the book thrown at me" is a great way to put it - I feel like lots of the spite in this thread (and in /r/programming) is borne out of the practices instigated by *The C Programming Language*, which provided code like: while (--argc &gt; 0 &amp;&amp; (*++argv)[0] == '-') while (c = *++argv[0]) switch (c) { case 'x': except = 1; break; case 'n': number = 1; break; default: printf("find: illegal option %c\n", c); argc = 0; found = -1; break; } Thanks for your support: it means a lot to me.
&gt; #include &lt;stdio.h&gt; &gt; int main (int argc, char *argv[]) &gt; { &gt; for (int i=0, int j = 2; i &lt; 5; i++, j++) { &gt; printf("i = %d, j = %d\n", i, j); &gt; } &gt; return 0; &gt; } The first thing in the parentheses of a `for` loop must be a *statement*. What you have written: int i=0, int j = 2; is not a valid statement. If you want to express that with a valid statement, you must write it like this: int i = 0, j = 2; That statement declares two `int` variables called `i`&amp;nbsp;and&amp;nbsp;`j`, and initializes `i`&amp;nbsp;to&amp;nbsp;0 and `j`&amp;nbsp;to&amp;nbsp;2. So, using that statement in the `for` loop, you get this: for (int i = 0, j = 2; i &lt; 5; i++, j++) { /* ... */ }
ah yes thanks!! Only through your answer I realize that the parts separated by ; follow the normal rules of a statement. I somehow had gotten it in my head that the comma within a part of the for statements was something special. I will sleep better tonight :)
Only the first part is a statement. The other two parts are expressions. And actually that's not quite true either, because most statements are not allowed as the initialization of a `for` loop; only expression statements and declarations. The formal grammar for a `for` loop in&amp;nbsp;C99 gives two forms: &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;**for**&amp;nbsp;**(**&amp;nbsp;*expression\_opt*&amp;nbsp;**;**&amp;nbsp;*expression\_opt*&amp;nbsp;**;**&amp;nbsp;*expression\_opt*&amp;nbsp;**)**&amp;nbsp;*statement* &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;**for**&amp;nbsp;**(**&amp;nbsp;*declaration*&amp;nbsp;**;**&amp;nbsp;*expression\_opt*&amp;nbsp;**;**&amp;nbsp;*expression\_opt*&amp;nbsp;**)**&amp;nbsp;*statement*
FYI: Unless you've installed a real version of GCC, the gcc binary that comes with OS X is just a frontend for clang.
Sorry about that. Never used the code function before.
That is completely new to me. When I have this program (adjusted from above following zifyoips advice) #include &lt;stdio.h&gt; int main (int argc, char *argv[]) { for (int i=0, j = 2; i &lt; 5; i++, j++) { printf("i = %d, j = %d\n", i, j); } return 0; } this clang -Wall test.c -o testclang gives no error (and a working program), but this gcc -Wall test.c -o testgcc results in test.c: In function â€˜mainâ€™: test.c:5: error: â€˜forâ€™ loop initial declaration used outside C99 mode Which gives me the impression that gcc does things that clang does not do, i.e. that they are not pointing to the same clang binary. By the way, I have installed the command line tools of XCode, without which, as I understood it, there is no compiling from within the command line (Terminal) at all.
They're not pointing to the same binary, 'gcc' is a binary wrapper around clang which will make clang/LLVM behave as close to how gcc would with the same arguments. In this case the difference is in the -std= option, which with gcc defaults to gnu89 (which doesn't allow initial declarations in for loops), whereas when you invoke clang directly the default option is -std=c99 You can test the theory by running 'gcc' with no arguments. You should see the error message "clang: error: no input files". Also, the gcc manpage on OS X has this to say: In Apple's version of GCC, both cc and gcc are actually symbolic links to the llvm-gcc compiler. Similarly, c++ and g++ are links to llvm-g++.
thanks for the -std= explanation! About llvm/clang/gcc/llvm-gcc: you made me read up on it, really interesting stuff, I now know better how this works. But could there be a slight language confusion in your post? Stuff I found [on Stackoverflow](http://stackoverflow.com/a/5708732) and on the [page on llvmgcc](http://llvm.org/releases/2.9/docs/CommandGuide/html/llvmgcc.html) on llvm.org suggest to me that llvm-gcc is not a wrapper around clang. More that clang and llvm-gcc are each a separate compiler. Each with their own options/rules, and compiling to some LLVM intermediary format, which then is picked up by LLVM to be compiled to machine code. But, well, maybe you just meant that. Good to know that 'plain' gcc is not on my mac now. More [politically sensitive](http://techrights.org/2010/09/10/apple-may-block-gcc-contribs/) than I had suspected.
Definitely an interesting read. Overall I think these are very good rules. A few I might disagree with or argue, but that for a different post. I just wanted to congratulate you on listening and responding to all the feedback you're getting. Responding to arguments and questions, and changing your style guide based on them (in some cases even completely switching your position). It is (unfortunately) rare to see someone take such a positive approach to constructive criticism on the 'net. Only one comment on the actual guidelines for now, I'd add to the rules on using asserts a note that makes it clear that your code should never rely on assert being evaluated for correctness (i.e. your code should work if assertions are disabled). 
Acknowledging that performance is not always the most important thing. Traditionally a switch statement is compiled to a jump table, which can be much more efficient than a bunch of compares and boolean operators. This specific case, I'd hope any decent compiler would handle this particular case fine though. As mentioned in another comment, I think that avoiding switch should have be an exception for situations where you specifically want to use the fall through behavior more clean and straight-forward could.
The author has been responding to comments. Listening to arguements, and even on at least one occasion has switched his stance on a rule based on the feedback from this thread. While there are some rules I don't personally agree with, though they are mostly a matter of style. If you think many of the rules are 'bad style' you should provide an example or few and why you think they are bad. Based on what I've seen on this thread, you're likely to get a polite response which at least further explains the stance, and if you are correct that the rule is wrong/bad the author may even decide to change their guide on that particular one. Just saying you don't like it and that many of the rules are bad isn't helpful to anyone.
A lot has changed since the time of UNIX's original research and Plan 9. Especially in terms of software engineering and concerns about code-correctness's relevance to security. Over the years, we (as in researchers and programmers) have learned that there are a great many benefits to functional style programming. Both in terms of avoiding unwanted side-effects, and thread-safety. When you work in code-bases with thousands of threads running you'd be surprised how much pure-functions and immutable types help keep your sanity.
Interesting: hadn't thought of that as an example of fall-through behaviour (I think of it as multiple labels for the same target instead), but I guess you can look at it like that. However, real fall-through is *very* rare in my experience. However, switches are still very valuable for dispatching. Here's another example from my own code where I think the switch is appropriate: switch (*buf) { case 'Q': log_message("Shutdown command received"); shutdown_archiver(); ok = write_string(scon, "Shutdown\n"); break; case 'H': log_message("Temporary halt command received"); enable_buffer_write(fa_block_buffer, false); ok = write_string(scon, "Halted\n"); break; // quite a bit more of the same elided... default: ok = report_error(scon, client_name, "Unknown command"); break; } I know it's ... sort of ... the same as a chain of `if (*buf == 'Q') { ...} else if (*buf == 'H') { ... } ... else { ...}` -- but is that really better? Don't really think so, but maybe it's a style thing.
Can you show your code? It sounds to me like a crash if the cmd window is closing but I can't guess what from your description.
In general, don't use (`f`)`scanf` directly on an input stream; it's much cleaner and more easily error-checked if you read an entire line of characters (e.g., with `fgets`) and then use `sscanf` on that, because then you don't get the dangling input issues. If you want to stick with `scanf`, you can try putting a space at the beginning and end of your format specifier (e.g., `" %d "` instead of `"%d"`), which should cause it to eat the extra whitespace.
getchar() is picking up the enter (\n) you press after scanf(). .. to avoid this put another getchar();
I think the fastest way might be this: static int lookup_table[256] = { -1, -1, ... , 26, ... , 0, 1, 2, ... }; return lookup_table[c]; the "..." should be replaced by however many -1 values to line up the table properly
That snippet was taken from a part of a `find` program, that searches for a given pattern against `stdin`. See the whole program [here](https://gist.github.com/mcinglis/d790f5e2e07076273481). If I was writing an actual program that did that, I would provide something like a `FindOptions` struct to hold all the configuration values, and break the functions up to minimize the scope of variables, and to provide better reusability, testability, etc. However, the constraints of writing examples for a book are *very* different from writing actual programs. Most of the qualities of good software don't apply to good examples. Thus, I probably would've written a `getopt` function earlier in the book to parse options, and then used that throughout, including for this example. But, a translation of that K&amp;R snippet to something that: - doesn't depend on pointer arithmetic - doesn't use a `switch` - doesn't depend on implicit brackets - doesn't depend on obscure operator precedence rules (e.g. `++` vs `[]` vs `*`) - uses `bool` for boolean values - uses `const` where it can Is: for ( int i = 0; i &lt; argc; i += 1 ) { char const * const arg = argv[ i ]; if ( arg[ 0 ] == '-' ) { for ( int j = 1; arg[ j ] != '\0'; j += 1 ) { if ( arg[ j ] == 'v' ) { invert_match = true; } else if ( arg[ j ] == 'n' ) { show_line_num = true; } else { printf( "find: illegal option %c\n", arg[ j ] ); argc = 0; break; // a goto may be nicer here } } } else { break; } } But, like I say, I probably would've approached providing such an example differently, anyway. The four levels of indentation really bugs me here, and is a clear signal that something needs to be extracted to a method - parsing the options. Still, despite those problems, I find the above code so much easier to follow. Is that just me, though? What do you think? I'm genuinely curious.
If you have msdnaa you can get visual studio. It's for c++ but you code works in c. I love visual studio 2013 
Thanks for your support. I'm glad to hear you found this interesting. The hostility I received from lots of people in this thread and the one in /r/programming wasn't frustrating because of the insults or trivial criticisms, but because I really just wanted to have an interesting debate about the points I raised in this document. Thankfully, I did get to have a few interesting discussions, but it was certainly disappointing to see how many people took this the wrong way. I think it was partly my fault, though, including subjective rules and wording them too strongly. Since posting this, I've reworded the intro to provide a stronger disclaimer that this is really just a document for myself, and I only happen to be sharing it with the world in the hope that some parts of it are useful for some people. Still, I'm very thankful for all the corrections and arguments that have been raised by redditors. &gt; Only one comment on the actual guidelines for now, I'd add to the rules on using asserts a note that makes it clear that your code should never rely on assert being evaluated for correctness (i.e. your code should work if assertions are disabled). That's a really interesting angle on assertions that I haven't considered before. It's a very natural way to think about them. I've added such a note - thanks!
[Visual Studio 2013 supports most of C99](http://blogs.msdn.com/b/vcblog/archive/2013/07/19/c99-library-support-in-visual-studio-2013.aspx)
Yes, but isn't the result of the ternary always `true`?
No. Consider the case where count (or -count) is too large a value (or an invalid type) to be held in the operation.mt_count member.
I have to say I'm quite surprised you decided to stick with the `switch` code you provided originally. I figured you decided against it in favor of the boolean expression, which is why you didn't reply. It just goes to show we all think differently. For this other switch, it looks problematic for a few reasons. If calls to `log_message` and `write_string` are being repeated for every command value, but with different strings for each command, shouldn't you define functions to return those strings, and only call `log_message` and `write_string` once? This would be much more testable and reusable. char const c = *buf; log_message( log_message_for( c ) ); ok = run_command( c, scon, client_name ); bool run_command( char const c, FILE * const scon, BlockBuffer const fa_block_buffer, char const * const client_name ) { if ( c == 'Q' ) { shutdown_archiver(); } else if ( c == 'H' ) { enable_buffer_write( fa_block_buffer, false ); // ... } else { return report_error( scon, client_name, "Unknown command" ); } return write_string( scon, write_string_for( c ) ); } But, it will mean three sets of character comparisons to run each command. Also, it will break down as you need to use more variables in each of those `if` branches. I suspect that `switch` either came out of a huge function, or your code is very dependent on global variables. So, what you could do is define a command struct like: typedef struct Command { char character; char const * log_message; char const * write_string; void * data; bool action( void * data ); } And then iterate over an array of `Command`s rather than an array of characters. The `data` field could be used to hold variables that are needed to run the action for that command. You could also go the strongly-typed way and define different structs for each possible command, and then define `Command` as a union. This is probably what I'd investigate, because it would make defining and calling the action much easier. You'll still have to have a large `if`-`else`-`if` branch somewhere, but the goal is to make that as small and simple as possible. With a `union Command`, it should be as basic as comparing a `type` field, and calling a function as appropriate. Either way, I see this is as just another `switch` that's a code smell.
`fflush()` is to be used in **output** buffers, such `stdout`, `stderr`, and streams opened for write (or streams opened for read/write in which the last operation was not a read). If you use `fflush()` on input streams, like `stdint`, the behavior is undefined. Some compilers may implement that, but it's not guaranteed to work in every compiler/system. It will not work, for instance, in Linux. TL;DR: don't use `fflush(stdin)`; it's wrong.
No problem. It's a pretty common mistake among new C coders, especially those that come from dynamically typed languages (where assignments are all but guaranteed to be successful). 
I feel obligated to say that this is absolutely terrible code and you should not learn anything from this. The tar utility is terrible in itself. I once had to extract a single small file from a 80 GB archive on a slow hard disk. GNU Tar found it necessary to read the entire archive, so I quickly wrote a C program to seek to the correct place and extract the file I needed.
This is a very interesting use of the "," operator. Traditionally: if( i = 1, i == 1) This sets i = 1 and disregards the 1 that was returned by the assignment. Digression: i = j = 1; This works because the assignment "j = 1" returns whatever was assigned, in this case a 1, and assigns it to i. So in our code, i = 1 returns a 1, which is discarded because of the comma operator and i == 1 returns true, which is returned to be evaluated by the if statement. Thus, the if statement evaluates true. In this nifty implementation, there is a ternary operator within an if statement, so count &lt; 0 is not actually evaluated by the if statement, instead, it either executes the true part of the ternary statement or the false part. return i == 1 ? TRUE : FALSE; This code returns true if i is 1, and false otherwise. Note that the return is not considered part of the ternary statement. So basically what they do is multiple assignments within an if statement. Lets see why that would be useful. *There is a counter, and if the counter is greater than 5, we want to set i = 0 otherwise, we want to increment the counter. Now say we have a little more code we want to execute if the counter is greater than 5.* if(count &gt; 5 ? (i = 0, i == 0) : (count += 1, count &gt; 5)) { //other code that gets executed if count &gt; 5 } If count is 6, (i = 0, i ==0) is executed, setting i to 0 and returning true, since i is 0 (assigned in the previous step). So if count is 4, (count += 1, count &gt; 5) is executed, incrementing the value of count and returning false since count &gt; 5 is still not true Note that this is **NOT** what they did. They executed the if statement based on whether or not the value was successfully stored. This example is the coding equivalent of literary license. Once you understand the fundamentals, you can break them just to make other people cringe when they read your code. Edit: Formatting. Edit 2: Changed the part about the tar example always executing the if statement. Thanks to /u/AlpineCoder
How exactly did you seek to the correct point? FYI, TAR stands for Tape ARchive, i.e. [designed for sequential, not-random access](http://serverfault.com/questions/311842/is-there-a-quick-way-to-get-the-very-last-file-in-a-large-tar).
Each file is preceded by a short header specifying the file name and size. I just skipped over each file, until I got to the file with the name I wanted. Thousands and thousands of headers is still only a few megabytes to read. I scoured my archives and found the code: [The C file](http://pastebin.com/ffqT7j8r) and [the header](http://pastebin.com/9hHkXkUR). Be warned that this code is not my greatest work, and it's years old. I cannot vouch for it working well, it's just to show the principle of the thing.
Yeah, TAR isn't indexed like ZIP---it's intended primarily for usage with honest-to-goodness tapes, where a new version of a file could be written after the older version. That means that any time you want to extract a single file, it has to (a.) scan through to find that file in the first place, then (b.) scan through the rest to make sure there's no newer version later on.
God-fucking-damnit that option (if it works) could have saved me some time. Meh, at least I learned something.
As I said in another reply, I wrote a small tool to seek through the file, skipping file after file until it finds the file I'm looking for. By only reading the headers, it's effectively as fast as random access.
I'd expect that to depend very much on the number of files in the .tar vs. average file size. If you have a shit-ton of tiny little files, you're not going to get much performance boost (near linear-access); if you have mostly large files, you're going to get near-random-access speeds.
I did say performance wasn't everything and that for any decent compiler (gcc from 2.95 up would definitely fall into that category) it wouldn't make a difference. I certainly agree that in many cases (like your example) the switch is much easier to read than the if/else. If it was a bigger switch statement, the if/else would be horrible. I'd note, though if I look at the (switch version of) code out of context I might wonder, assuming '// A stuff' was non-empty, if the programmer had intended there to be a break there or not, but that is just a matter that of non-empty switch branches that fall-through should be commented as intentionally falling through. Switches were you want fall through or more than a very small number of things to have the same action (which includes the very common switch(char) do different actions for five different letters and default action for the rest) are certainly fine, and are generally much more readable than any alternative. The idea of switch's fall through behavior leads to errors is, however, a legit concern. Completely banning switches is certainly the wrong way to do it. If this were a language like, say java, one would have the compiler emit warning if switch branches don't have breaks, and provide an annotation for branches where you specifically don't want a break. I don't think that is anything could have a reasonable analog in C. A static analyzer that checks for branches without breaks would produce too many false positives unless one could come up with a good way to describe 'ok uses of non-breaking switch branches'. So I guess I agree with the style-guide that switches have this danger, but I think should just say don't use them. Requiring comments whenever you are intentionally excluding a break, except in cases like your RECORD_TYPE one above where it is extremely clear what you are doing. 
Thanks for your continuing discussion. Just in case you miss it, there's another comment thread still discussing `switch` [here](http://www.reddit.com/r/C_Programming/comments/1ni8oq/c_style_my_favorite_c_programming_practices/ccmprkr). &gt; I did say performance wasn't everything and that for any decent compiler (gcc from 2.95 up would definitely fall into that category) it wouldn't make a difference. Actually, `switch` *does* make a difference to `if`, but my point was that this is rarely important, and if it actually matters, there can be better solutions anyway. &gt; If it was a bigger switch statement, the if/else would be horrible. Any programmer experienced in object-oriented programming should be aware that a large conditional block, be it `if`s or a `switch`, is a code smell. Object-oriented languages can replace such conditionals with polymorphism via interfaces or inheritance. Now, C doesn't have those things, so it's not possible to actually remove the conditionals. But, with unions and other techniques, I posit that it's always possible to reduce each conditional block to a single line. In fact, this should always be the goal. See my suggestions in [this comment](http://www.reddit.com/r/C_Programming/comments/1ni8oq/c_style_my_favorite_c_programming_practices/ccofet3) and [this comment](http://www.reddit.com/r/C_Programming/comments/1ni8oq/c_style_my_favorite_c_programming_practices/ccoza1t). If each conditional block is just a single line, then I don't think a series of `if` is any worse than a `switch` block. While the comparison might be repeated, a `switch` will repeat `break`s. A `switch` with `n` cases will be `n` lines longer than the equivalent `if`, and about the same number of characters (when the subject being compared has a short name). That all said, as I've been making all these arguments against `switch`, I've realized that the main enemy is really just large conditionals, and not `switch` per se. It's still my opinion that `switch` is dangerous, complicated and verbose, and that an equivalent `if` is always clearer, even if (or especially if) you need the fall through. But, I'm considering adding another rule saying "prefer polymorphism or meta-programming to large conditionals". For example, I'm working on a strongly-typed JSON library (to be released in the next week, I hope - I'll post it on /r/c_programming), and I'm using this macro to eliminate repeated conditional blocks to call the appropriate function: #define CALL_JSON_FUNCTION( func, json, ... ) \ if ( json.type == JSON_TYPE_null ) { \ json_null_ ## func ( __VA_ARGS__ ); \ } else if ( json.type == JSON_TYPE_boolean ) { \ json_boolean_ ## func ( json.boolean, __VA_ARGS__ ); \ } else if ( json.type == JSON_TYPE_string ) { \ json_string_ ## func ( json.string, __VA_ARGS__ ); \ } else if ( json.type == JSON_TYPE_array ) { \ json_array_ ## func ( json.array, __VA_ARGS__ ); \ } else if ( json.type == JSON_TYPE_object ) { \ json_object_ ## func ( json.object, __VA_ARGS__ ); \ } else { \ assert( false ); \ } Then, I can write functions named like `json_null_print` and `json_object_print`, for each JSON type, and have a generic `json_print` function with the body of just `CALL_JSON_FUNCTION( print, json, file )`. I do understand the attraction of `switch`, in that it communicates that only a single value is being compared against a set of possible values. If C had the equivalent of [Haskell's `case`](http://learnyouahaskell.com/syntax-in-functions) or [Rust's `match`](http://static.rust-lang.org/doc/0.8/tutorial.html#pattern-matching), I use that in a heartbeat. But, in my opinion, `switch` as it is just isn't worth its drawbacks. Of course, if `switch` works better for the way you think, go for it!
Perhaps: return (0x1F &amp; c); If 0 for a space and 1-26 for the letters a-z is acceptable.
Short version: If you do float f = ...; int i = f; The behavior is undefined if i cannot represent the decimal part of f. 
Whenever I use scanf, which is rare after introductory type exercises, I try to always store whatever I'm scanning in a variable such as: int i; scanf("%d", &amp;i); You will fully understand what the &amp; sign does when you learn pointers. 
&gt; For some reason, your gist does not work properly. It created segfaults. So, it seems like there's now a `getline( char **, size_t, FILE * )` function in `stdio.h`, standardized in POSIX.1-2008. I didn't know about that, but I guess it's doing something weird when a different prototype is declared for `getline`. The code I provided in the gist doesn't actually implement the `getline` function (like the example in the book). You'd need to link in another object file to get the executable. Without that, and renaming `getline` to `krgetline`, you should get a linking error like `original.c:(.text+0x16e): undefined reference to 'krgetline'`. If I implement `krgetline` with just `return 0;`, I don't get any segmentation faults. &gt; I was unable to locate this in the K&amp;R 2nd edition, as well. If you could cite the page, I'd like to look over it to see if maybe a typo or two was made in your gist. Page 117. I only copied the code out of the book verbatim to provide context about the snippet I corrected. I wasn't intending to deliver a working `find` program. &gt; I learned that in dealing with pointers to arrays, it's helpful to use both the pointer arithmetic (to advance to the next array) and array indexing (to work with individual characters) together in order to ease mental overhead. I think if you're dereferencing elements from *any* array, you should treat that array as an array, not a pointer. This should apply for arrays of characters or arrays of arrays of arrays, because they're all just arrays. &gt; I chose to go solely with pointers to see if I could do it Hah. Like this document, I've been arguing for the merits of *good code*, not what we can or can't do. I guess we misunderstood each other. I encourage you to attempt solutions to practice concepts you want to practice. That's how I learned - and still am! Pointers and pointer arithmetic is really quite tricky, I think, so it's certainly worthwhile to get your head around it. But I won't judge or critique your solution, because it's been written for an entirely different purpose than what I thought we were debating, and for what I wrote my code for, and was interested in. I will point out, however, that your solution won't parse the `x` option when the program's invoked like `find -nx pattern`, as K&amp;R's code does, or my snippet does.
The internet is loaded with tons of open source C code. You'll learn more if you go read some other simple code and figure out how it works. 
Thank you, I've been looking through examples on programmingsimplified.com but I can't really find anything to help me understand this particular issue clearly.
You can't define a function inside another function.
The samples there aren't very good. Try this: http://c.happycodings.com/code_snippets/define-function-and-use-it.html 
In C, you can't define functions inside of another functions. Apart from that, the way you wrote sum() doesn't make much sense: It always returns 0 and does not take any parameters. While there _may be_ some real-world scenarios where such a function would be fine (i. e. a function manipulation global variables -- but that's not very clean), it's useless here. Here are some hints: The function needs to take two parameters (you certainly know how to do that, otherwise you wouldn't be given such an assignment). It also needs to return a value. You can use that value in your program.
But if I make a new function it forgets what a and b are. I know these questions must sound very stupid but I have only had 3 classes and Im finding it very difficult to grasp, thank you for responding.
Thank you, I will have a look when I get home.
&gt;you can't define functions inside of another functions How can I make the program remember the values of a and b when I crate a new separate function? I tried const int a = 10, b = 5 but it didn't work like I thought it would from reading my lecture notes. I probably should know more about parameters and how to phrase function but I have only had 3 classes and I missed one due to a doctor's visit so I'm finding it hard to figure all of this out from the lecture notes. So basically what I'm doing here is trying to figure it out by modifying the hello world program I made in the last class. Thank you for responding and for your help, it is greatly appreciated.
Okay, so you can pass parameters to functions: int my_function(int parameter_one, int parameter_two) { /* You can use parameter_one and parameter_two here */ return 42; /* You need to return an integer (see function declaration). */ } int main(void) { printf("my_function(1, 2) returns %d\n", my_function(1, 2)); /* More code... */ } Does this help?
You have to pass them to the function as parameters (also called arguments). It's a very common concept in almost every programming language.
I am a novice, but I figured it out. I don't want to post how I wrote it because I want you to think about it and learn for yourself. Here are my clues as to how I did it, and if I am wrong I hope somebody points out my errors. The variable c should get it's value by calling the sum() function. The sum() function should expect two parameters when called, and should return a value. Take a close look at Tblue's sample. One final hint. You might need to create another few variables somewhere other than the main() function.
Thank you, I think I'm starting to get it. Thank you very much. 
Great! (I didn't want to provide a complete solution because I think you should actually _learn_ something. :-) You really only need to modify the code above slightly now...)
Thanks and I appreciate that, I didn't come to reddit to get someone to do my assignment but it can be wry hard to get code help from google explained like a human. Thank you :)
Sure! You're welcome. :)
`s/decimal/integral/` Talking about the "decimal" part of floating-point/real numbers is confusing.
I'm not sure exactly which part you're stumped on, so here's a few links to relevant topics that I think may answer your question: * [How to read and write a file in C](http://www.cprogramming.com/tutorial/cfileio.html) * [How to handle command line arguments in C](http://www.cprogramming.com/tutorial/c/lesson14.html) * [How to redirect stdin and stdout in bash](http://tldp.org/HOWTO/Bash-Prog-Intro-HOWTO-3.html) I think that either (a combination of 1 and 2) or (3 by itself) should solve your problem. I don't know what exactly you mean by "read the file off the command line". If you mean "read the file name off the command line", you want 1 and 2, if you want to send the file to your program from the command line, you want 3. Let me know if you need more help.
How the hell did you manage to get this to compile?
Thanks I'm totally new to this so my phrasing is noobish. Im sure its the combination of 1 and 2. What i meant was our program should run like this {vm01} reversecopy filename reversedfile the reversecopy being the program and filename being the file to reverse. and reversedfile will be the created file that was reversed. Also a further question about argc, argv[]. I understand that argc is the count and argv is a character array but what does that array contain? edit: nevermind i figured it out. 
Also im not supposed to use fopen and fread on the program. but from what i know its okay to use just open() and read() in there place right?
Op How did you get the compiler to compile that program? you have a function definition inside the main function and no function prototype. Also Hint ** that return 0 inside that function wont help. 
Get which to compile? The second one? I was using a website called compileonline, and, I don't know it just did when I hit Compile and Execute
&gt; Get which to compile? The second one? I was using a website called compileonline, and, I don't know it just did when I hit Compile and Execute 
Well, it should work in general, but see [this](http://stackoverflow.com/questions/1658476/c-fopen-vs-open) for some considerations about using open rather than fopen. The thing that would concern me is wondering whether the requirement to not use fopen and fread is supposed to also cover open and read. But if you're sure it's not, I guess it's fine.
thanks im sure the only ones I can use for it are open, read, write, lseek, and close. Also one more question if you dont mind, how do i read a file and copy its contents if i dont know the exact size of the file?
First learning about function parameters....ah, the nostalgia of being completely green
A few points: * This isn't an error persay, but it's something that relates directly to my field of work, and no one does it right, so maybe I can get you started on the right foot for habits. If you're only going to read a file, you shouldn't open it read/write, you should just open it for reading, and for writing, you should only open for writing. The security term is "least privilege". If you were writing production code, security policy may want to restrict you to only reading from file1, but you're requesting write access which you don't need which would cause an unnecessary error if you don't have write permissions. * I'm pretty sure you need to seek back to the beginning before reading. Otherwise you start at the end * If there's more than size to read, that's an error. If you just keep going and somehow you hit the while loop twice (this shouldn't happen) then you'd reverse chunks of the file rather than the whole thing. * Read moves where you are in the file. So you don't need to call lseek to move, and if you call read twice the second one will have you farther in the file. You're on the right track. I actually won't be able to respond again until Sunday at the earliest, so you're probably on your own for the rest of this. Good luck!
You were great help Thank You so much much! sorry if i was being a bother. Have a great weekend!:)
This will never work correctly, since you are forcing `argc` to be `2` but you access `argv[2]` -- which requires `argc &gt;= 3`... First char *file2[size]; This probably doesn't do what you expect. This will declare an array with `size` pointers to `char`. Chat you probably want is simply char file2[size]; Or, better: char file2 = malloc(size); /* you do the appropriate tests after */ This *file2[i]=*read(count, argv[1], size); Makes absolutely no sense to me. Why are you reading it again since you have just read the entire file in the `while ()` conditional test? And why are you trying to really in a loop if you requested the whole file to be read at once? As to the error you are getting, that's because you are trying to deference a `ssize_t` expression. But why? You should revamp that code. Check the definition of `write()` and `read()` and start over. Start with something simpler, like copying the file ipsis litteris and dumping it somewhere else, just to see if you understand these functions. Then you try to reverse the file.
&gt; You should revamp that code. No, OP should delete it and start from scratch. This is beyond wrong.
I was trying to read each char in the file and copy it backwards in another file. The project i am working on is reverse copy using read, write, open. and this was my first time used read and write. when i read into it it didnt make much sense. However I did fix it so it is read(file1, file1[size], 0) however i am having a trouble understanding what the 0 does in this case.
get a compiler for your computer. They are free. code block , dev c++ or pelles c will compile c programs.
I'm using xcode mostly, or codeblocks and the text editor on Linux
There's a whooooole buncha stuff wrong in this. - *Always* compile with the `-Wall` flag when you're doing stuff like this. That'll tell you when you do stupid things that C allows, like forgetting to return a value from `main` except in the one place. - For error conditions (e.g., `argc != 3`) you should really just `fprintf` to `stderr` (`stdout` should be reserved for normal output only!) and `return 1` immediately. That prevents you from having to nest your entire program inside an `else` condition. Errors, exceptions, and other unexpected stuff should be dealt with quickly and gotten out of the way so that code that's going to be executed under normal circumstances is presented as soon and with as little unnecessary framing as possible. - `size`, `count`, and `i` really, really, *really* should not be `int`s. `int`s are signed, and may be much smaller than the maximum size of both files and data structures. The types `size_t` and `ssize_t` are what you use for structures in memory; `size_t` is usually unsigned (technically doesn't have to be), and `ssize_t` is the guaranteed-signed version of it. The type `off_t` is what `lseek` returns, and it may be an entirely different type from both of thise. For example: - On a 32-bit system, `int` and `size_t` will both be 32-bit, and `size_t` will almost always be the same as `unsigned int`. `off_t` could be 32-bit (for a maximum file size of 2GiB) or 64-bit. - On a 64-bit system, `int` is 32-bit, but `size_t` is 64-bit, and usually identical to `unsigned long` or `unsigned long long`. `off_t` is usually 64-bit. - Another fun possibility---of less importance for consideration here but important nonetheless---is that `lseek` returns the maximum value for an `off_t`, and the `+1` overflows that to 0 or a negative value. - Your variable names leave much to be desired. The names `file1` and `file2` obscure what you're actually doing. `file1` would probably be better named `fd` or just `file`---it's the file descriptor for the file you're modifying. `file2` has nothing to do with file descriptors or files, and only holds the data from the file, so it'd probably be best named `data` or something along those lines. `count` is both the number of bytes remaining and the upper offset, and `i` is the lower offset; either `remain` and `offs`, or `j` and `i` would be better. - When you say `char file2[size];`, what you're doing is allocating `size` bytes on the stack. (The stack is where stuff like local variables, functions' return addresses, and some function arguments are stored. It's also used as scratch space.) The stack expands downwards, and the OS generally keeps a few pages ready beneath the bottom of the stack so that if it grows by leaps and bounds, it'll do so without killing the program. However, if you allocate too much memory you'll run right out of the area allowed for the stack and crash the program. *Never* allocate more than 4-8 KiB on the stack, 16 if you're feeling adventurous. /u/im-human-AMA noted that you should probably use `malloc`, although that code wouldn't work; what you actually want is something like this: // Before doing this, you probably want to check if size &lt; 2, // and if so you should just return because there's nothing to reverse. char *data = (char *)malloc(size); if(!data) { // malloc doesn't always have to succeed; when it doesn't, it // returns NULL. fputs("error: insufficient memory\n", stderr); return 2; } If this needs to work for bigger files than would fit in memory, you may want/need to figure out how to reverse it without keeping the entire thing in memory. (It is possible.) - Why are you `read`ing and `write`ing like that? You really want to read in the file in one big step, since you're going to have to by the end anyway, then reverse it in memory, and then `write` it out in one big step. (This also ensures that you have an all-or-nothing approach with regard to I/O; if an error happens while reading, you haven't glitched the file at all.) If you keep calling `read`, `write`, and `lseek` like that you're going to slow your program down by an immense factor, because each one of those requires a rather involved call into the OS kernel. How to read something in its entirety: remain = count; offs = 0; while(remain) { ssize_t k = read(fd, data + offs, remain); if(k &lt; 0) { // You may want to use errno for this; #include &lt;errno.h&gt; and &lt;string.h&gt;, // set errno=0 before calling read, and use strerror(errno) to print out // the error returned from `read`. fputs("error: unable to read from file\n", stderr); return 3; } remain -= k; offs += k; } Writing something in its entirety is very similar. - Return 0 from `main` if it completes successfully. If `main` completes without hitting a `return` statement, it will return garbage, and anything calling your program will think it encountered errors if it doesn't return 0.
Thank you for your advice it is greatly appreciated. So i made some of the changes you suggested however i have never used malloc before and put it in exactly as you stated and i got an error "warning: incompatible implicit declaration of built-in function â€˜mallocâ€™ what does this mean? Also how would i go about declaring the ssize_t to the lseek off_t? 
ok thanks i will figure it out you already helped me enough. 
It means you need to `#include &lt;stdlib.h&gt;`, which includes something like the line void *malloc(size_t size); (that's called a "prototype") which tells the C compiler what kind of thing `malloc` is. `ssize_t` itself is a `typedef` that should be defined by `&lt;unistd.h&gt;`. If you want to declare a variable of type `ssize_t`, you just throw it down like you would any other declaration, and like I did with variable `k` in the reading-in-entirety example: ssize_t variable; Or maybe I'm not understanding what you're asking about `lseek`, `ssize_t`, and `off_t`.
This is outside the topic, but I *really* dislike this style of page layout. Besides wasting screen space, it's broken because makes the page unreadable with page-scrolling. Try hitting pgdown or space and watch the browser skip over a chunk of text. I don't want to have to constantly correct the overscroll as I read the page.
clang, on the other hand, has no support for them.
Usually ends up either being the only function in the file (do a bit of setup in the main and call into the main loop), or it's at the end of the file. What typically happens is I write code in order to not have to predeclare any internal functions (except for when it's not possible, like two functions calling each other), and when the main() function depends on something and it's the entry point, well, it gets bumped to the bottom organically.
This is so perfect, thank you
Everything you won't use outside the current module should be declared static, so it is restricted to the module. If you don't, it will be exported by implicit extern keyword. I don't use static inside functions very often (this has got a completely different semantics). Why I use static? Because I instantly know that the function is private (not exported to other modules) and I don't need to care about it in other modules. Yes I mean a mechanism like ctags, for example. It is easy to use and you can follow the call graph very easily.
1) A module is the file that is a result of a compilation (usually .o or .obj). I am saying "module" because static/extern are keywords and terms that refer to the linker and not to the compiler. You can easily #include a second c file into the current module. For simplicity everyone assumes that one function is in one source file, yes. I don't include function definitions in a file usually. 2) Yes. Even when a declaration is missing (you will get a warning), you can still link against a variable or function without the static keyword. static is a mechanism to make a function or variable global to the module and not to the whole application. static greatly reduces your responsibility to look through the entire source base, when you modify the behavior concerning the variable or function.
There are many sources where you can read about managing a project in C. You can even look at OpenSource projects or simply in Wikipedia. Or buy a book. You should understand the concept of a compiler and linker. We are a bit off-topic here at the moment. Some things which I have talked about here go even further than C. Limiting the scope is always a good thing but is a general property in all modern languages.
If you ctags your whole project, you can ctrl-] on any function and vim will jump to it's definition. Then you read it, and hit ctrl-o to get back to where you were. 
Separate: http://ctags.sourceforge.net/ Install it and run `ctags -R` in your projects root directory. Vim does the rest. 
Sorry on nitpicking, it's called translation unit, not module.
I always write main first, often adding in the empty brackets of functions I'm going to write later as I go. I leave main at the bottom though.
I try to have dependencies before dependants. Since `main` doesn't have any dependants, it goes last. The rest of the functions in the same file go before `main` are almost always `static`.
This is often how I write all my programs, either in C, C++, etc. it's just easier to maintain later on. To compile multiple files you can use either a Makefile, which I prefer, or the command line arguments for the C compiler supports multiple .c files. 
It's obvious he's got a long way to go... he needs more than an answer to this one question. If you have to come to reddit to ask a basic question like this, you need to break yourself of that habit and learn how to learn. 
ha, that's the same reason I put commonly used things like `main` at the bottom. I wonder how much my choice in tools, like Vim, affects my code structure.
I stand by the answer. BusyPedro will learn more quickly by learning how to read code than by being told what to write. Also, I did point him in another direction, just below, which you didn't seem to have seen. 
Isn't [this](http://www.reddit.com/r/C_Programming/comments/1oa19v/do_you_write_your_programs_in_development_order/ccq5vvw) ironic. 
He probably means that the string struct would be something like struct String { int len; char *ptr; size_t size; }; `len` is the number of meaningful `char` stored in the "buffer" (the allocated array). `size` is the maximum number of `char` you can store in that buffer. Since you now don't have to store '\0', you have one more byte available. The typedef could be something like typedef struct String string_t; And now you can substitute `string_t for `struct String` everywhere in your code.
I wasn't rummaging through post history. I actually read a lot of /r/C_Programming. It's not a condescending post to tell someone that the question they are asking is basic enough that they can, and should, find the answer readily themselves. It's *also* very easy to find a list of resources that are highly recommended by people working in the field. If you can't do those basic things, my telling you the answer is of no use. It's not a lazy answer. A lazy (and ill-considered answer) is to just tell someone "do X". 
It is GNU tar though, so of course it will depend on gcc's behaviour.
Or you could have used something like dar, which is pretty similar to tar but designed for Disk ARchiving, but among other things have an index and the forat itself supports extended attrributes unlike tar which has several mutually incompatible extensions for fileattributes.
Did not know about that, thanks!
At bottom. My "main" is usually about 5 lines, so it's not really meaningful. There are really only two types of applications: 1) a procedural one which parses some input (command line + whatever), performs an operation and exits, 2) a long-lived one that enters an event loop, which doesn't exit until a particular signal is received or is given a particular input (or detects an unrecoverable error condition). I re-use the same "main" a lot. 
/u/rjw57 hit the nail on the head, but I'll expand on it with some more explanation. When you call `return` you'll exit the current function, and consequently return the value provided. When you exit the `main` function your entire program will exit. As a result everything after the first `return` statement in `main` is unreachable. It reads like this to the computer: int main(int argc, const char * argv[]) { apple(1000); return 0; // We're done here... } Try answering /u/rjw57 's question about what your thinking was about adding `return 0;` after every call, then try removing all but the last one.
Clearly my lack of understanding. I see now I need to remove them and call it once at the end. Thanks!
Thats a good idea. I wish I knew that in my first year of uni.
Some general advise... When calling printf(), make sure you have "\n" on the end, or else it wont print a new line. Ex: printf("This Is Correct\n"); There's a bug in your google() function. You should be passing "googlereturn" to printf(), not googleinvest. Once you start to learn more about C, structs will make your life much easier. Example: typedef struct StockListing { char *company; double share_value; } StockListing; StockListing stocks[] = { {"Apple", 45.82}, {"Akamai", 12.67}, {"Red Hat", 10.46} }; void print_stock_value(StockListing stock, int num_shares) { printf("Your %s investment is $%.2f\n", stock.company, stock.share_value * num_shares); } int main(int argc, char **argv) { int i; int n_stocks = sizeof(stocks) / sizeof(stocks[0]); for (i = 0; i &lt; n_stocks; i++) { print_stock_value(stocks[i], 1000); } return 0; }
If you are just learning C may I recommend writing in a Linux environment? It's a lot easier and much more enjoyable and something I seriously think you should consider. A compiler and text editor usually comes installed and ready to go on any Linux flavor. If going down this path I would recommend the K&amp;R book, "The C Programming Language" as it is written with Unix like systems in mind and will introduce you to both C and the Unix way.
Wikipedia has a discussion of this problem: http://en.wikipedia.org/wiki/Tower_of_Hanoi#Recursive_solution And there's a discussion of this exact problem at this reddit discussion: http://www.reddit.com/r/C_Programming/comments/1om3z9/ 
your link sends me to this thread?
http://www.jargondb.org/glossary/recursion
Please tell me this was an intentional example of recursion and not a simple typo!
Because that's what recursion is
Code::Blocks + OllyDBG.
This won't help with homework. I just got inspired. It's Friday... int f_add(int a, int b) { return(a + b); } int f_sub(int a, int b) { return(a - b); } int f_mul(int a, int b) { return(a * b); } int f_mod(int a, int b) { return(a % b); } int f_equ(int a, int b) { return(a == b); } int f_gth(int a, int b) { return(a &gt; b); } int f_lte(int a, int b) { return(a &lt;= b); } int f_and(int a, int b) { return(a &amp;&amp; b); } int f_ppA(int a, int b) { return(++a); } int f_Bpp(int a, int b) { return(b++); } int f_Ban(int a, int b) { return(a &amp; b); } int f_Bor(int a, int b) { return(a | b); } int f_Bsl(int a, int b) { return(a &lt;&lt; 1); } int f_Bsr(int a, int b) { return(b &gt;&gt; 1); } int (*arr[])() = { f_add, f_sub, f_mul, f_mod, f_equ, f_gth, f_lte, f_and, f_ppA, f_Bpp, f_Ban, f_Bor, f_Bsl, f_Bsr, (int (*)()) 0 }; int main() { int a, b, m, i; a = 7; b = 9; for (i = 0; arr[i]; i++) { m = arr[i](a, b); printf("m = %d\n", m); } }
Very clever. I'm grinning from ear to ear... I'm thinking of a tree...
I know this thread is getting elderly, but are you still working on/do you still need help on this?
You probably don't have the definition of printf. It's compiled into an object file you already have that you link against (libc). If you wanted to see the source for it, you'd probably want to google for it. printf is also an exception, since it uses varargs (or, a variable number of arguments can be passed to it), which may also be confusing you. 
You can browse the GNU C Standard Library source code here: http://sourceware.org/git/?p=glibc.git;a=tree For example, the source code to vfprintf is here: http://sourceware.org/git/?p=glibc.git;a=blob;f=stdio-common/vfprintf.c
Thanks. That makes sense. I downloaded the source code to glibc and it has the code to printf written in C. I was able to find some of the code for the format parsing. That makes the standard library seem much more approachable and less mysterious.
Why do so many identifiers in the header files begin with two underscores? Is it simply to avoid naming collisions/conflicts? Edit: grammar
It should be noted that none of the functions you've given are portable, and they will only work on systems that use ASCII. (Granted, ASCII is a given for most modern systems, but it should be noted that none of the solutions you've given technically satisfy the problem as far as the standard is concerned.)
So if I include stdio.h in my source and make a function call to printf(), does the compiler look for libgcc.a (I made that up but I'm interested in what its real name may be) during compilation and link it into my program? Will my executable contain unused functions from stdio.h such as puts()?