Indeed! Like I said in a post above, the blend between the more software engineering mindset (where my goals would be maintability, reducing the impact of change, few dependecies / little couple, using relevant design patterns, and most important writing *understandable* code) and the more "CS" mindset (where my goals would be to optimize the *heck* out of everything) is very interesting. By default, whenever I program, I'm almost always in the software engineering mindset, and trying to write clean and maintable. I only switch over to the more optimization focused mindset when I have to -- and to be honest, it's a rather seldom occasion! Gotta love the years and years of engineering that have gone into compilers and interpreters than can let me write possibly bad code that still runs hella fast. It's really a blessing, haha, having such high level language like Java or Python or JS or what have you. And that's such an interesting job! I'm also in "Intro to Digital Logic" course, where we start with gates and build all sorts of circuits, like adders and what have you. It's very interesting! And it's rewarding too -- I feel like I'm *finally* beginning to grasp how a computer works (well correction: I have a semi ok notion of it, lol). We even had a lecture or two that went even further down to the transistor level! So now I can describe how we can go from transistors to gates to things like flip-flops for memory and adders for an ALU in the CPU. It's so neat! And a big takeaway I've been getting from the class (that maybe will resonate with you, I don't know) is, perhaps ironically, that you *really* don't need a computer for everything. Gates fundamentally let you implement logic, right? So if you want to say "If A is on and B is on and C is off, then light wire D" then you do NOT need a comptuter -- you need some gates (and, or, not, etc)! And moreover, I would actually argue that you should *not* use a computer -- that the gates solution is better! For starters, it's obviously cheaper -- designing some circuit and then getting an IC of it made is (I think, I don't know precisely) around the order of, what, cents? Then you could just slap your IC on a breadpoard or PCB or something and you're good to go -- you're logic is implemented! But if you did this with say, a RaspberryPi, then that's at least gonna be like $10 or something. But moreover, the bigger reason in my mind to go the gate / more electrical engineering route is *maintability*. Like, if your little circuit-in-an-IC fails, then what's there to debug? A few gates? Easy. I mean heck for this application, it's literally two and gates -- if it's not working, just replace the AND gates! But if you went the computer route... well heck, there's so much more that could be broken! Maybe the rPi isn't working properly; maybe your GPIO pins are shot; maybe the program you wrote is wrong; etc etc. So not only is the more ee route cheaper, but it's more maintainable. To me at least, you'd jump up to computer level (with maybe an rPI or Arduino) when you either a) need to do a lot of arithmetic computation (because, whilst I might be able to implement an ok adder, I'm sure what the CompEng wizards have made is gonna be much better) or b) just have a lot of logic to implement. But if you don't need any of that, to be honest, for a simple case like above, all you need is gates! To me that's been a real neat takeaway. Thanks for the reply!
Hey, thanks! I did think it was very interesting indeed. Obviously I think in most day to day to cases, since you aren't writing your own sort routine, it's probably not the most important thing to remember, haha. But it is certainly a really enlightening thing to study! At least to me, my DS&amp;A class just feels like a lot of super interesting puzzles, and you're just asked to solve the puzzle (and probably analyze your solution's runtime too). Like on the last exam I had, we literally had a question that said "Design an algorithm that returns the highest count in the array. In other words, find the element with the highest count in a given array -- but note that you need not actually return the element. Just return that maximal count. Then, analyze the runtime of your algorithm using order notation. All other things considered, the more efficient your solution, the higher your grade." Now, that's a cool problem in my opinion! It's not easy, and I certainly had to battle with it some, but it's just such a neat problem. All these puzzles that we learn in class are just very thought provoking. I feel like, if you could actually master all this material (which I'm gonna say is *pretty* difficult, considering our textbook is 1300 pages and it's called *INTRO* to Algorithms, haha), then you'd really be a computational wizard. It's just such a neat field -- and when you bring it together with hardware, I feel it gives you such a richer view of programming.
This may be the funniest nerd ass shit I’ve read in a while. Thank you, lol.
Maybe [the OpenBSD source](https://cvsweb.openbsd.org/src/), and in particular, [the programs in `bin/`](https://cvsweb.openbsd.org/src/bin/)?
This explanation resembles the way it is taught [by Richard Buckland in this video](https://www.youtube.com/watch?v=Rxvv9krECNw).
How are binaries built to use your library accessing its internals? Are you providing a header file that can be included in the source code, or are your functions supposed to be looked up by symbol?
I'd recommend the same, here's [OpenBSD's style Guide](https://man.openbsd.org/style) , if you get confused by some decisions &amp;#x200B; &amp;#x200B; &amp;#x200B;
Right now I have two header files, one for the functions I want to keep internal, and another for the API functions that I want the user to be able to use. I have included the header for the internal functions within the .c file for the external functions. Also, this is what my makefile looks like. I'm not sure if this is relevant or not. main: main.o libparser.so $(CC) $(CFLAGS) $(LDFLAGS) -o main main.o -lparser libparser.so: parser.o parserInternal.o $(CC) -shared -o bin/libparser.so bin/parser.o bin/parserInternal.o parser.o: parser.c parser.h $(CC) $(CFLAGS) -c -fpic -o bin/parser.o src/parser.c parserInternal.o: parserInternal.c parserInternal.h $(CC) $(CFLAGS) -c -o bin/parserInternal.o src/parserInternal.c 
Actually /uk_x90 ‘s question was the answer itself, if you think a about it.
So, provide the header for the API functions, and then programs that use that header will only link against those functions.
Thank you so much for the help - I'm sorry if I'm misunderstanding you, I'm still relatively new to C. As of right now I am only including the API header in my main.c, however when I try and call one of the internal functions from within main.c it still works. Granted, it does give me an implicit declaration warning. Would the fact that I can still call these functions be because I included the internal functions header in the API .c file? Or am I not supposed to be including the internal .o file when I create the .so file in my makefile?
redis
To be able to mkdir /home/files/disk/disk1.disk, you first need to mkdir /home/files/disk. 
Github
You can call the functions from your main because you linked it against them by using the header for those functions. If you don't use the header, the linkage doesn't occur.
In short: The value of an int is an integer. The value of a pointer is an address. &amp;#x200B; If you point on a address, you get the value of that address, but you have to tell the programm how to read that address. &amp;#x200B; Example: int a = 1; int *b; // int *b = &amp;a would work, too, but the next line is less confusing b = &amp;a; // sets the value of b to the address of a to get their values: printf("a: %i , *b: %i\n", a, *b); Should respond: a: 1 , *b: 1 &amp;#x200B; Now the more confusing use: int a = 1; void *b; // void *b = &amp;a would work, too, but the next line is less confusing b = &amp;a; // sets the value of b to the address of a to get their values: printf("a: %i , *((int *) b): %i\n", a, *((int *) b)); Should respond: a: 1 , *((int *) b): 1 If you think about it, it's really "simple". You have to tell the computer, that b is an address with an int, that's why you have to use (int \*) before it, now you have to tell it to read that address: \*((int \*) b) &amp;#x200B; Proof: #include &lt;stdio.h&gt; int main() { int a = 1; void *b; b = &amp;a; printf( "Adresses:\n" "&amp;a: %p\n" " b: %p\n" "(int *) b: %p\n", &amp;a, b, (int *) b); return 0; } Output: Adresses: &amp;a: 0x7f7fffff262c b: 0x7f7fffff262c (int *) b: 0x7f7fffff262c
You could #include the file with the non-api files in the file with the api dwclarations, and declare the functions static. The organization wouldn't be too much of a headache with only two source modules.
sqlite
So that's what I thought at first, I tried doing that but when I declare the functions in the internal parser .c and .h files as static, include the internal .h file in the API .c file, and try to call an internal function in the API .c file I get a linker error saying undefined symbols.
Just code an example: #include &lt;stdio.h&gt; #include &lt;stdlib.h&gt; int main() { int a; int *b; for (int i = 0; i != 8; i++) { a = rand(); b = &amp;a; printf("*b: %i\n", *b); // Next line is really stupid :) if ((b = NULL) == NULL) printf(" b: %p\n", b); } return 0; } Output: *b: 354196995 b: 0x0 *b: 537893876 b: 0x0 *b: 465484328 b: 0x0 *b: 2128323871 b: 0x0 *b: 1964551873 b: 0x0 *b: 1799633540 b: 0x0 *b: 190875645 b: 0x0 *b: 146132114 b: 0x0 &amp;#x200B;
I meant declaring only the functions in parserInternal.c as static, but not the functions in parser.c... They could be declared as extern, but otherwise I'm not really certain why the public functions (extern should be their storage class by default) in parser.c wouldn't be visible... It's a little hard to read the make rules above. And I'm not certain of the linker's characteristics. This may be ot, but to really hide private functions, it's often necessary to declare an opaque type that supports your calling convention, and use some sort of IPC to communicate with private functions in a different process space. 
Linux
redis and sqlite are two codebases that are small enough for someone to realistically read and are both considered high quality and well written. don't look at an OS because OSes tend to be messy (a lot of different people contributing) and are pretty difficult to understand. 
There's an example which describes the computer's memory as a very long string of mailboxes. When you declare a variable, the program or compiler, depending, assigns its storage to one of the mailboxes. If you just use the name of a variable, it refers to the mailbox. If you precede the variable name with '*' it refers to the contents of the mailbox. If you precede the variable name with '&amp;' it refers to the address on the outside of the mailbox. This is true of any variable; whether the contents of any of the mailboxes are valid is up to the program. hth!
Header file != Linkage. It's just function definitions (which may or may not exist) and nothing more. You can still call functions without including a header, with compiler warnings of course.
They're very similar except for 1 key difference: in a string, the final byte is always a null terminator (0), and similarly no bytes before it can be 0 or that would then become the end of the string. A simple byte array can have as many 0 bytes as it wants as long as the size is known.
From a shared object file? Are they dylinked by symbol?
It's a mixed bag. Some of it is old and crusty, some of it is modern and sleek.
[https://github.com/uhub/awesome-c](https://github.com/uhub/awesome-c)
To make symbols internal, declare them with [`__attribute__ ((visibility ("hidden")))`](https://gcc.gnu.org/wiki/Visibility).
memcached
some c build systems in their most lax mode can still build an exe with no declarations of a used function. the compiler doesn't know what this function is and sees no declaration, and kicks it up to the linker to figure out. the linker sees all the symbol names and goes 'well this function probably means this guy' and does the linking for you. this is obviously very bad and can be made an error, but many build systems can be configured to make that a warning instead for "implicit declaration".
There are two main ways to do this: * first, you can annotate every single function with a visibility attribute as /u/kloetzl said. This is intrusive and rather tedious * alternatively, you can write a [*version script*](https://sourceware.org/binutils/docs/ld/VERSION.html) indicating which symbols you want to export. Ignoring the *version* part for now, this script allows you to specify which symbols are exported from your shared library and which aren't. It supports globs, so it could be as easy as `{ global: mylib_*; local: * }`.
That's bullshit.
Search through this subreddit's archive. This question has been asked many times before.
What operating system are you programming for?
I've also got much help from Reddit. Here's a link to it. [https://www.reddit.com/r/C\_Programming/comments/a8mqh8/so\_called\_wellwritten\_source\_code/](https://www.reddit.com/r/C_Programming/comments/a8mqh8/so_called_wellwritten_source_code/)
[bearssl](https://bearssl.org/)
cglm might be good idea (it's graphics math lib). We can learn about good structure, platform checking to execute different code path via preprocessor, integration with SSE/AVX on Intel/AMD, and NEON on ARM for parallel operations over matrix, vector, memory alignment (which required to work well with SSE/AVX, NEON). Another would be SDL2, good structure, embed assembly code alongside C code to get some information from CPU like CPU's cache line size, or operation that needs optimization, cross-platform code for several platforms. Although it might be large, but most likely we can look for certain function on how they implement. Hey good luck :)
Ain't C cookbooks what you're looking for? I know the one from O'Reilly is good.
Wireshark. It has a large codebase but I found their documentation to be extremely well written so it is easy to read their files. 
If the creator of rollar coaster tycoon can write their game in assembly... This is definitely possible
i know! thank you for the encouragement :D my C knowledge isnt good enough to do this yet... :( thats why im here to get pointed in the right direction from you lovely people!
If i'm understanding you correctly, i think both [Irssi](https://irssi-import.github.io/themes/) and [BitchX](http://bitchx.sourceforge.net/category/screenshots.html) implement the sort of TUI you're after, so maybe reading their source might prove fruitful.
The new release brings a lot of new features: - file picker mode - repo of user-contributed scripts - substring search for filters (option `-s`) - version sort (option `-n`) - disk usage calculation abort with `^C` - create sym/hard link(s) to files in selection - archiving of selection - show dir symlinks along with dirs in top - fixed CJK character handling at prompts - key `N` (1 &lt;= N &lt;= 4) to switch to context N - bring back `NNN_OPENER` to specify file opener - env var `NNN_NOTE` and keybind `^N` for quick notes - handle multiple arguments in VISUAL/EDITOR - show the current directory being scanned in `du` mode - select all files with `Y` - show command prompt with `^P` - key `,` is the new alternative Leader Key - keybind for visit pinned directory is now &lt;kbd&gt;^B&lt;/kbd&gt; - additional key `^V` to run or select custom script - use libreadline for command prompt - reduce delay on `Esc` press - config option to avoid unexpected behaviour on 0-byte file open (see #187) - rename config option `DISABLE_FILE_OPEN_ON_NAV` to `NNN_RESTRICT_NAV_OPEN` - keys removed - `$`, `^`, `Backspace`, `^H`, `^P`, `^M`, `^W`
Windows 10
What is the criteria for determining if a C program is well-written?
What quality do you need? After a brief search, I found this: https://wiki.sei.cmu.edu/confluence/display/c/MSC30-C.+Do+not+use+the+rand%28%29+function+for+generating+pseudorandom+numbers
I have some questions about this. Why is `/dev/random` so choppy? I understand that it needs to collect entropy, but it doesn't seem to be doing that at some constant predictable pace. What's the source? On the other hand `/dev/urandom` gives a stream at about 140MB/s on my machine (`dd if=/dev/urandom of=/dev/null status=progress`). For the OP's question, I'd use the above stream to directly write over mantissa bits, would that be a good approach? 
I'm not sure it would be a popular opinion but check out the Vulkan API. I personally think it's beautiful how you setup structures for the various options and pass them to functions.
Free just marks that memory as available to be allocated. It doesn't guarantee that the memory is zeroed.
Keep in mind, there is essentially nothing special about the integer value 0. It doesn't mean "no value here", it is just one of the very numerous values that an integer can have. Why would the free function bother setting the allocated memory to any particular value, including 0, when you just told it that you're not going to use that block of memory? When you think of it that way, setting the number to all zeros (especially if it's a large block of memory), seems like a waste of time, doesn't it (
In that case, why did two of the values go to 0? I would expect it to not change until some other process uses that memory. And then I should not be able to read from there?
Correct. That's why it's also a good idea to assume that any memory allocated with malloc will not be set to zero. That's the reason calloc exists.
It's worth noting, that while it's good to experiment and try to understand how things work. It is "undefined behavior" to access memory that you freed, compiler is not required to produce anything resembling a meaningful result when you do so.
It's worth noting, that while it's good to experiment and try to understand how things work. It is "undefined behavior" to access memory that you freed, compiler is not required to produce anything resembling a meaningful result when you do so. So the answer to your question is... That's just what the compiler/alligator shows to do when you asked it to perform an undefined operation.
Ah I see. Thanks!
Your program accesses previously `malloc()`ed memory after it was `free()`d, which invokes undefined behavior. This is a technical term which you will encounter frequently if you're serious about learning C, which basically means that your program is syntactically and semantically correct but still does something it's not allowed to do. This is different from a constraint violation (which always produces a compiler error) in that it is not always possible or practical for the compiler to detect that the code is incorrect. For instance, you could teach a compiler to recognize some, perhaps even most, instances of use-after-free, but it is [provably impossible](https://en.wikipedia.org/wiki/Halting_problem) for a compiler to detect *all* such instances. The C standard says that when a program invokes undefined behavior, the compiler can do absolutely anything: it can refuse to compile it, or magically transform it into a different program that does what you actually intended, or produce a program that does the exact opposite of what you intended, or even delete all your files and set your computer on fire. In most cases, compilers go `¯\\\_(ツ)\_/¯` and produce a program that does some approximation of what you would expect it to do once you realize the mistake. In other cases, you can get really, really weird results, like infinite loops or skipping entire sections of the program, because the compiler will look at the code and go “oh, if X is true, this code invokes undefined behavior, therefore X can't possibly be true, therefore I can ignore any part of the program that only executes if X is true”. **TL;DR: don't do that.** PS: Please don't write `*(arr+i)`. The correct syntax is `arr[i]`. If you learned this from a book or tutorial, throw that book or tutorial away.
It was reused by `printf()`.
It looks awesome! I'll keep an eye on this project, because I think I'll use it. :D
thanks! i have added saving/loading too. i'll push the changes on github when I get some time
Alright gonna throw my professor out the window. :)
Awesome, thank you! Actually this could be an awesome tool to use for drawing sprites. I'll take a look at the code, if I can load it from other languages easily. 
I sometimes find it helpful to think of `malloc()` as taking a block of memory from a pool of blocks that are available for allocation, and `free` as returning a block of memory to the pool. In some cases, `malloc` and `free` will make use of the space within unused blocks to keep track of what blocks are available, in which case `free` will overwrite some of the storage in a block, but from the point of view of the Standard such changes are not "observable". You might be expecting behavior similar to some library implementations that zero out storage which is given to `free`, for the purpose of guarding against scenarios like: unsigned char *confidentialData = malloc(someSize); ... do stuff with confidential data for (int i=0; i&lt;someSize; i++) confidentialData[i] = 0; free(confidentialData); ... unsigned char *nonConfidentialData = malloc(1000); strcpy(nonConfidentialData, "Hi"); fwrite(nonConfidentialData, 1, 2, someFile); A conforming implementation could omit the operations that write `condidentialData` before freeing it, since from the Standard does not define any means by which such storage could be read. On the other hand, the Standard also offers no guarantee about the initial contents of storage returned by `malloc`--in particular, it does not guarantee that such storage will not "by coincidence" hold some of the same byte values that had been in `confidentialData[0..someSize-1]` just before it was zeroed. Having the library implementation of `free()` zero out the memory may guard against this optimization (and resulting unintended data leakage), and perhaps that is what you were expecting. Note that depending upon the caching architecture, zeroing out storage before `free` may be cheaper than zeroing it on `malloc`. If the memory in question is cached and dirty, zeroing it before the cache gets written out may be very cheap, and the processor will then be able to write out the data at its convenience. Zeroing out storage that isn't cached (e.g. as part of `malloc`) may be more expensive since the processor may not be able to execute the code after `malloc` until it has acquired all the cache lines. Note also that from the point of view of the Standard, not only will the contents of any storage block passed to `free` cease to exist, but any pointer which identifies all or part of such a block will *itself* become a meaningless bunch of bits. Examining the bytes that make up a pointer (as distinct from trying to use it *as* a pointer) would would not yield UB, but nothing is guaranteed about the values of those bytes, nor their relationship to the byte values comprising any other pointers. Doing anything with such a pointer other than examining the bytes that make it up would invoke Undefined Behavior, and the Standard imposes no requirements upon how implementations behave if code tries to do so. Some library implementations may offer stronger guarantees that required by the Standard, but some behaviors which used to be guaranteed by design are no longer reliable on "modern" implementations. 
On the hardware level a 1 is represented as a charge build-up on a capacitor (in RAM), and this has to be constantly refreshed to stop it being depleted. A value of 0 is less expensive.
the suckless utilities are all very readable and definitely short enough to read and with great documentation!! as well as a definite style guide.
https://suckless.org/
I thought memory holds bits in flip flops. And those are always under voltage, even when they hold a zero, no?
A flip-flop is used to store data in SRAM, for example in your processor's cache since it has extremely low latency. However SRAM is expensive and has generally low capacities. Your computer's RAM is actually DRAM.
Yet another approach is to select a naming prefix and, for any symbol that shouldn't be used in external code, use something like `#define myFunction zz__mySuperLibrary__myFunction` within a project-internal header file. That wouldn't prevent the function from getting invoked from outside code that knows its name, but it would prevent any naming conflicts with client-side symbols whose names happen to match. Of course, using `static` may be better when practical, but that won't work if a symbol is needed within multiple compilation units within the library. Use of a naming convention like the above would avoid naming conflicts without special compiler or linker trickery. Incidentally, in some cases it might be helpful to give such names to all exported symbols in a library, including public ones, but then have header files define macros which map the public names to the prefixed forms. This will avoid problems if one part of an application uses `fooLib`, which contains a `woozle()` function, and another part uses `barLib` which coincidentally also contains a `woozle()` function. If no compilation units needs to include the header files from both libraries, the existence of two `woozle` functions won't create any conflict, since each compilation unit would use whatever `woozle` was appropriate for the header files it included. 
I don't understand why you've been downvoted. Is, according to the redditors, Linux kernel badly written?
No clue. One of the most efficient and consistent code bases. Maybe Reddit c folks aren't a fan of Linus' standards
My guess is that it is just too much code to have any idea what you are even looking at. 
The downvotes aren't the right way to respond, but I disagree with the kernel being a good resource for this. It's a large codebase with a lot of components that can seem inscrutable unless you're already familiar with the subject matter in question. Something like sqlite or even git would be more accessible to "average experience programmer", because you likely are familiar with the concepts.
Makes sense. Though not a kernel Dev myself I still appreciate the structure of Linux. It has been one of the most influential repos for me. 
Get the kernigan and Ritchie book.. It is like the C coders Bible..
You might want to use the curses library. Your system probably provides it implemented by ncurses.
Your choice of compiler is constrained by the system you're targeting and the system you're compiling on. There might only be one option. Even if you have a choice, there usually isn't a "best." That depends on what you want: fastest performing code, standards conformance, library compatibility, fastest compilation time, compiler extensions, licensing cost and agreements, IDE and build tool integration, vendor support, developer friendliness, and so much more.
The one provided by the system you are programming for. Don't fixate on a single compiler too much. That just leads to unportable programs.
Doom 3 source
This
RMS says gcc.
Its not that C as a first language is bad its that C has a whole smorgasbord of traps and catches can make a moderately complicated project feel like trying to build a house out of spaghetti. Other languages with less of a steep learning curve are probably the better way to start off as a newbie. C will let you cruicify yourself a lot more often than something like Java would. 
COrange is a small C game engine that’s not too bad.
Turning either of them into a good random float is hard. Since Linux 3.17 getrandom(2) is preferred anyways. Other OS provide similar APIs.
Both use “environmental noise” and /d/random blocks if insufficient entropy is available. See `man 4 random`. When you write only the mantissa, you can not produce a number &lt; 1/2. You also have to pick a random exponent. However, note that just copying bits will lead to a severely skewed distribution. Much worse than the old `rand()/(double)Rand_Max`!
Turbo C on DOS, for sure.
Judging by the majority of questions we get here, the unwillingness to put in effort seems to be universal among self taught and university students.
I do agree with that.
On some platforms, computing an approximate square root may be faster than a precise one. Because the Standard does not mandate how accurate the results have to be, someone designing an implementation might guess (rightly or wrongly) that customers would find speed more important than accuracy. If you need an accurate integer square root, I would suggest checking whether `(result+1u)*(result+1u)` exceeds the number whose square root was taken and, if not, increment the result and try again until the product exceeds the original number being used. Unless `sqrt` is really terrible, this won't take very many repetitions (most likely none, possibly one, and probably not any more than that).
You're definitely going to want a C compiler.
RMS says a lot of things. I'm selective about what advice I take from a guy who eats his toenails on stage.
&gt; a guy who eats his toenails on stage. You got a source on this?
A modern one if at all possible. Otherwise, time to change platforms, 😉
I miss Chris Sawyer games :(. Loved the shit out of transport tycoon deluxe.
Ya this is probably doable.
Can you add more details? Are you writing the code for arduino or PC? What exactly is the code supposed to do?
I'd rather have a compiler that recognizes a few nice syntactic additions that should have been in the language since 1989, but focuses on the quality of simple straightforward code generation and 1990s-style optimizations, than one which focuses on more modern "optimizations".
Reading Standard along with various defect reports, it's clear that there has never been a consensus as to what many of the words used in the rules mean. I don't think it's possible to formulate any set of definitions for the words used in the Standard that would yield defined behavior in all cases which quality implementations should obviously treat as defined, without needlessly blocking useful optimizations. The language would have been much better off if it had specified that implementations need not allow for aliasing between seemingly-unrelated references to objects, but the question of when references are recognized as related is a Quality of Implementation issue. The authors of the Standard should have never allowed it to be abused to bully programmers. 
Yup. These guys obsess about it.
I'd recommend going straight for termbox as opposed to ncurses. It's far more simple. Modify the examples and go from there.
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/devsjunky] [Is there any place I could look for examples of well-written C programs? : C\_Programming](https://www.reddit.com/r/DevsJunky/comments/ash65r/is_there_any_place_i_could_look_for_examples_of/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
This feels like good advice. I've been trying to learn C on Windows and it's really been quite awkward. Nothing fits together nicely the way it seems to on my Linux VM.
Not so tough. Just create a uint64_t and shove the bytes in there. Then do a bit of math between 0 and unint64_type_max and you're done. However this is all a moot topic given that there is no such thing as good random data on any computer that doesn't have a rng hardware option. 
Better to get a PCI board with a radiation decay source and then you have true random data source.
Not sure if its really that good, but nginx is written in c.
Ugh, SQLite is aesthetically horrible.
Obvious spam is spam
Hi, This forum is for C programming. You might have more luck with C++ questions over at /r/cpp, or /r/cpp_questions. Looks like there is a game dev specific site at /r/gamedev. What you might want to do is go search for stuff like this on GitHub, where you can look at how others built something like this. [Here might be a good example](https://github.com/Adri22/RPG) Good luck!
I'll try that, thank you!
You're in C sub, try /r/cpp. Is there a way I can make a function outside of the main file, but can still call there? You need to learn about classes. &amp;#x200B; &amp;#x200B; &amp;#x200B;
 What you just said has nothing to do with malloc() and free(). It also has nothing to do with C.
Thats old style. 
Clang. It's modern and cross platform targeting is easy. 
If it were available for other processes to use, you wouldn't be able to use it at all. You'd get a segmentation fault. Heap memory is owned by your process at runtime. Allocating memory that's already owned via malloc and requesting additional memory from the kernel are two different things.
The termios manual page is a good starting point for setting the terminal's raw mode, in the highly likely situation where you want the progrm to read, process , and echo the input one character at a time, including erasures.
&gt; Not so tough. Just create a uint64_t and shove the bytes in there. Then do a bit of math between 0 and unint64_type_max and you're done. That is not enough. The smallest non-zero number you can produce with that is 2^-64. However, there is a huge amount of floating point numbers between zero and 2^-64.
My first choice for new projects would be clang. The whole tooling around (clang-tidy, static-analyzer, sanitizers) is just amazing. Nevertheless, sometimes gcc still produces slightly better code.
Unfortunately this is heavily dependent on the toolchain you are using. For gcc it's \_\_attribute\_\_ ((visibility ("hidden"))); For msvc it's [\_\_declspec(dllexport)](http://msdn.microsoft.com/en-us/library/a90k134d%28v=vs.80%29.aspx) If you are using cmake for building your project (which I would strongly recommend), there is a module called GenerateExportHeader which automatically generates a header file you can use to make symbols visible.
Say you want to average the last 10 values from some source (say frames/second data), you can make a buffer of size 10 to store values as they come in. Keep an index of which value to replace next that cycles from 0-9 repeatedly (e.g. index = (index + 1) % 10). Then each time you get a new value, average all 10 values in your buffer.
Care to post some code?
Code you have + example would be really nice
You want a floating point number that is random to how many digits?
Every time you are given a new value and want to calculate a new average, replace the oldest value in the array with the new one. You need to remember the index for that replacement and advance it by one after replacing, also making sure it stays within bounds. An alternative that runs faster: remember the sum of the values so far, and at each step subtract the oldest value (kept in array) and add the newest value. That function still needs the array, but runs much faster.
&gt;Such actions are categorized as "Undefined Behavior" [characterizing them as "Implementation-Defined Behavior" would imply that all implementations should document their behavior]. IT IS NOT AN IMPLICATION. IT IS AN EXPLICIT REQUIREMENT OF THE STANDARD FOR “IMPLEMENTATION-DEFINED BEHAVIOR”. If you can’t grasp the glossary at the beginning of the standard by definition you can’t understand the rest of the standard. At least now I know who keeps writing memcpy where memmove is required.
Maybe [King's "C Programming: A Modern Approach"](http://knking.com/books/c/)?
IF you want a 200 day MA, keep 200 days of data in a FIFO structure(e.g. a linked list) and it's total sum. When you want the next MA, remove the 200th past day and subtract it from the total and push the new day and add it to the total. 
on pc am trying to get signals sent from arduino with bluetooth like for an exemple if a led light up i want my pc to show me message that the 1st led is working for an exemple 
Here you can choose between different skill level: *https://notabug.org/koz.ross/awesome-c#learning-reference-and-tutorials *Reference: https://en.cppreference.com/w/c *If you need a lib: https://notabug.org/koz.ross/awesome-c#contents Happy Coding :)
Try another test, allocate and free **two** blocks. Maybe your malloc uses that 0,0 area for something, like a pointer in a list of free blocks.
I'm just starting but, I found this useful https://github.com/EbookFoundation/free-programming-books/blob/master/free-programming-books.md#c
Dude stay in your lane. I was pointing out a difference in variables having different values in reply to someone who said all possible values are essentially the same.
Wrong sub. This is for C.
Wrong subreddit mate : try /r/chsarp
You cant beat reading the k&amp;r book imo for a concise introduction to the language. It lacks programming exercises, but it reads very well and is a well known c classic.
I can't find it right now, but i think n Zed Shaw's book *Learn C the Hard Way* that was available free preview while he was still writing had an example that did this without keeping a buffer. There are github repos that have the code in it.
Thanks :)
Gotcha :) silly me
Once you hop over the initial learning wall, reading man pages and source code for Linux utilities can teach you real applications of C, rather than how to make colorful outputs.
CLion isnt the only one with a debugger, Visual Studio (not vsc) also has a debugger, and in my opinion, for figuring out what your program is doing, debugging is the most instant tool. Learning how to debug is very important. As my professors have said, a good debugger makes a good programmer. If you insist on using vsc, you can always write the values of your variables to console as frequently as you see fit, just so you can physically see what happens to them when you run your program. 
Generally, IDEs integrate external build tools, compilers and debuggers that you can also use from the terminal. If you are not using and IDE, than the typical setup is to use an editor write your code, then use command line tools to build and debug your program. For example: &amp;#x200B; \+ editor: VSC, Atom or vim \+ compiler: gcc or clang \+ build-tools: make, cmake or bake \+ debugger: GDB &amp;#x200B;
Don't forget * memory debugging, memory leak detection, and profiling: Valgrind
You can also use \_\_FILE\_\_, \_\_LINE\_\_, \_\_VA\_ARGS\_\_, and \_\_func\_\_ to define macros used for debugging your code like this: #include &lt;stdio.h&gt; #define log_info(M, ...) fprintf(stderr, "[INFO] (%s:%d) " M "\n",\ __FILE__, __LINE__, ##__VA_ARGS__) int main(int argc, char *argv[]) { log_info("Start of program"); int var = 10; log_info("my variable is %d", var); log_info("End of program"); } &amp;#x200B;
yeah, my teacher showed that as an alternative, but he wanted us to see the first example too for us to understand what happens.
Will bookmark, thanks.
Google "awesome-con github" 
With autohell... I mean autotools, you don't have to worry about Makefiles much. Automake will do almost everything for you. It only needs few macros to know some things about the program name, project structure etc. Autoconf is a bit more complicated and it's very easy to screw things up if you write your own macros. May be difficult to debug the output configure script. Google "autotools tutorial" and try it out. 
[https://www.gnu.org/software/make/manual/](https://www.gnu.org/software/make/manual/) is a decent resource, IDE's I often struggle with look through buried sub-menus trying to find the option you know ought to be there! It's quite possible to end up with a boiler plate Makefile that will automagically compile and link together all the sources in your src dir, leaving all the .o files in an obj folder, I also have rules for some project that will build just the binary resources where 3d shapes have changed in my resources folder, in short it can really be a kind of swiss arm knife of build / project management with any number of custom task available, even providing cross platform compilation from the same rules! While initially it will seem a little steep and even obscure, its worth learning as its really powerful, also check out pkg-config which is very useful....
I no longer mess around with makefiles. Doing dependency management by hand is just hell. Getting the dependencies right could be done running gcc, but it's not really cross-platform. I always rely on cmake, which is a generator for various build environments (i.e. Makefiles, ninja, visual studio etc.). Most IDEs know have good support for cmake. To answer your third questions: No there is not really a difference between C and C++ if it come down to make.
CMake is a tool that generates Makefiles. CMake is not a `make` implementation. It doesn't really matter which `make` implementation you use CMake with. &gt; Should I be afraid of make? How bad can I screw up with a shitty make? Versioning code is enough to save me? You should not be afraid of `make`, but you should make sure not to overcomplicate your Makefiles. Many `make` tutorials contain ridiculously complicated constructs that are entirely unnecessary for most projects. Note also that `make` and GNU make are two pretty different things as GNU make adds so many extensions, that the original tool is barely recognisable. 
Visual Studio code actually does have a debugger for c and c++ programs, but it is fairly rudimentary. Essentially you point to the executable built from your program with the '-g' debugging flag, and it allows you to step through the program w/ breakpoints. I actually use VS Code for any C projects along with CMake or Makefiles depending on which OS I plan on building on. VSCode has an integrated terminal which is where I would recommend you do most of your non-coding tasks, meaning building and debugging, at least to start off. The reason for this is it will give you a better understanding and feel for what an IDE actually does in the background. CMake is powerful and I would recommend learning it, but writing CMakeLists can be a pain at first for anything other than a small program.
You gave an incorrect answer to question. I'm not sure what 'lane' you're talking about, but perhaps you should look into it. Semiconductor physics has nothing to do with why OP saw what they saw after freeing memory and accessing it. Undefined Behavior is what was observed.
1. For the beginning GNU Make is completely sufficient, works and is well tested. 2. Technically a bad written make file can screw you up (e.g. through a rm command), but a basic shell knowledge mitigates these issues and is necessary anyway. And using a version control system is always a good idea. 3. Make per se is language agnostic, so if you want you can use make even for Java, Python or JavaScript projects. 4. I highly recommend reading the GNU Make manual. It is worth it and provides all important information. 5. CMake is quite different and has other goals as Make. If you want to use CLion you need to learn CMake because Make plays a background role. Overall I would recommend to learn just the basics of CMake to handle the project configuration for CLion if you start with a fresh project. [CLion documentation](https://www.jetbrains.com/help/clion/meet-clion.html) [CMake Reference](https://cmake.org/cmake/help/latest/)
I clearly wasn't replying to OP
Earth is round too, but that has nothing to do with malloc() and free(). Stay in your lane. /r/ECE is over there. This is /r/C_Programming.
Woa sorry big man &gt; Keep in mind, there is essentially nothing special about the integer value 0. It doesn't mean "no value here", it is just one of the very numerous values that an integer can have. I was commenting on this observation. What I said is relevant and interesting. Don't be so obtuse
Thanks. I was going to include Valgrind, but wasn't able to pull the name out of my groggy brain first thing this morning.
I would just copy a makefile from some super simple but professional looking project on Github. Learn the basic syntax to modify it accordingly. It is not worth your effort to actually learn all the in and out of make, it is more important to know the philosophy and understand the basic features. For complicated projects, IDE or automation tools like CMake and autoconf will generate better Makefile anyway. Make is basically a shell script in a declarative style language. If you know shell script, then it should be easy.
I'm the author of the first article. `make` is a solid tool, and it's perfectly reasonable to manually maintain a Makefile for projects that don't have more than, say, a dozen or so source files. After that you probably want to automate it one way or another, even if it's as simple as manually doing this when you change your includes: $ gcc -MM *.c &gt;&gt; Makefile $ $EDITOR Makefile # fix it up The risk is getting a stale build, but only when building incrementally. In other words, you run `make`, edit a file, then run `make` again, but not everything is rebuilt due to a missing dependency. For C and C++, it's very easy to write a Makefile where, if you start fresh (`make clean`), you'll get a correct build. I personally avoid using GNU Make features because it's a dependency I don't really need. Occasionally I'm on systems that don't have GNU Make, but rather some other flavor, so it's nice to not depend on it. A few of GNU Make extensions are nice, but most of its extra features aren't useful. Some don't even work correctly (`VPATH`). In a few cases they're even harmful, such as plowing on silently through errors: example: (exit 1); echo You should not see this message Result with GNU Make: $ make You should not see this message Putting it in `.POSIX` compatibility mode: .POSIX: example: (exit 1); echo You should not see this message Result: $ make Makefile:3: recipe for target 'example' failed make: *** [example] Error 1 Its "directory search" feature also breaks some inference rules. Unfortunately this cannot be disabled. $ cat Makefile hello: hello.c $ echo 'main(){}' &gt; hello.c $ touch hello.o # this confuses GNU Make $ make cc hello.o hello.c -o hello hello.o: file not recognized: File truncated collect2: error: ld returned 1 exit status &lt;builtin&gt;: recipe for target 'hello' failed make: *** [hello] Error 1 
This saved me: https://spin.atomicobject.com/2016/08/26/makefile-c-projects/
&gt; 3.) Is make different for C or C++? Does it matter if I learn for one or for the other? The shell commands used in the rules will be different, but that's all. Make can be used to build any type of project. ([Here's](https://github.com/ethagnawl/tools-deps-experiment/blob/master/Makefile) an example of it being used to build a Clojure project.)
I studied on it for my exam, it's an amazing book. Full of examples and explains everything in detail. If it wasn't for this book I would've never understood linked lists, my professor made a mess with typedefs and recursion when teaching them.
Not to get into a editor war here but Emacs has some excellent support for C and C++. You can edit, debug, compile, run valgrind, interface with git, etc... all inside the emacs editor. You can also open up shells and run commands, has an excellent filesystem explorer, is programmable, etc. All available from the keyboard. Seriously, IMHO, the more you need to reach for a mouse to do any action (especially those that you repeat all the time) the less efficient you are. If anything, learn and memorize all your IDE's keyboard shortcuts and use them.
Sorry I left out the word "meaningfully". There was a defect report addressing the question of what's actually required, which allowed for the fact that a low-quality implementation could be "conforming" even if its documentation was so vague as to be essentially useless, but I don't think the authors of the Standard wanted to encourage that. If something like left-shifting a negative number was Implementation-Defined, and if implementations were actually required to meaningfully document a consistent behavior, that requirement would likely have increased the cost of some signed left-shift operations on ones'-complement or sign-magnitude hardware. On something like a 36-bit Unisys (ones'-complement) platform, if `x` happened to be in a register, the most efficient ways of handling `x=(y&lt;&lt;1)` and `x=(y&lt;&lt;27)`, for various combinations of memory and register operands, would treat negative numbers differently. If "add" instructions trap on overflow, these differences could result in some treatments trapping under circumstances where others wouldn't. Such extra costs could have been avoided, if the behavior of `x&lt;&lt;y` with negative `x` was Implementation-Defined, by implementer willing to document the behavior as "either yield some kind of value or an overflow trap, whichever the compiler happens to find most convenient at the time". Based on the aforementioned Defect Report (sorry I don't remember the number) it would seem that such documentation would probably have been "conforming" even if `x&lt;&lt;y` with negative `x` had been classified as Implementation-Defined, but the authors of the Standard probably thought it was better to treat such action as UB, even though the behavior had been sensibly defined on 99% of implementations, rather than make it IDB and say that the only way some implementations could process it efficiently would be to write a behavioral description so vague it could apply to *any* implementation. Reading the published Rationale for the C Programming Standard, I think it's clear that the authors of the Standard expected that people seeking to produce quality implementations would treat the situations where the Standard imposes no requirements *the same way as people seeking to produce quality "almost anythings" treat situations where applicable standards impose no requirements*: by determining what course of action by them would best fulfill customer needs. This is a normal role of markets, and I can't imagine what else the authors of the Standard might have been intending when they said "The goal of adopting this categorization is to allow a certain variety among implementations which permits quality of implementation to be an active force in the marketplace as well as to allow certain popular extensions, without removing the cachet of conformance to the Standard." The only time it should *matter* whether the Standard mandates that implementations process some action a certain way is when compiler writers would have reason to believe that *they could better serve their customers by doing something else*. If someone writing a compiler for the Unisys were to correctly believe that their customers would find it more useful to have the compiler process left-shifts of positive numbers more quickly, at the expense of making the behavior of left-shifting negative numbers unpredictable, they would be better placed than the Committee to judge whether that is useful. If the compiler writers guess wrong and their customers (programmers) say that this behavior is less useful than the slower alternatives, then if compiler writers respect the Spirit of C, they'll honor its first principle (listed in both the Charter and Rationale documents): "Trust the programmer". If a car maker thought that consumers would rather have a car that was $5 cheaper than one where the trunk would keep things dry, and customers complain about the trunk leaking, should the car maker suggest that customers wanting to keep things try in their trunk should push for lawmakers to mandate that trunks keep out water, or should the car maker accept that people would rather spend an extra $5 than have soggy groceries? Perhaps you think that programs' behavior should be fully defined by the Standard. I would agree with that, if the Standard were to provide ways by which programmers could specify behavioral requirements their programs might have that might be impractical on some implementations, while giving implementations the option to either honor those requirements *or reject the programs altogether*. As a simple example, the Standard could require that implementations not pre-define a macro `__STDC_SUPPORTS_GUARANTEES` unless it includes a header `&lt;stdguarantees.h&gt;`, and require that if `__STDC_NEG_LEFT_SHIFT` is either predefined or defined within that header, it must have a value where bit 2 is clear unless they always process left-shifts in C89 fashion, and not have a value where bit 1 is clear unless they process left-shifts by a value less than or equal to the word size in a fashion consistent with repeated multiplication (which would be allowed to trap on overflow), and it could recommend that implementations define `__STDC_SUPPORTS_GUARANTEES` and then define `__STDC_NEG_LEFT_SHIFT` with non-zero values when allowed to do so. Then, if a program starts with #ifdef __STDC_SUPPORTS_GUARANTEES #include &lt;stdcguarantees.h&gt; #endif #if !defined(__STDC_NEG_LEFT_SHIFT) || !(__STDC_NEG_LEFT_SHIFT &amp; 2) #error Sorry. This implementation doesn't support required left-shift semantics. #endif the behavior of `x&lt;&lt;y` with negative `x` would be fully defined within the balance of the compilation unit. The imposed cost upon implementations where guaranteeing such behaviors would be impractical? Zero. Simply don't define the macros. The imposed cost upon implementations that would naturally behave that way? Minimal--just include a `&lt;stdcguarantees.h&gt;` file with suitable definitions. I'm not sure the best level of granularity for behavioral guarantees, and suspect that intrinsics that would allow programmers to demand behaviors would be better than macro-based tests, since they would allow compilers to adjust behavior according to requirements, but in the absence of such features there's no way the Standard can meaningfully define everything necessary for programmers to efficiently accomplish all the tasks for which C was intended to be useful. 
 fread(*data, sizeof(int), size, file); Here `file` is your filename, not your file pointer. You have 2 variables called `file`, with different scopes. This is maybe what's confusing you.
You keep saying Bluetooth. Does that mean that the Arduino and your PC are communicating with each other using Bluetooth? If so, what Bluetooth controller are you using on your PC?
While the **theoretical** number of floating point numbers in any given range is infinite, we can only represent some discrete subset of that as N-bit patterns. "Huge amount" can be quantified if we know how many bits we have available to represent those numbers. Here's a good read on that: https://randomascii.wordpress.com/2012/02/25/comparing-floating-point-numbers-2012-edition/
My point was merely that diving two 64-bit integers will never give you 1e-74 which is a perfectly valid double.
&gt;However, note that just copying bits will lead to a severely skewed distribution. Why would that be the case? Say I fill a buffer of N random bits and then write them over my mantissa. Shouldn't the numbers be uniformly distributed? Why would they not be? I could then apply some arithmetic operations to get the required range and my numbers would still have N bits of entropy, would they not?
How about a poor man's RNG? https://security.stackexchange.com/questions/42428/is-generating-random-numbers-using-a-smartphone-camera-a-good-idea
cmake has some learning curve but i recommend it, especially in combination with ninja for faster builds.
Thank you so much feel, so stupid that I completely looked over that.
Oh right, we'd then need some method which maps our raw random N bits to our floating point number in such a way that all possible bit sequences which would fall in the required range are equally probable. Maybe the simplest method: write over ALL floating point number bits, if outside range just re-roll.
&gt;However, note that just copying bits will lead to a severely skewed distribution. Why would that be the case? Say I fill a buffer of N random bits to match the size of my float and then write them over, while discarding and re-rolling any numbers outside the required range. Shouldn't the numbers be uniformly distributed? Why would they not be?
Thank you for the complete reply.
Welp, the problem actually started doing that. Found a simple hello world opengl C project, imported to CLion and couldn't get it to compile. I guess knowing use and basic features should get me on track eh? Thanks!
&gt; It seems to be a few different flavours of make around. Should I focus in some specific release? I was considering going all GNU Make because why not. I'm running Debian after all. good god no, please don't contribute to gnu's hegemony. just use Cmake, it can be wonky, but it's a lot better than the various flavors of make.
(Let's agree this is all very academic.) The problem is that there are more "double bitpatterns" representing a number between 0 and 1/2 than between 1/2 and 1. So just copying bits will skew the distribution. Dividing two 64-bit integers is uniform but can not produce very small doubles. One thing that does work—please correct me if I am wrong—is divide an conquer. Repeatedly split the interval of interest in half until you have exceeded the floatingpoint range in that subinterval. That then is your number.
Awesome! Thanks for the input, and nice article on make! The only one I thought it was good! 
&gt;If something like left-shifting a negative number was Implementation-Defined... It’s not. The behavior is explicitly defined for a set of cases and explicitly “undefined behavior” for the rest. Unfortunately the rest of your response is as poorly researched and reasoned because you are reading irrelevant documents instead of the actual approved ISO standard and making up stories about things that aren’t part of reality.
I've found this to be a great reference: http://www.cs.yale.edu/homes/aspnes/classes/223/notes.html
I prefer Autotools as a way to generate Makefiles: https://autotools.io/index.html The investment pays off when you start building shared libraries, plugins, targetting multiple platforms, etc.
have you tested the Makefile as is without going through an IDE? just type 'make' from a terminal. You may need to change a few things to make it work. Also the IDE may decide to overwrite it or use other means. My experience with IDE is that they may have their own conventions. 
So in order to make it equal, first though would be to agree to never "roll" some patterns in the [0,1/2] interval to make the number of numbers equal in both intervals. How to decide, though, and what about subintervals... and so we get to your proposed algorithm. In other words divide the range in 2, and use the random bit to decide which subinterval to take. Continue doing that until we reach the smallest number. This conversation was insightful, thanks :)
1. Use either GNU Make or learn Plan 9's [mk](http://doc.cat-v.org/plan_9/4th_edition/papers/mk). Note that mk has many important differences and the two are incompatible, but many consider the mk extensions to be valuable.
1. If you're going to use vanilla make, there's two ways to do it. Either aim for broad compatibility (the common subset to all make), or ignore compatibility and just go for GNU Make, since it's available most anywhere anyway. I fall into the latter opinion. GNU Make has too many improvements to ignore. 2. Don't be afraid. The worst you might do is accidentally write a `clean` target that accidentally deletes all your work (you wouldn't be the first to have `rm * .o`), so absolutely you should use version control just for that reason. 3. No, it's the same. Makefiles don't really care about language, and you can even mix languages together in the same build if you want. About the only caveat to this is that the the linker is usually different for C vs C++ (something not exactly acknowledged by the built-in rules, unfortunately). There's some conventions to know on variable naming in this regard: `CC` is the C compiler, while `CXX` is the C++ compiler. `CPP` refers to the C preprocessor. Thus, `CFLAGS` are the flags for the C compiler, `CXXFLAGS` are the flags for the C++ compiler, and `CPPFLAGS` are the flags for the preprocessor (common to both). 4. That's a hard one. I mostly learned by just using it. There's a lot of bad Makefiles out there, and I have yet to find a good tutorial. But I'll give you a couple guidelines. First, almost everything should be a pattern rule (using `%` instead of full filenames). Don't ignore the built-in rules, which do most of what you might want for simple projects. Second, the hard part is defining all the dependencies. Use the `-M` family of options of GCC and Clang to generate those. If you have all the dependencies defined right, everything else usually falls into place, and when you don't is when terrible workarounds start creeping in. 5. CMake is a different beast. It's purpose is to create Makefiles that are then built which make.
C89 fully and unambiguously defined the behavior of left-shifting negative numbers in sensible fashion on all implementations that do not use padding bits on signed nor unsigned integer types. The mandated behavior was less than ideal for ones'-complement or sign-magnitude implementations or for two's-complement implementations that had documented trap-on-overflow behavior (because signed left shift is often used for power-of-two scaling, having it trap in the cases where repeated multiplication would do so may be more useful than the C89 behavior). C99 reclassified left-shifting of negative numbers as Undefined Behavior on all platforms, even in cases where the C89 definition had been simultaneously unambiguous and useful, without even mentioning the change in the Rationale. Which seems more likely: 1. They expected that implementations which had no reason to do anything else would continue to process the behavior in C89 fashion, and that programmers could continue to rely on the C89 behavior *unless their code might be called upon to work with one of the the extremely rare implementations where some other behavior might make more sense*. 2. They intended that any code which relied upon the C89 behavior should be regarded as "broken", whether or not anyone would ever have any interest in using it on anything other than the 99.9% of implementations where the C89 behavior made sense. What do you think the authors of the Standard intended? If you're not interested in anything outside the Standard itself, refer to N1570 section 4 paragraph 7: "**A conforming program is one that is acceptable to a conforming implementation**. [footnote 5: Strictly conforming programs are intended to be maximally portable among conforming implementations. Conforming programs may depend upon nonportable features of a conforming implementation.]". The question of whether an implementation can usefully process C programs that are conforming but not strictly conforming is left as a quality of implementation issue. Perhaps a retronym is needed to unambiguously refer to the widely-useful language the C Standard was written to describe, as opposed to the much more limited subset which is limited to tasks that can be performed using strictly conforming programs? 
Use cmake. Learn both, but your build systems should be done in cmake. Just google for a cmake tutorial. The basics are rather trivial cmake_minimum_required(VERSION 3.4) project(Hello C CXX) add_executable(HelloWorld main.cpp thing.cpp otherthing.cpp) find_package(OpenGL) target_link_libraries(HelloWorld PUBLIC OpenGL::GL) put that in the top of your project directory in a file called `CMakeLists.txt`. Then $ sudo apt install cmake ninja-build $ mkdir build &amp;&amp; cd build $ cmake -G Ninja -DCMAKE_BUILD_TYPE=Debug .. $ ninja cmake is a build system generator. It reads your `CMakeLists.txt` and configures a bunch of settings to the platform that you're on and uses it to generate a build system. It can generate Makefiles, Ninja files (Ninja is like make but MUCH FASTER, although you really can't write Ninja files by hand), Xcodeproj, Visual Studio project and a million other things. This works on literally every platform you can think of. 
Refresher? Absolutely. Introduction? No
You might also install the GNU make docs which are generally in a separate package, or also grab the source archive from gnu.org who h also has it. If you'working on projects to be installed elsewhere, you'll almost certainly meet to write the makefile components for automake, autoconf,and the rest of the build toolchain. All of these program's source packages contain the Texinfo manuals which contain examples (it's almost essential to install the texinfo/nakeinfo package first). 
yes am sorry if i didn't make it clear or i didn't pose the right question yeah am using the windows default bluetooth controller and thank your for your attention and help :D
No problem. Maybe my phrasing was wrong, but I don't think you got my question though. &amp;#x200B; From your answer however, it seems you have a Bluetooth enabled card connected to some port (PCI-e or USB) of your computer. To confirm that it works, have you tried sending/receiving files to your bluetooth-enabled smartphone? &amp;#x200B; If so, and it works, then you can look into the Bluetooth API provided by Microsoft. They ask you to use Windows sockets to communicate. [https://docs.microsoft.com/en-us/windows/desktop/bluetooth/bluetooth-start-page](https://docs.microsoft.com/en-us/windows/desktop/bluetooth/bluetooth-start-page) You can build a C application using the Visual Studio C compiler. &amp;#x200B; If my understanding is wrong, please let me know. 
Manual memory management. It will be up to your code to release any memory once it's no longer needed. Not overwriting arrays. It's definitely possible to do this: char a[5]; a[some_function_that_will_return_5()] = 'x'; Now you've written to memory you don't own and the code can continue running and only run into problems later. 
1. C is not Object Oriented. The closest thing you'll see to an object is a struct. This may require you to think slightly differently about how you solve your problems. 2. C Assumes you're smart. C does not do alot of things that modern languages do to protect you, such as array bounds checking or even some type checking. If not careful, you or users of your program can overwrite memory that you did not intend to overwrite. 3. C does Arrays and Strings differently. Probably the most frustrating difference I've found when moving from a higher level language like C# to C is how arrays and strings are handled. Arrays are only pointers to memory, and do not store things like the length along side of them. Strings are simply character arrays terminated with a `'\0'` character. This means that you must define all arrays and strings with a size at compile time or dynamically yourself using `malloc()`. You must also store the size of that array somewhere, except in the case of strings because you can use `strlen()` which counts until it sees the `'\0'` character. Also, I recommend learning C without an IDE. Writing C in vim (or nano) and using gcc at a bash prompt is a great way to get used to not being babied through the development of a program like with an IDE. It also helps you appreciate just how much modern tools assist with development today.
I’m fairly new to C, but off the top of my head... You don’t work with objects in C. You can work with structs, but there’s no concept of public/private or many of the other concepts that OOP relies on C has a lot more “sharp edges”—you need to be very careful about managing memory, and very wary of the language’s little nuances. For example, you need to use the strcmp() function to compare strings. You can write ==, but your code won’t work as intended and C probably won’t tell you why. That’s pretty tame compared to the other issues you can run into with C C is a difficult language can be very frustrating, but it’s a language that’s certainly worth learning. It’s a great way to learn a lot about memory, pointers, and really pushes you to understand how your code works rather than doing the heavy lifting for you
The last Makefile has been written in the 1980s and since then there has only been a successive series of copying and modifying.
C and C# are basically entirely different languages. You should learn C from the ground up without making the assumption that it's like C#.
We agree this is all very academic. Yes. OKay .. problem solved. Just divide two 128-bit numbers and that solves the problem for any long double which has 113 bits in the mantissa. 
The K&amp;R book describes the language invented by Dennis Ritchie. That language defines the behavior of many constructs which are not processed meaningfully in the language processed by gcc or clang with full optimizations enabled. For example, page 41 of K&amp;R2 says that the behavior of integer overflow is "machine-dependent". That would suggest that on a 32-bit quiet-wraparound two's complement machine, a function like unsigned mul(unsigned short x, unsigned short y) { return x*y; } should be expected to return an arithmetically-correct result for all values of `x` and `y`, though it may behave differently on other platforms. In the language processed by gcc with optimizations enabled, however, such code may negate laws of time and causality on such platforms if `x` exceeds `2147483647/y`. Likewise, K&amp;R2 would suggest that given typedef struct TT1 { int x; } T1; typedef struct TT2 { int x; } T2; union U {T1 v1; T2 v2;} uarr[10]; int readT1a(int i) { T1 *p1 = &amp;uarr[i].v1; return p1-&gt;x; } void writeT2a(int i, int v2) { T2 *p2 = &amp;uarr[i].v2; p2-&gt;x = v2; } int readT1b(int i) { return uarr[i].v1.x; } void writeT2b(int i, int v2) { uarr[i].v2.x = v2; } the behavior of `readT1a` and `writeT2a` should be equivalent to `readT1b` and `writeT2b`, respectively. In the language processed by gcc with full optimizations enabled, however, the first pair of functions may yield meaningless code in cases where the Standard would fully define the behavior of the other pair. Personally, I'm inclined to regard the language described in the K&amp;R2 book as "real" C, and the one processed by gcc with full optimizations enabled as an impostor, but programmers need to be aware that it describes the behavior of constructs which some compilers will process in nonsensical fashion. 
C# is closer to Java than any other language so approach learning C as if you are completely new at the language (because you will be.)
Better and simpler than trying to pick a random exponent is generating a random number in the half-open interval [1,2) and subtracting 1.0 from it. This will ensure that the probability of any particular exact number being chosen will be uniform throughout the range. If one were to try to pick the `double` value that is closest to some real number uniformly chosen from the half-open range [0,1) then the range of real values that would map to e.g. 0.75 would be twice as big as the range that would map to 0.375.
There is one scenario where null-terminated strings work better than any alternative: passing string literals to functions that will act upon characters sequentially. That's a such a common operation, even in really tiny programs, that a small gain in efficiency there would have been worth some sacrifices elsewhere. Unfortunately, C has yet to provide any practical way of expressing string literals in any other form without creating named symbols for them. One could do something like /* Define a length-prefixed struct called `helloThere` intialized to hold `Hello there!` */ LSTR_LIT(helloThere, "Hello there!"); showBetterString(helloThere); but that's a lot less convenient than being able to put a string literal directly in the function call. Were it not for that, I think zero-terminated strings would have been abandoned ages ago. 
In many cases, a key requirement for numerical stability is that rounding be performed in predictable fashion at controlled places. The use of a *slightly-extended*-precision type helped accomplish this in addition to improving the efficiency of floating-point math on machines without hardware floating-point support. It's too bad ANSI botched C's treatment of `long double` forced many compiler writers to choose between breaking the language or using broken floating-point semantics, and as a consequence many chose the latter. 
You haven't given enough information to provide any kind of specific help, so I'll give some big picture ideas. Segmentation Faults usually occur when an invalid pointer is being de referenced, something like printf ("%s\n", s) // if s is NULL, this will SEG_FAULT 
Do post some example code. It could be that whenever the if statement is empty, the offending operation is optimised out. Hard to say without seeing the code, though.
It was my introduction, and I found it useful. 
C# is as similar to C as Java is. 
Most computations produce values that will be used briefly and never needed after that. In many cases, the value in a container will cease to be useful at the same time as another useful value is produced. If all containers are the same size, code won't have to search for a place to store the new value--it will instead be able to simply reuse the old container. Reserving space for all necessary containers in advance of a sequence of calculations and then using the same containers over and over again will generally be much faster than having to find space to hold each result individually. It would theoretically be possible to compute an upper bound on the amount of space a calculation could require, knowing only the size of the operands, and then allocate enough space to hold the largest possible results, but for most non-trivial calculations this upper bound would so vastly exceed the capacity of any real-world machine as to make such allocations impossible. 
First error says main() needs to return an int so change your method to int main() and return a 0 it a 1 (success or failure) at the end of the main function. Second error is telling you you are spanning lines in your single quotes use. You can't do that. 
Agree on Autotools, it can take a reasonable effort to get a handle on, but its very low maintenance once you work it out, very portable to unix like systems, and it can do a lot more than the basics (even has a testing framework). There is little need for anyone to actually write a Makefile by hand. 
the second error is that he used a forward slash instead of a backslash. It should be '\t'
As /u/wsppan says, the first warning is just that ```main()``` isn't returning an ```int``` and it should. If you aren't flagging warnings as errors you can ignore it, but it costs little to fix it. just change it to ```int main()``` and throw in a ```return 0;``` at the end before the closing brace. The second item is an error; you are checking for a tab character, but that is ```\t``` not ```/t```. Easy fix, just swap the slash. The error message itself takes a bit more explanation but is ignorable for now. If you don't believe me, this should explain it: https://stackoverflow.com/questions/7755202/multi-character-constant-warnings
Everyone should know the basics of make, but I personally really like meson and ninja for my own projects
Thank you guys so much!
1) GNU Make is your friend. 2) There is nothing to fear. There are build targets and recipes. Targets can depend on other targets. Your recipes will express how to compile C and C++ source files into object files, and then link object files into executables. That's about the gist of it. You learn these couple bits, and while it's all very manual and verbose, you can write a makefile that will build your application. Everything else is variables and macros and embedded scripting that make things very fancy indeed. Don't get fancy. Make is not related to version control, and version control doesn't know anything about make. You should be using version control - it acts like a glorified undo stack across the whole project at a time. It's literally the first thing you should always do to start a project. Git is popular, but Mercurial is WAY easier. There are scripts to convert between the two. But also, there's nothing to screw up. Make isn't going to change your source files, it's going to read them and write output files. 3) No, doesn't matter. I know guys who use makefiles to run home automation, literally `make coffee`, and the grinder kicks off in the kitchen. 4) You're not missing anything. The thing is it's an old and generic tool, limited only by your imagination. There's fancy stuff you can do to try and make complicated builds that much simpler, complicated makefiles that much simpler. But the problem is everyone thinks they're make gurus and they try to use the fancy too much, making their processes complicated. 5) CMake is it's own devil. You see, everyone thinks make is hard, so they've tried to make all sorts of tools to simplify it. They all basically fail. CMake is no exception, and yet it's very popular. What makes it popular is you can describe a build process in CMake files, and then generate platform specific build scripts from that. It basically generates makefiles, or on windows, Visual Studio solutions. There are advantages to CMake, but its also complicated in its own right and very poorly documented (oh, there's a shitton of it, it just all sucks and typically omits the one obscure detail about CMake behavior you need). If you're building on Unity or actually really anything, most IDEs should be able to integrate libraries and headers into your project. CLion especially. Or VS Code. If you find you can't do it, you're just missing it.
Here is an example of a buffer-less moving average: https://www.daycounter.com/LabBook/Moving-Average.phtml
[These instructions have saved me a ton of headache.](https://stackoverflow.com/questions/3718998/fixing-segmentation-faults-in-c) I'm going to write them out here anyway (bc I hate when people just drop a link and go on their merry way lol) 1. Compile your application with \`-g\`, then you'll have debug symbols in the binary file. 2. Use \`gdb\` to open the gdb console. 3. Use \`file\` and pass it your application's binary file in the console. 4. Use \`run\` and pass in any arguments your application needs to start. 5. Do something to cause a *Segmentation Fault*. 6. Type \`backtrace\` (or \`bt\`) in the gdb console to get a stack trace of the *Segmentation Fault*. This will look like $ gcc -c program.cpp -o executable -g $ gdb executable -- This will open up the gdb console -- (gdb) $ run &lt;command line arguments&gt; -- This will show you what's causing the seg fault -- (gdb) $ backtrace -- This shows an in-depth breakdown of the seg fault -- I hope this helps, OP! &amp;#x200B;
I really only split the program up into separate source files to mess around with make. The whole program is [here in one piece](https://pastebin.com/jR72iS0B) for your convenience. I tried to optimise for good readability, structure and practice. Hope I succeeded.
A game can be drawn even if some squares are empty. If, for example, a game starts (numbers mark the sequence on which moves are played) X1 X3 -- -- O2 -- -- -- -- with O to play, and players block any two-in-a-rows, the next four moves will be forced: X1 X3 O4 O6 O2 X7 X5 -- -- and the game will be drawn, since neither player would be able to lose. 
Thanks for the tip! I plan on improving the program and adding features as I learn more about C. What are your thoughts on the quality of the code?
I think you're going to get acquainted with almost all types of build systems if you end up building sth you'll see on github from source. Your knowledge will build up. On what build system is good: I say don't worry about it much, but yeah like most of the others I'd say cmake, because it's the best (or least worst) that you're going to see. Having said that I should state that I haven't messed around with ninja. If you're making your own hobby project, and you dont work with other people(or operating systems), you should use the simplest thing possible (even a shell script), and then once you start seeing problems when your project gets more complex, you can build up to cmake. This way you wont spend a tremendous amount of time setting up stuff too early on, before you even write some simple stuff. Also makefiles are a good thing to know, because you may see them, or have to use them at some point, but keep it simple is what I suggest!
Oh wow, someone who actually finds caching and memory bottlenecks exciting, rather than just annoying things we need to deal with! That seems like a great way to look at programming! Well, unless they end up coding everything in assembly or eschewing any appropriate use of OOP to optimize every memory access...
The approach you use to find three-in-a-rows would quickly become unmanageable for anything larger than a 3x3 board. I'd suggest considering how you'd design a 4x4x4 game, though if I were trying to write a practical strategy engine going that route I would consider using values of 1 and 8 for X and O, so that one could count how many X's and O's there are on any particular line by adding the values. The bitwise-operator approach would be better for learning how to use bitwise operators, and would suffice for identifying wins and draws, but the addition approach would be better for identifying places one needs to move to prevent an opponent from winning.
All my programs henceforth will be written in unreadable C, with the sole goal of performance. In fact the shorter the line the better, and I'll make sure to minimize comments to keep the file size as small as possible ;). I'm kidding of course. And indeed, I do find this stuff cool! But don't worry, my first priority when writing any actual software would be maintainability and readability. Still, this stuff is real cool to know about, in case I ever do have have to dip down into the lower levels to to fix a bottleneck. Thanks for the comment by the way!
This is cool. Here's a good writeup on how to build an optimal AI opponent in case you decide to work on it more. [It's in Java but shouldn't be too tough to convert](https://medium.freecodecamp.org/how-to-make-your-tic-tac-toe-game-unbeatable-by-using-the-minimax-algorithm-9d690bad4b37)
[https://pastebin.com/Key3Ngfv](https://pastebin.com/Key3Ngfv) is an example, forgot to put it in the top disregard the function at the top
Depending on your needs, you might want to try a streaming parser (SAX style). If you don't actually need a structure that represents a complete JSON object in one place, you can pretty easily parse JSON without any dynamic allocation whatsoever. If you plan to have the code search ultimately search through the data for things anyway, this might work well. I wrote a JSON parser like this as an exercise that is extremely fast, doesn't use the standard library whatsoever (so no dynamic allocation). It clocks in at around 450 SLOC. If you're into high performance and scalability, one of the nice things about this kind of parser is that it works in a small constant amount of memory, so you could parse a 50 terabyte JSON dump with a few kilobytes of RAM, for example. If you really want performance, streaming is the way to go. You could also consider doing both: write a streaming parser that just fires callbacks, and make an "object builder" thing that can work on top of this.
Thanks for the reply! I didn't think about streaming on read. One of my goals was to make this to where someone could also "build" a JSON object and write it to disk as well. This way they could also load a JSON file and modify its contents, then save it to disk. Maybe I'll take your suggestion and do both. That way the user can just simply stream for JSON values without needing to build a huge structure in memory if they are only reading data.
&gt; without any dynamic allocation whatsoever If you want to detect input errors, you will need a stack at least as large as the input's deepest nesting. Otherwise, yes.
&gt;Anyone have any ideas on a possible better approach? https://github.com/zserge/jsmn
&gt; I've already implemented a nice string struct before with create, destroy, clone, set, etc. functions You are thinking like a C++ programmer, which is understandable. If you want to actually understand C rather than reimplement C++ you need to change your approach. Our string "struct" is a null terminated array of characters. Dynamic creation is done with malloc, free, memcpy etc. Wrapping these in a C++ style interface is rarely useful. I would suggest you implement your array as a linked list and your object as a hash table, there are multiple implementations you can choose from. Object polymorphism can be achieved by having a common parent type which stores the type allowing you to dynamically cast if required, anonymous structs are handy for this. Dynamic memory allocation is going to be necessary and shouldn't scare you. If you are consistent about your mallocs and frees it can be managed. Alternatively talloc will manage it all for you and is a good fit for your needs. Use valgrind to debug and identify memory leaks.
Thank you for your reply. I will agree, I started developing in C when I was 13 and switch to C++ around 18 and have been using it ever since (with Java and others mixed in). So I most likely am thinking like a C++ programmer. I'll try to rethink this in a C way. A question I have is what about the user interface? Should I expect the user to manage their own memory for it as well? Would I still provide create functions too? The way I've designed it so far is in [https://gitlab.com/cwink/cjson/blob/master/include/cjson.h](https://gitlab.com/cwink/cjson/blob/master/include/cjson.h) and [https://gitlab.com/cwink/cjson/blob/master/source/cjson.c](https://gitlab.com/cwink/cjson/blob/master/source/cjson.c) (I'm sure my interface is messy).
Space is free. Leave them on 3 lines.
You need a constructor, which would look like this class Entity { public: int health; int damage; int speed; Entity(int h, int d, int s){ health=h; damage=d; speed=s; } }; Entity goblin(100, 15, 5); Once you are confortable with writing constructors, I suggest looking at initializer list, which are even shorter 
That worked. Thank you!
Fair enough. 
i am a complete newbie at reading json, and i have been considering writing my own implementation of json, because i have been getting interested in interfacing with this reddit gizmo... it looks like you can just add a ".json" onto the tail of pretty much any reddit url and you get json, like this thread's url, for example: https://old.reddit.com/r/C_Programming/comments/aswyia/json_parser_in_c.json note that i use the "old." version of reddit, instead of the "www." version.. happy coding, zero/. 
you say that you are shallow regarding c and c++, and so i am going to tell you that i have been coding for 35 years and i have written code that is as much as 25,000 lines long, and i see no purpose for using a MAKE file, in fact i dont really know for what i need it for... if my source file is "hello.c", i just compile it: gcc hello.c -o hello done. i read in this thread that somebody uses "MAKE coffee" to control his coffee maker, but i cant see any reason to do this... if you are a beginner at C, it seems to me that there are many other things to focus on, like actually writing some code. later.
There are a bunch of ways to do it, personally I would start by using talloc. Most importantly however you should be explicit about memory management. Any function which allocates memory and/or returns a pointer should specify in the documentation how to manage that return value, be it free(), a destructor function etc.
&gt;people say C is a difficult language... back in 93, i was doing eight months in jail, and i handwrote C code for a client using a pen and legal pads, double spaced... i wrote a program designed to compile using microsoft's quickC for dos, it was a flat file database manager with full on-screen editing, including arrow keys, backspace, delete key, tab between fields, create record, edit record, save record, and delete record.. it was an editor of a datafile that contained name/address/city/state/zip/phone/comments i embedded one of my cellmates phone numbers into the code, for my future reference, as a comment, like this, so i would have his number upon release: //phu 276-9212 he was vietnamese, if i remember correctly... so i would mail out the code to my client/friend, and then i would wait about four days and then i would call my client and confirm that the code worked properly, and when i called my friend he told me that the code worked fine, and he told me that he stayed up all night looking in various computer books for the "phu" command, lol... omg, there is no phu command, i told him.. phu was this vietnamese kid that got gang raped in one of the cells and he was terrified, so i befriended him and i asked him for his phone number, i told him that i would call him when i got out, which i never did call him.. my friend/client paid me fifty bucks per week while i was in jail, which was the maximum that i could spend at the jailhouse commissary, and he gave me a $1,500.00 bonus on the day that i got out. it was like beautiful when i walked into his office and i saw that sexy program running on that 286 pc clone, because i had only seen it in my head and on paper. i almost cried when i saw it. now, let me give you a very good reason to focus on C... First, straight C is so tight and fast, and second, you aint gonna code no silly C sharp on an arduino.. /r/arduino if you write straight C, you can easily use the gcc compiler on most linux boxes, lol.. if your C program is "hello.c", it is so simple to compile C from the linux shell, like this: gcc hello.c -o hello and if for some odd reason you can code C# on a linux, just dont and say that you did, lol..
Even for learning, I don't start a programming task without looking at prior art. Reading other people's source code, good or bad, and evaluating existing solutions is a *great* way to learn what to do and not do. Going from scratch leaves you vulnerable from making the same mistakes many before you encountered.
&gt; Even for learning, &gt;vulnerable from making the same mistakes many before you encountered. These two don't compute together. The whole purpose of learning is to make mistakes (not intentionally, of course). I'd argue that the best learning comes from tackling a problem as best you can yourself, building up the problem's model and context in your head, and then deriving maximum benefit from contrasting your solution against better ones. 
They are very different with some basic concepts that are similar. I suggest learning python as a base for simple programming paradigms and OOP then c++. I personally don't recommend or enjoy programming in Java. Python is very powerful so I'm not suggesting it is only useful for beginner level programming just that it will be the easiest to learn first. Golang is the hot new(ish) language too so you should at least look at it around the time you learn c++. 
If the input can be reread an arbitrary number of times, the total number of bits of storage required to validate a JSON file of size N is O(lgN), regardless of the lengths of the keys involved. Having a stack will definitely improve performance, but isn't strictly necessary. If one may need to handle files that contain very deeply nested structures, one could use an algorithm which uses a stack to keep track of the innermost few layers of nesting, and a single position-based breadcrumb to handle everything outside that. Given the location of a start of an item within a JSON file, it's possible to determine how deeply nested it is and the location of the start of the parent. That in turn could be used to located the grandparent, etc. Obviously keeping track of the locations of ancestors will be faster than having to re-search for them, but one can validate files of essentially arbitrary size given limited storage if one can fall back to using a position-based breadcrumb if there's insufficient storage to keep track of ancestors more directly.
Python is one of the easiest languages to learn, and allows you to make an interesting or useful program pretty fast and with minimal frustration, so I’d definitely recommend you learn it first—the earlier, the better. *Automate the Boring Stuff* is a pretty interesting book that you can find online for free, and it’s the way I learned it. Java is an object oriented language, meaning that the way you organize your program might be different than Python or other non-object oriented languages, so you’ll get used to thinking about a problem in more than one way. C and C++ can be a little frustrating at times, and have a lot more “sharp edges.” I’d recommend learning both of them after Java; by that time, you’ll hopefully have experience finding and fixing bugs. It might be annoying, but they’re worth learning. Just a word of advice: don’t get married to one language. I have friends who learned Java in their freshman CS class and swear by it as “the best language.” Each language has its own unique strength and weaknesses; don’t limit yourself by only ever using/practicing your favorite
If everyone tried to make wheels instead of using the wheels others have make, we'd have gazzillions of wheels and no cars. If someone trustworthy wrote a code, useful to your project, use it.
If you're doing a direct comparison of the entered password to a string literal (ie something like strcmp(input, "myPassword");) there's a good chance it'll be viewable in the executable just by opening it in a text editor. Try it yourself, open your executable with notepad++ or even just regular notepad and skim through it. If it's quite large, use ctrl+F and type in the password you've hardcoded.
This is a C sub, not C++. There are links in the sidebar for C++ relates subs.
/r/cpp is over there This is a C sub.
I have a friend who works for a stock exchange and they exclusively use C++, because they measure everything by nanoseconds - they also laid off most of their Java devs. If you're interested in high frequency trading then C++ is really the way to go, or C. If you're looking at anything else, I'd recommend starting with python. I don't recommend Java, because it's like C++ but it treats *everything* as an object, and that's really not helpful, especially for someone trying to learn a language. OOP is cool, but needlessly inheriting, as taught by most courses, will give you more work than necessary - aggregation will usually get you there. Also Java's generics are nowhere near as powerful as C++.
If you like OOP at all, I recommend that you research Abstract Data Types and Opaque Pointers, they'll give you some semblance of objects. Also, check out creating your own virtual table. C# (hell, even C++) give the luxury of failing early during compilation, but C might not even issue a warning, which can translate into things going awry without notice. Every piece of code you write should have tests that pass in *both* GCC and Clang. There are also two types of variables, those on the stack and those allocated on the heap. Stack ones disappear immediately upon returning a function, but anything allocated needs to be manually freed. Use const where you can, that's just advice to keep your programs simple and easy to reason about. Finally, don't listen to the haters, get a decent IDE immediately. I've recently fallen in love with Visual Studio Code. It parses your make file and will tie together your references.
I would create a ring buffer abstract data type with an additional function to return the average of the contents. Your ring buffer adt would have a header like struct float_rngbuf; struct float_rngbuf* create_buf(int NumValues); void Addvalue(struct float_rngbuf* buf, float val); // or double float getAverage(struct float_rngbuf* buf); // or double void destroy_buf(struct float_rngbuf* buf) You don't need to remove values, at least right now. Is it overkill? A little, but I use ring buffers all the time, so they're an absolutely wonderful tool to keep as a library.
Oh, you think darkness is your ally. But you merely adopted the dark; I was born in it, moulded by it. I didn't see the light until I was already a man, by then it was nothing to me but BLINDING!
None of the above: Learn C
Definitely study Python. It’s simple, it’s quick to write code in. You’ll soon be able to hack together pretty good working prototypes pretty quickly. C++ is much faster, but, at least for me, it’s not as simple to write. Also, its error messages are often very hard to understand: you can simply forget a single symbol in your code and get a huge-ass error message. Python is much more friendly in this sense. Moreover, I know absolute experts in C++ who’ve been studying it for ages and ages (like _tens_ of years), are writing code in it every day and many say it’s hard AF, and you’d need at least some five years to even be able to really understand what you’re writing and how to write code to do the stuff you want.
Lots and lots of practice and experience, starting with small problems and working your way up to large ones. A good place to start would be the [Advent of Code](https://adventofcode.com/) website, which has ~200 problems and [its own subreddit](/r/adventofcode/).
Not to be rude but it sounds a bit like "I know how to draw circles and lines now show me how to paint the Mona Lisa" But basically if you want to know how every part of a game works, [handmade hero](https://handmadehero.org/watch) would be a start but it's kind of a lot.. he shows how to build a game in c from scratch. If you just want to learn how to build a simple game ASAP, just use a free game engine like Unity, try some stuff and if you're stuck, just search for tutorials on the thing you are trying to implement. In Unity you can implement simple behavior without code and if you want to do something more complex you can code it in C#. Regarding JS I have no Idea. Hope this helps :)
Thanks so much 
No matter how you hide the password in your program, a determined person can run the program in a debugger, and alter or skip parts of the program. You decide how much work to put into it, to make the crack more difficult.
Frankly, it's easier to just run the program in a debugger and bypass the check than try to figure out the password, even if it's not obfuscated.
Some C libraries print "(null)" instead. I don't know if it is in some standard, and not a big deal anyway. Just fyi.
i did i looked in to unity already but there is alot that i want my game to do that i would have to right the code for but im really hoping that handmade hero works thanks for the info
also advent of code is really great sit at frist glance
that is how i fell lol
You are bit harsh if player makes a typo in entering coordinates :) Apropos: I see you use unix (from the Makefile). Using the mouse for xy-input on xterm or equivalent looks surprisingly easy. Haven't tried myself though.
yeah thanks that's exactly what i need now i just need to learn more abt windows sockets to use them but you made it clearer for me so yeah thanks a lot :D
&gt; If you're into high performance and scalability, one of the nice things about this kind of parser is that it works in a small constant amount of memory, so you could parse a 50 terabyte JSON dump with a few kilobytes of RAM, for example If you're not using dynamic allocation, I assume a few kilobytes weren't stack memory. Were they all static memory, then? Did you attempt to make it multi-threaded, at all, or is limited to a single thread due to the static memory?
Calling that a "parser" is a stretch. It's a simpler tokenizer.
&gt;Our string "struct" is a null terminated array of characters. Dynamic creation is done with malloc, free, memcpy etc. Wrapping these in a C++ style interface is rarely useful. Terrible advice. Relying on the null-terminated string functions is the first mistake a C programmer can make. The second mistake is actually using the stdlib functions to manipulate stringz. 
This subreddit is exclusively about programming in C. Please try /r/learnprogramming for general advise.
&gt; Now what I want to ask is, after making an executable file out of that code, could someone hack the EXE file, see all the code and see the password? Yes. There are ways to prevent that, but none of them really work when faced with a determined attacker.
C++ is off topic in this subreddit. Please post C++ projects to /r/cplusplus instead.
C++ is off topic in this subreddit. Please post questions about C++ to /r/cplusplus instead.
This subreddit is about programming in C only. Please post C++ questions to /r/cpp_questions instead.
sorry about that was not sure where to go thanks for not just out right banning me or something thanks for the advice
Nah, it's fine. Many people make that mistake. Don't worry.
If everyone only used the wheels others made, we'd forget how to make wheels. Eh, kinda clunky, but I think you get my point. Retracing the paths other went before us is how we learn.
Start with the JSON spec and create a data structure that represents a JSON node. Use a union to save on memory. All json nodes have sibblings and children. When building or printing you will always walk a nodes children and then the next sibbling. Alot of your library functions will need to be recursive. 
I have used this library in past projects. K.I.S.S https://www.google.com/url?sa=t&amp;source=web&amp;rct=j&amp;url=https://github.com/DaveGamble/cJSON&amp;ved=2ahUKEwiB5JyTzczgAhVBc98KHcMjCl4QFjACegQIAhAB&amp;usg=AOvVaw2NdJ2zk9c9OOnNjUgFLEUV
If he was out to make wheels, then godspeed, but his objective is to make a car, he shouldn't make detours to make wheels, wipers and such. If the available wheels don't turn as fast as he'd like, then he can build their own.
I really like Beej's guides, here's the C basics one: https://beej.us/guide/bgc/html/multi/index.html
If you hard-code the password in your code, then that string will be literally represented somewhere in the final binary file. Even easier for someone examining your program: all variables and constants are placed in some continuous block, so they wouldn't have to check the entire executable to find it. Password obfuscation (and generally, code obfuscation) is a major topic in the programming field, you're not going to get an easy answer here on Reddit. If at some point you write non-trivial programs, you should consider researching this topic in depth.
mk is a lot of fun. Sadly, there is no good UNIX port available.
&gt; These two don't compute together. &gt;The whole purpose of learning is to make mistakes (not intentionally, of course). So you're saying it's impossible to learn from the mistakes of others by observing, and people need to put their hands on the stove to learn it will burn them or jump off a cliff themselves to learn it can be fatal? Maybe you can lead by example then, because my experience has shown that *not* to be true. I can study a dozen implementations *LONG* before you can write one. &gt;building up the problem's model and context in your head While likely missing a dozen bugs and countless brilliant optimizations won by the numerous hours already spent by those that came before you. &gt;and then deriving maximum benefit from contrasting your solution against better ones. What benefit can come from reinventing the wheel from scratch? It's been invented already. The smartest time is spent studying the most used wheels, learning from it's evolution, and possibly finding a way to make it better.
Thank you for the advice, but I treat these kind of things like I do puzzles. I only look at the solutions if I simply give up (which I'm pretty hardheaded when it comes to giving up), or at the end of my puzzle to see what I could have done to better solve it. It's more for fun than anything.
True. Originally, I had an assignment, but tried to be more concise. 
If your IDE has it you can use a memory browser to hop into the memory location and see what’s going on as you step through code. It shows just the raw data stored.
What if... you encrypt the message using the password? That way, the program doesn't need to know the password.
Good idea. 
Thanks so much
Yes I heard similar thoughts in r/financialcareers
If you implement a 4x4x4 game (which I really would suggest; such implementations games were popular in the 1970s), I'd suggest giving some thought to how you might keep track of the relationship between squares and lines. There are 48 lines where one of (x, y, z) changes, 24 where two of them change, and 4 where all three change--76 total. The 64 positions can each belong to 4, 5, or 7 lines, and I'd suggest you figure out a way to score each line that contains only X's or only O's according to something like the following (assuming the side on move is playing X): three X's -- +10000 three O's -- +2000 two X's -- +450 two O's -- +80 one X -- +16 one O -- +4 empty -- +1 For each square, add up the total score of all the lines in which it participates. I think picking the square with the highest score will be a winning strategy, though if not you might want to experiment with tweaking the numbers. Clearly the three X's score should be high enough to dominate anything else--if you can win, do so. The three O's square should dominate everything but three X's. There's no need to block an opponent if you can win directly, but any other play would lose. If there are no open threes on the board, a square on two lines with open double X's, should dominate because taking that square will win even if O would also have such a square. If X doesn't have a square but O does (and there are no open threes), failure to play in that square would be a loss for X. If none of those situations apply, it's not entirely clear how lines with 0 or 1 pieces should be weighted relative to a line with two, but it might be fun to experiment with that. 
Without some code their in no way you're going to get a definitive answer. &amp;#x200B; One possibility is that your `if` statement's condition has some form undefined behavior (ex: dereferencing a NULL pointer). When you remove the body of your `if` statement the compiler may be optimizing out the condition check entirely. &amp;#x200B; If you really can't post the code, at least run through valgrind and post the output.
Every implementation I've used has printed "(null)" in place of a NULL pointer, but the standard mentions nothing of it: &gt;**s**: If no **l** length modifier is present, the argument shall be a pointer to the initial element of an array of character type. Characters from the array are written up to (but not including) the terminating null character. If the precision is specified, no more than that many bytes are written. If the precision is not specified or is greater than the size of the array, the array shall contain a null character. §7.19.6.1
&gt;So you're saying it's impossible to learn from the mistakes of others by observing, and people need to put their hands on the stove to learn it will burn them or jump off a cliff themselves to learn it can be fatal? Not at all. What I am saying is that for a beginner, it absolutely makes sense to spend hours of playing around with code before having to look at model solutions. Getting into the habit of looking at good solutions up front denies them that experience and that learning that comes with practice. For more experienced programmers already fairly familiar with the domain, it might make sense to look at some sample solutions, but even they would benefit (given the luxury of time) to "fool around" a bit. &gt;Maybe you can lead by example then, because my experience has shown that not to be true. I can study a dozen implementations LONG before you can write one. That might be because (correct me if I'm wrong) you are already an experienced programmer. Also, the point is moot. It is trivial to read a solution and see the "happy path" as it were. However, if you are not familiar with the domain again, when things go wrong or specs change, it would be much harder to figure out the solution than if you had put in the time to get familiar with the domain through experimentation/prototyping. &gt;While likely missing a dozen bugs and countless brilliant optimizations won by the numerous hours already spent by those that came before you. Again, you seem to be missing the context of this thread entirely. OP is clearly a beginner who is trying to learn C by writing a JSON parser. I don't think your point is apropos. &gt;What benefit can come from reinventing the wheel from scratch? It's been invented already. The smartest time is spent studying the most used wheels, learning from it's evolution, and possibly finding a way to make it better. Again, I disagree. It reminds me of the anecdote that I'd read of the Brontë sisters who were, rightly so, considered young prodigies. The part which people don't remember though is that it took them years of scribbling through countless hours of mundane stories and mediocre writing in their childhood (as evidenced by the discovery of their childhood notes) before their styles started getting polished enough to be considered brilliant. The world only got exposed to their polished end-results, but never even saw the tomes of "bad writing" that took to get them there. Your point stands for an experienced developer who has a time crunch to deliver on a task. It certainly does not apply to someone who is just starting out.
Good approach.
I thought this was a joke or a typing error, but no, this article really is about making Win16 applications. Quaint!
Funny, the check is in printf of V7 unix, but not in BSD 2.11 [http://www.retro11.de/ouxr/u7ed/usr/src/libc/stdio/doprnt.s.html](http://www.retro11.de/ouxr/u7ed/usr/src/libc/stdio/doprnt.s.html) [http://www.retro11.de/ouxr/211bsd/usr/src/lib/libc/pdp/stdio/doprnt.s.html](https://www.retro11.de/ouxr/211bsd/usr/src/lib/libc/pdp/stdio/doprnt.s.html) V6 seems to print "" for NULL, hard to decipher :) [http://www.retro11.de/ouxr/u6ed/usr/source/iolib/printf.c.html](https://www.retro11.de/ouxr/u6ed/usr/source/iolib/printf.c.html)
Another fun thing you can do is write 32 bit programs to work on Windows 3.11 (!) through the Win32s runtime. I did this to make a program that works on Windows 3.11 through the 64 bit edition of Windows 10. With a bit (a lot) of runtime checks and dynamic loading the thing was even using visual styles, multi monitor high DPI compliant etc on Windows version that support it. 
Depending upon the nature of the message, one could include within the program a list of words that are in the message, along with some that aren't, and then use the message to encrypt a list of indices for words in the list. If one did that, many possible passwords would yield plausible-looking messages, but most of them would be meaningless. Typing in the correct password, "bunny", might yield the message "The treasure is located 5 miles east of the museum", while typing in wrong password "rabbit" might yield "The treasure is located 9 yards west of the station", and wrong password "harvey" might yield "The treasure is located 12 blocks north of the library". Depending upon how the program is designed, it may be impossible to determine from the program which password is correct. Imagine, for example, that the program is used for a treasure-hunting contest. Before the contest, the sponsor might try entering some arbitrary passwords, recording what locations it prints out, and then pick one of those locations to hide the treasure. Since the program would have no way of knowing which, if any, of the locations it had could output had been chosen to receive the treasure, even the most skilled analysis of the program would have no hope of revealing that information. If the program was capable of outputting many different plausible locations, the most that could be gained by examining it would be a list of all possible locations it might report. If the list would contain enough locations as to make searching them all impractical, it may be impractical to try to find the treasure unless one has the correct password. 
Unfortunately, from what I can tell, the C language favored in academia has morphed from being a language that was suitable for low-level programming into a language which retains all the hassles and dangers of a low-level programming language but ditches the ability to exploit low-level features of the target machine in ways that higher-level languages can't. C was designed to program the kinds of machines that were in use at the time, and which are still in use in small embedded devices, with compiler-complexity vs. execution-speed tradeoffs that were appropriate in the 20th Century. Tasks that don't require low-level programming would be better served by higher-level languages that are a better fit for today's world, than by a version of a low-level language that strips out all the low-level features, and those that do require low-level programming would of course be best served by a implementations that are designed to support it. 
Do you mean C++? Any C I’ve seen in academic use has been ANSI C, which is the original standard...
^/* Main window class and title */ static const char MainWndClass[] = "Win16 Example Application"; Now to hear the rants about them using the term "class".
&gt; Getting into the habit of looking at good solutions up front denies them that experience and the learning that comes with practice. I completely disagree. I've learned *WAY* more from studying well engineered solutions than from having to slog through all the same mistakes everyone else did on their way up. What you're suggesting is akin to saying people should learn alchemy before learning chemistry, or astronomers should learn the heliocentric model before learning geocentrism. Hell, might as well learn basic as a first language, with all it's bad practices before moving on to a more modern language. I can't for the life of me understand why *EVERY* learner should have to repeat *ALL* the classic mistakes for themselves, when they should be held up as an example of what *not* to do. &gt;That might be because (correct me if I'm wrong) you are already an experienced programmer. Meh, no doubt there are better. I do my damndest to not fall into the most common pitfalls, and turn out code that I can be proud of, and that's easy to support when I return to it years later. &gt;OP is clearly a beginner who is trying to learn C by writing a JSON parser. He said he's already written a parser in C++, so I wouldn't call him a beginner. He's improving his C skills, which is laudable. &gt;It reminds me of the anecdote that I'd read of the Brontë sisters who were, rightly so, considered young prodigies. The part which people don't remember though is that it took them years of scribbling through countless hours of mundane stories and mediocre writing in their childhood (as evidenced by the discovery of their childhood notes) before their styles started getting polished enough to be considered brilliant. I agree it takes time to "get good", but reading code is *just* as valuable as writing it. When you're parsing/groking the functionality embodied in other's code, you're going through the *same* sort of problem solving as you do writing it. &gt;Your point stands for an experienced developer who has a time crunch to deliver on a task. That's just it. I know from experience that it takes longer to write from scratch, than to study prior art. It informs a *lot* of the problem solving process before writing a single line. I find I can create much faster as a result, and avoid pitfalls that can be a real time suck. 
I don't know how the message could determine the indices. Surely, it would be, say, N bytes from a password hash? But if too many decrypted messages look plausible, then the player might not bother. But really, I was hinting at encrypted archives and possibly asymmetric encryption.
Cool! Does it work on ReactOS though?
The first thing that jumped out at me about the sample code was that it uses the Pascal calling convention. I didn't know Win16 did that. There's also a [companion piece about coding C for Win32](https://www.transmissionzero.co.uk/computing/win32-apps-with-mingw/) that would be more useful to the vast majority of readers. I'm interested because a current C daemon project of mine I intend to be buildable with MSVC, and for it and future projects I'd also like the possibility of cross-building from MinGW (*e.g.*, `apt-get install mingw-w64`). So I'm going to end up figuring out if there are any big differences between MSVS and MinGW when doing ANSI C. 
Tell me you did that for fun, and not to meet a business need. 
Holy nostalgia, Batman! &gt; he “WinMain” function is declared “PASCAL”, which expands to the “__pascal” keyword. This ensures the function is declared as a near function with the Pascal calling convention. 
That seems really cool. Is there a tutorial or example of this?
Also clang sanitizers cover much of what valgrind does these days.
Yes! Out of the box without any trouble. ReactOS is cool.
Ha, please no!
The 1974 C Reference Manual, both editions of "The C Programming Language", and many publicly-sold C compilers, all predate the publication of the C89 Standard. Implementations based upon those resources targeted many different platforms and were intended for many different purposes, and allowed programmers to exploit different sets of features and guarantees as appropriate for those platforms and purposes. The C Standard was written to describe a core set of features that were common to all implementations. It was not intended to describe everything an implementation must do to be suitable for any particular purpose (nor even, as noted in the published Rationale, *any useful purpose whatsoever*). There are many situations in which the K&amp;R books or other resources specified how a certain construct would behave, but where supporting such behavior would have been impractical on a few implementations. Rather than mandate that all implementations support such behavior, or attempt to classify implementations based upon what features they supported, the authors of the Standard instead opted to categorize such actions as invoking Undefined Behavior, allowing implementations to behave usefully or not at their leisure. I find it unfortunate that the C Standard does not bundle the associated Charter and Rationale documents, since those documents help describe how the authors intended implementations to behave in cases where the Standard imposes no requirements. The concept of "conformance" was described so loosely in C89 that one could have a "conforming implementation" which would unconditionally output the string `This is a diagnostic` and exit without even *looking* at the source file. Obviously, such an implementation would be of such absurdly low quality as to be useless, but it would nonetheless meet the requirements for conformance. Likewise, the fact that a low-quality implementation could "accept" almost any random file would make nearly all such files "conforming C programs". The only way the Standard can really be regarded as defining a useful language is if compiler writers recognize that failure to mandate that compilers should behave in useful fashion does not imply any judgment as to whether quality implementations should be expected to do so *anyway* if they don't document a good reason for doing otherwise. 
I think he's talking about a seven-segment display, though I'm really not sure either.
Whether it's a "solved problem" depends upon what one is trying to do. If e.g. people are supposed to find a password as part of a treasure hunt, having a program choose a number of words from sets of 4 to 32 by taking groups of 2 to 5 words from a password hash, in such a way that a significant fraction of arbitrary combinations would be plausible would mean that even a cryptographic expert who completely reverse-engineered the program would be unable to determine where the treasure was located. Note that in a typical treasure hunt, the goal for a legitimate player wouldn't be to reverse-engineer the password-to-clue program, but rather to solve enough of the earlier clues to discover the hidden password. The purpose of the encryption would be to discourage people from guessing at the password or trying to reverse-engineer it. If every password string one might attempt would reveal a seemingly-plausible place the treasure (or next clue) might be located, someone who wanted to try out a dozen guesses at the passwords would have no way of knowing if they'd gotten it right unless they visited all the places the treasure might plausibly be. By contrast, if the program used a more conventional form of encryption, trying out different passwords would be much quicker and easier. 
There is a way. You save only a reasonable hash of the password. So a SHA3-512 hash of the password would look like : $ /usr/local/bin/openssl version OpenSSL 1.1.1a 20 Nov 2018 $ echo "this is a passphrase" | /usr/local/bin/openssl dgst -sha3-512 (stdin)= 804c8fa9a648581203f806ceeab6c96a013742575269c0a9a7543b27ee3f5236a02c9025541df18f995b9df65153f0d7ec918af61783b14b9e8f7b499beacbe5 $ So that is the hash result string. Just save that data inside the software and you are done. 
Now do it with Linux and Gtk.. oh wait.. guess you can always use Motif (minus the high DPI ☹️)
Well, I don't even know OP's use case. All I know is there's a "secret message" hidden behind some arbitrary password. OP hasn't specified if false passwords should lead to plausible decryptions. If yes, then there is no password. It would just be a deterministic "random" location generator with a player-provided seed. 
It does not work. It doesnt show me an answer at the end. 
that isn't how scanf works also printf needs a character formatter, like %f #include &lt;stdio.h&gt; #include &lt;math.h&gt; int main() { float A; float B; float C; float discriminant; printf("To calculute the discriminant, input the the values of A, B and C\n"); scanf("%f", &amp;A); scanf("%f", &amp;B); scanf("%f", &amp;C); discriminant = (B*B) - (4*A*C); printf("the discrimiant is: %f", discriminant); return 0; }
I remember back in the day having to install the win32s runtime and the wing graphics library for windows games late in the life of win 3.11... good times :) Amazing to think such a powerful piece of software (3.11) was a 16 bit program.
lol thanks.
3.11 running in enhanced mode was actually [a 32 bit virtual machine manager](https://blogs.msdn.microsoft.com/oldnewthing/20100517-00/?p=14013/) running a 16 bit Windows standard mode VM and 16 bit DOS VMs! Win32s programs ran in proper 32 bit mode in the 16 bit Windows VM.
In my experience MinGW 64 is pretty darn good, not just on Unix but also on Windows itself. I did miss some more recent enum values and functions but those are easily declared manually.
1. Although there are technically different flavors of make, in practice it matters quite little until you do advanced things. Even then, the worst that could happen is that you could accidentally use a feature only in GNU Make but not BSD make. This isn't a big deal. Don't worry about it for a minute. And if you find a problem in the far future with portability, just fix it, and pat yourself on the back. 2. Don't be afraid of `make` at all. The worst thing that can happen is that you forget it needs tab characters to indent or it won't work right. Other than that, it's pretty hard to screw anything up worse. 3. There is virtually zero difference. On POSIX, I think the convention is to declare compiler flags as `CPPFLAGS=` instead of `CFLAGS=`. 5. "CMake" is a program to generate Makefiles from rules. Many, many other "build" systems do the same thing. It's basically an extra layer of abstraction over `make` for one reason or another. One of the major reasons is that CMake can generate the "project" file type that Microsoft's modern toolchains prefer over `make`, in addition to generating makefiles. I feel like this is probably the first time when I've seen a post where someone was especially concerned about `make` and not about the language, programming in general, or algorithms. ;) &gt; What is the defacto resource for make information? There's an O'Reilly book, but I keep an archive of interesting Makefiles that I find, and ones that I write. I have the start of a pretty good one for reproducible builds. When starting a new project, I normally start with a very basic Makefile from freehand. Here's one moderately-complex example: https://spin.atomicobject.com/2016/08/26/makefile-c-projects/ 
 $ gcc -MM *.c &gt;&gt; Makefile I never noticed this flag before. 
Your code is C++, and this sub is only for C; try asking in /r/cpp_questions.
This looks like C# not C. I don’t know C# but after 5 seconds of looking at this I think `Speak(message, VoiceGender)` probably needs to be `Speak(message, voiceGender)` as you want to pass the instance rather than the class.
I think you have the wrong subreddit...
Thank you that fixed it, sorry for posting on the wrong sub reddit its c#. I though it was for all c code language. 
C and C#, despite the names, are very distinct languages, like Java and JavaScript. (Indeed, i've actually had some people say that C# is closer to Java than to C ....)
&gt; (Indeed, i've actually had some people say that C# is closer to Java than to C ....) Apparently they're so close that when C# came out, it would compile straight Java code. I'm sure that doesn't include dependencies, and probably not a lot of the standard library, but still. Readers befuddled why this would be are invited to read about how [Microsoft tried to embrace, extend, and extinguish Java](https://en.wikipedia.org/wiki/Microsoft_Java_Virtual_Machine). C# was their own Java, with a few changes. And while both languages are popular, the battle over control of Java caused problems and stunted adoption. One of the problems was that Sun felt it couldn't risk releasing Java under an open-source license, especially not a permissive one. But without a good open-source implementation, the language wasn't seen as truly open. 
**Microsoft Java Virtual Machine** The Microsoft Java Virtual Machine (MSJVM) is a discontinued proprietary Java virtual machine from Microsoft. It was first made available for Internet Explorer 3 so that users could run Java applets when browsing on the World Wide Web. It was the fastest Windows-based implementation of a Java virtual machine for the first two years after its release. Sun Microsystems, the creator of Java, sued Microsoft in October 1997 for incompletely implementing the Java 1.1 standard. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/C_Programming/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
&gt; The first thing that jumped out at me about the sample code was that it uses the Pascal calling convention. I didn't know Win16 did that. Many old OSes did that, because it save instructions in the caller, useful when a function is called from different locations, when the called code is in ROM or when the called code is shared.
You should check the return value of scanf(). Here, it should return 1. scanf() is not really recommended for new C programmers. Better to use fgets() and sscanf(). I’d recommend looking them up.
[https://idownvotedbecau.se/imageofcode](https://idownvotedbecau.se/imageofcode) &amp;#x200B; What is your question exactly? Needing help can mean a lot of things. This sub has only one pinned post and it explains the exact issue with your post. Please read it. [https://www.reddit.com/r/C\_Programming/comments/9l0vuz/asking\_for\_help\_what\_to\_do\_when\_your\_program/](https://www.reddit.com/r/C_Programming/comments/9l0vuz/asking_for_help_what_to_do_when_your_program/) 
This reddit is for C, which is different than C++, which is very different that C#, which is very similar to Java. 
needs more blur
OMG, I remember, the Win32 extensions towards the end of Windows 3.1 and before Win95. Cool
Just pointing this out... you are trying to access certain index's inside an array that has already been free'd. To make sure you have properly free'd the mem allocated for *arr, just check if(arr == NULL) { do something here };
&gt; How many other operating systems can run applications written for a 30 year old operating system? z/VSE says "hi" and "do you remember DOS/360 back in '65?" &amp;nbsp; It's a pretty interesting article, even though it's not exactly the most useful thing in the here and now of 2019.
I would suggest you to further understand recurssion
Return NULL, to be on the safe side. It's not guaranteed to be at address 0x0, but it generally is. 
Not possible on every system I've ever written code for. See this answer: [https://www.quora.com/Why-does-virtual-memory-for-every-program-or-process-not-start-from-0](https://www.quora.com/Why-does-virtual-memory-for-every-program-or-process-not-start-from-0)
Downvoted for terrible attitude to someone who put in a lot of work to help you.
This is next level. Not just a screenshot, OP provides a blurry photograph of a fucking screen. Classic.
&gt; Return NULL, to be on the safe side. It's not guaranteed to be at address 0x0, but it generally is. Zero, cast to any pointer type, _always_ yields a null pointer, even on [architectures](http://c-faq.com/null/machexamp.html) where this isn't represented by an all-zero-bits object.
&gt; Is it possible that a Node will occupy the 00000... memory address to make this malfunction? It is not something you need to concern yourself with. By definition, returning 0 (which is the same as returning NULL, if the function has a pointer as its return type) will always yield a null pointer, and null pointer is guaranteed to compare unequal to any object pointer.
Returning 0 when you are returning an integer value and NULL when pointer makes your intent clearer. If I saw `return 0;` I'd personally assume the person wrote the code tried to return an integer 0 but somehow put wrong type in the function declaration. 
I totally agree with that.
You can technically use address zero like any other address. 
Use the [Common Gateway Interface](https://en.wikipedia.org/wiki/Common_Gateway_Interface) and put the compiled C program in cgi-bin/.
**Common Gateway Interface** In computing, Common Gateway Interface (CGI) offers a standard protocol for web servers to execute programs that execute like console applications (also called command-line interface programs) running on a server that generates web pages dynamically. Such programs are known as CGI scripts or simply as CGIs. The specifics of how the script is executed by the server are determined by the server. In the common case, a CGI script executes at the time a request is made and generates HTML.In brief, an HTTP POST request from the client will send the CGI program HTML form data via standard input. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/C_Programming/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
That
Your nodes did come from malloc et al, no? So they cannot ever be at NULL address. In a strange system, where variables live in very low addresses, even there address 0 would not likely cause trouble, because there are lots of misc variables and private C runtime stuff that gets the lowest addresses. extern char \*\*environ; and &amp;environ might be 0, for a contrived example.
Yes it is possible, for instance on an embedded system. In reality you'd have to go out of your way to make it happen.
I agree, this does seem possible.
While it is possible, you may safely assume that it doesn't happen.
C# is off topic in this subreddit. Please post C# questions to /r/csharp instead.
C++ is off topic in this subreddit. Please post C++ projects to /r/cplusplus or /r/cpp instead.
“It does not work” is not an error description. For future questions, make sure to explain exactly what you expect to happen and what happens instead.
&gt; You can technically use address zero like any other address. You can on some architectures, even when it is the null pointer, but standard C will never place an object there. You have to go outside standard C &amp;mdash; rely on some implementation-defined behaviour &amp;mdash; to do that.
Amazing!
Then it's time for you to RTFM once more! There are even more flags you never noticed before!
&gt; On POSIX, I think the convention is to declare compiler flags as CXXFLAGS= instead of CFLAGS=. Nope, that's just the difference between C++ (`CXX`) and C compiler flags.
 ok thanks for noticing
The difference is basically how the compiler should interpret the value if the program performs further operations with it. The type can also be reinterpreted with a type cast at any time, as long as the values are the same size (eg, int on 32 bit machines, long long int on 64 bit machines).
&gt; The difference is basically how the compiler should interpret the value You will note that I said "zero, cast to any pointer type". You are telling it how to interpret the value. Many C implementations literally have: #define NULL ((void *)0) in their `stddef.h` header file.
I think rather than saying that the language does not define this behaviour, it’s more appropriate to say that it’s specifically stated as “undefined behaviour”. The compiler is allowed to assume that, for example, pointers that you dereference can never be NULL (under some circumstances), which can lead to strange bugs: https://lwn.net/Articles/575563/
Jezzz, I've learnt a lot from this so now I wonder why do we use node / php / python for this instead of compiled c/c++? Is it only because it takes less time to write code for those? 
&gt; I think rather than saying that the language does not define this behaviour, it’s more appropriate to say that it’s specifically stated as “undefined behaviour”. You're right. The way I worded it, you could be thinking it was _unspecified_ behaviour, which is distinctly different from undefined behaviour. &gt; The compiler is allowed to assume that, for example, pointers that you dereference can never be NULL (under some circumstances), which can lead to strange bugs I guess what I'm saying though is that if you're on a system where having a null pointer refer to an object is something you might do, that same system ought to provide a definition for what it means to dereference a null pointer. Such a system would still be conformant.
It's been awhile but I think NULL (winapi) is #defined to `(void *)0`. 0 still is a valid memory address but I think the c standard treats any constant 0 as NULL. 
If I remember correctly, the c standard treats constant 0 as NULL. Yes 0 is a valid address but you would have to get it in another manner.
IIRC, CGI was a source of many, many detrimental security holes back in the day 
The benefit and drawback is that you can cast it to and from anything. An improper cast would blow up at runtime. 
Whether 0 is a valid memory address or not is implementation defined.
68k based macs used address 0. You could write values there and jump to it. I remember this because the way to force quit an app on 68k macs was to do "SM 0 0xA9F4" which set memory 0 to the ExitToShell trap instruction, and then a "G 0" to jump to it, thereby causing the app to exit. 
Because it's easier to write PHP or JS. The C equivalent of a functional PHP program would be several times longer. Plus the R-E-P loop of a PHP interpreter provides faster feedback than compiling, deploying and then running a large C program.
I see, I've learnt two things today
Yummy yummy yummy put that exploit in me tummy
For what it's worth, this works as expected (gcc 7.3.0): &amp;#x200B; #include &lt;stdio.h&gt; #include &lt;stdlib.h&gt; int main() { int factor_count = 13; int *factors = (int *)malloc(factor_count * sizeof(int)); for (int i = 0; i &lt; factor_count; i++) { factors[i] = i; } printf("%d\n", factors[0]); free(factors); return 0; } &amp;#x200B;
i am a fairly experienced cgi coder, in C, lol, but it has been more than fifteen years since i wrote any of it... the basics of cgi is that you write the following: #include &lt;stdio.h&gt; int main() { printf("Content-type: text/html\n\n"); printf("&lt;b&gt;welcome to my webpage!&lt;/b&gt;\n"); return(0); }//end of main then, assuming that the above file is named "mysite.c", compile the above, as follows: gcc mysite.c -o mysite.cgi and then, dont forget to do this: chmod 755 mysite.cgi note: somebody commented that you need to put your cgi code into the /cgi-bin subdirectory... this is correct and not entirely correct, at the same time.. when i was coding the web in C, i seem to remember that apache was the standard linux web server, and it has configuration options that allow you to put your ".cgi" programs anywhere that is web accessible and not only in the /cgi-bin subdirectory.. now, if you want to get really dirty, like i used to do, you could remove the apache web server from your linux box, and you could roll your own web server that actually contains your web code.. the positive thing about replacing apache with your own web server code that also contains your ".cgi" code is that you now control any configuration options, which will put you in total control over your web accessible application. also, if you roll your own web server that also contains your website code, ".cgi" code is now obsolete... ".cgi" is an extension that apache knows about, so if you roll your own web server, ".cgi" means nothing.. rolling your own web server means that you will have control over which types of files can be accessed in which subdirectories, etc.. to learn how to roll your own web server on linux(and windows, i think, tho i prefer linux), is the beej sockets documents: http://beej.us/guide/bgnet/html/single/bgnet.html happy coding, zero/. 
You can also do it entirely client side by compiling the C code to Javascript or WebAssembly using [Emscripten](https://emscripten.org/) or something similar and run that program on the users web browser.
cgi is one option. the other is to write the code that handles the web request in some other language and then make a "system" call to execute the c program. cgi is very old and widely supported, but it means modifying your c program slightly to read variables from the environment. calling from another language would let you pass those parameters on the command line and also let you do more processing of the results (for example, if the C program only calculates a numeric value, the calling code could then format that into a nicer looking web page).
I'm starting to chalk this up to Hackerrank, as my code seems to work flawlessly when compiled on my machine, and I still see no issue.
C, written poorly, can cause many problems.. But it is faster than greased butter..
If you’re interested in C++, there’s a good stackoverflow thread that lists a lot of recommended resources for learning C++. I believe r/learnprogramming has a similar list—I’ll try to find them for you. Just keep in mind that this isn’t a C++ subreddit, it’s a subreddit for C (C++’s parent language), so you’ll find more help in other places. Good luck!
It's almost everywhere defined that way. 
Not really on most modern systems that page will not be mapped and accessing it will always crash.
Idk, I can use address zero on Darwin, Linux and Windows just fine. I guess they don't classify as "modern". 
&gt; is that you forget it needs tab characters to indent or it won't work right Are you for real? https://stackoverflow.com/questions/2131213/can-you-make-valid-makefiles-without-tab-characters Yes yes you are. 
Is there a C++ subreddit?
Yes, /r/cplusplus
Here's a sneak peek of /r/Cplusplus using the [top posts](https://np.reddit.com/r/Cplusplus/top/?sort=top&amp;t=year) of the year! \#1: [Been creating a 3D game engine from scratch in C++ that is capable of running the original DOOM and wanted to share my progress with you guys who have helped out quite a bit along the way.](https://np.reddit.com/r/Cplusplus/comments/8apgra/been_creating_a_3d_game_engine_from_scratch_in_c/) \#2: [C++ Projects for Beginners](https://np.reddit.com/r/Cplusplus/comments/80sllf/c_projects_for_beginners/) \#3: [First Games in C++: Dungeons and Flagons - Update](https://np.reddit.com/r/Cplusplus/comments/a4af07/first_games_in_c_dungeons_and_flagons_update/) ---- ^^I'm ^^a ^^bot, ^^beep ^^boop ^^| ^^Downvote ^^to ^^remove ^^| [^^Contact ^^me](https://www.reddit.com/message/compose/?to=sneakpeekbot) ^^| [^^Info](https://np.reddit.com/r/sneakpeekbot/) ^^| [^^Opt-out](https://np.reddit.com/r/sneakpeekbot/comments/afd0dd/blacklist/)
C++ is off topic in this subreddit. Please post C++ questions to /r/cpp or /r/cplusplus instead.
Can you upload a sample file with this issue? I think these are just part of debug information for the C runtime initialisation code. Try to strip them out with `strip -g`. If they go away this way, then you have the confirmation that what you have there is just debug data.
For sure. Which is why I love it :D
There are plenty of ways: https://www.coralbits.com/libonion/ https://github.com/h2o/h2o/tree/master/examples/libh2o https://kore.io/ https://facil.io/ http://gwan.ch/ 
&gt; Can you upload a sample file with this issue? Hate to be uncooperative, but I don't believe I'm allowed to take any files off of the environment that I'm doing this on. But I'll double check and get back to you. &gt; Try to strip them out with strip -g. If they go away this way, then you have the confirmation that what you have there is just debug data. I just did this and I got the same checksum. I'm not familiar with the strip command. Does this at all violate the integrity of the checksum? Thanks! 
Why do you want this? If the binaries have to be identical, copy the binaries, not source ?
The difference between Implementation-Defined Behavior and Undefined Behavior is that the former requires that implementations produce and document a behavior that should be consistent, even if no possible behavior would be useful, but guaranteeing *anything* would be complicated or expensive. Note that even in cases where hardware can automatically trap behaviors with zero extra cost in non-trap cases, it may be expensive for a compiler to handle trap situations in a manner that can be consistently documented. If attempting to read from a null access were Implementation-Defined behavior, for example, and an environment trapped such reads in a way which programmers might be able to redirect to a user-defined signal handler, a proper behavioral definition should be able to say something useful about the effect of `test2(0)` on `someInteger`. extern int someInteger; int test(int restrict *p) { int total=0; for (int i=0; i&lt;100; i++) { someInteger++; // Note that the first inc precedes the first read of *p total += *p; } return total; } void test2(int restrict *p) { test(p); // Ignore the result } Making the behavior of a null read consistent would require that invocation of `test2` trap if it's passed a null pointer, even though it would have no other use for `p`. One could perhaps stretch the meaning of "implementation defined" sufficiently to allow for an implementation to document that an attempt to read from a null pointer either trap or yield a value that is never used, chosen arbitrarily, but even that might still preclude some optimizations. On some platforms, the most most efficient code to handle `test` in the non-null cases might be: int test(int restrict *p) { /* On some processors, multiplications are slow, but can overlap other operations, so starting the multiply early will save time. */ int temp = *p *100; someInteger += 100; return temp; } If the code were optimized in that fashion, however, the null read would have an unanticipated side-effect, *retroactively* preventing the increment of `someInteger` *from having occurred*. If an implementation is designed for purposes that would require precise semantics in such cases, it should refrain from optimizations that would affect them. Making null-reference behavior "Implentation-Defined", however, would forbid an implementation from making such optimizations whether or not customer code would be reliant upon the semantics thereof. The fact that the Committee didn't require that *all* implementations document a behavior does not imply any judgment that quality implementations should not be expected to offer useful behavioral guarantees in cases where doing so would obviously be useful and practical. Instead, the Committee thought that that "the marketplace" would be better placed than the Committee to identify when stronger behavioral guarantees would be practical and useful, and when they would be expensive but useless. 
An implementation is not allowed to place any object whose creation is described by the Standard, and whose address it controls, at an address that would compare equal to a null pointer. Many implementations define additional means via which objects can be brought into existence during *or before* program execution, but since the Standard doesn't recognize that such objects can even exist, it says nothing about the behavior of such objects or their addresses. That having been said, quality implementations for platforms which might have useful objects at an address matching a null pointer should generally provide some means of accessing such objects. A reasonable method would be to say that an access made via any "pointer to `volatile`-qualified object" will be processed as a load or store performed after all previous side-effects are complete, and before any subsequent side-effects are initiated. The behavior in that case could then easily be defined as "Behave in a fashion characteristic of the environment, which the environment may document or not as it sees fit". 
`strip -g` removes debug information from the binary. Everything else is left intact. Unless the `strip` program is defective, the program should behave exactly as before.
Thanks so much! This has been unbelievably helpful.
Well the original MacOS did it, and it was ROM-based. Now I wonder about Amiga and Atari ST. 
I had a gig in the nineties where I was developing a full e-commerce site in C... At the top of my code, upon entry, I made a note of the current time, in microseconds(I think), and then just before my code would terminate I would also notice the time in microseconds, so I could calculate the total time that it took my c code to generate a fairly detailed webpage.. I can't remember exactly how long it took to execute the page, but it was something trivial like 0.02 seconds.. I would display this value at the bottom of every page that executed. I was actually concerned that my client might be disappointed.. It showed something like this: Time: 0.0257 seconds My client, he was like, uh, that is fine, I can live with that.. And I was like, are you sure? Dinga dinga dang dang..
I made a hash of that sentence by conflating two different things, then editing it. Thanks for the correction. 
What do you think the `static` keyword does to `output0`?
I believe the C standard doesn't actually say anything about addresses like this. *Theoretically*, you could have an architecture and a C implementation that would put things at address zero, but I don't know of any such architectures, and it would be madness to make such a thing. C has an abstract constant known as NULL. NULL is not a legal address for anything, and is universally used for the "there's nothing here" value. This is what your function should return when no match is found. Now in practice, NULL is defined as 0 or ((void *)0) or some such thing on any system I've ever seen. More recent versions of the C standard take the value '0' to be the same thing as NULL whenever it's used as a pointer constant. The C FAQ has an interesting [section](http://c-faq.com/null/) on NULL pointers, and points out that there's been at least a couple of architectures that did in fact use a non-zero value for NULL.
Not the OP but having reproducible builds is a good thing, e.g. https://wiki.debian.org/ReproducibleBuilds I imagine they're approaching it with something like this in mind.
As you can imagine, this sort of question is asked quite often. That being the case, running a search on this subreddit will yield a good number of results for your consideration. https://www.reddit.com/r/C_Programming/search?q=title%3Abook&amp;restrict_sr=on&amp;include_over_18=on&amp;sort=new&amp;t=all
https://books.google.com/books/about/The_C_Programming_Language.html Often simply referred to as K&amp;R
`static` is the opposite of what you want. That makes it so every compilation unit (.c file) gets its own version of the variable. `volatile` probably isn't appropriate, either. Your header should have `extern Pin output0;` to declare the global variable, and some file (probably file1.c) should have `Pin output0;` to define it as a global.
I did gush that out before thinking. touche.
If a function accepts a pointer-to-void type, it will accept any kind of pointer without complaint. This will make it possible for the function to accept pointers to a variety of similar types of structures without ugly casts in the client code, but will prevent the compiler from notifying a programmer that anything is wrong if e.g. the programmer accidentally passes the address of a pointer to such a structure, rather than passing the address of such a structure, or vice versa. For example, it may be useful to have a function that can work with a variety of structure types having the following general form: struct { uint32_t count; uint16_t flags; someType data[]; } countedSomeTypeArray; so as to allow use with objects of static or automatic duration. Having such a function accept a pointer to any particular type would make it necessary for client code to cast the address of any type passed to such a function. Having the function accept `void*` would avoid that requirement. On the other hand, in cases like this, another approach can sometimes be useful: wrap the function call in a macro which effectively tests that the argument is likely to be a pointer of proper type. #define showThing(x) (doShowThing(sizeof (x)-&gt;count : (countedTypeArray)(x) : 0)) While it would be possible to invoke this macro with a pointer to some structure type that's unrelated to the above but happens to contain a member called `n`, this approach would ensure that e.g. accidentally passing a pointer to an array of such items would yield a compilation error. 
thanks :-)
I was reading about static yesterday and I came up with the conclusion it was okay to use it because in some examples they do this otherwise there are \*multiple definition\* errors because of inclusion of header in both main.c and file1.c. Now i realise I am wrong and still not understanding them. Do you have a good reading or book recommendation where they cover this kind of topics. thanks
Question from another beginner: wouldn't an optimised build not contain this debug info? If u/synchh wants reproducible builds, instead of using another program (strip), wouldn't just building with -O2 solve the problem?
How would this differ from just using the -s option with GCC?
It's for work and it's a requirement set by someone else.
K&amp;R is a must-read, especially for historical context, but the best book for a newbie is K.N. King's *C Programming: A Modern Approach* 2nd Edition
Since the best option is already presented, I can suggest wrapping the C code in another language, preferably higher level and is used to write web servers. For example, you can wrap C code in Python, then make a basic web server with Flask and serve your application through that. This would also let you sanitize your inputs with Python, which is harder to do in C.
Head First C - great book though it assumes you have some background in programming.
Head First C and Programming C: A Modern Approach are the 2 best books out there for beginners. There is also a free book called Modern C that gets kudos.
I'm reading through this now and find it more accessible than K&amp;R, especially for learning the basics.
I'm pretty sure that's the point of WebAssembly, right?
Optimisation does not make a difference and can be selected independently of debug information. That said, as the startup code already has debug information, turning the generation of debug information off won't have make a difference.
I second this.
For an object to be shared among multiple compilation units, what is typically required is that exactly one of them contain a definition without any qualifiers, and every compilation unit that uses the object without including such a definition before its first use declare it with an `extern` qualifier. If the compilation unit which defines the object without an `extern` qualifier does not use it prior to that definition, it may at its option include an `extern`-qualified declaration as well. Some coding-practice standards may require such `extern` declarations on the premise that every object at file scope should either be declared `static` or have an `extern` declaration in a header file, and any `extern` declarations should be validated against the definitions (if an `extern` and non-`extern` declaration are present in the same file, a compiler will squawk if they describe identical objects). 
The kernighan and Ritchie book is the original C programmers Bible. 
See our side bar.
Don't ask me, I'm a beginner too. u/FUZxxl can maybe chip in.
OK, I understood the first part, but what do you mean when you say that turning off generation of debug information won't make a difference? Wouldn't it just... not generate the debug info? Sorry if I'm missing something obvious.
I feel like learning something like microcontroller programming in C will help you really understand C itself. And once you get a good feel for what C is doing why why it's awesome (seriously, learn assembly at the same time and be amazed at how much more powerful C is) then read K&amp;R to learn how clever C is.
Do you know of any good resources for microcontroller programming?
I'm serious when I ask this, and very curious : why do you want to learn C? 
[removed]
This was incredible. Thanks for taking the time to write that out brother!
Thanks for the resources!
IMO the K&amp;R book is a great reference, it's a brutal introduction to actually using C though. &amp;#x200B; Personally I suggest the Learn X the hard way books. IMO they are much easier and more straight forward than others. [https://learncodethehardway.org/c/](https://learncodethehardway.org/c/) By "the hard way" they mean in excruciating detail (which IMO is needed to use C and not be confused when things don't go as expected). The actual tutorials are VERY easy and pleasant to follow.
Debug information is generated during compilation. Compilation is the process that turns a source file into an object file. After compilation, all object files belonging to a program are linked together to an executable. Whatever debug information is present is linked into the binary, too. One of the objects linked is the C runtime initialisation code, `crt0.o`. This object is provided by the libc and contains the programs entry point (where the kernel starts your program), a piece of code that eventually calls `main` and then `exit`. Since the libc was apparently build with debugging information, this object too contains debugging information which is included into each binary during linking, regardless of whether the compiler was told to generate debug information for any other object.
The `-s` option strips not just the debug information but also the entire symbol table. While this should not affect programs written in C, it makes finding out what this program does and what functions and variables are in it very hard.
C++ is off topic in this subreddit. Please post C++ questions to /r/cplusplus instead. Also, you might want to post the compiler invocation as well.
Thanks :D Sorry about that.
PLEASE STOP RECOMMENDING K&amp;R TO BEGINNERS PLEASE STOP, STOP FOR THE LOVE OF GOD, STOP ALREADY
Worked for me....
All of them.
I have been using learn-c.org and K&amp;R. However I have been told that C: the modern approach is the best.
I think K&amp;R forces beginners to search up stuff on their own which is good in my opinion. Half of programming is searching up documentation anyway, might as well get good at it when you start.
The OP said there was a secret message that could only be unlocked by typing the password. If a program would be incapable of outputting anything other than the correct message or a "sorry" message, that would make it impossible to completely guard against reverse-engineering attacks. Making the program capable of outputting thousands or millions of plausible-seeming messages without containing anything that would distinguish the one meaningful message from the countless meaningless ones, however, would make it possible to more solidly guard against the possibility of anyone getting meaningful information without the proper password.
I liked Stephen Kochan's Programming in C.
If the message is a *constant*, and it has already been encrypted using strong encryption, then reverse engineering will not be able to reveal it. If the message is *dynamically generated*, then reverse engineering could intercept the code that generates it. Some sort of obfuscation/DRM/RAM encryption would be needed to make that difficult.
Make has some advantages. I like it and use it, but I wouldn't blame anyone for using a script to automate compilation, at least up to a certain complexity level. Assuming the script worked, obviously. Make advantages: * Declarative, not imperative. * Automatic parallelization. For example, I have in my `.bashrc`: # from https://www.topbug.net/blog/2016/12/10/parallelize-make-by-default/ # if type nproc &amp;&gt;/dev/null; then # GNU/Linux export MAKEFLAGS="$MAKEFLAGS -j$(($(nproc)-1))" elif type sysctl -n hw.ncpu &amp;&gt;/dev/null; then # macOS, FreeBSD export MAKEFLAGS="$MAKEFLAGS -j$(($(sysctl -n hw.ncpu)-1))" fi * Automatic dependency handling. * Self-documenting. When you see a directory with a `Makefile`, it tends to be highly self-documenting. You know that you should be using `make`, and you can look at the targets in a high-level language and understand them. If something needs to be done, then the `Makefile` can test for it and exit, or spit out instructions, in the default target. Basically, see `Makefile`, start by typing `make`. 
it tells me too many functions in arguments.
Your function prototype only has 2 arguments
I don't quite understand what kind of “benefits and drawbacks” you expect. A pointer to `void` is a type. It does exactly what is says on a can and it's usually pretty clear when a variable should have this type (when the thing it points to can have any type) and when not (when the thing it points to has a fixed type). It's not really a thing where you weigh benefits against drawbacks and it's really hard to list any as I have no idea what you expect.
You can cast any pointer to and from any other pointer. Casts never “blow up” at runtime whatever that means.
Yuh youre right. When it goes to the function in the file1.c even the memory location changes and hence its contents. I used the \`static\` because I saw in in another files in the project and if I removed it it would generate \*multiple definition\* error. I read about static variables but still not very comfortable about them. Do you have book or web recommendations about this topic? &amp;#x200B; Thanks for your answer. 
when i make it three. it an error.
I like python
Your prototype function signature needs to match your implementation. Not only did your prototype miss an argument. Your function is also lacking the argument types: it should be ‚addNumbers(int A, int B, int C)‘.
&gt; gameboard[(i-65)][j] == j; assignment is one = what you did is check the conditional
line 3, change both "int" to float, add a third. You are working with all float variables, not int line 18, add float before "A" "B" "C". Otherwise the computer does not know what data type they are. It may seem redundant, but it is necessary. #include &lt;stdio.h&gt; float addNumbers(float a, float b, float c); // function prototype int main() { float A,B,C,sum; printf("Enters the three numbers: "); scanf("%f %f %f",&amp;A,&amp;B,&amp;C); sum = addNumbers(A, B, C); // function call printf("sum = %f",sum); return 0; } float addNumbers(float a,float b,float c) // function definition { float result; result = (b*b) - (4*a*c); return result; // return statement } &amp;#x200B;
Oh. My. God. Thanks. I need to learn to read 
yeah these are the tiny mistakes that make you want to pull your hair out. we'll now we both know what the "statement has no effect" warning means. If I ever see that I'm going to be looking for a double equal!
omg thanks. if i could kiss you, i would
Haha yeah! I would have thought it would complain about a conditional with no action. I guess that's sort of the same thing 
I’ve heard that Zed Shaw seems to be willfully ignorant about the finer points of what the standards actually imply. More like a less clueless Schildt.
It’s flatly false for a lot of embedded stuff that has/engages no CPU protection mechanisms, and it’s usually false for any software in kernel/supervisor/hypervisor/SMM mode; e.g., on x86, in real or VM86 mode, your startup IDT is at address (0:)0 (→fucking up your real-mode IDT is a great diagnostic for an oopsie null dereference in the kernel), and the DOS/Win PSP had an `int 0x20` instruction you could reach by calling to (CS:)0. Also, some UNIXes and POSIXes allow you to `mmap` address zero if you have certain flags or permissions set. In these cases the compiler can usually shit itself if it catches wind of there being a null pointer dereference, but as long as you keep it outside of the compiler’s view you’re fine.
He has a bad habit of taking the standard literally and not reading the subtext. It leads to a lot of hardline takes that aren’t entirely correct. That said, I don’t know of a set of easily consumable C tutorials that are better. It’s a really good way to get started (imo).
That only applies to casts the compiler can readily see are coming from zero, though, like `(char *)0`. If you’re punning zero bits from an `int` to a pointer, the compiler’s free to come up with something other than `NULL` as long as it’s not operating under the POSIX zero-bits-must-be-`NULL` rule.
I guess I was thinking if it can point to any type, why not exclusively use it instead of, say, long\*, double \*, char \*, etc.
POSIX treats zero bits as `NULL`, so most ABIs for architectures that want to run POSIX treat all-zeros as `NULL`. The C standard doesn’t say one way or the other, unless you’re explicitly casting constant/-ish 0 to a pointer.
If you want to completely forego the ability to tell what type a pointer points to, sure you can. This is very unidiomatic C usage though. Another thing is that since the compiler has no way to tell what a pointer to `void` points to, you cannot dereference it. You have to convert it to a pointer to some complete type first.
I mean space, accuracy, and speed. For addition, subtraction, and multiplication, an int (or long int) is more accurate and fast to my understanding.
Just about any book on programming will talk about `static` variables. The broader topic here is "linkage" if you want to google more reading material.
Note that you probably compiled with optimizations. You may wanna use either `-O0`, `-O1` or `-Og`, so lines don't randomly dissapear / gets optimized out / reordered.
Couple of other issues in addition to `static`: - Bitfields are not a good idea if you’re dealing with hardware. They’re a non-portable shitshow at the best of times, and should only be used if you’re really desperate to save memory. The compiler is not required (unless by specific ABI or an explicit directive) to pack adjacent sub-`char`-sized bitfields into a single `char`, it’s not required to pack accesses to adjacent bitfields into a single access even if they do fall into the same `char`, and of course `volatile` is likely to interact very poorly with bitfields. On a modern compiler on a modern platform you can *probably* get away with using a `volatile uint8_t` that you explicitly read/write/update yourself instead of a bitfielded `struct`, although that’s not necessarily going to be portable to other ABIs/compilers. - `uint8_t` is a really weird type to use for bitfields. Generally it’s best to stick to just `unsigned` or `_Bool`. Signed things like `int` can get weird if a sign bit happens to be affected, and otherwise the 8/16/32/64/etc.-ness of the `uintXX_t` types is entirely superfluous and may actually cause some compilers to throw an error. If you need the `struct` to be aligned in some way, use `_Alignas(…)`, `__attribute__((__aligned__(…)))`, or something equivalent, but again, a lone `unsigned char` or `uint8_t` is probably a better bet than using a `struct`. When you’re dealing with outputs to hardware MMIO/-ish stuff, you may need to fence accesses in addition to whatever `volatile`ness you involve; e.g., on GNU/-ish compilers you can do something like __asm__ __volatile__(".if 0\n.endif" ::: "memory"); before and after an access to strongly suggest that the compiler spill potentially-in-memory values before proceeding. Some hardware architectures will require actual fence or serialization instructions beyond this, however (e.g., `lfence` or `sfence` or `mfence` or `xchg` vs. memory) in order to push things out of the cache or avoid speculative prefetching. `volatile` is strictly a compiler thing, and the hardware will have no idea whether a load/store is `volatile` or not. If you’re referencing a specific address, you may be able to forego the `extern` and shared non-`static` variable stuff and do something along the lines of extern volatile uint8_t output; __asm__(".if 0\n.else\noutput = 0x12345\n.endif"); which tell the compiler to refer to an external `output`, then tell the assembler to assign `0x12345` to the (effectively `static`) symbol `output`. This would absolve you from needing to make a non-`extern` `uint8_t output` declaration or play games with linker scripts, although YMMV, and higher addresses can give the linker shit-fits on some ABIs. (Note: There are those pointless `.if`s there because they help prevent some compilers like Clang or IntelC from mucking about in your `asm` statements.)
Integer types are straightforward. You can always write down a number exactly how it is. To get bigger numbers, you need to add more bits. This simplicity means that the computer can do math with them very quickly. Floating point types are a different beast altogether. Think of them as binary scientific numbers (because that's what they are). You can get significantly larger range from the same number of bits (just increase the exponent), but the tradeoff is a loss of precision and slower computation times. It can be slightly harder to reason about floating point, especially if you're a beginner. In general, it's rarely a decision for which you need to use. The problem at hand almost always dictates which of the two is most appropriate for your situation.
&gt;You can cast any pointers of any type to and from any other pointer type A `void*` is the only circumstance where that conversion is implicit by standard in C. It's meant to represent an abstract type that often handles a provided buffer in bytes. &gt;Casts never “blow up” at runtime whatever that means. You can easily cast an object to a type with a larger size and end up dereferencing memory that hasn't been allocated leading to undefined behavior, so a bad cast can easily cause a seg fault at runtime.
&gt; A `void*` is the only circumstance where that conversion is implicit by standard in C. Eh... no? The only place where an implicit cast of pointer types happens is when passing arguments to prototyped functions. There, all pointers are implicitly converted. &gt; You can easily cast an object to a type with a larger size and end up dereferencing memory that hasn't been allocated leading to undefined behavior, so a bad cast can easily cause a seg fault at runtime. It's not the cast that “blows up,” it's you dereferencing an invalid pointer that blows up. The cast is innocent.
Additions, subtractions, and multiplications are always precise with integers (as long as the result is in range, that is). Integers cannot have fractional values. Integers are generally faster than floating point numbers. You can perform bitwise operations on integers but not on floating point numbers.
Nitpicking much? Yeah an improper cast then making use of the improperly cast pointer would blow up at runtime. 
The point is that the cast itself is entirely fine. In fact, people frequently program this way. So you saying that wrong casts can blow up your program are just fear mongering bullshit.
This is slightly beside the point, but… Why are you using `65` instead of `'A'`? They’re exactly synonymous on any system you’re likely to care about, but the former just looks like an arbitrary constant and the latter has a much more obvious meaning. And of course, neither 65 nor 72 nor `'A'` nor `'H'` has any direct connection to the array dimensions; either one dimension should be `'H'+1-'A'`, or you should be using 8 for the dimension and `'A'+8` instead of `72` or `'H'`. Whenever you can, create a semantic binding between informal and formal bounds, so either they change together or you get a warning/error if one changes without the other being changed to match. Not that `8` or `10` are good as standalone constants either, but toys will be toys. And not that the ASCII value is actually needed for any of the code here—it’s needed for display and input only, and it makes sense to offset by `'A'` in *those* contexts only, and never bother with letters elsewhere since they have nothing to do with your board layout or storage.
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/devsjunky] [What is the best book to learn C for self learning beginner?](https://www.reddit.com/r/DevsJunky/comments/atne0p/what_is_the_best_book_to_learn_c_for_self/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
Honestly when something is this basic you should be able to google... &gt;Converting any object pointer to void pointer requires no cast. C performs this conversion implicitly \- [http://www.iso-9899.info/wiki/Casting](http://www.iso-9899.info/wiki/Casting) I don't have what your compiler/linter tells you, that's the C standard. &amp;#x200B; &gt;It's not the cast that “blows up,” it's you dereferencing an invalid pointer that blows up. The cast is innocent. &amp;#x200B; @[xeveri](https://www.reddit.com/user/xeveri) was pointing of that in practice, a bad cast will lead to undefined behavior. That is obvious from the context of his post. Obviously a programmer isn't going to just cast a pointer for fun and then never use it, that would likely be optimized out by the compiler if that were the case. You're making a dumb argument.
I’ll throw out a brief proviso about integers: *Signed* integer overflow is not necessarily precise from a language/compiler point of view (downright unsafe), and bitwise operations with potentially negative signed integers are similarly unsafe. If you want wraparound on overflow and reasonable bitwise operations, integers involved need to be `unsigned`. In hardware, however, signed and unsigned integer operations are usually equally fast, usually minimally risky, and mostly implemented with the same instructions and circuitry.
Post some code of an instance where you would cast a pointer to another type and then never use it.
&gt; I don't care what your compiler/linter tells you, that's the C standard. Nope, that's some random-ass website. The actual standard (ISO/IEC 9899:2011) says for assignments (§6.5.16.1 ¶2): &gt; In *simple assignment* (=), the value of the right operand is converted to the type of the assignment expression and replaces the value stored in the object designated by the left operand. And for function calls (§6.5.2.2 ¶7): &gt; If the expression that denotes the called function has a type that does include a prototype, the arguments are implicitly converted, as if by assignment, to the types of the corresponding parameters, taking the type of each parameter to be the unqualified version of its declared type. The ellipsis notation in a function prototype declarator causes argument type conversion to stop after the last declared parameter. The default argument promotions are performed on trailing arguments. Note how in none of these cases it mentions that void pointers have some sort of special treatment? It's because there isn't any! Implicit casts happen regardless of whether the point points to `void` or to some other type. &gt; Obviously a programmer isn't going to just cast a pointer for fun and then never use it, that would likely be optimized out by the compiler if that were the case. You're making a dumb argument. You missed my point. What people do is casting pointers to objects of different types to a common dummy type to store them in arrays or structures. For example, the VFS interface in FreeBSD used to work this way with all function pointers having type `int (*)()`. Of course nobody dereferences these wrongly typed pointers, because that is what crashes your program. Simply casting pointers to some other type is completely fine and often useful, even if that type has no relation to the pointee's actual type.
Check out the source code of old FreeBSD releases, specifically the VFS interface. To make the VFS extensible, the functions of a VFS driver are stored in an array of type `int (vfs_ops[])(void)`. The actual types of the VFS functions differ and each caller would cast the desired VFS functions to the correct type before calling them. Similarly, suppose I write some generic data structure. To tell apart members of my data structure from members of other data structures, I declare an incomplete dummy structure to serve as the pointee for custom void pointers: struct foo_element; typedef struct foo_element *foo_elem; I never dereference these pointers, I just use this dummy type for better understanding. Similar applications exist.
We are calling the wiki page for the C99 standard a "random-ass website"? &amp;#x200B; It's funny how you either intentionally ignored the Conditions section of Simple Assignment, or you didn't understand it. Condition 4: &gt;The left operand has atomic, qualified, or unqualified pointer type, and (considering the type the left operand would have after lvalue conversion) one operand is a pointer to an object type, and the other is a pointer to a qualified or unqualified version of void, and the type pointed to by the left has all the qualifiers of the type pointed to by the right; This condition states the a given implementation must allow the implicit cast from a pointer type to a void pointer type. Condition 3: &gt;The left operand has atomic, qualified, or unqualified pointer type, and (consideringthe type the left operand would have after lvalue conversion) both operands are pointers to qualified or unqualified versions of compatible types, and the type pointed to by the left has all the qualifiers of the type pointed to by the right; This condition states the pointers on each side must be of compatible types.
&gt; random ass-website *** ^(Bleep-bloop, I'm a bot. This comment was inspired by )^[xkcd#37](https://xkcd.com/37)
&gt; We are calling the wiki page for the C99 standard a "random-ass website"? These are not the wiki pages for the C99 standard. This is a wiki some C programmers have registered to share their thoughts. This is not an official site. &gt; It's funny how you either intentionally ignored the Conditions section of Simple Assignment, or you didn't understand it. Point taken. This does not apply to function calls though.
&gt; random ass-website *** ^(Bleep-bloop, I'm a bot. This comment was inspired by )^[xkcd#37](https://xkcd.com/37)
Agreed. King's book is a fantastic read. K&amp;R is obviously a great book but not suited to a beginner. 
I don't think one can be a good C programmer without understanding the language the C Standard was written to describe, the language it actually defines, the difference between them, and the reasons for those differences as described by the published Rationale for the C Programming Standard. See http://www.open-std.org/JTC1/SC22/WG14/www/C99RationaleV5.10.pdf for a version describing C89 and C99. The Rationale doesn't cover all of the ways in which the Standard deviates from the language it was written to describe, since some of the deviations were unintentional, and some weren't really seen as deviations. Nonetheless, it does provide some insight about parts of the pre-existing language that the authors of the Standard expected most implementations would uphold even if the Standard didn't require that they do so. 
That would be the kind of drawback I was looking for.
You should not think in terms of benefits and drawbacks, but rather in terms of *purpose.* The purpose of a void pointer is to point to an object of unknown type. If you want to do that, use a void pointer. If that is not the case, a void pointer is not the right tool for the job.
I would suggest that there is probably an option to print out beejs sockets document as a single page, just in case beej's page disappears, lol.. I am so happy that you enjoyed my post.. If you have any questions, if you want to talk, just ask for my number via pm.. Happy Coding, Zero/.
&gt;Point taken. This does not apply to function calls though. Except it does because function calls will follow the Simple Assignment standard for their arguments. &gt;An argument may be an expression of any complete object type. In preparing for the call to a function, the arguments are evaluated, and each parameter is assigned the value of the corresponding argument. That's from §[6.5.2.2](https://6.5.2.2), another section you posted, and voluntarily ignored or didn't understand. Also the two paragraphs you quoted pertain nothing to the discussion at hand. ¶7 is obscurely referring to function pointers specifically, and that's only how the types are resolved, assignment still needs to take place.
The first thing I noticed is that you use = instead of == to compare two values. = is used for assigning values and will not work if you are comparing. Use `if (x == y)` instead. 
An address equal to zero would be a NULL pointer. So no.
This assignment is not an assignment operator so the rules you mentioned before do not apply.
 (i % 3 = 0) C uses `==` for equality comparisons. `=` is for assignment.
Thanks. I dont need the second = if im using !=, correct?
I think maybe you are getting MinGW confused with MSYS, a Cygwin-like POSIX translation layer for Windows? MinGW *is* GCC ported to Windows.
Blimey. It's almost like people don't read my comments. I know very well that the standard _says_ dereferencing a null pointer is undefined behaviour, and that there are good reasons for this. Nevertheless, it is perfectly valid for an implementation to have well-defined behaviour for it. "Undefined behaviour", in the standard, simply means that the standard "imposes no requirements" on it.
I did this, but it there is still issues with it when I try to compile
The compiler tells you where the errors are. Listen to what it says. For example, this line doesn't look right, either: int (main)
Happy cake day!! Brilliant story.
Yes, I should have made it clear: it has to be an integer constant expression with the value zero, not some arbitrary expression with the value zero.
Thanks brother!
Seriously learn to fucking read. I've actually implemented C compilers. I actually know what I'm talking about.
&gt;Time: 0.0257 seconds I find fascinating that pages nowadays, thirty years later, usually take 1000x times as much to load haha. It's even more fascinating that, if you assume 30 years have passed since the nineties (almost correct), and per Moore's law, the loading times should be about 1024x faster, so web is literally going through an inverse Moore's law.
Benefits: - `void *` and `char *` (or qualified variants thereof) are the only types that allow you *somewhat* direct access to the machine representation of values in memory without inducing undefined behavior. Restrictions still apply, of course, and they don’t necessarily allow you to convert freely between pointer types without breaking aliasing rules. - `void *` makes defining a good `NULL` muchmuch easier. With it, `NULL` can be defined as a singular constant that has a single pointer type, usually `((void *)0)` with minimal compiler-specific trickery. For comparison, in C++-pre11, because `void *` doesn’t freely coerce to any other type, `NULL` had to either be defined as just `0` or some special token like `__null` that *maybe* allowed the compiler to warn about misuse, but which otherwise behaved just like literal 0. Literal 0 is a *really* bad `NULL`, because it’s just an `int` unless the compiler has special knowledge otherwise, and in situations like `...` arguments or template jiggering the compiler has no such knowledge. Post-C++11, `NULL` can be defined as `nullptr`, which establishes `const volatile void *` and `decltype(nullptr)` as the top and bottom of the pointer type lattice, respectively—i.e., any pointer type coerces freely/safely to `const volatile void *` and `decltype(nullptr)` coerces freely/safely to any pointer type, but not vice versa. In C the top is `const volatile void *` and the bottom is `void *`, which means its “lattice” is really just a weird almost-loop. - You won’t need to implement things like `memset`, `malloc`, or `free` for every possible argument/return type, on the off chance the compiler has different calling conventions for `void *` and other pointer types. One size fits all. - `void *` is excellent for use with callbacks (e.g., `pthread_create`) where you need for the client code to be able to pass through in some arbitrary context, or for cleanup/destructors where you don’t care what’s in the referent memory as long as things that need cleaned up get cleaned up. I tend to prefer a scheme like void something(void (*cbfn)(void *p), const volatile void *cbfn_p); if my code doesn’t need to impose any requirements on the pointed-to type, and this lets the client pass through anything with any cast hidden inside the `something` implementation. - When GNU-style `&amp;&amp;label` expressions are used, they hand back a `void *`. You have no more type information than that, so you’re only allowed to use (possibly qualified) `void *`s with computed `goto`. I generally prefer `const void *` for computed `goto`, since there’s roughly zero chance I’ll want something to write through that pointer. Drawbacks: - You certainly wouldn’t/shouldn’t use `void *` if you have any concrete knowledge of its referent type; if you know it points to a `long`, use `long *` so you can dereference the thing without a cast. Otherwise you’re just kneecapping the type system for no good reason. - `void *` is typically used to refer to a raw memory buffer xor `NULL`, but that’s as self-documenting as it gets; beyond that, no telling what it might refer to. - `void *` makes it very easy to mask undefined behavior when type-punning. - `void *` may make it harder than necessary for the compiler to reason about aliasing, although C99 `restrict`/equivalent can help with that. - Pointers of any type can have weird interaction with literal `0`s, which usually coerce to null pointers in the right contexts. For example, in a `...` argument list or (in C only) an unspecified `()` argument list, literal `0` will be passed as `int`, not autoconverted somehow to `NULL`. You’ll need to either use `NULL` or explicitly cast `0` to the proper type. - Along these lines, `printf` format specifier `%p` accepts only `void *` per the standard, because other pointer/integer types may use other formats when being passed as arguments and under the varargs scheme the compiler doesn’t know to cast things for you. Portable code must therefore cast non-`void *`s to `void *` (with/without `const`/`volatile`) before passing them to `%p`, which is one of the very few common cases where you’ll need to explicitly cast to `void *`. Non-varargs arguments (i.e., not under the `...` umbrella) will coerce normally as long as the compiler can see the expected parameter type. - Both `...` arguments and function types with parameter list `()` have these same shortcomings, so you might need to cast explicitly to/from `void *` there too. (E.g., if a function expects an `int *` among its `...` args, passing `NULL` directly might break things.) - If you’re writing generic code (e.g., via macros), `void *` is a weird corner case. You can’t dereference it, and per the standards you can’t do something like `return (void)0` even from a function returning `void`. (GNU/-ish compilers may support `return (void)0` as an extension, however.) AFAIK no compiler will support `void` function arguments, since `(void)` as a parameter/argument list means “takes no arguments whatsoever.” Things like arrays of `void` aren’t permitted anywhere either, which is one of very few ways arrays-as-function-arguments and pointers diverge noticeably. - `void` has no size or format, so you can’t (per the standards) offset a `void *` by just adding/subtracting to it, and you can’t subtract one `void *` from another. GNU/-ish compilers may allow you to offset a `void *` by treating it internally like a `char *` in non-`pedantic` modes, but that’s a nonstandard extension and IMO a bad solution to an almost-nonexistent problem. `sizeof(void)` and `_Alignof(void)` (or GNU/ish `__alignof__(void)`) are generally not supported either, although IIRC GNU/-ish compilers hand back 1 for both in accordance with their support for offsetting `void *`. - Even if the compiler lets you offset a `void *`, doing so beyond the underlying memory block±1 is UB, and if you got the pointer from something like `malloc(0)` you can’t offset its value at all. You have to use `uintptr_t`/equivalent for lower-level address caculations, assuming your target ABI supports such a type. `void *` is usually presented as “just an address,” but it’s every bit as much a pointer as `int *` in terms of how it’s treated by the standards and compiler. - `void *` is a bit deceptive; any other (non-`const`/-`volatile`) pointer value will freely coerce to `void *` and back, but the “and back” part might not get you reasonable results for things like function pointers. Newer POSIX versions specify that `void *` has to support round-trip casts of function pointers in order for things like `dlsym` to work, but the C standards don’t require this and older POSIXes only implied it. Notable exceptions to the usual POSIX-compatible behavior include Harvard architectures (where code and data occupy different address spaces entirely) and some of the older/weirder segmented stuff like x86 real/VM86 mode (where code and data may use different pointer formats). - POSIX specifies that `void *` needs to support a `MAP_FAILED` value outside of the `NULL`-or-something-valid set for `mmap`’s return value. (This is usually represented as all-ones.) `MAP_FAILED` needn’t be supported by types other than `void *`, so if there’s some chance you have a `MAP_FAILED` value, you need to make sure you’ve checked for it before casting away from `void *`. - `void *` has different behavior in C and C++, so you’ll have to take extra care when writing code portable between the two languages. In both, any pointer value will silently coerce to `const volatile void *`, but C++ forbids coercion from `void *` to anything else, and requires an explicit cast. (Of course, explicitly casting away from `void *` will cause some C programmers to grind their teeth. But their teeth are tiny little nubs by now, beyond where a little grinding matters.) Furthermore, in C++, a round trip through `void *` can break the type system, so you have to be very careful using `void *` as an intermediary in dual-citizen code. If you have some `class A : B` where the the `B` part of the object starts at some offset into the `A` part (usually the case when vtables are in use), then a static-cast or coercion from `A *` to `B *` (or `A &amp;` to `B &amp;`) will work just fine, and the compiler will apply the appropriate adjustment to the pointer to set that up for you. (Ditto going backwards from `B *` to `A *`.) If you cast/coerce from `A *` to `void *` then to `B *`, the compiler won’t know that there’s supposed to be an adjustment applied, so you’ll end up with a bogus `B *` to the beginning of an `A` instead of to its superclass `B` component. And of course, C++ won’t let you use type `void &amp;`, so there’s that special case to consider too when dealing with generic code. - `void *` is iffy for out-parameters. E.g., if you have int get_buffer(unsigned id, void **ptr); void *p; then you can freely pass in `&amp;p` with no complaint from the compiler. Unfortunately, if you have `const void *p` because you don’t need to modify the buffer, you can either pass in `(void *)&amp;p`, discarding the qualifier, or use an extra throwaway variable like const void *p; void *q; … = get_buffer(ID, &amp;q); p = q; Similarly, if you know the buffer will be an `int[]` of some sort, you can’t do int *ip; … = get_buffer(ID, &amp;ip); because `int **` and `void **` are incompatible; you have to do void *q; … = get_buffer(ID, &amp;q); ip = q; If you had knowledge that all pointer formats were compatible-ish in the target ABI and wanted to support direct use of `&amp;ip`, you’d have to take an even-more-generic `void *` out-parameter type, which conveys very little information to the compiler/client about the parameter’s out-ness or what kind of argument should be passed there.
I liked C primer plus by Prata. If you are completely new to C, I recommend it. After reading it, K&amp;R book would be a good idea imho. Whatever you do just don't start with k&amp;r if you are an absolute beginner. 
I've found http://www.iso-9899.info/wiki/Books to be quite comprehensive. Also has a nope-nope list which is quite rare. KN King's book seems to have gathered the most recommendations lately, from what I saw. I'm learning currently from K&amp;R, but I'm not a beginner (this is more like re-learning). This book is really terse and I probably wouldn't recommend it to a total beginner, because I know if I started now, I likely wouldn't ask the questions needed to go further with the knowledge the book doesn't cover. Also a personal favorite: CS:APP. Not strictly C, but a fair amount of code in there with disassembly. 
The 64 bit fork is a better option, and building it yourself is the best option.
MinGW-w64 is the *only* option that is still actively developed and supported. If you've heard somebody say "MinGW" in the last five years, that's what they were talking about.
People always struggle with pointers when they come from a higher to lower level of abstraction. I struggled with thus fir a long time till I did 3 things: 1. Read [Code: The Hidden Language of Computer Software and Software](http://www.charlespetzold.com/code/) 2. Watched [Crash Course in Computer Science](https://www.youtube.com/playlist?list=PL8dPuuaLjXtNlUrzyH5r6jN9ulIgZBpdo) 3. Followed this [Tutorial on Pointers and Arrays in C](http://pweb.netcom.com/~tjensen/ptr/) The first two really helped by approaching C from a lower level of abstraction (actually the absolute lowest level and gradually adding layers of abstraction until you are at the C level which, by then is incredibly high!) The third is just the best tutorial in pointers in C. By far. 
Okay, fair enough. I will let my general claim that running a real \*nix with a compiler built from source should be the first order of business for a fresh CS student.
The basic chores of the CGI interface are to read the HTTP input from the client, and then output HTML based on it. Neither of them are specific to a particular language. Knowing how to manage multipart and/or HTTP encoded form data and GET/POST etc HTTP headers, are the issues that the program needs to deal with. The book, "Web Client Programming with Perl" by O'Reilly, helped me tremendously when I was doing the nutsy-boltsy stuff that most web API's now cover over with functions of their own. If you can still find a copy, I'd grab it.
Don't use more things than requires. Floats are not needed in a lot of circumstances. 
That's pretty old-fashioned. 
&gt; The Hidden Language of Computer Hardware and Software I've browsed a little this book in Amazon, read a few pages in their preview, it looks amazing. To the second option. I've got a 2.1 BSc (Honours) in Computing Science, I can happily skip that one. The tutorial looks like exactly what I was looking for. Either way your entire advice and perspective of instead of trying to see it from the higher level to the lower, try to even much lower (book 1) up, might be exactly what I have to do. I'll order the book in 1 as I already liked what I've read so far and see from there. I think I'll understand pointers much better iwth that book first and then jump to 3. Thank you very much for this. 
Yay, found the Ted Jensen book in Amaozn for just 7GBP [https://www.amazon.co.uk/Tutorial-Pointers-Arrays-C/dp/1521331049/ref=sr\_1\_3?s=books&amp;ie=UTF8&amp;qid=1550891227&amp;sr=1-3&amp;keywords=pointers+in+c](https://www.amazon.co.uk/Tutorial-Pointers-Arrays-C/dp/1521331049/ref=sr_1_3?s=books&amp;ie=UTF8&amp;qid=1550891227&amp;sr=1-3&amp;keywords=pointers+in+c)
Duly noted, thanks!
Do I see " atoi(optarg) " there ? Where atoi always returns an "int" which is signed and is 32-bit. However everywhere else you use uint32_t for nearly everything. Just making a guess without digging at all.
Thank you so much!
They explicitly state in the book that it's for people who already understand programming, not beginners. But I agree, it's good for making you look up and understand stuff
I can never seem to find a good assembly book. The best rated one was a book that taught the authors self created tool rather than assembly. 
Think about this one a bit more... This allows you to confirm a password without storing the password but how do you then release the secret message, without making it possible to just extract it directly? Or using a debugger to skip the password check? Also, have you thought about how long it might take to brute force the correct password given the availability of the hashed password? If the password is less than 5-6 characters I'd imagine you'd crack it in under a day. So you'd have both the secret message and the password.
Why would you want to? it's a randomly generated path (almost certainly with mktemp) precisely so it doesn't clash with any other temporary folder. What I've always wondered, is why does the compiler embed those dummy paths and other useless nonsense into the binaries in the first place? like, Thanks for wasting my space...
&gt; how do you then release the secret message I don't know what you mean. &gt; If the password is less than 5-6 characters I'd imagine you'd crack it in under a day. Near instantly is more correct. Weak passwords are weak. Salt would be needed. 
This is outstanding, thank you!
Ah, OK, that makes sense. Thank you so much for the detailed answer!
Ok so how do you introduce salt? You need to add it to whatever the user inputs right? So the salt is stored somewhere in the binary file right? See any problems there? The program requests a password and then returns a secret message if the password is correct. So while you may have verified the password, you still need to output the secret message. How?
\&gt; Kernel Version 0.12 Thanks, but I think I'll just get Robert Love's book instead.
I'm reading it now, but as a newbie, I thin this is also a good reference book before going into Robert Love's ver.2 kernel.
&gt; how do you introduce salt chicken and egg scene here ... I think a 16 char password would be needed. That would slow down the process but eventually even a 20 char password hash table will exist. So .. its a tough scenario all around. Well the secret message could also be hashed with a one-time pad and ... ya know what ? this is not really doable is it ? damn. 
Yup that's right. 
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/programming] [Complete noob, need help installing\/running a ".C" file utility](https://www.reddit.com/r/programming/comments/atsp82/complete_noob_need_help_installingrunning_a_c/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
You need to install make and run make from the folder with the makefile. Hope that helps!
Thanks, but how do I install? And run the file? Whenever I click on a file it just gives me the “how do you want to open this file?” prompt. Sorry, I know next to nothing about this type of thing haha. Can you give me a set of step by step instructions maybe?
Thanks, but how do I install? And run the file? Whenever I click on a file it just gives me the “how do you want to open this file?” prompt. Sorry, I know next to nothing about this type of thing haha. Can you give me a set of step by step instructions maybe?
Passing the checksum is a requirement for the project. And there's some info in the other comments about removing the extraneous info 
Make is a piece of software that takes that makefile file as input, and outputs the desired executable. (You can find it online and get it for free, as it is free software) The makefile essentially stores instructions about how the rest of the files (all the ".c" and other miscellaneous files) should be processed to build the software. The rest of the files are the actual source code of the sofware, they encode all the instructions and logic that the software will run.
I read that as Rob Lowe and had to take a double take.
Ok I’m gonna give it a shot, first time doing this type of thing. Wish me luck, I’ll let you know if I have any problems. By the way thank you so much for your help. I sincerely appreciate it.
Yeah I'm in way over my head haha. I have no idea what to do. Damn.
Thanks! This is amazing!
Hi, if you are under windows, chance is you didn't have a c compiler installed nor make (has already said in previous comments). So, install mingw (it's gnu c compiler for Windows) and make. Once done, open a command Line windows, go toi the directory containing the Makefile, and type make, it should create the executable (the program itself). Good Luck !
Well, the website i liked has a documentation (i.e. instructions on how to operate the Make software) link. So once you download Make you can learn how to use it from there or from somewhere else. (If you are just trying to install something made by someone else, you only need to know the very basics) You will need to use Make to 'install' the .c files (btw a .c file is a file that contains code written in the c programming language https://en.m.wikipedia.org/wiki/C_(programming_language) )
Desktop link: https://en.wikipedia.org/wiki/C_(programming_language) *** ^^/r/HelperBot_ ^^Downvote ^^to ^^remove. ^^Counter: ^^240319
**C (programming language)** C (, as in the letter c) is a general-purpose, imperative computer programming language, supporting structured programming, lexical variable scope and recursion, while a static type system prevents many unintended operations. By design, C provides constructs that map efficiently to typical machine instructions, and therefore it has found lasting use in applications that had formerly been coded in assembly language, including operating systems, as well as various application software for computers ranging from supercomputers to embedded systems. C was originally developed by Dennis Ritchie between 1972 and 1973 at Bell Labs. It was created to make utilities running on Unix. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/C_Programming/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
Are the dimensions swapped perhaps, in gameboard\[10\]\[8\]
I always feel tempted to recommend "learn 6502 first, then C will be a breeze" :)
Can you show what foo() is ?
This is a sub for the C programming language - C# is a separate language. But since your question is about learning programming more generally, try asking in /r/learnprogramming. 
Thanks man! 
Right. With that one I did a simple recursive descent thing, so it is using stack space proportional to the depth. Sorry, I misspoke when I said a small constant amount of memory: it uses no dynamic allocation, but it does use stack space proportional to the structural depth of the input. In a production (and especially Internet facing) parser you would have to limit this to prevent adversarial input from blowing the stack, but you would have to do the same thing for a dynamically allocated stack anyway.
Rob Lowe is a hell of a hacker.
Sorry, the "few kilobytes" was just a random value I pulled out of nowhere while quickly banging out that comment. This one is a simple classic recursive descent parser, so the amount of memory is something around a stack frame of a few words per nested object/array. There is no static data. Multi-threaded parsing doesn't really make sense, I/O is the bottleneck by a very wide margin. If you just mean whether the code is usable from different threads at the same time to parse different things, sure.
&gt; If you just mean whether the code is usable from different threads at the same time to parse different things, sure. Yes, this :) i.e. different threads are all parsing their own JSON for whatever reason. It wouldn't be possible if you were using static data, for instance.
What's next? You do what it is you wanted to learn C# for. Otherwise, it was pointless.
I see. I do what I always do with libraries and pass a context struct to everything that the caller has to allocate first. Static data is the devil :)
C# is off topic in this subreddit. Please post C# questions to /r/csharp instead.
1. install Visual Studio 2. make a new C/C++ project 3. remove all source files from the project 3. import noteshider.c and the resource folder into the project 4. press build 5. you should have a program now.
This is actually a native Windows program. OP needs MSVC to compile this.
To develop from scratch? Thousands to tens of thousands of dollars, and you probably don't want to use C. But there's plenty of off-the-shelf software that does what you need.
He has a PDF on his site. Not sure he's getting paid from that seller. 
There is a PDF for Code on his site as well
You want to develop this application? Do you want to write it in C? Do you want to build it from existing code? The price depends on a large number of factors. Which country do you live in?
Reddit has a save option for comments as well. 
Do you know any open source programs similar toy program ? 
There's a website called “[Google](https://google.com/)” that can help you find what you need. Funny name, I know, but trust me, it's great!
1- the program will be developed in C# 2- I didn't find a source code similar to mine 3- I live in Egypt and the program will be developed to small startup in Egypt too *I wanna know the factors as I don't have the background knowledge for this 
Despite this point, Do you know on what factors I can set my price ?!
Estimate the amount of work hours you need, multiply with your price per hour, give that as a price.
C# is off topic in this subreddit as this subreddit is exclusively about programming in C. Perhaps try to post this to [programmers.SE](http://programmers.stackexchange.com) instead.
The crash course in CS does what Petzold does in Code, just in video format. Good stuff. 
`-ansi` sets the compiler to only accept ANSI C (C89) constructs. You normally want `-std=c11` for C11 compliance these days as ANSI C is pretty outdated. `-pedantic` causes the compiler to warn about constructs that are not strictly legal but still accepted out of tradition. It does not change the language standard you work with and is mostly not useful for general software development.
Okay, I didn't see a subreddit for C#, all I wanted is how to price my work ? 
I said so before: estimate the amount of time you need to do this, multiply with your price per hour and give that as a price. The amount of time you need to do this is something only you can know.
If his name is used as author I would imagine he is. 
I know but I'm faster skimming through books when they are printed
I can't find it in the desktop version. 
BTW is gilding still a thing? Have you got a patreon or any micropayment account? I would like to say thank you properly 
If you want to understand how UNIX works, read Lion's Commentary. The MIT ported 6th edit UNIX to x86 in a project call [xv6](https://pdos.csail.mit.edu/6.828/2012/xv6.html) which is interesting if you want to see how these concepts translate to the x86.
I haven't read either. We have gathered known good c programming books in the sidebar. I advise you to read them instead.
Simple, Love programming as a programmer i think i should know C
No need. But thank you. 
By giving out the binary you are handing out the recipe to the 'secret sauce'. The only new information you can introduce which the attacker doesn't already have is the password. One possibility is you could hash the password into a fixed length key value, then use that key value for a symmetric key cipher. The password does not need to be super long so long as you use a hashing method intended to be resilient to brute force attacks, like bcrypt. The critical part here is that the password hash does not ship with the binary. A brute force attack is much harder because you don't know what the output is intended to be, so the computer would need to run heuristics to check possible valid secret messages (e.g. does the secret message contain english words?). This is different to the case when you know the hashed result of the password and you're just trying to figure out the password. You can even frustrate such heuristics in ways outlined in other posts, like having the output of the symmetric cipher be a set of hex numbers which reference into an extremely large dictionary. In that way, every password yields a set of english words. Would the attacker then have to write an algorithm to check on the grammatical correctness of the output? What if you intentionally use bad grammar in the real secret message? We're now talking possibly multiple seconds to check a single password, meaning brute force or even dictionary based attacks are no longer viable, even for relatively short passwords.
Good &amp; thorough answers should be rewarded. Enjoy the week of Reddit premium. 
Thanks man! 
It’s still used by NASA for rovers. C is still a great base language after assembly. Arm is trying to become big so you’ll need a close language to ARM. C compiles faster than C++ 
If you look at thé Makefile, it use gcc/mingw to compile files.
I implement all the algorithms I research in C.
I think K.N. King's C Programming: A Modern Approach should be in the list if possible
Still the fastest language that is high level enough to build practical things.
I use it for what people use C++ for. Games, for example. I just don't like all the complexity and warts of C++
Because when you need a UDP and TFPT impl in under 1k, nothing except for assembly comes close.
Linux kernel hacking. Educational tool to bridge the conceptual understanding gap between higher level languages and machine code -- especially when it comes to data representation in memory.
Because it's incredibly simple without being as rote as assembly and needlessly complicated as an OOP language. &amp;#x200B; I had an incredibly hard time learning to program because everyone would recommend something like Java or Python. They're fine until you're forced to learn meaningless abstract concepts like classes and methods. Then I switched to C and along with learning what it actually means to create a struct and what a string really is and how you're just putting numbers in a registry manipulating it and putting it back. I wish I would have known when I started learning that I should stay away from any programming language that uses OOP.
For embedded development. Why? Because for some wicked reason people still use it in 2019 so there's no tooling for the countless languages who do everything better than C. 
What makes C in particular "close to ARM"? I'm curious
You don’t have to use C but C already works and it’s the basis for pretty much every operating system right now. So it’s great for dealing with what’s already going on. I mean technically you can open up ARM assembly and write a compiler in Terry Davis’s HolyC, but it’s probably not going to translate well. Imagine this you have you basically machine code, and the hardware architecture that communicates to that is based on the chips instructions. For the most part that’s intel amd and arm. So you have assembly in arm x86 for 32 and 64 bit and you also have the intel 64 but assembly. This is a bare bones next to machine language instructions where you have assemble and link object files. This is where you start to write your boot loader and your compiler. The compiler from assembly is a bitch on its own. So you aren’t going to write the greatest high level language of all time based on it. You’ll want to write a basic compiler language to then begin to develop your basic architecture to begin writing higher level languages. This would take a while to go into, but basically you need to understand how computers work with operating systems to begin with. I’m also not a huge expert and I’m sure others on here would be better to explain it. 
&gt; Estimate the amount of work hours you need, multiply with your price per hour, give that as a price. You forgot “write a detailed specification”, which can take weeks or even months depending on the complexity of the application and its environment. Regulatory requirements, adherence to industry standards, interoperability with legacy application or competing product...
OP is located in Egypt where this probably doesn't matter too much. That's why I asked.
GNU has its own ARM C toolchain, which is reasonably effective as far as I understand. My question is basically why C translates to ARM better than other similar level-languages. There are native python interpreters for microcontrollers like Arduino, which use ARM instruction sets. The argument for using C over Python on an ARM system is the same as every other system architecture, at least as far as I know
It’s simpler than C++, has pretty good tooling and support, minimal runtime, and manual memory management. The main reason not to use it is that the standard is very unclear and there’s a contention between low-level features and assumptions the compiler can make when it optimizes. You might not always agree with how your compiler interprets the standard.
You should probably start with gcc if you're on an Unix environment (chances are it's already installed). On Windows it goes by the name of MinGW.
used them both also add -Wall to enables all compiler's warning messages. This option should always be used, in order to generate better code. 
It's stupid simple and the closest I'll get to the metal while still being practical. I do graphics and game programming in C just because I personally find it easier to organize code without worrying about all the boilerplate c++. When I first started, I used c++ but I quickly got into the problem where I would focus more on software engineering rather than writing code. This is not to say c++ is bad but my priorities lied on fast code over following software engineering protocol. 
This is likely good enough, but since it's integer based it might not be able to return all possible doubles. Since OP is on C++, [std::uniform_real_distribution](https://en.cppreference.com/w/cpp/numeric/random/uniform_real_distribution) should be better.
Its simple and works. A good amount of embedded programming is done in C
In the preface the author says version 0.12 was chosen because, although it is indeed a very old version, the design principles already show up and are still valid for modern kernels. According to him, only a single important subsystem for understanding the kernel is missing: VFS. 
In my opinion, the book by Stephen Kochan has less additional information that is needed for people that don't have any programming experience. With the book by Stephen Prata, you can start to learn C without any background in computer science, just know how to read and print on a keyboard.
You may want to checkout the "Releases" tab, which contains an installer: https://github.com/renatosilva/winutils/releases
Game engine development (specifically for consoles and mobile). 
This is not the subreddit you are looking for. C is not C++ goto /r/cpp_questions/
i know but other than classes and types they are close enough
well i at least wanted to check if anyway
C++ What? Eww I'm sure that C front 84 thing has it's own place elsewhere.
-_-
They are also different insofar as C is on-topic here and C++ is off-topic here!
It's small, very fast, without all the unnecessary abstractions needed for most code written these days. Especially at the OS and microcontroller level. I like the DIY mentality as opposed to the Lego block mentality when I want to solve a new problem or coding challenge. 
I use C because: 1. It is a simple language from perspective of a compiler. 2. Data and associated methods are kept apart. 3. It has pointers. 4. It doesn't change. 5. It doesn't get in my way. 6. You end up writing your own tools/libraries to learn C because you can't do anything with C out of the box. 7. You can optimize for memory efficiency. Sigh.. 8. Speed?.. Nah.. &amp;#x200B; Respectively, I get: 1. A very good approximation of portability, compile time isn't a bully, it is very easy to autogenerate sections of C code and you have many tools to choose from. 2. You are more likely to be successful to encapsulate the data and associated methods from C in your own weird way with that weird language you wrote your self.. just because... 3. Pointers are bullies, but I still need them. :'( 4. Less need to refactor over time. 5. Breaking idioms or lack of "rigor" doesn't shame me nor keeps me from sleep when I need to do something the language wasn't designed for. 6. Writing these libraries and tools makes you C why other languages have certain features and why they where written. You suddenly find your self having a X-ray vision into other programming languages which reduces the time to learn them. 7. Sometimes you can't help your self because you thing you need to write that stuff compact. It is a fucking pain, but C looks the other way while you do that painfully awkward ad-hoc thingy compact. 8. I personally don't program in C because of speed. When I need speed, I factor that in when I make decisions about the architecture of the software. The language of choice doesn't really matter in that regard. Speed has less to do with the language and more to do with your choices. . These are my excuses. The real reason is because I hate my self.
I have read both (I like to review programming books) so here is my very brief opinion of them. C Primer Plus is a very big book that often takes a long time to communicate something quite simple. This isn't a totally bad thing as it generally does a pretty good job at explaining each topic however it can be pretty draining especially for a new comer to programming (the books target audience). Personally I feel the editor(s) should have cut things back a bit on the simpler concepts. It is a good book just a third longer than it need be. Programming in C is also a good book. It isn't quite as well suited to a beginner to programming or to someone switching from an interpreted language imho. It covers mostly everything well and is a much easier read than C Primer Plus. Honestly though both are decent enough books. With C Primer Plus you will need a lot of willpower to work through it but you will come out quite competent. Programming in C will be a quicker read but you might have to spend a bit of extra time to really solidify your knowledge around any subjects you don't quite 'get' along the way. As others have mentioned probably the best book for a beginner these days is C Programming: A Modern Approach (2nd edition) by King. It is a well written book that gives just the right amount of detail for each topic without being too wordy. My only complaint is that some of the extras offered by the author on his website are limited to educators. As someone who enjoys self-learning this is frustrating to me. K&amp;R is often recommended for beginners which I *strongly* disagree with. Sure if you are coming from FORTRAN (like when the book was written) then it is fine but not for someone coming into programming with no experience with imperative, compiled programming. I do think K&amp;R has its place but I feel its biggest value isn’t from its content but from the problems it presents to the reader as tasks to solve. 
[This is a house of learned doctors](https://youtu.be/t6owObPHDyQ&amp;t=49s)
Pretty much every hardware you use on a daily basis works because of C. The lowest layers are written in C. It gives you a lot of control over time critical and space critical applications. When it comes to application or higher level software I can see how languages like python are so much more friendlier and offer a variety of libraries. 
You can call into C libraries from basically *any* language. I like using that fact, even if it is from Python ctypes and similar.
C++ is off topic in this subreddit. Please post C++ questions to /r/cpp_questions instead.
Im trying to find the discriminant. So the program does work correctly as intended but the first scanf makes me put 2 inputs. I still get correct answer at the end but somthing is wrong with the first printf/scanf. this is what the output looks like. Enters the three numbers Enter the first Variable:1 2 Enter the Second Variable:3 Enter the third Variable:4 sum = -8.000000 
“It doesn't work” is not an error description. That said, remove the `\n` from the `scanf` invocations.
my first scanf takes two inputs. but i still get correct results. dont know whats the problem
Im trying to find the discriminant. So the program does work correctly as intended but the first scanf makes me put 2 inputs. I still get correct answer at the end but somthing is wrong with the first printf/scanf. this is what the output looks like. Enters the three numbers Enter the first Variable:1 2 Enter the Second Variable:3 Enter the third Variable:4 sum = -8.000000
thanks not it works.
It does not take two inputs. What happens is that because you specified `\n`, `scanf` scans all whitespace until it encounters a non-whitespace character and only then returns. As I said, remove `\n` from the `scanf` strings and your program is going to work.
I don't know why you'd even mention bitwise operations in a question regarding floating point vs int. Also most bitwise operations on signed values are well defined. It's only shifting that is not defined by the standard.
thank you mate
I use it to receive a paycheck every two weeks. &amp;#x200B; (linux embedded networking). &amp;#x200B; I think C needs to be replaced. It's too easy to write bugs, and it is not good for multi-threading (because it's so easy to write bugs). I had/have hopes for Rust to do that but I think the learning curve is probably too steep. I don't think C++ is the answer either. &amp;#x200B;
/u/Forsaken_Attention being able to describe a problem more completely than just "it doesn't work" is a very valuable skill. The computer always does what it's told. A statement like "it doesn't work" or "it's broken" doesn't mean anything. If you learn to describe what you are expecting and what is actually happening you'll learn faster or be able to get better help.
This is probably the best answer. Alternatively you can set the default visibility to hidden and manually add visibility to the API functions. Gcc flag: -fvisibility=hidden
I'm working with legacy software that uses C. I love it for the simplicity and because I like the do-it-yourself nature of the language.
That's really not true, it works on Windows. You just need to make sure you select the C/C++ package. I think it defaults to Java. You could also use
Tadaaaaaa http://imgur.com/a/8XhZjZA
&gt; You can optimize for memory efficiency. Sigh.. Why the "sigh"? &gt; These are my excuses. The real reason is because I hate my self. From an outsider's perspective, it sounds like you use C because it's the best tool for the job for most (all?) of the projects you work on. If my assumption is right, then no need to hate yourself for that!
If you want to do some high performance computing stuff and do not want to work with Fortran than C is the way to go. And like many above have mentioned it already, it is a pretty fast language and yet fairly easy to use.
I learned C with K&amp;R in the 80's. After a long pause (doing boring programs for the financial industry), I went back to C and read King's book. It's an excellent book which enabled me to refresh my memory and learn the c11 standard. K&amp;R is fantastic, it teaches you to program well. To be honest, I sometimes read a chapter just for the pleasure. You have to read it very carefully. Explanations are short but deliver tons of informations: Almost each word counts. Kernighan is really a very gifted writer. 
C is the language that Linux(kernel) and Linux(general tools) are built around. If you're into Linux, the more C you know, the better.
It sounds like you don’t like what you do. 
I 100% agree. Though it is not free like the other resources. 
the division and modulus part is Weird. it shoes for 5/10 as zero. and 5 %10 as 5. which is wrong. Im guessing it have to do with data types but im not sure if thats why it is??
I work in embedded software. No other language is suitable for programming some of the smaller embedded microcontrollers.
The type `int` is for integers. It cannot represent fractional values. If you want to compute with fractional numbers, use a floating point type such as `double`. Print a `double` with `%f`.
Your division example is 0 because of integer truncation. 5/10 = 0.5, but it is stored in an integer type, which for this number is 0. If you do 3/2 and store the result of that in an int, you will have 1 (since the integer part of 1.5 is 1). You need a floating point type (float or double) to store fractional information. The modulus problem is because of the order of operands. Modulo gives you the remainder after division. For 5/10, 10 goes into 5 zero times, with a remainder of 5. If you'd done 11%10, your answer would be 1.
In the world of programming, there are levels of programming that are closer to the processor and closer to the human operator. - binary (the most basic of computing signals, each model of chip is different) - machine code (ASM, assembler. A common summarized expression of binary. Families of chips are usually based on the same machine code command set. It is possible for humans to code in this, but it is very tedious. It is assumed that if you can write it in ASM to the most efficient level, then you won't need to revise it on a binary level, which is pretty much impossible for humans.) - compiled low level code (C is on this level. Programs written by humans that have been auto translated to machine code, and then translated to binary. It's good enough for humans to understand but is still a bit raw. It's possible to write machine code that is more efficient than C, but it takes a lot of time to engineer solutions like that. It is assumed if you can resolve all errors and memory leaks, it's good enough for critical applications.) - compiled high level code (python, PHP, powershell, basic, etc. Higher level programming languages that is easy to work with for humans, but goes through a thicker compile process or is interpreted at runtime.. This can lead to slower code than revised low level code. Even if this code is logically perfect, it's possible for this code to crash at no fault from the programmer. This is not dependable for critical operations. It may be good enough for most general operations.) Using C would be important on the hardware and critical software levels. Using a higher level language for jobs that will eventually link to low level languages is fine. An example of this is Powershell for Windows. You can extract lots of information about your "software system" and "physical hardware" from powershell with simple commands that are crash proof. It's a high level language, but you're pulling info from linked low level processes. this is fine. In fact, it's safer than trying to invent your own low level programs. A company that makes hardware, like a mouse, would want to program it's driver software in C. This is because a lot of critical input and output goes through the mouse and computer processors. Writing a driver in a high level language would yield responses that are too slow. If you want to write a software that works faster with less memory hogging, doing it in C is a great medium. It's understandable by humans and is "close to the processor." Yes, there's that little gap in between C and ASM, but in general, that is considered negligible. some high end companies will have engineers work on that level. A good example of this would be an older videogame named DOOM from the 90's. It is written in C. At the time, Doom was amazing because nothing else looked as good or played so well. Previous offerings in the genre of gaming looked like children cartoons. DOOM gave people motion sickness. Also, since C is somewhat "port-able," which means you can translate it from one binary/ASM instruction set to another. A popular joke with engineers to do as a proof of concept was to compile DOOM (the 90's game) to other processors. Usually, processors made for other things. [Example.](https://www.youtube.com/watch?v=k-AnvqiKzjY) This doesn't compile perfectly. There was some rewriting that needed to be done. The point is that C is low level enough to translate to a lot of different things.
Thank you! 
Thanks bro.
It helped me understand the different types of variables, their memory foot prints, how memory works in general, pointers are incredibly useful/awesome, I could go on. It's a great language.
all the point here are good points but they are all true, but i personally think learning C even if you are not planing to program in C forever will make you a better programmer, after all most C is father of most programming languages 
Why do I hire people because of C experience? Because a lot of what we do involves embedded systems, and C has such a low operating overhead, and is such a powerful language, that it's perfect for the job (although we're also using Rust a bit now too). Why do I write small apps in C? Because it's easier for me to simply tell a computer what to do than to deal with higher levels of abstractions that I frankly don't understand.
By native do you mean the python interpreter is written straight from assembly? 
Think about how word wrap behaves in your word processor. This is a simplified version of that behavior. 
`-pedantic` is great as it identifies cases where your code relies on quirks of the compiler you're currently using. Essential if you're writing something to be used by other people , or yourself at another time with a different compiler. Only gets annoying if you are forced to use some third party headers that are nit standard-compliant
whatever people pay me for.
These options generate better diagnostics, not better assembly 
I don't have the book handy, but I believe they are saying that if your input is a line of text and the action you should perform is to change it from a single line of text into multiple lines. The breaks happen where there are already spaces or tabs and occur at the first space or tab before the n-th position in the input line. So, for example, if your input line is: &gt; I am a line of input text. And your "n" is 15, then your output lines would be: &gt; I am a line of &gt; input text. If the input can be broken up into more than 2 new lines based on your "n" value, then I'd assume you start counting anew from the start of the input where you left off after breaking out the first sub-line. And you should handle the situation where you don't have a space or tab before your desired break point. 
What sorts of algorithms are you working on?
I'm an indie dev and write desktop software for generating toolpaths from 2D image input for CNC machining/routing for hobby/DIY signmaking/engraving/etc. Generating toolpaths for automated machines is more of an art than it is a science and an arcane one at that. Toolpath-generating CAM software has generally commanded top-dollar prices compared to other types of desktop software and I figured there was a bit of an untapped market developing low-end niche CAM software for CNC art - which has proven to be the case. People will pay good money for good toolpath generation! I've been developing this software for ~18 months while running an Etsy business with my wife which we sell signs/engravings that she designs in Photoshop and that I cut on our CNC machine. This was the reason I started developing the program in the first place because my workflow using free and paid software was just tedious and I knew it could be streamline into a much more concise and straight-forward process. I figured if it was something we had a use for then surely other people would too. I've been working on it just about every single day, adding features/functionality and fixing bugs. In developing my software I draw on 20+ years of primarily hobby gamedev experience, insofar as programming in C goes, and about a decade of experience working for my late father in his precision machine shop that I never thought I'd do anything with. Lo-and-behold I finally got into CNC *after* my father passed. Oh the irony. I've also done quite a bit of freelance coding back in the day (rentacoder anyone?) and firmware coding jobs for tech startups. Developing my software in C allows me to write optimized and highly performant algorithms for the generation of toolpaths, and the calculations for performing virtual cuts in a live CNC machining simulation mode in the software - so that users can examine what their toolpaths will cut like once actually running on a machine. As of now there are just under 22k lines of actual code, making it the biggest project I've ever developed. My second largest project was a scriptable/moddable multiplayer procedurally generated voxel-world game engine - it was ~95% complete, and fully functional at about about 20k lines of actual code. Actually going out and promoting it and doing all the PR stuff I realized was going to be a project unto itself which I wasn't up for, and I just got burnt out. That whole project was written in C as well, written using SDL for platform abstraction and rendering with OpenGL. Netcode was something I devised over the last decade and is comparable with modern multiplayer game performance, and is something I take pride in pulling off as well as I did. Games are described via external text scripts, using a very simple command-based language for defining entity types, logic, geometry, behavior, and get compiled into a simple binary called a 'bitfile' which is distributed to players who join a game server so that they can execute logic on their end. In turn those players could start a game server running the downloaded bitfile and further distribute the game to other players who join their server too, allowing custom games to easily propagate and be distributed automatically, which I thought was clever ;) I'm sure I'll be doing even more projects in C in the future, because it's what I know best and most of my projects are computation/performance sensitive. I like to push the boundaries of the computational capacity of peoples' machines, and C lets me do that pretty effectively. I'm sure there are other languages I could learn to do some performance-sensitive work but why traverse a new learning curve when I already have a pretty deep reserve of C knowledge and expertise to drawn on? To each their own I figure. 
C is a good programming language for certain kinds of libraries. Nearly every programming language in existence can use libraries written in C. Write a highly optimized library in C and you can use it from other programming languages by writing some kind of wrapper. C is sometimes used as an intermediate language, some higher level languages use C--, a subset of C to pass to the compiler backend. 
Interesting comment. I'm tired of C (so much boiler-plate), and I think C++ is a mess, primarily because of its implementation of references, but also because of its age. On my own I do audio and midi programming (goal of getting on the Apple Store) with Objective-C and Swift, which I _like_, but neither of these can be considered c replacement candidates. 
I do research in heuristic search with pattern databases. See [24puzzle](http://github.com/fuzxxl/24puzzle) for some research code and [my thesis](https://opus4.kobv.de/opus4-zib/frontdoor/index/index/docId/6558) on the subject.
Just want to point out that 5 % 10 is 5. The other comment about integer vs floating point arithmetic is correct.
I know this a C sub and all but maybe you would like rust? It’s a great alternative to C++, it’s higher level than C, and a lot less tedious without a lot of the complexity of c++ (for example, it doesn’t have classes). Rust also has a lot of really nice gamedev libraries.
PIC micro-controllers embedded programming. 
Embedded microcontrollers.
I used C in fiscal cash registers at my old job(4 years ago). There were 2 models with linux in them. One of them was avr32 with about 4mb of ram that my program can use and the other was a armv7 with 32mb of ram. But oh boy, on top of completely different linux setups with obviously different kernels with different patches for respective cpu's compilers and standard libraries were different too. If i remember correctly uclibc and glibc. I was expected to create software that run on both with a single code base. Arm one was not much of a headache but the avr32 one was way off standard. Many stuff that should not even compile and does not compile on arm version did compile on avr32 one worked *most* of the time. Avr32 one did not segfault/crash on illegal memory access most of the time. Sad part is that they were both different versions of gcc with different patches, not completely different compilers. The problem was that code work only most of the time, but fiscal cash registers has a brutal and rigorous certification process(certified by both banks and government). They have to work perfectly *all* of the time lest the company can be held liable and even a .1 cent problem can hurt the company's business to the point where everyone uses competitor products. On top of it the gdb on the avr32 was not working properly because of the hardware and i had to compile, copy it over to the device via serial port and try to debug there. Arm one also came out about a year later so i started with avr device with no debugging and no simulator/emulator because there were closed source libraries and hardware spesific to device. Libraries did not have x86 or arm versions at the time so i could not just compile/debug it with a better compiler. I fixed about a 100 little bugs after the arm compiler/libraries and debugging support came along. I ended up writing a library/api of many things the device can do(display a menu/prompt/yes-no question/message, print a registered receipt, get payment from a credit card, get keypad input, make requests to server etc.) in C and exposed these to the Lua and created the actual apps in Lua. 
&gt; The main reason not to use it is that the standard is very unclear and there’s a contention between low-level features and assumptions the compiler can make when it optimizes. You might not always agree with how your compiler interprets the standard This is still better than using a language with no standard 
I disagree with their object oriented 100% philosophy. Data is data, trying to wrap it too tightly like OO does just causes a mountain of problems for interoperability. C++ generally has too many damn features and it's just more complicated than I need it to be, for people that use scripting languages like Ruby Python etc I'm sure the expanded standard library is useful, but for me it's just not what I need. Also, it just tends to be slower to compile and run than C. as for Generics/Templates, I write the code myself and use a _Generic macro.
&gt; It's too easy to write bugs Have you tried static and dynamic analysis? They make it more difficult to ship bugs.
That's a really good explanation. I had to learn Java first since that is what the college board wants. There is now another AP CS class, which you get to pick which language you want to develop in. However that means you won't be taught by the teacher how to use your language of choice. There is something to be said for OO languages though. My friend prototyped a solution to one of his projects in Java, but the project had to be done in C, which was giving him a lot of trouble.
I *love* rust. It’s complexity is starting to approach c++ though. Also it’s compile times are pretty high. I find myself using C when I want simplicity or low compile times. (Or on an arch that doesn’t have rust yet) This wasn’t meant to say the person you’re replying to wouldn’t like rust, more that there are reasons to choose C over it still.
I had people yelling at me on twitter last week because I used this as an argument against rewriting the world in rust. “Don’t use bad architectures” lmao. Spoken like someone who’s never worked in embedded.
I keep hearing this, but I also keep hearing about Rust. Is there a reason Rust won't work as well?
Do you know of any good tutorials for audio/MIDI?
Today it's mostly tooling and vendor support related. Long term it's viable
No, just that it's the pre-installed method of programming the device
I like your explanation
What's an abstraction you don't understand? Besides the majority of OO. I hear that one too much.
I was basically getting frustrated with all the books telling me a dog is a type of animal. I guess I'm more data oriented in how I want to be explained things. Watching the Handmade Hero videos and Casey explain it in terms of what exactly a computer is doing with the numbers you give it made it all really click for me.
How does using C++ make you want to do "software engineering"?
I third it!!!
lowlevel and fast good 4 kernel dev
&gt; I was basically getting frustrated with all the books telling me a dog is a type of animal. Sums up my first CS class.
Software engineering is about code organization and translation. OOP is a form of software engineering along with all the "programming patterns" and rules that come with it. And it doesn't "make me want" to software engineer, software engineering is literally the only reason people use it over functional and imperative programming. 
The sum should be equal to -7 if a ==1, b ==3, c==4. Have you heard of gdb? It is a debugger that helps you see what is happening in the code. [https://www.gnu.org/software/gdb/](https://www.gnu.org/software/gdb/)
I work on a highly available, distributed, high throughput, custom adserver that needs strict control over memory allocation.
Rust doesn't support any eight bit microcontrollers at all. It's pretty much 32-bit up* which means it's only really for more recent, higher powered microcontrollers. * With the exception of the 16 bit msp430 apparently, which it has partial support for and no standard library.
And in industries that are very cost conscious (e.g. automotive), 8-bit micos are still very widely used. 
`nextline` is not reading a line, it’s trying to read the entire file. Note that your `c = getchar()` test is against `EOF`, but you have nothing to test for whatever `\n\r\v\f`etc. sequence you’re treating as a newline. The multiline-comment-ness of the input file has nothing to do with it segfaulting, it’s just bumping the size of the file up enough above 1000 that you’re (luckily, since this is UB) crashing the thing. (Also, use `size_t` for sizes; `int` is not a good idea.)
Writing SQLite extensions.
The interface looks good but personally I think you are overdoing the macros. It is made worse by presenting them as functions, so the user isn't aware of potential issues. #define vector_back(vector) \ ((vector) ? vector[vector_size(vector) - 1] : 0) A fairly random example from your source code, it is an unsafe macro. Some like the push functions are worse but more complex to paste here. Because vector is used twice any term there is evaluated twice. A problematic example: int* vector_array[8]; // populate vector_array with vectors containing data i = 0; value = vector_back(vector_array[i++]); In this example i will be incremented twice and the value from the second array entry will be returned, not the first. It also breaks your safety check. There is lots of discussion on unsafe macros online and options to work around the problem. The MIN() and MAX() implementations are classic.
If you're not doing organization in C, you're doing it wrong. It's still very important, as it is in more languages. 
That should be undefined behavior, for one you're not passing an array/pointer you're passing an integer. Into a macro that requires a pointer, compiler would prevent you from doing that with an error along the lines of "attempting to dereference an integer". I do agree that multiple evaluations could be problematic, but for a pointer/array it shouldn't be an issue? For instance if you passed vector\_back(&amp;(vector\_array++)), I think that should produce undefined behavior. I tried to use macros as sparingly as possible, but vector\_back couldn't possibly return the correct type without a macro. Push would require a conversion to a void\* to be generic, and those simply go against the goal of being simply generic for the user. The push\_back shouldn't have a multiple evaluation problem for value it's only ever used once.
There are a few things wrong here. I’m not sure what you’re doing with your headers. Just include them, right? Nobody’s going to give you a `&lt;stdio.h&gt;` that will break if included twice, and no modern compiler×system header setup will even try to read the file if it’s already been included. And of course, there’s no standard `STDIO` or `STDLIB` macro defined by those headers. (Frankly, if your system headers define stuff like that, they’re not well-written.) And then there’s your definition of `NULL`, which (a.) is a bad idea because literal `0` is an `int`, and (b.) has an extra semicolon there, so any time you use `NULL` shit’s gonna break oddly. E.g., if(ptr == NULL) do_something(); a = foo ? NULL : &amp;ptr; would become if(ptr == 0;) do_something(); a = foo ? 0; : &amp;ptr; (Winky-faces are not generally encountered in functioning C code.) All preprocessor directives, whether they’re `#ifdef` or `#include` or `#define`, end at the first (unescaped) newline. The C preprocessor is a completely different language superimposed upon C, and it doesn’t care about things like semicolons. Don’t define `NULL` unless you’re writing system headers. Even an unhosted implementation has `&lt;stddef.h&gt;`, which defines `NULL` appropriately for your language and compiler, usually along the lines of #define NULL ((void *)0) or for GNU #define NULL (__extension__((void *)0)) So just `#include &lt;stddef.h&gt;`, which you should probably be doing anyway. In `LLcreate`, you have the following: Node n; newLL-&gt;first = &amp;n; newLL-&gt;last = &amp;n `n` is a local variable, and it will cease to exist as soon as it goes out of scope. That will likely leave your linked list aimed at some arbitrary part of the stack, although the compiler’s free to just leave `first` and `last` as `NULL` also because you’re inducing undefined behavior. You can either use `NULL` head/tail to indicate an empty list, or place the sentinel `Node` as a field within the `LL` structure that everything can refer to, and that won’t disappear suddenly. I’d go with `NULL` personally. Anyway, without fixing this initialization problem it becomes much more difficult to determine what’s going wrong elsewhere. In `LLaddNode`, you’re only linking the `parent` links (which I guess would usually be called `prev`?), not the `child` (a.k.a. `next`), and you can’t link the child from an empty list anyway because (a.) that `Node` is not really anywhere, and (b.) that `Node` would be shared by any other lists that happened to exist at the same time. For a `NULL`-terminated add-last, the usual procedure is newNode-&gt;next = NULL; *((newNode-&gt;prev = list-&gt;tail) ? &amp;newNode-&gt;prev-&gt;next : &amp;list-&gt;head) = newNode; list-&gt;tail = newNode; (Though it doesn’t look like you really need double-linking or both head and tail anyway, since you’re just doing a LIFO/stack. A stack push can be done with newNode-&gt;prev = stack-&gt;top; stack-&gt;top = newNode; and then a pop can be just if(!(node = stack-&gt;top)) return -1; stack-&gt;top = node-&gt;prev; which is a little easier to start with.) Your `LLfree` does if(ll-&gt;size &gt; 0) while(ll-&gt;size &gt; 0) and then calls into `LLpopNode`, which again does if(ll-&gt;size &gt; 0) Both the initial `if` and the internal `if` are superfluous, so you can just collapse those to while(LLpopNode(ll) &gt;= 0) (void)0; and there’s your loop. (A bit inefficient, but whatever. Usually you can just do something like Node *p, *q; if(!!(q = ll-&gt;head)) { p = q; while(!!(q = q-&gt;next)) { free(p); p = q; } free(p); } to empty the list quickly.) More generally, the usual way to handle things with a fixed size like `LL` is to leave allocation of the list itself up to the caller, and only manage de-/allocation of the nodes. This would give you an interface like void LLinit(LL *list); void LLdeinit(LL *list); int LLaddNode(LL *list, int datum); int LLpopNode(LL *list); Usually popping a node would have as its output the value that was popped, so you’d want to do int LLpopNode(LL *list, int *out_value); instead. Otherwise the client has to go peering into your structure, which is mostly a no-no outside of fields it contains directly.
Thanks a lot. Will improve my code )
Writing code for resource constrained network appliances is where I've used it. There isn't enough RAM or CPU to use something wasteful like Java or Python
I'd actually like to load the whole file., I might be wrong but the file is only 244 characters long, so it shouldn't exceed the size of the array right? When I remove the first three lines that contain the multiline comment, I don't get any errors.
I’m not seeing a segfault when I run it, and that’s at `-O0`, `-O4`, and `-Os` targeting Linux x86_64, x32, and i386. Try including debug info (`-g`) and running it through a debugger, maybe?
I am, just not object orientation
I'm pretty new to C, what does &gt; at -O0, -O4, and -Os mean?
\-O0: no optimization \-O4: optimization for better performance \-Os: optimization for smaller code size
No optimization, very high optimization, and optimizing for size. If a crash happens at one level but not another, makes it easier to guess at where/why that might be happening. Ditto the different architectural/ABI targets.
Simply typing 'make' will immediately install the code to /usr/include. This caught me off guard. It's better to make 'make' do nothing or do testing, and use 'make install' to install it. 
Is there a reason you wrote it all in a header file as opposed to building it into a library to link against? Other than that it looks pretty interesting I will look through the code some more when I have time. Whenever I write C projects I'm always torn between having memory allocated prior to function calls or have the function allocate them, but I usually settle with a unique allocate function earlier, so that the user remembers to call free after. From a quick glance at your example it looks like you call push(vector, i) where vector can be NULL, so I assume you allocate the memory inside the function. If so was that a conscious decision not to have a vector_init call first (or something similar), or did that not really come up?
Yeah, that makes sense I was split between which to make the default. I'll change that in a few.
There's been a lot of popularity with small single-header C libraries in the last few years. It makes it incredibly easy to copy, and Windows doesn't have any type of standard library locations so it's a fairly common way to accommodate that. If the code is small enough then this is the most convenient way to go.
So my code base is mostly macros with 3 actual functions (alloc, set\_size, set\_capacity), and it didn't make sense to split those up when there were so few functions. &amp;#x200B; I really like this question! Okay so my first Vector implementation looked like this struct Vector { void** data; size_t size; size_t capacity; }; in that case I had an init function and I agree with you I would totally write one for this, but it would actually be more painful to use. To show this here's a standard example of C\_Vector (for completeness) int* vector = NULL; for (int i = 0; i &lt; 100; i++) { vector_push_back(vector, i); } as you stated vector\_push\_back allocates memory. The only way to be able to write an init function would be void* vector_init(size_t size_of_data) { return __vector_alloc(NULL, 12, size_of_data); } which indeed works, and I may add this at some point, but I think it adds complexity. Here's the two in comparison use wise int* vector = NULL; // When you call push_back, it knows that it's a vector of ints // vs int* vector = vector_init(sizeof(int)); // you have to tell it that it's a vector of ints // either way you can vector_push_back(vector, 1); I personally think the bottom looks uglier and is less simplistic for the user since you'd have to provide the sizeof(TYPE) for the vector in the function call and not just in the declaration. I may just add the init just for good measure. 
https://blog.jetbrains.com/clion/2015/05/debug-clion/
You may or may not care for this idea, but when I was making my vector library I used a macro for my init function like this: #define new_vect(type, size) \ (type*)priv_vect_alloc(sizeof(type) * size); And I'd use it like this: int* vector = new_vect(int, 20); vect_add(vector, 1); That would allocate 20 integers. I liked that because it looks a bit like vector&lt;int&gt;. 
I really like the idea, I implemented it hopefully you don't mind :) it simplifies creating with a capacity int* vector = NULL; vector_reserve(vector, 100); //becomes int* vector = vector_init(int, 100); &amp;#x200B;
You have multiple problems with your program: * `fetch_lines()` is not declared properly. * You're not using `lineptr` properly. What is its type? (It's not `char *`) * You're not reading input in lines. Where's the check for newline? Where's the null byte to terminate a string? This program would not compile as it's written. Put `fetch_lines()` before `main()` or code a forwarder for it. Also, think about how your program will terminate. Will it read chars forever?
Its simple. I know it. There’s libraries for everything. It’s conservative and not changing radically every year. Don’t eat me wrong, I like Swift, Rust, etc, but I work on low level projects that live close to the metal. I need the fine grained control C gives me. Most of my code is written for me to do my research, and any shipped code is purely proof of concept code for my customers to use as a starting point. So C is the easiest way for me to get my job done. When I don’t feel like dealing with C, I stick to Python. 
Good catch with lineptr. It took me awhile to see the differences between ***char \****, ***char \[\]*** and ***char \*\[\]***. Thank you!!
Do you have a link for Casey's channel? I found Hero's already.
What’s wrong with ObjC? Besides the weird syntax, I actually really like it. It provides a thin layer of syntactic sugar over C and has a nice set of collections in Foundation. 
You're welcome. Your revised version is much improved, but still needs a little work, however. Why are you calling `free()`? Other than that, looks like it might compile and run. Although, you have a typo at the very end. Have you tried compiling it?
Sorry for asking for your help again :( By using unit tests and adopting some of the advice (some things like a more efficient way to get rid of nodes was left out), I was able to fix most of the problems. However, I cannot free ll after removing and freeing all nodes. in LLfree ll is assigned NULL but when returned back into LLfreeTestRemoveLLonly() or LLfreeTestTotalWipe() ll is not NULL. I find this behavior weird and not sure what to do :/ [link to code](https://hastebin.com/jipupiqoso.cpp) &amp;#x200B; nice thing about hastebin I can give line #. LLfreeTestRemoveLL.. is 224, the other one is 260 and LLfree() is 75 PS thanks for your help, last post was the best I have seen :)
Isn't the !! unnecessary ? for (Node *q, *p = ll-&gt;head; p; p = q) { q = p-&gt;next; free(p); }
No idea, but if I were doing embedded and had the chance, I'd use c++ at this point. 
Will not work with most basic code. // created some vector for (int i = 0; i &lt; 10; i++) { vector_erase(vector, i + 10); } This will expand to for (int i = 0; i &lt; 10; i++) { // vector_erase(vector, i+10); ({ if (vector &amp;&amp; i + 10 &lt; vector_size(vector)) { // unsigned char swap_temp[(signed)sizeof(*(vector))]; size_t i = i + 10; printf("%zu!!!\n", i); /* HERE */ for (; i &lt; vector_size(vector) - 1; i++) { // memcpy(swap_temp, &amp;vector[i], sizeof(*(vector))); vector[i] = vector[i + 1]; // memcpy(&amp;vector[i + 1], &amp;swap_temp, sizeof(*(vector))); } } // vector_pop_back(vector); }); } Have a look for yourself what that yields. Better guard your variables like the following (note that this still isn't good, as `vector` could expand to something that interacts badly with uses inside the macro, so better store that in another name-guarded variable as well). #define vector_erase(vector, location) \ ({ \ if (vector &amp;&amp; (location) &lt; vector_size(vector)) { \ unsigned char vector_erase_swap_temp__[(signed)sizeof(*(vector))]; \ for (size_t vector_erase_i__ = (location); \ vector_erase_i__ &lt; vector_size(vector) - 1; \ vector_erase_i__++) { \ memcpy(vector_erase_swap_temp__, &amp;vector[vector_erase_i__], \ sizeof(*(vector))); \ vector[vector_erase_i__] = vector[vector_erase_i__ + 1]; \ memcpy(&amp;vector[vector_erase_i__ + 1], \ &amp;vector_erase_swap_temp__, sizeof(*(vector))); \ } \ } \ vector_pop_back(vector); \ }) 
Was there a pop\_front ? Maybe I just missed it. Not the most sensible thing to want from an array, but ...
Solved by making malloc clients responsibility, same for free for LL.
That's equivalent to vector_erase(vector, 0). It's not something you'd want since it's linear time as a result it's in vector_extra.h 
The `!!` keeps the compiler from kvetching at you about maybe using `=` as if it were `==`. I like to unroll the loop by one, but as long as you free behind your fore-pointer it works out.
In embedded systems development, you don't care much about any particular standard. You'll be spending more time looking at the assembly your specific compiler outputs for the one device you care about.
That's interesting - could you explain why you'd do so?
Assigning to a pointer scoped to inside a function won’t have any effect on pointers outside the function; the pointer value is just a “name” of sorts for the memory block. Similarly, if I say int x = 4; int y = x; x = 3; then `y` still has the value 4. Assigning to `x` didn’t affect `y`, because there are different storage locations holding those values. Analogously: Joe is a fine fellow, who dies when a big light-up sign (“`free`”) falls on his head. His house and belongings stop being his, and they’re put up for auction. Other people can keep writing and talking about Joe long after his death and the loss of his things, but trying to contact him directly in any way or make use of his former possessions ends badly; e.g., communications timeout, haunting, possession, or arrest. De-analogously: If you make *any* use of a pointer after the memory block it refers to has been freed, you’re evoking undefined behavior. A C implementation is allowed to wander off and set any pointers-to-freed-memory it comes across to a trap representation, for example, and it’s allowed to do that whenever it wants, not just when `free` is called. (This allows for certain kinds of garbage collection, although implementing such a thing tends to be impractical or wildly expensive.) Normally you just let pointers-to-freed fall out of existence when their scope ends. Clearing variables to `NULL` isn’t necessarily safer unless you explicitly check for nullness after the fact, and it doesn’t always do anything if the compiler determines that the pointer’s `NULL` value isn’t used. Clearing pointers in the allocated nodes and list before freeing is a slightly better idea, although again, the compiler may discard those assignments if it understands what `free` does to memory blocks.
C++ allows you to create 0 cost abstractions. By that I mean that the code abstractions will not slow down your binaries compared to writing C or even writing assembly by hand. But those abstractions enable you to more easily follow good software design practices which will help your end users dramatically, and they help you write code more rapidly. With c++ you can create very convenient interfaces and wrappers much more easily than with raw c, and since there is a lot of tooling around templates, you have very good tools to tailor your systems to your particular environment. Check out [this video](https://youtu.be/zBkNBP00wJE) for an example of how much control c++ can give you over compile time behaviors. Dude ends up writing an entire game in very few lines of assembly with all the awesome interfaces from c++ still intact. For someone with experience in embedded systems, check out [this video](https://youtu.be/c9Xt6Me3mJ4). Or you can find a few other good videos on the subject. I'm not an embedded sys engineer, but I like getting kinda low level with my c++. The way I think about it is that the library writer's complexity goes up a bit with a switch to c++, but there are enough awesome built in abstractions that things are manageable, and the user side API becomes so much better than a C world would offer. You'll end up making less mistakes, creating more re-usable software, having stronger type and memory safety, and your end users will thank you. 
Yeah, you're completely right. I fixed most (maybe all) of those issues. I honestly haven't tested extra nearly enough, that's on the Todo list. Thank you :)
I'm not sure about other places but we use C11.
Often the most important thing is the shit and outdated version of a "C compiler" that your platform provides you with. If you're lucky and you're targeting linux on arm or something you can use GCC and therefore C-whatever. As a counter example: I haven't used Microchip in ages, but I think they're still stuck on C89 (with the addition of many, many bugs)
I'm not sure about "mostly" but I typically use C99, since it's supported by the xc8 compiler for 8-bit PIC microcontrollers. Nearly all other embedded platforms I've used support a wide range of compilers, such as GCC, and support C99 and newer. So I don't see why C89 would be mostly used still for embedded systems. I tend to target C99 personally because I love using stdint.h and declaring variables in a for loop declaration.
As I mentioned in my comment earlier, xc8 (compiler for 8-bit PIC) now supports C99 so I never have to use C89 ever on embedded systems anymore. :)
I've always had a negative view of using C++ on embedded systems, but this was an excellent and educational response. Thanks.
I don't know why you're being downvoted. For embedded systems like 8-bit platforms, I regularly check the output assembly. I rewrite my C code that isn't optimised well by the limited optimisation capabilities of the free xc8 compiler. Maybe embedded platforms today includes powerful ARM cores (like the Raspberry Pi) where checking the output assembly isn't typically done anymore?
Ah, ok. clang and gcc versions what I've used have been satisfied with just if ((x = y)).
I agree with you on the last point. Stdint is so great because it removes the ambiguity out of your code which is especially useful when you're targeting such precise machines. Even if you know the integer widths for that architecture. 
I'd like to note you don't need to implement result variable in addNumbers function. You can calculate discriminant right in return statement. This code use a bit less memory and a bit faster. float addNumbers(float a,float b,float c) // function definition { return (b*b) - (4*a*c); // return statement } &amp;#x200B;
new eyeglasses :I
 char fetch_buf[MAX_LINES] // max line count == max line length seems strange fetch_buf[++i] = '\0' // one byte gets skipped pt = (char *) malloc(MAX_LINES) // no need to cast. less wasted space by using i as size free(pt) // makes no sense, you want to keep the string in the array ?
Shouldn't be downvoted. I do this too. For 8 bit Atmel chips, I cannot trust the compiler to get certain bits of alignment correct when using other optimizations. The people downvoting this might be used to targeting 32 bit "embedded" architectures, but for 16 and 8 bit, all bets are off for standards, and undefined behavior that is a safe default on 32 and 64 bit architectures truly becomes undefined.
I haven't had to look at the assembly for a long time; only time I did was Hitech C for a Z80 embedded device where the compiler had a raft of awful bugs 
&gt; undefined behavior that is a safe default A problem that can be avoided by not relying on undefined behaviour in the first place
Another unfair downvote. Some embedded chips have excellent C++ compilers. Though I usually prefer C, sometimes C++ is a better solution. It's not like you're going to be doing dynamic casting on an 8 bit uController... (because it's impossible).
Can you comment on binary sizes? In my experience doing similar tasks in C++ results in a larger binary. (I haven't tried on my current platform though because it doesn't have a decent C++ compiler available; gcc dropped support for OABI ages ago) 
Ask in /r/csharp.
Nothing wrong with it. It just doesn't address the issues that C has -- it's as easy to write bugs in Objective-C as it is in C. 
&gt; That should be undefined behavior, No. It's passing an `int*` not `int`. int *vec = NULL; for (int i = 0; i &lt; 10; i++) { vector_push_back(vec, i); } int *current_vec = vec; vector_erase(vec++, 0); assert(current_vec + 1 == vec); This code is ok, and the assertion matches my expectation. But it doesn't work that way. And sadly, imho, there's no way for you to evaluate `vector` only once in `vector_erase` due to the lack of something like `decltype` in C. Maybe there is something similar that I don't know of? Anyway, the assertion fails and that's a problem. That's why I chose a different approach for my vector implementation that works without type information and just element sizes in normal functions and specific macros that take the type for faster operations.
Awesome explanation thanks for your time :-)
Of course. I've also honed my basic methods of writing code to minimize them (akin to the way the linux kernel is written). Mostly though I'm responsible for maintaining a bunch of spaghetti code someone else wrote that has threading issues that static analysis will never find. But that's not germane to my point. The language _itself_ should make writing bugs difficult. 
It's a misleading response but, yes, it is a response. Anything stating that a language, bada bing bada boom, does things automatically for you should always be looked at with a jaundiced eye.
Which chips have a C++ compiler built into them?
thanks :-), and good luck for your project. 
I wish. I've spent 3 years knocking myself against Apple's scant API documentation. I know how to write code, but audio units (AUv3) are complex beasts and it's difficult to write one that actually loads in Logic (even though it validates and runs in the sample app). The most crucual Apple documentation is actually a WWDC video, I think from 2016. There's "Digital Signal Processing for Audio Applications" by Anton Kavenov, but it's in Java (blechhh). Also the algorithm for chorusing is wrong (well, simplistic and I'm sure not how the big boys do it). There are great general DSP books too. Richard Lyons' "Understanding Digital Signal Processing" is my favorite, but (I probably have 5 or 6) each comes at it slightly differently so it's good to have multiple. And once you know what you're looking for you can usually do a web search and find something similar (often code).
Even less powerful ARM cores have excellent compilers which will do a better job than you can do by hand, except in a few very specific cases.
&gt; 6. Writing these libraries and tools makes you C why other languages have certain features and why they where written. You suddenly find your self having a X-ray vision into other programming languages which reduces the time to learn them. This is the reason why i want to learn C
This is C++
C++ is off topic in this subreddit. Please post C++ questions to /r/cpp_questions instead.
C# is off topic in this subreddit. Please post C# questions to /r/csharp instead.
I can use [errno](https://stackoverflow.com/questions/9856822/should-i-set-errno) for the integer part of the error code which is pretty powerful.
sorry
sorry
In embedded automotive industry we use primerly MISRA 2014 c and c++ standard.
Start with C11 and downgrade when you actually need to use a compiler that doesn’t support it.
I think it's that embedded systems is a broad term and the original statement could be seen as over generalizing. 
Also, isn't it a convention to install files to /usr/local/... when Makefiles are involved? 
&gt; Why the "sigh"? Sometimes, I'm no good in predicting how I'm perceived. At the time I wrote that "Sigh", I was remembering about a painful time I was optimizing for memory where it didn't go so well. I don't know how other people do it, but when I try to optimize for memory, I need to do some crazy things that I only know how to do in C. &amp;#x200B; &gt;From an outsider's perspective, it sounds like you use C because it's the best tool for the job for most (all?) of the projects you work on. If my assumption is right, then no need to hate yourself for that! &amp;#x200B; I came out darker than I indented. I love C even if I sometimes love to complain about it being too hard on me sometimes. . But thank you kind stranger! I take any positivity I can get my hands on :) 
I called free() cause I used malloc() to allocate space for what I thought of reserving space for the string to be in place in the array of pointers. &amp;#x200B; I will recompile and see what I get with further feedback. &amp;#x200B;
* fetch\_buf\[MAX\_LINES\] for an array to holder user input * fetch\_buf\[++i\], I overlooked this. * yes, I would want to the string in the array. &amp;#x200B;
C99 is as far as misra allows, 
MISRA 2014 only allows for c99.
I ment that the maximum length of a any line, and the number of lines to allow, are different things. Say, upto 1'000 lines, each 80 bytes or less.
thanks. I know about the process of running a debug. What I cant do is figure out how to set up clion for a simple project consisting of a couple .c and a .h file. I feel like this should be pretty straight forward.
It’s a good point there. I never think about preinstalled. I always think it’s trying to invent everything from the ground up. 
as long as you're someone who upvotes sane defaults, I applaud you.
For non-critical/recoverable runtime errors, I simply `fprint` to `stderr` and redirect `stderr` to a file. For anything critical, `assert` is great for pinpointing the root cause of the error. Is this too simplistic of a solution? What types of use cases are you trying to solve?
This is what I was talking about. Casey Muratori is the guy that's making handmade hero. Here is the playlist of the first videos he did where he goes over C.handmade hero [https://www.youtube.com/playlist?list=PLEMXAbCVnmY6RverunClc\_DMLNDd3ASRp](https://www.youtube.com/playlist?list=PLEMXAbCVnmY6RverunClc_DMLNDd3ASRp)
I don't use CLion myself (I use emacs with c-mode and gdb) but there is a lot of help online (and inside the IDE I assume), https://www.jetbrains.com/help/clion/creating-new-project-from-scratch.html
I'm very interested to hear your thoughts on the *analyzability* of C. For example: - What aspects of the language made creating a sound static analyzer difficult? - Conversely, are there any language changes or extensions that would make the task easier? - Are there any language changes or extensions that would facilitate expanding the scope of the analysis? I know that IKOS works on the llvm IR and not the source itself, so I guess another way of asking is: *what changes to the source language can be done to facilitate generating an IR that's more suitable for static analysis*?
Because `*root-&gt;data` actually means `*(root-&gt;data)`, and `root-&gt;data` is an `int`, not a pointer. I understand why you think the `*` operator is necessary, but it isn't, because it is implied by the `-&gt;` operator: `root-&gt;data` is shorthand for `(*root).data`.
 Aha I see thank you very much tho :)
shouldn't that be: if(value &lt; root-&gt;data){ &amp;#x200B;
"Gnu/Linux is *literally* the operating system"
Microchip bought the hi-tech compiler company in the late 2k's. Their tool chain is basically best-in-class for non-arm products.
Did you even read what I wrote? I never claimed anything is automatic. I talk about "enabling" and "library writer's complexity goes up"... In no way did I imply this was a free lunch. In fact its quite the opposite. I stated that c++ will increase your difficulty but improve your user's experience (if you know what you are doing). 
We use C99.
I guess that depends on what you are using. If you use a lot of streams and std::string (or a few other things), the binaries will bloat. But c++ is actively getting better about that. See [std::format](https://github.com/fmtlib/fmt). Note that std::format has been accepted into the language, is very fast, creates small binaries, and maintains type safety unlike std::printf, std::fprintf, std::sprintf, std::snprintf etc... Binary sizes are a problem, but the reason they are a problem is typically due to a few bloaty libraries. If you watch the [jason turner movie about creating a commodore 64 game using c++17](https://youtu.be/zBkNBP00wJE), you can really see how c++ enables someone to control the compiled output very deeply. Sometimes, it is very hard to do, but someone who knows their c++ very well can do it fairly easily. 
https://imgur.com/a/gBRbo6Y here is the text from my makelists file. cmake_minimum_required(VERSION 3.13) project(untitled1 C) set(CMAKE_C_STANDARD 99) set(SOURCE_FILES main.c memory.c cache.c) set(HEADER_FILES memory.h cache.h) option(DO_HW "Add HW files" ON) configure_file("./memory.h" "./cache.h") add_executable(testIT ${SOURCE_FILES} ${HEADER_FILES}) target_link_libraries(testIT m) I include the argument "memory.txt" in the configuration setting because the project reads from that file. I click run and I can tell it compiles my code because it prints out some warnings regarding it. I get no actual output from my project code but it prints out the Hello World from the main.c file generated when you create a new project, *even though there is no reference to that file in my makelists file.* I add a breakpoint at the top of my mainfunction in my, it doesnt even get triggered when i run debug.
The best practice I've seen is to have `make install` default to installing into /usr/local but also allow the user to specify a different prefix
It has become a general practice around where I work to have the final parameter to a function be a GError. The message portion of the GError may be popped up to the user in another application across a network. I have used a FILE\* parameter to fulfill error reporting the past. What makes it really cool are the [GNU String Streams](https://www.gnu.org/software/libc/manual/html_node/String-Streams.html) I guess rolling all this myself will be my best bet. I probably will still need to make some functions to make things easier for people. 
This subreddit is for the C programming language, not C#. Try r/csharp r/learncsharp r/dotnet r/powershell and maybe other subreddits (look into their sidebars).
Ah apologies and much appreciated
Yes but you’re copying the data into the pointer, setting it in the array then freeing it... so why ever set it in the first place? When you come back to it later it will be NULL.
Same here. When I saw that we had a relatively recent version of GCC, I immediately bumped to C11 in my project for three reasons. One being we now get aliagnas instead of having to manually use compiler attributes. Secondly is anonymous structs and unions which can be used to increase readability via decreased verbosity in areas where the use is constrained to a few short lines. And here is the biggest reason, static asserts. I do not want to use ugly multi line preprocessor code to ensure various characteristics about buffer sizes and whatntot. Instead, [static asserts](http://www.robertgamble.net/2012/01/c11-static-assertions.html) decrease the LOC down to 1 from three for a minimal compile time assertion. Regarding going to C99 instead of C89, to get rid of the nonsense that is not being able to declare a variable in a for loop. Why on earth should a variable used for indexing be kept in scope larger than needed?! That is very similar to the entire point of not putting everything in global scope. 
Thank you! That fixed it. But I've immediately ran into another problem. My insertion method looks like this: Node* insertNode(Node *root, int value) { if(root == NULL){ struct Node node; node.data = value; node.left = NULL; node.right = NULL; root = &amp;node; } else { if(value &lt; root-&gt;data){ root-&gt;left = insertNode(root-&gt;left, value); } else { root-&gt;right = insertNode(root-&gt;right, value); } } return root; } If I do: root=insertNode(root, 14); root=insertNode(root, 12); printf("%d", root-&gt;data); printf("%d", root-&gt;left-&gt;data); Then it returns 14 and 12, as you'd expect. But if I do: root=insertNode(root, 14); root=insertNode(root, 12); root = insertNode(root,17); printf("%d", root-&gt;data); printf("%d", root-&gt;left-&gt;data); Then it returns 14 and 17. Any idea why 17 is overwriting the left node rather than going right?
This really varies. I glance at the assembly for only critical code paths which I know must be extremely quick or not use the stack. But this is extremely rare, maybe at most once or twice a month. Most of my time is spent handling logic level bugs when doing low level drivers in Linux and MCU's. It's much cheaper for my company to just get a beefy Cortex M4 instead of some 8085 and having me spend so much time optimizing. I would understand if the volume is in the millions and it's a very low margin device, but thankfully I am in a company that has higher margins and can afford to spend $2 on a CortexM IC instead of $0.50 on some 8085.
If it helps, here is the contents of the arduino .ino side: #include &lt;AESLib.h&gt; #include &lt;SPI.h&gt; #include &lt;Adafruit_Sensor.h&gt; #include &lt;DHT.h&gt; #include "DHT.h" #include &lt;RH_ASK.h&gt; #define DHTPIN 2 #define DHTTYPE DHT22 // DHT 22 (AM2302), AM2321 DHT dht(DHTPIN, DHTTYPE); RH_ASK driver; bool takeSensorReading = true; float h; float t; String str_humid; String str_temp; String str_out; uint8_t key[] = {0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31}; char data[32]; char endChar[1] = "Ʊ"; void setup() { driver.init(); dht.begin(); Serial.begin(9600); } void loop() { // Wait a few seconds between measurements. delay(5000); int messagelength = sizeof(str_out); byte plain[messagelength]; if (takeSensorReading) { h = dht.readHumidity(); t = dht.readTemperature(); str_humid = String(h, 2); str_temp = String(t, 2); str_out = str_humid + "," + str_temp + ",0001"; str_out.toCharArray(data, 32); aes256_enc_single(key, data); } else { /*aes256_dec_single(key, data);*/ Serial.println(data); driver.send(strcat( (uint8_t *)data, endChar ), 33); driver.waitPacketSent(); } takeSensorReading = !takeSensorReading; } &amp;#x200B;
Yes, you are getting space for the string, but then immediately *unreserving* it by freeing it. You don't want to call `free()` until you're done with the memory you allocated. In this case, you're never done, so don't free it. Hey, I'm wondering if you are working in a terminal window using macOS or Linux? If so, would you be interested in trying out a text editor I am developing and tell me what you think of it? In exchange, I can help you learn C -- do mentoring. Send me a direct message if you're interested and we can discuss it.
As a general rule, I always program to the oldest standard that has the features I need. That maximizes the portability of my software.
Sorry I wasn't clear what parts of your post I was addressing, and got a bit distracted with questions of why the Standard classified the behavior as "Undefined". It is certainly true that when the Standard classifies something as Undefined Behavior, that would in no way suggest that a conforming implementations can't define the behavior. It does, however, sometimes add an annoying wrinkle: if an implementation specifies the behavior of a general class of behaviors that the Standard doesn't define (e.g. reading and writing arbitrary addresses), it may be unclear when and whether such a definition takes "priority" over a statement by the Standard that some particular action within that class invokes Undefined Behavior. Unfortunately, implementation documentation seldom makes clear whether general behavioral specifications apply *except* in cases which the Standard explicitly characterizes as UB, or *even* in such cases. The Standard does not recognize any category of behavior which an implementation defines without any obligation to do so. When the Standard characterizes the behavior of some action as "implementation-defined", it requires that all implementations document it. *The fact that an implementation happens to define a behavior does not make it "implementation-defined" in the sense that the Standard uses the term*. On thing that would help clarify many parts of the Standard and how they interact with implementations' documentation would be to recognize certain aspects of behavior which implementations may or may not treat as "observable" side-effects that need to be maintained, preferably defining standard macros or intrinsics to indicate such choices. If an aspect of behavior is characterizes as "not observable", that would not forbid programs--even strictly conforming ones--from e.g. branching based upon it, but implementations would be under no general obligation to make aspects of behavior that aren't observable behave consistently. If an execution environment documents the effects of loads and stores at every possible address, and if an implementation for that environment treats all aspects of every load and store as an observable side-effect (the Standard recognizes the possibility that implementations might do this, but fails to provide any means by which they can promise to do so) those two facts together should fully define the behavior of dereferencing a null pointer. If an implementation were to instead specify that it only considers timing of object accesses observable in certain situations, and if null pointer accesses are defined as triggering a trap in the execution environment, then some aspects of accesses' behavior wouldn't be defined, but behavior of the program as a whole would be defined if a programmer was prepared to accommodate any uncertainty in those aspects. Many programs will be called upon to process data which might be valid, erroneous, or even maliciously constructed, subject to two requirements: (1) When given valid data, they will be required to produce correct output; (2) When given erroneous or maliciously-constructed data, they will generally not be required to produce particularly *meaningful* output (if the input is meaningless, producing meaningful output might be impossible) but will generally still be required to refrain from doing anything particularly harmful. A behavioral model where all forms of UB completely jump the rails may improve the speed of programs that only have to meet the first requirement, but an implementation where most of them yield in loosely-defined behaviors would reduce the cost of meeting both requirements. 
Use the subreddit r/cpp , this subreddit is only for C programming
This subreddit is about programming in C. C# is off topic. Please post in /r/csharp instead.
C++ is off topic in this subreddit. Please post C++ questions to /r/cpp_questions instead.
you should dynamically create new root ... if(root == NULL){ root = malloc(sizeof(Node)); root-&gt;data = value; root-&gt;left = NULL; root-&gt;right = NULL; }... &amp;#x200B;
root = &amp;node; You store the address of node into the root. But the node variable placed on the stack and on exit of the scope(at the moment the scope is if-block) it will be destroy. Therefore, the value on root is unknown value. I hope it is readable. Sorry for my very bad english. I am beginner in English, but I already love English.
&gt;The language itself should make writing bugs difficult. That's a perfectly valid use-case, but it's orthogonal to the main purpose of standard C - to run everywhere and be very permissive. It would be great if there was a compiler that implemented language extensions giving a Rust-like level of memory safety (for example), but nobody has done that yet.
Are you decrypting with the same key you encrypted with?
Different ARM cores support different parts of the instruction set, and writing in C avoids the need to try to remember which instructions a particular processor supports. When targeting Cortex-M0 or Cortex-M3, I've generally found that of the three scenarios: 1. The compiler generates better code than I could have done. 2. The compiler generates code which is about the same as I would have done, and is probably optimal. 3. The compiler generates code that is not as good as what I could have done. the most common by far is #2. Most of the stuff I do isn't terribly complicated, so the biggest requirement for efficiency is simply to avoid redundant loads. Hand-written assembly for the ARM, especially for small projects, could often achieve better performance than would be possible on a compiler limited to using the ARM ABI, but unfortunately tools aren't set up for that. For example, hand-written assembly code may know that a group of objects [e.g. x, y, dx, and dy] will all be near each other, and process `x+=dx; y+=dy;` using code like: ldr r0,=x ldr r1,[r0] ldr r2,[r0,#(dx-x)] adr r1,r1,r2 str r1,[r0] ldr r1,[r0,#(y-x)] ldr r2,[r0,(dy-x)] adr r1,r1,r2 str r1,[r0,#(y-x)] This code won't work unless the displacement between those values is small enough to be expressed as an immediate offset on the target platform, but code elsewhere may treat `x`, `y`, `dx`, and `dy` as separate values when convenient. One could help a C compiler achieve similar results by throwing everything into a `struct`, but that would require either having the groupings baked into all users of the objects whether or not they exploited them, or else using abusing the preprocessor to replace `someName` with `someStruct.someName`, which is--to say the least--rather ugly. Note that in this particular example, code might benefit from `ldm`, but that would silently break if the organization of `x`, `y`, `dx`, and `dy` was not exactly as expected. The code above will tolerate any arrangement of those objects which fit the platform's displacement-addressing form.
The Committee made no attempt to mandate all the features that would be necessary for all tasks, but instead expected that implementations that are intended for various tasks would support whatever features would be needed to accomplish those tasks *without regard for whether or not the Standard required them to do so*. The range of tasks that could be done using only behaviors defined by the Standard is much smaller than the range of tasks that could be accomplished with the aid of what the Committee called "popular extensions" in the Rationale's discussion of Undefined Behavior (most notably those that would involve processing some action in a documented fashion characteristic of the environment).
Did you have a specific question? Are you just asking in general how the encrypt and decrypt functions should be used? The link you gave has the usage instructions. Each function takes the key and data and encrypts or decrypts the data using the key. It's a block cipher so it operates on fixed-size blocks, in this case 128 bits (16 bytes). To get this to work right you have to provide padding to make sure you're encrypting a multiple of 16 bytes. There are accepted ways to do the padding (not just putting zeros on the end) and picking the right method is important if you want it to interoperate with anything else. For general background on that stuff, I'd strongly recommend picking up a copy of Applied Cryptography by Bruce Schneier. You also have to decide what cipher mode you're going to use. If you're just calling aes128\_enc\_single() with each block of plaintext separately, that's electronic codebook (ECB) mode. It's not as secure because each block is entirely separate and an attacker can do things like substitute a block from one ciphertext into another and change the contents even if they don't know what's in it. Cipher block chaining (CBC) is more secure, and you do that by starting with an initialization vector (IV) and then mixing each plaintext block with the result of the last encryption. Again, Applied Cryptography is a great resource for learning this stuff. For now, stick to ECB until you're sure you know what you're doing. Ditch all of your file reading stuff for the moment and do some tests with fixed data, already padded to the appropriate length, to make sure your encrypt and decrypt functions work as expected. Once you've got that, then try it with file input and padding generation.
Both C and C++ suffer from a couple of related problems: 1. While the Standards explicitly recognize that some implementations will behave in documented ways that are characteristic of the environment in various situations where the Standard imposes no requirements, they provide no means by which a programmer can annotate some action to say "In this execution environment, there is one particularly-"natural" way of performing this action, the programmer knows how the environment will process that operations involved, and such behavior is necessary for the program to meet requirements". Instead, they rely upon implementations intended for various tasks to support whatever operations may be needed to accomplish those tasks, without regard for whether the Standard requires them to do so. 2. The authors of the Standards generally make little effort to characterize situations between those where all implementations are required to document a consistent behavior, and those where all implementations are allowed to jump the rails without having to uphold any behavioral guarantees whatsoever. Most notably, the Standards seem to avoid recognizing aspects of behavior that are "unobservable", with the semantics that the Standard would neither forbid observations that might influence program behavior, nor mandate that observations be consistent. Instead, the Standards generally say that any attempt to observe something that isn't observable invokes UB. A prime example of this principle is the treatment of some destructors that may or may not get run before the program terminates. The Standard makes clear that if such destructors have no observable effects, they will behave as no-ops, but it then says that if they have any observable effects they invoke Undefined Behavior, thus making it impossible to observe whether or not they execute. While making observable actions invoke UB would waive any obligation the implementation would otherwise have to either ensure they execute or ensure they don't, it's vastly overkill for that purpose. It would have been much better to simply say that such destructors may either run or not as the system sees fit, without regard for any effects it might have on program behavior.
Neither is K&amp;R's The C Programming Language, to be fair
A good exemplar will often be more useful than a poorly-written Standard. If a language has previously been defined by a few widely-recognized exemplars, a Standard which fails to accurately describe their behavior may be worse than useless. 
After trying to get an old-ish version of mingw setup for about half the day you just saved my ass! Why is the suggestion of mysys2 not the top google hit?!
Hi madsci, &amp;#x200B; Thanks very much for your reply. I'll see if I can grab a copy of Applied Cryptography :) As for my question, I think it's a bit more fundamental than using the encrypt and decrypt functions. I was wondering how I might import the AVR-Crypto-Lib library into my C program. I think I might be missing a bit of assumed knowledge Thanks again
In cases where some members of the Committee thought a construct should be forbidden, but some implementations supported it and some programs relied upon such support, the normal "compromise" was to make the construct a constraint violation, but not require conforming implementations to do anything about that provided that they issued at least one diagnostic. The authors of `gcc` used the term `-pedantic` to enable warnings about useful constructs that were forbidden, but in the gcc authors' opinion, shouldn't have been. Personally, I think a better attitude to the Standard would have been shown by making `-pedantic` unconditionally output "Warning: this program might contain violations of silly constraints". While the Standard has helped promote uniformity with regard to things like variable-argument handling, the reverence in which some compiler writers treat it has served to degrade the language to match the weakest dialects and stifle attempts to improve it. 
I am confused by the example. The example is pass by value not pointer. If int *y = &amp;x; behavior would be different and I think its closer to what I originally had. I think this would be closer to what actually happens in the code: #include &lt;stdio.h&gt; #include &lt;stdlib.h&gt; #include &lt;stddef.h&gt; void what(int *j){ *j=4; } int main(void){ int *i; int r =5; i=&amp;r; what(i); printf("%d\n",*i); } however doing this does not work: #include &lt;stdio.h&gt; #include &lt;stdlib.h&gt; #include &lt;stddef.h&gt; void what(int *j){ j=NULL; } int main(void){ int *i; int r =5; i=&amp;r; printf("%p\n",i); what(i); printf("%p\n",i); } If I understand it correctly this does not reassign the i pointer because j=NULL is considered like the example you shown. if so is there a way to alter the global pointer passed to what(int *j)? 
It's a good easy language that doesn't make me want to jump off a cliff like java or rust or whatever.
There are some hardware platforms where it would be impractical to have a "pointer to any pointer" type, but on most platforms such a type would be both practical and useful. Unfortunately, even on platforms where such a construct would be practical and useful, compiler writers may use the so-called "strict aliasing rule" as an excuse to prevent programmers from doing the things they need to do, even in situations where compilers should have no difficulty seeing the relationships among objects. 
&gt; Why on earth should a variable used for indexing be kept in scope larger than needed?! That is very similar to the entire point of not putting everything in global scope. 100% agree. Of late, on the rare occasion I find myself working in a C89 codebase, I will frequently introduce blocks specifically in order to limit variable scope. IOW: /* ... */ { int i; for (i = 0; i &lt; 10; ++i) { /* ... */ } } Not as good as being able to do it the C99/C11/C++ way, of course: for (int i = 0; i &lt; 10; ++i) { // ... } 
But once you have the `static_assert` macro, usage looks like: static_assert(sizeof(foo) &lt;= 4); What's the downside?
You are declaring `struct Node` as a local variable within your function, then returning a pointer to it. That invokes undefined behavior. You must allocate the new node using `malloc()` or `calloc()`.
Ok, you've already got the Arduino side going and you need it to work on the Raspberry Pi? I wouldn't mess with the AVR library - it's presumable for the AVR platform, which the Pi is not. It might have pure C functions but I see some assembly language modules and those definitely aren't going to work. AES is a standard cipher. I'm sure you can find a dozen generic implementations in C. [This](https://github.com/kokke/tiny-AES-c) is the first one that comes up on Google. To use a source library in C, you're typically going to include a header file in your source, and then your build system needs to know what other C files to compile and link in. If you have a makefile, you'll put them there. If you're using a managed make system it might pick them up as soon as you put them in the right place. Or you can just invoke the C compiler and linker from the command line if you want to do it all yourself. Are you using any particular development environment? The rest of my advice still stands - you're going to want to start simple with fixed blocks, ideally something that you know the expected result for. [This](https://csrc.nist.gov/csrc/media/publications/fips/197/final/documents/fips-197.pdf) is the official standard and appendix C contains example vectors. You can find others. Encrypting in aes-128, a key of 2b7e151628aed2a6abf7158809cf4f3c with plaintext 6bc1bee22e409f96e93d7e117393172a should give you ciphertext 3ad77bb40d7a3660a89ecaf32466ef97 . Plug those in and make sure you've got the right answer on each end. I fought with some XXTEA problems because the implementation I used didn't take into account endianness and I was going between little endian and big endian systems.
Thanks so much madsci, &amp;#x200B; Yes I have the Arduino side sorted, or at least sorted enough for now. I'll have a read through the standard and check out the library that you linked to and see if I can get something working. &amp;#x200B; Thanks again
Although there isn't a formal name for it, a large fraction of embedded projects (likely a majority depending upon how one counts) use a dialect that isn't described by C89, but can be derived from it using a simple recipe: 1. If one part of the Standard describes the behavior of some actions, and another part says that an overlapping category of actions invokes Undefined Behavior, either give priority to the first part or document a compelling reason for doing otherwise. 2. If an action would logically map to an operation on the target platform, and the target platform documents the behavior of that operation in more cases than mandated by the Standard, either process the action using the platform's characteristic behavior or document a compelling reason for doing otherwise. According to the Committee, the Spirit of C includes the principles "Trust the programmer" and "Don't prevent the programmer from doing what needs to be done". A corollary would seem to be "Trust the programmer to know what needs to be done", implying that compiler writers who want to know what needs to be done should listen to programmers rather than the Committee. Unfortunately, C seems to be diverging into two incompatible categories of dialects: those which uphold the above principles in a way which blocks what should *generally* be useful optimizations, or those which aggressively perform optimizations in ways that make them unsuitable for most tasks beyond those expressly accommodated by the Standard. Both needs could be accommodated in a single dialect if the Standard were to include constructs to explicitly indicate when precise semantics were or were not required. When the Standard was written, however, there were two main kinds of implementations: those which were designed for low-level programming and would support it even without such constructs [and which thus had no need for such constructs], and those which weren't designed for low-level programming and wouldn't really be suitable even with such constructs [and thus had no need for such constructs]. Since no implementations would have benefited from such constructs in 1989, there was no need to include them, and even in 1999 there still wasn't much need for them. By 2011 the need for them would have been apparent, but the fact that the language had survived for ten years without them made it impossible to overcome inertia so as to add them. C is thus in an awkward situation where no officially recognized standards for the language defines everything necessary to accomplish most of the tasks that programmers need to do, the only reason anything works is respect for tradition, and the level of respect for traditions is eroding without anything new being added to replace them.
You can do something like void what(int **jp) { int *j = *jp; ... *jp = NULL; } and call `what(&amp;i)`. But that’ll only enable you to affect `i`. You’re not going to be able to clear out all possible pointers to the same thing without implementing something along the lines of a garbage collector, which is …decidedly nontrivial, to put it lightly.
IMO the strict-aliasing stuff is a mix of unfortunate and fortunate, and some of the compiler leaps are simply because aliasing analysis can blow up in scale very easily once you have a few pointers or loops involved. If there were stricter rules on the language-as-text to prevent more accidental aliasing violations, or if there were more predictable boundaries like the compiler not being able to see around compilation unit barriers (i.e., limiting LTO sorts of things), or if there were ISO-standardized fences or treat-it-as-a-damn-address-per-se builtins of some sort, I’d have less of a problem with it. Alternatively, a couple of new modifiers for disabling/enabling possible aliasing with specific other pointers might be more generally useful than `restrict` has been. I guess for the out-pointer stuff specifically one could theoretically add a callback arg, so the out-arg type would be `const void *` and the callback could assign whatever underlying pointer properly… but the throwaway variable approach would probably be much cleaner and lower-overhead anyway.
I mean, it might not be your job, but someone is checking the assembly.
There are a lot of other ways to screw it up other than just making sure you're using registers. If you have a long variable dependency chain inside a nested loop that the compiler doesn't unroll that calls a frame function, for example, that will be a significant slow down. You don't care if it runs once, but if it's a hot code path, that's also going to be an issue.
Glad to hear it! That means it went from made-in-a-freshman-class to best-in-class.
IMHO, the rules could have been written most clearly and effectively if they had specified that the exact timing of when reads and writes are performed is not *generally* considered observable, but then specified certain cases where it is(*). If the Standard were to further specify that when a program's behavior would be affected by some aspect of behavior that is not observable, the implementation may act in any way consistent with observable aspects of behavior, and that each such situation may be acted upon independently, that would fix a lot of situations where anything resembling the current rules would require that either programmers or compilers generate sub-optimal code. (*) Two references alias if during some time when both references are active, the interleaving of operations using those references could affect program behavior. As a simple example of the latter principle, consider something like: struct foo {unsigned q; unsigned char x[100];} a,b,c; void test(int *locations) { struct foo temp; temp.q = 123; for (int i=0; i&lt;50; i++) temp.x[locations[i]] = i; a=temp; b=temp; c=b; } This example is based on the fact that automatic objects initially hold Indeterminate Value, but the same principles could apply if `temp` were a heap object that had previously bee written with some incompatible type. If the Standard were to say that the values of `temp` that haven't been written yet are not observable, but copies of such values are, then neither `a=temp;` nor `b=temp`; would represent an "observation" of any element of of `temp.x` that hadn't been written. Instead, any element of `a.x` or `b.x` whose corresponding value of `temp.x` hadn't been written would be independently set to an Unspecified value between 0 and `UCHAR_MAX`. The statement `c=b;`, however, would compel the compiler to ensure that all values in `c` match those in `b`. It could, at its leisure, rewrite the function as: struct foo {unsigned q; unsigned char x[100];} a,b,c; void test(int *locations) { a.q=123; c.q=123; for (int i=0; i&lt;50; i++) { int temp = locations[i]; a.x[temp] = i; c.x[temp] = i; } b=c; } It would be perfectly legitimate for some attempted observations of non-written values in `temp.x` to behave as though their contents happened to matched those in `c.x[]`, and the behavior of the above function would be consistent with that. &gt; or if there were ISO-standardized fences or treat-it-as-a-damn-address-per-se builtins of some sort, I’d have less of a problem with it. The problem is that such things weren't needed in days when most implementations would either support the required semantics without regard for whether the Standard required them to do so, or would be used only in ways not requiring such support. Even it shouldn't be difficult for any compiler to support such a construct, I see no way of overcoming inertia given how long the language has (supposedly) gotten along just fine without it. If the authors of the Standard had wanted to describe the "strict aliasing rule" without going into great detail, btw, a simple rule would have been to say that a compiler need not recognize the possibility of aliasing between references that do not identify the same object, elements of the same array, or an array and elements thereof, but quality implementations should not use this rule as an excuse to needlessly violate the Spirit of C: "Do not prevent [or obstruct] the programmer from doing what needs to be done". Most of the cases where the gcc/clang interpretation of the "strict aliasing" rule causes trouble could have been handled with the last clause. 
FYI anything o4 and higher has no effect with vanilla gcc/clang, they are all equivalent to o3.
&gt; Why is the suggestion of mysys2 not the top google hit?! Maybe there's nobody who knows how to SEO and also uses MSYS2? :) 
And what happens when the exemplar is updated (as is inevitable when new features are added to the language) and existing behaviour changes? Perhaps by accident?
If exemplars are reproducible or can be archived, and programs specify that they are designed to work with the exemplars as they existed on some particular date, then they will work with present or future devices that work as the exemplars did on that date. The biggest advantages that specifications have over exemplars is that they can better indicate what characteristics of a device should be expected to be consistent, and which characteristics should be allowed to vary. If nut and bolt manufacturers design parts each based on a single exemplar of a nut and bolt assembly, and each wants to reduce the amount of mechanical slop compared with the exemplar, the new nut might fit perfectly with the bolt exemplar, and the new bolt might fit perfectly with the nut exemplar, but the new nut and bolt wouldn't fit with each other. A solution to that problem is to have separate exemplars for "loose nut" and "loose bolt", which are too loose to grip usefully, as well as "tight nut" and "tight bolt" which are too tight to fit, but then specify that a conforming nut must work with both "tight bolt" and "loose bolt" exemplars, and likewise a conforming bolt must work with both "tight nut" and "loose nut" exemplars. Any nut which works with both bolt exemplars would then mate usefully with any bolt that works with both nut exemplars. Unfortunately, if one judges tolerances using exemplars rather than specifications, one will then either have to make the exemplars very precisely, or else allow uncertainty in their construction to be counted against one's allowable tolerance. Still while good specifications can be much better than exemplars, bad specifications can be worse than useless. C is a great language which has been undermined by incomplete specifications which pretend to be complete. 
Thanks for the book recommendations. Do you not like Java because it is object oriented or something else? Some people have been taking to using Kotlin, which targets the JVM, since it has things like anonymous classes IIRC.
Not sure why I'm not into java. I loved it when it first came out. It might be that I just prefer newer shiny stuff (although I still think objective-c is great, and it's pretty old). I know little about kotlin, although I think I read it's similar to swift, which I do like. Beware of languages that make targeting memory precisely difficult (swift has this issue). When I need to write audio code on the Mac I fall back to objective-c. Swift just is not the tool for the job (IMHO of course).
C is explicit. More so than other languages. You have to be very deliberate when writing the code. This is great for embedded platforms, particularly in industries where failure comes with extreme consequence, such as aviation.
&gt; standard C &lt;with&gt; language extensions giving a Rust-like level of memory safety That's a good idea. I fund the redox project in a minor way. That's about all I do to contribute to solving this problem. 
&gt; targeting memory What do you mean by this? Bitwise operations and pointers?
This subreddit is for C programming, not C++
&gt;Still while good specifications can be much better than exemplars, bad specifications can be worse than useless. C is a great language which has been undermined by incomplete specifications which pretend to be complete. There were already various C implementations with wildly different behaviour before the first Standard , it would be hard to argue that the standard didn't have the positive effect of pulling them all closer.
His code looks like C to me. Well except "int&amp;, int&amp;" not sure I've seen that before....
I've never stuck to a standard in C/embedded development since it makes no sense. I've used IAR Workbench C11/C17 and it's related extensions the most though, to answer your question. C is not the greatest language for embedded dev since it lacks any capability to communicate with hardware without using specific intrinsics and compiler extensions.
&gt; bitwise operations and pointers Yes. If I want to operate on memory, say as an array of floats, what do I have to do? Using Swift as an example, (and it's been awhile so I may have the details wrong), pointers don't really exist. You have to recast references in closures into some data structures that allow you to traverse from one float to the next. It's quite cumbersome. 
Static keyword, structs and function pointers. Take a look at windows COM for an example
Sorry but, what is windows COM? 
Use structs...
&gt; function pointers. no
https://en.wikipedia.org/wiki/Component_Object_Model
Look at Linux 
Maybe check out ["Object-oriented Programming with ANSI-C"](https://www.cs.rit.edu/~ats/books/ooc.pdf) [PDF]?
I think it's a bit silly to use function pointers just to most closely mimic the semantics of OOP languages.
Go on, tell us what is wrong with them.
Function pointers are a necessary evil. Imagine an API that relies on Application to supply a save to hardware non volatile memory. In order to keep the interface clean you’re going to give a callback on the API a link to some SaveToNovRam routine over in Application. Otherwise you’ve got some critical dependencies on the independent side of the API. Also same thing with time. Add a callback that links to a system function to provide tick counts and such. No way I’d like to have those built into code that has no business touching my hardware when the goal is a portable feature.
you use structs with function pointers to get encapsulation. but you don't get inheritance without extra work (typically not worth the effort). the basic pattern is struct-&gt;function(struct, ...) where "struct" is your object, with pointers to methods, and first argument is the struct itself, which contains state.
Before doing anything below, try to find a C++ compiler for your desired platform. C++ began as a way to incorporate OOP into C, and it is still well-suited for that. ----- There isn't a clean way to simulate inheritance AFAIK, but you can simulate encapsulation by doing something like the FILE struct in C: The internal layout of FILE in C is generally quite different between implementations, and 'good' C code never makes use of these internals, and instead handles FILE objects only through pointers. You can do a similar thing with your types: Create a struct containing all of the internal data for your class, and make sure any users of your code never access its internals. Next, for any instance methods, incorporate the name of the struct into the method name, and make it take a pointer to the struct as a first argument. For example, if you wanted a class called `Stack` with methods `void push(void *item)` and `void *pop()`, you would convert them into a `struct Stack`, and separate methods `void Stack_push(struct Stack *this, void *item)` and `void *Stack_pop(struct Stack *this)`. Make sure to include a constructor `struct Stack *Stack_new()`, and destructor `void Stack_delete(struct Stack *this)` to perform clean-up. `myStack = new Stack();` becomes `myStack = Stack_new()`. `myStack.push(myItem);` becomes `Stack_push(myStack, myItem)`. `delete myStack;` becomes `Stack_delete(myStack)`.
There's no doubt about that, but I think there's a slight misunderstanding. By the way the first post was worded, I thought the claim was that there was something exclusive to C's language constructs which maps very efficiently to ARM architectures. I think he just meant it in the sense that most embedded microprocessors/computers use ARM, therefore C tends to be compiled to ARM on these devices.
You make a convincing argument.
Just wanted to add an opinion based on my experience... Background: embedded. Most work happened on linux, freebsd, vxworks and a little on a few custom operating systems. All code was in C. Last job as a programmer: we were building a system stack and some joker - er, um, the guy who was spending the money - said lets build it in oo style. And so I did! How? Extensive use of "void*" and function pointers. In fact, the only real data type in the entire code base was "void*"!! For the record, I did mention at the start of that project that it would be easier to "just" do it in C++, but people didn't want to listen. So, what would I suggest? It's 2019! I can assure you that c++ compilers are as good as, and in fact maybe better than, most c compilers. In fact, just about a year ago me and a few oldies decided to do a unscientific check on the state of compilers, and two popular open source compilers - gnu and clang - produced better code with c++ front ends than with c front ends. Once again, in 2019, if you need oop features in c ... learn and use c++. Don't waste your time with "attempting to write" oop code in c with a lot of unnecessary scaffolding. C is great for lean+mean embedded work. Let's leave it at that. Just a suggestion. HTH! 
One you will see in libraries a lot is the opaque struct pointer based API. It is a pretty good encapsulation technique. It general there will be an init function that takes a pass by reference pointer. It will allocate the object for you and assign your pointer to the allocated object. The API will then take the struct pointer as the first argument of most function calls. So like: typedef struct foo foo_t; int init(foo_t **); void do_the_biz(foo_t *, int some_arg); ... And so on.
Don’t C is not an OOP language. If you want OOP use C++. 
&gt; Did you not try simulate OOP? No. &gt; If not, how do you manage your programs? Instead of using classes (structs) responsible for _managing_ things, use functions responsible for _doing_ things. Try to think about the data you are dealing with and the requirements of your software, and then find out transformations need to be applied to that data to satisfy the requirements. If you provide more detail on what you're doing, I can try to give a concrete example.
And how are you going to make a dynamic dispatch without function pointers?
We learned C as a first language at our University and I think it's great. Especially if you want to have a good foundation. As for resources I don't know for sure. Someone else might be better to help you.
I think that C is a fine language to start, but it will be more challenging than Python. The Python language abstracts you from a lot of detail that C will not. This does make it easier for beginners. It also is probably easier to run a Python program than it is a C program, which is a one time cost that you'll have to deal with. K&amp;R is a fine place to start. Read until you have a question, and then start trying to find out the answer for the question. It will be slow, but you will learn a lot through it. Good luck!
First and foremost ask yourself what you hope to accomplish by learning how to code. If you intend to get into web development, maybe JavaScript is a good place to start. If you intend to get into data science, Python is pretty good. If you want to eventually get into systems programming, you'll need to learn C... C is a language that forces you to understand how you are using memory, and forces you to be responsible for what you do with that memory. Java, Python, JavaScript (and other higher-level object-oriented languages) hide those details away from the programmer, and then they take care of everything for you behind the scenes. For this reason, many people recommend against starting with C so you can learn some of the concepts of programming without having to worry about the nitty-gritty details of memory management. Ultimately, once you learn one language, you'll find that the many of the concepts you've learned will be applicable when you learn other languages. For example, pretty much all languages will have the concepts of variable assignment, conditional logic, loops, functions etc., but the syntax and details of how those languages apply those concepts is going to differ from language to language. If you've never coded before, maybe start with a free course like codecademy. Start with the free JavaScript or Python track, and see how you like it. If you find it to be pretty easy and you enjoy the content, then maybe put some serious effort into finding a course that will offer you more guidance and more depth.
This sub is for C, it is *not* for C++.
Thanks for clarifying it. 
Yep, the default should be /usr/local. Ideally, this installation destination should be configurable. Then you might want to use autoconf/automake to generate configure.sh. 
C might be a good first choice (certainly better than C++). It’s a small and simple language. Yes, you have to allocate memory on your own, but that’s just a more explicit form of programming. In garbage collected languages like Python and JS, you’d need to understand the invisible things that happen implicitly, like the fact that primitive data types are passed by value and object instances are passed by reference., which is crucial to know when you want to mutate a parameter or copy an object. It gets more interesting with arrays and nested objects. This probably doesn’t make sense right now, but it will later on. C gives you a foundation to understand what higher level languages do automatically.
What is your thought about Stensal SDK ([https://stensal.com](https://stensal.com))? It builds quasi-memory-safe executables from C/C++ source code. It has extensions to deal with very C idiomatic usages and make them memory safer. &amp;#x200B; Disclaim: it's a product built by my company. 
No. Stop trying to get someone to do your homework. 
C is fine for starting up, although it can be less forgiving than some other languages. (Not necessarily a bad thing—you’ll certainly learn discipline by end-of-line-semicolon-ing a few loops and use-after-freeing a few pointers.) Once you get the feel for it, you should probably take a brief stint in assembly language, and then you’ll be set to go in pretty much any direction. The one *major* drawback to the language is the type expression syntax, which is appalling regardless of your experience level. You can work around that on GNU compilers with the `__typeof__` operator, not that you should rely on that sort of thing if you can avoid it. Anyway, if you stick with *no* optimizations whatsoever, C is fine for learning; as long as the compiler isn’t (with the best of intentions) assaulting your code from every angle, what you tell the program to do, it does quite literally. You should also enable all possible warnings and treat them as errors while you’re booting up, so you catch common mistakes. Probably hold off on the IDE so you don’t drown in features right away; use a good text editor and hand-compile/-run at the command line so you all that machinery isn’t papered over. (Along those lines, a good terminal and shell are a must as well. I prefer the KDE stuff on Linux, but there’s good stuff for pretty much any platform.) In terms of avenues of exploration, C is excellent. It’s high-level enough that you can avoid fiddling with calling conventions, register allocation, and stack frames, but low-level enough that you can either inline or call into assembly-language routines when you need to. Most OS code is also in C or C++, so you’ll be able to do stuff like write Linux kernel modules or stick your dick directly into the Win64 API if you want. Most stuff that offers libraries offers some means of calling fairly directly to/from C as well, so if you pick up (e.g.) CPython, C++, or Java later you’ll be able to hook right in. If you want to do weird things with GPUs, hey, OpenCL C is one option, GLSL (C-based) is another, CUDA’s another, OpenMP’s (partly) C-based, and Intel has their own set of `#pragma offload`s for the Xeon Phi (nèe MIC, nèe Larrabee, nèe bunch-of-P54Cs-pasted-together) GPGPU line. If you want to play with embedded stuff, pretty much any architecture has a C or almost-C compiler. If you want to play with multithreading, stack-switching, signal handling, NUMA, page-mapping tricks, syscalls, ioctls, networking, etc. etc., you can do that easily. You can write your own OS and system tools in it, if you’re sufficiently game. The GNU dialect of C is especially good for really-low-level programming, so I suggest going with GCC (or MinGW or Cygwin GCC), Clang, or Intel C. MSVC is reasonably good for C++ code, but its C support is …rough. (It *kinda* supports C99 things, but not at all well, and C11’s a no-go.) But again, no optimizations and all possible warnings as errors; use command-line flags `-O0 -Wall -Wextra -Werror` for those. If you want to stick strictly to the ISO C89, C99, or C11 standards, you can use `-pedantic` to kvetch about non-standard things, and `-std=c89` etc. to set the language standard. `-std=gnuXX` means C*XX* with GNU extensions. In terms of which language version you should pick up, C89 (a.k.a. ANSI C) is near-universally accepted at this point, and even when there are sub-C89 limitations it’s used as a baseline. C99 is well-supported as long as you’re not on MSVC; most of C11 is reasonably well-supported on non-Windows platforms, and can be emulated/sidestepped for Windows/MSVC. I’d start with GNU89 or GNU99, personally, and ratchet down to stricter C89 or C99 once you’re comfortable with it. C11 doesn’t offer a whole lot other than `_Static_assert` that you’d find use for early on. If you go professional with C, that’s the time to peruse the standards (or whatever unpaid-for draft PDFs you can find lying around) so you can get to language-lawyer status. C is often sold as a WYSIWYG sorta language, but there’s a lot of rules in the standards that the compiler can make use of to surprise the fuck out of you when it’s optimizing.
Firstly, this subreddit isn't for C++, try r/cpp_questions Secondly, consider posting your actual question instead of "can anyone help" and no other information given.
Try [https://bitbucket.org/yuneta/profile/projects](https://bitbucket.org/yuneta/profile/projects), instructions in [https://bitbucket.org/yuneta/packages/src/default/](https://bitbucket.org/yuneta/packages/src/default/), it's in production but sorry, no documentation.
Sure dude, but that's how OOP languages work under the hood, and that's how people have been doing OOP since before OOP languages were a thing. If you want to get rid of a programming paradigm twice your age, you're going to have to make a better argument than "I think it's a bit silly"
The Plan 9 C extensions (-fplan9-extensions in gcc) make it easy to do class composition, which can in many cases be used to achieve the same patterns as inheritance.
 if (superbad) abort(); is a 3rd reaction, which is not often enough used.
thanks, but do you think it's superbad to try to pop an empty stack?
I'll use a lot of "opaque structures" for information hiding, and so I can think in terms of interface. Of course, it'd be possible to incorporate a type field into each structure, and dispatch based on that, building something like a virtual function table. Each struct doesn't need its own function pointers. C++ is obviously a lot less clumsy, and probably has efficient implementations of vtables that would outperform this approach. Still, it's fun, and very flexible.
You are discovering that "Object-oriented Programming" isn't a feature of the language, but a feature of the program you are writing. Your program can be organised into objects in many languages -- it happens that *some* languages have features (usually syntax) that helps preserve this organisation in the source-code. Below are three methods I run into frequently; the "best" method will depend on what other compromises you are willing to make: # Objects are Structs The most common method is to consider an object as region of memory. This provides an easy way to break encapsulation, but with a little discipline this is usually successful. struct type { int size; void (*new)(struct object *o); void (*delete)(struct object *o); }; struct object { struct type*t; }; struct object*new(struct type*t) { struct object *o = malloc(t-&gt;size); t-&gt;new(o); return o; } void delete(struct object *o) { o-&gt;t-&gt;delete(o); free(o); } You'll see this pattern in many programs that take this approach. You can invent new types this way by relying on the fact that nested structures at the beginning of a structure are guaranteed to have the same layout as they would if the pointer was at them directly. struct car { struct object base; enum color c; }; static void car_constructor(struct object *o) { o-&gt;c = RED; } static void car_destructor(struct object *o) { } struct type car_type[] = {{ sizeof(struct car), car_constructor, car_destructor }}; Now we can create a new car with `struct car *c = new(car_type)` and you can see with a little thought how this can give you some inheritance and use function tables through our function vector are like vtable lookups. I believe this is what you're alluding to when you say you "simulate some of OOP functionalities": One great thing is that you can take this as pieces, and use (for example) just a function table for dispatch. This is very common for plug-ins and drivers which all implement a similar interface (type) but need no inheritance. It's also very difficult to implement multiple-dispatch in this way (attempts usually require run-time type lookups), and as you can see: objects have no privacy. With some discipline it's possible to remain broadly OOP and pay the performance costs for only what you absolutely need. Macros can help. Resources: * http://www.drdobbs.com/extending-c-for-object-oriented-programm/184402731 * https://dmitryfrank.com/articles/oop_in_c # Objects as id Another method is to hide the entire object system behind a messaging platform. This is what Objective-C [uses](https://developer.apple.com/documentation/objectivec/1456726-method_invoke?language=objc) under its pretty syntax, but it's also what Microsoft does for in-process COM. With some discipline it's possible to develop OOP in this way, but the full encapsulation means you're always paying the performance cost for method dispatch. Resources: * https://developer.apple.com/documentation/objectivec?language=objc * https://github.com/gnustep/libobjc2 * https://docs.microsoft.com/en-us/office/client-developer/outlook/mapi/implementing-objects-in-c # Objects as Processes Another method is to arrange each object as a process: You can create objects with fork() and communicate with it using IPC. This method gives you private encapsulation (you cannot easily peek inside another processes' private variables), but fork() is much more costly than malloc(), and send() much more costly than function calls (even indirect ones!) so generally people take this approach when they need their objects to be asynchronous workers. This is the (broadly) approach Windows takes in implementing (out-of-process) COM, but also the approach used in the Erlang and kdb+ programming languages -- which are like C, not frequently thought of as object-oriented. CORBA was once a "popular" (for some definitions of popular) C-based method for doing this as well. Resources: * https://9fans.github.io/plan9port/man/man9/intro.html * https://docs.oracle.com/cd/E13211_01/wle/cref/corba_ap.htm * https://www.codeproject.com/Articles/13601/COM-in-plain-C 
90% of the time, what people really mean when they talk about OOP is polymorphism. C has no high-level concept of polymorphism, but you can still do it. It's all structs and pointers.
I was playing around with inheritance in while ago and came up with this solution for ANSI C for single or multiple inheritance. If I recall correctly, its assembly for the casting/function calls has the same number of instructions as its C++ counterpart and I believe it is identical to how inheritance works in C++. It is legal ANSI C. https://gist.github.com/xianbaum/463f13a8d39014131b3f4fa67a241119
It depends on the case. You are asking if a car is better than a van. If an error message is needed, stderr is better than plain printf.
* Define a struct in your source file to contain the data of your "class". Let's call it `struct Foo`. * Put a `typedef struct Foo Foo` in your header file. This will allow the user to manipulate "pointers to Foo" but keep their contents invisible. * Add public "methods" to your source file. Use a prefix "foo" to their name to make it clear these are methods for the Foo "class", and pass a pointer to a Foo typedef as the first parameter. In other words, replace `foo-&gt;methodName(arg)` with `fooMethodName(foo, arg)` * If required, add private "methods" as static functions to the source file. * Add prototypes for your public "methods" to your header file. Leave out the private/static methods. You don't need to tell your users what they look like if they can't use them anyway. And that's it, really. You're missing some "real" OOP features such as subclassing (but that's frowned upon these days anyway), but you still have the data encapsulation that allows you to stop internal details from "leaking out". This approach means that users can't declare variables of type Foo, but only of "pointer to Foo". This is a blessing in disguise, because now they can only allocate a Foo by calling your code, which allows you to do any initialization you need. A drawback is that they can't rely on a destructor to clean up objects that go out of scope but have to explicitly call another function to destroy them. This does increase the likelihood of memory leaks, but that's what we have Valgrind for.
We could clean up the program... Can you show sample rtl\_433 output ?
C++ is off topic in this subreddit. Please post C++ questions to /r/cpp_questions instead.
I understand your argument is to prevent someone inexperienced from shooting his foot. But I do recommend trying it out just to learn the concept in it's raw form. Understanding that a function is just some place in the memory (one would also learn this if they programmed in asm). What inheritance is. What a type system does. Etc. For the fun of it. 
Hey oh5nxo, That would be great. I'm at work at the mo but can post a sample later on this evening from home (I'm GMT here) Thanks 
You either program as OOP or you don't. No need to "simulate" OOP functionality. You just do it.
Hmm, yep. D:
I agree with this. Not a big fan of C++ myself, but I think in this case you should write C in C++. Only use C++ for the OO-stuff but write the rest of the code like you would in C. Then a C-team can feel at home but still get the compiler help for the OO-stuff instead of messing with function pointers.
I am a C noob, so if this makes no sense ignore me. If you want OOP why not C++? It has classes.
And btw, that's the reason why always compiling your own sources with -fno-common (if you are using gcc) makes sense. This points out problems like this immediately.
To be clear, I am using the graph thing as an analogy. Im not trying to actually make a xy graph in the literal sense. I will be reading in data for the x axis value (wattage) which gives an integer value, if this value is say 5000, I want the corresponding y value (speed of LED lights). There should be a linear form to how I determine the speed of LED lights based on the amount of wattage being read.
Do not spam.
C++ is off topic in this subreddit. Please post C++ questions to /r/cpp_questions instead.
Also `#include &lt;iostream&gt;` and `using namespace std;`
Burn
Good material is rarely found in videos. I recommend you to read books and articles instead.
whoops srry and thank you
I have been trying to read some, but stuff like K&amp;R just confuses me and doesn't teach me anything. It just makes me confused and feel stupid.
Just curious - why is subclassing frowned on these days? Is this in general, or specific to certain OOP languages?
What confuses you about that book? If I can understand where your limits of understanding are, I can perhaps recommend you better texts for your level of knowledge.
I don't have it in front of me to show you the exact stuff. But, exercises ask you to do things you haven't been taught, the solutions end up using functions and libraries you haven't been taught etc
If that's your issue, I can recommend you to get familiar with the C standard library. Type `man 3 into` into the terminal to get an overview over the available functionality. Though generally, I recall that in K&amp;R, all exercises can be done using just what has been introduced so far.
Well for example (I don't know the exercise number sadly) the one where you have to copy input to output, I have no idea where the hell to start or how to do this. Everyone just says "It's easy, just think like a programmer" I have read everything up to this and nothing helps. I can't look it up cause then I get the answer. This makes zero sense.
Jacob Sorber is a computer scientist I regularly watch &amp;#x200B; [https://www.youtube.com/feed/subscriptions/UCwd5VFu4KoJNjkWJZMFJGHQ](https://www.youtube.com/feed/subscriptions/UCwd5VFu4KoJNjkWJZMFJGHQ)
I just want to get good so bad, but things like K&amp;R make no sense and confuse me...
If you know getchar() and putchar() and loops and conditional statements, you have all the tools necessary to solve this problem. No need for other libraries or functions. These things are definitely mentioned before the exercise.
&gt;TL;DR: Is C a suitable language to learn programming with for someone who is an absolute beginner? If so, how should I go about this? I'm going the downvote route and saying "no". you have to define your purpose in the language, **just like a real world language**. Are you a tourist? Are you meaning to conduct business in another culture? Are you a hobbyist? Do you want to know how badly the dubs/subtitles mangled your pure original intent? Do you want to compose works in the native language for native language consumers? Do you want to know why SOV and SVO mean ideas must be expressed differently, not just in different word order? All of these feed into your need for a language and the level of discourse. you might need total immersion, a 2 week rosetta course, a paid online translator, or a formal 8 year study abroad linguistics course that will teach you not only what the kanji are, but why they are so inscrutably different to the learned eye from even neighboring countries. C in this regard is a rigid simple language that requires you to do a lot of work, but because of this you can do essentially anything with it. It's a bit like learning Latin if you wanted to speak a european language. It won't teach you modern French or Italian, but you will understand those languages on a level even native speakers won't by the end of your studies. But is that really what you wanted? To *understand*, or just to know enough to get by with your goals (fiscal, curiosity or social)?
carl herold is a rare, tragic, exception.
They make no sense. They didn't even teach me about them. It says "oh these exist, now make a robot to bring peace to the middle east" it doesn't spend any time actually explaining them or teaching me how they work. I don't get them at all because of that.
Amen, just because C++ literally includes function pointers in their renamed structs, doesn't mean you have to literally do the same, that's taking "object-like" into straight the fuck up C++ behind the scenes.
Don't fret, everyone has different learning styles. The people for whom the light switch worked don't understand people fumbling in the dark, but given an exotic enough set of circumstances, everyone would be as clueless. Programming is...interesting, since the basics require you to understand a whole other set of basics that don't directly affect your programming per se. Most programming instruction is stuff like grammatical order of sentence logic, and you have to have learned what letters, vowels &amp; consonants, nouns &amp; verbs, adjectives &amp; adverbs and word order are for sentences before you can tap into grammatical order. And that's all before you can get into what you really wanted to do: write a crackin' adventure story! But like writing once you have the foundations, 'everything is on the table'. I had the best experience with [this channel](https://www.youtube.com/watch?v=Jlbs8ly6OKA&amp;list=PL76809ED684A081F3) in getting the foundational skills for programming settled in. After that, you can start to grok the higher level structures that are what really keep you from programming what you want to.
putchar() and getchar() are explained in **Chapter 1.5 Character Input and Output**.
I'll definitely work through this, thank you! I really wish there a codecademy for C.
Okay, but for **me** that isn't enough to understand what those due. I get it okay, you understand it right away and that is all you need to understand what it does. But sitting there acting like **no one ever** needs a better explanation of this stuff is really annoying. That isn't enough for to understand what the hell they do and how to use them.
You need function pointers to implement runtime polymorphism. It doesn't mean you need *every* function to be called indirectly, just those that are meant to be defined by multiple implementations of the same interface and used uniformly.
I guess what I'm unclear on is what you're not getting exactly. That seems pretty clear to me but I'll admit that such is likely due to not having been a beginner in a long while.
C++ uses vtables because it is the best know technique for implementing runtime polymorphism. If you need it in C, it makes perfect sense to do it the same way. Even if you dislike C++ it's stupid to reject something it does when it fits your usage pattern.
First of, please don't spam. Second, C# is off topic in this subreddit.
if you think you might be a visual learner, I recommend [Processing](https://processing.org/).
What do they do? getchar() what does it do? I thought it was like `userinput` from Python, but that's not what it does when I use it. What the hell is putchar? It makes ZERO sense to me. All K&amp;R did was sorta kinda in a tiny way tell you **very** losey what they sorta do, but they didn't give me an indepth understanding of what they do and I don't know how to use them. I need to fully understand all this stuff, but k&amp;r just kinda brushes on stuff and doesn't really go into it enough.
For the record, this generally is just a good way to write code that deals with any sort of specific data type. I structure parts of my code this way all the time, though instead of it being focused on a specific data type, it focuses more on a specific functionality of my program and whatever data is involved with it. One of the hallmarks of OO design is that the functionality is embedded in the datatype as part of the data (function pointers usually in this case) and polymorphism, which means that you're able to treat a data type as a superset of some other data type. So you end up with layers of data and functionality and data types that can be treated like and stored alongside other similar-but-not-quite types. In C that generally means lots of void pointers and type checking systems. Usually more effort than its wor6h, imo.
On GNU systems, you can use the `backtrace` function to compute and print a stack trace. Of course this only works if your binary still contains symbols. You can build your own solution based on this.
getchar() reads a *single* character from an input stream (which in this case is the keyboard). That's all it does. You can assign it to a variable via char c = getchar(). By itself, it doesn't take multiple characters... just one. Putchar(c) sends a single character (c) to the output stream (in this case, the screen). That's all it does. It won't send multiple characters. That's as in-depth as they get and if you're expecting more you're *really* overthinking them. Now, in order to operate on more than one character, you need a loop. which is what "while ((c = getchar()) != EOF)" is. This basically means, "While we're in the loop (which terminates upon receiving End-Of-File - ctrl-d), keep reading characters and performing the action in the curly brackets on it." Inside the curly brackets, we might have "putchar(c)", which says send the character to the output stream. I don't think K&amp;R brushes over this stuff at all but maybe it would be best to learn from other sources first and come back to it later if you're having these sorts of problems with it. Sometimes it's just different strokes for different folks.
I know loops and such as the concepts are the same from Python, but crap like getchar and putchar and stuff makes no sense and it's driving me fucking crazy. I'm at the point where I can't even open K&amp;R cause I don't know what the hell it's talking about so I sit there for hours staring at my screen. I use to use YouTube tutorials when I was learning Python to help with this, but C is harder to find for some reason.
The variable should be of type `int` so you can catch `EOF`.
I think it might be a good idea for you to put aside C for a while and come back when you have calmed down a bit and found some distance to these concepts. Right now you are not in a state where you are receptive to learning new concepts.
I'm not coding atm, just figured I'd ask for YouTube tutorial suggestions so when I get home to code I can already have suggestions to start with.
That's because K&amp;R and C are working on a much lower level. This is just basic text-manipulation work here. Taking input and manipulating what's sent out to the screen. getchar() and putchar() are a lot simpler (I **don't** mean *easier*... I mean literally simple in what they do; as in, by themselves, they're not doing anything complex at all) than what Python offers you so it takes more to make them work. The point is to show you that there is a *ton* you can do with even very basic tools such as these if you put your mind to applying them. This is what it means to work with a low-level language... you use it to do low-level stuff. It's also why most people aren't recommended to start their programming journey with C but are encouraged, instead, to focus on higher-level languages like Python.
What I mean is that `getchar` and `putchar` are really simple concepts. It seems like you got some misconceptions about how they work which stop you from understanding how this actually works. If I have this sort of problem, I usually take a step back from the material and wait a few days, reading it again once I have found some distance from my prior wrong understanding. Perhaps this might help you, too.
Oops, good catch!
I just need some extra resources to help nail in what these are :(
I just need some video tutorials. I need to hear a different explanation than the one that didn't work the first time and work with some examples where I can see and understand what they do. Instead of just trying the same thing that didn't work the first time.
It's not that I don't think I can't do it, I know I can. I just need some extra material to help me understand it.
It's not that I don't think I can't do it, I know I can. I just need some extra material to help me understand it.
Well, like others have said, maybe K&amp;R *isn't* the right entry point for you. That doesn't mean you can't do it; just that it doesn't mesh well with your learning style. :) There's nothing wrong with that at all and it absolutely doesn't mean you can't do this stuff. Learn somewhere else and come back to the exercises after you have a bit of C experience under your belt. When you do you'll laugh at the trouble you're having now!
I am doing learnc.org, but that doesn't get into specific functions. Tbh, this is extra studying outside my classes. I just want to do everything I can to learn C well.
To be brutally honest I don't have a great deal of experience with OOP, so I'm mostly just parroting what I've read online. The little experience I have is that, once you go beyond the textbook cats-and-dogs-are-both-animals examples you quickly get lost in a jungle of interdependencies between super- and subclasses, where it becomes painfully unclear which class is responsible for which behavior. Anyway, if you Google for "composition over inheritance" you'll find many discussions on the subject.
I honestly think you're just overthinking these two functions. They literally just, "do what's on the tin," so to speak.
It just helps to see a video of two of them in action so to speak. Like seeing someone else use then and explain then as they use then clears it up right away usually.
This is so weird though! I mean I got pointers, structs etc in a minuet, but these two stump me?
Like I said, I think the problem is that you're overthinking them. They literally just get and put single characters. That's it. :D
I did tey using them before, maybe if I write a separate program and then try the exercise?
I'll respond further via PM.
Thank you!
For some reason this opinion is not very popular but I believe you should begin at the lowest abstract of a computer and work your way up. This solid grounding in how computers work, and especially how computer memory is laid out and used) is fundamental to understanding and becoming proficient in telling a computer what to do (i.e. programming.) By the time you get to C it should be easier to grok the fundamentals of C (arrays, strings, structs, program flow, etc..) and help you wrap your head around the more challenging parts of the language (pointers, malloc, etc..) And as your programs get more complicated you start picking up other CS concepts (data structures, recursion, sorting, callbacks, etc..) that you have to implement yourself. Once you have C mastered then picking up other languages is a breeze and possibly desired as you explore other programming paradigms (Object Orientation, Functional, etc..) for your particular problem domain. The following helped me on my journey of becoming a decent programmer: 1. Read [Code: The Hidden Language of Computer Hardware and Software](http://www.charlespetzold.com/code/) 2. Watched [Crash Course in Computer Science](https://www.youtube.com/playlist?list=PL8dPuuaLjXtNlUrzyH5r6jN9ulIgZBpdo) 3. C Programming: A Modern Approach by K. N. King (non free book but worth it. if you want a free book then pick up [Modern C](http://icube-icps.unistra.fr/index.php/File:ModernC.pdf) by [Jens Gustedt](https://gustedt.wordpress.com/author/gustedt/)) 4. Followed this [Tutorial on Pointers and Arrays in C](http://pweb.netcom.com/~tjensen/ptr/) The first two really helped by approaching C from a lower level of abstraction (actually the absolute lowest level and gradually adding layers of abstraction until you are at the C level which, by then is incredibly high!) The third is the best book on learning C for beginners. The forth is just the best tutorial on pointers in C. From this foundation you can pick up books on Algorithms and Data Structures and bob's your uncle! Good luck on your journey. It's a long and rewarding path. Write a LOT of code!
I just posted this on another thread. Might help you as well. [https://www.reddit.com/r/C\_Programming/comments/auhzh8/c\_as\_a\_first\_programming\_languageintroduction\_to/eh8wvhw](https://www.reddit.com/r/C_Programming/comments/auhzh8/c_as_a_first_programming_languageintroduction_to/eh8wvhw)
Thank you so much!
I’ve found that mycodeschool does a pretty decent job of explaining topics clearly. 
Well he is not using K&amp;R so this should be easier maybe
There are a few places where implementations had been needlessly divergent, such as the question of what prototypes belonged in what header files and how variadic arguments should be handled, and perhaps even how *silent-wraparound two's-complement implementations* should handle promotion of short unsigned integers types (see below), and it was useful to have the Standard encourage uniformity in such areas. Unfortunately, the authors of the Standard failed to recognize that because of the diversity among the platforms implementations would run on, and the range of tasks they needed to perform, their most important rule should have been to establish things that implementations should do *when practical*, recognizing that higher-quality implementations would generally do such things whether or not they were strictly necessary, but that an implementation that documents an inability to behave in common fashion may for some purposes be more useful than one which takes on the expense of supporting common behavior. When processing code that will be used exclusively on implementations which use silent-wraparound or silent-promote-or-wraparound two's complement arithmetic, having short unsigned types promote to signed types is probably the most useful behavior. A lot of code written for other types of implementations, however, would have been reliant upon other semantics, and for some purposes it would be more useful to specify that implementations may at their discretion either accept certain constructs with signed-promotion semantics or reject them, depending upon what their customers would likely find most useful. Strictly conforming programs would be required to cast or coerce operands to suitable types to avoid such ambiguities. Unfortunately, I think the authors of C89 were under the mistaken belief that it would be better to try to have all implementations accept or reject the same programs than recognize that it's useful to have programs that will be accepted by some implementations and rejected by others. Given something like: unsigned mul(unsigned short x, unsigned short y) { return x*y; } there are three ways I can think of that implementations could treat it, ranked below in order of preference (with each choice being signficantly superior to the ones below): 1. Return the arithmetical product of `x` and `y`, mod-reduced to the range of `unsigned int`, for all values of `x` and `y`. 2. Refuse compilation unless the programmer casts or coerces one of the operands to `unsigned int` or a larger type, or else casts both to `int`. 3. Return the arithmetical product of `x` and `y` when it is below INT_MAX, and behave in arbitrary fashion otherwise. For the Standard to have mandated #1 would have required that some implementations perform expression evaluation in context-sensitive fashion, something the authors of the Standard seemed keen to avoid. I see no benefit, however, to having the Standard require that implementations accept such code while imposing no requirements on the behavior in cases where the temporary result exceeds `INT_MAX`. In the rare cases where a programmer would actually wants the code to use signed-integer semantics that behave differently from unsigned semantics, requiring casts to make such intention unambiguous both to compilers *and to any humans reading the code* would seem like an unreasonable burden than requiring that programmers ensure the function is only invoked with `x==0` or `y &lt;= INT_MAX/x`. 
As does mine :), I just want to get an extra leg up so to speak
Experiment with them yourself. Write a little program exploring what they do. That usually works for me. You can't just sit and read the book without trying out the code
As much as I would like to encourage this youngling to "learn" how to do c++ in c, i would say that it would take him at least a year to get anywhere close to the c++ to c translator that stroustrup wrote in (probably) 1984. Let's not forget stroustrup had a phd with strong foundations in compilers. I do not know the op's background, but based on the question, I would reckon (s)he's not at that level. Whether to do it or not is the op's choice, and whatever the community suggests to him, but ... I still feel it would be a massive waste of time, energy, money and talent. Just saying. 
Getting input in the C programming language is a little confusing. I had a lot of trouble with it myself. I found it helpful to write one line of code at a time and print out what you get to see if it is what you expected. If you try to write your program all at once without testing things it is hard to find your mistake.
another good topic is what IDE do people prefer ? Eclipse? some thing else ?
I use vim :)
well yeah .. xterm and vim is essentail however looking at slightly more GUI world wouldn't hurt.
How is it possible that no one has mentioned K. N. King's C Programming: A Mordern Approach?? Grab the 2nd edition. It is hands down the best book on C, from a learning perspective. Grab that and the K&amp;R ASCII standard 2nd edition, then supplement your learning with various internet resources. Stick to it, and don't just read. DO THE EXERCISES!! You can read all you like and tell yourself you understand, but you won't. Stick to one language, learn it inside and out, and do a large and complex project or three with it when you're done. Don't move from one chapter to another unless you feel you have a solid grasp of everything in the previous chapters. It starts out slow and simple, but it ramps up. Be vigilant, do at least an hour with it a day and you'll be competent before you know it. 
The language the C Standard was written to describe is simple, and would be fine for introducing oneself to the way smaller computers work. Compilers like gcc and clang will process that language when optimizations are enabled. The language the C Standard actually describes, and which gcc and clang process with full optimizations enabled, is monstrously more complicated and full of traps, and should only be used by experts doing things like high-end number-crunching on trusted inputs. For example, given a function like: int read_int(int *p) { return *p); the behavior in the former language could be "interpret `p` as a hardware address, and fetch an `int` value from that location using the execution environment's normal means of reading such values, behaving in whatever way the execution environment behaves when given such a request." Depending upon where `p` came from, conforming implementations may be required to guarantee how the environment will behave, but the operation would behave as described in all cases where the environment defines its behavior. In the language defined by the Standard, the behavior would be "in certain cases mandated by the Stanard, or in any other cases where the implementation feels like doing so, behave as though performing the hardware request, and in any other cases behave in arbitrary fashion". A full, complete, and unambiguous description of which cases are defined by the Standard would probably be impossible, since I don't think even the authors of the Standard ever reached a consensus about how all cases should be handled, but a 99.9%-accurate description would greatly complicate the spec while adding nothing to the semantics available to the programmer. 
The Linux kernel has some object like design in it. I can't find it right now, but there are some old (2+ years) articles on it. You might start with the kernel's linked list implementation - I think it was described as part of a bigger OOP discussion 
It has been said many times that it is not a good idea to start with that book. C primer plus by Prata would be a good start imho. Then you can read k&amp;r. I wouldnt recommend youtube in general to study programming. Books are the best. Good luck
I prefer terminalbfor my workflow
Ashley Mills
If k&amp;r is a little daunting try K. N king . I really loved that book plus it’s a bit simpler and modern compared to k&amp;r
 b i n g o. not the greatest language for embedded dev &lt;-- so true.
From what I've read, it seems to mostly be an issue with games and other really complicated AI problems where state and behavior are constantly in need of changing or where complex decisions need to be made by some AI.
It would look just like the D language in betterC mode! https://dlang.org https://dlang.org/spec/betterc.html https://dlang.org/blog/2017/08/23/d-as-a-better-c/ Highly recommend you take a look (and at D language generally!)
https://www.youtube.com/user/handmadeheroarchive/ - Handmade hero is a project of casey muratori where he makes a game on streams. 
While this is often true, keep in mind not all people learn best from text. Some people are visual learners and learn best from actual examples, some are physical and need interaction, some are auditory and need books on tape, and some are good with reading. The only reason books were so important for mankind for so long was because we didn't have much in the way of recording information in any other way. So try not to scoff at the medium a person needs to succeed. The person is rarely the problem.
I do understand that. However, while I know many C tutorial videos, not one of them is good enough for me to recommend. That's why I made this comment.
If you do decide to start with C (I did, and I'm glad I did, it worked really well for someone like me that really wants to understand "why" something is the way it is and not just how to do stuff), I would HIGHLY recommend following Harvard's free online course, CS50. In previous years, it's been taught in C, and will be much easier to self study than just reading K&amp;R.
Can you give an example? I learned C from that book in a week after only programmed in another language for a few months. I do not recall any libraries other than the standard io and standard lib. Unlike Python, Just remember to attach the header lines and you will be fine. Also if this is your problem, you are not reading the book properly. And video is actually the slowest way to learn anything properly. It is too passive and too entertaining at times. 
I see, I apologize, I misunderstood your point.
I’d check out JetBrains CLion if you like IDEs. Looks to be pretty decent. 
If you accept the fact that your first language won't be the one you end up using the most, I think C is the best to start with. Most languages are higher-level abstractions of C. E.g. I feel I'm somewhat at a disadvantage learning Java in university right now over C. I really don't feel like I'm laying the best foundation I can for being an exemplary programmer someday.
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/devsjunky] [C YouTubers](https://www.reddit.com/r/DevsJunky/comments/aunqg1/c_youtubers/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
Do you enjoy live coding as displayed in[Nested pointers vs. multidimensional arrays](https://www.youtube.com/watch?v=ZujDj979gYo)?
For me, it's really about seeing it in use and having an understanding of how it is being used and why.
I gave you a thumbs up because you took some time trying to understand OP. Also instead of throwing a answer that would just frustrate anyone struggling with learning you gave an alternative path. Nice!
Are you sure that `char out[MAXBFR]` is correctly initialized? From my understanding it has to be zero-initialized for `strncat` to work properly. &amp;#x200B;