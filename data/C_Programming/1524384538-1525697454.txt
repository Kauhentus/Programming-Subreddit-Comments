change 12 and 7 with 12.0 and 7.0
I wrote byte where I mean octet. Malloc takes the number of C “bytes” which are not octets. Sorry for that. &gt; Besides, you even contradicted yourself. [...] Which means that using sizeof (in the case of char) is completely fine No I haven’t contradicted myself. I never said it wasn’t fine to write sizeof(char), as it is one ALL THE TIME. It is just useless. &gt; even if char was 7 bits Also char CANNOT BE 7 BITS. Standard mandates at least 8 bit. &gt; Besides, speaking practically, what (modern) platform is char not 8 bits? Many DSPs don’t have 8 bits chars. Almost all other modern systems have bits chars.
This question is only relevant to 32 bit systems where you run into all sorts of interesting problems when allocating more than 2 GB of data and 4 GB is the hard limit of what you can do. You can just write your data to a file using `write` or `fwrite` as usual, there are no problems.
&gt; However, BSTs can be made intrusive and persistent. I don't see how you can't do the same thing with B-trees.
I don't know what you're trying to do but I would expect that to print garbage, since you've only allocated but not populated each line. Note: Some environments automatically set allocated bytes to 0x00 - in this case, NULL characters (which appear identical to spaces) would be printed instead of garbage. Also, in the line: for (int j = 0; j &lt; sizeof(code[i]); j++) { Why `j &lt; sizeof(code[i])`? `code[i]` is of type `(char *)` and is not an array on the stack, so `sizeof(code[i])` will give you the size of a char pointer - typically 8 bytes for a 64 bit system. In effect, you're printing the first 8 chars of each 'line', where each line holds garbage.
Would creating a new array and iterating over the original work? 
There are lots of intrusive BSTs and some persistent BSTs. I am yet to see an intrusive B-tree. Perhaps this is theoretically possible, but an intrusive B-tree will be more awkward and difficult to implement and to use. The added complexity will also make it less efficient (intrusive BSTs/lists are as fast as non-intrusive BSTs/lists). Persistent B-tree may be even more so.
Hm... I was actually thinking about building intrusive B-trees for the directory structure of a file system because they reduce the number of IO operations needed compared to non-intrusive B-trees or B+-trees while still allowing O(log n) lookup to directory entries and streaming of directory contents.
If the assignment is due tomorrow, why haven't you started earlier?
I’ve been swamped with tests the past week so been studying. Spent this weekend understanding c programming before I can start the assignment 
Is *a* the year and *j* the day then? The assignment is written in a strange way. 
Yeah sorry I forgot to mention that. J is day(d) a is year 
Forgot to mention this ‘j’ is day and ‘a’ is year
Thank you all for the responses. This was the solution I came up with: #include &lt;stdio.h&gt; #define SIZE 20 int main() { int v[SIZE] = {1,0,0,0,0,2,0,0,3,4,0,0,0,0,0,0,0,5,0,6}; int i, j, k, n=0; for(i = 0;i &lt; SIZE;i++) { printf("%d ",v[i]); } for(i = 0;i &lt; SIZE;i++) { if(v[i] &lt;= 0) { n++; for(j = i; j &lt; SIZE; j++) { if(j == SIZE - 1) { v[j] = 0; } else { k = j + 1; do { if(v[k] &gt; 0) { v[j] = v[k]; v[k] = 0; } k++; } while(v[j]&lt;=0&amp;&amp;k&lt;SIZE); } } } } printf("\n"); for(i = 0;i &lt; SIZE - n;i++) { printf("%d ",v[i]); } return(0); } 
Thanks, I edited my comment.
You don't need an array. Or a structure really, although using one to hold the 3 fields would be tidy. Write a function that takes the 3 fields and perform those tests and calculations. I recommend using more readable variable names than m, m1, n8 etc.
Under the assignment it’s labeled. Selection, arrays, structures. So I’m guessing we have to use a combination of all or some. I’m confused how to get an input of ddmmyyyy and then separate the month and year so I can do the tests. I’m gonna test a few ideas in abit and I’ll link it here
You'll probably want to use an array to get a string for the day of the week from f%7. i.e. map the integers 0-6 to the strings "Sunday"-"Saturday"
If you're going to delete and shift you'll probably want to use a linked list or at least memmove
Obviously didn't know what the day was!
It's fairly straight forward. Pretty much you have a `typedef struct InternetAddress InternetAddress` and when you read the first line of the file you want to allocate an array of that size, so `InternetAddress *addresses = malloc(sizeof(InternetAddress) * SIZE_FROM_FILE);` then you need to attribute each line to the array of structs like so `address[0].num1 = NUMBER1` you'll need to figure out how to parse out the period and convert it from ascii to an int.
Thanks a lot that clears up some confusion! I will attempt that in a bit 
coma can be used instead of semi\-colon to separate statements where precedence isn't important. So for example *a=1, b=2, c=3,* is perfectly fine, you are saying you dont care what order those values are assigned. It could end up like *b=2; a=1; c=3;* or *c=3;b=2;a=1;* its up to the compiler to decide because you have stated you dont care. So there is no correct answer for *a=1,2,3* it is implementation defined.
For the next time, do not post your questions as images. Always post text only.
If I cast it to an unsigned char, would that make it not atomic anymore? I was worried about doing this.
The variable has already been read at the point of the cast, so it doesn't matter.
I think I just realized a large problem with the way my code is set up. I allocated 20.4 GB to the database in one chunk. Then, at the end, I try to write this to a file. But if I remember correctly, I would be writing to a buffer in main memory before this buffer is stored in secondary memory. If this is the case, then 32 GB isn't enough. I would need 40.8 GB. I was thinking that I can divide the memory into chunks. My friend's computer has a CPU with 12 threads. I can allocate 20.4/12 GB of memory 12 time. At the end, I could then free up memory used by a thread, and this memory can be used for the file buffer. Instead of needing 40.8 GB, I would need 20.4 * 13/12 = 22.1 GB. I could change this: static atomic_uchar *database; into this: static atomic_uchar *database[NUM_THREADS]; Does anything that I said here sound incorrect? It seems to make a lot of sense to me. My friend tells me that on our first attempt, the program made a 3 GB file and the program never terminated. I hypothesize that it is because of the issue I stated above.
It’s difficult to explain it all with text 
&gt; So there is no correct answer for a=1,2,3 it is implementation defined. I'm not entirely sure what you mean by that. If you mean that the compiler is free to choose the order in which the expressions (`a=1`), (`2`), and (`3`) are evaluated, you're right (but note that if that's the whole statement, then `2` and `3` don't need to be evaluated at all, since they don't have any observable effect). If you mean that `a=1,2,3` could have the effect of assigning either `2` or `3` to `a`, then that's wrong: `a=1,2,3` is exactly the same as `(a=1),2,3` because `=` has higher precedence than `,`.
Your post is a picture of text. You could (and should!) transcribe the question into your post so people who are blind can read it. Next time you post a question with the question as an image, I'm going to delete it.
Fair enough. I apologize 
&gt; So for example a=1, b=2, c=3, is perfectly fine, you are saying you dont care what order those values are assigned. It could end up like b=2; a=1; c=3; or c=3;b=2;a=1; its up to the compiler to decide because you have stated you dont care. That's not right. The comma operator is a sequence point, so in `a=1,b=2,c=3`, the compiler must assign `1` to `a` before assigning `2` to `b`, and so on. The compiler is free to re-order the assignments only it it can prove that it doesn't make any difference (i.e., has no observable effect). That's the reason it's ok to write stuff like: if (p = f(), p != NULL) { /* ... */ } In the `if` condition, `p=f()` is guaranteed to execute before `p != NULL`.
No need to apologize, it's not that you could have known this.
Could you explain question b to me? I’m still unclear about What they are asking 
The computer we are using has 32 GB of RAM. 20.4 GB are being used to hold the data in RAM. If the data is stored in a buffer before being written to secondary memory, then this seems like it would be a problem. That buffer would also have to be 20.4 GB of memory. So there would have to be a total of 40.8 GB worth of main memory for this method to work. Based on your response, I assume I'm getting something very wrong here, but I'm not sure what. 
The middle expression in ?: has 'infinite' precedence: it is implicitly surrounded by parenthesis.
I experimented with doing this with Ruby. The reason I was interested in doing that was to embed some kind of interpreter for interactive use with a C program that did some heavy computation. It's much easier to go the opposite direction. Just write a shared library called through Python's FFI for heavy lifting.
This assignment does not make any sense without netmask. What university this is?
That coma is within a statment, its not seperating statments.
there's a text file that looks exactly like it's formatted below, and they want you store that information in the struct you created in question A.
Instead of making just a game, make a game *engine* that loads a game description from a file and runs that.
Ohh okay I see now! Thank you Needa figure out how to that now lol
I cant find it in the std, but this works #include &lt;stdio.h&gt; int main() { int a; int b; a=1, b=2, printf("a=%d\n", a), printf("b=%d\n", b); return a+b; } 
I pasted the snippet into a small program and ran it, and it works fine for me. Valgrind also didn't give any errors, so I didn't just get lucky. Have you tried it with Valgrind yourself? I don't see any obvious problems with your code. A minor nitpick is that `sizeof(char)` is unnecessary, it's defined as 1 regardless of the size in bytes. In fact, `sizeof` for other types doesn't give the answer in bytes, but in multiples of `char`. In 99.999% of the cases, `char`s are just bytes of course
Works for me. I'd argue against using *void for buf and rebuf, since you are doing pointer arithmetic with buf; it's non-portable. Are you *sure* the segmentation fault comes at the "contents[cursor] = 0" line? Perhaps it occurs later/elsewhere.
I think I got the idea from the program called nano. When I'm in the nano editor, it has an option called write buffer. I assumed that write also used a buffer in the sense that it got everything into memory that it was going to write to secondary memory, and then wrote it when it was done. It's nice to know that I don't have to worry about this now. Also, there was another interesting phenomenon that happened on our first attempt. [These](https://imgur.com/a/SCU69L9) are the results of the average heuristic from generating the 6, 7, and 8 edge database. The number of states at the maximum distance from the solved state are 365, 81, and 17 states respectively. When we ran the program, it got stuck at 100% for 2 hours. The way my program works is that gets to the lowest depth and then tries to run again at the next depth to see if it can find any new states that haven't been seen yet by performing moves on the states from the previous depth. I predict that the maximum number of moves for 9 edges is going to be 12 like the 8 edge database. This means that there will be about .0031 x 12!/3! x 512 = 127 million states at the maximum number of moves to solve. The search performs moves on every one of these states to see if it can find a new state, so the program spends a very long time at 100% completion.
You might find [Programming Your Own Adventure Games in Pascal](https://www.amazon.com/Programming-your-adventure-games-Pascal/dp/0830607684/ref=sr_1_sc_1?ie=UTF8&amp;qid=1524435422&amp;sr=8-1-spell&amp;keywords=progamming+adventure+game+in+pascal) helpful. Even though the code is in Pascal there should be no issue translating to C. I think it's a great book for a beginner as it emphasize some nice coding practices.
It's not [necessarily] crashing writing to the buffer. Since it is printing "Cursor is 6476" it's got past the read/realloc loop. Are you sure it is the `contents[cursor]` write? Have you tried running under a debugger?
You keep reallocing rebuf into buf. What I'd do instead is simply use `buf` and store the results of realloc() in `buf`. so: buf = realloc(buf, (cursor + READ_BUFFER_SIZE + 1) * sizeof(char)); 
oh, sorry, did maths wrong in my head.
Here is the unmodified source that failed before me experimenting and changing things around. I've tried different values of READ_BUFFER_SIZE; it started at 256. char* read_file_to_string(char* filename) { //open file FILE *input = fopen(filename, "r"); if(input == NULL) { return NULL; } //create input buffer int cursor = 0; char *buf = malloc(cursor); if(buf == NULL) { fclose(input); return NULL; } //read file into buffer, resizing as needed while(feof(input) == 0) { char *newbuf = realloc(buf, cursor + READ_BUFFER_SIZE); if(newbuf == NULL) { fclose(input); free(buf); return NULL; } buf = newbuf; cursor += fread(buf + cursor, sizeof(char), READ_BUFFER_SIZE, input); } //done reading. Close the file. fclose(input); //resize the buffer to the size read + 1 and add a null terminator to the //string char *contents = realloc(buf, cursor + 1); if(contents == NULL) { free(buf); return NULL; } contents[cursor] = '\0'; return contents; }
yeah, in fact it could have crashed after the snippet above.
I haven't tried with Valgrind, I'm in a crunch right now; this manifested really late (had been working with smaller input files that weren't causing the issue). Updated with a top-level comment with the whole original function if you want to look at that. sizeof(char) was just something I tried, just to try and cover my bases.
You need the original pointer to free in case the realloc fails. That's why he's using `rebuf` and overwriting `buf` immediately if the realloc succeeded
Never mind. It was somewhere else. I misunderstood the debugger and forgot to newline my debug string to flush the buffer.
Formatting
 if(polynom == 0.00001 || polynom == -0.00001 || polynom == 0.|| polynom == -0.) Print the value of `polynum`. It is unlikely to be exactly 0.0 or 0.00001 . You need to test whether it is *smaller* than your acceptable error.
Look up Hugo's House of Horrors. Awesome game.
Great catch. This is actually listed in Deep C Programming, Ch.2 "It's not a bug, it's a language feature" "Sins of Mission (things in C that seem misdirected or a bad fit)" "Some of the Operators have wrong precedence." pg 45.
[How to program a text adventure in C](https://helderman.github.io/htpataic/htpataic01.html)
I think the precedence of the comma operator is just right because it is most commonly used in this kind of situation: for (i = 0, j = 1; i &lt;= n; i++, j += i) .... I have also never seen an actual program that intentionally uses the comma operator in an operand to `=`, so it makes sense for `,` to have lower precedence than `=`. 
The example given in Deep C is: The expression: ``` i = 1,2; ``` Expected: `i = (1,2);` so `i == 2`. Actual: `(i = 1), 2;` so `i == 1`.
Upvoted for the Colossal Cave throwback.
Reformat your code.
Sorry updated with a pastebin link I couldn't figure out how to format 
You gave up too easily.
 printf("%.2f", number); %f is for printing floats. `number` isn't a float.
I was told that the functions arguments are placeholders, I guess the int is influencing it. Thanks!
Also perhaps an absolute wrapping it so you only have to test once
After your edits you have void Circumference(float number){ printf("%f", diameter); } The parameter is called `number`, not `diameter`
Ah, so it needs to match it exactly then? I was under the impression that it was a placeholder. Thanks again, it works perfectly now!
Loved playing that. :D
That's perfectly fine, the comma operator is in section [6.5.17 of the C99 standard](http://port70.net/~nsz/c/c99/n1256.html#6.5.17). What's wrong is saying that the order is undefined. The standard says there's a sequence point between the left and the right operands of the comma operator. That means the left operand is completely evaluated (including all side effects) before the the right is evaluated.
u should read the book first.
You can check out [my Text based RPG](https://drive.google.com/open?id=1iLYZCDh8oz2S6-hQgp2VjHE1AZeTatim) I've been working on as a hobby on and off for awhile now. Initially got me started with it by watching a tutorial called Wrathlands, a step by step on how to build your own text rpg in C++. The site is unfortunately no longer active, but you can still download it from [the web archive](https://web.archive.org/web/20110814104009/http://www.rdxgames.net/projects/wrathlands/index.html).
Yea, i remember it from uni 20 years ago... but maybe it was wrong at the time, or i remember it wrong. I wish it was true though, would be easy for parallel programming.
Let's see you data validate function. It might not be right.
Yes lets see the validate function, also I would probably put the scanf before the if statement. But without seeing how the validate function works, I'd probably set up the code to be while( b != 1){ scanf(blah blah blah); int x = date_validity(blah blah blah); // have it return a value. if its valid return 1, if it's invalid return 0 if(x == 1) b = 1 else fprint("invalid date"); } blah blah blah being the 3 different array cells you have in your scanf put that in the for loop to then increment. I believe you can also make your function return a BOOL value, but numbers have always just been easier for me. 
you just put 4 spaces before each line of code :) 
I can imagine. I don't really see the point for this expectation though.
C# is off topic in this subreddit. Please post C# questions to /r/csharp.
Some editors have a function format your code, otherwise you can use a online [C beautifier](https://codebeautify.org/c-formatter-beautifier). Also, you can use [Reddit Lint](http://redditlint.com/) to put 4 spaces in front of each line of your code before posting it.
They are very small numbers. Run the code inside another loop 10,000 times so it is running for about 30 seconds instead of 3 milliseconds and see if the difference is still there
Both of those times are so small that you're probably entirely measuring the overhead of actually launching the process rather than its runtime.
interesting, I'll give it a try. Although I have to say that the times I posted are entirely consistant even after running the executable multiple times.
Also make sure you're using the same compiler flags, like CLion could have enabled optimizations where in the terminal you didn't specify any optimization levels.
Frankly, I think the better idea is you don't read that book (other than for fun). Where you're going, there is no sun, only darkness. If OOP is what you need, use a language that has it.
Ok I did it like that and now they are both about the same. weird though that there is always a difference without the loop. For example I'll run from the terminal 10 times and the values are always around 0.003821 sec, and running from Clion 10 times and the values are always around 0.001350 sec 
absolutely no prints within the stopwatch. But what I find the most interesting is that Clion appears to be giving the correct answer since 13.2/10000 is much closer to 0.001350 than it is to 0.003821
There is overhead caused by one time cache misses, dynamic linking, page faults, etc. Benchmarks under 10s are suspicious, under 1s - useless.
Something like ``` int dayOfWeek(int year, int month, int day) { // The equation given return result; } ``` I assume by digit he means decimal digit, you can extract the nth (1 being m^0, 2 m^1, ...) digit in base m from a number as ``` int digit = (int)number / n % m; ``` That should be all the building blocks you need.
Please do not remove your code after receiving an answer. That's just bad style.
How much are you printing to the terminal? Printing on the terminal vs printing within the IDE can be drastically different.
Timings and benchmarking can be tricky. It would be better if you used a benchmarking library
Never within my stopwatch 
If the solution had been in one of the answers, I definitely would not have, because I agree -- it's part of the benefit of the community. As it was, the code I posted was not at fault and was 100% functional, the issue was in a different module entirely (specifically, it was a divide-by-zero on the read data). I determined that it would be better to avoid confusion that I remove the code entirely, since I can't change the title of a post. If there is interest, I'd be happy to repost the working code for others to use as an example in the same situation (though it definitely isn't flawless).
Yeah, I need to get it up and running. For context, I've been doing most of my work in Python but it was making me jump through hoops on this particular project, so I switched to C; unfortunately it was a bit late in the game and, C being C, there was a lot of supporting code (like the module that was the original subject of this post) that needed to be written, so, time crunch. Now that I know I'm going to be working in C, yeah, valgrind up. It's also normally a problem that I would put a way for a few hours (and margaritas) and then come back to, and immediately see the issue, but the time crunch was too severe on this one.
All -- I reposted the code, even though it ended up being irrelevant to the posted issue.
Yeah, like I said, I know the feeling. Good luck! 
Benchmarks under a second are far from useless.
Thanks so much for the help guys. I submitted my assignment. Question 2 was still abit messed up. If anyone wants to see the code tho. Send me message 
Thanks so much for the help. I submitted the assignment but Question 2 was still abit messed up. If anyone wants to see the code tho. Send me message 
Terminal is all the (right) one! 
Netmasks would have probably confused the students even more. Besides, think of this assignment as being written in the late 80s, when network addresses were still classified as A (what is now a /8), B (/16) and C (/24). 
As a followup, I just ran VCG and got what look like good (technically bad) results. Even if it is old, it doesn't appear to be worthless although, perhaps, incomplete.
I've used Coverity before. It has it's issues, but it's another option that is more up to date
This is literally my day job :) First step - make sure your compiler is working for you. Make sure the build is using -Wall (all warnings enabled) and clean up the output of that. While you're at it, follow up with some source fortification options, although those are platform dependent. Next, hit it with open source tools. Valgrind will do memory leak and performance analysis. Clang's scan-build tools will do static code analysis. Doing the above will have you 90% of the way there. If you want to take it a step farther, you will be getting into the realm of commercial analyzers. Coverity, Fortify, that sort of thing. Anything scan-build finds are definite bugs - these take it up a notch to finding potential bugs or bugs that might be revealed if something changes.
I have no experience with VCG but I just want to say that a tool with documentation that hasn't been touched in 6 years doesn't necessarily sound like a bad thing (especially compared to other languages) given that the C language advances very slowly. The vast majority of code out there is either using C99 or C89/90 which is very old. 
I’ll talk at a high level and hopefully someone can give you specific calls for Linux (on Windows Vista+ you could create a thread and assign it to the “Pro Audio” class using the Multimedia Class Scheduler Service — I don’t know off hand how you’d do it in Linux, but the ideas are the same). Basically, you’ll want a very high priority thread to grab the keyboard buffers and push your audio data to the audio output device/buffer/whatever. That processing needs to be fast: when you’re in the real-time class, you’ll need to be quick or you can end causing hitches in your streams, that kind of thing. You could also end up blocking further keyboard inputs, etc, depending on how many cores you have available. Realistically, you’ll have SOME delay, but it can be minimized to fractions of milliseconds if done right. Now, real-time for a “toy” project can be off by 5-10ms and be ok. If you’re trying to be true, you may need to hook into driver level or that sort of thing. On Linux, you may want multiple processes, for example, where on Windows you may want multiple threads within a single process. I wish I could give you more specifics, but I’d suggest googling real-time processing in Linux and go down that rabbit hole. If it’s just something you’re doing for experimentation, make it work, even with a delay, then work on fixing up the real-time aspect. I prefer to solve one problem at a time.
In the standard C library there's `clock`, and the `CLOCKS_PER_SEC` macro which is an implementation defined value, may be large enough for your needs depending on the platforms you're targetting. otherwise you'd be looking at other libraries, POSIX and the like, or even higher-level libraries built on top of them.
96000th of a second is close enough to a microsecond that it might as well be called that. I don't believe the linux or windows kernel offers time accuracy below 10 usecs (when sleeping)... so you may need to use the hpt timers in linux. I would look in to using an i/o scheduler with support for hpt timers, personally. 
I'm nowhere near as knowledgable as AssKoala, but sadly audio programming is nothing like that. Interrupts are nowhere near predictable enough to work on a sample at a time, and it ends up being really processor inefficient to work in such small batches. Instead, you'll want to write to a buffer in an audio callback function of some sort, probably around 128 samples or more. Jack seems to be the realtime Linux sound server of choice for synths and such, but you could probably work with PulseAudio or ALSA instead. The nice thing is, you don't have to worry about that timing. The callback function will be called whenever the sound server needs more data. The most important rule: Don't block on the audio callback. So, no malloc, or any blocking system call in that function. You'll probably want to keep input handling and most logic outside of the audio callback. That can be a tricky process to get right, but it should be doable with pthread_mutex_trylock, or atomic operations if you want to get fancy. I have no direct experience with this, so I could be totally off base, but I hope this helps as a starting point!
On Windows you should use `QueryPerformanceCounter` and on POSIX use `clock_gettime`.
`Wall` does not enable *all* warnings though. I usually at least use `-Wextra -pedantic`, too -- sometimes even some more specific flags.
[removed]
Another tool is CppCheck. It's not dynamic but can spot useful things that some compilers miss.
I had never used scan\-build before, thanks for mentioning it.
&gt; I need our development team to perform security code analysis as part of their release process run your product at max stress and observe max memory utilization usually it's more than you think it will be as dev. never put restraints on the memory they're gonna use (like we'll use 20% of system's RAM and defer tasks when we reach the limit) the symptom of not doing this is when you go to production oom killer kills your process other than that run valgrind for undefined behaviour detects (not for memory leaks as it's the wrost tool for finding those) and maybe asan for stack issues if any and maybe run the free coverity scan by uploading ur src to github as nobody can afford that overpriced tool
https://linux.die.net/man/2/nanosleep
That's horribly misinformed. POSIX provides `clock_gettime` which allows you to specify which clock (real time clock, CPU time clock, monotonous clock, etc) you want to use. You don't have to set up anything.
That's not really how audio works. You usually have a fixed bitrate and just send a stream of samples to the audio device. Your sound card turns this stream of samples into music. Look up the ALSA interface for details.
This is exactly what I had been trying to do. You can pipe the samples to a command to play audio. The issue is, by program runs too quickly and sends too many samples at once. Hence, why I need to slow it down. Also, I love the Covox Speech Thing. Do you know if it was ever meant for voice synthesis?
&gt; Also, I love the Covox Speech Thing. Do you know if it was ever meant for voice synthesis? I think it was meant as a dirt-cheap computer sound card. I actually have a [Covox clone](https://www.serdashop.com/CVX4) on my 80286, works like a charm.
This is important. My current project is using `-Wall -Wextra -Werror -pedantic`
That’s not exactly what I said. I said he could write to the device/buffer/whatever. Basically, that could be ALSA or whatever else. But what you’re suggesting is what I said when a little delay might be ok — I didn’t know what OP was trying to do. You write into that buffer and it makes it to the device some time, maybe a millisecond maybe 5, whatever. It’s probably more than ok for OP, but I want to specify what I was bringing up as an issue. In games, you usually have a 30-60ms delay between input and events being drawn on screen. That’s acceptable for games. For video playback, you might buffer ahead 500ms and that’s fine. For professional audio, a 10ms delay is inexcusable. If you’re writing out to the audio stream from data that came in from the keyboard (which is also buffered), you’re going to compound the delay by quite a bit. If it’s a toy program (press a key, get audio), that’s fine. A 60ms delay? Who cares. If it’s a “pro” type program, yeah that matters. 
Yep, Wextra is also a good one, forgot about it while writing my comment - I use it on all builds. I'm not a fan of pedantic though - it's a bit *too* pedantic in my opinion.
I don't think you get that much delay with ALSA. There is really not much going on between the library call and the driver putting the samples in the sample buffer. I think the delay is negligible.
People downvoting, he does have a point, I worded that poorly. Valgrind is an entire suite of tools - one of them is a memory analysis tool which can find things like leaks, use of uninitialized memory, and a number of other memory related problems. This was what I meant - it was poor wording on my part to just limit that to leaks. But that's just the default tool. It can do a bunch of other neat things by using other tools in the Valgrind suite. For example, the one I use most often is actually Callgrind (and to a lesser extent Cachegrind) for doing performance analysis.
Sure for the call itself, but the delay between the user pressing the keyboard, your process being scheduled, and getting around to writing out to ALSA can add up quick. Linux quanta’s are usually around .75ms to 6ms, so if OP’s process isn’t properly setup, let’s say three other things are running, the delay could be say 18ms before you even get scheduled if you’re just at normal priority. By the time you write out to the device and playback begins could be another 5ms, let’s say, and now you’re 23ms delay from press to play. That’s what I’m talking about when i say real-time processing. Where milliseconds matter. 
If the system is under so much load that the scheduler can't schedule you immediately, sure, that could happen. That's not really the typical case for a desktop computer and not really a case I would optimize a realtime application for. However, as far as I'm concerned, as long as resources are free, the scheduler pretty much immediately schedules your process once you receive data from the X server. No waiting for the next time slice to begin. I could be wrong about this, though. Sure, you can ask the kernel to put you in a realtime scheduling class, but that needs elevated privileges as far as I know.
That’s not heavy load. You could be under 20% load and still be scheduled behind some other process. And you absolutely schedule a real-time process around that — usually by having a simple high priority thread that responds to inputs immediately above others and the rest of the threads run normal to handle regular tasks. Maybe it’s fine most of the time, but every so often you get a hitch, not really acceptable if you’re “real-time”.
&gt; supports part of your point Sigh http://www.linuxprogrammingblog.com/using-valgrind-to-debug-memory-leaks this is the last time I'm doing "work" for you. valgrind really really sucks for practical mem leak detection this i say from personal experience take it or leave it
&gt; What tools (prefer free) are better at finding leaks? writing your own allocator or using an allocator that supports detailed memory profiling with call stack (eg. tcmalloc, jemalloc etc.) is the only way. basically u need an allocator that enables you to have reports like these: https://dirtysalt.github.io/html/images/gperftools-cpu-profier-0.gif 
Hi, A tip, you can write: char \*buf = malloc\(cursor\); if\(buf == NULL\) { fclose\(input\); return NULL; } as: char\* buf = NULL; since the realloc will behave as malloc if the ptr is a NULL pointer. Besides that, not on every system, will malloc\(0\) return a pointer. [https://linux.die.net/man/3/malloc](https://linux.die.net/man/3/malloc)
&gt;I'm not a programmer but I work with C developers. I'm a security engineer Out of interest: What does your job entail, if you're not a programmer?
A lot of those checks would be better performed at the source code level. It's a shame compiler writers only care about benchmarks and not the integrity of the code we write.
&gt;or using an allocator that supports detailed memory profiling with call stack \(eg. tcmalloc, jemalloc etc.\) Thanks. I've used those before, but before I had access to valgrind. I found those "kludgy", but I will accept that they're better/more thorough than valgrind. 
&gt;this is the last time I'm doing "work" for you. First, I guess I should be clear. I'm not trying to argue, I'm wanting to understand your statement that: \&gt; valgrind is the worst "leak" finding tool out there Your first link seemed \(to me\) to indicate that valgrind is fine at detecting memory leaks, while its usefulness surpasses that one feature. Now, your second supporting link clearly states about valgrind \&gt; I don't know a better tool to find memory leaks. which clearly contradicts your opening statement. So, I think my "work" is that valgrind is still one of several fine tools for finding memory leaks, whose limitations need to be understood. 
Learn how to write functions, it's quite difficult and confusing at first but you'll get to it.
Brain dump (mostly C tools): * Compile with multiple compilers - they all produce different warnings which will help find more issues. * Most compilers provide a bunch of command line flags for hardening produced binaries, which can reduce security issues with various trade-offs. There's a rundown of some of the options GCC gives here - https://developers.redhat.com/blog/2018/03/21/compiler-and-linker-flags-gcc/ and with a bit of googling you'll find similar lists for other compilers/platforms. * Static analysis (catch bugs at compile time), eg. https://clang-analyzer.llvm.org/ * Dynamic analysis (catch bugs at runtime/during testing), eg. [TIS Interpreter](https://trust-in-soft.com/tis-interpreter/) - ([GitHub](https://github.com/TrustInSoft/tis-interpreter)) / Clang's sanitizers (ctrl-f here https://clang.llvm.org/docs/index.html) * Fuzzers (fire "random" input at the program and watch it break), eg. http://lcamtuf.coredump.cx/afl/ * Code review/automated test suites/etc - this is hopefully part of the development procedure anyway, but is a great place to start on process-based security. http://sqlite.org/testing.html is a great place to look for ideas, and is far more involved than most software goes. If you go much beyond the last bullet point, you start entering the realm of safety critical systems, and would be looking at integrating formal processes and formally verifying the security properties of the software. I can talk more about that, but it's almost certainly overkill.
If you're doing open source development don't do Werror by default it's generally a pain for people not using the exact same compiler version as you.
I work in cyber security, a part of which requires meeting regulatory compliance requirements. One of those requirements is that development work goes through code security reviews. When I asked our dev team how they meet this, they go like this ¯\_(ツ)_/¯ and expect me to hand them an answer. So I figured I'd reach out to developers who actually have tackled this problem.
Please reupload or edit the code with the correct spaces, tabs and line breaks. This makes it a lot easier to read and understand the code. Perhaps open it with notepad++ It seems like you have to use a for loop to go through each element of the arrays, adding them up, and then divide with the total, but i've only skimmed the code in its current form
That was the "source fortification options" I mentioned. For example, -D_FORTIFY_SOURCE=2 will enable some strong memory protections under GCC at the cost of performance. -fstack-protector is in the same boat, enabling stack canaries, which provide a defense against stack smashing attacks at the cost of a bit of overhead on function calls.
&gt; If you go much beyond the last bullet point, you start entering the realm of safety critical systems, and would be looking at integrating formal processes and formally verifying the security properties of the software. I can talk more about that, but it's almost certainly overkill. OP, if you want your dev team to start conspiring to murder you, just tell them you're going to start enforcing the MISRA standards on your code :D
&gt; Have you tried the ALSA library? Haha I feel like an idiot. I'll give it a try. Thanks!
look, I don't want to be 'that' guy, but you should really look into it. You can't expect that some stranger on the internet will be handling your interview questions in the future. Try to analyze what you need to do and ask relevant and specific questions. Don't just copy/paste your whole exercise. Otherwise you are just waiting your time.
Your instructor has almost given you the `showTypeAverage` function in `categorize`. The `categorize` function is already counting how many parts there are per type. All you need to do is add some logic to increment the total overall value which your instructor has _already_ provided for you in `partType.typeSum`. So this: if (arr[i].code[0] == arrType[j].type) { arrType[j].quantity++; break; } Becomes something like this: if (arr[i].code[0] == arrType[j].type) { arrType[j].quantity++; arrType[j].typeSum += arr[i].value; break; } You'll also need to initialize `typeSum` to the value of the first part you encounter for a given type (where your instructor sets `.quantity = 1`). 
Then it's okay of course, although I prefer to write software portable, you never know. Also testing for bugs is usually easier because you can just switch toolchains.
Did that help? Hopefully it's easier to read now.
The issue with that is that I'm not writing a synth per se, I'm writing a method for those who write their own synth using a particular method to be able to test and playback their designs directly from the keyboard rather than writing to a file or relying on a program like Audacity.
In comparison to PortAudio, I've found that it's been much easier to jump into and get started with PortAudio, because all of the information on how to do so is readily available. I also don't want to risk learning a spacific library and relying on it, only to have it fade into obscurity. 
&gt;I also don't want to risk learning a spacific library and relying on it, only to have it fade into obscurity. Frankly, that's why I prefer with libsoundio. The code of libsoundio is much smaller (10k vs 50k), newer (means no legacy cruft or obscure audio backends), well documented, tested and easy to understand. This means to get into maintaining and extending your backend of interest (ALSA/PulseAudio/Jack in your case) for any person would require much less time compared to if one wanted to do the same for PortAudio.
No. Every instrument is compiled in the systems native language, it will be used for much more than audio playback and recording. What I'm writing is more like a generic set of tools useful for analog signal generation and manipulation. It can be applied more broadly to anything that can be interpreted as a wave. This particular application simply makes it easier to see if something actually works or not. I plan on creating more tools that can be used to apply Fourier analysis to a given function, making it hopefully easier to reproduce a complex signal. 
Do you mind sending me a link to the programming manual?
I did a similar thing back in HS for my Java project. Was definitely more interesting as an assignment than simply writing a text adventure game (since I'm bad at creating stories).
Read the example code.
Prepend each line of code with 4 spaces so that Reddit recognizes it as such.
I can second this. A couple of days ago I was looking to solve a similar problem and had it integrated into my project in under an hour. Very well documented and easy to use. Before that I tried to use libsoundio and found it was much more confusing and had very sparse documentation - perhaps due to the fact it hasn't been around too long.
i already know python. so i don't *need* OOP in C. can you explain why darkness awaits me?
Has anyone here used frama-c?
The error is probably in code you aren't showing us, not here. Judging from the error output, you're passing a function where a float is expected, so check if you didn't write the name of a function instead of the name of a variable.
Mean is a function but it is a float function. `float mean(float* array, int number_of_elements)` This is the mean function. 
It returns a float. I guess you are writing variance(ages, 42, mean); mean is a function, not a float. You could change it to variance(ages, 42, mean(ages,42)); or, more clearly float averageAge = mean(array,42); variance(array,42,averageAge);
Mean is another function. I think this is the problem. How do I call one function to another function. Also the assignment has 2 lists that it is doing at a time so how do I make it call the correct mean for the correct variance?
OK. All you've described is legit as far as I can see, so the error is somewhere else. Is there a change you could provide the line that actually fails? (The compiler probably says "error on line XX" or something like that.)
I fixed the problem. It was because I was calling it wrong but now I have a problem that says: `big_statistics_function.c:81: error: expected declaration specifiers or` `‘...’` `before ‘mean’` `big_statistics_function.c:132: error: too many arguments to function` `‘variance’` `big_statistics_function.c: At top level:` `big_statistics_function.c:222: error: expected declaration specifiers or` `‘...’ before ‘mean’` `big_statistics_function.c: In function ‘variance’:` `big_statistics_function.c:244: error: invalid operands to binary - (have` `‘float’ and ‘float (*)(float *, int)’)` `big_statistics_function.c:245: error: invalid operands to binary - (have` `‘float’ and ‘float (*)(float *, int)’)` Line 81: ` mean(array,number_of_elements));` 3rd parameter of the variance function Line 132: `variance_list1 =` `variance(list1_input_value,number_of_elements,mean_list1);` Line 222: It is the same problem as line 81 Line 244-245: ` variance_numerator += (array[element] - mean) *` `(array[element] - mean);`
I fixed the problem. It was because I was calling it wrong but now I have a problem that says: `big_statistics_function.c:81: error: expected declaration specifiers or` `‘...’` `before ‘mean’` `big_statistics_function.c:132: error: too many arguments to function` `‘variance’` `big_statistics_function.c: At top level:` `big_statistics_function.c:222: error: expected declaration specifiers or` `‘...’ before ‘mean’` `big_statistics_function.c: In function ‘variance’:` `big_statistics_function.c:244: error: invalid operands to binary - (have` `‘float’ and ‘float (*)(float *, int)’)` `big_statistics_function.c:245: error: invalid operands to binary - (have` `‘float’ and ‘float (*)(float *, int)’)` Line 81: ` mean(array,number_of_elements));` 3rd parameter of the variance function Line 132: `variance_list1 =` `variance(list1_input_value,number_of_elements,mean_list1);` Line 222: It is the same problem as line 81 Line 244-245: ` variance_numerator += (array[element] - mean) *` `(array[element] - mean);`
Having just the errors without the code is tricky. So is having the code without the errors. But at the end it is the same bug big_statistics_function.c:245: error: invalid operands to binary -(have ‘float’ and ‘float (*)(float *, int)’ variance_numerator += (array[element] - mean) * (array[element] - mean); mean is a function (`float(*)(float*,int)`) not a float.
What I want to be there is either mean_list1 or mean_list2. How do I write the function so that it takes the correct value? Also what do I put for line 81. It wants something before mean(array,number_of_elements). Do i put float, int, float * or is it something else?
If line 81 exists by itself, and is not a part of another line, then the problem is that you have two closing parentheses. Otherwise the code seems to be correct.
I solved the issue. I was making it harder than it had to be. Thank you for helping me though it really helps for somebody like me who doesn't know much about coding.
 $ cat audio.au &gt; /dev/dsp
I just use things like Maxima and Pari/GP for math stuff instead of trying to reinvent the wheel in C.
From the documentation &gt; If buffer is NULL, the invalid parameter handler is invoked. If execution is allowed to continue, the function returns -1 and errno is set to EINVAL. Is `edb1` NULL?
I believe, don't quote me on this, that the solution to this hypothetical is: In your main function, just create a new thread for the filter_thread function, and then that thread actually does all the work, while your main function uses some loops to handles all the input, because technically main is already running in a thread. 
I haven't used pthreads, but I *have* done a similar thing in OpenMP. I basically had a conditional based on the thread ID so that the correct code was being called. One thread was doing input, another was doing output, and a third was doing the necessary calculations. [The code in question starts at line 142](https://github.com/xxc3nsoredxx/gol/blob/master/src/main.c). It's not the best code, but it works (at least on my machine).
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [xxc3nsoredxx/gol/.../**main.c** (master → 3dc95db)](https://github.com/xxc3nsoredxx/gol/blob/3dc95db4ee69cd9bd7c84c5640fbdf815cb46004/src/main.c) ---- 
With a library like OpenSSL, LibreSSL, GNU TLS, etc.
edb1 is a parameter in the function signature. void load_edbs(uint8_t *edb1, uint8_t *edb2) The following is at the start of my main function. cdatabase, e1database, and e2database all have static scope so that other functions can reference them. // Get memory for the database. if (((cdatabase=(uint8_t*)malloc(C_DB_SIZE)) == NULL) || ((e1database=(uint8_t*)malloc(E_DB_SIZE)) == NULL) || ((e2database=(uint8_t*)malloc(E_DB_SIZE)) == NULL)) { perror("Not enough memory for database.\n"); exit(1); } // Load pattern databases and initialize turns. load_cdb(cdatabase); load_edbs(e1database, e2database); It doesn't appear to be undefined. I also don't know how it can be undefined on one machine, but not another. Here is the entire source code if you want to look at it. [IDAstar.c](https://github.com/AdamHayse/optimal-solve-rubikscube/blob/master/IDAstar.c) is currently spaghetti code, but I think [edatabase.c](https://github.com/AdamHayse/optimal-solve-rubikscube/blob/master/edatabase.c) is a little better.
How many times does it successfully read before giving an error? What is remain at that point?
Here is a picture where I print out all of the values: https://i.imgur.com/kmYV3n3.png
Maybe https://github.com/Z3Prover/z3
What's very strange is that I do this same exact thing previously, and nothing goes wrong there: remain = C_DB_SIZE; while ((amount=read(fd, cdb+C_DB_SIZE-remain, 1048576)) != 0) { if (amount == -1) { perror("Problem reading corners.patdb"); exit(1); } remain -= amount; }
Okay, that rules out one idea I had but gives me a new one. Completely untested, hypothetical musing: The debug version of the read() function looks at the pointer to the buffer, sees it's to a spot inside a malloc'ed section of memory, and knows how much space is available until the end of that allocated memory. It sees that the number of bytes requested to read is greater than that size, and raises an error. Try changing the number of bytes to read to the minimum of that number and remain? Also end the loop when remain is 0 (Which is related to my first idea of it raising an error because the pointer is outside of allocated memory).
Is the size of the file larger than your constant? If it is then you're going to blow past the end of the buffer and then all sorts of bad things will happen. read on Windows converts \n to \r\n so you can read more than there seems to be
Yes. And try not to implement your own key management either.
I'm reading a file that is essentially random bytes. The program that uses the file interprets meaning from it. From my understanding, the read system call attempts to read an amount of bytes from a file. It returns the number of bytes read, so I don't see what the problem is here. If the cursor in the file is at SEEK_END, then performing read on it should simply return 0 and no harm is done. 
but is it bigger than the buffer? And check that it's opened in binary mode
Yay. My next suggestion was since they seem to be big files to just memory map them with mmap() and whatever the Win32 equivalent is.
jedwardsol's answer was the right answer. Windows was inserting carriage returns because my file, which is binary, was being read as text. Inserting the O_BINARY flag completely removes this problem.
Don't worry about what people call "trivial"; it's almost more often used because the author is trying to be modest (i.e. to say it is nontrivial when others find it trivial might be embarrassing) than any other reason. In addition, this is simpler in higher level languages. Your input code is very complicated. You probably want to check out [strtok](http://www.cplusplus.com/reference/cstring/strtok/). To make it easier to manage you might consider making a function to read a single line from stdin and return a struct with your cup information.
I only skimmed your code, didn't analyze it much. This line immediately stands out as wrong: buf2 = realloc(buf2, sizeof(char) + sizeof(buf2)); This is equivalent to buf2 = realloc(buf2, 1 + sizeof (char *)); (Note that sizeof (char)==1 by definition.) C does not keep track of the size of your dynamically allocated arrays. You need to do that yourself in a separate variable, and calculate the correct size to give to malloc/realloc/calloc. You appear to be keeping track of buffer size with your variables "a" and "b" (but give them better names). So calculate the buffer size you need and give that to realloc. Also, you did not null-terminate the strings you read before calling strcpy() on them, which means that strcpy() will read past the end of your buffer until it sees a '\0' or your program crashes when it tries to access memory it's not supposed to.
So then something like buf2 = realloc(buf2, 1 + buf2size); would work? The null terminating part makes sense. My bad there! Thanks. 
&gt; this is simpler in higher level languages. However, it's not as fun as in C :) I'm checking out `strtok` right now, didn't even know that existed. Thanks.
How should I go about making the buffer be immune to overflowing? It's not significant for this program in particular but I'd rather know than not. I ran valgrind and seem to be getting errors for a conditional jump or move that depends on an uninitialized value. However, the code does work. I searched but couldn't find a solution for when using `realloc`. I updated my code on my post. Could you help by any chance?
Depends on the situation. If truncating overlong strings is fine, one way is to use something like strncpy() to copy a maximum amount of characters (be careful though, since strncpy() does not null-terminate if the source string is too long - read the documentation for details). A better way in general is to not use fixed-size buffers for variable-length content. You could use a `char *target;` and do `target=malloc(size);` before copying `size` bytes into `target`, or just `target=buffer;` depending on what needs to be done with `target` and `buffer` later.
That would be the producer consumer pattern. Easiest thing to do is couple a semaphore and a mutex to a shared list/vector/whatever. Your main thread would spawn the “consumer thread” then wait for input. Whenever it receives new input, it would lock the mutex, push the new data, unlock and call the semaphore post. The “consumer” simply sits around waiting on the semaphore. Once it’s triggered, it also has the lock, it can pull the next bit of data, unlock, process then go back to waiting on the semaphore. Google producer consumer pattern and marvel at all the different ways to handle this. You can kill the consumer thread in multiple ways, but it’s probably easiest to just have a book that tells it to exit that you set then fire off the semaphore. Once it’s woken and dead, you can exit. Or just call exit and let the OS handle it, whatever.
Take a look at [JACK](http://jackaudio.org/files/docs/html/index.html).
yea i have looked into those but i couldnt wrap my mind around how to use those . could i get some help as to the implementation or a link perhaps?
it clearly gives the disadvantages i guess u are probably a bit slow read the sections like "Not exactly a memory leak" also u realize that valgrind slows ur process by 20 times or more? that should be enough for any significant program to not use it for mem leak detection as mem leak usually occur after stressing the product for a significant time and it's impossible to "stress" your algorithms with valgrind sigh
If you are dead set on this, treat the pointer to the malloc'ed area as a unsigned character buffer. Then do some basic array math to get a pointer into this memory. You would then cast that char pointer to whatever you wanted the variable to be. unsigned char* ptr = mallocedBuffer[4096 * i]; // i being multiple offset of 4096 into the array.
Shouldn't that be mallocedbuffer + 4096 * i? Why the dereference?
You can just compute the address. char *buff = malloc(4096*10); The third write will be at the address `buff + 4096*2`. How you actually do the write depends on exactly what you want to write. You assign the address to a pointer to desired type and use an assignment statement. E.g. int *buffPtr = buff + 4096*2; *buffPtr = 42; Note, this approach _works_ because 4096 is a power of two - which means the addresses you compute will be aligned correctly. If this wasn't the case, you'd need to use `memcpy` if you were writing anything more than a single byte. 
I've never had reason to do this, but it should be possible to use the sizeof macro. Say, for a type T, loop index i, and a pointer to the desired location ptr, ptr[i * 4096/sizeof(T)] = x; Again, I haven't tried this, and I'd recommend verifying it on a smaller region of memory. Windows300's method is sure to work. If you need to zero all the memory (I'm not sure why you'd be writing data spaced out like that and not want the rest zeroed), you might want to look into using memset and memcpy.
Are you asking how to read a file encoded in utf-16?
[https://stackoverflow.com/questions/14601430/how\-to\-run\-a\-c\-program\-in\-bash\-script\-and\-give\-it\-2\-arguments](https://stackoverflow.com/questions/14601430/how-to-run-a-c-program-in-bash-script-and-give-it-2-arguments) you mean this right?
Reading any input while also gracefully handling errors in that input is a non-trivial task. If you're allowed to use POSIX.1-2008 extensions you might want to look into [`getline(3)`](http://pubs.opengroup.org/onlinepubs/9699919799/functions/getline.html) or the [`'m'` assignment-allocation extension of `scanf`](http://pubs.opengroup.org/onlinepubs/9699919799/functions/scanf.html) to simplify memory allocation. But the description imposes some restrictions on the input, so IMHO it's fair game to follow the *garbage in, garbage out* directive and just assume that the input follows the rules. Then `scanf`, `qsort` and `printf` are all you need.
I've looked at OpenAL, both for a game but also did some experiments with real time mixing, fairly flexible, API seems stable (its been around a bit) and didn't have any issues with it...
if you typedef a struct thats 4096 bytes in size.... " #include &lt;stdio.h&gt; #include &lt;stdlib.h&gt; typedef struct _blob { char b[4096]; } blob; void main(void) // cause ppl love it when you do this.... { blob* ptr = (blob*)malloc(sizeof(blob)*10); blob* oldPtr = ptr; for (int i=0; i&lt;10; i++) { printf("ptr=%lu\n", ptr); ptr = ptr + 1; } free(oldPtr); } "
Your code has a few small problems, namely: - getLine() suffers from buffer overflow attacks, readling a line that is longer than LIMIT causes the program to write to undefined memory outside of the string, which is dangerous. To fix that, put the condition "&amp;&amp; counter &lt; LIMIT" into the while loop. Alternatively, you could use the fgets function from stdio.h - expand() doesn't really do what it should do. I haven't tested the code yet, but the task is that the string "ab0-5x" should be turned into "ab012345x", and your code does not include the character copying of "a", "b", and "x". A more accurate approach would be: Is s1[i + 1] == '-'? If so, s1[i] is the start character and s1[i + 2] is the end character. Then copy these characters into s2 and increment i by 2. OTHERWISE: copy s1[i] to s2.
I am asking how to pass a launch argument as an char\_16t
Not really. I know how to run C program inside a shell script. Look on my code. The c variable is defined inside a code. I want it to be defined by a launch parameter, or at least by a stdin input.
Ehm... move the `printf` to after the `perror` or save and restore `errno` before invoking `perror`. `printf` might trash `errno`, so the error message you get is random.
I am confused, you are not very clear, because i think that is what i am saying in the second part right? &gt; To pass argument to a c program use the argc and argv arguments from main. Just keep in mind, the first argument, argv\[0\] is used for something else, can't remember what, so when you pass an argument it will be stored in argv\[1\]. 
The reference documentation should help you using the libraries. https://gnutls.org/reference/index.html https://www.libressl.org
Correct, or put a `&amp;` in front.
This exercise got my interest for some reason... Maybe because I haven't done one of these in a while? Anyway, [here's my solution](https://pastebin.com/49SNxQpn). I just sketched this up so take it for what it is: a quick solution from the top of my head. It could be useful to see what other approaches could be used. Note: I didn't use scanf to get the strings because I wanted to avoid input buffer overflows, which scanf doesn't check for.
Thank you. I was aware of the buffer. It looks like my code didnt save. As for ab0-5x turning into ab012345x, I must have misunderstood what was asked in the question. a-z0-9 works fine. But my logic doesn't work according to your interpretation of the question. I'll redo this.
Good luck and have fun coding!
There is a char16\_t c variable and I need to work on it. char, as used for launch arguments isn't wide enough to store a character I want to work on. Is there any way to pass an argument AS a char16\_t data type, not casual char.
Okay, this is a difficult thing. First: The input you get with `fgets()` or similar is in an implementation-defined encoding (sent in the encoding used by the terminal emulator) which needs to be multibyte if such a character is entered. To convert a multibyte character to a "wide character of fixed 16 bit length" `char16_t` you can use the `mbrto16c()` function, eg: #include &lt;stdlib.h&gt; #include &lt;stdio.h&gt; #include &lt;stdint.h&gt; #include &lt;inttypes.h&gt; #include &lt;errno.h&gt; #include &lt;locale.h&gt; #include &lt;uchar.h&gt; // (void) is different from () and creates a prototype int main(void) { // use LC_CTYPE from user environment setlocale(LC_CTYPE, ""); // read bytes of input, which will be encoded as a multibyte sequence char buf[64]; if (!fgets(buf, sizeof (buf), stdin)) { fprintf(stderr, "Error reading in\n"); return (1); } // the type to save our 16-bit encoded character char16_t c16; // zero-initialized mbstate, eg. for saving whether we are in a // surrogate pair, or in a specific shift state etc. mbstate_t mbs = { 0 }; // mbrtoc16 converts to an *implementation-defined* encoding. This // happens to be UTF-16 for me on Linux/glibc, at least when setting // LC_CTYPE to *.UTF-8 for (size_t n, i = 0; (n = mbrtoc16(&amp;c16, &amp;buf[i], sizeof (buf) - i, &amp;mbs));) { if (n == (size_t)-3) { continue; } // surrogate pair if (n == (size_t)-2) { fprintf(stderr, "Incomplete multibyte char\n"); break; } if (n == (size_t)-1) { perror("mbrtoc16"); break; } printf("U+%.4"PRIx16"\n", c16); i += n; } } `mbrtoc16()` converts the multibyte character string into single wide characters of the corresponding encoding. Apparently glibc chooses UTF-16 to be the "corresponding" 16-bit encoding to the multibyte UTF-8 encoding. If one chose to use `mbrtoc32()` instead, it converts to UTF-8 which is reasonable. However, it's not *that* simple. One reason: One UTF-16 "character" doesn't correspond necessarily to one "character" printed, eg. when surrogate pairs are used.
argv is not just a char, it is a char array, or in other words a string. i think what i said combined with /u/a4qbfb should be sufficient. Even though i don't see why something like bash couldn't do this, but i am not really familiair with bash.
&gt; argv is not just a char, it is a char array, or in other words a string No, it is an array of pointers to null-terminated strings. 
Oops, yes you are completly right, even though i wrote in my code \*\*argv, in my head i had \*argv. 
Hey, rowesca1, just a quick heads-up: **completly** is actually spelled **completely**. You can remember it by **ends with -ely**. Have a nice day! ^^^^The ^^^^parent ^^^^commenter ^^^^can ^^^^reply ^^^^with ^^^^'delete' ^^^^to ^^^^delete ^^^^this ^^^^comment.
Thank you!!!!
Thank you!!!!
Thank you!!!!
Also, I believe your compare result is inverted. The function should return negative if the first parameter is smaller than the second, positive if the first is greater, and zero if both are equal.
&gt; How should I go about making the buffer be immune to overflowing? The overrun possibility is not a property of the buffer, but of how your code uses it. Put simply, your code does not overflow a buffer `buf` of capacity `N` if all access to the buffer is within the bounds of the buffer (i.e. at offset `i` where 0 &lt;= `i` &lt; N). 
Thank u exactly what i needed. 
&gt; OpenMP isn't openMP dead? 
Why are you even calling `read` in a loop? Your input buffer is big enough, just read the whole file into it in one call. Short reads happen on (e.g.) network sockets, but not on disk files.
This is a performance issue rather than a correctness issue, but if you're repeatedly reallocating a buffer, it's generally considered data structures best practice to double its size each time (4-8-16-32...). realloc is a fairly expensive operation, and you want to minimize the number of times you do it. You will waste a small amount of memory, but memory is relatively cheap, and copy operations are expensive. Also, a realloc note. For just playing around learning programming it's not a big deal, but `var = realloc(var, size)` is actually a potential memory leak. realloc can fail and return NULL, and if that happens, you'll leak the old address since you didn't save it to another variable. It's a somewhat academic point (if it fails things are likely so screwed up there's no fixing it), but my day job is in C security programming and analysis, so I couldn't just let it go :)
Generally, when people handle currency amounts they use integers which represent the actual amount multiplied by 10 to the power of number of significant decimals (usually 3, or maybe 5). Eg, if you want 2 significant decimals, your `19.99` amount becomes `1999`. The way you represent this to the end users needs to take this into account though. This is done, to avoid weridness that can arise from floating numbers operations.
 dollar = (int)GST; This looks fine cents = dollar - GST; `cents` is declared as an `int`, which means it can't hold the decimal part of `GST`. You could try cents = (int)((dollar - GST)*100);
I was playing around with these recently. With glibc, mbrtoc16() generates surrogate pairs, but c16rtomb() treats them as errors; there's a C11 defect report about it that they haven't implemented yet. Annoying.
Wow. That is amazing. I definitely need to find out more about methods you've used. Sadly, there are no info on them on cppreference. Could you please elaborate on printf function syntax? What is PRIx16?
I've been living a lie
Hm, interesting, I need to try that. Thanks for the heads-up regarding the test-macros, completely forgot about them; I've adjusted my comment.
Give it a try and you might find out it makes everything easier. :)
He's right, handling floating point is not an easy task, you can have an idea of what it means here: https://docs.oracle.com/cd/E19957-01/806-3568/ncg_goldberg.html . Also, the modf* functions from math.h might help you
I can't open that on mobile, bloody popup add shows up 2 seconds after it loads. 
I don't know what to tell you, I'm on mobile and it opens up just fine, with no pop ups...
Try this, although it will have no highlighting: https://pastebin.com/raw/49SNxQpn
Yes, it is.
Yeah I'm not going to do your homework for you. 
You got this assignment today and you already gave up. Congrats. You probably should think about dropping out of Purdue if you can't handle this. I'm sure they wouldn't be happy to know someone is cheating on their homework.
Looks like I did it: http://codepad.org/8Uhg6YXV If I input `ab0-5x` as per your suggestion we get `ab0-5x`. If I input `-----abc-z--0-9--` as per your suggestion I get `-----abc-z--0-9--`. Thanks for helping me understand the question more clearly.
I suppose my critique is more of style, rather than the algorithm or details in memory security. 1. Comments! You don't have a single comment in the given code. While it is a simple enough example, try to make it a habit to document what you are doing as you go. 2. The null byte '\0' is equal to zero on the ASCII table, so on line 53, rather than having (s1[i] != '\0'), you can have (s1[i] != 0) or simply (s1[i]) for the case expression. 3. On line 40, you don't use braces for the while-loop, while on line 53, you do use braces for the other while loop. Try to be consistent in how you write your code. 4. I'm not super familiar with switch-case structures but you don't have a 'break' for the case '-': given on line 56. 5. Naming variables, so you have good descriptive variables on line 50 and 51, I can read that and see that one is a start character while the other is the end. Well on the two lines just above, you have an 'i' and 'j' variables. I have no clue what those are unless I read past that and figure it out myself. 
"Modern OpenGL" being C++. Post this in /r/cpp, not here.
Tbf, it's trivial to take his Skelton and write it in c. I got my toy game engine to the point of shadows and lighting (diffusion, etc...) by following c++ code examples. 
While jumping into a loop can *occasionally* be useful (See Duff's Device), it's usually a bad decision. I'd probably rewrite this to move most of the logic into another function that can then be called a couple of times.
Thanks, I replaced ``for`` with ``while(! timer_thread_exit)`` as per your suggestion, one less goto. &gt; possibility that ``wait`` ever points to a deallocated struct You just fixed a bug x) Now ``wait`` is a struct and is being used ``memcpy()`` to copy.
You shouldn't need memcpy to copy one struct into another - assignment should suffice.
Don't use the goto in C. Ever. 
Well, this guy knows what he's talking about: https://youtu.be/FGAWniPGKjc?t=70
Seems there's a hole in my understanding of structs. Thanks!
its c99 using modern OpenGL which is a C API...
You can put a test in to confirm that the struct is the same size as you originally created it, but that falls apart if the memory alignment changes (unless you pack the struct and eat the potential performance hit). But there comes a point where you have to stop caring. If a malicious actor wants to break your code they will find a way to do it. Do a reasonable job, comment and document, and then leave it in the hands of your favorite god to watch over.
When someone deliberately overwrites a const member she should expect havoc.
I'm not using the prepocessor to separate the test code. Instead, I'm using conditional compilation and so the production data won't be compiled for a test build. I don't see how I can incorporate this kind of memory test without adding explicit test code to the production code and compiling both sets of data. Please tell me if I missed something here.
1. Habit of commenting a code is bad. It's better to write a self-documented code by using more readable syntax and naming convention. 2. It's better to use (s1[i] != '\0') condition. It suggests to the reader that s1[i] contains a char value. 3. Yes, it's good to be consistent. A good habit is to use braces even around a single line body of a loop. It's more error resistant while editing a code. 4. I think the switch statement is superfluous here. ;)
After giving this one a [go myself](https://gist.github.com/kahless62003/a402387a659e7c67c7eeef7210d82de9), using it as an exercise to figure out a few more advanced ways of doing things, I'd say it is as non-trivial as the programmer makes it. It could be done with mostly basic methods and a lot less error checking. My attempt still has a lot of cruft and unneeded printf's from the development process which might be useful as an insight as to what the program is doing at any particular time, and whether things were correctly read-in and stored before trying to process them. I started putting it together with basic file handling with a fixed file name, then changed it to read from stdin so a different file could be re-directed to the program in the shell.
It’s quite reasonable for unwinding on error and breaking out of multiple levels of loop.
I don't know about the specific Python binding situation you are dealing with, but in general you can't (portably) make assumptions about the layout of a `va_list` like this. You need to call `va_start`, then read each element with `va_arg`, and finally call `va_end`. Also, the preprocessor does not allocate memory for use in the program like this (being a preprocessor). `va_list` is conceptually an interface for reading arguments directly from the stack. The way this actually works depends on the architecture, among other things, since calling conventions differ.
No it isn't. You unwind errors and break out of multiple levels of loops by modularizing your code. 
Your abhorrence doesn't validate your opinion. Maintainability is *never* enhanced with a goto. The code degrades into a spaghetti-like mess. Even Tom Duff doesn't use a goto in his device: https://www.lysator.liu.se/c/duffs-device.html#duffs-device
&gt; unfounded religious beliefs Wow, you really put me in my place. 
We are all put in places on this blessed day.
I don't really understand where the problem lies. If it is unit testing, you should test only your unit, i.e. your function, and not depend on other data, you should have control of all inputs inside your test. If you are testing `func()`, just have several structure variables in your test file and call `func()` with argument pointing to the appropriate one for each test. FILE_CFG_STR true_input = {true}; FILE_CFG_STR false_input = {false}; // test 1 : saveAtShutdown true =&gt; branch taken func(&amp;true_input, ...) // test 2 : saveAtShutdown false =&gt; branch not taken func(&amp;false_input, ...) But I have the feeling I am missing something. Is there a problem/limitation with the testing tool/framework?
Can you arrange to generate the test data from the original data? One way is to add a script to your build process that takes OriginalData.c and creates TestData.c as part of building your tests. Other approaches might involve using the pre-procssor in OriginalData.c so that it produces different data when compiled for production vs testing. You might want to take a look at [X Macros](https://en.wikipedia.org/wiki/X_Macro) for some clever ideas on how this can be done. 
**X Macro** X Macros are a technique for reliable maintenance of parallel lists, of code or data, whose corresponding items must appear in the same order. They are most useful where at least some of the lists cannot be composed by indexing, such as compile time. Examples of such lists particularly include initialization of arrays, in concert with declarations of enumeration constants and function prototypes, generation of statement sequences and switch arms, etc. Usage of X Macros dates back to 1960's. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/C_Programming/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
I completely disagree. While they should be avoided where possible, there are legitimate use cases. My only use cases are multi-for breakout and error handling, but I am sure there are others.
Those aren't legitimate use cases. Every goto can be replaced with proper modularization. 
If you care more about dancing around the use of `goto` than making straightforward logic, I agree.
I've been on mobile, so haven't been able to find evidence to support my position. First, I generally agree with the avoidance of goto. It can *usually* be avoided with more maintainable code. One example I can think of off of the top of my head is resource cleanup. The idea is something like (pardon the pseudo-code): if (! allocate_resource()) { report_error(); goto done; } if (! open_resource()) { report_error(); goto deallocate_resource; } if (! uise_resource()) { report_error(); goto finish_resource(); } finish_resource: close_resource(); deallocate_resource: free_resource(); done: return; } This is something that RAII in C++ would make cleaner. This eliminates a lot of boilerplate NULL checks. It *could* be refactored into nested function calls, but that sometimes spreads the logic too much. Again, I'm not saying that goto is ever the *best* solution to a problem, but sometimes (infrequently) it's part of a *good* solution. 
Username checks out: https://i.imgur.com/RXqupiE.jpg
Well met. 
I care about readable maintainable code. Obviously I have not danced around anything. If you're still not clear, here it is once more: *never use a goto in C*. 
So you've created two code paths to land on the label. A disaster waiting to happen. I have logic to add after the loop: do I put it above the label or after the label? Can I goto the label from above the loop? 
Your code is not a good solution. You have code paths to land on deallocate_resource. You have three code paths to land on done. 
There is clearly a bit of dogma here, but others have correctly pointed out that there are good uses of goto in leiu of RAII in c. By good, I mean more maintainable by having source code map more directly to your logical flow chart. Resource cleanup based on progress into an operation is a very obvious case where goto leads to less code duplication, nesting, carried return values/conditional flags. In most cases it can be avoided unless you have other bad dogma that prevents early returns / fail fast behavior. Duff's device is interesting but generally not needed much. A good role of thumb is to avoid using goto, and be extra skeptical if you goto a label that is above the goto statement. Also, as a c/c++ programmer, avoid dogma.
How do we handle va_args type safely? (and yes I know that sounds stupid) 
What do you mean by “handl[ing] `va_args` type safely”? From the C side of things, as long as you `va_start` and `va_end` it properly, there’s not a whole lot else you /need to|can/ worry about. (Arguably `va_copy`, but you’ll mostly be either using a `va_list` directly or passing it in as a function argument.) From the Python side of things, I’d probably avoid using varargs entirely and deal with Python lists/tuples instead, even if that handoff requires an extra (Python) wrapper function. Varargs functions are lovely syntactically, but they’re mostly unpleasant to use even without considering multiple languages’ interactions.
&gt; This can be difficult to do with functions that access global variables or perform I/O. It's hard to set up the appropriate conditions for the test, and hard to capture the results to check whether they're what you expected. In order to do it, you end up having to duplicate large chunks of your program. Why? Globals are just extra inputs in addition to the function parameters, and extra outputs in addition to the function return value and the function pointed parameters. You set them as you wish before calling the function, and you check them after the function is done. 
&gt; `va_arg` can pull things out as `char`/`short`/`float`, but under the hood it’s pulling out the promoted form and casting it to the requested type. Not true. My stdarg.h man page says: &gt; If there is no actual next argument, or if type is not compatible with the type of the actual next argument (as promoted according to the default argument promotions), the behavior is undefined, except for the following cases: ... And none of "the following cases" cover things like `char` vs. `int`. Calling `va_arg()` with an underpromoted `type` is undefined behaviour, full stop.
"dogma" is whatever you disagree with. I explained why the resource cleanup concept is a mess when implemented with gotos.
&gt; If you're still not clear, here it is once more: never use a goto in C. If you're still not clear, here it is once more: There are legitimate use cases for goto in C.
No you didn't, you just provided an alternative "modularize your code"
va_list is accessed with va_start, va_arg, va_copy and va_end. No other way. If you actually want to know how it's actually implemented you need to find the ABI document for the particular system you're on. But judging from how pretty much every sentence you wrote has some error or misunderstanding I suspect you're not ready for it.
Are conditionals bad? Because if statements lead to two code paths "landing on" the same label (not real label, just closing bracket of conditional).
wouldn't i then have to deal with deletion of nodes once the function has returned? 
Thank you. Although I'm not sure how passing `quotient`would help. What would that accomplish and where would it passed?
May be wrong but we just learned about this in class so I believe if you create your destructor it will automatically be called when it goes out of scope
You're copying the parameter "n" into "quotient", not using n later on. Thus, you're wasting one word of stack space. When you're writing recursive functions, a recursion depth of 10000 would lead to 32k-64k overhead on the stack.
That's the idea. Like I said, it's an okay solution to how to have one block of cleanup code without a bunch of NULL tests. Nominally, the code just flows through. But, if a critical error happens, only the needed cleanup happens. It's really the same number of paths that would be required using if branches in that block. As I said, this is one of the problems that C++ solves with RAII and exceptions. It could be done differently in C, but I still contend that this solution is acceptable in some cases. 
I think he means "why are you not calling that first argument quotient instead of n?"
Exactly. Thank you.
A destructor would work if OP is using C++ ... but this is the C programming reddit, so I assume OP is using C (and C doesn't have destructors). In that case, yes OP, you'd have to free your memory when you're finished.
You obviously have no logical points remaining. Have a nice day.
If course I did. That's the point. My architecture is superior to yours. 
Conditionals are not bad (in C) because you can't get to the closing bracket from potentially *any* other point in the function, as you could if you employ gotos. 
You're out of arguments. Have a nice day.
We will agree to disagree. I can live with that. 
You can certainly do it that way, if you want to. Making all the inputs into parameters just makes them easier to keep track of. For example, if you forget to set one of your global variables, your tests might fail, and there won't be any clear indication why until you step through them in a debugger. If you forget a parameter to a function, on the other hand, you'll get a compile error and know right away what your mistake was.
In regards to you, yes, I am out of arguments. Have a pleasant day. I do expect at least one more reply from you, likely so you can feel like you got the last word.
"any c code that contains goto" is not an architecture. Either way, don't tell my boss, he might offer you my job!
Yes, that is exactly what I meant. Ok, how about we get exotic... Let's say I have an enum of all the basic types (integers, floats, strings, etc) Can I make a macro to generate an array containing all of the arguments to the function's type, and pass that in as an additional parameter?
Happy Cake Day! ^You ^can ^participate ^in ^r/HappyCakeDayClub ^until ^midnight!
Same here
I know that `typeof` exists (in various formats depending on the compiler) and I believe it's compile time, not run time, but how would you switch based on it's type? IDK what the typeof equals in order to do the comparison.
`strtol`
I don't need the last word because I have the truth on my side.
I can barely do my own job. 😉
 int bin[4]; bin[0] = 1; bin[1] = 1; bin[2] = 1; bin[3] = 1; int dec = 0; for (int i = 0; i &lt; 4; i++) { // XOR with a rotate binary to the i position dec = dec^(bin[i] &lt;&lt; i); } int bin2 = 0b1111; int dec2 = 0; int count = 0; while (bin2 != 0) { dec2 += ((bin2 &amp; 0b1) &lt;&lt; count); count++; bin2 = (bin2 &gt;&gt; 1); } printf("example 1: %d\n", dec); printf("example 2: %d\n", dec2); or you could do what the other user suggested. Look into bit shifting if you plan on moving on to embedded systems, otherwise just use `strtol`
How can I find the length (number of digits) in an int array?
C has a boolean type. Use it; don't reinvent the wheel.
It would probably be easier to figure out that after converting it to decimal. Then continually divide by `10` until the number reaches `0` and your done.
Thanks, I was told by the professor to use this because C90 doesn't support boolean type natively but anyway I guess I'll include stdbool.h or whatever the right header is called next time. Apart from this, do you have any other suggestion?
There's more than one way to do it, but the most generic would be to input a null terminated char string. Then I believe what they want you do to do is parse the char string and convert it into an int. ie parse the 1's and 0's. The magic lies in the binary_string_to_int function. Remembering the base method in maths to convert ie place values and base 10 and go from right to left is one straight forward way, write out the pseudo code and work from there. If you're crazy, you could do something like this unsigned int count=0; // ugly global unsigned int binstr_2_uint(char* ch) { return (!ch[1]) ? (!(count=0)&amp;&amp;ch[0]=='1') : binstr_2_uint (ch+1) + ((ch[0]=='1'*1&lt;&lt;++count)); } Lots of ways, this is a fun one, but don't use it in your assignment unless you understand it and can explain it to your teacher(s)! It's also ugly because it has a global, but I couldn't figure out how to pass the place value back as the recursion unwound as c only returns one value on the stack. Posting this in the event some other clever person can enlighten me, lol
It's 2018. Limiting yourself to a 30 year old version of the language is dumb.
I'm aware of that and I'll remember this but I still want to pass my exam.
It's not a real test, just a homework. The code works, I tested it both on my machine and in the website, I was just looking for some reviews/improvements.
Yeah and comments would really help people give you ideas. It’s hard to read uncommented code so you are losing a lot of people from the get go.
Sorry I totally misunderstood your statement, english is not my first language lol. I thought I was missing reddit comments because people thought I was posting during a test. Anyway you're right but I have the bad habit to comment only when I'm happy with the code.
As far as code goes... If someone puts in more than 63 characters, I think you would go into quite the lengthy loop, since there would be no '\n'. So you might want to filter on '\0' as well (which is automatically added by fgets). I have never seen %zu before in a printf, and quick googling tells me its a C99 feature, so if you are lmited to C90, you might want to look into not using this? I think 'get_ball' should probably be named 'shuffle_ball', and if it were me writing the function I would probably swap the parameters around. Kinda picky, but I think it makes more sense logically.
why not read char one by one and allow having any length of input ? Also I would only keep the number of where the ball is. 
&gt;Also I would only keep the number of where the ball is. Actually, I think the way it is simplifies swapping the cups. Requires fewer conditionals.
[code](https://www.dropbox.com/s/0v2d75niddfzasj/main.c?dl=0) 
Better yet instead of using `int`, imho is treating `size_t` as `unsigned long`, thus `%lu`. It is *theoretically possible* for it to be bigger than that and the value stored too -- but it's unlikely, and in that case and `int` would have long ago failed. `int` is signed *and* not as big as `long`: Not really suitable to save a lenght/size in. `size_t` is a C90 feature, use it.
Thing is, I'm the one creating the matching data manually. To test that piece of code I have to create two different structs. Since it's const member I'm assigning it when the struct is initialized, with one having a FALSE value for saveAtShutdown and the other a TRUE. I'm wondering if I'm going about this the correct way
I'm using mock data when touching or dealing with global data. This function just like many others is only concerned with its supplied arguments. For safety reasons, the design dictates to use consts for some components' configuration. And I'd have different variation of each component, which is why we have that sort of condition: if(fileManagerCfg-&gt;saveAtShutdown == TRUE) { //do something }
My concern is with creating, documenting and maintaining this fake inputs. It would be much easier for example if I'm able to manipulate the original data for a specific test and then reset it, but since I'm dealing with consts, I couldn't find a way to do it without undefined behavior. 
This is a very interesting about auto generating test data. But since I have a lot of those structs, at least 20, I'm worried about diminishing returns. These structs look different and I'm worried it's too much work to figure out how to write this kind of script. Not to mention the script need to be able to parse the type of the member, which could add a lot of complexity. Regarding the pre-processor, as I mentioned I'd much prefer to stay away from it for this kind thing.
Good riddance.
First, codewise: I'd probably not use `enum` for `TRUE`/`FALSE` since that's not what C99 did, so if I want code that's C90 compliant, I'd do something like this: #if __STDC_VERSION__ &gt;= 199901L # include &lt;stdbool.h&gt; typedef bool bool_t; # define FALSE false # define TRUE true #else typedef char bool_t; # define FALSE 0 # define TRUE 1 #endif Next to the loop in `get_ball()` which is confusingly named: It loops forever if no newline is read which is totally possible, eg. if the input is terminated by flushing the buffer directly (press `[CTRL]+D`) or more than `BUFSIZE-` characters (excluding `\n` are read). But `fgets()` is a "nice" function that guarantees to `NUL`-terminate your string -- use that. Also you're programming totally around the spec: &gt; The fgets() function shall read bytes from stream into the array pointed to &gt; by s, until n−1 bytes are read, or a &lt;newline&gt; is read and transferred to s, &gt; or an end-of-file condition is encountered. The string is then terminated &gt; with a null byte. 1. with `n = BUFSIZE-1` only `BUFSIZE-2` bytes are read max! 2. within `in[0]` and `in[BUFSIZE-2] there's guaranteed to be a `\0` So first you change `BUFSIZE-1` to `BUFSIZE`, then in the loop in `get_ball()` you loop until `*s = '\0'`. --- But now to the IMHO more crucial part: Choosing appropriate data structures. That's the key to the solution of the problem. A good data structure should at least not encode data redundantly (if `cups[x] == TRUE` then it holds that for all `n != x`: `cups[n] == FALSE`). Additionally you read in a whole line, although you only need the current character. But let's first improve the former issue and just save the *current* position: int main(void) { // Yes, we want to start with 0, will be useful later enum POSITION { POS_L = 0, POS_M = 1, POS_R = 2, }; enum POSITION current = POS_L; /* TODO */ } We could now create some code that's gives us the new position only based on `current` and the kind of swap. To make this easy, we choose another data structure, a lookup table: int main(void) { enum POSITION { POS_L = 0, POS_M = 1, POS_R = 2, }; const enum POSITION tab[3][3] = { { POS_M, POS_L, POS_R }, { POS_L, POS_R, POS_M }, { POS_R, POS_M, POS_L }, }; enum POSITION current = POS_L; /* TODO */ } This table works as a matrix of combinations: | \\ | `'A'` | `'B'` | `'C'` | |-|-|-|-| | `POS_L` | `POS_M` | `POS_L` | `POS_R` | | `POS_M` | `POS_L` | `POS_R` | `POS_M` | | `POS_R` | `POS_R` | `POS_M` | `POS_L` | The idea is to pre-compute the actual logic. We can now index this table with `current` and a move in the range of `[0,2]` to get a new position. The only thing left is mapping the input to the numbers 0, 1 and 2: #include &lt;stdio.h&gt; int main(void) { enum POSITION { POS_L = 0, POS_M = 1, POS_R = 2, }; const enum POSITION tab[3][3] = { { POS_M, POS_L, POS_R }, { POS_L, POS_R, POS_M }, { POS_R, POS_M, POS_L }, }; enum POSITION current = POS_L; int err; while ((err = fgetc(stdin)) != EOF &amp;&amp; err != '\n') { unsigned char c = (unsigned char)err; if (c &gt;= 'A' &amp;&amp; c - 'A' &lt;= 2) { c = c - 'A'; } else if (c &gt;= 'a' &amp;&amp; c - 'a' &lt;= 2) { c = c - 'a'; } else { continue; } current = tab[current][c]; } printf("%u\n", current+1); } We simply loop til `EOF` is reached or a newline is read. We check that the characters are valid input and map them as described, using them as an index to our lookup table. Looking good? Let's improve again, from an algorithmic standpoint and let's write it as a matrix: | 2 1 3 | | 1 3 2 | = M | 3 2 1 | Seing a pattern? Well, if we denote with `j \in {0,1,2}` the column, and `i \in {0, 1, 2}` the row, every `M[i][j] = (3-j)+(2-i) %3`. You can observe the last row to be 3, 2, 1 and the row above to be the value of this row +1, modulo 3, the row above again. Let's write some code: #include &lt;stdio.h&gt; int main(void) { unsigned char current = 1; int err; while ((err = fgetc(stdin)) != EOF &amp;&amp; err != '\n') { unsigned char c = (unsigned char)err; if (c - 'A' &lt;= 2) { c = c - 'A'; } else if (c - 'a' &lt;= 2) { c = c - 'a'; } else { continue; } unsigned char a = 3 - c; // A -&gt; 3, B -&gt; 2, C -&gt; 1 unsigned char b = 3 - current; // 3 -&gt; 0, 2 -&gt; 1, 1 -&gt; 2 current = a+b % 3; } printf("%hhu\n", current); } Basically the same code as before, except for the `enum` being not necessary anymore as we only do plain arithmetic. We convert the move into the value that would be our `3-j` and our current position into `2-i` (we use `3 - current` because the position has values of `[1,3]` not `[0,2]`) and use the formula.
The representation of signed integers in C is implementation defined. The standard provides three options of which the implementation needs to choose exactly one. Signed integers can be represented as... 1. two's complement 2. one's complement (i.e. `-x == ~x`) 3. sign and magnitude (i.e. like floats but without an exponent) Historically, all three representations were used but computers quickly converged on representation (1) in the 90s. There are still some esoteric applications using two's complement and as far as I know, at least POSIX mandates that there be a C language environment with two's complement on conforming platforms.
by curiosity I implemented a solution keeping track of the ball position #include &lt;stdio.h&gt; #include &lt;stdlib.h&gt; /* move the ball if it is under one of the two swapped cups */ int swapCups ( const int current, const int a, const int b ) { if( current == a ) { return b;} if( current == b ) { return a;} return current; } /* move the ball according to the type of swap desired */ int readSwap ( const int swapType, const int current ) { switch( swapType ) { case 'A': return swapCups(current, 1, 2); case 'B': return swapCups(current, 2, 3); case 'C': return swapCups(current, 1, 3); } /* invalid move, do nothing */ return current; } int main( int argc, char **argv ) { int current = 1; /* ball position */ while ( EOF != ( int input = fgetc( stdin ) ) ) { current = readSwap( input, current ); } printf("%i\n", current); return EXIT_SUCCESS; } Again, keeping track of the ball seems more *natural* to me.
ELI5 for novice programmer please. 
Nothing changes. Some obsolete ways you could implement C that nobody ever used were removed. Nothing that matters for you.
That's great news!
You surely meant one's complement in the last sentence.
&gt; define signed overflow. I haven't read the draft proposal, but if it's 2 complement then overflow has a defined value, just like it does at the ISA level on all cpus.
No, I mean two's complement. Two's complement has `-x == ~x + 1` and is what most computers use nowadays. One's complement has `-x == ~x` and differentiates `+0` from `-0`. It's not really a common choice these days.
&gt; but computers quickly converged on representation (1) in the 90s. More like the 70s!
&gt; I haven't read the draft proposal, but if it's 2 complement then overflow has a defined value, just like it does at the ISA level on all cpus. Nope. For example, your CPU could still generate an exception on overflow (some IBM mainframes do). It could also use saturated addition (some DSPs do). Keeping signed overflow undefined is a good idea as it helps the compiler optimize your code and tools check your code for correctness. I don't remember any situation where I needed signed overflow to be defined and I can't imagine any good situation either.
(This may be a bit inaccurate because I'm a bit rusty on this) As you know, in a computer all numbers are strings of bits, so there's no clear way to represent the sign of a negative number. Thus, three forms were devised (in this historical order, I think). They all have in common that they reserve half of the possible numbers for negative number (those with the *leftmost* bit set to 1), but, of course, you can't have a cake and eat it. * sign and magnitude: you set the bit with the biggest weight (the *leftmost* one) to 1 when the number is negative, and the other bits are set to the absolute value of the number. For example, to represent -2 (using 8 bits), you would store 10000010. This may look like it makes sense, but addition of numbers of different sign is not straightforward (if the numbers have different sign you need a special operation), and there are two different zeros (positive and negative), which makes comparison with 0 awkward. * one's complement: you define a negative number as the positive number but negated bit-by-bit. Thus, -2 would be 11111101. This makes addition somewhat better (I think there was a rule about adding 1 if some part of it overflowed, but I don't remember the details), but there are still two zeros. * two's complement: You represent a negative number as the positive number, except you negate it and add 1 to it. Thus, -2 is 11111110. While this may seem the least intuitive one, it makes the addition straightforward (you just add as you would with any other number) and only one zero exists. And what this post says is that, finally, the C standard committee is forcing C compilers to use two's complement, which has been almost the only still existing scheme for many years. In fact, I have a vague memory (take it with a grain of salt) of someone from the original committee (1989) saying in an interview that even back then they weren't aware of any computer in use then implementing anything else, but they heard there was one that did, so they decided to make it implementation-defined.
Unless you're doing [RAII](https://eli.thegreenplace.net/2009/04/27/using-goto-for-error-handling-in-c), or doing [a very particular kind of error handling](https://deplinenoise.wordpress.com/2012/04/21/setjmp-goto-error-handling-bliss/). Otherwise yes, the common-wisdom is *don't use `goto`*, because except for the very, *very* limited use cases, the only thing that can come from `goto` is sadness.
&gt;converged on representation (1) in the 90s. There are still some esoteric applications using two's complement and as far as I know, They're referencing this. There is something misworded there it seems like. 
Oh yeah, note that especially x86 causes an exception when you get signed overflow during division (by dividing the least integer by `-1`). Do you really want to force the compiler to check for this case before every division so there is no undefined behaviour for signed overflow?
Ah, fixed. Thank you.
Thank you for your thorough explanation. 
&gt; Do you really want to force the compiler to check for this case before every division so there is no undefined behaviour for signed overflow? Yes! :) If I want fast idiv I'll call ______fast_idiv() or whatever.
I really don't want to sacrifice my performance for such an irrelevant edge case.
Then call ______fast_idiv() everywhere and properly document that you don't care/have already checked for bad inputs.
If I wanted a programming language were I had to declare every time whether I want something to be defined or not, I would get something like Rust and would get very insane in the process. There is a good reason why we want to have sane defaults.
&gt; There is a good reason why we want to have sane defaults. But yet you like UB? ¯\\_(ツ)_/¯
I have retrieved these for you _ _ *** ^^&amp;#32;To&amp;#32;prevent&amp;#32;anymore&amp;#32;lost&amp;#32;limbs&amp;#32;throughout&amp;#32;Reddit,&amp;#32;correctly&amp;#32;escape&amp;#32;the&amp;#32;arms&amp;#32;and&amp;#32;shoulders&amp;#32;by&amp;#32;typing&amp;#32;the&amp;#32;shrug&amp;#32;as&amp;#32;`¯\\\_(ツ)_/¯`&amp;#32;or&amp;#32;`¯\\\_(ツ)\_/¯` [^^Click&amp;#32;here&amp;#32;to&amp;#32;see&amp;#32;why&amp;#32;this&amp;#32;is&amp;#32;necessary](https://np.reddit.com/r/OutOfTheLoop/comments/3fbrg3/is_there_a_reason_why_the_arm_is_always_missing/ctn5gbf/)
The fact that CPUs do different things on overflow is a good reason not to define a specific overflow behavior, but it seems like not a good enough reason not to say the behavior is implementation-specified (which in practice would usually mean it does whatever your machine does).
THAT is fucking awesome!!! Maybe now they'll get on fixing the ancient error about the abs family of functions returning signed integers, causing an overflow...
Something being undefined is a good default if you are never going to do that intentionally. For example, many preprocessors warn you if you re-declare a macro. Of course you could do so intentionally, but more often than not it's an error and you can always use a little `#ifdef ... #undef` trick to get around the warning. Another point is that overflow being undefined is so damn useful for the compiler for optimizing your code that you really don't want to miss it.
If the behaviour is implementation-defined, the implementation must make sure that it always does the same thing, again hindering optimizations. For example, suppose you are on a computer where signed overflow in addition traps. If you declare this to be what your implementation does, you can suddenly no longer optimize code like `a + b - b` to just `a` because `a + b` might trap and a trap is an observable side effect. That's not at all helpful for the compiler.
Take a look at the [StringIO](https://github.com/bumblebritches57/FoundationIO/blob/b4e8bd3034e0c662533af3edf9d619658122ace7/libFoundationIO/src/StringIO.c) module in my library, FoundationIO. t's not 100% complete, but it'll get you to where you need to go.
Uh, what? No, UTF-16 has 2 byte order variants, but it's not at all "locale dependent". Are you referring to normalization?
&gt; Something being undefined is a good default if you are never going to do that intentionally. But the problem is that UB is often so easy to hit unintentionally. Security flaws aren't usually "intentional", and it's a shame that the C compiler writer and standard committee are hostile towards anything other than benchmark driven optimisations.
definitely, it also makes it possible to do some more algorithmic optimizations / simplifications, as I commented here: https://reddit.com/r/C_Programming/comments/8f87y9/what_could_i_have_done_better/dy1svap?context=3
Thank you!
I think you are going about it the wrong way and are generally overthinking it. Question though, you say you are using "conditional compilation" but not the preprocessor. What exactly do you mean? if you mean #ifdef and such, that is the preprocessor. Also, why not use the preprocessor? If the only change whether the flag is set or not, do something like: #ifdef DEBUG #define SAVE false #else #define SAVE true #endif FILE_CFG_STR foo = {1, 3, "name", SAVE, "bar"}; One set of data, conditional compilation switches between test and production, etc. but, having read your edit now... nope, you aren't missing anything obvious you just did everything in a way that all of the data needs to be maintained. It does seem like your test data is overly reliant on your production data though; they should be isolated from each other. Really, we don't care if production data is changed because test data is simply testing code paths and edge cases. So back to my original advice and don't worry so much about it. Create valid test data to test the code, not to test production. Comment the hell out of it and leave it to the gods.
thank you for the idea. 
but I managed to add to usr/include all header files. asm/checksum.h had include only, which I added as well. So I was expecting it would work.
Most of them did, but remember we're talking about C here. If any language is going to be supported on niche devices where different conventions might apply, it's C.
Again: the functions defined in the kernel are not available for userland programs. They are not part of any library. If you want to use a function defined in the kernel, you have to copy the function definition (i.e. the source file where the function is defined) into your project as there is no way to link against functions in the kernel.
C++ is off topic in this subreddit. Please post C++ questions to /r/cpp_questions.
By conditional compilation I mean that my makefile will determine which files to compile based on the build target, something like this: ifeq ($(MAKECMDGOALS),test) SRC := $(filter-out $(COMMON_ROOT)/src/Configurations/componentFoo_cfg.c, $(SRC)) SRC += $(wildcard $(COMMON_ROOT)/src/FakeConfigurations/*.c) endif Regarding the preprocessor, I've often used it for this kind of purpose before, but with this big project it's just not scalable enough for me. I think it's like you said, I should just pray to the gods to have mercy on my tests. 
... wait I thought C/C++ had already been using two’s complement for signed integers for decades. What are they using instead?
Ambiguity.
when you say negate it and add 1, you mean to flip the bits and add one? 00000010 -&gt; 11111101 -&gt; 11111110?
&gt; I also hate whoever is in the "undefined behaviour group", as they clearly like this kind of caveat ridden crap. Just forget everything that isn't x86 and arm and get on with it. Making wrap on overflow defined behaviour is also a problem on x86 and ARM, because it requires the compiler to insert overflow checks after every arithmetic operation, which can be a huge performance penalty. Suppose you have a loop like this: for (char i = 3; i != 2; ++i) { some_array[i] = some_function(); } The compiler may chose to store i in a register that is wider than 8 bits. With defined overflow it would have to transform the loop into the equivalent of: for (char i = 3; i != 2; i = (i + 1) &amp; 0xFF) { some_array[i] = some_function(); } And this does not apply to to just loop variables, but all arithmetic you do on types with a bit width less than the native register size. 
Yep, exactly that.
This is just standardizing the values for true and false. non-zero will still evaluate as logically true in conditionals. I.e. If (Foo) ... Will be taken for non zero values, while if(Foo == true) will be taken only if Foo is 1, which happens to be the value of true. I think.
[MbedTLS](https://tls.mbed.org/openssl-alternative) is also an option.
That's what I thought too, and I'm just as curious 
From what I read online, the bytes read can't be larger than SSIZE_MAX. Also, I'm struggling to understand how to make this compatible both on my ubuntu laptop and my windows desktop. Windows seems to not allow reading more bytes than are left in the file, and Ubuntu allows you to specify a number of bytes larger than the remaining file because it will only read until the EOF. Also, the O_BINARY flag doesn't seem to exist in the compiler I used on Ubuntu.
O_BINARY does not exist because there is no difference between binary and text files on UNIX systems.
By any chance do you know how to make it compatible? Whether it's somehow disabling Windows adding carriage returns or using some kind of preprocessor directive to check if it is Windows or Linux?
The architectures you've been running have largely all been two's complement, but the language spec itself avoided acknowledging for a long time to remain compatible with alternatives.
I'm not sure how read works on Windows but I'll have a look in a few minutes.
You need to find out what is different between the files. If you're using a unix-like system, you can use `cmp` to do this. Or you can do a hex dump using `xxd` and then use `diff`. Once you know the reason for the difference you can fix your program.
If you're not sure, don't worry about it. I'll google it later. I'm heading off school. Thanks for the info.
Sounds like you're not writing out the last byte of the decompressed file, or skipping the last byte when compressing.
I'd say the [formatted printing](http://git.musl-libc.org/cgit/musl/tree/src/stdio/vfprintf.c#n540) is more confusing to read. I found it out while researching on what [format it would use for `%p`](https://www.reddit.com/r/C_Programming/comments/8d5xid/how_does_printf_format_pointers/dxkjov4/) (It seems to be the same as `%x`).
Did it help at all, i'd like to try it too!
not yet. there are just too much -I fields I need to add (imports of other imports). I did find one header file with just one import with the function I wanted so I thought it would work but still does not. The error is not that it did not found all header files, it just does not compile with me using gcc.
The basic point seems to be comparing a word at a time, which is faster than comparing each individual byte. The first loop is for (; (uintptr_t)s % ALIGN; s++) if (!*s) return s-a; This compares individual bytes until the `s` pointer is aligned to the machine's word size. If a byte is zero, it returns the length which is `s - a`. Now it can start comparing a word at a time: for (w = (const void *)s; !HASZERO(*w); w++); `HASZERO` just checks whether the word contains any bytes that are zero. One increment of `w` is equal to one word sized number of bytes compared to zero. for (s = (const void *)w; *s; s++); Since the last word had a zero in it, the position of that zero in the word needs to be determined, so it reverts again to comparing individual bytes from where `w` left off, checking whether they are zero. Now `s` points to the null byte, so `return s-a;` just returns the number of bytes between the start and the null byte.
I've always liked these kinds of bit tricks. Let's unpack it a bit: #define ALIGN (sizeof(size_t)) ... for (; (uintptr_t)s % ALIGN; s++) if (!*s) return s-a; This loop *aligns* the pointer to a multiple of `ALIGN` (`sizeof(size_t)`, the word size). That's because the next loop is going to read the string a word at a time. Unaligned reads are either slow, unpredictable, or illegal, depending on the architecture. for (w = (const void *)s; !HASZERO(*w); w++); Here we're looping over the input string word-by-word. We stop as soon as any of the words contains a zero byte (`HASZERO`). How does that macro work? Let's unpack it too: #define ONES ((size_t)-1/UCHAR_MAX) This computes `0xFFFFFFFF.../255 == 0x01010101...`. This pattern occurs for similar reasons to why 1/9 = 0.1111... #define HIGHS (ONES * (UCHAR_MAX/2+1)) `HIGHS == ONES * 128 == ONES &lt;&lt; 7 == 0x80808080...`. A single bit set in the highest position of each byte, rather than the lowest. #define HASZERO(x) ((x)-ONES &amp; ~(x) &amp; HIGHS) That's a bit dense. We compute `x - ONES`, `~x &amp; HIGHS`, and mask them together. Why? Basically, we're trying to detect where borrows occurred in the subtraction, to tell us where any zero bytes are. Remember `ONES` is `0x01010101`. If you think about the subtraction happening byte-by-byte (i.e. in base 256), the only places where a borrow can occur is where a byte is zero. Think about it: `0xFF7701 - 0x010101 == 0xFE7600`, but `0xFF7700 - 0x010101 == 0xFE75FF`. In the latter case, the last byte has had its highest bit go from 0 to 1. We can detect this by finding all the bytes that didn't have their highest bit set before, but do after the subtraction. `HIGHS == 0x80808080...` is a mask of each byte's highest bit, so `~x &amp; HIGHS` find the bytes that originally had it unset -- it will be `0x008080` for our example. Then we mask them together -- `0xFE75FF &amp; 0x008080 == 0x000080` -- to find bytes that have it set now. Since this has a nonzero bit, there must have been a zero byte in the original word. Finally, the last line simply loops to find which byte it was: for (s = (const void *)w; *s; s++); And we return the length, the difference between the current pointer and the original start of the string: return s-a;
 if (0) { case 'o': a = fmt_o(arg.i, z); Ugh
 #define ALIGN (sizeof(size_t)) #define ONES ((size_t)-1/UCHAR_MAX) #define HIGHS (ONES * (UCHAR_MAX/2+1)) #define HASZERO(x) ((x)-ONES &amp; ~(x) &amp; HIGHS) size_t strlen(const char *s) { const char *a = s; const size_t *w; for (; (uintptr_t)s % ALIGN; s++) if (!*s) return s-a; for (w = (const void *)s; !HASZERO(*w); w++); for (s = (const void *)w; *s; s++); return s-a; } I am unconvinced this is really faster than a stupid byte per byte processing. This macro implies a number of arithmetic and logic operation that have the potential to stall the pipeline, each of them depending on the result of the previous one. gcc -O3 # strlen.c:15: for (w = (const void *)s; !HASZERO(*w); w++); movq (%rdi), %rax # MEM[(const size_t *)s_41], _23 movabsq $-72340172838076673, %rsi #, tmp139 movabsq $-9187201950435737472, %rcx #, tmp143 leaq (%rax,%rsi), %rdx #, tmp138 notq %rax # tmp140 andq %rdx, %rax # tmp138, tmp141 testq %rcx, %rax # tmp143, tmp141 jne .L19 #, .p2align 4,,10 .p2align 3 .L8: # strlen.c:15: for (w = (const void *)s; !HASZERO(*w); w++); addq $8, %rdi #, s movq (%rdi), %rax # MEM[base: w_26, offset: 0B], _7 leaq (%rax,%rsi), %rdx #, tmp144 notq %rax # tmp146 andq %rdx, %rax # tmp144, tmp147 testq %rcx, %rax # tmp149, tmp147 je .L8 #, The loop body is `L8`. 1. increment RDI (=s) 2. loads [RDI] into RAX *(needs new RDI from previous op)* 3. adds RAX and RSI, puts the result into RDX *(needs new RAX from previous op)* 4. invert RAX *(needs new RAX from 2 ops ago)* 5. 'and' RAX and RDX, puts the result into RAX *(needs new RAX from previous op, and RDX from 2 ops ago)* 6. 'and' RCX and RAX *(needs new RAX from previous op)* (and then once you have found a matching HASZERO, you reprocess again the same last word with the simple stupid method.) compared to the simple: .L10: # strlen.c:16: for (s = (const void *)w; *s; s++); addq $1, %rdi #, s .L19: cmpb $0, (%rdi) #,* s jne .L10 #, 1. increment RDI (=s) 2. is [RDI] zero *(needs new RDI from previous op)* (It should not trigger an external memory read for each byte, the long memory word should be read once and available.) Maybe for 64-bit systems (like this one) it is worth it, but I doubt it is true for 32-bit systems. But I guess that they have made real benchmarking, unlike I :-) 
&gt; The binary representation of a Unicode character is system- and locale-dependent. OK, tell me exactly which part of “the binary representation of a Unicode character is system- and locale-dependent” is wrong?
&gt;But I guess that they have made real benchmarking, unlike I :-) http://www.etalabs.net/compare_libcs.html
A nit: it's **ones'** and **two's**. Yes, I know it's confusing.
Unisys still make ones'-complement computers, although AFAICT from their documentation, their toolchain targets C89, so it's safe to assume they won't care.
Wow, TIL. Never thought of that possibility. In Spanish they're both singular, but admittedly neither name really makes sense.
Ethernet frames use a 32-bit CRC. TCP and UDP use a ones' complement 16-bit sum. The residual error rate for that is through the roof, so be happy that it's usually wrapped in something (like Ethernet) with better error detection.
&gt;So I'm trying to solve some problems from open.kattis, I didn't know that website and I think it's a nice place to learn something. &gt;My issue is about this exercise: https://open.kattis.com/problems/trik . &gt;I'd like you to critique my code and tell me how should I improve it. &gt; &gt; &gt; #include &lt;stdio.h&gt; &gt; #include &lt;stdlib.h&gt; &gt; #define BUFSIZE 64 &gt; typedef enum {FALSE, TRUE} bool_t; &gt; &gt; &gt; void get_ball(const char *s, bool_t *cups) &gt; { &gt; bool_t tmp; struct { boot_t **in; bool_t **out; } tbl = { { &amp;cups[1], &amp;cups[0] }, { &amp;cups[2], &amp;cups[1] }, { &amp;cups[2], &amp;cups[0] } }; &gt; while(*s != '\n') &gt; { int i = *s - 'A'; tmp = *tbl[i].in; *tbl[i].in = *tbl[i].out; *tbl[i].out = tmp; &gt; s++; &gt; } &gt; } &gt; There's nothing very different about your if statements. 
Wat. Was your comment generated by a poorly trained Markov chain, or do you literally not know anything about anything you just said? For starters: O(n/8) = O(n).
Yeah, nice invention of the Go developers. Pascal developers would never have known how to store the length of a string.
Cheers.
Pascal still lacks lambdas, what a joke.
...
glibc's strlen is doing something right.
Thanks. Well, this is an horrible "bench": #define BUFLEN 500000 size_t b_string_strlen(void *dummy) { char *buf = malloc(BUFLEN); size_t i; size_t cs = 0; memset(buf, 'a', BUFLEN-1); buf[BUFLEN-1] = 0; for (i=0; i&lt;100; i++) { buf[i] = '0'+i%8; cs += strlen(buf); } free(buf); return cs; } * it only tests for 1 length (unless I missed something obvious), * that one length is extremely long and is absolutely not representative of any real-use case, * (it also measures `malloc()`, `memset()` and `free()` time, but let's assume that it is not significant, considering the huge strings to process 100 times.) Despite having the same algorithm as GLibc, he ends up twice slower than GLibc according to his result table. Weird... GLibc loop body looks like this (still gcc -O3): # strlen-glibc.c:78: longword = *longword_ptr++; leaq 8(%rdi), %rdx #, longword_ptr movq -8(%rdx), %rax # MEM[base: longword_ptr_63, offset: -8B], longword # strlen-glibc.c:80: if (((longword - lomagic) &amp; ~longword &amp; himagic) != 0) leaq (%rax,%r8), %rcx #, tmp161 notq %rax # tmp163 andq %rcx, %rax # tmp161, tmp164 testq %rsi, %rax # tmp166, tmp164 je .L8 #, So that's basically the same loop. ---- DietLibc offers the two algorithms, because the simple one is better for short strings. I don't why it scores so bad in his results table. .L9: # strlen-dietlibc.c:42: word = *((word_t const *) t); t += sizeof word; movq %rsi, %rdi # t, s .L7: # strlen-dietlibc.c:42: word = *((word_t const *) t); t += sizeof word; movq (%rdi), %rdx # MEM[base: t_15, offset: 0B], word leaq 8(%rdi), %rsi #, t # strlen-dietlibc.c:43: word = (word - magic) &amp;~ word; leaq (%rdx,%r8), %rax #, _7 notq %rdx # _8 andq %rdx, %rax # _8, word # strlen-dietlibc.c:45: } while ((word == 0)); andq %rcx, %rax # tmp143, word je .L9 #, Again, pretty much the same. A tiny bit longer. ---------- Oh, now I see that he doesn't compile his benches with `-O ` but with `-Os`. Since the 3 algorithms are not written the same way in the 3 C source files, maybe they give very different results with less optimisation. Well, I am a bit too lazy to do the work again... :-)
Find something you are interested in and program it!
Everything you said is correct. `Foo == true` will convert the `bool` value of 1 to the type of `Foo`, so you effectively get the same thing as `if ( Foo == 1 )`. When converting any type to `bool`, on the other hand, it's as if you wrote `bool b = Foo != 0;`
What are you talking about? `bool` values can only be true or false. `if(Foo)` is exactly the same as `if(Foo == true)`. 
Saw you answer and when you posted it, although I did not comment, I definitely upvote your post, the best I could come with was removing the `switch` `case`. Your explanations of you solution is of very high quality. 
Thank you very much!
I would use a 3x3 array to store the coefficients for your system of equations. Then it wouldn't matter how the coefficients got into the array whether they were entered by the user or if they were loaded from a file. You would have two functions - one to prompt the user for the coefficients and one to load the coefficients from the file. Either way, the coefficients will be put into the same array.
There are multiple "binary representations" of a "unicode character", what do you even mean by "unicode character"? a UTF-32 (aka decoded) codepoint, a string of UTF-8 encoded code units that make up a single code point, a UTF-16 (big or little endian) string of code units that make up a codepoint? and ASCII is the format that is locale dependent, not Unicode. Ā aka U+100 is ALWAYS U+100 (or it could be U+0041 U+0304 in the decomposed form). Unicode is complicated, there's a LOT of moving parts, but none of them involve localization. What part of it is wrong? Literally all of it, you don't have the first clue about what you're talking about.
By "launch argument" I assume you mean from the command line/terminal an argument passed to the program? For that, you need to find out what OS your program is running on as a programmer (or set it as a user) and then pass in the unicode data you're trying to, in the correct transformation format (UTF-8 vs UTF-16).
So you got a homework assignment but couldn't be bothered to crack the books?
 scanf("%d",&amp;x); scanf("%d",&amp;n); x = x * 10; int cont = 1,total = 0; while(cont &lt;= n){ total += x + n; cont++; }
I think your approach in terms of asking for help is wrong. However, I am feeling super generous tonight, and will give you the code you are looking for. Please read my comments to make sure you understand it. Cheers mate.#include &lt;stdio.h&gt; int main() { // Declaration of variables: // Value holds what input we get from user. Power holds what power the user wants. // Counter is used to help us exit the wihle loop, and the sums are used for the math. int value=0, power=0, counter=0, sum=1, sum2=0; // Ask user for stuff, and get stuff back. Maybe put in some error checking here. puts("Enter a value you want me to use for the sum calculation."); scanf("%d", &amp;value); puts("What power do you want me to sum it up to?"); scanf("%d", &amp;power); // Disney Land while (counter &lt; power) { sum *= value; // Another way to write this is sum = sum * value. sum2 += sum; // Another way to write this is sum2 = sum2 + sum; //******** LEFT IN FOR ERROR CHECKING. UNCOMMENT THE PRINTF'S TO USE THEM.********* //printf("Sum is currently %d \n", sum); //printf("Sum is currently %d \n", sum2); counter++; // Incramentation of counter which helps us leave the loop. } // Final print statement to output data. printf("The sum of %d to the power of %d is: %d \n", value, power, sum2); return 0; } 
Were you purposefully trying to mess with the guy?
Note that musl only provides pretty much stub interfaces for wide characters, only UTF-8 is encoded and this is independent from your set locale.
The magic is in the HASZERO macro that « just checks whether the word contains any bytes that are zero ».I know how it works, but is much more black magic than the 3 loops.
&gt; We have tools to detect instances of undefined behaviour And it's a shame those tool aren't the compiler, which everyone has access to. Free static analysis tools suck. And the paid for ones suck jut as much, but you've paid for them so feel obliged to listen to their nonsense. (e.g. klockwork) &gt;so making “never intentionally used” behaviour undefined instead of defined allows these tools to loudly fail your program for showing it. The tool every one has access to, a C compiler, currently tried its hardest to make you program **not** loudly fail if you invoke undefined behaviour. Instead it proudly changes your intended code into whatever passes a benchmark best, at the possible expense of introducing security flaws. &gt;On the other hand, if signed overflow was defined, no diagnosis could be given because you could just have intentionally caused an overflow as a part of your program logic. Do you *genuinely* believe that if integer overflow was ratified then static analysis tools would be changed to not flag that up as a warning? `if (x = 5)` is not undefined behaviour but every static analysis tool I've ever used has an option to flag that kind of code because IT'S OBVIOUSLY A BUG and if you don't want it to be a bug then you put a specific comment next to it. How would this be any different from overflow? The reason for having overflow be defined is so that the code you write is deterministic across compiler versions and systems. Not because you want it to be caught by static analysis tools. 
&gt; The tool every one has access to, a C compiler, currently tried its hardest to make you program not loudly fail if you invoke undefined behaviour. Instead it proudly changes your intended code into whatever passes a benchmark best, at the possible expense of introducing security flaws. Check out UBSan, a compiler extension in clang which does exactly that. &gt; Do you genuinely believe that if integer overflow was ratified then static analysis tools would be changed to not flag that up as a warning? Yes. Because then people would use signed integers for modular arithmetic, just as they do with unsigned integers today. &gt; How would this be any different from overflow? Overflow has many valid uses. For example, if I compute a checksum, I want to admit that the number overflows. However, you can always simply use unsigned integers for when you want defined overflow to happen. &gt; The reason for having overflow be defined is so that the code you write is deterministic across compiler versions and systems. Not because you want it to be caught by static analysis tools. So wrong code should behave the same way across compilers? That's... an interesting proposition.
You did a great job explaining this. Thanks !
&gt;So code should behave the same way across compilers? That's... an interesting proposition. FTFY. Wrong code should be a compile error. Anything else just leads to security flaws that weren't security flaws last week, as we've seen time and time again. 
Wat? how does the example associate with the directive?
If we go by the example, it is actually correct.
If we go by the question, it is actually correct.
Be honest, you and others didn't read the directive, did you? I think the questioner is only putting it here as a sanity check, the question in it self is flawed.
I skimmed through it to analyse the nature of the post.
Well, now that you mention it. 31+32+33+34 is 130.. isn't it. I think someone just got me good... Damit, why am I always so gullible! ｡゜(｀Д´)゜｡
So, the language should also enforce where exactly storage is mapped so that int *a = 0x12345678, b; b = *a; always behaves the same way? That's basically the same thing you want.
The example was badly chosen. What you wrote was the point I was trying to make. Currently this is UB and the compile does not need to protect against it. If someone for some reason would want the loop with said behavior he or she could explicitly write it and incur the penalty of the additional instructions instead of forcing the compile of incuring it upon everyone in every arithmetic operation. A better example would be: x = (f() + g()) % 3; With UB the compiler can perform it in registers and simply assume no overflow happens. With defined wrapping it would again have to force the value of the addition to the defined range before applying the module.
&gt; Why would it? If it overflows, it overflows. The programmer can work with that. Currently, if it overflows, it's undefined behaviour. Now that I read your reply again: No, it won‘t, which is the whole point of it being UB. If it wasn’t UB the „overflow“ would have to be emulated each time you do arithmetic on a value that has a bitsize less than the register size of the machine executing the code.
It's obviously supposed to be 3^1 + 3^2 + ...
*its dammit
Same thing in README.md.
Of course, in the context of C, 5 / 2 = 2.
Actually, on x86 there is no problem because arithmetic operations are available in all operand sizes.
Is it that obvious? Well, maybe you are right. Still.. it is interesting choice of numbers, don't you think? No? Well, if you aren't careful, and fast calculate, you might trip like I did and the one below that deleted his answer. (Not everyone is as smart as you are.) Here is a video that I believe standards and test makers should watch. Watch from 6:10 onwards. https://www.youtube.com/watch?v=QyIIe5T5beM My personal motto is to keeping things simple while avoiding having to "explain" anything simply.. If they don't get it, what could happen to them at worst? Drop out of school?
Is it that obvious? Well, maybe you are right. Still.. it is interesting choice of numbers, don't you think? No? Well, if you aren't careful, and fast calculate, you might trip like I did and the one below that deleted his answer. (Not everyone is as smart as you are.) Here is a video that I believe standards and test makers should watch. Watch from 6:10 onwards. https://www.youtube.com/watch?v=QyIIe5T5beM My personal motto is to keeping things simple while avoiding having to "explain" anything simply.. If they don't get it, what could happen to them at worst? Drop out of school? 
Looks really nice! but note that the `_t` type suffix for is reserved by POSIX, so your program is technically non-conforming.
&gt; If it wasn’t UB the „overflow“ would have to be emulated each time you do arithmetic on a value that has a bitsize less than the register size of the machine executing the code. Good? Frankly this kind of "problem" can be solved by using the correct type from `stdint.h`. e.g. imagine if the C spec guaranteed overflow behaviour, and you wanted at least 8bits but didn't actually care about overflow, then you'd choose `int_fast8_t` or something. And if they were also defined to overflow properly, then `int_fast8v_t` or some other new type. I'd say that security and consistency should rank as more important than performance issues, and performance issues can always be profiled and optimised, whereas security is a much harder problem to get right, especially if the generated code flip-flops between compiler versions.
Fair point!
&gt; A better example would be: x = (f() + g()) % 3; &gt; &gt; With UB the compiler can perform it in registers and simply assume no overflow happens. With defined wrapping it would again have to force the value of the addition to the defined range before applying the module. I don't agree with this. If overflow is defined to happen as 2s compliment overflow then the compiler won't have to do anything extra. It can all happen in register and if it overflows it overflows. This is better than the current situation, where if it overflows it is UB and the compiler can do what it likes to that code. The situation where it *would* have to emit range-controlling code is when it hits different register sizes. But given the lax definitions of an int and long style types in C, does that matter much? I think it currently only matters if someone chooses one of the specific types, e.g. int8_t. An if they do chose one of those specific types then frankly I WANT the compiler to properly enforce 8bit behaviour on that on all machines! I'd prefer for the compiler to emit code that works **identically** across systems regardless of whatever the wordsize is on that machine, rather than have it perform one way on one machine and another way on a different machine. (e.g. if it chose a 64bit register to do that 8bit arithmetic in vs an 8 bit register). As a tangent, that code has a bug in it. If f() and g() are able to produce numbers such that `f()+g()` overflows an `int` but not a `long`, and therefore produces a different x for depending on the choice of `int` and `long` versions, then the programmers of that code haven't written it correctly, as they should have checked for overflow before hand and done the appropriate thing. (Which is [ironically difficult and verbose](https://wiki.sei.cmu.edu/confluence/display/c/INT32-C.+Ensure+that+operations+on+signed+integers+do+not+result+in+overflow) in current C, due to UB, and the fact that CPU flags can't be inspected) Most of my issues with UB come from writing secure C in a secure environment following security guidelines and it's so difficult, verbose and trap-laden to do so because of the horrible "guarantees" of the C spec. 
I do know what I'm talking about. Character encoding is one aspect of localization. The C and POSIX APIs that deal with character encoding modify their behavior based on the current locale. You could argue that the two should be disconnected from one another, but in C, they aren't. And yes, by “character” I meant “codepoint”. Sorry about the inaccuracy.
So for example on this site: https://www.thecrazyprogrammer.com/2015/03/c-program-for-binary-search-tree-insertion.html, it has a program to implement a bst. How would you go about rewriting the void insert(node *root,node *temp) on that site to use void insert(void)? 
You can't, unless you use a global variable. And you REALLY shouldn't do that.
How do you make a function to add nodes to a bst without giving that function the root node, or the node to add? You don't. I'm not saying it's impossible but that's kinda against the point of functions. Is there a reason why you can't pass in parameters?
Is it possible if I include a custom header file? How would you do it from that?
The root node is initialised to null in the main function, and the insert function is being called on
The existence or non-existence of a header file has nothing to do with the question
Yes, but in the function you have to tell it where the root node is, and what to add to it. You can do that through globals (bad idea) or parameters. Maybe post the exercise you're working on and give us a little more context to push you in the right direction
sure, you have a global (could be static global) node pointer that points to root (null initially). void insertnode(void) mallocs and adds a node to that pointer. since we are just abusing global vars you can get the data you want to store via some global variable. not sure at all why you would want to do this but it's possible. 
&gt; Write a program that will compute and display the **sum of the powers of x from the first power to the nth power** Yes, obvious.
What exactly is a call `insertnode()` supposes to do? I'm a bit confused as to what you want.
What have you tried? Show us your code!
#include &lt;stdio.h&gt; #include &lt;string.h&gt; #include &lt;stdlib.h&gt; #define NUMAX 10000 #define NOMAX 80 typedef struct { unsigned long l,c; double val; } elem; static elem v[NUMAX]; /*static char filename[NOMAX];*/ static double zero; static unsigned long minl, minc, maxl,maxc; static size_t count=0; void comando_a(elem v[], unsigned long lin, unsigned long col, double val); void comando_p(elem v[]); void comando_i(elem v[]); void comando_l(elem v[], unsigned long lin); void comando_c(elem v[], unsigned long col); void comando_o(elem v[]); void comando_z(elem v[], double zero); int main() { unsigned long lin, col; double val; char c/*,vo[NOMAX]*/; zero=0; while(1) { while((c = getchar()) != EOF &amp;&amp; (c = getchar()) != "\n"){ switch(c) { case 'a': val=0; scanf("%lu %lu %lf", &amp;lin, &amp;col, &amp;val); comando_a(v, lin, col, val); break; case 'p': comando_p(v); break; case 'i': comando_i(v); break; case 'l': scanf("%lu", &amp;lin); comando_l(v, lin); break; case 'c': scanf("%lu", &amp;col); comando_c(v, col); break; case 'o': /*fgets(vo,NOMAX,stdin); if (strcmp(vo," column")==0)*/ comando_o(v); break; case 'z': scanf("%le", &amp;zero); comando_z(v, zero); break; case 's': case 'w': case 'q': exit(0); } getchar(); } } return 0; }
You need to put four blanks in front of every line of code so reddit formats your code as code. Click “edit” and fix your comment to be formatted correctly.
done i think
That's because the functions defined in the kernel are not in any C library. If you want to use any of them, you have to copy their source code into your program and compile them yourself. There is no way to link against the kernel in a normal program.
Your explanation is one of the better ones. OS2200 is still in use as Unisys Clearpath Dorado and is ones' complement. That's a Univac 1100-descended architecture by way of Sperry. [Hobbyist license available here.](http://www.unisys.com/offerings/clearpath-forward/clearpath-forward-products/clearpath-os-2200-software/clearpath-os-2200-express) 
Now its done
Thank you!!!
Typically, if you insert the values [1,2,3,4,5] in sequence into a linked list, the head will point to 5, 5 will point to 4, 4 will point to 3, 3 will point to 2, 2 will point to 1, and 1 will point to NULL. In other words, the order of your linked list will be the reverse of the order you inserted the items. Does that help? If not, think of how this list gets built up in parts and why it's more efficient to do it this way.
For the next time: please do not delete your post after receiving an answer. Future readers might have the same question you have and could find help by reading your post and its answers. By deleting your question, you deny them this resource and erase the work of all the people who answered you, so don't do that!
You're right, I'm an idiot. I thought this was something different than it actually is.
wanna see something nuts typedef struct { int length; char* data; } my_string;
You should assign `head-&gt;next` to `temp`. Then temp-&gt;next = NULL. I'll try and describe what you're actually doing here: first, allocate a new node ```c temp = malloc(sizeof(node)); head = temp; ``` now both temp and head are pointing to the same data. allocate a second node: ```c temp = malloc(sizeof(node)); ``` now, temp and head are two different nodes. assign the brand new node to head-&gt;next, (becuase it is the next element in the list). ```c head-&gt;next = temp; temp-&gt;next = NULL; ``` now head has its next value pointing to the the new node (temp), and `temp` (the same data as `head-&gt;next`) has it's `next` value point to NULL. NULL just lets you know that this is the last element when iterating through the list in the future. I hope that helps... Not 100% sure I understood the question :P
Why would you want to obfuscate C code? Just compile, link, and strip it, then distribute the binary.
https://github.com/cstack/db_tutorial Try that tutorial and try and finish where he left off.
/r/blackmagicfuckery
I assume you have an error on this line dp=dotproduct(int a[2], int b[2]); and a warning on this one printf("The dot product is: %d"); ? Which one do you need help with?
Woops on the printf, but it would be the dp=dotproduct(int a[2], int b[2]); line, from my understanding I only need 4 arguments, does the a[2] and b[2] not cover that?
If the error is complaining about number of arguments then it is misleading you. Your syntax for calling the function is wrong. You need `a` not `int a[2]`
'dotproduct' is a function defined to take two arguments. Both of which are integer arrays. When you invoke the function, you need to pass the address to each array.so in this case dp = dotproduct(a,b); will do
That seemed to fix the first error, thanks a lot by the way. I'm getting an error for x and y "redeclared as different kind of symbol" I tried removing the array bracket but it's still the same error.
noice. and reimplement strcmp, etc. on top of that?
Cool! Here's what I noticed with the first couple things I tried: $ ./simplify --isolate x 'x^2 = 4' x = 2 What about `-2`? $ ./simplify --isolate x 'x^2 - x - 1 = 0' x = (1 + x) \ 2 Where did you get the `n \ 2` syntax for the square root of `n` from? Never seen that before. And obviously that's not the absolute simplest expression for `x`. Might be nice to recognize and solve quadratics (and maybe cubics and quartics). The documentation of Maxima's [solve](http://maxima.sourceforge.net/docs/manual/maxima_20.html#IDX806) function, for example, gives an overview of some possible solution approaches for equations.
A for effort
yeah the `\ 2` notations is odd, I think `^(1/2 )` would be much clearer
&gt; C/C++ continue to require expensive for-loops to calculate string length Not true for C++. `std::string` implementations store the length pretty much since forever and since C++11 this is even required by the standard. The vast majority of languages that come before Go do this too. Anyway, I hope you are just trolling. 
&gt; if you have real-world strings (a screen line is typically under 80 characters), the standard library version may not be optimal, because it uses long words when 32-bit words may be faster (with the same bit trick algorithm). But I should test specifically later, I'm not sure yet. Well, after testing more accurately, I'd say it must have been a measurement error. 32-bit and 64-bit versions are on par for short strings, and 64-bit is faster for long strings. Which makes sense.
Why wouldn't I want to. The more you play with a language the better you understand it. For example for this i learned a bit about it already.
It couldn't lio to libbamdb, possibly related to the rpath being unset. Install the library and configure it to properly link.
TIL std::string does the thing
C++ is off topic in this subreddit. Please ask C++-related questions in /r/cpp_questions.
I was just being silly. Vim is part of an ancient editor flame war. https://xkcd.com/378/ There are lots of great IDEs you have to pick from (VS Code, Atom, etc.). The trick is just to find one you enjoy using.
Yep, adding set(CMAKE_MACOSX_RPATH 1) to CMakeList.txt does make the error go away. Otherwise, I'm dealing with a linking error. 
What's confusing to me is that the CMake outputs Found HTSlib which is from `find_package(HTSlib REQUIRED)` within CMakeLists.txt However, there are clearly linking issues upon `make`. How would one best correct this in CMakeList.txt? 
Finding the package is only half of the problem. You need to use target_link_libraries (or the less preferred link_libraries) command to actually link to it.
Nope.
I actually have been. The biggest challenge has definitely been trying to apply FP concepts in C when it was not meant for it. It's much easier to apply them in JS and somewhat easier in C++ which are the other two languages I program in. I decided to start learning it as a way to expand my toolbox.
Right, that's there in CMakesList.txt https://github.com/D-Lo/bamdb/blob/master/CMakeLists.txt#L26 `MESSAGE()` htslib_CONFIG gives me nothing it appears. That's odd.... However, doing the same for `htslib_LIBRARIES` shows me that `libhts.dylib` exists in `/usr/local/lib/` $ pwd /usr/local/lib $ ls libhts* libhts.1.8-11-geeda089.dylib libhts.2.dylib libhts.a libhts.dylib This is very confusing....it should be linking correctly... 
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [D-Lo/bamdb/.../**CMakeLists.txt#L26** (master → f5f03d0)](https://github.com/D-Lo/bamdb/blob/f5f03d0faaa103a7a56472e870b28dfc85a62a44/CMakeLists.txt#L26) ---- 
...so does go you retard
I have removed your question as it doesn't seem to be related to C programming and you have failed to clarify how it does. Please ask general programming questions in /r/programming.
I ran your Dockerfile without any issues.
Yes, sorry I should have made it clear \- I was interested in finding out whether other C programmers had an interest in FP, and if they were trying to use FP techniques \(or at least an FP\-inspired style\) when writing in C. I tried to make the question generic to avoid "priming" answers one way or the other \- apologies if that made it confusing.
Thank you! I agree that using a FP style in C is not easy, especially because of things like tail call optimisation \(which depending on the compiler, might be available as an option\), but to be honest is not as bad as I thought it would be. My main problem is that I tend to use C when I need low\-level control, and FP is not always a good idea for that :\-\) Can I ask if you've been focusing on a specific FP language \(e.g., Haskell, Lisp, ...\), and what you've used to learn \(books, tutorials, videos, ...\)?
Thank you this really helped a lot. Especially cmp for the final check that everything is correct and same :D Thanks!
Thanks :\-\) Can I ask you why you haven't been curious to look into FP \(given all the hype around it recently\)?
I understand your intent, but sadly “I want to address people who program in C” does not make your question on topic. People come here to get content about C, abusing this audience for unrelated content is not something I can support.
C++ is off topic in this subreddit. Please post C++ questions to /r/cpp_questions.
Sure, I understand. Just for the sake of argument, would it be ok to ask a question about using FP techniques in C then? I won't post again of course, but I'd like to understand whether the problem is in me asking questions (as opposed to providing content), or in not mentioning C explicitly in the question. Thanks!
&gt; Just for the sake of argument, would it be ok to ask a question about using FP techniques in C then? Yes, that is strictly on topic. Feel free to post about this topic. &gt; but I'd like to understand whether the problem is in me asking questions (as opposed to providing content), or in not mentioning C explicitly in the question. Asking questions is generally fine. The problem is that your question shows no connection to the C programming language besides “I want to reach C programmers,” which insufficient. As soon as you can connect your question solidly to the C programming language, it is on topic.
Got it - thanks!
How is your post not contained as a comment in the source?
Can you please elaborate?
Mydata is a pointer to a char. It's probably null therefore it cannot be dereferenced. 
mydata isn't being dereferenced. If it's crashing there then it is because B is bad
Have you tried [debugging with gdb](https://gcc.gnu.org/bugs/segfault.html)?
&gt;Wouldn't this make sure it wasn't a null? I am not total sure &gt; &gt; &gt; &gt;if \(B==NULL\){ fprintf\(stderr, "Block error: calling hash\(\) on NULL block reference \\n"\); exit \(EXIT\_FAILURE\); }
Yes. That is what pointed out the error
B is 0x331. That is not a valid value for a pointer. It's pointing at the 1st page of memory which is unmapped
Pass -fno-builtin to clang to disable the malloc optimizations. https://godbolt.org/g/FMqEUt vs just -O3/-g: https://godbolt.org/g/D7S6Ri
Then that list contains a bad pointer for some reason. It's impossible to tell why from that
About optimizations in clang and other C compilers, I highly recommend reading all three parts of this post http://blog.llvm.org/2011/05/what-every-c-programmer-should-know.html. My real question though is why you bother calling malloc at all. Have your allocator return NULL.
Haha yes, what I meant was that there are good reasons to learn and use FP, and given all the discussion about them, you're unlikely not to know what they are. But as you said, FP might not be relevant for your use case. Thanks for your comment!
Is that also your function or was that given to you?
You can't reliably test memory allocation on modern platforms in the way you want, so you need to rethink your goals here. Memory allocation is the most heavily leaned upon system in the standard library, and it's been the subject of decades of serious optimization. The memory allocations are all virtual until you write to them. That is, the process is defined to have the full 64 bit address space of memory available to it, and is supposed to act like it does. That is to say, making sure there is actually enough memory is the OS's problem, not an application problem. The application's only responsibility is not to request more than it needs. This is because no memory is actually allocated until you write bytes to the memory in question. When you do write to the memory a page fault is triggered and the kernel actually handles the allocation at that time. Because the write almost always happens long after the actual malloc, there's no way to communicate an error for this if it goes wrong so it's rigged never to go wrong. Your allocations always succeed, the OS finds a way or it throws up it's hands and kills your program. Basically, malloc will for the most part always return true. Many veteran C programmers don't even check the return value of malloc if they're sure the argument is nonzero. 
I know it is a bit late but &gt; Another approach was to create an array that holds the return addresses &gt; and reverse iterate through that array to find the last stored return &gt;address before deleting that element. Doesn't this just say stack? Because it sounds like you just need to go from back to front again.
Before discussing what you are trying to do, according to the man page, malloc expects an argument of type size_t and you are passing a constant negative integer. I am not sure if there is a defined conversion for this or it is undefined behavior which would mean that compilers can handle however they like and they are still conforming.
On the other hand I think that he may be able to try with calloc
&gt; return !!malloc(-1); On a typical LP64 system, this should succeed and allocate 4 GB memory. That's because `-1` is of type `int` and casting it to a `size_t` results in 4294967295, which is 4 GB - 1.
Yea, but he says he's trying to test that the code properly handles an OOM condition, which makes me think he's making the classic mistake of thinking that you can detect and handle OOM using malloc in C. The best he can hope for is to trigger a kernel kill of his test suit when it tries to allocate more than his OS can handle, which seems unhelpful.
I think the grey in your beard is showing brother. size_t has been 8 bytes on everything I've laid eyes on for years. Maybe you're in a different vertical than me though. I spend almost all of my time on Linux.
I wouldn't expect calloc() to be any different, and, indeed, this little test gives the same optimization result: int my_calloc(void) { return !!calloc(-1, 1); } Outside of `volatile`, memory loads and stores aren't an observable side effect, so the compiler is free to optimize them away. The same is even true for calloc()'s zeroing. Clang optimizes the malloc() out of this program as well: int my_alloc_store_load(void) { int r, *p = malloc(-1); *p = !!p; // store r = *p; // load free(p); return r; } Even reading from a calloc() allocated buffer is optimized away: int my_calloc_read(void) { int *p = calloc(-1, 1); int r = *p + 1; free(p); return r; } Under Clang, each of the functions always returns 1. However, I found a Clang bug! Check this out: int my_calloc(void) { return !!calloc(-1, 2); } This also returns 1, but according the C spec it must always return 0. Clang's optimization eliminates calloc()'s overflow detection. 
You *can* in some cases handle OOM issues from C even in the presence of overcommit. Linux will refuse allocations that are larger than it knows it can ever commit, e.g. the allocation is larger than physical RAM + available swap. That's when malloc() actually returns NULL. This is the case I want my library to correctly handle.
&gt; volatile void *ptr = malloc(-1); That doesn't actually work (being *pointer to volatile*, not a volatile pointer itself), so I suspect you really mean this: void *volatile p = malloc(-1); That's basically what I ended up doing after submitting this post, and it cleanly solves my issue.
Oh yes, indeed. I should sleep more...
&gt; However, I found a Clang bug! Please report this! We need more people to report such issues.
`memset()` is too trivial, so it also gets optimized away: int my_calloc(void) { void *p = malloc(-1); memset(p, 0, -1); return !!p; } Same result: always returns 1; 
Well, no offense please, but I can't take a guy from the internet's word for it. Therefore, were I to use this "portable guaranteed way", I would have to find the spec and verify it. I would also have to add a test in any project that is using it to verify that this is true on a given platform/compiler suite. Or I can just forget about walking among the cliff and use UINT_MAX from limits.h.
Interesting, thanks for looking into it. Compilers are voodoo to me :)
In the typedef block is written with a small b. In your function argument u use a Capital B.
&gt; malloc will for the most part always return true Not always, for example on UNIX OS if you set a memory limit for a processor (either with the limit shell command or with setrlimit system call) and you try to allocate more memory than the limit malloc() should return NULL. 
THis is the proposal for C11: &gt; *Change* Conversion from signed to unsigned is always well-defined: the result is the unique value of the destination type that is congruent to the source integer modulo 2^N. But I don't actually know what it means. And since it's marked as a "change" I'd assume it's **not** trye prior to C11.
You have responded to the wrong post. &gt; What is this " !! " operator? There is no `!!` operator. `!!x` is like `!(!x)` which is like `x ? 1 : 0`. &gt; I bet that the 1 that you are seeing is because calloc is not failing as you think it should. Exactly. That's the whole point. `calloc(-1, 2)` must fail because it overflows the range of `size_t`.
&gt; What is this " !! " operator? It's two operators, double logical not. It prevents the pointer from escaping while still testing it, enabling this optimization. A single ! would be sufficient, but I find it easier to reason about this way. &gt; I tried your code snipets on godbold with varying optimization levels and the call to malloc is not optimized away, which compiler/version/opt level are you using. At `-O3` with any version of Clang at least through 6.0.0. Here it is: https://godbolt.org/g/D7S6Ri GCC doesn't make this optimization at least as of 7.3.0. &gt; calloc is not failing as you think it should. In the C99 spec, 7.20.3.1-2: &gt; The calloc function allocates space for an array of nmemb objects, each of whose size is size. The space is initialized to all bits zero. It's not possible to allocate `SIZE_MAX * 2` bytes of contiguous memory, let alone zero it, so there's an implicit overflow test in calloc(). This call to calloc() must always return a null pointer.
If that malloc() succeeded then you're on a 64-bit machine and `UINT_MAX` is a mere 4GB. It's not surprising that worked since you probably have lots more RAM and swap than that. Try something much larger. Here's a test I just did: #include &lt;stdlib.h&gt; #include &lt;stdio.h&gt; int main(void) { void *p = malloc(4UL * 1024 * 1024 * 1024); printf("%p\n", p); } I'm running Debian 9, default kernel without any special settings except that I don't use swap. My machine has 4GB of RAM. This prints `(nil)` for me because Linux know it can never satisfy that. Then I add 4GB of swap: # swapon /swapfile Running the same binary again I got `0x7ff06f2a5010`. 
From the C99 spec, 6.3.1.3-2, regarding integer casts: &gt; Otherwise, if the new type is unsigned, the value is converted by &gt; repeatedly adding or subtracting one more than the maximum value that &gt; can be represented in the new type until the value is in the range of &gt; the new type. The behavior I described is guaranteed by any conforming compiler on any platform. Lots of code uses it. 
One of the nice properties of `memset()` being in the standard library is that compilers can do really neat optimization tricks with it, often eliminating it altogether (sometimes at the cost to security).
I think that memset may be optimized away because there are no side effects of doing so: the memset memory is leaked when the function returns. all that is returned is the logical result of "not (not pointer)" which as I said before is 1 because if pointer !=NULL then !pointer == NULL and !NULL = 1. I do not remember what the standard says about null/0/1. Usually null is (void*)0 but I seem to remember that there is no requirement for that.
&gt; What is this " !! " operator? &gt; It's two operators, double logical not. It prevents the pointer from escaping while still testing it, enabling this optimization. A single ! would be sufficient, but I find it easier to reason about this way. !!(expr) is actually: !( !(expr) ) which is evaluated in this order. if expr is false then !expr is true and !!expr is false (ie 0). SImilarly if expr is true (ie non 0 ) then !!expr is true (ie 1). The function that you think is a clang bug does not return the allocated memory (which is in fact leaked) but simply the evaluation I just explained. What do you think that this check means anyway? &gt; I tried your code snipets on godbold with varying optimization levels and the call to malloc is not optimized away, which compiler/version/opt level are you using. &gt; &gt; At -O3 with any version of Clang at least through 6.0.0. Here it is: &gt; https://godbolt.org/g/D7S6Ri I need to check into this more, I can not explain it now. &gt; calloc is not failing as you think it should. &gt; &gt; In the C99 spec, 7.20.3.1-2: &gt; &gt; The calloc function allocates space for an array of nmemb objects, each of whose size is size. The space is initialized to all bits zero. &gt; &gt; It's not possible to allocate SIZE_MAX * 2 bytes of contiguous memory, let alone zero it, so there's an implicit overflow test in calloc(). This call to calloc() must always return a null pointer. Again, it's not possible says who? I have the impression that these are your own interpretations. I do understand that they seam reasonable but AFAIK this is not how things work (and for several reasons).
In both clang and gcc you can ask to compiler not to optimize a specific function using function attributes. https://stackoverflow.com/questions/26266820/in-clang-how-do-you-use-per-function-optimization-attributes
That was a good read. I've sometimes written code that I knew was not valid C, but I understood the hardware architecture well enough to know it would produce the result I wanted. It hadn't occurred to me the extent to which the compiler can use undefined behavior as a (valid) excuse to dramatically change the code produced.
my bad, I meant to use SIZE_MAX from stdint.h which actually returns NULL. 
C doesn't require two's compliment representation for signed integers ([though this may change in the next standard](https://www.reddit.com/r/C_Programming/comments/8f9y6z/not_only_is_the_c_standards_committee_happy_to/)). But it does guarantee two's complement semantics when converting from signed to unsigned regardless of the underlying representation (ISO/IEC 9899:1999 6.3.1.3-2).
&gt; There is no overflow and even if there was, AFAIK calloc is not required to fail because of this. There is overflow (`-1` cast to `size_t` is `SIZE_MAX` on a two's complement machine and `2 * SIZE_MAX` is definitely too large for a `size_t`) and yes, overflow must result in an error as specified in [the standard](http://pubs.opengroup.org/onlinepubs/9699919799/functions/calloc.html) as it says: &gt; If the space cannot be allocated, a null pointer shall be returned. As this much space can never be allocated, a null pointer must be returned.
Please consider posting your projects as links instead of self-posts so all the usual reddit mechanisms work.
Remove and repost or just for the next time?
Just for the next time. It's good style to post links as ... you know, links.
I understand why you expect this call to return NULL. However I am reluctant to assume any behavior. AFAIK these are examples of overflowing: size_t num = SIZE_MAX + 1; char name[10] ; name [10]='a'; If you want to be a language lawyer and work on the boundaries of the language you have to be precise.
Here's the part that bugs me the most, that I've seen in various examples: while (p-&gt;next != NULL) { p = p-&gt;next; So I **GET** what this code means, to have p point to p-&gt;next until the end of the list (p-&gt;next = NULL) is reached. However, in other examples, I see that p = head, and head-&gt;next = NULL, so it seems like p-&gt;next = NULL already occurs before the while loop runs.
The first argument to the first call of `strok()` is the string you want to parse. Thereafter you pass NULL and it continues parsing the original string, tracking it using internal state.
Yeah I know that and I use it that way, but the problem is somehow with getting the end of the string as a token. 
The C Standard never says calloc must return 0 
That is out of bounds access, not overflow
I'm saying that `my_calloc2()` must always return 0, because `calloc(-1, 2)` must always return NULL.
`-1` cast to `size_t` is `SIZE_MAX` on any machine, it does not matter what the representation of -1 was. The standard is unclear about what happens when the product of the `calloc` arguments would exceed SIZE_MAX. This has been debated before; some say it should try to allocate the mathematical result; some say it should perform a multiplication in `size_t` and allocate that amount; some say it is undefined behaviour by omission of specification. Next: the standard defines behaviour in terms of an abstract machine. The abstract machine could allocate 2^65 bytes just fine. The size of an allocation isn't *observable behaviour*. You could argue that any operating system that does lazy allocation is non-conforming, however I think that is moot: operating systems have decided that lazy allocation is the best way to go and the coder just has to deal with it. 
The amount of code using sth is not proof of that sth being good/correct. I spent 10 minutes trying to understand that paragraph and I couldn't. Even if you are correct and this this conversion is guaranteed to be so, isn't it yet obvious to you that it is a bad idea as it is not clear? Especially when there are the crystal clear _MAX typedefs?
Yes,I was my own victim: chat a[5]; strnpy( a, "123456"); OK now?
Without seeing your code it is hard to know what exactly went wrong, so show us your code please.
&gt;On a one's complement, -1 casted to size_t is SIZE_MAX - 1. On a sign-and-magnitude machine it is 1 as the sign is dropped. Where do you get these ideas from? From the C Standard. See N1570 6.3.1.3/2: &gt; Otherwise, if the new type is unsigned, the value is converted by repeatedly adding or subtracting one more than the maximum value that can be represented in the new type until the value is in the range of the new type So the result of converting `-1` to `size_t` is the mathematical result of (-1) + (SIZE_MAX + 1), i.e. `SIZE_MAX`. This is one of the first things anyone learns about unsigned arithmetic ...
&gt;The abstract machine is not allowed to do that as size_t is specified to be large enough to capture the size of any possible object The standard doesn't actually say that anywhere. You can infer something about static or automatic objects from the specification that `sizeof x` yields the size of `x`; however for allocated space there is no way to apply `sizeof` to it. 
&gt; From the C Standard. See N1570 6.3.1.3/2: Thank you for this clarification. I was under the impression that this only applies to two's complement.
These unsafe malloc optimizations were disabled by default on OpenBSD, along with several other functions. https://marc.info/?l=openbsd-cvs&amp;m=150125592126437&amp;w=2
`SIZE_MAX` wasn't originally defined in ANSI C, so `(size_t)-1` became a convention that continues to this day ([1](https://github.com/lattera/glibc/search?utf8=✓&amp;q=(size_t\)-1&amp;type=), [2](https://github.com/ifduyue/musl/search?utf8=✓&amp;q="(size_t\)+-1"&amp;type=), [3](https://github.com/torvalds/linux/search?utf8=✓&amp;q="(size_t\)+-1"&amp;type=), [4](https://github.com/openbsd/src/search?utf8=✓&amp;q="(size_t\)+-1"&amp;type=)). That's because it's guaranteed to work. Pretty much any fluent C programmer should be accustomed to seeing it since it's so ubiquitous. Due to it not being defined until C99, using -1 instead of `SIZE_MAX` is ever so slightly more portable, and it doesn't require an extra, uncommon include (`stdint.h`). The conversion is really straight forward. Here's the key part: &gt; [...] repeatedly adding or subtracting one more than the maximum value [...] Since -1 is out of range of `size_t`, add `SIZE_MAX + 1` to it (pretending we can represent that number). And then `-1 + SIZE_MAX +1 == SIZE_MAX`, which is now in range. That's really just a description of two's complement semantics.
Interesting, thanks! I actually did test to see if it would optimize `reallocarray()` on OpenBSD, and noticed that it didn't. I didn't think to check `malloc()` itself. FreeBSD still optimizes away `malloc()` but its `reallocarray()` is not.
No documentation = Bad
Function "strnpy" cannot be implicitly declared in this scope. More seriously, still out of bounds access. Overflow is specifically about values being assigned to integral variables that are outside of their range. E.g. `unsigned char a = 255; // Highest value of unsigned char on most? machines` `a++;` `a == 0;` a overflows because the value assigned to it is 256, but 255 is it's max value, so the assignment must be interpreted.
&gt; Read the code, it's literally better than documentation, since it won't lie. Right. And how do I know if I really want to read your code if I don't know what it does ahead of time? Reading code is much, much harder than writing code. Nobody wants to read your code. This is why you need to be able to demo your work and document it. It needs to be worth my time to read your code.
How do you call buffer overflow then?
Thing to note, one could define an identity name for the uint16_t of the struct and then be able to replace for(char i = 0; i &lt; 16; i++) by while(BCD.uint16val) and save approx. 0x5 program memory and a byte of data memory. Of course, this potentially reduces readability, but whatever floats your boat :)
In C, every string needs to be terminated with a `'\0'` (NUL) byte. This byte is needed to know where the string ends. Your code overwrites the terminating `'\0'` byte of `str` with a bunch of exclamation marks but fails to add a new terminator. It might appear to still work because possibly, the memory location after your final exclamation mark is randomly already a `'\0'` byte, but that's not something you can rely on.
Thank you! That makes it clearer. To follow up, in what case would memory allocation after my exclamation mark would not be a '\0' byte?
The null byte must terminate the string. Consider the following [https://i.imgur.com/7vMqIYh.png](https://i.imgur.com/7vMqIYh.png) [https://i.imgur.com/EGVjUP0.png](https://i.imgur.com/EGVjUP0.png)
/ &gt; Our play starts with a son and father sitting at the breakfast table. The son is munching on some cereal before school while the father reads his iPad before leaving for work. &gt; &gt; The son says: “Hey Dad, you said you were going to teach me how to drive after school today. Are we still going to do that?” &gt; &gt; The father, without looking up from his iPad, replies: “Of course, son. The car is in the garage and I laid out a set of wrenches on the workbench. Take the car apart and look at each piece, then put it back together. Once you’ve done that I’ll take you to the DMV for your driving test.” &gt; &gt; The son quietly continued eating his cereal. Steve Losh - [Teach, Don’t Tell](http://stevelosh.com/blog/2013/09/teach-dont-tell/#act-1-read-the-source), Act 1: Read the Source
Randomly. Unless you made sure there is a NUL byte after the string, there is no reason to expect one to be there.
I find it interesting that you parsed the string for the null to determine the string's end, but decided not replace it after appending the exclamation points. Never assume a null exists. Strcat() appends null.
How does that save memory? for every 4 bits, 6 values are unavailable...
simply because instead of using a counter variable and the associated computations, your checking for any value other than 0 in the initial 16bit integer to continue iterating.
Case: If you used the allocated space for anything else prior, ie if you were manipulating a longer string in that space, say with your very function moments before. Say you had a char astring[255] and it had some 100 char string in it prior. And you overwrote a new null terminated 50 char string into it and called your function with it. It’d still have part of the original 100 char string in it. Now, I get that you probably didn’t get an error if you ran that function just once. But that’s because c compilers today generally zero out newly allocated char arrays even if you don’t assign anything to them when you define them. This wasn’t always the case iirc. Don’t depend on this. Also, just be aware of this. &gt; Assume that there is enough room in the string. Bounds checking. Sometimes that can come back and bite you in the behind! 
Ah, I see where you're coming from. Yes, that's a buffer overflow. However a buffer overflow is not related to an overflow. Much like a stack overflow is unrelated to overflows or buffer overflows. If we want to be extra-specific we can refer to integer overflows or arithmetic overflows, but in standard usage (in a software context) "overflow" is unambiguous.
This line is redundant: BCD.DDabble_Container = (0x0000FFFF &amp; Decimal_uint); since `Decimal_uint` is a 16 bit unsigned int. You can just write: BCD.DDabble_Container = Decimal_uint; I don't know if this will have any effect on generated code size (depends on how smart the compiler is!). 
Pffft, who needs readability? If it works, no one needs to read it
- I'm a fan of uint16_t, uint32_t over unsigned int - I'm a fan of declaring loop variables in the loop rather than outside, it is scoped this way - I always declare variables to a known value, it is more deterministic this way - Declaring multiple variables on the same line is hard to read, but that is subjective - I don't love declaring all variables at the start of the function but that depends on your coding standard, etc. Harder for me to read as I have to look back and forth. - No need to start off with `best_score = sum_rows(matrix[0])`, just start your loop from 0 and have your best_score start at 0. This way any row will be a higher score than the starting score. - Instead of naming variables with arbitrary names like `tmp` use descriptive names like `sum_of_current_row`. - So each value of the matrix is an `unsigned int` but your sum is also `unsigned int` are you sure it will never overflow?
Hi, so you've found your way here and improved a bit on the code since you passed, nice! :) Just three things I spotted on mobile: 1. in `zish_touch()` your error says "couldn't create history file" -- probably a remnant of this not being an extra function. Now it should just say "couldn't create file &lt;path&gt;`. 2. Same function: Is there a specific reason why you use `open()` and not `fopen()` or other functions from ISO-C and not ISO-POSIX? 3. You often use `perror("zish")` but it would probably be better to also give the function that failed, eg. `perror("zish (chdir)")`. Otherwise, have fun with your kawaii(?) shell and see ya in uni!
IF all you want to do is have the computer print what is in your variable "a", you would use printf. If you wanted the computer to be able to print out whatever you entered, then it is a bit more complicated, but you could do it with a bit of nested if statements and a scanf or two.
&gt; SIZE_MAX wasn't originally defined in ANSI C, so (size_t)-1 became a convention that continues to this day (1, 2, 3, 4). That's because it's guaranteed to work and it matches programmers' intuitions about conversions. Pretty much any fluent C programmer should be accustomed to seeing it since it's so ubiquitous. Due to it not being defined until C99, using -1 instead of SIZE_MAX is ever so slightly more portable, and it doesn't require an extra, uncommon include (stdint.h). 1) You are not using size_t - 1. You are using -1 and this is what I criticized. None of the links that you sent me used an implicit type conversion from -1 to size_t. 2) stdint is only 209 lines on my system, it includes fixed width integer types and it was introduced in c99 which is now 19 years old so I expect any fluent programmer to know about it and use it instead of unnamed constants. BTW, unnamed constants is also sth I expect any fluent programmer to never do when there are std lib defs for it. &gt; The conversion is really straight forward. Here's the key part: &gt; &gt; [...] repeatedly adding or subtracting one more than the maximum value [...] &gt; &gt; Since -1 is below the range of size_t, add SIZE_MAX + 1 to it (pretending we can represent that number). And then -1 + SIZE_MAX + 1 == SIZE_MAX, which is now in range. That's really just a description of two's complement semantics. Thank you for taking the time to explain it to me. As I said before, I always avoid walking on the edges of the language definitions when there is an easier/ "safer" way. I only have a limited mental capacity and I prefer to use it on more productive things. It also makes my life easier.
You're right, although my original purpose was to also clear the rest of the 32-bit variable, but since it is progressively shifted out it is also kind of useless. One could even simply do BCD.uint16val = Decimal_uint; Which would do pretty much the same.
Thank you so much!! Your example really helped me!
I had limited time and added that as a little joke, documentation is of course planned, haha. Thanks for that awesome story.
I know, I had limited time, so I postponed it. It's planned of course.
1. That's a pretty fair one, I'll change it later. 2. `fopen` didn't create the file as I wanted it, might just me using it wrong, will take another look into it. 3. I thought I got rid of those, might have missed some. Will have another go at it. Thank you again, see you around.
https://bugs.llvm.org/show_bug.cgi?id=37304
1. I've made a PR 2. I made the mistake first, too, but with `"a"` it should work. The other flags you passed don't matter because you don't create a new pty (not tty) or fifo. 3. just use `$ grep perror *.c` with shell expansion or `find . -name "*.c" -exec grep perror {} +` with find which will also work on multiple levels of directories.
Is that dependent on the compiler or is that a standard? Nonetheless, interesting to know!
These two strings walk into a bar and sit down. The bartender says, "So what'll it be?" The first string says, "I think I'll have a beer quag fulk boorg jdk^CjfdLk jk3s d#f67howe%^U r89nvy~~owmc63^Dz x.xvcu" "Please excuse my friend," the second string says, "He isn't null-terminated." 
Theoretically you *should* be able to use the C++ package to compile and run C programs, but it can be kind of weird because Microsoft has their own libraries they prefer to use. I would recommend trying one of two things instead. * First you can use VirtualBox and run a small VM just to do development in. If your computer can handle Visual Studio then it can handle a small VM, especially if it's only used for coding. * Your second option (and I don't even know if this would work) is to install the Windows Subsystem for Linux which will allow you to run a Linux shell on Windows. This will include utilities such as GCC and Make, so you can compile your programs and run them from the shell. Any common text editor (Atom, VS Code, Sublime Text, etc) will be enough for making edits, you just need the shell to compile and run. I really would not advise using VS for C programming. My girlfriend just went through my CS program's C programming class and the asshat of a professor had everyone do everything through Visual Studio and it ended up being pretty weird. Libraries were constantly messed up and it just felt like the wrong tool for the job. Even though I have 2 years of C experience I couldn't help her with basic troubleshooting because debugging is different in VS than with traditional tools like GDB and Valgrind, and the standard libraries were definitely *not* standard at all.
in my college we use VS. But im opened to other compilers. I tried eclipse, with mingw, but too many errors in the process. If you have suggestion for good compiler, for windows i would appreciate it
Any compiler except microsoft Visual C++. MSVC is just plain terrible. I advise you to install WSL and use gcc on this pseudo-Linux.
something that will be easy to set?
If your school uses VS, use VS. You want the desktop C++ environment. By default, anything with a .c filename will be compiled as C code. Use what your school uses, don’t try to deal with more than just learning the basics of C.
I use VS for C development all the time. I think VS's debugger is really good. It's a lot better than most of the embedded debuggers I use for most things. It's true that Microsoft's libraries have a few differences from GCC libraries, but there is plenty of documentation for them. The only real problem is if you want to use both VS and GCC on the same code. Then you'll need to play some preprocessor games and that's not something I'd recommend for a beginner.
im dont need both libraries right now. thank you thoug, i hope i will be able to run "hello world" to confirm if it worked lol
Very nice. For my own stuff I was using [stb's stretchy buffers](https://github.com/nothings/stb/blob/master/stretchy_buffer.h) which are very good, though a lot simpler than what you have here.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [nothings/stb/.../**stretchy_buffer.h** (master → e6afb9c)](https://github.com/nothings/stb/blob/e6afb9cbae4064da8c3e69af3ff5c4629579c1d2/stretchy_buffer.h) ---- 
Just create a new *Windows Console Application*. You should be good to go.
It's just standard behaviour - if you assign a 16 bit unsigned int to a 32 bit unsigned int then the high bits of the unsigned int will be zeroed. If you think about it then it can't really be any other way, otherwise assignment would be broken! (Similarly for signed ints, you get sign extension rather than zeroing, again for obvious reasons.)
Thank you for the feedback :)
I installed everything related to c++ and c# . You should be good to go for your assignments.if you are low on disk space you can try installing only the desktop applications c++
You can also just install MinGw and atom, it's normaly what I use, and it works really well. Sure, it needs a bit of work to get it working, but in return you get a fast compiler and an editor with good support for C and many other languajes.
Its pretry simple, just google "how to install gcc on MinGW" And atom is just one of the best editors I've ever use. Its an open source code editor, that allows you to add support to several programming languajes by adding "packages". Those are addons that you can install on the settings menu. It also have hundreds of other addons, like arduino compilers, debbugers, etc... It usually uses less resources than VS and runs faster. And once you set it up, is as easy as writing your code and clicking compile &amp; run. And it also comes with github support so you can manage your projects a lot easier, and upload them to github.
Btw, remenber to add gcc to your PATH variable on windows, again, just google it, its a pretty simple process.
so it can compile and run c program? what addon for compiling should i use? i saw it supports 64bit, does it support 32bit too?
Yes, you can. It already comes with all the addons you'll need, but you can always write "c compiler" on the package search bar. I'm not sure, but you cam always look it up on atom's web site. 
i tried to use eclipse, which requires all these stuff with gcc and shit. the problem was with eclipse, so my gcc is installed in PATH is set(c:/mingw/bin right?)
Yes, but I've never use eclipse. Its really simple, but if you're still having problems with it, just install DevC++. 
But I recommend using atom, it'll save you a lot of time
No problem, whatever you need just let me know, good luck!
Thanks for your answer. I know the existence uint\* types and I can see why the're better but our course is pretty basic and they want us to stick with int, unsigned int etc. Also they told us that we are using C90 so I'm forced to declare loop variables and the other ones too at the beginning in the main because otherwise the compiler will complain about it \( I use \-std=c90 as an option\). You're absolutely right, I don't know what I was thinking with this assignment \( best\_score = sum\_rows\(matrix\[0\]\)\) but I guess that this is how you learn :D I'm pretty sure that the sum won't overflow because the marks go from 1 to 5 with 4 columns so the sum of a single row goes from 1 to 25 and an unsigned int will be enough of course.
Can you try Visual Studio Code with C++ extension? 
i installed it too, but couldnt figure out how to run c program there lol. why all these stuff are complicated? 
https://code.visualstudio.com/docs/languages/cpp try this documentation? 
While devouring your memory. Electron API is awful.
I know, setting up some stuff might be daunting and sometimes frustrating too, because seemingly nothing productive comes out of it, but it's sort of a one-time overhead for your work. 
Really? I've never used electron before (I know nothing about web developing), but atom have always worked fine for me, even in my old core 2 duo machine (with 4 gb ram)
Electron is an API that Atom uses. I'm confused how atom even launches on your core 2 duo machine. Atom would kill my work laptop with the same spec's whenever I tried to launch it.
I didn't know it is power hungry sine atom never had issues in my workstations (it was the same using windows 10 an manjaro), and many people recommend it. But isn't it quite popular?
Mostly because it is a github project (and having excess ram is common these days). They may have done some work in the last 6 months or so. One of the reasons it uses so much memory is the Electron API. Essentially Electron is an API that provides a web browser backend (chromium) so you can develop a desktop program using NodeJs. It's all of this extra code that's a huge hit on your memory. Plugins usually only make it worse.
Makes sense, although it seems that at least It has been optimize a little. Btw, do you know about memory optimization? I writing a simulator for Chip 8 for the esp8266, and while it's nowhere near complete, it is running slower than other projects I've seen on github. So I'd like to know how to make it a little more efficient.
Is `PTR_SIZE` used anywhere? A quick search suggests no...
Compiling on window with VS was what put me off learning C for over 2 years. I don't remember the error code, but what I ended up having to do was invoke the compiler commands in the 'visual studio command prompt' instead of regular cmd. For some reason that's tbh e only way it can find vsvars.dll (I think it was). I also have gcc and half a dozen versions of VS but out of all of them, only VS 2015 will work. I suspect that in installing all of those compilers, I screwed up my registry and or environment variables so bad I'll never bother trying to fix them. Thankfully, VS 2015 is more than enough, feature wise.
Nope, used that in the first prototypes and forgot to take it out. It's gone now. Thank you :\)
There's a version of MinGW that installs some unneeded package management stuff on your computer and that tries to install GCC over that (which is the version that the devs try to push), and there's a truly portable MinGW that is basically just GCC in a .zip file. The .zip one will likely cause way less headaches to get running if you run into trouble with the package manager MinGW thingy.
What, exactly, do you need help with? What do you mean by "storing" and what do you mean by the "top" values?
First, formatting: void top5Winners(){ int i, j; FILE *fp; fp = fopen("player.txt", "r+"); printf("\n\tPlayers wincounts: "); for(i = 0; i &lt; 5; i++) { fscanf(fp, "%s %d %d", pl[i].players, &amp;pl[i].balance, &amp;pl[i].wincount); printf("%d ", pl[i].wincount); } printf("\n"); fclose(fp); } ***** So the code here appears to be initializing an off-page array of structs with values. I believe you need to actually sort the array of structs first, then you can loop over the first five elements to output the 'top' five values. Are you asking for how to sort the array? Cause there are many ways to do that, although I'd say that [Bubble Sort](https://www.geeksforgeeks.org/bubble-sort/) is one of the easier ones to implement in code.
Sort an array with any sorting algorithm and return the address pointer.
&gt; I really would not advise using VS for C programming. My girlfriend just went through my CS program's C programming class and the asshat of a professor had everyone do everything through Visual Studio and it ended up being pretty weird. Libraries were constantly messed up and it just felt like the wrong tool for the job. Even though I have 2 years of C experience I couldn't help her with basic troubleshooting because debugging is different in VS than with traditional tools like GDB and Valgrind, and the standard libraries were definitely not standard at all. I'm too drunk to give a contructive reply, I will just link this https://i.imgur.com/JevlDNt.jpg
If this is just an intro programming course, sorting and picking the top 5 is probably just fine. But you should know that, in principle, this can be done more efficiently (O(N) vs O(NlogN)) using a variation of [Quickselect](https://en.wikipedia.org/wiki/Quickselect). 
Hey, install the C++ crap, then create a new project. In the prompt, select create Visual C++, Windows Desktop Wizard and press ok. Then, only have empty project selected. ONLY. Then, in your solution explorer, right click on source files, press on new, file, then name your new file with a .c file extension. Now you are able to compile C programs from vs. Enjoyn, hope this helped, reply if you need help.
GNU GCC.
Why use [this](https://github.com/NateSeymour/urbanize/blob/master/src/list.h#L12) or [this](https://github.com/NateSeymour/urbanize/blob/master/src/list.h#L13) , why not use unsigned char and unsigned char\* or better why not uin8\_t and uint8\_t\* ?
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [NateSeymour/urbanize/.../**list.h#L12** (master → e07817c)](https://github.com/NateSeymour/urbanize/blob/e07817c23053511e013d300b084edf1e35c9e003/src/list.h#L12) * [NateSeymour/urbanize/.../**list.h#L13** (master → e07817c)](https://github.com/NateSeymour/urbanize/blob/e07817c23053511e013d300b084edf1e35c9e003/src/list.h#L13) ---- 
Isn't it just 1 byte though? Shouldn't make a diff right?
if i understand what you're trying to do, then you're confusing `char *` and `char **` types. the former is a pointer to a (sequence of) char. the latter is a pointer to a (sequence of) pointers to the former.
Agreed..I'm trying to follow the code and there is so many #define for almost everything. Really hard to read
Here are issues I noticed: The standard reserves "all external identifiers that begin with an underscore." This includes all those "private" `_list_*` functions. You don't need to put prototypes for these functions in the header at all. Make them static so they don't have external linkage, and don't mention them in the header. The name `_S` is also reserved for the implementation as a macro. Some compilers support pointer arithmetic on void pointers as an extension, but this isn't permitted by the specification. To do byte arithmetic, use a char pointer instead. Don't use `#define` to define new types. Use a `typedef`. It's more precise. Really, don't define `BYTE` and `BYTE_POINTER` at least not in the header. That's identifier clutter that could collide with other identifiers (e.g. someone using both your library and `windows.h`). Generally avoid pointer typedefs. `_this` and `_S` is macro abuse. It makes the code a lot harder to read. Keep it simple. Like with the "private" functions, you can move your `struct _list` out of the header and make it an opaque type. This means you wouldn't need to use all the annoying `_` prefixes to indicate that they're private. Check for overflow before you multiply when computing the size of allocations. If the list grows large enough, you could end up silently shrinking it rather than growing it. Similarly, you should be using `size_t` for any quantity that holds the size of an object, and for indexing into these things. Similarly, do pointer arithmetic with `ptrdiff_t` or `size_t` rather than `int`. This example appears to be wrong since `list_at()` returns a pointer: int item = list_at(list, 0); As an overall design note: you're losing a lot of efficiency on x86 for small-sized (&lt;= 8 bytes) elements since `_item_size` is a run-time value. Those multiplies must be done explicitly with full multiply instructions rather than as part of an addressing mode. Those dynamic `memcpy()` calls are paying a similar heavy price. 
Good points. What are you implying when you mention the standard 'reserves' all external identifiers that begin with an underscore? 
Cool, you're doing good.
The implemenation may define internal functions and variables with these names. Since they're reserved it shouldn't be a problem. If you use them in a program then it may compile incorrectly or not at all.
It took me a moment to really 'get' how this works, but it's pretty clever. The use of `typeof` definitely makes things a lot cleaner, I wish that was standard. That said, I have a few few ideas that might make it easier to use. For `list_at`, I think there's a bit of a problem [here](https://github.com/NateSeymour/urbanize/blob/master/src/list.c#L10). Basically, regardless of the size of the thing being stored you treat it as though it is always a pointer and only return a pointer. This won't work in every situation, particularly for `struct`s which are larger then a pointer in length. This should result in errors when you attempt to use it, since the return type would be wrong and an implicit cast won't exist. You should be able to fix this by making `list_at` a macro instead, fixing the return type issue, but adding the issue of specifying the type (Note I haven't actually tested any of those code and didn't finish all of it, it should be close to correct though): #define list_at(list, index, type) \ *(typeof(type) *)(list-&gt;_heap_ptr + (list-&gt;_item_size * index)) With a statement expression or ternary, you should be able to also do the bounds checking you're doing as well. Alternatively, you could define `list_at` to take a pointer to a variable instead. This simplifies how it works, but makes it a bit less flexible since you can't just call `list_at` to pass to a function or etc.: void list_at(ptr_list list, int index, void *dest) { memcpy(dest, /* Stuff goes here */, list-&gt;_item_size); } If you hide it behind a macro you could also do some rudimentary type-checking: #define list_at(list, index, dest) \ ({ if (sizeof(*(dest)) != list-&gt;_item_size) { \ /* Some type of error here. You might need a separate return variable, and set it to NULL */ \ }; list_at(list, index, dest); }) You can use a similar technique to achieve a much easier to use foreach: #define list_foreach(list, var) \ for (int __finished_flag = 0; !__finished_flag;) \ for ((var) = list_at((list), 0); !__finished_flag; __finished_flag = 1) \ for (int __index = 0; __index &lt; ((list)-&gt;size); ({ int __tmp = ++__index; if (__tmp &lt; (list)-&gt;size) (var) = list_at((list), __tmp, typeof(var)); })) It looks pretty funky (because we can't initialize `var` on the same line we create `_index`) but it works like any other `for` does, meaning the statement or block after it is what will be executed, and `continue` and `break` will work as expected. The ugly garbage on the last line is to avoid two cases of undefined behavior. The first is that we can't use `__index` a second time after incrementing it, so we need a temporary (Which we can get easily via a statement expression. You could also use another `for` instead). The second is just that we can't use `list[__tmp]` if it is out of bounds, so we do an `if` check. I think you could so some tricky stuff with a ternary to avoid the `if`, but it would get weird - you couldn't use a regular ternary because there is no 'default' value for `var` that you could use. With that, you can do stuff like this: ptr_list list = create_list(sizeof(int)); /* Add entries here */ int val; list_foreach(list, val) printf("Value: %d\n", val); list_foreach(list, val) { printf("Stmt 1: %d\n", val); /* Don't run stmt 2 if val == 2 */ if (val == 2) continue; printf("Stmt 2: %d\n", val); } Much nicer then having to use a separate function and a function pointer. I also have a few comments on the header, mostly that there are too must implementation stuff leaking out in there. I don't personally like your use of the `_this` macro, but I've seen worse things. I would not use it in your header though - It's a bit confusing, and also `_this` is not really all that uncommon of a name, so you're risking breaking someone's code by having such a macro defined. With that, my suggested changes for `list_at` would make `BYTE` and `BYTE_ARRAY` unnecessary, which I also think should be removed. Also, don't declare functions in the header that shouldn't be called externally, such as `_list_set_blocks`, and don't declare the `_list` `typedef` if nobody needs it. If you need some of this stuff for your unit testing, you can hide it behind a separate header to be used by the tests. In your macros, don't use a name like `VAL` for your temporary. In the Linux kernel they like to do stuff like `____val` with lots of underscores to ensure there aren't any collisions. This is a bit of a pain-point in C and there's really no good solution to picking a unique identifier, but I think `VAL` is a bit too simple of a name.
Ah. So depending on the compiler there may be duplicate names . Gotcha.
Part 2: http://250bpm.com/blog:8
Cygwin for the rescue
In part2, &gt; However, imagine the object is contained in 10 different lists. struct person { struct person *prev; struct person *next; int age; int weight; }; struct { struct person *first; struct person *last; } people; How do we solve this problem efficiently in C? How to embed the same single person in 10 different People lists?
Thank you very much for your feedback, I have implemented a lot of your ideas, but would like to keep list_foreach how it is. I find it more practical to loop through with a function. This has been a great help, and I've learned a lot!
You could remove the prev/next elements and use 10 vectors of *\* person*, and the stick the same person-object in multiple lists.
Just add more prev/next pointers. BTW, this is called an intrusive linked list and you can find them in OS kernels everywhere.
&gt; why is there no function prototype for main() https://www.artsy.net/artwork/brian-kernighan-hello-world
Yeah, citing K&amp;R C... this is pre-ANSI-C89, ie. pre-standard C. We didn't have prototypes back then. We have since C89 though, and anything since should use them unless you want undebuggable errors.
ye line 17 should be kilojoules[i] i think
The bigger question is, if your person pointer is in 10 different lists, how can you prevent someone from deleting it while you're using it?
Yeah. Performance isn't as bad as you'd think: * If you don't care about order, just move the last element to the now empty spot for O(1) complexity. * If you do care about order, just memmove-ing a small list is still extremely quick. * Iterating on a linked list is abysmally slow (due to the frequent cache misses) that it dwarfs pretty much anything you'd want to do with a simple vector. [Here](https://www.youtube.com/watch?v=YQs6IC-vgmo) is a talk by Bjarne Stroustrup about it.
&gt; why is there no function prototype for `main()` What? I've never seen anyone specify a function prototype for *main*, and with a cursory glance at the top C repos on github I can't find any that does.
Please post your code as text instead of an image. 
&gt; But that’s because c compilers today generally zero out newly allocated char arrays No, they don't. But the OS zeroes out memory that it hands out to processes, so until you start freeing (and therefore reusing) memory, you're going to get zeroes.
I wrote [this program](https://github.com/idietmoran/Display-Lock) using VS and C. I just start an empty project, but you can just change the file extension to `.c` and everything should compile just fine.
&gt; f pointer !=NULL then !pointer == NULL The result of the `!` unary operator is always of type `int` and equal to either `0` or `1` (cf. C11 §6.5.3.3). &gt; I do not remember what the standard says about null/0/1. Usually null is (void*)0 but I seem to remember that there is no requirement for that. `NULL`, not “null”, expands to “an implementation-defined null pointer constant” (cf. C11 §7.19) which is defined as “[a]n integer constant expression with the value 0, or such an expression cast to type `void *`” (cf. C11 §6.3.2.3). The definition of an integer constant expression is slightly complicated, but can be roughly summarized as an expression of (or cast to) an integer type, with no side effects, which can be evaluated entirely at compile time (cf. C11 §6.6). It is common to use `((void *)0)` because it will throw a warning if used in non-pointer contexts.
Surely you mean `SIZE_MAX` in `&lt;stdint.h&gt;`.
Shit, thank you! I had been up for about 20 hours and was completely oblivious as to what I was doing wrong, didn't realize a character pointer took more space than a character
You have to use `std::reference_wrapper` to store references in things, and references still does not protect you from shared owner ship problems. Especially for something like ZeroMQ where the article (and comments) keep talking about the importance of threads in its design. C provides no safety there, and the amount of manual locking required is probably more of a performance hit than anything.
Thanks. You’re right. A false assumption corrected.
I do, eg.: * https://github.com/FFmpeg/FFmpeg/blob/3703f13333e24540a5ef132e7b2a9c0ded7e4531/libavutil/tests/cpu_init.c#L38 * (not main, but Linux' main function) https://github.com/torvalds/linux/blob/master/init/main.c#L1096 These were in the top definitely, found within a minute, all specifying a prototype for `main()` (I did not count those who specified the prototype of the form `main(int argc, char **argv)` or equivalent). Besides: There's good reason to always specify prototypes, why do an exception and write `main()` but for every other function taking no arguments write `f(void)`? Note: I do not mean `main()` should be forward-declared, that's something different.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [torvalds/linux/.../**main.c#L1096** (master → 2d618bd)](https://github.com/torvalds/linux/blob/2d618bdf71635463a4aa4ad0fe46ec852292bc0c/init/main.c#L1096) ---- [^delete](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dybl7r7.)
Also `main()` instead of `main(void)` and `int* p` instead of `int *p`. This is really full of C\+\+isms.
Yeah, that's what I meant with "no prototype for `main()`, `main(void)` is a prototype, `main()` isn't. And the pointer-style is horrible, but eh, it's at least up to taste and not that "wrong"
I mean references the general concept, not the specific c++ implementation
I'm glad you found it helpful :) I can understand the hesitation over `list_foreach`. I personally like having macros like that since it can make the code easier to read once you get used to them, but that one was particularly ugly. The Linux Kernel uses a different technique for implementing generic lists (Though they're linked-lists, not arrays, so the applications here are different), and their foreach turns out [much simpler](https://github.com/torvalds/linux/blob/master/include/linux/list.h#L463) since they don't need the extra `index` variable. Also I small thing, I saw you updated [`list_at`](https://github.com/NateSeymour/urbanize/blob/master/src/list.h#L22), I *think* you actually want to use `typeof(type)` in their rather then just `type`. It's weird, but with more exotic C types the `*` doesn't always go at the end. Ex. A function is `void()(int, int)`, and a function pointer is `void (*)(int, int)`. If you use `typeof`, then things just 'magically' work out because it's like using a `typedef`, but without `typeof` you might just get a syntax error.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [NateSeymour/urbanize/.../**list.h#L22** (master → 0738a17)](https://github.com/NateSeymour/urbanize/blob/0738a17923f964ee12daa1d080557571f9afabd9/src/list.h#L22) * [torvalds/linux/.../**list.h#L463** (master → 2d618bd)](https://github.com/torvalds/linux/blob/2d618bdf71635463a4aa4ad0fe46ec852292bc0c/include/linux/list.h#L463) ---- 
any idea what is this error? https://imgur.com/rFYXzia 
&gt; Note: I do not mean main() should be forward-declared, that's something different. Ah, I see. I thought you meant that. The typical definition of 'prototype' does not include the function body (e.g. Wikipedia:Function_prototype: [[...]a declaration of a function that specifies the function's name and type signature [...] but omits the function body](https://en.wikipedia.org/wiki/Function_prototype). But then again, actually reading the C standard you find that ["A function prototype is a declaration of a function that declares the types of its parameters."](http://www.open-std.org/jtc1/sc22/wg14/www/docs/n1548.pdf), so it *is* correct, but a little confusing.
&gt; Note: I do not mean main() should be forward-declared, that's something different. Ah, I see. I thought you meant that. The typical definition of 'prototype' does not include the function body (e.g. Wikipedia:Function_prototype: [[...]a declaration of a function that specifies the function's name and type signature [...] but omits the function body](https://en.wikipedia.org/wiki/Function_prototype)). But then again, actually reading the C standard you find that ["A function prototype is a declaration of a function that declares the types of its parameters."](http://www.open-std.org/jtc1/sc22/wg14/www/docs/n1548.pdf), so it *is* correct, but a little confusing.
Hm yeah, it's kinda difficult, since even a declaration in C doesn't necessarily constitute a prototype even, and a function declaration as part of a definition can specify its parameters but still not be a prototype: * `int main();` -- declaration but no prototype, specifying no parameters (not: takes no arguments, it says that there are no parameters specified, ie. the number of arguments it takes are unspecified) * `int main(void);` -- declaration and prototype specifying the parameter that this function takes no arguments * `int main() {}` -- declaration as part of a definition specifying that this function takes no parameters, yet no prototype for this function, that is the compiler is still not allowed to reject this code, if it cannot guarantee that it will always be UD * `int main(void) {}` -- declaration and prototype as part of a definition, specifying that it takes no arguments. So basically Wikipedia and C have really different notions of prototypes.
Hm. I read some more in the C11 standard, and it's a little more complicated: [6.7.6.3.14](http://www.open-std.org/jtc1/sc22/wg14/www/docs/n1548.pdf): An identifier list declares only the identifiers of the parameters of the function. An empty list in a function declarator that is part of a definition of that function specifies that the function has no parameters. The empty list in a function declarator that is not part of a definition of that function specifies that no information about the number or types of the parameters is supplied. So, `int main() {}` is actually a prototype since it is a part of a definition as well.
Read up on how to format code for Reddit.
Nah, it's like I said it says that `main()` has no parameters -- but it still doesn't constitute a prototype. Compare: $ cat test.c void f() { } void g(void) { f(32); /* UD but compilation allowed (warning) */ } void h(void) { g(32); /* Syntax error (error) */ } $ clang -std=c11 -pedantic -c test.c test.c:2:20: warning: too many arguments in call to 'f' void g(void) { f(32); /* UD but compilation allowed (warning) */ } ~ ^ test.c:3:18: error: too many arguments to function call, expected 0, have 1 void h(void) { g(32); /* Syntax error (error) */ } ~ ^~ test.c:2:1: note: 'g' declared here void g(void) { f(32); /* ok */ } ^ 1 warning and 1 error generated.
I think you are missing a feature/features needed to compile C. Run the installer and make sure you have C++ Desktop installed.
lol undebuggable error in the main prototype when you're not using command line parameters... yeah....
I didn't refer to `main()` exactly when I said "undebuggable errors". But just for consistency, why should you use `f(void)` everywhere but then use `main()` instead of `main(void)`? In most cases the reason why the programmer used the former, non-prototype variant, is because they don't know that `main()` doesn't specify a prototype and thus it's no (syntax) error to write `main(42);`.
Why can't each person just have a reference counter? When it's deleted from the list then you call a delete function on it. Under the hood the person pointer has a reference counter which gets decremented in this function. If the delete function detects the ref counter has hit zero, it deallocates the person object.
yea or &amp;kilojoules[i] does the same thing
I'm not sure you know what you're asking for. You'd like to avoid reinventing the wheel, however, you're looking to write a codec by hand in C of all languages. Furthermore, you're asking for a DLL (Dynamic-link library exclusively for Windows) that "takes the picture and encodes it" for your target platform, Linux. This is weird as hell - maybe you would be better off finding another project.
Checkout mplayer/mencoder
&gt; To avoid concurrency problems, I use mutexes whenever a write is made to one of the variables inside the shared struct. You will also need to use the mutex whenever a read is made from one of the variables. &gt; Right now I use a different mutex for each variable, so that I avoid locking a thread for trying to write to a variable that is not being written to, but I'd like to know if there is a better way to avoid this problem. This may be a good approach if you expect multiple threads to contend for different fields, rather than for the same field. But be wary of [false sharing](https://mechanical-sympathy.blogspot.com/2011/07/false-sharing.html), where updates to different variables in the same cache line cause contention just like updates to the same variable would. You can mitigate this effect by adding padding so the fields are on different cache lines. Depending on the complexity of your operations, you may be able to use atomic operations to update the fields rather than protecting them with locks. This is not always a performance win -- do some benchmarking to see. &gt; Also, considering that my struct has uint16_t variables, is there any additional measures I should take to guarantee data consistency across threads because of memory alignment and multiple variables stored on the same data register? What happens in registers doesn't matter -- those are never observable to other threads. To completely answer this question you need to know the details of the memory model for the platform and compiler you're targeting. In 99% of cases everything's fine, writes to one 16-bit field won't touch any other data in the struct. But there are some architectures that don't support small writes, and would implement a 16-bit write with a 32-bit read, mask, and 32-bit write.
BCD_t is 32 bits wide. I don't see why having a single one as a global variable really saves you anything over using local BCD_t variables wherever necessary. Presumably you have 32 bits of stack space available, even on a microcontroler? Also, does it really need to be `volatile`? But the idea is sane, this should definitely be faster and smaller code than repeated divisions/modulos to extract decimal digits.
This is a great comment, and I agree that many of the author's points seem either contrived or purposefully constructed in such a way that C appears to comes out on top. I left feeling completely unconvinced by their points.
Logical points, here is the problem: The final executable (decoding after transfer) has to run on linux, also the encoding would most likely need to be done on a drone (not sure what type of architecture or OS it will have, but native C solution should work anywhere). I would like to know of my options, could C++ solution work? What can I use, where can I start looking? To explain why I did not know what Im asking - Although Im not new to programming but hardware and linux arent exactly my field (Im working on C# mainly). P.S. When you dont know what to ask of google you ask people who have experience in the area, right?
Thanks, will check out right now.
I've read this article some time ago. To me it looks more like a complaint about someone else's (or one own) way of coding, since C++ provides tons of mechanisms that do not necessarily mean that they must be used by everyone everywhere. The point of the inability of using non-throwing constructors is a bit flawed, since you can default-construct and initialize all the instances (what you would be doing in C) or make static factory functions that return that wonderful error code like most C library interfaces do. In addition, I would take the article with a piece of salt. While most of the arguments make a point, some of them are already out of date (the article has 6 years old), such as exception specifications.
Oh, I think I understand your scope now and I really don't think your current approach would be very fruitful. Neither is this sub the most appropriate to ask product development, but you get that now. I'd look into some SOC's that can do h.264 on hardware or preferably h.265, if you want to do very high resolution. Size and weight would be restrictions though. The RPI3+ does 1080p30 h.264 encoding, is low power and could be written in almost whatever you'd like, so that's a starting point. Look into the 1.3 GHz video transmitting scene too, that'd give you a good platform for low latency and high bandwidth. Seems interesting though! 
Yeah, and references, by their nature, do not offer any protection from other people trying to delete your object while you're using it.
Yes. And in C, reference counting is manual and error prone. Almost completely negating any benefits of going to C in the first place. It's basically trading in one small annoyance in exchange for a huge problem.
How do you figure? The whole point of references to is track how many people are using an object and then delete the object when no one is using it.
christ man, i didn't ask your opinion about it
What you describe is called yoda conditions. Real LST: Use `-Wall -Werror`.
That's why I check all of my control flow statements first when I have unexpected results
Yeah, so? Doesn't mean they aren't wasting a lot more effort effort than necessary to get these things right, and to avoid risk accidentally breaking something.
Actually I disagree with you but I think it's one of the things you need to experience once you've managed very large C or C++ projects.
What makes you think I haven't?
It's a [cast](https://en.wikipedia.org/wiki/Type_conversion)
**Type conversion** In computer science, type conversion, type casting, and type coercion are different ways of changing an entity of one data type into another. An example would be the conversion of an integer value into a floating point value or its textual representation as a string, and vice versa. Type conversions can take advantage of certain features of type hierarchies or data representations. Two important aspects of a type conversion is whether it happens implicitly or explicitly, and whether the underlying data representation is converted from one representation into another, or a given representation is merely reinterpreted as the representation of another data type. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/C_Programming/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
You could also use `if (!(a-5))`if you don't want to go all Yoda. (Just kidding, don't do this)
C++ is off topic in this subreddit. Please post C++ questions to /r/cpp_questions.
For the next time: Please do not post pictures of error messages. Always post code and error message as text!
It's portable. And to be pedantic, there is no stack in the abstract machine.
In the era where compiler reports are so detailed and powerful this can't be a more appropriate thing to do. Using a linter helps to find these kind of silly errors.
Yeah helps with preventing accidental assignments which can be subtle to debug. 
I just don't understand how this becomes a problem. I always found the loss in readibility not worth it.
So, so, so many posts on /r/learnprogramming, here, and other help subs are solved by "use warnings and *pay attention to them*". People aren't taught how to properly use their tools or think critically, and it is painful to see.
Interesting.
Meh. If you make this mistake that often you just need more practice. Used to happen to me a bit, but I've learned to double check that stuff even as I'm typing it.
I also got same problem with when i upgraded my visual studio 2017. I guess the reason for this problem was not properly updating my visual studio with the latest updates. 
 for( init_stuff; test_stuff_for_true; change_values_of_stuff ) { do_once_per_loop_test_step(); } Take a shot at it.
Or maybe you're just not as competent as you think you are.
lol
for (count = 0; count &lt; MAX; count++) 
Playing around with bleeding edge C++, huh?
Hi BornTuft, I'm not going to help you fix your code, but I am going to point out a couple of things to get you thinking to help fix it. Before I touch on the code, did you plan the code by writing an algorithm first? Before I write a line of code I go through four steps. One, work through a problem logically and specifically. Two, do the same thing again, but this time write down your steps. Three, look at what you wrote and rewrite, this time generalize so that the program can solve the problem in general. Four, go through the generalized algorithm and make sure it works logically. Then, translate to code, test, fix, etc. Onto the code... Lines 15 &amp; 36 - getline() is already a function in stdio.h. Are you sure you meant to redefine getline()? Line 16 - Your prototype of int strindex(char source[], char searchfor[]); does not match the function definition in line 52 - int strindex(char s[], char t[]). I see in lines 42 and 45 that you are trying to fill a char type with int types. Compiler is going to have a hissy fit. I can see what you are trying to do, but leave it up to you to fix your code. That is what makes us learn as programmers. Welcome to C programming! 
Hello world code
I have thought about this before and I know, it’s aesthetics but I find putting the constant first just plain ugly. But it gives rise to interesting humorous constructions. if ( 8==D ) { ... 😝
Is that where it’s got to now?
C++20 is going to catch up to where Perl was in the early 90's and add the spaceship operator, yes.
I don't always write C++, but when I do, I use the absolute latest features. (because who needs compiler portability, right?)
Honestly, I would suggest trying out c on a Linux system as basically, the build process is simpler and almost everyone can help you. All you need here is to know how to use a text editor, learn how invoke gcc and learn how to write a simple make file. Honestly, once you have gotten the hang of make files, not having an IDE is not too big of a deal, I think, if your project is just a few thousand lines or less. I also find reading make files a lot easier than looking at vs project files but that is probably a bias and I will admit I don’t know how the current vs community edition is. After you learn that on a Linux, you’ll probably appreciate MinGW. There’s also http://www.smorgasbordet.com/pellesc/ I have to warn you, this compiler doesn’t nearly warn you enough cmp gcc, it will give you more rope to hang yourself but for a GUI wrapped ide for a single source file c program on windows, it’ll do the job, I’ve done small win32’s using this in the past (freely distributable license).
Between structured bindings, string_view and a few other things, not being able to use C++17 in a project is painful. Anything older than C++11 is unthinkable. 
Sounds like `struct blkpg_ioctl_arg` is never defined anywhere that the compiler can see before you start trying to use it.
Someone told me to include header files defined the missing structures or macros like BLKPG_RESIZE_PARTITION. phung@UbuntuHW15:/media/phung/b0a26f1f-6d1b-4b3b-a638-e3ef39763638/root/Documents/mxp/linux-xlnx$ git grep BLKPG_RESIZE_PARTITION block/ioctl.c: case BLKPG_RESIZE_PARTITION: include/uapi/linux/blkpg.h:#define BLKPG_RESIZE_PARTITION 3 I added this header file, but it still can't find BLKPG_RESIZE_PARTITION. Why ? #include "../include/uapi/linux/blkpg.h" 
I have no problem with reference counting. It's how to do it correctly. In C++, you have shared_ptrs. If, for performance reasons, you have to write your own, RAII can be used to make sure it's done correctly.
It really depends on why sharing is needed. It is unavoidable in some cases. For example, I am working on a codebase with copy-on-write objects. You need shared stuff for that because that is the definition of the problem. In other cases, I've seen shared pointers being used to share things like files or sockets or things like that over threads. In those cases they could be managed by a work queue instead.
The Linux kernel seems to get it right, most of the time. I wonder how they manage that, actually.
"Ripoff" well if you want to put it that way. Note that Apple was by no means the first to do this. But proper universal binary support like Apple's is much better and I wish Microsoft would do it. This solution is brittle. For one, it depends on x86 emulation. But it's too late for that now to make sweeping changes to Win32 things. Appxbundles are already universal. 
https://lwn.net/Articles/693038/ https://lwn.net/Articles/575460/ And yet, elementary mistakes get made very easily.
Sure, after Apple [ripped it off](https://en.wikipedia.org/wiki/Fat_binary) of Microsoft and Digital Research, of course.
Agreed. This is one area where C++ can help, even if C++ is still riddled with a ton of other problems. :/ That said... does shared_ptr demand that you use classes? Or can you just simply substitute raw pointers with it?
When I modify https://github.com/VectorBlox/linux-xlnx/blob/mxp/include/uapi/linux/blkpg.h#L1 to #ifdef _LINUX_BLKPG_H I have this error https://paste.ubuntu.com/p/36SMB5ShTV/ instead
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [VectorBlox/linux-xlnx/.../**blkpg.h#L1** (mxp → ad3a864)](https://github.com/VectorBlox/linux-xlnx/blob/ad3a864ee2b0745aed02e40e8d7a81683d2558f5/include/uapi/linux/blkpg.h#L1) ---- [^delete](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dydehfm.)
Thanks for the response. I hadn't accounted for shared cache lines, but I'm running on a single core cpu, that means I shouldn't have to worry about false sharing, right? Also, how can I work with atomic variables in C?
May be formatting code correctly would help a little to understand what you're doing.
Thanks for your reply. All of your concerns stem from the book I sourced. The code you see came from K&amp;R C Programming Language 2nd Edition. The exercise asks us to extend the code that is in the text book. The book mentions `getline()` is in `stdio.h` however they want us to understand how the process works and they show a simpler version of it. etc etc etc 
There are no "pure programming" downsides to using C++. The standard library is an awesome resource of fantastic programming. The reasons why I choose C over C++, the reason why I keep on mentioning "large project management" issues, is because of the actual *cost* of choosing C++ over C, like on a balance sheet. It literally costs more. The user is absolutely correct when he says that good programmers are more competent computer scientists, but when you hire C++ programmers, you end up with expensive programmers, who are harder to find, and ultimately take longer to train. The larger number features of C++ do not seriously increase the number of possible use apps, but it does increase the knowledge required to operate in that space competently. It's phenomenally easier to constrain novice programmers with C, and therefore bring them onto a project earlier in a safe way. The reason I didn't make that argument with the other user is that all of his arguments are "pure programming" arguments that come from the perspective of someone who has never had to consider the other perspectives of project management, and on top of that, he's rude. Although, that's typical for beginners. This is obvious when you compare the successes of large programming projects like linux, windows, qt, chromium, and firefox. That being said, C is NOT appropriate for every use application and I would at times choose C++, if I felt like it.
That heavens gate trick is beautiful, I'll have to look at it more closely later. It would be a cool hack but I think there'd be problems when the 64 bit version depends on 64 bit libraries that won't be loaded. Perhaps that can be addressed but I'm not sure yet how.
The exit code is traditionally masked to its least 8 bit. I advice you to only use low exit codes in the range of, say, 0–63, because the other exit codes have traditionally been used by the shell to signalize termination by signal.
This has something to do with your environment. Which operating system are you using, what do you use to launch your program?
Yeah, but then you're stuck with compiler and os dependant extensions instead of a standard way.
The [POSIX standard](http://pubs.opengroup.org/onlinepubs/9699919799/) specifies this sort of thing. The exit status specifically is specified [here](http://pubs.opengroup.org/onlinepubs/9699919799/functions/exit.html).
Also see the manpage for waitpid for how to get a process' exit reason and status from another program.
I have the book. I'll take a look tonight, and show you the approach I was taught by Duke University. I think it will help you. It helped me greatly. 
I was taught it's bad practice. 
Thanks. I'll be applying these and it will help with future code
I'd like when prototypes have parameter names, since it can be seen by e. g. code completion so you don't have to explicitly look up in the documentation.
Apple definitely didn't invent the concept. Id had been de-facto supported by OS X from the start because NeXTSTEP already supported it. IBM's OS/400 and i/OS line have supported it for as long as I know as well, on top of JIT compiling from bytecode if native code wasn't found.
&gt; It does need to be volatile, since it is being updated frequently in a couple of interrupts. Yep, that makes sense! &gt; As to the original question, well, since I have potentially 5 different values that might require conversion to BCD for display (for reference, duty cycle, rpm, voltage, count, temperature, using a BCD_t variable for each of them would mean using 20 bytes of data memory instead of 4 I'm not arguing you should have 5 globals instead of 1. I'm arguing you should have 5 *locals* instead of 1 global. Locals only take up stack space (which I don't count as part of "data memory" anyway) while the function they're defined in is executing. So if you have one in each of e.g. `display_duty_cycle()`, `display_rpm()` etc., they won't be taking up any memory outside of those functions.
&gt; but I'm running on a single core cpu, that means I shouldn't have to worry about false sharing, right? Correct. &gt; Also, how can I work with atomic variables in C? If you have C11 (or at least `&lt;stdatomic.h&gt;`), you can use the standard way: http://en.cppreference.com/w/c/atomic Older compilers/platforms may use the `__sync_*()` builtins: https://gcc.gnu.org/onlinedocs/gcc/_005f_005fsync-Builtins.html#g_t_005f_005fsync-Builtins
Any relation to the fact that \-1 in 2's complement binary is equal to 255 in regular binary?
Yes
Integer overflow.
You could probably be more efficient using just one for loop.
OP could even use Mono (C#) on the rPi
So fun, related story. Before we invested in "devops" we had an automation engineer all by his lonesome. He wrote a script that would scrape our brochure site and check for mixed content (http requests on an https page) and linked content that now 404'd. This was part of the build process and had to pass before it could be deployed. However, he decided to make the exit code the number of failures. During an incident where the build server had a very shitty connection it got 256 errors. :) 
You have to keep in mind the width when you say that. For an 8 bit integer that's true, but for an n bit integer it's 2^n-1 (all bits set, which I assume you know). And then from there the previous comment claims it's masked down to 8 bits, explaining why the number is 8 bits of 1s, even though one would presume the int on their machine is 32 bits (I think those rules can be pretty weird though).
Not sure about encryption, but some of your functions have like 10 parameters..I suggest you wrap some of this data in a struct?
Okie dokie, I thought about it for a bit and wrote strrindex(s, t). There are a couple of ways to skin this cat, but it looks like the book wants you to do it this way (I'm prototyping and declaring at the same time here): int strrindex( char s[], char t ) { int i; int count = -1; for(i=0; s[i] != '\0'; i++) { if(s[i] == t) { count = i; } } return count; } 
I don't have time to write the whole program, but I posted the strrindex function code. 
&gt; I would like to know if my code is up to the standards of 2018's encryption rules. The short answer is no. But it depends on your threat model. &gt; Is the number of bytes of plaintext always going to be equal to the encrypted bytestream? It's a matter of perspective. Case in point: your program uses AES-GCM. The ciphertext itself is the same length as the plaintext (rounded up to the nearest multiple of the block size), but successful decryption requires the IV and authentication tag to be transmitted along with the ciphertext. 
cmake can handle that kind of thing fine. The problem is the time it'd take to learn all of the cmakelists guff you'd need to make it work.
We use cmake for embedded and it works but it's kind of horrible. Complex cmake build systems seem to turn into layrinthine nightmares. I've been converting some of our stuff to meson+ninja instead. It's much easier to read and maintain than cmake but it's also a little limited in some ways.
So I can be sure that it can do this kind of work. The question is now: is the end result more maintainable than what I have right now?
I'm actually wondering if the problem lies with the complexity of the requirement and not tool itself. At least, is the cmake script readable? 
I’ve found it to be very maintainable. For example if I need to add a file into every project, I can make that change in a single location. Or if we add support for an additional reference hardware it is fairly trivial. I’ve become a firm believer in cmake to help manage build complexity. So take my thoughts with a grain of salt. Obviously you know what your system looks like today and how maintainable it is. Cmake does have its own unique quirks and I’ve spent hours fighting with it sometimes, but I think it has been net positive for my product. 
If you use "modern" CMake it is. CMake scripts used to be a lot hackish until around version 3 of CMake, but it became a lot more declarative then it used to be. Also checkout Meson, it's an alternative to CMake thats getting a lot of support.
Xcode doesn’t, shame it is macOS only
Even with Visual Studio, I'd still recommend using a different compiler (such as gcc or clang, for instance) if you have the patience to set it up. From my experience, MSVC can be kinda questionable.
Do you happen to have some rough idea about the "speed" of either CMake or Meson? Since I end up compiling a lot for testing and whatnot, it could be a valuable metric. 
can you explain how to switch from MS to gcc in VS?
why do you say so? cant they build their own MinGW and the needed packages , and install it during the installation ?
No, cmake is basically ugly as hell, alas. Whoever came up with that syntax must have been inspired by pure evil.
how do i select the proper keys? 
I find that cmake dramatically organizes things, especially cross-platform builds. It’s a solid way to structure builds for projects that target multiple environments, including embedded environments. Just about every modern operating system kernel has an associated cmake package available, so it’s easy to access as well. Quick example: https://github.com/mcandre/tonixxx/blob/master/examples/fewer/CMakeLists.txt
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [mcandre/tonixxx/.../**CMakeLists.txt** (master → fd04187)](https://github.com/mcandre/tonixxx/blob/fd041878bb1f5678aa21ba1c373ef8a3e26639d8/examples/fewer/CMakeLists.txt) ---- [^delete](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dyfsjto.)
As a newer user of CMake, I can say that it can go either way. If you put the time into learning how to set things up properly, and you have the diciplin to keep it maintainable, it's beautiful. Not only will you be able to build for a multitude of targets, but you'll be able to build on a multitude of hosts. Since the rest of my organization is using CMake, I'm able to easily bring in components into whatever IDE I'm using for my project, when these components were never directly meant for my application. I'd strongly encourage you to try it. Learning it can be quite arduous, but the payoff seems to make it worth it.
everyone here keep on telling me using linux, as if setting up linux is easy. i have windows in my pc. i dont know how to run linux simultaneously. it quite a process to setup all those stuff. although setting up linux stuff in windows is also a mess lol 
You're thinking only in your particular use case. Again, in general, C/C++ IDEs are for Linux and on Linux bundling a compiler is a bad idea. On Windows, professional development uses mostly MS compilers, so bundling MinGW is pointless, and can also be a legal issue (I don't know if one is allowed to redistribute MinGW). Also, there is the elephant in the room that if you can't install the compilers yourself *you shouldn't be programming in C in the first place*. Also, there are multiple versions of the compilers, which version will it bundle? With which libraries? Etc, etc. In the end it's just too much trouble for IDE makers to waste time on this, especially the open source / free ones.
Google for Windows Subsystem for Linux (also know as WSL). It's a feature of Windows 10. However, if you have trouble getting to use Linux (or installing MinGW, as it seems) perhaps you shouldn't be learning C programming in the first place. If you are learning how to program, try a higher level language first, like Python.
And because Windows belong to Microsoft (MS is shorthand for Microsoft).
must have missed your comment. looks nice, but: https://imgur.com/hrxJKmn now what? errors everywhere
I recently discovered [tup](http://gittup.org/tup/), gave it a try and was pleasantly impressed. I grew tired of the overly complicated DSLs that most build systems seem to employ, getting your builds to work is sometimes harder than writing the actual code. Tup's DSL is not a scripting language, but it does allow you to use any binary to generate rules, which means you can script it with the language of your choosing, not more having to deal with all the complexities, bugs and design decisions of each build system. If tup is too light for your needs, I also suggest [bazel](https://bazel.build), which IMHO is a more sane alternative to cmake.
It seems that your comment contains 1 or more links that are hard to tap for mobile users. I will extend those so they're easier for our sausage fingers to click! [Here is link number 1](http://gittup.org/tup/) - Previous text "tup" ---- ^Please ^PM ^/u/eganwall ^with ^issues ^or ^feedback! ^| ^[Delete](https://reddit.com/message/compose/?to=FatFingerHelperBot&amp;subject=delete&amp;message=delete%20ID_HERE) 
i did install mingw, no problems. i will keep learning c, cause thats what they teach us at college right now. and im asking questions about compilers, cause i love to compare all possibilities available to me. if its not the right stuff to ask those question, you can tell me. but dont tell me to quit c.
If you choose Code::Blocks as C IDE you can choose to download a bundle that includes mingw. Personally I am using VSCode with [mingw-w64](https://sourceforge.net/projects/mingw-w64/) - just install mingw and modify the settings to use GCC.
You haven't given the full definition of the function. What happens when n = 1 or n = 2? Can you show us your work so we can check if there is any mistake? The 10th Fibonacci number is 55, so there is something else amiss, too.
Can you post your calculation? Than maybe someone can spot the mistake
Oh yes sorry if it equals 0/1 it’ll return the number. Which will be 0/1 respectively 
Actually it is 1 in both cases
Okay I’ll work on that quick and try to post a link. Or is there a way for me to upload a photo to this thread 
I'm not sure how you're getting 50 or 40, to be honest. You should either be getting 55 or 89, depending on how you define the base case. Note that you didn't provide that in your post, and that's kinda important! Here's how the calculation goes. For brevity, I'm going to call the function `f()`. `f(10)` needs the values of `f(9)` and `f(8)`, so we need to calculate those two first. Likewise, `f(9)` needs the values of `f(8)` and `f(7)`. So we have to keep going until we get to the base cases of `f(1)` and `f(0)`. To visualize this, I'm going to make a list: f(10) = f(9) + f(8) = ? f(9) = f(8) + f(7) = ? f(8) = f(7) + f(6) = ? f(7) = f(6) + f(5) = ? f(6) = f(5) + f(4) = ? f(5) = f(4) + f(3) = ? f(4) = f(3) + f(2) = ? f(3) = f(2) + f(1) = ? f(2) = f(1) + f(0) = ? f(1) = 1 f(0) = 1 Now we can work our way back up the list, starting with `f(2)`, using `f(1)` and `f(0)` to calculate the value of 2. Likewise, we can fill in `f(3)` ... working our way back up until we get to `f(10)`, finishing with the final answer: 89. f(10) = f(9) + f(8) = 89 f(9) = f(8) + f(7) = 55 f(8) = f(7) + f(6) = 34 f(7) = f(6) + f(5) = 21 f(6) = f(5) + f(4) = 13 f(5) = f(4) + f(3) = 8 f(4) = f(3) + f(2) = 5 f(3) = f(2) + f(1) = 3 f(2) = f(1) + f(0) = 2 f(1) = 1 f(0) = 1
It works both ways. If you use `f(0) == 0` and `f(1) == 1` then `f(2) == 1` and all is consistent.
[Here's a guide on the subject, courtesy of Microsoft](https://blogs.msdn.microsoft.com/vcblog/2017/03/07/use-any-c-compiler-with-visual-studio/)
my savior 
Thank you! This helped a lot. I didn’t understand what they meant by base case but now I see if I work backwards first starting from base case it all makes sense. Thank you
Forgot about that, nevermind
Well I'm sorry that they are teaching C in your college, because C programming is a *very advanced* programming, you need to understand **a lot** about the inner workings of the computer to even start to understand C. C programming is like surgery in the medical school, you have to learn *a lot* first to start doing it right, and even then it's messy and dangerous. Try to learn a bit about Linux first, get a Linux Distribution (I suggest Ubuntu).
I've used cmake in the past for a c and c++ based firmware project that was fairly large which included some libraries. Originally we used iar which was friggen horrible, then Kiel which was also horrible. Auto complete for both of these was friggen string based, what the hell. Eventually settled on cmake and clion and were were happy. It was trivial to have cmake download dependqncies and have diffirent build types for each version of the product. It was great, and after that paired with a proper IDE like Clion with a proper debugger like Ozone from segger, I will never go back.
Just pick up one of the recommended books and start solving the problems, imo. 
Yep, base case is key. Basically, the recursion approach is to keep simplifying the problem till base case.
Just as a matter of interest, if you count the calls for each Fibonacci number computed, the ration of calls between successive number computations also approaches the golden mean... albeit slower than the actual Fibonacci series... I think the series itself approaches the golden mean at around number 20 but the ratio of calls at around 35 or something. And here's the funny thing.... The call counts are not Fibonacci numbers!
Depends on the application. For synchronous communication, you would typically use some variation of [Diffie-Hellman](https://en.wikipedia.org/wiki/Diffie%E2%80%93Hellman_key_exchange) to generate a random [session key](https://en.wikipedia.org/wiki/Session_key). For asynchronous communication, you would generate a random key which you would encrypt with the recipient's public key and transmit along with the ciphertext. For storage, you would use a [key derivation function](https://en.wikipedia.org/wiki/Key_derivation_function) to stretch and diffuse the user-provided passphrase.
Highly recommend Head First C and of course the bible of C, The C Programming Language.
https://www.learn-c.org/ is pretty good for interactive learning.
Stephen Prata's C Primer Plus is really good for beginner. I've tried other books before but decided C Primer Plus is better.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [github/gitignore/.../**CMake.gitignore** (master → 18e2874)](https://github.com/github/gitignore/blob/18e28746b0862059dbee8694fd366a679cb812fb/CMake.gitignore) ---- 
Today, it's almost completely risk free. For example, if you have enough memory (4 gigs) and a dual core i5 or something similar, you can set up a 2 gig virtual box Linux virtual machine. And because it's walled off from your host machine, whatever mistakes you make within are not likely to ever harm your host (usually, unless you share file systems, for example). I just test drove Ubuntu 18.04 LTS, and the install was so Mac like... And you won't have compatibility issues as you're installing to virtual box. I don't know your comfort level but I think you could expand your horizons and experience with very little risk.
Open to remote workers? Engineer with 2 years experience in C and linux programming, currently working for a major Cable network in U.S.
Because GCC and Clang are third party software?
No, on-site in NYC only.
Anyone else have a soft spot for the Cutler Normal Form-style of C?
Is he looking for someone to fix problems in existing C code? I doubt you're going to be able to learn enough C in a short amount of time to do very well at this. 
Install FreeBSD. This is how I learned. And then fetch the source code from Subversion and you can start reading some of the best C code out there. Every program you run in FreeBSD has a source code under /usr/src and you can find it's sources in Subversion.
There's Code::Blocks version which doesn't install MinGW. You can install and configure it separately. Link: [http://www.codeblocks.org](http://www.codeblocks.org)
That does look good for someone starting out. To do actual work and/or to learn own your own you still need to learn how to compile and link programs. You can use gcc on pretty much any platform, so finding a gcc tutorial would be a good second step.
Could not agree more. We (class) had a situation where we were trying to figure out what was wrong with our code and it ended up being a linking issue. It was a very valuable lesson.
I just started doing it and googled what I didn't know. That's pretty much how I learn everything though.
After learn very basics I use this to dive intro projects and learn more as I go. https://www.reddit.com/r/C_Programming/comments/872rlt/c_project_based_tutorials/?st=JGSQ0DTS&amp;sh=ec67c45d
Getting paid to, like, solve puzzles in C? You'll need to explain that.
Thanks!
I don’t know what the purpose of the article is. What the motivation is. What it actually achieves.
cool story bro
As a test project a while back, I made a [small demo](https://gitea.hak8or.com/hak8or/Minimal_Embedded) running on a microcontroller that lets me do everything, meaning I genuinely am in control of the first instruction the CPU executes upon power up. Using a linkerscript like [this](https://gitea.hak8or.com/hak8or/Minimal_Embedded/src/branch/master/src/linker_script.ld) you say what is expected to be put where by the time main runs. After passing this through the compiler, linker, and assembler, the information relevant to the bss/etc sections gets put into the binary. You also give a "vector table", [saying](https://gitea.hak8or.com/hak8or/Minimal_Embedded/src/branch/master/src/startup.s) that the first few bytes of flash (where CPU gets it's data from upon boot up) are the stack pointer and where the first instruction is. Then even using C like [this](https://gitea.hak8or.com/hak8or/Minimal_Embedded/src/branch/master/src/startup.s) With all this combined, using C like [this](https://gitea.hak8or.com/hak8or/Minimal_Embedded/src/branch/master/main.cpp#L19) you can initialize the C environment.
No, it's not misleading at all, and it's dishonest to ignore the significant compromises required by modern CPUs to maintain C support, as well as the complexity of the compiler transforms to continue the lie that 2018 processor design works nicely with a language designed for 1970s hardware.
Good read. 
You should read it \(again\), then.
&gt; ignore the significant compromises required by modern CPUs to maintain C support I don't get this. The article completely focusses on C, but would any other language allow better support for modern CPU architectures? Is there an alternative to C as "close to the metal"-language (besides assembly)? What exactly does the article complain about? That the industry didn't invent new languages next to new architectures? 
What are the alternatives? Currently, you can’t run something faster, you try and guess and speculatively execute things in tandem. I’m no CS / CPU architecture wiz, but I like learning, do you have any good leads/reads?
It's not about CPU design. It draws motivation from the current clusterfuck that's been happening, but only to reach the main point that is the woes of the unnecessary and ever increasing complexity. The focus of the text is on the language and its relationship with what's below it, pointing out how C isn't a low level language from the perspective of last 2 decades hardware, which is completely true. When he states that C isn't a low level language, he means it in a horizontal perspective rather than vertical: C is low level relatively to 40 y/o hardware and relatively to today's pool of language, but far from what a low level language should be considering today's hardware. So the purpose of the article would be informing and bringing awareness to an issue that's actually important if you care about performance and/or safety.
https://caseymuratori.com/blog_0028 basically even assembly isn't a low level language because even asm is abstracted in modern cpu anyway, op's article completely ignores the above point it can barely veil its contempt for C it makes obvious mistakes like saying controllers or processors run C code it makes issues of non issues like compiler isn't free to reorder structure element it bitches that C compilers are complex then complaints C doesn't allow compliers to do more work (which will obviously add to their complexity) it tries to link shenanigans the OS virtual memory manager does to deficiencies of C it confuses parallelism with speed forgets most workloads don't benefit from parallelism forgets humans are most comfortable writing sequential code and systems/toolchains that allows them to write mostly sequential code and still find a way execute it in parallel is a feature 
Yes, but wouldn’t you say the main reason we have superscalar out of order chips with tons of cache today is because we can’t get the silicon to go any faster, not really as a direct result of c... I get it, you need the chip to be performance enough to set up stack frames, they probably have instructions and means to ensure this is as fast as possible. Ultimately you come down to the same limits of silicon. Yes, more threads good, more SIMD good. If the problem is addressable to these methods, for sure. But the end result is still the same, to go faster, don’t you need to guess? And build multiple execution units, more piplelines. I agree, chips these days have gotten to the point where you cannot guarantee which instruction got executed first, Hell, quite likely they’re probably even translated to some other micro ops internally and reordered... But is c really responsible for this or just a convenient scapegoat given the fact that it forms a crucial backbone in so many areas?
&gt; branch predict on the fly based on code run. Itanium had branch prediction with history buffers. &gt; Also, not able to dynamically fine tune the execution of code. Modern chips can do this? I thought they just benefited from cache design. &gt; It’s my understanding these were pretty much fixed at compile time and the chip was 100% in order execution. Right.. because the idea was you would do all the out-of-order and advanced parallelism right in the compiler. Which I pointed out didn't really happen, not only because it's a difficult problem, but because even when it does work you don't get the benefits it promises. It's barely competitive with the "old way" and when it is, you have to throw a bunch of effort at the code to achieve this. &gt; I could be wrong Partially. Point is, it's not as easy as it seems to build a "better" architecture. 
It's just pointing out what's not rather than what is. I'm unaware of a language that's closer to the metal and isn't assembly, but the whole point is that answering this is beyond the point of the article. &gt; That the industry didn't invent new languages next to new architectures? It has to. But not directly, nor it's the main topic.
The point is how wasted all these CPU resources are when running C code, not that it all had to evolve because of C. He mentions that when talking about cache coherency, and especially the C memory model. Ideally, a language that interfaces with current CPUs would make different assumptions about the underlying architecture, giving an improved mix of safety, performance and control, and consequently, would mean simpler and thus more efficient circuitry and simpler compilers.
&gt; I think this word already implies the obvious stuff you said Not really.. the example I gave points out the difficulty in achieving these things through other "more advanced" means. It was an idea that was tried ant it was not nice in practice. &gt; all boiling down to complexity for the sake of backwards compatibility. Again.. even when the biggest chip maker in the game straight up threw backwards compatibility in the trash they weren't able to produce something as easy to use or as performant as modern offerings and they had to use nearly as much "complexity" as our current chips. The amount of silicon and engineering devoted to "backwards compatibility" is basically nil compared to the amount of effort in getting accurate branch predictors and fast cache memory into a chip. 
Off-topic but I have to admit, I was kinda happy it died. I would think that AMD64 was what people wanted. It meant backwards compatibility, which may mean a lot. On the other hand, around that same time, I believe we saw the effective death of ppc64. I... really wonder why ppc failed, it had some momentum, now it’s arm this and that.
I found the article a little hard to follow argument wise. The title did not follow the prose, there was no clear idea where it was going and no good closing but that’s me. A processor designed purely for speed, not for a compromise between speed and C support, would likely support large numbers of threads, have wide vector units, and have a much simpler memory model. Running C code on such a system would be problematic, so, given the large amount of legacy C code in the world, it would not likely be a commercial success. There is a common myth in software development that parallel programming is hard. This would come as a surprise to Alan Kay, who was able to teach an actor-model language to young children, with which they wrote working programs with more than 200 threads. It comes as a surprise to Erlang programmers, who commonly write programs with thousands of parallel components. It's more accurate to say that parallel programming in a language with a C-like abstract machine is difficult, and given the prevalence of parallel hardware, from multicore CPUs to many-core GPUs, that's just another way of saying that C doesn't map to modern hardware very well.
&gt; The amount of silicon and engineering devoted to "backwards compatibility" is basically nil compared to the amount of effort in getting accurate branch predictors and fast cache memory into a chip. What? x86's ever growing size and complexity is in itself a great example of why you're wrong. But to generalize, the whole point is about how wasted the engineering effort is when constraining powerful hardware to the limits of C and backwards compatibility in general. Or how expensive the limitations and assumptions made by the CPU are expensive, as a way of saying that ideally, a lesser burden would mean a mix of performance, safety and control.
http://cslibrary.stanford.edu Plus any book of your choice and online tutorials. This explanation on pointers is imo unbeatable 
Note you also get a warning about old-style-function-definition. If you want a function with no parameters, use `f(void)` and not `f()`.
Yes. The CPU architecture changes have opened a gap "below" C but no new language (that I know of) has arrived to fill the gap.
Sure, but the article doesn't propose that. One of its key points is that an explicitly parallel language is easier to compile for than a language which doesn't express the parallelism, leaving the compiler to infer parallelism by preforming extensive code analysis.
I can agree with that but not every problem is best fixed with... say, Erlang, for example.
Ppc failed because there was only one maker of consumer grade computers using it, and the available CPUs couldn't compete in performance or power consumption with what Intel offered.
http://trent.me/is-prefix-of-string-in-table/#IsPrefixOfStringInTable_x64_3-review That’s honestly where the good part of the article starts! There’s a lot of cruft leading up to that point you need to wade through though :P
Weird. I’ve written multithreaded code in C that scales over 240 threads and 60-core processor. I did use OpenMP to do so though.
should we tell this guy about x86?
Great write-up. I like how we get to see the progression of the algorithm and what actually ends up mattering for performance. It was interesting that removing an initial length check in [version 8](http://trent.me/is-prefix-of-string-in-table/#IsPrefixOfStringInTable_8) _improved_ runtimes. I do this kind of optimization frequently so now I have to second guess myself. :) 
 (int*)( &amp;addr + *addr++) = value; First of all, `&amp;addr` gets an address to the variable `addr`. I think you don't really wanted to do that. Also, the cast is suspicious. Second, you can just make it way easier: addr[addr[0]] = value; which is the same as: *(addr + *(addr + 0)) = value. 
&gt; C isn't useful for exactly suited to EVERY possible programming task, erm so what. The point of the article is that C isn't optimal precisely for standard processors.
Bitwise and: https://en.m.wikipedia.org/wiki/Bitwise_operation
Non-Mobile link: https://en.wikipedia.org/wiki/Bitwise_operation *** ^HelperBot ^v1.1 ^/r/HelperBot_ ^I ^am ^a ^bot. ^Please ^message ^/u/swim1929 ^with ^any ^feedback ^and/or ^hate. ^Counter: ^178590
**Bitwise operation** In digital computer programming, a bitwise operation operates on one or more bit patterns or binary numerals at the level of their individual bits. It is a fast, simple action directly supported by the processor, and is used to manipulate values for comparisons and calculations. On simple low-cost processors, typically, bitwise operations are substantially faster than division, several times faster than multiplication, and sometimes significantly faster than addition. While modern processors usually perform addition and multiplication just as fast as bitwise operations due to their longer instruction pipelines and other architectural design choices, bitwise operations do commonly use less power because of the reduced use of resources. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/C_Programming/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
0x1 is a hexadecimal value equal to 1. &amp; is bitwise-AND. It performs an AND operation on each bit of 2 values (variable b8 and value 1) and returns the combined result of each operation as a new number. The effect of ANDing with 1 is that every bit except the LSB (least-significant bit) will be 0, and the LSB will be whatever it was in the other value, in this case b8. The simplified end result is that it will tell you if b8 is odd (1) or even (0).
 int push(int *addr, int value){ if(*addr &lt; STACKSIZE){ I see you’re trying to bounds check, but STACKSIZE is the size of the stack, not the address of the TOS. Also, you’re dereferencing addr, you’d be getting what is currently tos. One option is to calculate the TOS by adding sizeof(int)+STACKSIZE to the initial address returned by malloc and cmp it to the address in addr for this.
Hah, I know right, writing this article forced me to question and justify all of the “oh if I write it like this it’ll be definitely faster”-type assumptions I usually make. It was particularly startling to see how bad the version I had in place for two years (the elaborate AVX2 one) was as soon as I benchmarked it. Investigating why that third assembly version was so slow yielded an awesome payoff though, as you can see toward the end. Thanks for the feedback!
Who said it was? It's a powerful language.
i have this nested ternary: subblock_x = ((b8&amp;0x1)==0)?(((b4&amp;0x1)==0)?0:1):(((b4&amp;0x1)==0)?2:3); so if LSB(b8) =0 so the result will be 0?
I’m not sure what gen purpose language the author has in mind. Even x86 assembly abstracts the cpu, I mean, an in-order atom and an out-of-order whatever 8 gen intel core are two entirely different beasts.
Ouch. Lol. If b8==0, yes, it will return the true condition of the top most ternary operator. Also from the code, I can’t help but wonder if the b? variables here could be bit union members.
I love the XML question (whether your app does stuff with XML). There have historically been some big security holes when parsing XML that it is a security code smell now if you are working with it (especially in lower level languages like C or C++).
b8 and b4 are declared as ints, the code works perfectly, now that i unsderstand this,i can move on, thank you very much :)
Yup, they couldn’t have been bit union struct members. I dunno where my brain was taking me. Time for bed! 
Lo and behold, there it is :) thanks for the pointer to the follow up. After all that, a rascally sync!
He’s trying to get the “new” revolution into programming. It’s his way of helping people.
Thank you very much!
&gt; what exactly these limitations are? yet there is no evidence to back up this point. That C isn't as much a "portable ASM" to x86 as it is to the PDP-11, which is why it can't be called low level from today's hardware perspective. That the language makes assumptions about the underlying architecture which hinders its potential. The examples are in the text. And I'm not suggesting to scrap and rebuild CPUs. If anything, the suggestion would be to ditch the language, to one designed with more current CPU constraints in mind, for example control over the cache, simpler coherency mechanisms, redundancies to ensure and make it easier for compilers and the CPU to decide on optimizations. If we're to have complexity, let it be for the right reasons, because die shrinking has practically capped, x86 is too complex and big, and even though great effort into making C run great has payed of, we're reaching the ceiling and one of those will have to give up.
it's okey ;)
Yeah sorry, fixed it
Yes, you can. Where are you stuck? What research have you done?
&gt; The examples are in the text. No they aren't. I fail to see how the C programming specification has _any_ impact on architecture _at all_. It's a flawed assumption, and until you can show me where our processors are intentionally leaving performance on the floor to cater to C it's just another one of these "cargo cult" positions that software engineers love to fall in love with. &gt; the suggestion would be to ditch the language, to one designed with more current CPU constraints in mind, for example control over the cache Well.. you're going to need to ditch the architecture, because regardless of what language you choose the architecture provides you zero access to the cache. &gt; simpler coherency mechanisms While at the same time adding more cores? Good luck getting all that parallelism you probably want. &gt; redundancies to ensure and make it easier for compilers and the CPU to decide on optimizations. I have no clue what you mean or how this would be implemented. Unless you mean something like the Mill where you compile to an abstract machine language that then gets JIT/specialized for the actual architecture it's going to run on. Unless you have some data that suggests this is going to unlock all the performance we're missing by using C, then I'm going to rely on history here and say: it isn't going to work. That is, it will fail to meet the necessary performance/employee time, performance/watt or performance/dollar metrics and will fail to replace anything other than these bizarre fantasies that C is "holding computing back". &gt; x86 is too complex and big Relative to what? Some other wildly successful architecture? ARM is too complex and big. Power is too complex and big. Why is this so? Because RAM has some serious physical limitations requiring huge amounts of architectural effort to make computing reasonable efficient in the face of slow-as-hell RAM busses, not because of some C language conspiracy. &gt; and even though great effort into making C run great has payed of Again.. what compromises have we made in CPU design to benefit C? The article does not cover this... it whines about how hard it is to make a C optimizer, but I really don't see how this wouldn't be true on _any other_ arch there is out there. Why does the state of my padding bits have any impact on performance? Isn't this literally an example of the architecture doing whatever it wants to be efficient and C having to work around it? How does this support the supposition that C is having an impact on arch design **at all**? It's such a wishy-washy and poorly thought out argument that gets trotted out by people who've never taken the time to try and design their own hardware. There is no silver bullet. C has no impact on arch design, and arch design is sufficiently complicated and filled with compromise that this "better architecture" only exists in fantasies and wasteful college essays. 
I’m pretty sure there is a function that does it. But I have no idea where to start searching (tried googling but I haven’t found anything in C)
So, `lnk` files are Microsoft Windows specific, so best would be to check the Microsoft [Knowledge Base](https://msdn.microsoft.com/de-de/library/cc952577.aspx). In there you find [this article](https://msdn.microsoft.com/en-us/library/bb776891%28VS.85%29.aspx) detailling what you need to do.
 man fork
You're looking for concurrency and parallelization I think. I would recommend pthreads. That's what I've used for threading in C programs before, and I believe it works quite well. 
oh damn just when I think I understand pointers shit like this comes up and reminds me that I know nothing.
The line is complete nonsense, if it doesn't make sense to you then you are doing something right!
`using namespace std;`? You might want to ask that in /r/cpp or /r/cpp_questions 
Not sure what you're trying to achieve here. If the (signal) handler receives a signal, it can increment a tick variable (or variables, depending on how you want to do things). The tick variable(s) can then be used to conditionally run your tasks. If you absolutely need to have the tasks running concurrently then use threads (though, on a single core/fewer cores than threads, it's a pseudoconcurrency and only appears to be running in parallel)
Gotchya will do 
Stop casting so much and just keep a `unsigned short` pointer.
just realized I didn't link the imgur album, my bad i'm tired as fuck. https://imgur.com/a/qaZoF0B
This is why after a 15 years of programming in C/C++, I switched to javascript. The answer to your question is WHO FUCKING CARES. Seriously... fuck that shit. What are you, a goddamn TI-99 calculator? Life is short. Make things.
what the fuck? Are you aware you can paste text? rather than screenshots? 
You want /r/cpp
While it's possible to crudely implement this using concurrency in a simulation that runs at real time — which it seems may be the purpose of this exercise — the more precise and scalable approach is to use an event priority queue (see [heaps](https://en.wikipedia.org/wiki/Heap_(data_structure))). Events in the queue are sorted by the time they're to take place. An event being executed may queue up new future events. For example, you would initially schedule a single "decrement fuel" event that decrements the fuel and then schedules itself again one second in the future. The main loop pumps this queue until it's empty or some other termination condition is met. An event queue is both deterministic and can run faster than real time. 
For 1) The argument to your thread functions is void*, it's up to you to interpret that. Right now you're passing the address of integers, and interpreting them as integers in your function. You could just as easily pass a struct of your multiple arguments in the same fashion. e.g. struct Arguments { int a; float b; } void* argfunc(void *arg) { int i; const struct args = *((struct Arguments*) arg); return NULL; } struct Arguments args = {1, 1.}; pthread_create(&amp;t1, NULL, argfunc, &amp;args);
I believe the standard only reserves identifiers that start with an underscore AND a capital, like _Generic, _Atomic, etc. Lower case, I think is fine.
I can't get my code to compile for the first part and I have no idea what to do for the 2nd part. 
The last two lines should be in main. What errors do you get?
Here's a full working version for your second part #include &lt;pthread.h&gt; #include &lt;stdio.h&gt; #include &lt;stdlib.h&gt; struct Arguments { int a; float b; struct IdLog* log; }; struct IdLog { pthread_mutex_t lock; int current; int* thread_ids; }; void IdLog_init(struct IdLog* log) { pthread_mutex_init(&amp;log-&gt;lock, NULL); log-&gt;current = 0; log-&gt;thread_ids = malloc(sizeof(int)*100); } void IdLog_write(struct IdLog* log, int id) { pthread_mutex_lock(&amp;log-&gt;lock); log-&gt;thread_ids[log-&gt;current] = id; log-&gt;current += 1; pthread_mutex_unlock(&amp;log-&gt;lock); } void* argfunc(void *arg) { const struct Arguments args = *((struct Arguments*) arg); IdLog_write(args.log, 5); printf("%d %f \n", args.a, args.b); return NULL; } int main() { pthread_t t1; struct IdLog log; IdLog_init(&amp;log); struct Arguments args = {1, 1., &amp;log}; pthread_create(&amp;t1, NULL, argfunc, &amp;args); pthread_join(t1, NULL); printf("%d %d", log.thread_ids[0], log.thread_ids[log.current]); }
I was scanning a and b in main and I was appending them as: struct Arguments args = {a, b}; and that didn't work. I appended them as a=args.a; and it worked like a charm. 
I agree with you. If the purpose is to truly simulate communication between a naval base and submarines, the least I would do would be to use IPC. It would also separate memory space so OP doesn't have to worry about synchronization and deadlocks.
It's working now. Thanks for the advice. Not sure if it's what you were intending for me to do. 
Look closer at the original code. /* When inputting */ i += 1; scanf("%d", &amp;grades[i]); /* Later when outputting */ for (j = 0; j&lt;=i ; j++) printf("\t%3d\n ", grades[j]); Since `i` is `0` at first, you increment it to `1` first before getting it's address so `scanf` can write to. So you're skipping writing to `grades[0]`. However, when printing, you print `grades[0]`, which is uninitalized since you never wrote it in the first place. Your new code just explicitly writes to that location.
No, op wants /r/cpp_questions. This would be deleted if posted to /r/cpp, which isn't a support forum.
Systemtap is another profiler that has different application/output. It requires kernel support to enable "probes" and then write a systemtap script defining actions to be done when certain user/kernel functions are called. It has no support from compiler. I'd say it's just another way to profile your application. https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/5/html/systemtap_beginners_guide/scripts#syscall-open If you use valgrind/callgrind, it can give you more/different kind of profile information - http://valgrind.org/docs/manual/cl-manual.html "Callgrind is a profiling tool that records the call history among functions in a program's run as a call-graph. By default, the collected data consists of the number of instructions executed, their relationship to source lines, the caller/callee relationship between functions, and the numbers of such calls. Optionally, cache simulation and/or branch prediction (similar to Cachegrind) can produce further information about the runtime behavior of an application." In short, there are several kinds of profilers available, each providing some sort of profiling information. There is not a single tool that measures everything and has multiple platform (MIPS, ARM, x86) support. What I describe in the blog post is another way to profile it, ie in terms of function/line frequency counters. HTH.
I am guessing you really just extended the `atof` function (the `if (tolower(s[i++]) == 'e')` part), correct? Well, in C89/C90 (which AFAIK your book is based on) didn't have the `//` comments and variables should be defined at the start of the function, but apparantly you're now "breaking" these rules. However, C99 "fixes" these things, thus you could just say your code is C99+, which is valid. Also, when the input string is `"123.45e-"` (or `"123.45e"`), in line 60, you check the the next character first. However, `s[i]` is already `'\0'`, so you're just accessing out of bounds. Also, any reason to just read one or two digits?
I am not one of the downvoters. Quite the contrary, I feel like it's great content and miles better than the simple homework questions that should be outright banned from this sub.
delete
&gt; miles better than the simple homework questions I think thats the problem.... people assume these posts are for homework. The funny thing is that I'm a mid 30s sales guy with no career change in anywhere in sight.... but *sigh* am I going to post that on every thread? :-/
I missed the `//` comments not being part of K&amp;R thanks And thanks for explaining about accessing out of bounds. My code is written assuming that the input string shows a proper float value. I'm not far enough into the book to do error checking at this point. I also read one of two digits afterwords because my understanding of floats is that the maximum exponent value it can have is `38`
If you're searching by value, you're doing something wrong and should probably be using a different data structure. As for how to do it: just iterate through the table until you find what you're looking for. Your average running time will be n / 2.
As long as you get critiqued, does it matter? Ignore the down votes on the post. That's not what you're here for.
\(not OP\) When you're getting downvoted to oblivion without any acknowledgement as to why, it can feel like you're being told to shut up and go away
It might be because when you just post a big block of code with no specific questions, you're asking others to put in a fair amount of work without knowing if there's something specific you're after, and for anyone else reading the thread it's not particularly interesting if you have to also go read the whole block of code to follow along. If you want a better response, try identifying one particular aspect of your solution that you'd like feedback on. Or you can just tell everyone that your solution is the best one, and wait for everyone to get mad and tell you all the ways in which you're wrong.
&gt;I wouldn't help and down vote. What?
Skimming your server code, a few things stand out. Not anywhere near enough error checking. You try to create a non blocking socket (in a non portable Linux specific way (that you use the wrong value for)) and then immediately clear the nonblocking flag on? Using strtok() in a multithreaded program. Not joining or detatching threads is a resource leak, plus other memory leaks. 
I helped with 4-1.
I mean i find those sentences contradictory. 
I totally feel you, It is so funny, I am taking a Java class at the same time, and I can literally blaze through it because our Professor is clear, and doesn't try and tie in so many different concepts at once. It gets frustrating because I just want to learn the concept, but I feel like it turns into a cryptic puzzle that just gets frustrating and stops me from actually getting the most out of these concepts. This class really makes me question my programing ability, while my Java class makes me feel like I am a strong programmer, and it is honestly down to who the instructor is. My Java teacher also teaches a C++ course and I guarantee I would have come out of his course with a much better handle on the language, simply because his expectations and projects are clearly defined. I will never question a Professors teaching approach, it just makes me realize you have to find a teacher with a style that works for you and stick with them if you can. Others in my C++ class get it, but struggle in my Java class, it is so funny, but kind of hammers that point home.
Pretty reminiscent of Stack Overflow isn't it? Not to mention that downvotes decrease visibility of your post and make it less likely for it to get critiqued
Hehe I drunk posted that last night.. but I'm leaving it. Cheers!
Error fixed.
Couldn't tell that from the correction though, it's very fluent. For me it's my last and only the second.
Nobody cares about your homework sweetie
Tupac cares, if don't nobody else care.
^The linked tweet was tweeted by [@spudowiar](https://twitter.com/spudowiar) on May 06, 2018 10:06:38 UTC (0 Retweets | 12 Favorites) ------------------------------------------------- \#define defer \_d0(\_\_COUNTER\_\_) \#define \_d0(X) \_d1(X) \#define \_d1(X) \_d2(s\#\#X,f\#\#X) \#define \_d2(S,F) auto void F(void\*);int S \_\_attribute\_\_((cleanup(F)));void F(void\*\_) \#include &amp;lt;stdio.h&amp;gt; int main() { puts("1st"); defer { puts("4th"); } defer { puts("3rd"); } puts("2nd"); } ------------------------------------------------- ^^• Beep boop I'm a bot • Find out more about me at /r/tweettranscriberbot/ •
📅 06/05/2018 ⏰ 10:06 [(UTC)](https://www.timeanddate.com/worldclock/converter.html?iso=20180506T100638&amp;p1=1440) &gt;\#define defer _d0(__COUNTER__) &gt;\#define _d0(X) _d1(X) &gt;\#define _d1(X) _d2(s\#\#X,f\#\#X) &gt;\#define _d2(S,F) auto void F(void*);int S __attribute__((cleanup(F)));void F(void*_) &gt;&amp;nbsp; &gt;\#include &amp;lt;stdio.h&amp;gt; &gt;int main() { &gt; puts("1st"); &gt; defer { puts("4th"); } &gt; defer { puts("3rd"); } &gt; puts("2nd"); &gt;} &gt;— Saleem Rashid ([@spudowiar](https://twitter.com/spudowiar)) 🔁️ 0 💟 12 &amp;nbsp; ^(I'm a bot and this action was done automatically)
You might be right about that. I will take your advice and mention how I am a C Programming Deity and my code is holier then though. I'll learn quicker than ever about my mistakes
Put 4 spaces in front of every line to format code for Reddit. Tools like RES make this trivial. Or share code on gist, pastebin, etc.
Unfortunately it seems there is no C subreddit that targets advanced C programmers. I wish the mods would do the same here. /r/learnprogramming isn't only for C++...
Why would you expect a function run in your **parent** process to modify a variable in a **child** process, which, being a separate process, has a separate address space and memory? That can only happen with shared memory that both processes map, which you don't appear to be using. Is your parent process supposed to notify the child processes to do things? If so, it should send them signals, or use some other interprocess communication mechanism (Of which there are tons; pipes, sockets, message queues, etc. etc. etc.) Also, post code, not pictures of code.
You have to remember that programming involves a lot of ego...some people get off on bashing you down. Just know it is much appreciated and makes for great learning and content!
You can search a hash table by value by doing a linear search through the table. This defeats the point of having a hash table though as youget an O(n) access time instead of the O(1) access time when looking up by value.
Thank you for your reply! I fixed the post so the code was added appropriately to the best of my ability. Regarding my code, I came here to ask because I'm learning how to code and the assignment I'm working on involves significant amounts of material only brushed upon, if not nonexistent, in the course I took, so I've been having to comb the internet for information on how to code things, as signals and using different terminals is completely new to me. I came here to ask how I can modify my code so that the alarm interacts with the children and not the parent. As for my parent sending the children signals, I am trying to figure out the alarm first and then how to make the unique terminals. Thank you for your understanding.
Your other options are /r/c_language and /r/cprog which are more in the spirit of /r/cpp. Please visit, subscribe and help those subreddits, there seems like little hope for this subreddit (because the majority don't want change)
Think you replied to the wrong person? I'm happy with this sub the way it is.
Not at all. I'm moving towards peak earnings in my career. I cant fathom moving in software and taking a massive pay cut as a junior dev. As far as you getting into sales is it software sales or what? Msg me if you want
/u/Neul caught all the issues that I see. But how about writing a unit test for this function? You could create a list of strings, both valid and invalid floats, and see whether your `atof` matches the result returned by the system's `atof` or `strtol`. Here are some strings to get you started. const char *test_strings[] = { /* valid strings */ "1", "", ".1", "+.1", ".-1E-2", /* invalid strings */ "", ".", ".e1", "0.e_1", /* add more. These are just a sample. */ }; You should be able to iterate through this list, pass each string to `atof` and to `strtol`, compare the results, and print a message when they don't match.
Shadowing i on line 66 is not strictly bad in this case but in general I would avoid it as it will cause confusion if you ever later added code to the end of the outer block. Note that bringing i into the for as in `for (int i;;)` would be much clearer here; then it would belong to the internal scope of the for loop, leaving the meaning of i unchanged for the rest of the outer block.
Please put four blanks in front of every line of code so your code appears readable.
LoL people who expect a mostly advanced sub on a topic on Reddit. And the mob buys it, and no one talks about flair/filtering system, which is what big subs do. This has to be the joke of the day.
C++ is off topic in this subreddit. Please post C++ questions elsewhere (e.g. to /r/cpp_questions) or do your own damn homework.
Aye relax man it was just a question
Your question was both off topic (read the topic!) and a very blatant attempt at getting someone to do your homework. Neither thing is welcome in this subreddit.
I'm not sure what this means. /r/cpp is an advanced sub and the mods maintain it well.
Here's a sneak peek of /r/cpp using the [top posts](https://np.reddit.com/r/cpp/top/?sort=top&amp;t=year) of the year! \#1: [I just found a use for the poop emoji in C++](https://np.reddit.com/r/cpp/comments/75gohf/i_just_found_a_use_for_the_poop_emoji_in_c/) \#2: [C++17 is formally approved](https://herbsutter.com/2017/09/06/c17-is-formally-approved/) | [97 comments](https://np.reddit.com/r/cpp/comments/6yjerb/c17_is_formally_approved/) \#3: [Bjarne Stroustrup awarded 2017 Faraday Medal](https://www.cs.columbia.edu/2017/bjarne-stroustrup-awarded-2017-faraday-medal/) | [23 comments](https://np.reddit.com/r/cpp/comments/72gxkw/bjarne_stroustrup_awarded_2017_faraday_medal/) ---- ^^I'm ^^a ^^bot, ^^beep ^^boop ^^| ^^Downvote ^^to ^^remove ^^| [^^Contact ^^me](https://www.reddit.com/message/compose/?to=sneakpeekbot) ^^| [^^Info](https://np.reddit.com/r/sneakpeekbot/) ^^| [^^Opt-out](https://np.reddit.com/r/sneakpeekbot/comments/7o7jnj/blacklist/)
The problem with those are that people aren't using them nearly as much. This leads to a few good posts every couple of days there, and some scattered ones here daily. If all the C subreddits could merge and move the exercise posts elsewhere, I think everyone would be happy.
I don't know about the reason, but you could consider posting your code on https://codereview.stackexchange.com/ 
Also with a removal of the questions, theres a chance we drive this subs traffic to 0. 
Not to start a language war, but I think C is slowly becoming more of a niche language than it once was. I still like the language, and enjoy programming in it, but it feels at times dwarfed by others. I'm not saying it'll disappear (hell, there's still production COBOL, FORTRAN, ALGOL, etc out there), but it feels like it's on a slow slide in popularity. /Dons flamesuit in advance 
Haha I'm not disagreeing either. Just assumed more people would be interested in writing core libraries in C for binding to other languages. That's the main reason I moved away from C++ recently to focus on writing language extensions.
I was just asking if anyone had a program with that that criteria, i wasn’t trying to do anything negative. Thanks for showing me what a nice community this sub has, and try to relax a little bit next time (its bad for your health to be like that)
&gt; sales guy Are you *trying* to make us hate you? ^^Kidding
Everyone's got all the big stuff. My recommendation is to use `size_t` instead of `int` for indices (e.g., your variables `i`) as a habit. The reason is that `int` is often 32 bits long, even on 64-bit systems (or, at the least, not guaranteed to be big enough to index every element in an array). If you ever deal with a string longer than 2 billion characters in length, you could run into undefined behaviour. (That's not very likely for a function like `atof`, but it's the principle of the thing)
On the other hand, there's also /r/C_Homework and it's even linked to in the sidebar.
I will try and briefly answer your questions. * `Can I pass multiple arguments to a thread?` No, you cannot. What you can do is create a nice struct (in C for example) and pass that as one arguments. Then you can access the struct's parts as different arguments. * `I'd like to isolate the identifiers` The commands associated with printing will have to be executed. Try using the function `pthread_join();`. This function works like a wait(&amp;status). It will allow you to run the threads in the order you created them. 
Check out https://troydhanson.github.io/uthash/userguide.html
Thanks man! I'll check it out
Sounds like you've read neither article... &gt; basically even assembly isn't a low level language because even asm is abstracted in modern cpu Yes. C and assembly are *both* using outdated models of programming that are not truly reflective of what's going on inside the machine anymore. First off, C and Unix were not designed for this era. Period. They were designed for different hardware, different circumstances, and were mainly created for AT&amp;T's budget. If you don't believe it, research the history of Unix starting with [this article on Wikipedia.](https://en.wikipedia.org/wiki/Space_Travel_(video_game)) I'm not putting down the authors of Unix; I think they did great work. But it was largely *for their time.* Much of the stuff they did, even their Unix philosophies are somewhat outdated in this day and age. They were only ever applicable to begin with because of hardware constraints. Going back to the OP's article, the author said article is picking on C specifically because C contributes to this problem greatly (even more so than assembly). As the authors of both your touted article and the OP's article point out, the real root of the issue here is with the way we deal with memory in computing. And C truly expects to be dealing with something that doesn't exist in modern hardware anymore. Hence this whole thing with caches and all the OS "shenanigans." These exist precisely because programmers have opted to use C and assembly (mostly C) which are ill-suited to modern hardware (and C is still more so than asm). Not that they have much of a choice; it's hard to rewrite all the code floating around out there to be in a better language / paradigm. Going back to the real root of the issue, the way we deal with memory, it is plain to see (and mentioned in both articles) that the flat memory model is stupid nowadays. It only exists because of programmers' irrational desire for such a memory model, probably because they were used to it back in the day. We would have been much better off in many ways by using a segmented memory model (which the author of your touted article states as well). Spectre and Meltdown never would have happened if we hadn't been pining for this no-longer-existent (and useless to begin with) flat memory model. The only reason that model even existed *was because of hardware constraints.* This is a recurring theme in the history of Unix. If you want to know more about this, read these links: - https://softwareengineering.stackexchange.com/questions/100047/why-not-segmentation - https://stackoverflow.com/questions/3029064/segmentation-in-linux-segmentation-paging-are-redundant - https://en.wikipedia.org/wiki/Virtual_memory#Segmented_virtual_memory - https://en.wikibooks.org/wiki/X86_Assembly/X86_Architecture#CPU_Operation_Modes - https://web.archive.org/web/20171116233739/http://www.osinfoblog.com/post/135/segmentation/ - https://web.archive.org/web/20171116235359/http://www.osinfoblog.com/post/136/segmentation-with-paging:-multics/ - https://web.archive.org/web/20171117000443/http://www.osinfoblog.com/post/137/segmentation-with-paging:-the-intel-pentium/ - https://web.archive.org/web/20171117000631/http://www.osinfoblog.com/post/127/shared-libraries-/-mapped-files/ You can clearly see that systems such as Multics were superior in a number of ways to competing systems because of their use of segmented memory. Segmented memory is ultimately what we're trying to achieve will all of this juggling and mess with caches. Yet, it is done in a very roundabout way. Computing today simply should not be designed with C and assembly in mind as we are used to them. Forth provides a better model of computation (as is basically suggested by the author of your touted article) and Lisp would be a great compliment to it. Other data-structured languages would also fit in rather nicely in this paradigm. Indeed, this sounds like a much pleasanter and more humane way of doing computing. I guess all of this is politically incorrect to say in the computing world, but it's the truth. It's a shame that people like you are getting to the top of posts like this which try to give people an inkling of some of this knowledge. You instead scare people away with baseless claims and senseless arguments that are not even valid, let alone sound. People making these same arguments as you are why things like Spectre and Meltdown occurred and will continue to occur.
I don't think "niche" status is the problem. Pretty much all popular languages are niche languages to some degree, some much more so than C. Yet somehow they keep their subs from being swamped by simple questions.
There are many different reasons why program startup could be faster one way or another. If you want to know how well your code performs, make your program take longer (on the order of a few seconds) so the output is less noisy.
What sort of hash table would you like to have? Is this a homework exercise?
I'm just trying to practice my programming skills but i don't really know how to implement hash table in a program.
Of course. You just sat down and made up a very specific list of criteria just for fun, half of which can't be satisfied by any C program at all, one of which specifically asks about the program being written in C++. And then you go to a subreddit for programming in C and post this question. How stupid do you think our readers are?
Ok. How much do you know about hash tables and how they work?
I know hashtable is like an array and the data value are also assigned with index value 
Okay. So, the trick with hash tables is that you want to use an array to store your data, but the thing you want to index by is not an integer but rather some random datum (e.g. a string). The key idea with hash tables is to build a function that turns your key into an integer which you can in turn use to index into an array. This function is called a *hash function.* Building a hash function is easy: Just jumble all the parts of your key together somehow (e.g. by adding them), reduce it modulo the table size and voilà, you get an index. Building a good hash function however is difficult and subject to a lot of research. A good hash function is one that reduces the number of *collisions.* A collision is when two keys are assigned the same entry in the table. A bad hash function, for example, is one that just sums the fields of its keys. Dealing with collisions is the other part of building a hash table. There are multiple schemes for dealing with collisions, the easiest one is to have each table entry be a linked list of entries with that hash. After computing the index into the hash table, you traverse the linked list for this entry, looking for a key that is equal to the key you are looking for. Let me know if you need more help.
cos i sometimes have to write extensions to an existing program which is all written in C
People write new libraries in C all the time. Yes, maybe they aren't all that vocal about it. It would still be nice to see that posted here instead of the standard Operating Systems homework assignments.
Ubiquity \- C is everywhere and is probably the most cross\-platform language. I don't see myself as choosing C instead of switching to another language though, as I think a good programmer can be proficient in 3\-5 languages rather than just one.
Let me know if you run into problems!
To add on to this, C is "low-level" (or "medium-level" or whatever you want to call it), meaning that it tries to closely model the machine. Higher-level languages give you a model which (arguably) is easier for humans to reason about, but sometimes you want to communicate with the *machine*. You want to specify to the machine "put this in this memory, make this system call, then do this". In higher-level languages, you lose the ability to know what your computer is actually doing when it's executing your code.
Will do :)
I write statically allocated [seccomp](https://en.wikipedia.org/wiki/Seccomp)-secured C (mostly for work) with valgrind, UBsan, and afl-fuzz – getting similar levels of performance and trust in your software in any other language is rather difficult. 
Because my job has me maintaining and developing a load of code that was previously written in C. 
C is the standard programming language for *nix, the most ubiquitous OS. Even if I'm writing Python or Go, I always have to think in terms of C, especially for OS facilities like interacting with the file system, processes, sockets, etc.
Because C has all the abstractions i need, and more importantly, i know their cost. With fancier languages, one tends to want to use those shiny features and end up shoehorning them in somehow.
Hard to say it better.
Simple and elegant, easy to understand, unsurprising.
Even though C's preprocessor tricks are so hard to figure out, syntax is easy (not programming) and its codes are straightforward. 
&gt; Are you trying to make us hate you? I was getting downvoted anyways. Whats a few more downvotes :) 
Ok I'll be honest. Most of the new kids are jumping straight into web development or mobile and have no idea about how their software actually runs on the hardware. I enjoy teaching them about walking data structures and how their bloated dates models are actually laid out in memory and how much memory they are using. You may be able to do this in other languages but the explicitness of C in nearly everything you do forced you to understand your code. 
My bad sorry.
My colleagues.Especially one dude who is above 50. He doesn't want to learn new C++ standards.