This one I don't understand. Where is the list argument? Is it implied somehow?
This isn't quite right. You want to take 3 points at a time, but only advance 1 point each recursion. So the first time you take points x1, x2, x3; but then the second time you want to take x2, x3, (head xs). Here's a fixed version (w/ a whichTurn that doesn't take lists): whichTurns (x1:x2:x3:xs) = whichTurn x1 x2 x3 : whichTurns (x2:x3:xs)
Oh, ha. You beat me to the correction. :) I haven't learned what the @ does in RWH yet.
Sure enough, good catch. Here's my corrected grahamScan function: grahamScan :: [Direction] -&gt; [Point] -&gt; [Point] grahamScan [] (x:_) = [x] grahamScan _ [] = [] grahamScan (x:xs) (p:ps) | x == Main.Left = (p : (grahamScan xs ps)) grahamScan (x:xs) (p:ps) | x == Main.Right = grahamScan ((whichTurn p p1 p2) : xs) nextPoints where nextPoints = drop 1 ps p1 = head nextPoints p2 = head (tail nextPoints) grahamScan (x:xs) (p:ps) | x == Main.Straight = (p : (grahamScan xs ps))
Lots of good tips in here, thanks!
Exactly. In the following: plus a b = a + b onePlus x = plus 1 x onePlus' = plus 1 onePlus and onePlus' are equivalent. 
Plenty of code like that, isn't there? Agda can do Ackermann, after all.
I've used Text.Xml.Light for getting data out of XML. I can't promise it is good code, but it is certainly working code: http://hackage.haskell.org/packages/archive/xcb-types/0.6.0/doc/html/src/Data-XCB-FromXML.html It is intended to parse these files: http://cgit.freedesktop.org/xcb/proto/tree/src?id=1.6
In what sense is the Ptr/IO Haskell solution "completely non portable"? 
The libraries don't have to be designed for async IO in Haskell. Just write a threadfork into your monad stack.
I assume you're referring to http://www.haskell.org/haskellwiki/DocTest, which is really nice imho ...but it'd be even better if it could be integrated with haskell-test-framework and/or HTF to complement quickcheck/hunit tests
`@` just names a pattern match, so for example: foo x@(Just y) = ... If you pass `Just 5` to `foo`, `x` will be bound to the whole of `Just 5`, whereas `y` will be bound to just the `5`. This is better than matching on `(Just y)` and then reconstructing `Just y` in your function definition, because while the two values *look* the same, they will not actually be shared (in memory).
Who's invited?
Sure. The problem is, the Haskell version of the same code runs in an immeasurably short time. I still occasionally get teased by my coworkers (who don't believe in the real-life applicability of formal methods) because of this.
That would be a great idea. Anyone feel like undertaking such a project?
OK, cool. That's pretty useful.
Thanks for the thorough break-down. I take it then that, excluding specific use cases where we don't need full XML compliance, you wouldn't be pushing this as a general solution?
Right... I wouldn't call it another general-purpose XML parser library for Haskell. It's rather specific to doing HTML and XML document manipulation with a very simple interface where you control the documents. Haskell has enough XML parsers already! Work there is better spent on improving the existing options.
Another excellent article Chris. You're doing a very thorough job of analyzing the issue, starting lots of good discussions, and even remembering to point out the good. My hat's off to you. I think one thing we could do to help the situation a bit is have multiple versions of Hackage. I have not fully fleshed out the idea yet, but it could go something like this: * By default, packages get uploaded to the "regular" Hackage, which is exactly the same as the current Hackage. * Uploaders can specify (in the cabal file perhaps) uploading specifically to a beta Hackage, where it is well known that the PVP will *not* be followed, code is not necessarily bug-free, etc. When writing deps in a cabal file, we can explicitly state that we want to use the beta version, but otherwise it will not be pulled. * We then have a new "validated" Hackage where packages are guaranteed to follow the PVP, and perhaps even some manual intervention is required, such as a moderator approving the package. This will only be for display purposes, so users can see the more mature/stable packages only. I would also like to see packdeps- or something similar- get included in Hackage somehow so package writers find out when they are behind on dependencies. In theory, if every package always compiled with the most recent version available, I think we would solve 99% of our problems.
I like how he has hard numbers about the difference in throughput and only theoretical speculation on the well-established difference in scalability. And I wonder how many poor Java devs came away from that presentation convinced that their throughput is the only thing that matters.
Interestingly his STUArray version with unsafe indexing suffers from rescrutinising a boxed value on every iteration of the inner loop. It looks like an (easy to spot) missed constructor specialisation opportunity, and I've submitted it to the GHC trac.
Or even: lowerpoint a b = comparing y a b `mappend` comparing x a b
I'm not sure I understand your point. I'm not making any claims about which is "faster," but rather which scales better. There is plenty of empirical evidence that async I/O does scale better than synchronous I/O, cf. node.js itself, Snap and nginx. The point of my post is that you can have your async I/O cake and eat it too - you can write with the clarity of blocking I/O and still get asynchronous I/O throughput.
 whichTurn :: Point -&gt; Point -&gt; Point -&gt; Direction whichTurn a b c = case compare (counterClockwise a b c) 0 of GT -&gt; Main.Left LT -&gt; Main.Right EQ -&gt; Main.Straight counterClockwise :: Point -&gt; Point -&gt; Point -&gt; Double counterClockwise a b c = (x b - x a) * (y c - y a) - (y b - y a) * (x c - x a) whichTurns :: [Point] -&gt; [Direction] whichTurns x | length x &lt; 3 = [] | otherwise = zipWith3 whichTurn x (tail x) (tail (tail x)) 
 lowerpoint = comparing y `mappend` comparing x
Are you asking what you need to do to use `Geometry` without loading it using `:l`? If so, then the answer is that you'd need to put your module into a package, and register the package with GHC. Basically, stick it in a directory, run `cabal init` to make a cabal file, edit the cabal file to specify the right version, exposed modules, and such... and then run `cabal install` to build and install it.
Oh, very nice! Although it uses even more machinery - double nested application of the Monoid instance for functions - I think this is the simplest and clearest definition. Even without thinking too much about the instance for functions, its meaning is crystal clear once you've grasped Lennart's point that the `Monoid Ord` instance is just the dictionary ordering.
Node.js is a dead-end. Generally I sugar coat that in more public forums but I can probably say it straight here without ruffling too many feathers. It's not that it's necessarily a bad idea, it's just a dead end. What's going to matter going forward is your thread isolation story and your underlying scheduler's abilities. Node.js doesn't appear to actually isolate threads (it looks like it's all just in the JS process space), and its scheduler is absolutely terrible. That is, it can't handle multiple cores _at all_, already a problem here in 2011. But far worse, the scheduler is so pathetic and anemic that it requires you, the programmer, to _manually_ compile your code such that every time you do something that might take a lot of time you have to break your control flow up. It's not just "blocking operations", if you want to do lengthy computation you're going to have to chop that up too. It's _anything_ that might take more than 10 or 20 ms, and even that time limit is generous if you're planning on a loaded server. Cooperative multithreading is the ancient past, not the breathtaking future. It is almost surreal to see a framework that requires such extensive manual compilation get such hype. I've got better things to do as a programmer than function as a compiler. Attempts to layer various fixes on top will sort of kind of work, but will also add a lot more complexity to the stack, as attempts to layer things that ought to be in your primitives into higher layers inevitably do. It'll also be quite a few versions before anything like clustering works truly properly. Attempts at adding in compilers to do the transforms for you are being shown (right on my schedule) but they are complicated and fragile. Node.js isn't going to be able to successfully escape from its terrible thread isolation story or terrible scheduler, and while I'm sure the hype has got another year or so to go before it finally burns out, expect more and more stories [like this](http://groups.google.com/group/nodejs/browse_thread/thread/c334947643c80968?hl=en&amp;pli=1). (And remember, that's a boiled-down version of the real problem. When it really occurs in your real program it's dozens of times more complicated, with branches and error cases and logic all mixed into your callbacks. Been there, done that.)
Love Neil Mitchell's work!
There is no such thing as "synchronous" and "asynchronous". At least not the way you've been recently taught to think about them; those words used to mean something but in the last year they've been brutally beaten and left for dead in the street. There is only blocking, and not blocking, and the question of how things that are currently ready-to-run are scheduled. You express confusion about having your "async IO cake" and yet writing with the clarity of synchronous code as if this is some sort of mathematical impossibility that has nonetheless been miraculously brought forth, but in fact what you've come to see is the fundamental wrongness of the recent use of the "async" term. What you have in (pure) Haskell is non-blocking code with one of the most sophisticated schedulers available for any language; to beat Erlang on thread-ring is no trivial feat! And for all the _sturm und drang_ around Node.js, it is, ultimately, strictly blocking, and its scheduler sucks. Node.js isn't non-blocking, it forces the _programmer_ to be non-blocking. And once you stop looking at things as "synchronous" as if that has some sort of real meaning, and examining the question of what blocks and how things are scheduled... you'll see that Node.js isn't very impressive at all.
this. but sadly, we're going to be fending off node groupies for years to come. the front end crowd really appears to think that server coding is nothing more than knowing the syntax. know js? well now apparently you know everything you need to know to write your backend too. and whats even more amusing is that some of them actually think that node is the only way to deploy a server predicated on epoll....a technology none of them even understand anyway 
i read the design docs for mongrel2...and while interesting theoretically, something about the pathological avoidance of blocking seemed to set my bullshit detector off...primarily because no one has bothered to try this route before, and also because it seems to solve a "problem" that isn't actually a problem for anyone but maybe i'm wrong...anyone seriously benchmark it?
Sigh
Wow... nothing makes we want to never read a blog again more than: Uh oh, you're about to all crucify me. Yep, here come the 'monad police' to attack me for being so, so, very... unorthodox!
&gt; So perhaps the true value of monads lies in their exclusionary nature. Say what?
(Original submitter) Ignoring the tone, what's the technical response to Gilad's argument? What's the comparison between Actors and Monads?
Dude... there is no monad police.
I am having trouble seeing how monads and actors are alike. Setting aside the matter of typedness, aren't actors about state isolation whereas many monads are for deliberate state-sharing? Working with monads, we think about building up larger programs by agglutination or running a small program inside a containing program. Whereas, the actors paradigm lays emphasis on building up larger programs through message passing between smaller ones.
This is awesome, but it seems to have a bug. When I use the search embedded on the website for, eg "list", I get results like http://haskell.org/hoogle/?hoogle=list&amp;prefix=%2Bhoogle (shouldn't have that "prefix hoogle" in there
&gt; The most important practical contribution of monads in programming is, I believe, the fact that they provide a mechanism to interface pure functional programming to the impure dysfunctional world. I take the above to be a strawman which the rest of the blog post attacks, and I disagree even with this one sentence in isolation. The most important practical contribution of monads, in my opinion, is that they give us a common language to talk about *many* things, imperative programming being one of them, and in the absence of the IO monad I would still find monads about as useful in my day-to-day programming as I do now. I also disagree with the notion that the real world is so obviously impure and dysfunctional. I would be *amazed* if somebody could actually show that the world can be simply modeled as a series of discrete, sequential instructions. I see the IO monad as merely a hack to get Haskell to interface natively with our conventional architectures and operating systems, nothing more.
I reread the article several times, and didn't find much of an argument there, honestly. Stream-based I/O, a la the early days of Haskell, seems to be somewhat similar to an "actor model" in some sense. But without a clear idea of how the author here is thinking of capturing I/O using actors, it's tricky to really answer that question. And the author spends basically no space elaborating on what an "actor model" of purely functional I/O would look like; and instead just makes bizarre and distracting comments.
To be honest, it's always seemed to me that the monad tutorials make monads seem a lot more difficult than they actually are, at least to use. With do syntax, it doesn't seem much different from a traditional language to me. I guess it can be confusing that the IO monad and the Maybe monad work differently. The great thing about monadic IO, in my opinion, is that the IO gets reflected in the type signature of your functions that do IO. This makes it easier to see what functions do, and it also probably helps the compiler find errors in your program. I think you would lose all of that with actors, and then I don't really see the point of using them for IO. You might as well throw the whole referential transparency thing out of the language entirely. Creating a monad is a bit more difficult, but you don't need to write your own monads to use Haskell effectively. As far as I can tell, anyway; I come from OCaml.
monad class is very simple, and there is nothing in it saying 'm' is hiding impurity. So IO monad instance captures a very simple view of the IO world. Sounds like actors want to capture something more. I would not be surprised if the monad instance would be a constraint for the actors instance. Googling "Purely functional actors" is not saying much btw.
That's by design - the embedded search in that little box is just for the Hoogle library, not for all of Hoogle (which searches the platform). When switching to the main website it adds +hoogle to show you how to search only the hoogle library.
&gt;With do syntax, it doesn't seem much different from a traditional language to me. The `do` notation certainly is convenient, but this little bit of syntactic sugar has a few drawbacks when it comes to helping people understand monads. The first is that it makes the program look imperative, which in the context specifically of the `IO` monad is just fine, but in the context of other monads perhaps not so fine. For example, what does `play` below do? import Control.Monad.State play = execState pro [] pro :: State [Bool] () pro = do pro s &lt;- get put (True : s) The mental model `do` notation encourages makes understanding why this code does what it does quite difficult (this example code comes from a submission to the Haskell reddit a year or two ago, I provided a [blow-by-blow breakdown at the time if you're interested](http://www.reddit.com/r/haskell/comments/ak9g1/fun_with_the_lazy_state_monad/c0i1dcq), but the gist of it is that the `pro` need not be executed until `s` is requested by `execState`). The second issue with the `do` syntax in my mind is it reinforces the notion in beginners that monads are somehow a special language feature requiring special support. The concept that `IO` is impure in Haskell prevails even among some moderately competent Haskell programmers, despite this manifestly not being the case. `do` notation, like list notation and list comprehensions, is a convenient bit of syntactic sugar, but like any syntactic sugar it can often serve to obscure what is "really" going on under the hood, which particularly to a newbie is not always a good thing. And the fact that so many people think monads are about IO, or at the very least about state, or worse about sequencing actions, are misconceptions that the `do` notation reinforces and that serve ultimately as a stumbling block to really understanding monads. I don't think beginners should use it much.
mapArray creates a new array though, so it will obviously be slower. See my array-util package for some implementations that might be useful for you. Hopefully most of it will be included in the array package in GHC 7.2.
I'm putting a bit of effort into properly understand the Kiselyov &amp; Shan paper on static capabilities without dependent types, and thought other people might be interested. It's mostly written for myself, to force myself to understand each section as much as possible. If you spot any mistakes please let me know!
Let me add: The "actor" models operational semantics are very much imperative. So while the language running inside an actor may be purely functional, its communication is most definitely not. This of course breaks down purity of the process totally. Now, the model is practiced by Erlang to great success (imperative communication + functional computation) but the imperative part is a poison to purity.
Those warnings have largely been removed by now too.
It's called [currying](http://www.haskell.org/haskellwiki/Currying). 
@ gilad, Nope, you can't crowdsource enlightenment. The deep ideas in life can't be taught, only learned. You may admire the slimness of Pierce's book but unless you knuckle down, put your nose to the grindstone, and work on the exercises, that admiration serves nothing but your own smug ignorance. (This is not to oversell Pierce. There are numerous other ways to understand monads, all of them intellectually challenging and deliciously satisfying. That you are wallowing in tutorials each written for a single individual that typically no longer exists indicates you're trying to shortcut the gratification.) As a noted leader in the community you really should know better. Shame on your sloth. 
 lowerpoint = comparing (y &amp;&amp;&amp; x) Not saying this is the best solution. But it's the shortest one I know of.
Note that asking for a list's length forces the entire spine of the list to be evaluated.
To be fair, the operational semantics (as far as they exist) of monadic I/O are also very much imperative. People are thinking about how to interact with the world in a more purely functional way, but they are seeking alternatives to monadic I/O -- for example, FRP. Of the options that are on the table for this article, all of them have the property that they view the world as being imperative at a top level, and try to carve out a bridge somewhere to provide for purely functional programming in the details. I do think it's a fair criticism, though, that doing something like actors would necessarily carve out that bridge at a much lower level, and push more of the application logic into the imperative world of interacting between different actors.
That kind of pre-emptive defense sets the author up to be able to justify any criticism as "I'm counter-culture and a free thinker" rather than "I'm wrong". Computer people hate being wrong.
Huh. I *completely* disagree here. Sure, do notation makes code look more imperative, which is exactly what you want to do when writing imperative code! It's also exactly what you want beginners to do when *they* are writing imperative code. It's also pretty thin as syntactic sugar goes; the translation from do notation to explicit use of &gt;&gt;= is simple and straightforward. If you'd said "people learning monads should spend some time getting familiar with how to stitch together monads without using do notation", I'd agree. But until then, beginners in Haskell should *definitely* use do notation to write their I/O actions, whether they know what a monad is or not. And do notation is also extremely convenient for *any* Haskell programmer who ends up needing to write code in that style.
&gt;Sure, do notation makes code look more imperative, which is exactly what you want to do when writing imperative code! ... [B]eginners in Haskell should definitely use do notation to write their I/O actions, whether they know what a monad is or not. You do make a good point here, actually. When you're just starting out, it *is* nice to be able to completely ignore what a monad is and still be able to write programs that do basic I/O. Although in my experience this breaks down pretty fast, because it can be conceptually difficult for beginners to wrap their heads around the difference between `let foo = ...` and `foo &lt;- ...` unless they have a monadic model of I/O in their heads already. The type system will tell them of course which one they need and I suppose you can teach it on those grounds, but I still think that it all becomes a lot simpler to grasp when you ignore `do` notation completely. Still, I'm glad it exists.
It's true (I've taught a few dozen people basic Haskell programming in my time) that beginners often are unsure of the difference between `let foo =` and `foo &lt;-`. But that's really a lot more about adjusting to thinking explicitly about effects than it is about monads. It's hard for some people to break old mindsets where effects are just assumed; but in the end, it's pretty easy to make the distinction between "give this a name" and "do this, and give a name to its result". At the point that you've made that distinction, monads have still not entered the picture. Indeed, throwing in new abstractions when people are still struggling to understand the very concrete case in front of them is doing them a disservice.
I've been following your package, but it doesn't appear to improve on my unchecked `STUArray` implementation. I don't observe any speed difference when compiled with `-fllvm` on GHC 7.0.1. I doubt much more can be done from a user's perspective to improve array performance on this microbenchmark than the code I have already written. Further performance improvements must come from improvements to the compiler. Please correct me if I'm mistaken.
It's a nice effort. It's a bit sad though to see people shirk away with such force from dependent types. The [refinement types for ML](http://portal.acm.org/citation.cfm?id=113468) by Freeman and Pfenning allows use of "lightweight" dependent types to perform exactly this kind of analysis, with far less burden on the user: one only needs to declare that certain constructions takes non empty lists for example, and the analysis verifies that this function is only applied to non empty lists. This is using technology from 1991. Anyone who would want to perform this analysis "by hand" seems a bit masochistic to me...
since there are three laws - there must be monad police.
I don't think this is shirking away -- rather it's showing how to get there with the tools already available.
Cheers, I will have a look at that paper (available [here to avoid ACM paywall](http://www.cs.cmu.edu/~fp/papers/pldi91.pdf)).
Also worth learning foundational logic (such as Bertrand russell and Godel's work) and type theory. Also lambda calculus, System F, System Fw, etc. Ben Pierce's Types and Programming Languages will help with that,
Pretty cool. Great way to sneak Haskell in elsewhere.
It would be helpful if you posted your vector code so that people might be able to see if there are improvements to be made. I believe vector works best is you use a more stream based approach, it should be easily possible for the compiler to perform a single pass over the array.
Low hanging fruit gets turned into papers. You'll have to get your [fruit picker](http://www.google.com/products/catalog?q=fruit+picker&amp;um=1&amp;ie=UTF-8&amp;cid=17942952901329330702&amp;ei=Ph8-TYm1H4H7lwe538X3Bg&amp;sa=X&amp;oi=product_catalog_result&amp;ct=result&amp;resnum=1&amp;ved=0CD8Q8wIwAA#) out
This reminds me of a couple recorded talks that Jane Street has available regarding their use of OCaml. Jane Street is an automated trading company and probably has the most OCaml experience besides the OCaml creators themselves. They talk about why they switched from the old mix of Visual Basic and Java and C# to almost exclusively OCaml and what it means for their business. http://www.janestreet.com/technology/ocaml.php
I'm also looking forward to helping out with GHC, but I'm waiting for the official move to Git first. Once this done, there's a number of small things I've spotted that need fixing (documentation fixes, some additions to the array package). I'm looking forward to helping out!
From my poor understanding, lots of the low hanging fruit doesn't require theory -- it requires compiler hacking, a knowledge of unix systems stuff, an understanding of abstract syntax trees, depending on where you focus, maybe an understanding of parsing technology or of the intricacies of `make`. To understand how GHC works, a good way to start is to read SPJ's STG book(s): http://research.microsoft.com/en-us/um/people/simonpj/papers/slpj-book-1987/ http://research.microsoft.com/en-us/um/people/simonpj/papers/pj-lester-book/ Then there are SPJ and Simon Marlow's papers, many of which describe aspects of GHC internals: http://lambda.haskell.org/~simonmar/bib/bib.html http://research.microsoft.com/en-us/um/people/simonpj/papers/papers.html The ones on exceptions, concurrency, and the runtime system are of particular interest, depending on what grabs you. But again -- there's plenty of stuff to be done to get your feet wet that would involve just cleaning up/improving ghci, ghc-pkg, or the like, I think(?) Note that even many of the more difficult bugs/features in the trac don't require thinking about, e.g., type theory, but rather chains of fancy interacting systems for managing either codegen or the runtime, with a few especially focused on making things work correctly/uniformly across unix and windows.
Thanks sclv. This is exactly the kind of response I was looking for. I'm interested in GHC, but when I think of where to start, I get a bit daunted.
Certainly. Here it is: import Data.Int import qualified Data.Vector.Unboxed.Mutable as V import Control.Monad.ST import System.Environment update :: Int64 -&gt; Int64 -&gt; Int64 update orig round = orig + round incrArray :: V.STVector s Int64 -&gt; Int64 -&gt; ST s () incrArray arr round = spin 0 (V.length arr - 1) where spin i n | i &gt; n = return () spin i n = do x &lt;- V.unsafeRead arr i V.unsafeWrite arr i $ update x round spin (i + 1) n main :: IO () main = do [_nr, _len] &lt;- getArgs let nRounds = read _nr :: Int64 len = read _len :: Int loop :: V.STVector s Int64 -&gt; Int64 -&gt; ST s () loop arr r | r &gt; nRounds = return () loop arr r = incrArray arr r &gt;&gt; loop arr (r + 1) stToIO $ do arr &lt;- V.replicate len 0 :: ST s (V.STVector s Int64) loop arr 1 I see: &gt; % ghc -O3 -fllvm -fforce-recomp --make ivec.hs &gt; [1 of 1] Compiling Main ( ivec.hs, ivec.o ) &gt; Linking ivec ... &gt; % time ./ivec 1E8 10 &gt; ./ivec 1E8 10 2.80s user 0.00s system 99% cpu 2.802 total This is 311% of the running time of the C version, and _slower_ than my own unchecked `STUArray` implementation. Replacing `Data.Vector.Unboxed` with `Data.Vector.Storable` gains about .5 seconds (a performance gain of about 17.5%), and makes it slightly better than my unchecked `STUArray` version. I don't believe stream fusion can simplify the problem to any less than an update loop over the elements of the array. Again, if I am wrong about this I would be delighted to be corrected.
You can build a README into the description portion of your cabal file. It then looks like http://hackage.haskell.org/package/ad Sadly you can't hyperlink out though.
I would be interested to see what performing something like loop arr round | round &gt; nRounds = arr loop arr round = V.map (update round) (loop arr (round+1)) the point of vector is to write high level code that compile to low level, fast code. there are a lot of potential optimisations that could be fired here (the main one would be that you only need to read from and write to the array once). Writing out the explicit array accesses completely misses the point of the vector package.
The partial compilation stuff would be nice but I don't find myself needing it that often. The type-in-context would be very nice though. Lambdabot has an asTypeIn combinator that allows for that indirectly, but it's a pain to use.
There's plenty of hacking needed in the run-time system that doesn't require much knowledge of compiling functional languages. To give a random example, the event logging (which is used for thread profiling) needs improvement.
...no syntax highlighting in your `ghci` dream session?
I also can't wait for that; it would definitely make GHC hacking at least seem more accessible to a lot of people.
The hackish way to do it is with the implicit params extension: &gt; :s -XImplicitParams &gt; :t putStrLn ?x putStrLn ?x :: (?x::String) =&gt; IO ()
That only works for some things. Sometimes you want to ask "what type does term X have in context Y", whereas the implicit params gives you "what's the type of the hole in context Y". I guess you could unify them with putStrLn (?x `asTypeOf` t) or something, but it'd still be more pleasant to have a dedicated system for this :P
This actually sounds like a mess. Four different languages, including a JVM language; some kind of custom message queue layered on top of Redis; their own pet version of daemontools? And they store all their data in RAM? Using a great language like Haskell won't help if you don't put more effort into keeping your overall system simple.
Welcome to the real world. :) At work we use many more languages than this (some homegrown).
Oh. I'm sorry.
Eh? I'm not sure if this is a serious comment, but I'll assume it is and do my best.. 1. There are several teams at Bump, and different coders use different tools for different tasks. Like.. pretty much every engineering organization with a diverse set of challenges. 2. The JVM language is to run on dalvik (Android), so the platform pretty much dictates we choose between a narrow set of options. 3. I don't know what's "custom" about it, Redis is a fine queue server. We use it for queue behavior the same way many do. 4. No, we only store a few hundred GB in RAM; all our data is many, many TB. Most in of it in Amazon S3. 5. And, uh, Haskell *has* helped us; that's the entire point of the post. 
It's funny. In the few arguments I've gotten into with the nodejs fanboys, they always state that "people already know JS!" as if it were a good thing. As a person who does both frontend and backend coding, the last thing I want is some self-described "JavaScripter" writing code that actually _affects_ data.
cool. This could be useable with a task to insert a markdown README into the description of the cabal file. Would be nicer if it was more automatic.
My complaint is similar to "==", "=", ":=" errors. No matter how well I know the syntax I mix those up in practice, especially when I am switching between languages. I have a new thing for not using contractions. I scan my text for contractions and expand them. It is like a Yoda-condition for English.
For me the partial compilation would be extremely valuable, for example if I build a bigger function out of smaller functions, where the smaller functions are done and type-check but the bigger function still needs to be finished. If I make a type error in the big function and reload my session, I can no longer ask for the types of the smaller functions I'd already finished.
There's nothing bad with this. Different languages excel on different problems.
If you want to do some bigger funner stuff, learn about type systems. Ben Pierce's Types and Programming Languages is good stuff. But most of the smaller jobs on GHC appear to be mostly compiler hacking.
Yeah, unfortunately, the format used by haddock and by markdown are largely incompatible. =(
I disagree that there's nothing bad with it. Every additional language requires you to pay attention to how objects are moved in and out of its internal representations, and how it is built, tested, deployed, and upgraded. If more than one language is used to deal with the same data structures, then any functions operating on those structures have to be duplicated. Even if the different languages operate in completely different places, it still imposes a cognitive load on any engineers who have to work with both - they need to remember syntax, libraries, idioms, etc., for two languages.
Is the libgmp linked dynamically or do you use a GHC without libgmp?
I've been doing GHC hacking recently (working on integrating/improving/maintaining compiler plugins.) Some observations: * GHC doesn't require a lot of type theory or anything for like 99% of the stuff you'd probably work on. It's a compiler - most of the low hanging fruit are of that nature, not of the type system nature. Things like fixing support for different platforms, better error messages, documentation fixes are ALL worthwhile projects to hack on and help everyone. * There is a ton of small hanging fruit in [trac](http://hackage.haskell.org/wiki/trac). Small fixes for GHCi, compiler crashes on non-tier-1 architectures (e.g. PPC,) feature requests, doc improvements and even some library improvements. There are also bugs for thing like RTS improvements - e.g. I seem to remember eventlog support needs work (files get large quickly, but eventlog just uses `fopen`, so you need support for big files, among other various things.) Many of these things can probably be tackled in a day or so, even without extensive GHC knowledge. * Most of the complexity to GHC in my opinion, is because a lot of the work revolves around GHC cooperating with 4 different things: * The compiler (and code generator) * The runtime system (a beast in itself) * The hardware * The operating system (of which, there are many...) So larger changes may require knowledge well outside of just compilers - you'll need your unix and some hardware knowledge afoot. Lots of the most difficult GHC bugs and tickets aren't even that hardcore theory wise, they're changes that affect lots of individual components and affect interactions between parts of GHC, like the code generator and the runtime system. The OS and hardware already being insanely complex - outside of GHC's influence - doesn't help make it any easier to manage. That's just the truth. * If you're stuck, glasgow-haskell-users and #ghc on freenode are great places to hang out with other GHC hackers. Very helpful commentary on the design of GHC, and how things should be done. * Pick something and work at it. If it's a bug or feature and you can make headway on it, everybody will appreciate it and others will help. Now, as for actual research papers, any papers by Simon &amp; Simon are worth reading, and you can find them here: http://research.microsoft.com/en-us/um/people/simonpj/papers/papers.html http://lambda.haskell.org/~simonmar/bib/bib.html Good luck!
&gt; is down, due to a hacker attack. I'm curious. Any details about this ?
Ok, so you're acting nonchalant! Quit rubbing our noses in it.
Keegan McAllister has a working Haskell Android port, completed for iPwn studios. I don't know what their plans are for public release, but given that they released the code for the Haskell iPhone port, its rather likely they will do the same here, so this likely doesn't really change anything. =)
for reference: http://developer.android.com/sdk/ndk/index.html
Summary: Basically, what it's saying is that in an upcoming version of the Android operating system, APIs will be in place to write complete applications in native code. I believe one of the major sticking points for Haskell on Android is the need for Java before you can package an application; so currently, even though many parts of an application can be written in native code, it has to be Java at the top level. Apparently, this is going away. Now, Gingerbread as a product release is still a few months off, and Gingerbread as the dominant version installed on actual phones is probably at least a year further in the future beyond that. But knowing this is coming probably changes the roadmap for getting Haskell running on Android significantly. It means that going the straight native compilation route is probably the reasonable choice. (Previously, there's been some speculation about compiling Haskell to Java bytecode.) I believe code generation already exists for ARM (in both unregisterized C, and probably LLVM now, too). And, of course, we've got x86 covered too (a lot of upcoming Android tablets are x86-based rather than ARM.) That still leaves a port of the runtime system; no tiny job, since the Android user space libraries are substantially different from any existing platform. And, of course, binding will have to be built to the Android libraries with Android 3.0. Still... perhaps part of this could be another GSoC project idea?
Gingerbread is already released, albeit only on a single phone (the Nexus S).
I've been working through this book for a while; I must say, it's no simple task.
Oops, you're right! I had my version code names confused! So things are better than I thought.
This is very good news! Thanks for the information. I'll keep my fingers crossed!
Actually, I use their pet version of daemontools as well. It's available for public use on github.
jvm doesn't completely go away (unless you're dealing with rooted devices), it's not any different than it was in previous versions of android. What you had to do before was write some thin java-JNI glue code to native but since android 2.3 google provides a framework that takes care of the glue code which means you don't have to touch any Java/JNI you can just write a shared library (or static because the glue becomes a shared library). When the application runs it's the jvm calling into native code. To make writing native code more useful they also provide more native libraries, stuff useful for games and soft real-time applications but there isn't native equivalent for android UI stuff that is in Java i believe. The NDK is gcc port targeting ARM architecture. Easiest way to get Haskell running on android is to make an unregistered port of GHC using the NDK's gcc or using JHC to use NDK gcc (which is trivial to do). The only problem with JHC is that sometimes it generates erroneous C code on complex expressions (and some other problems which you can get around using jhc/gcc flags). The only major thing to do is writing Haskell bindings to the native APIs. By the way Android 3.0 is for tablets only not phones.
not sure, just using a stock ghc 6.12 here.
Thanks for the information about JNI. I hadn't looked into the details of the new NDK stuff; just that they claim it's possible to build applications in straight C++. I suppose Haskell bindings could be given for the JNI calls, though. I think you're incorrect that Android 3.0 will be tablet only. For example, there are techniques in the SDK for building applications that scale to both tablet-sized and phone-sized screens. See http://pocketnow.com/android/matias-duarte-gives-glimpse-of-android-honeycomb-on-phones for an interview with a Google employee about Android 3.0 on phones.
Last week exim behaved strangely. A server reboot seemed to fix the problem, but we never found out what had caused it. Then when users started complaining that they were suddenly unable to access the server with their SSH keys, we investigated and found that OpenSSH on the server had been replaced by some other sshd that had been compiled within the past few days. At that point we promptly shut down the server and began working in earnest on the migration to a new machine that we anyway had been planning.
&gt; Thanks for the information about JNI. I hadn't looked into the details of the new NDK stuff; just that they claim it's possible to build applications in straight C++. I suppose Haskell bindings could be given for the JNI calls, though. They claim that because they've provided the glue code framework for you so you don't need to touch any Java/JNI code but your code is being called from the jvm, you use the NDK framework and implement an android_main. NDK's GCC (which is just gcc targetting ARM arch) can generate native executables but you can not run a native executables (not shared libraries) on the android echosystem offically (through the app stores,etc) unless you have a rooted (modified) phones/tablets, basically you can't disturbute native executables. You don't need to write Java bindings for Haskell, only if you want to use other libraries in the android framework not available in the NDK, this will typically be all the UI framework stuff. If you don't care about making general applications then you only need to write Haskell bindings to C/C++ libraries provided by the NDK.
Nice job guys! Good read, and good job glossing over the funny bits to make web programming in Haskell accessable to folks without a lot of prior exposure.
Thanks for putting the Quotes of the Week at the top!
I wrote a simple haskell HTTP server a while back: https://github.com/njoubert/HaskellDouche/blob/master/webserver.hs
Seconded :-)
This is a great tutorial! I am just starting to learn Haskell and I wish for more that explain things to this level of detail.
hahaha haskelldouche
:-)
I don't think that beautiful means what comes first to mind. Surely, it is a symmetrical solution but with a little thought one could implement something more elegant and 10 times faster. (tried this)
I'm pretty sure I did this one exactly this way.
It's question 9, this is pretty much the expected solution at this stage. OP, Check out http://en.wikipedia.org/wiki/Pythagorean_triple, specifically Euclid's formula when you get to the next Pythagorean triplet question.
Expected at this level doesn't mean genial (or beautiful) idea. Newton was expected to think God had thrown him an apple but he didn't. Caveman was expected to believe in the Almighty Bull but he discovered fire. Of course, I'm not saying this to discourage the man. It is great he decided to use Haskell for it but list comprehensions are really not a very beautiful solution.
It really doesn't build three lists due to laziness, does it?
Hey, I think that may be the templating language I've been reaching for for a while now. Is it smart enough to automatically ensure that you use the proper escaping type for what you are trying to interpolate? Is it smart enough to ensure validity of attributes by for example stripping out the characters that aren't allowed at all? If you interpolate an attribute name is it smart enough to check that? (If not now, is the underlying design solid enough that that wouldn't be that hard? I wouldn't mind adding it if it doesn't require a complete overhaul.) I read the docs as they exist now but I'm still not sure of the answer and I'm not in a position to try it right now. (General context for my statement: Outputting HTML is among the canonical problems that looks easy but is actually fantastically difficult. Template languages have focused way too much on making it easy to slam strings together and make it virtually impossible to get escaping correct. Usually once the template language is forced to consider the problem they still cop out with just HTML-encoding everything with no idea of what's valid where and hoping for the best. I'm very interested in a template language that is aware of the context it is in and instead makes it very hard to get wrong by including anything invalid. I also think such a template should actually try to look like HTML to make the context matters clear, when you're in a tag, when you're in an attribute, etc. Getting away from HTML-like syntax isn't necessary and makes it easy to forget where you are.)
The new hamlet syntax has the goal of being as close to html as possible, but eliminating any superfluous characters- the only thing different that is forced on you is that it is indentation-based instead of having closing tags. Yes, the templating has the smarts that you are looking for. The String type is html escaped and the Html type is not. When Html types are received from a form they will be xss sanitized.
I think OP meant "syntactically beautiful", in that it resembles a set comprehension... certainly a more superficial kind of beauty than what you're talking about, but maybe worth considering. Part of what we appreciate about Haskell is its concise, mathematical syntax, no?
It would be more "beautiful" to use head instead of !! 0
Oh hey, it’s `Fin`!
Any download links yet? I looked at the page mentioned but didn't see anything. Looks awesome though
It looks like something you should submit to WeAreTheMusicMakers subreddit as a “livecoding” example. 
In that case I was wrong to criticize
Not yet, I'll put the code up next week but I don't expect it to be useful for other people...
I fail to see the significance here - but it's probably my fault. Isn't this just a length-indexed vector? I don't see how that's interesting, seeing as people have done in Haskell for quite a while.
Length-indexed vector? Forgive me for I'm not familiar with the term, but he's actually produced a "list type" which is typed on its length, yet can be produced infinitely, with length-polymorphic functions still possible. It's pretty neat imo! 
Neat, yes. New, no. 
There isn't that much significance yet. He's just building up some general principles so far. But if the past is any guide, somewhere around 2/3 of the way through this series of posts, something really neat, surprising, and elegant should emerge. Until then, we just have to bear with the exposition :-)
Okay, that's cool. I don't know if the author ever claimed it to be new, though. He does [take it further here](http://www.reddit.com/r/haskell/comments/fc4bx/conal_elliott_doing_more_with_lengthtyped_vectors/).
Right. Not much (or perhaps anything) new yet. Hopefully useful for folks who haven't seen these techniques. And yeah, it's going somewhere. Originally I was writing a single blog post called "From numbers to vectors to trees", but it got too long, so I broke it up into six bite-sized pieces. After this six-part series, I'll move on to the motivating project, which is deriving low-level, imperative, data-parallel programs from elegant, functional specifications.
That is pretty awesome
&gt; Here's a challenge for you: write a quine that takes as input the name of a language and outputs the same thing implemented in the input language. Much harder than what I just wrote. That's a bit of an overstatement... just use some simple encoding to avoid having to implement n^2 string escaping rules. Here's one using Tunisian encoding: #!/usr/bin/perl $o = $_ = "dfahfgdgjgggjhacbgacmgbgoghgfhbghgfgacpgogacehigfgacdgpgngngbgogegacmgjgogfgockajancdgacacacacacacacacacdekajancigdhacacacacacacacaciebgdhlgfgmgmgkajancahmgacacacacacacacacaffgchmgka-c:dcjgogdgmgfhegfgacmddhehchjgoghgocigodkadcjgogdgmgfhegfgacmddhehegjgpgocigodkadgigbgchackcegacndaccc+ccldkajgogehacngbgjgogicjgogehacdgmcdgigbgchkckcghjcaclhkaacacdgigbgchackcehacndacdgacodacbdacpdacdhehchdgigchicdhehchdhehchicegmcghlfbdnfjcmchckdhcjclcbdackdacegldkaacachhigjgmgfgickcehbcndhcnchcjclhkaacacacackcehndndhclchcpdahchjgogehggicccfcdhccmcacicehncncmcegjcjckdahfhehdgigbgchicehlfadnflcehlfbdnfkcbdgdncbdgdedjdjcldkaacacacacehaclcndaccdldkaacacnhkaacacchfgehfhchogacadldkanhka-hs:jgngahpgchehacdfjhdhehfgngocfeogghjgchpgogngfgogehkajgngahpgchehaceebgehbgocmejgdhehkakaegacndaccc+cckakancncacogpgehacghfgchjhacahchfgehehjhockaegfhngahacndacahfhehdfehchacocacggkaacachhigfgchfgacggacichclchckddhjcacndacegaclclcacggacdhkaacacacacacacacacggacichcnchckdpfjcacndaclfnfkaacacacacacacacacggacicbgkdcgkddhjcacndacehpgfeogfhngacicggchpgngfeogfhngacbgaclcacbdgdackcacggchpgngfeogfhngaccgacncacbdgdedjdjcackdacggacdhkakangbgjgogacndacegpgkaacacngacmdncachgfgehbechhgdhkaacacdgbgdhfgacngacpgggkaacacacaclfnfacncodacegfhngahacegkaacacacaclfihnfacncodacegfhngahacocacehbgjgmgacocacegchpgahhfigjgmgfgacicpcndachckdhcjcacocacigfgbgegacocacggjgmgehfgchacicjgdhafchfgggjgihpeggacihjcacecacehbgjgmgdhacegka-pl:dcbcpcfhdhchpccgjgogpcahfgchmgkaecpgacndacecpfacndaccc+ccldkapcecbecfhegflfadnfkdpdiclfofncnflcjcpcldkaecpfndecbdldkaahchjgogehackgpgjgogacecpgmcacngbgahaclhacjhpcbgnckhpcadncjdbgncggpcldacahbgdglgacccigkcccmcacecpfacnhacdhahmgjgehacpcmflcpcmcacecpfldka-"; /$ARGV[0]:?([^-]+)/; $_=$1; print join $o, map { y/a-z/0-9a-f/; pack "h*", $_ } split /\+/, $_; Or you could just have Brainfuck as one of the languages, and write a BF interpreter in each language :)
Write a reddit post about a blog post about quines. Because everyone needs to be able to make "Everyone needs to be able to make quines."
Care to explain that long string of characters in line 2?
There is MVar which gives synchronized (mutable) variables and TVar which gives transactional (mutable) variables (STM). there is Chan which is a channel, forkIO spawns a lightweight thread. Just have a look at the [Control.Concurrent](http://www.haskell.org/ghc/docs/latest/html/libraries/base/Control-Concurrent.html) modules
In this case, an MVar sounds like a good fit. Take from the MVar when reading the mutable state, and put the modified value back when you're done.
I know Conal doesn't think it's new, but you seemed to think so.
My first DHT ([ogv of operation](http://web.cecs.pdx.edu/~dubuisst/dhtLeafSetView.ogv), [perhaps a better one](http://web.cecs.pdx.edu/~dubuisst/LeafSet50.ogv)) used Control-Engine (on hackage, lots of Chans, MVars and even some STM). This might have been fast, but was too complex, making me unhappy. I then built a simpler one (also more or less Pastry based) but first abstracted the DHT Router operation as a new typeclass "DHT", and made the entirety of the router a monad transformer (using StateT and ErrorT) so I can build DHT applications (or simulators) using this transformer and the DHT class. This isn't done (the simulator has been mostly done for six months!) but I'm much happier with the code. Unless you are concerned about extreme scalability (even in the face of contention) I really suggest you ignore the MVar path and go with a state monad.
Let's all pick a language and add it to make the worlds highest order quine!
My apologies; I am nothing as learned as either Conal or yourself :-) (i.e. this was the wrong place to start making comments)
I once wrote a small library and put it on hackage: http://hackage.haskell.org/package/mstate It offers basicly the same functionality as a regular StateT, with a convenient way to start new threads via "forkM" (don't mix it up with "forkIO", which shouldn't be used in your mstate-app at all). The library then takes care of all that MVar stuff required to share the state over all threads. However, it's pretty much experimental, so I can't guarantee for anything. If you experience any bugs, please let me know.
I like this. The State monad is a much more intuitive abstraction for mutable (concurrent or not) state than an MVar wrapped inside a Reader monad.
No worries. :)
Who would down vote such a thing? Anyway, thanks to the OP for bringing this to my attention and the content creator for sharing his knowledge. This came in the nick-o-time.
It looks like this: 111-c:222+222-pl:333+333-hs:444+444- Pass in a command line argument, and it will search for that (i.e. passing -hs will select the 333 part; passing nothing will select the 111 part). Once you've selected which part you want, it will decode and print it. The *111* part is the help message and list of languages, the other ones are the decoder in various languages. The encoding is just hexadecimal encoded with [a-p] rather than the usual [0-9a-f], because I'm lazy. Using only those characters lets you encode the whole thing without worrying about \, " and such. The + sign means "insert the entire string (undecoded) here".
well, ahem, done. *ducks* Seriously, I spend more than a little time pouring over your code Chris. I don't understand alot of it, but it does answer questions for me from time to time.
There's Haskell for iPhone? Link?
This is the Haskell subreddit. It's not the functional programming subreddit.
As a Haskell programmer I am also interested in F# and the promotion of FP in general.
It's addressed to Haskell'ers, sent to haskell-cafe, demonstrated love of Haskell is listed as desirable... doesn't seem particularly off-topic. "jobs" is listed in the sidebar too.
Anyone want to post this on Haskellers? http://www.haskellers.com/jobs/
This also isn't the meta-Haskell subreddit :)
Anyone interested in applying for this should also contact me. I am a headhunter based in London focused on functional programming roles globally (mainly in the finance sector). You can PM me or email jwood@kaizenpartnership.co.uk Happy to discuss what my clients look for etc. Thanks 
Free C compiler with every purchase.
taotree, this sounds like something that I might be interested in but the description has disappeared from odesk. If you are still looking for somebody, you can contact me via http://www.thegenetictrader.com
Haskell is only a great language when you dismiss its shortcomings.
This is true, but I'm not so concerned about this particular shortcoming. It doesn't much bother me if the compiler is several hundred megabytes. I've got about 100 GB free anyway, and 4 GB of memory sitting around. I'm quite happy to let GHC use them, even if it could have made do with much less.
There is a mailing list here: http://www.haskell.org/pipermail/iphone/ I don't know where to obtain the binaries/source at this time as I haven't been following it very closely.
This isn't about haskell -- its about ghc :-)
GHC is the legal name for Haskell. 
A lesser known fact is that the street name for Haskell is "Lady H".
What shortcomings bother you, aside from statically linking the entire universe into the compiler? 
This should be one of the quotes in the Haskell Weekly News.
...as opposed to {INSERT ARBITRARY DISLIKED LANGUAGE HERE}, which remains terrible after dismissing its shortcomings!
Correct me if I'm wrong, but I believe the file noted here: "57048 /usr/lib/ghc-6.12.1/ghc-6.12.1/libHSghc-6.12.1.a" Is the GHC compiler library. Not the GHC runtime library every program links to. The GHC compiler library is, pretty much, the entire compiler in library form: http://www.haskell.org/ghc/docs/latest/html/libraries/ghc/index.html This thing is a large binary because it's capabilities are quite extensive. You can poke at pretty much every equation involved in compiling and interpreting GHC Haskell. Preprocessing, parsing, type checking, code generation (Even for other platforms) all have equations exported by this library. Huuuuuuge
Doesn't the first program contain a race, and either print "parent done" or "child done", but not both? That's the obvious answer, so I imagine it's wrong.
Interesting. How have you implemented your network code. Are you using some rpc library. Or have you made your own serialized datatype that you send back and forth? Do you have any repo of your DHT code? Would be a great reference for me.
This package seem like a really good push in the right direction, thanks a lot!
Assuming that the thread delay imposes the intended ordering (I could have manually synchronized it with MVars, but that would have obscured the output), the program has deterministic behavior due to GHC's FIFO scheduling of blocked threads.
This is what I get: ubuntu pps ~ » ghc --version The Glorious Glasgow Haskell Compilation System, version 6.12.1 ubuntu pps ~ » cat mvartest.hs import Control.Concurrent.MVar import Control.Concurrent main = do x &lt;- newMVar 0 forkIO $ do putMVar x 1 putStrLn "child done" threadDelay 100 readMVar x putStrLn "parent done" ubuntu pps ~ » ghc --make -threaded -o mvartest mvartest.hs Linking mvartest ... ubuntu pps ~ » ./mvartest child done mvartest: thread blocked indefinitely in an MVar operation Am I doing something wrong? (I'm not an experienced haskell user.) Adding `takeMVar x` before `forkIO` stops it from hanging.
Nope, that is the "expected" behavior.
Well, okay, you talk about Haskell a lot in your article, but what about *haskel*?
heh, good catch - all the title references were messed and now fixed. Unfortunately, there's no way to change the title in reddit...
no one's asked for the final `l` yet
Nice :D
If you need root on a flexible system, rent a micro spot instance from Amazon for a penny an hour (http://aws.amazon.com/ec2) and use an Amazon Machine Image from Ubuntu (http://uec-images.ubuntu.com/releases/lucid/release/).
I need root on my employer's system.
I've located a thread started by a guy whose goal is similar to mine (although our problems are different - he wants to distribute binary libraries, I want to distribute a C source package): http://www.haskell.org/pipermail/haskell-cafe/2009-April/060483.html My takeaway from that discussion is that GHC is almost definitely not feasible for actual C generation. JHC, however, is vaguely recommended, which kinda strengthens the case for it; someone even mentions that JHC's region inference means it doesn't even need a runtime. Still, I've read bits of scary news that JHC can crash on large enough inputs, hence my feeling that it is a bit too experimental as yet; I wonder how large is large enough.
JHC seems nice.
If it's enough to install GHC locally, perhaps you don't need to pass by the sys admins? I've done that in my office environment. It wasn't easy, because the kernel version we use is from 2006. So I had to start with a pretty old binary distribution of GHC: 6.6.1, then install 6.10.4 from source to finally be able to install GHC 7.0.1 from source.
JHC generates a small runtime with the C code in C of-course, jhc depends on a small amount of Posix functions and even those are not essential if I remember correctly. Pretty much all the Haskell compilers are dependant on posix compatible operation system. JHC has become a more stable recently, John (the developer behind jhc) has been mostly working on fixing bugs over the last few months. There was a new version released about a week ago. Before it would generate erroneous C code on complex expressions but this is mostly fixed and when it does still happen sometimes you can make it work but just shifting expressions around. If you do have a problem with JHC send an email on the mailing list and John (the author) will reply quite quickly most of the time and try to fix the problem or help you with a work around if he can. JHC is probably more stable than UHC, it's a fork of an older version of JHC and I don't think anyone has been working on UHC a for a couple of years now. Still JHC does have some stability issues so I'd be careful about committing to something if you need to develop and release quite soon unless you can afford to send bug report emails and wait for fixes. There is only one person working JHC which is quite an achievement and John needs people to test and send him bug reports to help.
Wow. At about 100Mb download each version, that seems kinda difficult. Getting the files from the internet is possible but a barrier also still exists; "too much data" would be difficult to transfer from the internet station to our office systems. 
JHC's region inference lessens the need for GC, not a runtime. When compiling Haskell, JHC spits out a single large C file that includes the runtime in it.
I've been in a similar situation, but I was able to compile it, rather than needing to send source. GHC outputs quite tidy binaries, depending on only a few basics - libc, libm and company. You can get tied into knots with a binary distribution, but if there's a standard OS or two you can support to cover most people (in my case, SL5 and Debian stable), then that might not be a big issue. Particularly if you're able to to distribute the source too, so that those on standard platforms can get the binary, and the weirder ones aren't left out. Dunno if that applies in your case, but might be simpler in the long run
Download them at home and use a usb stick?
&gt; JHC is probably more stable than UHC, it's a fork of an older version of JHC and I don't think anyone has been working on UHC a for a couple of years now. What UHC are you talking about? UHC to me is the Utrecht Haskell Compiler, which seems to be actively developed, and I also believe it is completely independent of JHC. Maybe you mean LHC? (which indeed started as a fork of JHC, but then morphed to be a GHC backend)
I'm surprised no one else has pointed this out, but what you're describing sounds like poor software engineering practices. What I mean is that because you are going to be writing code that your sysadmins don't support (via installed compilers, libraries, etc.), it sounds like you are the only person in your office who is going to be able to maintain this software. Why not write your project in C/C++?
Very nice article! I currently do this for two people using a spreadsheet, which makes the calculation very transparent. Your solver is nice and general. "Solve" sounds fuzzy, but presumably this code never fails to solve. I've been wanting to provide this feature in hledger. Something like: $ hledger equalize acct1=40% acct2=$100 acct3 to generate balancing transactions: 2011/2/3 equalization (acct1=40% acct2=$100 acct3) acct1 $N acct2 $-N 2011/2/3 equalization (acct1=40% acct2=$100 acct3) acct2 $M acct3 $-M 
Let's say you wanted to extend this function to also exclude public holidays (given a list), what's the best way to do that?
What of [Haskore](http://www.haskell.org/haskellwiki/Haskore)? Edit: and [HasSound](http://www.haskell.org/haskellwiki/CSound)
If you wanted to avoid the enumerate-filter-count method as much as possible you could probably break the entire range up into smaller ranges, then sum that. I've not thought about off-by-one issues but something like this: workingdays2 hols start end = sum $ zipWith workingdays alldates (tail alldates) where alldates = foldr insertOrdered [] (start:end:hols)
Ah yup that makes sense
Because I started in C++, and I got dissatisfied with the ever-lengthening link times (boost::variant is *big*). I know Haskell can probably get longer compile times, but at least during development, I *don't* have to compile (Hugs). The program I'm trying to write will operate as a batch processor, so efficiency's not that big a deal. However, I'm writing it as a set of transformations on an abstract tree, modelled as a multi-stage transformer - either I make lots of little programs (one each stage, connected by Unix pipes) to amortize compilation and deal with a ton of marshalling/unmarshalling from text streams as well as a ton of little programs, or I make a single program with the stages as components and go get irritated whenever I need to debug one stage while waiting for the linker to just read through each stage's object file (did I mention boost::variant is ***big***?) As for "it sounds like you are the only person in your office who is going to be able to maintain this software", I'm the only person in the office who can even conceive of this software (I don't work as a software engineer, I work as a hardware engineer). For that matter, I can write shorter, faster programs than our software group can write (I've done one actual, unofficial "parallel" development just to check that assertion, without releasing my version anyway). And again, talking to the software group requires red tape.
not allowed either
&gt; I know Haskell can probably get longer compile times, but at least during development, I don't have to compile (Hugs). Have you actually tried that yet? I would think you'd be better off using GHCi, and compiling occasionally so that modules that don't change often load much faster. Edit: oh, just saw the bit about how you can't get GHC working. You really should fix that. :)
That's what I'm already doing. Again, my program is written as several stages. I can always just load one stage into Hugs to debug just that stage (something I need to twist my build system into doing if I stick with C++). Having slow performance for the whole tool is not a big deal as it does something that normally takes far, far longer by hand. Having slow link times is a big deal because I'm getting irate that having a small change in one stage means I'm waiting for 3-4 minutes of link time, plus about a 1-2 minutes of compiling just that stage (did I mention boost::variant is ***BIG***?).
There are also Haskell bindings for SuperCollider, under the moniker of HSC3, which seems to be a better language still.
Not sure what you mean by skips the phase of selecting the device. Just click on the device you want to install on. That should activate the install button.
can't wait for this to mature. The relational algebra used in Rails now is very productive- I wouldn't want to be without it.
You need to have the developer tools installed first. http://developer.apple.com/technologies/xcode.html. Once you install xcode, you should be able to use the installer for ghc.
HasSound, unfortunately, seems to suffer from exactly this problem.
Me either, major kudos to the guys that are making this happen.
&gt; If you do have a problem with JHC send an email on the mailing list and John (the author) will reply quite quickly most of the time and try to fix the problem or help you with a work around if he can. Can't second this enough. While JHC is definitely experimental software, John is extremely timely in responding to emails about problems people run into with it. And the stability has been improving very quickly these past few months.
Well, from your description, sysads are not that responsive. You could try hanging around hacker channels waiting for a local privilege escalation to come up? Or maybe that's not what you meant ;-) It sounds like it sucks to work in your environment.
I would love a link to the paper if it's available. All I could find was the [title listed on the conference website](http://www.seamus2011.org/?page_id=26).
Yeah, they aren't distributing papers themselves. I've posted it at http://tanimoto.us/~jwlato/papers/XDSP_SEAMUS2011.pdf
Sysads *are* responsive, it's management that's tying their hands. Our work network is physically disconnected from the Internet. To download anything, you get the *official* USB stick, go to the sysad, and ask them to put the data on the work network. USB ports are disabled (at BIOS level) on all work computers, so only one computer (in the server room) has it enabled (and only sysads are supposed to stay long in the server room).
I don't see why you'd avoid the enumerate-filter-count method, though. Just use something like this: countBetween start end = length . filter (inRange (start, end)) Day is an instance of Enum, after all. [Data.Time.Calendar](http://hackage.haskell.org/packages/archive/time/latest/doc/html/Data-Time-Calendar.html#t:Day)
I meant the installer just doenst run that stage and jumps straight to confirming the install. I went to make a gallery of the install process, but it didnt skip the step this time. I still cant install as you can see in the sexy album I made the final install confirmation isnt available. http://imgur.com/a/ToZNG I have tried three times to get these in order, imgur just doesnt like me.
Well, it's always cheaper to do an integer division than to enumerate and test a few hundred items. The whole thing could be easily done with a filter but I wanted to explore the problem to see what else it revealed.
For the tl;dr crowd: No, Yesod 0.7 is not yet released. This is just the migration guide. Release should happen within the next 48 hours or so.
Thanks!
"Transformers: Monads in Disguise" Nearly fell off my chair laughing! I think we have another quote for the Haskell Weekly News.
Quote my wife Miriam Snoyman on that one, she came up with it a while ago. Actually, my son (~3 years old) has a Transformers (the robots) t-shirt, and he calls it his monads shirt. Yes, we're a very geeky family. I'm very proud.
I'm too old to know for sure, but can t-shirts really be considered "bling"? 
I should get myself a ~~monadic~~ warm fuzzy hoody.
You are my role model. I definitely want my children to be using the word "monad" at a very early age. (No, I'm not joking. Seriously, I think this is freaking awesome.)
Do you know any haskellers that would wear a big, heavy, gold-plated double-lambda-with-fwoosh thingey? I totally would for Halloween or something, but geeks aren't the "real" blingy type.
i second that
I've been trying to get "Stop...Hoogle time!" to catch on at StackOverflow for a little while. Either it'll become a community-wide meme or my own trademark. So far it looks like the latter (not entirely disappointing).
I'm encouraging calling the double-lambda-with-fwoosh haskell logo the lambind.
No doubt. Still, I think it might be a categorical error: No instance for (Bling TShirt) arising from a use of `checkOutMyGrillMofo' I love the Transformers design though. 
Just finished reading it, nice paper. I was a bit struck by one thing in your introduction... "this influence has reached audio programming as well, notably through Common Music and FAUST". It makes it sound like Common Music was influenced by F#. I'm sure you know that CM is quite old, [started in 1989](http://commonmusic.sourceforge.net/#history), so it made me do a bit of a double-take. Aside from that however, very well-written paper, if a bit terse in some parts, but I was able to follow it and I'd look forward to trying it out! (And maybe work on more back-ends!) I suggest sticking it on Hackage for easier installation.. 
+1 -- I'm going to have to remember that.
Thanks very much. There are a lot of topics I didn't have space to cover, particularly on the backend implementation; I plan to write more on that and some other issues soon. A bit more editing wouldn't hurt either, as you've noticed.
Sounds awful.
I suggest getting GHC to work on the system without installing it globally. Perhaps you could compile the libraries it requires manually? The other compilers are really either experimental or old. jhc is the only one that comes even close to usability, and even then ...
My crew throws lambda signs around, it makes us think we're gansta.
snoyberg is my hero just because he created his own awesome.
This seems subtly different but check out the problem discussed here: http://stackoverflow.com/questions/4550665/install-button-disabled-for-haskell-ghc-installer-on-my-mac-please-help-noth One crucial bit of XCode seems to be the 'unix development support' bit. Maybe that's at the root of this too?
I'd been calling it "lambinda" but yours works too!
Seems like a reasonable effort, I really dont want to have to down load xcode again though. Thanks
All right, NHC officially sucks. The 1.22 download time inexplicably gets LONGER the more I download it, until it utterly stops at about 1/3 finished. Probably some anti-spamming measure by haskell.org. The 1.20 download gets me an "undefined reference to `FN_Control_46Exception_46try'", which automatically loses my trust in that implementation (sad, since nhc promises to be able to emit C code that can be compiled without nhc itself; after all, it seems that's how nhc distributes itself). JHC requires GHC, while GHC requires GHC. Chicken, meet egg. I've tried getting Haskell platform to work but it doesn't. I'm digging through GHC unregistered builds.
OK, sometimes there is justification for that sort of security policy, but for deployment, not on the systems you'd do development on. I'd imagine developing the software on a somewhat freer system and just shipping binaries to a deployment network via the clunky data transfer. If your processes don't allow for that then it really is an awful environment to program in. You have my sympathy.
To quote a great novel, "We apologize for the inconvenience." Well, a funny book at least.
Is there any sort of existing Haskell shell library, with something like streams that can be processed without fully loading the file, etc? (BTW, I did Goolge around and there are packages claiming to be it, I'm curious if anybody would volunteer a package as "Yes, I've tried it and it works nicely", rather than just "There's this thing on Hackage that has the word 'shell' in it.")
For scripting tasks, lazy I/O actually works great for that. For long-running servers the picture is different, but lazy I/O is exactly the right thing for script-ish tasks.
Thanks for the suggestion. I just made them; check it out. http://haskellers.spreadshirt.com/warm-and-fuzzy-for-men-A7048016/customize/color/6 . There's also Women's and Kids' versions. 
If you can take a pic of that and send it to me, I'll make the shirt!
Excellent, thanks for that. Now I have to put my money where my mouth is :-D
Well, it *is* a hardware design shop, not a software one. Hence the fact that it's not at all geared towards software development.
Challenge accepted. Now I must herd da cats.
Awesome! Thank's Michael (et al)!
Nice
Just for the record, I've totally deprecated MonadInvertIO in favor of MonadPeelIO: it is a far superior approach. I'd be very interested in hearing Anders's take on monad-control.
Incidentally, there was a docstring bug in 0.1 that messed up the documentation. 0.1.0.1 tells you that the first parameter to the parse functions are source file names for better error messages.
Well, if you have experience with web programming, why not try a [Haskell web framework](http://docs.yesodweb.com/)? Or better yet, think of what you *want* to accomplish and then figure out how to do it in Haskell.
SnapFramework or Yesod provide Sinatra/Rails like path for web development. They are quite well documented and simple to use. I tried snap myself and was able to build a simple toy web app interfacing the MS SQL database through HDBC. And even interacting with java server (clojure) via HTTP package. This way you can use java libraries as a service (poor man's FFI) 
Ooh that looks interesting. I'm not a big web applications developer at heart but I do like to sometimes whip up a quick web front end for stuff. I'm pretty useless with coming up with ideas on the spot, most come and go in my head like water down a drainpipe so I can never remember them or drum up the motivation to think of them. I guess my question is, what would be a suitable little toy project to get started with using Haskell?
How did you start creating your applications? Did you just look through some code on github and got some inspiration from there? 
I see two general approaches: * Try to do in Haskell what you would normally do in Java/Ruby/etc. * Try to do things in Haskell that you would never dream of doing in Java/Ruby/etc. If you're interested in the latter, I would recommend learning parsing: it's a great way to start using monads without actually needing to know how they work, and I think it's one of those things considered a "dark art" in the imperative world but is commonplace in Haskell. I know that I was intimidated by parsing at first, but now it's second nature. If you really want a project to work on, contact me privately, I usually have a few small projects on my wishlist, and I'm not opposed to giving a helping hand to someone who's starting out.
&gt; I'm not really a maths person, in fact I'm pretty useless with maths beyond basic algebra and arithmetic. Perhaps Haskell isn't for me in this case? It's tangential to your question, but I would say the opposite is true. Haskell has been a great way for me to learns maths that I would otherwise find too abstract to get a handle on.
 import Network (listenOn, withSocketsDo, accept, PortID(..), Socket) import System (getArgs) import System.IO (hSetBuffering, hGetLine, hPutStrLn, BufferMode(..), Handle) import Control.Concurrent (forkIO) Ahh, a sane person avoiding unqualified open imports. This is how to quickly spot a software engineer :-)
I recently did this in PHP from a standing start (knowing no PHP and never used MySQL in anger): * write a web app which lets the user sign up with username/password/date of birth/email address * when the user logs in they are shown their daily horoscope (screen scrape from yahoo.com or something) * data should be cached so you're not repeatedly screen-scraping the same horoscope several times a day * let the user optionally receive horoscopes in their mail, but let them choose which ones --- maybe they want to know predictions for everyone in their family or something This was a screening assessment that a company gave me, and it gives you a good idea of authentication, sessions, security, user input parsing, parsing data (either naively or with a proper parser) and web templates. Obviously you can adapt to your wishes; personally I have no interest in astrology but maybe football scores or chart releases would work as well.
You might look at http://gitit.net/paste.lhs , a fully commented pasteboard web app using happstack-server and HDBC-sqlite3. Start with that and add features. Or try porting it over to Snap or Yesod or Wai/Warp. 
 import System (getArgs) `System` is an old Haskell98 module name, and adds a dependency on the `haskell98` compatibility package. It is probably better to use a Haskell2010 import of `System.Environment` for getArgs. In this: args &lt;- getArgs let port = fromIntegral (read $ head args :: Int) We can replace the call to head with an explicit match on the args, verifying the arg list doesn't contain redundant/unused args: [portStr] &lt;- getArgs let port = fromIntegral (read portStr :: Int) &gt; The next line converts the first element in the argument list to an Integer. This is not true, the line: let port = fromIntegral (read $ head args :: Int) converts the port number to a "PortNumber" type, which wraps an Int (but actually does the host-to-network-byte-order conversion). sockHandler sock = do ... sockHandler sock the explicit recursion can be replaced with a `forever` (from `Control.Monad`) loop: sockHandler sock = forever $ do ... Also, it might be a good idea to use a "finally" on the forkIO'd thread that closes the socket to make sure it doesn't leak sockets or wait for GC to clean them up. If you're really after robustness, even use `onException` to install an exception handler on the hSetBuffering/forkIO calls to close the socket too. That whole forever loop along with this exception handling and forkIO is useful as its own action for most simple servers. I would use pattern-matching here too: let cmd = words line case (head cmd) of ("echo") -&gt; echoCommand handle cmd ("add") -&gt; addCommand handle cmd replaced with: let (cmd:data) = words line case cmd of "echo" -&gt; echoCommand handle data "add" -&gt; addCommand handle data This avoids the need to "tail" within these two functions, too. Explicit loop in commandProcessor replaced with `forever` too.. addCommand :: Handle -&gt; [String] -&gt; IO () addCommand handle cmd = do hPutStrLn handle $ show $ (read $ cmd !! 1) + (read $ cmd !! 2) Again, I'd use pattern matching rather than `!!` here: addCommand :: Handle -&gt; [String] -&gt; IO () addCommand handle [x, y] = do hPutStrLn handle $ show $ read x + read y Also, `hPrint h = hPutStrLn h . show`, so you can use: hPrint handle $ read x + read y It's a bit unsafe to use "read" there (beyond the command line handling, perhaps), so maybe use reads (or wrap it with `readM :: Read a =&gt; String -&gt; Maybe a`).
Snap provides a small tutorial to get started. And it has a new project generator that creates all the files for you. All you need is just add new routes to Site.hs file. 
I had that problem too when I started but the best way to get something done is to just do it. Give it a shot and see where your fingers take you. Pick small projects and try and make them work. That's what I am doing.
Like zombocom, you can do anything with Haskell; the only limit is your imagination.
Maybe someone can set up a darcs&lt;-&gt;git gateway?
I think a better question would be: What can't you do with Haskell?
You seem to be begging for a link to [Real World Haskell](http://book.realworldhaskell.org/read/). Go forth and do real-world Haskelly things.
Yes.
I just started with Snap a few days ago and between that tutorial and the API docs it's been pretty easy to get started. Snap right now is more of a nice API to a web server than a framework but that's what I was looking for and it's been great.
Quick links: [Hackage page](http://hackage.haskell.org/package/pwstore-fast), with [Haddock docs](http://hackage.haskell.org/packages/archive/pwstore-fast/2.0/doc/html/Crypto-PasswordStore.html). Also, the source code is [on GitHub](https://github.com/PeterScott/pwstore/). One thing that really stuck out as painful here was the lack of fast, pure-Haskell secure hashing algorithms. The closest we have is the [SHA package](http://hackage.haskell.org/package/SHA-1.4.1.3), which is about 25 times slower than the equivalent C code, by my informal benchmarks.
Yay, I love not doing stuff myself. I especially love not doing security related tasks myself and pwstore seems useful.
&gt;With Haskell it's hard for me to get my head around this, a lot of the tutorials and reading material that I've used just seem to talk about creating mathematical abstractions. I'm not really a maths person, in fact I'm pretty useless with maths beyond basic algebra and arithmetic. Perhaps Haskell isn't for me in this case? *Au contraire*, it is perhaps THE thing for you. Haskell is the BEST way to learn mathematics. And logic. And Computer science. Of course, one should take a leap of faith, if necessary, to understand that a bit of theory and mathematics is mandatory. I hope I am not sounding like a stupid fanboy, but look at what someone like dons (or bos) [does with haskell](http://donsbot.files.wordpress.com/2009/01/semicolon.pdf). Or blogs of sigfpe. And my personal favourite is [this](http://logicaltypes.blogspot.com). That should prove to that you can wield Haskell like a handgun or something. If I may advice, forget the urge to *do* something (If you really can't resist the itch write a parser to the log4j log to do log-based-debugging of your java application). For now, do nothing. Just Enjoy the time. Imagine you have halted at the [Lothlorien](http://en.wikipedia.org/wiki/Lothl%C3%B3rien) on your quest. Be patient. You can expect boats and gifts from Lady Galadriel.
I have upvoted this post. I didn't think I would do that given the title. But the great attitude of the poster and the great answers of the responders make this into a post worth reading.
So simple yet so true.
I'll tell you a bit about a small app I wrote a couple weeks ago. Like many people, I listen to electronic music while I work. I usually just listen to streams on di.fm, which has many genre-specific streams you can listen to. Using an Arduino and some parts from Spark Fun, I made a hardware device that sits on my desk and displays the name of the station I'm listening to, and has buttons for choosing different stations. You know, like in your car. The Aruidno is programmed in C, of course. It connects to my PC via serial-over-USB, where it communicates to a Haskell program I wrote that actually controls the streams. The Haskell program has three main parts: * It needs to be able to communicate with the hardware device. This means listening to station commands, and sending information about the station and track that is currently playing. * It needs to be able to control the audio. I actually use Audacious (linux) to play the audio. My Haskell program uses audtool to control Audacious. * I've got another feature for recording streams (not legit on di.fm, but is allowed for other stations I listen to). This is achieved using Stream Ripper. For the stations where recording is allowed, Stream Ripper is just always running, recording every track. On my hardware device there is a "save" button, which I can press at any time during a song, and will cause my Haskell app to not-delete the recorded file. (All files which don't get the "save" treatment get deleted by the Haskell app). There's a few things going on in this program: * Lots of use of IO: serial communications, running audtool, controlling streamripper, managing saved files. * Some configuration information (where to save the files, X windows configuration info for audtool) I used the StateT transformer for the stateful things (like the file handle for the serial comms), and the ReaderT transformer for the configuration information. The different aspects of the program don't really interact much, though, so I partitioned the application up a bit: I created classes like MonadHardwareController, which is basically just MonadState (Maybe Handle), and had functions with signatures like executeSerialCommand :: (MonadRecorder m, MonadAudacious m, MonadSerial m) =&gt; Command -&gt; m () saveTrack :: MonadRecorder m =&gt; m () It's been said that Haskell is the best imperative programming language. I think there's a lot of wisdom in this: the combination of monads + type classes make it easy to write precisely-specified imperative programs, even though the language is "officially" functional.
*Parsing*. So true. I've written many parsers, but it wasn't until Parsec that I started to view the activity as "easily doable."
Err, why don't you use the `bcrypt` algorithm? "Iterating SHA-256 a few thousand times" sounds a lot like "roll your own" to me... 
You could look at Flask: http://www.eecs.harvard.edu/~mainland/projects/flask/ They use Haskell as a meta language and both NesC (C-like language) and a stripped down-Haskell as object languages. The end result is generated NesC code which can be executed on sensor-networks. We used their strategy as part of our bachelorproject, though targeting the Arduino instead of sensor motes.
By parsing do you mean parsing a language? Or parsing something like XML?
Football scores would certainly be very interesting, I suppose I should look into the Haskell libraries a bit more, I didn't realise there was proper HTML scrapers out there. 
Yes, to a certain degree it sounds like "roll your own", but I think it's not as bad as when it would've said "Iterating *my own SHA-256 implementation* a few thousand times". Although the iterating part sounds quite simple, you are right to think that a lot of things can go wrong in the implementation of this *simple* task. Maybe just wait for some reviews, assuming sketerpot has posted the code to a few mailinglists (Seclist, Haskell Cafe, etc.) If you're looking for an expensive *hash* function, you could use the [PBKDF2 binding](http://hackage.haskell.org/packages/archive/PBKDF2/0.3/doc/html/Crypto-PBKDF2.html). [PBKDF2](http://en.wikipedia.org/wiki/PBKDF2) is a key derivation function that allows you to specify a certain number of iterations (key strengthening) and a salt for the generation of a final output.
Ah, I have no question about his code being correct, the thing that bothers me is that iterating SHA-256 is an ad-hoc method. In contrast, there is a paper describing `bcrypt` for the purpose of password hashing and its properties are probably well-known.
A X window manager like Xmonad :) http://xmonad.org/
decide statements in Peano arithmetic, but that doesn't stop me from trying anyway :)
Thanks for the links. I'd be interested to know how Haskell will help me understand more complicated tranches of mathematics, but at the same time it's nice to think I might be unintentionally taking some material in (at a higher level)
Heh that sort of thing is *exactly* what I'm looking for, a nice simple walkthrough the code with an end result, thanks!
That's awesome, it seems like your stack is multi-faceted and uses Haskell to glue everything together. Nice. Your answer has certainly inspired me to keep going :D
I didn't realise you could read this book online, thanks, I'll have a flick through it later
Looks interesting. Checked http://www.freebsd.org/doc/handbook/one-time-passwords.html used in FreeBSD as well which is quite good.
I was thinking of XML-style parsing, though don't let my thoughts limit you.
The paper describes a two level *type system* -- which is to say that the types of the embedded language are directly represented in the host language. You can accomplish something like this with GADTs. However, I recall that combining this with everything else one might want is somewhat painful. I'm not sure how this relates to what you're proposing, which is more like simple macros, which template-haskell indeed does give you. The problem is that most TH macros can't be guaranteed to generate well-typed code (although you find that you when you typecheck the generated code, it would be nicer to find that out when you attempted to typecheck the macro code itself). Further support for this is planned, however. MetaOCaml, at the cost of some expressiveness, provides something like this now.
Why don't you just use the [PBKDF2 library](http://hackage.haskell.org/package/PBKDF2)? Seeing as you are basically implementing the same standard you could have polished PBKDF2 a little and just released a new version of that.
Shameless plug: Perhaps my article ["Fun with Morse Code"][1] can give an impression of how you can learn computer science (dichotomic search, reverse polish notation) with Haskell. [1]: http://apfelmus.nfshost.com/articles/fun-with-morse-code.html
this link seems to be related to topic: http://codahale.com/how-to-safely-store-a-password/
It's not an ad hoc method. It's a well-known method called PBKDF1, and I implemented it right out of RFC 2898. It does essentially the same thing as PBKDF2, but it's simpler and has a fixed-length output. For this, we don't need variable-length output, so PBKDF1 was easier to implement.
The actual PBKDF1 part of this library is literally three lines of code. And one of those is the type signature. Using the existing PBKDF2 library would have taken more time than it saved, and introduced a lot more cryptographic code. The more code in the security-critical areas, the more opportunity for bugs to creep in. Plus, the API and code for the PBKDF2 library aren't ByteString-based, which makes speed more difficult. I wanted to offer people the option of fast code (or the option of pure Haskell code). I mean no offense here to the author of PBKDF2, but it just wasn't the right tool for the job.
You need to define more precisely what you mean by "two-level language". There's an extensive body of work on context calculi, which can more or less be seen as a pair of lambda-calculi wrapped around each other with requisite notions of reduction for both levels. From here, it's only a short hop to allowing the lambda-calculus at "level 2" to metaprogram on the calculus at "level 1".
Snap init is broken. It generates code that gives an error on Timer type about incompatible types. 
It works for me. Are you sure all your packages are updated and you ran "cabal clean" before building?
This seems like a good place to also put interfaces to OS-specific password storage procedures.
Ah, I see, it's indeed a standard. Though I would still prefer `bcrypt`. :)
I've talked to Hartman (author of PBKDF2) about updating it to use ByteString and the crypto-api interface for hashes (basically, a rewrite that steals the package name and uses any hash + bytestrings) and he sounded positive as long as someone else did the work - the package is just floating right now. Obviously you're already "done" so changing paths is silly, this is just me reflecting at this point. BTW, could you add the string "pbkdf" to your synopsis? It makes it easier to search hackage if people mention the standards their package implements.
Glancing at the package I notice it is very BSD/Linux specific. Perhaps you'd be willing to add a crypto-api dependency and use `System.Crypto.Random.getEntropy 16` for a platform independent source of entropy? EDIT: I see you use `cryptohash` which already has `crypto-api` as a dep, so there really isn't any extra cost in making this switch.
Ah that must be it. I did not do cabal clean. 
What exactly do you mean by that? Would gnome-keyring be an example of an OS-specific back-end your thinking of that you'd want an abstraction over?
Yeah, most OSes have a fairly secure central location to store login credentials for things. Not relevant if you're storing a username/password database for a website, but if you're making end-user programs that run locally, it'd be nice to have an abstraction that lets me simply stick a password into the keyring/chain and retrieve it using the native API.
That seems arbitrary. bcrypt is specifically blowfish, if someone finds a large weakness in blowfish then bcrypt as we know it is not useful without a sizable change to the underlying C library and the Haskell bindings. (feel free to educate me on bcrypt if I misunderstood) The implemented pwstore-fast, otoh, is basically hash agnostic. If someone finds a weakness in SHA256 then it's possible (now or in the future) to switch the package to another hash or generalize it to allow use of any hash (via the Hash typeclass) by changing very few lines.
For password hashing, speed is a weakness, while hashing algorithms are designed to be fast. `Bcrypt` specifically alters Blowfish to make it much slower, that's why it stuck with me. But you are right, I now realize that it shouldn't matter which hash algorithm to use when it comes to the the property of being a one-way function. Hm, then again, it's not clear to me why multiple iterations of a hash algorithm should be harder to compute than just a few. There could be a shortcut for computing 100 iterations of SHA in just one iteration. The thing is that this does not seem to be a weakness of the hashing algorithm *in general*.
Thanks. I just [released](http://article.gmane.org/gmane.comp.lang.haskell.general/18480) version 0.2.
It's not entirely arbitrary. The key schedule steps in bcrypt require more RAM to operate than the SHA256 in PBKDF. This is no problem for a general purpose computer trying to check a users hash, but increases the amount of hardware needed for a high speed parallel hardware brute-force cracker. [scrypt](http://www.tarsnap.com/scrypt.html) takes this resistance to hardware brute force attacks even further. The scrypt paper has some cost/time to break comparisons for bcrypt and PBKDF2, as well as scrypt itself. If one was going to be paranoid and require something more brute-force resistant than PBKDF, then I'd suggest implementing scrypt over bcrypt.
That's a different problem from what this library is meant to solve. When you're storing your own passwords in a local keyring, then you've got an encrypted file containing all the passwords. This is meant more for web sites that need to store hashes of users' passwords; if they just encrypted a file with the passwords, then any attacker who got their hands on that file and the key would have every user's password. If someone else wants to write a library with interfaces to platform-specific encrypted password storage, that would be useful, but not for this application.
"In computer science, the Satisfiability Modulo Theories (SMT) problem is a decision problem for logical formulas with respect to combinations of background theories expressed in classical first-order logic with equality. Examples of theories typically used in computer science are the theory of real numbers, the theory of integers, and the theories of various data structures such as lists, arrays, bit vectors and so on." - Wikipedia
That's a good idea, thanks. It looks like S.C.R.getEntropy uses /dev/urandom or the Windows equivalent, depending on the system, which is exactly the Right Thing. I'll add that to my to-do list for the next minor version of the not-pure-Haskell version.
oh fair enough. I'd misunderstood :)
&gt; which is more like simple macros I suppose the simple example was confusing. I'm not showing a transformation from a "shortcut" module syntax to some existing syntax I already have. What I'm really pointing to is the `fun'c` bit. Here's a more complex library example where I put functions into stages of a hardware pipeline, mediated by channels (which are typically one-cycle delayed). Of course, I suppose I'm assuming that the paper I referenced has actually been read.... fun'c stage[f :: a -&gt; b] -&gt;'c [Chan[a]] -&gt;'c Chan[b]: let'c fun'c sub[inc :: Chan[a]] -&gt;'c Chan[b]: let'c channel outc : b always: outc ! f(?inc) in outc in sub fun'c pipeline[f :: [Chan[a]] -&gt;'c Chan[b], g :: [Chan[b]] -&gt;'c Chan[c]] -&gt;'c [Chan[a]] -&gt;'c Chan[c]: let'c fun'c sub[in :: Chan[a]] -&gt;'c Chan[c]: g[f[in]] in sub fun'c mkPipe[f :: [Chan[a]] -&gt;'c Chan[b], in :: Chan[a]] -&gt;'c Chan[b]: f[in] Basically, `fun'c` operates on entities in the target language `fun` domain. `stage` "lifts" a function to the compile-time domain, `pipeline` does something to the lifted functions, and then `mkPipe` returns it to the target language, where I can then do something like: channel in :: Int[0, 3] fun mulby5(x :: Int[0, 3]) -&gt; Int[0, 15]: x * 5 fun add2(x :: Int[0, 15]) -&gt; Int[2, 17]: x + 2 channel out = mkPipe[pipeline[stage[mulby5], stage[add2]]] ------ In any case, thanks for the other bits. Some minor readthrough of GADT's shows Phantom Types, which are Haskell98 (I [can't run GHC](http://www.reddit.com/r/haskell/comments/fe9wp/looking_for_a_haskelltoc_compiler_that_is_easy_to/)). I'll check that. In any case I'm going more for an interpreter/compiler approach rather than an embedded/nonstandard-interpretation approach; tips useful for this?
I actually don't know what I mean by that; the paper I linked to drops the term as if it were common, and also points to a 1994 paper by Neilson and Neilson, which I can't get to (non-academe, so no access to journals or other non-online collections of papers). I'll be looking at context calculi then. Having "level 2" metaprogram the calculus of "level 1" is something I think I want.
Flask looks interesting! Its Red sub-language has nearly the same limitations as the SAFL language (recursion limited to tail-recursion; I plan to extend this to have user-defined non-recursive data-types). Thanks! Although personally I'd rather not use an embedded language. Hmm. Let me think more about it.
Can this also be used to spot that butterfly effect possibly?
Not as-is, no. I think detecting such a situation would require using a __lot__ more memory. Right now, I only keep information on the most recent version of a package, and the butterfly effect would probably require old versions as well.
On a related note, the demo hackage-2 server already does reverse package deps and it is now doing nearly-live mirroring. It gives a lot of information, but not quite the same information as the haskellers.com packdeps feature. Compare: * ['Chart' package page on packdeps](http://packdeps.haskellers.com/reverse/Chart) * ['Chart' package page on hackage-2](http://sparky.haskell.org:8080/package/Chart-0.14/reverse) Note that the hackage-2 page tells you how many packages depend on an older version, but it does not list them. I hope that should be straightforward to add. It already has other details like: * packages where the latest version does not depend on Chart, but older versions of that package did * direct and indirect reverse dependencies with counts * reverse dependencies for specific versions of the package I think it's great to have multiple independent implementations like this. Good for experimenting with what is useful, and how to best present info. Plus it provides a useful cross-check, for example I notice that packdeps and hackage-2 disagree on the number of packages that depend on Cabal but not the latest version (26 vs 18).
One of the reasons I did not understand the feature request at first is specifically *because* this reverse packdeps is to similar so reverse hackage. I think ideally the functionality from packdeps- if deemed generally useful enough- should be included with hackage-2. I do not see an advantage to fracturing our tools. I've implemented it separately for now because: * I don't actually control what will end up in hackage-2. * As you imply, starting a service separately is a great way of expermineting. It would be foolish to throw in every feature someone dreams up into Hackage. Testing elsewhere first can let us know if it's a good idea. * I wanted these services available now, and hackage-2 isn't there yet. Most of the functionality from the packdeps site is included in the accompanying [packdeps library](http://hackage.haskell.org/package/packdeps). If anyone on the hackage-2 team wants to include these features, feel free to. And if packdeps ends up as being redundant, I have no issue with turning it into a simple forwarding page to hackage-2. And the disagreement between packdeps and hackage regarding Cabal is interesting: I know that packdeps is using Cabal itself to determine outdated dependency information. It seems like a stretch (I doubt eight packages relying on old versions of Cabal have been released recently), but when was the last time hackage-2 did a sync?
&gt; It seems like a stretch (I doubt eight packages relying on old versions of Cabal have been released recently), but when was the last time hackage-2 did a sync? Looks like about 7 hours ago.
The infinite is possible with Haskell: [1..] The only limit is your heap size.
There was no one there when I joined #smt?
Ok, so it should be ##smt (why?).
It's freenode channel naming policy. # for "official" channels for a specific named thing, and ## for unofficial channels or interest groups.
What's the difference?
Single-hash channels are official, that is, backed by the company or organization referred to by the channel name. Double-hash channels are unofficial.
Ah thanks to the both of you!
Simple and elegant API.
Aha -- gotcha. I was missing that you weren't looking for an edsl approach, but to define your own staged language. I'd really look into metaocaml then, and also the latest work on it, which I just saw -- the stripped down "BER MetaOCaml": http://okmij.org/ftp/ML/index.html#ber-metaocaml I think the right way to approach this is to start with your runtime language, and then to define a compile-time language as a conservative extension of it. Rather than thinking of "lifting" functions to compile-time, it might be cleaner to think of "lifting" as reifying functions into abstract but well-typed representations of functions, which can then be manipulated at compile-time. The final "output" of your program is a single main function representation which is then executed. The one concern I have about this approach (and what I understand of yours as well) is that compile-time functions are more powerful than runtime functions -- so your meta and object languages don't correspond. So I question whether you need lifting and lowering at all -- why not simply have two mutually incompatible languages, but a nice quoting mechanism that allows you to write object language literals in your meta language? 
&gt; although you find that you when you typecheck the generated code, it would be nicer to find that out when you attempted to typecheck the macro code itself That sounds trivially undecidable unless you neuter TH.
Precisely! http://hackage.haskell.org/trac/ghc/blog/Template%20Haskell%20Proposal
I managed to download 1.22 easily and get mostway through a build. Then I ran into linker errors having to with x86_64 vs. i386. I'm sure the flags to fix this are reasonable enough, but I haven't pursued it any further. You can post to -cafe asking for help with nhc, if you want to keep at making it work. The authors have since moved on to other things, but as I understand it, maintain an interest in keeping it compilable and usable.
This comment is of no particular relevance, but everytime I read the name monad-control, I get the following (*massive*) remix stuck in my head: http://www.youtube.com/watch?v=tjtnCfIjKY4
Trying to read through this just tied my brain in knots. It looks interesting, but I'm not sure of the utility.
Nice, that could be the *monad-control*'s theme song!
Nice one for going to all the trouble to make that out. I almost didn't upvote because you said my proposal was bad, but then I did because you were right. It might be an idea to have a push for more/better proposals though since the two fora in use are a bit stale.
&gt; Rather than thinking of "lifting" functions to compile-time, it might be cleaner to think of "lifting" as reifying functions into abstract but well-typed representations of functions, which can then be manipulated at compile-time. The final "output" of your program is a single main function representation which is then executed. Uhm, that *is* what I want to do. I guess my explanation of the library is kinda unclear. In the post you replied to, a "stage" is a hardware pipeline stage. The object-language function (e.g. `mulby5`, `add2`) are simply functions from one type to another. The `stage[]` meta-function converts them to meta-objects that can be manipulated by `pipeline[]` meta-function, until the `mkPipe[]` meta-function actually generates the scaffolding and returns final output channel. So in that case I used the term "lift" to mean that I create a meta-object from my object-language function. &gt; so your meta and object languages don't correspond. So I question whether you need lifting and lowering at all -- why not simply have two mutually incompatible languages, Well, yes. Meta-language uses `fun'c`, object-language uses `fun`. Uhm. So that is, uhm, two mutually incompatible languages. Actually right now I'm thinking `s/fun'c/gen`, just to avoid the funky `'` Of course I think maybe `let'c` might be better split into a `let'c` that does normal variable assignment, and a `declare'c` that allows declarations to be inserted into the top-level program of the object language, automatically splices bound meta-variables, and binds meta-variables to the generated names of the inserted declarations: let'c type = funRetType[f] in declare'c type Maybe = Just(x :: type) | Nothing() in {Maybe, Just, Nothing} ==&gt; let'c type = funRetType[f] in -- monadic, so use &lt;- to extract the return value. do'c {Maybe, [Just, Nothing]} &lt;- typeDeclaration["Maybe", [("Just", ["x", type]), ("Nothing",[])]] return {Maybe, Just, Nothing} 
&gt; I almost didn't upvote because you said my proposal was bad, but then I did because you were right. "Admitting error clears the score and proves you wiser than before." &gt; It might be an idea to have a push for more/better proposals though since the two fora in use are a bit stale. This page is part of my push effort. :) But seriously, I wonder how many new proposals there could be? There are only a few slots, and still a lot of good ideas left.
Having the summary of who did what is good, and it's interesting to see what's gone on. And the effort is appreciated. But: "The only one that bothers me is the EclipseFP project." Seriously? ~~While some of your measures of success or failure are a bit flimsy (reverse dependencies on hackage for instance, though at least that's quantifiable)~~[*], personally biasing what you're about to present just feels wrong. OTOH, I strongly disagree with your assessment of the EclipseFP project (having, working on and improving an eclipse plugin would be _wonderful_ for Haskell in the long run), which could be why it irked me so much. Reading on...perhaps I misunderstood, did you mean it was a bad idea for a GSOC project just because of its scale (as it turned out to be)?, or Eclipse + Haskell is really a bad idea in general? [*] I appreciate the discussion on ranking popularity at the end which mitigates this complaint. 
...is the video of the presentation available as well?
Well, EclipseFP is alive and kicking, and we even have some users! So maybe the GSOC project didn't achieve its goal, but at least it contributed to keep the project alive, and more: the current architecture of EclipseFP is still based on the GSOC work (using the scion library and the GHC API, JSON communications between Eclipse and the Haskell code, etc.). 
I really appreciate this roundup. But I think the bar is set somewhat too high for success. A success in this framework seems to be a significant and exciting improvement for the entire Haskell community. And there have certainly been a number of those. But there are also projects that are well done, produce results that live on, but which aren't immediately recognizable as awesome new things. So I think there needs to be a slightly more granular scale that can capture that. Among the projects that I think weren't slam dunks but were modest successes: GHC-plugins -- Not only was the work completed and does it stand a chance of being merged, but it explored the design space in a useful way for future GHC development, and was part of Max becoming more familiar with GHC internals. Since then he's contributed a few *very nice* and useful patches to GHC, including, as I recall, the magnificant TupleSections extension. GHC refactoring -- It seems unfair to classify work that was taken into the mainline as unsuccessful. The improvement weren't large, but my understanding is that they were things that we wanted to happen for GHC, and that were quite time consuming because they were cross-cutting. So this wasn't exciting work, but it was yeoman's work helpful in taking the GHC API forward. It's still messy, I'm given to understand, and it still breaks between releases, but it has an increasing number of clients lately, as witnessed by discussions on -cafe. Darcs performance -- by the account of Eric Kow &amp; other core darcs guys, the hashed-storage stuff led to large improvements (and not only in performance) (http://blog.darcs.net/2010/11/coming-in-darcs-28-read-only-support.html) -- the fact that there's plenty more to be done shouldn't be counted as a mark against it. Also we should take further community involvement into account. GSoc lists some of its goals explicitly as such: * Get more open source code created and released for the benefit of all; * Inspire young developers to begin participating in open source development; * Help open source projects identify and bring in new developers and committers; For example, I don't know of any direct uptake of the code from the HaskellNet project, but the author did go on to write a small textbook on Haskell in Japanese. As another example, Roman (of Hpysics) has, as I understand it, been involved in a Russian language functional programming magazine.
You and me both. RWH, as mentioned already, will start to open up some possibilities. However, the example applications are still on the small side and do not juggle a lot of state. It does show that it's not painful to knock off some small unix-style do-one-thing-only utilities once you get into the haskell mindset. Where I got stuck is trying to develop an application that's not small and has quite a bit of state. The tendency I was getting pulled into was to write StateT stacks, but this appeared to promote giant monolith types. I never got past this. Most blogs and books will construct simple utitlites or libraries or solve problems that are stateless but, as you mentioned, this doesn't cover a range of software projects that require a lot of state to be usable. I'm all for code reviewing existing projects that do this, so if anyone knows of some open source haskell projects that are designed to handle a lot of state and are not just one-task-solving utilities, please clue me in. I'd love to learn from it, because this was by far my largest hurdle with haskell.
So maybe a grid is the right thing: [ ] Student completed (i.e. got final payment) [ ] Project found use (i.e. as a lib has at least one consumer, or got merged into a broader codebase) [ ] Project had significant impact (i.e. wide use/noticable impact) [ ] Student continued to participate/make contributions to Haskell community
Cool stuff! Pedro did a great job on implementing this in UHC. Nice to see GHC catching up :-)
Thanks for the clarifications. That makes much more sense.
Another question is: why separate the whole language into two levels? The motivation is presumably to have a compile-time language and a run-time language, and all the compile-time stuff can be 'done' already before runtime (and thus have no overhead). Lots of languages do this. GHC with GADTs, Dependent ML, ATS, ... However, separating the languages is (arguably) inelegant, and causes duplication. In ATS or GHC, you have compile-time/type-level natural numbers, and then run-time/value-level natural numbers, possibly indexed by the compile-time variety. And `Vec` is indexed by the compile-time naturals, but you need to do tricks to get them related to the run-time naturals. Instead, you can go with dependent typing, and have modalities that track which things are static and which are dynamic. This can be arranged such that what 'level' a thing is unrelated it its type, but is rather tied to its use. So, we have: data Nat : Type where zero : Nat suc : Nat -&gt; Nat data Vec (A : Type) : Nat -&gt; Type where [] : Vec A zero (::) : A -&gt; Vec A n -&gt; Vec A (suc n) replicate : forall (A : Type) -&gt; pi (n : Nat) -&gt; A -&gt; Vec A n replicate zero x = [] replicate (suc n) x = x :: replicate n x map : forall (A B : Type) (n : Nat) -&gt; (A -&gt; B) -&gt; Vec A n -&gt; Vec B n map f [] = [] map f (x :: xs) = f x :: map f xs The first function, `replicate`, keeps the `Nat` around at runtime, because it needs to know what it is to produce the vector. `n` must still appear in the result type, though, so this is done with `pi`. The second, `map` takes `n` as purely a static phenomenon; it doesn't look at the value, but it's used in the types. Thus, it uses `forall` for quantification, and it can be erased at runtime. So we still have a static-dynamic divide, but it doesn't split the entire language into two pieces in quite the same way. This sort of thing is being integrated into Agda (and Andreas Abel's mini Agda), and it's been written about under the name of Erasure Pure Type Systems, dependent intersection (and union) types, modal type theory, and probably some others I'm forgetting. Tim Sheard, Nathan Mishra-Linger, and Andreas Abel are names to look for.
7.0 and 7.P?
Maybe he meant ℙ, the set of prime numbers? Or maybe the positive numbers, but why not just use ℕ then?
Maybe ℙ the set of polynomial functions? It'd still be well-typed and make about as much sense.
I ams not looks too closely. As much as syntax ams not supposed to be important, I ams not haves patiences for stuff that ugly. And a lot of the "can be as fasts as C" comes from using inline C, from what I ams seens.
The positives. You're right, I should've used N.
My esteemed fellow [gentlemen](http://i.imgur.com/mawFg.jpg) and [lady](http://i.imgur.com/FgMvS.jpg) [Haskelleers](http://i.imgur.com/alwC3.jpg), forgive my impertinence, but I dare to contemplate that I may have, most surprisingly and [inadvertently](http://i.imgur.com/8kITj.jpg), [ventured forth](http://i.imgur.com/e2lKk.jpg) into a most strange [parallel land](http://buttersafe.com/2008/10/23/the-detour/) whereupon my shamefully limited and parochial grasp of your methods of communication has completely fled my meagre noggin like a dandy at the first hint of inclement weather, leaving me most [discombobulated](http://i.imgur.com/P4DCZ.jpg).
Skwisgaar Skwigelf -- taller than a tree. Actually, the idea that [Skwisgaar](http://en.wikipedia.org/wiki/Dethklok#Skwisgaar_Skwigelf) is a Haskell programmer isn't too far fetched, given all the Gothenburg metal heads who learn FP at Chalmers...
Typical haskellers. They start with a concrete question and they end up arguing about some math issue. ℕ is the set of natural numbers, including zero. So no need to ∪{0}. 
Agreed. I would really like to know the answer to the question too if anybody has one.
I ams agrees that the syntax ams pretty ugly... that ams the sorts of things that ams easiest to change if a big community ams starts works on a small languages. I mean, ams basically a cosmetic changes. Coulds make lots looks like Haskell if really wants - would maybe be a silly mods for one persons to go through all sources and change every parts of the parser, but if a whole community wants to incorporates a language, just needs to decide on the right words. "Makes it looks like Haskells" would be pretty easy to agrees.
&gt; why separate the whole language into two levels? Because the object-level is a hardware description language with a statically-allocatable restriction: 1. Types are non-recursive and should be representable using only a "flat" (pointerless) structure with finite size (as this is exactly how it ends up being represented in hardware) 2. Recursive functions can only do recursion in tail position (this is actually implied by 1, as a CPS-conversion followed by explicit-closure-conversion will show that recursion in non-tail position results in a recursive type for the representing closure). This includes interrecursion - tail position only! The static-allocatability represents the fact that the compiler-slash-synthesizer has to allocate a specific, finite amount of hardware for the design that will execute the functional program. On the other hand the meta-level language is not so restricted, as it is effectively a code generator. Enforcing the limitations above on the object-level is easy if there is a separate object-level and meta-level, but I can't imagine how to make such restrictions for "dynamic" code while removing that restriction on "static" code - and more saliently, how to *explain* that to a designer. I'd love to be able to code as if there were no such divide in the code, though. However I fear it might not be easy for a designer (i.e. me) to imagine how much hardware resources that will end up taking. It's one thing to have "theoretically infinite" structures that you process using a von Neumann drinking straw on a almost-completely-sequential computer (8 hardware threads?? how unparallel!), it's kinda another to have limited silicon space to divide into highly-parallel (but severely space-constrained) mini-processors (72 bits of data line?? how memoryless!). For that matter your example uses the classic Nat representation, which arguably isn't a number (but is trivially convertible to/from integer - on a von Neumann machine anyway). Although it could just be a toy example, and you may be able to get numbers to/from Nat-domain with dedicated compiler.
You know everyone is going to talk about your notation and not Haskell now, right?
My favourite part about Haskell is this. The compiler is alive.
Okay. That's a good reason to have such a separation, as I agree you're probably not going to bake together one language that can do both jobs. My head was off in another space, thinking about more Haskell-alike languages (since people brought up GADTs and the like), where I think a divide is less justified, because you end up having two of the same language layered together with minor differences, and plenty of duplication. You may want to look at the [Singleton](http://www.cse.unsw.edu.au/~chak/papers/singleton.pdf) language. It's essentially an assembly language with a Coq-like layer above it, with facilities for proving that a particular assembly algorithm implements a particular high-level Coq-alike function. Seems more like the kind of thing you're interested in. Although, your situation sounds even more limited, and I'm not sure how much metaprogramming it allows.
While I'd **really** like an answer to my question about dynamic libraries, a discussion about notation is still important.
ATS is an innovative language but is it really a good replacement for a C backend? I guess it still allows all the "unsafe" operations so you can write bindings to OpenSSL and friends. The syntax could really be cleaned up. 
ATS might be a better backend for Agda -- Haskell would have to work around the ATS type system. It's honestly easier to write a backend to assembly language.
I think it is generally up to convention whether ℕ includes zero or not. To unambiguously denote which set you mean, put 1 or 0 as subscript to specify whether the lowest number is 1 or 0, respectively.
Maybe you should have also used a finite set, as only a finite number of versions of GHC will exist at any point in time :)
Depends on how you plan to go on observing GHC releases: http://www.renyi.hu/pub/algebraic-logic/uc08.pdf
Not even that if you discard items as you are done with them.
Madness.
It might be better for that, but I doubt it'd be good. It's rather like saying that now that GHC has GADTs and type families, it's a good backend for Agda (type-wise), because those can be used to simulate dependent types and type functions. Actually translating between the two systems isn't so pretty.
No. THIS, IS, OLEG.
A direct product of basing it on the OCaml syntax family and not the Haskell syntax family, I'm guessing?
Kick to `_|_` of deep hole.
This message renewed my resolve to never use HList. :)
http://haskell.spreadshirt.com/oleg-already-did-it-A6499531/customize/color/63
ATS is essentially an expansion of Dependent ML, by the same author. Dependent ML was restricted (as I recall) to integers, with a restricted set of functions thereon, in the type system. ATS extends that to, I think, arbitrary algebraic types (as well as some other fancy stuff; linear types and whatnot). (This is also why it's fast, I think. A big part of the goal of Dependent ML was getting some dependent nicety while being fast, so the author has been working on that since before it was even called ATS. By contrast, I think most folks authoring dependently typed languages are more interested in exploring what fancy types are possible, rather than performance tuning. And I can't really blame them.) So, yes.
MMM is this still so? Because I think the subscript was usual before but the current convention is the naturals include the zero. Is this really so?
My olegometer shows 1 oleg.
I thought that only VB and Pascal programmers started counting at one.
That's Great! Two things i love: GameDev and Haskell =)
Something that is needed is EGL
According to the ATS home page, ATS is currently implemented by compiling to C. I don't think that would work well for a "runtime backend". The previous version of GHC also compiled down to C: Haskell --&gt; restricted Haskell --&gt; C. Replacing the restricted Haskell with ATS *might* yield some modest advantage, but you 'd definitely do a hell of a lot of work before you'd know. 
I can definitely point you to whole mathematics communities that believe very strongly that 0 is not a natural number. That convention certainly is much more common in CS; but still, you're better off saying so in the introduction if it matters.
Regarding the `fib'` definition, where he skips the explanation of why that works. If I understand, it's because Haskell's language definition states that implementations must memoize named functions? (Is this related to the pattern matching mechanism?)
This works because `fib` is in [Constant Applicative Form](http://www.haskell.org/haskellwiki/Constant_applicative_form). The same internal list is computed once for the `fib` value and it is reused each time that list is indexed by applying `fib` to an argument. Haskell is not, however, required to memoize named functions.
All current versions of Fedora are shipping shared libraries. Fedora 15 will even have binlib executables linked dynamically. But you are probably asking about support for dynamic libraries on different platforms and archs? Not sure if there has been much development there since 6.12.
Hackage page: http://hackage.haskell.org/package/barley
Okay, thanks. So the memoization that takes place here is only an optimization "strongly suggested" to the compiler due to the form, but Haskell does not actually require it?
That looks nice. Way to go Johan and Mark.
Haskell-The-Specification does not require *any* operational semantics. Any given implementation of Haskell (such as GHC) may make any particular set of promises regarding operational characteristics that it desires.
Right, that makes sense... but since operational semantics (like memoization) have an effect on algorithm time complexity, doesn't that make it hard to guarantee these properties of your code across compilers? Do we just write code with the optimistic assumption of a smart compiler? (I'm not suggesting there's anything wrong with that, just trying to clarify it. After all, we do this to a certain degree even with C.)
In practice, we (or at least I) write code that clearly expresses the intended meaning. Then, if the target compiler (generally the version of GHC corresponding to the latest release of the Haskell Platform) has bad behavior, the code is changed (and, depending on the issue, bug reports are filed :-)). You're right that we need to care about both Haskell-The-Implementation and Haskell-The-Specification, but having a notion of a operational-semantics-independent specification allows us to vary Haskell-The-Implementation in order to make it smarter and better while at least not breaking the *meaning* of existing programs. It would be great to have equally formal specification of the operational semantics of Haskell programs, and better yet, a choice from among such specifications. But yes, one writes for Haskell in general (or Haskell + extensions x y z) and optimizes for one Haskell compiler or another in particular.
dear self.reddit forum, today I finally understood what monads are all about.
It was a whole lot of fun. The Dojo was INSANELY packed yesterday though - there was a machine learning workshop (100 people) + the Learn Haskell workshop made it another 70 people. There were lots of interesting people and some good talks by Don and Johan that I really enjoyed. Here's to hoping I can make it to PhiHac III as well.
There was also a group of 20ish upstairs as well.
FunGEn 0.3, a simple 2D game engine for haskell, now released and seeking developers: [http://thread.gmane.org/gmane.comp.lang.haskell.cafe/86330](http://thread.gmane.org/gmane.comp.lang.haskell.cafe/86330)
someone throw up a public demo! :)
Can I comment on the design? I think the "points" need more margin around them. Suggestion: #info .points { margin: 6px 12x 0 0; } And I'm not to keen on the purple text, but that's just being picky. :-)
I was actually going to upvote for the purple text. So my upvote goes to purple. (Sorry for the bikeshed) Other than that though it is good. Easy instruction links for per-platform installs, information on what is installed and info on when the platform was built. Basically everything I look for. The only thing that I think might be missing (for the security conscious) are hashes to compare the downloads against.
Thanks all for coming and making BayHac a great success! 
It's missing a lack of flowers.
Agreed
Thanks for organizing !
Every release has a seasonal theme. Since I was in Mountain View for the hackathon, and the cherry trees were blossoming, this seemed to be the appropriate seasonal image.
The changed since last release section of the [changelog](http://code.galois.com/darcs/haskell-platform/download-website/changelog.html) lists the ghc version as 7.0.2 instead of 7.0.1.
The main page claims current version is: 2011.1.0.0. But I can't find the installer/torrent for it anywhere. What am I missing?
This isn't the live site, this is the preview, before the installers go up. You can find release candidates on the mailing list, [here](http://projects.haskell.org/pipermail/haskell-platform/2011-February/001400.html)
I find it beautiful. An enjoyable way to start programming. I'm not sure if a change in the font color would improve the look. Need to see it. But the current color is fine, I think.
A web handler threw an exception. Details: getCurrentDirectory: does not exist (No such file or directory)
I like this idea. :-) We should come up with codenames for the various releases (Cherry Blossom or maybe even Sakura) and then actively advertise them as such. It would explain the background.
That's because the release will use 7.0.2.
In general I think it is hard to see how updated the different platforms and variants are with respect to the latest stable release - and if they are lagging what kind of timeline one could expect before general avaiability
Thanks a lot for organizing. It's a lot of work (which I learned organizing ZuriHac).
Everyone seems say this is madness and unusable. Is it bad that to me, it looks quite ok and usable? :-S
It still can compile via C.
It used to work this morning. Google cache: http://webcache.googleusercontent.com/search?q=cache:BLBfv5F8b40J:darcsden.com/simon/fungen+http://darcsden.com/simon/fungen&amp;cd=1 And darcs get http://darcsden.com/simon/fungen still works.
I am writing a book introducing people to Agda programming in the "Learn you a" style. I have an illustrator working on illustrations for it as well. While it's still very early in development, my [github page](https://github.com/liamoc/learn-you-an-agda) will have updates soon.
That idea is seriously awesome. I wish you the very best of luck.
The way I see it, there are two *main* reasons you visit the HP page: * To download HP * To read more about HP The former is well displayed in the middle, but the latter has two redundant links buried in the paragraphs at the bottom. Maybe the links could be merged, split out underneath the paragraphs, and given more emphasis as the other 'significant' option on the page? And I concur that purple might not be the best color...
My point is, that compiling to C is the *only* way any ATS code gets executed (currently). The title of the post asks "Future runtime backends, maybe?", so I'm raining on the following long parade: 1) generate an ATS file from Haskell source; 2) generate a C file from ATS; 3) generate executable from C; 4) finally, load and execute. I just don't see that as a good plan for *runtime*. 
Ok, then the libraries section of the changelog lists ghc version 7.0.1 instead of 7.0.2. Unless that ghc is different from the ghc in the changed since last release section. But they link to the same place so it seems like they should take on the same value.
A similar talk: https://docs.google.com/present/view?id=dwxjd43_22gncm5gg5&amp;revision=_latest&amp;start=0&amp;theme=blank&amp;cwj=true This: template &lt;typename T&gt; // Must be template to also support unicode bool AreReversedWordsSame(const vector&lt;T&gt;&amp; words) { vector&lt;T&gt; revWords; for (vector&lt;T&gt;::const_iterator it = words.begin(); it != words.end(); ++it) revWords.push_back(reverse(*it)); sort(revWords); vector&lt;T&gt; sortedWords = words; sort(sortedWords); return revWords == sortedWords; } Becomes: areReversedWordsSame words = sort revWords == sort words where revWords = map reverse words 
Here's a [MediaFire link](http://www.mediafire.com/?cz8yvgzjy29s108) so you can avoid the retarded Scribd link. Dons, why the hell would you use Scribd when a simple PDF download would suffice? Such a waste of resources.
Or even areReversedWordsSame = on (==) sort &lt;*&gt; map reverse
For the second, there's no point to *mixing* dynamic and static code if you don't have the usual safety guarantees within the static part (int parameters will only get int values, etc.) For the third, the only reason to use a dynamically typed language at all is to avoid writing manual annotations, so an awkward annotation-heavy approach is no substitute for a partially typed language. I think the idea is more to take it as given that dynamically typed languages are useful, and figure out how you can gradually add type information with minimal effort at each step.
How mature is JHC? I was under the impression that it wasn't capable of compiling complex code yet. e: I can't even get jhc to build `bytestring`, so this tutorial isn't going to work for any real-world use cases. Fun to play around with, though.
Chill out, bro. This is what was linked to on twitter. 
zounds.
I don't know how old that talk is, but I think we might be at the point of removing the last to points on the last slide ("Cons"): * Libraries not as mature * Esoteric/mostly-academic crowd This weeks hackathon had a pretty non-academic crowd!
Seems like a very cool hackathon, wish I could be there. The linked talk was given about 2 months ago. I'd say that relative to Python, the Haskell crowd does tend to be pretty academic, or at least that's the impression I get from the interwebs. Regarding libraries/maturity etc. Earlier at the day of giving the talk, I installed Haskell (GHC + misc) on the mac laptop I was going to use for giving the talk. I discovered that the recommended installer for Haskell, the "Haskell platform", installs a version [that gets broken after doing a few cabal installs (ghc-pkg check finds errors)](http://cdsmith.wordpress.com/2011/01/17/the-butterfly-effect-in-cabal/), and I had to uninstall it and install a cutting edge version of GHC to avoid this hackage breakage. Perhaps Python doesn't have a fancy system like hackage but it's not that hard to get a stable *old* version installed and working.. I like Haskell a lot but it does have its share of "cons". 
Which installer were you using? Do you get such results with the alpha preview of [the New Mac Installer](http://www.galois.com/~dons/code/)?
I think it was http://lambda.haskell.org/hp-tmp/2010.2.0.0/haskell-platform-2010.2.0.0.i386.dmg And if I remember correctly, same happens with older releases. I'll give the preview installer a try tomorrow and will update you on the results. My premature guess is that it will probably work fine, as new versions of GHC/platform tend to work and old ones tend to break as a result of hackage changing.
You are right there. It is really rare that I cannot find a package on hackage that does what I want it to do.
dunno maybe you'll get an answer posting on the OP's blog.
I've been thinking about Haskell's expressiveness. The major long-term cost of software is simply getting it into the next programmer's head. Which version do you think is going to be easiest to understand? It seems to me that once you hit a certain level of concision anything shorter is more likely to be a WTF one-liner than a clear description of a algorithm. 
Great job by Mark hosting Bayhac at Hacker Dojo. It was gratifying to see so much interest in Haskell and having access to some of the most active Haskellers in the community. I would be interested in ideas about what sort of events we could host at Hacker Dojo on a more frequent basis that would help build momentum (even if that goes against the Haskell motto :-) 
time for a new meme: don't flame me bro
 areReversedWordsSame words = sort (map reverse words) == sort words a little bit redundant, but much easier to understand while still one line
Much improved version. Would all 3 versions compile to the same code?
Then the answer will be on some random blog that nobody knows or cares about, instead of on Reddit where future searchers might find it.
Haha I read "cons" and thought "wait? the slides covered lisp too!?"
Can people come up with a list of reasons **against** Haskell? That would be more interesting, IMHO, because you then you could target your efforts towards those weaknesses.
Perhaps a temporary darcsden outage. Good to know, please do report these to alexsuraci on #haskell and perhaps at [http://darcsden.com/alex/darcsden/issues](http://darcsden.com/alex/darcsden/issues)
It would be nice if I could see the entire presentation. Following the slides alone is pretty hard. 
Where is the function "on" from? Hayoo is down.
For those who are curious about the code change, Matt optimized the request processing code significantly. You can [view the commit on Github](https://github.com/snoyberg/warp/commit/aca7bfba18332bb70cc7b3e82af2c790a4339629).
Try Hoogle: http://www.haskell.org/hoogle/?hoogle=on It is exported by Data.Function: on :: (b -&gt; b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; a -&gt; c Example: Data.List.sortBy (compare `on` fst)
Thanks!
Who cares? Semantically, they are equivalent.
&gt; "Add NVIDIA CUDA backend for Data Parallel Haskell": DPH is rarely used; Because it's not mature yet, yes. Once it's fully developed it should see substantial pickup in scientific computing and other areas. I know that a bunch of computer vision researchers at NICTA have decided to take it up. &gt; a CUDA backend would be even more rarely utilized; CUDA has a reputation for being difficult to coax performance out of; and difficulties would likely be exacerbated by the usual Haskell issues with space usage &amp; laziness. (DPH/CUDA use unboxed strict data, but there are interface issues with the rest of the boxed lazy Haskell universe.) This is basically disingenuous. You state how DPH uses unboxed strict data, and then say that it would have "interface issues" to transform between boxed lazy data and unboxed strict data as a way to justify your complaint that it would have problems with it. For starters, once completed, transforming between the two should be as simple as calling a function, I hardly think that's a problem - CUDA would work just fine with unboxed strict data, and you can transform between them easily. That said, CUDA can be finnicky, but it's substantially easier to write accelerate code than to write CUDA code. Accelerate might be a bit less performant, but that is more than made up for in productivity gains. &gt; All in all, there are many better SoCs. I think this is an unnecessary SoC seeing as a DPH-like backend has already been produced for the Accelerate library. It should be easy to adapt to DPH once everything's a bit more mature. But, when trying to come up with some impartial analysis, please don't let your own personal opinions about the DPH project colour your judgement.
Here's my list of gripes: 1. Learning curve is more substantial than other languages. 2. Monadic syntax can be ugly, and large monadic imperative programs are less readable than a traditional imperative language. 3. uhh.. Laziness can sometimes introduce unexpected performance problems or space leaks. That's all I can think of for now, but unfortunately most of these problems can't be addressed in Haskell because they're fundamental design decisions (laziness).
This is the version I would actually use. The one above was mostly to provide a more extreme counter-point. ;)
Actually, in this case they would after inlining.
As of recent (say the past 6 months) jhc has become a lot more stable than it use to be it still has a bit to go but I think it's almost there. I've managed to get a few of my [lazyfoohaskell](https://github.com/snkkid/LazyFooHaskell) ports working which uses Haskell SDL bindings to C. The only programs that don't currently work for me are ones involving complex stack of monad transformers with IO. Jhc can build the transformers library but using them with IO causes compilation errors. I've sent an email to the mailing list about it so hopefully John will figure that out soon. If you don't mind not using monad transformers for now you could use explicit state plumbing or IORef/IOArray (seems like IOUArray is missing but I don't think it would be hard to get it). Apparently jhc does support higher-rank polymorphism so ST monad should be quite possible. How did you try to build bytestring? I don't think cabal works with the current version of jhc, jhc can make packages using a simplified version of cabal format (a normal cabal file wont work) and you need to give arguments to jhc to build a library, including options for FFI. Now I probably wouldn't recommend to someone to use it for serious projects just yet unless you can afford to send bug reports and wait for bug fixes or workarounds (which John is quite quick to respond to).
It was a while since I looked at Haskell, but I found it annoying that you had to define both a Monad and a MonadT to get composability. Seems like this could be done for me. Laziness is both a strength and a weakness. It can give strange bugs. Maybe Disciple will be better? Static typing can be constraining if you are used to (for example) Python and Clojure. Sometimes my brain says "I would just like to map some objects to some other objects" and as long as they are hashable, Python will let me do this. Haskell will usually not let you get away with hacks. Related to dynamism, there is the perennial question of how you make a general "memoize" in Haskell. This turns out to be almost impossible (although experienced Haskellers will surely claim that it is easy, but I have never seen an implementation that I liked) and has been the subject of many discussions, rants and at least one PhD-thesis (from this, now famous, forum post: http://groups.google.com/group/clojure/msg/c2a9e8d27a1fff3f). In a dynamic language that would be trivial. Sometimes the most natural way to express something is to modify objects. Haskell will let you do something similar through various monads, but it is not really the same. The canonical Haskell example, the oneliner quicksort, is actually quite bad. A good quicksort implementation modifies the array in-place. On the plus side, Haskell code tends to be succinct and sometimes readable. Programs that pass compilation have fewer bugs than in any other language I have ever used. These were just from the top of my head.
There's plenty that can be done about these issues. For example, good heap use debuggers with heap walking analysis. Static analysis that gives you an idea about the likely memory use of various compositions as well as warnings about common mistakes. More readable core output could help too. For learning curve, a type based IDE could help a lot. Monadic syntax could of course be arbitrarily improved...
Your quick sort example doesn't illustrate it is natural, but that it is efficient on contemporary machines. Modifying objects in place is not more natural than computing a new modified object. 
&gt; The only programs that don't currently work for me are ones involving complex stack of monad transformers with IO. Jhc can build the transformers library but using them with IO causes compilation errors. Unfortunately, this describes about 90% of my libraries/applications. &gt; How did you try to build bytestring? I don't think cabal works with the current version of jhc, jhc can make packages using a simplified version of cabal format (a normal cabal file wont work) and you need to give arguments to jhc to build a library, including options for FFI. I had tried to use cabal-1.10 + cabal-install-0.9.6 , which seemed to work with JHC. Compiling resulted in an unhelpful internal error.
drat, beat me to posting by 18 seconds. That's what I get for taking time thinking of the right title :-)
Very cool. I think I'd have kept the original one-liner in the source as documentation.
The original one-liner wasn't really a one-liner, it was built on a different set of custom functions. If you look later in the commit history, you'll see that I removed a lot of code.
This loooks very interesting... has any independent 3rd party been able to confirm the numbers yet?
~~The Snap team saw similar results on our previous benchmark~~ See comment below. We have considered creating a reproducible benchmark using an amazon ec2 instance, but at the moment we would rather spend our time elsewhere. Truth be told, the pong benchmark should not be that important- people do setup monitoring tools that will run a simple request like that a few times a minute. There are usually bigger factors in real real world web application performance- but it is much more difficult to setup those benchmarks.
I would say that for humans, modifying objects is natural, and so it is sometimes more natural when programming. That doesn't mean it's better though. I think it's swings and roundabouts, and haskell shows that not having variable variables is less of a handicap than one might think.
[In Haskell we think for quite a long time before starting typing.](http://www.reddit.com/r/haskell/comments/ert1h/in_haskell_we_think_for_quite_a_long_time_before/)
And now I wish I were more than just an amateur Haskeller. I'd love to actually be paid to work with it.
[Downvoted](http://www.reddit.com/r/programming/comments/es0jw/in_haskell_we_think_for_quite_a_long_time_before/c1ah2ku)
I'm the author of the blog post. I couldn't actually make JHC compile my real Haskell code, because of this: http://www.haskell.org/pipermail/jhc/2011-February/000889.html
That rate is web-monkey low, though.
The jobs where I live are pretty terrible, so that would be decent pay here. Our standard web-related jobs tend to pay around 30k USD (~71 GBP/day) and I've only seen one software developer position starting above 50k (~119 GBP/day). I suppose I probably ought to move...
You don't really need to move. Do remote contract work for companies in "nearby" cities.
Actually we've never been able to reproduce the numbers you're seeing: the biggest gap we've been able to show is ~90k for Warp and ~40k for Snap. Still better than 2:1 on PONG but a long ways off the multiple you guys are getting. 
That cabal install issue happens to me but I always thought it was due to the butterfly effect (to use cdsmith's term) rather than the HP itself.
It is very much due to the butterfly effect. But, if you don't have a new GHC, the effect becomes unavoidable as the newest versions of some packages can not be installed.
Thanks for clarifying, and please send any numbers our way if you do any new benchmarking. Perhaps we do need an ec2 instance.
One reason against: can cause a frustration. Haskell is very interesting to learn &amp; use. The community around is also beautiful. Haskell allows to average application programmer rediscover mathematics and purity again. Unfortunately not everybody has a chance to have day job with programming language he likes. Without knowing Haskell he can pretend the satisfaction and avoid frustration from in-mature environment. 
Not sure what happened. I was told the mailing list would have an announcement in Jan. concerning te Feb event. I volunteered my time, and was told by Mark he would get back to me about it. I was curious yesterday (has this event happened yet?). I looked in my mail for mention of this event, only to find nothing. I was very sad to have missed it, and hope it wasn't because I've been missing mail.
This. I missed it this time around and would just want there to be more events.
Ah, gotcha.
"before starting typing" is perfectly correct and understandable. It is a bit clumsy though and the two alternatives you give are nicer IMO, so your instincts are good :-)
Excellent!
Was a fascinating talk. Johan's making some good steps here that look promising.
One thing I do not understand is why we are getting people to check over their own repos when they have perfectly good clones of the repos on their own machines. Could we not just rm -fr every repo that the author has a copy of on their local machine and just get them to re-upload them? That way they would be in the same state as they were before with no chance of tampering (from the hacker).
Have you tested the most recent version of Warp? That's the one that includes the newest performance enhancements that are being discussed here. I think we came to the determination that this *might* be due to GHC's bad-performance-past-4-cores issue. Matt's machine is exactly four cores, and so handles things fine. Aren't you running on some 8 core beast (you lucky.... :-) ) Anyway, if we all had all the time in the world, the EC2 benchmark instance for every interested web framework would be nice. BTW, speaking of benchmarks, have you figured out what was happening with bigtable?
"Patricia tree" derives from "PATRICIA trie", a regrettable acronym (hiding in Titlecase embarrassment) for what is now better known as a radix tree. Knowing this makes the talk easier to understand.
The problem is that some people put unrecorded files into the working copy of the repo on the server, e.g. release tarballs, generated html etc. It's almost certainly not a good idea. So it's those repos that we could not automatically verify and that it would be unsafe to `rm -rf`.
If only there were more Haskellers in RTP. This looks insanely awesome. 
Note that it's basically full time for at least a year, not short term contracting. But yes, we could probably make more money if we gave up and went to work for a bank. That'd be less fun though :-)
Sometimes you want a mutable hash table, but Data.HashTable doesn't perform well at all. I'm working on a replacement, here are some quick benchmarks: https://gist.github.com/826935 Currently it's about 10x faster than Data.Map for inserts and better than 5x faster for lookups.
Just curious, what were the final numbers after the new primops?
I thought this was nothing new until I realized the brilliance of the sentence "Patricia trees implement a sparse, persistent array of *size 2^32 *".
Can you write some more about it? Where to find that kind of job? I was looking through www.vworker.com lately and rates are terribly low, even comparing to rates in my country. Do those companies are looking only for experienced specialist? I don't have big experience (actually i started working as a professional developer about 8 months ago).
could you please confirm that the RSA host key actually changed (the new fingerprint reported by ssh starts with 21:b8:59:ff:...) ? (I guess there was a complete OS reinstall, which generates a new key).
You mentioned having taking HashMap about as far as intended with future work focused on HAMT. Any chance of seeing unordered-containers on hackage? Would you accept patches adding `alter`, `alterM`, `minView*` etc? I have a use (drop-in replacing Data.Map) that this might cover well.
I still don't quite get that sentence.
What's wrong with just putting in a comment?
In terms of most **typical** statically typed functional languages nothing can compare to Haskell, especially with Haskell extensions and Template Haskell. Haskell is quite different from your typical functional language (both statically or dynamically typed) in that: * The language can enforce referential transparency and it is the default. * It has non-strict semantics by default, it is a lazy functional language. Since you have never learned functional programming yet it is probably not the best idea now to pick based solely on expressiveness. The reason why it is a good idea to learn functional programming with Haskell is Haskell is a purely functional language (referential transparency) all the other typical functional languages are impure and/or hybrid languages so you will not truly understand what functional programming really is by picking one which is impure/hybrid first.
This is true. In a class I took recently the professor covered Racket (which is based on scheme i think) as a functional language, but she mostly used it similarly to any other language. There was a few recursion examples, but she did not make it clear why this was different from recursion in c or c++.
I've not read much into it, but I think it was at the size being 2^32, ~4 billion.
\+1. As a beginner, being forced to do things in an unusual way will make you actually learn how it works, instead of just programming the same way you're always used to.
...why would somebody put unrecorded files on the host machine...isn't that what your local machine is for? Though I guess I don't know if that might be required or not but since some do not I would guess no. Edit: But thanks for all the work that you have done so far verifying everything and getting as much as possible back online.
The idea is that a patricia tree is, in a way, a huge array of 2^32 entries. Moreover, it's *sparse*, only the entries that have a value actually take up space. These are marvelous properties if you are used to arrays being contiguous blocks of memory.
Well my reasons for learning Haskell are based on its power. It is the only language that I have found where you can parrallelise a function so easily. And any code I write in it is always about three times smaller and more 'correct' than the other code. Not only that but pure Haskell is cross-platform by default; something that I really appreciate having to code on Linux, Mac and Windows PC's on a regular basis.
we'll, I'm still learning haskell, so take what I say with a grain of salt (about 3/4 trough lyah and did some project euler challenges with it), but I tried scheme/common lisp before. With lisp it was like "well, i can see the appeal but this is somewhat cumbersome and must of the stuff I can do in Lisp are doable in an imperative/multi-paradigm language as well" Whereas Haskell went more along the lines of "hum ok, this is weird... wow, this is actually quite useful and I have no idea how i'd accomplish this in an imperative language... ohmygosh this rocks, I'll have to annoy all my friends with it until that run away when I even mention the word 'Haskell'". There a lot of nice features that aren't quite as beautiful when implemented in other functional languages, also, haskell somewhat requires you to learn non-trivial concepts, whereas in most other functional languages you could live quite happily without (I think I get monads by now, though I'm not yet at the point where I've created my own). So sure, go for it, but don't be frustrated if you don't pick it up in 24 hours. I think haskell is one of the few functional languages that you don't just use for fun for some weeks and then forget about, but that you'll actually use for serious stuff, just because you prefer it for a certain problem domain.
Racket is a scheme dialect - they just thought that Racket sounds much nicer than DrScheme. Tail-call in Scheme (i.e., statements of the form "return f(something)") get optimized into a simple goto, which makes it possible to use recursion more efficiently than it would be in C. And, of course, the point where Scheme gets "interesting" (scare quotes, with a big helping of scariness), is call/cc, which is to normal function calls what a Tardis is to a normal phone booth. More to the point, Scheme would be the other option besides Haskell for a functional language one might learn (including most of the stuff from Common Lisp, with a cleaner language design).
It's different. By "other FPL's" I assume you mean "LISP, Scheme, O'CaML, and F#". Haskell is superficially similar to O'CaML/F# since they all share the Hindley-Milner lineage of 'strong static typing with type inference'. Perhaps I've been brainwashed by the Haskell community, but I hardly think of things in terms of 'functional language' (i.e. functions are first class values), and now think of it more in terms of 'pure language', by which standard LISP and Scheme fall far short. Purity is the thing that matters, and while to do anything useful with purity you really need first class functions, just having first class functions doesn't automatically get you the benefits of purity.
I strongly agree that you can use Haskell for real world tasks. I find that Haskell excels at real world tasks.
Just pick one of Standard ML, O'Caml or Haskell. To a beginner, they're all much of a muchness. People will try to sell you on the advanced features of Haskell forgetting that O'Caml has many advanced features that Haskell does not (for instance, a usable module system, polymorphic variants, etc.), the fact that a beginner doesn't care whether your language has GADTs or not, on Haskell's purity (yawn), and on the language's laziness.
[Learn you a Haskell...](http://learnyouahaskell.com/) [Real World Haskell](http://book.realworldhaskell.org/read/)
It's Turing complete.
From a didactic perspective I agree with you 100%. From a practical perspective, these days the Haskell community is orders of magnitude larger than the others.
Personally, I find it easy to use Scheme as a purely functional language despite its multi-paradigm nature.
I know some c compilers (like gcc) perform tail call optimization. But its true that recursion is not the norm in c, and tail call optimization isn't mandated by the standard so it shouldn't be relied on in portable c programs
So are Brainfuck and Unlambda. Turing-completeness is not a sufficient reason to use a language.
CPS is less of a win than I expected. Interesting.
I agree. I started learning FP with Common Lisp, then decided to switch to Haskell because of its purity, to force me to use FP idioms. But the bad side is that now I don't want to go back ;-)
I don't have them handy. I think the numbers for insert are about 80% of the ones I gave in the slides so about 3% faster than Data.Map.
I will definitely put it on Hackage when it's fleshed out enough to be useful. I'm trying to put some thought into the API design so that we don't end up with Data.Map. For example, Data.Map's alter is almost useless as it's lazy in the higher order argument (i.e. you can't use it to update an Int value efficiently). I'm curious, how would you implement minView efficiently when the map isn't ordered?
We've been getting widely different results from CPS. See for example Bryan's bug against GHC 7: http://hackage.haskell.org/trac/ghc/ticket/4965 
&gt; I will definitely put it on Hackage when it's fleshed out enough to be useful. I'm looking forward to it! &gt; I'm trying to put some thought into the API design so that we don't end up with Data.Map. For example, Data.Map's alter is almost useless as it's lazy in the higher order argument (i.e. you can't use it to update an Int value efficiently). The importance of performance and evaluation strategy hardly seems enough reason to consider the 'alter' function to be almost useless. I use it a lot and like the interface. It is true that the Data.Map module has a (very) large collections of functions and might be cleaned up a bit. But, when you look at the API closely, many of these functions are actually very useful. For example, I just recently discovered the power of functions like 'differenceWith', 'fromListWith' and 'splitLookup'. &gt; I'm curious, how would you implement minView efficiently when the map isn't ordered? Is it the hashing that destroys the ordering? Is there anything you could do about that? I really like the fact that binary trees allow you to very efficiently extract a (both descending or ascending) ordered slice, given a range on the keys. 
Right. 21:b8:59:ff:39:69:58:7a:51:ef:c1:d8:c6:24:6e:f7
Several projects are doing things like hosting release tarballs there, when they should probably be using the project webspace instead.
There's also the matter of it being a workable rate anywhere but the US, on account of the health care system. It's too much for Medicaid but not enough to afford decent individual coverage.
Unfortunately networking is by far the most effective way to find these jobs. You pretty much have to make them come to you. Also, hourly contract work can scale up much better than salaried employment where you are expected to work 40ish hours a week.
&gt; The importance of performance and evaluation strategy hardly seems enough reason to consider the 'alter' function to be almost useless. I use it a lot and like the interface. Well, it causes you to use O(number of calls to alter) space instead of O(number of key/value pairs) space which kinda sucks. If you do something simple, like key a map of counters (e.g. Ints), and use alter your performance will likely be terrible. &gt; It is true that the Data.Map module has a (very) large collections of functions and might be cleaned up a bit. But, when you look at the API closely, many of these functions are actually very useful. For example, I just recently discovered the power of functions like 'differenceWith', 'fromListWith' and 'splitLookup'. It actually needs even more functions as most higher-order functions are lazy and need a strict counterpart. &gt; Is it the hashing that destroys the ordering? Is there anything you could do about that? Yes, the hashing destroys the order. That's a fundamental problem and is a big part of the reason why most languages have both an ordered and unordered map type. The hash map type is usually faster but the ordered map type support more operations.
CPS meaning continuation-passing style? 
I actually find it somewhat easy to use C++ as a purely functional language now. But that wasn't until I learned Haskell :) I am sure my coworkers hate me.
And Agda isn't Turing-complete. ;)
This is more a cultural thing than a language thing, but idiomatic programming in Common Lisp isn't actually particularly functional a lot of the time. It's certainly a powerful language for metaprogramming; but I'm not sure I'd call it functional programming. The Scheme community tends to adopt and encourage a much more functional style.
Yes.
You can write a corecursive program to generate the trace of a Turing machine. You just can't persuade Agda to unfold that corecursion arbitrarily while typechecking. "Agda isn't Turing-complete" thus falls in the same category of statement as "Haskell can't do IO"...
I was wondering where the HWN had gone as http://www.haskell.org/haskellwiki/HWN only points to out of date sites. Should this be updated?
&gt;Sometimes the most natural way to express something is to modify objects. Haskell will let you do something similar through various monads, but it is not really the same. The canonical Haskell example, the oneliner quicksort, is actually quite bad. A good quicksort implementation modifies the array in-place. In-place quicksort: iqsort xs | len &lt; 2 = return () | otherwise = do p &lt;- xs `read` (len `div` 2) m &lt;- unstablePartition (&lt; p) xs iqsort $ slice 0 m xs iqsort $ slice (m+1) (len-m-1) xs where len = length xs
Yeah, Standard ML is impure, but look at any large codebase written in it (for instance, Isabelle) and the code is almost 100% pure. Impure features are only really used for generating fresh names, and interacting with the user, and here SML programmers usually make an effort to push these to the "top level", so the the core of the program is completely pure. Sometimes I really wish there was a modern, impure, strict functional language with a sane syntax (i.e. not O'Caml's).
Unless you want to rank them in terms of expressiveness and power.
How about OCaml itself with the revised syntax? Its just a command line flag away. Of course, most OCaml code is written with the original syntax.
Aye, it's pretty tough being self-employed in the US and having to pay for your health insurance on an individual basis. Ironic that is easier to be an entrepreneur in a country with "socialist" healthcare. Anyway, the point with the rate is not to insult you, it's not far from what we pay ourselves. It's just unfortunate that what is a decent living wage in most of the world is not enough for contractors living in the US.
Well, I was thinking more like there being a single common language, similar to Haskell, which served as a testbed for development/messing with the type system. I don't think O'Caml fills that niche. It essentially exists to write a proof assistant in (c.f. the standard library).
It depends. Do you consider parens to be expressive? ;) haskell has a lot fewer than a lisp-based language
Where do we resend ssh keys to? Mine seems incorrect and community.haskell.org only says "it works". NB: If an ssh key exists for my user name then it's not a malicious incident, the backup probably missed my very recent change of ssh key.
Where are you getting `read` from? That's not Prelude.read. And unstablePartition?
http://hackage.haskell.org/package/vector Data.Vector.Generic.Mutable
The functional constructs in C++ feel so ugly and inadequate after writing some Haskell.
http://en.wikipedia.org/wiki/Turing_tarpit The notion that any turing complete language is as expressive and powerful as any other is both trivial and silly.
One language to consider besides Haskell would be Clojure. It's clean and practical Lisp-dialect to work with JVM. Works nicely as functional language. 
I'm so excited about this. I can't wait.
I am not a very experienced Haskell programmer. Could you perhaps explain what is so appealing about this feature?
So say you have a matrix library that is very fast and can do very fast matrix multiplication. But if you pass it matrices whose sizes don't match up then, because it is so fast and doesn't waste time checking things, it will do terrible things all over your memory. So, naturally, you generally call this matrix library wrapped in a function that checks that you won't mismatch sizes and segfault. This check A) has a runtime cost and B) only fails at runtime. Now, suppose you can teach the compiler enough math so that it can statically check that your sizes match. Now, you have a compile time guarantee that your multiplication will be sound, and you pay no runtime cost for a check. That's one use-case for this extension.
It allows more precise specification within types, automatically checked. For instance, `map` preserves length, `(++)` sums lengths, and `take` subtracts lengths. map :: (a -&gt; b) -&gt; Vec n a -&gt; Vec n b (++) :: Vec n a -&gt; Vec m a -&gt; Vec (n+m) a take :: Nat n -&gt; Vec (n+m) a -&gt; Vec m a By capturing these constraints in static types, the compiler can find more errors. 
I don't know that I'm terribly experienced, either, but one use I can see is to allow for types polymorphic, and checkable, by shape. For instance, suppose you want a matrix type. Unless I'm misunderstanding, this would allow you to define it so that each instance has a statically defined width and height. Thus, you can push the problem of validating matrix sizes off onto the type checker. Something like: matMult :: Matrix a n c -&gt; Matrix n b c -&gt; Matrix a b c matMult m1 m2 = ... And thus the type checker will verify that the width of the second matrix equals the height of the first. Being able to do this at compile time would certainly make code safer, and probably faster.
And http://codepad.org/ is based on geordi. See its [about](http://codepad.org/about) page. Edit: I just saw dons submitted the link 2 years ago to r/programming...
Woooo! Go Iavor and thanks for the awesome feature!
From a language design pov, it's slightly irksome that it's only for one specific kind and based on a solver where you can't be sure if it'll find a solution, though I appreciate that to do this more generally basically means going all the way to dependent types. No doubt it could be useful of course, statically sized vectors, physical dimension types etc.
Aha! I see now the utility. The type system can represent not just natural numbers, but operations on them. What would be the type of `filter` in this instance, though? You know that it is a function from [a] -&gt; [a] but I can't see how a type system could in general enforce size restrictions meaningfully on anything comes from a filter function. Do you just bail out of the size checks if you include these kinds of functions? 
Even with type level natural numbers, I can't imagine making a type system that could check most code which a person might write at the matrix level. I write tons of Matlab code at work, and I can think of all sorts of array operations which are useful and which do not have type inferable effects on matrix size. This isn't necessarily an argument against having the compiler help, of course, but it seems to suggest you still need those run time checks for a subset of operations which may "leak" into almost any part of your program.
I guess all we can know is that the input is at least as long as the output, i.e., filter :: (a -&gt; Bool) -&gt; Vec (m + n) a -&gt; Vec n a I wonder whether the new support can confirm this type. Whether this example or a trickier one, I expect there will be cases in which the type-checker just isn't smart enough to find a proof. In those cases, I guess we say "oh well" and either settle for what we get or move on to full dependent typing, where we can control proof generation. 
I bet you're right about running into limits of inferability with matrix operations. Would you come up with a few examples, ideally ones that are easily expressed. Such examples would help us understand where the limits are, and may also help to make this new facility more useful.
I have these two concerns as well. Hopefully we'll get some helpful experience with the implementation and use of this special case (number kinds), then a couple more special case kinds, and then see our way to some elegant and useful generalizations to replace the special cases.
Wow, a natural language version of sed.
Let's not ignore that a Nat of any kind makes our types a little more precise even if we can't express fancy properties like in conal's example or get neat static guarantees. It's better documentation. In Haskell code, you see Int all the time in places where Nat is meant. What's `take -1`? or `fib -1`? Does `length` ever return something smaller than 0, as its type `[a] -&gt; Int` implies it might? Is there a single use of Int in Data.List where negative values are actually meaningful? And so on.
I wonder you could push ranges into the type system. I actually cannot think of many operations where the _bounds_ of the sizes are not know at compile time. However, merely knowing the bounds doesn't let the compiler write smarter code. However, tracing JITs like those in the Java VM and Matlab seem to do a great job of solving the problem of bounds checking for most non-trivial jobs, so maybe we should only consider the benefits of having the compiler be smarter?
I'm not sure I see how to do physical dimensions with just type-level nats? Or rather, the way I can imagine it (hlist of exponents, with typed-bools indicating sign) doesn't seem that much better than what's already possible.
From the linked ticket, it appears that there's a `&lt;=` constraint added. So you could get, I guess: filter :: (m &lt;= n) =&gt; (a -&gt; Bool) -&gt; Vec n -&gt; Vec m
Well, there are generalizations of the notion of filtering. In matlab, there is a frequent idiom where we use an array of indexes to slice and dice another array or matrix. These indices are obviously computed at run time, and so it seems at first glance that they would not be checkable until then. However, I'd imagine in haskell the type of array indices would be different than the type of the array being indexed. Could one imagine having a type which was dependent on the matrix being indexed and was a subset of the integers based on the compile-time-known matrix size? I am pretty much an amateur at type level thinking, but it doesn't seem impossible to me to imagine the compiler being able to check something like this. But I guess computations on integers can only be checked at run time, ultimately and so it wouldn't be possible in all cases to ensure that the indexes in the index type array fell into a sub-range of integers. Now that I am put in a spot about it, its hard to imagine any completely indispensable operations (aside from filtering) apart from loading and saving data which have dimensionality which are not known at compile time, or couldn't be annotated at compile time without much trouble. I presume loading and saving data would take place in some kind of IO monad, but does that provide a way of also integrating the size information? If not, that seems to be a problem too, since data is rarely entered into the source code for big data problems. The data size is, however, often known at compile time. Does haskell have facilities for importing data information at compile time?
Sure you'll need runtime checks/casts as a last resort, or for, e.g., creating matricies of size only determined at runtime. Although you'd be surprised at the evil tricks that are possible. But, like conal, I am curious about specific examples that you think are not checkable.
Not for some definition of expressiveness (the precise one).
Interesting that CPS is so much slower on 64-bit platforms. I've been dubious of CPS's supposed performance gains because I could never reproduce them, but perhaps it's just because I don't have any old systems lying around to test on.
Then we're agreed.
I would like to see something like that as well. I think a pure, strict language that incorporates a lot of the lessons learned from Haskell would be great.
I think its an interesting direction: imagine a pluggable type system where additional theory solvers can be specified on the command line -- System F + Presburger + Linear Arithmetic + ... etc. SMT solvers took SAT in this direction, is System F (MT) around the corner?
It just keeps getting better!
Would something like this compile? main = do vec1 &lt;- readVecFromFile "file1" :: IO (Vec n Double) vec2 &lt;- readVecFromFile "file2" :: IO (Vec n Double) print $ vec1 `vecSum` vec2 where vecSum :: Num a =&gt; Vec n a -&gt; Vec n a -&gt; Vec n a Cons x xs `vecSum` Cons y ys = Cons (x+y) (xs `vecSum` ys) Nil `vecSum` Nil = Nil Would it complain about incomplete patterns for *vecSum*?
You can use existentials/GADTs (or continuation passing style) to get pretty far with this stuff. E.g. a function `withParsedVec :: Natural n =&gt; String -&gt; (forall n. Natural n =&gt; Vec n -&gt; a) -&gt; Maybe a`. Then you call `withParsedVec someString $ \vec -&gt; Vec.sum vec`. You have an intermediate n that isn't statically known, but that's ok because you've "promised" the compiler that it will be *some* natural. This only goes so far before things get hairy though :-)
The issue is that a lot of these theories can't be combined and still be solvable. The other is that even Presburger solving is doubly exponential, and some of the more complicated ones like real closed fields are way more than that. This particular field considers plain old exponential solutions a breakthrough in performance :/ But for most types, you only have a couple of quantified variables so it could be genuinely useful, and I am very excited. As I mentioned on the ticket, I'd still really like to see the option to drop into some deeper proving mode for things the solver can't figure out (and not unsafeCoerce). I also mentioned to Iavor that it'd be nice to integrate his Presburger solver directly into this thing, so that we are at least confident the solver is complete for a fairly common case. For Peano, I'm quite satisfied with it giving up at some point, as doing otherwise is basically impossible.
The way you'd deal with external input is with some sort of "decision GADT", usually. It'd be a Maybe-like type whose "true" case would refine the type index to something that matches your expectations, or whose false case does not (and thus you would not be able to apply your functions to it). You'd also have an existential (or a rank-2 CPS-ish function, which is equivalent) to hide the typed-ness from the outside world, that can't know it statically. This is true even in a dependently typed language to an extent, but it's easier to project out the witness for the existential there. But no, it shouldn't complain about incomplete patterns for `vecSum` (although [SPJ disagrees](http://hackage.haskell.org/trac/ghc/ticket/462)). But you also wouldn't be able to write `IO (Vec n Double)` like that, as the `n` isn't bound there. That's where you'd use the Maybe-like types I mentioned earlier.
&gt; The compiler The compiler**s**. Apparently this feature originated in UHC before making its way over to GHC. I know most people only *use* GHC, but I think it's important to at least *pay attention* to the other implementations.
that syntax makes me a little cross-eyed, but I get it.
`minView` was a poorly thought out example, my real interest is actually alterM - it seems silly when using `Data.Map` that I have a hand-rolled `alterM` that is slower than it needs to be (I'm using lookup/monadic op/update).
I've yet to think of a property that couldn't be verified at the type level, personally. &gt; loading and saving data which have dimensionality which are not known at compile time, or couldn't be annotated at compile time without much trouble. The general way of doing this is to make the result type for these functions existential instead of universal. For example, instead of saying that a load function should be able to return a matrix of *any* dimension where context dictates what that dimension is, you say that it is able to return a matrix of *some* dimension, where the function gets to determine the dimension rather than context. You can then only use functions on the resulting data that can work on matrices of *any* dimension. Among these functions, however, could be functions that dynamically check the dimensions and then `Maybe` give you a matrix with some static guarantees that you didn't previously know about the input.
&gt;You are encouraged to get involved in Happstack. The easiest way to get involved is to complain about how horrible Happstack is xD
It looked fine to me. I'd apply if I were qualified.
Do you have a small example for such a GADT? I can't see what exactly you mean.
For a lot of the places where I want to use type level naturals Presburger arithmetic just isn't enough. You quickly get to multiplication and powers (of 2). So I think one just have to give up on having a complete constraint solver if one wants something practical. That said, of course I'd like a Presburger solver for those cases where it is enough. It would be nice to have a disciplined way to give proof hints. Perhaps by providing instance declarations for the problematic cases?
That m would have to be existentially quantified.
The type variable n is problematic here, because in the first read you want it to be instantiated (I imagine).
Fascinating. I wish I had a good excuse to work with/on Haskell, but the career, so far, has not furnished one.
Great!
That's what I mean though. It's fairly easy to check if a formula is (superficially) solvable by a Presburger solver, and call that when it is. Otherwise you can fall back on heuristics and rule matching for dealing with multiplication by variables. You won't necessarily catch everything that is _eventually_ solvable by Presburger through some sequence of simplifications of a Peano formula, but everything that's obviously Presburger will definitely get solved. It's also worth nothing, I think, that the solver might be simplified by the fact that we don't usually need natural-valued existentials. I've never come across anything talking about the universal-only fragment of Peano/Presburger, but for real closed fields at least it actually simplifies the problem somewhat. As far as a disciplined way to give proof hints, it seems like a series of explicit conversions between quantities we know to be equivalent but can't necessarily always prove would be sufficient. Most statements we care about are probably some composition of the algebraic laws we know the naturals to satisfy, so if we just had timesComm :: Nat (n * m) -&gt; Nat (m * n) timesComm = unsafeCoerce -- keep the "unsafety" in the library timesDistrib :: Nat (a * (b + c)) -&gt; Nat (a * b + a *c) timesDistrib = unsafeCoerce we could compose simple statements like these and arrive at the conclusion we desired just like we'd do in e.g. Agda. It's not exactly elegant but it feels less ugly than saying "yo, trust me" in user code.
Say you were looking for a vector of a specific length, say 9: data IsVecOfSizeNine n a where Yeah :: Vec 9 a -&gt; IsVecOfSizeNine 9 a Meh :: IsVecOfSizeNine n a Now you'd have: readVectorOfSize9 :: String -&gt; (forall n. IsVecOfSizeNine n a -&gt; r) -&gt; r where the higher-ranked continuation is a way of making `exists n. Vec n a`. A consumer of an existential must be able to consume any value of the existentially quantified variable, so we just say that, and save ourselves from needing dependent types by not actually refining our output type by an unrefined input type like String (if we'd had a GADT whose types were refined in the same way as input, we wouldn't need that). Then you'd have something similar to `liftA2` but on the higher-ranked continuation above, to talk about two separate vectors to read, and inside there you'd be able to place your code that actually assumes type nats to be the same. You could also do this without GADTs (as long as you take the type equality from TypeFamilies) but they make pattern matching a lot more useful and definitions a bit more natural sometimes.
&gt; However, merely knowing the bounds doesn't let the compiler write smarter code. It can do, for example if the compiler can guarantee that an array is never going to be accessed outside of it's bounds, then it doesn't have to do a check every time.
I wasn't disagreeing with you. :) I could imagine giving proof hints by something like instance a * (b + c) ~ a * b + b * c where -- not sure what goes here Now, it would be nice to have these instances be local to some definition.
- haskell is maximum power - haskell is web scale so it's good to learn haskell if you want to lean a functional programming language. 
I mean bounds on bounds.
Okay, but there's no way to define the size of a vector on runtime? Add one dimension and you have to recompile your whole program?
 filter :: (a -&gt; Bool) -&gt; Vec n a -&gt; exists m. (m &lt;= n) *&gt; Vec m a or filter :: (a -&gt; Bool) -&gt; Vec n a -&gt; (forall m. (m &lt;= n) =&gt; Vec m a -&gt; r) -&gt; r
No, there are plenty of clever ways to do it, but it's not always trivial. For your case, you'd probably want something like data Eq a b where Refl :: Eq a a and data Dec d where Yes :: d -&gt; Dec d No :: Dec d -- not really no, but not yes and you might have a pair of types representing your naturals: data Z = Z newtype S n = S n class Nat n where ... then you might have an eqNat :: (Nat n, Nat m) =&gt; n -&gt; m -&gt; Dec (Eq n m) that would let you decide at runtime whether the two naturals are equal, _and_ also provide a type-level witness that they are the same. 
Right. In what I gave above, the caller of filter would get to "choose" the m, which makes no sense.
If you are just starting out, the difference between Haskell the ML family of languages is not so great. Once you have seen the basics, Haskell may have type classes and more interesting examples of laziness, but it's also worth learning something in the ML family to see a real module system, and O'Caml in particular to see polymorphic variants and the dual class system.
&gt; "yo, trust me" Admitted.
Hmmm ok, I might have to play around a bit with it once its available. Thanks for your effort!
If you're used to C/C++ executables, and roll your eyes at being trapped in an interpreted Scheme session or IDE, then consider Bigloo Scheme http://www-sop.inria.fr/mimosa/fp/Bigloo/ If you want a nice first project, write a processor to remove most parentheses, by using | to open a paren that auto-closes and $ to indicate an empty outline level. Hint: Disregard all published solutions, by people who don't actually code in their solution. Don't fear a frenzied backlash by Lisp purists claiming parentheses build character, no one's home anymore. Haskell unquestionably has the smartest user community of any language that can be put to practical use. To go more fringe, one ends up in dependently typed languages like Coq, of primary use as a proof assistant. Haskell has a steep learning curve; one can only start with it by discovering the most minimal core language that writes actual code. Every language has a significant number of dogs chasing their own tails; look at C++ template meta-programming for a mainstream example. There's plenty of tail-chasing in Haskell; you can only learn Haskell by aggressively ignoring this contingent, until you actual crave the mental stimulation that's egging them on.
Doesn't [Disciple](http://www.haskell.org/haskellwiki/DDC) cover your needs ?
&gt; Haskell's purity (yawn) Haskell's purity is one area where Haskell has a real edge over impure functional languages, like the ML family. The problem with the way MLs implement impurity is that it's almost completely uncontrolled: the use of impurity is not reflected in a program's types. This undermines both the functional nature of those languages, and the power of their type systems to express program properties. This issue makes Haskell one of the only languages in which it is possible to fully explore the benefits of a functional approach to programming. The MLs are unfortunately encumbered with a limiting approach to (im)purity that they inherited from the non-functional languages. 
&gt; even though the code **hash** been well tested Trying to decide if this was on purpose.. ;)
&gt; I'm working on an even faster version, based on hash-array mapped tries From my meager experience, it looks like [tries](http://en.wikipedia.org/wiki/Trie) are going to be the new buzzword pretty soon. Not a moment too soon, I'm getting tired of everyone buzzing about [TCO](http://en.wikipedia.org/wiki/Tail-call_optimization).
This is more a matter of library code than language. You could trivially write the following in C++: template&lt;typename XS&gt; bool are_reversed_words_same(XS const &amp; xs){ return make_set(map(reverse, xs)) == make_set(xs); }
I am also interested in the natural language part. And it tries to educate the programmer do not copy :) Nice piece of work! How much time was to develop it? 
Many talk about purity, but I find the biggest difference re scheme and ML/haskell etc is the typesystem. Alebraic types, pattern matching and type inference along with recursion fit very well with many programming tasks. So far as purity is concerned, Haskell just makes the use of io and state explicit.
Except, for the Haskell and Python version I used functions that are in the library (I even did a favor to C++ and pretended that `reverse` exists). Also, it's a little bit more tricky than that. Let's say you want to split it a bit and make `revWords` a variable and not an expression. Now your template needs to be on the item in the vector because you need to mention the type of the set. Not to mention, what's with the crying face at the end of that line? :) `return make_set(map(reverse, xs)) == make_set(xs`**);** It's crying because it's C++ code :)
Total Cost of Operation
I added a Traversable instance and sent in a pull request. If you add that I can percolate it through my class hierarchy and provide Apply, Bind, Insert, Adjust, etc. instances.
Not exactly. As you can imagine, I've typed the word "hash" quite a few times lately. :)
Disciple is great, but it's not got the same approach to effects as Haskell. Maybe its approach will turn out to be better, but we don't know yet, so there is still room for a "strict Haskell", IMHO. BTW, sorry for hijacking this thread - I just noticed that dpm_edinburgh actually said he wanted an impure language.
Tell us why your language "Haskell" sucks: http://www.drmaciver.com/2008/02/tell-us-why-your-language-sucks/ 
Nice spiel. Contentless, and largely predictable. But nice. &gt; This issue makes Haskell one of the only languages in which it is possible to fully explore the benefits of a functional approach to programming. The MLs are unfortunately encumbered with a limiting approach to (im)purity that they inherited from the non-functional languages. No it doesn't. There isn't a damned thing that you can do in Haskell that you can't do in an ML dialect that's strictly related to Haskell's purity. It's perfectly possible to program purely in ML, and virtually any large codebase written in ML is mostly pure, with all the "impurity" pushed to the "top", just as you'd find in any well organised Haskell codebase. Most ML programmers were programming "purely" when Haskell was nothing more than an idea. As I said, please check out the likes of Isabelle's sources before you comment. Even the supposed optimisation benefits of purity in Haskell are largely a chimera. Are the major Haskell implementations reliably producing executables that are as fast as Stalin or MLTon, compilers for two *impure* functional languages? No they aren't, and any cursory look at Haskell benchmarks shows a lot of gaming going on, producing code that makes your eyes bleed and is completely unidiomatic. What does that tell us? It tells us that the likes of fusion optimisations, those that purity really helps with, are largely irrelevant in the face of whole program optimisations, which can just as easily be done in an impure language as they can in a pure one. Look, Haskell *needs* to be pure because it's lazy. But don't try to turn that around and claim that the ML's impurity is some sort of grave deficiency. Because it clearly isn't.
-.- I linked it for a reason
[And there is an Android App!](https://market.android.com/details?id=coolcherrytrees.software.detexify) EDIT: [And there is an iPhone app!](http://itunes.apple.com/ca/app/detexify/id328805329?mt=8)
Or as I like to call it, ML /troll
Strictness isn't the interesting thing about DDC. Even GHC has a strict core (Int# etc). The interesting thing is the effect type system.
&gt; Except, for the Haskell and Python version I used functions that are in the library ... With that logic, you could grab Java's standard library and try to argue that Java &gt; Haskell. &gt; Now your template needs to be on the item in the vector because you need to mention the type of the set. Most likely, you'll have XS::value_type, or you could BOOST_AUTO.
&gt; With that logic, you could grab Java's standard library and try to argue that Java &gt; Haskell. It could be an argument for Java. But, with my limited experience with Java I didn't come to feel this way anyhow. afaik Java doesn't have an equivalent of `map` in the standard library.. Still, maybe Java's stdlib is good. I don't know. But I still won't pick Java for other reasons.
If you choose to define expressiveness to deal with what programs *can* be expressed, rather than what programs can be conveniently expressed, then you've just made the term practically useless for comparing languages since almost all of them are turing complete. You'd still need a term for what most of us use "expressiveness" to denote, which do you prefer?
&gt; Nice spiel. Contentless, and largely predictable. But nice. This sort of meta-comment is not a very good indicator for the ability to have a rational discussion. But based on the rest of your response, you seem to have missed the content of my comment, so I'll expand on it. &gt; There isn't a damned thing that you can do in Haskell that you can't do in an ML dialect that's strictly related to Haskell's purity. That's a Turing completeness argument, which always misses the point about differences between languages, particularly when it comes to properties such as expressiveness and checkability, that go beyond simply what the program does when it runs. &gt; It's perfectly possible to program purely in ML Certainly, but that's beside the point. The challenge arises in how a language manages both purity and impurity. Haskell requires that a function's purity or impurity be explicitly expressed via its type. This has the usual benefits that types provide: it allows certain important program properties to be expressed explicitly, and also allows those properties to be statically checked. Since you're defending ML, you presumably have some sort of respect for types and typechecking, otherwise you'd be defending something like Scheme instead. Being able to use types to help manage effects is at least as valuable as the things that ML uses types for, if not more so, given the consequences that effects can have. In fact, ML does use the ```ref``` type to help manage variable mutation. Haskell shows that this approach can usefully be taken much further. Effects are a major cause of bugs in programs, but they're also a necessary evil in a large class of non-trivial programs. Arguing that types shouldn't be used to help with this, or that they're not necessary in an otherwise typed language, seems like a strange position to take. The optimization issue that you raised is unrelated to this point. &gt; Look, Haskell needs to be pure because it's lazy. A more general version of that statement would be more relevant to this discussion: any language needs to clearly distinguish pure functions from impure functions in order to properly support management of effects. Doing so in Haskell's case helps with laziness, but that's just a special case of a more general benefit that comes from being able to express and check the distinction between pure and impure functions in a language. &gt; But don't try to turn that around and claim that the ML's impurity is some sort of grave deficiency. I'm not claiming that impurity is a deficiency, I'm pointing out that being unable to express many kinds of purity or impurity in the type system is a deficiency, i.e. a useful ability that is lacking in ML. If you can defend that - i.e., why you consider types important for everything *but* effects, or why types for effects should *not* be supported, I'd be interested to hear it. "Grave deficiency" may overstate this lack - after all, there are useful languages that don't express or check types at all. But if one considers types important, then it's hard to see why one wouldn't consider the ability to express effects via types important. The reasons for that choice seem more historical than technical. I'd love to see more attention paid to this in future MLs.
I'm used to the mathematical meaning of expressiveness, which is not confined to programming languages, so that's what I take it to mean. As for measuring the degree to which a language lends itself to a particular type of task, that's a very subjective thing. 
In an imperative language I can easily analyze: for (int i = 0; i &lt; 1000000; ++i) { sum += i; } It is going to take O(n) time and O(1) memory. I can write the same in ML: let add i n accum = if (i &lt; n) then add (i + 1) (accum + i) else accum in add 0 n 0 Given tail recursion optimization, is going to take O(n) time and O(1) memory. Not so in Haskell: [http://www.haskell.org/haskellwiki/Foldr_Foldl_Foldl'](http://www.haskell.org/haskellwiki/Foldr_Foldl_Foldl\'). As far as I can tell the only difference is lazyness.
http://www.haskell.org/haskellwiki/Foldr_Foldl_Foldl\' Fixed link
THe effect system is definitely interesting. But somewhat disappointing: more complexity is added in the foundations to support pragmatic concerns. I wonder if there is a way to auto-magically lift a function to its monadic counterpart (scrap your boilerplate) and if not, where the limitation lies?
(fairly) direct translation of ML code into Haskell: add x = add' 0 x 0 where add' i n acc | i &lt; n = add' (i+1) n (acc+i) | otherwise = acc Doesn't Haskell have tail recursion optimization as well? You could just make the arguments strict with annotations if it's really an issue, right? Also: it's not entirely correct to say it takes O(1) memory, since you'll start dealing with big integers that take up O(log(n)) space.
Thanks for the reply. The trouble is that, as an amateur Haskeller, I can't tell what the space usage for the function you wrote is. Neither do you ;) I can't either naively refactor it to use fold(l) as I would do with the ML version. The strictness remark is spot on. If everything were strict, then the run-time behavior of the code would be much clearer. Strictness does matter. Edit: more lazy memory unfunness: http://stackoverflow.com/questions/412919/how-does-haskell-tail-recursion-work
Haskell makes it possible to switch between strict and lazy evaluation at will. With that power, comes more cognitive burden and responsibility. You have the tools though: ! and friends can ensure you annotate the strictness you want. 
&gt; more cognitive burden That is a serious problem. I love the purity and the type-system of haskell and I'm having fun using it, but can't seriously consider using it in production because of this very issue. My brain is too small :/ With regard to the strictness annotations, I'm afraid it's a pervasive issue. In a large project and/or when using libraries, I can't afford to annotate all the dependencies.
Roughly speaking, one needn't do optimization of tail recursion in a lazy language, because stack usage is not necessarily associated with _any_ function calls (in a good implementation). Rather, stack is necessary for evaluating within a nested expression. So, if we want to evaluate the expression `(1 + 2) + 3` we need to: 1. Push '`_ + 3`' on the stack 2. Evaluate '`1 + 2`' 3. Pop the stack and evaluate the new expression '`3 + 3`' But this is due to `(+)` being strict,* and thus requiring its first argument to be evaluated. However, if we consider instead `foldr const 0 [1,2,3]`, then we go through the steps: 1. foldr is strict in its third argument, so push '`foldr const 0 _`' and evaluate `[1,2,3]`, and expand the matching 2. evaluate '`const 1 (foldr const 0 [2,3])`' 3. This reduces immediately to `1` So, we use stack for where `foldr` is strict, but not _linear_ stack despite the fact that `foldr` is not tail recursive. It's unnecessary because `const` is non-strict, so the nested, recurisve `foldr` call needn't be evaluated. Or, if you prefer, `foldr f z (x:xs)` is always a tail call to `f`, with the arguments `x` and `foldr f z xs`, neither of which need be evaluated unless `f` says so. So we never inherently need to use stack to perform function calls. This isn't really hard, or unclear. It just isn't like most other languages that people have spent years internalizing. [*] One can expand the definition of `(+)` such that it isn't really the function calls using stack. Rather, we have something that looks like: m + n = case m of ... So, to evaluate `(1 + 2) + 3` what we do is: 1. Expand to 'case 1 + 2 of ...' 2. Push 'case _ of ...' onto the stack and evaluate '1 + 2' ... n. Pop and evaluate 'case 3 of ...' ... It just so happens that `(+)` immediately performs case analysis, which uses stack.
Folding over const misses the point of fold, which is to propagate some value along each element in the list. Can you please describe how evaluation would progress if one would like to fold over a list and replace each element with it's index typed using a simple ctor: data Peano = Zero | Succ Peano? Feel free to trigger some computations by taking and displaying the first k elements of the new list, in case the folding is lazy enough to not actually compute anything.
&gt; Certainly, but that's beside the point. No it isn't. You claimed Haskell was one of the few languages where it's possible to "explore functional programming", or words to that effect. This is clearly nonsense for anybody who's even vaguely familiar with substantial codebases written in an ML dialect, as I said. People were programming in a largely pure style in the MLs before Haskell was even thought about. &gt; The challenge arises in how a language manages both purity and impurity. Haskell requires that a function's purity or impurity be explicitly expressed via its type. This has the usual benefits that types provide: it allows certain important program properties to be expressed explicitly, and also allows those properties to be statically checked. Yes, I like static type checking. But this doesn't mean I have to lose my head and enforce a braindead fundamentalism in my programming language of choice. &gt; A more general version of that statement would be more relevant to this discussion: any language needs to clearly distinguish pure functions from impure functions in order to properly support management of effects I disagree. In what ways are ML programs *lacking* in reliability because the type system isn't annotating which functions are effectful? Do you think O'Caml and Standard ML programmers have a problem reasoning about their code because the type system doesn't feature monads, effect typing, and so on? This will be news to them, I'm sure. The *compiler* needs to reason about effects for optimisation purposes, but then the compiler needs to reason about a lot of things which aren't revealed to the programmer via the type system. &gt; The optimization issue that you raised is unrelated to this point. The optimisation point is clearly relevant. In Haskell, type system enforced purity is a *requirement* due to the evaluation strategy, yet it's routinely sold as a *virtue* by which other languages are measured against. IMO the MLs have hit a sweet spot. They provide enough functional features to program almost completely purely with yet also provide pragmatic means for getting shit done without having to restructure your whole program around monads. &gt; Effects are a major cause of bugs in programs, but they're also a necessary evil in a large class of non-trivial programs. Arguing that types shouldn't be used to help with this, or that they're not necessary in an otherwise typed language, seems like a strange position to take. No it doesn't. It's completely pragmatic position to take. Contrary to what you stated, effects are not a major source of bugs in ML code.
Yeah, but that switch is very painful. If you forget a single ! or seq or something in a large subprogram, it will break, and there are no good tools to localize the problem. What I want is a language extension which switches to a strict dialect of Haskell, with optional laziness, so that you can control strictness on the module level; similarly as now basically you have lots of versions of Haskell specified by language pragmas on the top of the file. Unfortunately, then we should just keep this secret, or people just learning Haskell will never confront full laziness :)
I have a small brain, too, but the secret is that it has become accustomed to lazy evaluation. ;-)
I and many others are using Haskell in production for large systems. We have no more difficulty getting a handle on space usage for Haskell than for imperative languages. It's a very different kind of thinking, but it's no harder once you get used to it. Just like with any language, it takes experience, not a larger brain. It's good to know that strictness annotations are there for when you need them, but for garden-variety real-life programs it's pretty unusual to need them very much. The excessive use of strictness annotations that you are describing is mostly the practice of programmers who grew up in imperative programming and are still "pining for the fjords".
Right, except fixing those will break pretty much all Haskell code in existence.
If I'm not mistaken, since this release drops the AES package dependency, it is now completely 100% Haskell code. Great work Vincent!
People still use Alex?
And by 2.2, you mean 2.2.10 :)
It does indeed drop the AES package dependency, and use the pure haskell implementation of AES. however it still use cryptohash which is mostly C. Also i may reintroduce optional implementation in C of AES, since the haskell version is not really able to come close performance wise (despite trying with mutable vectors, mutablebytearray&amp;bytearray, ..)
That is not the purpose of `foldr`, really. The purpose of `foldr` is to break down a list into some result type. The result may be a list, but it's not necessarily the best way to produce a new list by processing an existing one. What you want to do is better accomplished by `mapAccumL`. Here's a simplified version that doesn't produce the final accumulator value in the end, which simplifies the control flow: mapAccumL :: (acc -&gt; a -&gt; (acc, b)) -&gt; acc -&gt; [a] -&gt; [b] mapAccumL f acc (x:xs) = case f acc x of (acc', y) -&gt; y : mapAccumL f acc' xs mapAccumL _ _ [ ] = [] Now we can write your desired function as `mapAccumL (\n _ -&gt; (Succ n, n)) Zero`. And it evaluates like: mapAccumL f Zero [1..] expand case [1..] of (x:xs) -&gt; ... ; [] -&gt; [] push 'case _ of ...' [1..] expand 1:[2..] pop case 1:[2..] of (x:xs) -&gt; ... ; [] -&gt; [] first case matches, reduce case (\n _ -&gt; (Succ n, n)) Zero 1 of (acc', y) -&gt; ... push 'case _ of ...' (\n _ -&gt; (Succ n, n)) Zero 1 beta reduce (Succ Zero, Zero) pop case (Succ Zero, Zero) of (acc', y) -&gt; ... reduce Zero : mapAccumL f (Succ Zero) [2..] And then we stop evaluating, because we have reached a top-most constructor `(:)`. This version of `mapAccumL` can be written using `unfoldr`, which is the superior primitive for this particular situation: mapAccumL f v l = unfoldr g (v, l) where g (_ , [ ]) = Nothing g (acc, x:xs) = case f acc x of (acc', y) -&gt; Just (y, (acc', xs))
If you want the compiler to reason about effects, then you have to teach it about them, I don't see any way around that. Remember that Haskell's use of monads is not the end of the story, other monadic systems such as in Wadler and Thiemann's "The Marriage of Effects and Monads" also distinguish between read and write effects. Haskell's monadic encoding of effects is quite useful, but is limited in that 1) different state-ish monads are hard compose 2) they over-constrian the ordering of effectful computations. For this second point, note that most read/write effects are actually commutable, but with the thread-the-world style encoding the compiler cannot exploit this fact during optimisation. 
In the practical sense, Disciple's "approach to effects" is a combination of type system support and appropriate definitions in the base libraries. You could manage your effects with a Haskell-style IO monad if you swapped in an appropriate Prelude. 
No don't go backwards. Stick with pure Haskell so that it will just work everywhere. I always try and find the pure Haskell version of a package first; even if it is slower.
Fixing `Data.List` might, yes. But why throw good code after bad and forbid *new* code from using Nat?
&gt; the haskell version is not really able to come close performance wise If you wanted dons to optimize it for you, that may have been the right tactic. Next, try to get it included in the shootout.
It's very hard to build a performance competitive cryptographic algorithm in pure Haskell, take Crypto, SHA, SHA2, pureMD5, or cryptocipher (AES imp) as strong evidence. It's not like there's any portability or correctness sacrificed so long as a decent suite of KATs are used.
You do realize that there are now a few AES-specific instructions in the latest Intel chips... you don't seriously suggest that we give up the performance and energy consumption benefits in favor of a "pure" solution? IMO, the better thing to focus on is whether or not the interface is pure, not the implementation.
No not really; it does not make sense to ignore the benefits of the impure version. I guess that I was trying to suggest having a pure fork for those that don't care about speed or just want to develop it first, and then have the impure version that people can just use atthe end and has all of the speed benefits. That way you can have thebest of both worlds and, as far as I can tell, he's most of the way to being able to make a pure fork anyway so maybe it's not as much effort as we would think to have those parallel forks.
What do they use?
I do agree with you, however the whole protocol is linked to how the crypto performs (this is pretty much the only reason why every other ssl/tls library have assembly optimized crypto). For the record, the "normal" haskell version is about 60x time slower, and after optimisation (and using bytearray, unsafeIO, magichash, and such), about 16x. So, while I'm not going to ditch the haskell version away, i think i'll offer along the package a C version for serious use -- which will remain pure in its interface.
haha, that would be nice to get some help of some seasoned optimisers ;) I did try my fair share of optimisations though, mostly through reading bytestring, hashable, and such packages, and i'm even learning some ghc-core at the moment.
&gt; &gt; Certainly, but that's beside the point. &gt; No it isn't. You claimed Haskell was one of the few languages where it's possible to "explore functional programming", or words to that effect. This is clearly nonsense I wrote "**fully** explore". The aspect that ML doesn't support fully exploring is expressing types for many kinds of effects and statically checking them. &gt; Yes, I like static type checking. But this doesn't mean I have to lose my head and enforce a braindead fundamentalism in my programming language of choice. That's not an argument, and doesn't address the question of why you favor one form of typechecking and believe another is unnecessary. &gt; In what ways are ML programs lacking in reliability because the type system isn't annotating which functions are effectful? ML programmers manage effects in their heads, in the same way that programmers in untyped languages manage types. The arguments for typing of effects are the same as the arguments for types in general. There are many benefits, including supporting a more precise and explicit expression of a program's intent and behavior, and of course static checking of that. &gt; Do you think O'Caml and Standard ML programmers have a problem reasoning about their code because the type system doesn't feature monads, effect typing, and so on? This will be news to them, I'm sure. Do you think that Scheme and Python programmers have a problem reasoning about their code because the type system doesn't feature much beyond a universal value type? This will be news to them, I'm sure. If you want to argue that expressing types for effects is unnecessary, you need to be able to explain why it differs from other kinds of types, i.e. why the exact arguments in favor of other kinds of types don't apply to effect types. &gt; In Haskell, type system enforced purity is a requirement due to the evaluation strategy, yet it's routinely sold as a virtue by which other languages are measured against. There's no conflict between those two points. It certainly can be a virtue, although like most things it has pros and cons. The fact that it was arrived at via a requirement doesn't somehow preclude it from being a virtue. &gt; IMO the MLs have hit a sweet spot. I agree. Nevertheless, they have done that by making a large concession to untyped imperative programming, which limits the ability to fully explore functional programming in ML. &gt; yet also provide pragmatic means for getting shit done without having to restructure your whole program around monads. There might be an opportunity there for an ML to improve its sweet spot by addressing effect types, possibly in a way that's more in tune with the design of ML. You seem to deny that there's any benefit to doing so, but have you really given it any serious thought? I think you're coming from a position of defending ML as it is, rather than being open to improvements that might be made. &gt; It's [a] completely pragmatic position to take. I'd agree that the ML approach to effects was pragmatic at the time the decisions were made - those were the "historical reasons" I referred to previously. The techniques for programming imperatively in Haskell that programmers rely on today didn't exist back then, and choosing not to try and invent them was certainly pragmatic. But I'm suggesting that Haskell's approach has actually borne fruit, and that ML could benefit from that experience. &gt; Contrary to what you stated, effects are not a major source of bugs in ML code. Relative to other kinds of bugs? Do you have any data on that point? My point is that effects in general tend to be a major source of bugs in any language. They're one of the worst kind, too, because they tend to be dynamically complex, with cause and effect separated in space and time in a way that's not statically amenable in general, either to the compiler or the human mind. Haskell's type system helps with this.
`{-# LANGUAGE SanePrelude #-}`
&gt; The arguments for typing of effects are the same as the arguments for types in general. There are many benefits, including supporting a more precise and explicit expression of a program's intent and behavior, and of course static checking of that. And a lot of downsides too, including (in Clean) an impenetrable type system which appears rather *ad hoc* and in Haskell the promotion of one particular design pattern to "most favoured status" along with making debugging a nightmare, as a simple "putStrLn" cannot be inserted without changing the whole type signature of the current function and all that depend upon it, amongst many other problems. Ironically, GHC somewhat addresses the last point by providing the (impure) *trace* function and related Debug.Trace module! Subverting purity to actually obtain a usable language. Who'd have thought? &gt; Do you think that Scheme and Python programmers have a problem reasoning about their code because the type system doesn't feature much beyond a universal value type? Yes. &gt; The techniques for programming imperatively in Haskell that programmers rely on today didn't exist back then, and choosing not to try and invent them was certainly pragmatic. What a load of revisionist poppycock. O'Caml was initially released in 1996 a full four years after Wadler published "The essence of functional programming" and three years after the publication of "Imperative functional programming". F# was initially released just a few years ago. The reason why those designing ML derivatives aren't including monads or effect types in the language is not because they're ignorant of the current body of research, it's because they don't really care. Hell, even the definition of SML was revised in 1997, and they had plenty of opportunity then to add an effect system, if needed. Even now, the Successor ML [page](http://successor-ml.org/index.php?title=Main_Page) merely lists an effect system as "desirable", behind such pressing revisions as updating the syntax of comments. &gt; You seem to deny that there's any benefit to doing so, but have you really given it any serious thought? Yes, I've given it plenty thought. People can disagree over language purity (especially as implemented in the current pure languages like Haskell and Clean) after having thought extensively about the problem. &gt; I think you're coming from a position of defending ML as it is, rather than being open to improvements that might be made. Not at all. There's plenty of suggestions I can make for improving the MLs, many of them pioneered in Haskell, including type classes for handling polymorphism, Haskell's syntax (which is a lot nicer than MLs), and so on. But "purity" is not one of them. &gt; Relative to other kinds of bugs? Do you have any data on that point? You're the one who claimed that effects are a major source of bugs *in every language* including the MLs. In fact, you're still doing so. How about you backing up your claims with some data? Are effects (in ML) a major source of bugs, and how often do they cause bugs compared to other language features? Further, is the price that one must pay to "fix" these bugs worth it? If you don't have this data to hand, then why are you so convinced that there's an urgent need to make the MLs pure? An act of faith?
Ghci on Windows 7 64bit still crashes when press Ctrl-L. http://hackage.haskell.org/trac/ghc/ticket/4415 Would someone fix it in the coming 7.0.2? As I saw they set the milestone to 7.0.2.
Me, I just like saying "zygohistomorphic prepromorphism"... no need to use it!
I'm curious about the plan for the Haskell Platform. It seems like 2010.2 was released with GHC 6.12.3; so does that mean that we have to wait for the autumn release before there is a Haskell Platform with GHC 7?
2011.1 is due in about two weeks, with GHC 7.0.2
... oops. When I wrote that, I had in my mind that it was 2010. *sheepish grin*
Commuting non-critical effects could probably be done with a semi-strict indexed monad, quite easily, so I'd argue that what we need is (even) better support for type-level programming. That is, conciliate a) short type signatures for users and b) retaining full type inference with c) (relatively) painless type-level code. Currently, it's choose one of three.
Duh... everyone knows it's still 2009.
Parsec :)
Exactly. You *can* understand these lazy evaluation related issues and solve them; but furthermore, they aren't necessarily harder. Those new to Haskell tend to ignore all of the things that are made more difficult by applicative order evaluation because they are used to that cognitive load already, and haven't yet shed it. Applicative order is an optimization that is only sometimes correct -- sometimes it produces the same correct answer as normal order (or lazy evaluation) would, but other times it makes your program fail to terminate or fail with an error. But mainstream languages have specified that it's nearly always done (and "nearly always" becomes always for things you wrote yourself; the exceptions are hard coded into the language spec), whether it is correct or not. So you have to reason about what will happen to your code when the compiler performs broken optimizations. Now *that* is very confusing, unless you've trained yourself to think imperatively all the time and pay attention to it. The difference is this: Haskell's extra cognitive load is in reasoning about performance. Mainstream languages dump that extra load on you when reasoning about correctness. Yes, that's a biased way of putting it; but I don't think it's really incorrect.
It didn't seem like a good idea at the start, it was too heavy (also, if you use Parsec you don't need to use Happy anymore, too :D) I'll switch to it someday.
There is always a conflict between simple operational semantics and higher abstractions. There is a vast space between the machine-instructions required to meet a specification and the specification itself. Programs can be more similar to one at the expense of being different from the other. Haskell allows programs to be very far on the specification side, but that does mean you lose a lot of "operational reasoning" (i.e: similarity to the machine instructions).
Thanks! I was having a bit of trouble with the State monad over the weekend, this cleared things up nicely.
&gt; There isn't a damned thing that you can do in Haskell that you can't do in an ML dialect that's strictly related to Haskell's purity. Depending what you mean by "strictly related", you cannot use the type system to enforce [resource lifetime and guaranteed cleanup](http://lambda-the-ultimate.org/node/2926). &gt; Even the supposed optimisation benefits of purity in Haskell are largely a chimera. Are the major Haskell implementations reliably producing executables that are as fast as Stalin or MLTon, compilers for two impure functional languages? Sure, see [Supero](http://community.haskell.org/~ndm/supero/) or other forms of [Supercompilation](http://hackage.haskell.org/trac/ghc/wiki/Supercompilation). These sorts of optimizations are only possible in pure languages, but they're still experimenting with them, so they haven't been adopted. I don't know of any ML where effect types are used in this way. MLkit is the only ML that even uses effect typing, that I'm aware of.
&gt; Depending what you mean by "strictly related", you cannot use the type system to enforce resource lifetime and guaranteed cleanup. You mean impossible full-stop, or impossible to copy the particular approach that Oleg followed? If the second, then, so what? If the first, then I simply don't believe that the ML module system cannot be massaged into doing something similar. &gt; These sorts of optimizations are only possible in pure languages, but they're still experimenting with them, so they haven't been adopted. By "only possible" you mean "currently only investigated in the context of pure languages". Which isn't really surprising, given that Haskell, a language expressly designed for experimenting with is pure, SML is mostly dead, OCaml is in a state of near-zombification and F# is little more than a year old. I don't see any reason why supercompilation is impossible for an impure language, other than a lack of funding. Why do you believe that it is?
Nice tutorial. Nitpick: there's a typo in "in the moand".
&gt; If the first, then I simply don't believe that the ML module system cannot be massaged into doing something similar. You can mimic the structure of monadic regions, sure, but the fact that monads are not used pervasively for handling effects means you can't *guarantee* proper resource handling with the standard library. You'd have to rewrite the standard library from scratch modelling everything with monads, and then you just have a strict Haskell. [Ocsigen](http://ocsigen.org/) did something similar, and provide their own thread, I/O, etc. libraries based on monads with a little macro glue to make it pretty. &gt; I don't see any reason why supercompilation is impossible for an impure language, other than a lack of funding. Why do you believe that it is? Any implementation for an impure language will have to be overly conservative because the compiler can't know where there's a side-effect that can't be reordered, or duplicated, or combined. Thus, you lose many optimization opportunities.
Hmm, I perceive Parsec as much more lightweight than Alex and Happy, especially because you can put it in the same file as other code and even use it in function-local definitions.
&gt; Exercise: Find a function which is both linear and bilinear. The zero function. Any others?
f(x,0) + f(0,v) = f(x+0,0+v) = f(x,0)+f(x,v)+f(0,0)+f(0,v).
&gt; You can mimic the structure of monadic regions, sure, but the fact that monads are not used pervasively for handling effects means you can't guarantee proper resource handling with the standard library. You'd have to rewrite the standard library from scratch modelling everything with monads, and then you just have a strict Haskell. Right, but this is something different to what I was originally claiming. Idiomatic Haskell makes heavy use of monads, to be sure. But the fact that I can't interface my monadic region library well with the O'Caml standard library isn't because of Haskell's purity, it's because of design decisions made in the O'Caml standard library. &gt; Any implementation for an impure language will have to be overly conservative because the compiler can't know where there's a side-effect that can't be reordered, or duplicated, or combined. Thus, you lose many optimization opportunities. How? What's wrong with just performing an effect analysis beforehand?
I'll try it too and will post a comparison. Soon, after I finish my current hobby program.
&gt; But the fact that I can't interface my monadic region library well with the O'Caml standard library isn't because of Haskell's purity, it's because of design decisions made in the O'Caml standard library. Yes, Turing completeness ensures that you can do anything in just about any language, and the type systems are sufficiently similar that the guarantees do translate (though all your type class code will need to be wrapped in functors, so it's a little more unwieldy). The question is what's feasible given the standard tools. OCaml and SML don't give you standard tools you would need to support monadic regions, you'd essentially have to rebuild half the toolset yourself. That's beyond the scope of most projects that need to choose between Haskell and SML, so if these sorts of guarantees are important, you're better off with Haskell for now. Still, if you're willing to do the work, you can get [some nice abstractions out of OCaml](http://lambda-the-ultimate.org/node/2892). &gt; How? What's wrong with just performing an effect analysis beforehand? You can only infer effects that are defined within the language, ie. mutable refs for instance. I/O affects the outside world, which require annotations at the I/O library interface; any FFI requires effect annotations in fact. As I said, this requires rewriting the standard interfaces to the environment.
Is that transformation in `sumT` supposed to semantically invariant? To me it doesn't seem so, as the `i` variable referenced in `a` and `b` in the where-clause changes for each recursion call...
Ah yikes! Tired :}
blurb: &gt; As part of the Microsoft Research Cambridge Talks series, Ben Gaster presents an embedding of OpenCL into the functional programming language Haskell, with the intention of allowing programmers to increase productivity while also allowing them to write any OpenCL program. He qualitatively demonstrates that using Haskell as a host language improves programming productivity, and provides Haskell programmers access to some of the performance benefits of modern CPUs and GPUs.
&gt; Notice how any function that needs to use or modify the state, or call one that might, has to return a result in the monad, not a simple value. A simple thing to note, but it definitely helped me understand the state monad a bit more.
Good work by Garrett getting Haskell into AMD. 
This is very cool but does anybody know if it is indeed possible to get rid of the required initCL function? Also the notes on Haskell not being popular enough may be true but hopefully that is changing.
&gt;OpenCL^TM Open and .^TM just don't go together, imho.
I think we hold a constructive proof that they actually do.
Linux™ begs to differ. It's only trademark, you can recover when shit happens. E.g. Hudson/Jenkins, Open/LibreOffice.
&gt; I wonder if there is a way to auto-magically lift a function to its monadic counterpart Sure, give your language a monadic interpretation normally executing in Identity. But this is exactly a type and effect system like DDC.
Looks great so far - are slides available anywhere? It's a bit hard to read the code on YouTube's resolution.
Thanks. (And thanks to Iain for sorting out the video.) There are no slides as such, but darcs get http://personal.cis.strath.ac.uk/~conor/pub/dtp gets the (evolving) repo of literate agda notes and exercises.
And here I was hoping for a solution using Haskell's ability to have infinite data structures. This uses an approximation technique on a 25x25 resister grid.
Well I would too but the simple fact of the matter is that you have to turn the problem into a non-infinite one if you are going to ever compute it. No computer can compute infinites, you cannot even compute an infinite expression in mathematics without finding a trick that allows you to approximate it or turn it into something non infinite.
This isn't really a solution, just an approximation of one. There does exist a closed form solution... the trick is to do it spectrally.
Somebody should write an introductory book for Agda, a la http://learnyouahaskell.com They could call it LAAG: Learn Agda. Alright? Good.
Looks like this guy is having a go: https://github.com/liamoc/learn-you-an-agda
You could compute it in the sense of [CReal](http://hackage.haskell.org/packages/archive/numbers/2009.5.28.1/doc/html/Data-Number-CReal.html). Bonus points for doing so by writing down an infinite lazy grid of CReals defined in terms of their neighbors. Of course, to get the convergence to start somewhere you need some a priori bounds on the values, and specifying those could be tricky. I feel like this is the kind of thing that Conal or sigfpe has written about somewhere...
Can someone translate this into python?
It would definitely be nice to have signed integers as well.
Perfect, just what I'm looking for, thank you!
Wait, where's the [zygohistomorphic prepromorphism](http://stackoverflow.com/questions/5057136/real-world-applications-of-zygohistomorphic-prepromorphisms) story?
Yes, I understand that... I was just expecting some neat Haskell trick to turn the generic, infinite definition into a finite evaluation. I'm still in the early phases of learning haskell and was looking for a "ohh, cool" moment :-)
 import xkcd c = xkcd.comic(356) print c.solve(network_size=25) 
It seems that that story was released on Feb 20. This HWN covers Feb 13-19. Seeing how many points that question has gotten, I'm almost sure we'll see it in next week's HWN. Thanks asking, I had a look just to make sure I didn't have a bug in my process.
Ah, yes, it even says 13-19 in the HWN, itself. I thought the deadline was closer to the release date.
Does this work with the new save file format? Otherwise awesome work - combining my favourite two things!
Good opportunity for some real world haskell programming.
I might have to consider moving the release day to Tuesday, and still keep a dadline of Sat. Problem with that is that that doesn't give much time to the stories published later in the week enough time to "compete in the market" with the rest of the stories. Any thoughts?
Base inclusion on average rate of upvotes/day, then sort by total, disqualify after the second appearance in HWN. That would mean good stories can appear twice, once at the end of the list, once nearer the top. OTOH, you could just keep it as is. People getting to know stackoverflow questions via HWN probably aren't too interested in day-to-day actuality.
This implement the Minecraft network protocol and a proxy for modifying messages in flight. I thought I'd share it after seeing the library for editing the binary file format. The "template haskell" branch has been updated for the new version 9 of the protocol.
Indeed, and it is good to see that the Haskell job market is growing. Galois and others have shown that this leads to more streamlining for real-life use. As a bonus, it makes the Haskell ecosystem more attractive to a wider audience.
Well, pi has an infinite number of digits. We can still compute it to any specified arbitrary accuracy.
Wow, the induction of a sexawesome hacker.
Wow, this article is pure gold :)
This library can read the chunk files from the old save format as well as the level.dat files from both save formats. The new save format is basically a custom archive format which contains multiple old-style chunk files, so if you unpack it, you could use this library to read the files inside.
"Instead of parsing via the Applicative typeclass, we now use a custom parsing monad, improving both ease of use and performance." Does this mean, I can't do instance JSON Foo where readJSON (JSObject jso) = Foo &lt;$&gt; valFromObj "x" jso &lt;*&gt; valFromObj "y" jso anymore?
I've also had [4x performance improvement by CPSing (actually Codensitying) my monad](http://r6.ca/blog/20071028T162529Z.html).
You totally can, because the type is still an instance of Applicative. You just don't get to choose the concrete type used for parsing, but that's all to the good anyway.
Can you compare against GHC 7. That original work inspired much of the work, though it is increasingly becoming hit-or-miss as GHC gets better at non-CPS code.
GHC 7 clobbers attoparsec performance rather badly: 70% fall-off in 7.0.1, 22% in the upcoming 7.0.2. SimonPJ's going to look into it.
Thanks for the kind words! I've been meaning to add a layer over this nuts-and-bolts level that operates on a whole collection of world files at once. Unfortunately, grad school has pushed that back a few months ;)
Awesome! I'm working on [a mod](http://www.minecraftforum.net/viewtopic.php?f=25&amp;t=178787) that makes the maximum height larger on a per chunk basis, but to do that I had to extend and reorder the savefile format and now everybody wants a converter from old-style world to new ones. This looks like just the thing I need!
Check out Chris Okasaki, Ralf Hinze, and Stefan Kahrs and red-black trees. The latter two have pulished on how to enforce the R-B condition with types.
Yep, seems like [Stefan did the same thing 10 years ago](http://journals.cambridge.org/action/displayAbstract?fromPage=online&amp;aid=83905). But it's behind a pay-wall so it's hard to tell :( Edit: or did he? after finding what appears to be the code (thx nominolo), it seems that black-depth isn't enforced?
The paper is behind a pay-wall, but [the code](http://www.cs.kent.ac.uk/people/staff/smk/redblack/rb.html) isn't.
I'm trying to understand this code just as part of learning the language. Could somebody point out where I could read about the "data ... where" syntax, with the "Nil :: ..." etc below? Or explain it if it's quick?
The `data where` syntax is GADTs, (generalized algebraic data types), a common Haskell language extension. It would be a tad involved to describe in a reddit comment, but Google should have something to say about them.
As cdsmith pointed out, this is the syntax for GADTs. With the standard ADT syntax you specify the constructor name and its field types. With GADTs, you also specify the constructor's result type. The result type basically has to be the data type you are creating, but it can be given specific type parameters.
I don't know if I am the only one but I actually find the Scala syntax hard. I also have a hard time with OO/FP hybrids and never know what paradigm to use. Maybe this is due to scala being a relatively young language and the idioms are not yet well known. In general, I find Haskell as a language much more coherent as a whole than Scala.
The blog post focuses on improvements to aeson, which is nice. But at the moment I am more interested in attoparsec. What do attoparsec users need to know about? Glancing over the bitbucket commit messages, it appears that there are three main API changes: * The `number` parser - why would I want to use this rather than `decimal` or `double`? * The `parseOnly` function - how is `parseOnly p` different than `eitherResult . parse p`? * The `ensure` parser is now gone - I'll admit that I never needed it, but why was it removed? Where can we expect to see performance improvements? Would these improvements tempt us to use attoparsec differently than before?
seen here http://www.cs.rice.edu/~dmp4866/presentations.html
I'm learning some Scala right now (and have a little experience with Haskell) and I think this is an accurate criticism. Scala supports a LOT of different syntactic elements. I'm not sold on the idea that laziness is strictly superior than strictness, personally though, and it cannot be argued that Haskell has good record syntax for types.
&gt; laziness is strictly superior than strictness Thats' a bit like saying "graphs are strictly superior to arrays" -- ideally, your language supports both, so you can choose the right technique for the job at hand.
Wow, this was an amazingly worthwhile read, given what discussions with that kind of title usually turn out to be. Tony Morris' post is just fantastic.
Ironically enough, I primarily work with a language that has rather rubbish support for graphs but excellent support for sequential structures (KDB+). I should have clarified though: I'm not sold on the idea that laziness *by default* is strictly superior to strictness by default.
&gt; I'm not sold on the idea that laziness by default is strictly superior to strictness by default. That might be even the majority view :-)
[This paper](http://www.scribd.com/doc/49610475/Red-Black) ?
Note: this is not actually about how lazy functional languages work at the low level, but how we can automatically classify the source language of compiled code by running SVM on the low-level assembly. On that note, anyone know if there's a more annotated version of the talk (video?) The graphs need a bit more explanation.
I believe it's a fifty-fifty situation: half of the time I want lazy by default and the other half I want strict by default (ok, maybe it's more like 70-30). Really, I want to control which one is the default on the module level. Also, lazy by default seems to be strictly superior for small programs, when there are no performance/memory issues.
Hmm, there's something I don't get regarding Stefan's version: I seem to be able to construct illegal trees.. I load [his code](http://www.cs.kent.ac.uk/people/staff/smk/redblack/TypedExist.hs) (thx nominolo for finding it) in ghci, and I can simply construct illegal trees: ghci&gt; ENC (B (C (B (C E, 3, C E)), 5, C E)) B(B(E,3,E),5,E) Using my version: ghci&gt; Tree (BlackNode (ItsBlack (BlackNode (ItsBlack Nil) 3 (ItsBlack Nil))) 5 (ItsBlack Nil)) &lt;interactive&gt;:1:83: Couldn't match expected type `Succ Zero' against inferred type `Zero' Expected type: BlackNode (Succ Zero) a Inferred type: BlackNode Zero a1 In the first argument of `ItsBlack', namely `Nil' In the third argument of `BlackNode', namely `(ItsBlack Nil)' 
&gt; I can do all those things that our industry seems to think are useful, though I secretly contend are pathetic, and I hope our children do too. That was a great line. 
I am glad this one was picked up and is actually discussed. I like haskell and spend lot more reading haskell blogs and books than any other but, at the moment, I took a refuge with scala. I settled with scala because it seemed the most reasonable choice for a statically typed strictly-evaluated functional programming language (I tried Ocaml - perhaps one wont like it after spending time with haskell. I did not invest too much of time with typed-racket owing to some superstitions). I will mention a few things that *scare* my lizard brain about haskell. I am mentioning them here purely in the hope of getting some advice from the knowledgeable people here. * The other day I started reading Oleg's presentation on [iteratee - pdf](http://okmij.org/ftp/Haskell/Iteratee/DEFUN08-talk-notes.pdf) and here's a quote from the same &gt; I can talk a lot how disturbingly, distressingly wrong lazy IO is theoretically, how it breaks all equational reasoning. This was the most distressing line I have ever read about haskell. I mean, till then I did not realize such kinds of problems exist with lazyIO. I was under the impression that iteratee is a sort of design pattern for improved performance or something. That it is **mandatory** for production code is a big blow to my confidence.. Among other things, it clearly shows that *I* simply do not understand all the nuances of laziness. Perhaps I am not good enough for it? I still have no clue of doing analysis related to algorithmic complexity in a haskell program. How exactly do I *get* this lazy evaluation thing? And how can I be sure that I am not missing some thing else that I will encounter two years from now? * Am perhaps wrong in this but I think a decent build tool is missing in haskell world? With all its problems, I find maven quite good.(I was actually in the middle of designing one when I encountered the above iteratee presentation - which meant, i had to stop my work and understand iteratees. I gave up). At the moment, I think i will wait till haskell's FFI supports integrating with a safer language than C, before I ever consider my most favorite language for any production application. I am very upset about this situation. I hope am wrong some where. And that it is just a matter of another book to study.
The problems with lazy IO are with _lazy IO_, not laziness. Essentially, lazy IO is a way of introducing light-weight, implicit, cooperative concurrency to your program. When you write: ... ; x &lt;- unsafeInterleaveIO m ; ... you can think of spawning a cooperative thread for `m`. Then, when we actually need the value of `x`, we jump the other thread, actually execute `m`, and jump back with the value of `x` recording it. And quite frankly, I think Oleg overstates his case against lazy IO. The fact is, we can model the above using threads, channels and whatnot, and _it has the same problems as lazy IO_. The difference is that a bunch of our code will live in IO, and use klunky IO constructs, instead of being pure. For instance, here's a common lazy IO problem: do h &lt;- hOpenWhatever ... s &lt;- hGetContents h hClose h foo s This has the problem that the handle is manually closed before any input is read, always resulting in the empty string. But this same situation can be set up without lazy IO: readToChan :: Handle -&gt; IO (Chan Char) readToChan h = do chan &lt;- newChan forkIO $ fillChan h chan return chan where -- This isn't really kosher, but it's an idea. fillChan h chan = do c &lt;- hGetChar h writeChan chan c fillChan h chan ... do h &lt;- hOpenWhatever ... c &lt;- readToChan h hClose h foo c Here, we fork a thread to read the contents of a handle, then close it in the original thread. Same error, no lazy IO. Also, lazy IO has problems with dangling resources (like file descriptors). But so does forking off a separate thread in this way; releasing the resources is the job of the forked thread, not something you control in the main thread, so dangling resources are an easy result of cooperative threading. So, inasmuch as as lazy IO has practical problems, they are problems with making IO concurrent in inadvisable ways. As for problems with equational reasoning.... I've read arguments he's put forth, and they generally aren't that compelling. Some have claimed lazy IO breaks referential transparency, but I haven't seen an up-to-date example. Oleg has claimed it violates Sabry's notion of purity, but I don't recall being impressed by his example. It seemed likely that it could be handled by an (admittedly non-pretty; but so is the above concurrency stuff) appropriate operational interpretation of IO. Ultimately, I think lazy IO can be accomodated by the evilness of the `IO` monad, without hurting the equational reasoning in the pure portion of the language, if you so desire. But, I wouldn't recommend it for serious, IO-intensive programs. This is, as I said, a problem with lazy _IO_, though. Not a problem with laziness. Having the latter doesn't imply having the former.
lazy evaluation is fine. lazy IO is "readFile" handing you something that it pretends is a String, but actually goes out and hits the disc a little bit at a time as you work through the string. It's not a problem at all unless there are writes into the file while you are reading from it. It's not much more problematic than any other use of unsafeInterleaveIO - your program won't segfault, you don't risk exploits, you don't lose type safety, you just have some values floating around that might not obey referential transparency quite like pure values should.
Frankly, people have used lazy IO in production code for *years*. Moving to iteratees can yield improved guarantees of correctness in certain regards, and appears at times to yield improved performance. But there's nothing deeply wrong with lazy IO other than that it's sometimes tricky. But it's tricky in the same sorts of ways that IO is nearly always tricky with imperative code -- basically in that you have to think about the order of evaluation of things when you aren't always necessarily explicitly asking each action to happen directly. As far as build tools, I find cabal fine for straightforward builds and distribution. For fancier builds, I don't see a problem with make, although of course we could all stand to replace it with something better (maybe, e.g., https://github.com/batterseapower/openshake). Maven is really for dependency management however, and I think that cabal/hackage, when extended with lightweight local hackages, fits the bill fine. Finally, I don't see the point of Haskell's FFI integrating with a safer language than C? Or rather, I do, but it makes sense for me that it goes through C. Which is to say that we have a few bridges to python, a bridge to ruby, a few to .NET, and they all are built via the C based FFI, since the C calling convention is one of the only real broadly respected standards. The obstacle to dealing with Java is basically just that there hasn't been enough demand/will for somebody to feel compelled to both write and maintain one. And now, with libraries like thrift and protocol-buffers, I think the direct FFI concern is much less of an issue anyway, I should add.
&gt;I'm not sold on the idea that laziness is strictly superior than strictness What about the idea that laziness is *lazily* superior than strictness? ;)
I want type distinctions between the following three things: values in x, suspended pure computations for values in x, suspended effectful computations for values in x. The ones in the middle get the thunk-overwritten-by-value treatment. What should be on by default? Informative typing and programmer control, that's what.
God what awful answers. They're all just talking about which language is easier/better in different contexts. He was asking what *things* are harder/easier to accomplish.
&gt;&gt; I can talk a lot how disturbingly, distressingly wrong lazy IO is theoretically, how it breaks all equational reasoning. I have always considered this fear-mongering. There is nothing wrong with lazy IO if you know how to use it. [Duncan Coutts][1] (author of the ByteString library in case you need authoritative credentials) [explains how to use it][2]. [1]: http://www.well-typed.com/who_we_are [2]: http://stackoverflow.com/questions/2981582/haskell-lazy-i-o-and-closing-files/2984556#2984556 Also, for the problem they intend to solve, I have always considered Iteratees to be a rather low-level abstraction. 
Oh man, this brings back the pain of using Matlab. It doesn't even have linked lists out of the box.
Having done some Haskell and some Scala, here's my take: * Scala is awesome if you're in a Java environment and you want to program in a functional way * Haskell is awesome is most other environments because it hasn't had to make the same compromises for compatibility and is a more consistent language as a result
&gt; Frankly, people have used lazy IO in production code for years. Right, and I'd argue that there are lots of places where lazy I/O is still exactly the right thing to use. Iteratees are useful when you've got a long-running process whose general operation revolves around handling I/O through many successive file handles... something like a network server. Not that network servers can't work just fine with lazy I/O too (see Happstack), but in any case they are a little more finicky and easy to leak file handles if you aren't careful. But for pretty much anything else, lazy I/O works great, and is a substantially simpler model for handling I/O than something like iteratees.
There are few referential transparency issues with IO that cannot be handled by the model that pretends the entire (infinite?) string to return was decided at the getContents (readFile/etc) call. If it happens you only observe certain sorts of results in the cases where you pipe the output of the program back to the input and such, well, you just got lucky.
Like what? Can you give an example?
Scala is horribly disfigured because of the various compromises for the sake of syntax and type compatibility with java. It's a stranger to both houses. Java OO programmers reject it because they find it too complex and confusing. While programmers coming from functional world and spoiled by the beauty of haskell syntax and elegant power of its type system reject scala because it is too ugly and limited. It's a pity that such talented engineer as Martin Odersky took on creating a language for hordes of cubicle drones, instead of creating a language for himself, like Rich Hickey did. 
It's not possible that two different parts of the program see different values when they inspect the string returned by readFile, so you can claim that all the elements are oracularly determined at the point of the call. This breaks down if you have a program that writes to the file while consuming the string, and you want to reason about what values it is actually possible to see.
I think signs point to Haskell finally becoming accepted in the mainstream very soon. Most programmers these days are at least aware of functional programming and its benefits for concurrency. Owing to the great new interest in concurrent programming, a lot of people consider functional languages like Erlang and Haskell. 
Actually my path went: 1. Haskell? Sounds interesting 2. Wow, this looks really complicated - it even looks too compact with all these operators! 3. Hrm, this type system seems really useful! But it's getting in my way a bit... 4. Dabble back in, read some more tutorials/RWH comes out. 5. Decide to actually man up and write something. 6. "Holy smokes, this stuff is cool" 7. I guess I better learn about Functors, Monads, Monoids, Applicative Functors... and hey, none of this stuff is really too hard now that I can practice using them. 4 is separated because that stage got reached numerous times. 
&gt; Why doesn't the class keyword instantiate an object? Sounds like you're still confused.
No. Not to the degree C++ and Java or even ruby have "caught on". I consider this a feature, though. But it's definitely going to get bigger than it is right now.
Define "catch on". Functional languages have (finally) climbed to [all of 4.1%](http://www.tiobe.com/index.php/content/paperinfo/tpci/index.html) on Tiobe's index, after spending the past decade &lt; 1%. Haskell's playing for maybe a 25% of that 4%. Are we going to see say 10% of programmers using FP languages by 2020? Maybe. Then we get a 3% market share for Haskell programmers, beyond the 0.4% currently. That'd be a great result. But 3% of the market would put it in the top 10 languages by use. Hardly any languages are actually used in any significant amount: [check this graph](http://i.imgur.com/mNftV.png) I'd settle for just 1%. Think about this: Ruby's got 1.6%. How do we get to that? There are only 6 programming languages with more than 5% of market share ------------ **So how do we get to 1%? Use Haskell yourself, tell people about it, write stuff, contribute code, fix things, create things. And teach!**
..... okay. &gt; The text you are currently reading is a quote. Its words belong to a generic Haskell learner. These words, on the other hand, belong to reddit user *firexq*, who understands that `data` defines custom data structures, and `class` defines a generic set of functions for these structures.
My point was that in no language (ok that I'm aware of off the top of my head) does the keyword "class" instantiate an object. It defines a class. Or I guess if you're talking about ruby it does instantiate an object but that object is of type Class.
&gt; The text you are currently reading is a quote. Its words belong to a generic imperative programmer. These words, on the other hand, belong to reddit user *firexq*, whose knowledge about programming is in no way represented by the caricatures he creates.
Great for quicksort? really?
Haskell will "catch on" when it becomes easy for programmers to easily use the bad programming practices they are used to. In other words, hopefully, never. One of the things I love most about Haskell is it practically *makes* you design a good solution for your problem. The type system + emphasis on function composition generally leads to well-designed programs, in my experience. Sadly, in today's world, it's not very important to design a program well. You just need to crank it out as fast as possible, and it's much easier to do that in PHP/Python/Java/C#/etc.
Haskell not catching on has nothing to do with bad practices from other languages. Haskell is just fucking hard to learn, even if you are a super programmer otherwise.
Well I think that people will get to Haskell eventually but it will be a process. I think that many people, especially Java/Lisp people, will take the Scala / Clojure route before coming to Haskell proper. Haskell would be a better language for everyone because of the benefits that it brings to the table but the simple fact of the matter is that you have to convince everyone of that. Well people are being convinced but slowly and it may take time...either way I love the community as it is now. I love that I actually see the same names more than once and that everybody does not just get lost in the crowd; I will not be sad to see Haskell get bigger but I think we will may risk loosing some of the cool close knit social stuff when Haskell gets bigger. (But I may be wrong there; Haskell may very well always stay as an awesome community that feels close knit)
Mainstream language popularity has almost nothing to do with the merits of the language. [Languages ride platforms to popularity](http://abstractfactory.blogspot.com/2010/05/how-to-design-popular-programming.html). However, it is clear that the next big thing will probably have a lot of Haskell-like features. It will also have to own some new platform that everyone wants to use. And (this is my own observation) languages tend to win when they are not doctrinaire about who they play with. C# is a great positive example, and Lisp is a great negative example. So, Scala might be the language that fits the bill of bringing functional programming and type inference to the masses in a very compromised form. However, it isn't currently attached to any must-have application or framework, so that isn't certain. Backwards compatibility with the JVM may be enough for some enterprise markets, as C++ was with C.
&gt;Frankly, people have used lazy IO in production code for years. ah! Thats nice to know. A while back, there were a slew of packages with 'ports' of many existing packages to iteratees and stuff. And the number of blogs explaining iteratee is perhaps second only to the number of them explaining monads. Thanks for the link about oss version of handshake. &gt;Finally, I don't see the point of Haskell's FFI integrating with a safer language than C? Or rather, I do, but it makes sense for me that it goes through C In my mind (after reading Oleg's presentation), the ideal application seemed to be one which is implemented in Haskell and an *IP* (where IP is some imperative language). All the non-IO part in Haskell. IO and integration with outside world in *IP*. In such a case one would expect type safety and support for namespaces/modules from IP. Having an extra layer of C would work but it would end up being the weakest layer.
What, particularly, is so hard to learn, and why?
&gt; Do you think Haskell will ever catch on? Not if we can help it! Avoid success at all costs! 
The tough thing about learning Haskell is that tutorials will try to introduce concepts before you know why they're beneficial. Whenever concepts seem overly abstruse, hack some code for a while. Often, you will encounter a frustrating problem, and realize that one of the concepts you read about resolves it easily. For example, I had trouble understanding Functors as a new Haskeller. Then tried to write this simple function that returns the words from a line of standard input. getWords :: IO [String] getWords = do line &lt;- getLine return (words line) I thought: "Well, this is dumb. Why do I have to unwrap the `IO` from `IO String` from `getLine` just to apply a quick function to it, when I'm just going to rewrap it with `return` anyway. Then it dawned on me: getWords :: IO [String] getWords = fmap words getLine 
This doesn't sound like a failure of referential transparency to me. What I really meant is a code example. What you need to do to show that referential transparency has been broken is to write two definitions: x = &lt;e&gt; y = &lt;e&gt; where x and y have identical right-hand sides (and they should probably have identical types, too). You may then use `x` and `y` however you want in other expressions. Finally, you must show that there is some context `C[]` that distinguishes `x` from `y`. So, for instance, I'd accept that the following proves that `unsafeInterleaveST` breaks referential transparency: comp = do r &lt;- newSTRef 'a' u &lt;- unsafeInterleaveST $ writeSTRef r 'b' &gt;&gt; return () v &lt;- unsafeInterleaveST $ readSTRef r return (u, v) pair = let x = runST comp y = runST comp in case fst x of () -&gt; (snd x, snd y) main = print pair In this example, `snd x` evaluates to `'b'`, while `snd y` evaluates to `'a'`, despite the fact that `x` and `y` are defined identically. And, frankly, I don't believe such an example can be constructed for `unsafeInterleaveIO` (that `runST` exists is key to the above). Certainly not until I see one. Also, as an aside, I'm not sure your example breaks how I'd dump the evilness into IO. A `readFile` must already be able to return essentially anything, because another process could be interleaved arbitrarily with your program, and write to the file you're reading. So it must already select the 'right' string under those conditions. This can even happen with strict IO, except the window is narrowed, and you don't have to worry about stuff that happens later in your program affecting the unofficial way you determine what `readFile` will just so happen to return, by miraculous coincidence. Of course, you can get similar problems by bone-headedly having a thread that reads the file run concurrently with a thread that writes to the file. I won't argue that it makes reasoning about what actually happens in your program messy. The problem with this is that we want to reason (more) easily about what happens in the embedded `IO` language. That's a real problem, and should be taken seriously. What I don't agree with is the statement that the above problems break properties of the overall pure language. If I see an example, I'll accept it. But I haven't seen a convincing one. It's easy to want to jump to asserting such a problem with the overall language, because it would be perceived as a more serious problem. Even someone as brilliant as Oleg can probably make such an error. Of course, I may well be wrong, too.
Confusing _hard_ and _different_ i assume.
I feel that there is a more productive way of looking at things. All we need to do to succeed is build up a base of packages large enough to support any reasonable project. As an example, I can currently do a webapp in Haskell and be bottlenecked on the non-hacking parts. I am not concerned with being unable to write the thing in Haskell. My concern is that my user interaction sucks. Also, I can see there are other people doing the same things that I am doing. You could email me at baguasquirrel AT gmail if you want me to send you a link to an older prototype (it's got XSS vulnerabilities up the wazoo due to my previous lack of javascript skills so I'm not going to go public with it). What this amounts to is: If you want to hack in Haskell today, it is actually a reasonable prospect. You got bigger things to worry about than Haskell. I think that's great.
most functional programmers will argue that lisp is at the top. haskell is that awful language that people who needs to worry about types use
Its community with too much emphasis on monad fapping.
A few days ago I teached a 15 year old kid the basics of Haskell, up to higher order functions, in ~45 minutes. This was someone with now prior programming experience. If you are an experienced C or Java programmer then Haskell can be hard because it is so different. The reverse is probably also true.
It seems like there should be a way to use one of the typeclass-based generics libraries here... Write serialize, deserialize as typeclass methods; then write layer propagation as a typeclass method. Iterate inside runLayers with an accompanying accumulator to keep track of the count after converting your initial Layer into the generic sum-of-products representation. Perhaps with something like: http://hackage.haskell.org/package/regular-0.3.2
This is also not the (main) reason. Haskell up to higher order functions and even further isn't hard to learn per se. Using Haskell to build something real (beyond solutions to project euler problems) is.
Ah, the well known _lasagna-code_ pattern. Spaghetti is not the only pasta.
Not sure why you got downvoted; while this clearly isn't the point of this post, quicksort is indeed a bad example. The usual, in-place implementation of quicksort is a largely imperative algorithm relying on mutation.
Context: implementing insertion sort so that it works on infinite lists. :)
Forgive my ignorance, but why wouldn't the first version work on infinite lists (as long as they are *productive* that is: can be reduced to a term of the form x:l with l productive)?
I have to agree with that. But the same is true for any other programming language. It is a skill acquired over many years of practice.
I'm a bit confused, as insert already kind of works on infinite lists, as long as you don't have an infinite number of elements smaller than what you want to insert. Also, you can't possibly sort an arbitrary infinite list, as there are an infinite number of elements to examine before you can produce the smallest element. You would have to make additional assumptions about your input in order for this to work.
Yeah, the first one does - the second is just a very silly version of the first. 
Yes, it's tongue-in-cheek. They "work" because there is a smallest element in the list (0). Without that (like you say), you can't produce anything.
Haven't thought about it greatly but `runLayers (lower,upper)` seems very close to `runState initState`.
Lisp is a fine language and it has a functional core at its heart, but much Lisp programming isn't expressly functional - e.g symbols and macros have no analogue in the Lambda calculus. "Functional programmers" have been enjoying a rocky ride with Lisp for 3 or 4 decades - witness, for example, the ICFP pushing aside Lisp in 1996 when it changed from the Lisp and Functional Programming conference.
&gt; Haskell is just fucking hard to learn... *... by yourself*. I agree that it can be hard to learn if you have prior exposure to imperative programming languages and no teacher to guide you. I'm experimenting with [video lectures][1] a bit to help out, but I could use a hand or two. [1]: http://apfelmus.nfshost.com/blog/2010/07/02-fixed-points-video.html
I had a full lecture on formal methods and functional programming using Haskell. I am by no means someone who has trouble learning. Haskell just has an exceptionally steep learning curve if you want to use it for real world applications.
Try running 'cabal haddock' instead.
I find the classic functional implementation of QuickSort in Haskell very lucid about what the underlying mechanism of QuickSort is, even if it's not at all efficient. :-)
I'll wait for the final date and decide. It can be good combination to visit Paris (first time) and see some experienced Darcs/Haskell developers in action :-)
Right, but then how do you produce the second element? You still have to look at an infinite number of elements to determine if it's a 0, 1, or something else. You could produce a sorting algorithm under the assumption that an element is never more than k elements past the position it would be in a sorted list.
What kind of real world applications? I never had any problems with Haskell.
I'm guessing you don't have a .cabal file for this project. At ten source files you should have one. Not only will it make building cleaner and more repeatable, it will allow you to build haddock with the simple command ezyang gave. Haddock can't easily be run by hand: it needs flags to set up exactly the same compilation environment that building the code uses. The easiest way to do this is let cabal do it.
Yes, currently there's no cabal file, but I was planning on adding one if the project got bigger. This seemed to be some other problem altogether: removing all comments from before the "module" keyword at the start of the file seemed to fix the problem. Bizarre.
No they won't.
Let layer2' :: Bytestring -&gt; Bytestring layer2' = serialize2 . layer2 . deserialize1 -- ... layer100' :: Bytestring -&gt; Bytestring layer100' = serialize100 . layer100 . deserialize99 Then these form a family of functions with the same type signature, which you can then stuff in a list. Then just use sublist extaction and fold over composition. Alternately, you could use a Thrist: data Transition a b = Transition (Bytestring -&gt; a) (a -&gt; b) (b -&gt; Bytestring) stack :: Thrist Transition Layer1 Layer100 stack = ... data CalculationState x a = Before Int Int Bytestring | During Int a | After Bytestring eval :: CalculationState x a -&gt; Transition a b -&gt; CalculationState x b eval (After y) _ = After y eval (During 0 b) (Transition _ f s) = After $ s $ f b eval (During j b) (Transition _ f _) = During (j-1) $ f b eval (Before 0 0 y) (Transition u f s) = After $ s $ f $ u y eval (Before 0 j y) (Transition u f _) = During j $ f $ u y eval (Before i j y) _ = Before (i-1) j y layers :: Layers -&gt; Bytestring -&gt; Maybe Bytestring layers (Layers n m) y = case foldlThrist eval (Before n (m-n) y) stack of After y' -&gt; Just y' _ -&gt; Nothing Edit: Now that I'm no longer on a phone, longer type/constructor names! 
Hey, thanks for the quick tutorial! :-)
If you can stand a little dynamic type checking you can make all the layers have the same type (using an existential and a cast). 
ICFP was a combination of FPCA and LFP, both conferences running every other year. So it was not a renaming of a single conference.
Or just using Dynamic :-)
But what if the list you're sorting does not have 0 in it? You'd have to look forever to decide not to put 0 first. Sorting just isn't something you can do on infinite lists unless you have some extra knowledge.
You also want some (semi-)automagic conversion between some of those. For instance, it's nice to have 'or = foldr (||) False' work in a short-circuiting fashion even on a list Bool values. (We do have a language that distinguishes the three cases, but no clever conversions. It's not always that pleasent.)
I believe you are right. Once you have used IO almost anything can be explained as the outside world doing evil things to you. :)
Well, that's exactly what Dynamic is. So I consider Dynamic one instance of my suggestion, but he might want something more specialized.
By switching to type level numbers you should be able to code up the function you want. Then if you need to have the layer numbers be dynamic you can use reification. 
I'm being a bit pedantic here (w/r/t Haskell... shocking, I know), and I know you actually know this, but Dynamic is actually `data Dynamic = Dynamic TypeRep Obj` where in GHC, `type Obj = Any`. http://www.haskell.org/ghc/docs/latest/html/libraries/base/src/Data-Dynamic.html#Dynamic
I think you can do this cleanly with a thrist: http://hackage.haskell.org/package/thrist
I'd give two answers, which differ in their definition of "catch on". 1. Haskell has already caught on. It's got a nice library ecosystem, some really awesome people, and it's possible you might find it in use, anywhere you look. We get occasional instances of "Hey look, they're using Haskell at..." We have some excellent tools, a nice community, and basically you can use Haskell where ever you want. 2. If the earlier answer isn't convincing you, then I don't think Haskell will "catch on". It'll never be the next Java or Python. It simply asks too much of people who in many cases really don't care. But that's okay. A language doesn't need to ever be the next Big Thing to be useful. Haskell has a specialized audience, and that's okay.
[Relevant](http://i.imgur.com/hF6mS.jpg)
One interesting thought that came to my head as I was thinking about this is that Haskell probably seems hard because the things that programmers expect to be "the easy stuff" are not the easy parts of Haskell. But it would be a mistake to then assume that the "hard stuff" is also proportionally more difficult in Haskell. This seems to lead to a lot of confusion, as people give up before they get to the point of solving hard problems and realizing that Haskell is the right language for them. I think it's fair to say that, in a sense, solving hard problems is the problem space where Haskell excels most; the proverbial job for which it is the right tool. To a new programmer, this may seem hard to believe or grasp as they are having to learn a lot of new ideas just to do the boring stuff.
this. i'm working on something at the moment that was so easy to do in haskell (from small to big picture), right down to one task, which would be the easiest thing ever in an imperative language (for the record, this was mapping over a list but wanting to refer to the previous list item for something. i worked it out but it did cause me 20 mins of shouting at the monitor WHY CAN'T YOU BE MORE LIKE C?)
fmapping. fmap fmap fmap.
That's how GHC happens to implement it. Another implementation is data Dynamic = forall a . (Typeable a) =&gt; D a toDyn :: (Typeable a) =&gt; a -&gt; Dynamic toDyn = D fromDynamic :: (Typeable a) =&gt; Dynamic -&gt; Maybe a fromDynamic (D x) = cast x Which is (if I remember correctly) how hbc implemented it. 
I was not claiming that readFile breaks referential transparency. I was claiming to opposite - that readFile does not break referential transparency, in the semantics that says the values flowing from IO into the pure world are essentially arbitrary. On the other hand, if want a more precise semantics that actually lets you say something about external behavior, that semantics might conclude that at least some cases violate referential transparency. For example, f, g :: () -&gt; () -&gt; () f x y = x `seq` y g x y = y `seq` x These are equivalent in the pure language. in a context ([*] is the hole) main = do x &lt;- unsafeInterleaveIO (putStrLn "X" &gt;&gt; return ()) y &lt;- unsafeInterleaveIO (putStrLn "Y" &gt;&gt; return ()) evaluate ([*] x y) If you say nothing about output, or if your semantics that both programs could output "X\nY\n" or "Y\nX\n" (no matter what you see when you run them), then they are equivalent and referential transparency has been preserved. If your semantics lets you prove that with "f" the only possible result is "X\nY\n" and with "g" the only possible result is "Y\nX\n", then it says this context involving unsafeInterleaveIO can distinguish equivalent pure functions.
That would be Functor, not Monad.
One cannot say that "X\nY\n" is the only possible result with `f` anyway, because `x` is not guaranteed to be evaluated before `y` in that case. Both `f` and `g` are strict in both arguments, so the compiler is free to reorder the evaluation if it chooses. One would have to go with `pseq` to truly guarantee the behavior. As I understand it, the operational semantics given along with the imprecise exceptions stuff would have no trouble even talking about the output in your example. We observe an arbitrary interleaving of the IO actions, chosen out of the set of all valid interleavings. Of course, in practice, we will expect something more definite to happen. Edit: Going back and reading your initial response, I may have misinterpreted you. My mental internet grammar correction was on, and I read "there are few ... issues ... that cannot" as "there are a few ...". My bad.
Point granted :-)
I think imperative programmers need to be weened into functional Haskell. Show newbies from imperative langues do notation, iorefs, forM, etc. first, then when they are comfortable with Haskell show them it's functional side. 
&gt; But what if the list you're sorting does not have 0 in it? You'd have to look forever to decide not to put 0 first. Yup. But working on infinite lists isn't the point of the code... maybe I shouldn't have placed that comment ;P
That's why "work" is in quotes :) We know that any zeroes must come first in the list, and they are generated as found (the naïve insert is correct but doesn't generate any items). Without any more information, that's the best you can do, so it's equivalent to `filter (==0) xs` for infinite `xs`. Anyway, the infinite list thing isn't the point of this post. Perhaps I should haven't placed that comment. :P
I'd amend that as follows. Show them the pure functional side in a straightforward way. Also show them do notation and forM and such (maybe not IORefs). And as they become familiar with both sides, start drawing the bridge between the two. Trying to teach Haskell from the beginning as an imperative language risks making it look like a pretty awful alternative to Python.
 mapWithPrev :: ((a, a) -&gt; a) -&gt; [a] -&gt; [a] mapWithPrev _ [] = [] mapWithPrev f xs = map f (zip (tail xs) xs) Example: $ mapWtihPrev (uncurry (+)) [1..5] [3, 5, 7, 9]
Your edit seems to be correct. I meant that I don't know of any failures of referential transparency in the model where IO behaves pretty arbitrarily. The question is whether that model lets you prove as much as you like about the IO behavior of programs in tricky cases. You are right about argument evaluation order and strictness of seq. In general, though, optimizations that change evaluation order are only justified by the assumption of referential transparency. Fixing lazy normal-order reduction, I think these definitions force the evaluation order. f x y = case x of () -&gt; case y () -&gt; () g x y = case y of () -&gt; case x () -&gt; ()
btw, the meme rather goes like 'Y U NO C?'
List zippers do this tasks beautifully: http://hackage.haskell.org/package/ListZipper-1.2.0.1 Zippers allow you to traverse a list, while having acces to preceding and succeeding elements. The ListZipper package module also provides folds. I understand that in the beginning it is hard to know where to look. The friendly haskell-beginners mailing list can help tremendously to start out.
as quicksilver noted, zip`ap`tail: the aztec god of consecutive numbers
Wow. I was just about to ask the same question in SO. Does anyone know? +1
Setting up cabal is so easy I just do it before I even start writing any code for the project: Mkdir project &amp;&amp; cd project Cabal init Git init
You can use jhc to target the NDK's gcc compiler. It is quite simple to do but jhc has some stability problems currently with complex code such as monad transformers with IO. You could also try making an unregistered port of GHC targeting the NDK, I spoke to somebody a while go on #haskell who apparently has done this but I don't believe it's publicly available. The problem isn't so much getting a Haskell compiler targeting the NDK, the biggest problem is someone needs to write binding APIs for the NDK which is a huge boring task and it gets worse if you need to use android UI framework because there is no NDK APIs for that you will have to write bindings to Java using JNI &amp; C. 
Another way to say "Haskell is hard" might be "clear &amp; rigorous thinking is hard", and I'd agree that it is. Haskell encourages and sometimes requires clear &amp; rigorous thinking. (I'm talking about the denotative subset of Haskell, rather than the imperative wrappers like `IO` &amp; `STM`.) Because I aim for clarity &amp; rigor, I'm grateful.
&gt; no NDK APIs for that you will have to write bindings to Java using JNI &amp; C. That's changed with Gingerbread.
I've never used CAL in production. Honestly for Android development I settle for Scala.
liftM liftM liftM just doesn't have the same ring to it.
Are you sure? I have not seen anything related to android UI framework in NDK.
I just remember hearing that native applications no longer require a Java shim for many sorts of applications. It might not be the entire UI framework, but I know it is now possible to write an android app without a line of Java.
&gt; Haskell just has an exceptionally steep learning curve if you want to use it for real world applications. Languages aside, maybe the crux of this issue is that we don't yet know how to think clearly &amp; rigorously about "real world applications". Which only becomes apparent when using a language like (the denotative subset of) Haskell. (*Edit:* added quote for clarity.)
Android 2.3 provides the glue code framework so you can write all your code in C/C++, before you had to write the glue code yourself. It is still going through JNI calling into your code. The native libraries google added that wasn't in the previous versions is APIs for using the asset management, and OpenSL ES. They've improved the NDK APIs for writing games not for writing general android apps, there are no android UI APIs for NDK it's all java as far as I'm aware.
Nice! So: mapWithPrev f = map f . (zip `ap` drop 1) or if you like Arrows and hate points: mapWithPrevArr = curry $ app ^&lt;&lt; map *** (zip `ap` drop 1) 
and it unnecessarily lacks generality. btw, i prefer `fmap . fmap` to `fmap fmap fmap` because the former more clearly states what's general and what's specific. And the former pattern holds up with other SECs besides `fmap`.
Even with a finite list the only improvement I can see in lazyness of the regular sort function is that you can produce the spine of the list before looking at any of the elements. And that's almost trivial to do.
I don't think that's the problem. Haskell is a fine imperative language and you can use the usual techniques to deal with the world. That said, I'd love to see a better way of doing it. 
The latter certainly *looks* like ascii art for a winged snake deity :-)
I didn't mean lazier in terms of Haskell, but on the part of the programmer (the `orReturn` function). THIS IS A DISASTER
Yes, which once again underlines the need to get Haskell into Java byte code. From what I read a year ago it is hard, on my phone right now so sorry for not linking sources.
Thanks. I'm not even going to try to use the UI framework from Haskell. Basically, anything I would do in IO in a Haskell-only program I'm expecting to do in Java here.
It's a bit sad GHC still has no decent support for cross compiling applications. JHC is still a bit immature. Something I haven't looked into is whether the new LLVM backend is going to make something like this possible? Has anyone else looked at this and is it applicable to the Android?
Well, there are two ways to approach this. One is to try to compile Haskell to Java bytecode; the other is to write bindings to the user interface stuff via JNI. It's always seemed to me like the latter is a better solution, for a number of reasons. First and most importantly, it means that you can write substantial parts of applications in Haskell pretty easily; we can already target all the processors in Android devices via existing GHC code (mainline, or from the existing iPhone port). The runtime system may need some work because the C library on Android systems is not the same as in targets like Linux. That, then, is a huge step forward. Frankly, I don't much care about building the user interface layer in Haskell. In fact, just building command line applications and running them from Java would be fine. If one of those command line applications were GHCi, I'd be in heaven. Second, it seems people are ignoring the really serious problems that are likely to pop up trying to generate Java bytecode from Haskell, which we won't be able to solve because the runtime system is at too high a level of abstraction. Having Java garbage collection, having to get all the thunk handling code to work in Java bytecode... Haskell to Java bytecode strikes me as being very likely the wrong level of abstraction.
I'm much more interested in when we'll be able to get this work back out of AMD, I'd love to play with it.
[0 questions tagged android haskell](http://stackoverflow.com/questions/tagged/android+haskell). Go for it!
I *knew* I had to learn ML one day after picking up a book on it, and it was pattern matching, foldl, foldr, and map that sold me. Not so much the functions themselves, but the fact I could easily write my own if those three didn't quite do what I wanted very naturally.
Done right here: http://stackoverflow.com/questions/5151858/running-a-haskell-program-on-the-android-os With my best attempt at humour thrown in too ... okay, I know I'm not funny.
&gt; the other is to write bindings to the user interface stuff via JNI. Do you know if anyone is currently working on this?
cool thanks! this is more or less what i ended up with, what i was getting at with the example was that in a language like c, i can just array[i-1] or whatever without thinking, but i do wonder how much of that is down to me remembering the pattern rather than being implicitly "easier".
this is interesting, thanks for the suggestion!
&gt; Something I haven't looked into is whether the new LLVM backend is going to make something like this possible? Has anyone else looked at this and is it applicable to the Android? Should be quite feasible just for one exception, you can not distribute native executables on the app store. It will probably work fine on a rooted phone just as you can compile native executables with the NDK gcc port but you can not deploy them on the app store. What is typically done with the NDK is you compile to a shared library for which a java program calls into. As I've mentioned in android 2.3 google provides the Java/C glue code framework so you don't have touch any Java/JNI but you have to either build your code as a shared library or statically link with the official glue code (which itself is a shared library itself). 
You're confusing the beginners with the community. The community uses Monads a lot, because they're so damn useful, but people aren't really emphasizing or excited about it anymore. It's old news.
It would be nice to have a structural code editor to edit Haskell. Then, we could have type-inference as-you-edit. Then we could have the editor automatically display general functions according to their inferred type. So the program will structurally be "fmap fmap fmap" in any case, but displayed as "fmap . fmap" (if that's your configured viewing preference, anyway!).
I agree that "instantiate an object" is a bad example of what an imperative programmer would think. But I also agree that it is beside the point.
How is `Any` defined?
I don't understand the benefit of mixing OO into FP languages. Haskell's type-classes and existential types give you everything possibly positive you can get from OO and then far more. Why would you want OO syntax and other artifacts thrown in?
The default matters though...
If you assume lazy I/O is non-deterministic, then lazy I/O means you cannot use real-world constructs (such as closing a file handle), and you do not lose referential transparency. In my opinion this approach is truly not usable, which is why everyone using lazy I/O eventually... assumes that lazy I/O *is* deterministic and exposes evaluation order. Most lazy I/O programs I've seen (that spanned more than 5-10 lines) *did* use operations like closing a handle, and so relied on the so-called "non-deterministic" behavior which exposes evaluation order and destroys referential transparency. So, you get to choose between keeping your referential transparency, and using practical I/O operations. Another horrible issue with lazy I/O is that the "unsafeInterleaveIO" tag does not propagate. The libraries that wrap unsafeInterleaveIO are also unsafe, but "unsafe" is nowhere to be found in their names. If everyone started calling it "unsafeGetContents" I think people would finally do the right thing and dump lazy I/O.
While I agree with you point that the current suite of tools may suffice to make apps for Android the whole getting onto the JVM is not strictly speaking because of Android. The JVM itself is a problem for Haskell or any functional language (there was a good talk on F# and what MS had to do to get with their VM to be nice as a host). The political reason for this is that a lot of companies are looking to turn away from Java but still has a large code-base that they would like to either convert or integrate with. This has already given Scala a boost and I think this could be an interesting vector for Haskell to pull in more people from the mainstream. I am dreaming though in many ways...
Probably not, since there isn't a widely available way to build Haskell for the native part of the Android operating system, except for jhc, which is not really ready for serious use. I've heard rumors that there's a port in place that works... but I haven't seen a public release of it.
For the record, I've also created the *#hakyll* channel on [freenode](http://freenode.net/). Discussions and questions welcome...
Deep magic: http://darcs.haskell.org/ghc/compiler/prelude/TysPrim.lhs
Thanks. As an Android owner and Haskell-curious programmer I'd be interested in becoming part of an open source effort but don't have the background to start it. I assume if one moves above ground it will be posted to this subreddit eventually.
Once you get used to functional programming, writing a function like `mapWithPrev` is more intuitive than a for loop. It's also easier to see the corner cases (the empty list and singleton lists), and you only have to write `mapWithPrev` once. It is easy to write a for loop that doesn't handle the corner cases correctly, and you have to handle the corner cases every time you write that for loop.
I don't think you're really disagreeing with my statement, since "the usual techniques" of imperative programming do not show us "how to think clearly &amp; rigorously about 'real world applications'". Maybe you read the "we" in my remark as meaning Haskell folks, while I really meant it much more broadly, as computer scientists and software developers.
We have an IP -- it's called the IO Monad, and the things that we need an FFI for are to bind to existing APIs already provided in C -- i.e. posix functions &amp; such.
It's being worked on. A couple of people (me included) are interested in seeing GHC become a cross compiler and are trying to work on it (it's just a really really really big amount of work) - join #ghc and help if you'd like. :) RE: LLVM, in theory, yes, in practice, not at the moment. The issue is that while LLVM bitcode can feasibly be generated to ignore things like endianness in the target ISAs, in practice, the actual bitcode generated has dependencies on things that are included at compile time: such as the system headers, which define macros and values and whatnot based on the target platform (this is not just a problem for GHC, but also e.g. Clang - it can output bitcode, but that bitcode was created from input which at compile time may #include specific things for say, x86_64.) Furthermore, GHC uses a custom calling convention in LLVM to generate code for STG machine (so it knows where certain STG registers live on entry/exit to the GC, for one,) and this calling convention requires support and cooperation between GHC and LLVM itself. Currently it is only implemented for x86_64 and x86, and to extend that to ARM will require modifications to LLVM, and coordination with GHC to make sure the calling convention works and is what the RTS etc expects. Currently, there's no support for ARM at all in the RTS or anything. That would need support before anything else (mainly the register mappings for native code from my understanding.) It'd be nice to work on a true ARM backend for GHC now that the new code generator is in and proper cross compilation is being worked on, but that's still a little ways off. ARM support in LLVM has improved a lot in the past while, so it would be quite feasible to make LLVM work for this as well if we can get it to support the GHC calling convention on ARM. Stay tuned!
Even with ghc-llvm working on ARM, I don't think we can officially distribute native executables through the app store as I've mentioned earlier. It would need to be able to build shared libraries that can be used by JNI.
What I read in to your remark (that you didn't really write) was that it is more difficult to write "real world applications" in Haskell than traditional language because Haskell forces you to think clearly. This is what I disagreed with, since I think the same muddy thinking used in imperative languages can be used in Haskell. And it often is. BTW, will I see you next week? 
The second one. I would change the associativity of ($), get rid of the fail method of Monad, bring Applicative in and fix up the subclass constraints for the Functor hierarchy, remove the Eq and Show subclass constraints from Num, consider generalising map and (.),.... More advanced features of the language are changing at a reasonable pace already, so there'd be no need to exert my will to change them. On a large scale, Haskell is slowly moving toward being a dependently typed language. It might be worth a revolution to force the issue, but I think the current approach of incremental steps toward it while trying to maintain a high degree of type inference is fine.
I'd want some way to define a package that defines instances for a data type in one package for a class in another package that would automatically be imported if either was brought into scope. This would help reduce the 'onion'-like structure of cabal, where later packages have to deal with the incidental complexity of either defining all the instances for the libraries that came before them, and thereby bloating themselves to unusability, or splinter off a bunch of dangerous orphan-instance-filled packages. A package writer who defines a new class or data type must either currently accept orphans or wind up screwed into accepting every type system extension that any package below them in the onion has felt was indispensible. Related to the former, I'd want to be able to retroactively introduce superclasses without breaking every instance or consumer. This makes the current class hierarchy very rigid as it stands. This leans in the direction of one of the various class alias/superclass partial definition proposals. This would fix the asymptotic amount of code you have to write, and enable you to build deeper class hierarchies with fewer repurcussions. The class system Haskell has now favors a few abstractions with a good power to weight ratio, at the expense of accuracy. With those two painfully expensive changes, a lot of existing hemming and hawing over competing concerns bleeds away. With a fixed class system enabling you to use something other than black box composition, I'd want to revisit the Prelude. I have a series of packages that provide for a number of semigroupoid-like structures, (things like monad without return, Applicative without pure, etc), but with the above, it would be dramatically more concise, and you lose the counter-argument that it creates a few more lines of work for library writers. Similarly the numeric tower could be fixed, because moving deeper in a class hierarchy no longer causes you to increase the amount of work linearly in the number of superclasses, but instead gets to reduce the amount of stuff you have to write proportionally to the amount of extra information gained from the new laws available to you as you go deeper. Finally, I'd bury the existing Arrows in a shallow unmarked grave, and build up a new quiver of them off of cartesian closed categories, or semi-cartesian closed semigroupoids, which are topped by Arrow and their ilk. I have these in category-extras/categories, but because you have to define a dozen superclasses, it is not really viable to expect the end user to do this. Then I'd flip the associativity of ($), add bifunctors and comonads to the standard library, and then probably take a nap, because that sounds like a very long day. ;) I would however, want to stop shy of replacing (.) with fmap -- just moving Category into the prelude would be better from a principle of least surprise perspective -- or replacing flip with f (a -&gt; b) -&gt; a -&gt; f b.
My comments mainly apply to GHC, since it implements the most extensions to Haskell: What I'd really like is a re-organisation of some of the core libraries. For example, it would be great if the containers library used type classes and type families to automatically pick the most appropriate mapping type (currently you have to manually pick an IntMap, but there's no reason to ever choose to use a plain Map with Int keys that I know of). Admittedly, you could easily write a wrapper around containers that did this. But it's not just this. I'm not a Haskell expert by any means, but there are large parts of the standard library that seem like they could easily be generalised or made more flexible, especially if the various extensions to Haskell 98 are used. Another annoyance is libraries that are tied to specific text representations. We have Strings and then various types trying to fix the problems with Strings, like ByteStrings and Text. Then we have lots of libraries which are tied to exactly one of these, most commonly ByteStrings, and to get them all to work together you have to stick explicit type conversions all over the place and/or pick the same named function from a number of different modules. I really, really don't like it when you have multiple modules implementing the same interface and you have to import several of them qualified and manually pick the right one. It makes it harder than it should be to write general code over the different types, when a function might work equally well with anything that's fundamentally a sequence of characters. This could partly be fixed if the standard Prelude generalised the list functions to work on sequences in general, and / or even moving text functions into a specialised type class for text types. And if type classes aren't an appropriate solution, there ought to be something - repeating yourself unnecessarily is bad, but it seems like a habit in some parts of the Haskell libraries. I know that there are some attempts to generalise, like the Foldable and Traversable type classes, but they aren't used enough in the standard prelude and thus come with their own annoyances, such as the fact that not everybody uses them. Not that I don't love using Haskell, I just really don't like a missed opportunity to generalise. And these kind of things really have to live and be widely used in the base set of libraries, because if they don't then they have a hard time gaining widespread usage, which then means that you still have to work around other people not using them.
Actually, there is a reason to use Map Int. IntMap.size is O(n), Map.size is O(1) ;)
I really like the idea of [constraint synonyms](http://tomschrijvers.blogspot.com/2009/11/haskell-type-constraints-unleashed.html).
I'd like eager/strict language with type classes and effect system, without any monads.
Make `String` and `FilePath` types abstract.
Hacking on DDC?
I'm sorry I don't understand this one? Do you mean don't make it so that they are interchangeable? 
Giving credit where it's due, Ben Gaster has a much longer history around Haskell than I do, and is responsible for much of the direction of the work I did and any attention it's received within AMD.
I think he means so that `String` is not a synonym for `[Char]` which is incredibly restrictive.
Well, we want a couple things: * `[Char]` has an overhead of about six hundred megabytes per string, and it takes O(N) to stick something on the end. It's the first thing to get ditched if you're optimizing a program. * `FilePath` is, well, not as clean as it should be. On Linux a path is a byte string, on OS X it's a Unicode string, on Windows the component separators are different. If it were even just given a `newtype` wrapper we wouldn't have to worry so much about it as the interface can enforce various constraints. * A function like `writeFile :: FilePath -&gt; String -&gt; IO ()` takes two arguments which *intuitively* seem like they should be different types.
I'm a bit out of the loop at this point. When I left Sunnyvale, the hope was that the Haskell bindings would be released, but there were some unresolved issues (in particular, a memory leak) that I was hoping to track down. There are also some mildly unpleasant performance issues that I was hoping to abstract around, but hadn't had a chance to look at yet. Unfortunately, I haven't had time to really revisit either point since returning to my work at Portland State. Ben's Template Haskell work has all been done since I left; I assume that he's also interested in getting that to the community.
But could you honestly call it Haskell if it weren't lazy and didn't have monads?
Extensible records, polymorphic variants, maybe ML-style parametrized modules.
&gt; This is very cool but does anybody know if it is indeed possible to get rid of the required initCL function? Ben's been rid of it since those slides were made.
We're on the same page here: Haskell embraces non-denotative programming in the form of `IO`, and so welcomes muddy thinking. Which is why I'm encouraging a revival of Peter Landin's recommended term "denotative programming" as a replacement for the fuzzy term "functional programming". Yes. I'll see you in Texas next week. Looking forward to it!
I've implemented Talpin's work (type and effect discipline) as a project for tapl classes, and currently I'm trying to rewrite and extend it. it's not going very fast - 8h of python a day is much more interesting (or so I keep telling myself). but if I had that much money and influence (as OP stated) I'd force all those people writing monad papers to go and help Ben Lippmeier.
Funny. I would add Comonad to the core libraries, of course featuring a win method.
Without monads? It's a very useful abstraction -- why wouldn't you want it? Besides, Monad is just a library. Do you want a language that isn't powerful enough to write that library?
&gt; I'd like ... without any monads. List, function, &amp; pair-with-monoid are all monads. You wouldn't really want to eliminate them, would you?
Yes!
&gt; It's a very useful abstraction it's not. there's only a handful of monads and that doesn't change the fact, that you use only one monad - the father of all monads (IO+State+Exceptions). have you ever used probability monad? there's a whole module (Control.Monad) of functions, that work for free with this monad, yet I've never needed it. let's take more basic example - list monad - the primary monad people learn when studying haskell, again everybody prefers using list comprehensions or regular functional style (map+filter+fold). there's a reason nobody's using prolog - it's awkward and people don't want to code in that style. so at the end of the day you write imperative code in the one true monad, and that code needs a lot of boilerplate in haskell, and it's still a bad substitue for proper effect system. &gt; Do you want a language that isn't powerful enough to write that library? of course not, I want a language that makes writing regular code easy. I still want this language to allow people like Oleg or edwardkmett to write abusive code for fun.
Fix records.
600 MEGAbytes, really? Do you have a reference? 
I (and probably a lot more people) was drawn to haskell because it's a great functional language (much better syntax than ocaml), it has a really great type system (typeclasses allow you to write print method/function, ocaml people still talk about queue module parametrized over comparable type - I use print every day, can't remember the last time I've use queues). and it's pure - which makes me feel safe. these are the things that I'm excited about haskell. monads are just a step that must be taken to work in this world. and laziness stops being so exciting after you realise that you can only use "zip [0..]" so many times a day to sacrifice so much in return.
you know what I meant. I'd prefer monads to be use the way they're use in ocaml - yes, it's possible and we sometimes use it, we do have macros to make it easier, yet we do it so rarely, that it's not something that needs two chapters in every book.
&gt;there's only a handful of monads and that doesn't change the fact, that you use only one monad - the father of all monads (IO+State+Exceptions). I certainly don't. &gt;have you ever used probability monad? Yes. &gt;there's a whole module (Control.Monad) of functions, that work for free with this monad, yet I've never needed it. Well I have, and I am sure many others have. You don't have to stuff all your logic into The One Monad Onion To Rule Them All, you know?
do you have any useful examples of functors? the only one I've seen is using lexers in parsec.
Add a sane implementation of session types, and I'm good to go!
Yep, right [here](http://en.wikipedia.org/wiki/Hyperbole)
where have you used probability monad? (this is a bonus, offrant question). even the fact, that you did use probability monad, doesn't change the default situation of boilerplate everywhere. I do not argue against having Control.Monad, do-syntax (though I'd prefer something like camlpX instead of compiler builtin syntax sugar), I argue against requirng every day coding to rely on such tricks. I argue against people that think "f &gt;&gt;= g &gt;&gt;= h" in Maybe monad is so awesome, because Nothing autopropagates, and nobody remembers that "h (g (f x))" works the same way in every other language. &gt; You don't have to stuff all your logic into The One Monad Onion could you expand on that?
&lt;-- is far too literal.
That would still mean you could code the bulk of your app in Haskell which is fine by me.
*cough*
laziness let's you be far more expressive than just "zip [0..]"
so does John Hughes keep telling me. but I'm not interested in writing sudoku solvers and project euler problems. I want a perfect language for writing scripts to download things. give me one, and I'll be able to drop my interest in plt, stop reading reddit and start coding.
If you program in IO+State+Exception you are doing it wrong. 
As an aside, gist.github.com doesn't have adds and supports haskell syntax highlighting :D
IO+State+Exception is unavoidable, wouldn't you agree? 
when people say they don't want monads, they means they don't care about IO being typechecked
I'd like to see `if&lt;-` and `case&lt;-` constructs a la Habit, so that we can get rid of all the boilerplate that comes from binding a name just to scrutinize it immediately.
Also, a good deal of things in `Control.Monad` should be reexported from the Prelude. At a bare minimum `liftM`, `(&lt;=&lt;)`, and `join`. And in the past I've definitely wanted `MonadPlus(..)` on that list.
Type and effect systems cover a set of concerns that only slightly overlaps with what I use (co)monads to manage. I'm not sure that any people are actively writing any papers on the topic in general at this point -- pretty much everything worth saying in the abstract has been said. Maybe Russell and his beloved store comonad, or my comonad-transformers still count. I've already implemented one substructural region-based type-and-effect system, but hated working with it, so while I'm interested to see what Ben comes up with, I have little desire to hack on or use DDC at this point. ;)
I spend the vast majority of my time in other monads.
Clearly the dual of fail should be succeed.
`HERE DOC`s, a reasonable collections interface (which allows me to `map` over `ByteString`, `Set t` and `[t]`), type-based name resolution (related to the previous point), cleaner meta-programming or staged compilation, overloaded list literals. In fact, putting some real thought into overloading of numbers, strings and list/collection literals would be great. Overloaded literals I've found to be very helpful when working on programs that generate programs. Overloading `0` should not require me to define or punt on implementations of `+`, `-`, &amp;c. JAVA integration for those times when you want to write once and test everywhere. 
Must second this :)
Laziness to me is valuable because of the number of things that collapse into one thing *syntactically* in a lazy language. In a lazy language you can push anything you want into a where clause, drop the value-restriction, and it is easier to justify the relaxation of the distinction between let and letrec because the order of effects doesn't matter. It lets me reason about algorithms in isolation and let the forcing of thunks merge them together with asymptotically optimal interleaving. In exchange I pay the small tax of having monadic syntax, to be explicit about where and how I choose to order my effects gladly. zip [0..] is just a cute side-effect.
um, in java and most other languages, `h (g (f x)))` where `x` for instance is null isn't `null` but rather `NullPointerException`. So, yeah.
I just realized, by the way, that for another set of applications of `size` (name generation) you can use `findMax` instead and get O(log n).
And the type? `w a -&gt; String` ?
Writing scripts to download things is if not a solved problem then a *very nearly solved enough* problem; start coding.
I don't think any serious Haskell user would agree. I use the monad functions all the time; no offence, but it sounds like you haven't quite got a hold of exploiting the higher levels of abstraction monads can offer yet.
Honestly if I was given free reign over the language, I'd add dependent types and it'd go downhill from there. But **add an ML-style module system**, flip the associativity of ($), reorganise the stdlib, make some functions more general a la Caleskell, and fix Num or at least make it better. That would be good.
Enforce that for all core types, read/show held *and* that anything produced by show was valid Haskell code. Add another typeclass for pretty printing (just to avoid abuse of show, not for any principled reason), and put a prettyshow function as part of show that printed the same nonwhitespace characters as show, just formatted nicely, perhaps with multiple lines. A fully extensible typechecker with two initial plugins -- units of measure done right, and extensible records with proper subtype inference. An API that allows runtime compilation/linking/loading with low overhead. Optional linear types. (edit: I actually meant uniqueness types, a la Clean).
Okasaki's data structures library is much neater with ML functors than even associated types, I'm given to understand. Lennart also had a good example a while ago: http://augustss.blogspot.com/2008/12/somewhat-failed-adventure-in-haskell.html Eventually a decent Haskell solution was proposed (see followups), but still..
It might be helpful to release the work (under an open source licence) so that others might be able to help you figure out some of these problems.
The key enabler for pushing computation into a where clause is purity, not laziness.
&gt; `liftM` Rearrange the monad hierarchy like everyone keeps saying and you can use `fmap`. Speaking of which (sort of): why does `Control.Applicative.liftA` exist? It even *is* a subclass of `Functor`… 
If we didn't have monads what would we write all our tutorials about :)
I'm curious if you have a proposal for "fix Num" that doesn't double the number of built-in numeric type classes. It's certainly not theoretically sound in any way, but I've always found it to be a strong argument for the current system that everyone who sets out to design an alternative ends up with about 20 type classes just to describe numeric types. *edit*: Oops, this comment seemed to end up in the wrong place. Sorry!
&gt; Speaking of which (sort of): why does `Control.Applicative.liftA` exist? It even *is* a subclass of `Functor`… Two likely reasons: 1. because of the existance of `liftA2`, `liftA3`, people would probably expect `liftA` to exist. 2. If you are implementing `Applicative` for a new type, you can write the `Applicative` instance first, then use this for implementing `Functor`: --- instance Functor MyType where fmap = liftA 
That's like saying everything runs on your architecture's machine code so you should just program in that. We use abstractions like monads to avoid polluting code with unrelated concerns and to factor out common patterns. So yes, `main :: IO a`, but who the fuck cares? I push my effectful code as close to it as possible and program everything else with other monads, pure code, or who knows what other abstractions.
And a pony? :)
Re 2: Sorry, I thought `liftA` was just an alias for `fmap`, not a default definition, for some reason. Brain fart.
I'm have some really off the point questions about the [category-extras](http://hackage.haskell.org/cgi-bin/hackage-scripts/package/category-extras) package. Firstly, where do you go to learn about category theory math? I've never learnt it during uni (since I did a software engineering degree rather than a computer science degree). I've also never found a good online resource that teaches this stuff. To give the level of knowledge I have in this area, here is one sentence out of the category-extras documentation about the class PreCartesian, with the phrases I don't understand bolded: &gt; NB: This is weaker than **traditional category with products**! That is **Cartesian**, below. The problem is (-&gt;) lacks an **initial object**, since every **type is inhabited** in Haskell. Consequently its **coproduct** is merely a semigroup, not a monoid as it has no identity, and since we want to be able to describe its **dual category**, which has this non-traditional form being built over a **category with an associative bifunctor** rather than as a **monoidal category** for the **product monoid**. Note that I'm not asking you to explain that, just know I need to learn so that I can understand it myself. Secondly, how is this package, and stuff like bifunctors and comonads useful? Is it only useful when dealing with mathematical problems, or it is a more general purpose library (like `Data.Monoid` and `Control.Monad`). If he latter, do you have any real world examples where using this library makes the code simpler? 
No, I would not. Out of the last 100 kloc if Haskell I've written, none is in that monad. Maybe 5% is in IO, probably less. Some is in State+Exception, some in State, but a lot is not monadic at all.
&gt; Make String and FilePath types abstract. +1
 f _ = "YEAAAAAAAH"
Let's not forget [class aliases](http://haskell.org/haskellwiki/Comparing_class_alias_proposals), originally proposed by [John Meacham](http://repetae.net/recent/out/classalias.html). It's on Luke Palmer's list of [Haskell's Big Three](http://lukepalmer.wordpress.com/2010/03/19/haskells-big-three/).
&gt; ...a .cabal file ...At ten source files you should have one. Forget about ten source files. Get in the habit - the way you compile Haskell is cabal init cabal install It only takes a few seconds. Even for throw-away hacks, you'll be sorry later often enough if you don't.
The interesting part is that all the terms you emphasized are based on really simple definitions. The naming choices though obscure the meaning. Related in spirit: http://www.reddit.com/r/math/comments/fus25/hitler_learns_topology/
I'd remove if/then/else from the language and add the functions bool :: a -&gt; a -&gt; Bool -&gt; a (analogous to maybe and either, with useful argument order in many many cases), if :: Bool -&gt; a -&gt; a -&gt; a (just reordered to read nicely in some other cases), and ifM :: (Monad m) =&gt; m Bool -&gt; m a -&gt; m a -&gt; m a (and perhaps by analogy ifM_). There are others, but most of them have been mentioned here already. Also, I suppose it is worth pointing out that there are several here I wouldn't like to see.
You want $ to be left associative? Really?!?!
&gt;where have you used probability monad? Doing some kind of statistics. I can't remember. I am a mathematician, and I work in pattern recognition and machine learning. I use that kind of stuff all the time. &gt;I argue against people that think "f &gt;&gt;= g &gt;&gt;= h" in Maybe monad is so awesome, because Nothing autopropagates, and nobody remembers that "h (g (f x))" works the same way in every other language. You have obviously never worked in Java. `foo.returnsNull().doStuff()` throws a RunTimeException. `null` certainly doesn't propagate automatically. If you use only static methods, which is bad form, `null` might or might not propagate. &gt;could you expand on that? This has been commented on elsewhere, but the idea is to separate the pure and effectful code.
You could try and take a look at the first chapters of [this](http://www.google.com/url?sa=t&amp;source=web&amp;cd=4&amp;ved=0CDQQFjAD&amp;url=http%3A%2F%2Fwww.cs.unibo.it%2F~asperti%2FPAPERS%2Fbook.pdf&amp;ei=1gBuTb3FENKHhQfv96WPDA&amp;usg=AFQjCNELMS8EgoDF1qetg9-t85bO_NjEeg) (warning: PDF), there are dozens of introductions to category theory out there. Wikipedia even has decent coverage of the basic concepts.
&gt; On a large scale, Haskell is slowly moving toward being a dependently typed language. I say just make the intermediate language the [Calculus of (Inductive) Constructions](http://en.wikipedia.org/wiki/Calculus_of_constructions).
Allow for the control of typeclases instances' scopes (as a bonus: allow for typeclasses dictionnaries reification) as in scala.