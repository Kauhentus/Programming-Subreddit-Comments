I symphatize with this, because for some some expressions like sorted list it happens that English and Haskell are indistinguishable. You are also right about the nature of function names, this is why the often cited essay "Execution in the Kingdom of Nouns" is mistaken when it describes Haskell as a land of verbs. No! (pure) Haskell expressions do nothing, they just describe what is! (Now this reminds me on that old joke: The lesser known programming languages - SARTRE. SARTRE programs do nothing they just are. ....)
Frankly I like the PVP's supermajor `A`. It gives you an extra axis of control. The major number isn't good enough for this kind of control because you're forced to bump it any time there is a backwards-incompatible change. The supermajor is a number that maintainers have complete discretionary control over. Here some possibilities of ways I can think of to use it: * `A` = 0 represents unstable/experimental/more-prone-to-change for some package-specific definition of what that means * New `A` represents particularly large breaking changes, possibly requiring significantly more work to upgrade to * Each `A` represents some kind of official release with a commitment for longer term support (i.e. continued security and bug fixes, etc) I'm sure there are other useful schemes.
For starters, ghcmod still doesn't work with 7.10
Very good point, thank you. What you said should be included in the PVP!
So the O(log N) factor bites you at several levels (physical layout, memory hierarchy, and just plain indexing). However, let's focus on just indexing since that is the easiest one to explain even though it has the smallest constant factors. In order to index into an array that has N elements you must provide an index that is O(log N) bits. That means that in order to actually look up the element you must minimally scan O(log N) bits of the address the user provides. The only reason you don't notice this time complexity is that on a single machine `log N` never exceeds 64 bits, but asymptotic time complexity has to consider numbers larger than what fits on a machine. These O(log N) bits are analogous to the O(log N) binary decisions that you have to make when descending into a binary tree.
Is syntastic a vim plugin?
Nothing wrong with ghcmod-vim. It was ghcmod that didn't work correctly because I had GHC 7.8.4 and a version of cabal-install that was too new (if I remember correctly).
Ultimately you're not asking anything about functional programming here. You have left no way to construct an array at all. _All_ of those ways require a form of mutation. So effectively you're asking whether or not a transdichotomous RAM model can execute an algorithm faster than a transdichotomous pointer model (with some extra limitations on the transdichotomous pointer model to disallow mutation.) The answer is, boringly, yes, before you even add the "functional" part into the mix. That said, just having `accumArray` in your program doesn't mean your system is any less functional. It just happens to be able to cheat behind the scenes and offer you an O(n) time solution whereas the "pure functional" version of an array either pays a logarithmic tax for all operations, or [plays games](http://www.cs.ox.ac.uk/ralf.hinze/publications/ICFP02.pdf) to trade off worse update costs for cheaper access or vice versa. You can't semantically tell which I'm using, so I can substitute in the faster part without changing your semantics. This isn't the same as admitting all mutation. It is a very limited form of computation, `accumArray` is still a function. Regarding radix sort: _You are building an array of n buckets._ This takes O(n) time! I can build an array of n buckets, too, that just happen to hold linked lists instead of counters in the _exact_ same time. Radix sort is no slower than the counting sort you offer up in the imperative setting under a transdichotomous model. It touches exactly the exact same number of buckets you do. It just writes in (x:) instead of (1+). Going back through afterwards to replace those lists by their length is also an O(n) operation. Unlike with a counting sort, I could _choose_ to do this radix sort in multiple passes, to use fewer buckets, but that isn't required to match your asymptotics _exactly_ in an imperative setting where array mutation is allowed.
Yeah, that's right. I think the `traverse` is not supposed to be there. I'll fix my parent coomment.
I actually implemented that algorithm in C and it appears to be logarithmic with respect to the number of bins. I used a length of 1,000,000. Also, [Data.IntMap](http://hackage.haskell.org/package/containers-0.1.0.1/docs/Data-IntMap.html) uses big-endian patricia trees, which have an update complexity of O(min(bins, word size)), which seems close to what you want. #include &lt;stdlib.h&gt; #include &lt;stdio.h&gt; #include &lt;time.h&gt; int main(int argc, char **argv) { srand(time(0)); if (argc == 3) { int max = atoi(argv[1]); int length = atoi(argv[2]); int *arr = malloc(max*sizeof(int)); for (int i = 0; i &lt; length; i++) { arr[rand() % max]++; } // Print the result to keep it from being optimized away printf("%d\n", arr[0]); } else { printf("%s &lt;max&gt; &lt;length&gt;\n", argv[0]); } } ---- ints time 1 0.02 2 0.018 4 0.014 8 0.013 16 0.012 32 0.02 64 0.013 128 0.012 256 0.013 512 0.013 1024 0.01 2048 0.01 4096 0.01 8192 0.01 16384 0.014 32768 0.02 65536 0.021 131072 0.016 262144 0.015 524288 0.014 1048576 0.021 2097152 0.039 4194304 0.048 8388608 0.062 16777216 0.061 33554432 0.083 67108864 0.176 100663296 0.243 134217728 0.22 201326592 0.392 268435456 0.407 402653184 0.627 536870912 0.673 805306368 0.906 1073741824 0.955
Basically every situation. Suppose I'm typing `map f (i`. If `f :: String -&gt; Int`, then obviously `isJust :: Maybe a -&gt; Bool` is not a valid auto-complete suggestion at this point. On the other hand, `id` would be. `intList :: [Int]` would not, `inventory :: [String]` would, `init :: [a] -&gt; [a]` would, `instance` would *most definitely* not. Notepadd++ would suggest all of these tokens and more, however.
This is indeed very interesting. Is there any tool in existence that can do that ? I wonder.
The typeclass approach has the advantage of guaranteeing that the result is an instance of Run, which could be (haven't checked) equivalent to saying "this is an instance of functor, monad, etc" which guarantees many properties. I think that is an advantage of the typeclass approach. Would you agree or do actual functions have an advantage here I don't see as well? Have you seen [Next Level MTL](https://www.youtube.com/watch?v=GZPup5Iuaqw)? I used to think using typeclasses instead of functions was too magic and not worth it until I watched this video. What are your opinions?
I only point this out because then you can immediately infer the relevant functor laws. Using your `MonT` functor as an example: MonT id = id MonT (f . g) = MonT f . MonT g ... which is in fact how the `Category` instance is defined for `MonT`. Calling it a "functor" is a convenient way to summarize that.
I just use this [.dir-locals.el](https://github.com/commercialhaskell/stack/blob/master/.dir-locals.el) and it works flawlessly.
At least in a purely functional _strict_ setting it is known that some algorithms necessarily take a logarithmic factor longer, [Pippenger showed this back in 1996](http://www.cs.princeton.edu/courses/archive/fall03/cs528/handouts/Pure%20Versus%20Impure%20LISP.pdf). That said, call-by-need is a limited form of mutation, so his proof doesn't apply to Haskell. Regarding the split between pointer and random access models: There are algorithms such as interval union find for which there is a known hard Ω(α(m, m + n)) bound for all separable pointer algorithms, but for which there is a known Ο(1) amortized algorithm by Gabow and Tarjan in the RAM model.
Your problem is O(m) (lenght of the list)
A nice lecture on linear-time sorting (from MIT OCW): https://www.youtube.com/watch?v=Nz1KZXbghj8
Yeah, I'm aware of Pippenger's result. His counterexample isn't very natural, though. I'd be surprised if something as simple as my problem turned out to be a counterexample, and in the more powerful laziness setting to boot. That's why I decided to ask /r/haskell. Oh well, it looks like there's no known trick at least. Thanks for the help!
&gt; Can you observe that fact? No I think the OP question is on a theoretical point of view. Using a mutating array under the hood is answering the question.
&gt; Does such a solution exist? Unfortunately, no, there is no solution which will satisfy you. But be warned: you're not going to like my explanation of why that is the case. There *are* pure implementations which match the complexity of the imperative implementation. But there are no solutions which satisfy those criteria and which *in addition* will satisfy you. Proof: we already gave you great answers which received many upvotes, but you claimed they were not "in the spirit" of your question. I can only conclude that the question you are asking is not the same as the one we are answering, that there are some extra requirements hidden in the phrase "pure in spirit, not just in letter" which do not match the interpretation of "pure in spirit" which everybody else here (the commenters and the upvoters) have. In particular, you say that the simplest implementation would probably use "a functional array with logarithmic update time". What kind of functional array are you thinking about? Some funky "fibonacci tree" or similar structure offering logarithmic insertions and updates? I wouldn't call that a functional array, but a functional sequence. [`Data.Sequence.index`](http://hackage.haskell.org/package/containers-0.5.6.3/docs/Data-Sequence.html#v:index) and [`Data.Sequence.update`](http://hackage.haskell.org/package/containers-0.5.6.3/docs/Data-Sequence.html#v:update) are both logarithmic time. A functional array, in my opinion, would be an immutable data structure offering O(1) insertions but O(n) updates, due to the fact that the entire array has to be copied. [`Data.Vector.(!)`](http://hackage.haskell.org/package/vector-0.10.12.3/docs/Data-Vector.html#v:-33-) is O(1) and [`Data.Vector.(//)`](http://hackage.haskell.org/package/vector-0.10.12.3/docs/Data-Vector.html#v:-47--47-) is O(n). I believe that also applies to [`Data.Array`](http://hackage.haskell.org/package/array-0.5.1.0/docs/Data-Array.html), the de facto definition of "functional array", even though the documentation doesn't specify any complexity bounds. Note how the second argument of `(//)` is a list of (index, value) pairs, not a single pair. That's because updating the elements of an array one at a time, at a cost of O(n) each time, would be hopelessly inefficient, which is why the API is encouraging batched updates. This way performing `n` updates in one batch is a single O(n) operation, the same cost as `n` consecutive O(1) updates in an imperative language. At this point, I'm sure you're preparing to say: "aha! This batched update is implemented imperatively, and hence this answer does not satisfy the spirit of the question". I disagree: mutable arrays are imperative, and immutable arrays are pure, whether they implement batched updates and `accumArray`-style updates or not. Proof: many imperative languages offer immutable strings, but how many offer immutable arrays? All this to say, if you want to disallow functional arrays, you should ask something more specific than "pure in spirit". Here's a question which I think you might be trying to ask: is it possible to implement an efficient version of `histogram` in a language in which every structure allocated has a fixed size determined at compile time? For example, a cons has size 2, a data structure with `k` fields has size `k`, and so does a function which has access to `k` variables, but you can't allocate or access a structure of size `n` for a `n` which is obtained at runtime. I think the answer to that question is no, but finding an impossibility proof will require a bit of work.
as far as I can see, Run is just used as a global method dictionary. the result isn't an instance of Run, Run is just used as a bag to throw in the functions you want to use. I do agree that there are advantages of making things polymorphic over typeclasses, but I don't think this is the case here
well, it's the case for enough monads to make the link not very meaningful :) 
For the lazy, here's a plot of the points: http://fooplot.com/plot/mhxo5mw2bw
Alright thanks. I'll take a deeper look later as well.
[yup](https://github.com/scrooloose/syntastic)
On Windows: - Sublime Text 3 - SublimeHaskell [hsdev branch](https://github.com/SublimeHaskell/SublimeHaskell/tree/hsdev) - SublimeRepl - hsdev, hlint, stylish-haskell
* Emacs with `haskell-mode` * ghci * GHC for Mac OS X / hvr's GHC PPA * Stackage (Stack soon probably, which obviates the former list item *and* Stackage) 
I never thanked you for explaining your viewpoint. If you end up writing blog posts about your project, you should definitely post them to this sub.
So the following implementation of a different problem would have satisfied your criterion even though it batch-updates the elements of an array? import Data.Array -- | O(length xs). -- Returns the elements of xs in sorted order, discarding duplicates. -- The elements must be in the range [1..length xs]. -- -- &gt;&gt;&gt; sortUniq (listArray (1, 4) [3,1,4,1]) -- [1,3,4] sortUniq :: Array Int Int -&gt; [Int] sortUniq xs = [i | i &lt;- range (bounds xs) , bitfield ! i ] where -- True if the key is present in xs. bitfield :: Array Int Bool bitfield = listArray (bounds xs) (repeat False) // [(i, True) | i &lt;- elems xs] 
Thank you for chiming in, and I'm sorry I was so hard on your response here. You're a central figure in GHC development, so a thumbs down from you is a big deal, while a pat on the back is a powerful bit of encouragement for anyone who wants to contribute. While I appreciate your desire to inject some gray, I submit that setting things out as check boxes, say, rather than "reasons against" would be a much less discouraging way to indicate that there are issues to work through. Vibrant open source projects are in large part vibrant because the core team is committed to metaphorically high-fiving every passer by so that everyone feels valued. That kind of thing isn't a big part of the GHC culture, but inadvertently pushing people away doesn't help with the lack of manpower problem.
+ **vim** : I probably tried all the plugins available but don't really use any. (I disabled ghc-mod and syntastic the other day, never managed to make them inside a sandbox). I don't use syntax highlight but instead a tweak configuration of vim-rainbow which colorize the full block instead of just the parentheses. + **tmux** + **ghcid** I discovered it recently and I love it (it's blazing fast) (when I don't forget to use it) + **cabal** sandbox + **google** to browse package on hackage. I wish I could browse the doc (in textmode) straight from vim. I think that's what `codex` but I haven't had time to try it. What I'm missing : having vim configured properly to jump/show to type/function definitions `]i`. It's probably just a problem of setting `path` correctly. Be able to have the type of an expression under the cursor when the file doesn't compile. 
For me, talking about cabal sandbox is inevitable, as I was using haskellmode-vim until I started using sandboxes. AFAICT, haskellmode-vim does not work with sandboxes, so now I use cabal repl with `:set editor vim`. (Most of my usage of haskellmode-vim was just checking types with `:GHCi`, so maybe ghc-mod would work for me?). I use the FPComplete-hosted hoogle every once in a while.
You make a good point contra what I was saying the other day :)
&gt; anyone reading your account of things would be more discouraged than someone just reading GHC tickets for themselves I don't see how you can extrapolate that. I was discouraged by goldfire's comment when I first read it several months ago, and I've been discouraged by the apparent attitude that nothing can ever be improved because something somewhere might break. The recent [import syntax proposal](https://mail.haskell.org/pipermail/haskell-cafe/2015-June/120035.html), [Native -XCPP proposal](https://mail.haskell.org/pipermail/haskell-cafe/2015-May/119528.html), and [the records library](https://mail.haskell.org/pipermail/ghc-devs/2015-January/008049.html) threads are good examples of this. I get the impression that too many people are eager to point out problems in order to make themselves feel like they're contributing (too many cooks, so to speak) rather than offer solutions to work towards a common goal. 
&gt; What happens when we accept that there is a problem here and try to do better? While this is very off-topic to Elm, I should note that there was a very exciting recent paper ["Counter-Factual Typing for Debugging Type Errors"](http://web.engr.orst.edu/~erwig/papers/CF-Typing_POPL14.pdf) describing machine-learning-based technique that would improve GHC for a better way to identify "real error locations", e.g. when you have a type-mismatch, make a better guess of an actual location where bug happened, not where compiler ended up when checking type-matches. Very interesting read! 
My apologies, I was trying to give an example which used laziness to defer the definition of the values of an array, but then I updated my example to use `(//)` instead of thunks, then I realized my example no longer used laziness, so I changed the description to say "even though it batch-updates", and now my comment and example no longer make any sense as a response to your "no mutation" request. Let me try again: So the following implementation of a different problem would have satisfied your criterion even though it uses laziness to defer the definition of the values of an array until after the array has been allocated? fibs :: Array Int Integer fibs = listArray (0,100) [if i &lt; 2 then 1 else fibs ! (i-1) + fibs ! (i-2) | i &lt;- [0..100]] *edit*: added missing base case.
ghci-ng and haskell-mode with proper goto-definition, get any subexpr type, quick type-checking and jump to error, and more are a life-safer for me.
Wow, I didn't realize Haskell could do that, though it's obvious in retrospect. It's kind of a borderline case, because listArray can't be implemented without mutating the array under the hood, but now I'm really curious if allowing listArray would help with the original problem. It seems like a form of mutation that's almost as nice as regular laziness.
I use emacs + haskell-mode, and no other bells or whistles. 
which part exactly should be added to the PVP text?
This would be awesome 
 $ sudo apt-get install hoogle $ hoogle "a -&gt; a -&gt; a" | head I also have this in my ~/.ghci, which lets me `:hoogle a -&gt; a` (no quotes needed) right in GHCi: :def hoogle \s -&gt; return $ ":! hoogle --count=15 \"" ++ s ++ "\""
afaik, there's a [threshold of originality](https://en.wikipedia.org/wiki/Threshold_of_originality) for a work to be copyrightable in the first place...
I use emacs with ghc-mod, haskell-mode, haskell-indentation-mode and company-mode. Also magit and haskell-cabal-mode. ido-mode helps with navigating large projects. ghc-mod is vital, working without it feels like having a hand tied behind my back. Seeing type errors without the need to compile and being able to inspect types under the cursor is vastly more efficient. I tried several haskell indentation packages for emacs, haskell-indentation-mode is the only one which hasn't broken on my yet. Others were quite brittle, notably failing when I used ghc syntactical extensions. I'm still experimenting with company-mode. It feels like black magic. Generally I have a browser window open to the haddock documentation in my local sandbox. This is still somewhat awkward due Cabal issues [#1465](https://github.com/haskell/cabal/issues/1465) and [#462](https://github.com/haskell/cabal/issues/462). It would be very nice if I could run a local webserver serving the documentation from all my sandboxes with actual working hyperlinks.
From a strictly CS perspective, they do in fact have the same time complexity. To see this, reduce to the absurd: Suppose you want to change just one of a long vigintillion items all contained in an array. You need a big-int for this index, which is slower than an int64. And an int64 on 64 bit architecture is slower than an int32 on 32 bit architecture of the same scale, the int64 requires more gates to do every option, meaning a bigger chip, meaning larger travel times, even if the gates are just as small. We treat them as constant, but that's only because we've applied a ceiling function to all int64s and made the smallest run every bit as slow as the biggest. So in the theory sense, the difference between an array lookup and bitwise binary tree really is a constant factor, just a big one.
Perhaps you want it to be a [retraction](http://ncatlab.org/nlab/show/retract) in `Cat`.
I've been switching from vim to emacs for Haskell recently, and one thing I really miss from my vim setup is having both ghc-mod and hlint run on every save. One day I'll get that back. It'll be a good day.
Any sort of any size in any existing computer is O(1).
That is a good perspective that I should probably flesh out. I've [asked on math overflow](http://math.stackexchange.com/questions/1346173/is-there-a-special-name-for-functors-from-a-category-c-to-a-subcategory-of-c) if there's a standard name for these sorts of functors, but there's not much response yet.
I can sort of see how this is related, but it seems like a very specific instance of a much more general concept. In particular, not all retractions would be what I've dubbed category transformers.
&gt; can you create immutable array "from scratch" ie without import just plain ADT no ffi etc? Of course not, you need support from the language. Just as you do in an imperative language: you can't construct arrays out of plain classes, you need `malloc` or `new[]`. In both cases, you need help from the language (or in the case of ffi, help from another language). &gt; About the tuple they creation is atomic, ie you can't see or use their "intermediate" state in Haskell code , therefore they can be seen as immutable. That is true of arrays as well. When you do `xs // [...]`, you are not mutating `xs`, you are creating a brand new array which shares no storage with the old `xs`. By the time you get the result, all the mutations have already happened, just like the mutations which happen during the creation of a tuple. &gt; If you can inspect an immutable array an update a slot even when creating it then you are mutating it. Are you talking about the `fib` definition in my reply to OP, in which later values are defined in terms of earlier values? I can do the same trick with tuples: fibs :: (Integer, Integer, Integer, Integer) fibs = (1, 1, fibs `at` 0 + fibs `at` 1, fibs `at` 1 + fibs `at` 2) at :: (a,a,a,a) -&gt; Int -&gt; a at (x,_,_,_) 0 = x at (_,x,_,_) 1 = x at (_,_,x,_) 2 = x at (_,_,_,x) 3 = x It's just the "limited mutability provided by laziness and thunks" which OP explicitly allows.
That should be easy to configure in emacs, though, I would imagine?
It probably is but at the same time it's so hard to tell with blogs as they tend to be all over the place, mostly banal or trivial and rarely, but it does happen, insightful.
I use vim with nothing special installed (configured to expand tabs to spaces, to maintain the indentation of the previous line, and to do some syntax highlighting and that's about it), and I typically just keep ghci running in a separate window alongside it, and a browser open to relevant haddock documentation. Really just about any text editor will do, so long as it can be configured to do the tabs to spaces thing, I don't even mind using gedit or gobby (though gobby's lack of undo is unfortunate, the multiplayer ability makes up for it, I suppose). I have used emacs a bit in the past, but I mostly found haskell-mode to be disruptive (especially its weird indenting shenanigans which got in the way more often than helping). The only reason I use vim over gedit is that I'm somewhat used to its keys for search/replace and go-to-line, and perhaps the fact that it runs in the terminal window I opened it from is sometimes a bonus. I use the arrow keys, and some vim users would hate the amount of time I spend in insert mode. The only thing I can imagine being useful as far as editor features go, but which to the best of my knowledge, no editor actually does correctly, is that any edit which directly or indirectly causes the position of the first non-whitespace character following a layout keyword (i.e. "let", "do", "of", "where", "mdo") to change, should cause the entire following block to move accordingly. That would save a small bit of time spent reformatting things occasionally. Everything else I've found pretty underwhelming and/or seemingly unnecessary. I guess jump to definition might be nice in some cases? I dunno, it's not something I've ever really worried about. If it's a library definition and I want to see the implementation, usually I'll just click the source link from its haddock. If for some reason there's no haddock, I'll either load the file it's in and use search to get there, or if I'm really that unfamiliar, I suppose I'll grep for it at the commandline. Sure, having a feature which does that for you is nice if you do it a lot, but I dunno, it somehow just doesn't bother me at all. Most of the cases where jump-to-definition will actually work seem like cases where you're likely to have a haddock generated for browsing documentation anyway. Being able to see the types of terms in the editor is another nice feature idea, but how useful is it really when you have ghci open right there, and can ask it yourself? It would be really handy in a handful of cases where you're working on local definitions with complicated types to be able to see the types that actually got inferred by the typechecker. The typechecker in my head usually suffices somehow though, especially given that ghci is going to check my work and will tell me what types things had when I go wrong. So I dunno, every time I installed things like ghc-mod, it ended up being like "well, that's pretty neat" while I played around with my editor for about 10 minutes, and then I proceeded to somehow avoid any circumstance where the features would be useful and justify the time spent setting them up.
&gt; ...all of this in a async way that will not hang Emacs This sounds wonderful. Does anything similar exist for Vim? There's a bunch of Vim stuff on [HaskellWiki](https://wiki.haskell.org/Vim), but it seems to be rather stale.
&gt; I'm the author of hsimport. I haven't used stack yet. If you want hsimport to work with Stack you'll want to look at the "Package Authors" section [here](http://www.stackage.org/).
Ooh, I should start using the stack commands setup.
The key is using consummate v's
Where is the evidence that that fluff is helpful for beginners? It's fundamentally useless, information-free verbiage. Even beginners know that a compiler does type checking. "Type error" is established terminology. I don't need the life story.
Is [stack-mode](https://github.com/commercialhaskell/stack-ide/tree/master/stack-mode) basically the stack variant of [ide-backend-mode](https://github.com/chrisdone/ide-backend-mode)?
 data Person = Adult { age :: Age, weight :: Weight, address :: Address } | Child { age :: Age, weight :: Weight, parent :: Person } Might be worth pointing out that `address` and `parent` can fail at runtime, as you do with `head` and `tail` later. That's assuming you intend to keep peddling this partial smut to impressionable new Haskellers, you filthy degenerate. ;) &gt; This cartoon is of course not exactly what happens when Haskell code is run ...what cartoon? I wanna see the cartoon! **Edit:** Finished the first one. That `replaceWithMin` example at the end is really cool!
I've been using it on 7.10 for quite a while now. There are some bugs which I guess are keeping the maintainers wary. I'll see if I can help move it forward tomorrow.
Would it not be a full functor? If it's ismorphic it'd be a fully faithful functor, and a faithful functor if it was injective. I'm not sure if a full functor can be fully faithful, but it probably can. I don't think there's anything more specific than that. https://en.wikipedia.org/wiki/Full_and_faithful_functors
More interesting would be an article investigating having a Haskell compile to JVM or .NET CLR. The real question is what is it about the JVM that makes it hard to have a Haskell - like language. Or has just no one done it yet. I know there is Scala and Clojure - so we can get close, but they are not Haskell.
thanks for mentioning hblas in the readme! i'm hoping to announce a new release pretty soon, that incorporates a whole bunch of great work from a gsoc student this summer.
This is super exciting!
But the point is, in a binary tree the pointers are log N bits wide too, so if you have to follow log N pointers you take log^2 N time. Whereas the array indexing is still just log N.
The thread parent opened with: &gt; The obvious answer is the main distinguishing feature of OOP: subtyping. ...and went on to present examples of Java-style "subtype == subclass" single-inheritance, so that was the assumed context for my comments. You're right that this definition was too narrow on *both* ends, not just the one I highlighted -- but you didn't bring that up immediately, and I think we were talking past each other for a moment there. :) I agree with all the points you just made. (And there's a `type` vs. `newtype` joke in here somewhere.)
These are the blocking issues: https://github.com/kazu-yamamoto/ghc-mod/issues?q=is%3Aopen+is%3Aissue+milestone%3A%22Next+release%22
A binary tree doesn't have to be encoded using pointers. There are more efficient representations that /u/edwardkmett describes [in this talk](https://www.youtube.com/watch?v=uA0Z7_4J7u8)
The link to flycheck-haskell is broken.
The link to flycheck-haskell is broken. 
This looks great
How do you get go to definition? Which part of your setup gives you this feature? haskell-mode alone can do that ?
Too many people interpreted my post as "Haskell sux", when the intended interpretation was "here's a nice math problem". Arguing against the assumptions of a given math problem is usually unproductive. For example, "you can't square a circle using only a compass and ruler" is a deep mathematical fact that opens up a whole new area, while saying "it's unfair to restrict yourself to compass and ruler, you're biased against geometry" is an argument that leads nowhere. The right attitude for a researcher is to sometimes adopt unreasonable assumptions and run with them, just to learn exactly where they will take you. The impossibility of squaring a circle doesn't mean that "geometry sux", Arrow's theorem doesn't mean that "democracy sux", and the CAP theorem doesn't mean that "databases sux". These are interesting facts that expand our knowledge of the world, but you'll never discover them if you get stuck on arguing which assumptions are "fair".
Ah, perhaps you could help me. I understand it is possible to toggle ghc compile and hlint warnings with ghc-mod. I try C-c C-c twice but I don't get the hlint warnings, just the ghc compile output. Is that the correct toggle? Edit: I see that on [line 46](https://github.com/haskell/haskell-mode/blob/master/haskell.el#L46) of haskell.el in haskell mode: (define-key map (kbd "C-c C-c") 'haskell-process-cabal-build) Whereas on [line 67](https://github.com/kazu-yamamoto/ghc-mod/blob/master/elisp/ghc.el#L67) of ghc.el in ghc-mod: (defvar ghc-toggle-key "\C-c\C-c") In my emacs setup, `C-h k` then `C-c C-c` shows that this is bound to `haskell-process-cabal-build`, because my haskell-mode setup in init.el comes after the ghc-mod setup. My workaround is to just run `ghc-toggle-check-command` in my Haskell file buffer to switch between hlint and GHC syntax checking. 
In emacs you don't even have to wait until you save. It's live in buffer. I recommend checking out [spacemacs](https://github.com/syl20bnr/spacemacs) if you're coming from vim.
Such techniques are not tailored to my problem, they can apply to all uses of imperative arrays. That means if you can overcome the logarithmic slowdown using such techniques, you'll prove that every imperative program can be converted to a functional program with the same asymptotic complexity in bit operations. But actually that's provably false in the strict setting, and an open problem in the lazy setting. So maybe worth double-checking your reasoning here, because you might have a big new result on your hands :-)
Btw, semi-static completion-sets like pragmas and ghc options are easy to get async-completed. You basically just need to fetch the information once per GHC binary (either lazy on-demand or at eagerly at startup). `haskell-mode` was able to do this for ages (I don't remember though if it was part of the upstream distribution, or just my local configuration where it worked). So that's something that should be easily doable for Vim in a non-blocking way as well. The more interesting part is where you need to query the running GHCi process for completions (in-scope symbols, type-information, visible module names, module prefixes and so on).
If you are interested in the purely theoretical question, pick up Chris Okasaki's "Purely Functional Data Structures", this book is based on his thesis with the same title and is concerned with two topics: 1) Efficient, purely functional data structures/algorithms, like purely functional queues, etc. and 2) the proof that for any given imperative algorithm there is a purely functional algorithm that is, at worst, O(log n) slower. So it's possible there are equally fast/faster functional algorithms, but the best purely functional one will never be more than log n slower than the imperative.
Thank you for this short and informative post! I didn't know about taggy-lens or wreq.
I've read Okasaki and it's one of my favorite books. The question is whether we can avoid the logarithmic slowdown in this specific problem.
mildly off-topic – is there a better way of feeding durations to `threadDelay`? usually i just do `threadDelay $ 5 * 1000 * 1000`, you've just got `threadDelay 5000000` but there must be a less error-prone way of saying "wait 5 seconds"
My main environment is Sublime Text plus [ghcid](https://github.com/ndmitchell/ghcid#readme). I also use Google and [Hoogle](http://hoogle.haskell.org/) and GitHub extensively, although through Chrome.
can you get "go to definition" with atom ? 
just checked: it is next on the todo list , so not yet
The whole ethos of [ghcid](https://github.com/ndmitchell/ghcid#readme) is a bit of an IDE for those who couldn't get over the barrier to entry. If it isn't working in 5 minutes, that's a bug.
I use flycheck. I have bound it to &lt;f5&gt;, so when I want to typecheck the code, I press &lt;f5&gt; and it automatically highlights all errors and warnings.
You can do most of these things with emacs: * Jump to definition: you can do that with tags * Show usages: do you mean call graph? I don't think emacs does that, but there are extern tools for that (sourceGraph) * Good searching + find and replace: c-s, m-x query-replace, m-x replace-string, etc... * Show type: c-c c-t, or c-u c-c c-t to insert the type * Autocomplete: m-/ * On-the-fly syntax and type error highlighting: flycheck-mode * project files etc: I am using ecb with good succes. You need to edit `ecb-source-path` to add project directories. I have set `ecb-layout-name` to `left-15`, which shows the project list and functions to the left. 
Yeah. There's a bit of rivalry between laziness and uniqueness types, because both are disciplined forms of mutation. Since I already know that uniqueness types allow translating any imperative program into a functional program with the same asymptotic complexity (learned that from /u/Felicia_Svilling a while ago), in the current post I focus on laziness only.
does ghci-ng work with ghc 7.10 ?
Well, it's not particularly hard to define `sleep s = threadDelay s*1000000` but I'm assuming you'd like something a bit more general. Here's what I was able to find relatively quickly: http://fundeps.com/posts/haskell/2014-05-24-An-Experiment-with-Typed-Time/
The question is extremely clear: can you compute a histogram in O(n) in lazy lambda calculus. The fact that the Haskell community reacted in this way is an indication of the fanboyism pervasive in the Haskell community, not of the quality of the question.
Interesting! I tried hsdev branch of Sublime Haskell but it was "hanging" Sublime badly all the time (program is not responding). Are these issues fixed now?
&gt; The question is extremely clear: can you compute a histogram in O(n) in lazy lambda calculus. The question is "How do you compute a histogram in a pure language?" and has been posted to /r/haskell. I think it's reasonable to expect that the pure language in question was intended to be Haskell, not the lazy lambda calculus. &gt; The fact that the Haskell community reacted in this way is an indication of the fanboyism pervasive in the Haskell community, not of the quality of the question. What do you mean? That we love Haskell so much that we assume that all questions are about Haskell instead of the lambda calculus? Or that we're insisting that the question has meaning A instead of meaning B because we want the answer to be "Yes! Of course Haskell can do that"?
Rather, it's not known if it's possible. I mostly agree with your rephrasing in terms of the lazy lambda calculus, but then you also need some built-in operations to be O(1) and it's not trivial to say which ones. It just seemed easier to ask about Haskell and hope that people will get the spirit of the question. Maybe that was a mistake.
Yeah, something like that might work, but bit complexity is really fiddly. For example, tries use pointers, and pointers can also be wider than a machine word if n is large enough. I think it's easier to just assume that all inputs can fit in a machine word and allow the lazy language to use O(1) operations on machine words, which leads to /u/edwardkmett's [comment](https://www.reddit.com/r/haskell/comments/3bqlis/how_do_you_compute_a_histogram_in_a_pure_language/csoya8q) on transdichotomous RAM vs transdichotomous pointer model.
I don't think there is a natural set of operations that would make it O(n), short of assuming some sort of O(1) array update with O(log n) bits indices... The fundamental problem is that the total amount of data is O(n log n) bits. If you assume that the numbers exceed the machine width, you can match the imperative asymptotics with a trie. [See here.](https://www.reddit.com/r/haskell/comments/3bqlis/how_do_you_compute_a_histogram_in_a_pure_language/cspsgmw)
Some functions are named after what they do (e.g. `map`) and others after what comes out (e.g. `length`). There is no naming standard in the "standard" Haskell libraries.
Yeah, at least the latest git version does.
Who can forget [this one](https://wiki.haskell.org/wikiupload/8/85/NarleyYeeaaahh.jpg) ?
I really do like [this logo](https://wiki.haskell.org/File:04_Logo.png), wouldn't have complained if that was the official one.
And do not forget the [haskell mascot](https://mail.haskell.org/pipermail/haskell/attachments/20090401/9fb8fa05/haskell-mascot.jpg).
oh my god.
possible typo. Given (==) :: Eq a =&gt; a -&gt; a -&gt; Logic a class Eq b =&gt; Eq (a -&gt; b) where (f==g) a = f a == g a shouldn't it be type instance Logic (a -&gt; b) = a -&gt; Logic b 
Another typo: ghci&gt; filter ( (&gt;='c') &amp;&amp; (&lt;'f') || (/='q') ) ['a'..'z'] "cdeq" I think you meant `(=='q')`
Emacs has [evil mode](http://wikemacs.org/wiki/Evil) with vi bindings if there are no other alternatives yet.
&gt; you can (and frequently do!) have mutation in pure languages If it's not observable or expressible it's not really "in" the language.
I just posted it because most of the feedback I've received on that article has to do with Haskell, which is virtually unknown in sociology/biology circles. I'm not selling or advocating anything (not even computational psychohistory :), just interested in really, really huge systems.
That made my day.
Thanks! Fixed.
Thanks! Fixed.
Wow, [taggy-lens](http://hackage.haskell.org/package/taggy-lens) looks awesome. I recently wrote about [scraping websites with Haskell](http://taylor.fausak.me/2015/05/21/scraping-websites-with-haskell/) and I used [xml-conduit](http://hackage.haskell.org/package/xml-conduit), which is a little more verbose. cursor $// element "h2" &amp;// contents
I think "interesting" is even more ill-defined than "pure-in-spirit" :)
I have to agree; even Jef Raskin states in “The Humane Interface“ something along the lines of “There should be no distinction made between beginner and expert mode.” Even worse is an “always beginner mode” in my opinion. Minimize noise, maximize signal to noise. And then have a good tutorial explaining it in a beginner-friendly way.
Uhm, I think you just retypoed. You've currently got &gt; (==) :: Eq a =&gt; a -&gt; b -&gt; Logic b &gt; &gt; ... &gt; &gt; type instance Logic (a -&gt; b) = Logic b &gt; &gt; class Eq b =&gt; Eq (a -&gt; b) where &gt; (f==g) a = f a == g a &gt; But according to [`SubHask/Algebra.hs`](https://github.com/mikeizbicki/subhask/blob/master/src/SubHask/Algebra.hs) &gt; type instance Logic (a-&gt;b) = a -&gt; Logic b &gt; &gt; ... &gt; &gt; class Eq_ a where &gt; &gt; infix 4 == &gt; (==) :: a -&gt; a -&gt; Logic a &gt; &gt; ... &gt; &gt; instance Eq_ b =&gt; Eq_ (a -&gt; b) where &gt; {-# INLINE (==) #-} &gt; (f==g) a = f a == g a 
fixed ty
I am the author of the job listing, so feel free to ask any questions here or send me an email at amartin@layer3com.com
Yes, I apologized to Herbert off-list for calling him out here. He, you, Ed, Gershom, Richie, and Austin were all on my list of people I'd call out by name because I'm familiar enough with your contributions to the community that I consider you to fill kind of ownership roles on various areas. I've been told that this came across too strongly here, so I will in the future give only Ed and G a hard time because, frankly, they deserve* it. The passage you quoted was in the context of predictions of widespread breakage due to a syntax extension, which were in this case being wildly overstated if historical evidence is to be considered. However, setting aside predictions about breakage, inexplicable abuse, concerns about editor support, and a desire of some parties to not change things, the bike shedding was, I think, terminal to the proposal as opinions are very strong (the "+1 if my way, -1000 if not" feedback). GHC is what it is; I've learned my lesson. Perhaps these things can better be addressed by external tooling, so I'll try pushing on that end of things. (* They know any guff I give them comes from a place of genuine care and respect.)
Based on what information will company mode autocomplete ? 
That'd do it, thanks. I don't know why I can't find other people on the web having this problem, given that the list of haskell interactive model key bindings in https://github.com/serras/emacs-haskell-tutorial/blob/master/tutorial.md#list-of-key-bindings assumes the ghc-mod binding to `ghc-toggle-check-command`, but if the "interactive commands" section earlier in the tutorial is followed https://github.com/serras/emacs-haskell-tutorial/blob/master/tutorial.md#interactive-commands , then `C-c C-c` binds to haskell-process-cabal-build .
&gt; The only thing I can imagine being useful as far as editor features go, but which to the best of my knowledge, no editor actually does correctly, is that any edit which directly or indirectly causes the position of the first non-whitespace character following a layout keyword (i.e. "let", "do", "of", "where", "mdo") to change, should cause the entire following block to move accordingly. That would save a small bit of time spent reformatting things occasionally. Isn't structured-haskell-mode doing something like that ? 
The Haskell logo contest was my first exposure to the language and I voted on that logo (back then I was one of the people who treated the whole thing as a joke). If I remember correctly it almost came in first place
it works together with hasktags or codex
In that picture: How not to be taken seriously by the industry as an up and coming programming language.
Agree whole-heartedly. It also why I have it as a sticker on my laptop. The single ugliest logo I know is the emacs logo, though Haskell would have been at the top of that list if almost any logo from that link would have won.
In my opinion this is the best solution in this thread. I'm okay with trading O(n) for O(n * log(n)) for anything I'd write in Haskell, and this is by far the most declarative solution in the thread. Almost perfectly captures what a histogram *is*, as opposed to what procedure to use to calculate it.
Yes, I just installed it from source as well to use with 7.10. Works ok for me.
If you're going through all the trouble to make somewhat more "real" functors, why limit them to endofunctors?
Michael Snoyman mentioned this on the mailing list (https://groups.google.com/forum/#!topic/haskell-stack/8HJ6DHAinU0), it did the trick for me : cabal configure --package-db=clear --package-db=global --package-db=$(stack path --snapshot-pkg-db) --package-db=$(stack path --local-pkg-db)
I think it uses GHCi's :complete command, which completes based on available identifiers (imported functions), module names, language pragmas
https://wiki.haskell.org/wikiupload/5/5d/Sgf-logo-blue.png Hey this is the NixOS logo nowadays right?
Unfortunately, it is not. But thank you for asking about it.
Thanks! That got me past cabal configure and cabal build. However cabal install still tried to download and install all packages. And I was still not able to run ghc-mod as it still cannot find any dependencies.
There's a good [LaTeX style file](https://wiki.haskell.org/ThompsonWheelerLogo#LaTeX_.28TikZ.29) for the current one. You can extend it this way: % % The Italian version as per /u/vitalijzad's request % \definecolor{hasklogo@italiangreen}{HTML}{009246} \definecolor{hasklogo@italianred}{HTML}{ce2b37} \pgfkeys{/hasklogo/italian/.style={ lambdabehind, logo color=hasklogo@italianred, first rangle color=hasklogo@italiangreen, second rangle color=hasklogo@italiangreen }} to get [this colourization](http://i.imgur.com/NpRO9JK.png)
I need to make a sticker from this.
Note that there is an `if'` already, but it doesn't to *quite* what you propose: if' :: Bool -&gt; a -&gt; a if' True x _ = x if' False _ x = x ...which "fixes" the semi-common complaints about `if` being special syntax construct for no good reason.
Yes (thankfully! this is a biggie for me...) as Edsko writes: "atom supports CTAGS out of the box ... just call `hasktags -x -c` . in your project directory, and then you can jump to any tag using `⌘-Shift-R`"
Yeah, I know about that version, but something like this might be more useful/readable in some cases. One of the arguments for the if\_then\_else\_ special syntax was the readability due to having that 'else' as part of the syntax.
Avoid success at all costs.
fresh box ? what do you mean ?
Sorry for no links. On the phone.
Glad we didn't go with the ones bearing resemblance to a swatika O_O
How do you use go to def in Atom ? ide-haskell does not support it afaik. 
Good point. Will keep this in mind in my own code.
Unfortunately, I don't have Cygwin installed. However, I tried ghcid-0.5 and it hangs on all MSYS2 shells: MSYS, MinGW-w64 Win64, and MinGW-w64 Win32 Shell. I'm ran a command `ghcid "--command=cabal repl"` in a project. This works in the `cmd.exe`.
I just tried an MSYS2 shell and it worked just fine. It didn't detect the console height (defaults to 8 lines I think), but otherwise worked just as well as my cmd.
In this case that problem can be easily solved by flipping `if'`. I just can't think of what to call it. The "only two hard things in computer science..." problem strikes again :)
There is an Emacs state in evil right? Couldn't that be used to integrate the two?
`'fi`, obviously.
You can learn many things from category theory - turns out graphic design isn't one of them.
Yes, I think this is correct. I'll have to think about how to incorporate this without just making everything more confusing though.
I think that might be possible. I just hope that some day there is a way to use it with Vim/Evil keys 
Looking at all these designs makes me realize just how good the current logo is. Anyone know the story behind it?
Have you thought about [ghcid](https://github.com/ndmitchell/ghcid#readme), which basically is your watch workflow refined and packaged?
I haven't thought about licensing in this context. What sort of things might one want to do that require such a license? (I'm open to the idea, though.)
I think if you build hoogle docs with --local, and then run a server with --local, it will serve the docs and rewrite the links to make them all work in a web server. I don't remember exactly how to do that, but I have done it in the past.
Naturally as a vim user I have to make this into a pissing contest and point out that the Vim logo ain't that great, either.
Whoa, I didn't know about snow in Latin. The story is pretty cool :)
I once wrote if string is "": in Python because I was thinking in English "if the string is empty...". CPython might intern the empty string for all I know, but you want structural (not pointer) equality, to be safe.
Ruby: class Restaurant def initialize(opts = {}) @inspections = opts[:inspections] end def latest_inspection @inspections.last end end Haskell: data Restaurant = Restaurant { inspections :: [Inspection] } data Inspection = Inspection { date :: Date , score :: Int } lastInspection :: Restaurant -&gt; Maybe Inspection lastInspection restaurant = let inspects = inspections restaurant in if null inspects then Nothing else Just (last inspects) 
&gt; &lt;10% woah
could you explain? thanks
All the freedoms that relate to software relate entirely to other works just as well. See http://freedomdefined.org/Definition To be specific to your case, one might do any of these sorts of things: * print out sections of your blog to hand out to students in a class (yes, that is arguably copyright infringement, despite happening all the time) * make a version that has more thorough exercises / added content * translate this to another language * make a video version * edit the text to add more humor or to make wordings yet more concise or who knows what other creative things * make a derivative work that mixes bits of your explanations (which are *excellent* btw! Now that I've started really reading this, THANKS!) with other resources, such as the freely-licensed https://wiki.haskell.org content or the generally excellent [Haskell Wikibook](http://en.wikibooks.org/wiki/Haskell) * use some of the nice illustrations in a totally different writing about lists or whatever and so on and so on… Now, regarding licensing, just as the Free Software and Open Source definitions reject weird clauses about non-commercial use or no-distributing changes, such clauses are equally problematic here. Thus, please don't use NC or ND clauses that Creative Commons licenses offer as options. Effectively those are the anti-Wikipedia clauses and such, as they just divide the commons and make things awkward. The two other clauses that CC licenses offer are BY (require people to give you credit) and SA (share-alike, i.e. copyleft, like the GPL for software). The [Creative Commons license chooser](https://creativecommons.org/choose/) is pretty good (although it leads too many people to choose the NC license without realizing its problems, but at least they mark that as non-free). Like with software, the SA (copyleft) clause keeps things in the commons and discourages unfair exploitation (e.g. someone using your work in their *proprietary* textbook) but can cause unwanted license incompatibilities. So, you should use CC0 to waive all rights (effectively a public domain grant), CC-BY to license in a way that is like the MIT license for software, or CC-BY-SA to license more like GPL. Thanks for your consideration of these things, and thanks for your contribution to the community! I'm happy to answer other questions or provide other links about the issues if you want.
minor note on the first post: &gt; lists of known elements can be written naturally with the elements inside braces I personally found this confusing because I think of "brace" as curly brace { and I call square things like [ "brackets". I think that's more common overall, so I'd suggest that change. also &gt; you’ll notice that it runs much faster, not that total has been evaluated. TYPO: s/not/now
&gt; It would be very nice if I could run a local webserver serving the documentation from all my sandboxes with actual working hyperlinks. Maybe a nice feature for stack?
Wonder why [this one] didn't win. :D [this one]: https://wiki.haskell.org/wikiupload/2/26/HaskellLogoTDavie2.png
Alas! Hit me up if that changes.
For OO with "classes" an object instance is just a closure with some features missing. So a closure in IO with some way to pass in messages (either arguments to the closure you repeatedly call or a thread with Chan/TChan) is the same thing. Though definitely not idiomatic
`solveHisto` is O(n log(n)).
https://wiki.haskell.org/Haskell_logos#Current_Haskell_logo
Mr. Haskelly Pants.
I'm not very familiar with Haskell syntax yet. Could you explain what's going on here?
Maybe the logo should be a graphic version of `:v/i/m.` (which is a valid Vim command that doesn't do anything). Or maybe something leaning the V the other way, with angles like this: `\|||\/\`
Haskellers must be nomads. But nomads are really hard to get your head around, at first. 
I don't claim they're all good ideas :)
The simplest way of doing it - and modules like Data.Map and System.IO, do this - is to write your data type as a record/ADT, but not export its constructors. Instead, you just export "legitimate" functions from your module. These functions, in turn, can do whatever validation you want. Now, if you want persistence and message-sending, it's probably best to use a framework, although that's a lot more complicated a subject.
My favourite. Just needs to be a bit graphic-designered and it'd be amazing.
`Restaurant` and `Inspection` are regular data types. The curly braces are shorthand for this sort of stuff: data Restaurant = Restaurant [Inspection] inspections :: Restaurant -&gt; [Inspection] -- boilerplate getter function inspections (Restaurant ins) = ins `Maybe` is a pre-defined data type, defined as data Maybe a = Nothing | Just a and corresponds to something like nullable/optional. If you have a type `Maybe a`, you'll either get a `Nothing` or a `Just a`. lastInspections takes a Restaurant and checks whether the list of its inspections is empty (`null inspects`). If it is, you get `Nothing`. If it isn't, it gives you `Just &lt;the last element of the inspection-list&gt;`. `null` and `last` are just regular functions. The let-clause defines `inspects` to be a shorthand for `inspections restaurant`. I hope this helps. EDIT: /u/int_index expressed the same functionality in a shorter way. You chain 3 functions together (from right to left): first, you get the Restaurant's list of inspections (`inspections`), then you reverse it so that the last inspection comes first, and then you call `listToMaybe` on that reversed list. `listToMaybe` is defined in Data.Maybe as listToMaybe :: [a] -&gt; Maybe a listToMaybe [] = Nothing listToMaybe (firstElem:rest) = Just firstElem
Actually, to be totally honest if I really needed to emulate stateful objects somehow, I'd probably use threads with internal state machines and channels to communicate. Maybe I've been doing too much Go recently.
This writer explains (with some commentary) [the Vim code base](http://geoff.greer.fm/2015/01/15/why-neovim-is-better-than-vim/)
&gt; I guess I could approximate some of Haskell's behavior if I used only static (class) methods and defined other classes which are essentially named collections of simpler data types. And, I suppose, only used set-once constant attributes. This will only get you so far. At the end of the day it is simply impossible to get the same compile-time guarantees in Java that you can get in Haskell because Java is not a pure language. I gave an example of this in [this talk](https://vimeo.com/59215336) that was directly inspired by my experience with Java. This was a significant part of my original motivation to start looking at Haskell.
It appears to show roughly logarithmic behavior until about 2 billion ints. I believe that might be because it hits the capacity of my RAM, since free tells me I have 9.5 GB of free RAM. Past that it stays constant. [Here's a plot of 935 data points](http://imgur.com/HdUeC4E)
That's pretty cool. Sounds very easy to keep everything together.
There's actually a ton of mutation under the hood, due to laziness.
yes, simple, elegant, idiomatic Haskell. I hate 3D logos, they're garish. 
I tried again in a few different ways (with/without `--topmost`, `--height`, with `ghci` or `cabal repl`) but it still hangs. I have Windows 8, MinGHC 7.10.1, cabal-install version 1.22.6.0. I noticed it does start a new `bash.exe` process in task manager but it seems to be headless. However, when running with `cmd.exe`, I get this output: Loading cabal repl... Preprocessing library clafer-0.3.11... GHCi, version 7.10.1: http://www.haskell.org/ghc/ :? for help Ok, modules loaded: &lt;long list of modules&gt; _exception :: e = _ Unable to list source for &lt;exception thrown&gt; Try rerunning with :trace, :back then :list So maybe that's a problem with my project? EDIT. I created a fresh dummy project and indeed it works in MSYS shells. So, it must be related to my project. 
looks pretty nice on a cm storm :) [Img](http://i.imgur.com/uTgoRh9.jpg)
&gt; Too many people interpreted my post as "Haskell sux", when the intended interpretation was "here's a nice math problem". Maybe I can help you understand how this happened with an analogy. If you're used to being teased for being geeky and nerdy whilst at school and told you'd never get a girlfriend, and then years later someone asks on www.weareallsogeeky.com/forum/ "is it technically possible for a nerd to have a proper relationship?" then I'm likely to pipe in with the fact that I'm married to someone much more attractive than me and have two children. If you then discount this example because I have too many social skills to be a genuine nerd, and you discount Bill Gates because he's rich and so on, you're likely to get called out for essentially redefining the question until you get the answer you wanted, making your conclusion vacuous. Now it may be that you intended to have a discussion about how we define the word nerd, and wondered innocently whether we should exclude people with enough social skills to have a long-term relationship, but if so, you've framed it too combatively with people who've been rejected previously for nerdiness. Coming back to Haskell, I've had so many conversations over the decades where people who don't do pure FP assert that it's an academic toy that's inherently and inevitably drastically slower than imperative code, that it's hard to hear someone ask "can this possibly be as fast as imperative code?" without feeling attacked yet again. &gt; &lt;paraphrased from first version&gt; You don't know what I crave and shouldn't make assumptions &lt;/paraphrased&gt; I wasn't so much making assumptions as reading the mood music: &gt; Partly it's a reaction to claims that laziness is a particularly disciplined form of mutation and can be used to replace less disciplined forms. I hope you can see how it appears that you were looking for a counterargument to the benefits of pure FP. I'm afraid I'm still struggling to see this as a purely neutral theoretical thought experiment coming from a completely open mind. For instance it struck me quite firmly that you didn't explicitly assert that you aren't developing ammunition for such an such an argument and instead told me I didn't know what you were trying to achieve. &gt; Radix sort complexity depends on integer width, which is O(log n) in my problem. ...whilst array lookup is O(1) because integer width is ... oh... wait.. hang on... er? This is the sort of thing that feels like goalpost-shifting. If integer length is really O(log n) then perhaps you should upgrade your server from 32 to 64 bit before sorting your 4 billion customer records.
Let me know if this helps: http://imgur.com/l6hkqYr 
?
Fwd: Racket &amp; Nim
 [RUBY](https://www.ruby-lang.org/en/)
I voted for the logo that eventually won as my first choice, the snowflake of lambdas logo as my second choice , and the "freedom from state" one as my third choice.
Also the typesystem of Haskell is way more advanced (called Hindly-Milner), and stays away from the billion dollar 'null' mistake. 
flif :P
And you'd probably also have a function that takes a Restaurant and returns a new Restaurant with an additional Inspection appended to the end of its [Inspection]. Mutability just becomes functions returning new/updated "copies" of the same object instead of updating them in place.
There's a [pull request for hdevtools](https://github.com/schell/hdevtools/pull/8) that lets it use the stack databases without any cabal configuration required.
&gt; your invariants can often be expressed thanks to the richer type system, so you don't need to hide anything This feels like it should be sloganed to parody the cliche attack against privacy: "If you can prove what you've constructed is legal, you have nothing to hide." But for sure. Most classes you work in practice are really just mungled up records. If all you have is data to pass around, what would you even want to hide? Highly stateful components are often closely tied to some IO. If I have a database connection or a network manager or a thread pool, all the state is going to live at the IO layer. 
Find the largest size of the memory of any existing computer. Find the slowest processor in any existing computer. Calculate the longest time a sort would take on a computer with this memory size and processor speed. This is a constant. All sorts on existing computers will take less than this constant. Therefore all sorts on existing computers are O(1). The point I'm trying to make is that if you artificially restrict an aspect of the quantity you are measuring with a big-O expression, the big-O expression becomes meaningless.
 Yeah putting 3 package-db: &lt;stack db paths&gt; statements in cabal.config did the trick as well! 
I disagree. We should [distinguish beginners from early programmers](http://zedshaw.com/2015/06/16/early-vs-beginning-coders/). The first do know about type checking, the others don't. When I started learning Haskell it took me ages to decipher the error messages. They just didn't tell you what the problem was, just stuff about things not matching where I never told them to match. Additionally ghc constantly told me to add instances to basic data types, as if this was _my_ job. Messages like "all things in a list must have the same type" would have been great at this time. I agree that the amount of text would be overwhelming in most situations, though. So maybe this is a problem where a compiler flag could help out?
It's perfect! :-D However, I think I will use this one: https://pbs.twimg.com/profile_images/616706223027589120/HmT-yq3x.png
In the first example it tells you kind of what the problem is, but not why this is a problem. Many programming languages would be fine with lists of multiple types. This would be a very helpful information for beginners. In the second example it gets even worse: &gt; it reads fairly clearly to me. Yes, If only there wasn't this halfhearted attempt to help the user by stating: &gt; Possible fix: add an instance declaration for (Num [Char]) This is an attempt to provide similar help like discussed in the Blog post. I just points in the wrong direction 95% of the time this error occurs. Maybe we should think about providing error messages of different detail levels.
You can use `id` instead of `\f -&gt; f`
I like the idea that if could be a useful function on its own.
Actually, the bigger issue here is the implicit and unenforced assumption that the list of inspections is ordered by the `Date`s. If the code that constructs and maintains these lists breaks that assumption, both of these `lastInspection` functions will return incorrect results. Assuming a restaurant is inspected at most once on each `Date`, and that `Date` has an `Ord` instance, this strikes me as a better solution: import Data.Maybe import Data.Map (Map) import qualified Data.Map as Map import Whatever.Date newtype Restaurant = Restaurant { inspections :: Map Date Inspection } newtype Inspection = Inspection { score :: Int } lastInspection :: Restaurant -&gt; Maybe Inspection lastInspection = listToMaybe . map fst . Map.toDescList . inspections Note that the key idea here is that [`Data.Map`](https://hackage.haskell.org/package/containers-0.5.6.3/docs/Data-Map-Lazy.html) is an ordered search tree, so it takes care of keeping entries ordered by their key. So [`Map.toDescList`](https://hackage.haskell.org/package/containers-0.5.6.3/docs/Data-Map-Lazy.html#v:toDescList) gives us constant-time access to the last entry in the map. Note that this is an excellent example of two techniques that others have mentioned in the threads: * Make illegal states unrepresentable. In this case, by representing the collection of inspections as a `Map` keyed by `Date`, it's impossible to have them out of order. * The `Data.Map` module itself relies on encapsulation to enforce that invariant. It doesn't export the constructors for the `Map` type, because that would allow clients to construct invalid maps.
I agree that `Eq` instances should probably be required to represent equivalence relations. But then what do you do with `Float`? Do you violate IEEE? Or do you simply not include an `Eq` instance, instead forcing the programmer to use a newtype?
[Hlskell](https://wiki.haskell.org/File:Haskell_logo_bonus.png)
Quoted: &gt; We are pleased to announce the first release candidate for GHC 7.10.2: &gt; &gt; https://downloads.haskell.org/~ghc/7.10.2-rc2 &gt; https://downloads.haskell.org/~ghc/7.10.2-rc2/docs/html/ &gt; &gt; This includes the source tarball and bindists for Windows, and Debian &gt; Linux. FreeBSD, OS X and Solaris binaries will follow soon. These &gt; binaries and tarballs have an accompanying SHA256SUMS file signed by &gt; my GPG key id (0x3B58D86F). &gt; &gt; We plan to make the 7.10.2 final release in a week or two - so please &gt; test as much as possible; bugs are much cheaper if we find them before &gt; the release! &gt; &gt; -- &gt; Regards, &gt; &gt; Austin Seipp, Haskell Consultant &gt; Well-Typed LLP, http://www.well-typed.com/
hmm, shouldn't you always be explicit what precision the equality has for floating-point numbers? like `eqFloat range x y`.
Yes, but the programmer should still be able to do a direct comparison if desired. The more basic problem is that equality of `Float`s is not reflexive, because `NaN == NaN` evaluates to `False`.
Or Spacemacs. Loving it so far.
Here is a high level understanding of mine (coming from OO): There are 2 kinds of problems : 1) transformational (e.g. compiler) 2) interactive (GUI, CRUD, video game, etc). In transformational problems, OO class= Data Types (class)+Immutable Data Structures (collections) + Lenses (properties) + Pure Functions that transform data (methods) In interactive problems, OO class = Data Types wrapped into a Behaviour in an FRP System (class)+ Immutable Data Structures (Collections) + Lenses (properties) + Functions that describe the time evolution of the Behaviour (methods) 
I have a very particular view of the problem. IMHO the goal in a language like Haskell is to express the problem in a way that the top level entities of the problem are elements of an algebra, or talking in more practical terms, to construct an EDSL (Embedded Domain-Specific Language) in which the entities of the problem are first class. That means that they may be combined to solve the particular problem and all the problems in which these elements may be involved. Since the top level elements of the problem are the elements of the EDSL, that also means that - ideally - they must appear to the EDSL as if they would have no internal structure. That means that they have no setters/getters, no methods, no state. They are elements. By combining different EDSLs for different problems: persistence, caching, web page composition, form combinators, page navigation etc the problem can be solved. This is - in my humble opinion - the Haskell way. What this means in your particular problem? `Restaurant` and `Inspection` are two elements. but they have no properties except that a Restaurant contains inspections and that both are serializable. Since they have no properties, they can not be combined, so they are raw data, and a EDSL can do little more than an OOP language with it. So all the suggestions in the comments for handling the data here are Ok for me. Maybe I would use an EDSL that may ideally automatically cache, transact, query, save and retrieve the data to/from whatever permanent storage when it is needed, using STM. The TCache package does that. But your problem has many other elements that have properties and can be combined: pages, HTML elements, form elements, web navigations. These are inherently made to be combined: two form elements makes a form. a page can contain many forms or links. they trigger an invocation to the server. A combination of pages makes a route or a navigation. Navigations or routes can be combined to create an application. There are many implementations of the formlet concept in haskell to combine form elements and produce statically typed form results. All major Haskell web frameworks have it. But none treat the rest of the elements of a web application the same way There is a package "MFlow" that treat forms, links, pages and navigations/routes as elements in a monadic EDSL. For people coming from other languages it is weird since they think in terms of HTML and request-response handlers, not in terms of combinations of elements of the domain problem. Who thinks in that way? paradoxically two kinds of people: the category theorists on one side and the client, the people who write the specification in the other side. They naturally talk about elements that may involve an entire navigation, like payment. or a set of routes, like "visit the catalog". If the framework manage the same terms and combine them in the way the client need then the code may follow the specification more closely , would need munch less documentation and can be maintained with much less problems. It is not weird functional academicism. the goal is to get closer and closer to the specification level. That is why functional programming could be higher level and could allow faster and more flexible, more intuitive and error free programming if the programmer uses his full potentiality and does not limit himself to clone OOP solutions.
&gt;&gt; Radix sort complexity depends on integer width, which is O(log n) in my problem. &gt; ...whilst array lookup is O(1) because integer width is ... oh... wait.. hang on... er? If we count individual bit operations, I think a purely functional radix sort still has higher asymptotic complexity than doing n imperative array lookups, by a factor of log(n) or similar. For a radix sort with log(n) passes, you need to copy the whole data log(n) times. For a radix sort with one pass, you need a large data structure for the buckets, with log(n) update time (similar to the counting sort in my post).
[The Emacs logo](http://ergoemacs.org/emacs/emacs_logo/emacs_logo_no_border.png) is terrible indeed. I think [the icon](https://upload.wikimedia.org/wikipedia/commons/thumb/0/08/EmacsIcon.svg/1024px-EmacsIcon.svg.png), on the other hand, is okay.
Yeah, that style often works better in terms of how much editing you need to do, but I don't like how it looks to read quite as much as the hanging style.
I'm pretty sure `(` and `)` are not valid characters to have in identifiers in Haskell anyway.
That does look like the problem - it seems to get wedged. Do you have a .ghci file? If you run `cabal repl` does it get wedged too? My guess is in the MSYS shell it gets equally wedged, but the buffer doesn't flush so you don't find out and it appears like it hangs.
&gt; One key insight though is that (n) here isn't the number of elements, it is the 'amount of the structure we have to inspect' I always thought the O(n) asymptotics on radix sort was a _teensy_ bit disingenuous because it's only linear in the number of elements if you have a fixed number of keys. I've always preferred to think of it as O(m*n), where m is the size of your keys, and in most use cases m ≈ log(n). 
The reason GC is easier without mutation is because old objects can't point to new objects. That means you don't need a write barrier for generational GC. It makes concurrent collection easier. That doesn't apply to Haskell because there is mutation in the IO and ST monads and for lazy values. Lazy values work by allocating a location in memory where the value will eventually be stored, and in there you store a pointer to the code that can create that value. When somebody needs the value you run the code behind the pointer and store the value in the location. This mutates the location. It will only be mutated once`*`, but that doesn't help much with GC. Therefore on a high level the Haskell GC is the same as a Java GC. `*` unless you have multiple threads
Just to confirm: does `stack build` install any of those packages?
Yes, `stack build` and `stack install` work just fine.
If the length is n and the alphabet size is a, then it will be O(n log(a)), just like with a map. However, by viewing the histogram as a monoidal structure, we pave the way for an O(a log(n)) algorithm.
&gt; data Maybe = Nothing | Just a should be data Maybe a = Nothing | Just a
Deja vu :-)
&gt; I doubt full-blown dependenent types will ever be in hte standard. Even languages like Java and C++ have adopted large extensions to their standards.
It might be an interaction due to having `.ghci` present in the project but using `cabal repl`. Normal `cabal repl` works fine and, after removing `.ghci`, executing `ghcid "--command=cabal repl" --topmost --height=8` produces: Loading cabal repl... Preprocessing library clafer-0.3.11... GHCi, version 7.10.1: http://www.haskell.org/ghc/ :? for help Ok, modules loaded: &lt;...&gt; All good (36 modules) my `.ghci` was :set -fbreak-on-exception :set -fbreak-on-error :set prompt "\\&gt;" :set stop :list :def seq (\arg -&gt; return ("seq (" ++ arg ++ ") ()")) :set -fprint-evld-with-show Which explains everything. It was breaking on exception and MSYS didn't flush. The only strange thing is where does the exception come from? Just doing `cabal repl` does not result in any exceptions. 
Ah, I know, you have break-on-exception set! In ghcid, the way I determine that the stdout and stderr are both flushed is to signal an error and wait for it to appear on stdout. However, you want to break into the debugger at that point, and it goes horribly wrong. I guess I should pass `-fno-break-on-exception` to turn it off for ghcid. Tracking here: https://github.com/ndmitchell/ghcid/issues/43
I'd buy that shirt.
Half of those are about the version that is not displayed correctly because people have been compiling from the repo manually because there is no release.
Oh, I think I know what the problem is: you probably aren't using stack from HEAD, which changed the `stack path` output a bit to make this all work. What does `stack path --snapshot-pkg-db` look like? You can try running `stack upgrade --git` and see if that fixes things.
Right you are. I fixed it.
You were right to do that. You only need to be careful of Python's different string types: &gt;&gt;&gt; "foo" is "foo" True &gt;&gt;&gt; "foo" is u"foo" False &gt;&gt;&gt; "foo" == u"foo" True
You're right, I'm not. Sorry, didn't know that was required. And thanks for pointing me to the `upgrade` command; didn't know about that.
I like the `?` idea: https://www.reddit.com/r/haskell/comments/3bwc61/python_style_ternary_if_in_haskell/csq5b5t
If you're going to go through `Data.Map`, why not use [`maxView`](https://hackage.haskell.org/package/containers-0.5.6.3/docs/Data-Map-Lazy.html#v:maxView)? lastInspection = fmap fst . Map.maxView . inspections
Yeah, it's doing something with the cons function.
Oops, sorry Hindley :) So is there a better classification for Haskell's type system?
aye, and a bit more of a writeup as to how to get it running / what the desired use case is!
https://ghc.haskell.org/trac/ghc/query?status=closed&amp;resolution=fixed&amp;milestone=7.10.2
Probably the big flat part of the plot is when nearly every memory access is a cache miss. If you were to continue growing until you run out of RAM and have to swap, you would see it start to slowly grow again as more memory accesses have to go to the hard drive, and then it would flatten out again until you run out of swap space, and then you have to find some other way to store your data, at which point it will begin slowing again. How far can this go? At the theoretical limits of the real world, accessing any sort of memory is probably O(∛n); if you imagine you are in the center of a perfect sphere of data and you want to access some point within that sphere, this is how long it would take the light to travel.
The immutability may help with concurrent GC. There is talk in the [paper](http://community.haskell.org/~simonmar/papers/parallel-gc.pdf) about using immutability to reduce per object synchronisation. The idea is that if an object is immutable its not the end of the world if it's copied twice especially as contention is rare anyway. I don't know if this is implemented in GHC though. 
Is there an issue for this? Can't we just use `stripTrailingPathSeparator` or something like that?
There was an issue and we fixed it elsewhere. It's just not reflected in `stack path` yet. An issue about this would be great.
Ben's sleuthing on that bug is incredible! Thanks so much for filing it, it would be somewhat disastrous for me if it made it to release.
https://github.com/commercialhaskell/stack/issues/506
Neat, hadn't seen the [these](https://hackage.haskell.org/package/these) package before. 
I manually wiped .stack-work, dist/ and now after a new `stack build`, hdevtools works just fine. Sorry for taking so much of your time, but I'm glad that it's working now. :)
Because it appears later in the page than `toDescList`, of course! More seriously, I suspect it doesn't make a significant difference.
You're really struggling to say this is O(n) while being ambiguous enough to not actually say it. This just seems like you are complicating the matter in order to be misleading on purpose. Answer the following questions with a 'yes' or 'no': A) Given an list of random stings, will this algorithm sort them with an average time complexity of O(n)? B) Given an list of random stings, will this algorithm sort them with an average time complexity asymptotically less than O(n log n)? - On standard hardware. - Where comparison of two fixed-size portions of strings (say 64 bits) counts as one operation.
&gt; This just seems like you are complicating the matter in order to be misleading on purpose. No. I'm endeavoring to explain why my bounds and the ones that lpsmith offered disagree and to distinguish between the word size `w` and the alphabet size `σ` that arise in a radix sort, because if you _don't_ take the more complicated analysis above, then they are easily conflated. Given a list of strings R _every_ algorithm that is capable of sorting it must pay at least Ω(ΣLCP(R)). Where ΣLCP(R) is the amount of the strings you must inspect to disambiguate them. Period. Game over. I can't escape that bound. Nobody can. So I can't sort strings in `O(n)` time, unless `ΣLCP(R) = O(n)`. One situation that happens in is when we're sorting fixed sized integers with a fixed number of radix passes. On the other hand, when the strings are _random_, like you've requested, the expected value of the longest common prefix involved becomes: E(ΣLCP(R)) = O(n log n) So the bounds I gave become a randomized upper bound: O(n log n + n log σ) = O(n log n) and we're bounded below with a randomized bound of: Ω(ΣLCP(R)) = Ω(n log n) So: A.) No. B.) No. No paradox is introduced and the universe doesn't collapse. On the other hand, what I _am_ saying is that the complexity involved here is less than that of a symbol-comparison sort under other input distributions. Then the complexity here O(ΣLCP(R) + n log σ) can dominate the achievable complexity of any pairwise symbol based string sort: Ω(ΣLCP(R) + n log n) I'd like to point out that the questions you asked are merely about _random_ strings, however, where the `ΣLCP(R)` term dominates, while the bounds I gave above **hold in all situations**. If we take `N = ΣLCP(R)`, then the `O(ΣLCP(R) + n log σ)` complexity bound on MSD radix sorting is linear in `N` as we can absorb the `n log σ` bound into the coefficient for `N`, as `log σ` is a constant, and `n &lt;= ΣLCP(R)`. We get no such collapse for the lower bound on symbolwise comparison. We only have `n &lt;= ΣLCP(R)`, so I can only get a sloppier bound `n log n &lt;= N log N`, so in those terms if we collapse the terms together to get a single variable Ω(ΣLCP(R) + n log n) = O(N + N log N) = O(N log N) and then we're talking `O(N)` for radix techniques vs `O(N log N)` for comparison based techniques. Our equation now only involve one variable. But notice that `N = n log n` under your assumptions of randomness. The latter bound was made sloppy by merging the two terms, `Ω` devolved to an `O`, making it useless. In the case you are interested in, when our strings are random we can see the tighter "one variable" bound: Θ(ΣLCP(R) + n log n) = Θ(n log n + n log n) = Θ(n log n) for symbolwise comparisons agrees with that for radix-style comparison. In my original description the `n` was `N = ΣLCP(R)`, not the number of strings. I included the more detailed breakdown above not to "throw up dust" but to explain why we're starting with the same facts and yielding different conclusions.
Performance-wise, I expect they're near-identical.
&gt; I use [the these package] all the time Do you have blog posts / source files describing interesting use cases for `These`?
how hard is it to torrent?
good to know. and now that I've been trying to "observe sharing" (see the data-reify package) with IORefs and StableNames ("eqStableName" in Haskell is like "is" in Python), I feel like I'm writing Python again :) 
A small typo: sudo apt get install zlib1g-dgb should be sudo apt-get install zlib1g-dbg
It was a very fun &amp; instructing read Francesco - great work!
I will totally make sure my personal Mac build has them :) Your directions on the wiki good!?
Absolutely not. I'm not even sure why it is in this package.
I installed `hdevtools` from the linked stack branch and I copied the newly created binary to my path. I also tried launching it manually and it seems to launch the server / shutdown fine. `which hdevtools` points to the right binary. The path has be set in `linter-hdevtools` and `hover-tooltips-hdevtools` Settings. Now when I open my Stack project (opened it using the File -&gt; Add Project Folder) in Atom, I don't see any errors being report when I deliberately break things -- it just says No Issues. How do I use `hdevtools` with an existing stack project? Do I need to specify anything in the settings? I have only used plain vanilla Vim before so this is all new to me. Thanks! 
I have no idea how Atom works, sorry! You should be able to do `hdevtools check $(path to your source file)`. That will perform the check if there's anything wrong. The file also has to be written to disk in order for it to check, so it can't do checking "live"
I've encountered some issues with hvr's ppa snapshot build of ghc 7.10.2 of a few days ago. It seems to fail to infer at least a couple of fundeps that 7.8.3 inferred successfully. Does anyone know if anything changed w.r.t fundeps with 7.10?
/u/ranjitjhala, a stand alone file works well. It gives me inline error messages. I built my project using `stack build` and then opened the individual `.hs` files in Atom but it does not do any error checking. I noticed my standalone file has a blue `x` icon in Atom while my stack projects have a regular black one. I have never used `hdevtools` so not sure if I am missing a step. I just opened my project after building it using stack build. Do I need to setup any specific settings in stack to make this work with hdevtools? When I did check like /u/ephrion suggested I get this hdevtools check rest-api/src/Main.hs hdevtools: &lt;socket: 4&gt;: hGetLine: end of file 
I installed ghc-mod with stack today and it seems to work.
hmm... can you try killing all the `hdevtools` processes and restarting? so: $ killall hdevtools (To be sure, I always do it a bunch of times) and then try again: $ hdevtools check rest-api/src/Main.hs ? 
Thanks!
I quite like the Warm Fuzzy Thing mascot: https://wiki.haskell.org/wikiupload/1/1d/Monica_monad_falconnl.png
Btw, I suddenly remembered -- can you see this: https://github.com/bitc/hdevtools/issues/40 There is a known issue with older versions of ghc/cabal -- I had a similar problem myself, and was solved by moving to GHC 7.10 and cabal 1.22. 
**Update:** installing the latest version of `stack` seems to be working ok in GHCi ~~but not inside Atom~~. **Update2**: Thank you /u/ranjitjhala. It seems to be working inside Atom now. I did `hdevtools check on Main.hs` and re-opened Atom and that seems to have helped. It sometimes does not show an error but that could also be due to long compile times for my project that has Persistent's Template Haskell. I recompiled the project with the newer version of stack. I have not changed anything else -- not sure if I need to update my old stack.yaml file or anything. *Stack&gt; getStackConfig' "/Users/dev/rest-api/src/Main.hs" Just (StackConfig {stackDist = "Caching build plan\nPopulating index cache ...\nPopulated index cache.\n.stack-work/dist/x86_64-osx/Cabal-1.22.2.0/", stackDbs = ["/Users/dev/rest-api/.stack-work/install/x86_64-osx/nightly-2015-06-17/7.10.1/pkgdb","/Users/dev/.stack/snapshots/x86_64-osx/nightly-2015-06-17/7.10.1/pkgdb", "/Users/dev/.stack/programs/x86_64-osx/ghc-7.10.1/lib/ghc-7.10.1/package.conf.d"]}) For completeness: I am on `hdevtools: version 0.1.0.8 (ghc-7.10.1-x86_64-darwin, cabal-1.22.4.0)` ~~I am on GHC 7.10.1 and I don't think I even bothered to install cabal. Here is an updated error message after fixing the `stack` command not found issue.~~ ~~Stack&gt; getStackConfig' "/Users/dev/rest-api/src/Main.hs"~~ ~~Invalid option `--dist-dir'~~ ~~Usage: stack path [--help] COMMAND~~ ~~Print path information for certain things~~ ~~*** Exception: readCreateProcess: stack path --dist-dir (exit 1): failed~~ ~~Is this because of the version of stack I am using?~~
Part two has some code. Also don't confuse informal math with 'bad math'. The discussion of quotients for example went a little deeper than you might think, etc. Sure, if you know ZFC cold this is an intro talk and not for you. But as a popularization it can serve a valuable purpose.
My point is the sort isn't actually O(n) in general so the package description and documentation is misleading without further clarification. 
Thanks bro. :)
So all good now, right?
/u/ranjitjhala, it works but it is a bit flaky. It works for certain modules in the project but not all -- it did work for the other ones for a bit and then stopped working. I am not sure why that's the case -- cannot really tell any clear pattern. I initially thought the modules that import things that were based on Template Haskell were the culprit but that theory did not hold water for long. Will let you know if I notice any clear pattern as to when it fails. 
I very much wish that SublimeHaskell were better as well :(
Is stack a replacement for cabal?
Oh for crying out loud. Lisp was traditionally given set theoretic semantics. The speaker actually gave a variant of this talk at lispnyc and it was quite well received. Nor was the lambda calculus found "inconsistent" because it wasn't used as a logic initially but just to give a formal system for expressing computability. The typed lambda calculus was introduced to capture termination, not "consistency" (because the latter doesn't make sense relative to a 'theory of computation'). In fact, when Lisp was initially developed, it was _not_ in any sense at all based on the lambda calculus, which led to the famous funargs problem. Only with the development of scheme did people adopt proper lexical scoping. You may find it interesting to review the paper in which Howard developed what we call the "Curry-Howard correspondence" -- in it, his method of interpretation of types and formulae is done entirely in set-theoretic terms. I'm glad you have an interest in history. But please try to get it right, or at least not to sound off about it when you are not confident that you are not wrong.
For all my projects I've just been using `stack init` without any problems. The only time I write my own stack.yaml is when I need to add dependencies not on stackage.
Great job! Does Leksah (well it and other GUI applications) work fine from virtualbox? I'm guessing it does since it would just use X11 forwarding right?
I see that annotating intervals carries some value, but I wonder whether you can recover much of this value with annotating on points. For instance, in my mini-proposal a `Doc` annotated with `Type`s would look something like, ty1, ty2 :: Type msg :: Doc Type msg = text "Failed to unify"&lt;+&gt;pure ty1&lt;+&gt;"with"&lt;+&gt;pure ty2 This is, of course, a simplified example; in the compiler you'd want to embed more than just `Type`s. You'd likely want to lift `Type` into an ADT which can encode the various atoms that you might want in an error message; in the type checker these might include types, expression, instance lookup results, names, etc. Alongside these values this type also include hints to the client describing how to best format the value (e.g. whether to fully qualify a name for clarity). Once you have constructed such an annotated `Doc`, you may want to flatten out annotations into something printable. Given `prettyType :: Doc Type -&gt; Doc Void`I envision this looking something like `msg &gt;&gt;= prettyType` where `Doc`'s bind has subtree replacement semantics (e.g. take a pure node, feed the value through the function to yield a new `Doc` which will replace the old node). Alternatively, GHC API users might decide to simply work with the annotated `Doc`, rendering the text elements with inline rich elements in place of the effect nodes. Ideally the GHC API would expose just enough of its internal formatting functionality to allow clients to do this without reimplementing most of the logic for rendering `Doc`. I'm not sure I see how annotating regions of a `Doc` would improve code-reuse over annotating points.
&gt; Nor was the lambda calculus found "inconsistent" because it wasn't used as a logic initially but just to give a formal system for expressing computability. The typed lambda calculus was introduced to capture termination, not "consistency" (because the latter doesn't make sense relative to a 'theory of computation'). Lambda calculus was intended as a logic, initially. Dana Scott mentions it at 14:45 in this talk: https://www.youtube.com/watch?v=7cPtCpyBPNI&amp;t=886 
Here is the screen cast https://youtu.be/UwDpskjf7BA
Do you have problems with download speed?
Agreed. Opened a ticket https://ghc.haskell.org/trac/ghc/ticket/10601 . We also need support on `cabal` for building with DWARF, like we do for profiling libraries. Edit: opened a ticked for `cabal` too: https://github.com/haskell/cabal/issues/2702 
Wow, that's classy, where did you..?
You are right, I wish I could change the text in the heading.
Basically a VBox 5 RC 1 is what I am running it on. 
You should go with Atom or Sublime Text then, they work ok out of the box. If you don't want to spend time with your most basic tools, no IDE in the world will be really useful.
Not a complete equivalent, however, given that [*edit*]sum types consistently occupy as much space as their hungriest variant needs.
The factor you're referring to is (word size), not log (n) unless you are making the patently absurd assumption that you're always sorting enough data to bring you to the maximum possible addressable space, in which case from a practical point of view you'll have to take into account all the thrashing to disk (unless of course you're flying on bare metal without an operating system! ). If you're going down to bits you have to do so for both, and your comparisons for nearby data (which you inevitably do a fair bit of in a comparison based sort) all take (word size) time to do as well, so you can't assert nlogn any more there. Again, you can't split hairs on one side and gloss over them on the other without falling prey to yet more accusations of in-match goalpost-shifting. You again don't deny that this is just part of an anti pure fp argument/viewpoint/bias, I notice. 
That does indeed make it a lot more clear! :)
Woohoo the author is sometimes an Idris user too! ;-)
Hello. I noticed you posted this in a very old thread. Do you know if there is any implementation of an optimal reduction evaluator which I can download/use right now for a project that needs to evaluate lambda calculus terms to the normal form as quick as possible? Even a toy implementation would help me a lot. Thank you!
Ninegua, do you know what is the fastest way to reduce lambda calculus terms today? That is, I have a lot of lambda calculus terms and I want to reduce them efficiently, for games and such. I can't use Haskell for that, since Haskell doesn't give access to normal forms. On the net, I could only find toy interpreters. Do you have any recommendation? 
Slightly off-topic, but: Is there emacs integration for hdevtools? There's flymake-hdevtools, but the github page says it's unmaintained.
I see. Thanks
As I recall, hdevtools was designed to be pretty similar to ghc-mod, so it shouldn't be too hard to get it to work. I'd investigate the ide-backend client emacs tool that /u/chrisdoner has worked on.
Congratulations on making a solution that actually follows the spirit, not just the letter, of Project Euler. 
thanks, is there some introductory video explaining how to use it? i cannot find any on youtube
[This](https://github.com/flycheck/flycheck-hdevtools) works pretty well (despite the dire warning in the README.)
Overkill, indeed.
But that's not a problem with standardization, that's a problem with there not being any other compiler writers doing enough work (to catch up with GHC) to make a new standard an interesting question. Even if the Standard mandated a new better error reporting, that wouldn't make GHC magically able to implement it.
Ambiguous Datatype Terminology is probably what you really meant. 
perf often attributes cost to the instruction *after* the expensive one (out-of-order scheduling in the CPU? or something). So it's not the register-to-register move or the xor, it's the memory read instruction before it that is taking the time.
Let us know when your presentation for beginners is ready!
(For a short answer, jump to the dividing line.) For starters, if you don't know, this: PeopleRecord.hs:24:34: ...means the error was detected on line 24 of your source, starting at character 34 of that line. The root cause might be elsewhere, but this is always a good place to start. Your line 24 is: location Adult{address = addr} = addr The rest of the error is telling you that this is returning an `Address` when the compiler expects it to return a `String` (a.k.a. `[Char]`, they're synonymous -- that inconsistent representation is admittedly a blemish in the GHC error output). So there are four questions here: why does it expect a `String`, why is it returning an `Address`, which of those behaviors do you actually want, and what change needs to be made? It's best to investigate them in that order, because the earlier ones can give hints about the later ones. The reason it expects a `String` is right on the previous line: you told it so! location :: Person -&gt; String While it's possible you were mistaken about what you actually want to return from this function, it's unlikely, so this also answers question three as well; you really do want a `String`. So then, where does the `Address` come from? Well, there's pretty much only one place it could -- your function is only one line long, and has only one way of getting data: `Adult{address}`. And indeed... data Person = Adult { ... address :: Address } So what, exactly, is an `Address`? It's defined just above: newtype Address = Address String And this gives us two possible solutions, which I'll take a moment to explain in detail because I spent a while confused about `type` and `newtype`too. :) A `type` is just a synonym. All it does is take an existing type and give it a different name, at which point the two names are completely interchangeable. As illustrated in that slightly inconsistent error message, `String` is a `type` for `[Char]`. Your `location` function is written as if `Address` were a `type` for `String`. But it's not -- it's a `newtype`, which is more like a _wrapper_ around another type, not just an alias. The compiler still knows it's the same type underneath, so `newtype` costs nothing at runtime just like `type`, but in the source you're *required* to pretend they're two completely different types with nothing in common. Basically, you need to treat it as if it were a `data` type. So `newtype` is less convenient to use, but when you think about it, it's often the more useful of the two. In your code here, you have an *absolute guarantee* that arbitrary strings cannot be placed in the `address` field: they have to be wrapped into `Address` first. On its own, this prevents a lot of accidents; and if you want an even stronger guarantee, you can put this in a module and _not export the `Address` constructor_, instead exposing only functions to wrap (and maybe unwrap) an `Address` -- allowing you full control over what strings can legally _become_ `Address`es. (`newtype`s also allow new typeclass instances, which is a more advanced topic but also extremely useful.) That's not to say there are no good uses for `type`. Types can get complex, and it's handy to be able to abbreviate them. For instance, when writing a reddit bot, I declared this: type BotM a = RedditT (StateT CommentID (ReaderT BotInfo IO)) a ...because that's a hell of a thing to put in every type signature! Essentially, `type` is great when you want a shortcut for a _more specific_ version (or combination) of existing types. (Or, in the case of `String`, just a friendlier name.) --- So let's finally solve your error. :) As I said, there are two options. Either change your `Address` definition: type Address = String Or, explicitly unwrap the `newtype` in your `location` pattern-match: location Adult{address = (Address addr)} = addr I recommend the latter, because you really don't gain anything from the former. (In fact, the `type` solution is exactly equivalent to changing your `Person` to use `address :: String`. So I guess that's technically a third option.)
Main editor frequencies... * vim: 14 * emacs: 13 * Atom: 4 * SublimeText: 4 * vi: 2
Thanks! we will add that to the README.
Well, it depends, if you want encapsulation, then as mentioned by others, abstract data types with operations that preserve their invariants are the way to go (though there seems to be an unspoken rule, of which I very approve, that one should export all innards of a package anyway, under an "Internal" subtree. It can save the users a lot of trouble in some edge cases). If you want state, there are a couple of possibilities: * [State](https://hackage.haskell.org/package/mtl-2.2.1/docs/Control-Monad-State-Lazy.html) - it's a wrapper over (s -&gt; (a, s)) functions that lets you treat those as if it was just an "a" value with some added context. Can be a bit awkward to use as of itself, but [really shines when combined with lenses](http://www.haskellforall.com/2013/05/program-imperatively-using-haskell.html). * [ST](https://hackage.haskell.org/package/base-4.8.0.0/docs/Control-Monad-ST.html) - for operations that are pure in the end, but use mutable state internally. * [STM](https://hackage.haskell.org/package/stm-2.4.4/docs/Control-Concurrent-STM.html) - Software Transactional Memory, for concurrently accessed variables without the fear of deadlocks. * [IORefs](https://hackage.haskell.org/package/base-4.8.0.0/docs/Data-IORef.html) - to be honest I haven't ever used them in any bigger code; if I need mutability I probably also need concurrency, and then I feel safer using TVars, even if I'm not sure if I'd ever try to access the same variable from multiple threads. This being said, avoiding mutable state in IO/STM whenever possible makes the code safer and way easier to test (especially that Haskell's modularity is rather underwhelming, and you can't just replace IO with some mockup monad for the purpose of testing). I know /u/tekmo is/was working on some tools to combine concurrency (sort of) with purity, like [MVC](https://hackage.haskell.org/package/mvc), or apparently abandoned [arrowized pipes](http://stackoverflow.com/a/19204253). Though for me it all breaks when I need to make some Haskell computation concurrent to some external code (for example I don't want the GUI to freeze while the program is doing some number crunching). Oh well. EDIT: There's also one "design pattern" i found emerging in my code for communicating with [HsQML](https://hackage.haskell.org/package/hsqml). To be able to fire signals into QML I need ObjRefs, and to keep track of main program state (where I store various things displayed by the GUI) I need TVars, so I have a toplevel record from which the context object will be created: data Toplevel = Toplevel { foo'tv :: TVar Foo , bar'obj :: ObjRef Bar ... } I need an ObjRef to the context object (or any of helper sub-objects) at hand at all times, so I use a reader monad: type ObjT a = ReaderT (ObjRef a) To make things easier I define some helper functions, like: member :: Monad m =&gt; (a -&gt; b) -&gt; ObjT a m b member f = asks $ f . fromObjRef readVar :: (a -&gt; TVar b) -&gt; ObjT a STM a readVar = lift . readTVar &lt;=&lt; member subobj :: Monad m =&gt; (a -&gt; ObjRef b) -&gt; ObjT b m c -&gt; ObjT a m c subobj f = withReaderT $ f . fromObjRef (I wish I could wrap also fireSignal, defMethod, and the like from HsQML, but I can't get through the type black magic) Voilà! Now I have something almost, but not entirely unlike, object oriented programming ;)
It's funny that you call this "Python style". This ternary operator exists in many modern languages, sometimes with varying syntax, and they all ultimately got it from C. If you also want to consider extinct languages, the [Wikipedia article](https://en.wikipedia.org/wiki/%3F:) claims that the ternary operator originated in CPL and came to C via BCPL and B. EDIT: Oh because you want to reverse the order of the condition and the then-clause? Python is indeed one of the only languages that does that.
I get a different type for `bool`: Prelude&gt; import Data.Bool Prelude Data.Bool&gt; :i bool bool :: a -&gt; a -&gt; Bool -&gt; a -- Defined in ‘Data.Bool’ That type makes `bool` the [catamorphism](https://en.wikipedia.org/wiki/Catamorphism) for `Bool`. 
I saw this presentation at LambdaConf - it was great then and great now. It got me started with my first Haskell web app.
Do you find that in any language, or just Haskell? I think I feel the same way,but haven't been able to articulate it.
Not so fast. I did not take part in the survey and i'm using emacs. 
I've found it in other languages too. I feel all of these "rail-like" (including rails) frameworks are most useful for small projects and prototypes. As the project becomes big you end up using just a subset of it (which means they end up getting in the way). I haven't used yesod or snap on large projects but I have done Play (Scala) and Rails (for a short while) and felt the same way. 
that narwall is doing the [dean scream](https://youtu.be/l6i-gYRAwM0?t=24) @ time 0:36 for those who happened to miss this [cultural oddity of US politics in 2004](https://en.wikipedia.org/wiki/Howard_Dean#Iowa_Caucus_Setback_and_the_.22Dean_Scream.22_media_gaffe)
Link to survey https://www.reddit.com/r/haskell/comments/3bqy5h/survey_which_haskell_development_tools_are_you/
&gt; that is probably the way to go as Vim is not very capable ~~in attaching background processes.~~ FTFY. ;)
I received a second report along similar lines to what you were experiencing, so I'm trying to get more information to help diagnose it. I've sent [an email to the list](https://groups.google.com/d/msg/haskell-stack/19Vgh265bgw/ftnlUObDV68J) requesting more information, if you have anything to add there, it would be much appreciated.
I disagree. The main benefit is that of named arguments. Positional arguments, especially ones of the same type are terrible and error prone. The if syntax labels the branches to make argument meanings clearer and an error less likely. I also disagree strongly with your claim about syntax trees. Most of us agree that infix is extremely useful notation, and your argument is that "we think in syntax trees" so infix should be harder, but it's just not true. Infix is easier for most people. Python places the condition in the middle such that if you nest conditions the visual shape of the expression more closely resembles the shape of the condition tree (the branches in each tree node are the true/false decisions, which you'd draw on the left and right). 
Really? I've had it open for about a month and don't see any issues. What OS are you on and what version of spacemacs?
Assuming we do this (and I really hope we do!), is this gonna be part of the next Haskell Standard?
Thanks for writing the book, got it as soon as it came out. Lots of great code snippets. I always keep referencing the chart of string-like type transformations.
It seems that Emacs and Vim are the main editors. I expect that Atom will quickly become another main editor given how quickly it gained popularity (combined with that it is open source and uses JS for plugins). There is very little mention of `hdevtools` (2) and `ide-backend` (0). On the other hand I was surprised to see so many using `hlint`. Finally this survey pointed me to the existence of `ghcid`, somehow I never heard of it before. In the comments of this survey we also found many pointers that it is hard to install sophisticated Haskell tooling. For that reason many used more conservative tooling then what is currently available. Much of these installation problems could probably be solved by Stack. Then I found out about [stack-ide](https://github.com/commercialhaskell/stack-ide), which is an Stack aware kind of `ide-backend` that has my hopes for bringing easily installable advanced Haskell tooling to the masses.
Yeah, we're around 100 employees, so we do not deal with work visas at this point. Sorry.
&gt; What OS are you on He already said he was on emacs :P
&gt; one should export all innards of a package anyway, under an "Internal" subtree This suggests, to me, nested modules. Those aren't supported, so how do you expose the separately like that?
My experience has been that only people who can tolerate this are people who would have been fine using vi instead of vim anyway. Evil mode is by far the best emulation, but lacks support for all of the plugins that I use and behaves slightly differently even for some stock text objects. Arguments about embedding other processes never really won me over. There's no reason to emulate a window manager in Emacs. Just use a window manager. Even just tmux is fine and supports sending keys if that's your thing. Ive tried it four times now and have only lasted about six days total before going back :(
Short summary: * build from small, provably correct pieces (functions like `length`, `mapM`,...), * leverage type classes &amp; their laws (Monoid, Applicative, etc.), * use equational reasoning, * (implicitly) make use of lazy evaluation and use custom control structures instead of writing the control flow by hand. All good advice, but the title might be misleading. The talk is about *building* provably correct software, not about proving *existing* software correct. It's one thing to maintain strict discipline when writing and designing a system, but if you have some slapdash legacy application, none of this is going to help you anymore. A C model checker, on the other hand, might be useful even then.
Ha, funny! In mathematics we define a monad precisely as things ``m`` such that you can collapse sequences ``m (m (… (m x)))` to `m x` with `join`, and it doesn't matter which order you use. Also, you need a compatible function `return`. Bind is only secondary. The Haskell definition and the mathematics definition are equivalent though.
&gt; maybe you've never needed to check whether the reversal of an infinite list was empty, It's not a question with a well defined answer. 
That's correct. I should have clarified that this cannot be easily applied to an existing codebase without heavy refactoring, although I have done this as an exercise on a few occasions and it is possible.
At that point I feel like I'd use the free monad over the IO Functor.
So optimal reduction *is* the best way to do it today? What flavor of it? Would you, if not abusing, point me to a resource explaining exactly what I have to implement? Thank you.
Why not? Shouldn't the concept of 'is the same length as' still have meaning even in infinity, even if it is hard to actualy calculate the length?
Am I the only one that spells mempty, mappend as `m empty` and `m append`?
I'm half and half. I pronounce them "em empty" and "mappend".
I think limit can be treated as a monoid if the library recognizes there can be only one and if you ask for it twice just let's the smaller one win. e.g: limit 5 &lt;&gt; limit 6 = limit 5 limit 6 &lt;&gt; limit 5 = limit 5 It's probably fine to let the rightmost one win also.
There's a lot of stuff that is still ongoing in development, people aren't always going to have answers to your questions, and they very may well never have answers that make sense to you. You can look at that as an opportunity to shape your own programming philosophy more, that's basically what I did when I got stuck - try to find opportunity in what is actually quite challenging. That said, I don't program a ton in Haskell, so take my advice with a handful of salt. Some things just take a really long time to learn, and so much of what you learn along the way is going to be stuff you have to theoretically come to grips with yourself. If you've got hard questions without answers, the best person to try to answer those questions correctly is yourself. A friend of mine says that "perfect is the enemy of good". Just start somewhere, a little bit, instead of not starting. I get the phobia of starting, of making decisions, when you are working with something that seems like it was built from perfect decisions. But even the tiniest of steps might help you see that you are actually making gigantic strides in progress, it's just hard to evaluate when it's all crammed up in your head, and not a lot of people care about the topic to pay enough attention to listen to you, and those that do are either too shy or too sure of themselves to help provide a clue. 
&gt; In mathematics we define a monad precisely as [mu and eta obeying laws] Can't you also define the Kleisli structure for an endofunctor, and then define a monad to be the additional structure needed for that endofunctor to make Kleisli a category? Kleisli composition is pretty close to Haskell's bind operation, I don't think one (bind or join) is more primitive than the other.
This trick seems related to the unambiguous choice operations provided by the [unamb package](http://hackage.haskell.org/package/unamb). I used to know a nice article introducing the reader to an unambiguous `zip` operation closely related to your assertSpine, but I can't find it anymore. If anyone has a hunch what I'm talking about, please do post the link! For now, I can only remember one [article by Conal Elliott](http://conal.net/blog/posts/functional-concurrency-with-unambiguous-choice), which is about the library in general only though.
I am not an expert on pseudorandom numbers, but this is a strange approach because each time you run the program you are seeding the random number generator with a new seed. If you replace the last line with mapM_ print (take 9399 (randoms g :: [Victim])) and just run the program once you'll generate all your random names from the same seeded generator. Regardless, I'm not sure your data is unexpected. I get a similar spread when I do it this way, but Henry is not always at the top. $ cat log | sort | uniq -c 1919 Bob 1883 Carlo 1869 Deepak 1851 Henry 1877 Joe
I don't know much about emacs but I've been using it for a little while with spacemacs. Does anyone know how to configure spacemacs to use the github-master version of haskell mode?
Just dive in and build something, even if it's not the best approach. Haskell's very, very, very refactor-friendly because of the awesome type system, so you don't need to architect your application correctly the first time.
&gt; Haskell's very, very, very refactor-friendly As a relative noob, I'd like to voice my agreement here. In my only serious project so far, near the end I realized that I'd made a bad decision in module that half the code relied on. I needed to change it and everything that used it. Fixing that was one of the easiest tasks in the entire project! I don't think it took more than 15 minutes. It's very true that as a newbie, figuring out how to structure things in Haskell can be a huge and frightening task. But the thing that's not intuitive when you're used to imperative languages is that once you've got a structure in mind, you've basically written the code in your head because Haskell is a language for describing structures. All that's left is to type it out. And with the flexibility of higher-order types and functions, you can do this one small piece at a time.
Hasn't unamb been pretty much obsoleted by Lindsey Kuper's work on `LVar`s, and the [lvish](http://hackage.haskell.org/package/lvish) library? Unamb sometimes works and sometimes doesn't. The way I understand it, Kuper's work provides a theoretical underpinning for when it works and when not, and provides an alternative whose types ensure that you use it only in the cases that are guaranteed to work.
It is this sort of comment that makes me wonder if I should have taken more math theory before blundering into programming
You wouldn't have learned category theory, so I wouldn't sweat it personally. There's a good number of working mathematicians who have never heard of it.
Yeah, to make an analogy: Monoid : Monad :: mempty : return Monoid : Monad :: mappend : join Monoid : Monad :: mconcat : Control.Monad.Free.retract Monoid : Monad :: [a] : Free f Monoid : Monad :: (a, a, a, a, a) : f (f (f (f (f r)))) So to make an analogy to simpler types, your solution is one that works on "lists" of functors of arbitrary size, and my solution works on "tuples" of functors of fixed size.
Most of those are solved by unrelated techniques and don't require any equational reasoning: &gt; world simulation gives exactly the same results on all clients Encode your world simulation as a `Fold` from my `foldl` library so that you have a pure update function and it's automatically pure and reproducible on all clients &gt; game doesn't leak info ... This is impossible to prove in Haskell as far as I can tell. I have no clue how you would do this. &gt; impossible moves .... are impossible ... This is the job of the type system. Make impossible states unrepresentable &gt; all possible combinations of item and commands properly handled without game crash This is also the job of the type system: "A well typed program does not go wrong" which in this context means that it will never crash. This is even truer in "total" programming languages, which statically guarantee that all expressions terminate without any errors
&gt; game doesn't leak info ... This could potentially be encoded in an information flow control (IFC) system. One example of this in Haskell is the [LIO framework](https://hackage.haskell.org/package/lio)—it is dynamically enforced, but there are also static versions of IFC which put secrecy/integrity in the type system rather than a special runtime layer. They tend to be much trickier to work with, though. My suspicion is that static IFC would require a lot of dependent types to make it practically useable, and as far as I'm aware, there's still a lot of work to be done in that area.
Unfortunately though mine's actually a solution for lists of monads, not functors. That said yours is too (?). There is a generalization you can make for functors, but you lose the ability to control the ordering. As I typed this I realized/remembered you only need applicatives for either.
That's disappointing. [This](https://www.reddit.com/r/haskell/comments/2tb8k2/modular_code_and_lazy_evaluation_in_haskell/) is the other thread I was talking about, which as it turns out also explains why `sort` itself needs to be modified to have O(n) indexing.
Wow, thanks!
Haskabelle is nigh-on unusable, to be honest. I don't know why the Isabelle maintainers keep it in their tree.
Have you tried using [`uniform`](http://hackage.haskell.org/package/MonadRandom-0.4/docs/Control-Monad-Random.html#v:uniform) from Control.Monad.Random? I think something like this might work: main = evalRandIO $ uniform [(minBound :: Victim) .. (maxBound :: Victim)]
&gt; In my opinion, a distinction between relative and absolute paths isn't terribly useful, because they're not categorically incompatible. For instance, you could append a relative path to an absolute one or a relative one to another relative one. Perhaps paths could be a category? Like: Path Root Directory . Path Directory File = Path Root File I swear I saw someone mention this concept somewhere, but I can't find it now. &gt;In the case of separation between directories and files, outlawing something like "file1.txt" &lt;&gt; "folder" might be nice, but impossible, as both are valid directory names. I think it should be up to the programmer to decide at the moment of the path's creation whether it's supposed to be a file or directory path. Perhaps something could be done with TH to validate literals for a given platform.
On the second syllable, just like the word "append".
It's like learning a foreign language. The person is old, but the vocabulary is like children's. I started learning programming when I was very young. And then when I started learned C++ I thought it was like any other (imperative) languages. Now I have been using C++ for about 7 years, and Haskell only 2 years. When I had been using C++ for 2 years, I didn't know my code could be beautiful and readable. All that makes me happy is that my code works and runs fast. But now that I know, there is always someone inside my mind saying "your code is ugly". Hard to get rid of it. I must be having [OCPD](https://en.wikipedia.org/wiki/Obsessive%E2%80%93compulsive_personality_disorder). -_-||
Then you have something more than a monoid though. 
"joins can be deduced automatically from an join graph." Any ideas on generating the join graph? Particularly for DBs that don't have the constraints built in.
Partially initializing a record doesn't really make sense in haskell. After all it is immutable so you can't initialize those fields later. There are basically two ways around the problem of not being able to initalize everything at once. Often you can use Applicative style to initialize your datastructure. If that's not possible I recommend either using a different structure if the intermediate state is actually used and not just an intermediate state or just sticking with tuples until you can fill your complete structure. So to summarize: If there is an undefined in your code it is very likely to be a bug and you should search for the place where it is in your code and fix that part.
His question is pretty much "how do I find the location of this bug?" so your advice to "find the bug and fix it" is probably not helpful.
Have you tried compiling with `-Wall` (or `-Werror`)? Another option is to compile with profiling enabled and pass the `-xc` RTS flag. https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/runtime-control.html Record fields are lazy by default. If you annotate the fields as strict then the compiler will throw an error if they are not initialized: data F = F { foo :: !Int }
Well, maybe I'm mistaken about what proving software correct actually means - I was under the impression that it can be used in place of QuickCheck, just instead of testing small, random subset of inputs it would use equational reasoning? And that there are some tools (proof assistants?) that could be integrated into build process? If not, could you please give couple examples of what kinds of proofs would be written for game software? The example game engine features I listed prove to be very, very hard to achieve even in commercial engines, so please understand my skepticism that the solution can't simply be just using better type system? I mean wouldn't you need to somehow prove that the program is "well" typed? As for not leaking info to the players - it's about players not having information about other not seen players physically in their RAM, I already plan for having central trusted server, so that actually make it possible without homomorphic encryption:)
I was kind of expecting it to be like that, will try anyway. Are there other similar tools or currently most of proving is done by hand?
This is a connection I really wanted to explore, but unfortunately never got round to. I'd also love to hear if this is a real connection or not.
Tried setting a breakpoint inside the code and see where it fails when stepping through the program?
Not sure if this is what you're looking for, but you can try [LiquidHaskell](http://goto.ucsd.edu/~rjhala/liquid/haskell/blog/about), that adds refinement types to Haskell.
&gt; it is just impossible to calculate the first element of the reversal in finite time Which is to say, reversal is not well-defined over infinite lists. If you want to reason about the behaviour of reversal on infinite lists, you must first expand the definition of reversal to be well-defined over infinite lists. Actually, what this means is that the "last element" function is not well-defined over infinite lists. It is not simply the case that it takes infinitely long to compute the last element of an infinite list; the "last element" *does not exist*, any more than the "last element" of an empty list exists. &gt; we do know that reversal is a shape-preserving operation We know that reversal is a shape-preserving operation on finite lists. That does not imply that a suitably extended concept of reversal would have the same property over infinite lists.
&gt; the latter doesn't make sense relative to a 'theory of computation' It may not have been a motivating factor for the typed lambda calculus, but as I understand things, there's at least one straightforward interpretation of "consistency" w.r.t. a type-based theory of computation: that it is impossible to produce a term of type ⊥.
I think there is more difference than syntax and math concepts: there is culture.
1st step to "undebuggable" bugs: misunderstand bugs.
Have you tried to feel for 'undefined' (in all packages). If they are too many replace them by an error with a message indicating the location.
Nope! For example, fibs = 1 : 1 : zipWith (+) fibs (tail fibs) would require f(1) = 1 f(1) = 2 Which contradicts the definition of `f` as a pure function (as required by the type of `iterate`).
&gt; What do you gain by encapsulating your state? It is still just as many bits of state I think what you said is probably called packaging, not encapsulation. Encapsulation means hiding the more (implementation details), and exposing the less (interface). If not, then what or how do you encapsulate?
On the face of it, as long as it's not consistent, an error by roughly 1% from the expected frequency doesn't seem a big deal. However, assuming a flat distribution, the probability of getting more than 2000 Henrys from that run is about 0.001, and to get more than 2015 had exceeded the accuracy of the online binomial calculator I just used (it said 0.000). In any case, the chance of you both getting over 2000 for the same item is less than one in a million, but given that it was a different favourite, that's 1 in 250 000. I think it's fair to conclude that the distribution is unlikely to be completely flat. 
A useful replacement error value is `assert False undefined` which then gives the precise error location when it gets executed
That message only comes from direct invocation of `undefined`, any other thing (such as partial record construction) has its own special error message (usually with line numbers). So, try grepping for `undefined` -- *someone* is calling it on purpose.
Working &gt;&gt;&gt; pretty. Hard thing to follow but the truth. Also keep in mind that lots of OOP still works in Haskell, encapsulation is obviously still beneficial, you just have objects that return modified versions instead of internally managed state.
I teach Haskell courses and one of the issues that repeats every year is difficulties with deeply nested expressions. For a beginner sbf = concat . sortBy (comparing length) . group . sort Is pretty much indistinguishable from sbf = concat . sortBy . comparing length . group . sort Or any such variation, whereas the more ugly sbf xs = let runs = group (sort xs) orderedRuns = sortBy compareByLength runs result = concat orderedRuns compareByLength a b = compare (length a) (length b) in result is often understood more easily. (Also, you can add types to the latter if necessary) Also, in my experience, where beginners fall, experts may stumble. 
Perhaps "at all costs" is too strong an expression.
HERMIT can be used for this, although the reasoning is very fast and loose.
Take this with a grain of salt as I am still a Haskell newb myself. If you are familiar with `ActiveRecord`, [`persistent`](https://hackage.haskell.org/package/persistent) is a similar idea. It can take care of the CRUD operations for you. There is also [`esqueleto`](https://hackage.haskell.org/package/esqueleto) which I believe is meant to be similar. As for web frameworks, you have [`yesod`](https://hackage.haskell.org/package/yesod) which I hear most people referring to as the most rails-like framework in Haskell. There is also [`snap`](https://hackage.haskell.org/package/snap), which people relate to sinatra. [`happstack`](https://hackage.haskell.org/package/happstack-server) is unlike the other frameworks, I believe it is also the oldest. There is also [`scotty`](https://hackage.haskell.org/package/scotty) which to me seems the most Sinatra like. And finally there is [`spock`](https://hackage.haskell.org/package/Spock) which appears to be another Sinatra-esque framework. If you are just building an API to produce JSON or something, [`servant`](http://hackage.haskell.org/package/servant) looks really interesting. However, I have also heard that it is a fairly high entry barrier. Keep in mind there are probably a fair few more web frameworks worth looking at in Haskell. These are just the ones that I have had referred to me in the past. *Note*: I've not done anything serious in any of these packages. I've really only followed along with the tutorials.
First part here: https://www.reddit.com/r/haskell/comments/3c1fzy/set_theory_and_haskell_part_1/
&gt; sbf = concat . sortBy (comparing length) . group . sort &gt; Is pretty much indistinguishable from &gt; &gt; sbf = concat . sortBy . comparing length . group . sort But the second one won't typecheck. What is the problem?
Yep this is something I struggle with. The only way I can decipher those concatenated expressions is by constantly checking the type of the expression I've written so far using :t in GHCi. Even then, when it all fits together, it still seems like voodoo to me.
Understanding *why* it doesn't type check is presumably the problem. A big pain point of learning anything (in terms of programming) is when your expectations don't match up with the compiler's interpretation. To the beginner, they may read the two as equivalent, but in reality they are very different. Understanding that can be hard work.
You can encapsulate many implementation details but you can not encapsulate state. n bit of state mean 2^n different behaviours for your object, it doesn't matter whether we know if that state is one 4 Byte String or one 32bit Integer.
I appreciate the link, but would it be possible to present it in a format more digestable than a 12 page technical paper?
http://snapframework.com/media/img/pong-bench-20101117.png (Happstack and Snap are Haskell *frameworks*, Node.js isn't even a framework so no wonder it's able to be fast!) http://www.yesodweb.com/assets/new-warp/result.png (Mighty/Mighttpd is an HTTP server written in Haskell) http://www.aosabook.org/en/posa/warp-images/benchmark.png ( - || - ) Haskell is much web scale.
&gt; What am I getting myself into trying to write a REST API with a haskell framework? (I don't know haskell, but am very interested) A fun learning experience. Surly what you want to do is possible with Haskell, and not even that hard. But as you have probably already noticed there is a learning curve to Haskell that is steeper than, lets say, learning Ruby when you already know Java. &gt; Like I said, I don't mind if it costs me extra time since I'm learning, but what are we talking here? I'd advise you to learn Haskell basic by first making a non-networked app. I think Chris-bitemyapp-Allen's [advice on how to start](https://github.com/bitemyapp/learnhaskell) is great. Basically, don't use Haskell Platform, and start with the [cis194](http://www.seas.upenn.edu/%7Ecis194/spring13/lectures.html) course. Once that course is done you will be --in my opinion-- good to go with making a basic API server. Search this subreddit for discussions on the best REST API libraries, as there are a few. Maybe you want to try out more then one before deciding the on the lib to use for your work projects.
I agree with the spirit of the post, but I think it goes too far. In my early Haskell days, I noticed a very clear trend that when I tried to string things together too much it took me much longer to get it working than if I broke things down and assigned a name to each step. This was especially true with monadic code. Part of the problem was that I was still learning monads, but I've noticed that the trend persists in my programming (although much reduced) even today. So I think a balance must be struck. You don't want to name everything because finding the right names is hard. But you don't want to name too few things because then the code becomes hard to understand and modify. This is the crux of why building good software is hard. There are rarely absolutes--almost everything is a tradeoff. I also think this is why Haskellers use one letter names so much. Haskell allows us to break things down a lot more than most other languages, so in many places we don't need descriptive names. When you have "n = length xs" in some where clause, you don't need a more descriptive name because you can look at the definition and immediately know. This, of course, is not the case everywhere. In many places you really do need good names. At the end of the day, the right approach is to not go around mindlessly applying rules.
&gt; We also show that with Mio, McNettle (an SDN controller written in Haskell) can scale effectively to 40+ cores, reach a throughput of over 20 million new requests per second on a single machine, and hence become the fastest of all existing SDN controllers. &gt; &gt; […] &gt; &gt; For comparison, we also measure the performance of nginx, arguably the world’s fastest web server, written in C specifically for high performance. […] the graph also demonstrates that a realistic web server in Haskell, mighty, performs within a factor of 2.5x of nginx for every number of cores and performs within 2x of nginx for 8 cores and higher. &gt; &gt; […] &gt; &gt; As a result, the performance of network servers in Haskell rivals that of servers written in C. Tip for the future: if you're not interested in the problem or how it was solved, only the actual solution, look at the "abstract", "result" (in this case called "evaluation") and "conclusion" parts of a paper. And skim based on headlines and diagram captions.
That was a silly mistake on my part. I'll update the codebase to use the https URL. Thanks for the report /u/tkx68. __EDIT__ In order to get the newest version in this case, either download the binaries from the download page or manually clone the repo.
From what I've seen from beginners, learning Haskell through a project like this is a very frustrating experience. I would suggest at the very least following one of the courses aimed at teaching you Haskell before trying out one of the web frameworks. Other than that, Haskell is very very well suited for the task.
You can see [the ongoing discussion on uninstall](https://github.com/commercialhaskell/stack/issues/361). For upgrading packages: in general with stack, you'd just modify the `resolver` value in your stack.yaml to point to a newer snapshot, and stack will handle all necessary upgrading. We definitely aim to make dependency issues less of a problem, mostly through usage of curation and making reproducible builds first-class. But "versioning / dependency issues" is a broad enough category that I'm certain there are lots of things that we're not solving.
Yes, these are quite nice but quite old as well. See also this [reddit thread about WAI being #3 in some benchmarks](https://www.reddit.com/r/haskell/comments/346px3/web_frameworks_benchmark_wai_3/).
&gt; encapsulation is obviously still beneficial What do you think stands beside encapsulation? 
You're not getting yourself into too much trouble. I was rather new to haskell and was very happy with scotty to get started and expose a simple API.
Does this have any negative gotchas? instance (Functor f, Monoid a) =&gt; Monoid (Free f a) where mempty = pure mempty mappend = liftA2 mappend job' :: Int -&gt; Free IO () job' n = do liftF . putStrLn $ "Generating job " ++ show n liftF . putStrLn $ "Packaging job " ++ show n main = retract $ job' 3 &lt;&gt; job' 5 
As I have matured in my Haskell writing, I have actually started writing what I perceive to be more "beginner friendly" code for the reason you mention. If I have to choose between zip [1..] (lines log) and zip [1..] $ lines log I will pretty much always choose the former. I rarely, if ever, use dollar signs for "parentheses avoidance" these days except for when I'm using functions that look like control structures. It is for example not odd for me to do parseLog log = fromList $ do (i, line) &lt;- zip [1..] (lines log) rights [fmap (reqIdL .~ i) (parseOnly parseLine line)] (As can be seen in that example, I also have started preferring `fmap` over `&lt;$&gt;` unless the latter really does make the code nice.) I'm also writing a lot more of gravity entity = if entity ^. entityFalling then entity &amp; entityYSpeed +~ 1 else entity over gravity entity = entity &amp; if entity ^. entityFalling then entityYSpeed +~ 1 else id or even worse gravity = bool (entityYSpeed +~ 1) id =&lt;&lt; view entityFalling or whatever weird incantation it could be defined as.
That prints them out in sequence for me, which isn't the desired behaviour. The correct output of that is supposed to be: Generating job 3 Generating job 5 Packaging job 3 Packaging job 5 But yours has jobs 3 and 5's generating and packaging squished together.
Nix is not a Haskell tool. My point being that Pythonistas have pip to manage packages (install/update/unisntall), Javascripters have npm. Haskellers surely shouldn't be expected to use cabal/stack + Nix for package management, especially if you consider the overhead in getting started. For comparison, To use pip, one need only understand how to write a Setup.py. For what you seem to be suggesting, I need to not only have a stack.yml file, but a nix expression as well. I hope that something comes of the discussion /u/snoyberg linked to (especially if you consider [the last comment as I write this](https://github.com/commercialhaskell/stack/issues/361#issuecomment-118689398).
Significantly incompatible changes, as in the majority of packages no longer built. One was splitting syb out of base. Another major change I remember was the switch to new style exceptions. I'm sure there were other changes too.
There's already a stack-ide project for ide-backend support which is pretty far along. I don't know about ghc-mod, but some people have discussed it in the past.
&gt; Node.js isn't even a framework so no wonder it's able to be fast!) And that benchmark compares apples to oranges, err... snap to Ruby on Rails. Good job.
Nope.
Structure mainly. You still have members and functions that operate on those members even if you don't have `this`.
This is a bit of a disingenuous statement since both install packages. What tool do you use to install cabal packages that your project is dependent on? 
It doesn't. A list is an enumeration (a mapping from N to your set). The reversal is not an enumeration as don't have a way to define it: no matter what value 1 maps to, it will be incorrect. 
`ghc-pkg` is a package manager. it does not build however.
*In the olden days, it was par for the course to answer queries about haskell package management with "cabal is not a package manager".* This is a true statement, but it omits the fact that ghc-pkg is the package manager for GHC Haskell. Stack is an alternative build tool to cabal-install, but it still installs your packages using ghc-pkg (via the cabal library).
ghc-pkg is also a package manager.
&gt; There are uncountably infinitely many infinite streams Ah, but the uncomputable ones won't be inhabitants of `S`!
I suspect most of the performance improvements you're seeing are due to using intermediate arrays, and not from any secret sauce radix sort magic. See http://lpaste.net/136015 for an example of another sort that beats the default sort by a large margin, just by copying everything into an array and sorting it there. sort :: Ord a =&gt; [a] -&gt; [a] sort xs = Vec.toList $ runST $ do let l = length xs arr &lt;- MVec.new l _ &lt;- drain l xs arr I.sort arr Vec.unsafeFreeze arr My results: -7212132088802539410 "local" -7212132088802539410 CPU time: 0.64s "Standard" -7212132088802539410 CPU time: 4.27s 
I suspect the term you are looking for is compose but I am not sure how the fact that an object is the composition of multiple primitive values applies to the question whether or not you are able to encapsulate (hide) the number of possible states your object can have.
Does LIO (or anything similar) handle the case of leaking information through timing attacks?
Indeed, I've later written an inplace quicksort and it performs really well. I would like to see if I can further optimize the radix sort to beat it but the asymptotics were only the reason I started looking into it. It wasn't really my goal to show that it's faster than some other implementation.
`s` is `streamMap head (streamIterate streamTail s)` if that helps.
The asymptotics are fishy anyway. People get all hot and bothered about radix sort on fixed size ints being O(n), but that's just wrong. Radix sort on fixed size ints is O(nw) where w is the size of the int type, and w &gt;&gt; log(n) in every practical example.
Waves indeed. It's still the [top rated r/haskell post of all time!](https://www.reddit.com/r/haskell/top/?sort=top&amp;t=all)
This looks like really exciting stuff, but the slides don't seem to be synced to the video for me, which is making this somewhat harder to view than it could have been. Is it just me?
It seems you're right, which is very unfortunate; the slides seem to be rotated by half the talk's length or something. I was there live so I didn't watch the video myself, and skimming through it didn't look awkward enough to notice. I'll see what I can do about this, sorry about the borked link.
Very handy to know about GHCi, thanks! For -xc, will `--enable-executable-profiling --enable-library-profiling` in Cabal be enough, or do I need -prof?
I thought I had -Wall on, but I didn't. That's a very neat trick about the strict annotations. Thanks!
Personally I've always preferred the style of gravity entity = if entity ^. entityFalling then entity &amp; entityYSpeed +~ 1 else entity since my if-then-elses in Haskell often have 1 line, but if they do have more than one line: fastGravity entity = if entity ^. entityFalling then do entity &amp; entityYSpeed += 1 entity &amp; entityYSpeed += 1 else entity would be better IMO. I don't do this in any other language, strangely, if I'm using a C-like language I put the open brace on the same line as the if. I'd say the reason why I prefer it this way in Haskell is because the `then` and `else` are logically contained in the `if`, and therefore should be more indented, but just indenting `else` looks ugly.
100%
Credit for the trick goes here http://dev.stephendiehl.com/hask/#bottoms 
Of course, this is a discussion thread about style, though. I'm just sharing my slight permutation on valid haskell syntax ;)
Fair enough. I was focusing on differences that would still be visible after lexing. Hence my confusion!
Not heard of halive - will definitely try this. Thanks!
w is a constant. log(n) is not. how the heck is that fishy.
I think the cabal flags are enough and shouldn't be combined with `-prof`. However, you will probably need to pass `--auto-all` to GHC.
My comment was all Haskell. Although note that 'up to isomorphism': That means ((1, 2), 3) should be equivalent to (1, (2, 3)), which obviously isn't true for Haskell.
Yes, there is. Deleting in 5...
The short tale of how I got over a pet peeve - that `Applicative` laws did not seem to have a presentation as neat as that of the `Monad` ones using `(&lt;=&lt;)`. I'm quite certain the resulting presentation of `Applicative` in terms of static arrows is already well-established (even if there is no popular literature about it) though not terribly useful (which explains why there is no popular literature about it). In any case, it is a fun thing to know about. (Link fixed, thanks /u/dougmcclean .)
I never actually understood that. w is only the size of the int type (if by size you mean the number of bits) if you use two buckets, but why would you use two buckets? I chose 256 of them so as far as I understand my algorithm (and profiling also shows that) iterates through each element 8 times. That's less than log n for n &gt; 256.
w is a parameter, not a constant. 
I've always wondered why each language needs to have its own package manager. Do they all not boil down to roughly the same thing? I wouldn't be unhappy to use a non-Haskell-specific package tool, as long as I wasn't missing out on features.
but you said "fixed size ints" and "w is the size of the int type." so w is fixed. to be the size of the int type. which is fixed.
Exactly, that why n has to be more than 256. Let's say n = 2^16 and we're sorting 64bit integers. nlogn sort will do 16 * 2^16 comparisons/swaps, while radix sort will only do 8 * 2^16 + 8 * 256. You could make the argument that you radix loses out when we increase the number of bits in the integer but I'm pretty sure comparing two 128bit numbers will be twice as slow as comparing two 64bit ones. 256 buckets and 2 buckets is not the same thing in this context since the w parameter describes exactly that number.
Error from the first bit of code: test.hs:9:14: Could not deduce (t ~ String) from the context (Show t) bound by the type signature for getProperty :: Show t =&gt; Test -&gt; t at test.hs:6:16-34 ‘t’ is a rigid type variable bound by the type signature for getProperty :: Show t =&gt; Test -&gt; t at test.hs:6:16 Relevant bindings include getProperty :: Test -&gt; t (bound at test.hs:7:1) In the expression: a test In a case alternative: A -&gt; a test test.hs:14:5: No instance for (Show s0) arising from a use of ‘print’ The type variable ‘s0’ is ambiguous Note: there are several potential instances: instance Show Double -- Defined in ‘GHC.Float’ instance Show Float -- Defined in ‘GHC.Float’ instance (Integral a, Show a) =&gt; Show (GHC.Real.Ratio a) -- Defined in ‘GHC.Real’ ...plus 24 others In the expression: print In a stmt of a 'do' block: print $ getProperty t In the expression: do { let t = ...; print $ getProperty t }
On any particular machine, w is fixed. When talking about radix sort in general, w is a parameter. Also, I'd like to defer to [wikipedia](https://en.wikipedia.org/wiki/Radix_sort#Efficiency)
You are creating a new database connection on every request! That can't be a good example template. Did you find it difficult to keep state across requests? 
#####&amp;#009; ######&amp;#009; ####&amp;#009; Section 1. [**Efficiency**](https://en.wikipedia.org/wiki/Radix_sort#Efficiency) of article [**Radix sort**](https://en.wikipedia.org/wiki/Radix%20sort): [](#sfw) --- &gt;The topic of the efficiency of radix sort compared to other sorting algorithms is somewhat tricky and subject to quite a lot of misunderstandings. Whether radix sort is equally efficient, less efficient or more efficient than the best comparison-based algorithms depends on the details of the assumptions made. Radix sort complexity is *O*(*wn*) for n keys which are integers of [word size](https://en.wikipedia.org/wiki/Word_size) w. Sometimes w is presented as a constant, which would make radix sort better (for sufficiently large n) than the best comparison-based sorting algorithms, which all perform *O*(*n* log *n*) comparisons to sort n keys. However, in general w cannot be considered a constant: if all n keys are distinct, then w has to be at least log *n* for a [random-access machine](https://en.wikipedia.org/wiki/Random-access_machine) to be able to store them in memory, which gives at best a time complexity *O*(*n* log *n*). That would seem to make radix sort at most equally efficient as the best comparison-based sorts (and worse if keys are much longer than log *n*). &gt; --- ^Relevant: [^American ^flag ^sort](https://en.wikipedia.org/wiki/American_flag_sort) ^| [^Counting ^sort](https://en.wikipedia.org/wiki/Counting_sort) ^| [^Sorting ^algorithm](https://en.wikipedia.org/wiki/Sorting_algorithm) ^| [^Bucket ^sort](https://en.wikipedia.org/wiki/Bucket_sort) ^Parent ^commenter ^can [^toggle ^NSFW](/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+csu9zyv) ^or[](#or) [^delete](/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+csu9zyv)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](/r/autowikibot/wiki/index) ^| [^Mods](/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Call ^Me](/r/autowikibot/comments/1ux484/ask_wikibot/)
Why not guards? gravity entity | entity ^. entityFalling = entity &amp; entityYSpeed +~ 1 | otherwise = entity 
`getProperty :: Show t =&gt; Test -&gt; t` means, that you can get `getProperty` to return any `t` which has an instance for `Show`, but this choice has to take place statically. For example you could specialize `getProperty` by replacing the type variable `t` with `Int`, `String`, and `Bool` respectively, e.g. getPropertyInt :: Test -&gt; Int getPropertyInt = getProperty getPropertyString :: Test -&gt; String getPropertyString = getProperty getPropertyBool :: Test -&gt; Bool getPropertyBool = getProperty For your function these wouldn't make sense, as depending on what `Test` value you pass into one of those functions, they may in all three cases still return either a `String` or an `Int`. One way to fix this is to use a sum type instead, e.g. data Either a b = Left a -- This is also defined in the Prelude | Right b deriving (Show) getProperty :: Test -&gt; Either String Int getProperty test = case mode test of A -&gt; Left $ a test B -&gt; Right $ b test main = do let t = Test { a = "yes!", b = 5, mode = A } print $ getProperty t Here the compiler does not complain that `getProperty` may return either a `String` or an `Int`, because for both cases - `A` and `B` - it returns an `Either String Int` ;) Another way to fix this would be to use a ghc extension for [existentially quantified types](https://wiki.haskell.org/Existential_type). In your original type signature you universally quantify over `t`: the signature `getProperty :: Show t =&gt; Test -&gt; t` can be read as *forall types `t` which have a `Show` instance, take a `Test` and return a `t`*. Using existential types, you could construct a type signature, which reads as *take a `Test` and return something for which exists some type which has a `Show` instance*. Without knowing more about your usecase, I'd guess the first solution is probably what you are looking for. edit: I think in this particular case, the approach suggested in this [comment](https://www.reddit.com/r/haskell/comments/3cclva/id_appreciate_help_figuring_out_how_to_use/csubho0) is the way to go.
I tried to prove this once but I didn't manage it. Congrats for working it out! Has anyone written down the proof before, or are you the first?
I gave this a shot, but was still unable to write a `getProperty` function. I guess my real question is how can I write `getProperty` such that it can return any showable.
That would actually look better in this case. Not sure why I didn't think of that. Thanks!
Looks good though I think tables have a few more commonly used pieces of code, particularly when you take server-side pagination into account. I suppose to fully implement that general case in Yesod we would need something like a subsite that can be parametrized by data and row count fetching functions though and then we would get into the whole problem of identifying the first element of the nth page (ideally not just via limit and offset in case someone added rows while we were browsing pages). So in summary i guess that would be a much larger scope than yesod-table currently has.
Imagine if you could create the function you want, so you have a function that can return any showable. What could you do with the value it returns? Well, you don't know what type it is (it could be anything!), so you can't really do anything with it except `show` it, because you *do* know it's showable. But that's the only thing you know about it, so `show` it is literally the only thing you could do with it. If the only thing you can do with the return value is `show` it, why not `show` it directly in the `getProperty` function? You will have to `show` it sooner or later anyway, because you can't do anything else with it! So an equivalent function would be getProperty :: Test -&gt; String getProperty test = show (case mode test of A -&gt; show a test B -&gt; show b test) and that should actually typecheck!
Huh. So with probability 1, a randomly chosen instance of *any* infinite data structure is uncomputable.
Is this going into production? If so, and your time-table isn't above 6 months, don't do it. Seriously - Don't. I might say otherwise for someone coming from FP, but that's about how long it takes before you can produce anything of remote quality - and that's if you're good. 1 month basics of type system, 2 months in monads + typeclassopedia applicaton, 2+ more months in editor tooling and full acquisition of the paradigm. Perhaps another month on top of that if you want to _write_ monads as well. The Haskell community is filled with savants that will tell you otherwise, but this is quite realistic for a non-FP'er that is just starting.
I really like using F#'s/Elm's forward application operator `|&gt;` (I think Haskell got `&amp;`). OOP programmers feel immediately at home, plus you don't have to read from right to left: sbf items = items |&gt; sort |&gt; group |&gt; sortBy (comparing length) |&gt; concat Splitting `comparing length` into its own line would be awkward. The whole syntax is also reminiscent of bash's `|` operator. I get the part about the naming though. `items` is not descriptive at all, but I'd imagine this syntax is much easier to follow. I know the drawbacks wrt. splitting such an expression up, but that's something one discovers later on.
they can be tested only if hoisted out to the toplevel.
I think I'll end up using the `Either` approach. I would have used existentially quantified types, but I still would have needed to do something like `case getProperty t of Showable s -&gt; print s` in order to call a function on it, at least as far as I could tell. That's not the worst, but it's not quite as terse as I was hoping for. Ultimately, I think doing this with `Either` will be pretty easy to read, even if it isn't as terse as I was hoping for.
Here's an example: [how I implemented a combinations generator]( https://gist.github.com/jgcoded/957f59cbded3b78b4ae8). As you can see, I have very strong roots with imperative languages. After making that, I wondered how others did it. This was one of those one liners that [are super elegant]( https://gist.github.com/tom-lpsd/1115336). I was blown away when I saw that because it was so simple. It's coming up with those simple concatenated one-liners that I find difficult to do.
Not that I know of
Yeah, I think this is a bit of an XY-problem. For the curious, I'm using [vty-ui](http://hackage.haskell.org/package/vty-ui-1.9) to make a terminal ui. The have a lot of functions that operate on some `Widget a`. As far as I can tell, they don't belong to any typeclass (though I'm not sure that would really change my present situation). I was implementing a `Widget` that wraps `Widget Edit` and `Widget FormattedText`, only one of which is displayed at a time. I wanted a function with a type kind of like `getDisplayedWidget :: Widget MyWidget -&gt; Widget a` so that I could do `render $ getDisplayedWidget w` or `growHorizontal $ getDisplayedWidget w`. I ended up getting things to compile by having a function like `getDisplayedWidget :: Widget MyWidget -&gt; Either (Widget Edit) (Widget FormattedText)`. It's not quite as terse as I would have liked, but the file is still small enough that I don't think it will end up being that much of an issue.
:(
&gt; and w &gt;&gt; log(n) in every practical example. This doesn't fit with my experience. It comes down to the fundamental question of _why_ we are sorting? Are we sorting because we know _absolutely nothing_ about the order of the data we're given? Or are we sorting because it isn't _quite_ in the right order? In the former case stability buys us nothing, so we heavily encumber our sort with having to distinguish every single bit of information in the source some how in the interest of determinism. There? Sure, your assumption of `w &gt;&gt; log n` is likely true. Every sort must pay at least the sum of the lengths of the longest common prefixes between adjacent keys in the result worth of work, which is potentially a bit smaller. But the very reason I like to use stable sorts is because almost every data source I have will spit out my data in _some_ order, it just may not be exactly the one I want. I may want to regather it stably by employee, or trades by whether they are long/short, etc. This is typically far fewer bits of stable re-sorting than there are records in the data I'm re-organizing! Given how much of my day to day existence is consumed by problems precisely of this sort, we seem to have fundamentally different experiences on this front. I use stable radix-like sorts, hash-joins, etc. when appropriate because they let me exploit that reduction in problem size whenever log n does happen to be σ or whenever it _might_. Any sort will pay a cost that is at least the permutation entropy between the two orders. Then we just need to analyze the cost of keeping the data indexed in the right order against the cost of paying the appropriate permutation cost when asked based on whatever our problem is. Using a radix sort is just one tool in the toolbox for getting closer to that optimal permutation entropy bound. Your argument is grounded in the fact that there are cases where the radix sort must devolve to the same bound as a symbol-wise comparison sort, (random strings, etc.). Mine is grounded in the fact that there are cases where a stable radix sort wins over any stable-or-unstable symbol-wise comparison sort. These two claims are not mutually contradictory.
I mostly did it the bulky way to split up the different ideas; showing the structure first and then how to implement said structure.
Honestly, I don't think a very complicated solution is needed for this use case. getProperty :: Test -&gt; String getProperty test = case mode test of A -&gt; show (a test) B -&gt; show (b test)
That actually sounds like one of the rare appropriate spots for an existentially qualified data type: data GenericWidget where Forget :: Widget a -&gt; GenericWidget
My main beef with the rah-rah over radix sorts is that they don't do much for you when you're sorting fixed size ints, but people use the fixed size of int to then claim radix sort over them is O(n). Radix sort is great when dealing with larger lexicographic types, because the cost of comparison can become so much larger.
That implies w &gt; log(n). So wn &gt; n log(n). Which implies radix sort is not faster than a good comparison sort (within a constant factor, also with comparison cost being O(1))
How does the proof work for IO, knowing that this wrapper is prone to side effects?
Thanks. Fixed.
The detailed claim I make about a radix sort is that irrespective of word size `w = σ^k`, for any set of strings S, using σ buckets at a time, an MSD radix sort is `O(ΣLCP(S) + n log σ)` while the bound on symbolwise comparison-based sorting is `Θ(ΣLCP(S) + n log n)`, where ΣLCP(S) is the length of the longest common prefixes of neighbors in the final sorted result. What is increasing when you raise the word-size is w (and k), not σ. Depending on ΣLCP(S) this _may_ be a win, only caring about the distribution of strings in S, not the original order, while every comparison sort (even symbolwise) must always pay full price. So to turn your phraseology back on you, my main beef with folks who dismiss radix sorts is that they say "well it doesn't win in every case so hrmmph." =)
As far as I'm aware, LIO has nothing to prevent leakage through side-channels like timing attacks. I don't know of any language-level (i.e. type-driven) way of preventing side-channel attacks, but I would be fascinated if anybody here has heard of something like this.
Since we're talking at such an abstract level: no, features that solve problems people are facing do not go against stack's philosophy. Though I don't think that statement is either surprising of illuminating ☺
Its primary target right now is Emacs, though we hope to support all commonly used editors. It's not actually designed for the web based IDE at all.
Hm, I messed with existentially qualified types a bit. I'll try this again.
I appreciate the straight advice. What is FP/FPer? Doing my own research, combined with your comment I've decided to learn haskell on a hobby-project, and develop my more important stuff with node/rails because they are so simple
Functional Programming
Wow that is cool! How did you come up with that?
I can't take credit for it. It happens to be known around some parts of the community. Anyway, it can be read in English as "for each element in the list, generate results both excluding and including it."
Lambdascope is a very compact implementation. Or you can try other approaches as outlined in Ian Mackie or J S Pinto's papers. They are all based on Interaction Nets. 
The following `Monoid` instance is always guaranteed to satisfy the `Monoid` laws: instance (Applicative f, Monoid a) =&gt; Monoid (f a) where mempty = pure mempty mappend = liftA2 mappend So if you specialize `f` to `IO` (which implements `Applicative`) you get: instance Monoid a =&gt; Monoid (IO a) where mempty = pure mempty mappend = liftA2 mappend So the correctness of the proof depends entirely on whether `IO` obeys the `Applicative` laws. We also know that if `IO` implements `Monad` correctly then `IO` also implements `Applicative` correctly. However, we have to take on faith that `IO` obeys the `Monad` laws.
One of the key tricks that makes this all work is that any interaction with external components can be "purified". The way you do this is that you mock the interaction by building a syntax tree representing external interactions and then a thin interpreter that translates the syntax tree to side effects. I wrote a [post on purifying `IO`](http://www.haskellforall.com/2012/07/purify-code-using-free-monads.html) that explains this in much more detail. Once you have that syntax tree, you can project that syntax tree into the types in a language with dependent types and encode tests over that syntax tree's behavior that get evaluated at compile time. Note that this will not work in Haskell because Haskell does not have dependent types, but it would work in Idris. There is one vulnerable part of the program, which is the interpreter that translates the syntax tree to side effects. The correctness of that can only be established by end-to-end tests or human inspection, so that's why you want to keep it as thin as possible.
Okay, that's pretty awesome, but... &gt; `[False ..]` ...I think *this* qualifies as "too clever." `[False,True]` is only two more characters and considerably clearer.
That's generally how I do it too, though particularly short things I'll sometimes squeeze into one line. I haven't yet decided whether I prefer indenting `then` and `else`, though; `if` doesn't actually require it and, being the same length, they still create a nice flow even if you keep them all at the same level: gravity entity = if entity ^. entityFalling then entity &amp; entityYSpeed +~ 1 else entity **Edit:** This is probably related to my preference for two-space indentation, now that I think about it; `then` and `else` are both two characters longer than `if` so it just looks right to me as-is.
Can you please link to the pull request? I searched but couldn't find it.
That is awesome! Is there any documentation on how to set it up with emacs? I found the video demonstrating 'ide-backend-mode' on FPComplete blog but https://github.com/chrisdone/ide-backend-mode/ seems to have disappeared. Is it still under development? 
I'm not sure that that description applies to any monad or describes what a monad is, so the answer is probably no :) 
Great job! I suppose that the amazing improvement in speed is because the current GHC IO manager block all threads when it is doing an OS call. Can I assume that Mio will be the IO manager of GHC in the next release? 
no, it's not. portage is package manager
TIL "static arrow" is exactly equivalent to [`Applicarrow`](http://stackoverflow.com/q/24668313/477476), which leads to the question of what the proper term for `Arrplicative` is.
You're welcome :)
I think this is where it lives: https://github.com/commercialhaskell/stack-ide/tree/master/stack-mode No documentation though
I'm pretty much a novice at haskell. I didn't even think of trying to keep state across requests. I just linked to the github repo to show that writing a simple (but not very efficient apparently) API with servant isn't very complicated. I'll see if I have some time to try and make it faster. 
I don't think you can easily distinguish valid/invalid `FilePath`s easily at construction time, as you'd have to know which (mounted) filesystem they're gonna be applied to. Linux for one is quite liberal on what values a valid `char pathname[]` may contain, and only when the fs layer for the respective filesystem gets passed the bytestring you may get an invalid-argument response. So I think the current proposal, i.e. not trying to be clever with `FilePath`s and consider them opaque handles is the safe and reasonable thing to do.
I believe the video for this talk will be available reasonably soon, along with the videos for all the other ZuriHac talks.
Yeah, but the gain in readability for newcomers is worth it, IMO. At least for getting things going, that is.
So it's a responsibility of the programmers to implement IO actions satisfying the Monad laws? The programming language can offer no guarantees when using impure functions?
Another alternative formulation of Applicative is in terms of a `zip` like operation: class Functor f =&gt; Zippy f where unit :: f () zip :: f a -&gt; f b -&gt; f (a,b) with the laws zip x unit = fmap (,()) x zip unit y = fmap ((),) y zip (zip x y) z = fmap assoc (zip x (zip y z)) If I understand category theory well enough, this is saying that f is a lax monoidal functor. See also [this stack overflow question](http://stackoverflow.com/questions/23316255/lax-monoidal-functors-with-a-different-monoidal-structure)
I think that's a good plan. You can likely do something equally good or better in Haskell than ruby, but not if it's your learning project. 
&gt; though not terribly useful Well, there is also the formulation in terms of a lax monoidal functor, as described in the [Typeclassopedia](https://wiki.haskell.org/Typeclassopedia#Alternative_formulation). It also seems to generalize better, for example to (shameless self insert) [mutable references](http://hackage.haskell.org/package/zoom-refs-0.0.0.0/docs/Control-Concurrent-STM-TVar-Zoom.html). But your version is interesting too, actually, I think you can easily get an Arrow (as in a type class instance, not the category theory concept) from functions of type (Applicative f =&gt; f a -&gt; f b), though I'm not sure if you can get back to Applicative then.
Did you try these steps - https://www.reddit.com/r/haskell/comments/3bw95a/using_cabal_and_stack_together/csqdbe2
I thought we are still on the topic of newbie friendliness, and by "test" /u/htebalaka meant "see if it compiles and how it works".
What does “not great core” mean (slide 18)? Does it refer to the GHC intermediate representation?
This is not quite finished yet as `runEffect` isn't implemented, but I'm sure Patryk will upload that by the end of the week. The interesting thing about this if you compare it to `pipes` is that it has two continuations, so it is able to observe end of stream in a type safe way (as opposed to `conduit` which does it by convention), while (I think?) retaining all of the nice categorical properties of `pipes`. Quiver: type Producer b' b f r = forall a' a . P a' a b' b f r type Consumer a' a f r = forall b' b . P a' a b' b f r data P a' a b' b f r = Consume a' (a -&gt; P a' a b' b f r ) (Producer b' b f r) | Produce b (b' -&gt; P a' a b' b f r ) (Consumer a' a f r) | Enclose (f (P a' a b' b f r)) | Deliver r Pipes: data Proxy a' a b' b m r = Request a' (a -&gt; Proxy a' a b' b m r ) | Respond b (b' -&gt; Proxy a' a b' b m r ) | M (m (Proxy a' a b' b m r)) | Pure r I'm super interested to hear what Gabriel and Michael think about this approach and how it compares to their libraries.
Thanks for your reply! That's the one I wanted to recall. I've been using bytestring-trie (thanks for it! BTW, I had to take out some code out of its internals and modify some functions for the construction of a trie because the provided ones didn't perform well in my case; I'm going to show my modifications after a while if I find them sensible on a second look) and had a look through other packages by you. That's when I noticed it and thought why not possibly unify it with `these` if there are no substantial differences...
Thanks for pointing that out. I was not aware that that even existed. I'll will be an instance for `Divisible` (once I really get my head around what it means).
Yep. Sorry, this is all a bit clearer with the video. Look out for when those are available. Should be soonish, the ZuriHack organisers had to get everything past the google video copyright release form bureaucracy.
Thanks. I had never heard anyone else verbalize this sentiment. I just kind of developed over time by watching people use applications, but it's good to know that there are others who feel the same.
...continuing my comments regarding additions to `bytestring-trie` interface: I see now that I didn't find something like an issue tracker to publish my additions to, that's why I didn't post them immediately.
In case you want the most efficient solution, you should probably look into using only arrays, thus improving cache locality. Assuming that you know the size of the input, which unfortunately isn't the case with lists, one could use a single array for the buckets and an array for the fill level of each bucket (where a bucket is just an offset).
The https code probably shouldn't be relying on /dev/random which can block. If you trust the kernel CSPRNG enough to be using /dev/random then just use [urandom](http://www.2uo.de/myths-about-urandom/) for TLS. Not that havaged isn't a good idea on VMs.
Not an answer to your question but I see scalability as 4 variables, picture it as a radar/spider chart and draw the dots for your platform of choice. 1) Raw horsepower, how fast does it take to serve a request 2) Concurrent horsepower, how many requests can you process 3) Single developer productivity 4) Multiple developer productivity Nodes great, it's fast you can do lots with it just look at the number of repos on Github so I won't discredit it. It ticks boxes 1-3. It is easy to block the event loop though in which case being single threaded it's not so great. Also being dynamically typed the more developers and the larger the project the more likely you are to see issues arising from code changes, it's not compiled so you only find them at runtime. A lot of new languages are coming out treating JS as a bytecode to get around some of the language weaknesses while getting benefit of the Node environment. Statically typed languages do better at 4, you get help from the compiler making changes. Then with things like FP and immutability you also get rid of things like mutating state bugs. There's a certainly more upfront investment in learning compared to JS, infact if you've done lots of OO for a long time it can be a huge upfront investment due to a complete mind shift, eventually it starts to click and make sense although it can be a slow journey. I'm only learning Haskell myself playing with Spock/Scotty so can't give any better answers. I've worked hell of a lot with PHP / JS / Groovy before which are all dynamic and the past 3 years Scala professionally. All my pain points with PHP / JS / Groovy have turned out to be dynamic runtime issues, Scala automatically looses a large proportion of the bugs I had if you make the type system work for you, hence I'm now at Haskell.
Would something like the base3/4 scheme have worked for the superclass-changing AMP?
Good point in general, but note that in this case the log factor is still there even if w is not constant. Radix sort is O(wn) and comparison sort is O(wn log n).
And how do you compare O(w) sized keys in O(1) in the worst case?
I don't think it would. The crucial thing with two base versions is that the types are the same, so base3:Int is the same type as base4:Int, which means that you can depend on one package that uses base 3 and another package that uses base 4. With AMP, if base 5 had had the new Monad class, then what would base 4 provide? It can't re-export the base 5 Monad class, and it cannot really define a new one that lacks the superclass (because then you couldn't mix packages using base 4 with those using base 5). So no, you'd need some additional trick to make that work (like superclass defaults).
Everything is in a rough state right now, so it may not be the best time to jump in. You can see status on the Github issue: https://github.com/commercialhaskell/stack/issues/232
We're working on it.
Referring to features of the core datatype in the respective libraries, here are the differences: pipes: has bidirectionality conduit: has termination detection, finalizers, and leftovers Based on what I'm reading here, this would be: quiver: has bidirectionality, termination detection If this abstraction also obeys all of the `Category` laws (like pipes does), it seems like a very nice extension of pipes. From the conduit perspective, leftovers are still vital to how we do things (though finalizers less so). If you could figure out a way to add in leftovers, it seems like you'd have the holy grail of streaming abstractions! :) Note: I'm fairly certain it's _impossible_ to have leftovers with a properly behaving Category, since composing two components together will necessarily discard leftovers from the downstream one. In other words: if you find a solution to this, awesome! But don't spend the next 20 years staring at a chalkboard hoping it'll happen.
Where can I buy those machines?
A really good answer. Thank you! 
There's no need for snark. A machine that does operations on w-bit words in O(1) time is a standard computer science trope. A 32-bit machine compares 32 bits at a time, etc.
Apologies for the snark. My the point is that there is still a low upper bound in practice, so this is a purely theoretical concern. In fact the theoretical model contradicts the laws of physics... The real question here is whether radix sort is faster than comparison sort for word sized integers, *in practice*. [The answer is yes, it's 2x faster.](http://erik.gorset.no/2011/04/radix-sort-is-faster-than-quicksort.html)
No. It's the result of the programming language implementers to provide an `IO` type that satisfies the `Monad` laws (which it does as far as I can tell). Programmers don't need to make any special effort.
This looks interesting. There is one thing missing compared to `pipes` which is other composition operators like `(&gt;~&gt;)` and `(&gt;+&gt;)`. However, there may be generalized versions of these operators that works for the `P` type such that the `consume` and `produce` functions are the identities of those generalized operators.
In the meantime, here's a quick script that can be used to configure `Cabal` to use `stack`'s package databases and GHC. This fixes tools which rely on `Cabal`'s stored information, such as `ghc-mod`. There are probably a lot of corner cases that the script doesn't handle, but it has been working for my projects. #!/bin/bash ghc_version="7.10.1" [ -z "$1" ] || ghc_version="$1"; shift cabal configure --package-db=clear \ --package-db=global \ --package-db="$(stack --no-system-ghc path --snapshot-pkg-db)" \ --package-db="$(stack --no-system-ghc path --local-pkg-db)" \ --with-compiler="$(stack --no-system-ghc path --ghc-paths | head)/ghc-${ghc_version}/bin/ghc" \ $@ Note that the GHC version has to be specified manually. As far as I'm aware, `stack` currently has no interface for determining the GHC it's going to use. The script is based on an [idea](https://github.com/kazu-yamamoto/ghc-mod/issues/498#issuecomment-118322260) by Github user DanielG.
 main = (putStrLn . ((++) &lt;*&gt; show)) "main = (putStrLn . ((++) &lt;*&gt; show)) " is rather nice, too.
What IDE should a beginner use? Is there an Haskell-ready Emacs bundle? Is there a project to localize and translate to other languages Haskell tutorials?
&gt; It'd be interesting to see what doesn't work out for mutable reference when we try to define an Applicative instance. Well first of all, mutable references can't even have a Functor instance in Haskell, because it's not enough to have a single function to map between references. There must be some way to update the old value with the new one, and that's precisely a lens. Though the mapping zoom :: Lens' a b -&gt; TVar a -&gt; TVar b might be a functor, as lenses also form a category. When I was writing my package I haven't thought about the applicative-like mapping app :: TVar (Lens' a b) -&gt; TVar a -&gt; TVar b and I'm not sure how does it relate to pair :: TVar a -&gt; TVar b -&gt; TVar (a,b) and the "zoom" above, but zoom' :: Lens' (ALens' a b, a) b zoom' k (l,a) = f &lt;$&gt; k (a^#l) where f b = (l, a &amp; l #~ b) seems to be okay, though I'm not sure how to go the other way around. Perhaps my version doesn't really generalize better, and it was just my oversight.
Emacs [`Prelude`](https://github.com/bbatsov/prelude) has some stuff setup for Haskell support. It is how I got started. If you like Emacs, everyone over at `#haskell-emacs` on Freenode are really helpful. *Edit*: I just remembered about [`haskell-emacs`](https://github.com/knupfer/haskell-emacs). It allows you to extend Emacs using Haskell code. Could be another fun way of learning Haskell and Emacs.
If you're arguing that radix sort and comparison sorts on distinct ints are asymptotically similar, then I agree. If you're arguing that radix sorts have a better constant factor in their runtime than comparison sorts on distinct ints do, then I say it's irrelevant. 
Why wasn't the functionality provided by the [`ScopedTypeVariables` extension](https://wiki.haskell.org/Scoped_type_variables) present in the language from the beginning (preferably without the extra `forall`s)? It seems way more useful than what you get with Haskell98.
It's going to baby-step you through concepts you probably already know but if you just look through the index, [learn you a haskell](http://learnyouahaskell.com/chapters) is a great lookup for simple syntax/concepts.
I typically use `stack exec which ghc` for that, which should even work on Windows since we include msys in the PATH. But theoretically we could just add this to `stack path` as well, I don't see a problem with doing so. I wonder if the `--with-compiler` and `--no-system-ghc` stuff is what's necessary to solve [this undiagnosed issue](https://groups.google.com/d/msg/haskell-stack/19Vgh265bgw/ftnlUObDV68J).
1. Yes and no, Haskell doesn't have a trivial way to fix n and m; to do that within the type system we'd need dependent types (provided by languages like Idris). As for mixing several types in a data-structure — a go-to way to accomplish that is to use algebraic data types to describe what can go into your data structure. [Data keyword](https://wiki.haskell.org/Keywords#data) To ensure that your data structure has correct dimensions, I'd offer you doing the following trick: from the module where you define your data structure, you export types but not constructors (syntax is ``SomeType()`` in the list of exports) and provide so called "smart constructors" that will make sure that nonsensical things are impossible. 2. Yes. Even stronger, you can change the underlying type as you go. You may want to have a look at typeclasses called "Functor" and "Traversable". 3. Yes. That would impose some restrictions on underlying type, however. http://hackage.haskell.org/package/monad-par-0.1.0.1/docs/Control-Monad-Par.html and namely http://hackage.haskell.org/package/monad-par-0.1.0.1/docs/Control-Monad-Par.html#v:parMap 4. I can't see why not. The least generic zip possible (which operates on lists) has type ``zip ∷ [a] → [b] → [(a, b)] ``, but you can have a generalized zip. 5. Same as (4). As a hint on generalization, you can take a [look here](https://wiki.haskell.org/Foldable_and_Traversable#Generalising_zipWith). 6. Yes. Used to be called PFP, now it's just http://hackage.haskell.org/package/probability 
Does that really describe an *IDE* that a *beginner* should use?
1) Yes. You could have a list of tuples, or other more esoteric options. You generally don't actually want to do this, there are often more clever solutions. 2) Yes. This is trivial for the tuple formulation, just write a function that takes and returns tuples. 3) Both, and they are not the same thing. Concurrently means asynchronously in a non-deterministic model, parallel means using more hardware to split up work and compute a deterministic (IE identical with each run) result faster. 4,5) Yes, but this is better handled by those more esoteric versions. 6) Something like [this](https://hackage.haskell.org/package/statistics-0.3.5)?
&gt; Is there a project to localize and translate to other languages Haskell tutorials? For examples, rather than tutorials, try [Rosetta Code](http://rosettacode.org/wiki/Category:Haskell)
Even though non-trivial, Haskell syntax is arguably simpler than syntax of Scala, besides, Scala-refugees aren't that common. Hence there is no sense in writing such a document, so — to my knowledge — it doesn't exist. That being said, to a person who knows a handful of languages, learning syntax of Haskell (or any other language for that matter) shouldn't present an obstacle. Just grab literally any book in Haskell or even [Haskell Wiki](https://wiki.haskell.org/Category:Syntax) and do some reading, read code of other people on Github and you'll get the gist.
Probably just because `do`-notation desugars into `&gt;&gt;=`. I do find `=&lt;&lt;` more readable when I'm writing small expressions where `do`-notation isn't warranted.
GHC always links in the entire runtime. There is a lot of magic there that you'll almost always want for real programs. I suppose there could be a special HelloWorld mode, but no one has found it valuable enough to implement.
&gt; Any existing resources using this style of explanation? The Haskell Wikibook acknowledges that point of view in a [post scriptum section](https://en.wikibooks.org/wiki/Haskell/Applicative_functors_II#A_sliding_scale_of_power) in which `Functor`, `Applicative` and `Monad` are compared. Personally, ever since I got the hang of the involved abstractions I have felt `(=&lt;&lt;)` to be the most natural operator as well. The issue, I guess, is that learning resources understandably tend to introduce monads from the "how to sequence effects"/"understanding do-notation" point of view, rather than the "how to map different kinds of arrows" one. But the latter approach could be interesting to try as well.
I don't think it makes enough of a difference to care very much one way or the other.
Is laziness really a good idea? 
How are databases and external data integrations (i.e. JSON unpacking) generally dealt with in real code?
In practice it's not too hard the other way either. Typically, you'll need to change the types of only a few functions. In any given expression, there will be at most one or two touch points that need to become stateful, so you split the expression at those points. If there is much more than that to do, then you are changing the entire point of the program in a deep way, and you probably would have needed a total re-write in any language. In this direction as well as the other, the compiler uses the type system to guide you through what ever changes are needed. It shows you what needs to be changed and makes sure you get it right.
This is now quite dated as it focuses on Haskell 98, but it was good for me coming from a Common Lisp background: https://www.haskell.org/tutorial/ Another great exercise for someone already familiar with FP, but new to Haskell, is to read the Prelude and then, in a different sitting, reimplement the common types and functions (you can ignore the typeclasses to keep it tractable). The cool thing about the Prelude exercise is that you become more familiar with the stuff that is already in scope, and it makes a nice baseline. By the time you have a decent familiarity with the Prelude, I think you're ready to write some real, but simple, programs. Finally, the type system in Haskell is hard for beginners, so I recommend reading/implementing typing Haskell in Haskell: http://web.cecs.pdx.edu/~mpj/thih/
I don't think the runtime alone is the whole story. I have a program that uses yesod, but doesn't use yesod-auth. But many yesod-auth strings still appear in the binary. Seems that ghc doesn't eliminate unused symbols when linking in libraries. Wouldn't be surprised if there are unused bits of Prelude or GHC.* in the hello binary, although in the hello case, the runtime (some 770kb of it) does dominate. Size matters. Large binaries push haskell developers toward making more monolithic utilities, and make haskell less useful for making small unix-style tools. Dynamic linking to haskell libraries might be the final solution though, rather than trying to optimize static library linking.
The wikipedia complexity article is making a very large assumption. It is assuming you'll never actually _use_ stability to exploit your existing order, and that you'd never try to partition your data on less than something capable of distinguishing all of it. The very point of having a stable sort is that these properties are available for your use. When you don't need stability you can get better constant factors. e.g. American Flag sort is an unstable radix sort variant that is typically 2x faster.
Conduit, pipes. Which one should I learn? Where's the best material to learn it? (I particularly like learning by short examples.)
I'm wondering how I can control laziness better. When I was even newer to haskell than I am now I wrote a simple physical simulation that would output the positions of each mass for each iteration. It was running slowly (as I expected) so I profiled it and the memory profile was a giant triangle. I assume this is because it was accumulating all the partial results (the upslope of the triangle) and then evaluating them all at once just before the file write (the downslope). I looked into how to force the evaluation every iteration but I never managed to get/understand a satisfactory answer. Assume that I only want one file write :) Conversely: How can I make sure the code I'm writing will be lazy? Is there a test I should use or maybe some static code check? 
Oh, yeah ok, let me try: Well for options I'd use [optparse-applicative](https://hackage.haskell.org/package/optparse-applicative) For IO with text files, [text](http://hackage.haskell.org/package/text) For IO with non text files, [bytestring](https://hackage.haskell.org/package/bytestring)
I am well-typed but I won't tell you what my type is let a x = (x,x); b = a.a; c = b.b; d = c.c; e = d.d; f = e.e; g = f.f in g ()
What abstract algebra or geometry or math books/papers must I read to be able to best grok the esoteric stuff around monads and monoids? Edit: [I was recommended this book, but I haven't started it yet.](http://smile.amazon.com/gp/product/0070501386?psc=1&amp;redirect=true&amp;ref_=oh_aui_detailpage_o06_s00) Is that all I might need? Edit 2: [Here is a link to a free copy of Aluffi's "Algebra, Chapter Zero"](http://homepages.math.uic.edu/~acamer4/math300/aluffi.pdf) (PDF) mentioned below.
are you asking how to write command-line option parsing code? if so i would reccomend [docopt](https://github.com/docopt/docopt.hs) or [optparse-applicative](https://github.com/pcapriotti/optparse-applicative). the former uses some metaprogramming magic, which comes with it's own paines in the haskell world but works out quite well here i think. the latter is more haskelly in it's style in that it uses combinators to build up a tiny dsl for describing your command line arguments
I think cabal only does it on `cabal install` though, which is never what I actually want :/
What is the practical way to debug running programs? I am writing a game-playing AI and cannot get the alpha-beta pruning algorithm to work. Aside from trace (which was somewhat helpful) what ways do you find useful for debugging?
No, I'm arguing that radix sort is faster both in theory (unless you make really dumb assumptions) and in practice. If you want to assume that O(w) work can be done in O(1), that's fine, I can do that trick too! Here's an algorithm for radix sort of machine words in O(n): 1. Split words up into chunks of size n/k where k is fixed. 2. Radix sort with the lexicographic ordering on the chunks. Still a factor O(log n) faster.
Thanks, but I will say that this is the kind of answer that puts me off Haskell—I asked for the preferred, canonical, idiomatic way to achieve something which is really quite straightforward in many, many other general purpose languages and the answer contains multiple options, one of them involving painful “metaprogramming magic”.
Haskell does not have a beginner-friendly IDE (yet). Use your favorite text editor.
Good question! In H98, there is no function that you can write where the type cannot be fully inferred. As such, `ScopedTypeVariables` is never necessary. The trick back in the days before it was to use `asTypeOf` to fix types that might need to be pinned down. With the addition of more extensions, `RankNTypes` in particular, `ScopedTypeVariables` became increasingly useful/important.
You can use [FP Haskell Center by FPComplete](https://www.fpcomplete.com/business/haskell-center/overview/) It's an IDE "in the cloud" so you need constant internet access and its free version is a bit limited, but it is the easiest to use Haskell IDE out there. It also supports Vim and Emacs key bindings
Not at all a new Haskeller, but I'll ask a question about something that I'm new to: I'm trying to get into reading FP research papers, and they've got this weird notation for proving things that I don't even know what it's called. Is there a good intro to 'how to read this stuff' somewhere? Also, I don't really remember what most of the greek letters are called, and I don't know what they conventionally refer to in these papers, so it makes reading these papers that much more painful.
This should be called "hask anything"
I just started reading about Haskell recently. What are some good recently sized projects or problems to work on that will get me familiar with the language?
Any old text editor and [`ghcid`](https://github.com/ndmitchell/ghcid) works pretty well.
Haskell does cross package optimisation, iirc. So some constants and expressions from other packages may get inlined into the compiled form of some packages.
*(two questions probably related due to my poor understanding of Monads and Transformers)* 1. I have always found it hard to work with Monads of third party libraries. When I ask questions, the developers may ask 'are you sure this can be run inside this Monad?' and I don't know what to say. Let's say I am inside Persistent's Monad and I want to invoke another library (maybe calls an external api over the network), what I do need to make sure the other library can be used inside the Persistent Monad? 2. I recently started using `MaybeT` for chaining computations that should all succeed together or fail if one of them fails. But it does not give me any error reporting. What should I be using instead? Is it `ErrorT`. Also, how can I make sure it can be used with say `runDb` of Persistent? 
Your example needs a correction (if it is a intended to be a good learning material): incByOne &lt;$&gt; (incByOne &lt;$&gt; Just 1) :: Maybe Int because `&lt;$&gt;` is [surprizingly](http://stackoverflow.com/q/30911093/94687) left-associative like most normal operators, and unlike `$` (which is [disliked](http://stackoverflow.com/a/3032839/94687) by some); `=&lt;&lt;` is more like `$` though: Prelude Main&gt; :m + Control.Applicative Prelude Control.Applicative Main&gt; :i (&lt;$&gt;) (&lt;$&gt;) :: Functor f =&gt; (a -&gt; b) -&gt; f a -&gt; f b -- Defined in `Data.Functor' infixl 4 &lt;$&gt; Prelude Control.Applicative Main&gt; :i (=&lt;&lt;) (=&lt;&lt;) :: Monad m =&gt; (a -&gt; m b) -&gt; m a -&gt; m b -- Defined in `Control.Monad' infixr 1 =&lt;&lt; Prelude Control.Applicative Main&gt; 
yes
Just view it as there not being a definite idiomatic, preferred and canonical way of doing it and instead there is a set of valid choices to pick from depending on your personal preferences and needs.
I like how we gave very similar responses :D
`deriving Generic` and `aeson` make JSON as simple as: data MyData = MyData { ... } deriving Generic instance ToJSON MyData instance FromJSON MyData
Am I missing a joke or something because b isn't well typed.
Scalaz is basically most of Haskell's features ported to Scala. This includes type classes, monads, arrows, lenses and more, you should check it out if you don't want to dive into something so radically different
The link to the repository from the end of the talk is here: http://code.xkrd.net/groups/skete 
Is there a cleaner example of how to make a mutable hash table than what is featured in [Hackage](https://hackage.haskell.org/package/hashtables)? import qualified Data.HashTable.IO as H type HashTable k v = H.BasicHashTable k v foo :: IO (HashTable Int Int) foo = do ht &lt;- H.new H.insert ht 1 1 return ht
I'll have a go at answering 2). Sorry, this got quite long. I generally prefer `ExceptT` to `ErrorT`, since the latter encourages using `String` as the type of your errors. I think `ErrorT` is deprecated in favour of `ExceptT` for this reason. If you don't need any other monad transformer effects, I would recommend using `Either` instead, whose Monad instance usually has the behaviour you want. Regarding running inside other monads, there's a tradeoff to be made. One solution to use `Either`; that is, a monad which has the ability to throw and catch errors but nothing else. This means it can run pretty much anywhere; you can compose them with functions like `(&gt;=&gt;)` as much as you want and get an `Either error result` out at the end. However, you may find you want other features in your monad as well, such as those provided by `ReaderT` or `WriterT` or `IO`. The more of these features you add, the fewer things it will be able to run inside. I suppose there are two types of effects that monad transformers can provide: those which can't be replicated in pure code, ie `IO`, and those which can, ie `ReaderT`, `StateT`. If you want to ensure that computations in a monad A can be run inside another monad B, it is sufficient to go through each effect and see whether B supports it. For example, `IO` is easy; if A can do `IO`, then B will also need to be able to do `IO`. For the other kind (`ReaderT` etc), you can look at the function that unwraps that particular layer. Often it will require a value to be passed in; `ReaderT` requires an environment to be passed in to `runReaderT`, and `StateT` requires an initial state to `runStateT`. In this case, the test is "can I obtain that value inside B". So for example, if B has a `MonadReader r` instance, and A needs access to an environment `s`, and you can write a function `r -&gt; s`, then you're good. Take STM, for example. (I don't know Persistent, sorry). Since STM will often retry transactions, it forbids any IO from happening inside an `STM` computation. One way around this is to do any IO beforehand, obtain the result, and then pass that into the `STM` computation.
Emacs has its own learning curve, but it's worth taking it on. I haven't tried spacemacs, but I've heard only good things about it, so that would be something to try for a good off-the-shelf experience. But just following the instructions for setting up haskell-mode would probably work out, too. While it may seem onerous to be encouraged to learn a new editor at the same time as a new programming language, the investment in emacs will make trying out things like Agda, Idris, Coq, etc. that much easier.
You can do this with `turtle`'s newly added command line options parsing, which is a beginner-friendly wrapper around the `optparse-applicative` library. For the example you gave, you would write: {-# LANGUAGE OverloadedStrings #-} import Turtle import Prelude hiding (FilePath) parser :: Parser (Bool, FilePath, FilePath) parser = (,,) &lt;$&gt; switch "flag" 'f' "Some flag" &lt;*&gt; argPath "inputFilePath" "The input file path" &lt;*&gt; argPath "outputFilePath" "The output file path" main = do (bool, path1, path2) &lt;- options parser "A simple program" ... -- and off you go! To learn more, you can read the [`turtle` tutorial](https://hackage.haskell.org/package/turtle-1.2.0/docs/Turtle-Tutorial.html) and specifically the section on [options parsing](https://hackage.haskell.org/package/turtle-1.2.0/docs/Turtle-Tutorial.html#g:17). For even more details you can read the API in the [Turtle.Options module](https://hackage.haskell.org/package/turtle-1.2.0/docs/Turtle-Options.html).
I think this is the advice I need. I've sort of been debugging it from the top down. Duh! I'm using Haskell! No need to do that. Thanks.
I consider myself more or less beyond the absolute beginner stages of Haskell. I get Monads and Applicatives, I've read and understood Lenses more or less, I've written small scripts, and so on. But from here, there's a lot of things in Haskell that are still a mystery to me and it's hard to move on from where I'm at. It's been mentioned elsewhere that Haskell doesn't have very good intermediate level tutorials, and I'm really feeling this. (Template Haskell?, Various Shakespearian Libraries (and others)?, How do I manage data in a state monad (how do I know if I'm screwing up best practices)?, FFI?, How do I even begin to design a non-trivial program in Haskell (the approach is different from the data oriented approach of OO/procedural world?)) How did you get over this hump?
&gt; *Is there an Haskell-ready Emacs bundle?* I believe I made an attempt at answering this question.
For an alpha-beta pruning algorithm, you might consider writing QuickCheck tests which compare alpha-beta with plain minimax for small trees. QuickCheck can be used to "shrink" test cases until they no longer fail the test, which can be used to produce small examples which should be easier to debug. 
s/He/She/
Revenge of the naming convention part heux.
Yes, that you did.
I stand corrected thought through the types 
Good question. Others have recommended FPCompletes online IDE named Haskell Center (as good as no setup time), and Emacs (has quite a learning curve of it's own). While I have not (yet) tried it myself, I expect there to be an option in between: Atom with [ide-haskell](https://atom.io/packages/ide-haskell). Not as easy as Haskell center, but a lot more beginner friendly than Emacs. 
And repeated every 2 weeks :)
Actually, as a beginner, I found it pretty easy to get up and running with Leksah, even though it's quite clumsy. Also, there's quite a nice plugin for sublime edit
What's the deal with Arrays? Why are they faster sometimes and when should I use them?
Don't you answer your own question in the first paragraph? :P Do you need any more reasons? I'm going to reply on the assumption that this post wasn't a question about emphasis of `&gt;&gt;=` over `=&lt;&lt;` and more about arguing for the usefulness of `=&lt;&lt;`. For the record, I did write a blog post that approached monads from the perspective of `=&lt;&lt;` -- http://blog.jle.im/entry/inside-my-world-ode-to-functor-and-monad There are a couple situations where I think `&gt;&gt;=` truly shines: (1) With lambda notation, `&gt;&gt;=` gives much better visualization of data flow. Consider \y -&gt; z =&lt;&lt; x as opposed to x &gt;&gt;= \y -&gt; z The data "flows" the same way. x flows into y which flows into z. In the first example, x to y to z is a weird zig zaggy thing. Also the shapes of the `&gt;&gt;=` and ` \ ` and `-&gt;` all align, so it looks rather nice visually too. `=&lt;&lt;` does shine with do notation, however: z &lt;- y =&lt;&lt; x as opposed to z &lt;- x &gt;&gt;= y The "data" flows from x to y to y, and the `=&lt;&lt;` version shows this better, instead of jumping back and forth like for the `&gt;&gt;=` example. (2) It helps you reason better with ordering of effects. With haskell, using `(.)` instead f `(&gt;&gt;&gt;)` (`flip (.)`) is a lot better because it helps you reason with the ordering of evaluation. Evaluation is driven/starts with the first thing, then goes to the second thing, etc. However, with monads, a lot of times what we care about is the *ordering of effects*, and not the ordering of evaluation. If we sequence two IO effects, it's often much more important to make clear the ordering of *effects*. For exmaple: getLine &gt;&gt;= putStrLn vs. putStrLn =&lt;&lt; getLine In the first one, the ordering of effects is the dominant feature. `getLine` happens first, and then `putStrLn`. The second way sort of implies the ordering by using data demand arguments, and if you remember `=&lt;&lt;`, then you can easily see the ordering...but the first one undeniably makes the ordering take front-stage. *If* you want to emphasize ordering of effects in your code, `&gt;&gt;=` is the better choice. Many people use monads to deal with/handle effects and ordering of effects, so this situation comes up very often Also let me suggest some ways to rewrite what you wrote that i feel looks a little nicer. instead of incByOne &lt;$&gt; incByOne &lt;$&gt; Just 1 -- which is also badly parenthesized, i think why not incByOne . incByOne &lt;$&gt; Just 1 instead of incByOneM =&lt;&lt; incByOne &lt;$&gt; Just 1 why not incByOneM . incByOne =&lt;&lt; Just 1 And maybe incByOne &lt;$&gt; (incByOneM =&lt;&lt; Just 1) might be nicer as fmap incByOne . incByOneM =&lt;&lt; Just 1 to save the parentheses. 
`flip` only works for the special case where `c = (-&gt;)`. You can't flip the arguments of `(=&lt;&lt;)` for arbitrary categories.
They do type-check, and are even equivalent to what you wanted, but for different reasons (explained under the "surprizingly" link) ;)
Why aren't there more/any machine learning libraries? Is this a result of implementation being difficult in a purely functional language, or because the crossover between the stats/machine-learning/numerical-optimization and haskell communities just isn't very large? I know about [HLearn](https://github.com/mikeizbicki/HLearn), but it doesn't seem to be nearly as complete as R and Python machine learning libraries. Also, I don't believe I've seen a single neural networks library, though I am not searching rigorously. To expound on implementation being difficult: is it simply difficult to be flexible to feature data types given the category theory restrictions? Is it just hard to implement the trickery necessary to do matrix multiplication optimization? Is calling out the BLAS and other optimized linear algebra fortran libraries difficult? I'm definitely stepping into territory I'm not familiar with - how category theory and linear algebra + stats + multivariable differentiation relate - so please forgive my ignorance. edit: added link to hlearn and last paragraph
b is a^(2), so c is a^(4), d is a^(8)... and g is a^(64). At first, our argument `()` has a string representation of length 2. If `(length . show) x == n`, then: (length . show) (a x) == length (show (x, x)) == length ("(" ++ show x ++ ", " ++ show x ++ ")") == length "(" + length (show x) + length ", " + length (show x) + length ")" == 1 + n + 2 + n + 1 == 2*n + 4 So the length of representation of the final result is: iterate (\n -&gt; 2*n + 4) 2 !! 64 == 110680464442257309692
You're basically seeing a real life version of the law of large numbers from mathematics, here. If you sample enough times, there is obviously going to be something that is chosen more than the other --- the probability that they are all the same as extremely small. If henry accrues a large lead near the beginning by coincidence, and you assume that the other 9000 totals are perfectly uniformly distributed, then henry's "large" lead at 400 items in becomes a "pretty negligible" lead at 9400 items in...which is what we'd expect. The law of large numbers says that even if things are weirdly chosen in the beginning --- or if there are weird streaks in the middle --- as you go on, the "percent difference" caused by these streaks and weird values has to go to 0. What we're seeing here is a % difference that is like 1%, so it looks like we're seeing something decently random here. But the point is that *we can expect* something to be at the top. And we can expect what's at the top to be a fairly large "absolute" number (50), bounded by a small "percent" difference. But I'd check with other seeds, and also rearrange the constructors to see what happens.
I'm trying to get into using Haskell for real world applications, but struggling to find information for the more practical side, particularly cabal. Any pointers?
What is a Monad? No abstract math language allowed.
We at Snowdrift.coop welcome new Haskellers. If you are interested in funding freely-licensed works, then yay! That's what we're trying to do. Our system uses the Yesod Web Framework. Some links: * Our [how-to-help](https://snowdrift.coop/p/snowdrift/w/en/how-to-help) wiki page. * Our [README](https://git.gnu.io/snowdrift/snowdrift/blob/master/README.md).
Ah, I see, your argument is that `=&lt;&lt;` looks more like a Kleisli lift of `a -&gt; m b` to `m a -&gt; m b`. I sort of buy that. But by pre and post-composition you can still recover one from the other, so this nonetheless feels slightly "syntactic" :-)
I programmed a [dson parser](https://github.com/alvare/dson-parsec) and a lot of things clicked during that, and have been clicking since. Just try to program something "real-like", not just a couple of files for a project euler problem, but an actual project, with dependencies and versions and structure and stuff.
Array's, like in most languages, are just a data structure that provides O(1) lookup (and if mutable, O(1) update) performance characteristics.
So what other books could I read to grasp category theory?
I don't think there are any matrix libraries that implement optimal ordering, but it should be possible to hack in. It'll never be as fast as hand-tuned fortran written by buddhist monks on the moon, but it should help.
Piggy-backing on /u/sclv's response, [this](http://logitext.mit.edu/logitext.fcgi/tutorial) tutorial (by /u/ezyang, IIRC) does a good job of introducing sequent calculus.
"Conceptual Mathematics: A First Introduction to Categories" by Lawvere is very nice. Instead of Pinter there's also Aluffi's "Algebra: Chapter 0" which weaves in Category Theory right from the beginning.
Maybe look at HashMap/HashSet from [unordered-containers](https://hackage.haskell.org/package/unordered-containers)?
Both Arrays and Vectors provide constant time access to data. They are sometimes faster than lists because to access the 100th element of a list, you have step past the first 99 elements. For Vectors and Arrays, the elements are stored in contiguous memory, and thus can be accessed by computing an offset and jumping right to the 100th element.
[stack](https://github.com/commercialhaskell/stack/wiki/Downloads) has recently been released as a way to quickly start haskell projects, and was created to get around some of the pain points that cabal-install has. It might be worth checking out.
&gt;You are creating a new database connection on every request! That can't be a good example template. Did you find it difficult to keep state across requests? Here's an example of using a database pool with Servant and Persistent: https://github.com/codygman/servant-persistent-benchmarks/blob/master/src/Main.hs
Checkout https://github.com/codygman/servant-persistent-benchmarks/blob/master/src/Main.hs
[What I Wish I Knew When Learning Haskell](http://dev.stephendiehl.com/hask/) (as /u/lambdafool mentioned elsewhere in the thread) is shaping up to be a great intermediate level... not quite tutorial, but jumping-off point. Regarding your particular questions (beware, opinions): * Template Haskell Don't worry about it. When you need meta-programming, then come to it, but not before. One useful thing to remember when you do is that you can run `Q`-monadic expressions at the `ghci` top-loop with `runQ`. * Shakespearian Libraries Again, don't worry about it (IIUC, you're talking about the Yesod stuff like Hamlet and Cassius). It's documented for when you need it, but unless you're doing web dev, it's not important. If you are doing web dev, there was a good post [here](https://www.reddit.com/r/haskell/comments/3c36gd/developing_web_applications_with_haskell/) a bit ago. * Manage data in `State` I can't really help there (I don't know what you mean), but one thing that helped me was trying to write the same code without `State`: each function takes an extra parameter (for the "state" before it ran) and returns an extra parameter (for the "state" after) and you need to ensure that you correctly pass the state along. Once you've done that a couple of times, realize that the `State` monad just abstracts away that annoying and error prone pattern. * How to design programs You still want to be data-oriented. Half the difficulty is picking the right datatypes. You're going to want to design the interface early; that is, the data types and the functions using them. Stub out the functions with holes. Take a look at "denotational design" (the best link I found quickly was [this](https://www.reddit.com/r/haskell/comments/2qvbhp/denotational_design_does_not_work/), a reddit discussion of a blog post critical of the concept; you can get to the original concept by a short link walk). /u/Tekmo has a lot of good general advice on his [blog](http://www.haskellforall.com/) You get over the hump by keeping at it.
This is what I mean http://www.drdobbs.com/architecture-and-design/algorithm-improvement-through-performanc/221600153
&gt; t algebra or geometry or math books/papers must I read to be able to best grok the esoteric stuff around monads and monoids I’d say none. Monads are simple. Monoids are even simpler. :)
Who said anything about anything being &gt; “too complex”. I know there are many options. [This page](https://wiki.haskell.org/Command_line_option_parsers) lists 15! And that's why I phrased my question so as to find out which (it turns out to be ````optparse-applicative````) is preferred. And while of course there are many ways to parse command line arguments in Python, there's only one *preferred* way, which for Python 2.7 and later is ````argparse```` 
There is also different fork of `ghc-mod` with some hacky stack support. It is based on release 5.2.1.2, which I find more stable than master. https://github.com/esmolanka/ghc-mod At first, you `stack build` your project and then just use `ghc-mod`. Update: yes, it might not work with GHC 7.10, but works fine with GHC 7.8
That seems to be a good steer, thanks.
Syntastic gets you syntax-checking via ghc-mod ghc-mod has a vim plugin of it's own that lets you do type inspection on the cursor, getting information at the cursor (similar to `:i` in GHCi), and a few other niceties. hdevtools is very similar to ghc-mod, with two main differences: 1. It is *way* faster. 2. It can only get type info if the file doesn't have errors. There's a `stylish-haskell` plugin that takes care of most of your formatting, though most of the time I just do `:%!stylish-haskell` to have it format everything. `haskell-vim` is nice for syntax indentation. [`vim2hs`](https://github.com/dag/vimhttps://github.com/dag/vim2hs2hs) provides some pretty cool syntax stuff. I mostly like the unicode arrows and lambdas 
Re-reading it, I've had to edit it a lot because of all the predictive swipe misses there were, and started to wonder how anyone made any sense of it! 
There's [haskell-vim-now](https://github.com/begriffs/haskell-vim-now) which transforms vim into a Haskell IDE. It seems really neat, but if you already use vim for other things, it may not be appropriate.
Cackle :)
I don't remember where the quote is from, but if I recall correctly, it's quoted slightly out of context by /u/PM_ME_UR_OBSIDIAN. I *think* what SPJ was saying was that he doesn't think Haskell will ever be a dominant language like Java or Python. He does think some other (pure?) functional language will eventually come along and take that position though. He thinks *that* language will be strict, because that's what people are *so* used to, it's simply unlikely a lazy language would get widespread adoption.
&gt; One useful thing to remember when you do is that you can run Q-monadic expressions at the ghci top-loop with runQ. Ohhh why didn't I know this already? Silly me.
`b` is well-typed because `a` is polymorphic. Each use creates a different set of unification variables. It's the same reason `id id ()` is well-typed. The second `id` has type `() -&gt; ()`, and so the first has type `(() -&gt; ()) -&gt; () -&gt; ()`. This also explains the performance problem, because you can see the type grows exponentially...
I'm still slowly getting over that hump, like I expect most Haskell users are. You never really stop marvelling at the fantastic libraries some users are able to create. But here's how I'm navigating the hump: I write code, as much of it as I can. Whenever I start a new pet project, I think, "Hey, among all these technologies I've never used... can I incorporate one of them in this?" and usually the answer is yes. So I do. I hit my head against the wall a hundred times, but I learn out of necessity. I try to only do one new technology per project too, so I don't get overwhelmed. The latest thing I'm exploring is [tables](https://hackage.haskell.org/package/tables-0.4.1.1/docs/Data-Table.html), right after [vector](https://hackage.haskell.org/package/vector-0.10.12.3/docs/Data-Vector.html). Before that, [gloss](https://hackage.haskell.org/package/gloss-1.9.2.1/docs/Graphics-Gloss.html), and before that [wreq](http://hackage.haskell.org/package/wreq-0.4.0.0/docs/Network-Wreq.html) and [taggy-lens](https://hackage.haskell.org/package/taggy-lens) (rare case of doing two new technologies at once, but they fit so well together!). Before those, [lenses](https://hackage.haskell.org/package/lens-family-1.0.0/docs/Lens-Family2.html), and going back even further, [blaze](https://hackage.haskell.org/package/blaze-markup-0.7.0.2/docs/Text-Blaze.html), [attoparsec](https://hackage.haskell.org/package/attoparsec-0.13.0.0/docs/Data-Attoparsec-Text.html) and [aeson](https://hackage.haskell.org/package/aeson-0.9.0.1/docs/Data-Aeson.html) in that order. There's lots to learn! On my list for the future is, among other things, finding a use for [vinyl](http://www.jonmsterling.com/posts/2013-04-06-vinyl-modern-records-for-haskell.html).
btw. --make is the default now so: ghc --make === ghc
not so much a library, and certainly not worried about getting perfect performance, but I've recently implemented a Neural Network as a class project. See: [https://github.com/Stratege/Coevolutionary-Neural-Network](https://github.com/Stratege/Coevolutionary-Neural-Network) in particular [this](https://github.com/Stratege/Coevolutionary-Neural-Network/blob/master/perceptron.hs) file implements a multilayer perceptron (for which, in the other files, there is a rather adhoc coevolutionary genetic algorithm defined to train them to play simple games, like TicTacToe or Global Thermonuclear War (I just couldn't resist naming it that, given the topic)) edit: note the lack of category theory being used. It really isn't necessary to get work done, although I suppose it helps at times.
I'd argue that function application is traditionally written the wrong way if you focus on data flow. 
As seen [here](http://www.cs.nott.ac.uk/~gmh/appsem-slides/peytonjones.ppt), SPJ is basically saying that laziness is not worth the tradeoffs. The single biggest advantage of laziness is that it forced us to develop monadic IO and other interesting abstractions, but those don't depend on laziness for their existence.
What do you mean by cleaner?
I'm not sure `turtle` pulls much more that `optparse-applicative`.
And if you're using unboxed or primitive vectors, you don't even have to deal with following pointers to access objects (which can be slow) – the objects are stored directly in the array!
How can I become as good as @chrisdone or @jaspervdj?
&gt;I suppose there could be a special HelloWorld mode, but no one has found it valuable enough to implement. Smart-linking is a fairly standard feature in many other languages: only link the functions that actually get called. Now, I'm guessing that if I talk to the right people, I could volunteer to implement it, but I'm rather much of a Haskell noob myself, even though I'm good with compilers and have some decent FP experience from other languages (Coq and Scala).
Is there a README of any sort anywhere?
Compared to cabal, with stack, you don't need to worry as much about manual package management. Compared to cabal sandboxes, using stack will generally take up less space (as long as your projects share the same version of LTS Haskell). However, stack is still fairly liberal in its use of space. Stack makes it trivial to get the stuff you need. It's not quite so trivial to selectively get rid of stuff you don't need and reclaim that space. These days disk space is cheap so it's not something we've been terribly worried about implementing, but there are open tickets about it. I personally am interested in adding sharing between LTS minor versions of the same major series, more for the sake of avoiding compile times than for the sake of saving space.
That's fair, but also has little to do with how much of the library you use, and more to do with how big the library is.
I think you misconstrue what `turtle` does. Turtle does not implement a new shell environment. `turtle` just provides Haskell analogs of Unix commands. For example, there is `mv`, which is just: mv :: FilePath -&gt; FilePath -&gt; IO () ... and that's it. There is no magic super-environment that these commands have to be run in.
Exactly. So if I can solve my problem perfectly well with library `A` then I'd prefer not to use library `B`(which uses `A`) unless there's a compelling advantage.
Does anyone have an example `.emacs` file available, with comments/documentation maybe, for setting up both `haskell-mode` and `ghc-mod` (or some other type-inspection tool, like `hdevtools` or `ghci-ng`) with `stack` yet? All of these tools move fast, and I don't know of a central place to look for IDE "news" besides following the individual projects on github (the [serras tutorial](https://github.com/serras/emacs-haskell-tutorial/blob/master/tutorial.md) at one point seemed comprehensive, but it was written before stack). I'm new to using emacs, haskell, ghc-mod, and the cabal/stack ecosystem; so far, of the threads I've seen (for instance [this](https://www.reddit.com/r/haskell/comments/3byf25/stack_hdevtools_together_at_last/)) where people discuss their attempts &amp; successes at IDE and haskell tooling, they're all speaking from significant experience with those tools, and seem a bit intimidating to a newbie like me.
That is reasonable, of course. The (potential) advantage here is that turtle might be easier to use. If that's not worth it to you, then not using turtle sounds rational. You just made it sound like turtle was not even worth considering, which I objected to. :)
Maybe so, I'm only going by what the description says. But even so, how much sense does it make to pull in a library which implements analogues of a bunch of unix commands as haskell functions in order to write an actual unix command of my own? 
Depends a bit on what you mean by FRP. By its strictest definition, no, pipes doesn't capture that behavior. That said, most things calling themselves FRP today don't either. I'd suggest trying pipes for your particular application, if indeed you are particular-application driven here, or, otherwise, just learn some of the "real" FRP libraries: reactive-banana, sodium, reactive for normal FRP and netwire or yampa for the arrowized version.
TBH I think most beginners could figure out what it means. That's not my issue. The extra thought, "...oh, right, `Bool` has an `Enum` instance," *is an extra thought.* It takes longer than simply recognizing a symbol. Every moment like that makes reading code a little slower. If we were talking about an enumeration with more than two or three members, the savings from making the code shorter and easier to scan would likely be worth it; `..]` is perfectly useful notation in general. But in this (very specific!) case, it really isn't. **Edit:** In fact, this is *literally* a matter of evaluating a thunk vs. normal form. When you know for a fact that you've only got one operation and you're going to require it, laziness probably isn't the best choice. :)
You should consider: Is it really category theory you want to learn about? Category theory is a tool for mathematicians, not programmers. The problems it solves are mostly irrelevant to people who want to write software. The lessons we learn from it are things like: * It's important to study sets, spaces, and algebras with a suitable notion of function, map, or homomorphism in mind. * Many interesting definitions boil down to the unique existence of of a particular morphism. * Commutative diagrams are useful tools for reasoning about the relations between objects. * Commutative diagrams in one settings can often be transferred to other settings. For instance, homology associates abelian groups to topological spaces and homomorphisms to the continuous maps between them. * Free constructions in algebra can be formalized in a straightforward and uniform fashion. * Many order-theoretic notions can be upgraded to work with objects and morphisms. All of these ideas are really fascinating. But they don't address *any* concerns a software developer ever sees in his or her lifetime. If you're interested, I highly recommend Awodey's book. It's written from a general viewpoint, leaning towards that of a logician, I believe. Don't expect to see much on monads (they are discussed briefly in the last chapter) or Haskell (although the lambda calculus gets some mention). You also have to be prepared for the fact it is a mathematical text, and requires some amount of what's called mathematical maturity to read. (Basically, familiarity with the standard mathematical conventions you learn about as an undergrad math major). Algebra Chapter 0, as /u/MarkovHiding mentioned, is another excellent book. It's focus is highly algebraic, focusing on groups, rings, and modules. However, it only uses the absolute basics of category theory until the last few chapters, and by then, you're neck-deep in graduate-level homological algebra. There's also the \#\#categorytheory channel on Freenode IRC. They are very helpful (although the channel is sometimes a bit slow). And of, course, asking in \#haskell or \#\#math can get your questions answered if you're persistent.
At least at first, don't think that a monad *is* any thing. In other words, your question should have no answer. Instead, ask "what does it mean for some thing, X, to be a monad?". In this case, it means that `X` is a Functor first. This means that you write it with a parameter like `X a` and, in many but not all cases, can think of `X a` as being a "container" holding values of type `a`. Then `X` being a Monad means that there's a way to flatten two layers of the `X` container together ("2 -&gt; 1") as well as a way to wrap a value in the `X` container to start ("0 -&gt; 1"). That means we have functions join :: X (X a) -&gt; X a return :: a -&gt; X a What do these functions do? It depends upon what specific `X` you have. For instance, `Maybe` is a monad and `Maybe (Maybe a) -&gt; Maybe a` returns `Nothing` if *either* layer is `Nothing`. It turns out that often `join` somehow "sequences" the two layers causing them to happen one after another. It also turns out that `return` creates a "neutral" layer... but again, the actual meaning of these functions depends exactly on what `X` is.
Mostly category theory, actually. Other than that, algebraic topology. Slight problem is, algebraic topology, category theory, and functional programming are all turning out to be part-and-parcel with each-other, but nobody has completely documented the shared concepts in *one* book yet.
I'm wondering how 'runEffect' would be implemented. One keeps coming on constructors (Produce and Consume) containing *two* quivers, each returning whatever the other returns. Do you throw one out? Maybe the answer will be obvious if I can figure out the idea behind the additional Quiver in each case.
My first impulse was incByOneM &lt;=&lt; incByOneM $ 1
So it seems that in the Stack Overflow answer that /u/gergoerdi linked I stated that Wadler et al. proved that an Applicative is equivalent to an Arrow `arr` with isomorphism `arr a b ~ arr () (a -&gt; b)`. You seem to have claimed something stronger: all you need is a `Category`.
Is there a connection between `void` and `Void`?
Not as far as I am aware. 
Consider the following: mapM_ yield [1,2,3] $$ CL.map show =$ CL.peek The `CL.peek` will call `leftover "1"`. How do you re-leftover that up from `CL.map`? And this is just a trivial case; imagine things like receiving a `ByteString`, converting to `Text, consuming the first 20 characters, and calling leftover on the rest. You may be able to see it better if you look at [functions for conduit composition with leftovers](http://haddock.stackage.org/nightly-2015-07-07/conduit-1.2.4.2/Data-Conduit.html#g:12).
Ditto. When developing a tricky function, I like to sprinkle type annotations all over the place as sanity checks. Once the function is written, it is usually just fine to remove the type annotations.
The two libraries have very similar underpinnings. It's out of date, but you might be interested in checking out my old series of blog posts: [Pipes to Conduits](https://unknownparallel.wordpress.com/2012/07/24/pipes-to-conduits-part-0-combining-functors/).
I made an app that converts a string containing a simple mathematical expression into reverse polish notation, and then calculates the answer. It is extensible, and has many functions of any number of arguments. I'm very happy with it. Then suddenly it didn't work, because my import (the Split package) got broken (I suspect by Cabal) without me having anything to do with it. This is, I suppose, the famed lack of reliable libraries that I've heard so much about. Here's my question: why should I bother with a language that is really hard to do most simple things in and which has a development environment that is fundamentally flawed and breaks the stuff I make?
What does "re-leftover that up" mean?
I found [this stackoverflow answer](http://stackoverflow.com/a/3858684) to be good description of the advantages of green threads. It addresses the Node.js comparison directly. As others have noted, the GHC runtime has a very efficient green threads implementation. Don Stewart's answer below is also very informative and gets into the details a bit more.
It means that the next thing monadically composed upstream would have access to those leftovers. Consider this: mapM_ yield [1,2,3] $$ ((CL.map show =$ CL.peek) &gt;&gt; CL.mapM_ print) What would you expect the output to be?
&gt; Haskell noob Dude, you introduced me to Haskell like, 10 years ago.
~~Oh I see, the leftover in this case is 2? And it needs to be saved so that `print` can have it?~~ ~~Actually I guess the output should be~~~~ ~~1 1 2 3~~ ~~Is that right?~~ No, I'm just confused. What's the answer? :S
It's O(log(n)), but there's some hidden assumptions behind both asymptotics. Arrays should be faster in general due to cache-locality.
Bottoms. Bottoms *everywhere.*
Maybe this example is confusing the situation, since it's not actually demonstrating directly the category violation, but rather why the violation exists. Here's the violation: idC :: Monad m =&gt; Conduit a m a idC = awaitForever yield -- same as CL.map id idC =$= CL.peek /= CL.peek However: fuseLeftovers id idC CL.peek == CL.peek
I can but agree. There are a few things from Haskell I *really* miss when I have to work in other languages. Turtle is one of them.
The runtime representation of pure and effectful things are precisely the same -- "thunk" nodes in the "g-machine" (http://stackoverflow.com/questions/11921683/understanding-stg). Under the hood, an `IO a` value is represented as a "state-like" function `#RealWorld -&gt; (#RealWorld, a)` where `#RealWorld` is a magic zero-width token just meant for book-keeping purposes. Not sure if that helps or if you're interested at a different level (i.e., the FFI).
Yes
&gt; Category theory is a tool for mathematicians, not programmers. The problems it solves are mostly irrelevant to people who want to write software. Tell that to Edward.
What makes it the best way compared to other options?
Well he said he wanted to grok the esoteric stuff, not just understand the programs he's writing.
Have you worked with other programming languages? Do the same things with Haskell you would do in those! If Haskell is your first language, here are some ideas: make simple text-based games, create computer-generated animations, convert files of different formats, create some blog software... you're free to do whatever you want! Pick something that sounds fun, and make sure it's something small enough you'll be able to complete it in a few weeks.
You can use two monads with a [monad transformer](http://book.realworldhaskell.org/read/monad-transformers.html), but the state monad probably isn't necessary in this case. The type signature you specified doesn't include the IO type anywhere so you couldn't really prompt from within that function. Instead, you'll need a separate function that prompts for the values which can then be passed to the function you specified: f :: String -&gt; String -&gt; String -&gt; Foo fPrompt :: IO Foo fPrompt = do s1 &lt;- prompt "Question 1" s2 &lt;- prompt "Question 2" s3 &lt;- prompt "Question 3" s4 &lt;- prompt "Question 4" return $ f s1 s2 s3 s4 prompt :: String -&gt; IO String prompt p = putStr p &gt;&gt; getLine There are many more elegant ways to do this, but this seems the clearest to me. 
I hadn't seen this before, and it looks terrific. Cheers!
Thanks for pointing that out! I edited the question. I was actually not aware of this. It breaks the analogy to `($)`, which is often used to avoid the parentheses, which are required with `(&lt;$&gt;)`. :-(
The compiler maybe can still do nice lazy optimization so on the code while there's automatic evaluation when it's brought into your data type? That'd be my guess but probably better reasons out there.
I hadn't seen that, that's pretty awesome! I think a route like this is ideal, because GHC's pretty printing code is fairly old.
What is the difference between Functor's fmap and Traversables traverse? I for the love of god cannot find this out. The example Tree traverse in the docs just makes use of fmap using its infix &lt;$&gt; operator. Thus, the traverse functions looks like just a complex way of calling fmap to me. I don't see how they are *not* identical. 
It can't possibly have been 10 years! But I got road-blocked at roughly an SML level of functional programming way back, and it's taken me a long-ass time to sit down and force myself to learn to deal properly with Haskell-specific things like laziness, type-classes, category theory, proper practical usage of major type-class instances (like mixing exception, parsing, and IO monads), etc. So since I'm only most of the way through "Write You a Scheme", I'd say I'm a Haskell noob.
From what I understand, an `IO t` value is represented as any other thunk, except it does I/O when evaluated. If that is true, your guess is not bad.
so cabal is basically a single-use package manager? ;-)
Going from strict to lazy being hard is mostly a matter of libraries and ecosystem. Due to the default effect, anyone who writes a library in a strict-by-default language is likely to make their library strict. That means their library will be useless when you want to use it with your lazy data, and you have to duplicate the library except with lazy semantics.
Despite being around for almost 25 years now, Haskell has previously not been in as widespread use as it is now. That means the ecosystem has some growing pains. The boundaries for what it is capable of get tested in a hardcore way every day, as tons of users try to bend it to their will. This happens to all languages as they become more popular. =) In your specific case, a very promising solution is to not install package versions willy-nilly as has been the default for a while. Doing so is bound to create breakages when you upgrade some packages to versions that are not compatible with older versions. The solution is to 1. Use sandboxes. This means you'll be able to have different versions of libraries for different projects. This should be familiar to you if you've used a language like Python, with its virtual environments. 2. Pull packages from Stackage. The Stackage set of packages are curated to ensure they all work together as a unit. By pulling from Stackage you can't accidentally install the wrong version of a package – you get the version specified by the Stackage snapshot you're working against. To get both of those things almost for free, take a look at the new (terribly named) [Stack](https://github.com/commercialhaskell/stack#readme) tool. Stack is basically what you expect coming from other, more popular languages. If you are not willing to put up with the growing pains of an ecosystem, maybe check back in ten years or so? By then growth should have slowed down a bit and the ecosystem evolved significantly, to the point where many hyper-popular languages are today.
`fmap` can let you take any function from `(a -&gt; b)` and apply it to an entire container full of `a`s and give you a container full of `b`s. "Container" here is potentially misleading as you might have a data type that has a function in it! data Foo a = Foo { runFoo :: Int -&gt; a } and yet we can still fmap over it instance Functor Foo where fmap f (Foo g) = Foo (f . g) In fact functions themselves form a Functor instance Functor ((-&gt;) x) where fmap = (.) which lets you map over the output of the function. We can also build more interesting Functors where the 'a's occur in weird positions. A fun exercise is figuring out how to write the Functor instance for newtype Cont r a = Cont { runCont :: (a -&gt; r) -&gt; r } We could change out all of the potentially uncountably infinite results of our function (in the Foo case) with `fmap`. On the other hand `traverse` gives you a bit more power, in exchange for allowing slightly fewer instances it says you can run any action you want, which may have Applicative 'side effects'. traverse :: (Traversable t, Applicative f) =&gt; (a -&gt; f b) -&gt; t a -&gt; f (t b) Since all we know about `f` is that it is Applicative, we're stuck using a very limited palette of operations to pull this off: pure :: Applicative f =&gt; a -&gt; f a (&lt;*&gt;) :: Applicative f =&gt; f (a -&gt; b) -&gt; f a -&gt; f b (&lt;$&gt;) :: Functor f =&gt; (a -&gt; b) -&gt; f a -&gt; f b This set of operations combined with the laws of Traversable requires you not only 'apply a function to each element `a` in your container, but to be able to "line them up" in an order. If you view the combinators we just gave you `(&lt;*&gt;)` is the only one that lets you do anything with _two_ `f`s. and when you apply your action to two different a's in your structure you'll get two `f b`s you need to deal with. Sticking them together with `(&lt;*&gt;)` this forms a tree-like structure out of the combinators you are using. The laws prevent you from using the function on the same element multiple times or from skipping some of the `a`s in your structure, so this enforces that you have to be able to put all the elements in your container into some order and enumerate them. This is a stronger claim. We didn't know _anything_ about the argument to the function in the `Functor ((-&gt;) x)` instance, but here we'd have to be able to enumerate all the `x`s in some canonical order, and then build a new function that held onto all the answers in order to even attempt a `Traversable ((-&gt;) x)` instance. At least there it is still potentially tractable, but if you start with the `Functor (Cont r)` example above, it is a lost cause to derive a `Traversable (Cont r)` instance. On the other hand, most of the time we're really dealing with something that is more "like a container" and the extra power of `Traversable` is easy to harness.
As far as I know, it's not that Haskell is *bad* for numerical methods (quite the opposite), but just because the Ancient and Most Respected Custom of the statistics community is to use R and that of the machine-learning community is to use Matlab and Python. (In a minor example, I needed to calculate a certain correlation coefficient for my MSc thesis, at my advisor's bidding. After it turned out that no implementation existed for that statistic in anything other than R, my advisor hacked out an implementation in Java one night. I still have to translate it into idiomatic Java or Scala Real Soon Now and follow up on my promise to open-source the damn thing.) The Haskell community is disproportionately filled with programming-languages theory geeks, which means it's very probably disproportionately *lacking* statistics and ML geeks who would volunteer to reimplement major libraries in Haskell.
Is theorem-proving considered *that* esoteric? Huh, I guess living in Coqland for a while gave me a skewed perspective.
Thank you /u/kqr and /u/sclv: that's very interesting that IO actions are represented in memory exactly the same as pure thunks. I then take it that GHC just needs to carefully generate code that ensures that these IO thunks are executed in the right order, instead of relying on the usual on-demand evaluation.
Well, that's the purpose of that "state-like" bit in the "secret structure" of the IO monad above. If you use unsafePerformIO, it really does turn those IO thunks into thunks like any other, and the effect happens whenever they happen to be first forced.
Like many people, I find that the `.` function composition operator seems backwards. For normal function application - well, with two functions, you already probably need parentheses. OK, yes I know about `$`, but that's still kind-of the point - you need an extra operator that isn't from mathematics to clean up the mess and help keep things readable. I also remember that a commonly claimed advantage of Forth is that, although losing prefix and infix notations for arithmetic is jarring at first, in the long run the advantage is that all operations are written in the order they are performed. Where there are effects, I think writing operations in the order they are done is particularly important. Writing the last effect first is rarely intuitive. When's the last time you gave instructions to someone separating all the steps with "after you" rather than "then"? Despite the function composition rule, my experience is that even mathematics lectures describe the first step first *then* the later steps. And of course "then" is the common name for the `&gt;&gt;` operator. 
&gt;Option #1 is the answer. No reason not to use a generic Haskell Docker image for development (or even one with a couple of extra things installed, but which you don't change over the course of development) and then deploy by building a separate image that copies your executable into [haskell-scratch](https://github.com/snoyberg/haskell-scratch). That would be an interesting blog post. 
Consider a data type like data Tree k v = Nil | Bin Int (Tree k v) k v (Tree k v) where the Int is carrying around a size of the tree below you. As you insert you might bump the counter by one, but you aren't actually changing a machine int by one, you're putting in place a thunk that says 'hey go read the old value add 1 and then replace me with that answer'. As your program runs you'll keep building up more and more of these thunks until someone finally needs to look at the size, e.g. for balancing or because someone asks for the size of the structure. Those thunks have been sitting there wasting space this entire time, and you now have to go and compute their answer, which takes as long as it takes. On the other hand, if we switch to data Tree k v = Nil | Bin !Int !(Tree k v) k v !(Tree k v) relying on the `-funbox-small-strict-fields` extension that is on as part of modern GHC versions to fully inline the Int directly into the parent constructor, we're actually getting a machine int, and bumping that counter as part of inserting is now writing to a machine int inside of the `Bin` constructor. This takes less memory, since we're not randomly leaking thunks in the vain hope of avoiding a simple (+1) at each step. My recommendation is nowhere near as strong as tibbe's. I tend to recommend unboxing any value of concrete type that you know will otherwise accumulate a bunch of thunks like this. Next, you'll notice that I also put (!)'s in the sub-trees. Why? Now we can know that the trees below us are already evaluated. This can rather drastically improve the cost of computations that are doing things like searching the tree, because they can know that they'll never have to enter a thunk, do some unbounded amount of computation and come back. They get evaluated when the structure is constructed. This means the initial construction becomes, potentially, more expensive, but the `lookup` parts of your code later become faster, at the expense of losing the possibility of storing infinite trees. (On the other hand, memoizing `size` in the node, already cost us infinite trees!) Finally, I recommend that you sit down and analyze how many thunks will build up in your structure as it changes over time, and if you can't bound it, ask yourself a.) why not, and b.) how you can change it so you know. The best book for learning how to do this is Okasaki's ["Purely Functional Data Structures"](http://www.amazon.com/Purely-Functional-Structures-Chris-Okasaki/dp/0521663504). It is incredibly dense, but it is a treasure trove of "Hey, I never knew that was possible!" results that I still mine today. (If you aren't willing to throw money at Amazon for a copy of the book, [his thesis](http://www.cs.cmu.edu/~rwh/theses/okasaki.pdf) is almost as readable, but lacks the Haskell appendix and some later examples.
Thanks! I actually have that book lying somewhere here, but I haven't come around to reading more than the first few pages of it yet. I should probably bump it up my list.
Yes. Lazy algorithms compose, and may wind up with better asymptotics than the parts from which they are made. When strict algorithms compose you always pay full price. This is fundamentally antimodular. I want to design the best possible building blocks once: algorithms that I can pour every trick I know into how to optimize them, algorithms that I'm just barely smart enough to fit all the details in my head. I want to test them, prove them correct and move on to new problems. Consider a simple example: take :: Int -&gt; [a] -&gt; [a] take _ [] = [] take 0 _ = [] take n (x:xs) = x: take (n - 1) xs and sort :: Ord a =&gt; [a] -&gt; [a] sort [] = [] sort (x:xs) = sort ys ++ [x] ++ sort zs where (ys, zs) = partition (&lt;x) xs In a strict language if I were to compose take 10 . sort I'd pay full price to fully sort the list, when I only care about the relative ordering of the top 10 elements. In a lazy language the computation of the relative ordering after the top 10 elements is never computed at all. The result has the same asymptotics as a variant of 'quickselect' tuned for this particular problem. Moreover, as the number of elements I take grows to equal or exceed the length of the list I'm sorting my asymptotics smoothly vary between the two extremes. I was able to put together two algorithms written without knowledge of each other, and get a better algorithm from the result. In a strict setting, what would you do? Once you realized it was inefficient, instead of using the algorithms you'd already written, you'd go and fuse them together by hand. This requires you to hold in your head all the invariants of both algorithms at the same time, prove that they all hold, and laboriously rebuild everything. You can then ship a fused-together program which has been less well tested, but which doesn't pay this higher cost. You only managed to do so by giving up on code-reuse. On the other hand we have things like "tying the knot" for building up circular structures. In a strict language to make a circular linked list or the like you start off by initializing something with a null pointer and then overwrite it after the fact. This requires this null sentinel value to be an observable thing, and it requires a form of mutation. When we go to define things like `letrec` in a language like scheme it means that occasionally you'll see these initialization values when there are cycles `#f`s will just start popping up in your code. In a lazy setting we have a benign form of mutation available to us, thunk evaluation, which is capable of overwriting these things without requiring us to either a.) introduce mutation everywhere in the system and deal with spurious nulls when we're building up cycles or b.) pay a logarithmic factor slowdown to emulate references to avoid mutation. I know the tradeoffs on the strict side of the equation. They bore me. Laziness is interesting to me precisely because the bulk of the programmers in the world aren't looking at it. There is still low hanging fruit to be had over here. There are still interesting algorithms to discover. It permits incredibly nice algorithms and data structures that interact nicely with non-determinism while still paying less asymptotically than any pure strict language can. If what I want is purity, correctness, and reusable code with good asymptotics then laziness follows.
https://www.fpcomplete.com/school/starting-with-haskell
Oh, fine. I guess I can accept that.
This is purely an issue of psychology and sociology. In cultures with left-to-right writing schemes, (.) is the odd artifact, wheras (&gt;&gt;=) manifests the right way™. Due to Western acculturation, we are used to two directions in which information flow (text) can go: right and down. (&gt;&gt;=), in conjuction with the do-notation, goes in these 2 directions: f = do x &lt;- m1 y &lt;- m2 return $ g x y f = m1 &gt;&gt;= (\x -&gt; m2 &gt;&gt;= (\y -&gt; return $ g x y)) On the other hand (.) has the signature `(b -&gt; c) -&gt; (a -&gt; b) -&gt; (a -&gt; c)` for compatibility with Euler's idiosyncratically chosen prefix function application: f (g (h x)) = (f . g . h) x While I think people would have an easier time with an `(~&gt;) :: (a -&gt; b) -&gt; (b -&gt; c) -&gt; (a -&gt; c)`, prefix function application, baked into Haskell's grammar, sabotages the effort. Sending one x through a series of functions is easy enough (`x &amp; f ~&gt; g ~&gt; h`), but it'd be murder to convert an expression like `f (g (h x) y) (z x x)` into postfix form.
The first thing I noticed from your example was that it illustrates the identity `traverse f = sequence . fmap f`. Cool.
I guess so; I always assumed this is what everyone did, but apparently not.
There are modal logics that capture monadic reasoning in the way you lay out, but some care is needed. The reasoning corresponding to `bind` is a bit tricky, to do with the strength of the "if", and many modal logics will reject the reasoning. In these logics, it's not enough to have just "if A then possibly B"; we need to know in addition that this "if" still holds in the possible situation where A is true. (That is, `fmap` is suspicious from this point of view.) Caveat: I've mostly looked at classical modal logics, and know much less about intuitionistic ones, which are the ones that matter here. But I believe the above points still apply.
Ah, I see now! Thanks :)
&gt; There is also esqueleto which I believe is meant to be similar. Esqueleto works with persistent, but provides a more detailed set of primitives for querying SQL databases. Persistent can use redis, mongodb etc. as backends, but as a consequence, doesn't have an API for joins. Esqueleto limits itself to SQL backends, but lets you express more query types.
For parsing command line arguments I use [simple-get-opt]( http://hackage.haskell.org/package/simple-get-opt). It is just a simple wrapper around get-opt, capturing a common use pattern.
Forgive me for being uptight. I found that my own introduction to CT through Haskell was severely misguided. And so I have a distaste for the community's evangelism of the subject. I clearly find category theory important and interesting, as my post above suggests. But I think its role is misrepresented among Haskellers. I believe most would say CT is important because of some vague appeal to composability. Or maybe they will make an erroneous remark about how it helps them capture side effects purely. They will tell you category theory is important because of monads. The one place that comes to my mind where CT is important in computer science is in semantics. F-algebras give us a good idea of how algebraic types ought to behave in a language. Domain theory gives a solid foundation for partially-defined objects and settles some technical issues in dynamic languages. Categorical logic shows that universal and existential quantification can be described in terms of adjoint functors. But when you're writing software, most of your focus should be on the engineering aspects. An electrical engineer doesn't need to concern his or herself with Lesbesgue integrals, and a meteorologist doesn't care so much about de Rham theory, even though each describes the technical underlying theory behind his or her work. And so that's my feelings on the matter. Category theory is important and it's interesting, but the Haskell community has created a weird echo chamber of enthusiasm for the subject. For anyone wanting to learn about it, I feel I need to remind them that category theory was developed for mathematicians. And without some mathematical sophistication (or a willingness to learn algebra and/or topology), it will likely leave you scratching your head for why you should care.
I'm pretty new to Haskell and finding it really hard to do anything really productive in it. I come from an imperative programming background. I would like to know how long it took you to become proficient in Haskell (if you came from an imperative programming background like me). By proficient, I mean being able to read and understand most Haskell programs, and pretty quickly build yourself a web application using a particular framework like Snap, Yesod or whatever.
[My post on pipes-parse](http://www.haskellforall.com/2014/02/pipes-parse-30-lens-based-parsing.html) explains the problem and how `pipes-parse` solves this without dropping leftovers. I can summarize the solution by saying that pipes does not use pipe composition for parsing purposes but instead uses lenses/getters between `Producer`s.
How can I get beyond a beginner / early intermediate stage? How can I find a mentor or work with more experienced Haskell developers? I've been messing around with Haskell off and on since 2011. Only this year have I finally felt moderately productive on [my latest project](http://github.com/seanhess/serials), but I still get thrown completely off balance when I do something outside my comfort zone. For example today I was trying to get ReaderT to work with servant. People totally helped me, but I didn't understand their answers, and had to apply their advice blindly. As a beginner I read LYAH, followed [bitemyapp's guide](https://github.com/bitemyapp/learnhaskell), and did problems on https://www.hackerrank.com/. What should I do now that I'm an "early intermediate" haskeller besides hacking on projects? I feel like I learn very slowly, and if I were working with a more experienced haskell dev I would learn much faster. Given that haskell teams are rare (where one normally gets this sort of mentoring), is anyone interested in some kind of mentoring relationship? I'm more than willing to pay it forward. 
Yup, and that's exactly what `traverse` does in Haskell terms! It first "installs" the action everywhere with `fmap`, and the executes all of them in order with `sequenceA`!
Other people have explained the "what," so I'll give more of a "why" answer. `Monad` is an **interface**: a designated way that clients can interact with any type that implements the interface's operations and obeys the interface's contracts. Interfaces are good for several reasons. For example: * If you code to an interface without "peeking" into the implementation, your code will be reusable with other types that implement the same interface. * If you're familiar with an interface, and you come across an unfamiliar library that implements the same interface, your previous knowledge is applicable to this unfamiliar library. Since in Haskell the `Monad` class is used all over the place, we have lots of generic tools that work with any implementation of that interface. Just looking at near-universal ones: * `do`-notation works with any `Monad` instance. * The functions in `Control.Monad` work with any monad instance. * The `Foldable` and `Traversable` classes provide several functions that work with any monad instance. * Monad transformers are generic tools for "augmenting" any `Monad` instance with extra functionality. Then there's also a lot of libraries that provide novel `Monad` instances. Any time you encounter such a library you still have to learn how it works, but if you've grasped the ecosystem around monads you already will know dozens of utilities that will work with it—it's a question of figuring out how the combination works.
Interesting, I haven't thought about it this way! So, having `&amp;` and `&gt;&gt;=` supports left to right. What about something at the functor level analogous to `&lt;$&gt;`?
What happens if you flush the handle without closing it?
Not quite. The closest thing to an "FRP for pipes" is my `mvc` library, but that has no abstraction for continuous time.
This project is super old so I can't vouch for it any more, but I wrote a web server with MongoDB a few years ago: https://github.com/seanhess/robotquest Scotty might be easier than raw WAI, not sure. It was for me (because it looked so much like so many other things I was familiar with). 
My question deals with hw04 form CIS194. *P a* represents a type class of polynomials, where *a* is the list of a polynomials's coefficients from degree 0 to n. The last problem of the homework is to write a function for the derivative of a polynomial, and my solution was: instance (Num a, Enum a) =&gt; Differentiable (Poly a) where deriv (P a) = P (zipWith (*) [1..] (tail a)) Initially I didn't have Enum as a constraint, and my code wouldn't compile. Why does *a* have to be an instance of Enum? Is it simply because the list is being mapped to the integers, or am I missing something? 
What's a decent IDE for haskell? I've been using eclipsefp, which comes in under "best of the worst" category for me. At least it keeps cabal from screwing things up in a new and interesting way every time I look at it. But I'm finding the low level of code completion and general IDE helpfulness impacting my productivity, since I can't offload the business of remembering trivialities to the IDE any more.
I have finished reading *Learn You a Haskell for Great Good* and have written some Haskell code(about 1000 lines)，but I am stuck with cabal. what would be the best tutorial to learn the workflow of cabal?
Thank you for your helpful link. Unfortunately this currently 503's, but I'll keep a watchful eye on it :)
Jesus. That's offensively nice.
Instead of "after you" or "then" read "of". So `f . g . h $ x` is "f of g of h of x"
Thank you for the link! Looking over https://github.com/seanhess/robotquest/blob/master/Api.hs is giving me hope that maybe I don't need to construct some new convoluted monad transformer stack to do what I need. I don't need routing in my app, since I'm just taking the path from the request and using it to look up a file in GridFS, so Scotty seems overkill. I'm also really trying to avoid learning a framework-specific solution, right now. I'm trying a rewrite now to make my app work more like your Api.hs. Fingers crossed.
Yeah, that's the one I was envisioning.
You gaze at the compiler until code manifests itself. \*joking\*
`[1..]` gets a list of 1, and all values that follow it in a sequence. Not all numeric types have a clear successor function (try complex numbers, for example), so you require an `Enum` constraint as well to say that your type `a` has that successor function defined. More technically, `[1..]` is a pretty way of calling the function `enumFrom 1`, which gives you a list of all values that succeed its parameter. As a bonus, it means you can easily construct lists like this from any enumerable type, including `Bool`, `Char`, and anything else you can think of.
&gt;casual stream functions Thanks for the mental image :o) 
[Atom](https://atom.io/) with the [ide-haskell](https://atom.io/packages/ide-haskell) package is wonderful and easy to set up. I scrapped my Vim setup in favor of Atom and haven't looked back.
I think what's happening is that Data.ByteString.Lazy.hGet doesn't complete until either it has read 1024 bytes or the client closes the connection, and the client isn't writing that many bytes. Use Data.ByteString.hGetSome instead? (N.b. not lazy.) For streaming data frameworks, look at the pipes (or conduit) library.
As someone that is just finally wrapping up getting ghc ported to a new Linux, the shake build system bit somewhat worries me. As long as I can easily port things via cross compilers I'm ok with it. I have to do an arm port yet so don't change anything for a while. &gt;.&lt;
Sure, there's a word you can use that makes function composition "read right", but when's the last time you gave someone IRL instructions as a sequence of steps combined using function composition? Would you try to find a wording for "the second right . the first left", or would you just say "the first left then the second right"? In any case, `&gt;&gt;=` isn't `.` - there are (potentially) "real" effects to account for. Effects have a natural ordering, which is a significant part of what monads are for in Haskell, even when the effects are an abstraction created by the monad with a pure underling implementation. IOW for `&gt;&gt;=` there's at least two conflicting lines of reasoning for where to draw your ordering intuition from. If the argument ordering for simple function composition were universally considered intuitive, it still wouldn't be a conclusive argument, because effect ordering has just as strong a claim and conflicts. The fact that function composition isn't universally considered intuitive only adds to that - unless you know a significant group who find sequencing effects first-to-last unintuitive. Note - OP doesn't count because OP wasn't thinking in terms of effects. 
I think I understand FP pretty well including higher order functions, so perhaps that will help
Yep, I was directed on the irc to conduit, and I'm going to give that a shot, thanks. 
C++ mangles its symbols in order to support overloading, templates, and namespaces. So whereas a C function like `int foo(const char*)` would be compiled as simply `foo`, in C++ the symbol name includes type information, e.g., in the IA-64 mangling convention, it would be `_Z3fooPKc`. You can get the mangled symbol from your compiler and do an ordinary `foreign import` declaration: foreign import ccall "_Z3fooPKc" foo :: CString -&gt; IO CInt When I was working at Facebook and we had to interoperate with C++ a lot, I wrote a plugin for `hsc2hs` that would let you write this: foreign import ccall #{ mangled int foo(const char*) } foo :: CString -&gt; IO CInt They haven’t open-sourced it, though. I could bug them about it or rewrite it—it only worked for IA-64, because we were only using GCC on Linux, but in principle it could be extended to support other mangling conventions. Interoperating with any of the nontrivial parts of C++ will typically require making some compiler/platform-specific assumptions, too, but generally your interop layer shouldn’t be fancy.
What if we had `Contravariant` functors as the superclass? I'm thinking of something like: import Data.Functor.Contravariant class Contravariant f =&gt; Zippy f where unit :: f () zip :: f a -&gt; f b -&gt; f (a, b) newtype Effect a = Effect (a -&gt; IO ()) instance Contravariant Effect where contramap f (Effect io) = Effect (io . f) instance Zippy Effect where unit = Effect $ const $ return () zip (Effect io1) (Effect io2) = Effect $ \ (a, b) -&gt; io1 a &gt;&gt; io2 b with analogous laws: zip x unit = contramap fst x zip unit x = contramap snd x zip (zip x y) z = contramap assoc' (zip x (zip y z)) This "Zippy" definition seems to extend to contravariant functors, while the standard Applicative definition with `pure` and `&lt;*&gt;` is impossible. I'm not an expert on these things by any means, so... does the above make any sense at all? Does it have a name? Could it be useful?
Atom is quickly becoming a beginner friendly alternative. I've been using it now for quite some time and I prefer it over Sublime. There are updates almost every day and the quality is rapidly improving.
If I think of it as a declarative description rather than an imperative sequence of actions, then the "of" reading is the one that feels most natural to me. Since I'm thinking declaratively and functionally rather than imperatively, then the sequencing makes sense to me. Additionally, `f (g x) --&gt; f . g $ x` reads very naturally as a rewrite.
Thanks. I guess what I was getting at was why do both lists have to be enumerable? For instance, if I had a list of complex numbers I know has to be finite, can I map that list to the integers using `[1..]`? If that's possible, does the presence of a function in the mapping throw things off? Looking at the type declaration for the partial function zipWith (*) [1..] :: (Enum c, Num c) =&gt; [c] -&gt; [c] seems to answer my specific example. 
Great point! I think we can fix this, but it is hard in existing languages.
ekmett's link is to the group. It has individual repos under it http://code.xkrd.net/skete/skete/tree/master has a README
I'm going to assume you mean [Learn you a Haskell](http://learnyouahaskell.com/) (aka LYAH) and [Real-World Haskell](http://book.realworldhaskell.org/) (aka RWH). * LYAH tends to be better for people who are totally new to Haskell. It's certainly not as formal in presentation. * RWH tends to be better for people who need to know very pragmatic things about the language. How to use the profiler to optimize their code, how to make an FFI binding, how to get work done with the IO monad, and when to make a custom monad using transformers. I could be wrong, but I think RWH has more intermediate to advanced topics than LYAH. They're both for beginners, but LYAH is probably a bit more basic. The biggest downside to RWH is that it's older and not all of the examples worked the last time I looked at it. If you follow the meat of the text and use that to understand the examples and ask around you should be able to move past any of the issues with the examples. If you are coming from an SML background, then I would say start with RWH and the assumption that not all the code samples will compile on the first try. I hope that helps!
This looks decent for how to run cabal commands: http://katychuang.com/cabal-guide/ There is also a `cabal init` command that will generate a simple cabal file that you can edit. From there, the normal [user guide](https://www.haskell.org/cabal/users-guide/) should give you a sense of how to extend the example. Of course, don't hesitate to ask more specific cabal questions!
I don't use an IDE so I can't really recommend one, but I can tell you what I do and maybe that will be useful information? I use plain vim and emacs for Haskell code. I use [hoogle](https://www.haskell.org/hoogle/) to find library functions, or find and grep within my local codebase. I use cabal sandboxes and that keeps me out of weird territory with cabal.
I can't give you much first hand experience, so instead I'll point you towards some introductory material and some more in depth material: * First go read Chris Done's intro to [Typeable](http://chrisdone.com/posts/data-typeable). This will work well for certain classes of problems. * There is also the [reflection package](https://hackage.haskell.org/package/reflection). * You can also do generic programming in Haskell with lots of [libraries](https://wiki.haskell.org/Generics). The only one I've used in GHC Generics. It was hard to grok at first, but it works remarkably well. * Finally, [Template Haskell](https://wiki.haskell.org/Template_Haskell) can do just about anything because it allows you to run Haskell code at compile time. That gives you chance to do code generation, for instance. It's also the most unwieldy of the bunch. Which makes sense considering it's very general and very powerful. I hope that helps.
Think of `cabal install` as an alias for several other commands, including configure, build, copy and maybe some others I'm not thinking of right now. It's less that `cabal install` is smarter and more that it just does more.
My claim is that there *exist* people who find the argument order for `.` unintuitive. Counter-examples are not disproofs. I never claimed that view is universal. &gt; If I think of it as a declarative description rather than an imperative sequence of actions Declarative doesn't imply any particular ordering. You can declare a route in any order - whatever is convenient for the context. There is no one ordering that's convenient for all contexts, so IMO "being declarative" is irrelevant here. Going back to the traffic directions example, a route isn't imperative - it's declarative. The imperative actions only happen when someone follows that route. The reason to identify a route is presumably so that it can be followed, but that's just why the "imperative" ordering is usually natural in that context. In some cases, the same applies to the steps needed to compose a sequence of functions. That's one of the reason pure computations are occasionally expressed in `do` notation when even the abstraction (as far as one exists) has no effects. The argument ordering for `.` isn't a result of declarative thinking, it's an accident of the fact that functions are written in a prefix notation. If functions happened to be written postfix, no doubt your simple rewrite would have been `(x g) f --&gt; x $ g . f`. It's just as declarative - only notational conventions have changed. That was part of my point when I mentioned Forth earlier, though I was putting all the stress on effects so that wasn't very clear. There's a tendency to view math-based abstractions as "high level" and imperative abstractions as "low level". IMO the point of a high level abstraction is to be close to the actual problem you're trying to solve, not merely as far as possible away from the abstractions provided by machine code. Depending on the problem, function application can be as far from your problem as `goto`. The historic accident of function application being written prefix, therefore, is not a good reason to write problem-oriented code any particular way. TBH the objection I originally expected was about non-strict evaluation order - after referring to the correlation between strict-evaluation ordering and notational ordering in Forth, I wasn't very clear when I said "writing operations in the order they are done" that that doesn't mean the literal evaluation order. That's kind of relevant here. First-thing-first can absolutely be a high-level aspect of an abstraction, irrespective of low-level implementation details. 
From a category theory perspective I think the most basic is `join :: m (m a) -&gt; m a`, but then I agree if you decide to use Kleisli-style presentations `(=&lt;&lt;)` is clearer. The reason people use `(&gt;&gt;=)` is the analogy with "effectful let" when the function argument is a lambda -- less useful in presence of a `do` notation or `let!` syntax.
`(*)` has type `Num a =&gt; a -&gt; a -&gt; a`, so `zipWith (*)` forces the lists to be the same type, as you can see in `ghci` Prelude&gt; :t zipWith (*) zipWith (*) :: Num c =&gt; [c] -&gt; [c] -&gt; [c] `[1..]` desugars to `enumFrom 1`, which has an enum constraint Prelude&gt; :t [1..] [1..] :: (Enum t, Num t) =&gt; [t] &gt; why do both lists have to be enumerable? Because `(*)` requires that both lists must be of the same type. &gt; can I map that list to the integers using `[1..]`? IIUC, no, but I'm not sure how you expect this mapping to behave.
The way I've done FFI with C++ is to make a C binding and then make an FFI to the C binding. I think this is why people say it's a pain. The effort to make an explicit C binding feels wasteful.
Does this command work? cabal configure --enable-benchmarks &amp;&amp; cabal build &amp;&amp; cabal bench I got that from here, but I've actually never used `cabal bench`: http://blog.johantibell.com/2012/04/cabal-bench.html 
It took me around 3 months to get to the point where I felt comfortable reading all the papers I was reading on Haskell and on type theory and functional programming. It was nearly 6 months before I even really tried writing any code in Haskell that wasn't just me trying to copy code out of an old paper and just get it to compile to follow along.
Hm. I don't think point-free code is inherently readable or unreadable. that's like saying you avoid using the letter 'e' in variable names...the point is to be readable, and if that happens to be point-free, then so be it. it might be a little indiscriminate to just say "all point free code is unreadable =&gt; never use point free." point-free for the sake of point free is bad of course, but using e's in your variable names just because you want to use e's is also equally weird. There is readable code that happens to be point-free, and unreadable code that happens to be point-free...the point-freeness is an arbitrary side-thing, i think. For example, I don't know anyone who would write mapM_ (\x -&gt; print x) xs instead of mapM_ print xs by looking at the second one, seeing that it's point-free, and disregarding it as a hard and fast rule. readability should be the main concern, i think...point-freeness or non-pointfreeness might be a red herring if used as "the reason" for not picking something, and trying avoiding all point-free solutions as a policy might be a self-defeating strategy :) Also, i think if data flow is what you care about, then isn't `(.)` the quintessential data flow operator? `f . g` is "it goes through g then f". `(.)` exists basically literally for data flow, right? f . g $ x it flows from x, to g, to f f . g &lt;$&gt; x it flows from x, to go, to f f . g =&lt;&lt; x it flows from x, to g, to f. I think the point of using `.` *is* to emphasize data flow, over function application. `.` is sort of the go-to thing for visualizing and representing data-flow in haskell, so the reason for using `.` in general *is* to demonstrate data-flow, I think...is there a particular reason why `.` is anti-data-flow to you?
I'm too tired to get at the core of it right now, but I'll try to give a partial answer. Haskell is a pure language. This means that compilers have a lot of freedom when it comes to implementation details. For example, a compiler could opt to memoize all function results. (This generally won't happen -- but the point is that Haskell's semantics force us to admit it as a possibility.) If you compiled a program using your `print` function with a memoizing compiler, you would find that your program would never print the same thing more than once. It gets even worse with non-strict semantics. Haskell is allowed to evaluate an expression in any order -- even in parallel -- as long as it ends up calculating the most-defined possible result (modulo strictness hackery). With your `print` function, you would be unable to tell Haskell to print things out in a specific order.
This! The laziness *forced* Haskell to use monads for IO, but the benefits of purely functional code are independent of the laziness. Even outputting something to the screen (or, say, "printing" an HTTP request to delete your bank account...) is a meaningful side-effect that needs to be tracked.
There exists a type that has zero values. [This wikipedia article](https://en.wikipedia.org/wiki/Bottom_type) offers various names for this type, including "Bottom", "Empty", "Zero". Haskeller sometimes refer to this type as "Void". That's fine, I'm not overly concerned with what name we wish to use to refer to that type. Let's just go with "Void" for the sake of this particular discussion. Haskellers often then say that this type has a special value which they call "bottom". For example: * http://dev.stephendiehl.com/hask/#bottoms "The bottom is a singular value that inhabits every type". * https://wiki.haskell.org/Bottom "Bottom is a member of any type" * https://en.wikibooks.org/wiki/Haskell/Denotational_semantics "we introduce a special value ⊥, named bottom" * http://blog.ezyang.com/2010/12/hussling-haskell-types-into-hasse-diagrams/ "These diagrams are quite good for forcing yourself to explicitly see the bottoms lurking in every type." Why is this mental model of how bottom types work so popular? Instead of postulating the existence of some special value, it seems a much cleaner mental model is to say all values are normal, that the Void type truly has zero values, and that `undefined` is a function whose return type is Void (or Bottom or whatever you want to call it), not because it can magically return a value from a type that is explicitly defined to have no values, but because it does not return: it either raises an exception (as seen in [here](http://hackage.haskell.org/package/base-4.8.0.0/docs/src/GHC-Err.html#undefined), where they offer the implementation `undefined = error "Prelude.undefined"`) or it enters into an infinite loop (as proposed [here](https://wiki.haskell.org/Bottom), where they offer the implementation `undefined = undefined`). Why isn't this latter mental model more popular in Haskell? Is there some scenario that the latter model fails to explain, but the former model succeeds?
That's the same approach I ended up taking. [Main.hs](https://bitbucket.org/btubbs/khask/src/b8042f34ea72d30c5a498c2507b261c39ec4c892/Main.hs?at=default) Good news: It's working now! Bad news: If I were making a more complex app that connected to more things (as I often do in my day job), it'd get ugly in a hurry. I want to conquer this monad transformers thing.
Category theory concerns itself with the ways things relate to other things. One way that it does this is by describing common ways in which you can internalize properties of the category you are enriched in into your category. Constructions like powers and copowers, products and coproducts, etc. are really about borrowing a lot of set-like structure from the category you live in into your category itself. Once we've found this structure can live inside a category you can use it in the "internal language" of the category without ever having to leave it. It is the language you have while stuck within whatever set of constraints you've got. The code we write lives in just such an internal language, within the system we're manipulating. Moreover, the more of what we want to talk about that we can internalize in this manner the more our code can talk about itself, the more can be said about how we want to evolve and build things without having to extending the language itself. Category theory gives us a ton of useful isomorphisms: ways to push around structure without changing or abridging its meaning. To a mathematician these are just the "moves in the game", in that they let them rearrange equations to get to some end-goal, and any path is effectively as good as any other path. A programmer, however, cares about the "how" a great deal. This large pool of isomorphisms gives the programmer a series of mutations they can apply to their code that have non-trivial performance trade-offs. Spotting that I can transform any `Monad` in Haskell (nay, any Functor) into a Codensity monad and that I can retract that transformation isn't unduly interesting, except insofar as it lets me avoid paying high costs for left associated (&gt;&gt;=)'s in my code as long as I only inspect the result once. I've seen this take code that was running in O( n^2 ) time because it was written naively and bring it up to O(n) -- for straight line code written in a "free monad" just so the user could mock up the interface for their interaction with an external system. Mocking is very much a concern the average Joe on the street developer deals with every day. Knowing about f-algebras and f-coalgebras lets me fuse together multiple operations that are building up and tearing down a structure, and avoid even constructing the intermediate representation at all at times. We encourage functional programmers to think in terms of folds. This gives a more general framework, and tools they can use to begin fusing such folds together. I like to build things that exist for "deep fundamental reasons". One of the ways I try to ensure that is by trying to stick to building initial or terminal objects for whatever concept it is that I'm exploring. Why? Then any other solution is going to factor through it. Either there will be an embedding from whatever is stated using my initial solution into theirs, or my terminal solution can be used to interpret theirs. Einstein used to talk about how he was a physicist and not a mathematician because in pure mathematics the space was too big and he wouldn't know how to find the interesting problems. For me category theory is a nice way of constraining that enormous space back down. It gives me a good source of questions to ask about each data type or concept I go to introduce in a library, it gives me a vocabulary that has been vetted by 70 years for use across domains I've never even heard of. We just don't have anything that has had that many eyeballs on it for that much time in the "software development" ecosystem. Given that we have a topology of algorithms and semialgorithms and the like, and that many of category theory's early successes were in topology, homotopy.. it stands to reason that we have a lot to learn. I've gained a lot from doing category theory badly in Haskell. Do you _need_ to know category theory to write Haskell? No. Do you need to be able to read sheet music to play a guitar? No. Does it help? Is it a great step towards being able to build upon others' work? I think so.
You can work your way through all of the statements folks make in a CPO about `_|_` in the vocabulary you are using here. It gets pretty tedious as the problems grow more complicated, however. The benefit of the `_|_` vocabulary is we can simply say that all functions are monotone. That is to say, for something like Bool, we have `{_|_, False, True }` as inhabitants with `_|_ &lt; False`, `_|_ &lt; True` and _every_ function from `Bool` to `Bool` is monotone with respect to this ordering. So e.g. if you take `_|_ -&gt; _|_` you can do whatever you like for `False` or `True`, but if you take `_|_ -&gt; True`, then you must also take `False -&gt; True` and `True -&gt; True`. Simply stating that all functions are monotone, or more properly talking about functions being ["Scott continuous"](https://en.wikipedia.org/wiki/Complete_partial_order#Continuous_functions_and_fixpoints) carries with it a _lot_ of meaning about how `_|_`'s must be treated in your language! You could of course build up all that machinery using a different vocabulary, but you'd lose all the existing terminology and tools of [CPOs](https://en.wikipedia.org/wiki/Complete_partial_order) and the like. These tools are already well studied in other areas, and let us borrow results from areas like lattice theory, rather than force us to make up terminology and tooling as we go, and it makes it easy to formally talk about things like one function is more defined than another just in terms of reasoning about partial orders. As a friend of mine likes to say, "continuity means nothing weird happens at infinity". A large part of the genius of Dana Scott's original work on dcpos and denotational semantics is that the notion of continuity that he built up is good enough to make a ton of non-trivial problems just vanish. The mappings between any two dcpos themselves form a dcpo, giving us a nice cartesian closed category, so "it looks like turtles all the way down" Could you build all of this up explicitly? Probably, but it'd get to be a pain in the ass to actually work with once you started dealing with recursion. Moreover, you'd have to make up your own vocabulary for everything, and then pretty much anything you published in the field of denotational semantics, nobody would be able to read.
Well, ``undefined`` simply isn't a functino whose return type is Void, it's a function whose return type is ``a``. In other words its return type is anything. Thus, if you have a function ``f :: Int -&gt; Int`` then the type of ``f undefined`` is ``Int``. That typechecks, try it in GHCI. If ``undefined`` was a distinct type, it wouldn't; it would be a compile-time error. Your way of thinking has to do with what happens at runtime. But the types don't even necessarily exist at runtime. They exist when typechecking is done, during compile. And at that time, ``undefined`` will typecheck as any type. 
&gt; Thus, if you have a function `f :: Int -&gt; Int` then the type of `f undefined` is `Int`. That typechecks, try it in GHCI. If `undefined` was a distinct type, it wouldn't; it would be a compile-time error. I disagree: Void is assignable to all types. Given `f :: Int -&gt; Int`, the type of `f someFunctionThatReturnsVoid`would be `Int`. &gt; Your way of thinking has to do with what happens at runtime. I disagree, and I offer as evidence that you talked about "compile time errors" in your previous paragraph to show that the mental model does indeed talk about things happening at compile time rather than at runtime. &gt; And at that time, `undefined` will typecheck as any type. Taking the mental model that `undefined` has the type `Void` and than `Void` is a subtype of all other types yields the same predictions.
Well, there actually are functions like that, you can construct them using ``unsafePerformIO``. Here is an example: https://hackage.haskell.org/package/base-4.8.0.0/docs/Debug-Trace.html#v:trace (There's a link to the source code there on the right.) Other people have covered why doing that kind of thing is a bad idea and goes against the whole idea behind the language. I just wanted to point out it's still possible.
Review code by people skilled in it. Studying the masters is a good idea when it comes to anything!
Here's a working example I basically took off the homepage to test out: import Options.Applicative data Sample = Sample { hello :: String , quiet :: Bool } sample :: Parser Sample sample = Sample &lt;$&gt; strOption ( long "hello" &lt;&gt; metavar "TARGET" &lt;&gt; help "Target for the greeting" ) &lt;*&gt; switch ( long "quiet" &lt;&gt; help "Whether to be quiet" ) greet :: Sample -&gt; IO () greet (Sample h False) = putStrLn $ "Hello, " ++ h greet _ = return () main :: IO () main = execParser opts &gt;&gt;= greet where opts = info (helper &lt;*&gt; sample) ( fullDesc &lt;&gt; progDesc "Print a greeting for TARGET" &lt;&gt; header "hello - a test for optparse-applicative" ) Here's an example of using it: ~ $ stack ghc blah.hs Using resolver: lts-2.16 from global config file: /home/cody/.stack/global/stack.yaml [1 of 1] Compiling Main ( blah.hs, blah.o ) Linking blah ... ~ $ ./blah Usage: blah --hello TARGET [--quiet] Print a greeting for TARGET ~ $ ./blah --hello cody Hello, cody 
`liftA4`, pls.
You'd probably be best suited to just use optparse-applicative just on the basis you seem against depending on the whole of Turtle. Did you see the example I linked: https://www.reddit.com/r/haskell/comments/3cfax3/new_haskellers_ask_anything/csvyu8f By the way, you should *really* tryout Turtle to replace complex shell scripts. I used to use Python for this Niche, but dev time with Turtle is just as fast with all the assurances of Haskell. 
I only did a bit of Haskell in the evenings on the side, and it took me probably around two years until I started being reasonably comfortable with it. Going by the three answers now, maybe 400–500 hours, then?
&gt; I didn't understand their answers, and had to apply their advice blindly. Haha. I have done this *a lot* as I've been learning. When I've used things blindly enough times, when I've parroted enough existing code, I start seeing a pattern and I then understand how it works. The way I learned to work with lenses was through just rolling a dice on which combinator to use in each situation, and then after a while the dice were right more and more times.
From my armchair (minimal AI experience) it seems the only advantage Python has over Haskell there (albeit a big one) is library support. However I'm not intimately familiar with how much easier things like heterogeneous containers help out with AI.
&gt; So if you're "in" a given monad M (e.g. writing a do block of type M a), check whether M is an instance of MonadIO. If it is, you can use liftIO to embed arbitrary IO within it. I believe I've heard the advantage of this is that you have to be more explicit about using just any Monad while inside a Monad whose purpose is specified for something else.
&gt; (Template Haskell?, Various Shakespearian Libraries (and others)? Have you seen this page: http://www.yesodweb.com/book/shakespearean-templates &gt; FFI? http://book.realworldhaskell.org/read/interfacing-with-c-the-ffi.html https://en.wikibooks.org/wiki/Haskell/FFI http://blog.ezyang.com/2010/07/safety-first-ffi-and-threading/ [Haskell FFI Tutorial](https://github.com/ifesdjeen/haskell-ffi-tutorial) *UNTESTED/UNREVIEWED* &gt; How do I even begin to design a non-trivial program in Haskell (the approach is different from the data oriented approach of OO/procedural world?)) http://howistart.org/posts/haskell/1 https://en.wikibooks.org/wiki/Write_Yourself_a_Scheme_in_48_Hours http://www.ccs.neu.edu/home/matthias/HtDP2e/ (not Haskell specifically, but functional and using [Racket](http://racket-lang.org/)) [Redoing Make - Haskell from Scratch #1](https://www.youtube.com/watch?v=zZ_nI9E9g0I) [Haskell Deconstructed by Jekor](https://www.youtube.com/playlist?list=PLxj9UAX4Em-Lz5btngxMVZxf_B44GETVz) [Beginning Haskell](http://www.amazon.com/Beginning-Haskell-A-Project-Based-Approach/dp/1430262508) has a great approach much like [Real World Haskell](http://book.realworldhaskell.org/read/) with a focus on exercises. I have heard complaints about typos, however they never really bothered me much. &gt; How did you get over this hump? Understand implementing Functors, Monads, Monoids, etc by completing the [NICTA Functional Programming Course](https://github.com/NICTA/course). Additionally, you might want to make sure you've followed /u/bitemyapp's [learn haskell guide](https://github.com/bitemyapp/learnhaskell). Trying to use the now unmaintained [Shpider](https://github.com/codygman/hs-scrape/commit/1e21e53650e2591a8885353b1b16c98787010681) for Haskell scraping goodness, starting to update it, then finding I wanted to make some changes, and creating [hs-scrape](https://github.com/codygman/hs-scrape/). Before I wrote that with a State Monad though I was just passing state with tuples through all of my functions. You can kind of get an idea from the linked section of the [Wikibooks State Monad tutorial](https://en.wikibooks.org/wiki/Haskell/Understanding_monads#Dice_without_IO). tl;dr **Write LOTS of crappy Haskell code, finish the project before moving on, then analyze how it's crappy and solicit feedback from the community**
Additionally *don't be afraid* to modify, experiment, and totally try to break their code. Try to implement on top of their code and twist them into weird chimeras. Bottom line, understand by experimentation and getting your hands dirty.
Can you get away with liftA4?
You might have an easier time using [Stack](https://github.com/commercialhaskell/stack) rather than Cabal which sidesteps issues like the one you had if I'm thinking straight this late in the morning. I'm sure someone will sanity check my logic above ;)
typo fixed!
Having used both Haskell where type inference works wonderfully and Scala where it doesn't really work at all whenever things get complicated enough for it to be useful, I'll stick to Haskell. It is a lot easier to 'explore' when you have usable inference. You can also rip off your explicit signatures and look at the type to see if you really are using all the constraints you think you are. For instance, this was how we discovered the more general formulation of lenses that we use today.
Wow, is that really true? What about functions like `show . read`? Sure it's `String -&gt; String` but it still doesn't know which `show` to pick.
Dunno man, [this hits close to home](http://youtu.be/i5oc-70Fby4).
Nice, indeed. [This issue](https://github.com/tonsky/FiraCode/issues/8) will make me stay with [Hasklig](https://github.com/i-tu/Hasklig) though.
I don't know about &lt;&amp;&gt;, but lens adds &amp;. However, it's left-associative, which makes it incompatible with &gt;&gt;=.
 &gt; `ScopedTypeVariables` doesn't *require* explicit quantification, it just allows it. Explicit quantification is *allowed* even without `ScopedTypeVariables`. But to get type variables to be scoped, explicit quantification is *required*. From the [GHC docs](https://downloads.haskell.org/~ghc/7.8.4/docs/html/users_guide/other-type-extensions.html#scoped-type-variables): "The 'forall a' brings 'a' into scope in the definition of 'f'. This only happens if: The quantification in f's type signature is explicit."
Actually I think of `traverse` as being `fmap` but where the mapping function can do something extra. Like map vs mapM, but over applicative instead of monad, and over any traversable instead of just lists. 
I made [a very modest start on trying to document this](https://github.com/chrisdone/haskell-container-types). I have to flesh out the other data structures and then was hoping to implement a number of benchmarks in that repo that anyone can clone and run, comparing them all for science and justice. I'd appreciate any contributions to that repo. Only so much time in the day.
Ah, that's a good point. So I guess the fuller answer to OP is: `ScopedTypeVariables` wasn't present in the language to begin with because it's rarely needed - for the final version of a a function. But while developing it was found to be very useful.
consider the following code process ih = do eof &lt;- hIsEOF ih if eof then return [] :: IO String else do str &lt;- hGetLine ih recursion &lt;- process ih return (str ++ " newline " ++ recursion) is there some quirk in the do notation that would avoid the extra line to bind the recursive call to process to "recursion'? I would to write something like "bind this expression to the current value" instead of "recursion" n the last line. Is there some idiomatic way to achieve that? 
Are you thinking of something like Conor McBride's [Frank language](http://homepages.inf.ed.ac.uk/slindley/papers/frankly-draft-march2014.pdf)?
Looks promising but probably needs some polish: "OOP Model in fficxx To be explained" Unless it's using LLVM under the hood I would surprised if it's able to parse C++ robustly, but I could be mistaken.
&gt; *two questions probably related* OK, here is one way to relate them: &gt; So if you're "in" a given monad `M` (e.g. writing a `do` block of type `M a`), check whether `M` is an instance of `MonadIO`. If it is, you can use `liftIO` to embed arbitrary `IO` within it. The same is true for exceptions. Check whether `M` is an instance of `MonadThrow`. If it is, you can use `throwM` to throw an exception in whatever way is already supported by this monad. If you also need to catch exceptions, check if there is a `MonadCatch` instance, and if so, use `catch` or one of the other functions derived from it. See the [exceptions](http://hackage.haskell.org/package/exceptions) package. 
&gt; Sometimes ligatures evolve into actual symbols or letters, like &amp; has become an "and" symbol This is interesting. I didn't know of [ampersand's history](//en.wikipedia.org/wiki/Ampersand#History). I always thought of it as a symbol. Thanks for pointing it out.
Is that haskell? that doesn't look like haskell
Yea... Since when is =&gt; valid in pipes?
Some people just shouldn't [touch](http://i.imgur.com/75VUMvE.png) [typography](http://i.imgur.com/h4tGgx2.png)
Since discovering that `&amp;`, `&gt;&gt;=` and `&lt;&amp;&gt;` all have the same infixl precedence, I've switched to mostly using just these operators (instead of `$`, `=&lt;&lt;` and `&lt;$&gt;` which I used before). Chaining them line by line without worrying about parenthesis is so liberating! 
Evaluation order is lazy, which means the particular order forced by the particular implementation of `&gt;&gt;=` will determine the evaluation order of `c &gt;&gt;= b &gt;&gt;= a`. For `a $ b $ c` it is indeed true that `a` is evaluated first. But `b` may not be evaluated at all, or it may and `c` may not. It's not really easy to reason about what will get evaluated without knowing the code.
Is there an article/text version? I'm unable to listen to the talk.
Wouldn't you write the bottom line as (\x -&gt; E) x : A -&gt; B since the first `x` below the line is a local binding and doesn't refer to the same thing as the `x` above the line? Also I don't think you'd bother with the turnstile if the environment is empty 
Integrational tests and "optional logging" are something that helps quite a lot, so nothing new or specific to Haskell.
Looks pretty, but It also looks like a good way to make it harder for learners to join the elite club of haskell.
Check out a HaskellCast episode with authors of both libraries before choosing! http://www.haskellcast.com/episode/006-gabriel-gonzalez-and-michael-snoyman-on-pipes-and-conduit/
Sorry, you are correct. Explicit quantification requires `ExplicitForAll`. That extension is implied by several others, one of which is `ScopedTypeVariables`.
You are describing what ghc does. It's not the only way to do things.
It's kind of weird saying a function is incorrect when it's fixed by adding a type annotation. Couldn't you use the same logic to say that any Haskell2010 that needs an annotation is also incorrect?
By 'opposite' do you mean taking the arguments in the other order, or something that appends a single element to a list? (In either case, the answer is no.)
you may want to try out io-streams which is a thin abstraction layer over handles and sockets and yet handles this exact issue youre experiencing
I think it would be fair to say that an FRP computation can be realized in `pipes`... but it's a bit weird to call it an FRP lib.
While I'm sympathetic to your POV here... these are not invalid questions! ("A mammal is a classification of animals—humans are part of this class and we can tell because we don't lay eggs and we do breastfeed" and "A book is a method of storing and transferring information through time and space that really dominated in that role through most of human history") Of course there's no way to answer this question without talking about math (there's, in a sense I subscribe to, no way to talk about nearly anything without it) but it's not unfair to ask us to turn down the jargon and in-jokes ;P
&gt; size matters As a memory-managed language, Haskell binaries will always require a runtime and therefore will never be as small as C binaries (well, right, unless you count dynamic linking). But I believe JHC produces much leaner binaries than GHC.
Just don't write a blog and paste the ligatures into the blogposts. It's horrible :( Same applies to academic papers. Don't do it! Now I have no idea what I'm reading. Ligatures are fine in private, just not out in public.
Because it's a font, I could presumably install it in my editor and it wouldn't affect anyone else. The same cannot be said for using Haskell's unicode extensions.
&gt; Just don't write a blog and paste the ligatures into the blogposts. This is not even possible unless your blog uses the ligature-supporting font. The text is not changing when you use this font, just the way the text is rendered. Any other application looking at your text will see e.g. `-` followed by `&gt;`—two separate characters.
OCaml is also managed, and it produces much smaller binaries.
How does that compare to conduits? 
Even shorter if `ap` were in the Prelude.
why do you think that Shake would make cross-compiling more difficult?
&gt; see my 20-page manifesto entitled "The inky foe of civilisation". I would read it.
Nope. Most of them only kick in for Haskell files. `ghc-mod` gets used by Syntastic without any plugin if it's available.
&gt; The general problem with such unsafePerformIO memoization is that with parallelism the overhead of synchronization is worse is greater than the computation avoided. I agree that that is a problem in some cases. But in this case we assume the whole system relies on that there is but one thread executing the FRP code. I've had an implementation which used parallelism, but the overhead was greater than the gain. Note that a user cannot introduce parallelism in FRP code by using par or something: this will evaluate the computation, but will execute it, this is done solely by the main loop. &gt;You mention that mixing timelines causes problems which are detected dynamically, but that the ST rank-2 trick can prevent that. But you also state that "we have opted not to apply this technique, because it pervades client code". What does that mean? It just means that it is a bit annoying to have these 's'es popping up in all types, clutters client code. &gt; If you redefined M with the ST monad (as opposed to what was actually done, and merely borrowing its rank-2 trick), couldn't you both statically prevent these errors, and get rid of the parallel memoization problem (and unsafePerformIO quesiness) by using STRef? No, sorry that will not work. The unsafePerformIO trick is used to implicitly associate a mutable variable with expressions such as b `switch` e. With ST, we will still need to do that (or unsafePerformST or something). See section 6.2
I don’t understand your comment…
Cool thanks!
True -- it would be better to specify "there are many possible runtime representations. the one chosen by GHC, the haskell compiler that nearly all of us use these days is as follows:" :-)
Ah, I should have specified! There is no function you can write where a top-level type signature cannot be fully inferred :-)
For one, the types and the abstraction is much simpler (and thus easier to understand, less cognitive overhead etc). Take a look at the [`io-streams` tutorial](http://hackage.haskell.org/package/io-streams/docs/System-IO-Streams-Tutorial.html) to get an idea how easy everything becomes in the familiar `IO` monad. Then there's `pipes` in case `io-stream` doesn't fit the bill and something more generic is needed, as you can implement [`io-streams` on top of `pipes`](http://www.haskellforall.com/2013/04/pipes-and-io-streams.html).
In threads like these, I find it helpful to load all the comments and sort by new. Otherwise, things slip through the cracks.
The `===` ligature is the worst thing I've seen in a font. Three-bar equality has a meaning, separate from two-bar. `===` is just "sort of more equal, or something... we've gotta ship tomorrow"
The version I posted earlier ([thread](https://www.reddit.com/r/haskell/comments/3cbo5u/anonymous_records_in_haskell_nikita_volkov_on_his/)) had the slides out of sync with the video, which should now be fixed.
One thing that I do in my project is running [HLint](https://hackage.haskell.org/package/hlint). There's always good suggestions, this is the output I got from your code, most of suggestions are straightforward, feel free to ask any question: ./code/api/app/Main.hs:3:1: Warning: Redundant as Found: import qualified Lib as Lib Why not: import qualified Lib ./code/api/src/Model.hs:78:11: Warning: Use . Found: foldr (&amp;&amp;.) (val True) $ map (parseCond item) xs Why not: foldr ((&amp;&amp;.) . parseCond item) (val True) xs ./code/api/src/Controllers/Sessions.hs:29:3: Warning: Redundant $ Found: post "/sessions" $ _save Why not: post "/sessions" _save ./code/api/src/Controllers/Sessions.hs:30:3: Warning: Redundant $ Found: delete "/sessions" $ _delete Why not: delete "/sessions" _delete ./code/api/src/Controllers/Users.hs:22:10: Error: Redundant do Found: do post "/users" $ _save Why not: post "/users" $ _save ./code/api/src/Controllers/Users.hs:23:3: Warning: Redundant $ Found: post "/users" $ _save Why not: post "/users" _save ./code/api/test/Controllers/LogTag.hs:51:21: Warning: Use liftM Found: loginTestUser &gt;&gt;= return . getCookie Why not: Control.Monad.liftM getCookie loginTestUser ./code/api/test/Controllers/LogTag.hs:62:21: Warning: Use liftM Found: loginTestUser &gt;&gt;= return . getCookie Why not: Control.Monad.liftM getCookie loginTestUser ./code/api/test/Controllers/LogTag.hs:75:15: Warning: Use liftM Found: loginUser "dummy" &gt;&gt;= return . getCookie Why not: Control.Monad.liftM getCookie (loginUser "dummy") ./code/api/test/Controllers/Logs.hs:43:21: Warning: Use liftM Found: loginTestUser &gt;&gt;= return . getCookie Why not: Control.Monad.liftM getCookie loginTestUser ./code/api/test/Controllers/Logs.hs:47:9: Warning: Reduce duplication Found: setupInitialData (Just c) &lt;- loginTestUser &gt;&gt;= return . getCookie request methodPost "/logs" [("Cookie", c)] [json|{message:"test 3"}|] `shouldRespondWith` 201 Why not: Combine with ./code/api/test/Controllers/Logs.hs:60:9 ./code/api/test/Controllers/Logs.hs:47:9: Warning: Reduce duplication Found: setupInitialData (Just c) &lt;- loginTestUser &gt;&gt;= return . getCookie request methodPost "/logs" [("Cookie", c)] [json|{message:"test 3"}|] `shouldRespondWith` 201 Why not: Combine with ./code/api/test/Controllers/Logs.hs:74:9 ./code/api/test/Controllers/Logs.hs:48:21: Warning: Use liftM Found: loginTestUser &gt;&gt;= return . getCookie Why not: Control.Monad.liftM getCookie loginTestUser ./code/api/test/Controllers/Logs.hs:52:18: Warning: Redundant bracket Found: body `shouldSatisfy` (isInfixOf ":\"test 3\"") Why not: body `shouldSatisfy` isInfixOf ":\"test 3\"" ./code/api/test/Controllers/Logs.hs:56:21: Warning: Use liftM Found: loginTestUser &gt;&gt;= return . getCookie Why not: Control.Monad.liftM getCookie loginTestUser ./code/api/test/Controllers/Logs.hs:61:21: Warning: Use liftM Found: loginTestUser &gt;&gt;= return . getCookie Why not: Control.Monad.liftM getCookie loginTestUser ./code/api/test/Controllers/Logs.hs:66:18: Warning: Redundant bracket Found: body `shouldSatisfy` (isInfixOf "test-test 3") Why not: body `shouldSatisfy` isInfixOf "test-test 3" ./code/api/test/Controllers/Logs.hs:70:21: Warning: Use liftM Found: loginTestUser &gt;&gt;= return . getCookie Why not: Control.Monad.liftM getCookie loginTestUser ./code/api/test/Controllers/Logs.hs:75:21: Warning: Use liftM Found: loginTestUser &gt;&gt;= return . getCookie Why not: Control.Monad.liftM getCookie loginTestUser ./code/api/test/Controllers/Logs.hs:81:21: Warning: Use liftM Found: loginTestUser &gt;&gt;= return . getCookie Why not: Control.Monad.liftM getCookie loginTestUser ./code/api/test/Controllers/Logs.hs:88:15: Warning: Use liftM Found: loginUser "dummy" &gt;&gt;= return . getCookie Why not: Control.Monad.liftM getCookie (loginUser "dummy") ./code/api/test/Controllers/Sessions.hs:15:65: Error: Redundant do Found: do post "/sessions" [json|{invalid:"input"}|] `shouldRespondWith` 400 Why not: post "/sessions" [json|{invalid:"input"}|] `shouldRespondWith` 400 ./code/api/test/Controllers/Sessions.hs:22:69: Error: Redundant do Found: do loginTestUser `shouldRespondWith` 400 Why not: loginTestUser `shouldRespondWith` 400 ./code/api/test/Controllers/Sessions.hs:30:72: Error: Redundant do Found: do get "/sessions" `shouldRespondWith` 404 Why not: get "/sessions" `shouldRespondWith` 404 ./code/api/test/Controllers/Sessions.hs:35:24: Warning: Use liftM Found: loginTestUser &gt;&gt;= return . getCookie Why not: Control.Monad.liftM getCookie loginTestUser ./code/api/test/Controllers/Sessions.hs:44:77: Error: Redundant do Found: do delete "/sessions" `shouldRespondWith` 200 Why not: delete "/sessions" `shouldRespondWith` 200 ./code/api/test/Controllers/Tags.hs:39:21: Warning: Use liftM Found: loginTestUser &gt;&gt;= return . getCookie Why not: Control.Monad.liftM getCookie loginTestUser ./code/api/test/Controllers/Tags.hs:43:9: Warning: Reduce duplication Found: setupInitialData (Just c) &lt;- loginTestUser &gt;&gt;= return . getCookie request methodPost "/tags" [("Cookie", c)] [json|{name:"test 3"}|] `shouldRespondWith` 201 Why not: Combine with ./code/api/test/Controllers/Tags.hs:54:9 ./code/api/test/Controllers/Tags.hs:43:9: Warning: Reduce duplication Found: setupInitialData (Just c) &lt;- loginTestUser &gt;&gt;= return . getCookie request methodPost "/tags" [("Cookie", c)] [json|{name:"test 3"}|] `shouldRespondWith` 201 Why not: Combine with ./code/api/test/Controllers/Tags.hs:66:9 ./code/api/test/Controllers/Tags.hs:44:21: Warning: Use liftM Found: loginTestUser &gt;&gt;= return . getCookie Why not: Control.Monad.liftM getCookie loginTestUser ./code/api/test/Controllers/Tags.hs:50:21: Warning: Use liftM Found: loginTestUser &gt;&gt;= return . getCookie Why not: Control.Monad.liftM getCookie loginTestUser ./code/api/test/Controllers/Tags.hs:55:21: Warning: Use liftM Found: loginTestUser &gt;&gt;= return . getCookie Why not: Control.Monad.liftM getCookie loginTestUser ./code/api/test/Controllers/Tags.hs:62:21: Warning: Use liftM Found: loginTestUser &gt;&gt;= return . getCookie Why not: Control.Monad.liftM getCookie loginTestUser ./code/api/test/Controllers/Tags.hs:67:21: Warning: Use liftM Found: loginTestUser &gt;&gt;= return . getCookie Why not: Control.Monad.liftM getCookie loginTestUser ./code/api/test/Controllers/Tags.hs:74:21: Warning: Use liftM Found: loginTestUser &gt;&gt;= return . getCookie Why not: Control.Monad.liftM getCookie loginTestUser ./code/api/test/Controllers/Tags.hs:81:15: Warning: Use liftM Found: loginUser "dummy" &gt;&gt;= return . getCookie Why not: Control.Monad.liftM getCookie (loginUser "dummy") 36 suggestions 
I think the [Hackage](https://hackage.haskell.org/package/optparse-applicative) description is enough of a proof: data Sample = Sample { hello :: String , quiet :: Bool } sample :: Parser Sample sample = Sample &lt;$&gt; strOption ( long "hello" &lt;&gt; metavar "TARGET" &lt;&gt; help "Target for the greeting" ) &lt;*&gt; switch ( long "quiet" &lt;&gt; help "Whether to be quiet" ) greet :: Sample -&gt; IO () greet (Sample h False) = putStrLn $ "Hello, " ++ h greet _ = return () main :: IO () main = execParser opts &gt;&gt;= greet where opts = info (helper &lt;*&gt; sample) ( fullDesc &lt;&gt; progDesc "Print a greeting for TARGET" &lt;&gt; header "hello - a test for optparse-applicative" ) This is one of the moments when i catch myself thinking, "how can people even do their daily programming without applicative functors?"
Why is this "[deleted]"? Maybe I could have prevent that, it's just that I was thinking what to reply.
Yes, precisely. I was alluding to Levy's "call-by-push-value", which Frank's core calculus elaborates to.
E.g. http://comonad.com/reader/2011/free-monads-for-less-3/ is the way that we implement IO in ermine. (More or less, once you add exceptions to the mix it gets a bit uglier.) 
I wonder as well: why you do not want to use the FRP libs out there?
See what others use [here](https://www.reddit.com/r/haskell/comments/3c5383/development_tools_survey_results/)
&gt; In H98, there is no function that you can write where the type cannot be fully inferred. The big exception to that is selecting class instances. The type of `show :: Int -&gt; String` can't be inferred; you need annotations to write `show . read`. I don't know if you'd ever need STV when you're playing [stupid tricks](http://stackoverflow.com/a/27220354/1186208) to force inference with `asTypeOf`. *Edit*: I see /u/Darwin226 already noted this. As you said in your reply, it's always possible to infer *a* type signature, if perhaps not *the* type signature that's desired.
To expand on /u/dagit's answer: Judicious use of inlining and specialization (which may require pragmas to expose cross-module optimization) will often eliminate the runtime dispatch. To conclusively determine whether GHC has succeeded in eliminating the dictionary lookup, examine the generated Core output with `-ddump-simpl`.
One of the things I really love of our community is the immense sense of respect we have for each other as developers and human beings. I'm pretty sure that in any other community this release, which is happening weeks after the initial stack announcement, might have been the perfect "excuse" to make it cabal-install-centric, (en)forcing a fracture between people wanting to try out or switch to stack &amp; normal user of cabal install. Even so, WT could have written this post in a cold-war fashion, without even mentioning stack. I want to seize the chance to thank WT - I interact with some of their devs on a everyday basis and I'm constantly amazed by their professionalism &amp; competence. A big thank you also to all the people hacking daily to make the Haskell ecosystem a better place.
&gt; All computations perform side effects such as heating your cpu Fundamentally, Haskell is a *language*; a system of expressing statements. Running a program on a computer is of course very helpful, but that's not always the best way to look at it. And with an optimizing compiler, your statement isn't as true as you'd think, even in a language like C; some computations might be performed at compile time rather than run time, or re-ordered, or changed, or eliminated; so long as the observable behavior complies with the language or compiler specification. Your `print` function, for instance, is equivalent to `id`, or a redundant cast in C, and should have no runtime effect at all because it'll be inlined away or ignored.
Yeah, I’ve been told about `reflex`. May you elaborate on your answer? Which subpart of FRP does it use?
Thanks! I’ll have a look. I’m also thinking of using `pipes`, which could be a great use for streaming values to views once bound to my `Future`s. **EDIT: that is a very good link! I devored all the content! Thanks again!**
&gt; People totally helped me, but I didn't understand their answers, and had to apply their advice blindly. I'll echo /u/kqr: don't feel too bad about that! The first step to understanding X is knowing that X exists and that you don't understand it. Knowing that X addresses your problem is a good starting place; now you can look at X more closely, try to figure out why it solves or doesn't solve your problem, what it's called, what other problems it solves, etc. And anyway, life is full of things we use but don't fully understand. This particular community is pretty idealistic about fully understanding everything, but that's an ideal; you don't have to get there right away, and you don't have to fully understand the same set of things as anyone else. 
Where do `&amp;` and `&lt;&amp;&gt;` come from? I've never seen them before, and they don't show up in a hoogle search.
Are we referring to the sharp S (eszet), or the small beta character here? If the latter I think we're using the wrong character, lower case beta should not have an even bottom.
I'm not sure about *idiomatic*, but to wrap your head around laziness spend some time understanding the [Tardis monad](http://hackage.haskell.org/package/tardis) and figuring out what you can and cannot write using it. Also try doing various crazy things using `fix` and `mfix` and seeing whether your program crashes or succeeds.
Thanks :)
Thanks :)
Check these out: http://hackage.haskell.org/package/hashtables
That's actually a really good feature request! Please add it to: https://github.com/haskell/cabal/issues And if there is any other command besides `bench` that you feel should do the same, please mention it too!
You may want to have a look at the auto library: https://hackage.haskell.org/package/auto
It will be easier, or your money back :)
I already know it! It can be used to implement *arrowized FRP*. :)
Nice to see this back. One question I wanted to ask in the previous thread was if using anonymous records with sum types would incur a small performance cost. The slides seemed to state there wouldn't be, but it only compared using a newtyped anonymous record to a regular record. It seems like sum types would complicate things.
&gt; P a' a b b' f (r, [a'], [b]) That looks so scary.
Ooh, I've never seen that reduce duplication warning before. I like that a lot.
&gt; Similar things were done with "db", where people just made one vertical stroke, or "tz", which got welded into a weird-looking glyph because writers were too lazy pick up the pen between letters. All of these practices are understandable, but I reckon that they were transported into the world of typesetting because of traditionalism, not because they aided reading or looked good. Really, we should be drawing a little ox head instead of writing the letter A. And don't get me started on this lower case nonsense.
They mean that if you screenshot a page of code (who are these monsters, by the way?) it will be illegible to someone not already familiar with these ligatures.
&gt; And don't get me started on this lower case nonsense. Funny that you should mention that! I am against letter case as it happens! In fact, I regard capitalization as pointless tedium whose minuscule benefits are only appreciated by the sort of neurotic ass-kissers who think that its omission "disrespects" the object of a proper noun. A teacher of mine in elementary school was one such... she always beat me when I "disrespected" a classmate by not capitalizing his name. Come to think of it... That, coupled with my inability to read those god-damned Fraktur and Sütterlin scripts, probably explains a great deal of my present loathing for typographical trickery.
I'm not sure it's a problem with many programmers. As programmers like to code and play with abstractions and implementations. It's a very different focus and hence, possibly, the "rude" comments. But if you find it problematic, it'll be interesting to see what you come up with.
While you're right on linux, all unixes are not made equal regarding their handling of random/urandom; it's the reason that random is probbed first. The bug (that was fixed a week ago) happened to be, that hGetSome is not returning if there no data, as initially assumed. It's now the proper call that return without blocking.
Hmm to be honest I'm not sure. I've been doing a lot of matlab and python at my job and I just wanted to get away from them and revisit FP, but without going back to SML. So I was actually looking at the first chapter of the Real-World-Haskell and Real-World-Ocaml and I actually like the both of them. From what I've read the Ocaml one sort of gives you suggestions on things to write and such and I was hoping the Haskell one does the same. If it doesn't then I'll probably just run over to the other side.
As I understand it, [their addition was not an error](http://unicode.org/faq/ligature_digraph.html). Quote: &gt; The existing ligatures exist basically for compatibility and round-tripping with non-Unicode character sets. Their use is discouraged. No more will be encoded in any circumstances. Unicode wouldn't have gained as much traction if it didn't support round-tripping with legacy encodings.
Oh, the Haskell one gives you things to write too. More broadly, if you're looking for a good "toy project" to start with, the write yourself a scheme in 48 hours is a classic.
&gt; minuscule benefits I see what you did there.
I feel like I'm really studying too much theory(as compared to other programming languages I learned). I sometimes practice Haskell in codewars.com but I feel like working on couple of my own projects will make the adapting of Haskell much quicker. So basically, I'm looking for newbie project ideas :P
I've been using prettify-symbols-mode in emacs to do something similar and I've found it quite helpful after a long day of staring at code.
I always disliked ligatures too and never could understand how on earth they are supposed to make anything look better (in logotypes, okay, but never in ordinary typesetting – apart from the issue with serif fonts discussed below). However, I have to admit that I can't say ligatures are inherently bad – or at least they definitely aren't bad enough to deserve the amount of irrational hatred I feel towards them. I guess I'm just very annoyed with all those people who think they are inherently good and mentally give bonus points to whatever site/book that happens to use them (I think Apple used them in the past, for instance, and maybe even now). I'd also be quite glad if everyone just forgot about ligatures and stopped spending any effort whatsoever on supporting them in various fonts, because once it happens, I'm sure nobody would feel any need to revive them. 
This just got posted to haskell-cafe https://mail.haskell.org/pipermail/haskell-cafe/2015-July/120395.html
&gt; Node can't even scale past a single CPU core last time I checked. You probably haven't read past the "NodeJS is single-threaded" titles. Properly designed NodeJS applications are (virtually) stateless (sounds familiar?), which makes it trivial to scale them among as many cores as you like. The NodeJS runtime even can automatically route requests among different worker threads, it's explicitly designed to easily scale well. For properly written NodeJS applications the only thing you need to do to run it on multiple cores is to run it with a process manager like [pm2](https://github.com/Unitech/pm2) (e.g. `pm2 start -i &lt;numberOfProcesses&gt; myApp`). There aren't many platforms out there that make it easier to write multi-process web applications than NodeJS. As for the callback hell: there are tons of articles already written about it. Any half-decent (Javascript) dev knows how to prevent this. Perhaps the most well-known strategy is to use the promise monad, but there are dozens of other techniques out there. The upcoming versions of Javascript also have a large focus on improving asynchronous programming, so it will even get better. I have written web servers in several languages (Among things, Java, Python, NodeJS, C# and even C++. Unfortunately not yet in Haskell), but in 9/10 real world use cases NodeJS is a lot faster than the others (it actually doesn't perform that well in synthetic benchmarks usually). NodeJS isn't really good at CPU intensive tasks, but it extremely good in handling I/O (mostly because it forces you to do it non blocking). So if most of your application consists of responding to web requests and doing stuff like DB lookups then NodeJS will likely outperform most other choices. I can't make a personal judgement on Haskell vs NodeJS for web since I haven't yet written a lot of code in Haskell. Haskell will definitely outperform NodeJS on synthetic benchmarks, if it also outperforms NodeJS in real world application depends most on the frameworks available. I'm not familiar enough with those though to say anything about that.
Mi main intuition for both Monad and Applicative is in terms of trains: &gt; In an Applicative, effects build the railroad upon which the locomotive of function application will travel. All the effects take place while building the railroad, not when the locomotive moves. The locomotive cannot change its course in any way. &gt; A Monad is like having a locomotive in a railroad which is still under construction. Passengers can actually yell to the construction crew a few meters ahead to tell them things like "I like the scenery on this side, please lay the tracks a bit more towards the right" or "you can stop laying tracks, I get out right here". Of course, for this strange railway, the company can't provide a timetable or a fixed list of stops that can be checked before embarking on the train. For more train analogies, see these slides on railway-oriented programming: http://fsharpforfunandprofit.com/rop/
Yes. I tried using it but felt that it wasn't much more than syntax colouring and some ability to group projects. I've also tried the Idea haskell plugin. Eclipsefp seems to be the best fit, as far as things go, but I do find it quite primitive compared to the sort of functionality I get from java or scala IDEs. As a side-note. I'm a vi person, rather than an emacs person. I've tried emacs several times, but it's never taken.
Remember that compilation will elide anything that isn't used from the final binary that gets built. It's totally okay to depend upon libraries that you only use one part of; in fact, it's normal and unavoidable. This is, more generally, the fundamental problem with libraries. Most of the time, you don't want the whole thing.
Thank you /u/conklech and /u/dagit :)
There is a [Early Access Program](http://blog.reactiveprogramming.org/?p=183) for a FRP book that might be of interest to you.
/u/ndmitchell gave a talk about how some bank ported their 10kloc Makefile to Shake. not as proven as make, but that's some proof right there.
Good for you, I guess. It seems to me that, as far as imperative languages go, the only thing I've gained by learning Haskell is gag reflex whenever I see code like import matplotlib.pyplot as plt %matplotlib inline plt.rcParams['figure.figsize'] = (10, 10) plt.rcParams['image.interpolation'] = 'nearest' plt.rcParams['image.cmap'] = 'gray' EDIT: Okay, as for the downvotes, allow me to elaborate. I have to use Python and Matlab from time to time, as they're the languages of choice among the scientific community which I aspire to join. I'm not sure if programming in Haskell has made me any better in these languages, but it surely has made me painfully aware how limited they are. Dynamic typing is sometimes presented as freeing from the tyranny of the compiler, but in reality it robs me of not only all the toys that make Haskell worthwhile (First and foremost type classes. Not to even mention fancier type system extensions, that for example made the [Units of Measure](http://adam.gundry.co.uk/pub/typechecker-plugins) plugin, for which I want to sincerely thank Adam Gundry, because not only it's very useful to me, but it also makes peddling Haskell a lot easier) but also a lot of the documentation. In Haskell, I can often just look at the type of a library function to learn a lot about how it works. Not so much in imperative languages, where for the "freedom" of being able to overwrite variables comes with a can of worms containing global variables (It's just painful to see them used where a reader monad would suffice) and thread safety (when in Haskell I only really need to worry about bindings to thread-unsafe C libraries). Obviously, with some effort it is possible to write in a functional style in these languages, especially because they do support high-order functions, just like it's possible for a Haskell function to launch the proverbial missiles off unsafePerformIO, or a Template Haskell macro to thrash my hard drive and sign me up for ISIS. But the language support and community practices matter. I have to constantly think about things that these languages hide, while Haskell makes them explicit (like the issue of concurrent states /u/TheCriticalSkeptic mentioned). Every now and then I'd run into a library with an outright unfunctional interface. It's an uphill battle.
I haven't used SMT solvers, but otherwise I have thought about this a bit. I think products can be done with extensions already. Something like {-# LANGUAGE TypeFamilies, DataKinds, KindSignatures #-} data Product :: [*] -&gt; * where Unit :: Product '[] Cons :: x -&gt; Product xs -&gt; Product (x ': xs) (++) :: Product a -&gt; Product b -&gt; Product (Append a b) (Cons a as) ++ bs = Cons a (as ++ bs) Unit ++ bs = bs type family Append (xs :: [*]) (ys :: [*]) :: [*] where Append (x ': xs) ys = x ': Append xs ys Append '[] ys = ys Associativity will hold for this type, but commutativity is still only true up to isomorphism. Of course this is still just a linked list memory-wise. I think for sums you need dependent types. Something like: data Sums :: [*] -&gt; * where MkSum :: (n :: Nat) -&gt; (ts !! n) -&gt; Sums ts Edit: I think sums might also work now, thought I haven't compiled any of this: {-# LANGUAGE KindSignatures, GADTs, DataKinds, TypeFamilies #-} data Nat = S Nat | Z data Natty :: Nat -&gt; * where Zy :: Natty Z Sy :: Natty a -&gt; Natty (S a) data Sums :: [*] -&gt; * where MkSum :: Natty n -&gt; Lookup ts n -&gt; Sums ts type family Lookup (ts :: [*]) (n :: Nat) :: * where Lookup (x ': xs) Z = x Lookup (x ': xs) (S a) = Lookup xs a
Idris has [Control.Isomorphism](http://www.idris-lang.org/docs/base_doc/docs/Control.Isomorphism.html) to define isomorphisms between types of the same size (e.g. between Either 1 1 and Fin 2), but I haven't seen much use of it, though I imagine, with dependent types, it could be used to generalize over these structures.
But that code would look almost the same in Haskell wouldn't it?
So much for ignoring things like pleasantness for a programmer to use. =)
Hi, I've updated the high level overview of the project [here](https://github.com/eckyputrady/livelog). I've also added a test to cover "user's journey" in [here](https://github.com/eckyputrady/livelog/blob/master/code/api/test/UserJourney.hs). It's not yet implemented, but I hope it gets the idea about how the application works.
For me, hdevtools fails if it is run within a directory where you have run `stack init` and succeeds if you run it in a normal directory that doesn't have a stack.yaml in it. Using VIM, it fails any time that you try to edit a file that is within a "stack" directory (e.g., a directory that was setup by `stack init`) even if VIM was opened from a parent directory above the "stack" directory. Versions: ~/a/p/h/test1 ❯❯❯ hdevtools --version hdevtools: version 0.1.0.9 (ghc-7.8.4-x86_64-darwin, cabal-1.18.1.5) ~/a/p/h/test1 ❯❯❯ stack --version Version 0.1.2.0, Git revision 65246552936b7da4b64b38372feac903d96a8911 
Good catch, thanks. I think it's overkill to name results you aren't interested in. I can't take credit for the idea of using it to chain composition, but I haven't seen using binds for naming intermediate values anywhere else yet.
I think there's a lot to be said for user-supplied annotations; maybe someday somebody will bring that approach to Haskell. It might be possible then to tweak some mismatches between Haskell and JSON representations.
Thanks for the feedback. &gt; Instead of this `requireUser` function, I'd recommend instead creating an additional monadic context which is a reader over the user id. This allows you to express the dependency on the user being logged in more elegantly, but also will clean up some of your handlers. Can you give a pseudo-code example for this? I don't understand how this is done.
I've seen tons of systems try to offer subtyping. I've seen none of them succeed at making it do what a type system is supposed to do. The purpose of a type system is to rule out common errors while avoiding ruling out too many good programs in the process. When we look at languages like Scala you get folks building data types like Sets and Maps with covariance ingrained in the type. class Set[+K] { def contains(key : K): Boolean ... } Why? Because once you have covariance it starts to look a lot like parametricity, and if you don't want to pick an 'a' and instead what to let something upcast from `Void` or `Nothing` you do this. This is precisely what you are asking for here. case class Empty extends Set[Nothing] { def contains(k: Nothing) = false } Then we go about our business, building binary tree nodes or whatever we need internally: case class Bin[+K] extends Set[K] { ... } So then you take your shiny new `Set[Int]` and accidentally try to look up a Double in it, and it says 'False'. Why? Because it upconverted from `Int` to `Set[Int]` to `Set[Any]` to do the lookup, then failed. Trying to use subtyping as a proxy for parametricity here caused us to accept something we should have rejected. Similar issues lead to lovely results like: List(1,2,3).toSet() yielding the most intuitive possible result: () rather than the Set you might expect. ;) Why? It converted it to a set, then looked up a () in it -- and failed. ... and we still need parametricity anyways. I have access to tons of langauges without decent type inference. F#, Scala, Java, C++. People work in them every day. If you're willing to ignore "pleasantness for a programmer to use" you can excuse anything. I can express "exactly the same set of programs as in the Haskell I know and love" in raw assembly, I just wouldn't, it'd be too painful. For "how important is type inference to Haskell" I think it is a big deal though. Conor McBride makes a pretty compelling argument that we should abandon type inference, but there is a huge usability gap when you make that leap, and the rewards on the other side haven't been big enough for me at least to warrant giving up the benefits of inference, and the each of use and teachability of this side of the divide. I sit there and engage in a dialogue with my compiler all day almost every day. A large part of that dialog isn't just me asking it to take dictation, but rather me asking it for the types of things, building up contexts, filling holes. You're asking for half of that give-and-take relationship to go away -- to remove the feedback cycle. From a [control theory](https://en.wikipedia.org/wiki/Control_theory) perspective that is a pretty bad bet, especially when here it is buying you less than nothing.
&gt; Consider the undying popularity of Comic Sans among non-typographers. And famous Haskellers!
Thanks! I've fixed most of them. Although I intentionally left some behind for the consistency with surrounding code. :)
Honestly I prefer the look of ASCII characters over the mess of Unicode turds people always drag out in these discussions. And frankly, I don't want the number of operators to keep increasing and I sure as hell don't want to have to remember Unicode code points or use a special typing system just to do my work.
In all fairness to the imperative people, though, this is substantially uglier and much less readable.
Thanks for giving the link to the new place! Theoretically, I like the possibilitites of darcs more, but practically I have had to use git most of the time. I'm always a bit sorry when darcs is not used...
&gt; it seems a much cleaner mental model is to say all values are normal, that the Void type truly has zero values, and that undefined is a function whose return type is Void How can it be a function if it doesn't take any arguments?
Watching first video. Its very nice that he stops and asks questions through the lecture, very good pedagogical style. Love it
&gt; Apparently, there were some talks about making this into a proper GHC extension. I haven't seen anything on the mailing lists about this. Do you have any links?
Nice effort! Thanks!
Beware, FP in Scala book is really good, from the people behind scalaz. Not a random java 8 fp book :)
I think you can have intuitions about specific instances of monad. Like you can have an intuition for Maybe, Cont, Reader, etc but it's hard to have a unified intuition. The unified intuition tends to 'degenerate' into just memorizing the mathematical definition :) PM if you have trouble finding a copy.
&gt;IA-64 Don't you mean x64? I have never heard of IA-64 being used in production.
"Programmable semicolon" and all that. My working definition of a monad is "anything that needs an interpreter". This is pretty broad, and with good cause. Everything is a monad.
I was referring to the [Itanium C++ ABI](https://mentorembedded.github.io/cxx-abi/abi.html) mangling convention, which is often referred to as IA-64 or Itanium even though the mangling is used by GCC on x86 and x86-64. The naming is a bit crap.
Anything that requires an interpreter.
What it looks like is not really the issue. The problem, as far as I can tell, is that it is modifying some global state.
I use vim but training is required.
Thanks, I didn't know about (&amp;~). But I posted that as a curiosity really, I think the style with &amp;'s at the left is clear enough.
Having already massively re-factored twice I can definitely see the benefits in that. Though with less pure code the refactoring wasn't as smooth as I've found on less IO heavy code. There are downsides too. Trying to reason about why my `WriterT` was dropping logs during an exception was frustrating. But when I refactored the logs into the `StateT` it involved a handful of trivial changes. In that sense you win some, lose some and some are rained out.
Well yes, I thought that is obvious. Also the use of strings to select the fields, and name the color and the interpolation method.
`(&amp;~)` started as a thought experiment. We at first thought we might be able to replace the need for separate `.~` and `.=` operators and the like by using it to open a `do` block, but once we got more than an hour or two into the enterprise we realized that of course the lack of type-changing-assignment killed this goal.
Could you elaborate on GHC's poor tab handling? I use tabs for indenting without much trouble...
&gt; I thought that is obvious. It seems it wasn't obvious to /u/nolrai. &gt; Also the use of strings to select the fields, and name the color and the interpolation method. Quite right. Perhaps I'm so caught up in a strongly-typed mindset I didn't even notice it!
Ah, I didn't use the plugin. I thought Sublime fully supports Haskell out of the box. Thanks!
I write Haskell all day every day and I use [Atom](https://atom.io/) to do it. See an [old blog post](http://www.edsko.net/2015/03/07/vim-to-atom/) of mine for some pointers, although I have to say that in Haskell "automatic indentation" is something I hate with a passion -- I want to decide my own layout :) Some support with layout is useful of course, as I point out in the blog post. Also, the [ide-haskell](https://github.com/atom-haskell/ide-haskell/) plugin for Atom is currently under very active development, and works rather well with [ghc-mod](https://hackage.haskell.org/package/ghc-mod). I'm currently working with the developer to integrate my own package for [cabal support](https://atom.io/packages/cabal) with `ide-haskell` as well. As a bonus, you can write [plugins in Haskell](http://www.edsko.net/2015/02/14/atom-haskell/)! :)
If you use hindent then you can just write x=do{let{x=1;y=2};return(case x of {1-&gt;();_-&gt;3})} and hit `M-q` (e.g. in Emacs) and get x = do let x = 1 y = 2 return (case x of 1 -&gt; () _ -&gt; 3) I've been pondering recently that I might write a much simpler structured-haskell-mode which pre-supposes hindent (as it was originally written a year or so before hindent) and lets you write and edit in the former fully explicit style when you put your cursor there; it'd have some key to convert to "explicit style", because fully explicit delimiters are *much much* easier to manipulate/edit unambiguously (see Lisp and paredit, unfortunately that Haskell community settled on the prettiest style which is hardest to work with programmatically), and then hit a key to pretty print it in the significant whitespace way as above. Never use the tab key / tab cycle or worry about alignment again. The downside is that it means to use this method on a declaration you have to ultimately reformat it, which isn't good for contributing to other people's projects that don't use automatic formatting.
Yup, edwardkmett has hit this on the head: that's exactly what I meant when I was distinguishing catas from generalised folds.
No, that's just what Nikita mentioned during the Q&amp;A part of the talk.
&gt; Write code that's trivially parallel I'm not sure what do you mean by that, but I think that automatic parallelization is impossible in Haskell because of the lazy evaluation. Though lazy evaluation allows one to choose how to evaluate some particular piece of data, as in [Control.Parallel.Strategies](https://hackage.haskell.org/package/parallel-3.2.0.6/docs/Control-Parallel-Strategies.html)
I stand corrected, but is it actually done by GHC? It would be still misleading to boast about trivial parallelism when it really isn't so trivial.
You could use the [singletons](https://github.com/goldfirere/singletons) library for sums.
Specifically, I'd be interested in seeing a syllabus. 
That's an interesting idea. Unfortunately one can't make the equalities on `(,)` or `Either` hold directly, because they would be inconsistent (since datatype constructors are injective). One could imagine defining a commutative monoid as a new kind, then making the monoid laws hold via a type-checker plugin (much as the type-nats plugins do, or my [uom-plugin](https://github.com/adamgundry/uom-plugin) does for an abelian group). Then it would be possible to index a datatype by the commutative monoid, but whether that is useful for defining open sums/products is another question... it may be easier to index by a list (i.e. a free (non-commutative) monoid), as /u/htebalaka observes.
In some cases, OverlappingInstances [can be considered harmful](http://stackoverflow.com/questions/10942136/whats-so-bad-about-overlappinginstances).
Even the manual tools we have like Strategies can be pretty trivial - I managed to change one of my maps into a parallel map in about ten minutes of reading and one line of code.
Is there a way to compile Haskell programs statically so I can just drop the binary on a Linux machine (mostly Ubuntu if that matters) and it'll run without issues? There's a lot of information about the build system but I couldn't find a definite answer to this question. We have a few small programs at work that would benefit from simple deployment. (Also Haskell at work could be interesting!)
Little offtopic: Is there a single one blog post explaining GADT *without* `class Expr` as an example? I’m not complaining in any way, it’s just for some statistics :) 
Same here.
You could also use Dynamic under the hood of sums. I have some half-baked code for that here: http://hub.darcs.net/mjm/polymorph/browse/src/Data/Polymorph.hs together with partial and total case statements, but the types sometimes get quite hairy and could really use a solver.
Some of the hardest things about parallellism – low level primitives, mutable state/reference semantics, uncontrolled side effects – are a nonproblem in Haskell. That trivialises parallell programming.
The phrase "considered harmful" is considered harmful. 
+1, the format and content make this really helpful. Thank you! I hope you'll add more, keep them on one page, and make each extension linkable.
Although I prefer vim for common programming, I still suggest you use emacs rather than other editors. There's several Haskell-like and ML-like languages, such as Idris, Agda, Coq, ATS, properly supports only emacs.
No, it's not done by ghc. Robert Ennals' branch of ghc was never merged, because it had rather extensive changes with not much gain. A problem with this level of parallelism is that you get too much of it. No matter how cheap thread creation is, it's still going to cost something, and this cost can dominate if the actual work is very small. Finding the right granularity of parallel tasks seems difficult to do automatically, so in that respect i agree that it's not trivial.
&gt; An alternate way around the above issue is to enforce that a has to be Char via some constraint, for example: class CharType a instance CharType Char instance CharType a =&gt; Truthy [a] where truthy s = length s /= 0 Can be written as instance a ~ Char =&gt; Truthy [a] where truthy s = {- could use fact a is a Char! -} s == "true" EDIT: reference https://www.reddit.com/r/haskell/comments/3afi3t/the_constraint_trick_for_instances/
So…building the `optparse-applicative` example app given elsewhere in this thread produces a 2.5Mb executable (BTW: WTF!? it's a program which does as near to nothing as makes no difference! What's in there?) and over 100 cycles, doing `cabal clean; cabal build` takes an average of 6.6s on my machine^† . The most basic example using `turtle` from the tutorial for that library produces a *6Mb* executable (and it does *even less!*) and takes an average (again, over 100 cycles) of 7.8s to clean/build. If you like quick feedback on changes this is the road to ruin. ---- ^† fedora 22 VM running on a Mac with 1.7 GHz i7, 8Gb RAM, SSD. 
idris has a great vim mode
I think this thread was a great idea; hopefully I'm not too late to the party. I think the deriving keyword is pretty awesome, and I saw here (https://en.wikibooks.org/wiki/Haskell/Classes_and_types) that you can derive Eq,Ord,Enum,Bounded,Show, and Read. (1) How is that implemented? But there are other typeclasses that (can) have trivial implementations, for example NFData and Arbitrary. (2) Why isn't the same deriving functionality provided for those? I've seen Foldable, Traversable and (I think) Monoid being derived in some cases - (3) what makes these possible?
How do you deal with the mismatch between Haskell and Coq types? Are all functions assumed total, are all Haskell `data` declarations interpreted as codata?
I was coming to the comments section to say exactly this, but you beat me :)
I stand by my statement. That difference is one tiny detail. I, and most experienced Haskellers I know, use mostly or exclusively &gt;&gt;=. I find that code absolutely readable, clear, and logical, and I see know reason to change. People who use =&lt;&lt;, well, it feels weird to me, but it's still readable and logical, so that's fine too. There are many much more important matters to worry about.
The problem with measuring that is that Haskell is quite different than other languages in that it is both simpler than most languages, while at the other hand being a lot deeper. Simple because it uses a few concepts through and through, deep because people have done amazing things with those concepts. Contrast that to a language like Java or C, where you learn the language and best practices and you are done. In Haskell that’s just the start; while it is possible to be productive after a week.
ghc hard wires a tab stop at 8, so if your editor says different, you see the file differently to ghc. There are plenty of stack overflow questions which are resolved by using tabs instead of spaces. If you set your editor to indent by inserting the appropriate number of spaces, you can't have a misunderstanding between it and ghc. 
I am fully mollified now! Am I to buy this in 3 easy installments of $29.99? :) Also this is as good a time as any, so in my porting efforts I've become a bit inspired/interested in helping out with ghc itself. While I doubt I can fix ghc emitting pie assembly today, or in a year or more really, I am fairly good at dealing with autoconf (years of c/kernel stuff will... warp your brain to tolerate it, like spice) so maybe I can help with the porting and whatnot.
Yep no worries on that, I'm only thinking of having to deal with the fallout of converting to other build tools, *cough* cmake, which makes cross compilation not as tolerable. Or did at the time, I've since shied away from it in the past.
https://github.com/jwiegley/coq-pipes#the-compromise
&gt;(for example, `runEffect` must be a fixpoint) Can someone elaborate? I'm unconvinced that this "compromise" doesn't in fact imply absurdity. I don't know much about Coq inductive data types, but couldn't you show that the base case both is and is not reachable?
in the former case (i.e. directory where you have stack.yaml) can you try: $ killall hdevtools $ hdevtools check path/to/src.hs and let me know what happens?
In the meantime you can use the two hdevtools plugins which work with stack.
To me, especially compared to the Haskell RTS, NodeJS seems very much like implementers being lazy at the cost of language users. It feels like it would be very much a maintenance and debugging nightmare with all the callback hell and no static typing or even simple checks for valid variables at compile time.
Everything is trivial once you understand it!
Thanks for the heads-up. I'll check 'm out. :)
How much "essentially equivalent"? Also, any Applicative+Category alternative to WAI, then?
𝕴 𝖈𝖆𝖓'𝖙 𝖎𝖒𝖆𝖌𝖎𝖓𝖊 𝖜𝖍𝖆𝖙 𝖞𝖔𝖚'𝖗𝖊 𝖙𝖆𝖑𝖐𝖎𝖓𝖌 𝖆𝖇𝖔𝖚𝖙. 𝕻𝖑𝖊𝖆𝖘𝖊 𝖊𝖝𝖕𝖑𝖆𝖎𝖓.
http://shakebuild.com/ is the Shake website and Standard Chartered was the bank. There are some details at http://shakebuild.com/#who-uses-shake. Hopefully cross compilation will be a lot easier with Shake, since we no longer have to worry about 100 different versions of grep all of which take subtly different flags, or take identical flags but with subtly different meanings!
&gt; unfortunately that Haskell community settled on the prettiest style which is hardest to work with programmatically "Pretty" is kind of derogatory, don't you think? It's not exclusively an aesthetic concern; the idea at least is that it's easier to read. (Who knows whether that's true.) Programmatic support is of course great for editing, but your eyes don't have programmatic support. It's a question of perspective. I probably read ten or twenty times as much Haskell code as I write; generally *not* in a programmatic environment. My interests are probably a bit skewed as a result.
Did something neat or unexpected come out from writing these proofs?
Yes, I was being facetious as it is too much text, if nothing else. I just find it funny how we agonize over the few sentences there and end up pleasing nobody, and then have an unprompted testament like this that is so much more powerful.
I'm sorry you (or others) are having an unpleasant experience with the Haskell IRC channels. My nick is `lispy` and I do everything I can to be welcoming and friendly, understand newbies, and hopefully help newbies when I'm there, but alas I don't spend much time on IRC these days. I think something many of us struggle with is politely getting to the bottom of a beginner's problem. Often times an expert uses terminology in a very precise way that actually creates communication problems. I've seen this type of interaction turn gruff and unfriendly too many times :(
I've heard /u/Tekmo say that the WriterT in base is incorrect, and one should use StateT instead. possibly related.
Actually, the appropriate solution is to use a Producer to log values as I [describe here](http://www.haskellforall.com/2014/02/streaming-logging.html).
I think this deserve a repost in Coq subreddit.
I think they are actually equivalent.
Indexed monads and indexed state by default could have been nice.
You mean ArrowMonad? 
Well, one thing I notice when I write Java nowadays is that every time I write something like this: private void whatever(Blah blah) { // ... } I find myself thinking: "How the heck can the caller know if `whatever` does the right thing if it returns no value as evidence that it even did anything?" One example of this is that nowadays when I write a [Visitor](https://en.wikipedia.org/wiki/Visitor_pattern) interface, instead of doing the texbook thing: interface Foo { void accept(FooVisitor visitor); } class Bar extends Foo { void accept(FooVisitor visitor) { visitor.visitBar(this); } } // ... interface FooVisitor { void visitBar(Bar bar); void visitBaz(Baz baz); // ... } I write this instead: interface Foo { &lt;R&gt; R accept(FooVisitor&lt;R&gt; visitor); } class Bar extends Foo { &lt;R&gt; R accept(FooVisitor&lt;R&gt; visitor) { return visitor.visitBar(this.getThing1(), this.getThing2()); } } // ... interface FooVisitor&lt;R&gt; { R visitBar(Thing1 thing1, Thing2 thing2); R visitBaz(Thing3 thing3); // ... } ...with the logic that the second version provides greater compile-time assurances that the code is handling all the cases it ought to, because the code won't compile unless I go to the trouble of actually returning something of type `R` (ok, or throwing an exception). Also, the `Thing1`/`Thing2` thing: instead of making the visitor's methods take the exact subclasses of `Foo`, write them so that they're called with some sort of *observation* of the *subcases* of `Foo`. More sum type-ish.
It sure does, but I found https://github.com/begriffs/haskell-vim-now to be an excellent default setup. The tool setup pretty much everything by itself. Of course my typical Haskell workflow is type in one terminal with ghci opened in another so I'm not the most demanding user but it's worth checking out for newbies.
Yeah, whatever the wrapper is called :)
So the meson link on shakebuild.com is giving a 404. But http://hackage.haskell.org/package/shake-language-c looks interesting. I need to test that out and shake itself and if it makes building my c stuff easier for cross compiling, I'm going to ditch autotools like the bad habit they are.
try one of the IntelliJ IDEA plugis: * [HaskForce](https://carymrobbins.github.io/intellij-haskforce/) * [haskell-idea-plugin](https://plugins.jetbrains.com/plugin/7453) * [IntelliJ Haskell](https://rikvdkleij.github.io/intellij-haskell/) 
Well, it seems to keep expanding. Avant, in Chicago, is interested in Haskellizing some of its technology. It's mostly Ruby and R (with some Clojure) and will stay that way for some time; 10-20% market share in a year would be a pretty good showing. We do know that some of our services are going to need to be rewritten in a high-performance language in order to scale, and while Clojure is very elegant (and has a *fantastic* community) it tends to force a tradeoff between elegance/beauty and performance. You can get one or the other, but it's rare that you get both, just because the underlying platform (JVM) is so janktastic. Also, I think that Haskell's a much better long-term bet than Java (even with Scala in the mix) for high-performance programming's future, but that's another debate. Hiring existing Haskell talent can be really tough, so the goal is to train it from within. Honestly, the course started as an intern course that would be 5 or 6 sessions, just as a way of starting the conversation. Then it grew beyond the intern program because so many engineers wanted to take it. So it became the *summer* Haskell class. Then we started having outside people in Chicago hear about it and want to sit in. So, at some point, I just decided to put it on Youtube. The video editing is pretty poor (it was done by me, not a professional) but I hope that the information is useful. It's been a lot of work to build this class (I'm about 1/3 done) but it's been fun as hell. The "first half" is 4 weeks (1 lab, 7 lectures) and will cover enough to get a person to read and write code at a beginning professional level. Not a Haskell expert, but enough exposure to take the language on. The "second half" (which begins when I come back from vacation) will be 5-6 weeks, more lab-focused, and cover some of the libraries (e.g. lens, aeson, bench, scotty or snap). I'm still deciding how I want to broach Haskell and the Web: I'll probably use Scotty just because I don't think that I'll be able to learn Yesod or Snap (given all the other things I have going on) well enough to teach them. 