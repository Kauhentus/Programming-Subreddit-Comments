True. The type thing is an issue though, and not just that the rule is huge, since the runtime type is a dynamic property, this means that you have very limited static reasoning when it comes to eta. You need to trace to prove properties (or use something like Hoare logic).
I agree that most monad copies in dynamic languages are wrong. Dynamic languages are already in a huge do-notation, with binding and the world as the hidden state, variable assignment is syntax sugar for state operation. Objects in dynamic language are already maybe / either / multiple either monads, because unlike static languages, they carry type information with themselves. But I don't agree with your opinion to `l.map(id)`. If something matches 3 monad laws, it is definitely a monad. Language features have nothing to do with it. In haskell you still can implement wrong monad, because the `Monad` type class doesn't guarantee the laws. It's the implementer's job to make sure his warm fuzzy thing obeys monad laws (haskell compiler can do code transformations by monad laws, so if some fake monad violates the law, behavior of the program will be unpredictable).
Well, I don't know if there's something I'm missing, but I sure believe I can write basic applicatives already. Unless you mean something like write them in a special way that decomposes them into even more basic ones, like ~~ekmett~~ gbaz1 is doing in the article. (I once went through the exercise of figuring out Functor instances for the Free monad to yield a few of the canonical monads like `Maybe` and `[]`, which I feel is similar to some of what ~~ekmett~~ gbaz1 is demonstrating with, e.g., `Product (Const w) Identity` for Applicatives.)
I think you should use orc, see: http://code.galois.com/paper/2010/Hask-Orc-DRAFT.pdf
A terminal doesn't have to be a glorified typewriter limited to 80 characters. If we can believe in that illusion why not another?
I'm wondering what an HW simulator is? edit: ah hardware
I believe it is a program to simulate microchips.
&gt; I just emphasize this because of my "oh, now I get it!" moments in learning Haskell and the surrounding theory was the realization that the function of one type to another type is itself type. Right — that's what we have in Haskell. In dynamic (unityped) languages there is really one type. That's what I mean by &gt; We could as well say that &gt;&gt;= and return both have type U — remember, there’s only one type! In a unityped language, U -&gt; U, U x U, U + U are all the same as U.
Gee, thank God I didn't prefix that with "the inimitable" or something corny like that... (fixed)
[This one](http://www.valuedlessons.com/2008/01/monads-in-python-with-nice-syntax.html) was just on HN again. I think it shows both a correct implementation and one that tries to be clever with syntax but is wrong. Also I think it would be better to phrase it as none of those clever syntax tricks permit you to capture a closure in the context of the caller properly. That Python trick with iterators is nifty when it works, but generators can not be reused. (Quite fundamentally so in Python. Even tricks to try to make it appear generators can be reused all work by wrapping a generator with something that records what it generates and allows static replays of the stream. This is not adequate for implementations of the monad interface.) It's no use capturing different stack frame, you need the context of the frame _using_ the monadic value. It's possible Ruby blocks might be adequate if greased with the right support code, but I don't quite have enough Ruby knowledge to validate a given implementation quickly. I just have to go off the fact that others who have looked at the implementations I've seen say they are not correct.
Mostly the fact that any line that ends up with spaces accidently will break the file without a warning and it is hard to find the line where a tab was intended but is missing. This is especially annoying since many Makefiles are not written by hand but generated and if the generator produces some spaces in the wrong place you don't even know where to start.
It is not a big deal. The tool looks useful but is after all only a convenience shortcut. I just thought I'd mention it since your instructions made it sound like you had tested it with more than one haddock version and ran into compatibility problems so you might be interested in hearing how it works with other versions.
One solution which does work is using the FFI and having the global live in C, but this is pretty ugly.
 I certainly don't have it all figured out, and that is a good question. My guiding principle is to remove as much "unnecessary" syntactic variation as possible, where "unnecessary" is quite debatable. Ideally we could pick a reasonable standard and enforce it syntactically. (By reasonable I mean camel case or snake case, but not both. Tabs or spaces, but not both. Etc.) Regarding manual alignment, I think you could do without it. Expressions would either be on the same line, or on the next line indented according to the semantic nesting level. # same line foo = do this [ "x", "y" ] # next line foo = do this [ "w", "x", "y", "z", ] # blending the two (not sure if this should be supported) foo = do this [ "w", "x", "y", "z" ]
Well, single line obviously isn't feasible for all situations. There will always be something too long for a single line. While we are on the topic I prefer to use Utrecht style commas in lists and similar situations (leading commas). This has advantages for version control (adding or removing the last line won't touch an addditional line just for the comma). Using that style it the first line of course still doesn't have a comma so it looks natural to put the opening list [ in that place like this. foo = do this [ x &amp;&amp; y , !x &amp;&amp; y , x &amp;&amp; !y &amp;&amp; z ] I don't see something using just indentation with no alignment at all as anywhere near that readable, especially if we are talking some sort of literal where the alignment of the lines is supposed to point out similarities and differences in the lines as with the negation in my example.
Have you thought about it, or are you just going by inherent obviousness? I'm notoriously untalented at estimating the scope of projects and the involved time/effort, so if you could flesh out for me what things you think the huge amount of work would be going towards, it would be swell.
What people like Harper call a 'semantics' is hardly necessary for proving things; else the concept of proof would be ungrounded. If by 'having a semantics' you mean that the words have content, or actually mean something, then it is plain that the terms defined in a Haskell module have content. (The lambda calculus was from the beginning intended to be a systematic representation of *definition*.) See *mutatis mutandis* the frequent animadversions of Martin Lof against supplying a 'semantics' for his system, which would in his view entail that it is a complete failure. A 'formal semantics', as he sees and as is in fact obvious, is only a rational demand for symbolisms that are intuitively meaningless; otherwise ungroundedness would again immediately threaten. 
&gt; Something like that. I haven't worked it through fully, but it seems you could separate the "dependencies" bit from the parser bit, and define the parser directly as what it "does" rather than a tree to be interpreted. I'm not sure I understand correctly what you suggest, but I think I prefer the current solution, which defines a "description" for a parser and an "execution" for it, clearly separated. This allowed me, for example, to define an alternative execution (for bash completion) without touching the `Parser` type at all. &gt; Also, not that it probably has user-facing consequences, but your GADT as written breaks the applicative laws, as "pure id &lt;*&gt; v" is not the same as "v" (the former adds the observable "MultP" constructor). I'm aware of this, but it's not easy to fix. It is possible to build a free Applicative which obeys the laws (modulo parametricity), but my `Parser` type is also a free Alternative and a free Monad at the same time, and I'm not sure how to separate it into three separate constructions. In practice, I don't think it matters, as the Applicative laws do hold "observationally", anyway.
What I mean is that if I write a library as a parameterized module, the client can choose what *my* imports are, as long as they satisfy the signatures I expect. This is something qualified imports have nothing to do with.
Ah ok. I see. Yes that is quite different and what I was attempting to ask about at the end. Can you point to any projects where this is used? Obviously it will be in OCaml and not Haskell...
The lambda calculus actually has all three flavours of semantics, and a large body of theory studying them (see e.g. Barendregt's 1984 monograph). By definition, a semantics is something that gives meaning to terms; so, yes, without one, terms mean nothing.
Can you point me to some code where people use this Reader monad approach to handle "global variables"? I have heard people advocating more then once but I don't know how it works out in practice. In particular, I wonder how people deal with writing monadic instead of "regular" code...
Why not? You could create a new unique handle and then just pass it around, using it to generate Uniques whenever you wanted. initUnique :: IO UniqueHandle newUnique :: UniqueHandle -&gt; IO Unique Of course, you could deliberately break it if you wanted to by creating multiple unique handles and calls to newUnique on one might result in a value returned by the other. It's not like Haskell won't let you shoot yourself in the foot if you aim the gun at your foot, disengage the safety and pull the trigger.
If you decide to global variables anyway rather than params or monads then there are libraries around such as [global-variables](http://hackage.haskell.org/package/global-variables)
That approach is interesting, but it still uses `unsafePerformIO` hacks, and just hides them away in a maintainable place (which has advantages). It also means the compiler cannot tell if any given symbol will exist, because it does runtime name resolution.
lazy IO doesn't use `unsafePerformIO`! It uses `unsafeInterleaveIO`! Big difference. Huge! And I hardly think that lazy IO set back adoption of Haskell and garnered it a reputation as a toy language. Many other things contributed to that reputation (including the fact that, for a long time, we didn't have performant libraries like ByteString at all -- forget about how they did IO!). I'm still a partisan of lazy IO in small doses, and I know I'm not alone here.
As a matter of fact, I can: [Link](http://stackoverflow.com/questions/14029945/what-are-design-patterns-for-tasks-with-storing-some-state-in-haskell/14034244#14034244).
There's also [safe-globals](http://hackage.haskell.org/package/safe-globals).
What tricks does ST employ? I have actually never used it.
Basically, you tag everything with a phantom type, and then use rank-2 polymorphism to ensure separation. data STRef s a = ... newSTRef :: a -&gt; ST s (STRef s a) readSTRef :: STRef s a -&gt; ST s a writeSTRef :: STRef s a -&gt; a -&gt; ST s () data ST s a = ... runST :: (forall s. ST s a) -&gt; a Because the argument to `runST` must be polymorphic in `s`, this ensures that the argument can only access things it has created in its own local "thread" or "heap". And moreover, because all that access is local, we know that the result must be pure (because the `s` can't escape, and no `s'` can sneak in). This exact thing may not be powerful enough for what you want to do. A more powerful version would be to use region types, but that gets a lot messier. Take a look at BitC, DDC, or some of the things listed on [DDC's bibliography](http://disciple.ouroborus.net/wiki/Related).
Interesting, I'll have to look into it! Thank you for the reply.
&gt;Tabs for indentation still requires spaces for alignment Tab only means you indent code, and you do not align it at arbitrary columns. This means getting tabs and spaces right is a non-issue, and you don't have to deal with all the noisy diffs alignment causes.
I use this trick to implement concurrent queues. I do not know any other way to do it. {-# NOINLINE printJob #-} printJob ∷ String → IO ∅ printJob = unsafePerformIO $ do p ← newEmptyMVar return (λcnStr → bracket_ (putMVar p True) (takeMVar p) (do ... here you do any work that requires access to a shared resource like printer for example. ) 
I don't see those as being issues - essentially what some people are asking for is for a list of functions to be evaluated before main. After all, all of the complaints you make are similar to one you could make about "main". 
Which will of course change much more than necessary causing far more change conflicts and loss of work time. 
This submission has been randomly featured in /r/serendipity, a bot-driven subreddit discovery engine. More here: http://www.reddit.com/r/Serendipity/comments/15kc5q/free_monads_for_structuring_haskell_web_apps/
The producer is also a Monad. You can use (&gt;&gt;) to "vertically" compose the producer with (return []). This changes the return type from () to [a]. Then you can "horizontally" compose with your Consumer that builds the list.
&gt; A more principled, compositional approach to UI construction would help immensely, but now you've moved from "large-scale open source project" to "research". This is what propelled my initial wondering. If you have compositional graphics (diagrams) and compositional logic (FRP), you might be quite some distance down the path to a solution already, and what else is still missing? Maybe the only way to answer that question would be by experiment.
Is there a reason to prefer the arrow-heavy hxt over the simple/light XML parsing packages? hxt seems over-engineered, does someone have a deeper insight about it?
How about: data ProgramEnv = ProgramEnv { envPrintJob :: String -&gt; IO a , otherSharedResources :: ... } mkProgramEnv :: IO ProgramEnv mkProgramEnv = do .. no unsafePerformIO needed here .. -- My program's monad newtype M = M (ReaderT ProgramEnv IO a) deriving (Functor, Applicative, Monad, MonadIO) Then put your program's actions in M, rather than in IO.
Should work now. Retry with latest version?
Author here. I agree, I would've preferred using something simpler too. Probably something that'd look more like [json](http://hackage.haskell.org/cgi-bin/hackage-scripts/package/json) or [aeson](http://hackage.haskell.org/package/aeson) I was first going to try HaXml but my quick googling didn't turn up much introductory material on it. There was much more material on HXT on the Haskell wiki and elsewhere, so I chose that instead. I have to admit it took quite a bit of mind bending first time I used HXT, that's why I figured someone might find my sample code useful. Apart from HXT and HaXml, what are the recommended simpler, well-documented XML libraries? [Xml](http://hackage.haskell.org/package/xml), perhaps?
So what you are saying is that for the oh so great benefit of being able to resize your tabs we should give up any sort of alignment and with it all the readability gains you get from that? And all just because the only semi-sane tab use, the one with spaces for alignment, is too complex. All you are really proving here is that tabs have no leg to stand on if those are your best arguments for it.
I alway think of it as "per module per process", the problem is sharing it across all imports of the same module.
The last bit seems to use the function instances of Functor and Applicative. This strikes me as useful for the usecase where you want to convert a function on some fields of a record into a function of the record, e.g. data DiffHunk = DiffHunk { hunkStart :: Int , hunkLength :: Int } deriving (Show) hunkEnd :: DiffHunk -&gt; Int hunkEnd = (+) &lt;$&gt; hunkStart &lt;*&gt; hunkLength
The first and best solution is to consume the list as it is being generated. Then you don't need to return it at all. If you are providing this to somebody else, just provide them your producer and let them compose their `Consumer` of choice downstream of your `Producer`. Think of the producer as being your new lazy list. The next best solution is to use the following optimized `toList` function: toList' :: (Monad m, Proxy p) =&gt; a' -&gt; p a' a a' a (StateT [a] m) r toList' = useD $ \b -&gt; StateT $ \bs -&gt; return ((), b:bs) I can't find anything that gives better constant factors than that, and it has linear time complexity. On my dinky laptop it comes out to about 3 microseconds per list element. Also, like `lpsmith` said, using `WriterT` is pretty fast. Even on 1 million elements it's only twice as slow as the above tuned version. I made a mistake in the current `toListD` implementation, which not only reverses the element order but doesn't have linear time complexity because it mappends in the wrong order, so I will fix it in the next update. I never caught this because I don't actually use `toListD` in my own code since I just consume the elements as they are being generated to avoid bringing anything into memory I also want to explain the whole "every pipe must return the same value" thing, because I think it's important to understand why that's a useful behavior. The first advantage of everything returning the same value is that when you write a component you don't have to rewrite it depending on what position it is in the pipeline. If the return values change from stage to stage it makes the component ecosystem significantly more brittle. The second advantage is that it makes things like proxy transformers possible. Without the uniform return value you wouldn't be able to implement things like native error handling, local state, and local parsing. The fact that these only worked when the return values are uniform was one of my big clues that the uniform return value was the correct approach. The third advantage is that it simplifies the proxy model considerably. Proxies only do one thing and do it well: they interleave Kleisli arrows. This is why proxies outsource every other feature to proxy transformers or the base monad, including folding values. Everything has to have the same return value, otherwise interleaving doesn't do the right thing when a proxy terminates. If you want to know what a good rule of thumb is, have everything return `()` if they terminate and use the base monad to store any extraneous values (i.e. `WriterT` or `StateT`). By doing so, you can then easily `hoist` other pipes that don't return any extra information to work with those that do. I also saw that `lpsmith` advocated adding `forever` at the end to make the return type polymorphic. That is okay, but I don't strongly advise it because you can't really predict ahead of time whether control will need to flow upstream or downstream. You should always write proxies to be as context-free as possible and assume they will be used in the most punishing circumstances. That's why I strongly recommend the base monad approach, because it works even if complete strangers take no special pains to accommodate any sort of protocol that you have established. You can't ALWAYS avoid protocol and there are lots of legitimate uses that I have found for the "end with a `Nothing`" trick and other sorts of protocols. Proxies obviously can't enforce context-sensitive communications because they don't use indexed monads and in those cases you do simply have to establish a protocol that interoperating proxies must follow. Also, I was planning on adding to `pipes` something very similar to what `lpsmith` proposed, that looks like this: guardK :: (Monad m, Proxy p) =&gt; (() -&gt; Producer p a m ()) -&gt; (() -&gt; Producer p (Maybe a) m ()) guardK p () = runIdentityP $ do (mapD Just &lt;-&lt; IdentityP . p) () respond Nothing The only difference from his solution is that it doesn't end in `forever $ respond Nothing`. I find that it is very useful to use this multiple times to repeatedly punctuate a stream. However, I will probably still provide the `forever` version, too, to give behavior familiar to `conduit` users to help them transition away from `conduit`.
I gave the Set category just as a simple example (and so did Moggi in his [Notions of computation and monads](http://www.disi.unige.it/person/MoggiE/ftp/ic91.pdf), p.3). There are certainly languages for which simple Set-theoretic semantics exist — like the simply typed lambda calculus. For more complex examples, yes, you'd probably need a CPO cateogry.
Holy Water? 
As far as the needed database generation for (2) is concerned I'd refer you to the ["Running a fully local Hoogle"](http://newartisans.com/2012/09/running-a-fully-local-hoogle/) blog post. The benefit of this method is that you can gain access to to the documentation for *all* Hackage packages (i.e. not only those build and installed locally). The downside is that the `hoogle combine` operation takes *very* long. Also related are the following rather old Cabal issues: * https://github.com/haskell/cabal/issues/395 * https://github.com/haskell/cabal/issues/510 
No, it will change precisely what you tell it to, and nothing else. Read the man page.
I do not understand your code. Are you passing every time newly created MVar to printJob function? If so, then it is broken. The very purpose of unsafePerformIO is to make sure we have a Singleton. Otherwise concurrent users will send batches of files to printer at the same time and printer will mix them instead of printing entire batch for every user.
Yeah, i had it like that first time around. Then it turned out that my main module have to know about and explicitly initiate every technical thingy irrelevant to it. Pull every module irrelevant to it. All this for correct initialization of one function used on one of probably hundreds other modules only. I do not have ideological OCD towards unsafePerformIO. If it makes my code clean, contained and simple to use, without introducing a lot of boilerplate and contaminate places that do not even need to know about those little technical details (leaking abstractions), then i'm gonna use it. 
No, you are confusing two things. I am saying: A) Use a single indent character to indicate a single level of indentation. B) In response to people complaining about using indents for indenting, but spaces for alignment, I suggest re-thinking doing alignment. Alignment is entirely a negative. You generate noisy diffs for absolutely no gain.
But the "simple method" doesn't do what you want at all, I'm afraid. It'll simply return [], if anything. I mention it only to point out that yes, you can change what a proxy returns. It's not obvious that you can return a lazy list of all the elements sent with the proxy construction itself, and my guess is that it is in fact impossible. The only obvious solution to me is to use either WriterT/tell or ContT/mapCont or maybe even IO, any of which should be able to work if done correctly.
Yes, we were considering formal languages in which '=' is the sign for definition -- which by the way evidently cannot be defined. The only other things presupposed by my use of that 'sequence of characters' is reading definitions from left to right and of course the use of variable expressions; from those constraints - which aren't a matter of supplying a semantics or a model, but are everywhere presupposed when we do give such a thing - much more follows than you seem willing to recognize. What I write for the children is perfectly 'formal', it is a considerable achievement to employ letters after the fashion of `x` in such a definition, and to grasp the idea of explicit definition and its expression with `=` or the like. Note that this thread isn't about what 'domain' you 'prefer' to 'stay' in, but with your groundless attack on the idea that Tekmo has ever done anything we can call 'equational reasoning'. Your present recourse to 'sequences of characters with no intrinsic meaning' -- a highly theoretical concept with a *lot* of theoretical baggage in it, by the way, all of it narrowly philosophical in character and completely alien to me though evidently not to you -- this amounts to saying that Tekmo's claim is in doubt since obviously `"map Just \"abc\""` and `"[Just 'a', Just 'b', Just 'c']"` are really just different "sequences of characters" -- so recourse to (what for you only seems to be) the definition of `map` in order to show anything about their relation depends on some 'semantical' theory. One might as well say one needs a special semantical theory to prove that "abc" and "abc" are the same, since they are 'obviously' different 'sequences of characters' -- here being separated by the word 'and'. There is, by the way, no reason at all why a formal language should be expressed or expressible in sequences of characters (what are 'characters,' by the way? and a sequence?), this is a concession to 19th c. typesetting -- as is again a complete commonplace and the abc's of the present topic -- the original formal languages were not expressed as sequences but in other sorts of structure, e.g. certain sorts of trees, and could not be printed without an expert mathematical typesetter, see as usual [Frege 1879] and e.g. http://www.ctan.org/tex-archive/macros/latex/contrib/begriff ; similarly Gentzen presupposed that a proof, considered purely syntactically, is a certain sort of graph.
&gt;You can automatically produce working code but can you automatically decide what is indentation and what is alignment? That doesn't even make sense. You choose what you want to be indented and what you want to be aligned. Just like you choose where you want braces, whether or not you want spaces around operators, etc, etc. You choose the formatting style you want, and the tool formats the code to that style. You can't do this with haskell because the syntax doesn't allow it, the same code could have different meanings when formatted differently. Most languages are designed to allow this though. I have no idea why you are talking about HTML, which has absolutely nothing to do with anything.
The pains of a compiler which is not referentially transparent.
No. The idea is that the user calls `initPrinter` once at the beginning of their `main` and then passes that `MVar` to every call of `printJob`. It does not enforce that you don't call `initPrinter` multiple times, but I think the risk of them calling it more than once is less than the risk of `unsafePerformIO` causing problems.
I spoke to someone on IRC the other day that said he was using Clojure, interestingly enough.
Having a main that knows other modules is not really a problem
&gt;which is a problem if the library getting loaded happens to cause a runtime error or a segmentation fault. You find chances of this more likely than mistakenly initiating an MVar again? I do not remember such a case even though i programmed for more than 20 years. &gt;Then you have pure computations causing segmentation faults, which would be extraordinarily hard to debug, Indeed, but i would not use unsafePerformIO for pure computations. Here you can see that it is used for IO actions. I think programmers who use haskell are generally good engineers and have a good sense of what to use when. 
You are forgetting the *very* important problem of "look-and-feel". Outside of research interestests, this framework is certainly not worth exploring if you can't make it look native everywhere. How many people do you know that reach for Swing? I've only seen it successfully used when the userbase has no choice whatsoever, such as in-house tool development. I don't see Haskell breaking into the internally developed enterprise application market in the immediate future. It's only just starting to move away from Java-for-everything.
Alright. I'll reluctantly agree that at least your proposed usage of for MVars is okay. I still think we shouldn't declare the use of globals safe in general, though.
Oh, that's interesting! Well, so there is an easy solution after all. Just use the LAZY `WriterT [a]` in your base monad. I've pasted the code for you here: import Control.Proxy import qualified Control.Monad.Trans.Writer.Lazy as W foldLazyD :: (Proxy p, Monad m, Monoid w) =&gt; (a -&gt; w) -&gt; a' -&gt; p a' a a' a (W.WriterT w m) r foldLazyD f = runIdentityK go where go a' = do a &lt;- request a' lift $ W.tell (f a) a'2 &lt;- respond a go a'2 toListLazyD = foldLazyD (\a -&gt; [a]) main = print $ W.execWriter $ runProxy $ fromListS [0..] &gt;-&gt; toListLazyD This will lazily print out all the values. Enjoy!
That's perfect - it even runs in linear time. Thanks!
It's funny that i eventually settled on this specific decision precisely because of distrust towards global vars :) You can see that i'm closing over MVar, so it is not available as a global variable at all. printJob is the only function that has accesses it. Otherwise i cannot guarantee that other programmers would not try to put something in it and thus break the queue or just initiate a new one (even worse). 
1) the proof is quite simple (iff f always returns finite lists): Lemma: f [] = [] --- f [] = l f ([] ++ []) = l f [] ++ f [] = l f [] ++ l = l That is only the case for infinite lists and the empty list (this is slightly handwaved because phone). Theorem: f = concatMap f' where f' x = f [x] --- f (xs ++ ys) = f xs ++ f ys f ([x] ++ xs) = f [x] ++ f xs f (x : xs) = f' x ++ f xs A bit of handwaving involving previously proved lemma QED 2) I don't see how this is impractical.
Thanks; this article is helpful, as is the link provided in the first paragraph to [this blog entry on the `operational` package](http://apfelmus.nfshost.com/articles/operational-monad.html). One thing I've abortively tried to find out, without a lot of success, is what exactly is the Applicative-only version of all of this free monad/operational monad idea. We had ["Static Analysis with Applicatives"](http://gergo.erdi.hu/blog/2012-12-01-static_analysis_with_applicatives/) touch on free applicatives earlier this month ([discussion thread](http://gergo.erdi.hu/blog/2012-12-01-static_analysis_with_applicatives/)), but I just couldn't follow it. The recent ["Abstracting with Applicatives"](http://comonad.com/reader/2012/abstracting-with-applicatives/) ([discussion thread](http://www.reddit.com/r/haskell/comments/15hu5n/abstracting_with_applicatives/)) revisits that briefly, and I haven't worked through it, so maybe that'll help.
Again, I chose the Data.Unique example on purpose. Without the global, it would have to leak internals in that all users must use the same initialized state, or else non-unique values could be generated. So users must know this and the compiler cannot enforce it.
Well, I'm sure somebody can help you figure that out if you put the code up somewhere. Also, you might find a [blog post of mine](http://blog.melding-monads.com/2010/03/14/writer-monads-via-continuations/) helpful in understanding the issue.
I don't think that eyu100's property implies that f distributes over ++. 
The real answer is that the return value does get use, but I abstracted all common uses of the return value into proxy transformers. That is why you rarely see non-unit return values.
I enjoy the articles on the blog but the extremely narrow post width is *really* difficult to read code.
Right. Given: forall x1. exists y1. f (x0++x1) == f x0 ++ y1 if we skolemize, we get: exists y1. forall x1. f (x0++x1) == f x0 ++ {y1 x1} and if we assume there's a Haskell-definable function interpreting the skolem: exists g. forall x1. f (x0++x1) == f x0 ++ g x1 Distributivity of `f` over `(++)` requires the additional constraint that `g==f`, rather than merely asserting existence. But, it's not entirely clear what the quantifier on `x0` is; is it for some fixed `x0` or is it universally quantified? If it's universally quantified such that `y1` is invariant in `x0`: forall x1. exists y1. forall x0. f (x0++x1) == f x0 ++ y1 then we have that: f x0 == f ([]++x0) == f [] ++ g x0 So, if we have `f [] == []` as well, then it follows that `g==f`. Whereas, if we don't have `f [] == []`, then we have: f x0 == f [] ++ cycle (g []) ++ g x0 On the other hand, if `x0` is universally quantified such that `y1` can depend on it, then skolemization yields: forall x0, x1. f (x0++x1) == f x0 ++ g x0 x1 From which we can deduce: f x0 == f ([]++x0) == f [] ++ g [] x0 f x0 == f (x0++[]) == f x0 ++ g x0 [] which also holds for any other splitting of `x0`. But now I've lost interest in exactly what weakening of distributivity this results in. In any case, it's not clear whether eyu100 actually requires this weaker property, or whether it would be sufficient to impose proper distributivity.
Laziness (or lack thereof) never changes associativity of any operator. =)
What about testing? Perhaps there you don't even want to use the same global state for each test? Perhaps you want to reinitialize it on every test so information from one test does not leak to the next?
I suspect that looking native is overrated, as long as you can make it look *good*. But you're right in another sense: *behaving* native is more important, and there's the whole category of problems like integrating with the clipboard, with drag&amp;drop, with input methods, with accessibility, with so on and so forth, which are not so easily shrugged aside and which, indeed, would be a lot of work. So that's what I was forgetting, finally.
Turns out my first experience with the 'xml' package was actually worse than with 'hxt'. I tried using it to parse a .tcx file. I must be using it somehow wrong, as I can't get even the simplest things to work with it. I think this is due to the way namespaces are handled in Text.XML.Light. I have code like so: child :: String -&gt; Element -&gt; Maybe Element child n = findElement (unqual n) children :: String -&gt; Element -&gt; [Element] children n = findElements (unqual n) main :: IO () main = do doc &lt;- parseXMLDoc &lt;$&gt; readFile "test.tcx" print $ (doc &gt;&gt;= child "TrainingCenterDatabase") print $ (doc &gt;&gt;= child "Activity") print $ (doc &gt;&gt;= child "Activities") All of the above print "Nothing" with my .tcx input file ([the input file](http://hpaste.org/79948)). I would've expected the last line to find the "Activities" element, but no, it can't find it, even though clearly it's present in my input. Turns out this is because Text.XML.Light annotates XML nodes with namespace information: [Element {elName = QName {qName = "Activities", qURI = Just "http://www.garmin.com/xmlschemas/TrainingCenterDatabase/v2", qPrefix = Nothing}, elAttribs = [], elContent ... and when searching for an element with name, it uses the fully qualified name for name comparison (as opposed to just matching qName). Since the name I'm looking for is `unqual "Activities"` it can't find it as qURI in my search is qURI. In order to fix this, I'd need to somehow lookup the namespace information as I traverse the document.. but that's not something I'd call simple anymore.
Ah, I see, I misunderstood something in the original statement.
thank you for your work
Can you give an actual example? And yeah, about the other points: I do believe that Haskell is hard to fit in a place which would outright reject it, not Haskell's fault of course. If the code quality need not be high, it doesn't really matter what you use, so Haskell's upsides are lessened whereas the difficulty of learning Haskell is still there, so for non-Haskellers it won't pay off. And Haskell does tend to encourage less "readable" code (well, squeeze a lot more meaning into less code, which makes it more effort to read). Also it tends to use lots of idioms that people don't know, also harming some notion of "readability". I think the last point is not as important as someone might think -- given that the types can be super-readable, and mean that the code is split into tiny (potentially unreadable) units with a very readable spec (type) that you can just rewrite at whim, and type-check to verify that you got it right.
I used it for XMLs without any fancy namespacing, so didn't have to worry about that. If you know the qualification before-hand, you could just search for that? How about [filterElementName](http://hackage.haskell.org/packages/archive/xml/1.3.3/doc/html/Text-XML-Light-Proc.html#v:filterElementName)?
I didn't want to deal with fancy namespaces either, my input format just happened to use them.. And unlike with 'hxt', I couldn't just ignore them with Text.XML.Light. But yes, I can implement my own combinators with `filterElementName` by supplying it with a predicate that ignores the qURI and qPrefix components, something like this: child :: String -&gt; Element -&gt; Maybe Element child n = listToMaybe . filterElementName ((n ==) . qName) (This might be a better default for functions that search elements by name.) The building blocks for parsing XML definitely seem to be in the 'xml' package too, but I guess I like what I got out of the box with 'hxt'. For simpler uses, an applicative API like in 'aeson' or 'json' would be a better default than the low-level primitives you get with 'xml'. Arrows like in 'hxt' OTOH might be taking it a bit too far.
How long did it take for you to start from "hmmm I'm missing Ix" and arrive at rangeM?
I dislike the Arrow class in general, because all arrows must support the "arr" operation, that means your arrow type must encode an (a-&gt;) within it. And if it does, might as well take it outside of the arrow, and just compose the result part with Applicative.
I don't think that by "small doses" sclv meant "small files". He probably meant: in cases where the IO logic is not overly complex.
Care to clarify some of the misunderstandings? I agree 100% with him about the IO monad and laziness. I think it really hurts the practicality of Haskell when you have to jump through mental gymnastics to do simple things like IO. I understand the theoretical beauty of such things, but in practice, not so much. Regarding laziness, it is such an unnatural way to reason about code and computing. I've never felt comfortable with it. Significant whitespace is meh to me, I can live with it or without it in a language. I'm not knowledgeable enough about the FFI, but it sure seems like he is.
1. Use braces when generating. No need to use whitespace. 2. The IO monad is there so that stuff isn't lazy. That's its reason for existence. 3. Use c2hs or such. We have bloody gtk+ bindings, don't tell us that it's not possible. 4. Some people agree that lazy-by-default isn't a good idea for a systems language, but Haskell is rather in the rapid-prototyping, DSL and incidentally rather fast camp. 5. No, not everybody else has them. Virtually noone has them. And last, but not least, #haskell on freenode. 
Good point about the white space. I think regarding the FFI, OP was saying that the documentation is problematic. I'm sure the FFI can do everything you say. You are a Haskell expert after all. :D It just seems that OP had a very hard time figuring out how to do anything with the FFI given the available documentation. That is a problem.
Basically, the post is just a lot of not-very-specific stuff and hand waviness. It very clearly characterizes how he feels (which is fine, it's his blog,) but if you're reading it for *content* there's basically nothing there. &gt; Significant whitespace I'm not exactly sure what his complaint here is; Haskell is for the most part a language that has conventional braces that are optional with well-defined whitespace, unlike python. He mentions code-generation, which is fine. But I'll ask this: Is there anything actually dependent on whitespace in the language? I'm not entirely sure. `where` can be translated to `let` which can be done with braces and semicolons, `do` notation can be output using braces and semicolons... am I actually missing anything else? You can machine-generate some ugly-as-hell code with no whitespace ambiguity, I'm fairly sure. &gt; The IO monad This whole section is nothing but a lot of nonsense and rambling and I'm not sure what the point is. He says most haskell examples use the IO monad, that the IO monad basically gives you imperative structure, and that's *like really hard to understand*. Oh, and that also it's "slow and has overhead", whatever that means (GHC already eliminates things like `State#` entirely at the Cmm level, it's a regular low-level IR without an IO monad in sight, so he needs to back up with an actual example here.) As for mental gymnastics, I would like you to list one "simple thing" to do with IO that can't be done in like 3 lines in Haskell, and similarly in another language with the support. There's really not a lot to say here, honestly, short of giving an example and showing me where the gymnastics are. Haskell stacks up pretty damn well as an imperative language and GHC runs circles around plenty of others in its features, this isn't entirely new either. &gt; Unexpressive FFI. I'm not sure what his complaint with passing or returning structure is, or what "delving into the depths of compilers" means. If he's talking about bugs or outright wrongness, he should be specific. Most of the stuff you would expect like calling conventions, safe vs unsafe, and FFI semantics (including wrappers and the like) are pretty well specified in the standard. I'll grant there's not a lot of great examples of using the FFI beyond some basic stuff however; even Storable could use more docs, as well as wrappers, `StableName`s and exports, and GHC actually has a lot of machinery which means the FFI has more possible implications (like what `safe` vs `unsafe` does for a particular GHC RTS capability, and it has its own extensions too, of course, for interruptibility of a capability.) And wrapping FFIs into 'usable', Haskell-like APIs has its own set of challenges, but you'd face that in any other language, too. &gt; Laziness This is basically a "I feel this way" point that is unarguable in the slightest so I'm not even going to go much further than this. As a useless counterpoint though, I'll say I've written OCaml recently, and the lack of laziness is easily the number one thing that completely makes me hate writing it. And it really has nothing to do with how computers operate underneath, and everything to do with how I write programs and how they compose. &gt; Lack of optional/labelled args This is a specific feature we lack that some people like. Frankly optional arguments are ugly as hell in OCaml IIRC (arguments have odd positional semantics from what I remember, and there are weird kludges when you want to have things like functions with only optional parameters, etc) although labeled arguments are kind of nice. People have all kinds of opinions on using 'default structures' with record accessor syntax for this stuff, though. &gt; Also I get the impression from reading online that Haskell is widely studied and often pimped, but not used very much in reality. More nonsense and FUD, not even worth addressing.
* **Significant whitespace:** in Haskell, the significant whitespace is syntactical sugar that's on by default, because it's pretty convenient to read and write code in it as a human being. If you want to generate code (*which is something the author is having problems with*), use the more canonical notation (wrap blocks in `{}`, seperate lines with `;`) instead. (You could pass the code through tools to beautify it if you want, but why bother.) * **The IO monad:** the issue here seems twofold. One is that the author doesn't seem to understand monads. The other is that lazy I/O is hard to get right. The latter is still a topic of experimentation (don't know the state of the art, though), but the "standard" way is generally not nice to work with. The former, though, is just a fallacy of xenophobia. * **Unexpressive FFI:** I haven't played around with the FFI either. But in general: documentation for feature X in Haskell is either very broad or very lacking. * **Laziness:** Non-strict evaluation is actually a pretty natural way to reason about computing, but not when you only think of Turing machines when you hear that word. I do think there is a lot of room for making it easier to grok strictness, but that would be a task for a different language. * **Lack of optional/labelled args.** "Everyone else has them" is [not a good argument](http://en.wikipedia.org/wiki/Argumentum_ad_populum). In a language that is already very different from what you know, do you still need them?
&gt; Care to clarify some of the misunderstandings? I also felt that most of the post missed the mark, so I'll give my $.05 although I'm not dons :) First, the only point that I felt hit the nail on the head is the one about Haskell's FFI and its documentation being a mess. However, his suggested solution: &gt; It would have been much better to define a C API and write FFIs in C. does not cut it. Calling C from Haskell is basically as fast as an inter-module C from C call, but calling Haskell from C is *dog slow*, for some value of "dog" with four amputated legs and super-glue-grade affinity with ground. It's not an option, unless there's some magic way of making it fast. Also, I agree that optional args would be nice, but that's just a nice-to-have. The point about laziness not being a good default is something I think the jury is still out on, so it's a reasonable opinion either way, but his given reason "It’s not how any real computers work" is simply nonsense. You could say the same about e.g. a garbage collector, OO-style "objects", files, threads (if you don't have a hyperthreading CPU), sockets, etc. It's a criticism without any content. The rest of the points miss the mark entirely, I think. It's not at all reasonable to say that "monads are obscure and hard to understand" is a disadvantage of the IO monad. You don't need to know basically anything about monads to use the IO monad. Any difficulty about monads is not about understanding any *particular* monad, but of the abstraction itself, which you don't really need to understand if you want to deal with only one or two common monads. IIRC the IO monad also does not incur any overhead. The point about whitespace is simply nonsense, and I'll not rehash that holy war here. (I like both Python and Haskell, and like significant whitespace; others don't, it's mostly a matter of taste, and people feel as strongly about it as they feel about vi vs. emacs..)
Is this flamebait?
&gt; simple things like IO If the IO is actually simple, then you don't really need to understand monads to do it.
Even `where` can use braces and semicolons. Everything can. The whitespace gets desugared.
...depending on your definition of "stuff".
I'm becoming interested in F-algebras, after noticing that I have stuff like `runFoos :: [Foo] -&gt; Foo` which appears to be an algebra over `([], runFoos)`. If anyone has pointers as to what I should read to learn more about F-algebras - hit me!
I agree that the FFI documentation is generally bad. I don't agree on the other points. &gt; There are some real problems with Haskell which make it less than useful as a real programming language. &gt; &gt; Significant whitespace... No, significant whitespace is not a real problem with a language. I think Python is the obvious counter-example. Plus, you can still use curly brackets and colons to get back insignificant white-spaces. &gt; The IO monad. Most Haskell examples use the IO monad, which serializes everything, making the code the same as more ordinary languages Haskell lets you opt in to impurity. Other languages do not let you opt out. That seems like the right choice. More importantly, Haskell encourages you to factor out impure segments to make your code reason about, which is good programming practice. &gt; Laziness .. should not be the default. It’s not how any real computers work, or have ever worked, or are likely to work in the future. Neither are types. There are articulate arguments against laziness, but this is not one of them. I actually agree that we should be able to selectively opt out of laziness in a clear and well-defined manner. However, without laziness you lose the ability to define your own control structures, leaving you begging the language authors to incorporate X feature like people have to do in other languages. &gt; Lack of optional/labelled args. Optional arguments are not a good idea in a statically typed language. That is what we have `Maybe` for. You *could* make an argument that you have syntactic sugar for optional arguments that desugars to the equivalent `Maybe` version, but I'm not sure I would like it or that it would be popular. People like types and explicitness because they don't silently do the wrong thing. We like the lack of an argument to fail very loudly as a compile time error so that we can refactor more bravely. &gt; Also I get the impression from reading online that Haskell is widely studied and often pimped, but not used very much in reality. This is mainly because Haskell has had to evolve a lot in order to make laziness and enforced purity practical for commercial programming. Contrast this with Java, which is about as old Haskell, which does not look very different from when it was released. I would say that Haskell has only recently taken off because it has only recently solved a lot of key challenges, not the least of which is a library ecosystem and overcoming the network effect.
The `IO` monad is not about strict vs. lazy. It's about sequencing side effects. You don't expect that `return x` forces `x` do you?
Why should it? `x` isn't a monadic action. If it's not going to be used in another one, it's not going to be forced at all. I thought it was apparent from my use of the heavily colloquial term "stuff" that the whole thing is colloquial. Though admittedly that probably makes more sense with the rant's complaint about "IO isn't useful for anything" in mind. My point, in a nutshell, is: If Haskell weren't lazy we wouldn't be using any kind of IO monad.
&gt; The Monad abstraction is simple: just two operations and three laws. I didn't mean to imply that it's not, just that for simple `IO` it's optional
What was the point of that Int argument on the Func datatype? It didn't seem to offer anything useful. In particular, the fact that chr is an arrow from intAlgebra to charAlgebra2 is completely trivial because the two F-algebras *ignore* the x component of the Func datatype, and so chr could be replaced with any arbitrary function --- even one that maps all inputs to \_|\_ --- and you'd get the same result.
* Encode your intent in your types (but don't break inference (too badly)) * Minimize impurities * Write the most general, simple thing (take advantage of parametricity) * Composition trumps all
What's the point of posting this? It's just some blogger who doesn't know what he's talking about.
I'm a bit late, but I noticed something cute: (.:) = (.).(.) fnow = (&gt;&gt;&gt;) . (&gt;&gt;&gt;) . (&lt;&gt;) flater = flip (.:) . flip (.:) . ((&lt;&gt;) .) since `(&gt;&gt;&gt;)` is equivalent to `flip (.)` for functions. `fnow` is nicer, but `flater` has a sort of charm (and it sort of looks like it ends with a period, a sort of pleasant mnemonic). I guess if I wanted to get cheat-y I would just (:.) = flip (.:) flater = (:.) . (:.) . ((&lt;&gt;) .)
I'm actually working my way through Bananas, Lenses, Envelopes and Barbed Wire right now actually - now that I'm getting my head around the syntax it's starting to make sense. In fact, that paper is probably why I'm seeing catamorphisms everywhere :) Jumping in at the deep end might not be a bad idea, as at least then I can learn what I don't know.
Tom Moertel commented on the blog post, saying something I had never brought to the forefront of my mind before: &gt; The beauty of the IO monad is not that it improves IO but that it improves everything else. So, while IO really is a sin bin, it still serves the purpose of simplifying the rest of your life. If you will forgive the cliché, the IO monad means that what happens in Vegas stays in Vegas.
You mean that's not a burrito? ;)
It's awesome how much Haskellers like to blog. The OP makes fun, but it really is a great virtue of this programming community.
It's venting.
Re whitespace complicating code generation: My difficulties generating Haskell code ended when I bit the bullet and learned template Haskell.
&gt; Regarding laziness, it is such an unnatural way to reason about code and computing. I've never felt comfortable with it. Yet, you do it all the time in other languages without even realizing it. For example, consider an `if` statement in Java: if (predicate) { doSomething; } else { doSomethingElse; } You implicitly assume that the `if` won't evaluate either alternative until it first evaluates the predicate. That's laziness. Or how about a for loop: for (int i = 0: i &lt; 100; i++) { doSomethingWith(i); } You implicitly assume that it does not evaluate the loop body before checking the loop predicate. That's laziness. Or how about exception-handling? try { doSomething; } catch (Whatever e) { doSomethingElse; } You implicitly assume that it won't evaluate the exception handler unless an exception is thrown. That's laziness. Laziness arises every time a language defines control structures. So really, you don't have trouble reasoning about laziness; you just have trouble reasoning about unfamiliar control structures.
First monthly meeting will be Jan 10th, 2013 @ 18:30, at Caravan Coop: http://caravan.coop/
A Tutorial on (Co)Algebras and (Co)Induction - Jacobs and Rutten ... [[citeseer](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.37.1418)] [[from author](http://www.cs.ru.nl/B.Jacobs/PAPERS/JR.ps), pdf] Algebraic and Coalgebraic Methods in the Mathematics of Program Construction - Backhouse, Crole, and Gibbons ... [[personal server](http://wellnowwhat.net/Programming/acmmpc-bcg.pdf), pdf] An Introduction to (Co)Algebras and (Co)Induction and their Application to the Semantics of Programming Languages - Glesner ... [[citeseer](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.69.5894&amp;rep=rep1&amp;type=pdf), pdf]
&gt; The Monad abstraction is simple: just two operations and three laws. If you were to memorize these by rote, then sure, it wouldn't take long. That wouldn't mean that you understand monads as used in Haskell, any more than the ability to move a brush around makes you a painter. The complaint is about having to learn all that just to do IO. Of course, you don't need to learn all that just to do IO, so it's not a valid complaint, but there it is.
There, I updated it to use the concrete GLUT example and show how you might use it to simplify callback-heavy frameworks.
In other news, I figured out why your blog rendered incorrectly in Firefox for me. The issue was with the Privacyfix extension, and disabling it fixes your blog (and many other sites as well that had been broken).
Thanks. That helps
This being the first ever meeting, we'll figure this out as we go. My tentative agenda is meet and greet first; then, presentations. So far, 2 20-minutes talks have been scheduled (with a 3rd, still opened, should you feel like presenting something). One's a quick demo of QuickCheck (a lot of people on the mailing list are newcomers to the language), the other an account of how Haskell changed an experienced C++ programmer for the better.
That's the plan!
This was lovely, thanks.
A free monad provides a uniform interface to assembling syntax trees: (&gt;=&gt;). A continuation monad provides a uniform interface to composing callbacks: (&gt;=&gt;). Edit: A really good exercise is to think about what forM_ does for each monad. Use withFile as a canonical example of a Cont Monad and use pipes as a canonical example of a free monad.
It takes some serious digging (well beyond yer basic "the type characterises the interface") to be sure that the constraint has no bearing on the problem. For this presumably simplified example, an obfuscated version of (1 :: Int), it's clear by parametricity and polarity that the Show instance can't possibly have anything to do with the behaviour of bar. But how about goo :: (Read a, Show a) =&gt; (Int, a) goo = (i, a) where a = read "banana"; i = length (show a) bar :: Int bar = fst goo Now the type at which the variable a is instantiated really does make a difference. What's a good basis to distinguish these examples?
`Cont` and `Codensity` are actually fairly different. The `Codensity` monad is a lot weaker because of the quantifier over the return type. For example, you can't define call/cc for `Codensity`. It is precisely this weakness which makes `Codensity` efficient for `Free`.
So, I don't know how relevant it is, but in the case of OchaCaml which natively supports delimited continuations, you have a form of 'answer type modification' which allows you to be polymorphic in a way you might want. For example, we may simply delimit a continuation and do nothing, returning a value: # reset (fun () -&gt; 2 + 3 + 4) ;; - : int = 9 Alternatively, we may capture the continuation (using `shift`) and do something with it: # reset (fun () -&gt; 2 + 3 - shift (fun k -&gt; k 10)) ;; - : int = -5 Here, we delimit the continuation and then capture it. It is bound to `k`, and `k` basically captures the 'frame' of computation from `shift` up to `reset`. Then we invoke the continuation with 10 and replace the 'hole' that `shift` left. When we call `shift`, the computation up to `reset` is actually discarded, and the result of `shift` is returned. So in this case, we invoke the continuation and return the result of the whole thing. So now here's the fun part. We can abandon that slice entirely, aborting and returning our own value: # reset (fun () -&gt; 2 + 3 - shift (fun _ -&gt; 0)) ;; - : int = 0 But now we can do more perplexing things like return something of a different type entirely: # reset (fun () -&gt; 2 + 3 - shift (fun _ -&gt; "hello")) ;; - : string = "hello" Or we can return the slice itself, too (ignore the funny type): # let f = reset (fun () -&gt; 2 + 3 - shift (fun k -&gt; k)) ;; f : int =&gt; int = &lt;fun&gt; # f 1 ;; - : int = 4 And again the other way around: # reset (fun () -&gt; "hello " ^ shift (fun k -&gt; k "world")) ;; - : string = "hello world" # reset (fun () -&gt; "hello " ^ shift (fun _ -&gt; 5)) ;; - : int = 5 The key here is what they call 'answer type modification'. If you have a hole like this: reset (fun () -&gt; [...] ^ " world") ;; then the original 'answer type' of the reset is is `string`. If we don't have `shift` in the hole (or, more precisely, if we do not change the answer type with `shift`,) then anything put in there is *polymorphic in the answer type*, because it doesn't change it at all. We can only put a string-like thing there: # #answer "all";; # reset (fun () -&gt; shift (fun k -&gt; k) ^ "world") ;; - : string / '_a -&gt; string / '_a = &lt;fun&gt; So this continuation takes a string and returns one, and does not modify the answer type of `reset` (the type `A / S -&gt; B / T` boils down to 'a function from A to B that modifies the answer type from S to T.') But `shift` can in fact modify the answer type of the enclosing `reset` if you are to discard the continuation, for example. It's all a bit magical but you can find it all [here](http://pllab.is.ocha.ac.jp/~asai/OchaCaml/) and in [Oleg's tutorial](http://pllab.is.ocha.ac.jp/~asai/cw2011tutorial/main-e.pdf). (Mac users, rejoice: I have a homebrew formula [right here](https://github.com/thoughtpolice/homebrew-personal) in my personal tap.) I'm not entirely sure if delimited continuations are the generalization you're thinking of, but they're mighty neat and you might want to look at them.
Yes, I think we're basically on the same page. I was more alluding to the fact you don't really need `Cont` to implement it as grandparent mentioned, because it's basically just a slightly weaker version of `Cont` already, but it was a bit poorly worded.
You're welcome! I have been exploring delimited continuations a little recently, and I found OchaCaml and I reeeeally like it - very fun to play with. The idea of answer types for delimited continuations to make the continuation polymorphic in its return is very intriguing. I think I might write about it more soon, after I read more of Asais' papers.
Actually, you can get return type modification without indexed monads in Haskell as well. One way is to start by getting GHC to tell you the most general type of Cont's bind: &gt; let bind m f = \k -&gt; m (\a -&gt; f a k) &gt; :t bind bind :: ((t2 -&gt; t1) -&gt; t) -&gt; (t2 -&gt; t3 -&gt; t1) -&gt; t3 -&gt; t Then with a little bit of unification, we can make this type a bit easier to read, and only restrict it a little bit: type Cont a b s = (s -&gt; a) -&gt; b bind :: Cont g k a -&gt; (a -&gt; Cont f g b) -&gt; Cont f k b Similarly, we can "discover" that the general type of return is ret :: s -&gt; Cont a a s I played around with [this file](http://okmij.org/ftp/Haskell/ShiftResetGenuine.hs) a couple years back and found it rather enlightening. You can pull the same trick with the state monad and allow the type of the state to change over time, though I got distracted before I tried it with other monad examples. Edit: Oops, that is the indexed monad you are referring to. Funnily enough, I was playing with Oleg's file at the same time sigfpe wrote his post.
Yeah, I really should play around with this some more. I guess my goal would be coming up with a good example of an application and also figuring out the intuition behind some of the really interesting most general types you get out of computations; it seems to give a lot more insight into the behavior of the computation but I don't know how to interpret it very well yet.
&gt; It is precisely this weakness which makes Codensity efficient for Free. I don't think this is really true. Sure, you might not get as good performance if the term happens to use callcc, and the operation to get out of the codensity transform has type `Monad m =&gt; Cont (m r) r -&gt; m r` which is not completely general (but works with codensity), but any term you can construct with `Codensity` you can construct with `Cont` or `ContT` and it will work exactly the same way. Using `Codensity` is good because you should not use more power than you need. 
Most likely because that would make it very hard to make sure libraries type-check properly without writing 100% coverage tests.
You might want to reword the sentence "One way to make polymorphic code is to use typeclasses." You don't use the word "polymorphic" anywhere else in the article, and ["polymorphic code"][1] can have a different meaning than you intended. Maybe instead: "One way to make code more reusable is to use typeclasses." [1]: http://en.wikipedia.org/wiki/Polymorphic_code
At around 34m, he mentions a paper which justifies treating type expressions as numeric expressions WRT algebraic simplification, even when the intermediate expressions may not make sense. But the audio spazzes out as he cites the author's name; all I get is that he or she's a mathematician possibly from Glasgow. Point me in the right direction?
Actually, I discovered this is already on hackage as indexed and indexed-extras. As I edited into my original comment, this is actually the same concept Tekmo was referring to; but as sigfpe wrote his post at the same time (or shortly after) I was playing with Oleg's file, I remember the file more than the post. It's still fun stuff though; it would be nice to come up with some compelling applications and a better intuition about interpreting the types.
Oh, neat! Thanks.
Thanks! I just saw the search+button combo. You're right, the whole "a Haskell site" idea seems cool.
I'm pretty sure everyone understood it. Edit: ok, ok, I'll fix it.
Hmm ... can we get a sexy design for hayoo too?
Ditch the "DB" -- Hackage is the canonical name.
Who is it targeted at?
To people who use hackage.
I like it! 
This is great! Nice work!
I don't see how this page is a good front page for an abstract "hackage user". The main space is wasted listing more or less the same set of base packages several times. For a haskell beginner, this page doesn't make it clear how to install packages. After she finds out how to do that, she's inclined to install those Most Downloaded packages, which is wrong as most of them are already present in her installation of the platform. For a seasoned haskeller the front page should first of all contain the search field — 90% of times we go to hackage to look for some specific packages.
I appreciate the author's enthusiasm (and hey, he's 16 - kudos for writing about something so technical!), but this is not the blog post to read if you don't know what existentials are (or what they're for). The "usual" polymorphism in Haskell lets the *caller* choose the type; existentials simply flip this upside down and let the *callee* choose the type. That's the closest I've come to a what-and-why in a single sentence.
Or to look for some package to perform some specific function or to look for packages related to some existing package (e.g. all Yesod packages, an Attoparsec parser for X, QuickCheck Arbitrary instances for X,...) So yeah, one or more search fields would be best.
Yes, it's the same set of packages all over the place (I didn't wanted to put just some random text and I didn't had the exact statistics...) So in an ideal world, they would be filled with the correct packages. The goal with showing most downloaded packages (or other statistics) is that good quality packages (hopefully) stand out from the rest (an example is https://npmjs.org/ for node.js packages, it can be very useful) Again, the algorithm for finding these stats can be tweaked to not include the packages that are already in the base platform, etc. I think you're right with the fact that search field should be on the first page, because it's probably the most used feature of the website.
I don't think Haskell can easily be compared to other languages for this because Haskell packages tend to be much smaller on average and you need a larger number of them as dependencies compared to the average language. This makes toplists less useful.
The reason I never write a monad tutorial is because the [best monad tutorial](http://blog.sigfpe.com/2006/08/you-could-have-invented-monads-and.html) has already been written
Link for the slide: http://www.inf.ed.ac.uk/teaching/courses/inf1/fp/lectures/2011/lect01.pdf
If you establish a bijection like that between A &lt;=&gt; B, one often useful thing is to have a function: withBi f = bw . f . fw
Good points. I nominate you for the organization of future designs!
It's not the version that Wadler used in his class. FYI.
What about http://www.haskell.org/haskellwiki/Arrow_tutorial#Kleisli_Arrows then? The decomposition you do reminds me of pipes, but here the action flows backwards because the inputs are negated -- `a -&gt; r` rather than `a`.
Without disrespect, but I don't think Wadler is that good of a teacher. He doesn't seem to understand how a beginners mind works.
Yes, it is very similar to pipes, where Kleisli composition for the continuation monad resembles pipe pre-composition. You could imagine that you replace all the continuation holes with `request` statements with the `Hole` argument as the request parameter. and then pre-compose a pipe upstream that handles those `request`s. Using the `GLUT` example from the post, instea of this: largeProgram () = ContT $ \k -&gt; do ... x &lt;- k (Hole1 y) ... k Hole2 ... k (Hole29 spawn foo) ... you would do this: largeProgram () = do ... x &lt;- request (Hole1 y) ... request (Hole2) ... request (Hole29 spawn foo) ... and then precompose a `pipe` upstream that handles those `request`s, using the parameter to know which callback it was servicing. However, I would still recommend the continuation monad for this purpose. For example, `pipes` does not guarantee that the upstream handler that services the `request`s will reply the same way for multiple invocations of a given callback, whereas the continuation monad does. The other reason is that the continuation monad falls out more naturally from the intuition of leaving things incomplete, but the `pipes` model favors more of a streaming intuition.
Just finished this course IRL. 
Thanks for the detailed reply! Much clearer now :)
I have a moderately functioning web service that can apply arbitrary constraints/flags to the entire set of packages on Hackage. This allows you to run queries like: * Show me all packages that are compatible with haskell-platform 2012.4.x * Find the most recent versions of [x, y, z] that are compatible with [foo -any, bar == 1.3, baz &gt; 3.0] Which could turn into some interesting features: * Browse Hackage but see only packages/versions that are compatible with a preferred set (Haskell Platform X, Tibbe's favorites, etc) * Surpress deprecated and buggy packages from all results * Dynamically expand per-package version constraints. Why upload a new version of a package just to expand the versions? Cabal-install could query Hackage instead of running the solver locally It's important to have relevant packages displayed to a user. That may mean using statistics (downloads, revdeps, etc) for ranking, though it's tricky to do well, or relying on human curated sets. Highlighting the most downloaded packages may not be very informative, as others have mentioned. I really dislike wading through Hackage and having packages show up that rely on utf8-string or monads-tf or some other package I'd like to avoid. It's even worse when I do want to install something and it depends on 5 other packages that conflict with crap I already have installed. I'd love it if Hackage could make my life easier here.
I found I learnt more from just reading LearnYouAHaskell and doing the tutorial exercises than lectures ( but the lecturer this semester wasn't Wadler) . The course was fairly easy because nothing further than algebraic data types was covered in the exam. 
Email: http://www.haskell.org/pipermail/haskell-cafe/2013-January/105599.html
I'm stuck in an infinite link clicking loop, HELP M—
I would argue that Shae's project was successful in that he has created a multi-user collaborative ghci session. However the approach he took is not one where you leave it running on a server like gwern wants. It is intended for more like 'call for help' scenarios.
Excellent talk. Lots of material, accessibly presented.
Don't worry! The actual article doesn't link back to either the email or the Reddit submission, so you'll escape that infinite loop eventually if you just click links at random!
I no longer think these summaries are even modestly useful, because the judgement criteria are too harsh and too arbitrary. They reflect a bias towards "success" of a gsoc project as something measurable directly measurable in uptake and users within a relatively short span of time. GSoC projects are chosen, on the other hand, with an eye towards long-term payoff in Haskell infrastructure. The criteria that would yield us "high success" projects in the sense judged here would also yield us projects that weren't very interesting, useful, or important.
It doesn't really matter. Doing a quick sum of all the non-2012 successful/unsuccessful ratings, I get a chance of being successful at `(4+6+2+3+3+4) / ((4+2+5+1+2+4) + (4+6+2+3+3+4)) = 0.55`, which isn't terribly different from a guess of 0.5. (Why not redo it with 0.55? Well, I already had the code written for doing it with 0.5...)
http://hackage.haskell.org/package/fsnotify Despite the "Accelerating Haskell Application Development" project being cut short, it still produced a very useful cross-platform file watching library that is now being used by Yesod and we expect eventually all Haskell file watching needs. The student has been very slowly fixing bugs &amp; improving the library from real world feedback. By the definition of success given here (produce a library that others depend on) this project will most certainly meet it, although it may need more time to reach popularity goals. This project is succeeding despite the circumstances by design because it had incremental goals (given more GSoC time even more would have been achieved).
The article now links to the reddit submission (as a link with the text "Edward Kmett"), which means you may now get stuck in an infinite loop after all!
I think this summary is very useful. But I agree that judgement is entirely too quick. Better would be to actually talk to each project and ask them to give their success criteria and an estimate of when they think it will be reached.
D'oh!
I've yet to see an "intro to Haskell" course that's anywhere near as good as just reading LYAH and RWH in parallel.
Well, the Yoneda lemma is subtle afterall... I'd recommend looking at it in the category theoretic context. When doing it in Haskell you loose too much type information because of conflating (a) Haskell morphisms, (b) Haskell exponential objects, (c) metatheoretic mappings, etc. Also, (a) mappings as metatheoretic things, (b) mappings as Haskell functions, (c) mappings as set-theoretic functions, i.e., sets of pairs, etc. As a Haskell implementation it looks trivial, and in some sense it is trivial I suppose, but the significance of the Yoneda lemma makes a heck of a lot more sense when you keep the category theoretic details rigidly distinct. Also, it helps to manually work through some examples. The *Joy of Cats* [PDF](http://katmat.math.uni-bremen.de/acc/acc.pdf) has some good exercises IIRC.
Yeah, I definitely need to take a closer look. Thanks for the PDF, it's going in the library!
In all fairness, GHC will only rarely do this optimization for you.
It's only a programming language. You can spell out its whole name without fear of mystical repercussions. 
Exactly. It’s not exactly like something like it could make us would feel threatened and feel obsessed to slay the “heretic”… Everyone knows how things are.
it's not even necessarily an optimisation, if it changes the lifetime of the value - if it's a large but lazy value, computing it twice could have much better memory behaviour.
Well, you *can* make yourself a little code pre-processor. :) If there already is a parser and a way to transform it back into code (like those things that auto-layout code), you just have to inject your little transformations in-between.
No, you are confusing call-by-need with common subexpression elimination. CSE is not trivial since it might change the semantics. This is true in impure languages (duh), and in Haskell because of issues related to memory usage.
But why? The second element of the tuple isn't evaluated in the first example and now it is. How is that the same?
I like basic-prelude. Hooray for upgrading the defaults; I thought that's what Haskell2010 was going to be about! That'll learn me. But, &gt; Error messages become more confusing. regarding classy-prelude is a show-stopper. AfC
But when it is trying to unify all of the signatures, it considers the bodies of the functions. Why can't it determine that the expression is never referenced? It seems like the default behavior would be to assume the type is correct unless it fails to unify with the others. In this case it won't fail because nothing actually cares what the type is. Usually we can easily define polymorphic expressions, I don't yet see the difference. Perhaps I just don't know enough about the type system, but if I can see that it's fine without running it, what stops the compiler? You don't need to tell it anything extra for it to see that the expression will not be used no matter what.
Thanks!
But obviously the point was that other common string functions still need to be qualified and personally I hate qualifying them.
Hm, cool! I'll try this out next time I need to watch files. I tried the inotify lib in the past. Everything installed correctly and I made the trivialist of examples, but it wouldn't pick up anything. Was pretty frustrating, I can tell you! I killed a man!
I think the drawbacks of Classy prelude show that it is a totally wrong idea to go about fixing Haskell's namespacing issues.
&gt; Perhaps I just don't know enough about the type system, but if I can see that it's fine without running it, what stops the compiler? You don't need to tell it anything extra for it to see that the expression will not be used no matter what. Determining whether expressions are never used is actually undecidable, it's equivalent to the Halting problem. In other words, there is no algorithm to detect unused expressions that is guaranteed to terminate. Keep in mind that compilers can't do "proof by inspection" as humans can do. We *want* the compiler to follow a systematic, terminating and easy-to-follow recipe for type checking. The current recipe rejects programs as mentioned by the OP for good reason: we currently do not know any simple recipe for accepting them. ---- Note that ambiguous types like `Show a =&gt; Int` are actually problematic only because type classes introduce an interesting feature to the language: how to evaluate an expression can now depend on the type of an expression. In other words, different `a` can yield different integers here. There are weaker forms of type classes, where this cannot happen and ambiguous types are ok. These systems exclude things like the `Read` class.
Or, slightly clearer, mappend in backquotes.
**Partial application** refers to being able to say `(+) 3` instead of `\x -&gt; 3 + x`. That's usually a good thing in Haskell to do. (Strictly) **partial functions** don't take a value in their codomain for one or more values in their domain. The example here is `head`: head :: [a] -&gt; a head (x:_) = x head [] = error "Prelude.head: empty list" Note how the empty list is a value of the type `[a]` (for any `a`), but `head []` is *not* a value of type `a`. Instead, it evaluates to a so-called *bottom value*. `error` takes a `String` and results in a "value" of any type, but it's not really any "real" value. Abnormal termination is a bottom, and so is non-termination: head :: [a] -&gt; a head (x:_) = x head [] = let x = x in x Normally, you'd want to express the possibility of failure in the type of a function: head :: [a] -&gt; Maybe a head (x:_) = Just x head [] = Nothing Then it becomes a *total function*: for *every* value in the domain, the function ~~assigns a value in the codomain to it~~ computes a value in the codomain, using only finite time (and finite space).
There's a little mixup here involving two terms: *partial* functions and *partially applied* functions. Partially applied functions are functions like `((+) 5)`, where there is still at least one argument left to be applied. Partial functions are functions that don't handle everything, which means for certain values they are undefined. The canonical example is `head` in the Prelude because it works when you do something like `head [1,2,3]`, but when something like `head []` is evaluated, the program just exits with an error. In other words, because `head` isn't defined for any and every value of type `[a]`, it's called partial. Additionally, functions that *are* defined for all values of their arguments are called total.
How the hell did I miss that? Thanks! :)
I think the author has simply forgotten to export some modules. Here is how I think it should work: import Data.Text (pack) import Filesystem.Path.CurrentOS (fromText) import System.FSNotify path = fromText $ pack "/tmp" pred = const True --ActionPredicate ~ (Event -&gt; Bool) act = print --Action ~ (Event -&gt; IO ()) main = do wm &lt;- startManager --for some reason withManager didn’t work for me watchDir wm path pred act stopManager wm
Hi Chris, we will add an example in the README, glad you got that far :) import System.FSNotify import System.FSNotify.Devel watch :: IO WatchManager watch = do wd &lt;- getWorkingDirectory man &lt;- startManager coffee man wd return man coffee ∷ WatchManager -&gt; FilePath -&gt; IO () coffee man dir = do putStrLn $ "coffee " ++ encodeString dir treeExtExists man dir "coffee" $ \fp -&gt; do compileCoffee2Javascript fp -- invokes the 'coffee' executable
&gt; even though continuation style code has all this extra machinery it can actually produce faster code I'm not sure it would in that case: Tekmo's method implies building an ad-hoc datatype for each hole. A value of that datatype is created before the hole, and then the "callback" deconstructs it to see what it has to answer to. So there's a lot of construction/deconstruction that happens, whereas callbacks/continuations normally avoid the construction of intermediary values. Indeed, their point is that -- when something happens -- we directly _call_ the relevant code instead of building something that the next piece of code will have to dispatch upon, hence the speedup. Maybe GHC is able to do something intelligent with that, like skipping the construction of an intermediary datatype if it's to be deconstructed rightaway (a la loop/stream fusion), but this idea of using continuations to build more intermediary data [*] bugs me a little, as it feels counterintuitive. [*] Which implies a lot of ad-hoc datatypes to create in a big project. Advantage: more formalization of the interfaces: we directly see the possible inlets/outlets of some part of the code. Drawback: more code to write. Solution: crank up structural polymorphism (a.k.a. anonymous datatypes) into GHC (esp. the type inferer)! It would enable us to let the compiler derive the datatypes describing the inlets/outlets. **EDIT:** Other (far simpler) solution: use ImplicitParams to define (and name) holes and plug components together: someProc = do y &lt;- doStuff x &lt;- ?hole1 y z &lt;- moreStuff t &lt;- ?hole2 z finalStuff t x then runAllTheThings s = s where ?hole1 = \y -&gt; ... -- lambda-style necessary, GHC don't like ?hole2 = \z -&gt; ... -- implicit params being declared like functions. Sad. and plug it with: runAllTheThings someProc Drop the ad-hoc data declarations, drop the constructions/deconstructions, keep the type-safety, keep the naming of the holes, keep the fun. But you can still keep the use of Cont if you want the holes to be able to interrupt someProc, for instance. (Although then, another monad capable of that like Maybe or Either would be more suitable)
&gt;For example see regexp libraries, the guide exists, which is good as there are many libraries, but it is still hard figure out how to actually use them to match string against regexp and read matched groups. You need pcre-light. http://hackage.haskell.org/package/pcre-light-0.4 Unlike regex-base and friends it is not heavily overloaded.
What kind of namespacing solution could work here though? Many of those names (length, insert, etc) really look like the kind of thing that you would want to overload and I thing you can only do that with type classes.
The intermediate expressions might not "make sense", but they are all legitimate data types with the expected cardinality and topology. That is probably the most useful technique you ever learn about types :)
This is what Edward's `lens` package is solving. He's coming up with all sorts of principled answers for what is the right definition for these overloaded operations.
Man, I hate the lack of `inotify` on Linux, so I was planning on using this to implement some equivalent of `inotifywait` for OS X. This is awesome, :)
I think he's got a point, though - why are those functions exposed in the top-level module of the package, if you can't use them?
The brochure said there would only be a few preludes!
Wouldn't it be better to serialize `Text` as UTF-16, as that is how it is stored in memory? Then you can avoid the impact of encoding conversion and serialize the byte array directly, I imagine? [Relevant](http://jaspervdj.be/posts/2011-08-19-text-utf8-the-aftermath.html).
I made it through the first half of the first video. I remember why I hated college. Wadler spends the first half hour talking down to his audience, and pushing the (utterly false) idea that his choice of textbook is the best way to get something out of his class.
You are right. This one looks easy to use.
Thanks for explaining this, but I'm still not super clear on why this is equivalent to the halting problem. The compiler *has* the entire source and a complete description of what every token means. Indeed there is no way that this program could ever be made to access that second value without changing the source and the types of every function therein indicates that it isn't used. fst :: (a, b) -&gt; a is crystal clear. There's no reason to need to know the type of `b`. I agree that some, perhaps even most, programs *do* have ambiguity in some aspect that affects the computation and so concrete types are needed, but by the same process the compiler should be able to ignore those types which don't. There is nothing to "force" the compiler to determine the type because no one asks for it. Did you mean `Show a =&gt; a -&gt; Int`? Otherwise I don't understand that example either.
I think `mappend` is a stupid name, honestly. It's really disingenuous and no one ever describes monoidal operations as "appending". It should at least be `mcombine`. `&lt;&gt;` is a great operator here. It "feels" like the arithmetic binary operators we all know and love, but somehow doesn't carry any baggage or prejudice about what it "should do". 
I noticed the exact same problem several months ago. I first raised the issue here: https://github.com/GaloisInc/cereal/issues/7 If you directly read and write to `Handle`s instead of going through `ByteString`s you get a HUGE performance boost. So I ended up writing my own serialization library for my own projects that does exactly that.
Interesting technique, for heavy image serialization I've used successfully the combo mutable "fat" vector (think ~100ko per vector) converted to bytestring once full with a final lazy bytestring. Support the serialization of heavy JPG without blowing memory (like with a naive cereal use)
&gt; The Text type's internal representation is UTF-16, using the platform's native endianness. I suppose for serialization, we want the portability of a fixed endianness, so we'd settle on whatever seems to be the most common native one and take the conversion hit on other platforms. Maybe still a smaller hit than converting to UTF-8?
[Yes.](http://en.wikipedia.org/wiki/Byte_order_mark) But probably little-endian: &gt; * `Floats` are encoded as IEEE 754 values with their octets in little-endian order. &gt; * `Doubles` are encoded as IEEE 754 values with their octets in little-endian order. 
Since one of this lib's goals seems to be for sending data over the wire, network byte-order seems appropriate. Anything else would be asinine for this use case.
The article writer seems badly misinformed.
It lost me at that part, too, even though being less monomorphic is one of the reasons I use my own tweaked Prelude. I'm strongly opposed to type classes where polymorphic code doesn't have a well-defined meaning distinct from any law-abiding instance. We already have atrocities like `Enum`, adding yet more incoherent type classes is not the way to improve matters. Some manner of support for custom defaulting rules would also be required to deal with ambiguous-yet-well-defined types in an alternate Prelude that goes significantly outside of the standard type classes.
Please [elaborate](http://www.reddit.com/r/haskell/comments/15frw8/douglas_crockford_monads_and_gonads_yuiconf/c7mrj9n)?
What's happening is that randomNorm' is getting treated as a constant by the compiler. Because its type is Float, the compiler doesn't think it will ever change, so it will only do the computation once and reuse the same value elsewhere. This is exactly why unsafePerformIO is *unsafe*. It results in unexpected behavior if you use it wrong. You should only use unsafePerformIO when you can guarantee that the result will be the same no matter what... that means basically never. Edit: BTW, this is exactly the sort of question SO like to answer.
You don't need to use `unsafePerformIO`. In fact, it is very non-idiomatic to use it. This is what you actually want: ghci&gt; x &lt;- randomNorm ghci&gt; print x ghci&gt; y &lt;- randomNorm ghci&gt; print y ... or you can skip the intermediate variable using: ghci&gt; randomNorm &gt;&gt;= print
&gt; The short version of the message is that monads are a way of getting around the immutability that is a central principle of functional programming. This means that within a language that has mutable data structures and functions, monads aren't really relevant and you don't need to worry about them That's nonsense.
I will rebut your linked comment by asking you to implement the pipe monad in Javascript. I will even make it easy for you by just asking you to implement the non-monad-transformer version of it: data Pipe a b r = Await (a -&gt; Pipe a b r) | Yield b (Pipe a b r) | Pure r instance Monad (Pipe a b) where return = Pure m &gt;&gt;= f = case m of Pure r -&gt; f r Await k -&gt; Await (\a -&gt; k a &gt;&gt;= f) Yield b p -&gt; Yield b (p &gt;&gt;= f) You don't even have to implement pipe composition. I'm just asking you to implement the pipe monad alone. You do have to prove it satisfies the monad laws, though. You also said that "the JS semicolon can be interpreted as a monadic bind", so you get extra credit if you can overload the JS semicolon to use the pipe monad.
unsafePerformIO *never* makes code purer. It just covers up the impurities and forces you, the imperfect programmer, to handle them yourself. This is bad in Haskell. We want the compiler to handle that for us because it will not make mistakes. This is why Haskell code is so robust. See Tekmo's post below for the best way to "get rid" of the IO monad (or whichever monad you happen to be in). I personally do all my random number stuff in the Random monad provided by the MonadRandom package on hackage. MonadRandom seems to be the simplest package, but there are quite a few others available on hackage.
&gt; allowing myself to maintain "purer" ... They aren't purer. In fact, they're much less so! They no longer just depend on the arguments being passed in. The compiler is perfectly allowed to assume that they do however, resulting in the problem you found. You suddenly cannot reason about your functions in the same way &gt; Obviously the reasoning behind using unsafePerformIO here is in attempt to "get rid" of the IO You've lost the IO in the type but there is still IO. &gt; Especially if the function I pass it to covers any unsafe edge-case (e.g. division by 0)? I think you're misunderstanding the use of 'unsafe', it doesn't mean the code is more likely to crash, it's that you can't make any guarantees about whether or not the IO is performed at all.
Very unsafe. `unsafePerformIO` is only safe to use if the function returns the EXACT same output given the same input, and does not have any side effects. Your function satisfies the latter condition, but not the former. The reason the former condition is important is that the compiler assumes that a pure value will always evaluate to the same thing, so if your randomNorm has the following type: randomNorm' :: Double ... then the compiler assumes it can reuse the very first evaluation of `randomNorm` for each subsequent call. You didn't notice this in `ghci` because `ghci` doesn't do any optimization, but you'll notice a big difference if you compile it and it starts optimizing away repeated calls to `randomNorm` to a single call. It's also unsafe in principle because now you make it impossible to reason about pure code. The Haskell contract is that pure code always returns the same value, but the moment you break that contract then all hell breaks loose and you can't reason about the correctness of any code that depends on that function. You really should never use `unsafePerformIO` for anything. It's completely unnecessary for any Haskell programming. Just mark your function as impure by having it return a value in the `IO` monad and you won't have any problems.
The hilarious irony here is that what monadic I/O replaced was an awkward, unprincipled sort of request/response stream processor thing. So in a way we've come full circle; it could be (and likely has been) argued that something much closer to `pipes` is what we should have had in the first place.
Yeah, I was the one who argued it, [remember](http://www.reddit.com/r/haskell/comments/swffy/why_do_we_not_define_io_as_a_free_monad/)?
A good rule of thumb is that you really don't want to use unsafePerformIO at all until you know enough about Haskell to know why it is a bad idea 99.9+% of the time. That one always worked for me so far. Other unsafe* functions are similar.
Actually, I'd forgotten! But I doubt you either were the first or will be the last to make that argument.
&gt; A good rule of thumb is that you really don't want to use unsafePerformIO at all until you know enough about Haskell to know why it is a bad idea 99.9+% of the time. My preferred formulation of this is "Never use it, at all, until you understand Haskell well enough to understand and agree with this rule."
It's a new operator, and it looks like "not equal to" from multiple languages.
From your linked comment: &gt; But IO aside, what about other monad instances? Thing is, JS already has effectful ways to allow exceptions, parsing (I would also argue that applicatives are almost always a better choice here), IO, and most other things monad instances are used for in Haskell. This is incorrect. JS has an effectful way to do *all* of those things, in unrestricted combination, all the time, whether you want to or not. What JS does not have is a structured way to explicitly *not do* something. This is a much bigger problem than a clumsy IO system.
We do not need yet another library for this. We need to pick one and then work on making that one better (if needed).
Is it arguably or measurably a much bigger problem?
What semantics can you give to an arbitrary law-abiding `Enum` instance? What behaviors do you expect an `Enum` instance to have in relation to other types used by `Enum`'s functions? How many of the following do you think should always be true, assuming all values are non-bottom? succ x /= x succ . pred === pred . succ === id toEnum (fromEnum x + 1) == succ x map fromEnum (enumFromThenTo x y z) == enumFromThenTo (fromEnum x) (fromEnum y) (fromEnum z)
A clumsy IO system is much easier to fix, and a lack of controlled effects makes a lot of useful things basically impossible to implement. It's not that hard to see which is more objectively detrimental to a language.
So... is your point that the (sensible) laws you listed do not hold in general for instances of Enum, or merely that they are not explicitly stated in the documentation in the class?
I agree that floating point types and Rational violate the laws, but all of the other types which have instances listed for Enum (and all of the types for which I have ever written an instance) are indeed isomorphic to Int. So really your argument seems to just be against the instances for the floating point types and Rational.
Here's the flaw in your argument. You say: "Javascript can implement X, Y, and Z. Haskell has a monadic interface to X, Y, and Z. Therefore, Javascript has a monadic interface to X, Y, and Z." The mistake in your reasoning is that you are conflating with the interface (i.e. `Monad`) with the things that implement that interface (`Maybe`, `[]`, `Parser`, etc.). Just because Javascript can implement these types does not mean that it has a monadic interface to these types. What do I mean by monadic interface? I mean the ability to program generically over monads. That includes: * Generic combinators from `Control.Monad` like `mapM` and `forever` * `do` notation Javascript provides neither of these. That's why we say that Javascript does not have support for monads.
Of course, "isomorphic to `Int`" is not terribly inspiring as a type class. And that's not even getting into other issues, like how `Enum`, `Ord`, and `Bounded` interrelate (or don't). If you try to relate `Enum` to other numeric type classes you probably hit problems with the instances for fixed-point numbers as well. Anyway, there are multiple useful, consistently-defined things that `Enum` *could* be. Right now it is none of them because it is trying to be all of them at once. If a version of `Monoid` existed that was similarly ill-defined, the proper solution would be to turn it into `Monoid`, not shrug and say it meant types isomorphic to lists or something like that. My argument is not against specific instances as such. My argument is that `Enum` is presently too ill-defined to even say which instances are reasonable. I'm not even confident that the seemingly well-behaved instances don't also violate my expectations in some way, in large part because I don't even have anything firm to base those expectations on! And that's why I call it an atrocity. :]
Except network byte order is big endian and the vast majority of machines performance critical network code will run on are little endian. Conceptually it's nicer to use network byte order, but it's accepting an overhead without any actual advantage other than the conceptual one. 
You are certainly entitled to your own opinion, but the criticisms you have been making are incredibly vague. I see now reason why, excluding the floating point types and Rational, we could not establish laws like the ones you listed that are consistent with the other classes.
So you'd propose to make range syntax not usable with the standard fractional types, so that something like `[0, 0.5..]` is no longer allowed? I'm sure nobody would miss that. But anyway, I've said all this many times before and I'm tired of reiterating it. If you can't see why `Enum` is hopelessly ill-defined in its current form and why adding a few laws isn't a helpful solution I'm afraid I don't have the time to elaborate in the necessary detail.
So, to give you a concrete example of when it might make sense to use unsafePerformIO, here is where I have used it in the past. I once worked on a numerical code in Haskell where the datatypes were multidimensional arrays backed by chunks of raw memory; I did it this way because it was easiest to express most of the computations efficiently in Fortran (which has first-class support for multi-dimensional arrays), especially since I was usually making calls to other Fortran libraries to do the brunt work. Because of this model, I used unsafePerformIO everywhere that my code passed its arrays to the Fortran code, because although there was raw memory manipulation going on under the hood the calculations were pure --- given the same inputs, you would always get the same outputs --- and so it made more sense for the functions to be pure then to live in the IO monad. So in short: as others have said here, only use unsafePerformIO in the rare case where side-effectful stuff is going on under the hood but the resulting function is pure.
&gt; Other unsafe* functions are similar. I occasionally use unsafeCoerce for coercing between GADTs that I know have the same type index but I don't want to prove the type equality to GHC's satisfaction. This props up occasionally when doing type hackery.
Ah, I assumed it was more important. Further research shows NBO is just a fluke of history.
&gt; Maybe the perfect package is out there, but how exactly is a programmer supposed to go from problem to solution? Asking on #haskell on freenode usually works for me.
&gt; [...] every method has vocal critics, even comparing LOC's is non-trivial. I've seen ruby coders that are vertical compression maniacs For comparing code sizes, isn't length of compressed (say gzipped) source a commonly accepted metric?
&gt; Some libraries have scientific paper (or thesis) as their only usable documentation. Some libraries have no documentation..
&gt; No, but I think that if the worst can be said of a class is that there are a couple of exceptions that break its rules then I don't think it is such an atrocity after all. What? I can't think of many things worse than that to say about a class at all. Fiddly "may not work with certain combinations of `seq` and bottom" is one thing, completely broken expectations is utterly unacceptable. &gt; Fair enough, but in the future if you aren't willing to back up your remarks then you might consider refraining from making them in the first place rather than telling other people that the problem is that they can't see what you can obviously see. :-) Ok, that's great. Thanks! I'm glad you appreciate my honest attempt to be helpful and informative, that makes me feel much better about the time I did spend writing these comments. I'm sorry I apparently made no effort whatsoever to justify my remarks! Sigh.
Good idea to exploit our new more flexible Handle interface - something we couldn't do in the past.
Google's NativeClient might be able to fix that for Chrome.
&gt; Thanks for your time, at any rate. (I mean that sincerely, not sarcasm this time) Likewise! :-) Major points scored for being gracious! :-D (And no, I am not being sarcastic, I'm just crazy. :-) ) And I agree with you that commenting on reddit is not a productive use of your time if you are not having fun doing it. There are enough people on reddit who actually enjoying arguing with people like myself that it is not your job to do so. :-)
&gt; There are enough people on reddit who actually enjoying arguing with people like myself that it is not your job to do so. :-) Correct. Rather, my job is to [deal with Reddit's incomprehensible and deranged spam filter](http://i.imgur.com/X3MyA.png).
Still, I completely understand the temptation to at least play with unsafePerformIO. "Don't EVER touch this box over here". \*touch\*
I think cloud haskell makes the same mistake at the moment and uses nbo. We would probably be better served if it standardized on little endian as well.
Good question! Much of the time it is true that you can just mark the foreign function as being pure, but there were two issues that prevented me from being able to do this. First, in Fortran all arguments are passed by reference rather than by value, so you need to use something like alloca (which does a temporary allocation on the strack) to allocate a temporary buffer for each of the arguments that you want to pass to it. Second, in my case I was managing the memory for the multi-dimensional arrays in Haskell using ForeignPtrs, which meant I had to use withForeignPtr in order to get a raw pointer to the memory to pass to my Fortran routines.
Is there a video to go with this?
&gt; unsafePerformIO is only safe to use if the function returns the EXACT same output given the same input, and does not have any side effects External functional purity is not enough. Haskell lacks the value restriction, so `unsafePerformIO` can be used to break the type system. There are a small number of good uses for unsafePerfomIO, mostly related to performance (like difference arrays). 
Goddamn that was terrible.
Minor but you can also use `=&lt;&lt;` to make it read like normal application: print =&lt;&lt; randomNorm
There's definitely something to be said for creating yet another library. Particularly when the problem is manageable, and the existing choices have compatibility issues and like cereal and binary, are very, very widely used. Better to create something and prove we can do better, than get bogged down for months navigating political and backward-compatibility purgatory waiting for the chance. NOW, that we have the improved code, let's keep an eye on ways to reduce fragmentation, see if there are really use cases for all of the existing libraries, and perhaps merge or deprecate as needed to point people to the best existing answer.
It's possible. But it's conceptually equivalent to using unsafePerformIO. You're calling a foreign function. It might do anything. Whether you explicitly use unsafePerformIO or just leave off the IO in the type of the foreign declaration, you're saying to the compiler, "I know this function is safe. Trust me." (SafeHaskell disallows both forms.)
In [GHC 7.6.1](http://www.haskell.org/ghc/docs/7.6.1/html/users_guide/release-7-6-1.html): &gt; The Text.Read module now exports functions &gt; &gt; readEither :: Read a =&gt; String -&gt; Either String a &gt; readMaybe :: Read a =&gt; String -&gt; Maybe a But yeah, us still stuck on older versions would prefer `readMay` from `safe`.
Coming up with great ideas sometimes needs fresher grounds. Assessing that a new implementation should be the new default (i.e. the one that you should first think of) takes time (i.e. it should be used by a lot of people, for different use cases). Also, different but related libraries can have different advantages and disadvantages, so they can live side by side and you can choose the one that fits your needs. I posted the link exactly because I want to share the fact there are alternatives, that can be possibly better than what already exists, or in this case, possibly better than what people already know. Now as perhaps you suggest, I would be glad if you could provide benchmarks, or a blog post detailing the pros and cons of the related libraries, so "we" could "pick one".
First of all, as I see it, Haskell has been struggling with IO since the dawn of its time and until now (cf. iteratee libraries). Second, JavaScript, although deeply flawed from a language design standpoint, has been employed for obviously a lot more *useful applications* than all of pure languages combined. And this is much more important. Note that I'm not defending JavaScript and I'm a big fan of purity, but I just want to be reasonable. Haskell looses because it's trying to be too smart for its own good. What if we try to emphasise APIs that don't require you to learn 10 new concepts, just to make something basic? What if we hide all those new and difficult concepts underneath self-explanatory interfaces? You know, DDD style, for human beings. Maybe then Haskell won't be just an inspiration. We never do, and that is the real Curse of the Monad.
No, we have a misunderstanding. I'm saying “therefore JavaScript doesn't need a monadic interface to X, Y, and Z”.
&gt; Thanks for explaining this, but I'm still not super clear on why this is equivalent to the halting problem. Consider the following program: turingMachine :: String turingMachine = ... -- some description of a Turing machine here foo :: a -&gt; a foo x = if turingMachine `executeOnInput` 0 then x else undefined Will the function `foo` go into an infinite loop or will it eventually terminate and return the value `x`? The compiler cannot determine whether the value `x` is used or not without deciding whether the Turing machine specified in the source code of `turingMachine` halts on the input `0` or not. Any compiler that tries to do that will, however, itself go into an infinite loop when compiling some programs. &gt; `fst :: (a, b) -&gt; a` is crystal clear. There's no reason to need to know the type of `b`. Pigworker showed an example where you need to know the type of `b` in order to calculate the `a`. Essentially, the type of `bar` is actually bar :: (Show a, Read a) =&gt; Int and the value of `bar` depends on what the type `a` is. The second element of the tuple is never used in the invocation of `fst`, but you still need to know its type in order to know which value `bar` has.
Not in place, they won't. Moreover, I may well want to work with mutable structures threaded through with multiple pointer chains for fast access via more than just a tree structure, and I may be happy to ensure that the data structure remains unshared, but I can't express that deal.
There are three rebuttals to that. The first is that you've never used a monadic interface effectively. For example, consider asynchronous programming (i.e. Node.js). Using Haskell's `Cont` monad you can take asynchronous code and program as if it were synchronous code. Moreover, given a way to open one resource at a time using: foo :: Resource -&gt; (Handle -&gt; IO a) -&gt; IO a ... I can use the generic monad combinator `forM_` to lift `foo` to process a list of resources at a time: forM_ resources $ \resource -&gt; do handle &lt;- Cont (foo resource) lift $ doSomethingWith handle You can't do that with Javascript (Well, maybe you can, but I imagine it would be incredibly awkward and difficult to use and full of bugs and invariants that no reasonable programmer could maintain). That leads me to my second rebuttal: none of the operations across these different types X, Y, and Z are generic across all monads. That means that each time a programmer wants to learn a new type they must relearn all new operations for each type. That means they have to learn the bind command for `X`, the bind command for `Y`, the bind command for `Z`, the `forM_` command for `X`, the `form_` command for `Y`, the `forM_` command for `Z`, the `forever` command for `X`, the `forever` command for `Y`, the `forever` command for `Z`, and so forth. Multiply this by the number of commands and the number of types that you are trying to not use monads for and you impose a huge burden on the programmer. Javascript does not have monads **in practice** because you would have to learn a new API for each type that was trying to simulate monads from scratch, which nobody actually wants to do. With Haskell, you learn the `Control.Monad` API once and it instantly applies to every type that implements `Monad`, so there is no penalty for picking up new types that implement `Monad` and using them. The third rebuttal is that Javascript only provides pseudo-monadic support for a limited number of types. This is why my very first comment asked you to implement the Pipe monad, because I specifically picked a monad that Javascript does not provide language support for and that doesn't lend itself well to cheating by using side effects or mutation to solve the problem. Haskell solves the problems of monads in *general*, whereas Javascript only solves it for a few types that implement monad. If all you are interested in doing are side effects and exception handling, then knock yourself out, but if you want to build your own abstractions that Javascript does not support, then you are out of luck because then you have to ask the language designers to implement it for you, and good luck with that!
&gt; NOW, that we have the improved code, let's keep an eye on ways to reduce fragmentation, see if there are really use cases for all of the existing libraries, and perhaps merge or deprecate as needed to point people to the best existing answer. +1
And also the One True Byte Order :)
I *think* the idiomatic solution is either explicitly pass the random value down or use a writer monad to implicitly do the same. 
This requires a very careful understanding of how `ghc` optimizes programs. Just making your program CPS is not some magic bullet that makes your program run faster. In fact, it can even make your program slower if you are not careful. I will eventually do a simple write-up for a toy example, but the short answer is that continuation passing style only works if it exposes more optimization opportunities to the compiler.
So... you are saying that JavaScript needs monads but can't have them?
then why?
I'm just a Haskell dilettante so I found quite a lot to chew on here. Very nice set of slides, straightforwardly presented, no video necessary to follow it along.
I'll take a stab. Prelude&gt; :t (.) (.) :: (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c so concat is taking the place of (b -&gt; c) in (concat .) and you're left needing to supply something that will do a -&gt; b where b is [[a1]] Prelude&gt; :t (concat .) (concat .) :: (a -&gt; [[a1]]) -&gt; a -&gt; [a1] No I'm stuck :-) 
When we pass f :: a -&gt; b to the third concatMap', we get concatMap' f = ((concat .) . map) f = (concat .) (map f) = \l -&gt; concat ((map f) l) = concat . map f using the definition of function composition ((.)). Comment on your edit: use squirk when you want to feed 2 parameters into g and thread its output into f.
This might be of help: http://missingfaktor.blogspot.ro/2011/12/sections-of.html. Especially: &gt; (f .) is a function that modifies the return type of another function by f. &gt; (. f) is a function that modifies the argument type of another function by f.
This is a little more enlightening: Prelude&gt; let squirk = \f g -&gt; (f .) . g Prelude&gt; :t squirk squirk :: (b -&gt; c) -&gt; (a -&gt; a1 -&gt; b) -&gt; a -&gt; a1 -&gt; c squirk takes a function of one parameter and a function of two parameters, and "chains" them together. concat is the function of 1 param, and map is the function of 2.
In the light of this I think I'd prefer: f -&lt; g = (f .) . g concatMap' = concat -&lt; map The -&lt; symbol should denote the fact that the function on the right takes two arguments and its returned value is fed into the function on the left.
 concatMap f [] = [] concatMap f (x:xs) = f x ++ concatMap f xs
Protobuf for example cannot serialize sum types. You want serialize Maybe? Sorry. It also works other way round. You specify data types using protobuf language. It couldn't be used to serialize existing data type.
Two reasons: 1) Network byte order is so widespread in every spec that library support and programmer assumptions abound 2) Big Endian is easier to debug because it actually makes sense to humans Neither of these matter internally, but both matter in file formats and on the wire.
We all appreciate Simon's support. Thank you, Simon. We will do our very best to live up to your high expectations of FP Complete.
Use `pipes` to decompose your application into the `IO` part that generates random numbers and the pure part that processes them. I'll use the following code to demonstrate this: import Control.Monad.Trans.Writer.Strict import Control.Proxy import System.Random -- This is an impure random number generator randomsD :: (Proxy p) =&gt; () -&gt; Producer p Double IO r randomsD () = runIdentityP $ forever $ do x &lt;- lift $ randomRIO (0, 1) respond x -- This is a pure number generator (obviously not random) mockRandomsD :: (Monad m, Proxy p) =&gt; () -&gt; Producer p Double m () mockRandomsD = fromListS [0.3425, 0.12415] pureSum :: Double pureSum = getSum . execWriter . runProxy $ raiseK mockRandomsD -- Generate mock randoms purely &gt;-&gt; sumD -- Sum them purely impureSum :: IO Double impureSum = fmap getSum . execWriterT . runProxy $ raiseK randomsD -- Generate randoms impurely &gt;-&gt; takeB_ 10 -- Take the first 10 randoms &gt;-&gt; sumD -- Sum them purely Notice that `randomsD` is the only impure function. `pipes` lets you decompose your impure bits into their own modular component so that it doesn't contaminate other components (like `takeB_` and `sumD`, which are both pure). Every other component has a polymorphic base monad, which will type-check as `Identity` if every component in the pipeline is pure. By factoring out all the impurity into its own component, I can then switch it out for a pure component (like `mockRandoms`) and generate an entirely pure computation (i.e. `pureSum`). This is the proof that the impure bits reside entirely in `randomsD`. So `pipes` is the idiomatic solution for separating `IO` from pure logic in this particular case. If you want to learn more about the `pipes` library, read [Control.Proxy.Tutorial](http://hackage.haskell.org/packages/archive/pipes/3.0.0/doc/html/Control-Proxy-Tutorial.html).
`squirk` can also be written: oo = (.) . (.) And is the operator for composing some function with a two-argument function.
Often-times you can have your cake and eat it too, though this is not always true, and new ways have to be discovered all the time. But one example in the slides, the "mean" function, is the example used by Max Rabkin’s [Beautiful folding](http://squing.blogspot.co.il/2008/11/beautiful-folding.html) post. Conal Elliott [improved on it](http://conal.net/blog/posts/another-lovely-example-of-type-class-morphisms), too. You can write high-performance code that looks and feels like the naive code, and can be reasoned about like the naive code. But I agree that the extra abstractions that let us reason so nicely about the correctness of our code can also make it harder to reason about the performance of our code. In this trade-off, especially due to the decent-performance-by-default nature of Haskell, I much prefer (most of the time!) to get correctness for cheap than performance for cheap.
&gt; But one example in the slides, the "mean" function, is the example used by Max Rabkin’s Beautiful folding post. Conal Elliott improved on it, too. Yeah, I was going to make that point but chose not to. Oftentimes, if you can make use of standard library functions for doing things, you can get reasonable performance. This really speaks to an important point about functional languages: if you're writing recursive algorithms directly, it's possible you're doing it wrong. But even then, taking this example in particular, you need to understand the difference between foldl and foldr, not to mention their strict equivalents, in order to make the right choices, and those differences require some rather deep reasoning about laziness. &gt; In this trade-off, especially due to the **decent-performance-by-default nature of Haskell**, I much prefer (most of the time!) to get correctness for cheap than performance for cheap. See, this is the part where I really have to disagree. I mean, if we want to pick on Haskell, the most obvious place to start is Strings, where not only is performance not decent by default, it's truly awful. And we're *still* only starting to see standardization on the Text library (which has had performance warts of its own)... although even then sometimes ByteString is the way to go... sometimes. Maybe. And don't get me started on the performance of the standard Haskell data structures... picking the right structure for any given application can be black magic in and of itself... do I pick an array? An unboxed array? What about using the IO monad and a mutable array? Hmm, decisions decisions... Then again, maybe we just have different interpretations of the word "decent". For toy programs, absolutely, Haskell will perform just fine by default. But writing even a simple, naive text filter for a multi-megabyte file (ie, something that reads a file, processes it in some way, then writes it back out) is very easy to write very poorly with plain ol' idiomatic Haskell (a fact that we've seen illustrated many times here on /r/haskell). Update: Incidentally, I should point out that I really love writing Haskell code when I have the opportunity... I just loathe trying to write Haskell code that had to have even reasonable performance, as it always felt like I was on the verge of cargo-cult programming. I simply can't seem to write a reasonably complex Haskell program without a space leak somewhere...
Honestly, most existing serialization formats are nothing to write home about, even considered on their best merits. Their shortcomings become especially glaring when looked at from a Haskell perspective, as they are universally not designed with a rich type system in mind.
i am always curious about what is the most "functional" way of defining a graph. Hope this sheds some light on my understanding.
Looks better with less whitespace ;) (.).(.)
I think that would be cool if it works 100%, but it's probably little bit difficult to achieve. I'm using a plugin called ghcmod-vim (https://github.com/eagletmt/ghcmod-vim) it has a command for :GhcModType which gives you the type of what's under the cursor, but unfortunately sometimes (or most of the time for me) it doesn't work.
Part of the difficulty is probably that a lot of the interesting types are in the current module and that won't parse 90% of the time while you are in the middle of typing something.
A fourth concatMap: concatMap f = foldr (\x xs -&gt; f x ++ xs) [] (Sorry, I couldn't resist.)
So?
There is a plugin for sublime-text too that works like it - but it's just to clumsy to use - I end up using ghci 90% of the time - but working feature like this would be great. As zzalpha mentioned: it kinda works in EclipseFp and for F# in VS it works really well (but of course F#s type-system is 'weaker')
Not vim per se, but after a recent post from Pike (IIRC) stating that syntax was distracting, I thought that a non colored buffer with a 'semantic' status bar informing about the current `word` could be a good thing (and be generalized to any kind of data, and paradigm). &lt;/2cts&gt;
See also the [ICFP presentation](http://www.youtube.com/watch?v=tQGh5oemhkw) of this paper
"(f .) . g" means: Compose f with the function *returned* by g (rather than composing f with g itself).
That's great! Let's just hope nobody ever before has used `-&lt;` for something else completely different, or ever after dares to...
Yes, that's the reason I mentioned `compose (compose f) g` because to me that clearly says "compose f with the function returned by g", whereas `(f .) . g` somehow doesn't sink in.
Why should we sacrifice quality for popularity?
&gt; Second, JavaScript, although deeply flawed from a language design standpoint, has been employed for obviously a lot more useful applications than all of pure languages combined. And this is much more important. No, it's not important. I don't really care how many people use Javascript or COBOL or Brainfuck or PHP or whatever else instead of Haskell. That's their loss, not mine. I'm all for improving Haskell--things can always be better. But only if it's an actual improvement, not some attempt to market the language to people who aren't interested in what it has to offer. The Haskell community is active and growing. Where's the problem here? &gt; You know, DDD style, for human beings. Maybe then Haskell won't be just an inspiration. There are plenty of human beings already using Haskell as more than just an inspiration. If by "human beings" what you really mean is "people afraid of abstraction and learning new concepts" then I don't think you appreciate how little their opinion matters to me.
please open up some crowd funding option, or go public.. at least one haskell zealot here wants to put money where their mind is!
[Source code](https://github.com/ghcjs/ghcjs-examples/)
Oh, I wasn't recommending *using* Eclipse... as a Vim user that's practically heretical. I was suggesting that if you want to see how to implement these kinds of live coding features in Vim, you might be well advised to see how EclipseFP manages to do what it does.
Turns out it's a keyword in [arrow notation](http://www.haskell.org/hoogle/?hoogle=%28-%3C%29).
whoops: [http://www.haskell.org/ghc/docs/7.0.3/html/users_guide/arrow-notation.html](http://www.haskell.org/ghc/docs/7.0.3/html/users_guide/arrow-notation.html)
Haskell does support mutually recursive modules. It is ghc that does not. */me glares at ghc*. I'm pretty upset with whoever conflated units of compilation with logical groupings of functionality.
http://hackage.haskell.org/trac/ghc/ticket/1409 My idea for a partial solution: http://hackage.haskell.org/trac/ghc/ticket/1409#comment:53
Not in place, but it behaves very well towards the garbage collector. (A sufficiently smart compiler could mutate in place for traversal and change.) But yeah, having a non-tree structure won't work with zippers.
&gt; dil·et·tante &gt; /ˌdiliˈtänt/ &gt; Noun &gt; * A person who claims an area of interest, such as the arts, without real commitment or knowledge. &gt; * A person with an amateur interest in the arts. &gt; [**[Etymology](http://www.etymonline.com/index.php?term=dilettante)**] &gt; &gt; 1733, borrowing of Italian *dilettante* "lover of music or painting," from *dilettare* "to delight," from Latin *delectare* (see delight (n.)). Originally without negative connotation, "devoted amateur," the pejorative sense emerged late 18c. by contrast with professional. Cool. 
FP Complete is in the business of making money, and more power to them if they do. Meanwhile, we really need to put together community infrastructure to solidify hosting and administration of our core resources (hackage, haskell.org, etc.) and ideally crowdfund development on our technical infrastructure (the cabal ecosystem, etc.) as well. All this is not-for-profit and I think a much better use of possible donation dollars from all us haskell zealots. We need to do a little work to get it going, but hopefully in the not insanely distant future, there can be a place you can put your spare samolians, while even getting a tax deduction in the process.
I'd definitely support something like this, where I have little interest (and some potential conflict of interest) in investing in a for-profit business. I also work for an employer that matches some donations to non-profits, as I suspect a lot of people do. Not sure what the criteria are for that, but it's another potential benefit to organization beyond just taxes. Do we have enough business or legal types in the community to handle something like this?
&gt; It is ghc that does not. Yes it can, albeit awkwardly: http://www.haskell.org/ghc/docs/7.4.1/html/users_guide/separate-compilation.html#mutual-recursion
&gt; two breasts with a stab in the middle I don't...?
Thanks to Jason Dagit's work last year haskell.org is now officially part of SPI, so most of the work is already done. One of the big goals for this next year is to shake out the cobwebs from the donation process. Both sclv and I also work for a company that does matching donations, so it has been very much on our minds. =)
A reference to a scene from *Pulp Fiction* in which Uma Thurman gets a syringe stabbed in her chest.
This is fantastic! It's garbage collecting correctly (unlike the previous time I saw this) and the js size is smaller than I expected. But here's the problem: now that I'm all excited to use it again, I'm reminded about how lost I got the last time I tried to build it. Before I sink time into it again, has the install process been improved at all?
&gt; I'm pretty upset with whoever conflated units of compilation with logical groupings of functionality. I agree that the current situation is unfortunate, but, out of genuine curiosity, how do *you* think we should specify what the compilation unit are? It seems to me that while our current situation is less than ideal, in practice having modules be the unit of compilation is more convenient then having to do extra work to specify what each unit of compilation should be. The only real problem is the case of mutually recursive modules, but in practice it is not a lot of work (although I am not saying that it isn't *annoying*) to either just merge the modules together or to break them apart until you no longer have mutually recursive modules, and sometimes this is even *desirable* because it makes the dependencies more clear. By contrast, using some other explicit means of specifying what the compilation units are would require additional work *all* the time, not just in special cases. Another option is to have the compiler determine implicitly what the units of compilation should be by analyzing the minimum strongly connected components (I think that's right; please correct me if I got the term wrong) but this breaks separate compilation since in the worst case (which of course might be rare) you would have to recompile *everything* if anything is changed. Of course, if you have a good idea on how this problem should be solved then I look forward to reading it. :-)
I'll dissent: Mutually recursive modules are something you want to avoid. For larger systems, they can significantly reduce compile times, making builds very slow. As such, I don't think it is a good thing for mutually recursive modules to be transparent. A program that uses them should very explicit about doing so, and should be able to limit the scope to exactly the cycle intended. Otherwise, it would be way to easy for a code base to grow all entangled before anyone noticed, and past the point at which one will disentangle it - but be cursing your compile times. I agree that mutually recursive modules are occasionally the easiest way, and in that case, I'm willing to put up with GHC's solution. It seems to have the right balance of annoyance to keep one from using it often. In all my Haskell coding, I've had to do this exactly once.
Most people I've seen are afraid of new concepts, and I speak as a teacher. It doesn't mean that they can't contribute anything if you help them understand. Hiding behing the “you're not good enough to understand” shtick is, frankly, disgusting.
&gt; Big Endian is easier to debug because it actually makes sense to humans If you screw up rendering the data by putting the highest digit index on the left and the highest byte index on the right then you're going to have problems. Screwing up your data to match your screwed up visualization is backwards. Use text if you want easy to debug and little-endian binary if you need efficiency.
I didn't say *anything* about popularity. There wasn't even such a word in my comment.
I'd estimate that Haskell infrastructure really needs about four full time engineers (or equivalent). That's north of 500k US$ / year. I don't want to think in terms of binary opposition either, but modest, repeated donations won't come close. But a few companies, willing to let a dozen employees (in total) work 1/3rd time on Haskell infrastructure.... Now we're getting somewhere! 
Yes, it turned out keeping the "evaluateOnce" function on the thunks after it had been executed was the problem. This was the [fix](https://github.com/ghcjs/ghcjs/commit/ad00fdd3831640631765584a394e5f7e43e95753). My family were quite confused when I got up from the couch and ran around the house shouting that I had found the memory leak, best Christmas ever! I should point out that blackh (who wrote this Freecell demo), got it working with GHC and WebKitGTK before he even installed GHCJS! All the examples work without GHCJS. If you want to try them out this way there are instructions in the ghcjs-examples readme and you can see it building on [travis](https://travis-ci.org/ghcjs/ghcjs-examples). When you do come to install GHCJS, there is a shell script now that does most of the work. The instructions are in the readme for https://github.com/ghcjs/ghcjs. Please add an issue if you have trouble building it on Linux (you can add one for Windows or OS X builds too, but we are concentrating our efforts on Linux at this point). Unfortunately building the integrated GHCJS seems to be too much for travis to swallow... [I'm sorry but the VM stalled during your build and was not recoverable.](https://travis-ci.org/ghcjs/ghc).
I've hit the wall of mutually recursive modules numerous times, and while it can be avoided, this takes me completely out of the zone. In fact, my current project has way more modules than I'd actually like just to *avoid* having to deal with a massive rearrangement of my module hierarchy, all because of cycles. If GHC could just figure this out, I could get on with my life/job of writing good code.
The problem is the efficient use of time. I understand the hs-boot hack. My haskell implementation of protocol buffers (hprotoc) generates haskell code that sometimes uses hs-boot for mutual recursion. This was how I enforced the desired namespaces. The other choice would have been to put everything into one internal module/namespace and then create a hierarchy of module/namespaces that re-export renamed subsets of the internal namespace. Extending the compiler would take time, and require re-architecting quite a lot about the intermediate files. Improving the performance of all this would again take time. If this much time is put into the module/namespace system then it ought to be used to vastly improve modules (e.g. perhaps more ML-like) rather than merely replace the hs-boot hack.
Great! thank you very much :D
Re Elm replacing guards with multiway-if: Were definitions like this (with no otherwise guard) already illegal in Elm? foo x 0 | 0 &lt; x = 2 * x foo x y = x * y
Is there an extension for Haskell with Elm-like extensible records?
Ignorant idiot here - if explicit specification of compilation units would be awkward, can't it be done implicitly? If it's just a matter of identifying the largest strongly connected components in the graph... Even I can see a problem there, in that the components you identify for a libraries dependencies may be different from the ones you identify for that libraries users (larger set of) dependencies. But then I have the impression that there's a certain amount of dependency hell, so there's a pressure towards per-complete-project builds (with all dependencies included as part of the project) anyway. So maybe you shouldn't be able to *build* a library independently at all - only to do static checks on it. A compilation unit should only be fully defined and built within the context of a full project, when the full dependency graph is known. As I said - ignorant idiot here, but does that make any sense? 
Hugs has an extensible record system called Trex.
Hmm.. maybe we have different emphases in mind. We aren't going to be paying high-quality seasoned functional programmers any significant amount out of community funds, sure. We absolutely need companies willing to let employees work part-time on core infrastructure. And on that count, I think we do a poor job of soliciting, co-ordinating and managing work and contributions from the universe of potential contributors -- though that in itself is hard work. Would giving some people a modest stipend to give them time to do some of that work make sense? Well-typed brought on Eric Kow to do part time work as a parallel haskell community ambassador, for example. 1/3 time is also very ambitious. Clojure/core, for example only has a 1/5 time baseline promise. Community funds are no substitute for what you describe. I'm just arguing that we should find some place to direct them. Even for core infrastructure, note that associate membership in the IHG is only 1000 pounds a year (somewhat over $1600). I'm really just trying to point out where there are any number of people, myself included, who would be very happy to donate to the continued development of the Haskell ecosystem, and it's a shame if we don't try to take advantage of that in some productive fashion.
Just prefix the functions with their module name internally, export them at their original name.
I only have one case where I need it, but right now I use CPP and #include t work around it, which feels gross and has some strange side-effects.
Mutual re-exports will get very messy, though.
I'm also experiencing this exact same issue. Network builds fine but building against it I get the same error you mention. I would love to find a solution for it.
please also see http://www.reddit.com/r/programming/comments/15vsyy/one_persons_functional_programming_take_on_web/
\Not that I know of. However, if it is any help you can simulate an extensible record by just giving the data type a polymorphic field: data MyExtensibleType a = E { field1 :: Double , field2 :: String , extension :: a } Then you can stuff as much extra junk in there as you want. For example, if the field is empty, then you just have: noExtension :: MyExtensibleType () ... if you have a `Bool` inside there, you just do: boolExtension :: MyExtensibleType Bool ... but you don't have to limit yourself to simple types to store in there. You can even store itself: inception :: MyExtensibleType (MyExtensibleType ()) You can also write code that doesn't care about the extension at all, in which case it will be polymorphic in the extension field's type: dontUseExtension :: MyExtensibleType a -&gt; ...
Yes, that was never possible in Elm. Elm does not permit multi-line definitions, so the expressive power of guards and multi-way ifs really are equivalent there.
Technically, Trex is not quite as powerful as Elm's record system. Specifically, Trex requires that you can only add a field to a record that "lacks" the field already. This makes the types of everything a bit more complicated. In practice I don't know how often this becomes an issue though, so the system may be practically equivalent if not theoretically equivalent. [This paper](http://research.microsoft.com/pubs/65409/scopedlabels.pdf) has more info on this.
&gt; Fay, a CoffeeScript analog for Haskell No, Fay *is* Haskell. Just a subset with some extra libraries. 
It's nice to see the algebraic descriptions of this stuff. Statisticians "never" use algebra.
This is relevant: http://www.haskell.org/haskellwiki/Do_notation_considered_harmful
I'm not the author, but in case anybody else is curious - the likely reason that you see things like `%1['setProtocol'](...)` instead of `%1.setProtocol(...)` is to appease the google closure compiler. The closure compiler will rewrite non-string attribute accesses that haven't been explicitly externed. Telling the compiler what to extern requires a special incantation whenever things are compiled. This is a hassle and easy to forget, so this is a great solution to that issue. 
I prefer this definition of `Void` to the `-XEmptyDataDecls` version: newtype Void = Void { absurdVoid :: forall a. a } This gives you `absurdVoid :: Void -&gt; a` which is exactly what you need out of this type for this style of logical programming.
Also, there is a `--closure` flag which will warn about any FFI strings that use direct property access. For example: $ fay examples/jquery.hs --closure Warning: examples/jquery.hs:47:1: Dot ref syntax used in FFI JS code: Fay$$fayToJs(["user","JQuery",[]],$p2).addClass(Fay$$fayToJs(["function", [["double"],["string"],["action",[["string"]]]]],$p1)) Because I wrote: addClassWith :: (Double -&gt; String -&gt; Fay String) -&gt; JQuery -&gt; Fay JQuery addClassWith = ffi "%2.addClass(%1)" 
You are inventing things you expect me to say and ignoring what I've actually said. Is there an actual point to any of this? I'm not sure what, if anything, you're even trying to advocate or demonstrate here.
This is a neat article. I never knew about the theoretical connections, and seeing them with working code is very cool. This is one of those rare articles that expanded my worldview in a lot of different fields. I hope there are more to come.
It boggles my mind that "algebraic statistics" is such a growing and hot field, but they never talk about programming with algebra. It's all about polynomial ideals and solving roots, and I'm not too sure what it even means. 
Thanks! Let me know if you have any qustions.
I missed this one when it came out. This seems to be less like the [Reduceron](http://www.cs.york.ac.uk/fp/reduceron/) and more like [Atom](http://hackage.haskell.org/package/atom/) for FPGAs.
Oops, [here it is](http://research.microsoft.com/en-us/um/people/simonpj/Haskell/records.html)!
It would be greater though, if there was some kind of Java-like virtual machine that would dynamically compile parts to CPU machine language, GPU machine language, and to FPGA programming codes (whatever they are), *during execution*. Dynamically allocating non-parallelizable stuff to the FPGA, and parallelizable stuff to the GPU if no FPGA space is left, with the generic leftovers that don’t fit anything going to the generic CPU. And preferably, they should all be on one single chip, with a large common cache, and a very fast RAM interconnect. I would buy the hell out of such a system, and make a game with *amazing* features *exclusively* for it, to help it gain traction.
Dynamic compilation to an FPGA bitstream is not an option for anything but the simplest FPGAs. It has been a few years since I worked with them but for the then largest FPGAs (such as the Virtex 5) the compilation process for a decently sized program would take a day. Providing a ready-made bitstream with your program is the only option and I think the characteristics of FPGAs are specific enough that it is in most cases possible to determine during development where you can benefit from execution on an FPGA.
Haven't put much thought to it (heading out), but it reminds me of `sequenceA`: sequenceA :: (Traversable ((-&gt;) a),Applicative f) =&gt; (a -&gt; f r) -&gt; f (a -&gt; r) If that's at all interesting.
Yes, the countable package includes "(Finite a) =&gt; Traversable ((-&gt;) a)".
It's based on a haskell implementation.
The basic idea is that if "a" is finite, then "a -&gt; r" is basically a finite tuple of "r". For instance, "Bool -&gt; r" is isomorphic to (r,r), and it's easy to write assemblePair :: forall f r. (Applicative f) =&gt; (f r,f r) -&gt; f (r,r)
I was thinking of using this with an FFI interface when I wanted to build an frp library for fay. I ended up trying to reimplement this: https://github.com/leonidas/codeblog/blob/master/2012/2012-01-17-declarative-game-logic-afrp.md But I couldn't achieve sufficient laziness to get ArrowLoop working. Here's what I ended up with: https://github.com/boothead/fay-frp
I think the only case you can safely use the dot notation is when you have properties that are never imported or exported outside the context you run closure. So for Fay that means that you need string access when you communicate with JS or any external resources (such as the server side), which both use the FFI. 
I was thinking of fay ffi
Interesting. Exp is very "Applicative-ish". The idea is that the Closed/Open structure is *static*, while the "b -&gt; b -&gt; ... r" inside the "f" is *dynamic*. For instance, you can statically list out the "a" values without needing to dynamically evaluate the inner "b -&gt; b -&gt; ... r".
I feel like a bit of a simpleton, but I don't see what you're gaining here by setting up all of this scaffolding. You have an impure function which returns `IO Double`s and a pure one which doesn't. Then you have two functions which sum them and return an `IO Double` and a `Double`, respectively. Is that not how it always works anyway? You are getting this polymorphic base monad which is assumed to be identity unless it's forced to be something else, but since the identity monad does absolutely nothing I don't see how this differs from regular old pure code.
The original question I was answering was not what the simplest solution for `impureSum` and `pureSum`, but how to write them in such a way that you separate out `IO` logic deeply embedded in code so that you still gain the benefits of purity for the rest of your code. Here are some example benefits of keeping the rest of your code pure: * You can reason about what the code does because it is does not cheat and use `IO` * You can test it purely * You can prove the code is correct by equational reasoning The problem he desribed commonly arises when somebody writes an event loop of some sort: if just one step in the event loop does `IO` it normally contaminates the entire event loop and negates the benefits I listed above. However, what `pipes` lets you do is splice open an event loop into modular and separable stages. This means that you can separate out the parts that do `IO` so that you can reason about the rest of the event loop with all the benefits of purity. For example, by factoring out double generation into its own stage, I could replace it with a mock pure stage. That means that I can now test the correctness of the code with QuickCheck because the entire computation is now pure and I can have QuickCheck generate arbitrary test input lists. Similarly, I can look at each stage and verify they are correct in isolation and compose their correctness proofs. Rather than prove that the entire event loop is correct, something which is very difficult to do for larger loops, I can just prove that each stage is correct and then reason about the entire pipeline as simply the composite of each correct stage. I used a trivial sum example so the benefits of this are less obvious, but consider this more sophisticated example: runProxy $ randomsD &gt;-&gt; filterD (&lt; 0.7) &gt;-&gt; takeWhileD (&gt; 0.5) &gt;-&gt; takeB_ 10 &gt;-&gt; raiseK sumD You can tell at a glance what it does because It reads just like a Haskell function composition chain: -- pseudocode "sum . take10 . takeWhile (&gt; 0.5) . filter (&lt;0.7) $ randomsD" ... except that the pseudocode version is not legal, nor can you get it to work and stream in limited memory as `pipes` would. Before `pipes`, a lot of people would try to work around this limitation by using `unsafePerformIO` to pretend that `randomsD` was a pure list so that they could reuse their prelude functions to perform these transformations, but now that they have `pipes` they can still perform these compositional stream transformations because `pipes` lets them separate out the `IO` part from the pure transformations. And if you want to see the value in this scaffolding, the easiest way is to write the example I just gave without using `pipes` in a hand-written loop.
Ah, missed it. Thanks for the clarify! :)
Yes, more or less. I've written an [indexed kleene store comonad](http://hpaste.org/73261) before. (note: edwardk is the author of that hpaste, and the person who taught me how to generalize kleene store comonads to indexed kleene store comonads). What you have there would likely be an *indexed kleene store comonad transformer*. Thus you may find the indexed comonad operations of extract :: (Comonad f) =&gt; Exp a a f r -&gt; r duplicate :: (Comonad f) =&gt; Exp a c f r -&gt; Exp a b f (Exp b c f r) useful in your work. You could probably generalize the `Comonad` constraints to indexed comonad constraints through some tricky type family business. ~~Such a device exists (internally) in Edward's lens library under then name [BazaarT](http://hackage.haskell.org/packages/archive/lens/3.7.1.2/doc/html/Control-Lens-Internal.html#t:BazaarT) however Edward is using a different, but morally isomorphic, representation. I can spell out the isomorphism if you request.~~ Oops; apparently BazaarT isn't a transformer. :(
I read this post out of curiosity and googling "kleene store" results in no explanatory pages. What is a kleene store?
I like the one used in Data.Void, though its similar to yours. newtype Void = Void Void absurd (Void v) = absurd v Absurd really uses the intrinsic property of the infinite loop.
What I'm calling a Kleene store has gone by many names over time. Indeed, I was calling it a Cartesian store before wren suggested that Kleene store was a better name. I first encountered it in [twanvl's blog](http://twanvl.nl/blog/haskell/non-regular1) where he called it `FunList`. It is (isomorphic) to an infinite sum of towers of store comonad transformers: KleeneStore s = Identity :+: StoreT s Identity :+: StoreT s (StoreT s Identity) :+: ... which is more or less like a kleene star operation applied to the StoreT transformer. This simplifies to the isomorphic representation: KleeneStore s = Store () :+: Store s :+: Store (s,s) :+: Store (s,s,s) :+: ... An isomorphic version can be define recursively a newtype KleeneStore s a = KleeneStore ((Identity :+: StoreT s (KleeneStore s)) a) That said, I realize I haven't really answered your question. Assuming you are familiar with store comonads, a kleene store is a a store indexed by `s`^*n* for some natural number *n*.
Thank you greatly.
It looks like extract and duplicate can be written in terms of runExp :: (Functor f) =&gt; Exp a b f r -&gt; f ((a -&gt; b) -&gt; r)
I was like 85% sure that it was already in one of your categorialish libraries...
Oops, not duplicate of course. That is interesting.
`succ :: Enum a =&gt; a -&gt; a`. It's for enumeration. Enumeration oughtn't go in circles. If there isn't a next, it's an error. False is vaguely "before" True.
He does: class IComonad w where extract :: w i i a -&gt; a duplicate :: w i k a -&gt; w i j (w j k a) extend :: (w j k a -&gt; b) -&gt; w i k a -&gt; w i j b class IMonad m where -- for reference pure :: a -&gt; m i i a join :: m j k (m i j a) -&gt; m i k a bind :: (a -&gt; m i j b) -&gt; m j k a -&gt; m i k b
This is a so nice example of why unsafePerformIO is unsafe, should be in any tutorial.
Yup- Bool is defined in the prelude as data Bool = False | True deriving (Eq, Ord, Enum, Read, Show, Bounded) So there's nothing after True
In addition, [the documentation for Enum](http://www.haskell.org/ghc/docs/latest/html/libraries/base/Prelude.html#t:Enum) says the following: &gt; For any type that is an instance of class `Bounded` as well as `Enum`, the following should hold: &gt; * The calls `succ maxBound` and `pred minBound` should result in a runtime error. 
Can you give the applicative code?
 ffmap :: (Applicative f) =&gt; f (p -&gt; q) -&gt; Exp a b f p -&gt; Exp a b f q ffmap fpq (Closed fp) = Closed (fpq &lt;*&gt; fp) ffmap fpq (Open a ebp) = Open a (ffmap (fmap (\pq bp -&gt; pq . bp) fpq) ebp) instance (Applicative f) =&gt; Applicative (Exp x b f) where pure t = Closed (pure t) (Closed fpq) &lt;*&gt; ep = ffmap fpq ep (Open a ebpq) &lt;*&gt; ep = Open a ((fmap (\bpq p b -&gt; bpq b p) ebpq) &lt;*&gt; ep) 
I haven't used `runExp` in any of my work. I've used: study :: (Applicative f) =&gt; (a -&gt; f b) -&gt; Exp a b Identity r -&gt; f r However, I don't know how to generalize `Identity` to an arbitrary functor of some class.
Awesome! Oidalization FTW
But isn't it weird to define True and False as ordered enums? Why not True &lt; False instead? Seems quite arbitrary and hence code smell.
good to know, thanks.
Sure, but you would still need `Eq`, and potentially O(n) work, to find the current position in that list.
It gives you a nice symmetry in some sense. allElements :: (Bounded a, Enum a) =&gt; [a] allElements = [minBound .. maxBound] &gt; allElements::[Bool] [False,True] &gt; allElements::[Ordering] [LT,EQ,GT] &gt; take 5 allElements::[Int] [-2147483648,-2147483647,-2147483646,-2147483645,-2147483644] And so on. How useful this is is up for debate, but it would be handy for brute-forcing a satisfiability problem (particularly where you don't know that the variables and operations are boolean in advance). As to ordering it does allow you to do a few more boolean operations that would otherwise be more complex. x|y|&lt;|/=|&gt;|&lt;=|==|&gt;=| --:|--:|--:|--:|--:|--:|--:|--:| F|F|F|F|F|T|T|T F|T|T|T|F|T|F|F T|F|F|T|T|F|F|T T|T|F|F|F|T|T|T And it's worth noting that x → y ⇔ x &lt;= y (where the former reads "x implies y") and x ⊕ y ⇔ x /= y (where the former is the boolean exclusive or), both boolean operations in their own right.
&gt; Why not True &lt; False instead? Seems quite arbitrary and hence code smell. In a Boolean algebra, a &lt;= b if and only if a = a /\ b. Since False = False /\ True, False &lt;= True.
But what is the motivation for that definition of &lt;=?
In this situation, it's not. (I made a comment to this effect on the blog some while back). When a type has more than one constructor then they differ because you can do: \case (Just x) -&gt; f x; Nothing -&gt; g
I still prefer not defining absurdity at all. Just use negation: newtype Not a = Not { exfalso :: forall r. a -&gt; r }
&gt; Why not True &lt; False instead? Seems quite arbitrary [..] Because identifying `False` with 0 and `True` with 1 allows you to map `x∧y` to ordinary multiplication `xy` and negation `¬x` to `1-x` (and from those two mappings the mapping for `x∨y` can be inferred). And so `False &lt; True` is not such an arbitrary choice IMHO.
The HLearn library generates them using gnuplot. I tried using the Haskell gnuplot library on hackage, but it was too awkward to use because it didn't use the original gnuplot syntax and that's what I already know. So when you call the "plotDistribution" function, it just writes a gnuplot script to a file and then calls gnuplot on the file, generating the plots.
I don't understand what does it represent and how is it useful, can you give some pointers/resources/code examples for a novice like me ?
There *has* to be a better name for it than "indexed Kleene store comonad transformer".
It comes from defining a [Boolean algebra's](http://en.wikipedia.org/wiki/Boolean_algebra_%28structure%29#Definition) partial order using inclusion of subsets. So the above is saying "a &lt;= b" iff a is a subset of b, when considered as sets. If your universe is just True and False, it's the (unique) two element Boolean algebra, so the ordering in the question falls out from that. Seems a bit arbitrary in isolation (and the answer could have been "they just picked on of the options"), but it's actually a specialisation of a general mathematical structure.
Boolean algebras are lattices, so the definition of `(&lt;=)` comes from being higher or lower on the lattice. Boolean algebras are also models of your classical logics, and the `(&lt;=)` relation corresponds with the entailment relation. That is, `a &lt;= b` iff `a -&gt; b`. Thus, we see that `False` must be the bottom of the lattice, since falsehood implies anything: `False -&gt; b` and therefore `False &lt;= b`. Conversely, we see that `True` must be the top of the lattice, since everything implies triviality: `a -&gt; True` and therefore `a &lt;= True`. This correspondence is a fact about logics in general. The difference between classical and intuitionistic logics has to do with negation. In [Boolean algebras](http://en.wikipedia.org/wiki/Boolean_algebra_%28structure%29) you get classical negation; in [Heyting algebras](http://en.wikipedia.org/wiki/Heyting_algebra) you get intuitionistic negation.
&gt;I hope there are more to come. There's definitely a lot more theory to come. I have a data set of beer quality I want to write up a post about too, but there's a lot more theory I need to work through before I'll be able to write about that data.
It would be even more awesome if it existed. :)
But what if an API I need uses them? Last week I wanted to use an HTTP library. I couldn't find one that didn't throw exceptions on failure. Should I catch all exceptions at the API boundary and convert them into Either or EitherT? That's what I ended up doing, but I'd rather the API designer had done this for me.
Requiring `Eq` should be reasonable, since `Enum` defines an isomorphism to `Int`s. `Enum` could really be called `IntLike` to better illustrate how silly it is :)
You mean the following normalized version? join :: m i j (m j k a) -&gt; m i k a bind :: m i j a -&gt; (a -&gt; m j k b) -&gt; m i k b 
Because it behaves like other instances of the Enum class, it has a finite number of nullary constructors.
The [errors Package](http://hackage.haskell.org/package/errors-1.3.1) has some utility functions for this.
yeah
You can convert an exception to an Either if necessary. It is morally sound. You need to be careful about laziness when doing so.
&gt; the goddamn type doesn’t communicate exceptional behavior it seems java methods are more verbosely typed when it comes to exceptions... right? at least in java i can see what exceptions can be thrown from it. is this weird or is there some reason for not exposing then?
For booleans, the cycling function is called "not". not False == True, not True == False, job done. For bounded enumerates without Eq (if any such thing exists) you don't need an "if" - you can convert to integers, do an increment modulo the number of values within the bounds, then convert back. I'm assuming the lower-bound enumerate always equates to zero, but even if that's not true, all that's missing is some offsetting by the integer for the lower bound before and after. This may be worthwhile even if you *do* have Eq. It doesn't sound like much of a gain, and the source may even be a bit messier, but if you use it a lot, modern processors are often slowed down by conditional code (branch prediction failures) but integer arithmetic operations including remainders and modulo are reliably fast. That said, good optimizers may be able to optimize simple conditional code to avoid branches anyway. 
Java methods *can* be more verbosely typed. Exceptions do not need to be checked in Java, and I believe the latest fashion in the Java world is to hate checked exceptions. In the Haskell world, on the other hand, I do not know. Why would they not be exposed? Keeping them implicit seems to be against the spirit of Haskell.
Sure, but I have to know the exception exists first. Nothing in the type `IO a` tells me that it can throw an exception, unless I *always* have to assume anything of that type may throw an exception, in which case that's a big headache.
Ok. Is this the `LambdaCase` language extension? Because it doesn't seem to match up with [what Haskell Prime has to say about it](http://hackage.haskell.org/trac/haskell-prime/wiki/LambdaCase).
You could drop the 'comonad' if there's no ambiguity in the 'Kleene store'. That's savings of 3 syllables right there.
Thanks for such a thorough answer. I do recognize the benefit of sequestering the `IO` code away so that libraries like QuickCheck can be used, I just didn't see how this was really doing it. I think you're right, I haven't been forced into such a corner yet, where I *need* `IO` deep in the guts of my code, so the value of this hasn't been made concrete for me. When that day comes I'm sure you'll here from me again. :) At any rate, it's clear that what is going on here is both clever and well done, so thanks for pouring so much time into this problem. I look forward to seeing where this, and libraries like it, take everyone.
I'm not sure we're doing the same thing. The original question was looking for a function `Enum a =&gt; a -&gt; a` that cycled back to beginning after hitting the end of the enumeration. Note you're starting in a specific place, possibly millions of elements in from `minBound` for some instances, and looking for one more element.
I don't know that it's all that silly. Being in bijection with (possibly some finite prefix of) the natural numbers is precisely what it means to be enumerable. Substituting integers for naturals is reasonable. `Int` is less reasonable, granted...
What's the value of the cake pattern? I have never seen it described as a pattern i.e. solution to a problem in a context. Furthermore why would we want to copy it in Haskell?
I haven't had a lot of time to look into Shake and got further deterred by sentiment that "if Cabal is good enough, don't use Shake". But, ignoring that, what kinds of things does Shake do that make it tangibly better than Make/Cabal?
[Video from ICFP 2012](http://www.youtube.com/watch?v=xYCPpXVlqFM).
I've always seen it described as dependency injection... which seems interesting theoretically for Haskell, but I feel like it's largely solved by pure code. I suppose some amount of injected memoization could help.
Typically the cake pattern is used as compile-time dependency injection. It also makes wiring up interfaces with their implementations easy, at the object-part ( traits ) level. Or being able to extend a trait into a set of composable parts that can be added orthogonally as needed, with each trait building on / enhancing its parent or other traits that may be part of said object As for abstract members vs parameterization.. List[T] is a generic super class of List that can handle types T. When you use it, you say something like List[String]() or List[Int](). Scala uses square brackets for type specifiers. :) This is (type) parametrization. Abstract members are unimplemented members that subclasses must implement. abstract class foo{ def bazzle():String } Since bazzle() doesn't have a body, its a abstract method def. Since the class foo has an abstract unimplemented method, it must be declared abstract class bar extends foo { override def bazzle():String = "Hello" } Here, foo was extended by bar, which provides a concrete impl of bazzle that returns the string "Hello". Scala is expression oriented language, so this works, no need for explicit return. Both of these are still on the object side of the expression problem. :) That said, you can impl type classes in scala. 
Thanks. I still don't get a couple of things though: 1. What is the "equivalent" of traits in Haskell or more conventional OO languages? (In terms of something that gives similar expressive power or modularity) I don't use Scala so whenever people start talking about traits I get hopelessly lost. From what I understand, its kind of like mixins/inheritance, but demanding an interface as a parameter? How does it compare to typeclasses? I understand the point of typeclasses well enough but traits have this OO heritage that just makes everything more confusing to me (in particular because it often comes with analogies with inheritance or mixins, things I avoid using anyway...) 2. As for the abstract methods vs parametrization, I already knew what they meant but didn't express my question well enough. In what situations is parametrization better then abstract members and in what situations are abstract members better then parametrization? In addidion to that, how would you code in "abstract member style" in Haskell? (Haskell does parameterized syle by default with the typeclasses, right?)
It's not *really* an alternative to Cabal. I mean, people have done that. But at heart, it's an alternative to `make`, so what you should be asking is: why would I use Shake over `make`? For a lot of projects IMO, `make` is frankly pretty awful once you get to writing non-trivial build logic with it, which basically means "it always becomes awful," in correlation to software growth (because builds seem to *always* be or become non-trivial for one reason or another.) I'd prefer to have an actual 20 year old programming language at hand as opposed to a 20 year old DSL that enforces tabs :) It also has other nice features like reporting code for builds, which basically shows dominators for build time and a few other metrics. (Note I contributed quite a bit of the original reporting code to Shake, so I'm biased because I like it.) It's also written in Haskell clearly and the idea has seen a lot of real world use. :) EDIT: I'll go ahead and throw in a few points for `make` too: if your project *can* be built in a simple manner (an engineering feat in itself for many things,) your project is quite small, meant to be used on a lot of systems or it's "more trouble than its worth" to get Shake working over make for your potential users, you should consider `make`. I still use it for plenty of my own C projects, precisely because I try and design them to have simple build requirements. And it works well. But it does get nuts once you get to [GHC level](http://hackage.haskell.org/trac/ghc/wiki/Building/Architecture).
I have always believed that if you have two inter-dependent modules then you actually only have one module. If you have a larger cycle then you have an even bigger problem. Where modules form a DAG then you can talk in a meaningful way about higher or lower level modules because a DAG defines a partial order. This makes your structuring clear. Once you allow cycles you have a big ball of mud with no structure. With a DAG you can separate out lower level modules into a reusable element with little effort. If everything depends on everything else then you can't. This Wikipedia article http://en.wikipedia.org/wiki/Cohesion_%28computer_science%29 quotes a hierarchy of cohesion types, from accidental up to functional. The second lowest is "logical" cohesion, meaning that modules are made up of elements that are grouped according to some taxonomy, but don't actually have much else in common (e.g. a math library containing sin, cos etc). If you group stuff into modules in this way then you are likely to need recursive dependencies, but it merely reflects the poor cohesion. Therefore when I encounter a need for recursive dependencies I think about the dependencies between elements in the library. Where they have recursive dependencies I put them into a single module, and where they don't I separate them. This has always improved my designs.
Tip: add to your `.gitignore`: dist/ *.o *.hi cabal-dev/ 
As far as I can tell, twanvl's original funlist (i.e. Exp without the transformer bit) is *precisely* the free applicative over "Const a"! edit: note that free applicatives don't actually require that their underlying "f" be a functor, which is.. odd.
Ah, thanks will do
Gotcha, gotcha! I clearly got carried away thinking it was replacing Cabal. 
The entire typeclass is pretty pointless. 
Well, `runAp` requires a natural transformation to an applicative functor, so that should even it out :P
1) In haskell, I don't know. Type classes kinda-sorta come close, a type can be a part of many type classes, all of which may add functionality. And type classes can depend on other type classes. Traits are very similar to mixins. At the basic level, traits are interfaces which can also carry implementation with them. The idea of Concepts in the latest C++ std ( later removed ) is kinda similar. Mixins are pretty damn close, as is multiple inheritence, though traits avoid some of the mess. trait Position{ var x: Double var y: Double } trait Movable{ self: Position =&gt; def move(dx:Double, dy:Double{ x = x + dx y = y + dy } } class Player extends Costume with Position with Movable class SceneElement extends Costume with Position Trait position defines the state needed to store the position of something. Trait movable, via the self:Position declaration says it can only be applied to classes that extend the Position traut. It updates the position vars, since it know classes that we apply it to will have x/y vars from Position. Class SceneElement has a fixed position ( rocks don't move ), so it gets the position trait, but no Movable trait. You can put it some place, but it can't be moved. ( Really to enforce this, I should make the position x/y vars private with public getters! ). Class Player has a position, but can also be moved, so it has Position and Movable traits. If Objects are protons, Traits are quarks. :) 2) Don't know how to do it Haskell w/o some hackery. You need the notion of inheritence to make it work. abstract Class AutoFactory{ def build():Auto // Abstract method, race cars and trucks are built differently! // So superclass can't really specify how its built, but here we spec the // API } Class TruckFactory extends AutoFactory{ // In order to override build(), return type must be same as overrided method override def build():Auto{build and return truck } } Class RaceCarFactory extends AutoFactory{ override def build():Auto{ build and return race car} } Class Truck extends Auto Class RaceCar extends Auto In this case, if I define a Truck and a RaceCar factory as subclasses, when I call build(), it only returns Auto. I have to KNOW that the particular AutoRactory subclass really returns RaceCar/Truck, and cast as appropriate if I want to use it as a RaceCar/Truck. var truck:Truck = TruckFactory.build().asInstanceOf[Truck] With parametrization, I can do this Class AutoFactory[A &lt;: Auto]{ def build():A // abstract generic method } Class TruckFactory extends AutoFactory[Truck]{ // Due to type specifier, I can now override the abstract method, AND have it // return the proper type! override def build():Truck{...} // Impls build, returns Truck } So here, I have given AutoFactory a bounded type parameter A that says it must be a subclass of Auto. Then in truck, we implement the method so it returns Truck, as specified by the type param. So here we have abstract methods combined with type parametrization. And now we can say var truck:Truck = truckFactory.build() // No need for cast now Also, scala has a better type inferencer than java, so in many cases, you can say var truck = truckFactory.build() and it knows truck is type Truck. Most commonly, we regularly encounter generics and parameterization in collection classes. var autoList = List[Auto](truckFactory.build(), racecarFactory.build() ) So here we have a list of autos. But the most we can say is that when we access them, we get back a 'auto'. var auto = autoList(0) But scala has pattern matching. So when grab a auto out of the list, we can match against its real concrete type! autoList foreach { auto =&gt; auto match { case truck: Truck =&gt; {println("Got a truck!"); truck.haulShit(); println(truck.weight);} case raceCar: RaceCar =&gt; {println("Got a race car!"); raceCar.vroom(); println(raceCar.weight);} case auto: Auto =&gt; {println("Hmm, got some sort of generic auto thingy"); println(auto.weight);} } } Here, we assume the super class Auto has a field or getter called 'weight' that gets the weight of the vehicle. ;)
You actually should listen to the judgements of people you know nothing about. Rational arguments are faceless.
&gt; It can’t be said often enough: Use your own senses. And your own brain. Using my own senses and my own brain, I have come to the conclusion that other individuals are *usually* trustworthy — and enough like me in the relevant ways that it can pay to listen to them. Say, with probability exceeding ½. What, am I supposed to read every book, try every product, because the judgments of others are "meaningless"? You're putting too much stock in a Cartesian kind of doubt.
Author of the post here. I've got a post that I'm working on to reflect on how I chose to handle that. In Buster, I used the errors *errors* package. This was my first time dealing with exceptions in Haskell to any significant degree, and my first time using EitherT or transformers in general. It was *really* painful to deal with multiple exception types being thrown. *http-conduit* can throw parse errors, http exceptions, timeout errors, etc. I'm sure I will refine my approach as I go on but in general, I had 2 approaches: 1. Use the *Script* monad provided by *errors* in Main around code that was all or nothing. If the config fails to parse, there's no file, or a couple other error conditions, Script will print the error message and exit with exitFailure: https://github.com/MichaelXavier/Buster/blob/master/src/Buster/Main.hs#L21 2. In code that absolutely cannot blow up, I converted code that could throw exceptions into code that returned either and used an error logger to log and continue: https://github.com/MichaelXavier/Buster/blob/master/src/Buster/Request.hs#L28 Buster requests in a loop, 1 thread per URL. I couldn't let exceptions bubble up because I wanted to control the display of the error (log with errorM) and 2: it would kill the thread and thus the timer loop would die. The one tip I can add is that you can't just catch *SomeException*. Haskell uses *SomeException* for lots of internal features like signal handling, etc. If you just catch those willy nilly, You'll have all manner of problems. I used this [Pokemon exceptions](http://pastie.org/5479236) idea to let only critical exceptions escape. For my own personal projects, I'm going to try to encode errors in the type (probably with Either) and avoid using exceptions as much as possible, now that I know how maddening they are to deal with! I much prefer users of my code to have to confront errors (or handle them gracefully with EitherT) than to leave it up to documentation. The type system helps so much, it only makes sense to me that it should help remind you when you're not dealing with error cases. 
But if you mapped False to 1 and True to 0, you would get the same thing, just with x∨y being mapped to multiplication instead of x∧y.
It's too bad one can't write class (forall i. Monad (m i i)) =&gt; (IMonad m)
I put a logic library named logic-classes on hackage a while back. I wrote for a project I'm doing. There may be some things in there you would find useful. (Oh, I see there is a build failure there. I'll have to take a look at that.)
I would really see one day an usable (= no `newtype`s) typeclass for monads indexed by categories. That would subsume this kind of monads *and* monads indexed by monoids (which also come up fairly often).
Okay, sure, but just about everything in math is, ultimately, just conventional. There are natural reasons to consider `(&lt;=)` as implication: e.g., if `x` is an element of `A` and `A` is a subset of `B` then `x` is an element of `B`. I'm not aware of any particularly natural reasons to consider `(&gt;=)` as implication. The only ones I can come up with offhand have to do with induction, but then if we consider the standard notion of ordinals, these are really just the elementation or subset inclusion orders in disguise. I'm all for questioning convention, but I don't see any compelling reason to do so here.
I think you had a good.. Or at least interesting idea there..why'd you delete your post? :-$ 
Ah of course, the classic love story of Romeo and *Julia*.
first sighted (or cited) here: https://news.ycombinator.com/item?id=5018518
Actually I'm about to use Ap with f as a witness type (definitely not a functor). Here's an example of the sort of thing: data W :: * -&gt; * where IntW :: W Int CharW :: W Char ListW :: W a -&gt; W [a] Now "Ap W r" represents functions of types such as "r", "Int -&gt; r", "Char -&gt; [Int] -&gt; r", etc.
The statements about Haskell are confused at a number of levels. E.g. &gt; In a lazy language what you need is to make your algorithm co-recursive. Also, `[a]` isn't strictly an inductive type. It models both *data* and *codata* but without looking at the code producing it, we can't tell which. Programming in Agda helps tremendously in clarifying such concepts.
Sure. I just don't really think the "well in boolean algebra False &lt;= True" is *that* compelling justification for the behavior of `instance Enum Bool`. Particularly because classical logic has such wonderful symmetry. One nitpick though: It might make sense to consider (&lt;=) to be **entailment**, but perhaps not **implication**. It is easy to imagine a logic corresponding to a lattice that is not a Heyting algebra, just like we often use categories that don't have exponents, and many people use programming languages that lack higher order functions. 
I find it deeply funny that this guy tries to compare Cabal unfavourably against automake/libtool, strong candidates for the worst build system imaginable. It's true that Cabal doesn't do cross-language interop very well, but snarky, entitled whining deserves nothing more than downvotes.
I would pretty much agree. RWH is a good book overall, but I'd say it's more of a reference/usage example than beginner book. If I was going to learn Haskell again, I'd probably start with LYAH and then move to RWH.
I disagree. The existing C toolchain sucks beyond imagining, so I don't think compiler writers and build system writers should be imprisoned by autotools.
Thanks :-) No, I'm not saying that other languages should do nothing to get better and there's always little things that could be made better, but unfortunately dealing with the C world is hard because it got so many problems and limitations. It's probably not going to change now (the C world), and everyone probably need to live or cohabitate with those problems, but blame need to be attributed properly :-)
Hmmm, this seems to assume that the (abominable) autotools is available on all platforms. On, e.g., Windows, autotools is only available with a lot of installation and pain.
&gt; Also, [a] isn't strictly an inductive type. It models both data and codata but without looking at the code producing it, we can't tell which. I find it kinda funny. Haskell loves to tote safety, but a misuse of a coinductive list as if it were an inductive list is a really hard-to-spot, novel bug for maintainers to deal with.
For the systems you are building with make, do the makefiles follow some formulaic pattern from the source files? e.g. build all .c files in the directory, with the right header files, then link them together? If so, you might be able to automate them all so they all share the same Shake build system, at which point it perhaps becomes worth the trouble again.
All of the above, plus one key thing is that shake lets you add new dependencies as you are building, whereas make does not. For an example see the start of my ICFP paper: http://community.haskell.org/~ndm/downloads/paper-shake_before_building-10_sep_2012.pdf Most larger build systems tend to have a couple of examples of the above pattern, and as soon as you have one instance, the rest of your build system is hacks to paper over it.
Quite a nice feature as well indeed. Conceptually, you "could" with a makefile too, it's just likely to make you want to gouge your eyes out, gives pretty bad integration results and a broken dependencies graph. For example you could re-exec make with a generated makefile by the makefile.
Yeah, thanks for the tip! I'll check it out right now.
Cool, if you give me a shout once it works, i'd love to check it out.
Yes, the problem is you have to separate builds into phases - things that run before you reexec, and things that run after. Sometimes you need more than one reexec step. Once you have this, you no longer have the ability to compose independently written rules, but have to always be juggling phase numbers in your head. As you say, eye gouging then ensues.
I think the main issue the author has is that &gt; if you want to compile a mixed Haskell/C library, you have to compile your .c files with ghc This should actually not be that hard to fix, right? Is ghc doing anything beyond passing the right flags to gcc? If not, then pkg-config could solve the author's problem.
While I agree with your sentiment that downvoting this submission is appropriate, I heartily believe that the author does not whine, or at least, if I were to believe it, I would hopefully force myself to be generous and not to mention it. ;-)
Or check out [GitHub's gitignore file for Haskell](https://github.com/github/gitignore/blob/master/Haskell.gitignore) which has the ones you list and a bit more (e.g. FFI files) ;-)
IINM, codata is not strictly-infinity, merely potentially-infinite. But data is strictly-finite. So `[a]` can't model data because it's not strictly-finite, it's potentially-infinite.
Right. The defining property of codata is to admit case analysis. To achieve this, one need only be just-in-time productive. Of course, Haskell's [a] doesn't characterize that either, as it is quite possible to generate non-productive "values" of that type. In this sort of discussion, it's good to be clear what one expects "model" to mean: characterizing the relevant values, or merely including them?
But… but… that would mean he couldn’t keep his convenient ignorance!
Alternative: never use the autotools.
I'm a bit surprised by the negative, largely unconstructive response here. I certainly have found it bizarre in the past that I had to compile C files with GHC (a true "wat" moment for me) if I wanted them to work properly, so he initially had me hooked on that there may be something to improve here. In particular: * Is it that there's a better way to integrate C and GHC builds? * Do people just hate autotools so much that it's better not to touch them? * Is the way he wants it to work impossible? * Do most people just not care about this aspect of the tooling? I don't read his post as saying autotools is *good*, only that its there and has such a large mass that it's probably not going to budge very easily.
~~Morally, codata is defined by its destructors and is always strictly-infinite, but you can return a sum to "force" it to terminate.~~ EDIT: I was wrong.
It would be nice to solve this, but I've found it a mild annoyance at worst.
I'm really saddened to see this get upvoted so highly. It reaks of smugness. If he's so wrong, why aren't you all submitting errata? For someone like me who is just starting to learn about co-* stuff, just knowing "well this is *obviously* wrong" is absolutely not helpful.
You must be reading different literature than me. Ok, the destructors thing, pretty common, sure. But the strictly-infinite? Not even remotely. Codata is greatest fixed points, at least in all the literature I've read, and that's *not* strictly-infinite by any means.
Sorry, I didn't intend to appear smug. I didn't provide errata simply because I didn't read the article thoroughly enough to provide one that would be truly helpful. Furthermore, I said "not so sure" because I wasn't sure if the author was simply playing fast and loose with the language he uses, or if he truly had problems understanding the material he was working from. As an example: &gt; it means that the set "List of a" is the smallest set such that [] is in the List of a, and if [a] is in the List of a and a is in a, then a : [a] is in List of a. This is not true in Haskell, because you could easily based on that reasoning show inductively that all inhabitants of some list type are finite, when we have many infinite lists in Haskell. This is because Haskell types don't neatly fit into the "data" boxes, because they allow corecursive infinite definitions, or "codata" boxes, because they admit nonproductive terms like `fix id`. Formally, you could define this inductively: [] finite xs finite =&gt; x:xs finite And then if you wanted to prove `forall x : [Int]. x finite`, you could use structural induction on `x`, and prove this trivially, even though there exists an `x : [Int]` which is _not_ finite, such as `fix (1:)` 
I keep hearing that autotools suck, and I frankly have to assume that the people saying this never tried to package their software in the pre-GNU/Linux world of multiple incompatible unices. Nowadays we can take for granted that there is a working Python or Perl on a system, but up until the early 90s that was most certainly not the case. It's not like the autotools are too complex if you grok m4 and shell, and understand that they need to work only with the bare minimum of what was installed on terrible non-compliant systems like HP-UX or Xenix.
Or we just wait for Rust to take over C's place, and hope for the best. :D
&gt; codata is not strictly-infinity, merely potentially-infinite Correct! What I meant by `[a]` modelling data, i.e. strictly-finite, is manually -- via hand-checked proofs! Contrast the situation with Agda. E.g. take `map :: [a] -&gt; [a]`. Given one of the usual implementations, I can prove the type signature, i.e. given data it would return also data, *not* codata. Fwiw, if I were serious about this I'd prove it for `foldr` and try to rewrite everything that has type `[a] -&gt; [a]` using `foldr`, to DRY the proof.
Welll the type signature doesn't say that you map data to data, so I'm not sure if that's what you really mean, but certainly you might be able to prove, for any given definition, that you take data to data and codata to codata. But I'm not sure that's a hugely interesting thing to know.
By model I mean include, and with additional effort characterize, in the same way that one could do typeful programming, albeit painfully, by manually annotating the types in a unityped language. So like the situation with FRP, `[a]` is not a *snug* model of data, because there's junk. I still think of it as a model though. 
&gt; For someone like me who is just starting to learn about co-* stuff The stuff sure is tricky, which is why I reddit'ed this post. Whatever the inaccuracies, it helps to know there are others groping along too. We are all groping, just in different ways and in different places. 
Are you actually being serious about this? ;) Truthiness… the first step to the Idiocracy… You have already walked it.
The difference between a web of trust and a web of lies is security of the physical device and faith that P /= NP.
Of course it’s OK to trust people. I was saying you should stop acting as if it had anything to do with facts or reality. It’s a hypothesis, at best. And usually more like a belief that overrides reality. The problem is that you run around acting as if those beliefs were hard reality, unaware of how much of it is really just not sure *at all*. And I observed that that fact *really scares* dumb people. Also, trustworthiness is not everything. You need to multiply it by the individual good/bad coming from that person. And I have come to a very different conclusion. People turned out to be not really trustworthy, but trying to trick you at every possibility. And I noticed that I’m definitely very different from the average human. So much in fact, that I’m not entirely sure I’m still the same species. That is not a negative judgment. It’s an observation. The average human is mind-boggingly stupid from my p.o.v. I mean the average person stares at me like a cow, or even starts insulting me out of a sheer inferiority complex, when I even mention (quantum) physics, neurology, mathematics, computer programming, psychology / social dynamics, or similar things that everybody of the same species should absolutely know. It’s like talking to a monkey. And again, that is not a judgment, because to me it’s neither harmful nor useful. It simply a plain conclusion based on my plain observations.
As interesting as all the stuff about data vs. codata is, the point of the post seems to be functions rather than values, and pointing out that in a strict language, total tail-recursive functions are efficient, while in a non-strict language productive co-recursion can also be good.
I think I agree, we will remove this and have optional training modules for web development.
As someone who does a lot of (purely functional) Scala programming, I have never found a real use for the cake pattern that wouldn't be handled as well or better just using the reader monad and/or ordinary function composition, argument passing, and HOFs. My theory is that there is a very different style of architecture in use in large-scale functional programs that renders fancy DI techniques like the cake pattern unnecessary. I guess it is useful for certain styles of programming / architecture though.
&gt; Of course it’s OK to trust people. I was saying you should stop acting as if it had anything to do with facts or reality. It’s a hypothesis, at best. And usually more like a belief that overrides reality. Having read many book reviews and many books, I have found that the experience of other persons provides at least *some* information with respect to how useful or enjoyable I shall find a book. I admit that I find your level of skepticism in this regard confusing and fascinating. Yes, I only have an hypothesis as to the usefulness of any particular book review, but this hypothesis has not been drawn out of nowhere—it has genuine content based on my prior experiences. Is the review written well, free of errors? Do I approve of his prior recommendations? And so on. These are inputs to a function continually refined (in a Bayesian sense), and although they don’t usually think of the process in such terms, everyone makes this kind of probabilistic judgment, you included. This isn’t a denial of “facts or reality”, it’s what it means to be a rational animal in the first place. &gt; And I noticed that I’m definitely very different from the average human. I don’t know. To me, you look like any other pompous sophomore who’s Figured Things Out after an introductory philosophy class. You’re not different from the average human, except perhaps in your level of self-deception.
So what's the defining property of `data`, then?
LYAH (modulo the IO chapter) is really good. I never felt the need for another book after that.
&gt; Nowadays we can take for granted that there is a working Python or Perl on a system, No we can't. We *can*, however, document "have a working python or perl on your system". I've been told that autoconf made more sense pre-`pkg-config`, but we *have* `pkg-config` now, and it works really well. The vote against the autotools always comes (for me) when I can write a less-than-20-line makefile that builds the project and it just works, where the autotools failed. The frequency of this is unfortunately high.
Thus confirming my suspicion that "model" was being used in different ways. Thanks for the clarification.
I don't see that Coq and Agda manage to make an improvement over e.g. [Hagino's](http://www.tom.sfc.keio.ac.jp/~hagino/codat.pdf) [CPL](http://hackage.haskell.org/package/CPL), or the famous remarks of D Turner on the present topic. --In Coq and Agda we finally have languages that make totality important, which is an obvious advance, though this seems to be in service of the typechecker; the matter of data vs codata, totality vs productivity, etc., seems to be buried in an irrelevant excess of structure, of which no one can get a survey, thus, e.g. the memorable episode noted [here](http://sneezy.cs.nott.ac.uk/fplunch/weblog/?p=102) . It seems somehow as if the moment for real clarification of the topic was missed; people didn't listen to Hagino and Turner. They go round and round dogmatically on this topic because they lack a simple and genuine implementation -- one that might have made things clear by making it a matter of practical experience.
You're right, I don't know what I was thinking when I wrote that. Sorry.
I'm just glad I can access history here [1] or else I wouldn't have enough of a clue where you're coming from to understand what you're saying. [1] http://www.reddit.com/r/haskell/comments/132kg0/agda_epigram_or_idris_which_one_to_learn/
&gt; while in a non-strict language productive co-recursion can also be good. But this is the exact conflation that's used to (not saying you're doing it here, obviously) banishes lazy inductive types without giving them a chance to state their case. And their case can be summarized in two words: they exist! So whatever theory that's cooked up had better accommodate them.
Have there been any attempts to implement optimal reduction for Haskell? I believe in that case (and only that case) CSE wouldn't change the semantics.
You might want to consider using type classes for the subsets rather than sum types. If the subset has some property in common, you can express it as a type class interface, which should simplify the functions using them. It depends what kind of properties the subsets share though.
I looked at your implementation. Can't you use a GADT to enforce a type equality? data Packet t where AsymmetricSessionKeyPacket :: ... -&gt; Packet AsymFoo SymmetricSessionKeyPacket :: ... -&gt; Packet SymBar and so on and so forth, where `Foo` and `Bar` are the types you want. You can sort of categorize them however you want, and you could even add more indicies in the `Packet` type to add more 'dimensions'. Then you can have functions like: f :: Packet SymBar -&gt; ... and you'd only be able to match the `SymmetricSessionKeyPacket` constructor. This has the downside you need to introduce the phantom types to index on; but if you used `DataKinds` you could declare them as a data type, then restrict the `t` parameter of packet to be of only that specific, promoted kind. This is basically the same as what `kamatsu` suggested. (In my own crypto API, I've ended up adopting [the approach used by repa3](http://neocontra.blogspot.com/2012/12/phantom-types-for-crypto-keys.html) to enforce constraints on key usage. But your case is a little more complex.)
&gt;The problem is that you run around acting as if those beliefs were hard reality, unaware of how much of it is really just not sure at all. If you're going to start arguing epistemology, it gets a lot worse than just things you've been told. There is no sound basis to say that any of your physical experience is "real" either. If you take that kind of argument to its extreme, there is pretty much nothing at all we can truly say we "know". At some point you have stop and assume that some things are true, otherwise you have nothing to work with. Furthermore, you seem to be assuming that the only value in reading a review is an objective reading of the facts it conveys. You can also take a far more subjective and critical reading, and decide what the review tells you about why the person writing has those opinions, and how that might relate to how you would see it. &gt;I’m definitely very different from the average human. ... &gt;The average human is mind-boggingly stupid from my p.o.v. ... &gt;inferiority complex It sounds more like you have superiority complex to me.
I am currently working on a complete makeover for Hayoo. Stay tuned :)
In this case... maybe, but I don't think so, since the property they all share is "being data that is related to each other" and implementation details will probably want to pattern match against the full (or a large part of) the sum in many cases.
I'm a little unclear what kind of subsets you're talking about, and what kind of functions you want to perform on them. Could you give an example?
Can I ask why you wouldn't use them? I've used them for years in GHC quite happily and while major type-checker changes in practice have broken some small stuff, it's never been a huge issue in all honesty (and OTOH, that's not totally fair since some type checker changes made the code *better* and cleaner.) The only reason I abstain from them in `salt` is because I try to keep it Haskell2010, and relatively compiler-agnostic. And the key type is much simpler with only 1 constructor, so the indexes can be created just with phantom types at almost no loss in any way. And yes, this should be equally doable with phantom types and smart constructors. You can recover pattern matching using view patterns, but that again throws you outside H2010 and into GHC-only-land.
Yeah, I'm ok with experimental features and language extensions in experiments, but I really want my production code to be easy to understand and portable, which means sticking to H2010. So, I may not use this solution in this particular case, but it does interest me as being quite close to what I was going for, if I were going to propose an extension to handle this.
Not that I know of. It can increase the costs in cb-need with respect to memory by increasing sharing. http://www.haskell.org/haskellwiki/GHC:FAQ#Does_GHC_do_common_subexpression_elimination.3F I should have been more clear on the "change the semantics" thing. It is just an issue with memory usage (potentially unbound memory usage, but what ever).
this makes me sad. GADTs are **so** awesome that using Haskell without them is really limited by comparison. It might be the right decision for your project, but I really wish people weren't being forced to use a such limited version of the language. In Haskell 2010 the only way to get/fake higher ranked polymorphism at all is typeclasses (And that is not even the moral equivalent of polymorphic containers). In Haskell 2010 there are no existential types, and no way to fake them. In Haskell 2010 you don't get type equalities...not even newtype TEQ a b = Leibniz (forall f. f a -&gt; f b) and so on...
Sure. But the point is that data values are constructed only with construct and codata values are observed only with observe. Data support more observations (catamorphisms) and codata support more constructions (anamorphisms).
It looks like [version 1.4.7](http://hackage.haskell.org/package/logic-classes-1.4.7) built successfully - hooray!
&gt; GADTs are so awesome that using Haskell without them is really limited by comparison. I spend a lot of time working outside of Haskell, and so it seems pretty awesome to me most of the time. GADTs are not a thing I have even missed or seen use for, before now. &gt; In Haskell 2010 the only way to get/fake higher ranked polymorphism at all is typeclasses Yes. I think of all the language extensions, `Rank2Types` probably has the most support from me. &gt; In Haskell 2010 there are no existential types I consider existential types a code smell in most cases.
I see a slight inelegance in double negation with this definition; it becomes Not (Not a) ~ (forall r1. (forall r2. a -&gt; r2) -&gt; r1) as opposed to Not (Not a) ~ (a -&gt; Void) -&gt; Void ~ (a -&gt; (forall r. r)) -&gt; (forall r. r) For some reason the latter type seems more elegant to me, even if they are isomorphic in System F.
I listened to Hagino and Turner, and hence I wrote [this](http://strictlypositive.org/ObsCoin.pdf). I don't imagine anyone's thinking of implementing it, though.
I'm getting confused trying to understand what GADTs are about, since their syntax is so different from what I am used to. Could anyone answer a couple of questions to help me clear things up? 1. If you encoded some common ADTs (List, Boolean, etc) using GADT syntax, how would they look? 2. What features of GADTs prevent them from having a more ADT like syntax?
Existential types are absolutely essential for any type-heavy Haskell code.
Nicolas Frisby tries to address this problem in his [WGP'12 paper](http://www.ittc.ku.edu/~nfrisby/papers/yoko.pdf) ([video](http://www.youtube.com/watch?v=YXN_2fEIHH4)). Another option is [Data.HList.Variant](http://hackage.haskell.org/packages/archive/HList/0.2.3/doc/html/Data-HList-Variant.html).
1) Here: data List :: * -&gt; * where [] :: List a (:) :: a -&gt; List a -&gt; List a 2) Nothing, in fact it's already possible, it's just that it would be much uglier to write. You can do it though. For example, the length indexed vector GADT: data Nat = Zero | Succ Nat data Vec :: * -&gt; Nat -&gt; * where Nil :: Vec a Zero Cons :: a -&gt; Vec a n -&gt; Vec a (Succ n) This can be written as: data Nat = Zero | Succ Nat data Vec a (n :: Nat) = (n ~ Zero) =&gt; Nil | (n ~ Succ n') =&gt; Cons a (Vec a n')
I was planning on writing a longer response to this but there is sitll too much stuff I would need to clear up before I would be able to do so. anyway, I have a quick question here: Is it really the case that Scala forces you to use the same result type in method overrides? I would expect that a language with subtyping would allow methods in subclasses you to return more specific types and receive less specific types.
First off, any 'regular' ADT can be specified 'like' a GADT, but without all their power using the `-XGADTSyntax` extension. Then you can turn this: data Foo a b = Bar a b | Baz a b into this: data Foo a b where Bar :: a -&gt; b -&gt; Foo a b Baz :: a -&gt; b -&gt; Foo a b So it's basically just a more explicit syntax, where you spell out the type of a constructor more like a function's type signature. Second, the easiest way to understand GADTs in my opinion (and answer your questions) is by understanding that the purpose of GADT is to serve witness to a type equality. What that means is that when you see a constructor like: Bar ... The `Bar` carries along a constraint that some type variable is equal to something else. To make this concrete: data Foo a b where Bar :: Int -&gt; Bool -&gt; Foo Int Bool Baz :: Int -&gt; Int -&gt; Foo Int Int Note the types `a` and `b` in the constructor types are fixed. So, `Bar` enforces the equality that `a ~ Int` and `b ~ Bool` (read as: 'a is equal to Int' or 'b is equal to bool.') The `Baz` constructor enforces the equalities `a ~ Int, b ~ Int`. This is why GADTs are more powerful and general than ADTs: because they can specify type equalities, and the return type of the constructor may be 'generalized' to things other than the 'raw' type variables we're abstracting over (`a` and `b` in this case.) To better illustrate this, there is a direct translation from GADT syntax to 'regular' ADT syntax. The above type then becomes: data Foo a b = (a ~ Int, b ~ Bool) =&gt; Bar a b | (a ~ Int, b ~ Int) =&gt; Baz a b Now, we see the constructors carry along the equalities as *constraints* on the value. So when you say something like: foo = Bar then the type of `foo` becomes: foo :: (a ~ Int, b ~ Bool) =&gt; a -&gt; b -&gt; Foo a b foo :: Int -&gt; Bool -&gt; Foo Int Bool This `(~)` operator is the special type equality operator. It builds constraints, just like `Ord a` specifies a constraint on `a`. With `ConstraintKinds` there is an elegant kind for both of these (both type equalities and type class constraints.) But I'll stop there. EDIT: rearrangement and typos.
Interesting, I didn't know you could add constraints like those.
I agree. I find GADTs invaluable for specifying all sorts of static properties; same with `RankNTypes.` Of course, I technically think you can encode *anything* with RankNTypes, and I believe you can basically encode a GADTs properties using a finally tagless encoding of your data type. But GADTs are simpler, lighter, and I think their abstract representation (I elaborated on elsewhere in this thread) makes it clear they're useful for concisely enforcing all kinds of things. Data type validation, 'correct' construction of things like ASTs, and often I find they make code clearer.
Yes, I can't remember what extension you need exactly to use `(~)` (several of them imply use of that operator.) You'll often see it used with type families, since you can say things like `Thing a ~ Bool` in a constraint, where `Thing` is a type family.
GADTs and type families both give you (essentially the same) new power...but it is new power (type equalities) 
Isn't that supposed to be `(B SA)` instead of `(B A)`?
Yes, thanks.
This is ours: # GHC build goo dist/ *.dump-* *.hi *.hi-boot *.lo *.o *.o-boot *.p_hi *.p_o # GHC profiling files .hpc/ *.hcr *.hp *.prof # GHC stub files *_stub.c *_stub.h
I think I understand how this GADT trick is supposed to work now. But would you encode properties that don't fit into a neat hierarchy or aren't mutually exclusive. For example, say we have a datatype data MyType = A | B | C And functions `op1`, `op2` and `op3` where `op1` works on `A` and `B`, `op2` works on `B` and `C` and `op3` works on `A` and `C`. I guess you could use a new phantom type for each property but that would get super ugly really fast wouldn't it? data Yes -- Yes/No ought to be more typesafe but I don't know how to do that yet. data No data MyType o1 o2 o3 where --I'm not really sure about the syntax in this line... A :: MyType Yes No Yes B :: MyType Yes Yes No C :: MyType No Yes Yes op1 :: MyType Yes o2 o3 -&gt; ... op2 :: MyType o1 Yes o3 -&gt; ... The other thing that came to my mind was using typeclasses so the types would be like op1 :: Op1 a =&gt; a -&gt; ... op2 :: Op2 a =&gt; a -&gt; ... But I guess that would probably be a bad idea. Since typeclasses are open for extending with new instances, we wouldn't be able to use pattern matching and would need to implement all functionality as typeclass methods wouldn't we?
Running 'ghc -v3 foo.hs' should get it to dump out the command line it uses to drive gcc/ld. I believe most (or all?) of the info comes directly from Cabal's installed package database, so it might be easy enough to export into other formats like pkg-config.
 data MyType tag where A :: MyType TagA B :: MyType TagB C :: MyType TagC class HasPropOne a instance HasPropOne TagA instance HasPropOne TagB op1 :: HasPropOne a =&gt; MyType a -&gt; ... You still get pattern matching this way. You just can't "prove" to the compiler you won't hit the unreachable cases, so if you have fwarn-incomplete-patterns then you'll want some default cases. You could also use the new datakinds toys to make sure that only e.g. PropIdentifiers can be used for HasClassOne and for tags.
Yep, pick your language: [German](http://de.wikipedia.org/wiki/Romeo_und_Julia), [Dutch](http://nl.wikipedia.org/wiki/Romeo_en_Julia), [Hungarian](http://hu.wikipedia.org/wiki/R%C3%B3me%C3%B3_%C3%A9s_J%C3%BAlia), [Finnish](http://fi.wikipedia.org/wiki/Romeo_ja_Julia), [Swedish](http://sv.wikipedia.org/wiki/Romeo_och_Julia), ...
Yes, you can do that in Scala, and it's done all over the place in the standard lib. 
The more examples I see of `pipes` code, the more thrilled I am to be using it as my default IO library.
Good to hear, I find Zeitgeist to be a bit off base in terms of practicality... but Haskell is in my mind, the most important language for future tech... and capable of so much to leverage the web to push open innovation like - www.opensourceecology.org and www.thefnf.org ... online collaboration and currencies and such :)
They were [discussing a feature that implements this][1] for Rust, though I'm not sure if it's actually been implemented (doesn't seem so from the bug report). I admire the directness of their approach. This might be the most common use case for GADTs, and while in one sense GADTs are far more general, they get awkward for this kind of thing once you go beyond simple cases (i.e. if you have constructors which should be allowed in more than one, but less than all, constructor-sets). The difficulty is really the OR-ing and otherwise-ing inherent in the question, "does &lt;set of constructors&gt; contain &lt;constructor&gt;", which is something that Haskell's type system has heretofore been very bad at (completely incapable of it I think, if you disregard OverlappingInstances). Type classes on the constructor-proxies aren't much good, because type classes are open, so the compiler can't use them to prove very much. I suspect a decent encoding of the idea might be possible with the new [ordered overlapping type family instances][2], with the remaining principal irritation being that you can't refer to data constructor tags at the type level directly, so you still have to come up with some kind of proxies for them. (Maybe I'll edit something in later if I figure it out.) [1]: http://smallcultfollowing.com/babysteps/blog/2012/08/24/datasort-refinements/ [2]: http://typesandkinds.wordpress.com/2012/12/22/ordered-overlapping-type-family-instances/
&gt; How do you wrap 'instantiate this template with your own class as parameter'? Is there any FFI for any language that actually does this? Honest question.
&gt; I consider it as an advantage: it's a spam filter. The extremely high level of competence in /r/haskell is remarkable. 
It does exist, but it is not open. And I can answer questions. ;)
Is there any connection between codata/data and lifted/unlifted types? I haven't dived very deeply into the material here but I notice that if you have the usual newtype Fix f = Fix (f (Fix f)) data ListF a x = Nil | Cons a x type List a = Fix (ListF a) you can also write data StrictListF a x = Nil | Cons a !x type StrictList a = Fix (StrictListF a) where `!x` can be seen as the unlifted version of `x`, not inhabited by bottoms. And `List` winds up being potentially-infinite, while `StrictList` is strictly-finite (yay pun), seemingly parallel to the descriptions of codata and data from the other comments. But Haskell datatypes allegedly conflate data and codata, which I haven't fully grokked yet, and I'm using Haskell datatypes above, so I'm not sure if this is making sense. EDIT - Hmm, of course languages like Agda which distinguish data and codata don't /have/ bottoms... so maybe it's somewhere in that area.
Exactly. It's informal — I should probably clarify it if it's confusing. Without this qualifier, it's not clear what "zero" means, for example. In the article it refers to the identity element, but in the context of multiplicative semigroups it would mean an absorbing element. *update:* clarified that in the article
&gt; Conal Elliott defines torsors in Haskell under the name of affine spaces. Rather, if the group *D* is a vector space, then a torsor over it is called an affine space.
Am I right in thinking that this line add(diff(u,v),v)=v should be add(diff(u,v),u)=v (what's `u` doing there, otherwise?) Otherwise, cool article! One thing I would like to understand is why file patches can't be thought of as forming a group, but only an inverse-semigroup (though I guess the obvious solution is "read the paper!") Presumably, it's because it's not the case that the effect of p;p^-1 is the same as the (null) effect of the identity patch, though I can't think of an example, but maybe I'm just stuck in a darcs mindset.
No, it should be add(diff(u,v),v)=u (in other words, (u-v)+v=u). I fixed it in the article — thanks! &gt; One thing I would like to understand is why file patches can't be thought of as forming a group, but only an inverse-semigroup Frankly, I haven't studied that paper myself and don't know the darcs theory well. But I would guess that the reason is that not every patch is applicable to every file. Hence the composition of some patches becomes "zero" (an error), which is incompatible with the definition of group.
here is the best explanation on GADTs I have seen. [Make sure you watch the video.](http://apfelmus.nfshost.com/blog/2010/06/01-gadts-video.html) 
&gt; No, it should be &gt; &gt; add(diff(u,v),v)=u Ah yes, of course. I assume that I was thinking about diff being symmetric in some sense. &gt; But I would guess that the reason is that not every patch is applicable to every file. Hence the composition of some patches becomes "zero" (an error), which is incompatible with the definition of group. Yes, this sounds right - I had stupidly overlooked the fact that you can't restrict the group operation to those patches that "make sense" to compose - thanks!
Thank you!
Frisby's package on hackage: http://hackage.haskell.org/package/yoko
There's also the (completely equivalent) notion of multiplicative group (in which case the identity element is often called "1" instead of "0"). It's all about whichever interpretation feels more natural to you. For instance, the group of transformations of the plane is almost always viewed as being multiplicative, but it feels much more intuitive to regard the group of differences in this case as being additive.
Personally I felt this [Scheme interpreter tutorial](http://jonathan.tang.name/files/scheme_in_48/tutorial/overview.html) was the most helpful for giving me a grasp of writing software with Haskell. While I didn't understand the precise details of everything that was going on the first time I read it, it unambiguously showed the power of the monadic approach from the beginning. When you can so clearly see the usefulness of something, there's no need anymore to sift through endless tutorials trying to convert you. That said, I read the "Gentle" Introduction first, which I actually really enjoyed, but then again I majored in math, so I think at this point I have an affinity for its particular style, whereas it could be jolting for many programmers.
&gt; In addition to serving witness for type equality, they also allow placing constraints on each constructor. Even your translation to an ordinary ADT depended on the extra ability to place the equality constraints themselves. Of course (I figured that followed from the example, but I didn't mention it.) If you have `ConstraintKinds`, you can even use a single GADT to reify all constraints as a dictionary by doing this: data Dict (ctx :: Constraint) where Dict :: ctx =&gt; Dict ctx The only reason I showed the the translation is because it roughly approximates how GHC would treat the type equalities anyway I think, and the translation is straightforward for people who already understand constraints.
Most likely no. That would bring all the complexity of C++ into that language in addition to the language's own complexity.
Obligatory Baez link: http://math.ucr.edu/home/baez/torsors.html
Richard's new overlapping type family instances ought to clean up my yoko package considerably, ftr.
I would like to up-vote this more. Excellent article. I thoroughly enjoy learning things that would have improved so much of my earlier education! (Reddit ignoramus here. Is there a convention for this sort of comment? À la "+10", but less vacuous?)
Here's what I'd do: * Fitness should be more general and operate over the entire population: '(Monad m, Ord o) =&gt; Set g -&gt; m (Map g o)'. Some fitness functions are inherently parallel. * Remove the IO base and replace it with an arbitrary Monad m. Since you're just using it for parallel evaluation (see previous point) it's very restrictive. * Don't do equality tests on Float/Double values (line 29).
Ah, thanks. Would you consider forking and making a pull request?
All right! These articles keep getting more interesting. HLearn is beginning to look like a library I'll need to add to my toolbox. My only question is how to get all those pretty colors into the histograms, or did you need to add those yourself? EDIT: May want to add a recommendation to `cabal update` before installing the library since the package was updated today. Additionally, the fourth line of the `pdf simonDist`s should read `pdf simonDist White`.
TL;DR: GHC already has a notion of type equalities in the form of GADTs, and other rich extensions offered by its intermediate language, FC^Pro . With other features (like polymorphic kinds and datatype promotion,) we can have a hefty amount of static reasoning to our programs. But unfortunately, FC as it stands does not express a notion of *kind* equality, which would also give rise to things like promoted GADTs. Essentially, type-level programming does not benefit from, or have access to similar facilities we get at the value-level offered by such power extensions (like a notion of 'type level equality.') The above paper basically addresses this issue by collapsing the stratified levels of 'types' and 'kinds' and unifying them, with a dependently-typed-inspired extension of FC.
TL;DR: GHC already has a notion of type equalities in the form of GADTs, and other rich extensions offered by its intermediate language, FC^Pro . With other features (like polymorphic kinds and datatype promotion,) we can have a hefty amount of static reasoning for our programs. But unfortunately, FC as it stands does not express a notion of *kind* equality, which would also give rise to things like promoted GADTs. Essentially, type-level programming does not benefit from, or have access to similar facilities we get at the value-level via extensions like GADTs (such as a notion of 'type level equality'.) The above paper basically addresses this issue by collapsing the stratified levels of 'types' and 'kinds' and unifying them, with a dependently-typed-inspired extension of FC.
There is no guarantee that basic tools will be installed, even today, on AIX, Tru64, HP-UX, or even (sadly) Solaris. And the tools they do have, despite having the same name, will be incompatible in multiple subtle and not so subtle ways. It's not just the toolchain, either. Are you going to force your potential users to install glibc? Because as someone who has wrestled with this before, let me tell you: Solaris libc is nothing like glibc. Not one bit. Sometimes it's better, but usually it's worse. And lots of corporate users are running ancient versions. I saw Solaris 8 just recently, on SPARC. The problem with the GNU/Linux generation is that you guys assume that if it works on your systems, it will work everywhere. But you don't check non-Linux systems and you don't check systems that aren't on some variant of x86. And while these are increasinly a rarity, the reality is that the baroque and unweildly autotools aren't meant to make your life -- yours, the developer's -- easier, they're there to make it easier on your potential users. Singpolyma's 20-line makefile may work perfectly well on his system, but I'll bet you dollars to donuts that he hasn't seen HP-UX's tortured idea of what constitutes dynamic linking. If he had and his project is anything more complex than Hello World, I don't think he'd be so sure of himself.
Thanks. I wouldn't say the library is quite "toolbox ready" yet. There's still some syntax issues I have to work out for making multivariate distributions easy to work with, and that might end up breaking a lot of stuff for the univariate case. Ditto for the machine learning stuff as well. (Edit: that said, everything will be backwards compatible with the blog articles at least until the first major release.) About the colors---yeah, I manually added them in afterward. I'm not sure that there's a good way to automatically pick colors without some sort of domain knowledge, although I'll definitely take suggestions.
I fixed the typo. Thanks.
&gt; I wouldn't say the library is quite "toolbox ready" yet. Even so, it does some very cool things with data, and I'm very excited to see where it goes from here. If nothing else, I'll be forking you on github, for sure.
Does the overlapping tyfams stuff provide some form of type equality check?
Yes, that's the convention. This statement, for instance, would give heartburn: &gt; We do not require + to be commutative to preserve generality, even though it is commutative for our current example. 
Enabling either type families or GADTs enables equality constraints, sensibly enough. I don't think it's possible to enable them alone.
I've found the solution now. For anyone interested: foldr f e (xs++ys) = foldr f e (foldr (:) ys xs) EDIT: I will use this relabelled fusion law from here: f' . foldr g' a = foldr h' b, where f' a = b, f' (g' x y) = h' x (f' y), f' is strict. Consider the fusion law where f' = foldr f e, g' = (:), h' = f, a = ys, b = foldr f e ys. Clearly f' a = foldr f e a = foldr f e ys = b. As well, h' x (f' y) = f x (foldr f e y) = foldr f e (x:y) = foldr f e ((:) x y) = foldr f e (g' x y) = f' (g' x y), so the fusion law can be applied here. Thus foldr f e (foldr (:) ys xs) = foldr f (foldr f e ys) xs as required.
It's completely unnoteworthy, except it sat on Hacker news front page for a few hours Sunday, and probably a lot of people read it. I figured nobody would pay attention here
Not idiomatic. Haskell lacks essential features for typeclass free programming (like module subtyping). Typeclasses are more than just "overloading done right", they provide a very powerful (although a bit ugly) language for type directed code generation. If typeclasses make sense for your module, use them. On the other hand, don't use them if they don't make sense. 
So if this is implemented, does that mean GHC Haskell *values* will still not be dependently typed, but its *types* will be? &gt;:)
There is a tendency, I think, for beginners to overuse type classes in Haskell code where really all you want is an abstract interface.
 unkleene :: Kleene c c a -&gt; a note the double `c`.
I wrote the "Scrap your type classes" post and my opinion has mellowed a bit since I wrote it, but here are some general guidelines for when to use a type class: * A type class should have laws for what constitutes correct behavior for instances. Users of your type class should be able to use these laws to reason a little about how instances of the type class behave without knowing any instance implementation details. * Does the equivalent dictionary require dependent types? If so, then use a type class. * Are there multiple valid instance for any given type? Then prefer a dictionary. * Does your type class have only one function? Then usually the dictionary version is more flexible for users. For an example of this, see the `xxxBy` functions in `Data.List` such as `groupBy` or `sortBy`, which basically are the dictionary transformations of `group` and `sort`. * Do you want to automate things for users and the types uniquely determine the correct behavior? Then prefer a type class. * Do you find yourself needing extensions to get the type class to work? Then prefer the dictionary. * Do you find yourself using newtypes excessively to get type classes to work? Then prefer the dictionary. Out of all the above rules, I consider the first one ("Does it have laws?") to be the most important one and generally the best prognosticator of whether a type class will be well-received by users. Otherwise there is no way that users of the type class can reason about how instances behave without consulting the source code. The laws don't necessarily have to be grounded in category theory (although that is generally a big plus), but just spend a little time thinking about how the type class interacts with itself or other functions and specify that behavior in the form of equations.
no. It means the relationship between values and types will more closely match the relationship between types and kinds. I don't think "dependently typed" has a precise meaning, but certainly $\Pi$ types qualify. We want *those*. In GHC 7.6 you can write the following function (a sort of "return") makeVec :: forall (a :: *) (n :: Nat). Sing n -&gt; a -&gt; Vec n a if we had pi types, you would instead write makeVec :: forall a. Pi (n :: Nat). a -&gt; Vec n a which you could then call like makeVec 3 'a' and get out something like Vec ['a','a','a'] so far we cant do that, and instead have to write makeVec (sing :: Sing 3) 'a' which isn't that much worse, but that is only because type class magic hides the true ugly-ness of the term. If we didn't have typeclasses we would have to say makeVec (Succ (Succ (Succ (Zero)))) 'a' assuming data Nat = S Nat | Z data SingNat n where Succ :: SingNat n -&gt; SingNat (S n) Zero :: SingNat Z this is all a lie...since the built in `Nat` kind has a singelton type that is represented at runtime using `Integer`...I don't even know what the correct term would be. Anyways, the point is real dependent types would be nicer.
You'd get better quality feedback if you describe your plans in greater detail or better yet, hpaste your code. Lots of awesome advice on type classes on stackoverflow. E.g. [1]. My own experience is that existentials + type classes = gigantic code smell, especially if author comes from an OOP background. [1] http://stackoverflow.com/questions/4029455/is-there-a-haskell-equivalent-of-oops-abstract-classes-using-algebraic-data-ty
&gt; Does the equivalent dictionary require dependent types? If so, then use a type class I am a bit confused about this. Typeclasses are not dependent in Haskell. Are you talking about Finally Taggles? Or perhaps you are pointing to the common usage pattern in things like `Map` and `Set` where the type should be parameterized on the ordering relation? If so, than you should know that the "unique instance" property does not hold in recent versions of GHC. I have been meaning to write a blog post about this...but then I get concerned that once I do GHCHQ will "fix" it, which would be a total shame since the hack (it takes like one extension and I think is safe Haskell) lets you do some **really** cool and worthwhile stuff. 
The standard `Num` class hierarchy is pretty close to that. Having values in covariant positions in most of those classes is a *huge* pain, and I think everyone agrees those few methods should be factored out into separate classes (but changes to the Prelude have to come slowly). EDIT: this was a parenthetical remark. I semi-agree with camcann and think that most people should try to *not* write OO code in Haskell so as they can learn the Haskell way. This is only mostly, because OO is just the otherside of the duality and not inherently evil even in pure functional languages. EDIT2: I am at a loss as to what the pain I was thinking of was when I wrote this.
Code and bug tracking: http://hub.darcs.net/stepcut/clckwrks/issues
In what sense are you and camcann using the words "covariant position" and "contravariant position"? I only associate meaning to these words in the context of subtyping.
Are you sure there's dependent types in this proposal? I only skimmed the paper a bit, but from type formation rules I see a System Fomega-ish type theory with an impredicative universe and type equality.
I was using essentially the same meaning as you get in languages with more subtyping (Haskell might not have subtyping, but classes give you something close enough...I mean that in a somewhat precise way..not just that they provide overloading). That is, in `a -&gt; b` we have `a` in a contravariant position and `b` in a covariant position. Same goes for `c =&gt; d` even though that is a different kind of implication. Ordinary types in Haskell don't subtype per-say, but Constraints at least behave as if they do. And I think it is useful to understand typeclasses as presenting a logic with search procedure, whose proof language is OO. That is the type of proofs of `Applicative a` is a subtype of the type of proofs of `Functor a`. Because of the magic of Haskell's self applying forall types you combined with this subtyping of constraints you get what behaves like subtyping of terms, but you have to use contravariance! (forall f. Functor f =&gt; h f) behaves like a subtype of (forall f. Applicative f =&gt; h f) and that is what provides the magic to libraries like `Control.Lens`. 
I went on a rant about Pi types, but it was essentially unrelated to this paper. My point was supposed to be that this paper is **not** about dependency...
Great, thanks! :-) If you have another few minutes to spare (and no worries if you don't), could you explain to me why it is a pain to have values in the covariant position? I still don't quite see what you are getting at.
Very strong +1 for this. As someone whose primary development platform is Windows, I am filled with dread whenever I see a library which is compiled with Autotools or anything which depends on Cygwin. While I agree that Autotools is 'out there' and can, with a (very) following wind eventually be made to work on almost any platform, it is far from doing a decent job of abstracting the platform - at best it just about tolerably abstracts the differences between Posix-like systems, and that is really no longer good enough. A good start would be a build system which 'just works' for Windows, Linux and OSX. With respect, other forms of Unix are now very much minority platforms whereas the three above must represent around 99% of the installed non-mobile base. In addition, I suspect that a system which worked for the 'big three' would be readily extensible to other Unices. Scons is actually about the closest thing I have seen, but it brings quite a bit of its own weirdness, especially for cross-compilation.
Yes, looks like a typo. The one on page 2 (with the GADT-affecting-kind) is correct.
vladley's is correct, but they paper has constructors like `TyInt :: Type Int`, when the data type itself is `TypeRef`.
That is an excellent resource. Thanks!
Nice work! One comment: stating the fusion law this way &gt; `f . foldr g a = foldr h b`, where `f a = b`, `f (g x y) = h x (f y)`, f is strict is misleading because it suggests that fusion always takes place. What's actually true is that **if** I find a h meeting these conditions, **then** I get fusion. Bragging rights to whoever first gives a concrete example where no such h exists. I'll come back in a few days and paste my favorite example. ;) 
Certainly a nice project, but I see nothing new on the linked page, just the usual homepage of the project, already linked a few times here.
Um, no. I'm already enrolled in a Masters program.
I think that's more a matter of the translation into English. The mathematics makes the duality clear: datatypes are initial algebras; codatatypes are terminal coalgebras. Let's unpack that. A functor, `f`, gives you a general notion of structure-with-substructures. The `fmap` operation cashes that out by giving you the means to act on all the substructures whilst preserving the superstructure, a bit like a neutron bomb. You can think of `f` as characterizing "how to make one node". An `f`-algebra `g :: f t -&gt; t` is a way to interpret nodes as `t`s. (Link to reality: think of `f` as offering you a choice of operators and variables; an "algebra" is a way to give meaning to those operators and variables; al-Khwarizmi's "al-gebr", the method of balancing, explains how to work with terms built from operators and variables in a way which respects every meaningful interpretation of them.) An `f`-coalgebra `h :: s -&gt; f s` is a way to grow nodes from `s`s. The datatype `Mu f` is given by the constructor `In :: f (Mu f) -&gt; Mu f`, so `In` is an `f`-algebra with carrier `Mu f`. An `f`-algebra is a semantics for `f`-nodes; `In` just builds the *syntax* of `f`-nodes. As long as elements of `Mu f` are built up from below using `In`, you can interpret them any way you like: fold :: (f t -&gt; t) -&gt; Mu f -&gt; t fold g (In fmuf) = g (fmap (fold g) fmuf) or fold g . In = g . fmap (fold g) That's what makes `In` *initial*: you can replace `In` with any other algebra and get a meaningful thing. Replacing `In` with itself gives the identity. `fold In = id`. Meanwhile, a coalgebra, `h :: s -&gt; f s` is just what you need to build `f`-structures on demand, if you have an element of `s` to grow them from. The codatatype `Nu f` supports the coalgebra `out :: Nu f -&gt; f (Nu f)` which exposes the `f`-structure of the top node, dismantling from above. It's a *terminal* coalgebra, because you can build a `Nu f` value for which `out` is defined, given any coalgebra. unfold :: (s -&gt; f s) -&gt; s -&gt; Nu f out (unfold h s) = fmap (unfold h) (h s) or out . unfold h = fmap (unfold h) . h or, if we have newtype Nu f = In {out :: f (Nu f)} unfold h s = In (fmap (unfold h) (h s)) and (like al-Khwarizmi) apply `out` to both sides preserving the truth of the symbolic equation. Crucially, in order to ensure that any algebra can be used to interpret data, we need data to be built using only `In` in a well founded way. In order to ensure that codata can be built on demand from any coalgebra, we must promise to observe them using only `out`. So "built from below" with initial algebra `In` and "dismantled from above" with terminal coalgebra `out` are indeed dual notions.
Have you spoken to your supervisor(s) about it, then?
But sorting out how to cope with telescopic dependency (x :: X, y :: Y x, z :: Z x y) without type-kind-whatever inflation creates the conditions in which Pi-types can be added, so this paper is a step in the right direction. And Pi-types are the step after it. It'll happen.
Yes I have, and we did come up with a few ideas (mostly programming something in haskell). But we decided to take some more time exploring, and I think the community could help where to look.
I prefer dictionaries to use constraints in function types which I guess is a bit like subtyping. The effect is that a function requires the minimum API on an argument. To do the same with dictionaries is cumbersome. So: - If an API is possible to partition, then using type classes can be simpler.
My idea. There is this language, Frege, which is a subset of Haskell that runs on the JVM. It would be nice to write a GHCI like front end to it and port it to run on Android Dalvik. So I can evaluate expressions on my phone. Not too complicated.
This sounds like a nice idea to hack on as a hobby project, but for a master thesis, not so much. Thanks!
Well, I'd think there was a free applicative given rise to by a functor that remembered the "functorness" of the underlying functor, but freely generated the monoidalness.
Can we please stop using fundamentalist nonsense like “idiomatic”? We’re too intelligent of a community, to think in terms of rigid beliefs. We’re not a religion. Let alone one that deliberately uses Latin/Greek words to compensate self-insecurity with arrogance. Of all communities, we’re one of the *least* who need that. Let’s abolish that unword, once and for all.
I meant the coherency issue, such as when you need to make sure that the same `Ord` instance is used throughout a `Set`'s lifetime.
Thank you, that firmly established a large number of concepts that I've been wrestling with for a while, both in fleshing out the "importance" of f-algebras (and coalgebras) and how that connects to data/codata, recursion/corecursion. I've got to meditate on this for a while now.
I still haven't assessed `ImplicitParams` completely, but the thing I liked about class aliases was that it not only made it easier to write type signatures and combine constraints into one, but it also made it easier to define combined type classes and have the definitions propagate back to the smaller type classes. That would help relieve a lot of excessive type classes that Edward's libraries sometimes have (i.e. `Alt`, `Bind`, etc.). One of the reasons I had trouble understanding `ImplicitParams` is that I never understood the problem it was solving. Based off of what I'm reading and your explanation, it seems like it automatically consolidates dictionary passing (sort of as if it were a `Reader Dictionary` monad, except without the `do` notation) and it also helps guarantee coherency by reusing the same dictionary. Is that right?
&gt; it also helps guarantee coherency by reusing the same dictionary (I reply to this in another post to keep things legible). Okay, so I think that can be solved through the use of a closure. What you want actually is to fix one specific implementation, so that this implementation and only this one is used throughout one specific part of your program, right? Say this specific part of your program consists of two functions: f :: (?doStuff :: a -&gt; IO ()) =&gt; X -&gt; Y -&gt; Z -&gt; IO () g :: (?doStuff :: a -&gt; IO ()) =&gt; W -&gt; IO () You want to be sure that f and g are called with the same implementation for ?doStuff, that's right? So written this way, this guarantee is broken. But if you write it so: myFunctions :: (?doStuff :: a -&gt; IO ()) =&gt; (X -&gt; Y -&gt; Z -&gt; IO (), W -&gt; IO ()) Then you have a tuple of two functions that will necessarily close over the same ?doStuff parameter. You can create a datatype to replace the tuple and hold those functions if you want more formalization. And I think GADTs and ConstraintKinds could be of help, here, to control exactly where the implicit param is known and where it is not. Tell me if this answers your question, or a least provides you some insight.
I get your point, but you have to give a name to "the method that will surprise people the less, and that the compiler will have the easiest time to optimize". "Idiomatic" is not a dogmatic word. We can say it, we just have to remember that it's OK to break idioms if we have reasons to do so (that's how new idioms appear, after all).
Simon PJ works for microsoft research now, IIRC.
Yes, that answers my question. Thanks a lot!
I'm puzzled. Can't you generate a _.a_ library out of your C files (with any build system you want) and then link said library with GHC? I think I did that by the past. Plus, even if I clearly remember having compiled C files with GHC, I also remember I did it for convenience, and I don't think I was forced to do so. What are exactly the problems one may encounter? (as sadly, "doesn't work properly" may not be informative enough)
You're welcome. Be aware that if you do so, the type signature is not optional (so you _really_ need ConstraintKinds if you want to lighten the signatures). f x = x + ?y works, but x :: (?y :: Int) =&gt; Int x = ?y + 42 being a variable, needs a type signature, or else the compiler will complain about ?y not being defined. I assume then that x will behave pretty much like a polymorphic variable (though I've no source to back up this, it is just a wild guess). **EDIT:** ;) Guess what? Just like with polymorphic variables, if you activate NoMonomorphismRestriction you're good to go, even without type signatures. Not necessarily a good idea outside GHCi, though.
Well, that's the irony: Simon PJ has been working on Haskell for years, at Microsoft, yet the video says "don't use Microsoft crap" (to be fair, the video clarifies that it's **C#** Microsoft crap; I wonder what he thinks of F#)
[Part One](http://pragprog.com/magazines/2012-12/web-programming-in-haskell), because I could not find a link to it anywhere on the blog itself.
Neat! I can't wait for `NoMonomorphismRestriction` to be the default for `ghci`.
This has been written a million times, but though it sounds scary, the only potential negative effect of `UndecidableInstances` is nontermination in the compiler. That is, GHC lifts the rather conservative restrictions that allow it to prove termination of the instance selection algorithm, and the proof burden is taken over by the programmer. If you're not careful, you could get GHC into an infinite loop (or rather it will reach the maximum context stack depth and give up, so not even that). But morally speaking, that's as good a compile error as any, and if your code does compile then it won't have any unfavorable semantic properties. (unlike with `OverlappingInstances`)
What are you conflating? Whether or not the result is statically guaranteed to be finite, a function written in corecursive style can be space-efficient under non-strict evaluation, which may be surprising if you are only used to strict evaluation. Strictly evaluating definitions of infinite values is a bad idea, of course, but other combinations make sense.
Yeah, there is no significant news since I made this blog post about the new plugin architecture: http://clckwrks.com/clck/view-page-slug/9/ann--clckwrks-0-13---a-brand-new-plugins-architecture There is ongoing development, but nothing especially noteworthy. Just generally bug fixes and improvements. The current focuses include: 1. fixing any shortcomings of the new plugins stuff 2. improvements to happstack-authenticate 3. factoring the blog/CMS features into a separate plugin and improving them Plus other things in the bug tracker: http://www.clckwrks.com/bugs/timeline Also, I am not the creator of Happstack. It was created by Alex Jacobson (under the name HAppS), and then forked (with permission) by Matthew Elder to become Happstack, and then handed off to me. :) But, I do appreciate the hype :) 
The official bug list is here: http://www.clckwrks.com/bugs/timeline Additional bugs can be submitted here: http://clckwrks.com/bugs/submit-bug Though, to be fair, it is not a very good bugtracker at the moment :) Patches accepted!
The extension itself, while sound, is usually a warning sign. I sort of treat it as the proverbial "canary in the coal mine" that you might be using too much type class magic.
&gt; e.g. : automatically reify an instance into its dictionary I've thought about this. Do you have any ideas about how it could look? I got as far as that you could have data family Dictionary (c :: * -&gt; Constraint) :: * -&gt; * dictionary :: c a =&gt; Dictionary c a and for instance data instance Dictionary Eq a = Eq { (==) :: a -&gt; a -&gt; Bool, (/=) :: a -&gt; a -&gt; Bool } except this would be automatically available from the compiler. And you could write for example dictionary :: Dictionary Eq Int to gain access to it. But this raises a ton of questions: - How do we deal with name clashes? Put another way, where do the names come from? By `(==)` do you mean the one in `Eq` or the one in `Dictionary Eq`? The normal way this works is that names come into scope by declaring them or importing them from modules. So maybe there could there be a special way to import the Dictionary instances (and then you could use qualification to disambiguate)? How would that work? In addition, the part of the compiler that deals with names and imports comes earlier than the one that deals with types and instances. Or maybe instead of importing there would be some special syntax that when applied to the name of a method, refers to the name of the corresponding function in the dictionary? (Also you have to solve the data constructor.) - What about superclasses? What about superclass equality constraints? What about fundeps? - What about classes with different kinds, or different numbers of parameters? And I'm sure there's more...
It's also required for type families that use other type families. This fails without UndecidableInstances: {-# LANGUAGE TypeFamilies #-} type family Foo a :: * type instance Foo a = Bar a type family Bar a :: * type instance Bar a = Int
That's probably reasonable.
Thank you for this, it answers my question perfectly! It seems that typeclasses are the way to go in my case, according to all of your criteria.
This is a great link, thanks!
&gt; Can we please stop using fundamentalist nonsense like “idiomatic”? As I encounter your posts more frequently around here, you begin to seem like a very silly (and ludicrously self-important) individual. Why do you jump immediately from the word *idiomatic* to "fundamentalist nonsense" and "rigid beliefs"? The question here involves a choice between two contested styles that seem (from a beginner's position) more-or-less equivalent in the final accounting. So which one should he choose? The one that'll seem natural to other speakers of the Haskell language. I.e., *whichever style is idiomatic*. No dogma, no fundamentalist nonsense. Idiom as fundamentalism, yeesh. My advisor told me the other day not to put the cart before the horse — a trite phrase, maybe, but perfectly reasonable in (and only in) the context of our shared dialect of English. How should I describe this?
I really hope they're taking inspiration from Drupal rather than WordPress! The ability to configure a tree of interrelated, fielded entities/content types and export it into code that can be tracked through version control would be pretty damn amazing if you ask me.
I get where he/she's coming from. This is thankgodfully not a problem in the Haskell community, but especially in the non-Haskell areas of StackOverflow, there's a tendency for people to answer questions like, "Which way should I do this and why?", with things like "[this way] *because it's idiomatic*." ("shut up and stop asking silly questions, it'll get you into trouble.") It drives me up the wall.
You're welcome!
GADTs are a subset of dependent types; sans the actual data constructors, GADTs can be captured by type classes (e.g., Finally Tagless). Hence, type classes are kinda sorta dependent.
I just released a new version to http://hackage.haskell.org/package/mustache2hs based on `text-format` and `Data.Text.Lazy.Builder`.
There's nothing wrong with using an OO style when it's the most natural fit for the problem domain. Lack of subtyping can be awkward at times, but oh well. The problem is when people 1) use OO style where it's not a natural fit, which includes most day-to-day problem domains, or 2) assume that because something is called a "class" they should use it for OO-style code. The correct way to use OO style in Haskell is with records of functions produced by smart constructors, where the record type represents an abstract base type of sorts. Subtyping is painful at best to simulate, so it's better to use explicit conversions. A canonical conversion from type A to type B makes A a subtype of B, with all the expected variance stuff following from that. `Functor` instances are covariant in their parameter type, function arguments are contravariant, &amp;c. Note that functions like `fromInteger` and `toInteger` are overloaded canonical conversions from or to `Integer`, which is why the former is appropriate in covariant position while the latter is for contravariant position.
As it happens, I recall [a related question on StackOverflow](http://stackoverflow.com/q/9302739/157360). The answer I wrote there mentions a few ways that the concepts relate (most of which have already been mentioned in this thread anyway, I think).
&gt; If you're not careful, you could get GHC into an infinite loop (or rather it will reach the maximum context stack depth and give up, so not even that). Actually, it's entirely possible to send GHC into an infinite loop that never increases the context stack beyond some fixed size. It's hard to do this *unintentionally*, however.
There are very often good reasons for what is or is not considered idiomatic in any given language. That doesn't mean everyone understands, or is even aware of, those reasons, however.
You can get a sneak peek by changing /lec01.html to /lec02.html :) But I'm still working on finalizing the other lectures so please excuse the spelling/grammar/syntax/etc.
Nice!! Thanks for sharing the materials.
I would advise against trying to go into C++ FFI (as the git repo lecture note outline mentions) issues in an introductory course. That kind of thing would get bogged down with C++ related issues more than anything Haskell related (name mangling, templates and inheritance (libraries telling you to derive from one of their classes I mean) being hard to use via an FFI,...).
Hmm I agree. There's plenty of other topics I can cover instead. Thank you for the feed back!
That's great! Can you say a few words about UniversityofReddit? This is the first time I hear about it, and I haven't found any description on its page. In which way does it use reddit, except for advertising (in the good sense) the courses? As I understood, the education platform is a separate website. Can you compare it to other online education systems? (There was at least one other that allowed everyone to create courses, but I can't recall its name now...)
I'm glad you've understood my original point. :P
finally tagless can be implemented with records...
Ooops, I guess there is more than a week between Dec 2 and Jan 5...
Glad to help :D
Is there a way to "T.A." a UoR course?
The conflation is the usual one: "Coinductive types (possibly infinite) are by their nature lazy while inductive types are naturally eager." See http://www.reddit.com/r/haskell/comments/yunln/polarity_in_type_theory/c5z3bmm 
Hey guys! Thought I'd put this up here in case anyone is interested. I wasn't able to get all the solutions working, and would love it if someone could provide some tips (or a pull request) to have all solutions running successfully :-)
&gt; Haskel Must be the Christmas edition ;)
I see that the ListEncoding got 120 potatoes, that's great, hooray for potatoes. Interpretive got OVER 9000 eggplants, that's plentiful. But Senior 3 got 500... something. You might want to add units, because we cannot deduce them from the different graphs, the printed numbers don't relate to each others. :]
A great idea for playing with criterion.
&gt;too theoretical for /r/haskell I am not sure any such thing exists.
You might also like [The Monoid Instance of Ordering](http://brandon.si/code/the-monoid-instance-for-ordering/) if you are interested in getting an intuition for this kind of thing.
Theoretically?
Aren't `ImplicitParams` unscoped/global? With type classes I can say I want specifically the `Ord` class defined in `Data.Ord`, with implicit parameters I can only say I want a `?cmp` function and it can come from anywhere...?
Pull request made. By the way, you probably also want to add the alternative definitions to the mix: `fac'` in 19, 20, and 21 and `fac''` in 20 and 21.
I'm not sure I understand your concern. &gt; With type classes I can say I want specifically the Ord class defined in Data.Ord Yes, because the instances are fixed to types _statically_, and because if you import more than one acceptable instance then the compiler will complain. With ImpicitParams you cannot just _import_ an implementation, you must bind it *manually*, and this binding will **only** be seen higher in the stack, i.e. in the **called** context (see the section about Dynamic scoping in http://en.wikipedia.org/wiki/Scope_%28computer_science%29). So yes you can tell specifically which implementation you want at some point: f x y z = let ?cmp = My.Module.myCmp in sortSomeStuff x y z -- only this line will see the ?cmp value bound just above, the rest of the code won't But you cannot be sure that sortSomeStuff will not rebind ?cmp internally. **But that's exactly the same with classes**: should it be done with typeclasses, you couldn't be sure that sortSomeStuff wasn't using some newtype trick to wrap your type and use another instance. (The kind of trick used in transformers by Control.Applicative.Backwards, which defines a new Applicative instance by using the Applicative instance of the type it wraps)
The connection between Ureddit and reddit lies primarily in the community; it's "sold", if you will, as a 'reddit teachers reddit' / 'by redditors for redditors' Additionally, I think Ureddit classes usually have subreddits associated with them. (for example there was a category theory class there that I poked through that I've just been reminded of...)
&gt; because there was the other ?cmp higher up You meant lower down, right? A broader context will always be lower in the stack. &gt; I fear it would be too easy to bind the parameter at a larger scope accidentally Okay, indeed this is less likely to happen with classes. Maybe some techinques like monadic regions would help with that, by adding a type to the parameter to know where it has been bound. But that's just a quick idea, and it implies some heavy type trickery. &gt; with explicit parameters I can't accidentally pass the wrong dictionary without explicitly writing it wrong. Yes, and then you can make the same mistake: just explicitely passing a parameter that comes from lower in the stack while forgetting to modify it while you pass it up. Considering it less likely to happen is subject to debate (I sure know I have already made that mistake in the past).
Yeah, I like potatoes, or perhaps milli-potatoes, or it might be tempting to use SI for milli-potatoes : french fries.
I might go with the imperial "Chips".
Ah the pull request still had the argument set to 10000 :)
Right, I just uncommented the calls. I figured I'd leave picking the values to you since you're the one publishing the results.
At round the ~19 min mark, you can use `compare` to get rid of the nested `if` expressions: let present = length safelocations case compare present needed of LT -&gt; Command.Get.start file GT -&gt; Command.Drop.start p EQ -&gt; stop
woopsie
I'll go out on a limb and attempt. Let: f x = abs x g x y = x - y f $ g x y = abs $ x - y h x $ f y = h x (abs y) Intiutively, I can reason that I can't get h x (abs y) = abs $ x - y, but I don't know how to prove it.
How about log-scale on the x-axis?
Thanks for the chips-isation, it's much clearer now :)
I think this might work? f $ g x y = abs $ x - y is simply the distance between the two numbers, and if you do abs y then you lose information about the distance that cannot be regained, right?
That's my reasoning. But how can you prove this rigorously?
You could just help out answering questions, like in coursera (they select 'advisors' (I forget the exact title) from the student community who are more advanced than the average). From all I've seen of UoR, it's really informal -- the forum is exactly like this. So you can jump right in.
I'm sure patches are welcome ;)
I've just used Criterion's default output, I agree that a log scale would make a lot of sense.
Something else that also occurred to me is that this line of reasoning presumes an *if and only if*. It's not hard to show that the conditions are enough to give fusion. But do we know if they are necessary?
&gt; _edwardk_: type level ieee floats are a crime against nature. i had to implement them in c++ for template meta programming once. never ever again !!!
Is the last one supposed to say picoseconds?
Just to double check my understanding, that's because reverse [1..] is undefined (is the right terminology bottom?)?
Oh, that's also true: (reverse . reverse) evaluates input, id does not. Forgot about that.
Right, you can't substitute one for the other Prelude&gt; head (id [1..]) 1 Prelude&gt; head (reverse . reverse $ [1..]) ^C 
ahh ok, I had't thought about that. I was thinking about finite lists.
Thank you!
but those are not equal when `[a]` is an infinite list.
&gt; I get where he/she's coming from. Eh. I'd agree with him — or at least heartily sympathize — if he'd phrased it as you did. But he can't seem to make a post without veering off into some diatribe about how people should wake up and throw off the shackles of dogmatism and conformity, which he has been able to do (as he is eager to inform us) through an intense study of philosophy and neurology and so on. So I'm inclined to take his statement at face value and not as well-intentioned hyperbole. It is a personal failing on my part, perhaps, that he irritates me enough that I feel the need to respond.
Nope :)
I think we need a better graph library than Data.Graph, but a less weird graph library than FGL.
That's a uselessly open-ended question. The quality and availability of libraries depends on what you want to do with them, so the answer is different for everyone.
I'm not expecting a single definitive list of every library that Haskell doesn't have. I'm asking for people's opinions.
Indeed. Although, Haskell and some crazy extension magic can probably encode it at the type level. 
Memory profiling is important too. When the memoized version hits memory limits will it still be fastest ?
The regular expression libraries and their documentation are ... interesting. ["XXX THIS HADDOCK DOCUMENTATION IS OUT OF DATE XXX"](http://hackage.haskell.org/packages/archive/regex-base/latest/doc/html/Text-Regex-Base-Context.html)
[Here](http://hpaste.org/80550) is a concrete example of how you can use Agda to formalize and prove such a property.
Is there a cross-platform library for system clipboard interaction, like python's pyperclip?
This is a very broad question. For the kinds of work I've been doing lately, I've been looking for a really good memcached client. There are a couple on hackage, but they don't seem to be updated recently. There's also, as far as I can see nothing about consistent hashing in there, or much documentation about failure conditions. My other big wish with haskell network client libraries (think db drivers, http clients etc) is that they usually don't ship with any kind of instrumentation hooks. For example, I'd like to be able to hook into a db client and get an IO action fired with the amount of time a query took, so I can plot it on a graph (also number of queries a second/etc). Sure, I can wrap my clients to provide these things, but most of them *should* be built in, it encourages the practice of monitoring your stuff as it runs in production a heck of a lot more.
`igraph` look like it fulfilled that, but it's unfortunately LGPL and since the .c code is bound with the library (as opposed to being dynamically linked in a `.so`) and GHC links statically (since it doesn't do dynamic by default yet,) this would seem to stretch to users/clients, too. And many would consider that unfortunate.
I've never used `MissingH`, but cassava seems to have fairly generic dependencies so I'd be surprised if for some reason it didn't work on windows. Curses should come as no surprise - that's not a problem with Haskell, it's a problem with there not being a widely available and fully compatible curses binding for Windows (pdcurses is close, but doesn't have everything. Building it isn't easy either, last I tried.) Various terminal incompatibilities lend to this. And honestly, the reality is that while windows is a staggeringly large amount of the downloads for things like The Platform, the amount of windows hackers is *literally* almost zero, on the GHC side, and the "contributors at large" side. People *use* GHC on Windows, but grade-A Windows support is not their primary motivation. I'm not sure what we can do about this at all, because asking contributors at large to invest even *more* time to fix things they aren't concerned about (or may not even have the resources for) seems unfair. So what do we do? Individuals just tend to deal with things as they arise and move on. There are no heavy hitters fixing problems in this area (and there are many of them.) The real way to go forward to fix it is to get Real Windows Programmers doing it, not unix hackers maintaining it. It turns out that's hard to come by in this community, apparently.
One was created during this past Google Summer of Code [and it works everywhere](http://hackage.haskell.org/package/fsnotify) and the author is maintaining it. He would surely like helpers, I bet.
I did not know about that. Thank you.
It would be nice to be able to *declare* that a function has a property like reverse . reverse == id Essentially; give the compiler access to QuickCheck rules, so that it might be able to perform additional transformations using all of the additional invariants. 
One day I should properly take on Agda. Until then, how does Agda deal with (printing out) infinite lists since its guaranteed to terminate..?
An easy to use smtp library with authentication.
We have the rewrite rules system for that. Here's [an example I wrote a few years ago](http://www.haskell.org/haskellwiki/Playing_by_the_rules#reverse_._reverse_.3D_id).
I figured something like it must exist, thanks for the writeup.
&gt; Basic libraries like MissingH I'd hardly call `MissingH` "basic".
For my part, I'm missing some libraries to handle semantic web data. The only one that does is swish. I'd need triplestores/graph databases or driver to communicate with already existing technologies (like AllegroGraph, Virtuoso, 4store, etc). But this may not be Haskell's fault, as a lot of those libraries are in Java and very bound to the Java ecosystem. And the Java -&gt; JNI -&gt; C -&gt; FFI -&gt; Haskell path is scary enough so when one needs to use that kind of library he may just sigh "oh what the hell, I will just use Clojure (or Scala)". And having fully compliant SOAP server and client libraries would help to integrate with existing systems and workflows. (Some may prefer a stateless protocol based simply on HTTP for web services, but SOAP web services exist and we have to be able to interact with them)
I'm a lecturer myself and this guy gives us all a bad name. He seems to be completely mystified at the use of a computer.
Clearly the reason why edwardk uses Haskell now is because every C++ compiler has a restraining order against him.
Ok, this is terrifyingly awesome. Am I right in understanding that the proofs are 'aside' reverse? Can I use these proofs to later write a HOF that takes a user defined reverse, but only if these laws hold?
I suspect a lot of major improvements to Fay are contingent on haskell-suite, which aims to provide a type checker and other tools to go with haskell-src-exts. Unfortunately, there's a distinct lack of information easily available on what the status of that project is or how one might contribute. :[
Well, there's [JuicyPixels](http://hackage.haskell.org/package/JuicyPixels) as a start...
&gt; Am I right in understanding that the proofs are 'aside' reverse? Yes, that's right. Programs and proofs are all mixed up in that example file (for Agda, they are the same thing!), but the verification bit usually goes into a separate `Properties` module. &gt; Can I use these proofs to later write a HOF that takes a user defined &gt; reverse, but only if these laws hold? I'm not sure what you mean exactly, but what you can certainly do is have things that resemble Haskell type classes, but with specifications. A classical example is decidable equality. In Haskell you have the `Eq` type class, with a function `(==) :: a -&gt; a -&gt; Bool`. It is often assumed that this function should return `True` iff the two arguments are equal, but there is no way to specify that in Haskell. In Agda you can write something like: data Dec (P : Set) : Set where yes : P → Dec P no : ¬ P → Dec P Eq : Set → Set Eq A = (a b : A) → Dec (a ≡ b) This says that something of type `Eq A` is a function `==` that takes two arguments of type `A`, and returns a `Dec` value, which is either a proof that they are equal or a proof that they are not. Then in your code you can convert this `Dec` value to a normal boolean, if you prefer, but you can also use the returned proofs to prove properties of the code calling `==`, no matter what the actual implementation of `==` is. 
Agda separates finite (inductive) lists from possibly infinite (coinductive) lists. If you directly translate the Haskell definition of lists into Agda, you get finite lists. You can use Agda's support for [coinduction](http://wiki.portal.chalmers.se/agda/agda.php?n=ReferenceManual.Codatatypes) to define lists that behave sort of like Haskell lists, but then you won't be able to define functions like `reverse` on them (or even `length`).
I'm only a few pages into this paper but so far very interesting and exciting. I have always been a little unsatisfied with the current methods of graph representation in functional languages, I was hoping to do a little research in this area myself someday. I wonder what the limitations and weaknesses are for this method?
I didn't see an XSD-based XML parser (+code generator), like Java's jibx or jaxb. Pity.
The regular expressions library in Haskell Platform has spotty, worrisome documentation.
Good point! I created this project to play with Criterion, but it doesn't look like there are memory profiling facilities available. I am curious about the usage, so I might try to use the GHC memory profiling tools to create a harness to test each solution.
Yeah, but do people really use regular expressions now that we have `attoparsec` and other mature parser combinator libraries?
In other languages people try to write parsers for non-regular languages using regexps; in Haskell, people write elaborate parsers to match simple regular languages. :]
Yes, you'd need dependently typed languages, which Haskell is not. Off the top of my head: Upside -- only programs guaranteed to halt are expressible. Downside -- not Turing complete. I even believe it may be the case that some halting programs are also beyond dependently-typed power, but I could be mistaken.
&gt; only programs guaranteed to halt are expressible. You can also use corecursion, which describe non-halting programs, so that's an oversimplification. Also, you can enable general recursion (a la Haskell) in a dependently typed language but your type system no longer corresponds to a consistent logic. This may not actually be a problem for "lightweight" verification approaches like Haskell uses, which give evidence for correctness but not proof. &gt; I even believe it may be the case that some halting programs are also beyond dependently-typed power, but I could be mistaken. Depends on what you mean. Take mergesort, which is not, by default, structurally recursive. Typically this problem is solved by separating partitioning and sorting into two structurally recursive subproblems, but you could also solve it naively by taking an additional parameter `n` of type `Nat`, and decrementing it on each recursive call, and returning `Nothing` if `n` is zero, or `Just` the result otherwise. Then you just need to prove that `forall x. exists n r. mergesort n x = Just r`, which is a traditional proof of termination for mergesort. Then just rewrite by that to produce a more sensibly typed `mergesort' :: Ord a =&gt; [a] -&gt; [a]`. 
Yes, this has indeed been the plan for a while.
I knew I should have made more noise about JuicyPixels, but at least I'm going to propose it in march for the new Haskell Platform, now that all the dependencies I need are in the platform.
Looks interesting. Unfortunately, the core parts of the public API seem to depend on unexported types, so it's tough to tell how to use it, except by digging around in the source or copy-and-paste of someone else's code. Poking further, the mystery types are just synonyms: type Action = Event -&gt; IO () type ActionPredicate = Event -&gt; Bool type EventChannel = Chan Event It would make the library a lot more sensible and approachable if those type synonyms went away -- or were exported so you could tell what they are without reading the library's source code, but honestly they don't really add anything anyway.
We need a 2D game library like Lua's Löve2D, Python's PyGame and Pyglet, C++'s SFML, Java's LWJGL etc.
There's a SFML binding being worked on, FWIW: https://github.com/jeannekamikaze/SFML
There's algo ghcjs, that actually plans on supporting the whole language, unlike Fay AFAIK.
Yes, you can put proof obligations of arguments: use-someone's-reverse : (rev : {A : Set} -&gt; List A -&gt; List A) -&gt; ({A : Set} -&gt; (xs : A) -&gt; rev (rev xs) === xs) -&gt; rest of type signature use-someone's-reverse rev involutive-proof ... = ... It's quite common to put properties like that in a record, see for example http://www.cse.chalmers.se/~nad/listings/lib-0.6/Algebra.html#1 in the standard library: follow the links IsSemigroup, IsMonoid etc... 
To be fair, the included batteries in Python tend to suck, and most people use third-party solutions anyway.
I've made some progress on this during the summer. Haven't gotten around to make a proper release yet. [Github repo](https://github.com/jaspervdj/firefly).
Actually, what I was shooting for was less "libtool is correct because it's old" and more "libtool is hard to shift because it's old, don't repeat its mistakes".
It's more that I believe autotools have so much inertia that you have to play nice with them. They're warty and easy to misuse but they have improved massively (automake in particular) over recent years.
Oh, I can easily knock up a 20-line makefile that builds the project and "just works" on my machine, but then try writing in the support for all the stuff the GNU standards expects: things like `make install DESTDIR=/foo/bar` and `make install prefix=/foo/bar` (which is completely different and hugely important for e.g., `stow`). Then there's `make dist` and `make distcheck`, getting dependency tracking right (across several compilers) and suddenly automake is starting to look pretty good.
I actually much prefer flip to a function having its own name. It doesn't make sense to me to have two functions that do the exact same thing but name them different things. In doing it that way, instead of needing to know n + 1 functions, I need to know 2n. I think I remember a function called pam that was the map function with arguments reversed. I saw it and thought "wtf is pam??"
&gt; I think I remember a function called pam that was the map function with arguments reversed. I saw it and thought "wtf is pam??" It's a spray you use to grease up a list so that the function you're applying doesn't stick. Duh. 
I see that `for = flip map` (by analogy with `forM = flip mapM`) has been proposed many times before. Don't know if it has actually made it into standard location.
I sometimes feel like explicitly permuting arguments (`\x y -&gt; function y x`), rather than using a combinator (`flip function`). Currying and uncurrying often feels redundant too.
Just out of curiosity.. has anyone written up guidance for ordering arguments in Haskell? I know when I think about argument order, I'm always considering the order that would best make sense for partial application.
That's what I do, too. I order arguments by "variability": the more sense it makes to consider an argument variable, the later it should go in the argument chain. (Or, reversed logic, the more constant an argument, the earlier I put it).
There's also the thing where if this is a function that is part of a cluster of functions operating on a common data structure, the data structure comes last in the arguments. Sort of like map and fold accept a list as the last argument.
Your ambiguous use of f is making my head spin. I think I understand everything here except for this equality. Can you clarify how you are doing this and which f is which? f x (foldr f e y) = foldr f e (x:y)
I'm not sure if `for` is, but `forM` and `mapM` are both in the standard (`forM` being a fairly recent addition, iirc).
Xml libraries in general seem to be pretty bad, strangely. There are a lot of them, but none of them are great. I'd like one that uses Text instead of String, is fast, safe to use (no runtime crashes when asking for attributes of a text node etc.), can do xml validation (I don't really need XSD), and can also do streaming parsing. Ideally it would come with picklers (see hxt) and an XPath implementation.
Are the slides used in this talk available online?
Note that, even in agda, you cannot have a term of type ‘reverse . reverse ≡ id’, since that would require function extensionality. So even in an expressive type theory like Agda expressing higher-order laws is quite annoying... This doesn't prevent you from proving ‘∀ {xs} → reverse (reverse xs) ≡ id xs’, but you won't get rid of that quantification.
Aye, sir. We need to buck this trend of writing programs for one platform. Now that you mention it, Hackage's random number generating packages have poor Windows support; it's quite difficult to understand how to use the API to write cross-platform Haskell programs that use random numbers.
It also contains a thousand other things. Off topic: are you aware of the `split` package?
Flipping is a smell if the function is flipped before use in 51% of its applications.
Ahh, much better. Makes perfect sense now!
https://github.com/chris-taylor/LondonHUG
hinotify works on all the BSDs, so it should work fine on OSX. You just need to install libinotify. https://github.com/dmatveev/libinotify-kqueue
A GUI library that is easy to install would be on the top of my list.
&gt; The real way to go forward to fix it is to get Real Windows Programmers doing it, not unix hackers maintaining it. It turns out that's hard to come by in this community, apparently. My experience as a package maintainer is that Windows users complain the most and contribute the least. I always try to keep my stuff portable and I'm always happy to get a patch that fixes something that broke on Windows, but I haven't had a working Windows machine for more than six years now, and I'm unwilling to invest the time needed to set one up.
51% of its partial applications, perhaps. If you're giving all the parameters, you probably don't flip the function, but you wouldn't have flipped the function with the other order either. There is one case that's tricky. Functions with functions as parameters often have more appealing properties where if you place the function parameter first, partial application becomes meaningful in its own right. For example, `map` turns `(a -&gt; b)` into `([a] -&gt; [b])`. On the other hand, if you expect to write a complex lambda inline, it's *far* easier to wrap the line meaningfully if the lambda comes last. This is the case for `forM` and `mapM` for example... `mapM` is theoretically nicer, but the whole reason for `forM` is that you can say things like: forM myEnemies $ \enemy -&gt; do insult enemy spitOn enemy kick enemy runAway In this case, having two names is sometimes justified.
&gt; needs its own name Nah. &gt; is defined wrongly Possibly. It comes down to what other have said: If you use flip 51% of the time, then yeah, it's defined wrongly. &gt; (people just add flip to make it compile) I've got some torches if you've got the pitchforks. :) Basically, flip is a convenience only and isn't technically *required*, ever. Use let/where if you really don't like it!
Ever heard of [Factor](http://factorcode.org/) / stack programming? This sort of thing is done so often that several special flip-like functions are available as stdlib.
Regular expressions have different semantics from backtracking parsers?
Getting your arguments (properly) passed to words depends on those stack operators. So they are 10x more necessary than they are in Haskell.
While were shouting out wishes: more AWS support would make a lot of startups very happy.
Is that the Abuse monad?
That one doesn't include `runAway`, so you have to use the Flee monad as well.
Would a regular language parser written idiomatically in attoparsec actually be nonlinear though?
Yup! I was surprised.
Evaluates input? If you're just interested in equality, all you have to worry about in this case are 1) are they actually equal (`reverse . reverse` and `id` are not equal for infinite lists), and 2) do we have extensionality? (ie: `forall x, f x = g x` implies `f = g`).
[Guidelines to maximize partial application](http://stackoverflow.com/q/5863128/83805).
What specifically are you looking for?
And the quantity of said libraries.
Someone please, please, please put 'for' into the Prelude :-)
Well, aristidb's library is a good start, but its sort of dated. Keeping it up to date and adding support for the modern libraries and the modern services. For example, SimpleDB is supported but DynamoDB isn't.
I just install the latest version of the haskell platform via their installer whenever it's available. I haven't hit very many problems with doing things this way (the only issue I have had was a ghc on 64bit osx issue, and it was extremely minor).
Yup, start here: http://www.haskell.org/platform/mac.html Just about everything you'll need to get started. There are lots of great plugins for whatever text editor you might use: * https://github.com/SublimeHaskell/SublimeHaskell * https://github.com/haskell/haskell-mode * https://github.com/lukerandall/haskellmode-vim * https://github.com/scrooloose/syntastic/ * https://github.com/eagletmt/ghcmod-vim * https://github.com/ujihisa/neco-ghc (I use vim personally, so I know of a few more for that :D)
I use homebrew, since it's easier to upgrade the whole thing that way.
I'd agree completely. I think the maintainer ended up getting a job during the GSoC, and so the library wasn't as fleshed out as it might have been otherwise. So I think it could definitely use a lot of improvement, but otherwise I think it's a good base. The lack of examples is annoying, though.
These plugins might come in handy, thanks!
The algebraic encoding of a list is: L(x) = 1/(1-x) and its derivative is L'(x) = 1/(1-x)^2 = L(x) * L(x) but the list Zipper data Zipper x = Zip [x] x [x] has the algebraic encoding Z(x) = x * L(x) * L(x) Shouldn't `Z(x)` be equivalent to `L'(x)`? Why aren't they equivalent?
I think you can include code blocks by starting off with 4 spaces. I am do not know how to do it inline though. (`f` x) (`runStateT` myState) 
I would like to use only one package manager for clarity reasons. Which advantages does homebrew have comparing to macports? 
Because Haskell's `return` isn't quite confusing enough to newcomers? :-)
I was half joking, but I still have a few reasons: - I often define it myself in utils modules of projects and use it fairly often. Good for big-body inline lambdas. - this idea comes up every X weeks on reddit. It is obviously a good fit for prelude - why not just add it?
That's because a derivative gives you *one-hole context*, that is: data ListContext x = Context [x] [x] Zipper is then simply a value with its context (which tells us the position of the focus inside the whole structure). data ListZipper x = Zipper x (ListContext x) But this gives us: ListContext x ~ [x] * [x] ListZipper x ~ x * ListContext x ~ x * [x] * [x]
There is a haskell "platform" package in the macports DB, it is what I installed and haven't had any problems. Not sure if it is kept as up to date as the hack age version. Works for me, YMMV.
Oh! So a zipper is a with its context, not just its context. That makes sense.
I can't seem to run 'cabal update' on cabal that comes with the macports haskell-platform package because cabal is outdated. And I can't seem to update cabal because it installs a new version in a different directory. You don't have these problems?
Yes. Prelude Text.Parsec&gt; runParser (many (char 'a') &gt;&gt; char 'a') () "" "aaa" Left (line 1, column 4): unexpected end of input expecting "a" Prelude Text.Regex.Applicative&gt; match (many (sym 'a') *&gt; sym 'a') "aaa" Just 'a'
It's not hard to give a (contrived) example of a regular expression which cannot be efficiently recognized using backtracking (see e.g. [Regular Expression Matching Can Be Simple And Fast](http://swtch.com/~rsc/regexp/regexp1.html)). It's hard to judge how often such cases are encountered in practice — I'm not aware of any research about this. But with regular expressions you don't even need to think about it — whatever you write will be linear in the length of the input.
I wish! I've been wanting to learn Haskell for a while now, tried a couple times but the syntax is so... weird (I know Python). But I am in the twin cities! If there's a meetup, I'd like to go and listen to people talk about it.
I've tried xml-conduit a few times wanting pretty simple usage, similar to Aeson's simple parser interface, and I've always gotten incredibly lost since it adopts the obscure (to me anyhow) notion of axes for parsing XML. It is based on XPath though, I think.
Take a second look at fmap's type, as well.
There may be some 'beauty' to that organisation, but it can harm usefulness. I want to look up a bunch of keys from a Map more often that I sit back and admire the shape of the function call and the fact that the "constant" value in that partial is last is annoying. Partially applicability should trump structure complexity ordering, I think.
Nice practical application, which helps bridge the gap between the theoretical side and practical matters. If I read another "practical" article using a calculus of simple lambda terms as their example, I will say some naughty words. So URL matching is nice to see. That said, I felt like this required a lot of work to read, but I'm not sure how it could be made easier in a clear way. I prefer longer articles, so this is in my ball park. But it was a lot of code to wade through and I'm still not sure the structures came to the fore in amongst the details. Will read it again later to see if that stuck with me. Still, a worthwhile effort and I enjoyed the motivations. I am a little bit smarter and hopefully a better coder for having read it.
As fluffynukeit says, if you're doing `flip f` more often than `f`, maybe the argument order is not the best. Nevertheless, there is one annoyance here: record accessors are routinely used as the shortcut to define functions like `runReader` or `runState`, which I believe then end up with the wrong argument order, so you end up with code like this: doBlah init = flip runState init $ do { ... } Tekmo's trick in this thread is a variant: doBlah init = (`runState` init) $ do { ... } I like to think of this in terms of a linguistics concept of [heavy NP shift](http://en.wikipedia.org/wiki/Heavy_NP_shift); the "heavy" argument to any function "wants" to be the last one—and little is heavier than a `do` block or a lambda. cdsmith and others have mentioned this informally in their comments. Another alternative that bears mentioning is that there are many "local" cases where you want the opposite argument order and the simple solution is just a local definition in that scope—either a `where` binding or a module-private definition. Overall, what I see as the larger issue is what to put in the standard libraries—e.g., the argument here about whether `for = flip map` should be in the `Prelude`. I can always write my own module of my own private utility functions like this, but I feel like that only that helps my own productivity, and hinders newcomers to my code.
Thanks. I'm trying to find the right balance between saying too much and saying too little. It is definitely a longer article, and a bit heavy on code -- a lot of which isn't that interesting. *I* definitely learned a lot writing it, hopefully some people benefit from reading it.
I definitely don't want to criticise it too much, because it *is* hard. So I'd only say that there might be some alternate presentation of things like this that would make it even more awesome and I really want to encourage you to keep searching for it by writing more like this!
The question is how you escape ticks inside an inline code segment. That's the part I could not get correct.
Your call to `cabal update` successfully completed, it's just letting you know there's a new version available. Installing haskell platform via the installer, homebrew, or macports likely isn't going to make a difference. This is because the `cabal-install` on hackage is a later version than what's provided in the latest haskell platform. And that's okay, you don't *need* to upgrade cabal as most packages are compatible with the haskell platform version. I installed through homebrew and get that note as well, but I've never run into any cabal version issues. If you still want to upgrade your cabal, the steps you've taken so far are correct. Except, you want to add `~/.cabal/bin` near the beginning of your `$PATH` environment variable to make sure you're running the correct cabal version. It's also convenient if you happen to install any other binaries via cabal. 
Funnily enough I had it figured for fmap, but not map!
I don't think that's true. For small little prototypes, the built in JSON, logging and command line option parsing all work great - and we don't have any of these. This is just off the top of my head too, and I'm not a Python programmer. I'm saying we copy Python feature-for-feature, obviously we can do it all better ;) But there's a lot we're missing, that's for sure (in the platform).
If it helps, I find that when my post is getting too long I try to split it into multiple separate posts. For example, with this post you could split it into several topics, each of which is worth its own post: * How to route using a parser (i.e. `StateT s Maybe`) * How to route using a free monad * Compare and contrast `free` and `operational`
Couple of silly questions here 1. The capture function lets we dynamically specify the URL structure, depending on the value matched. Can someone show me an example where this is useful (I'm kind of a webdev noob so I can't think of anything right now) ? I ask this because I think that if the structure were static, we could avoid some of the recursion complexities that show up and that Free/Operational are supposed to help with. 2. What are the Term constructors about (and why does Route need to receive a type parameter)? Why can't we do fine with just Match, Capture and Choice constructors? 
I'm in Wisconsin, but this might be the closest thing. If something goes down, let me know! 
Definitely, if you have any friends who might be interested please bring them too!
Yeah, a zipper is always a comonad.
Put your inline code in double backticks ``like this (`f` x) lalala``.
I like to get newcomers to Haskell to this point as soon as I can. It is one of the first "mind blowing^Wexpanding" moments.
Or you could just use http://hackage.haskell.org/packages/archive/acme-inator/0.1.0.0/doc/html/Acme-Inator.html on you enemy...
The `flip` in the definition of `Applicative` for Kleene stores seemed like smell to me, especially because there are always at least two definitions of applicative, one "forwards" and another "backwards". With Kleene stores it is not really obvious which one of the two definitions is "forwards". After lots of thought, I finally decided the one with `flip` was, in fact, the forward direction. One key insight was that this direction was lazier. You can see this in Twan's definition of Kleene stores, which he calls `FunList`. In his [first blog post](http://twanvl.nl/blog/haskell/non-regular1) he get's his applicative definition right, but then in his [second blog post](http://twanvl.nl/blog/haskell/non-regular2) he switches to the backwards definition, due in part to the the use of `flip` in his first definition, when in fact `withImpure` should be: withImpure imp (More a f) = imp a &lt;**&gt; withImpure imp f which is completely non-obvious.
One would need to be careful with such rules of course. I can imagine a compiler where such a transformation was added, as well as an optimization for non-termination to `sleep infinite`. Of course, to cause sleep to be chosen, you'd assign some negative cost associated with it, to prefer sleep over non-termination that loops endlessly. In such a case, the program "`1`" can legally be rewriten to "`head . id [1..]`" to "`head . reverse . reverse [1..]`" to "`head . sleep infinite`" to "`sleep infinite`", which has lower cost than the original program of "`1`". Ok, that was absurd, but one can imagine more reasonable rewrite paths and identities that could get you into a similar bind.
Yes. However, the penalty you pay is that operations near the head of the list are more expensive for the codensity version. This is why `pipes` uses an ordinary free monad, because these operations dominate the cost of composition.
I haven't used macports, but I was told by someone on IRC that Homebrew has more packages and is being updated more frequently. I don't know if that is true, I just happen to choose homebrew over other package managers at the very beginning of my Mac-owning period.
Set something up! You might also advertise in the clojuremn google group.
Thanks, I'm gonna give this a try tomorrow
simplejson and argparse are alright, but also recent additions. The logging module has lots of problems due mostly to the use of globals, and is why Logbook was created. wsgiref? Everyone uses webob or werkzeug. urllib2? Use requests for HTTP instead. Then there are all these libraries in the stdlib that are just plain broken, like shlex which can't handle unicode. There *is* useful stuff in there too, but let's not oversell it. ;-)
The outside "tube" represents the "monadic context" that the interior operates in. I was planning on doing the State monad if someone was interested in that also. "Bind" connects the outside tube of the monad context to the interior "function squares" which are pure. While, "return" takes the outside tube and combines it with a pure value to give that value with the monadic context. 
1. This is why there is also `match`, which is static like you suggest. Capture is necessary in dynamic applications where you need to for example read a number segment into a blog post id, as in `do match "posts"; n &lt;- capture readMaybe; post &lt;- query $ GetBlogPost n`. 2. Term corresponds to "return" in the monad. It is necessary for holding values in the `Route'` type without side-effects. Similarly, the type parameter is necessary to make a monad, as every monad is kind `* -&gt; *`, i.e. takes one parameter. Consider `return`. It is allowed to take a value of any type, and put it inside our monad (`Route'`). How would that be done without the type parameter to hold the type of the "contained" value, and where would the value go without `Term`? Monad isn't itself necessary for routing, but routing wouldn't be very useful if we couldn't return a routed `Response` at the end, and also for building that response we may need other monads like IO for database queries.
Works great. I wonder if it's somehow possible to make this work for a `broken' file (like when you're in progress of writing a function and it doesn't typechecks yet, but you want to know the types of something you're calling).
Yes! Learning bit by bit. Saw a dude at a cafe with a lambda sticker too. There are at least a couple.
That's a funny looking burrito.
Thanks for this post. Real world examples for these constructs simply make them way more useful. Keep up awesome tutorials like this!
Operational has O(1) composition while allowing O(1) operations on the head. If I remember correctly, the problem with the free monad implementation is that this is not possible. You can slap a codensity on it to trade one O(1) for the other, but it's not possible to slap something else on it and get both. Essentially, if I'm not mistaken, the issue was that free will also do substitution with multiple continuations (think `mplus`) while the operational approach is strictly a linear list of instructions. Doesn't make much of a difference when writing interpreters, but does affect possible optimizations.
Disclaimer: I'm the author of the `operational` package. :-) &gt; The operational monad provided benefits similar to the Free monad, but with a nicer interface. The operational monad is reportedly nearly isomorphic to the Free monad. So, it should be able to do almost everything the Free monad can. I used to think the other way round, namely that operational is more general than the Free monad, but apparently, they are actually equivalent in expressiveness. An interpreter written in one approach can always be translated into an interpreter written in the other approach. &gt; I've heard rumors that the Free monad can have quadratic runtime, while the operational monad does stuff to avoid that? Though there is also some way to use the codensity monad to fix the Free monad? True. Operational is designed to offer both a O(1) `(&gt;&gt;=)` bind operation^* and O(1) `view` function. The Free monad has to choose between them. ^* Fine print: Only for single-threaded uses, not for persistent uses. For instance, evaluating the value of `x` defined by `let x = m &gt;&gt;= k` twice will also evaluate the bind twice, but this rarely happens. &gt; So, experts, which implementation should I use for this example? And what are the benefits of that solution over the other alternatives? My inclination is to use the operational. The interpretive solution clearly provides the most flexibility. Of the three intepretive solutions, the operational monad seems like the easiest solution to implement and to understand. I am not sure what (if anything) I am missing out on by using operational instead of Free... I favor operational, too, though I may be a bit biased here. :-) You don't miss out on anything by using it. 
Is this a joke? This seems way more complicated than just reading the code.
Yea I probably should have commented it :( Thanks for the tips though!
It does not help that the code doesn't compile. x &lt;- ask :: String should be x &lt;- ask :: Reader String String if you want to give a type annotation, although x &lt;- ask works just as well. I find it difficult to understand the picture. Partly this is because you have to teach me what your notation means, and it does not seem particularly easy. Why is `length` an arrow? * the blue-green looks like it is separating a double height input from single height inputs above and below it * I don't understand why the left inner colour in the `return` block changes colour from blue-green to dark blue. What does it mean? I understand `Monad`s in general and `Reader` in particular. But I don't understand your diagram.
This one is so much more helpful: http://blog.sigfpe.com/2006/10/monads-field-guide.html
1. But in your notation, `/{number}` is dynamic. edit: OK I think I see what you mean. It's dynamic but not in the sense that it, for example, requires IO. But actually there are such routes, for example you don't want to match if the blog post ID isn't in the database. We could just match, and return a 404 response, but by using this routing "cascade" we can easily for example have a catch-all route at the end with a custom 404 page, and it even works with handlers we don't control. There are other ways to do that, of course, but this is one way and it's flexible and straight-forward. 2. Yes, we have `web-routes` for this kind of routing. We first parse the URL into a custom data type and then pattern match on that to select a handler. This is generally the preferred solution, because we can statically guarantee that all routes have a handler and we can generate URLs that match our parser. It's a little bit more user code (not a lot) but generally worth it. The dynamic routing combinators described in this post is an older design in Happstack, and maybe still useful sometimes because they're much simpler, and also mean `happstack-server` can include basic routing support without extra dependencies and without coupling web-routes into happstack-server (it is for example possible to use it with Snap). Also the point of the post itself is probably more about exploring the various options for designing monads, with routing merely used as a simple real-world example.
1. Sorry if I wasn't clear. I was trying to refer to the overall structure of the URL depending on the *value* of the number matched. For example, a contrived example would be trying to match `/posts/{number}/negative` if the number is negative and `/posts/{number}/positive` if the number is positive. 2. Ah, that clears it up. Guess I should look into web-routes then :)
Care to comment on [this SO question](http://stackoverflow.com/questions/14263363/is-operational-really-isomorphic-to-a-free-monad) about operational and free?
hdevtools HdevtoolsType depends on the current dir, so if you do something like `cd ../../` it could lost some import and start annoying with warnings. hdevtools is faster because it running all the time in background, when ghc-mod is starting every time you wanna it. But ghcmod-vim have more functions like hlint etc. Another thing is that ghc-mod put his messages in separate preview window, but hdevtools use just output error window, here is a [screen](https://zfh.so/hell/561f8a9ae9b40450bfe5cf60aff424950ba8947d.png). So, when you press some key, error will go away. With ghc-mod you could autorun :GhcModCheck after saving a file, and just looking in appeared error without any hotkeys and changing any focus. Actually, I preferred to use them both (fast typing of HdevtoolsType and other ModLint and ModCheck from ghcmod-vim): autocmd FileType haskell nnoremap &lt;buffer&gt; &lt;F3&gt; :GhcModLint&lt;CR&gt; autocmd FileType haskell nnoremap &lt;buffer&gt; &lt;F4&gt; :GhcModCheck&lt;CR&gt; autocmd FileType haskell nnoremap &lt;buffer&gt; &lt;F5&gt; :HdevtoolsType&lt;CR&gt; autocmd FileType haskell nnoremap &lt;buffer&gt; &lt;F6&gt; :HdevtoolsClear&lt;CR&gt; 
If you wanted to serve files you might do something like this: dir match "files" isAuthorized serveDirectory "/path/to/files/on/disk" If the user visits '/files/foo/bar.txt', you first check that they are authorized to view things in that directory (for example, it could be a members only area), and then you have 'serveDirectory' look at the remainder of the path and then you call 'serveDirectory' which looks at the remainder of the URL and serves '/path/to/files/on/disk/foo/bar.txt'. There are also other combinators we skipped over such as 'host' (useful for virtual hosts), method (for matching on the request method), etc. It can be useful to delay these up front has well. For example, you might have a function that handles some form: myForm = do method GET showMyForm &lt;|&gt; do method POST processMyForm I have mixed feelings about this system myself (I didn't design it). I generally use `web-routes` (which I did design). But it still ties into this system. It's not clear to me that making your web framework monad an instance of `MonadPlus` is really that desirable in the end. But, at the moment, we all do :) Definitely going to look at other alternatives for Happstack 8 -- if you want to discuss more ideas I would be happy to hear them.
[My pleasure.](http://stackoverflow.com/a/14295488/403805) :-)
Or you can pattern-match on [] instead of using `null`. OP, did you run HLint? I think it will give you a lot of suggestions.
Thanks! I've edited the post to include that suggestion. Shame it doesn't work on Emacs :)
&gt; I will also move the false branching argument to the first position. There is an underlying theoretical reason for this, but run with me for now! Uh, I don't think there is. The church encoding for booleans usually has true ≡ λa.λb. a false ≡ λa.λb. b Which is the exact opposite of the presentation here. fiilter p = foldRight (if' id . (:) &lt;*&gt; p) [] This is a bit too clever, in my opinion. There's no real benefit to be gained from expressing filter this way.
According to this graphs... Is the version with explicit recursion slightly faster than the version using foldr(l)?
Agreed. I've written filter a few times (for fusion research), and I usually end up with something like: filter _ [] = [] filter p xs0 = go xs0 where go [] = [] go (x:xs) | p x = x : go xs | otherwise = go xs 
Yeah, that is a tough balancing act. :) Fortunately, with `happstack-lite` we were able to take a subset of functions from `happstack-server` and give them simpler type signatures with out having to change the code. 
Is there a need for the first case of `filter`?
I think this would be clearer if `go` was called `filter_p`: filter _ [] = [] filter p xs0 = filter_p xs0 where filter_p [] = [] filter_p (x:xs) | p x = x : filter_p xs | otherwise = filter_p xs 
I think it's a poor idiom, and `filter_p` is clearer. When I was a Haskell beginner, reading foldl f z0 xs0 = lgo z0 xs0 where lgo z [] = z lgo z (x:xs) = lgo (f z x) xs was very confusing. Reading foldl f z0 xs0 = foldl_f z0 xs0 where foldl_f z [] = z foldl_f z (x:xs) = foldl_f (f z x) xs would have been much more helpful. 
Still efficient to keep it, even after the static arg transformation introduces 'go', but the benefit is only marginal -- GHC will see a lot of the arguments and just jump straight to the worker. 
Too much significance given to the worker, which only has local scope -- hence should have a 'smaller' name.
Understanding the "worker" is critical to understanding the function. Indeed it comprises the entire definition of the function. Thus it's hard to give it "too much" significance. 
Remember that libraries are written to be read by experts, and `go` has great significance to experts -- it has a [fine history](http://stackoverflow.com/a/5844850/83805). Your long names mean I have to think harder to realize it is a go-style loop.
I hope people don't mind a 3 year repost and posting old ICFP videos, but I just came across this one and the visual explanation of monads in terms of join was really enlightening, especially when trying to understand how on earth a vector space could form a monad. I'm sure I'm not the only subscriber who only discovered Haskell after 2009!
&gt;I am often asked if this has a pronunciation and although I don’t know of an agreed term, I have heard: “spaceship operator”, “starship operator”, “angle bum.”↩ "Apply". It's even called `ap` for monads. Thinking of it as (impure/modal/generalized) application makes sense for at least five reasons: (where `pure = return`, `apply = &lt;*&gt;`) - `apply` for `Identity` is `($)` - `apply (pure id) = id` means "applying the pure identity function is the identity` - `apply (pure f) (pure x) = pure (f x)` means "applying a pure function to a pure value is the same as the pure result of just applying the function to the value". - `(&lt;*&gt; pure x) = apply (pure ($ x))` means "applying to pure value is the same as purely applying to it". - `pure (.) &lt;*&gt; u &lt;*&gt; v &lt;*&gt; w = u &lt;*&gt; (v &lt;*&gt; w)` means "pure composition of impure functions gives usual composition, but with impure application". It's easier to see with an operator: `(f &lt;.&gt; g) x = f &lt;*&gt; (g &lt;*&gt; x)`
Can you go into more detail about what exactly "go" communicates to experts (beyond it being a tail-recursive worker function, as you have already explained in your Stack Overflow answer)? Why wouldn't `go_filter_p` be the best of both worlds?
I guess you mean `apply (pure f) (pure x) = pure (f x)`.
Oops, I do. My memory has failed me. Thank you.
Right, in that case, why doesn't he just say so? No "underlying theoretical reason" or any such nonsense.
&gt; Why wouldn't `go_filter_p` be the best of both worlds? Because it is longer. And `go_filter_p` also looks very similar to `go filter_p` or `go_filter p` or even `go filter p`. That is also a good argument against names like `filter_p` in general: it is too visually similar to `filter p`, which is even a valid expression in that context.
Because it is semantically equivalent to `filter p`, `filter_p` should be visually similar to `filter p`.
If it's used in exactly 51% of applications (or thereabouts), this seems a clear argument for there being two functions: one offering each argument ordering, since both variants are equally useful. It's only when you're using `flip` for the clear majority of applications (e.g., &gt;66% or &gt;75%) that it becomes obvious you specified the argument order wrongly.
I've always used [Fink](http://www.finkproject.org/) and never run into any of the problems the MacPorts people have. FWIW, this includes not just getting Haskell installed, but every other thing I need too (LaTeX, non-system Python, non-system Perl, OCaml, Cairo, Git, Mercurial,...) Granted, the GHC included in Fink has a history of not always being up-to-date so you may want to compile your own--- but doing so is easy with Fink since you can install all the other *nix tools you need. Or course, if you're on a newer Mac, the [HP](http://www.haskell.org/platform/mac.html) bundles are quite nice and I used them up until they dropped 10.5 support. If I were to try anything different on a new computer, I'd finally get around to testing out [Nix](http://nixos.org/nix/).
&gt; Is it ok to use an outdated cabal? Unless it's *severely* outdated, it's fine. Newer versions add new features, but the necessary are present in any moderately up-to-date version. I've been avoiding an update for a little bit since there was some buggy hiccup recently, though I think that's been ironed out now.
Typo in the fourth paragraph on page 11: &gt; ...a distinction between types can kinds can be eliminated...
This particular video was the one that made monads really work for me. 
Using conventional names helps reduce cognitive load, because we know what those names mean and so we don't need to think about them. Why do Haksellers use the convention `(x:xs)` rather than some other thing like `(firstElem:otherElems)`? Because it's a pattern that shows up so often that (a) short names reduce clutter, and (b) everyone knows what `(x:xs)` means, how `x` is related to `xs`, etc. Whereas using `(firstElem:otherElems)` requires one to stop and think and ask why the developer chose to oppose convention--- clearly these variables must be related in some special way different than how `x` and `xs` are related, so the developer chose the names to highlight this contrast. It's the exact same reason why `for(i=0;i&lt;n;++i)` is so ubiquitous in C. The conventional use of the name `i` (or `j`,...) signals that this is yet another for-loop just like any other one and deserving of no special attention.
Yes, `lgo` is a horrible name. Should've just been `go`.
&gt; Uh, I don't think there is. He has phrased it rather confusingly, but I believe he refers to the "underlying theoretical reason" for placing the test in the last position rather for putting the false branch first. See the corresponding footnote.
Now I'm philosophically puzzled, because I completely agree with the conventional usage of `(x:xs)` and `for (i = 0 ,...)` but I find myself repulsed by `go`.
I agree. I always call it loop. 
The transformation of 'if p then f x else f y' to 'f (if p then x else y)' is invalid. Just pick p=undefined and f=const 1.
I can help with this one! https://github.com/jhickner/smtp-mail
Anecdotally, lookups account for ~90% of my use of flip, and it's often annoying/obscures readability. Using backticks instead helps slightly, but I think I'd prefer lookup functions to take their arguments in reverse order, even if that doesn't seem so aesthetic when fully applied.
I wonder if there's any lists, blogs or whatnot in MN to publicize it? Maybe some people at the U of M's various IT departments would be interested?
Thanks! I did not know... any of that.
Do you really find it any different at all? It's a local "where" clause, and neither "filter_p" nor "go" communicate any extra useful information besides connecting the references here.
this presents a problem for dons' argument: you don't even need to leave scb to find haskell expects who disagree on this one. 
Yes I find them very different. One name makes it clear it's forming a closure around an argument, the other name imparts no useful information about the relationship to `filter`.
We may be discussing different notions of zippers. Perhaps you're referring to the original zipper in Huet 1997? GP was under the impression that a zipper was just the "context" in the sense of the functor's derivative. In analogy with d/dx a^p = pa^p-1 , the functor derivative can be written as `(p,(p-1)-&gt;a)` where `p` is the type of positions and `p-1` is the type of positions modulo a hole in each. If you bundle the derivative with an additional `a`, you get the store comonad: `(p,p-&gt;a)`, which is what I was merely pointing out. 
I wish I had time to delve into that right now.
Isn't the expression undefined in both cases ?
The twist: the theoretical reason he mentions is "they are the same up to isomorphism".
The fact it closes over all the arguments is immediately apparent because it is in a `where` clause.
That's not what I mean. The closure need not *actually* use any of the arguments, however "`filter_p`" strongly suggests that it is indeed using `p`. These kinds of assessments are always hard because there is a great deal of subjectivity about them. The only objective thing that I can say is that when I was learning Haskell a couple of years ago I found the implementation in terms of `go` impossible to understand. ("What on earth has this strange '`go`' got to do with filtering?"). If `go` had instead been named `filter_p` I think I would have understood it immediately. 
No, the latter evaluates to `1`.
Well, perhaps it is worth mentioning that "go" inside a "filter" definition can be read as "filter.go". And "go" is a common idiom that means a "worker loop" in the wrapper/worker pattern.
I like your version because it corresponds nicely to `foldr` being a combination of `map` and composition of a "chain" of functions. (http://web.jaguarpaw.co.uk/~tom/blog/posts/2012-11-04-what-is-foldr-made-of.html)
Sorry for code golfing, but why not something similar like list comprehension? filter p xs = [ x | x &lt;- xs, p x] 
The use in vector is for a very specific reason - Roman prefers loop instead of go, but more importantly, there are tests for constructor specialization that require unique stable names from the compiler for the workers, so we had to sacrifice readability for a special testing setup. See e.g in the non fusion implementations,http://hackage.haskell.org/packages/archive/stream-fusion/0.1.2.5/doc/html/src/Data-List-Stream.html#find
I wondered whether there was a reason for the seemingly unneeded `..._head` `..._tail` bit (now I want to learn about these tests!), which is indeed attached to things to be specialized. It's readable enough and would be more so if we were used to it -- but I ought to have stopped short at defending `loop` as a legitimate `go`. Note that R.L. also tends to mark local bindings with asterisks and so on; I don't thing we'll find camel-cased expressions defined after where. 
I think so, I'll see if we can get some fliers up once we settle on a meeting time 
This is a fantastic talk, so it really can't hurt.
If you're interested in learning Haskell, and in learning to build things using good abstractions (or inventing new abstractions yourself) then picking up bits of category theory by way of Haskell, as you encounter them. As opposed to "studying category theory" directly. Also I'm almost certain your question has been answered on SO at some point.
Category theory in and of itself is a *very abstract* branch of pure mathematics and much of the areas of active research are not of use to most Haskell programmers. That being said many of the foundational ideas from category theory do actually find direct translation into Haskell ( i.e. Functors, Natural Transformations, Monads, Kleisli Categories, Adjunctions, etc ) and even some of the results ( Duality, Yoneda lemma ) have consequences for Haskell programs. Though that is not to say that one *needs* to know category theory to do Haskell, it is quite possible to be a great Haskell programmer and not be concerned with the abstract nonsense of category theory. * [Category Design Pattern](http://www.haskellforall.com/2012/08/the-category-design-pattern.html) * [Resources for Learning Category Theory](http://mathoverflow.net/questions/903/resources-for-learning-practical-category-theory)
Here's my perspective. For reference, I'm relatively new to Haskell, but nearly finished with my PhD in pure mathematics, so I see Category Theory from the math side. As freyrs3 says, Category Theory is a *very abstract* branch of pure mathematics. In particular, nearly all examples in Category Theory require some combination of abstract algebra and topology. If you don't feel comfortable with either of those, then I believe that Category Theory is likely to sound entirely like abstract nonsense with very little application. It may be interesting, and it may be understandable, but it won't seem natural unless you have the context of abstract algebra and topology. So my suggestion agrees with jberryman - pick up what you need when it comes up. Studying Category Theory by itself may be a prolonged battle for little gain. However, you say that you like the mathematical side of CS (me too!). Studying abstract algebra (then topology, if you like) will stimulate those same brain muscles that category theory does. If abstract algebra is not of interest, you could start with an abstract version of linear algebra. Your program may have already required you to take a linear algebra course, in which case you can probably jump right in to [Hoffman and Kunze](http://www.amazon.com/Linear-Algebra-Edition-Kenneth-Hoffman/dp/0135367972), which your university's library may have a copy of (since it is unreasonably expensive). It's my favorite book for learning linear algebra at a level of full abstraction, especially once you get to the chapters on spectral theory and inner product spaces. As a bonus, if you do learn one of these more abstract branches of math, Category Theory (at least, the early parst like monoids, functors, and natural transformations) will be significantly less obtuse, since you'll be used to dealing with these types of abstractions, *and* you'll have some examples of these things that you have already learned.
BTW - I hope this type of practical posts are ok here. I learn better by concrete examples rather than looking at API docs, so I figured someone else here might appreciate this too. 
There are plenty of illegal monads out there. The monad police don't come and arrest you, you just lose the ability to reason about them sanely in your code.
Category theory reduces software complexity. Let's use the simple example of function composition: (.) :: (b -&gt; c) -&gt; (a -&gt; b) -&gt; (a -&gt; c) We take two functions, each of which has one input and one output, and combine them to generate a new function, which also has one input and one output. In other words, the combined function is just as simple as the two original functions and we gained no complexity by combining then. In many languages, you have lots of frameworks that claim to let you combine components and abstract over internal details. However, the vast majority of them do not live up to that promise and their internal interfaces leak as you combine more and more components. Category theory lives up to that promise by defining abstractions that never leak, so that when you combine things the result is always just as simple as your starting materials. Category theory is all about taking that simple notion of function composition that we began from and finding analogs of it everywhere, all of which preserve the ability to abstract perfectly without leaking.
I really don't like defining `if'` the way Tony does: reversing argument order should cause a new name, like `boolCata`. It is also not necessary if' b t f = if b then t else f ignore :: a -&gt; b -&gt; b ignore = const id -- or flip const using these we can go from \a b -&gt; if p a then a:b else b to \a b -&gt; if p a then (a:) b else ignore a b which I read as "if p of a then cons a onto b else ignore a and give you b" which clearly simplifies to (fast and loose) \a -&gt; if p a then (a:) else ignore a = \a -&gt; if' (p a) (a:) (ignore a) the reoccurring `a` can go by putting it in the function monad either if' &lt;$&gt; p &lt;*&gt; (:) &lt;*&gt; ignore or liftA3 if' p (:) ignore which I think is much more readable than what Tony has and just as short and point free: "if p then cons else ignore" is exactly what the function does 
I've been digging into AngularJS as well. Thanks for writing this!
Very useful! 
Are not of use to most Haskell programmers... Yet. But since Category Theory is the grand unified theory of everything, chances are quite a bit of it will start showing up where you least expect it. Costate comonad coalgebras anyone?
? It defines a *bijection* to some subset of the natural numbers, possibly strict (as the case of Bool clearly illustrates).
I can't imagine what type of post would be *more* okay than this! It may not attract the same quantity of upvotes that discussion bait or major community announcements do, but concrete examples of Haskell applications (even small ones) with links to the source on github are great.
FWIW, I tend to vacillate between `loop` and `go`. I use `go` more for accumulator/CPS transformed code; whereas I use `loop` more for things which are distinctly repetitive/monadic. Both conventions are fine IMO, I just think it's good to have such a convention(s) rather than giving everything idiosyncratic/ad-hoc names. For cases like the vector package, I'd argue that the compiler should choose names like `foo_bar` (rather than just `bar`) for the lambda-lifted `bar` defined locally in the definition of `foo`. This would make it easier to debug core without sacrificing legibility of the source code. The one place where I do follow the `foo_x` convention is in my (unreleased) package for partial evaluation, where `foo_x` is the name given to the result of `(foo x)`. But this is rather different than the worker/wrapper transformation, and it helps improve legibility when you need to distinguish `f`, `f_x`, `f_x_y`, etc which are all in scope locally.
Any book recommendations for abstract algebra? (For those of us not in school anymore) edit: thank you all for your thoughtful replies! I will look at each of these.
As a minor matter of style, point-free functions can sometimes be more readable than their pointful equivalents. Instead of `\(i,x) -&gt; x == e`, you could write `(e ==) . snd`. They both say “`e` equals the second element”, but the latter is less syntactically noisy.
I was going to write something on your second question, but then realized my points were already written up in section 1 and 2 on the Haskell Wiki's [Monad laws](http://www.haskell.org/haskellwiki/Monad_laws) page.
&gt; Are not of use to most Haskell programmers... Yet. Sure at least in principle, but in practice quite a bit of category theory revolves around describing very pure mathematical structures that aren't of much use to programmers. It was originally created to describe certain ideas in algebraic topology which I have yet to see gain a firm foothold in computer science. Need only browse the [ncatlab](http://ncatlab.org/nlab/show/All+Pages) to see how broad the field is. &gt; But since Category Theory is the grand unified theory of everything, Not sure I'd use that phrase, but it certainly is a useful model for a lot of fields.
Very pure mathematical structures that aren't of much use to programmers... Like functors. And natural transformations. And monads. Oh, wait...
 f = map (* 2)
 for (int i = 0; i &lt; l.length; i++) { l[i] = l[i]*2; }
Let's define a function `fun`. `fun` takes a list of integers and produces a list of integers, so fun :: [Integer] -&gt; [Integer] If we want to double every value in an empty list, `[]`, then we just return the empty list. fun [] = [] If we have any other list, then there's a head and a tail. We write this as `(h : t)`. (or any other variable numbers) fun (h:t) = (2 * h) : (fun t) This is a common pattern, so we have a function for it, `map`. `map :: (a -&gt; b) -&gt; [a] -&gt; [b]`. The function we have here is `(\x -&gt; 2 * x)`. We can write this in shorthand as `(2*)`. So altogether, we have: fun = map (2*) 
**No**, you don't *need* to learn Category Theory to be a good Haskell programmer (and understand the underlying concepts). Category Theory (CT) is *sometimes* useful to help get mathematical design right. It is very abstract and provide very general concepts (that conversely tend to tell you little about your current problem); if you're a mathematician trying to do something in a way that just doesn't quite work, and you notice that your construction looks like an instance of a CT concept, but isn't quite the same, then making it exactly like this CT concept may help you get a solid design that will stand the test of time. This has occurred a few times oven the dozen of years programming language theory has been studied through rigorous mathematical practice; the most well-known example is the way Eugenio Moggi used *monads* to structure semantics for lambda-calculi with effects -- he also used CT ideas to structure type systems for staged meta-programming (an Idealized MetaML). But once your mathematical idea is developed, you shouldn't need CT to explain it; the application domain will usually suggest explanations that are much easier to understand than doing the grand tour through CT. Monad laws are more easily explained as natural properties expected, for example, of an effectful `let-binding` form, than going through the "monoid over endofunctors" definition of natural transformations -- or simply stating the law and muttering that they're correct "because they come from Category Theory". Similarly, when people use category constructions to apply the notions of functors or natural transformations to actual programming language, this usually ends up being insanely complicated, or borderline wrong, or both. You can go along with a simplified model that allows to apply CT ideas on an idealized formal language, but in the end it's still much simpler to think of a `map` function on datastructures, of parametric polymorphism, etc., with programmer rather than categorist intuitions. If you plan to be a Haskell practitioner, you don't need to learn CT at all. If you plan to become a researcher in programming language theory, you may learn some CT along the way (though a lot of people successfully avoid the more theoretical-minded communities and still do extremely interesting and useful stuff), if not to do your own research then at least to understand some extremely obtuse stuff that has been written by people that knew CT too well. For some reason it is considered fashionable in some circles to talk about CT to practicing programmers and hope this will expand their horizon in incredible way. I believe mathematics are an extremely interesting topic and would encourage *anybody* interested in them to study any math subdomain, CT included, on his/her spare time. But don't expect that to be actually useful to your programming practice, or you will most likely be disappointed (or delusional). Now there are some people that actually are expert of category theory and do great things with it. Edward Kmett has an impressive mastery, among other things, of the obscure ways you can classify recursion patterns, that is somehow related to his CT expertize (and is a sport that has been played by the Bird-Merteens / Squiggol crowd for a long time). [Neelakantan Krishnaswami](http://semantic-domain.blogspot.com/) has done very nice research work in programming language theory, some of it using impressive (and slightly frightening) amounts of category theory. You should be very happy to let those extremely dedicated people spend a ton of time mastering these theoretical concepts, and applying them wherever they see a need, without having to get your hands dirty by yourself. If you still wish to learn CT, the good news is that you actually need to know very little to understand most of the people talking about CT to a programming audience. The definition of a category is quite easy to learn, and functors and natural transformations are reasonbly simple (you need to work various different examples by yourself to really master those concepts). From this you can get monads. Anything more advanced (adjunctions, pullbacks, comma categories, inductive/projective limits...) is most probably overkill. Regarding learning materials, Benjamin Pierce's [Basic Category Theory for Computer Scientists](http://www.amazon.com/Category-Computer-Scientists-Foundations-Computing/dp/0262660717) is a reasonable choice, if maybe a bit terse (could have used more examples). If you want to get really serious about CT, the Youtube video of [The Catsters](http://www.youtube.com/user/TheCatsters) are probably a very good starting point. But, again, I don't think you need all that.
Fairly certain GP meant things like using category theory to work with homology/cohomology, and to prove equivalences of homologies/cohomologies (deRham, singular, etc). In my category theory course, these were some of the first structures talked about after functors and natural transformations. As far as I know (granted, this is not much), the ideas behind homology and cohomology have not been widely applied in CS yet. But they are practically fundamental in modern pure mathematics - it just gets more complicated from there. Certainly, as you say, some ideas from category theory have fantastic applications. But that doesn't mean that many, or even a fair portion, do. At least, not yet. Though I hope that you are right, and that somebody will find a common pattern that can be represented through a homological structure placed on code.
I echo this! While amusingly, Haskell has made me begin a 6 year part time undergrad degree in maths, it's mostly for the abstract algebra stuff - so the more resources I have, the better!
Wow, what a nice answer. Thanks. After reading all the answers I get that I should no worry about CT too much, but I still don't get why do people talk about it so much. I think that is something I'll only figure out after reading a lot more pappers.
If you use msum . map rather than mapMaybe you get a bit more generality. 
Reference for your last statement, might be useful to some people: http://blog.sigfpe.com/2006/11/why-isnt-listt-monad.html
My very favorite abstract algebra book is [Dummit and Foote](http://www.amazon.com/Abstract-Algebra-3rd-David-Dummit/dp/0471433349). It's quite popular, so used old editions are readily available for quite cheap (I use the second edition, myself). It's as close to a complete guide of the entire field as any book I know of, but remains much more concrete than, say, [Hungerford](http://www.amazon.com/Abstract-Algebra-Introduction-Thomas-Hungerford/dp/0030105595). Basic abstract algebra is probably only the first third or fourth of the book, everything else is a survey of advanced topics, and a self-contained reference. The only problem with it is that I think it is intended for graduate students. An ambitious undergrad, or someone with exposure to abstract mathematical reasoning, should be able to work through it, but it may not be the most friendly introduction. Unfortunately, I don't remember (and wasn't very fond of) my undergraduate text, and haven't had the opportunity to teach abstract algebra yet, so I don't know have a good recommendation for undergraduate-level texts. On a separate note: while I don't recommend Hungerford as a text to learn (or reference) abstract algebra, it it amazing in it's unique treatment of abstract algebra from the perspective of category theory. This may not apply to you, but someone else reading this thread might be interested in reading Hungerford if they want to gain a more workaday knowledge of category theory, and they've already had exposure to abstract algebra.
Note that your `BadMonad` is almost identical to the `Gen` monad from `Test.QuickCheck`: newtype Gen a = MkGen{ unGen :: StdGen -&gt; Int -&gt; a } instance Monad Gen where return x = MkGen (\_ _ -&gt; x) MkGen m &gt;&gt;= k = MkGen (\r n -&gt; let (r1,r2) = split r MkGen m' = k (m r1 n) in m' r2 n ) 
I’m actually more comfortable with very abstract pure mathematics. IMO it’s the most powerful, if you can see the ton of applications for it. And despite being a Math n00b from your standpoint, I can. Maybe because of Haskell. I don’t find it confusing at all It’s so elegant… and beautiful!
I've been wondering recently if a pragma to mark particular instances of Monad typeclasses as satisfying the monad laws would be useful to the compiler. In the worst case it would probably be equivalent to add some RULES for the particular laws (and might introduce some possibly conflicts between the rules system now that I think about it more). I'm not up to speed on what GHC does under the hood enough to speculate about the best case. Does anyone have any thoughts?
In my opinion, as a practical programmer whose major concern is to "solve" a certain problem using haskell (or whatever adequate), you don't need to know any more knowledge about Category theory than programmers using other languages do. Even though many useful haskell library/concept uses the vocabulary from the CT, it can be regarded as just a vocabulary like OOP programmer's XXX pattern which you don't usually need to seek for behind the words. For example, Monad is useful even without thinking of its very meaning in CT. By exposing yourself frequently in appropriate context (like IO, State, Reader, List ...), one can capture at least roughly what is 'monad'-like, and often that's enough for doing program in many cases as we don't need any complicated theory for, say, observer pattern in OOP. The thing is that if we start trying to 'understand' why this similar pattern occurs, and if we want to make progress using the fact that we have such common structures in programming, then the CT becomes very useful theory/terminology/philosophy. That's where active research is going on and why many haskell library are written and rewritten in terms of more transparent CT concept. I think that once you mastered haskell and are fluent in haskell, you would've already grokked important categorical concepts in haskeller's word, and will just appreciate how the CT broadens your eyes. If you have a little bit of mathematical background, I can compare this with learning differential geometry without any CT knowledge. Once mastered, one can realize that actually the basic concepts in DG can be regarded as examples of CT concept very straightforward, but no need to learn CT first for learning DG. p.s. but I admit that it's very tantalizing to study category theory when CT terminology appears while learning haskell. 
Do you mean Junior2, vs. Senior2?
flip was introduced by Schoenfinkel as T (the 'interchange function'). &gt; The function *T* makes it possible to alter the order of the terms of an expression, and in this way it compensates to a certain extent for the lack of a commutative law.
Runar Bjarnason gave a talk about the machines library before one of the YOW conferences - I would really like to hear more about it and/or see how it fits into the current discussions about streaming IO etc... How is the status of machines these days?
Algebra: Chapter 0 by Paulo Aluffi http://www.mimuw.edu.pl/~jarekw/pdf/Algebra0TextboookAluffi.pdf It's a fantastic intro to basic abstract algebra and category theory.
For the autodidact, I highly recommend Charles Pinter's [A Book of Abstract Algebra](http://www.amazon.com/Book-Abstract-Algebra-Edition-Mathematics/dp/0486474178/ref=sr_1_1?ie=UTF8&amp;qid=1358134127&amp;sr=8-1&amp;keywords=pinter+algebra). Pinter's writing is very engaging, and he explains things in a very intuitive way. In fact, the first chapter is a (somewhat simplified) historical overview, to put things in perspective, and throughout the book he reechoes the big picture, so you never get lost in the weeds. The only thing is that much of the coverage is actually in the exercises. The chapters are super short, usually shorter than the exercises. It was a pedagogical decision: learn by doing. Some people don't like that, but I find it the best way to learn. It's not the most advanced book, but it's a great intro as well as refresher, and it's actually *fun*. (Also, I think even advanced students will find lots of engagement in the exercises. He has coding examples, automata theory, etc.) Also, it's *cheap*.
so good
i thought he was refering to other functions like maybe and either `maybe (nothing case) (just case)` `either (left case) (right case)` so now `if (false case) (true case)` 
The idea of a fixpoint (perhaps *the* fundamental idea in computer science) comes from topology. Dana Scott got many of his ideas from algebraic topology. He won the Turing award for one of his more minor ideas (inventing nondeterminstic automata and thus founding the field of complexity theory), but his perhaps biggest inovation was essentially topological: that the category of pointed complete partial orders had solutions to `t = t^t` and thus could serve as a foundation for everything. Maurice Herlihy won the Dijkstra prize for inventing software transactional memory, but his winning of the Gödel prize was for applying ideas from combinatorial/early algebraic topology to the theory of distributed system. And, the homotopy type theory crowd are doing some of the most interesting things in PL theory right now. I would say algebraic topology has a proud and varied history in computer science and is likely going to be a very fruitful place for future development. IMHO (as a non mathematician), algebraic topology has more relevance to CS than most areas of math (geometry, number theory, analysis ... )
I think the most compelling visual representation of monads comes from string diagrams, which use arrows to represent types and function boxes for functions. Like this: +---+ +---+ ---&gt;| f |---&gt;| g |---&gt; a +---+ b +---+ c Then a Functor can be represented as a labeled stripe around a sequence of arrows and boxes, like this: .............&lt;list&gt; ................. ......+---+...... ------&gt;-----&gt;| f |-----&gt;-----&gt; 'a ..'a..+---+..'b.. 'b list ................. list ................. Note that the fmap operation is implicit in the way we built the diagram: the `'a list` type "loses" its computational context inside the `&lt;list&gt;` stripe and is interpreted as `a`. But we can also represent monad operations for types which support them. Here's unit, or pure: ..&lt;list&gt; ...... -----&gt;@-----&gt;-----&gt; 'a ..'a.. 'a ...... list And here's join. Notice how the rightward edge of the inner `list` constructor just disappears, so the two stripes kindof merge into one. And the types actually reflect this. ................-.&lt;list&gt; .............../...... .......+---&lt;list&gt;..... .......| ........ .......| ........ -----&gt;------&gt;+-----&gt;-------&gt;------&gt; 'a ..'a...| 'a ...'a... 'a list .list..| ........ list list .......+------,....... ...............\...... The functor + monad laws behave as diagram transformation rules. Here are the functor laws: ...&lt;T&gt; ------&gt;----&gt;-----&gt; = ------&gt; fmap id = id 'a T .'a.. 'a T 'a T ..........................&lt;T&gt; ...............&lt;T&gt; ...............&lt;T&gt; ......+---+......+---+...... ......+---+...... ......+---+...... ------&gt;-----&gt;| f |-----&gt;| g |-----&gt;------&gt; = ------&gt;----&gt;| f |-----&gt;------&gt;-----&gt;| g |-----&gt;-----&gt; fmap (f . g) = (fmap f) . (fmap g) 'a T ..'a..+---+..'b..+---+..'c.. 'c T 'a T ..'a..+---+..'b.. 'b T ..'b..+---+..'c.. 'c T ............................ ................. ................. Here's one of the monad laws, the oneinvolving fmap and unit: ...........&lt;T&gt; +---+ ..&lt;T&gt; ....+---+.... --&gt;| f |--&gt;@--&gt;-----&gt; = ---&gt;@--&gt;| f |---&gt;------&gt; unit . f = (fmap f) . unit a +---+ b ..b. b T a . a +---+.b.. b T ............. The laws involving join are slightly hairier. Here's `join . fmap join = join . join`: .............................--..&lt;T&gt; .....................- -...&lt;T&gt; ............................/...... ..................../ / ..... .........+---------------&lt;T&gt; ...... .........+-------&lt;T&gt; / ...... .........| / ........ .........| / ...... .........| +-&lt;T&gt; ......... .........| +-&lt;T&gt; ...... .........| | .......... .........| | ...... -----------&gt;--------&gt;|------&gt;|----------------&gt;------&gt; = -----------&gt;--------&gt;|------&gt;|------------&gt;------&gt; 'a T T T .'a T T..| 'a T | 'a .......... 'a T 'a T T T .'a T T..| 'a T | 'a ...... 'a T .........| +---, ......... .........| +---, ...... .........| \ ........ .........| \ ...... .........+-----------------, ...... .........+---------, \ ...... ............................\...... ....................\ \...... Here is `join . fmap unit = join . unit = id` : ............-.&lt;T&gt; ................-.&lt;T&gt; .........../ ... .............../ ... .....+--&lt;T&gt; ... .........+--&lt;T&gt; ... .....| ... .........| ... ------&gt;----&gt;@---------&gt;------&gt; = ------&gt;@-------&gt;|---------&gt;------&gt; = ------&gt; 'a T .'a..| 'a ... 'a T 'a T ..'a T...| 'a ... 'a T 'a T .....+----, ... .........+----, ... ...........\.... ...............\.... Finally, here's `join . fmap (fmap f) = fmap f . join` : .......................-.&lt;T&gt; .............-............&lt;T&gt; ....................../ ... ............/ .............. .......+-----------&lt;T&gt; ... .......+-&lt;T&gt; .............. .......| ... .......| .............. .......| +---+ ... .......| ....+---+..... --------&gt;------&gt;|----&gt;| f |--------&gt;-------&gt; = --------&gt;------&gt;|---------&gt;| f |----&gt;-------&gt; 'a T T .'a T..| 'a +---+ 'b ... 'b T 'a T T .'a T..| 'a ....+---+ 'b.. 'b T .......| ... .......| .............. .......+------------, ... .......+---, .............. .....................\ ... ............\ .............. (Infrmally, you can account for the graphical transformations relating `pure` and `join` by supposing that `join` merges edges associatively and that edges created by `pure` are special, in that they interact unitally with the `join` operation. My choice of a `@` symbol for the `pure` operation was intended to reflect this, as well as the 'lifting' of an element of `'a` into `'a T`) One problem with this representation is that monads don't interact naturally with the cartesian product structure. Basically, this means that every "stripe" can only have a single entry and a single exit point, and that functor and monad laws must also deal with a single arrow at a time. Thus, product types, unit types and first class functions must be artificially "packed" into single object arrows, even though their ordinary representations in string diagrams are rather more expressive. Admittedly, it is hard to relate this representation to the one that's commonly used, based on the `bind` operator. When this is important, it may be better to represent Kleisli categories directly, based on the following conversions: ....&lt;T&gt; ----&gt;@----&gt;-----&gt; ~~ ----------&gt; 'a ..'a.. 'a T 'A +---+ +---+ ----&gt;| f |------&gt; ~~ ----&gt;| F |----&gt; 'a +---+ 'b T 'A +---+ 'B ........................-..&lt;T&gt; ......................./ ... .................+--&lt;T&gt; .... +---+ .....+---+.......| ..... +---+ +---+ ----&gt;| f |------&gt;----&gt;| g |------&gt;+---------&gt;------&gt; ~~ ----&gt;| F |----&gt;| G |----&gt; 'a +---+ 'b T .'b..+---+.'c T..| 'c ..... 'c T 'A +---+ 'B +---+ 'C .................+----, .... .......................\.... Note that the Kleisli composition has been converted to composition of morphisms in the Kleisli category.
Dummit and Foote is the textbook my university uses for an algebra class suited for seniors and grad students. I've really enjoyed it so far. David Dummit is actually my abstract algebra professor this semester.
So I've been using AngularJS with Express (Node.js) backends for my projects lately. I can program in an almost purely functional style in JS these days, thanks to Underscore and the like, but I still miss types from time to time. I've been looking into Haskell web frameworks, but haven't actually dived in yet. How has your experience been so far? I'm not very experienced with Haskell in particular, so some of the higher-order concepts I see in Snap can be a bit intimidating. What have been some of the pros and cons so far?
Blaze-html violates the monad laws, all it wants is a monoid. I wish they wouldn't do that.
It's not absolutely necessary to learn. But then again, so few things in life are. Category Theory is something like a Rosetta Stone for mathematics. It allows you to translate ideas to seemingly unrelated fields. The way it does this is that it's only basic notion is that of a map. Everything in mathematics relies on some notion of a map. Set theory is about total functions. Topology is about continuous maps. Algebra is about homomorphisms. Computer science is about computable (and often partial) functions. Logic is about implication. Category Theory ignores all of the details. What elements make up the objects. What restrictions are placed on their construction. All that matters is the maps going in and out of each object. When you can rewrite some well-understood theory in terms of maps, amazing things happen. You can translate that idea into any category. A group in the cat of sets is just a group. But in the cat of topological spaces, it's a topological group. In the cat of manifolds, it's a lie group. That's pretty amazing.... as if all you have to do is say "I want to add a group that's also a blah" and when I do the translation of a group into the category of blahs, I get a blah group. Category Theory challenges the notion of object identity (something that is also of importance in programming). Strictly speaking, you can have virtually identical copies of things floating around in a category. However, in many applications of category theory, we consider "weak" cases where isomorphic things are treated as if they were identical. In basic category theory, objects are never compared for equality... only maps are. In a 2-category, though, equality is important for 2-maps, and less important for objects or 1-maps. In higher category theory, there is no absolute notion of "equal"... something that seems very reasonable when you live in a world full of transient pointers and mutable data. Many of the ideas in category theory are about finding the "purest" examples of things. Limits and colimits are about finding objects that give you exactly the properties you want "and nothing else". A product object is just barely good enough to give you two projection maps. Naturality says something about how our choices don't depend on the examples we choose. Adjointness gives you (in some exact sense) the simplest way to undo something. As a programmer, I don't think there's any fundamental reason to learn much category theory. It's annoying in Haskell that the culture is such that the theory bleeds so heavily into your every day hacking. It is sometimes very distracting. A lot of the research is extremely pointless. Even most of the really promising stuff is still not packaged right for a working programmer's consumption. However, if you have any interest in the mathematical foundations of programming languages, Category Theory is definitely worth some investigation. I find it very interesting. Although, I feel like the material for programmers is still either lacking or hard to find. I personally feel like it's better to think of Category Theory as a *language* to speak about programming, rather than a subject you should study to understand programming. 
Dude, this blog is brilliant. Just finished part 6 and it really helped.
Currently its in a 'worksforme' state. We use it here at S&amp;P Capital IQ for a few things. I'd like to expand the vocabulary it offers for input types and write some documentation, but i'm pretty pleased with the overall shape of the API.
Indeed, I missed the "1" that was wrapped alone on a new line. Thanks.
Oh thanks for that answer, very enlightening. I'm very interested in languages in general, more specifically, I'm interested in finding better ways to comunicated my ideas (specially algorithms, since I'm a computer scientist). Would you say then: learning abstract algebra, topology, category theory, etc (as cited by you and brianshourd) is worth the effort for this more theoretical side of computer science?
They're definitely welcome!
double_it = map (2*)
We need an HsPD to uphold the appropriate laws for all instances on Hackage. Sounds like a fun project.
I wonder about this too. I'm also interested in hearing about online courses a la coursera, Khan academy etc. that people have tried. I came across a musty copy of Irving Adler's "The new mathematics" when I was 18 and I _devoured_ that, but I never got the occasion to take classes in abstract algebra. I made some attempts to study it myself from books, but with little success. Most books aren't made for self-study, and it's hard to build on early concepts without some external feedback that you've got it right (or wrong, as it may be). 
Does ListT violate in general, or only with certain base monads?
I agree with him, CT constructions pop up out of nowhere nearly anywhere. Also, CT and CS are strongly connected, like CS and logic, it's not unusual to port CT concepts to CS and viceversa.
My advice would be to *be lazy*: look at what other people working on the same problem as you are doing, and if they find illuminating techniques using one of these fields, learn at least what you need to understand their work. The rest should be done by personal inclination rather than wishful thinking.
I do agree that Category Theory (CT) is interesting as a language for mathematics (despite most mathematicians actually staying at a reasonable distance from pure CT, except in some precise fields. I think it's important not to blow things out of proportion and look at what the practitioners are doing; just like set theory before it, most mathematicians work in their problem domains without necessarily paying much attention to these foundational approaches). Another thing of note is the way CT lets us talk clearly about the difference between the "specification" and the "implementation" of a concept, in a way that wasn't present in foundational set theory and more closely matches, I think, the actual mathematical practice -- but I don't like when people relate that to the mythical behavioral feeling of object-oriented programming, because it's a rather void comparison. However, I suspect theory-interested programmers could invest more effort into learning type theory, as it is definitely more relevant to the actual practice of programming, for example if you're trying to understand the crazy GHC type system extensions. Some notions that are very clear in type theory (for example impredicative polymorphism in System F, or dependent types in Martin-Löf type theory), have much less well-understood categorical equivalent, and they tend to be fairly sophisticated. Studying their categorical models is a very interesting research question that will certainly help us understand them better, but then, so are Ludics and Geometry of Interaction, and nobody's suggesting we should study them. 
Very nicely explained, and nice to see haskell's semantics not conflated with its implementation, etc. I haven't come across Data.Unique before, but it seems the motivating issue for using data-reify could be solved with the (super useful for some APIs, and often-overlooked) MonadFix class and GHC's recursive do notation: do rec bob &lt;- UniquePerson "Bob" [alice] &lt;$&gt; newUnique alice &lt;- UniquePerson "Alice" [bob] &lt;$&gt; newUnique
Ah, I am actually aware of MonadFix, but I was so sure that `newUnique` would be too strict to use recursively (without `unsafeInterleaveIO`) that I didn't even test it. :D Now I'm wondering why it works. Nevertheless, my main motivation was to write a short tutorial on using `Data.Reify` as I personally found its API documentation a bit hard to decipher. The person relations just were the simplest (semi-useful) example that I could come up with.
Right. Maybe I should add the disclaimers from `System.Mem.StableName` regarding false negatives. Do you have any pointers when `Data.Reify` is likely to fail? I haven't managed to break it so far in my (admittedly simple) tests.
I usually think of it this way, if your program would give wrong results if additional sharing was found, then you'd better not use observable sharing. If additional sharing being found merely speeds up your program and doesn't observably change the answer then its an acceptable tool.
You can't interleave or `unsafePerformIO` code that accesses `newUnique` safely across GHC versions. Until recently it was implemented via a `TVar`.
My sense was that Scott's work drew less from algebraic topology than point-set topology? I'd love to be enlightened on this score. If anything, I see the connection more to set theory and model theory, via his work on forcing and boolean valued models? Fixpoints are definitely topological in origin, but they also precede anything resembling algebraic topology.
It's not an exact equivalent (≡), but it's a refinement (⊑), isn't it? I don't see how that's a problem here -- I was under the impression that most informal "equational" reasoning in Haskell (such as in the submission) was actually "refinement" reasoning, because people don't usually care when their bottoms disappear. (If it was going the other way, it'd be a problem, of course.)
&gt; nearly all examples in Category Theory require some combination of abstract algebra and topology. I think that’s an illusion, and one which scares off some people who might otherwise be quite ready and able to profitably learn some category theory. It certainly can appear to be the case, since category theory arose originally from these areas, and so in the older textbooks, almost all the examples are indeed from these areas. However, CT can be introduced with a very different emphasis instead: see for instance Steve Awodey’s book [Category Theory](http://books.google.com/books?id=IK_sIDI2TCwC), Robin Cockett’s [Category Theory for Computer Science (online draft)](http://pages.cpsc.ucalgary.ca/~robin/class/617/notes.pdf), or Richard Walters’ [Categories and Computer Science](http://books.google.com/books/about/Categories_and_Computer_Science.html?id=QBjAZ_HwYZMC) (I haven’t read this last one, but I’ve heard good things about it). Some familiarity with abstract algebra is certainly helpful for learning CT, since a category is itself an algebraic structure. Topology definitely isn’t necessary, though — it gives lots of fascinating examples, but I can’t think of anything fundamental that *only* has good examples from topology. **Edit:** from the intro to Awodey’s book: &gt;Why write a new textbook on Category Theory, when we already have Mac Lane’s *Categories for the Working Mathematician?* Simply put: because Mac Lane’s book is for the working (and aspiring) mathematician. What is needed now, after 30 years of spreading into various other disciplines and places in the curriculum, is a book for everyone else.
Uh, why not just use mdo?
&gt; biggest inovation was essentially topological: that the category of pointed sets and partial orders had solutions to t = t^t and thus could serve as a foundation for everything. I don't recognize that in order to google it. What fixpoint is that, what solution is that? And if the answer to that doesn't make it clear, which "foundation for everything"? Denotational semantics?
There's a fair amount of refinement reasoning, but I firmly believe you need to be aware of when you are using it. And point out when you're using it. If you're not aware, you might use the transformation in the reverse direction and get into trouble.
Thanks for the links. What is the name of "t = t^t" and what is the name of the "foundation for everything" in question here?
I'm tempted to argue that the work by Hinze and James you cite does not actually use that much category theory. They have commutative diagrams, that are used everywhere in formal sciences (to prove the correctness of compilers or reason about bisimulations in concurrent calculi) and are not categorical in nature. Then they use the concept of fixpoint of a functor as an initial algebra, and some slightly more advanced recursion patterns (para- and apo-morphisms, that dually correspond to "access to children arguments" and "define whole future results at once") that are actually defined in a type-theoric way rather than category-theoretic way in the paper. This whole development can be conducted in type theory (initial algebras corresponding to the well-understood and equally simple notion of recursive eliminator). That is therefore a good example of a paper that was internally produced with design intuitions coming from category theory, but that can just as easily understood by someone with no knowledge of CT at all. I can't comment on whether CT notions *helped* to get the results presented (maybe they've been essential, or maybe they're more a choice of presentation that the authors like, it's impossible to say in retrospect as the nicest idea appear very simple); but they're probably not necessary to understand the paper in depth and apply the techniques to similar problems.
D =~ D =&gt; D. I take it to mean the "foundation for everything" really means domain-theoretic denotational semantics, and "everything" means "all programming language semantics".
I'm so glad to hear this! Thanks for pointing it out, I'll have to check out these books.
Hmm... they're using f-algebras pervasively. Sure you can say those are type theoretic rather than category theoretic in their presentation, but only, I suspect, because they're categorical constructs that have been _so pervasively accepted_ into CS. The place where CT really seems to come into play is in the dual constructions. Using initial algebras pays off, for example, because things are then transported to final coalgebras. The adjoint folds paper perhaps plays its categorical hand more overtly: http://www.cs.ox.ac.uk/ralf.hinze/SSGIP10/AdjointFolds.pdf
This definitely looks like a bug in the 2.7 documentation. Still WIP, but this module provides some quasi-quoters that let you do this: https://github.com/mgsloan/quasi-extras/blob/master/src/Language/Quasi/Ast/TH.hs The goal is to allow any subcomponent of a TH AST to be spliced - currently it's missing anything that's stored as a list. The nifty thing is that it also works for pattern matching, though this may not be wise for writing reliable code (and TH code should be reliable..). I remember seeing that someone else had done something similar (using a haskell parser to generate a templator / pattern matcher), but haven't been able to find it after a quick search. Anyone know what I'm talking about?
Others have mentioned that this can be done with mdo. It can also be done with plain old recursion, if you generate the Unique values ahead of time: main = do maryID &lt;- newUnique bobID &lt;- newUnique let mary = UniquePerson "Mary" [bob] maryID bob = UniquePerson "Bob" [mary] bobID print $ (friends bob !! 0) `is` mary print $ (friends mary !! 0) `is` bob 
Good question although I'm not sure if I'm in a good position to answer. I've already dabbled with Snap quite a bit (at Snaplet level and putting a couple of small apps together) and I'm probably already pretty blind to the types of problems a Snap beginner would see. I do remember that in the beginning I would've wanted to see more comprehensive example projects; as I couldn't find any, I wrote my own example projects with the help of the Snap devs on #snapframework (esp. mightbyte!). One of these is now included as Snap's "default project template" and the other one is what this blog post talks about. I don't have any practical experience on Node.js, other than what I've read about it on the internet. I guess I highly favor Haskell over other programming languages, and developing anything more complicated on the backend side would feel pretty daunting in a dynamic language like JavaScript. Interestingly, many people see JavaScript daunting on the frontend side too, and that's why we have things like Fay. :)
The "enumerator" package defines Iteratee as a monad, and exposes its constructors. Using those constructors, it's possible to construct an iteratee that violates the monad laws. This isn't a fatal problem, but it's annoying because "fixing" the ability of users to violate monad laws would require significant changes to the public API.
Only certain base monads, but you shouldn't use it. The correct implementation is here: http://www.haskell.org/haskellwiki/ListT_done_right That one is "correct by construction" and won't violate the laws in any circumstance.
`t^t` is just algebraic notation for `t -&gt; t`. This is the type of terms in the untyped lambda calculus, a *universal* formal system. A century ago it was noticed that this had no non-trivial solutions in set theory (without contradictions). I was being somewhat flippant when I said "foundation for everything." The idea is that since the lambda calculus is in some sense "the most powerful" system you are ever going to come across, and it is reasonable to think about the foundations of math as being computation (Robert Constable and his intellectual descendants make this point particularly well), so denotational semantics for the lambda calculus is of deep significance. Actually, as sclv says, Dana Scott was working on *typed* lambda calculi. I believe it was Reynolds who showed that even pretty restrictive type languages don't have set theoretic models. It turns out that untyped lambda calculus is really a special case of typed lambda calculus with recursive types (even though it is a *universal* special case). In haskell you could newtype Univ = Lam (Univ -&gt; Univ) it turns out that all though this is universal, it is not very useful (you can perform arbitrary computation, but not get it back to Haskel...also GHC chokes on it sometimes). A better type is approximately the type of values in Scheme. data Scheme = Fun (Scheme -&gt; Scheme) | Cons Scheme Scheme | Nil | Number Integer | Label String which you can think of as being a surprisingly usable dynamic language embeded into Haskell. As Robert Harper says: untyped languages are typed languages.
This might be a newbie question in all this discussion about the name of the inner function, but here goes: Why would you have it at all? Seeing such a vertical function when I expected a horizontal one was so confusing.
Could you explain that?
Perhaps I over sold my case: I just wanted to argue that topology is of relevance to computer science. Modern "algebraic topology" is perhaps more algebra than topology, but no one thinks that algebra has nothing to do with CS. 
Nobody has really needed it badly enough. Also, I think `LogicT` fills this niche, too.
Well, anything like this is going to boil down to arbitrarily decided limits. To my mind, it feels like list comprehensions with guards are a "built in syntax for filter and map wrapped up in one package" and would thus not be "allowed" when re-implementing filter. Other people might disagree, though.
Well, I guess. I'd almost rather `ListT` not be exported than have a wrong one exported.
He joined the unit but it didn't make any difference.
Well it's good to know the Snap devs are there to help. When I was looking at Snap the lack of examples did make it hard to get a feel for things. Sounds like I'll have to give it a try though. Do you favor Snap over other Haskell frameworks for developing RESTful services?
What's the difference in ghci between: &gt; :m Data.List and &gt; import Data.List ?
I haven't used other Haskell web frameworks, so I don't really know. I'd imagine implementing simple REST interfaces will be similar in all the major ones..
typing fail, I did not intend to say "pointed sets and partial orders" but rather "pointed partially ordered set and monotonic functions." Actually, the real category of interest is "pointed directed complete posets with supreme preserving maps." Googling "cpo lambda calculus" or "poset lambda calculus" will find you much denotational semantics goodness. I am not really an expert in denotational semantics so can only give you a very introductory feel for these things. Well, in most versions of set theory, I am pretty sure that $\not \exist x. x = x -&gt; x$ But if `1` is the set that contains only the empty set, than 1 is isomorphic to `1 -&gt; 1`. So, it matters what you meen by "solution." In CPO you have infinitely large solutions (which is good, there are infinitely many programs in the lambda calculus). 