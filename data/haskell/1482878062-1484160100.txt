&gt; Monoid is any type that is appendable so that two elements can be combined and create a third of the same type. That's a magma, and (almost?) useless. A Monoid is a binary operation and a default value, which relate to each other via identity and associativity. 
`forall a b. a -&gt; b`. I don't think it's possible to infer type class constraints for rank-2 types.
[These](http://hackage.haskell.org/package/hledger-lib-1.0.1/docs/Hledger-Data-Dates.html#v:splitSpan) seem expanded by default, I don't know why.
idk the syntax, but the collapsible section is -- ==== __Examples__ -- -- ... while the non-collapsible is -- === Examples: -- ... Maybe the newline?
[Sections are only rendered as collapsible if the title is surrounded with `__` as in `== __Examples__`.](https://github.com/haskell/haddock/blob/240bc38b94ed2d0af27333b23392d03eeb615e82/doc/markup.rst#headings) **EDIT:** I have opened [an issue to update the Haddock user guide](https://github.com/haskell-infra/hl/issues/189).
Addition is idempotent for algebraic structures?
I wish they would nix is awesome. Unfortunately I have been seeing the pattern of re implementing nix ideas a lot in the programming language development community lately. 
This. Make it possible to ask for documentation directly on Hackage. Make it possible to comment on Hackage. Make it possible to warn users about unmaintained packages, and alternatives. Just make it possible for users to engage. This is a rehash of the upper bounds discussion where the end was that some elite people could set these. Don't fear the mutability! 
The `tasty-travis` stack template includes doctests. If want to customize your own template, you can use it like stack new my-project https://raw.githubusercontent.com/sjakobi/stack-templates/master/my-template.hsfiles
I've not seen an algorithm presented for Rank-2 types with type class constraints. There's definitely one for Rank-2 types without type class constraints. --- To unify two concrete types (e.g. `Int` and `Char`) into a common type class constraint, I think you'd have to work with a closed hierarchy of type classes (very not-Haskell). So, you'd get universal quantification. To unify a constrained type and a concrete type (e.g. `Num a =&gt; a` and `Char`), you'd have to check the constraint context and see if the concrete type satisfies the constraint. If so (i.e. if `Num Char` is a known instance in scope), then you can just use the constrained type. If not (i.e. if `Num Char` is not [yet] present), then you promote to universal quantification. Note that, in the absence of orphans and constraint kinds `Num Char` can be statically located, or it will not exist. To unify two constrained types (e.g. `Functor m =&gt; m` and `Monad m =&gt; m`) you just combine the constraints discharging the weaker one if possible (e.g. instead of unifying to `Functor m, Monad m =&gt; m` you unify to just `Monad m =&gt; m`).
I don't think so. Any time in any other language I attempt to use a new library I "learn" to do that through the documentation. Haskell is absolutely no different in this manner, except sometimes (more often than I would care to mention) such libraries simply don't come with good enough documentation. Yes, the types are enough for more experienced users but it is just anti-newbie to imply it should ever be enough for beginners. It isn't. I'll take your point that maybe at some point it will be, but then they are not exactly a beginner, no?
Is it really better to be total by replacing bottom with unsensible/wrong results? 
Yeah, but is it sensible to have it infered at all then? Would it be useful?
Yeah, I would like to dig deeper to find out why this happens. Right now it is on my "nice to have" list since it isn't blocking me doing anything.
That bit was, of course, almost entirely a joke.
Oh oops. I reread the original comment, you are right. 
There's nothing academic about associativity. When I wrote enterprise Java MapReduce jobs, I had to be very aware of associativity (under the phrase "does grouping matter?") and commutativity (under the phrase "does order matter?"). Monoids are about identity and associativity: you can always provide a value `mempty` from nothing, `(&lt;&gt;)` doesn't need parens for grouping, you can parallelize computations by arbitrarily grouping different subsequences, you can fold from the start or from the end, etc. Doesn't matter what you call them, but you can't skip over that. Besides, I'm not against developing intuition, I think in examples more than in definitions. The thread is full of examples of monoids (lists, booleans, numbers, functions, etc). I was just correcting your mistake. (and lol I said "magma" because I know you're a troll, I never use that jargon)
Say we make the following list: ones :: [Int] ones = 1:ones If we ran this directly in an interpreter, it would produce an infinite list containing the integer 1 repeatedly. Now say we do take 3 ones One would think that the interpreter would attempt to first produce the whole list (which isn't possible, since it's infinite), but since Haskell is lazy, it'll really just compute until the list has 3 elements which it will return. Eager evaluation is the inverse scenario, where the interpreter would actually attempt to compute the full list before performing the take function on it. This is the way my functional programming professor taught us in uni. Hope it helps you as much as it helped me. 
Yes I have renamed the project yesterday, I need to update the blog. Thanks for the reminder!
If the docs were high quality with examples, then sure. But if they're not, then crowd-sourcing is a way to shore that up without using up dev time. Heck, community can self-police. Allow down-votes, or flagging posts or something.
A small proposal to improve the Prelude: https://www.reddit.com/r/haskell/comments/5kobcf/proposal_add_readmaybe_and_possibly_readeither_to/
Why do we need implicit imports? I have always thought they were a bad idea.
There is no standard replacement prelude. That's what we need, so new packages remain consistent with each other.
It absolutely isn't a false comparison, sorry, but you're wrong on that particular point. If you are to use a new library in Haskell regardless of whether or not you are supplied types, or a novel's worth of information - you *learn* to use it - simple as. I'm not standing on the other side of the chasm, I am slowly working my way over, and I will get there, but as you say, it does *not* need to be as hard as it is right now. 
As just a regular Haskell user, +100/0/+1
If you are following this article, you are doing fine.
+1. It is somewhat fascinating that every time someone raises the idea of "fixing Prelude", someone else mentions how impractical that would be because of legacy code, while so many low-hanging and harmless fruits remain to be picked.
This seems relevant... https://xkcd.com/927/
Well, there's a pretty big difference between "Fixing the prelude" and "incrementally improving the prelude." The former implies broad-strokes changes that *would* be awful for legacy code. The latter implies the low-hanging fruit.
It never happens in this case because 1000000 is not 0, so purely because the critical parameter was a hard-coded constant.
What I mean is that there is a continuum between "doing nothing, because legacy" and "breaking everything now for the sake of correctness". The risks of the latter often serve as an excuse for choosing the former. Improving the prelude, updating documentation to emphasize the risks of partial functions and encourage the use of safe alternatives, marking unsafe functions as obsolete and reporting their use with a compiler warning for a few major GHC versions before removing them sounds relatively reasonable. Actively used packages will be updated in time, developers who care about their code compiling with the latest GHC versions will implement the advised changes and the other ones can still use an outdated compiler and set of packages with Stack. It's not like we are bound to maintain backward compatibility forever. That's already not the case, anyway: 5+ years old Haskell code frequently does not compile with newest GHC.
I agree in principle. However the typical Prelude partial function fails with some completely useless error message last time I checked. Something like *** Exception: divide by zero without any context on where in your program it happens is worse than a Nothing you can turn into a loud error that gives you context.
I don't think anyone disagrees with the general concept of moving the prelude forward. People are making sure we don't destroy the entire Hackage ecosystem with one big prelude update. In reality, I think pretty much everyone agrees we should make incremental steps. The debate is just over *how fast* we do that.
Don't get me wrong, I'm fully in support of a prelude without partial function, actually I'm using one alternative prelude. However I have two points in favor of partial functions sometimes. There is a lot of function which are not naturally total, for example in the math domain, `div`, `factorial`, most trigonometric functions, and I don't have a solution. I want safety, but in the other hand, I don't want to write numerical code wrapped by `Functor` and `Applicative` code, because that becomes totally unreadable and that stuff propagate like hell ;) Who wants to write: liftA2 (*) (return pi) (liftA2 (cos theta) (liftA2 (*) (1 / alpha) (exp =&lt;&lt; (alpha * t / factor)))) Instead of: pi * cos theta * (1 / alpha) * exp (alpha * t / factor) Perhaps a `Num` instance for `Maybe` can help actually? There is also a lot of algorithms which are total but internally you can write them using partial functions. I realized this this month when I was doing the puzzles of [Advent Of Code](http://adventofcode.com/2016/) which involves writing lot of small algorithms. I was stuck by the amount of boilerplate I needed to write just to get ride of the partial functions. For example, I needed the first char of the third item of a `[[Char]]`. A trivial solution is `head (l !! 3)`, however it introduces two partial functions `head` and `(!!)`. Finally I ended writing it as: case head =&lt;&lt; (index 3 l) of Nothing -&gt; error "I did something stupid somewhere because this is not supposed to happen" Just v -&gt; v (This suppose `head :: [a] -&gt; Maybe a` and `index :: Int -&gt; [a] -&gt; Maybe a`). Finally I earn nothing, except that my code is now less readable, so here I rather prefer using an `unsafeHead` and `unsafeIndex`. I have the same issue with `NonEmpty`, it is a wonderful type when you don't `uncons` items from the list, but in this case you don't need the `NonEmpty` guarantees, but it starts to sucks when you write an algorithm which consume the list. And wrapping an infinite list inside a `NonEmpty` infinite one is kind of boring ;) So I'm in favor of a total prelude, but with access to the partial function. Or keeping the prelude as it is and trigger warning on the usage of partial function (Warning which can be suppressed locally in the program with a pragma for example, or suppressed by liquidhaskell when it can proove it is safe). That's why I think that just getting ride of the partial function is not a solution.
What is the right pattern to get the first matching element from an infinite list, then? I think the two obvious choices are to inverse the predicate and use `head` + `dropWhile` or to use `fromJust` + `find`. Both are partial but the function is as well - either it finds a solution or it doesn't return. Sometimes you can do it in a less roundabout way like `until` but I had this problem several times during the advent of code riddles when searching in a recursive list definition.
Somewhat relatedly, [here](https://github.com/aelve/haskell-issues/issues/51)'s an idea on how to reduce the cost of GHC upgrades.
How about also `nonEmpty` and `(:|)`, and the type constructor `NonEmpty` ? People using these will often still need a qualified import of `Data.List.NonEmpty`, but having the basic constructors in `Prelude` will hopefully encourage them to be used much more often. To be clear - this is a separate proposal that should not affect acceptance of the existing one.
I know. There still are many packages that contain their own definitions of `readMaybe`: https://github.com/search?l=Haskell&amp;q=readMaybe&amp;type=Code BTW, if anyone knows how to search for definitions (both exposed and internal) in Haskell packages, I'd be glad to hear about it. So far, Github search (where I can't differentiate between definitions and usage) and Hoogling on Stackage (where only exposed definitions are shown) seem to be the best options.
Well, if you compile with profiling, then you can actually extract a stack trace. Hopefully Haskell will catch up with every other language implementation and always have useful stack traces. Until then, it's probably better to have partial functions always take an error message to use for context. That could also be a neat indicator for partiality at the call-site and a chance to explain why it should never happen.
You're right, unfortunately there do seem to be some packages that define their own copy of that function with the same name. I found at least a dozen of them with a Google search, and Google isn't very good at this kind of thing. It would be helpful if someone with easy access to a full Hackage tree would get an accurate list of such packages.
Why not encode a single element `e` as `(1, e)`?
I'm not doubting that you can pack it into a memory-dense representation, but I'm thinking the advantages are MUCH lesser than with dense matrices since the access patterns are worse.
So basically the more people feel the need to write their own identical definitions of a function the less likely it is to be added to Prelude? That seems backwards.
I'm pro `readMaybe`, but until it's in the standard prelude, I'd like to see more emphasis given to `reads` over `read` in tutorials, since `readMaybe` can be easily implemented in terms of `reads`: readMaybe s = case reads s of [(x, "")] -&gt; Just x ; _ -&gt; Nothing
[Annotating patches with updaters](https://github.com/aelve/haskell-issues/issues/46) would trivialize this and also solve the problems in the other comments.
I don't mind the tail calls being squashed into a single stack trace entry. That would also be very useful, but we don't even get that.
Tangentially relevant [`Semigroup` / `Monoid`](https://prime.haskell.org/wiki/Libraries/Proposals/SemigroupMonoid) proposal
It uses a Web interface only in two utility programs: * ELFWeb: a kind of objdump * X86Web: list of the X86 instructions recognized by the disassembler (the dumped output is here: http://haskus.org/~haskus/haskus-x86/) I need to publish the code of examples using the DRM rendering interface.
Vaguely related comment. I never used `darcs` but people tell often that the mental model used when dealing with it is simpler that the `git` one. Is this valid in this kind of situation?
Yes! And it should be noted that `Text.Read` is in `base`, not the `text` package.
Do you have any particular reason why you made it a web interface? Just out of curiosity.
GUI programming in Haskell is quite immature (cf https://github.com/Gabriel439/post-rfc/blob/master/sotu.md#standalone-gui-applications), hence I tend to use an embedded web server (happstack) when I want to provide a GUI interface. It is not the panacea but it is quite simple.
Not tangential. Since Semigroup-Monoid is in progress but not done, now is not a good time to add `&lt;&gt;` to the Prelude. Adding the one from `Monoid` interferes with the transition, and adding the one from `Semigroup` could break many packages until they finish their transition. There's also the clash with pretty printing libraries. But that seems to be less of an issue now. Especially since at least the classic Hughes and Peyton Jones pretty printer in the [pretty](http://hackage.haskell.org/package/pretty) package has now added a `Semigroup` instance that makes its own `&lt;&gt;` and the one from `Semigroup` coincide. I would love to have `&lt;&gt;` in the Prelude, but I think we may have to wait a release or two for that one.
Care to elaborate? The amortized time/space usage also depends very much on the actual algorithm used, e.g. whether transpositions are required or not. I also wonder why /u/haskell_caveman explicitly requires `repa` ..
I should note I'm not referring to a quickcheck property - my interest is more toward optimization. In particular, while quickcheck may say functions are equal which are not, this would be more restrictive. 
I eventually started using `Stack` in 2016. Stack is a must have in every Haskeller toolset. 
I eventually learned how Automatic Differentiation works, thanks to [Daniel Brice](https://www.youtube.com/watch?v=q1DUKEOUoxA) great talk about implementing AD in Haskell.
[An algebra of graphs](https://www.reddit.com/r/haskell/comments/5gzpqb/an_algebra_of_graphs/?) and it's [alga](https://github.com/snowleopard/alga) library is a simple and elegant piece of work.
Maybe React's approach? You could produce a pure tree on each frame, and write a separate phase which diffs those two trees and converts the diff into imperative actions which create, modify, and delete widgets. I began working on a [library](https://github.com/gelisam/memento) based on that idea a while ago, but I got distracted.
How does FLTKHS deal with callbacks and listeners? Do/cab you abstract over them with ContT or something?
Recursion Schemes [March 2013 : Recursion Schemes by Example](http://www.timphilipwilliams.com/slides.html) http://blog.sumtypeofway.com/ 
[Data types à la carte](http://www.cs.ru.nl/~W.Swierstra/Publications/DataTypesALaCarte.pdf)
fisheye views, treemaps
My last company used darcs before switching to git. It wasn't a Haskell shop. I found darcs good at the one thing it's meant to do (reordering patches) and bad at everything else, with a terrible interface too.
&gt; it isn't that difficult to remove the conflict markers. Do you think this way where you apply changes to both the base and the target section is less error-prone? Exactly, having git-mediate verify your patch reduces errors. &gt; Also, how does it handle those multiple-head merges where you get nested conflict markers and labels like "temporary merge result 1"? Actually, I haven't ever figured out what triggers that behaviour. I actually haven't seen those.. Don't know.. &gt; Finally, does it do/need to do something interesting about indentation changes? The same method should work for all types of changes. Is there something special you think it should do for indentation changes?
&gt; (Bonus) I don't understand why Pipes.Attoparsec.parsed accepts its input as a function arg. I would have expected the parser to just be a Pipe that accepts input by connecting it with (&gt;-&gt;). I've put the type below for convenience: A `Pipe`by itself can't detect end-of-input. A function from `Producer` to `Producer` can; that's the reason pipes parsers tend to have that form.
All the reasons against the PVP seem mostly aesthetic, save the issues with bounds and deprecation (which I don't think SemVer does any better). The *advantages* to PVP seemed mostly functional and more important. But all told, the difference seems much more negligible than the downside of having inconsistency across Hackage. I would never advocate PVP in a primarily SemVer community, nor vice versa, because consistency matters far more than any of the differences outlined in the OP. So I think it's harmful to the community to dissent for such negligible reasons. EDIT: But at the end of the day, it doesn't matter much and you can use whatever version scheme you want.
It makes sense to divide a dense matrix into blocks if you only need to use two blocks at any one time to generate the result of your operation. This helps because hopefully both blocks get cached so the operations are fast. I don't see how the same technique would apply if you're working with a sparse matrix since you no longer have the "close in the matrix == close in memory" property. That being said, I know very little about sparse representations. 
I wish more people knew about Nix. Stack is great, and I recommend it highly. But I personally have found Nix to be a more powerful system that solves all the same problems as Stack and more. And yet people act like Stack is the only viable option. Edit: Really, I'm just trying to say that Nix is a viable alternative to Stack. Not trying to say that anyone has to prefer Nix. I understand most people prefer Stack's simplicity.
Perhaps you could write up a document/blog post on how you use Nix for haskell development - I for one would be very interested.
Yea the main draw to Nix to me is in building and deploying production systems. When publishing just general open source Haskell packages, I'm not going to include the Nix files, because they're often not super relevant. But I have found myself more and more using Nix for local development instead of Stack, even if I don't put the configs on github. Anyway my point was just that Nix can be a fine alternative to Stack. You don't *have* to use Stack
Yep. My biggest problem has been seeing people do something with Nix, and having NO idea how to replicate it in my use case. Just a docs problem.
Fantastic story!
&gt; Anyway my point was just that Nix can be a fine alternative to Stack. You don't have to use Stack Not right now in my opinion. Stack **Just Works**. I go back to nixos every once in a while and I never get to program anything because I'm busy trying to figure nix out. Perhaps I'm just slow, but I put a lot of hours into trying to be productive with nix+Haskell. I even went through the [nix pills](http://lethalman.blogspot.it/2014/07/nix-pill-1-why-you-should-give-it-try.html) course, have read the Nixos docs for Haskell development many times, and have even contributed to the library packages for Nix. However for some reason every time I try to just go do Haskell development on Nix something stops me. I wish I had something more concrete to provide. Next time I try using nix for Haskell development I'll try to document my progression. All in all though, I'm kind of sad I can't use NixOS for all of my daily computing without taking a big hit to my productivity.
&gt; I've been using Nix for Haskell development on OS X at work. Nix itself works perfectly well on OS X; How recently? I know about a year back coreutils or something was broken which kept anything from building on OS X. That kept me from using NixOS at work :/
Really making good use of lenses, once you start using them they become a hammer that in a world where it turns out there are a lot more nails than you realised existed, which can all be beaten in a consistent, composable way. Also making great use of classy prisms for error handling allowing for composable error hierarchies, along with much greater use of mtl style monadic code so I can precisely specify the requirements each function has for its behaviour making them much more composable (I'm seeing a trend here).
There is a little of that in the first video above. See also http://hledger.org/developer-guide.html#implementation-notes
&gt;That leads to widely-used, rock-solid packages on major version 0. Why is that a problem? 
That's the thing, though. SemVer can be mechanically enforced. That's what Elm does! The PVP can't be done mechanically because all it can do is give you an option for breaking changes. A human still has to decide. Edited to add: Elm requires published packages to be `&gt;= 1.0.0`. So it sidesteps the whole 0.y.z problem.
&gt; which can all be beaten in a consistent, composable way. Somewhat related: I hope lenses become a part of GHC in the vague future. They're in pretty much every big project eventually. 
I learned a lot more in terms of actual architecture, but implementing a website in `yesod` was my big project so far. And I submitted a package to [hackage](https://hackage.haskell.org/package/qr-imager), which felt cool.
A declarative approach is really useful. Look at React for example.
Great idea! I opened issues about those problems. 
Same here. The best advice I could give to someone new to them is to start with the [microlens](https://hackage.haskell.org/package/microlens-0.4.7.0/docs/Lens-Micro.html) library, since it has the most accessible documentation for newcomers. The lens library has a separate module for lenses, traversals, prisms, etc., which makes it much harder to figure out what it is you actually need.
I'm in favour of adding as many [safe](https://hackage.haskell.org/package/safe-0.3.10/docs/Safe.html) alternatives to partial functions to Prelude as possible. On a related note, the main obstacle to changing Prelude is that it could potentially block maintainers of legacy code from upgrading to newer versions of GHC. One potential workaround for this would be to have a series of modules called `Prelude_8.0.1`, etc., which would then allow consumers to import an older version of Prelude for use with newer GHCs (by using NoImplicitPrelude). The only difficulty with this would be the question of who actually maintains it.
&gt; SemVer treats all breaking changes the same, which I think is the right approach. A small change from the package author’s point of view might be a big change from the package user’s point of view. Similarly, a ground-up rewrite might not introduce any breaking changes at all. This pretty much sums up an attitude that I think Haskellers need to work hard at avoiding. Of *course* it matters whether a new version just renames a function, or makes fundamental breaking changes to the underlying concepts of the library. One of those is a 5 minute fix, while the other could stall a project for years! To argue that both of these should be treated as equivalent merely because they both meet a technical definition of incompatible is a level of reductionism that really isn't reasonable. That we can convince ourselves that it *is* reasonable is a serious flaw in our community. It's easy to see where it comes from. Haskellers tend to care a lot about correctness and provability. But it's a mistake to let that blind us to legitimate software engineering concerns simply because they aren't amenable to proof. We can't forget that, in the end, software engineering is a communication discipline. I'm not really taking a side on which version scheme is right for Haskell. If I did state an opinion, it would be to say that a debate over which version policies should be adopted is pretty useless until you have a concrete answer for what it means, exactly, for a version policy to be adopted by Haskell). But here, I'm just responding to that comment, and the general perspective that it represents.
You could certainly enforce that backward-incompatible changes correctly bump a minor version number... The question is whether you'd want to. The question there isn't so much about whether it's technically feasible, as about what purpose Hackage is intended to solve. If you ban packages that don't follow the PVP from Hackage, we'd lose a lot. And what would we do with the existing package history, which certainly doesn't meet those constraints? What is the cost of losing packages that someone might be willing to upload, but not reversion to fit the enforced scheme? What about the many packages that follow a widely recommended pattern of exposing implementation detail in internal modules, which explicitly do not guarantee compatibility? This question gets much harder if you want to copy something like Elm's requirement that published packages have non-0.x versions. Why would you want to push early adopters of these packages to an inferior development workflow by keeping them from using Hackage? Version 1.0 as an announcement of widespread suitability for general use is a powerful communication tool, and taking it away just because some proposed version rules aren't flexible enough to allow this widely known convention would be a shame.
The scheme I use is to bump A if the change will break any of my own packages, and bump B if it could break anybody else's under the PVP. This single change reduced about 80% of the administrative overhead of package maintenance for me. Prior to this change, just dealing with ripple effects from bumping a version below me took almost all my available development time. Those were very dark and boring years. =)
Consistency here is what matters. Right now every single package I depend upon follows the PVP. Though, some have minor caveats that `.Internal` module changes aren't considered API breaking and allow minor bumps. (Including my own.) If all of a sudden a fifth of hackage starts switching to SemVer that is an additional useless bit of information I have to track in my head about every single package and if I get it wrong, all the code using SemVer will get overly liberal bounds that mean my upper bounds are too liberal. * There isn't a migration scheme. * The PVP predates SemVer. * There isn't an appreciable advantage to recoloring the bikeshed. We become a bit better aligned with the rest of the world in exchange for literally every single package out there having huge version numbers. There were folks back in the day that used to use dates for version numbers in the haskell ecosystem, too. Fortunately, every one of those that is still in use seems to have recovered from their madness, but the version numbers for those packages will never recover. See [`numbers` version 3000](http://hackage.haskell.org/package/numbers). ;) * We lose the extra A.B major version number and drastically increase the amount of work I have to do on a daily basis. For that last reason alone I personally will likely be the last person to turn out the lights on the PVP.
Why not use runghc instead of compiling then running?
In 2016 I've learnt quite a bit about the intricacies of dependently typed programming in Haskell. I recommend reading Richard Eisenberg's brilliant thesis, ["DEPENDENT TYPES IN HASKELL: THEORY AND PRACTICE"](https://github.com/goldfirere/thesis/raw/master/built/thesis.pdf), to anyone interested in the topic - lots of interesting stuff and a very promising direction for future Haskell.
This is not likely to happen. First off, Idris is strict by default, so Haskell libraries are unlikely to interact well with it. Second off, while the type systems seem to resemble each other at a surface level, the dependent type theory that Idris is based on is quite different than the System FC that is at the core of GHC, and impedance mismatches could be unpleasant. I'd be happy to be proved wrong, though!
awesome. the other story is how? the way I understand it is in imperative paradigm the mutation is explicit while in the state monad it is implicit which means only the functions chained together using state monad will use that state. so that means the computations that are chained using the state monad could be evaluated in parallel simultaneously with other haskell functions/expression. please correct me if am wrong. edit : is doing mutation through state monad keep the referential transparency for our program 
&gt; Why would you want to push early adopters of these packages to an inferior development workflow by keeping them from using Hackage? The problem is that every such experimental upload ends up causing costs for multiple systems and people involved. It starts by being included in the primary package index eternally w/o the chance to ever remove it, i.e. every single cabal client needs to download the meta-data for experimental packages, our mirror infrastructure replicates this data, cabal (and to some degree the solver) have increased overhead to process all this meta-data, experimental packages reduce the signal/noise ratio on Hackage, and last but not least, Hackage Trustees get overloaded by often rather costly to fixup experimental packages. Here's [an older but related discussion going into more details about the costs](https://github.com/haskell/hackage-server/issues/461). However, there's actually an idea implied in that discussion on how to address these problems and reduce the tension, and improve the overall quality on Hackage (which is something I care deeply about): Finish the "package candidate" feature and implement the secondary "candidate package index" (which is a mutable index; hence you can remove packages again), and allow experimental packages to use the Hackage (candidate) workflow (i.e. cabal clients can install packages from the candidate index seamlessly, *if* they opt into the "candidate" distribution channel; by default, cabal clients only install from the "production" primary index - c.f. `apt` which has a similar feature) while not polluting the primary "production" index. Hackage Trustees would mostly ignore the candidate index and would only need to focus on packages whose authors have promoted them into the primary/production index, thereby committing to them and declaring them ready for general use. The Matrix builder could be made to allow testing also packages from the candidate index (albeit with lower priority) to give early feedback. We already have most of the pieces in place in Hackage &amp; cabal, we just need to finish this up at last.
Perhaps you coud use the Reader monad instead of defining a new type, something like `Reader FooId Bar`.
90% maybe for some of us. I wouldn't be interested in the example here exposed since the type already told me exactly what the function would do (`[Either a b] -&gt; [a]` can't do anything else reasonably). There's also tons of case where just the type + description are enough to dispel any confusion. Examples are really useful either for beginners or for quite complicated functions, that would require several examples to better understand.
"Populist" is a politician you don't like. If you like him, he is "pragmatic". The same here, "vocal people" are people you disagree with. It is much more honest to say "they are wrong".
I started four or five months ago. There were some teething issues, but things have been working pretty well lately.
I disagree. "Vocal" to me is a term that describes people who make their opinion known in a very noticeable manner, often giving it more attention than the part of the population holding that opinion would otherwise warrant. In this particular case I do not think they are wrong, I just think on this issue their desire to be warning free without code updates should not be given so much priority over other concerns.
The topmost thing I learned about Haskell was... Haskell. Well, that's not entirely true, I had a course on Haskell first year at university (and somehow I even got a perfect score on that test), but I never felt like I actually *knew* Haskell. This year I started working on a personal project (a compiler) and I forced myself to use a variety of techniques/libraries that I'd heard of but never really grasped. It's kind of funny actually, most people talk about Haskell's major learning curve as though it were the language itself, but in my experience the language is fairly simple (if you have a bit of a mathematical mindset anyway). The ecosystem, and learning how to build "real" applications in Haskell, *that's* the learning curve.
&gt; Version 4.14 of lens would not be matched by a constraint of &gt;= 4.14.0. That’s because version 4.14 is less than 4.14.0 according to Data.Version. Data.Version is not doing something unique to the PVP here. joey@darkstar:~&gt;dpkg --compare-versions 4.14 '&lt;=' 4.14.0 - exit 0 joey@darkstar:~&gt;dpkg --compare-versions 4.14 '&gt;=' 4.14.0 - exit 1 
"Don't build an interface that relies on `-XTypeApplications` (if you can at all avoid it)." I don't know if this will get better with full dependent Haskell or explicit type arguments, but for now, dealing with the resulting ambiguous types and/or non-injective type functions tends to be painful.
That's true. My point was that the PVP allows for different numbers of version components and that Data.Version is the authoritative way to compare them. By comparison, SemVer requires exactly 3 components. 
I used Nix for three to four months. Honestly, Stack has a much better user experience than Nix. Also, Nix has a much more learning curve. You have to be comfortable with it's language and nixpkgs to start writing the nix configuration file.
Yep. Completely agree. But Nix is more powerful I think
I started to have some doubts about trying to sell Haskell/FP using the "look ma no hands" thing like scan/fold/map/etc as a winning point. Most of no-FP devs will look at it as just another "library" that can be easily embedded into some compiler/interpreter/sdk (i.e. Java 8). What it should be explained better is the "orthogonal" approach between FP and OO and why FP is eventually better. 
To be clear, I support deprecation warning. IMO warning free policy renders warnings useless.
Just noticed a typo. Simon Peyton John should be Simon Peyton Jones
Yeah, let's wait for Semigroups. 
Thanks, I'll have to try again. I really like Nix, I think I just started trying to use it when too much blood was required for making things work ;)
What would the advantage be of having `lens` be part of GHC?
But `-XAllowAmbiguousTypes` + `-XTypeApplications` lets you turn foo :: Proxy a -&gt; Int -style typeclass functions into just foo :: Int !
Why? Once you already know Nix, it's pretty easy to get a build system equivalent to Stack, and then you never have to touch it again. It shouldn't consume any of your time after that
I think I know what you mean, but could you show an example?
No, it doesn't talk about the reverse mode. And I have no idea how it works. That would be great if someone can point to a good learning material on that.
&gt; I think this is a good place, I learnt this directly from Tony: http://tonymorris.github.io/talks/#9ae5e0fa2e1b0db7589dc5398bab4b07dc971e21 Interesting slides and [talk](https://www.youtube.com/watch?v=UXLZAynA0sY). The typeclasses they describe seem to be a bit different from the typical TH-generated "classy lenses" in that they take more parameters... I wonder how widespread is that style.
Terminal emulators are sloooooow. Do your timing on runs where you're redirecting the output to a file or something. 
guaraqe told you two things: How to write `getPents` in idiomatic Haskell code (without changing the algorithm), and secondly, how to change the algorithm of `getPents` to make it fast. As you've discovered, just rewriting the code in idiomatic Haskell has not changed the performance at all, so, yes, you need to use his other suggestion.
Hopefully someone can affirm/disaffirm what i'm saying, but I believe the state monad only works for finite concurrent operations as the operation on the separate thread has to finish before its new state can be merged back in. So, for instance, a polling thread is not useful with the state monad. 
Yes, and that looks nice as an idea, but I've found that if you're actually using it it's annoying: either 1. You're not really using `foo` as a type class method on type `a`, or 2. You're going to find yourself having to turn on `-XScopedTypeVariables` and use explicit `forall`s and type applications everywhere instead of just passing a parameter along.
From a quick glance, this looks _very_ impressive. However the name is unfortunate, if you speak some Slavic languages. In Polish it literally means "race". Not the one with fast cars. The one "racism" derives from.
yi-rope is a GPLv2 library and your code is MIT licensed. Those licenses are not compatible. I don't think the author of yi-rope will mind that you are using his code, but I think you should at least know this. It might be a good idea to contact him about it. His concact info is on hackage.
People would stop complaining about how long it takes to build or suggesting that people not use it in packages. On a more serious note, it would provide an in-language/canonical fix to the problem of nested record syntax, and eliminate the need to use TemplateHaskell to do anything with nested records. 
Being an editor in Haskell, how does it compare to Yi, which is supposed almost to be a "library for creating text editors"?
At the moment Rasa exports a single function which is imported into the user's config where they set up a `main` that calls the function with their configuration; See the example config [here](https://github.com/ChrisPenner/rasa/blob/master/rasa-example-config/app/Main.hs). So right now it needs to be recompiled with every change. This is unfortunate of course, so I've already started looking into [Dyre](https://hackage.haskell.org/package/dyre) Which I believe is how Yi does things (it allows dynamic recompilation). It doesn't look too tough to work with, and the fact that it's still compiled should help with performance and error checking. As far as pluggability, that's pretty much the core concept of Rasa, getting the proper granularity/abstraction level is tough, but I think that the `Action ()` and `BufAction ()` monads work pretty well.
[removed]
When I get some more of that elusive spare time I'll try to document my attempts/struggles. I think I know the nix language pretty well, but it's the overall architecture that ends up confusing me.
Yeah, I'm not sure I picked the best quote for the title. I think what the author is saying is that some people associate the mathematical language and academic form of many Haskell ressources with elitism. The author doesn't really comment on the conduct of the Haskell community.
As someone new to Haskell, this criticism holds a lot of weight. I'm not sure I'd go so far as to call the community as a whole elitist or cold, but the available documentation is very much this way. There is a lot of pressure put on the novice to grasp, for example, "basic category theory" and the "simple Monad". The rampant reek of academia does the new guy no favors, and is hands down the worst thing about the language. Combine this with a Prelude that is mysteriously crippling in some profoundly arcane ways, and a near total lack of functioning tooling / editor support on anything that isn't Vim or Emacs, and it's a wonder that anyone interested in getting things done sticks with it long enough to learn what the pros are. I'm really glad I did, and having gotten past the foothills of the mountainous learning curve I am feeling better about it all the time, but I will say that of the dozen or so languages that I've achieved basic literacy in, Haskell was far and above the most brutally uninviting experience.
But this can't be true when Rust is literally Haskell.
Which one is that, if you don't mind? 
Alternatively, you could release your code under GPLv2.
The only correct answer is that you should interleave learning bits of all of them. Take lambda calculus, for example. The basic idea of lambda calculus is just substitution as a model for evaluating functions. That's pretty basic, and you should learn it early on. The idea of anonymous functions is pretty simple, too; though I'd say it's used somewhat less often than is suggested by typical intro materials. It's an occasional syntactic convenience, but more valuable as a forcing function for understanding functions as values. But deeper ideas in lambda calculus include consideration of evaluation orders, the Church-Rosser property, normal-order versus applicative-order evaluation, normalization properties, and typed lambda calculi. These can wait. Another deeper track within lambda calculus is understanding how the untyped lambda calculus *alone*, with no added primitives, is computationally universal. This is unnecessary for basic Haskell programming (since Haskell *does* add primitives), but working through this can give you some intuition for ideas that can come up in Haskell, too, and you'll see discussions of Church encodings and such in blogs and libraries. Category theory is similar. The idea of function composition is pretty fundamental, and you should become familiar with it quickly. Certain categorical structures around functors, that are widely used in Haskell, are worth learning quickly (though not as quickly as sometimes believed; you certainly don't need to know any depth of category theory to be minimally productive in Haskell). You can get a long ways before you need to understand the Yoneda lemma! If you forced me to pick, I'd choose lambda calculus as a first topic to *start* learning; but not to finish (if that's even possible) before moving on to something new. 
I think it would be better if pvp asked always for the 4 components. Comparison of "4.14.0" and "4.14" is ambiguous from my POV.
I've never tried it with Pipes, but [incremental-parser](http://hackage.haskell.org/package/incremental-parser) was written for scenarios like these. 
I think that using 2st digit is secure when changes needed client-side are mechanical (i.e. could be automated or documented as finite steps). If a big rewrite is required to continue using the library, then 1st digit should be augmented. Other cases are gray area, some kind of guideline would be welcomed.
Ahh, that makes sense, yeah we definitely don't use the Monad instance to its fullest. Switching to a record like this would mean defining an instance of `Monoid` for the record which sequences the monads and concats the lists, at this point each extension defines its own Record: myExtension :: Scheduler myExtension = Scheduler { eventListeners=[Callback thing1, Callback thing2] , onInit= ... , beforeRender=return () , onRender=return () , afterRender=return () , onExit=return () } Then we'd export that and need to combine all the extensions into a single set of hooks, rasa [] $ mconcat [ myExtension1 , myExtension2 ... ] This lets us work at the appropriate power level, but I've got to admit that it seems a bit clunkier than the do-notation version; and it still implies that there's some ordering to the hooks (even though there still isn't). All said and done, this is really just bike-shedding since the functionality stays the same either way :P Any other critiques? I really appreciate you taking a look at it :)
To be quite honest, I think there is some truth in this assessment, and I think it hasn't always been that way. But the days of Haskell being an "underdog" are slowly slipping away and it's actually gaining meaningful popularity. Rust, on the other hand, is still (albeit, not for long) an underdog in the programming world. I believe this self-perception of being an underdog actually helps bring communities together; it helps everyone have a common sense of purpose. But as soon as lots of money gets involves, lots of books, lots of ideas, lots of possibilities, and lots of people, that's when a reality sets in that is hard (if not impossible) to avoid. Not to be pessimistic for its own sake, but I expect that Rust will experience more "coldness" in years to come.
&gt; I love that kind of stuff, but for a lot of people they don't care about Lambda Calculus or Monads or Lenses or Functors. They just want stuff that works well or know how to use things. To what extent, does this really mean 'they just want to do things the way they already do them'? People don't care but they should (at least care about some of them). What makes telling them that they should 'elitism'?
Yeah. I love kak, but writing stuff for it at a high level is a pain. Scripts _are_ slow, no matter how much you want them not to be. Elisp, with all its warts, is still less hackish than kak scripts.
All good points! I'd originally looked at `vault` when I was getting started with this, I can't remember exactly why I didn't go with it, but so far as I can tell the current implementation works basically the exact same way but with one exception, once you make keys for your vault (which for vault requires IO which was annoying at the time) you'd still need to store those keys somewhere, we'd probably end up storing them by their type (since the keys are typed anyways), and we'd end up with a slightly harder to use version of what we've already got. If there are other possible benefits I'd love to hear them though :) One requirement though; say we have two extensions A and B; ext A has some state which stores, extension B must be able to use actions exposed by A which depend on A's state, I'm not sure how the Action could get hold of the keys it would need, unless it looked them up by the needed type, but then we're right back where we started again.
I wanted tabs across the top, a _tabular rasa_ if you will.
Oh my... this is brilliant.
&gt; The PVP has no special case for major version 0. &gt; The PVP encourages packages to stay on major version 0. Semver does no better here. What I see happening in the Rust ecosystem is that people are reluctant to release 1.0, which in practice leads to a versioning scheme of `0.major.minor`, just like what happened with the PVP. If you look at the “most downloaded” list on [crates.io](https://crates.io/), you will find that *all* of the crates in the top 10 have a `0.x` version, even though these are the most widely used crates in the ecosystem. They are key crates with many other crates depending on them. They are the most battle-tested crates of the ecosystem. Whether the authors consider them stable or not has become irrelevant; people depend on them anyway. &gt; SemVer treats all breaking changes the same, which I think is the right approach. I used to think this until I watched [spec-ulation](https://www.youtube.com/watch?v=oyLBGkS5ICk). Now I am not sure any more. Like others in this topic have said, do you really want something like “add an extra argument to a function, but you can just pass `Nothing` to upgrade” to have the same implications for the version number as “remove half of the functions in the package”? Semver makes major version upgrades scary because they *might* break everything, yet for many users such an upgrade does not actually break anything at all. Though arguably it is the job of a good changelog to address this. A single number can only encode so much information.
&gt; They presumably have already written the solutions to ensure the exercises are correct. Yes, but you'd be surprised how many fuck-ups slipped in earlier in the book's development when I would modify the exercise and forget to check the answer or prose surrounding it.
&gt; is fairly small Ha ha ha Not only is that not true, we want people to not look up the answers anyway. Another reason we didn't provide answers is that we didn't want people to feel bad or intimidated that they didn't make something "pretty". It's more important that you do 60% of the exercises by yourself or with minor hints from other learners/teachers than that you "do" 100% but hit the answers for each as soon as you're stuck.
But how can it feel cold and uninviting, when we have burritos?
I think a better phrasing would have been that the academic feel makes the Haskell community more foreboding, rather than cold.
&gt; but that section runs FAST (0.05s for everything except the final step) That's because it hadn't done anything till the last step. You can drop the line with `n == 100000` and notice it will still take 0.05s.
Here is a version of the program (that consumes all input, and doesn't read how many lines should be retrieved first) that runs fast. Notice how it doesn't do anything fancy WRT encoding. It was the algorithm that was slow: module Main where import qualified Data.Vector as V buildpent :: Int -&gt; Int -&gt; [Int] buildpent n sumpents | n == 0 = 0 : buildpent 1 0 | otherwise = (5*(n-1)+sumpents-(n-2)*2-1) : buildpent (n+1) (5*(n-1)+sumpents-(n-2)*2-1) vpents :: V.Vector Int vpents = V.fromList (take 100000 (buildpent 0 0)) main :: IO () main = interact (unlines . map (show . getPent . read) . lines) where getPent n = vpents V.! n 
And this one should be fast enough: module Main where import qualified Data.ByteString.Builder as B import qualified Data.ByteString.Lazy as BS import qualified Data.ByteString.Lazy.Char8 as B8 import Data.Monoid main :: IO () main = BS.interact (B.toLazyByteString . mconcat . map ((&lt;&gt; B.word8 0xa) . B.intDec . getPent . readB) . B8.lines) where readB :: BS.ByteString -&gt; Int readB = BS.foldl' (\c n -&gt; c * 10 + fromIntegral n - 0x30) 0 getPent :: Int -&gt; Int getPent n = n * (3*n-1) `div` 2 
&gt; the community always feels academic Can you please explain and expand on this? I'm not sure I understand what you mean &gt; Haskell is following its "avoiding success at all costs" slogan Can you please explain and expand on this as well? What do you think this slogan means and how does Haskell follow it?
That's an interesting approach. The only issue is how to handle the case where you need multiple keys (e.g. FooId and BazId), but I suppose you could handle that with a different kind of record type.
This is really cool! I used a similar trick to make it possible to write plugins for my IRC bot. There's a datatype `Plugin s`, where `s` is the state that the plugin keeps, which is saved between invocations of the plugins. The core loop of the bot then only knows about a list of plugins data PluginFoo f = forall a. Enabled (f a) | forall a. Disabled (f a) plugins :: [PluginFoo] plugins = [ pingPlugin , someOtherPlugin ] So we lose information about the state type, but we don't really care, since we just want to run them and let them do whatever they want. Meanwhile, the plugin writers retain type safety within their plugin, which is cool. The type system really lets you model some things nicely, and even "drop information" when necessary, while still keeping things safe where it matters.
&gt; Given `{-# LANGUAGE ConstraintKinds #-}`, you can write: Awesome, that's exactly what I was looking for. &gt; The solution of representing the "pre" functions as lambdas is perfectly reasonable, though, and in fact a very nice solution. Cool, good to know that this is about as good as it gets then. &gt; If your code is annoying then your data model may be wrong. Perhaps you really want a nullable foreign key? That's an interesting point, but I'm fairly certain that the model is right. I think the main reason for my difficulties is that I wrote the code for constructing the Bar records before I was familiar with persistent, and then decided to keep that code pure instead of just moving it into the monad on principle.
It's interesting because I've had the opposite experience—the Rust meetups I've been to had people interrupting and trying to talk over me whenever I wanted to talk, and the Haskell meetups seemed very friendly. The leaders of the Haskell community seem to have set a good example—SPJ seems humble when he speaks, something I'd say is rare. I made a comment at a Rust meetup about higher-kinded types and I got shut down pretty abruptly by someone from Mozilla, some kind of community liaison or something like that. It reminded me of the kind of gruff machismo that people usually grow out of by the end of college. This mirrors my experiences online. Asking questions or posting answers on Stack Overflow—when they're about Haskell, I feel like people may have technical disagreements but that's to be expected. On the other hand, I've had negative interactions with members of the Rust community online, arguments about pointless things, etc. From my reading of the post, your entire complaint is that Haskell uses abstract concepts, and the mere *fact* that it uses abstract concepts (and that people in the community like to talk about it) makes it "cold and uninviting". I am just trying to summarize here—I'm not really trying to editorialize, but the logic of the article seems to be founded on the assumption that academic things are inherently elitist. That, and the constant comments about how "X should be rewritten in Rust because then it won't have bugs in it" have soured me a bit from the Rust community.
[removed]
`SPC m s b` - m = major mode (i.e. Haskell) command - s = repl (“shell”) - b = load buffer Here are the two Haskell-related layers I use: dotspacemacs-configuration-layers '( auto-completion (haskell :variables haskell-completion-backend 'intero) ) (Why ~~two layers need to be told about intero, and why~~ an _auto-completion_ layer is the secret sauce that makes everything work I don't know…)
I'm glad you did something about it. I'm thinking a part of the problem is a lack of a standard language in which to innovate in. And by language I mean something that can express vim motion/action/object combinations. Vim has an ad hoc implementation of a text manipulation grammar, but a level below that in which people can innovate is lacking, imo.
haskell-lang.org also has [a very nice tutorial for optparse-applicative](https://haskell-lang.org/library/optparse-applicative)!
Another good tool is [optparse-generic](https://hackage.haskell.org/package/optparse-generic-1.1.4/docs/Options-Generic.html), which allows you to write almost nothing at all!
Finance tends to use a lot of experimental and cutting edge technology, so it's not surprising that this would extend to programming languages. I also don't think finance is conservative; people in finance tend to be willing to take calculated risks more than in most other industries.
The GPL and MIT licenses are perfectly compatible, see https://www.gnu.org/licenses/license-list.en.html#Expat and https://en.wikipedia.org/wiki/MIT_License.
Wrong kind of conservatism. I defer to Frank Zappa's speech about the music industry. They may not know what they are doing, as long as they don't hold onto some sort of delusion of what works or not. What works or doesn't can only be figured out through new investments, and seeding small projects.
NYC is a center for a lot of social activity!
Oh right. I *think* this is because GHC just doesn't work on Sierra, and you're just not gonna have much luck with it until the next GHC release (which also has to make it into GHCJS). *(Take this with a grain of salt; I'm not sure I'm right)*
'thats great you can write one. But what the fuck is a corecursive endofunctor good for?' Me with most Haskell tutorials. 
[removed]
Yeah, that loads the buffer fine, but doesn't run the main function. I have to jump to the ghci window and type main manually, which is annoying. Thanks for your answer all the same. 
I learned that my decision to learn haskell was the right one.
Not entirely sure what you mean by manipulating trees. ASTs? Making an n-ary tree GUI? In case it helps I have a small [tree demo](https://github.com/deech/fltkhs-demos/blob/master/src/Examples/tree-simple.hs).
&gt; &gt; I feel like other programming language communities treat their users like they're idiots &gt; I don't think that is the case. Some communities are perhaps more about "I want to get this done as fast as possible, so spare me the details". Yeah that's a fairer assessment, much as I can appreciate GP's post. When have coders last been unable to look under the hood when they wished to grasp the internals of this or that built-in? Must have been back during the "Turbo Borland" / MS VB6/VBA days? Even when MS .NET arrived, before long Mono appeared and prior to that people would just browse the entire framework (disassembled from MSIL but highly readable) "code" base with Reflector. All the other widely used languages/compilers/interpreters are largely open-source anyway, whether we're talking Python / Golang / Rust / Ruby / Lisps / gcc / heck even silly ole PHP.. GP sums up the entire topic adequately with: &gt; &gt; and it's all just emotions and perceptions That's all indeed. Anyone who when approaching a language builds up expectations towards what emotions they'd like to feel, has simply erected themselves a huge stumbling block smack on the road to efficiently-getting-things-done-and-acquiring-the-necessary-knowledge-basis-along-the-way. No matter the language specifics or common eco-system background. That's perhaps the real lesson OP (not GP) will take away from all this --- one way or the other, sooner or later.
Is this an usual number of packages removed? It sure seems like a lot is being removed.
That's a shame. I think the rewrite x in Rust is stupid since it ignores the whole software cost of rewriting an entire code base. I get that it will pay off in the long run but just coming up to a project and say hey do this in Rust is quite frankly naive and insulting as well as off putting and drives people away from adoption. I've seen some more enthusiastic people open up issues on repos about this and I can say the community wasn't all happy when we found out though some thought that was fine. It was split but the majority was against it. As for your experience with the community I'm sorry to hear that. I've kind of felt the opposite and I don't think everyone's experience will be the same. I don't think higher kinded types are bad and it's on the road map for Rust so I don't know why the community manager did that. I think that was a bad call. I tend to think a lot about things in terms of new people who might know some programming and theory but really not much because then I can explain concepts and tutorials with them in mind. More often then not with Haskell it's assumed knowledge and so jargon is used for short hand. If you don't know you'll have to spend a while figuring it all out and it just feels harder to be part of a conversation if you don't, whereas in Rust since those concepts aren't brought up often so it feels easier to be engaged. I guess we disagree and that just comes from our different perspectives but I do like hearing criticism of the Rust community as well because it tempers what I see with realism. Thanks for taking the time to pen out all your thoughts!
The best way to put it is that while there is some production stuff a lot more theory is what I'll see and often it can feel like without a CS degree one can get lost in what's going on. If you've been doing this for a while it can be hard to put those thoughts and ideas discussed into layman's terms and if your audience isn't a new programmer or newer to Haskell why would you? This can lead to it feeling like a lot more theory is talked about and that's not what everyone wants to do. Which is fine but it might alienate some people unintentionally. As for the slogan Haskell has been around for a long time and it's tooling had only now started with becoming better and trying to get it in production. This can conflict a bit with people who just want to do interesting things in Haskell not for production. Since there's no clear goal for Haskell as to what it wants to be it's hard to see where it will go in order to be successful. There's no clear goal whereas for Rust it has been replace languages like C and C++ and bring in new people to systems programming. You might disagree so if you do let me know why! Maybe I haven't seen something before that has defined a clear goal or maybe you don't agree it's academic. Let me know! I think having those view points lets me perceive the community better beyond my head
Agreed. 
Good to know! Thanks!
Well, that's a bit of an exaggeration, don't you think? It's also not really relevant, since we're talking about the community, not the language itself. Simon Peyton Jones says that Haskell is useless. https://www.youtube.com/watch?v=iSmkqocn0oQ
Am I the only who found the academic tone of the Haskell community to be warm and inviting? Different strokes for different folks.
&gt; The best way to put it is that while there is some production stuff a lot more theory is what I'll see and often it can feel like without a CS degree one can get lost in what's going on. I have a CS degree and it didn't help one bit when I tried to learn Haskell. I don't have a mathematical mindset either. I just did my best with whatever materials were around at the time. Things are a lot better now but I admit there could be more (prominent) cookbook-style guides. To those people for whom the very words monad, functor etc. evoke a visceral knee-jerk flight response maybe Haskell really isn't the right language. I spent a little time with Rust, and even did a [talk](https://www.youtube.com/watch?v=U3upi-y2pCk) on my initial experiences. Rust tool support, metaprogramming and documentation are great, but, although I didn't touch on it in the talk as it would have been divisive, I really did miss those abstractions all the time. Being able to see a pattern and not capture it is really frustrating. Perhaps in response Haskell (GHC + extensions, really) goes the other way to the point of creating and obsessing over abstractions few understand. But if it goes up to 11 it goes all the way down to 0 as well. There's no shame in setting the volume knob to whatever you find comfortable.
I've always tried to think about this from a UX designer's perspective; user's will 'default' into whichever style is most readily available, even if there's something better for them off the beaten path. If the interactive 'see what you're doing' approach that kak uses is what most people prefer (not that anyone's doing user-studies on this :P), then it should be the default and people should go off the beaten path only to do things that can't be expressed this way.
I can't speak for Intero, but aside from the odd unused import that'll have slipped through the cracks it's compliant with hlint.
Conduit uses a `Stream` type [internally](https://hackage.haskell.org/package/conduit-1.2.8/docs/Data-Conduit-Internal-Fusion.html). Because a conduit has a final return value, one writes data Step s a r and uses a constructor Done :: r -&gt; Step s a r See also this [post of Twan van Laarhoven](http://www.twanvl.nl/blog/haskell/streaming-vector) which contemplates using the vector `Stream` type directly as a sort of "ListT done right" (i.e. an effectful stream of elements with no final return value.) Note that the monad instance for all such types is basically an apocalypse from a performance point of view, so e.g. a = do yield 1 yield 2 yield 3 keeps its internal state with a `Stream` that keeps its internal state with a `Stream` that keeps its internal state with ... By contrast, the internal representation of that sequence of yields with e.g. the streaming library is &gt;&gt;&gt; S.yield 1 &gt;&gt; S.yield 2 &gt;&gt; S.yield 3 :: Stream (Of Int) Identity () Step (1 :&gt; Step (2 :&gt; Step (3 :&gt; Return ()))) which is pretty much like the corresponding list - boring and a little clunky but not toxic. All employment of the vector-style `Stream` type type must thus be through combinators or as a hidden way of optimizing combinators. If the user wants to define their own producer with yield statements, the usual recursive representation is used as the seed for the internal stream function. It should be noticed that in the places where an internal stream fusion framework has won the day - the `vector` and `text` libraries - we are not optimizing a standard issue recursive Haskell type, but an array type which is expensive to allocate but makes a very good internal state for a stream function. The attempt to get rid of the `build/foldr` fusion machinery for lists in ghc, in favor of the dual `stream/unstream` machinery, came to grief. This is a reason for thinking that `build/foldr` fusion, which also translates quite perfectly to the monadic case, is a more promising way of optimizing combinator-driven stream programming. There is no reason a priori to think that stream-fusion streams will be better at optimizing 'ListT done right' (the simplest effectful stream type) combinators than a `build/foldr` system. The question needs to be resolved by experience.
I think those tutorials exist but they're scattered all over the web. The easy thing to do would just be to collect them somewhere. Ok now maybe *I* should put my money where my mouth is. (Additionally, I'd love to read whatever you have to write!)
I think one thing you should consider planning for (and possibly adding) early on is some sort of built-in help function. One of Emacs' and vim's biggest strengths is the ability to discover a lot of things about the editor from within in the editor. In particular some way to figure out the keys bound in the current context would be useful and also some sort of help system for all those events and extensions where you can search by name and by category (as in event, key binding, extension,...). I find this is one of the really bad bits of vim's help system where Emacs really shines, that whole mess of having to remember how to specify a category with special syntax after :help in vim while emacs just has a menu with relatively intuitive shortcuts C-h f to search for functions, C-h v to search for variables,...
&gt; The best way to put it is that while there is some production stuff a lot more theory is what I'll see and often it can feel like without a CS degree one can get lost in what's going on I think that because Haskell is quite different than other popular imperative languages and has a different theoretical model some people (me included) think that starting with that theoretical model makes learning Haskell a lot easier, for example, understanding lambda calculus, a very small programming language which serves as somewhat the theoretical model for Haskell and many other languages, is going to be highly beneficial and will make learning Haskell a lot easier. This is probably the reason why the first chapter of [haskellbook.com](https://haskellbook.com) covers lambda calculus. I know that if I have learned lambda calculus prior to learning Haskell i'd have a much easier time. But I also understand there are people who dislike this approach or dismiss it. &gt; As for the slogan Haskell has been around for a long time and it's tooling had only now started with becoming better and trying to get it in production AFAIK that slogan does not mean "try hard to make Haskell fail at being popular" but rather "do things in a principaled way (or do things the right way) even if it will make Haskell a bit less popular". This does not mean that we should avoid having nice tooling or something. The tooling situation have been increasingly improving for a while now and will continue to improve.
[slides](https://github.com/burz/presentations/blob/5001381390f8772b662d5fdf82dd634a997972db/f-algebras/slides.md)
Haha, that would make a heck of a lot more sense :) Rasa at the moment is more of a shell to tack extensions onto, it's definitely capable of hosting extensions to handle those things, and a quick-fix-list style extension for things like hlint wouldn't be too tough to cobble together, but no nothing exists at the moment. That said, there's a fair amount of groundwork in other areas that should probably come first.
I definitely agree, I think that this is one of the best features of those other editors! It's definitely on the Roadmap: https://github.com/ChrisPenner/rasa/issues/2 I'm thinking I'll implement a "rasa-ext-help" extension which exports a "help" action which lets you register a doc under a given key on Initialization. Something like this: myExtension :: Scheduler () myExtension = onInit $ helpDoc "myExtension" "This extension..." The help extension would then handle searching/tab completion, etc. and unify it across all other extensions. I've got the shell of a vim-style ":" or Emacs `M-x` in the works, but I'm working on a lot of different areas at the moment: https://github.com/ChrisPenner/rasa/tree/master/rasa-ext-cmd
Well, as far as I understand (take everything I say with a grain of salt), it seems like the banks need a fast development cycle, they have some "strategy" or whatever that they want to implement correctly and quickly. It seems to me that this is precisely one of the things that Haskell is good at. Another example of this is, I suppose, what's going on at facebook. Maybe /u/smarlow can elaborate...
You know what would be an awesome addition to this. A program that would go through a long Haskell file and add everything to the export list. Then you could go and remove the functions you don't want to export. 
Banks do hire mathematicians. They also hire good programmers. I've seen more good programmers who like Haskell than mathematicians. 
I've started doing some triage on things that could use some work or areas where discussion would be useful, you can find the main [TODO list here](https://github.com/ChrisPenner/rasa/issues/2). Items on the list link to issues with discussions. Let's get some good discussions going! For instance, - [How should we build in a good help system?](https://github.com/ChrisPenner/rasa/issues/9) - [How should we generalize rendering preferences across extensions?](https://github.com/ChrisPenner/rasa/issues/8) - [How should we build a useful cmd line tool?](https://github.com/ChrisPenner/rasa/issues/7) All thoughts are helpful! I'd love to hear what you think! Also please join our gitter room if you have the slightest interest in the project! https://gitter.im/rasa-editor/Lobby
I'm looking to work with algebras within Haskell. I've seen some pretty good constructive algebra packages, but they don't 100% fulfill my needs so I'm writing my own. I was wondering if there was a book out there that really goes deep into the way that haskell works with Algebras. 
[/r/rust is blowing off some steam](https://www.reddit.com/r/rust/comments/5l08o5/rust_is_literally_haskell/dbryrpj/) by posting memes (which is normally not allowed) and silly things until 2017. Edit: it makes more sense in the context [of the previous theme](https://imgur.com/a/U5dol).
I'd argue that you'd only want to use this flag if you *already* knew what the thing you wanted was. If you don't know what LLVM IR *is*, you certainly aren't in the position to read it. I also don't personally think `--help` really needs to be particularly detailed either, especially not for a "low level" tool like `rustc` (there is a heavy push to get everyone using Cargo).
Very cool to see that apparently custom tooling can be enabled via #pragmas: &gt; {-# OPTIONS_GHC -F -pgmF autoexporter #-} TIL! Neat, as I'm just coming up with a need for exactly this functionality in the course of a project
This is a classic text that is written with ML, but it may provide some ideas: http://www.cs.man.ac.uk/~david/categories/book/book.pdf The interesting point I think is that there are two approaches. The more modern "embedded" approach tries to build algebraic structures directly reflecting their properties in types. This works _really_ well in a theorem prover / dependently typed language like Agda. In Haskell you only can capture some things, and it can be sort of ad-hoc. The older approach, as in the above link, is really just about "modeling" algebraic (or in this case categorical) structures using standard FP techniques. For actually wanting to "crank through" calculations its often still more effective, if less slick :-)
I see what you're saying, but the file's main purpose is to create a build plan, not to track historical data such as which packages were present in previous builds. Comments allow us to include unstructured information like this that doesn't fit into the structure expected by the program (stackage-curator) that consumes this file. Without comments, it seems more likely that such data would simply be lost. It's easy to see now, in retrospect, that keeping this data in this file might be valuable, because we were able to capture it in comments. But without the leeway of being able to have comments, it's harder to learn organically over time what data you might want to shift into the proper structure of your data.
&gt; but I'm fairly certain that the model is right. You would have the exact the same problem without persistent trying to tie the knot. I mean if you ignore the persistence layer and replace `fooId` by `foo :: Foo` you still can't create a Bar before having a Foo. However, I had (I think) similar problem and I just use a function `FooId -&gt; Bar` without newtype or if creating many `FoodId -&gt; [Bar]` (in fact it will be `whichever monad m =&gt; FooId -&gt; m Bar`. So the final code looks like do let createBar fooId = do -- do thins with the dabase return Bar ... fooId fooId &lt;- createFoo createBar foodId I'm not sure why you need a newtype. Another options instead of a Reader Monad, is to use implicit parameter. 
I think you're right by saying that the source code doesn't infringe any copyrights, so I don't think it is a problem now. But I think it is important to know. PS: Even if it is dynamically linked it is still a combined work. See https://www.gnu.org/licenses/gpl-faq.html#GPLStaticVsDynamic.
Thanks for the article! I already knew it, but each time I reread it I learn something new. So it seems that the `conduit` and `vector` are similar in strategy. Is there any reason why they cannot share code also, or the strategy is not so easy to isolate? &gt; `pipes` uses a different trick for optimization which is shortcut fusion via rewrite rules. But the optimization given by `foldr/build` or `stream/unstream` are given by rewrite rules too, no? Since the technique is different, do you have any source that explains it in more detail? This difference has to do something to do with the fact that `pipes` constitute a category while apparently `conduit` no? You say the trick is orthogonal to the non-recursive encoding, but are they mutually exclusive? Thanks!
I just noticed that a pointer was posted on the Dependent Types Reddit to the recent "Mathematical Components" text: https://math-comp.github.io/mcb/ This is Coq rather than Haskell/Agda, but if you want to do serious algebra in type theory this is the state of the art--what was used to prove the Four Color and Odd Order Theorems. The book seems to be an introduction to the background you need to understand the library.
Thanks. This is promising, but I would have to write an adapter using pipes-parse to make it fit and then stick it in. That's fine. I can see how to get the results of the parse so far, but I can't see how this solves the problem of splitting up the non-control data. The structure I'm planning to feed to the downstream pipes looks like this: data TelnetData = Data ByteString | Will Word8 | Wont Word8 | Do Word8 | Dont Word8 deriving (Eq, Show) `Data` is passed through, and `Will`/`Wont`/`Do`/`Dont` are telnet options that will be handled somewhere along the way. Interactivity aside, there's no real difference between `yield "abc" *&gt; yield "def"` and `yield "abcdef"` (assume `OverloadedStrings`). I'm trying to avoid consuming all input up to the next command sequence before yielding a `Data` element down the pipe. What I think I'm looking for is a function like `Text.ParserCombinators.Incremental.eof` that can detect end-of-chunk, not end-of-input. Does this make sense, or is there a better way that I'm missing?
The points Chris raises in this blog post very much reflects my own experience in my current day job. I also think that just using Haskell is not sufficient. You need to exploit Haskell's strengths and set good processes in place (code reviews, property based testing, CI, avoiding partial functions etc). 
Do you have an example? This sounds like its something that can be fixed.
It's discussed later in Coutts' thesis see e.g. sections 2.2.3-2.2.4 and 3.2-3.4 which contrast the two sorts of non-recursive encodings as 'Church encoding' and 'co-Church encoding', as does e.g. this paper by Hinze, Harper et al on [the theory and practice of fusion](https://www.cs.ox.ac.uk/ralf.hinze/publications/IFL10.pdf) and also Harper's [Library Writer’s Guide to Shortcut Fusion](https://www.cs.ox.ac.uk/files/4458/p47-harper.pdf) which I think I read first. There is a simple post [Squinting at fusion](https://unlines.wordpress.com/2009/09/29/squinting-at-fusion) by Roman Leshchinskiy the author of the `vector` library which brings out the opposition using Haskell. Note the duality of `foldr` and `unfoldr` employed to make the contrast. The threefold contrast of a 'regular Haskell recursive type' for streaming with, and the two associated non-recursive encodings can be illustrated with a "ListT done right" library like [list-t](http://hackage.haskell.org/package/list-t) or [list-transformer](http://hackage.haskell.org/package/list-transformer). The recursive `ListT` type in either library can equally be expressed with the church encoding found in [logict](http://hackage.haskell.org/package/logict) and the co-church encoding in `Data.Vector.Fusion.Stream.Monadic`. The combinators in `list-t` could be optimized under the hood, so that compositions fuse, with either of the other libraries. Using logict would give you a build/foldr system; using vector's streams would be an unfoldr/destroy or stream/unstream system. (Again, the `ListT` types differ from the usual streaming types by lacking a final return value, which affects their utility considerably, but the opposition is clear enough and could be extended to the other case, and indeed e.g. to a more general free monad transformer type like FreeT or the Stream type of the streaming library.) 
Actually, it's the opposite: `pipes` has nothing equivalent to the non-recursive representation (i.e. no analog of `Stream`) and uses the recursive representation exclusively.
Avoiding String, Int and Boolean blindness too. We have types for that. 
The ideal scenario is to put as much logic in `attoparsec` as possible and then only return to `pipes` when you need to emit something. That will give you the best mix of performance and interactivity So to answer your overarching question: you need your `attoparsec` parser to be able to stop parsing when it hits a `\255` byte. `attoparsec` provides a `takeTill` utility that does this
Simplicity is very important. Many of the data types and functions we use constantly are almost trivial data Maybe a = Nothing | Just a id x = x data List a = Empty | Cons a (List a) data Bool = False | True We can keep their behavior in our heads without fear of unexpected behaviors. They seem like toys at first but *life is simple, and the simple thing is the right thing*.
I wasn't trying to offer an exhaustive list, just a few things off the top of my head :).
What platform are you on? I realize you are likely tired of putting energy into it, but can you share what the show stopping issue was with haskell support in your preferred editor or IDE?
Also learning vim / emacs (particularly vim) is generally a great way to improve your productivity in the long run. And that is kind of the point of Haskell, short run learning curve -&gt; long run productivity and correctness.
I got excited to see this sub on the front page and realized it is because I'm subscribed. :O
Or when it can be optimized into loops due to the laziness offered by `String`, enabling some pretty impressive optimizations that may even give you faster speeds than Text in some cases, specifically cases where the String is never really stored in its full form in memory un-GC'd.
"Why did it take you so long to earn a million dollars. My friends and I each did so after a round of investment with a few venture capitalist firms and and an awesome app idea. Sure, it took having access to generally inaccessible people, but it was easy." I guess my point is that just because your experience was better doesn't negate someone else's. I suspect that if you are genuinely curious, better questions might be "what about the current resources did you find lacking?", or, "what was it that finally made those ideas easier to work around or with?". Indeed, I expect that different educational backgrounds played a part in it. A lot of Haskell literature assumes some familiarity with programming or mathematics.
But all they used was the internet and their brains... Like what is wrong with just reading all the way through learnyouahaskell? That doesn't take 5 years...
This somewhat applies: https://xkcd.com/1138/
[Image](http://imgs.xkcd.com/comics/heatmap.png) [Mobile](https://m.xkcd.com/1138/) **Title:** Heatmap **Title-text:** There are also a lot of global versions of this map showing traffic to English\-language websites which are indistinguishable from maps of the location of internet users who are native English speakers\. [Comic Explanation](https://www.explainxkcd.com/wiki/index.php/1138#Explanation) **Stats:** This comic has been referenced 934 times, representing 0.6576% of referenced xkcds. --- ^[xkcd.com](https://www.xkcd.com) ^| ^[xkcd sub](https://www.reddit.com/r/xkcd/) ^| ^[Problems/Bugs?](https://www.reddit.com/r/xkcd_transcriber/) ^| ^[Statistics](http://xkcdref.info/statistics/) ^| ^[Stop Replying](https://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=ignore%20me&amp;message=ignore%20me) ^| ^[Delete](https://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=delete&amp;message=delete%20t1_dbtocx7)
Ah, I used a newtype because I needed to make an instance for parsing it from a CSV file, and adding an instance for an arbitrary function type just seemed ugly.
Thanks, this looks like a great overview of what maintainability actually means! That said, I have a few follow-up questions: 1. **Coupling where it counts**: I would like to see an example of how to model authentication in the type system. Both a toy example and a link to a real-world open source code base that does this would be appreciated! 2. **Mock the world**: How do I move an existing application with mtl-style functions that use `MonadIO` with file and network access over to a mockable `FileIO` and `NetworkIO` approach? (These type classes are made-up on the spot, I don't know if these would be the right abstractions) Edit: Are there any good libraries that supply mock IO abstractions and implementations? 3. **Model the domain precisely**: Is there an open source application that makes great use of the type system to model its business domain? 4. **Automatic memory management**: I didn't like that this section glosses over the space leak problem. I remember that Neil Mitchell recommended to run test suites with limited stack space. I think I tried this with an existing HSpec- and/or QuickCheck-based testsuite but didn't get it to work. Has someone successfully integrated this recommendation into their CI? 
&gt;Another reason we didn't provide answers is that we didn't want people to feel bad or intimidated that they didn't make something "pretty". That's definitely something to consider. It can indeed be demotivating to glance at the answer of a tricky exercise you are stuck at and be greeted a monolith of code. Furthermore, adding copious explanations to the answers in an attempt to soften the blow can end up coupling the answers tightly to the main text.
Also Philly for some reason. In addition to the UPenn PL department, which has a bunch of Haskell people, there's also /u/acow, /u/Tekmo (formerly), byorgey, bergey...
I also want to know this. I usually go into the repl and type `main` but I'd like a faster way
&gt; Elm I think the lead dev cares first and foremost about teachability of the language even if it means forgoing certain academic features Just a quick note : I use Elm a lot, and that's not just "certain academic features" that are left. You still can't have anything "comparable" (ie. give it an Ord instance) but string, int, and tuples of them. Every module has an `andThen` and `map` function because there are no typeclasses. There is a whole industry of library helpers and code generators for json instances because there is no macro/template haskell/generics. Most operators have been banned, making nontrivial code unreadable. The last release had been particularly frustrating to me. It was only bikeshedding (dropping operators, banning the use of `'` in identifiers), and none of the fundamentals stumbling blocks have been addressed. This is not about leaving some particularly complicated theoretical aspect of the language to make it more approachable, this is a *huge* trade-off. (I think it's good, and it's important that Elm stays true to this spirit. But coming from Haskell, it's sometimes quite frustrating to write so much boilerplate)
Come one, a bunch of other languages have a `foo[bar][baz] or foo.bar.baz`-like syntax for nested structs access, and they're doing great. What should be burned in a great fire is the generation of functions of the name of the fields. This is just plain useless, and having a builtin syntax for record updating (like for instance `{ record | field: newvalue}` as they do in Elixir) would make the hassle of naming your fields disappear. 
I've been meaning to learn Haskell for its perspective-expanding properties for quite a while and I think my perspective may have some relevance: I can just never find time to justify the amount of up-front effort I perceive to be necessary to learn a language that seems so alien to my understanding of not only the paradigm, but learning how to think through who knows how many layers of abstraction to make sure I don't innocently trigger pathologically bad performance. (It's not specific to Haskell. For example, I'm wary of using C or C++ beyond what I did while earning my degree because quizzes on language footguns have shown me what scary places they are for people who aim for secure, reliable code.)
&gt; Modeling authentication in the type system Did they publish the full code that they used in that series? &gt; I might also suggest grenade a library for building/training neural networks that uses dependent types to prevent incorrect/nonsensical network composition. Thanks, grenade looks very interesting, but maybe a bit advanced. I'll have a closer look though! &gt; If you want to limiting the heap size, its simply a matter of compiling with something like -rtsopts +RTS -M&lt;whatever&gt; -RTS. I'm pretty sure I've done this at some time, but it must have been for something at work because I can't find it on my personal machine. I think Neil focussed on stack space leaks which can be detected with eg. `+RTS -K1K`. I see [that the shake testsuite uses that setting](https://github.com/ndmitchell/shake/blob/73f239e8900e0a6a1f5cdfc2a29d8c5e8d408de0/shake.cabal#L275), but it seems to be built from scratch, without any test framework.
It took me around 6 months or more to read through lyah. Because: - I'm not a native english speaker - I didn't know anyone else who uses it - I didn't have any project in mind - I had a lot of other stuff to do Even after finishing I had a lot of trouble writing anything that is not trivial. There are a lot of reasons why someone would take a long time to learn something. Please don't dismiss it.
&gt; It took me around 6 months or more to read through lyah. Ok maybe a month was a bit tight, but 5 years... &gt; I'm not a native english speaker That's fair, but tbh I feel like all but the most popular programming languages would have the same problem, you kind of want to be good at english to do lots of programming. &gt; I didn't know anyone else who uses it Neither did I. &gt; I didn't have any project in mind I'd suggest something like projecteuler if you really cant think of anything. Also maybe making command line games, like blackjack or poker or ascii chess or sudoku. &gt; I had a lot of other stuff to do I mean... like sure... but that's not Haskell's fault. If you don't have time to do something don't expect magic to happen. &gt; There are a lot of reasons why someone would take a long time to learn something. Please don't dismiss it. I mean yeah there are, and I'm not saying you should never take 5 years to learn Haskell, just that it will never be Haskell's fault directly. Because Haskell does NOT take 5 years if you put time into it and try (and I guess know english well, otherwise idk). I'm trying to defend Haskell, not attack him, although I do realize I phrased my original comment a little aggressively, I was just pissed someone was implying Haskell might take an average person 5 years to learn.
I haven't done much with these pragmas yet, but from what I gather from the [docs](http://ghc.readthedocs.io/en/latest/glasgow_exts.html?highlight=inlinable#inlinable-pragma), you could try to annotate your exported identifier with `INLINEABLE` and annotate every call site within your module with `inline`. Depending on the size of the function that may not be desirable. What is really needed would be some kind of 'specialization', where there is one version of a function for the exported binding and one version for the non-exported version, which profits from all such `static`-like optimizations.
Java is not dynamic btw. And right I get all that, but that doesn't address my point, which is that under normal circumstances it takes way way less than 5 years to learn Haskell.
could you please mention some open source software that illustrate that approach
&gt; I see that the shake testsuite uses that setting, but it seems to be built from scratch, without any test framework. Limiting the stack size is completely orthogonal to the test framework being used. Regardless of the test framework you can compile with extra RTS options to limit stack and/or heap size.
Been using it for my game experiments - the assets are loaded once, the game loop code is then reloaded after change. The experience is great!
If you have just a little time, the first half of Robert Geroch's *Mathematical Physics* is excellent. If you have more time, then Aluffi's *Algebra: Chapter Zero* is even better. Both of these books basically cover traditional abstract algebra in a categorical style. This means that they develop the stock of examples you need to really appreciate why categorical abstractions are organized the way they are. (This is also why Mac Lane's book is titled *Categories for the Working Mathematician* -- he assumes you have the examples already. This is not a safe assumption for a programmer or computer scientist though!)
Bartosz Milewski's [Category Theory for Programmers](https://bartoszmilewski.com/2014/10/28/category-theory-for-programmers-the-preface/) blog series was the best beginners material by far for me.
I'm not sure I understand. For Autoexporter, `build-depends` seems to work fine. 
I'm currently working on the AI for a dumb game I'm experimenting with, so this seems really useful. Can I ask, how much of a hassle was it to get this working? The docs make it seem almost too easy...
GPL-compatibility only means, that you are allowed to use MIT code in a GPL project. Not the other way around. If you use GPL code your project has to be GPL.
I'm following your guide and I'm having trouble installing stack. The stack script tries to grab a bindist from `https://www.stackage.org/stack/linux-arm`and gets a 404. Where was your rpi pulling a bindist from? EDIT: I was able to find a bindist at: https://github.com/commercialhaskell/stack/releases
Well, I'm not the OP, I've not really had a problem with this yet. But consider this: module Blah (g) where f :: Int -&gt; Int -&gt; Int f x = if x &gt; 5 then id else (+ 1) g :: Int -&gt; Int g x = f x x Leaving inlining aside (which is hard to predict in this artificial setting), GHC finds out that all call sites to `f` call `f` with two arguments. This is great news, because it can eta-expand `f` now, which in general means less thunk allocation. Why don't we always eta-expand? There is the possibility of losing shared work (the cost of which might be huge), which in this case hides in evaluating `x &gt; 5`. There might be code relying on this, e.g. some expression like `let f' = f 2 in f' 3 + f'4` which would compute the costly expression twice after eta-expanding `f`. Since we can't foresee all usages of `f` when we export it, we can't eta-expand in that case. That's just one example of an optimization I'm pretty familiar with currently.
Yeah, I'm comparing Java together with Haskell, they both looked verbose to me. Besides you were in a team, you can motivate each other, I was/am alone. So I needed to experience the pitfalls of dynamic language to realize all the effort I'm putting right now to learn Haskell are worthy.
I think you're confused about what it means for two licenses to be compatible or maybe I'm confused about what you mean by "use". Generally "compatible" means that two programs can be combined (as in linking them together or making one source tarball out of both) while satisfying the requirements of both licenses simultaneously. Since this relation is symmetric which program uses which does not matter.(https://www.gnu.org/licenses/gpl-faq.html#WhatIsCompatible) Of course this only applies as long as you don't actually copy any GPL licensed code into an MIT licensed program. In which case any distribution of even the source code would have to satisfy the terms of both the GPL and MIT licenses simultaneously but I assume by "use" we are talking about the common case of liking against a library.
[removed]
I can see this hypothetical scenario but until there is a concrete example where it is demonstrably causing poor performance it is impossible to say how important this is. 
Rust has a great repo for learning [here](https://github.com/ctjhoa/rust-learning). Perhaps something similar would be good.
A curated list for resources in each domain as well as general knowledge of Haskell? That might work out well
Hmmm I see exactly what you're saying. I learned scheme this year as part of my class using SICP and it just blew my mind how much Haskell borrowed from Lisp and just how a simple language could make up a very complex system. We covered Lambda Calculus at the end and if I had learned that first I think it would have helped a lot. Some people learn differently though. Having a wider variety of ways to learn never hurt and I think if the community sat down to really write out some stuff it could really onboard new people as well as help grow those who are here now. I think knowing the language (even if not used professionally) can really benefit people in how they program so I do want haskell to succeed in that sense.
I have Category Theory by Awodey and I've always been pleased with it. I have a BSc Economics for a rough idea of maths background/aptitude and it's OK. I don't have any other Category Theory books so I can't rate it relative to other titles.
https://github.com/BartAdv/hackslash/blob/master/src/DevelMain.hs The code is in DevelMain so that all indeed to reload it is to use 'intero-devel-reload' (Emacs + intero).
I was confused about whether the license applies only to his code or to the combined product of his code linked against the dependencies. I thought the license applies to the combined work in which case it would have to be GPL.
How does this compare with `ghcid --test=:main`? Could I use rapid together with `ghcid --test=update` to automatically update the application when I change code?
.cabal if it's "lightweight" (i.e. syntax changes, or semantic changes that I always prefer, like LambdaCase, RecordWildCards, ScopedTypeVariables), .hs if it's not (like GADTs, TemplateHaskell, OverloadedStrings). 
afaik, a build-tool dependency is only needed to build the package, a package dependency is needed by the built package itself. listing those executables in build-tools forces users to install them too. idk if this matters currently, because haskell packages unfortunately are almost exclusively installed by building from source. but with binary caching, like under nix, users wouldn't need build-tool dependencies if downloading the precompiled artifact. /u/hvr_ was that your point, or am i off? edit: wait can this be used for Template Haskell too? i.e. strip out any dependencies of the code generator from the generated code. that seems weird, since you would a have to make sure that any given symbol used by the macro isn't also used by the module. 
But Java isn't dynamic. And Haskell isn't verbose. 
I don't have much of a problem with `Int` and `String` parameters to non-exported functions, but I would also not write a function like: doSomthing :: String -&gt; String -&gt; String -&gt; String -&gt; String because its too easy to use incorrectly. At the API for a library, I often use [newtype wrappers](https://wiki.haskell.org/Newtype) around things like `Int` and `String`, for both function parameters and ADT fields. Doing this makes the API much more difficult to use incorrectly. I've often had colleagues review my code and suggest the addition of newtype wrappers. They are nearly always right. 
I'll definitely be trying this out later. Thanks for your hard work!
A few more analogies, hopefully correct : The values of type LFix have the property that given any f-algebra, they will produce a value of the carrier. They are natural transformation from (f _ -&gt; _) the functor of algebra to _ the identity functor We can take this universal _property_ as the type _definition_ As an end, it is also a right Khan extension of the identity along f, that is its smallest approximation going through f
Yeah agree type-name aliasing rocks and probably solves the "issue" in 99% of cases just-fine. On the other hand there have been times where I'm looking up some lib funcs in a doc with half-a-dozen+ different arg types each that I need to click through only to realize they all alias to simple well-known built-in scalar types.. yikes, the humanity! (OK more of a case for smarter doc-gens (w/ onhover-tooltips-over-arg-types or s.th.) than against aliasing, this.)
I find map and filter are two very simple functions too...and using them instead of a loop is so much saver (e.g. map never changes the number of elements in your container so you never need to take it into account when you have fewer or more elements than you expected; calculation of elements is map is independent so you never run into bugs resulting from loop iterations interfering with each other (forgot to clear a variable properly between loops); filter never increases the number of elements and never changes any of them).
Type aliases are something different and actually provide no extra safety whatsoever. Eg: λ &gt; type X = Int λ &gt; (1 :: X) + (2 :: Int) 3 whereas with a newtype: λ &gt; newtype X = X Int λ &gt; X 1 + 2 &lt;interactive&gt;:4:1: error: • No instance for (Num X) arising from a use of ‘+’ • In the expression: X 1 + 2 In an equation for ‘it’: it = X 1 + 2 
I think gmp is LGPL and that has an exception for dynamic linking. See https://www.gnu.org/licenses/gpl-faq.en.html#LGPLStaticVsDynamic.
Oh, that's cool to know. Thanks bro!
I've been thinking of expanding a comment I made about the simplest function `id` into a blog post, there is more to it than meets the eye (maybe I will add a section on `const`)
I find this kind of marketing speak cloying and irritating. I don't understand who it appeals to. I only ever see it come from FP Complete too.
Then again, I picked up Aluffi to understand Haskell better, aaaand things happened and now I'm going to study math in college (hopefully, that is), so OP beware ... just kidding, I really second this. It's a great book (at least the first seven chapters, since I haven't done the last two). Also, Geroch sounds interesting. What physical material does it cover?
[removed]
&gt;Records are bad for backwards compatibility, because there is no way to overload a record update with a custom new update. Lenses for updates would be better. This is a conclusion I've come to with my irc-client library. Every time I want to add something to one of the records, it's a breaking change. I need to just lens it up next time.
can we fix liquid haskell?
This is a neat trick. But it requires the new type to have a different name every time. Not necessarily a big problem, but quite inconvenient. 
Not really. You just have one type that you add fields to and add new phantom tags to indicate which fields "are in effect".
[removed]
Hmm. So the key question here is, how do you get the OurNewType Old into the hands of the legacy clients which are updating it, and then when do you normalize? I'll admit, this is as much a problem with Cabal itself, which failed to define any sort of well defined interface for interacting with itl in our case, the answers here are non-obvious.
Well, since you define a synonym, the compiler will force you to send the Old variant to old functions. Then you just make sure to use the New variant in the new functions you write. Again the compiler tells you where you need to normalize.
Always per-file so that readers can tell what's enabled.
Ah. That sounds like a lot of changes, because this type shows up in a lot of other types. All of those function would need to be duplicated?
Jane Street's method going forward for stability in ppx rewriters: https://blogs.janestreet.com/an-solution-to-the-ppx-versioning-problem/
To a certain degree, I think you're right, but it is also really disappointing that many "obvious and easy" ways of writing Haskell code (namely, exporting constructors of algebraic data types) are heavily discouraged in this style. No more pattern matching! (Is the lesson here that pattern matching should only be used on "mathematically timeless" types? Maybe.) When it comes to changing Cabal to make it more backwards compatible in the future, it's a bit of a pickle, because all of these changes are BC-breaking (rip the bandage off!) And of course, the number one rule for preserving BC is to not expose too much, but no one knows what the "right" amount of API to expose for Cabal is and so we keep kicking the can down the road. C'est la vie...
&gt; No more pattern matching! If record selectors should be replaced by lenses then ADT constructors should be replaced by prisms. You can emulate pattern matching (with some limitations) with the [total](https://hackage.haskell.org/package/total) library.
&gt; To a certain degree, I think you're right, but it is also really disappointing that many "obvious and easy" ways of writing Haskell code (namely, exporting constructors of algebraic data types) are heavily discouraged in this style. Yeah, I'm not to the point of actually advocating this style strongly. Not everyone needs this high level of backwards compatibility protection. I think it's important to assess how many people are likely to use the thing you're building. It seems pretty clear that packages like `containers`, `parsec`, `aeson` etc should have a stronger commitment to backwards compatibility than, say, [RNAlien](http://hackage.haskell.org/package/RNAlien) (the most recent package uploaded to hackage). And you probably don't need to think about backwards compatibility at all for end-user applications and small one-off projects that don't get uploaded to Hackage. But I should note that long before I came up with the above ideas I had stopped using constructor pattern matching in most of my code except in the case of sum types. Relying on positional placement is pretty brittle and not very readable. IMO using an actual field name (or lens) makes things much clearer. &gt; When it comes to changing Cabal to make it more backwards compatible in the future, it's a bit of a pickle, because all of these changes are BC-breaking (rip the bandage off!) Yeah, I'm not sure what Cabal should do. One pattern that I think works pretty well is to introduce some equivalent types that have better backwards compatibility patterns built in from the beginning and slowly transition to them with deprecation cycles over a long period of time. But yeah...it's not fun.
I see them basically as reference articles to answer the most common questions concerning adopting Haskell. It's just required in order to increase Haskell adoption. Nobody in their right minds will spend more than 30 minutes researching this language without some reference telling them that spending more time is a good idea.
Spacemacs!
I used atom but it is throwing issue and am thinking of switching, I didn't user vim or emac before so I dunno how easy is to configure haskell env and start ...
There was a reddit thread a couple weeks ago about configuration. Getting up and running is just a couple cabal-stack installs in addition to adding the correct lines to your dot file. Learning enough of spacemacs to match atom would take a day or so, but the available feature set is immense (baring a couple bugs in the Haskell layer) and the number of supported languages is also huge. 
Dunno if spacemacs has keybindings for intero yet. But intero is the best. Definite reason to go with (spac)emacs
I use evil-emacs + intero. My emacs config files are [here](https://gitlab.com/gilmi/dot-emacs/tree/master). I used [this guide](http://y.tsutsumi.io/emacs-from-scratch-part-2-package-management.html) to get started with configuring an emacs setup. Sometimes I use ghcid and haskell-mode as well when I work with a cabal based project instead of a stack based one.
This topic immediately brought to mind [Michael Stonebraker's recent Blog@CACM post on database decay](http://cacm.acm.org/blogs/blog-cacm/208958-database-decay-and-what-to-do-about-it/fulltext). Evolution of a database often requires implementing semantic and syntactic upgrades. The challenge is how to make them backwards compatible so that code changes in accessing routines are minimized or even unnecessary. In the database context it is clear that certain antipatterns should be avoided. For example, instead of code accessing the database directly using ODBC/JDBC, Stonebraker says that it is better to have all accesses through a messaging interface that can evolve as the database does. Features of Haskell that constrain changes (e.g., data constructors) might be flagged as antipatterns so the programmer knows the risk inherent in their use. In general, it is worth considering if the Haskell compiler should automatically generate indirect references to data (through getter and setter functions) that allow for data structure (and some semantic) changes without requiring recompilation of existing data access code. Perhaps this might even be the default unless the programmer explicitly takes responsibility for the brittleness inherent in direct references.
I use atom + haskell-ide. It's definitely not the best editor out there, it consumes a lot of RAM, but setting it up takes less than 2 minutes.
I use [geany](https://www.geany.org/) which is a pretty decent editor.
The main advantage I found is that you can query the type, info, and can goto definition of local variables (those introduced inside a function) which was basically my reason for switching over. I've also found it quicker to use than haskell-mode.
[The homepage does a better job explaining the advantages than I could in a comment](https://commercialhaskell.github.io/intero/).
I use Visual Studio Code + Haskell plugins. Best configuration I had so far. Vscode is pretty fast.
I'm new to Haskell and using vim with the syntax plugin has been great. I might consider moving to Atom + Haskell IDE + Vim mode once I start working on larger projects.
Similar discussion from a month ago https://www.reddit.com/r/haskell/comments/5gh2mx/what_is_your_haskell_dev_environmentworkflow/
[ergoemacs](https://ergoemacs.github.io/) + [intero](https://haskell-lang.org/intero)
I use Emacs and the standard Haskell mode with a bit of customization to work with Nix. It's not perfectly stable, but it works well enough that I'm too happy to put much effort into using something different like Intero (which is difficult because I don't generally use Stack).
&gt; Rust is great as a language and I could go on about how it makes me productive, or a better programmer, etc. but I got a lot of that from Haskell. Rust, however, has a better community by far. If you've read anything about Haskell or been to meetups, *it always has this air of academia that you can't escape from no matter if it's production or type theory things*. Don't get me wrong, I love that kind of stuff, but for a lot of people they don't care about Lambda Calculus or Monads or Lenses or Functors. They just want stuff that works well or know how to use things. With the Haskell community it's hard to teach or learn it without coming into contact with these kinds things or have it be talked about. *It gives it an air of elitism that, while not always intended, can scare people away from it. I've seen tutorials that start explaining things like you're in a higher level math class and it scares people off. I guess the best way I can describe it is the community can feel cold and uninviting*. ab-so-lu-te-ly And the worst of all is that most of the smug wennies that indulge on that are mediocre academics as well as mediocre programmers. Since Haskell is half away between hard academia and hard programming, but it is none of both, it is an ideal haven for such kind of simulators.
Pattern synonyms help with certain cases, but not record update, as they are implemented today.
Some paradigm shifts I've observed: - Multiple cursors (with multiple registers) - [Elastic Tabstops](http://nickgravgaard.com/elastic-tabstops/) - OrgMode - Kakoune's 'inverse' model - Motions in general - dot-repeat Most of those are implemented in vim or plugins, but I'd still consider them revolutionary ideas and it would be cool to see what an editor that was built with them as fundamental ideas would end up like.
Used to use sublime. Now using atom.
Terminal emacs with [intero](https://commercialhaskell.github.io/intero/). Pretty happy with it :) My [dotfiles](https://github.com/doppioslash/dotfiles/), though the intero setup is just a one liner.
GNU Emacs with Intero and bitemyapp's .emacs mildly customized to my preferences. Was using GNU Emacs with Emacs Prelude before and it works well too. Intero is really great in my experience.
It's only for VSCode. VSCode and VS are two completely different products. The former is essentially an Electron app written in JS. 
Spacemacs + intero. Works mostly fine. how come no one uses Haskell For Mac by Manuel Chakrobarty? 
Can you show us the complete assignment? Can you provide more code? If I get it right, this line: child.set_parent(parent) is almost all your program does. But in Haskell it does not make any sense at all! So if your assignment says something like 'create instances of cls, call `set_parent` method' the we cannot help because Haskell doesn't work like that. If the assignment is more generic then we (at least I) would happy to see the assignment itself. You *can* write mutating algorithms in Haskell, but it looks like you do not want to.
has anyone used this with Scotty successfully? after I evaluate develmain in the repl run `update` what is supposed to happen? do I still need to evaluate my updated web app in the repl or will this detect file changes and auto-compile? I tried setting this up yesterday but couldn't figure out how. 
Atom with all the haskell extensions. Works great though it's slow with stack.
Thank you for your answer. I updated the question to add the complete `Tree` class and the assignment. You're right, the task is more generic. IT is just to calculate the height of the tree. I'm open to writing mutating algorithms, but I'm not sure how to do it. What do you think is the best way to get the task done?
Is it slow for you?
&gt;I didn't user vim or emac before so I dunno how easy is to configure haskell env and start I wouldn't say it's trivial, but it took me a day or so to set up a vim/haskell workflow. The big advantage is that the the skills do port over to basically every other language and platform in existence (for both emacs and vim). Plus they're both very user-configurable. 
I've been having issues with installing intero for spacemacs on windows, have you heard anything about this? 
(For my own sanity) &gt; Multiple cursor (with multiple registers) I don't get the part about multiple registers &gt; Kakoune's 'inverse' model So because it's always `Object Verbs`, you always have to have your selection visible the during the construction of the action sentence. The situation with motions (cursor move commands, though I'm getting confused between motions and easy-motions), dot-repeat (repeat last x actions, chronologically), orgmode (TODO on crack), elastic tabstops (nominal) make more sense, readily.
Well, for extremely general types like `[a]`, the `a` can be literally anything, so what do you call it, `item`? e.g. `fmap :: Functor f =&gt; (a -&gt; b) -&gt; f a -&gt; f b` is much more legible to me then `fmap :: Functor functor =&gt; (input -&gt; output) -&gt; functor input -&gt; functor output`.
idk, to potential consultees? I found this article very concrete and concise.
`Tree` is [`Cofree []`](https://hackage.haskell.org/package/free-4.12.4/docs/Control-Comonad-Cofree.html) data Cofree f a = a :&lt; f (Cofree f a)
Not perfectly up to date, but I put together this for the project I'm involved with: https://git.snowdrift.coop/sd/snowdrift/blob/master/TEXTEDITORS.md
Try binding control elsewhere, like caps lock (where it was meant to be)
I have noticed that which surprise me really 
[Leksah](http://www.leksah.org/) is a nice Haskell IDE. I like it, and have been using it for hobby Haskell programming for many versions now. It has a nice set of useful features, helped introduce me to the Haskell build ecosystem (Cabal, QuickCheck, HLint), and it continues to improve. It has Stack support, and the meta-data provides a great way to navigate the code base. It's written in Haskell, so the better it gets, the better the Haskell desktop application story (hopefully). [Sceenshots](http://www.leksah.org/screenshots.html). [Download and build instructions](https://github.com/leksah/leksah/wiki/Leksah-0.15.2.0). Mac OS X and Windows have [pre-built packages](https://github.com/leksah/leksah/wiki/download). On Linux, I've built a previous version by following the [instructions](https://github.com/leksah/leksah#building-from-source). I currently use Nix to build. Building on Nix isn't currently straightforward, as some leksah dependencies are marked as broken (module version issues); however, it is possible with modifications to `~/.nixpkgs/config.nix`. It works on Debian/Ubuntu/Mint systems with NixPkgs 1.11 and a https://nixos.org/channels/nixpkgs-unstable as the nix-channel. These nixpkg build overrides may help get it working for you: { ## To enable flagged broken packages: allowBroken = true; packageOverrides = super: let self = super.pkgs; in rec { haskell = super.haskell // { packages = super.haskell.packages // { ghc7103 = super.haskell.packages.ghc7103.override { overrides = self: oversuper: { GLUT = super.haskell.lib.overrideCabal oversuper.GLUT (drv: { libraryHaskellDepends = drv.libraryHaskellDepends ++ [oversuper.random]; }); gtk2hs-buildtools = oversuper.gtk2hs-buildtools.override { Cabal = oversuper.Cabal_1_24_1_0; }; haddock-api = oversuper.haddock-api.override { Cabal = oversuper.Cabal_1_24_1_0; }; ltk = (super.haskell.lib.overrideCabal oversuper.ltk (drv: { preConfigure = ''sed -i -e 's,Cabal &gt;=1.6.0 &amp;&amp; &lt;1.23,Cabal &gt;=1.8.0,' ltk.cabal''; })).override { Cabal = oversuper.Cabal_1_24_1_0; }; leksah-server = (super.haskell.lib.overrideCabal oversuper.leksah-server (drv: { preConfigure = '' sed -i -e 's,haddock-library &gt;=1.2.0 &amp;&amp; &lt;1.3,haddock-library &gt;=1.2.0,' \ -e 's,haddock-api &gt;=2.16 &amp;&amp; &lt;2.17,haddock-api &gt;=2.16,' \ -e 's,Cabal &gt;=1.10.2.0 &amp;&amp; &lt;1.23,Cabal &gt;=1.10.2.0,' leksah-server.cabal ''; })).override { Cabal = oversuper.Cabal_1_24_1_0; }; haskell-gi = oversuper.haskell-gi.override { Cabal = oversuper.Cabal_1_24_1_0; }; gi-javascriptcore = oversuper.gi-javascriptcore.override { webkitgtk = super.webkitgtk24x; }; gi-webkit = oversuper.gi-webkit_3_0_6.override { webkit = super.webkitgtk24x; }; gi-gtk = oversuper.gi-gtk_3_0_6; gi-glib = oversuper.gi-glib_2_0_6; gi-cairo = super.haskell.lib.overrideCabal oversuper.gi-cairo (drv: { preCompileBuildDriver = '' PKG_CONFIG_PATH+=":${super.pkgs.cairo}/lib/pkgconfig" setupCompileFlags+=" $(pkg-config --libs cairo-gobject)" ''; }); gi-pango = super.haskell.lib.overrideCabal oversuper.gi-pango (drv: { preCompileBuildDriver = '' PKG_CONFIG_PATH+=":${super.pkgs.cairo}/lib/pkgconfig" setupCompileFlags+=" $(pkg-config --libs cairo-gobject)" ''; }); ghcjs-dom = super.haskell.lib.overrideCabal oversuper.ghcjs-dom (drv: { setupHaskellDepends = drv.libraryHaskellDepends ++ [ oversuper.Cabal_1_24_1_0 ]; }); webkitgtk3-javascriptcore = oversuper.webkitgtk3-javascriptcore.override { webkit = super.webkitgtk24x; }; leksah = (super.haskell.lib.overrideCabal oversuper.leksah (drv: { preConfigure = '' # if WEBKITGTK enabled: #sed -i -e 's,cpp-options: -DWEBKITGTK -DLEKSAH_WITH_CODE_MIRROR,cpp-options: -DWEBKITGTK,' leksah.cabal # if WEBKITGTK disabled: # Note: Removing webkitgtk didn't fix UI issues sed -i -e 's,cpp-options: -DWEBKITGTK -DLEKSAH_WITH_CODE_MIRROR,cpp-options: ,' leksah.cabal sed -i -e '1h;2,$H;$!d;g' -e 's/getValueUri :: IO Text.*fixSep x = x//g' ./src/IDE/Pane/WebKit/Output.hs sed -i -e 's,InspectState {..},InspectState {},' ./src/IDE/Pane/WebKit/Inspect.hs sed -i -e 's,openBrowser path,openBrowser (T.pack path),' ./src/IDE/Package.hs # disabled WEBKITGTK # remove bewleksah sed -i -e '1h;2,$H;$!d;g' -e 's/executable bewleksah.*hs-source-dirs: bew//g' leksah.cabal # relax constraints sed -i -e 's,cpphs &gt;=1.19 &amp;&amp; &lt;1.20,cpphs &gt;=1.19,' \ -e 's,jsaddle &gt;=0.3.0.0 &amp;&amp; &lt;0.4,jsaddle &gt;=0.3.0.0,' \ -e 's,webkitgtk3-javascriptcore &gt;=0.13 &amp;&amp; &lt;0.14,webkitgtk3-javascriptcore &gt;=0.13,g' \ -e 's,Cabal &gt;=1.10.2.0 &amp;&amp; &lt;1.23,Cabal &gt;=1.10.2.0,' leksah.cabal ''; })).override { Cabal = oversuper.Cabal_1_24_1_0; }; }; }; }; }; haskell710Env = self.haskell.packages.ghc7103.ghcWithPackages (haskellPackages: with haskellPackages; [ ## NOTE: include all the Haskell packages you use here: cabal-install leksah-server leksah ]); }; } Launch leksah with the command: `nix-shell --pure -p haskell710Env --run leksah` Note: `--pure` is needed, otherwise the Gnome and XDG paths in the environment may prevent Leksah from finding the correct UI files (UI will appear as text only). There is a version 16.x.x of Leksah in development. Hopefully when that is released, it will be as easy as `nix-env -i leksah` to install it again. 
Why do you explicitly thread the `r` argument instead of using a reader? Seems like an unnecessary annoyance when you're already using a monad. Looks really cool though, I can definitely see myself using this.
It works for me and I think I only have bare haskell mode (I do enable `ghci +c`).
a neovim window open, one ghci, one file browser and then, another floating neovim window open over the big main neovim window. Just works, I guess? I do that setup for all developing.
Neat, was hoping this functionality existed! Then again if this kills even the tiniest of optimizations, then I'll rather skip such "neat but non-crucial at my end" machinations.
Emacs + Interro is very nice !
I don't think it would be (apart from giving you a plethora of instances/functions for free)
I see it's only available at ghc8+ :( Glad to see a solution exists though! Thanks!
Nice! I had plans to write an implementation like this years ago but I got bogged in the generic programming aspects. I browsed quickly the code but the implementation was not immediately obvious, perhaps you could document them too?
Takes quite a while at startup but the delay for common commands are tolerable.
GNU Emacs with intero-mode. I find it awesome that you can drop 3 more lines of elisp into your `~/.emacs.d` and suddenly your editor is fully haskell aware. Emacs S2.
Haskell for Mac &lt;3
Well, it was available with ghci-ng pre ghc8.
What parts of it were bad? I've written a few smaller apps in it (including iOS via react native) and have found it to work really well -- though my day job is writing Clojure so I have a lot of familiarity with the ecosystem. Might be worth another shot. But also PureScript is really nice.
[removed]
Hi clahey, thanks for your answer. Yea, you're right, you don't really need to build the tree. However, they do do a check on how fast it runs, so I would have to implement it using memoisation. I was still unsatisfied by not being able to build the tree though!
[removed]
I'm still on ST2, is 3 "worth it" in your view? Is it as utterly robust, bug-free, smooth, fast and all-around care-free a code-editing experience as ST2 (in case you ever used the latter)? Still being actively developed?
&gt; One potential workaround for this would be to have a series of modules called Prelude_8.0.1, etc., which would then allow consumers to import an older version of Prelude for use with newer GHCs (by using NoImplicitPrelude). Something similar already exists in a few variations: https://hackage.haskell.org/package/prelude-compat Apart from base-compat there doesn't seem to be much adoption though!
&gt; Is lynx also well supported? Or better yet, a non-GUI way to view the information from ElfWeb and X86Web? After all, this is for systems programming. One of the big advantages of Linux is that a GUI environment is not required for systems programming. I haven't tried with lynx. `ElfWeb` and `X86Web` are just utility/debugging programs, not the core of the framework. Anyway, initially I was using a console based utility program or the REPL to test the disassembler: you can use the function `disassX86_64` from https://github.com/haskus/haskus-system/blob/master/src/lib/Haskus/Apps/Disassembler.hs to disassemble a Buffer into Text. &gt; Is stack really required? Or is that just your own way of working, and cabal also works? `cabal` should work, but as I don't follow the PVP (missing upper bounds in the .cabal file) you may encounter some issues if you use cabal-install directly.
I think it won't. E.g. many private code bases use http://packdeps.haskellers.com/reverse/opaleye or http://packdeps.haskellers.com/reverse/bloodhound but Hackage reverse dependencies doesn't know about them. Even http://packdeps.haskellers.com/reverse/these has only 6 revdeps!
I did, its right there on the reply
True, GitHub stuff will not show neither. But if packages that have similar goals are proportionally represented, the result can still give a good idea of the situation.
maybe downloads would be a better metric
Notes from my talk on the subject. https://github.com/chadbrewbaker/introToAWSLambda 
Check out my Endoscope project, https://github.com/chadbrewbaker/endoscope
I tried this, but was getting a return code from my program of -11 and nothing else. No stdout or stderr. I've never used nor been aware of ld-linux.so...perhaps that was the missing link? I'll give it a try and report back :)
It's not that good either. E.g. AFAIK `stack` gets stuff from fpco's S3 bucket, and even in front of official Hackage there is caching CDN. And on top of that `cabal` caches tarballs locally, even you rebuild them. All of that makes numbers be so small, that even relative comparison doesn't make much sense: The packages I listed have ~100 downloads in past 30 days (QuickCheck has 200, lens 600). I think that the amount of releases per time affects downloads counter on Hackage, as then you have to go past caches. EDIT: that said, it would be cool to see real usage metrics, but that's only "nice to have" feature :(
Since `Tree a` is a `Comonad`, and `depth :: Tree a -&gt; Int`, `extend depth :: Tree a -&gt; Tree Int`, you can get the local depths of each subtree. This is not part of the problem, but it's interesting.
I use `stack build --fast --file-watch`, no kidding. Stuff like `ghc-mod`, `intero`, `ghcid`.. don't work with GHCJS yet.
Couple of things to note, the c runtime stuff is unnecessary and will bloat any shared library meaning your eventual binary will be larger. It is not a gcc bug. All you need to do to produce a static binary is modify your cabal file: https://github.com/mitchty/alpine-static-tmux/blob/master/Dockerfile#L83 The stack options are fine, with the exception of pthread, that is unnecessary with alpine linux/musl.
There is no single place but I wrote some posts on [School of Haskell](https://www.schoolofhaskell.com/user/icelandj), [gist](https://gist.github.com/Icelandjack/5afdaa32f41adf3204ef9025d9da2a70). **Edit**: Here is the [comment thread] (https://m.reddit.com/r/haskell/comments/4rxxpv/comment/d559j7n).
1. For a library like this I would not use the reader monad. Monads are like frameworks: they make design choices for a large swath of application code and so a library isn't in a good place to know what is best. A user can always stick their FTP connection in an environment if need be. 2. I am not too familiar with FTPS but you should look at the TLS library's API and make a decision based on that. 3. Exceptions here are fine!
Here's a simple solution that uses only simple functions and types, nothing fancy. And straightforward - we build the tree, then traverse it to find its maximum depth. We represent the tree by an association list `[(Int, [Int])]`, where the key is the parent and the value is the list of its children. mkTree :: [Int] -&gt; [(Int, [Int])] mkTree = map gather . groupBy ((==) `on` fst) . sort . flip zip [0..] where gather xs@((x,_):_) = (x, map snd xs) depths :: [(Int, [Int])] -&gt; [Int] depths tree = go 0 (-1) where go depth node = depth : maybe [] (concatMap . go $ depth + 1) (lookup node tree) tree_max_depth :: [Int] -&gt; Int tree_max_depth = maximum . depths . mkTree EDIT: Use association lists instead of maps, to avoid the premature optimization and the need for the extra import. If you need to improve performance, it is easy to switch to a map. In that case, you will also want to make the `depth` parameter strict.
With the caveat that I haven't read the book yet, the intro does state &gt; I have assumed no more mathematical knowledge than might be acquired from an undergraduate degree at an ordinary British university, by which I assume he means an undergraduate *maths* degree. The phrase 'little math background' may need to be taken with a pinch of salt if you only have a high school level exposure to mathematics.
I'll agree with all three answers, with the stipulation that #3 is highly subjective. My rule of thumb is that if you *expect* the error to occur often, I put it in an `Either` or `MonadError`. If it's meant to be truly *exceptional*, I use `IO` exceptions. OP's case is in a grey area to me, and I think depends on the application's use-case (in which case I default to putting as much in the types as possible, without imposing a library, thus using `Either`)
You may want to replace `IO a` with `MonadIO m =&gt; m a` so people can use this in mtl using code without littering `liftIO`s.
I use Notepad++ and ghci. I wish jupyter worked for Haskell for Windows though!
Can someone explain to me why category theory is useful in Haskell? Sorry if this is a dumb question - just trying to understand the connection. I have a background in Math and so I do (sort of) understand the idea behind category theory. My experience with Haskell is no more than some basic tutorials but I am also familiar with the functional programming paradigm in other programming languages. 
&gt; My rule of thumb is that if you expect the error to occur often, I put it in an Either or MonadError. If it's meant to be truly exceptional, I use IO exceptions. That sounds like a very sane approach.
OK, thanks. About stack, I was asking if it is hard-wired into your machinery somehow. Dependencies with no upper bounds do not cause any significant problem building with modern cabal for a skilled user.
Philly is a mixed bag. We don't have a Haskell Users Group for example. But we do have two really smart professors, namely Stephanie Weirich (at UPenn) and Geoffrey Mainland (at Drexel). There is also HacPhi (Richard Eisenberg et al.) which is a wonderful event where some awesome people show up, including Edward Kmett, Daniel Bergey, Ryan Trinkle, etc. And we do some really awesome stuff at HacPhi (for example, last time I was there, Anthony Cowley gave us a tour of the GRASP lab). But all that being said, HacPhi is just once a year, and I would disagree that we have an active Haskell community (unless you happen to be inside UPenn and it's different there). That's just my opinion; living in Philly for the last 3 years (and self-taught Haskell user for the last 2 years).
Multiple registers as in each cursor keeps track of it's own copy-pasting; see [this gif](https://camo.githubusercontent.com/cb77f87941ff9e95c4cfa875f6572246b3899add/68747470733a2f2f662e636c6f75642e6769746875622e636f6d2f6173736574732f3930373133382f323334353136392f30636432343639382d613533302d313165332d383630642d3335386537323134323365662e676966) for an example. There's a great thread in the neovim issues on some cool stuff you can do with [multiple cursors here](https://github.com/neovim/neovim/issues/211). I don't understand what your last paragraph is asking, sorry :P
Monads are inspired by category theory, I think.
Considering the first few pages of the introduction present proofs dealing with homomorphisms, vector spaces, and rings it seems likely.
Good to know that I don't need a Reader. Less work for me!
Do you know how I could do this with bracket? I saw something about lifted-base, but that has a bracket with MonadBaseControl, which does not seem to like being called MonadIO.
Ok I'll probably do a typeclass like you suggested. That should also make it easy to add SFTP in the future.
[removed]
&gt; The Tisch page is a great intro to type level programming. It absolutely is. Servant motivated me to learn Haskell because the type-level programming seemed like awesome levels of magic, but that dive into Tisch really made it feel more approachable as a technique I could employ on my own.
Indeed. After reading the Tisch page I wouldn't hesitate to try it out, though I haven't looked at Silk's Opaleye later. Also, since Tisch is using GADT's a lot rather than existentials I'd think performance shouldn't be an issue.
Losing wreq has kept my script code base out of sync with my backend. Surprised it hasn't been noticed. 
Hopefully 10 years from now haskell will have good odbc libs for MS Sql Server and Oracle ;) 
Given that you're talking about `Functor`s: the Yoneda lemma [gives us `fmap` fusion](https://gist.github.com/co-dan/c874b9624ef09399a2c7).
Now I'm confused. I know that [Coyoneda] gives the free functor for any type function, which basically means `fmap` fusion. So Yoneda does the same?
&gt;Can someone explain to me why category theory is useful in Haskell? It's not useful for programming as presented to mathematicians, but if you want to understand the language design and theoretical aspects it's well worth your time. &gt;I am also familiar with the functional programming paradigm in other programming languages. Monads are from category theory initially, and they're useful both for parsers and for functional programming in general: http://www.johndcook.com/blog/2014/03/03/monads-are-hard-because/
I think that's almost certainly not what you want -- its an internal component of the way happstack does http. /u/stepcut251 can probably give better advice than me here. But I guess you're looking for websocket support? If so, this might not be ready for primetime but seems appropriate: https://github.com/Happstack/happstack-websockets Otherwise, maybe you could describe what you want to do in a bit more detail...
[removed]
You definitely would not use that. Are you trying to serve streaming text or retrieve streaming text? Assuming you are trying to serve stream text, where is this text coming from? Is it being generated by a pure function? read from a file? created dynamically using IO?
&gt;more haskell programmers like python programmers ? like Java ? Haskell is already incredibly mature for some domains and nearly useless in others. I could see there being a lot more interest in parallel computing that meant functional languages got a lot more attention. &gt;Everyone is talking about the future of functional programming.. haskell is one of them Haskell certainly isn't going to grow on hypes and fads.
Doing it in the `.cabal` file can cause problems where you don't remember what extensions are on, so I only put `OverloadedStrings` there. Everything else is on a per-file basis. So far I haven't had the need to use any one extension project-wide, but I could see that being the case with e.g. a `yesod` website.
I'm trying to serve streaming text that's being read from a file Edit for more details: I'm checking to see if a file's modification time is newer than the last time I sent data to the client, and if so send new data 
Basically, Haskell has an unusually rich type system for a programming language, and as a result many constructions from abstract algebra can be easily represented in Haskell code. As a result, you find yourself wanting to use categorical structures to organize these constructions, for basically mundane software engineering reasons like avoiding code duplication and boilerplate. The same thing would happen in any language with a rich enough for system -- even programmers in OO languages like Scala feel the same design imperative.
So tail -f guarded by modification time?
We can [expect `DependentTypes`](https://typesandkinds.wordpress.com/2016/07/24/dependent-types-in-haskell-progress-report/) in GHC 8.4 (**2018**) the earliest or in 8.6 or 8.8 (**2019**–**2020**). Other than that I'm sure we will discover new abstractions, new ideas and hopefully the community will continue to flourish.
why is this "streaming" and not just a response? is it over a websocket or something? or is it a normal response that keeps sending the file over and over as its modified without closing?
I don't reinvent wheels that badly. The closest I've come is one in-progress project which will replace my e-mail client and RSS reader and that's specifically because, in pursuit of greater user efficiency in the face of a flood of data, it uses a data model and UI design so different that it would be much more impractical to extend an existing program. (The initial work on it was actually my final project for my bachelor's degree.)
I confess I sometimes use `extract` instead of `snd` and [`ask`](http://hackage.haskell.org/package/comonad-5/docs/Control-Comonad-Env-Class.html) instead of `fst`, when a tuple is better understood as a value with some context. By the way, is this type a comonad? data Timeline s t = Timeline [(t,s,Timeline s t)] t I know how to write `extract`, but I'm unsure about `duplicate`. My intuition tells me that each `t` would be replaced by "the list of `(t,s,Timeline s t)` up to that `t`" but I'm not sure if this is correct.
`stack ghci package1 package1` will get crazy if those two packages have different `default-extensions`, so per-file is the more considerate option.
It's a normal response that sends the file as it's modified, closing it each time. The file may or may not be updated once every couple minutes. I'm trying to implement the first answer [here](http://stackoverflow.com/questions/13386681/streaming-data-with-python-and-flask) in a haskell server.
I think a list like the "state of the Haskell ecosystem" is a lot more readable and useful.
You'll probably want to look at [this](https://gist.github.com/thoughtpolice/5843762#file-coyoneda-hs-L57).
I am using IntelliJ with the plugin https://github.com/rikvdkleij/intellij-haskell
Not sure if this is sarcasm, isn't your standard monadic stack = `IO`?
I don't believe in "next year will be a year of parallel computing" -hype. OTOH, Duke Nukem Forever was actually published...
It is not sarcasm ;) It is something that I'm sure that will happen. the problem is not his feasibility, which has been [verified](https://github.com/transient-haskell/transient), but the acceptance of this "paradigm" that is functional in the hardest sense. But it will happen, since it makes things a lot easier. IO can not compose in the presence of effects like multithreading or distributed computing following algebraic and monadic rules. IO can compose file reads-writes monadically and so on. But apart from that, little else. This composition of something asynchronous like file reads is a huge win for programmers. But much more composability is needed.
Thanks for your patience, Gabriel, but I'm still not getting this. Here is what I have so far: {-# LANGUAGE LambdaCase #-} module Pipes.Network.Telnet where import Control.Monad import Control.Monad.State.Strict import Data.ByteString (ByteString) import qualified Data.ByteString as B import Data.Word8 import Pipes import qualified Pipes.Parse as PP data TelnetData = Data ByteString | Will Word8 | Wont Word8 | Do Word8 | Dont Word8 deriving (Eq, Show) will :: Word8 will = 251 wont :: Word8 wont = 252 do_ :: Word8 do_ = 253 dont :: Word8 dont = 254 iac :: Word8 iac = 255 telnetDataParser :: Monad m =&gt; StateT (Producer ByteString m ()) (Producer TelnetData m) () telnetDataParser = hoist lift PP.draw &gt;&gt;= \case Nothing -&gt; pure () Just chunk -&gt; do let (bytes, command) = B.break (== iac) chunk unless (B.null bytes) (lift . yield . Data $ bytes) -- Emit data downstream ASAP. This might be unwise...? if (B.null command) then telnetDataParser else (hoist lift $ PP.unDraw command) *&gt; iacParser iacParser :: Monad m =&gt; StateT (Producer ByteString m ()) (Producer TelnetData m) () iacParser = do -- I'm not sure how to fill this in. At this point I know that the ByteString will start with \255. -- I want to recognise at least the following two cases, for argument's sake: -- "\255\255" (yielding Data (B.pack [255])) and "\255\254x" (yielding Dont x, for any x) -- Then, the rest of the producer should go back into the passed-around state for telnetParser to have another go. undefined telnetDataParser parseTelnet :: Monad m =&gt; Producer ByteString m () -&gt; Producer TelnetData m (Producer ByteString m ()) parseTelnet = execStateT telnetDataParser [This remark in `Pipes.Text.Tutorial`](https://hackage.haskell.org/package/pipes-text-0.0.2.5/docs/Pipes-Text-Tutorial.html#g:9) sounds a lot like what I want: "Its use is easiest to see with an decoding lens like utf8, which "sees" a Text producer hidden inside a ByteString producer". I think I want to see a "TelnetData" producer inside the "ByteString" producer that is the socket, but however I try to write the function I seem to get caught up in the fact that I can't ask the parser to emit little `Data` chunks when it finishes parsing a ByteString with no `\255` values. I still haven't got my head around when it's appropriate to parse with lenses vs. `pipes-parse` (or something that builds on its primitives). What motivates the decision there?
This is actually very interesting!
Ugh, how has REST gone so wrong. Why don't you just use SOAP? EDIT: So here's what I would like to have: a library that helps you write proper REST services and proper REST clients. If you're thinking about URIs you're not doing REST.
The built-in editors are simple, but functional, and the performance is good (Leksah supports two editors that I know of: Yi, a Vi-like, and GTKSourceView, the default). Stability is fairly good, though it has crashed occasionally. I've never lost any work because it always saves any changes (the default behaviour continuously compiles code changes for error / warning updates). I find the meta-data symbol-info views very useful for checking type signatures and navigating the code base.
is Rust better for the memory access stuff?
Wow, thanks. I should have thought of using QuickCheck.
I don't know much about Rust, but I would expect it can do more fine grained memory management. GHC is using stop-the-world GC algorithm which is not the best if you are doing some real-time computations (like games). Rust, as I understand, is capable of determining when to remove some objects from memory at compile time. This is just what I heard about Rust, so don't know much but there was a discussion here so this thread might be helpful https://www.reddit.com/r/haskell/comments/47rjr4/comparing_haskell_and_rust_which_to_choose_when/
I've been using Opaleye since before it was publicly available, and you are correct, it requires an infuriating amount of boilerplate. I haven't looked at Frames or Tisch, but I would strongly encourage you to look at [Silk's Opaleye layer](https://github.com/silkapp/silk-opaleye). I'm traveling now, otherwise I would post an example of the above code using Silk's layer (maybe later). Also, I have found modules at the table (type) level help to keep things maintainable. I generally put my table types in their own modules, where the module hierarchy reflects the database's schema and table hierarchy (assuming you're not using schemas for user namespacing). Since queries can frequently involve more than one table, I use a separate, logical module organization for my queries. It's not perfect, but I've found it much better than having a lot of types in one module.
Paging /u/lexi-lambda
What characteristics do you need?
&gt; I really have almost zero interest in implementing type systems. [...] I probably can’t do all of this on my own. [...] So if you’re interested in Rascal and think you might be able to pitch in, please: I would appreciate even the littlest bits of help or guidance! [...] In the meantime, I will try to keep picking away at Rascal in the small amount of free time I currently have. And I think implementing a type system sounds like fun! The project seems super interesting, so I'd be eager to help, but I don't know Racket yet (I do know Scheme) and I don't have a lot of free time either. Among your [issues](https://github.com/lexi-lambda/rascal/issues), is there an easy one you'd recommend as a way to get my feet wet?
&gt; And I think implementing a type system sounds like fun! Awesome! Any help would be loved and appreciated, even if it’s just being a guinea pig. :) &gt; The project seems super interesting, so I'd be eager to help, but I don't know Racket yet (I do know Scheme) and I don't have a lot of free time either. Among your issues, is there an easy one you'd recommend as a way to get my feet wet? Bluntly: probably not. Fixing the [bugs](https://github.com/lexi-lambda/rascal/issues?q=is%3Aissue+is%3Aopen+label%3Abug) is the most important right now, but also probably the hardest, since the code is a mess and was totally hacked together. Also, for better or for worse, a solid understanding of Racket and its macro system is necessary to grok a solid portion of the current code, since Rascal sort of pushes the macro system to its limits (even potentially wading into undefined behavior in a couple spots). In theory, the easiest issues are probably [#10](https://github.com/lexi-lambda/rascal/issues/10) and [#11](https://github.com/lexi-lambda/rascal/issues/11), which are wholly syntactic and don’t involve touching the typechecker at all. However, for that reason, they also probably require more knowledge of Racket, since they involve some semi-advanced macrology. If you’re interested, chatting and asking questions is probably more useful than just throwing yourself at the codebase, so feel free to poke me in the #racket channel on Freenode or on the snek PLT slack community, which you can [sign up for here](http://snek.jneen.net).
You can try to do it now in haskell: https://hackage.haskell.org/package/inline-java 
Well, the first things you need will be a library for making HTTP requests; there are bindings for libcurl on hackage, as well as a native Haskell implementation of the HTTP protocol; and a library for handling JSON, for which Aeson is the canonical choice. You probably want to define types for your responses, and give them FromJSON instances, which makes them easier to handle in Haskell than Aeson's raw `Value`s. Then you need a way to do things in parallel; `forkIO` is your friend here. And you need a way to send data between threads; `Control.Concurrent` from `base` has plenty of primitives for this. A simple solution is to create an `MVar` with a data structure that holds your results, then spawn one thread per image to make the additional request. Each thread receives the MVar and an image ID, and updates the MVar atomically when it receives a response. The main thread just joins all the spawned threads to wait for all of them to finish, at which point the MVar will be completely populated. The rest of it is more a matter of basic web application design, hard to give you any useful pointers without knowing more about your chosen stack etc.
Kind of everything. I think it even inherited Clojure’s hideous error messages. Also there was exactly no tooling when I tried it.
This looks like a very interesting project. I do wish though, that it wasn't built on racket. Not that I have anything against racket. But racket having such a fancy macro system, you kind of lose what is required to make this work as an interpreter vs an embedded language. An implementation in haskell would clarify that as the interpreted language would not be part of the implementation language but wholy separate.
It's a funny habit to pick up. :) Did that type arise out of something you were working on?
oke, I did a lot of chapters of the haskell book but I did not get to the part that Mvar are explained 
As far as I can tell, not very much right now. The currently slated plans for that proposal mostly just involve improving TH to use the GHC AST, which, while nice, doesn’t change the power of GHC’s existing metaprogramming options in any meaningful way. More long term plans are potentially semi-related, in that they would allow the compiler to be more easily tweaked, but it doesn’t seem like there are any plans to allow Lisp-style syntactic abstractions. Extensible syntax is pretty hard unless your syntax is *very* simple, and only Lisps have been very successful with them. Haskell’s AST is remarkably complex, so it seems like it would be very hard to make those sorts of things work. Dylan has done some research into that sort of thing with “D-expressions”, but I don’t know how successful that has been in practice.
Hackett or Haskett are better names than Rascal in my opinion.
How about [Rathskeller](https://en.wikipedia.org/wiki/Ratskeller)?
Scallywag ? :)
I'd agree. When I was first looking to learn, it was the first one that I was pointed to.
Can you elaborate?
To elaborate, that link is to an excellent guide on the tutorials and guides for Haskell. It recommends CIS 194 as the first step.
It's basically the same thing anyway.
Things might be a pinch better in opaleye-tf: http://github.com/ocharles/opaleye-tf The downside is there is practically no documentation, and features are only added as we need them in CircuitHub. I haven't got confident enough in the API to announce it beyond odd comments yet, hence the lack of documentation/polish. https://github.com/ocharles/opaleye-tf/blob/master/test/Tests.hs may help you get going if you want to give it a spin.
Have you looked at Elixir? It seems to be an interesting mix of "normal" syntax with lisp-style macros.
But Rascall has that nice scheme/racket/guile vibe going on.
So why not compile to Core and use the Haskell ecosystem? I mean how is this more than a different syntax for Haskell?
There's also https://github.com/byteally/dbrecord-opaleye in a similar direction. I was pretty excited when I discovered it, but I'm not sure if it's actually moving toward a Hackage release.
/$
faster, active. do it
In other words transient will be a lot more popular? ;) Maybe. At the very least you've convinced me to take a closer look. Is this the best resource to learn from: https://github.com/transient-haskell/transient/wiki/Transient-tutorial ?
Highly recommend reading part 2 of this book: http://chimera.labs.oreilly.com/books/1230000000929
Indeed, Rascal happens to be a wonderfully perfect name… and sadly, it seems maybe *too* perfect. It is an obvious pun on the names Racket and Haskell, and yet it is a simple word that follows in the long tradition of Scheme names, going all the way back to Planner. I would be a little sad to part with it, but it would be rude to use it if something else in the same space is actively using it.
Are you aware of the [Corrode](https://github.com/jameysharp/corrode) C-to-Rust translator? Although the #define and #ifdef directives might pose a challenge.
Certainly. Quasiquoters have a number of drawbacks, but the two main ones are complexity and lack of composition. S-expressions happen to be simple, and this means s-expression macros have two lovely properties: they’re easy to write, given good libraries (Racket has [`syntax/parse`](http://docs.racket-lang.org/syntax/stxparse.html)), and they’re easy for tools to understand. Quasiquoters force implementors to write their own parsers from raw strings of characters, which is quite a heavy burden, and it usually means those syntaxes are confusing and brittle. To give a good example, consider persistent’s quasiquoters: they look *sort of* like Haskell data declarations, but they’re not really, and I honestly have no idea what their actual syntax really is. It feels pretty finicky, though. In contrast, a s-expression based version of the same syntax would basically look just like the usual datatype declaration form, plus perhaps some extra goodies. Additionally, s-expression macros *compose*, and this should probably be valued more than anything else. If you’re writing code that doesn’t compose, it’s usually a bad sign. So much of functional programming is about writing small, reusable pieces of code that can be composed together, and macros are no different. Racket’s `match`, for example, is an expression, and it contains expressions, so `match` can be nested within itself, as well as other arbitrary macros that produce expressions. Similarly, many Racket macros can be extended, which is possible due to having such uniform syntax. Making macros “stand out” is an issue of some subjectivity, but in my experience such a fear of macros tends to stem from a familiarity with bad macro systems (which, to be fair, is almost all of them) and poor tooling. I’ve found that, in practice, most of the reasons people want to know “is this a macro??” is because macros are scary black boxes and people want to know which things to be suspicious of. Really, though, one of the reasons macros are complicated isn’t knowing which things are macros, but it’s knowing *which identifiers are uses and which identifiers are bindings*, and things like that. [Solve that problem with tools that address the problem head on, not by making a syntax that makes macros second-class citizens.](http://i.imgur.com/HvYee19.png) One of the reasons I used the phrase “syntactic abstractions” in my blog post is because you specifically want them to be **abstractions**. If you have to think of a macro in terms of the thing it expands to then it isn’t a very watertight abstraction. You don’t think about Haskell pattern-matching in terms of what the patterns compile to, you just use them. Macros should be (and can be) just as fluid.
Sometimes I think I'm getting to know Haskell and then I read a comment like this. 
Will have a look :)
I am not intimately familiar with Elixir macros, but my understanding is that they are pretty much traditional Lisp macros, but they require all macro invocations to be well-formed Elixir code from the parser’s point of view. So you get the ability to create syntactic extensions, but you don’t get the ability to make new syntax. Lisp macros don’t really have the ability to make “new syntax” either, but Lisp syntax is so remarkably simple that it is effectively nonexistent, which means you aren’t restrained by your language’s AST. “Homoiconicity” is a lie, *all* languages are homoiconic with their AST. The only difference is that, in Lisps, your source syntax is so trivially mapped to its AST that you basically are *writing* the AST, so you get a flexibility you don’t get from other languages.
that's how I feel about Haskell, Emacs, and anything worth learning :-) 
Woah, thanks for the link! 
Thanks. I did stumble upon that but those are for the 2014 edition. would you recommend them?
Looking through the source, I see three instances of `GenericProfunctor` that cannot be instances of `Category`, plus one (`Star f`) which would require a stronger constraint on its argument. newtype Zip f a b = Zip { runZip :: a -&gt; a -&gt; Maybe (f b) } newtype Create f a b = Create { unCreate :: [f b] } newtype Consume f a b = Consume { unConsume :: f a }
I would, but I'm biased since I did the first one. I think they're for the spring 2013 version, which was pretty great - I haven't tried the other versions. 
Category theory gives a way of thinking that is useful in any programming language, not only Haskell. Here is my answer why Functors are useful (using JavaScript but everything can be translated into Haskell or any other language): http://stackoverflow.com/questions/2030863/in-functional-programming-what-is-a-functor/41368880#41368880
Thanks for the input.
Is there good source to learn simple practical applications of Yoneda lemma?
So I see 3 most obvious interpretations of "monoidal profunctor" and yours agrees with the 2 that I like, so that part of the name seems sensible to me. On the other hand, "bimonoidal" seems bad to me because it says you have 2 monoidal structures in mind but it doesn't say which ones, so I prefer "bicartesian monoidal profunctors". The only part that I can't justify is that it's `p a a` and not `p a a'`, do you need that? 1. A monoid object in the bicategory of cats, profunctors and natural transformations. As mentioned in another comment these are `Arrow`s minus `first`. I don't like this name though because a monoidal category is a monoid in a bicategory whose *objects* are categories, whereas this is a bicategory whose *arrows* are profunctors. 2. To get something whose objects are profunctors, we can take a category whose objects profunctors `P : C -|-&gt; D`, arrows from `P : C -|-&gt; D` to `Q : C' -|-&gt; D'` are triples of functors `α_l : C -&gt; C'`, `α_r : D -&gt; D'` and a natural transformation `α_m : P =&gt; Q(F_l,F_r)`. (This is usually defined as part of a double category). Then we can make this a monoidal category using product of categories and product in Set by `P x Q : C x C' -|-&gt; D x D'` defined by (P x Q)((d,d'),(c,c')) = P(d,c) x Q (d',c') and the unit being hom on the one-object category `I : `1 -|-&gt; 1`. I could keep going, but it turns out it's equivalent to the 3rd definition so I'll be more explicit there. I'll leave it as an exercise to show that they're equivalent :). 3. A profunctor is a functor `P : D^o x C -&gt; Set`, so if `D,C` have monoidal structure we could ask that `P` be lax monoidal (the usual kind of monoidal functor). Here, `C,D,Set = Hask`, but we'll use 2 different monoidal structures. For plus this would mean we have multiplication `μ : P(a,b) x P(a',b') =&gt; P(a + a', b + b')` and unit `η : 1 =&gt; P(0,0)`, which is exactly your requirement for Either, modulo currying and the fact that `a = a', b = b'` in yours. Then changing the + to a x gives you the one for pairs. I actually found a reference for this definition, they are called "lax monoidal distributors" [here](https://arxiv.org/abs/1405.7270) (distributors are a different name for profunctors), and they reserve the term monoidal distributor for something slightly different (not the strong monoidal ones), so I think you would be fine with going with "monoidal profunctor".
IIRC camlp4/ocaml was quite powerful. Basically, after a keyword you could parse input however you need. Probably same can be done for grammar of Haskell.
Thanks! Indeed the `a`s and `b`s don't need to be the same, and they aren't in the actual implementation. But at least at the moment, this fact isn't used anywhere. For the laws, the GHC.Generics docs say this: &gt; As :+: and :*: are just binary operators, one might ask what happens if the datatype has more than two constructors, or a constructor with more than two fields. The answer is simple: the operators are used several times, to combine all the constructors and fields as needed. However, users /should not rely on a specific nesting strategy/ for :+: and :*: being used. The compiler is free to choose any nesting it prefers. But this only means that `plus` and `mult` should be associative. And it would be fair to require that `void` and `unit` are their identities. But I'm not sure about distributivity. I think there are enough interesting generic functions where this fails to not require it. Also because Haskell datatypes are always sums of products, never products of sums.
Haskell is aimed at a much higher level of abstraction than Rust. Rust is aimed at more or less the same level as C, but with safety built in. Hence it will make sense to translate the C into Rust on a function-by-function basis. Haskell is much more declarative, so a translation to Haskell will involve reverse-engineering the design of the C, and then translating that design into Haskell. If this is done well then the result will be shorter and hence cheaper to maintain in the future, but it depends crucially on your ability to reverse engineer the design. If you have a clean design for your C then this is feasible. However if it is complex with a long maintenance history then you may find this difficult.
I was thinking serialisation was an obvious example, f.e. binary serialisation where plus outputs a 0 or 1 for the left or right choice. But then only the ordering of the output is different, so if you say "up to iso" it is indeed hard to come up with a good example.
Unfortunately not!
&gt; Can ML be stopped? NEVAR!
I'd suggest using [async](https://www.stackage.org/nightly/package/async) instead of working with raw threads. `Async` provides nice operations like `wait`/`poll`/`cancel` , and makes it impossible to silently ignore exceptions thrown in forked threads. It also provides a `Concurrently` type which is a wrapped `IO` with a special `Applicative` instance that automatically parallelize things for you, saving the trouble of forking/fetching.
I hope we have something better than monad transformers when dealing with effects.
&gt; I'd want to look for laws relating plus and mult in some way in particular, and what you get maybe are profunctors that respect the structure of "distributive categories". I'm confused by the categorical language but still interested. What laws does this translate to in the end?
Forking and splitting cryptohash was a right thing to do. I use those packages, thank you.
Ah, Opaleye even has `SumProfunctor`, even though it doesn't have an identity method. Good to see more possible uses!
Indeed, it needs a fetch polyfill.
*EDIT*: ah, now I see. let me think... The class in the `one-liner` can be generalised to have `a a'` arguments too: class Profunctor p =&gt; GenericProfunctor p where zero :: p (V1 a) (V1 a') unit :: p (U1 a) (U1 a') plus :: p (f a) (f' a') -&gt; p (g a) (g' a') -&gt; p ((f :+: g) a) ((f' :+: g') a') mult :: p (f a) (f' a') -&gt; p (g a) (g' a') -&gt; p ((f :*: g) a) ((f' :*: g') a') absurdV1 :: V1 a -&gt; b absurdV1 x = case x of {} instance Applicative f =&gt; GenericProfunctor (Star f) where zero = Star $ absurdV1 unit = Star $ \_ -&gt; pure U1 plus (Star f) (Star g) = Star $ \case L1 l -&gt; L1 &lt;$&gt; f l R1 r -&gt; R1 &lt;$&gt; g r mult (Star f) (Star g) = Star $ \(l :*: r) -&gt; (:*:) &lt;$&gt; f l &lt;*&gt; g r And instance Applicative f =&gt; GenericProfunctor (Zip f) where zero = Zip $ absurdV1 unit = Zip $ \_ _ -&gt; Just (pure U2)
I'm not a fan of pretending you're pure when you're not. Although exceptions are not liked by some, I don't see anything bad in using them to signal well... *exceptional* situation and shortcut execution. After all they are in the lang for a reason. You can wrap with `try` if you like.
If the replacement package is supposed to replace an existing package, then I think keeping the same module name makes sense as it makes porting easier. If you'd need to use both packages as direct dependencies, then it's a bad idea as you'll need to use package qualified imports. Indirect dependencies shouldn't be a problem though.
congratulations hvr ! So package with less stuff are less big than package with more stuff. I'm glad we worked it out. I'll be watching when people realize that putting stuff in small packages instead of multiple modules in one package mean exponentially shooting up your compilation time, or that if you need some other hash (apart from sha1, sha256, sha512 that you bothered making package of) you pretty much SOL and have to rely on some other package (or even cryptohash/cryptonite). But apart from this, which are somewhat not that relevant .. Where were you when you I was maintaining `cryptohash` and `cryptonite` on my own ? Where were you after `cryptohash` got rewritten to use `cryptonite` and you're were not happy about the dependency ? Did you offer to maintain cryptohash yourself maybe ? Did you manifested your disagreement about the dep ? Oh that's right, no. silently just forking the code in your corner, and make your own 5 minutes packages. profit &amp; glory ! Now, let's be clear I perfectly respect the right of people to fork anything they want, but don't expect me to be interested in this despicable community of yours. with all my contempt,
Just a check-in to see if I understand the other things listed.
Yeah sorry, it should be fixed now.
perfect respect for the act of forking code into another place (i.e. LICENSE says it's ok). god speed &amp; all the best &amp; yadda yadda.
I'm a fan of only using exceptions when you genuinely don't think this error should happen at runtime. For common errors that are *supposed* to happen, I think `Either` is better. Though I'm not too familiar with Flac or your code, it seems like your use of exceptions here is fine.
That package is supposed to serve as a drop-in `cryptohash` replacement for clients that only need SHA-256.
Thanks, I will read those
Thanks for the material
"Forking to another place" just sounds like a copy. I would consider it a fork only after a user tries to release some changes they've made. It sounds like hvr posted some github issues and wanted to take the project in a direction you didn't want to go with it. Eventually he did like haskell-lang.org and decided the best way to achieve what he wanted was to fork. Also, I want to just say this like it is: It is rude to sign your messages "with all my contempt" and obviously you intended it to be rude.
This blog post should probably be linked to from the package itself or its readme
I'm probably not part of the target audience but I had no clue what "Eventsource applications" and "Eventsourcing" means. I just looked and it seems to refer to "Event-driven programming" in the case of a web client whose event handler uses a specific web API (called `Eventsource`) to receive events from the server it is connected to.
My favorite line from the README: &gt; FLAC is awesome and Haskell is awesome, surely there should be a way to achieve an even higher level of awesomeness...
* If you know other languages: Don't judge too quickly based on syntax or go straight for mutation/arrays. * Do not dismiss simple things, it doesn't have to be complicated to be useful. data Bool = False | True data Maybe a = Nothing | Just a data Either a b = Left a | Right b data Ordering = LT | EQ | GT (.. what Edward Kmett refers to as [‘dumb reusable data’ ^^[**YouTube**]](https://www.youtube.com/watch?v=hIZxTQP1ifo&amp;feature=youtu.be&amp;t=1261)) id :: a -&gt; a id x = x const :: a -&gt; b -&gt; a const x _ = x * Write instances by hand, write them many times and make sure you understand the **un**intuitive ones. You know `fmap @[]` is the same as `map` (the `@` uses [*type applications*](https://downloads.haskell.org/~ghc/master/users-guide/glasgow_exts.html#ghc-flag--XTypeApplications)) but `fmap @((-&gt;) _) = (.)` may surprise you. Same with (see [video](https://www.youtube.com/watch?v=vV3jqTxJ9Wc) if interested) &gt;&gt;&gt; :set -XTypeApplications &gt;&gt;&gt; :t filterM @[] filterM @[] :: (a -&gt; [Bool]) -&gt; [a] -&gt; [[a]] &gt;&gt;&gt; &gt;&gt;&gt; filterM @[] (\_ -&gt; [False,True]) "ABC" ["", "C", "B", "BC", "A", "AC", "AB", "ABC"] * More language specific, use the language extension `InstanceSigs`. Type signatures help you understand what function you are writing {-# Language InstanceSigs #-} data AB = A | B instance Eq AB where (==) :: AB -&gt; AB -&gt; Bool A == A = True B == B = True _ == _ = False I will use `fmap @((-&gt;) _)` as an example: &gt;&gt;&gt; :t fmap @((-&gt;) _) fmap @((-&gt;) _) :: (a -&gt; b) -&gt; (t -&gt; a) -&gt; t -&gt; b To learn: write your own versions of type classes and try to understand the instances. {-# Language InstanceSigs #-} class Functor2 f where fmap2 :: (a -&gt; a') -&gt; (f a -&gt; f a') instance Functor2 [] where fmap2 :: (a -&gt; a') -&gt; ([a] -&gt; [a']) fmap2 = map instance Functor2 Maybe where fmap2 :: (a -&gt; a') -&gt; (Maybe a -&gt; Maybe a') fmap2 _ Nothing = Nothing fmap2 f (Just x) = Just (f x) instance Functor2 (Either a) where fmap2 :: (b -&gt; b') -&gt; ((Either a) b -&gt; (Either a) b') fmap2 _ (Left x) = Left x fmap2 f (Right y) = Right (f y) instance Functor2 ((-&gt;) a) where fmap2 :: (b -&gt; b') -&gt; ((a -&gt; b) -&gt; (a -&gt; b')) fmap2 = (.) * Enjoy the experience, don't get bogged down with boring details
* How the layout rule works! I thought you had to write code with tons of whitespace and alignment, but you can just newline+indent (with spaces, of course) after `do`, `let`, &amp;c. * How to read type signatures. In particular, `-&gt;` is right-associative, `forall` is implicit, and thinking of typeclass constraints as implicit parameters makes it clearer why they’re on the left of an arrow. * All the beginner questions that get endlessly repeated on Stack Overflow, like why `sum xs / length xs` doesn’t work. I don’t know if this would come up in the first two weeks, but I found `=&lt;&lt;` easier to understand than `&gt;&gt;=` as a beginner, because it gave me an intuition about other application-like operators: pure $ pure pure &lt;$&gt; impure impure =&lt;&lt; impure 
Great! I will remember that.
Yeah, these packages also have the common purpose to automatically generate instances. One difference is that Opaleye generate their instances via Template Haskell whereas one-liner uses GHC.Generics. one-liner seems to provide a way to implement lots of type classes generically, by picking an appropriate profunctor. Whereas from what I've seen Opaleye is more specifically concerned with generating `Default` transformations whose types are profunctors.
http://hackage.haskell.org/package/linearmap-category solves eigenvalue problems natively.
That it's not possible to go from `IO String` to `String` and that this is earth shattering and brilliant to a newcomer. (Don't even start with the `unsafeXYZ` comments.) 
`{-# LANGUAGE PackageImports #-}` and stop caring about it? Seems like a silly thing to be upset about 🙄🙄🙄
Teach them first how to print something, how to use do and in general how to do ordinary tasks like loops with forM etc until they can create ordinary programs. Only after this moment, teach them what is behind. Otherwise they will think that functional programming is something useless for people that just want to play with GHCI and write papers. And they will infer that right. That is what most of the haskell students learn from Haskell due to pedantic teachers and bad pedagogy. ---- NOTE: after this innocent comment I have been banned, my new user silenced and the comments below removed. This gives an idea about who rules this thing. Each one may extract their own conclusions. Probably they will remove this message too. For his interest I reproduce the deleted posts [here](http://lpaste.net/raw/350916) Fellow Real World programmers: For the future of Haskell, fight the sectarians! See you tomorrow right here. 
[removed]
I remember being annoyed by that for a sec. Like, ok I get that you like to have other people listen, now give me the value :) 
It's a fair comment which illustrates that different time horizon have different goals. What is bothering is mistake one for the other 
[This](https://www.reddit.com/r/haskellquestions/comments/5lj45c/tips_on_the_best_way_to_build_a_tree_not_a_binary/dbx96b6/). Which could also lead in to the Maybe monad, but I think the immediately important lesson is "prefer `fmap` over unwrap," because it takes the pain out of using total functions.
Thanks for your remarks, next time I will give more context to my article. I didn't know `EventSource` could mean something different in the IT world.
How does it affect latency?
That constructors are actually not constructors in a OOP way but just a "tag", which can be used to remember how the data has been "contructed". Basically, when you have data IntFloat = I Int | F Float doing `I 4` create something equivalent to `("I", 4)` not `("IntFloat", 4)`.
I think the graph would be more interesting with the `acme-everything` node removed :-)
That this: data Something = S Int Int Int getFirstInt :: Something -&gt; Int getFirstInt (S x _ _) = x is equal to: data Something = S {s1::Int,s2::Int,s3::Int} getFirstInt :: Something -&gt; Int getFirstInt s = s1 s Here giving names to a constructor's arguments doesn't save you much work, but it's very useful when data-type structures have to be longer (because it also makes the code more readable). And even better is that you can use `x {s1=5}` where `x :: Something` and get an altered version of `x`.
The most confusing thing for me was "what is a type constructor and what is a data constructor and how are they different"? Like, what in the world was "data Foo = Foo Int" and how come Foo is on both sides but mean different things? I thought things that start with uppercase are types and lower case are values??? The other thing was that pattern matching on the left hand side of = isn't just for defining functions and constants, but you can do "deep" destructuring assignments. I'm pretty sure the teacher explained that, but it didn't sink in for a while what to do instead of having dot/getters.
That monads don't do anything, they're just structures. IO doesn't do anything, it just builds up a structure. Do comprehensions don't do anything, they just build up a structure. It's up to the Haskell "language" to figure out what to do with the structure. --- In retrospective, I've taken a Haskell class at uni after having used it for a few years, and this is probably the biggest issue my classmates had understanding as well. People just expect monads to do things, or just in general expect code to do stuff, instead of just building up the proper data structure that gets evaluated later.
I find syntax gives a subset of my students quite a lot of trouble. Having lots of example code to refer to helps a bit here.
[I did a similar thing some time ago, it’s called `hackage-graph`.](https://github.com/quchen/hackage-graph) It was a very short-lived weekend hack though, so I’m not sure about whether it still compiles. It’s also not very efficient, but I think it’s faster than 45 minutes – probably because it ignores packages with 2 or less dependencies. Since it predates `acme-everything`, a filter of less-than-everything dependencies would also be a good additional feature.
You should tell them how to read type errors. GHC gives much longer type errors and if one ignores it thinking 'that's too long, almost as long as a C++ template error' (s)he will miss too much.
This is a great idea!!
Great point! I will remember that.
Nice! It's nice to have a reference. Now Hackage has around 10000 packages, which may explain the runtime difference too. I'll filter `acme-everything` soon.
Come on people, downvote is not disagree. This is an opinion clearly based on real personal experience, and it's presented respectfully. It certainly doesn't detract from the conversation, and could very well enrich it because there absolutely **is** a lingering perception of Haskell as impractical. Maybe debunking that for students is a good idea.
I don't know, I'd ask politely the Hackage trustees to delete it ? Though I know it sounds draconian ..
Fine, no need to get worked up. "My tool" (whatever that means) is http://packdeps.haskellers.com/reverse , and it should be made to filter out bogus results at the very least.
Sorry, I wasn't getting worked up. By "tool" I meant anything that counts reverse dependencies, or the scripts that have been posted in this thread. 
The Google search trend charts for event sourcing and (event sourcing + cqrs) are enlightening. I credit Greg Young with the current popularity of ES for this reason :-)
I learned about the naming from Greg Young -- not sure where I got the original link to his "talk" -- I think that was ~4 years ago. We'd been discussing something *very* similar at my work place because it seemed like such an obvious thing to do, especially if you have to keep an audit trail *anyway*. Apparently, it just needs a name for it to become a hype-thing :).
I couldn't agree more. I spent hours reading about monads trying to "understand" them completely. It wasn't until I really started programming in Haskell and using and writing type classes that I had the realization: *Oh, a monad is just a typeclass*. It sounds silly, but boiling it down to being just a typeclass and nothing more and nothing less really helped me get past the weird monad understandable bubble and let me focus on using them rather than studying them. There is a lot more hype around monads than other type classes and for good reason, but as a beginner they feel like some category theorists' inside joke and there is no way to understand them without a deep understanding of category theory.
Yesterday I learned `-XExtendedDefaultRules`.
The fact that you can go from `String` to `IO String` but not the other direction reminds me of a certain game, which used a scripting language (Lua) to script the user interface. The game developers bolted on a “tainting” mechanic to the scripting engine to prevent users from completely automating the game. Essentially, it adds a hidden marker to every value in the language that characterizes whether it originates from a trusted script (“untainted”) or from an untrusted user script (“tainted”). Many of the privileged API calls would refuse to execute if the arguments are tainted in any way. It's a one-way street: you can taint a value but can never cleanse it. Similarly, you can make something `IO a`, but you can never undo that and get `a` back (without hacks like `unsafe*`).
`(=&lt;&lt;)` has the benefit of lining up better with the rest of the hierarchy (&lt;$&gt;) :: (a -&gt; b) -&gt; f a -&gt; f b (&lt;*&gt;) :: f (a -&gt; b) -&gt; f a -&gt; f b (=&lt;&lt;) :: (a -&gt; m b) -&gt; m a -&gt; m b And if you convert that to curried notation: (&lt;$&gt;) :: (a -&gt; b) -&gt; (f a -&gt; f b) (&lt;*&gt;) :: f (a -&gt; b) -&gt; (f a -&gt; f b) (=&lt;&lt;) :: (a -&gt; m b) -&gt; (m a -&gt; m b) It helps it become more obvious that monads are just functors that also lift functions of the form `a -&gt; m b`
Maybe I'm missing something, but the decoder only lets you write to a wav file? Surely writing a file to disk isn't the complete decoding API that the C library provides?
Potentially unpopular opinion: Haskell's alignment sensitive layout while pretty is hard to get right as a beginner. I suggest starting with curly-brace semi-colon notation. Also watch out for the indented `then`: if ... then ... else ... as opposed to: if ... then ... else ... Curly braces don't help here. Also most initial incomprehensible errors are caused by the dreaded Monomorphism restriction. 
His original tweet didn't mention Herbert at all. It seems to me that Herbert inserted himself into this situation, assuming that it was about him. In fact, Michael's [most recent blog post](http://www.snoyman.com/blog/2017/01/conflicting-module-names) gives many examples of packages with overlapping module names. 
That's fine, I'm not interested in who started it. It's more about how the discussion ends up playing out. Herbert "inserted himself into this situation" with &gt; @snoyberg @shebang u may have missed that cryptohash was deprecated by its author; I dont consider that a maintained nor future-proof pkg... to which Michael replies &gt; @hvrgnu @shebang see, you complain when I tell you to grow up, and then you say half truths like this. This is why I've been ignoring you. and the flames grow ever higher. What a pleasant guy.
Don't use `return`. Use `pure` instead. `return` is badly named, especially if you come from another language. It just causes confusion. 
[removed]
I zoomed in and found a node named `5077525020280385846`
As a new comer from another language, I got a lot of confusions based conceptions from other languages. Unlike Clojure, Haskell does not try to explain the difference from other languages in details and demonstrate how wrong they are. That was my feeling when I switched to learn Clojure from Haskell.
- the syntax of a `data` declaration can be very confusing. Emphasize which capital letter thingiess live in the *type* world and which live in *term* land - just ignore list comprehensions; I practically only ever see them in beginner SO questions Thinking about popular stuff I see from beginners on SO all the time: - clarity on what is actually *syntax* (see questions like "help me understand the craaaazy syntax in `foo $^^ bar ..**.. blort`") - explain that if they want to know how or why a function from `base` does what it does that they can look at the haskell source; people seem to assume that it's all written in C or something - explain at least "Could not match expected type..." errors, and convince them that if you actually read the error message and think about it that it will help you - lists are not arrays But most of the first several months of my "learning haskell" was actually learning how to write algorithms using recursion.
I mention it elsewhere in this ~~threat~~ (**edit**: *thread* not *threat*), but &gt;&gt;&gt; :set -XTypeApplications &gt;&gt;&gt; :t fmap @[] fmap @[] :: (a -&gt; b) -&gt; [a] -&gt; [b] &gt;&gt;&gt; :t fmap @Maybe fmap @Maybe :: (a -&gt; b) -&gt; Maybe a -&gt; Maybe b &gt;&gt;&gt; :t fmap @IO fmap @IO :: (a -&gt; b) -&gt; IO a -&gt; IO b 
Yeah, the nodes are identified by their hashes, so I suppose it's a package that is in the dependency list of some package but not in Hackage, I don't know if this is possible?
Makes sense. I guess I'm just saying that you're describing the library as having "complete" bindings, but the decoder module only seems to cover the most trivial use case. I'd personally just suggest using different wording in lieu of providing access to the full breadth of libFLAC.
Idris and Agda are both very similar to Haskell, and I know Agda has a good Haskell ffi. They would allow for very powerful specifications.
Are these examples online somewhere? I'd love to be able to point people at them.
FYI, that thing after "something equivalent" is called a dependent sum or dependent pair: the type of the snd projection depends upon the value of the fst projection. (The "sum" label may be confusing because (,) is a product, of course, but it's in keeping with "discriminated union" being a sum.)
How do I hit a breakpoint and how do I write unit tests 
But that's expected, isn't it. Otherwise you could never define a new constant since `let a = 42` could always be followed by `in a+1`.
Seems like it's about to explode as a supernova.
Absolutely this, although I think it may be easier for students to get over that hurdle than for people with years of industry experience. I wanted to give up numerous times because it was so different compared to what I had done for the previous decade, but after stepping back and unlearning/learning, in hindsight it was all valuable indeed.
I know some of you dislike this and yes, yesod and conduit might not be my favourites either, but I'm rather pleased someone is proactively contributing systems for reliable &amp; reproducible builds. It's kind of annoying to have pure functions that don't build next time, or worse: build with unexpected runtime behavior. And yes, stack may not be the best way to solve the problems, but it came along to solve problems that existed for far too long.
[Something like this?](http://imgur.com/a/qiHJZ)
I cut `return` out of my custom prelude so a colleague I am introducing Haskell to doesn't get tripped up by it. I did have an odd situation recently when showing someone more experienced than myself some of my code. They were confused as to why I was using `pure` inside monadic code instead of `return`, even though they understood that Applicative is now a superclass of Monad and `return` and `pure` are now just synonyms. This in turn has confused me. Was it just their personal idiom?
This sounds like a good idea in terms of usability. It would be interesting to see data on such conflicts, and, in particular, on how many of them involve more than one package with significant usage -- hopefully few! 
Yup -- I prefer `:{` to `+m`, as it seems to hit less corner cases.
Complete beginners in Haskell, functional programming, programming in general...?
Complete beginners in both Haskell and functional programing. Mostly little programming experience.
It looks like banning a perfectly valid usecase because of the reasons, that have solutions nowadays. Imposing of the non-generally accepted practices in such a way looks like bad idea. The same would happen if hackage will decide to ban packages without upper bounds on dependencies, but they do not do it, though missing bounds considered bad practice by hackage team. Valid cases are `-compat` libraries, drop-in replacements for unmaintained packages, drop-in replacements for packages with unacceptable (for some users) design choices; test libraries, where you want to substitute mock or guided functions. Just different packages that have modules in the same field. Such solution should require globals synchrozation with first one wins behavior, and backwards compatibility breakage for the packages that were not the first. This can work only in dns-like module naming that is not used in Haskell.
Aww, was hoping for Tron.
This comes down to whether module names should encode *provenance*, or whether they should encode *function*. I'm glad that as an author I don't need to import `Data.CoolVector`, or instead `Data.CoolerVector`, or rewrite all my imports everywhere just because `Data.CoolerVector` is the new in-fashion thing, or because cool-vectors changed its API and I have to stick to cool-vectors-compat. I just import `Data.Vector`, `Data.Map`, `Data.Set` etc because vectors, maps and sets is what I want. I just need to worry about *function*. And I get my package metadata to explain where vectors, maps and sets come from, i.e. the *provenance*. But arguably, we don't have great tooling to make that work seamlessly yet. Ideally, it would be easy to rename in my package metadata any modules that I import that happen to conflict in the rare cases where conflicts arise. It's my understanding that GHC 8.2 will have some support to do exactly that.
Or collapse into a black hole. I'm not sure which is worse.
This is true, but I've yet to come across one in which flow control by exceptions was a good idea. They behave like non-local gotos, they compose badly and they break code that should work. Bottom line: Put the output of your function into the return type. It's as good a rule in Java. Personal opinion: if Haskell didn't have exceptions, it wouldn't have partial functions either and the world would be a nicer place.
AIUI, GHC supports parallel GC, which would be a bit pointless if it was stop-the-world. You can see a full list of options for tuning it [here](https://downloads.haskell.org/~ghc/7.4.2/docs/html/users_guide/runtime-control.html#rts-options-gc).
As a guy who doesn't really haskell good, can you explain that please?
I'd go with a graph database, they even have a built in query language for example you could try neo4j and upload the graphdata via rest and query it in their nice looking web gui and then your query would be(just guessing, haven't used it for some time now): MATCH (p:Package)-[u:Uses]-(n:Package) where n not in [&lt;list of some packages&gt;] RETURN p 
Acme-everything is the centre of the black hole, binding everything else to it...
What are the penalties for violating the code of conduct? Public shaming? Exclusion? That doesn't sit well with me and seems to be the very kind of behavior a code or conduct exists to prevent.
This is honestly getting ridiculous. Just block each other on twitter, seriously. Arguing like pouty children on public social media only serves to embarrass most of those involved. And snoyman honestly needs to tone it down -- it's getting really hard to see his point of view when he acts like that.
For Stackage you can check the hidden section in the build constraints: &lt;https://github.com/fpco/stackage/blob/2b4e44046b26d031aad539a013a92705df2058fa/build-constraints.yaml#L3337&gt;.
For convenience, here is the part of the Rust CoC which seems relevant to the linked discussions: &gt; * Please be kind and courteous. There’s no need to be mean or rude. &gt; * Respect that people have differences of opinion and that every design or implementation choice carries a trade-off and numerous costs. There is seldom a right answer. The /r/haskell sidebar also has a link to "Community Guidelines", but obviously that only applies to this subreddit, not to Twitter. Besides, the forbidden behaviours in those guidelines are a lot more vague than in the Rust CoC. The two most relevant rules are: &gt; * (for mods) when in doubt, leave it in. &gt; * (for users) Follow jfredett's patented "Don't be an idiot" rule: if you're being an idiot, stop.
I'd say that isn't really provenance. It is however perhaps a little weird and redundant.
May I suggest using SML instead? I think it's much easier for beginners: * No laziness * No monads * Less reliance on currying * Fewer indentation rules * Fewer unusual operators
Have you read the CoC? It's very short, and what happens upon violation is described in the second part.
Something like this on the Hackage pages would be very useful. The package categories are useless for browsing. Things like this graph help show what is actually being used. Unmaintained, broken stuff (which take up over half the Hackage listings) don't have reverse depencies (at least, not currently maintained reverse dependencies). I think Hackage needs some sort of "quality index", which would consider package age (older is better), number of reverse dependencies, amount of Haddock documentation, and whether it compiles on a recent GHC.
[removed]
Btw, I have a nice `show . read` example in my lecture slides using `-XTypeApplications` to solve problems :) http://slides.com/fp-ctd/lecture-15#/16
No advice, but good luck!
&gt; And snoyman honestly needs to tone it down -- it's getting really hard to see his point of view when he acts like that. Snoyman is at the center of a lot of important Haskell infrastructure, so there's a lot of weight on him to make the right decisions when it comes to these things; even for trivial matters like unique module names. I think the problem is that he can often be [accusatory](https://twitter.com/sclv/status/816360122691751936) and treats discussion as if he's under enemy fire, when we really just all need to stay on the same side. Ordinarily, this sort of thing wouldn't often be a problem for most people. But when you add the weight of his role in the community, there's a lot of opportunity for those aspects of him to show their worst. I would really appreciate it if he would avoid all this hostile language, but I respect the weight he must feel during discussions like these.
Thank you. This shows some of the potentially trickier cases. One example would be `zip` versus `zip-arhcive`: the much newer `zip` package is arguably the best option for most users (and I hazard the authors of both packages wouuld agree with that); `zip-archive`, however, has [quite a few reverse dependencies](http://packdeps.haskellers.com/reverse/zip-archive) because for a long time there was no unambigously better alternative.
Yep. I think it could be improved a little. Like why is it Data.Functor? I think having Data be for general Data types would work ok. And then another category for general type classes. And then a bunch of categories for more specific data and functions and classes. In the end it may end up being better to just drop it and make things like Maybe and Functor and Vector not in a namespace. And then only really take advantage of the namespacing for modules within or to extend a project. So Vector.Unboxed or Yesod.SomeAdditionalFeature.
This is great! C bindings are such a benefit to the community. Be warned, I have some opinions about writing bindings that I am about to share: I think any C binding should be split between a package that is just the bare minimum needed to make an FFI binding possible, and any packages that implement higher level wrappers on top of that. There is hardly ever one wrapper that satisfies everyone's needs and this allows multiple approaches without requiring new packages to fork or reduplicate the FFI work of the original binding. Generally when I write a binding I write: 1) FFI binding 2) Minimal wrappers (e.g. `withFoo`) that handle lib state initialization, memory allocation, deallocation, etc. 3) A high level interface To me, (1) and (2) can be part of the basic package and (3) should be a separate one. It is my opinion that all hackage packages should expose their internal modules. Marking them Internal is already enough warning that A) their interface won't be maintained with any consistency and B) using them is at the user's own peril. Too often have I had to fork a package because I needed to access its guts for whatever reason in my application code and the package deliberately hid these modules. Exceptions are the devil. It is ok of course if a particular high level binding decides to use exceptions as part of its interface (I would never want to use it). However it is a design decision that forces user code to be structured around it. IO exceptions are tolerable but exceptions that can occur in pure values are most definitely a flaw in the language.
I don't think that is the case. If I understand it correctly, "the Stackage build process" refers to what Stackage does server-side to ensure compatibility between its packages, and not to anything that you can do with Stack. That being so, if you e.g. create your brand new package *A* in your computer, it won't be hidden anywhere, and so nothing will change. The hiding would only becomes an issue if you got *A* into Stackage, at which point either *A* or the package it clashes with would be hidden by Stackage, and the sort of override I referred to would become necessary.
Hooray, HWN is back! (Not sure how gone it ever was or how many there were, but I didn’t see one for a few years.)
Although they are similar, this is not a continuation of the "official" [Haskell Weekly News](https://wiki.haskell.org/Haskell_Weekly_News). I think u/dstcruz published the last issue of that a couple years ago. 
While I don't disagree that he is one of the most important members of the community I feel like every time he posts a blog it sparks intense controversy. I see his name and think to myself "Here we go again."
Perhaps this is yet another reason for `ByteString`s to be un-pinned, as /u/dcoutts has suggested repeatedly in the past. Under the status quo `ShortByteString`s should be fine, however.
Good on you for the considerate name change!
Yes, it’s pure. Interacting with Racket will require going through a (very lightweight) FFI that will put everything in `IO` by default, since it’s impossible to statically determine if a Racket function is pure or not. There are ways to subvert the type system if you want, but no more easily than using `unsafeCoerce` or `unsafePerformIO`. The current implementation is strict, but I think I would like to make it lazy by default, though I haven’t decided on that yet for certain.
I prefer linking to primary sources. Instead of linking to the place that I found something, like Reddit or Planet Haskell, I'll link to the thing itself. I also prefer avoiding links to discussions. I will link to Reddit if it's a self post, but otherwise I assume readers can figure out where to discuss something. I can see how that makes it look like I avoid covering r/haskell, but I cover the content on this subreddit rather than the subreddit itself. I don't see the value in linking to Reddit discussions. If you want that, you can bookmark the [top posts of the week](https://www.reddit.com/r/haskell/top/?sort=top&amp;t=week). In my mind, Haskell Weekly's value add is that it's more than just an aggregate. It's a filtered, curated selection of stuff. 
Much like Rackett makes macros far easier, you can abuse GHC to practically make Haskell implement your type system for you. I'm not sure whether or not that would be a good practice though.
I knew it ephemerally, but didn't really believe it right away. Focus on the types. Almost entirely. Practically everything is simpler than it otherwise seems. I mean, I wish I knew everything I know now and probably a lot of things I don't even know yet, but I think that was the real turning point in my learning of the Haskell.
Is it easier to write a macro which rewrites strict code into lazy code or vice versa?
&gt; I would really appreciate it if he would avoid all this hostile language Well, maybe [he genuinely isn't aware of the semantics of his words](https://twitter.com/snoyberg/status/816957007982948353), but in that case some code of conduct may help to serve him and others as guidance. I think we clearly agree that we value his contributions but not so much his behavior. The idea is to be able to course-correct interactions when they start going sideways by making everyone involved aware of it it in the hopes they'll get back on track.
I would say that the Holy Trinity of map, fold and filter replace 99% of all for- loops. Lists are just one data type to apply the trinity to.
I still don't get why Haskell needs newtype. Why can't every data type with a single constructor and field automatically be interpreted as a newtype?
The *abstract* for that paper is above my pay grade, I don't even want to open the full thing.
We're going to abstract away actual source code, so you just sort of grunt at the compiler and it gives you what you might want.
What's with this community thing ? Are you a hippie ?
It's not unfriendly to say the truth to someone disrespecting your work..
Template haskell splices also compose if I understand what you mean by compose. I'll concede that macros in racket are much simpler though.
That just raises the question why the compiler doesn't turn any *strict* data into a newtype. Is strictness really the only issue here?
I agree with all that. For those who missed the previous heated discussion that kind of went off topic: https://www.reddit.com/r/haskell/comments/5lxv75/psa_please_use_unique_module_names_when_uploading/
Strictness really is the only issue - and there's no way to get data to have the same strictness properties. The linked answer only covers half the story. Pattern matching on a data constructor forces evaluation. Always. Pattern matching on a newtype constructor never forces evaluation. Now, you can use an irrefutable pattern to make any individual match on a data constructor act the same way as a newtype constructor match. But that's is control at the site of an individual pattern match. Using newtype sets the default behavior.
Template Haskell splices do compose, yes, but they’re not *syntactic abstractions*, as mentioned by the previous blog post. One response to that was “are quasiquoters syntactic abstractions?” and the answer was “in some sense yes, but they don’t compose”. So I guess you could say Template Haskell gives you both compositionality and syntactic abstractions, but never both at the same time. Unfortunately, it turns out that some real expressive power can only be achieved by using both at once.
I'd say something about performance model. At first I had an impression that Haskell was a declarative language, and I should just focus on writing nice looking code because GHC will take care of optimizations. It will show them at least where GHC is currently suitable, and where care needs to be taken. This is what really bugged me for a long time.
It would be nice for these to be posted to haskell-cafe like the old ones were. People appreciated that I think. :-)
What was inconsiderate about the old name?
There was another project with the same name.
Because people might be already constrained by other codes. Or flat out not want to spend time checking the guidelines of some weird kiddo. Or think the idea stupid enough for them to be repelled. Or it could be illegal to privately limit things which are of exclusive domain of their state. I am sure there other reasons though
Newtype *guarantees* that there is no overhead for using it, and that it is operationally equivalent to the thing it wraps, except for being a new type. It's a constraint. If it was simply something the compiler offered, it would be a convention, and *not something you can enforce*.
Things would have been so much simpler if only the language was strict :(
I think `haskell-gi` should be the default for new projects (it is awesome). There are [haskell-gi ports of some Gtk2Hs examples](https://github.com/haskell-gi/gi-gtk-examples) that might be useful. You should be aware though that the overloaded names feature in `haskell-gi` taxes GHC's type system to the point where compile times and ghci `:reload` times are very badly impacted (the overhead applies to anything that imports a `haskell-gi` module). As you are planning to "start a project with a rich GUI in Gtk" I would strongly recommend using the flags to turn overloading off. If you are using `cabal new-build` your `cabal.project` should look like [this](https://github.com/leksah/leksah/blob/master/cabal.project). If you are using `stack` your `stack.yaml` file should look like [this](https://github.com/leksah/leksah/blob/master/stack.yaml#L34). 
I guess you've never been to new york? :-) http://web.mta.info/nyct/service/CourtesyCounts.htm
 data Foo = Foo Int data Bar = Bar !Int newtype Baz = Baz Int case Foo undefined of Foo _ -&gt; ... -- OK case Bar undefined of Bar _ -&gt; ... -- error: undefined case Baz undefined of Baz _ -&gt; ... -- OK Hopefully this demonstrates the difference in strictness between a `newtype` and a strict `data`. Basically, pattern matching on the constructor of a `newtype` behaves somewhat lazily.
If you're porting it to Haskell, you should probably use Haskell :-P
I've wondered lately whether default strict fields in data in a lazy language might be the sweet spot (at least under a compiler that behaves like GHC).
&gt; rewriting strict code to lazy code or vice versa doesn’t make a ton of sense given that the source code is pretty much identical. I agree that rewriting an expression in a lazy language, for example `mapPair f (x,y) = (f x, f y)`, into a syntactically-equivalent expression in a strict language, for example `const mapPair = (f, [x, y]) =&gt; [f(x), f(y)];` (that's JavaScript ES6), does not make much sense. What I meant was to rewrite the lazy expression into a *semantically* equivalent expression in a strict language, so something like this: const mapPair = (lazy_f, lazy_pair) =&gt; { let x = "thunk"; const lazy_x = () =&gt; { if (x === "thunk") { const pair = lazy_pair(); const lazy_x = pair[0]; x = lazy_x(); } return x; }; let y = "thunk"; const lazy_y = () =&gt; { if (y === "thunk") { const pair = lazy_pair(); const lazy_y = pair[1]; y = lazy_y(); } return y; }; return () =&gt; [lazy_x, lazy_y]; }; Or, going in the other direction, converting `const mapPair = (f, [x, y]) =&gt; [f(x), f(y)];` into something like this: mapPair :: (Int -&gt; Int) -&gt; (Int, Int) -&gt; (Int, Int) mapPair !f (!x,!y) = let !x' = f $! x !y' = f $! y in (x', y') Here I am using Haskell and JavaScript in order to make the semantics clear, but the same kind of transformations could be used to transform an expression in hackett/strict into a semantically-equivalent (but syntactically very different) expression in hackett/lazy, and vice versa. If one direction is easier to implement, or if one of the two target languages is easier to implement, then it would be one tie-breaking factor which could help you decide whether to make the base language lazy or strict. &gt; I’ve started to feel like maybe I just need to pick pervasive laziness or pervasive strictness and mostly stick to that decision. I agree. There's a third choice though: [strictness polymorphism](http://h2.jaguarpaw.co.uk/posts/strictness-in-types/)! That is, function application could be a type class method `($?)` with two instances, `($)` for lazy evaluation and `($!)` for strict evaluation. Writing `(+) $? x $? y` instead of `x + y` gets tiresome fast though, so I haven't played much with the idea, and so I don't yet know whether strictness polymorphism brings together the best or the worst of both worlds. For this reason, I am not seriously suggesting that Hackett should use this, I'm just pointing out that the possibility exists. Now that you got me thinking about macros though, a macro would be a great way to automatically turn `x + y` into `(+) $? x $? y`, thereby making it easier to experiment with strictness polymorphism!
The [Haskell-cafe info page](https://mail.haskell.org/mailman/listinfo/haskell-cafe) says: &gt; To post a message to all the list members, send email to haskell-cafe@haskell.org. Is that all it takes? If so, I can add haskell-cafe as a subscriber. 
Could you please clear up a few things for me. &gt; Pattern matching on a data constructor forces evaluation It only forces evaluation to WHNF right? &gt; you can use an irrefutable pattern By irrefutable pattern, you mean a wildcard like _?
From what I've read, the decision of making Haskell lazy made the language extremely pure. I think the pureness of Haskell is well worth the costs.
GObject introspection isn't always available and is more of an addon feature in GTK. It may very well make your life easier but it's in a similar space addon space as QML being a higher level abstraction on top of Qt. If I had to choose today, I would go with Qtah since Qt is more predictable and regresses less than GTK3.
That is not surprising, given that a fold is equivalent in power to a foreach loop. You can express any foreach loop as a fold. The only difference between a Python for loop and a fold is that the changing part of the state is made explicit in the fold, i.e. you have to say "in each iteration, these variables are subject to change." 
Just remember that int i; char *str; should be read as "an expression `i` is of type `int` and an expression `*str` is of type `char`" and it makes much more sense. I wish more literature brought that up. 
Why do you miss it? I think there's a GHC extension for overloaded record fields, not sure what version it's in/will be in.
Don't we have this now?
you mean `StrictData`? that's what I meant
&gt; Paredit Any thoughts on [smartparens](https://github.com/Fuco1/smartparens) or [parinfer](https://shaunlebron.github.io/parinfer/) &gt; I think my next go-to language will have an s-expression syntax. I'm actually really hopeful/excited for [sweet expressions](http://readable.sourceforge.net/), which seem to me to capture most of the benefits of non-sexp syntax without paying too much for it.
FLTK is actively developed and supported. The Haskell binding has a nicer API, IMO compared to GTK and WxWidgets. Qt is much larger and runs on many more devices but has license restrictions for commercial development. On the Haskell side Qtah and HSQml look great. I hope to see the former especially become successful. Both Qt and FLTK run on multiple platforms but, IME, FLTK apps are *way* smaller and easier to compile/deploy.
[Typeclasses are still extremely important beyond the purpose of overloading names.](https://www.reddit.com/r/haskell/comments/5k3zgo/what_is_the_current_state_of_javascript_problem/dblv19a/)
Re the first point, as a beginner, I preferred seeing and using different identifiers: data Person = MkPerson {...}
I think the thing that scares me about (adhoc) function overloading is that I can't tell from the call-site what a function is going to do since it can always be overloaded. Save for maybe functional dependencies, I can at least know that the shape of the result is going to be the same, or that the transformation of arguments is going to be similar when using type class methods. Using your examples, I'd hate to know that name could return a different type depending on the argument (which is of some other product type). Unless I'm missing something, I'd have to look up the signature every time I wanted to use either of those functions.
You still need type classes to express e.g. "this function takes any argument that supports the toBinary operation." In other words, plain overloading lets you do one level of overloading, while tyoeclasses let you nest overloaded definitions arbitrarily deep and it Just Works. 
&gt;I use it all the time when writing production software. It basically gives you a lot of the benefits of type classes without having to actually use type classes. I guess I'm not clear on what the benefits are. Can you enumerate them? Sorry if it's a dumb question, I've just never really thought of it as something that was missing in Haskell before.
Just learned about the Neo4j bindings: https://hackage.haskell.org/package/hasbolt
some difficulty installing `haskell-gi` with stack
When I first switched to Haskell from C++, I missed overloading for a few months. Now I like the fact that it's fairly easy to know what's being called, and where the definition of a function lives, just by reading code. I don't have to keep in mind innumerable rules governing symbol lookup and value conversion, that sometimes only single-stepping in the debugger can fully unveil.
There is DuplicateRecordFields and the proposed OverloadedRecordFields language extensions, see https://ghc.haskell.org/trac/ghc/wiki/Records/OverloadedRecordFields. 
I wonder how [Idris' Elaboration](http://docs.idris-lang.org/en/latest/reference/elaborator-reflection.html) works out compared to Hackett's mechanism. Does it even compare?
The `tagged` library does help with this problem tremendously; it at least allows you to have these functions universal across all `Tagged` values. `GeneralizedNewTypeDeriving` works for most things as well, so actually for things that are accessed by typeclasses this works well enough; I haven't ever used `ClassyPerlude` but it seems to me that this example would actually work. 
Indeed, I deliberately stripped out the inflammatory parts from the title as I hoped to see a discussion about the (non)issue of module names uniqueness. So far I got the impression it's not a big deal after all. 
We'll get them eventually. This is what [Concepts](https://en.wikipedia.org/wiki/Concepts_(C%2B%2B\)) are. 
Latency? Websocket by design needs to receive the whole packet before it starts processing it. There are no primitives to send uncompressed messages (yet). But unless you do not send random stream you can send much more data quicker. In my case (mainly json) 10x, multiple frames arrive at the same time.
You are a good writer.
The beauty of using s-expressions for a language is that you automatically get all of these alternative representations for free: programmers just need to add the relevant foo-&gt;s-expression preprocessor to their Makefile, or write a bash alias, or use an alternative shebang, or whatever. In principle, the same thing can also be applied to output like error messages, etc. if they were represented structurally. The language itself only needs to support one representation, although multiple *could* be included, e.g. via a flag; it just-so-happens that 's-expressions with parentheses' is the most common choice, so there's no real need to change it. It's also easy enough to create a custom, *non-general* syntax (i.e. something less general than s-expressions/sweet expressions/etc.) that parses into the s-expression representation of some particular s-expression language. In general you can't use an off-the-shelf syntax like, say, Python, because its s-expression representation would have different tokens and structure than that expected by the compiler, and trying to fix it up by inspecting tokens breaks the "reading without parsing" idea. But it's easy enough to make syntaxes which are "Python-like", "C-like", etc. which *do* parse into correct programs; especially if the s-expression language lets you transform the results using macros. I think the only reason this isn't done more is that s-expressions and Lisps are too tightly coupled in programmers' minds. Sweet expressions are a nice reminder that Lisp doesn't imply s-expressions, but I think it's even more important that s-expressions don't imply Lisp; you can make an s-expression language without macros, without eval, with whatever approach to syntax, typing, etc. you like since it's just a representation which can be used for ASTs. As far as I'm aware, Lispers must stick to a general, s-expression-equivalent syntax, since more specific syntaxes can't represent all of the structures which a Lisper may want to implement with macros (I may be wrong, but I've not seen any syntax which *can* represent all Lisp programs, but *isn't* equivalent to s-expressions). Non-Lispers who don't need such generality *do* create specific, non-general surface syntaxes, like Haskell, Go, Dart, etc. Yet, to a first approximation, they *don't* particularly care about s-expressions. Rather than treating their surface syntax as a human-friendly representation of some underlying, canonical s-expression-like parse tree, instead the human-readable syntax is treated as the canonical form and things like parse trees are treated as machine-friendly representations. Unfortunately this can lead to a lot of duplicate work, as each tool (e.g. GHC, Hugs, HLint, Haddock, hindent, etc.) operate directly on the human syntax, each accepting slight variants of the language, with language updates breaking old tools, ambiguous parses depending on language version, unnecessary fragility (e.g. parsing the whole language just to find calls matching `map _ (map _ _)`), etc. with the lowest common denominator being the compiler, i.e. if the compiler can't parse it then the code's wrong, if some other tool can't parse it then that tool's wrong. Whilst I accept that many people don't want to write code directly as s-expressions, I do think it would be an improvement if the first step in such language's 'build pipeline' translates into some s-expression format (or equivalent, but might as well standardise), in such a way that it can be manipulated arbitrarily by intermediate steps (i.e. fragments of optimised AST scattered around some compiled binary's memory layout don't count!)
It would certainly be useful, but you would have to have more than just the Hackage index locally in order to have it, you must have the source of the previous versions in order to produce it, so it seems it's more than just a Haddock thing.
A git blame would be necessary, yes.
Those are demo apps aimed at learning the API. I'm working on some pretty demos. Theming is not available (but will be in the next stable release).
With Java/C++ style overloading I'd be able to write both these calls and have them resolve to the right definition: f [1,2,3] ["hello","world"] 1 f 1 Currently if you need something like this typeclass-hackery is the only way to go. I admit in Haskell this is kind of overloading is problematic. Not sure what currying has to do with this.
&gt; it would be really nice to integrate types into linting, which should be possible if types are available for macros to inspect. Would it help if there were tools which could, for example, querying types like "given this Haskell code, what is the type of this expression if it were to appear at this point?". It's certainly something I'd find useful, as I've previously had to rely on GHCi hacks similar to those used by IDE hooks. At the moment I think tools like GHC are too monolithic for this sort of thing, since they only really provide one workflow: take environment from Cabal/Stack -&gt; read string of code -&gt; parse -&gt; desugar -&gt; rename -&gt; typecheck -&gt; translate to core -&gt; ...
I think this could be done with this: https://github.com/blitzcode/hackage-diff
The documentation is uploaded manually: http://hackage.haskell.org/package/gi-gtk-3.0.11/docs/
Is this actually discoverable from https://hackage.haskell.org/package/gi-gtk?
I understand how function application forms a closure if not fully saturated. I still don't see the connection to overloading in the Java/C++ sense. TMK until GHC 8 TC's were the only overloading mechanism available.
[removed]
But wouldn't function overloading make type inference impossible, since there is no unique type for the compiler to infer? I think, it might be a nice language extension, but I for one, don't want to give up type inference for this. PS: I might be wrong in assuming it undoes type inference. If so, please correct me.
[removed]
[removed]
I think that counts as a "GHCi hack similar to those used by IDE hooks", and presumably requires a bunch of setup to ensure GHCi can find the right modules and packages, i.e. more than just "given this Haskell code" (where "this Haskell code" might be more than just a string of Haskell, e.g. it might specify a bunch of packages, modules, local files, git repos, etc. to provide the context)
Not really, but you would need to download all the published versions of the package from hackage.
It is a rule of GHCi that when the expression you enter has an I/O action as its value, instead of trying to print the value, it performs the I/O action and then prints the result. Your expression has the type `(Monad m) =&gt; m (Maybe ())`, and GHCi defaults the `Monad` constraint to `IO`. So it gets an I/O action, performs it, and prints the *resulting* value of `Nothing`. 
Does your introductory text have a section on the "simply-typed lambda calculus"? Haskell is a typed language, so a typed lambda calculus is a better model of Haskell. If you want a programming language with which you can play with the untyped lambda calculus, try Scheme, [Caramel](https://github.com/MaiaVictor/caramel), or even JavaScript.
Thanks for this, I actually figured there are no docs and I'll just try to get by with Gtk2Hs and Gtk docs... 
It's because the type of `x` in `\x -&gt; x x` is infinite. Start with the assumption that x is polymorphic as possible `x :: a`. Then note the term `x x` so x must be a function, `x :: b -&gt; c`, and since its argument is of type a, `b ~ a` (`~` is used to denote equality for types) so we have `x :: a -&gt; c` and `a ~ a -&gt; c`. We could go even further, but we've already run into a recursively defined type, which isn't allowed. We can get around this by using a `newtype`: &gt;&gt;&gt; newtype Mu a = Mu { runMu :: Mu a -&gt; a } This has the recursive properties we desire, using `Mu`/`runMu` to define an isomorphism between `Mu a -&gt; a` and `Mu a` rather than using type equality (`~`). Then consider the expression &gt;&gt;&gt; :t runMu (Mu $ \x -&gt; runMu x x) (Mu $ \x -&gt; runMu x x) runMu (Mu $ \x -&gt; runMu x x) (Mu $ \x -&gt; runMu x x) :: a
Implicit laziness isn't a safe optimization as it can change termination behavior. Like in my specific example if x is divergent but the cases that involve it aren't reached then your optimization changing termination. Also if x is cheap then this "optimization" will slow things down. Turing completeness is going to make the perfect compiler you have imagined a little hard or actually make. Same with the whole "if only part of your list is used the compiler can detect it", it takes almost nothing to make such analysis impossible or impractical to do at compile time. Dude you seriously need to actually try the strict version I gave you. I promise you that even when you use the whole list that unless you get lucky and hit one of the limited cases where stream fusion saves you, that you will get substantially more transient memory usage and worse performance. Just TRY IT. Also please remember that laziness is fully general and stream fusion is not. It will sometimes kick in and help a lot, but you cannot assume that it will kick in. zipWithIndex does not have a valid type signature. You need to make f at least a traversable. And yeah sure you can put explicit laziness absolutely everywhere to recover from this ridiculous decision, but it would really need to be everywhere. Where's explicit strictness is usually just in the form of a few foldl' calls and a few bang patterns here and there. As well as using the data structure with the right level of laziness. But go ahead and make your magical sufficiently genius compiler that gets all the benefits of laziness and none of the downsides. Have fun. 
for stack: compiler: ghc-8.0.2 compiler-check: match-exact resolver: lts-7.14 setup-info: ghc: linux64: 8.0.2: url: https://downloads.haskell.org/~ghc/8.0.2/ghc-8.0.2-x86_64-deb8-linux.tar.xz content-length: 114373576 sha1: f298b7d0f37cc9ded7ac66b2662b0fa5ac6d0809 macosx: 8.0.2: url: https://downloads.haskell.org/~ghc/8.0.2/ghc-8.0.2-x86_64-apple-darwin.tar.xz content-length: 113945284 sha1: 5bfd2cef149bd2936cc56bb4b526cbc56a0e7f28 windows64: 8.0.2: url: https://downloads.haskell.org/~ghc/8.0.2/ghc-8.0.2-x86_64-unknown-mingw32.tar.xz content-length: 156817208 sha1: 18d8eadb52e86583c14019be53910e02dbe84f1d 
As /u/chrisdoner mentions, this isn’t really due to Lisp syntax. Hackett is pervasively curried, and it supports infix operators, so you do still need more parentheses to delimit certain expressions, but the syntax is much more concise than most other Lisps. The way to write `sum . map sqrt . lin` in Hackett would be `{sum . (map sqrt) . lin}`.
Well, mostly psychological, I think: Elm hasn't yet reached 1.0, so every minor version incurs a major version bump of all libraries. Also, I made the 'mistake' of publishing libraries although I'm not really doing any Elm any more, so my only contact with the Elm eco system is the chaotic time between compiler versions, where half of your dependencies are abandoned and the other half hasn't updated yet. It's mostly frustration, I guess. Plus, there are more breaking changes than just those detectable by `elm-package` (which is hardly an argument against their approach). Strange enough, I don't have the same feeling about PureScript's eco system. Then again, I haven't published any PureScript packages yet...
It's funny that you mention PureScript! I feel like its ecosystem moves very quickly. I wrote some packages last year and they fell out of date in a few months. Their prelude is already on version 2.0.0, but I think they're getting closer to a 1.0.0 release of the compiler. That should stabilize things. It is a bit strange that Elm packages must follow SemVer (and therefore be at least a little stable) but the Elm compiler itself doesn't. 
Have fun talking to yourself with that arrogant tone onwards.
What model are you trying to learn? AFIK SGD is a general optimization strategy which can be used to learn a number of models. It looks like this is some sort of regression? Here is a pile of thoughts in no particular order: 1. `[Double]` (which is a linked list) isn't such a great encoding for vectors; perhaps when you feel up to it have a look at the `vector` library, which provides proper unboxed vectors 2. On line 24 you really want to you `foldl'` instead of `foldl`. The difference is one of strictness; `foldl` will build an accumulator consisting of O(n) thunks; `foldl'` will instead force the accumulator on every step through the input list. GHC will compile this to a nice imperative-style loop. 3. For someone new to the language your code is pretty nice; well done! 4. When writing iterative algorithms like optimizers I often find it helpful for the result to be a lazy list of iterates. This way I can decouple my optimizer from my convergence criterion. The `iterate` [function](http://hackage.haskell.org/package/base-4.9.0.0/docs/Prelude.html#v:iterate) can be a helpful building block for this. 5. You should likely shuffle your examples between training rounds to ensure you don't cycle; the `random-shuffle` [package](https://hackage.haskell.org/package/random-shuffle-0.0.4/docs/System-Random-Shuffle.html) might be helpful here. I didn't make it to the end but my build finished, so I'm off! I hope this is helpful, however. I may be back. 
Seems like it's more of a pre-1.0 problem. Let's see how Elm and PureScript fare when they stabilize.
 {-# LANGUAGE MultiParamTypeClasses #-} {-# LANGUAGE FlexibleInstances #-} module Temp where class F w z where f :: w -&gt; z instance F Int (IO ()) where f = print instance F [a] ([b] -&gt; Int -&gt; IO ()) where f _ _ = print then in ghci: λ :l Temp λ f (5 :: Int) :: IO () 5 λ f ([1,2,3] :: [Double]) ([3,4,5] :: [Float]) (7 :: Int) :: IO () 7 
I think the way to go is to use a DrawingArea in Haskell. From the Gtk docs: ["The GtkDrawingArea widget is used for creating custom user interface elements. It’s essentially a blank widget; you can draw on it."](https://developer.gnome.org/gtk3/stable/GtkDrawingArea.html). The [CarSim](https://github.com/haskell-gi/gi-gtk-examples/blob/master/carsim/CarSim.hs) and [inputmethod](https://github.com/haskell-gi/gi-gtk-examples/tree/master/inputmethod) examples both use DrawingArea.
&gt; It wasn't until I really started programming in Haskell and using and writing type classes that I had the realization: Oh, a monad is just a typeclass. Dude! I literally just had this same epiphany yesterday. For some reason I always had the impression that Monads, Functors, Applicatives, etc. were somehow baked into the language, especially all the crazy function syntax (&lt;&gt;, &lt;$&gt;, etc). But really they are just part of the Haskell library and anyone could in theory create their own versions of them. I think this does make it a lot easier to reason about Haskell as a language.
For #2, I’d say anything that maps an unbounded abstraction onto a bounded resource, e.g., by multiplexing or scheduling. GC, virtual memory, and lazy evaluation can simulate unbounded memory atop a finite amount of actual memory. Green threads and dispatchers can simulate an unbounded number of jobs atop a finite number of OS threads, machine cores, or machines. I’m finding it hard to come up with other examples, though. And of course, all of these abstractions will leak under high enough load. 
&gt; Could there be something like Rust's memory model, but analogous to threads? Yes, futures/promises. It is what Rust uses instead of green threads according with https://aturon.github.io/blog/2016/08/11/futures/ The problem of futures-promises is that there is a break in the code. It just tries to put the call and the callback close together so that the code is more readable, but variables from the calling and the callback are not shared (the green thread save and restore the call stack and this is the reason why variable scopes are shared before and after the asynchronous call, while in callbacks, futures and promises don´t). This is not definitively clear in my mind but it think that it is reasonable. 2 blocking file IO is an abstraction like garbage collection. Only that it is an abstraction older in history and it has been programmed right in the OS. Also OS processes are such kinds of abstractions I think that abstracting over the bare metal is possible thanks to these techniques. For example blocking IO and OS proceses allows the programmer to think and program as if all the IO processing and the CPU is running for him alone. Garbage Collection allows the programmer to program as if the computer has infinite memory.
&gt; Could there be something like Rust's memory model, but analogous to threads? That's pretty much of the approach of [ling](https://github.com/np/ling) with linear types. You specify which code blocks are sequential and which are parallel and you get deterministic concurrency generated at compile-time. You could then even use type inference to have the compiler decide what code blocks should be which when unspecified. There are several talks about ling by the author on youtube.
Interested in the topic. To put this into context.. I take it the paper, like the repo, is "current as of ~2010"?
&gt; a result of their semantics and not their syntax I disagree, or at least I don't fully agree. I think it has to do with function call syntax. If `foo` is a curried function of 3 arguments, a call in lispy syntax is `(((foo a) b) c)`, whereas if it's uncurried, it's `(foo a b c)`. I don't think it's coincidence that the only languages in which functions are typically curried are those for which calling curried functions imposes little-to-negative syntactic overhead. 
What about if you write it like: poll us action = action &gt;&gt;= go where go Nothing = do liftBase $ threadDelay us action &gt;&gt;= go go (Just a) = return a The problem is that it probably can't garbage collect `ma`, so the memory used by it just builds up each time it recurses. Using a worker wrapper like this is also probably more efficient because you save passing a function argument each iteration.
Dependent type systems tend to drop type inference, and I don't know of any literature on dependent types with coherent type classes (i.e. a la Haskell rather than a la Scala). I don't think these are fundamental problems, but I think that the fact that GHC exists means that reimplementing features it already has will be easier than implementing DT versions.
That's a great construction. Thanks for taking the time!
You could always use [coerce](https://hackage.haskell.org/package/base-4.9.0.0/docs/Data-Coerce.html#v:coerce) ;)
I was trying to analyze this more fully, but apparently I'm too tired. Yours is different from untilJust because untilJust loops on go as seen is this explicitly type annotated version of untilJust: untilJust :: forall m a. Monad m =&gt; m (Maybe a) -&gt; m a untilJust m = go where go :: m a go = do x &lt;- m case x of Nothing -&gt; go Just x -&gt; return x So it recurses on `m a` whereas yours recurses on `(MonadBaseControl IO m) =&gt; Int -&gt; m (Maybe a) -&gt; m a`. I'm not sure if yours would be tail recursive either, but I know that untilJust is. untilJust also uses a [worker-wrapper](https://wiki.haskell.org/Worker_wrapper) which I've heard is faster, but I don't know why and couldn't figure it out perusing the papers written on it within 5 minutes. Perhaps someone else can elaborate for the both of us ;)
Ah, I forgot about inference. That's certainly worth keeping as a Haskell-like language!
I think things might be a bit easier if you use a matrix representation, like hmatrix, for your data source. If you want to make things even easier you could prototype with the auto differentiation library (https://hackage.haskell.org/package/ad) to get gradients and make sure you're on the right track.
Hey, great stuff. Is it possible to handle partitioned streams with these abstractions (ala GetEventStore)?
Did you mean Prelude&gt; sequence [Nothing] Nothing it :: Maybe [a] ?
[removed]
LambdaCase is a pretty pointless extension. 
Is mtl implemented with return type polymorphism?
Aren't futures equivalent to continuations? In that case I'd think that working with them would be very similar to working with the IO monad, with all the same problems and advantages? Actually, isn't async await not just the same as do notation?
Because the example is so well prepared, I just tried it out and was somewhat surprised that it would not use more than 1 CPU. To make it use 4 CPUs on my system I had to compile it with: stack ghc --resolver lts-7.14 --install-ghc --package conduit-extra -- --make -threaded echo.hs And then run it with: ./echo +RTS -N4 Then you can see that with every new connection and long input it will use more and more CPU.
Not directly, but you can do the following (With `-XExistentialQuantification`): data Numeric = forall a. Num a =&gt; Numeric a Then you can keep numeric types in a list: `[Numeric 42, Numeric 42.0]`. This has no counterpart with C++'s proposed concepts.
C++ has `std::variant` and `std::any` already, so a similar class with a concept restriction would be possible when concepts are actually added to the standard. You could then have `std::vector&lt;std::constrained_any&lt;Number&gt;&gt;` for example.
https://www.stackage.org/nightly-2017-01-07 ?
I was using ATOM but I switched to visual studio code recently 
Thanks I switched to vs code too 
We're currently waiting for binary builds to finish; https://launchpad.net/~hvr/+archive/ubuntu/ghc already has final builds of GHC 8.0.2 specifically configured for Ubuntu 12.04/14.04/16.04/16.10 so Travis jobs can already start using/migrating to 8.0.2; I still need to build packages for Debian 8 &amp; Debian 9. There's also other people contributing binary packages/builds still being in progress. 
&gt; Since (/=) has a default implementation, we can only implement (==). The wording on this is a bit weird, seems like you can not implement both
I don't think it's fair to compare typeclasses with non-generic interfaces. With generic ones you can get pretty far in terms of what you can do with typeclasses.
Can you do what the post outlines? Why would it be unfair to compare them?
But why can’t `ma` be garbage collected? At least in `IO` `ma` should be evaluated in the `case` statement and can then be garbage collected.
TLA+ is best for specifying concurrent systems and in identifying obscure bugs in them. I can't see it being a good tool for specifying Haskell systems. You would probably get bogged down in irrelevant details long before you got anything useful out of it. The one tool that I have found useful when sketching out and debugging the structure of systems I am about to build has been Daniel Jackson's [Alloy Analyzer](http://alloy.mit.edu/alloy/). It is based on relations rather than functions but that is only a minor difference. The definitive reference is Jackson's book *Software Abstractions: Logic, Language and Analysis* (make sure you get the revised edition of 2011 because the syntax has changed since the first edition).
If I read this blog post right, it seems to promote [orphan instances](https://wiki.haskell.org/Orphan_instance) as a great and useful feature, rather than an anti-pattern. But what really worries me is the conflation of type classes with OOP interfaces and data types with OOP classes. Why not just write doSomething :: MyLarge -&gt; Int -&gt; IO () doSomething ... instead of creating a type class for only this function? Type classes are not the right abstraction for this. For example the equivalent of public abstract class MyLarge { public bool doSomeThing(int lol); } could be data MyLarge = MyLarge { doSomething :: Int -&gt; Bool } or why not use explicit dictionaries like this: data DoesSomething a = DoesSomething { doSomething :: a -&gt; Int -&gt; Bool , doSomethingElse :: a -&gt; Int -&gt; String } myLargeDoesSomething :: DoesSomething MyLarge myLargeDoesSomething = ... My point is this: consider alternatives to type classes, especially when there are no algebraic laws or when the instancing coherence isn't needed. Coherence is the main point of type classes IMHO, not pseudo OOP and ad-hoc overloading.
It's hard to say without digging much deeper, but maybe this will help you: If I manually expand and rewrite the terms by hand in a text editor I get something like this for your leaky implementation: poll us action = go where go = do ma &lt;- action case ma of Just a -&gt; return a Nothing -&gt; do liftBase $ threadDelay us go And this is what the second version expands to: poll us action = go where go = do ma &lt;- action x &lt;- case ma of Just a -&gt; return $ Just a Nothing -&gt; do liftBase $ threadDelay us return Nothing case x of Nothing -&gt; go Just x -&gt; return x
Can we preach about the typeclass trinity of Foldable, Traversable, and Witherable? Guess which one is Witherable.
I posted the answer [on stackoverflow](http://stackoverflow.com/a/41521876/110081). TL;DR: it's caused by the interaction of (double) recursion and type classes and is arguably a ghc bug.
Hooray! In fact, [2017-01-16](https://www.stackage.org/nightly-2017-01-06) was released an hour or so after my comment. 
`runghc` can't use `-threaded`. -- stack runghc -- -threaded +RTS -N main = pure () &gt; stack Main.hs runghc: the flag -N requires the program to be built with -threaded There's no way that I know of to use the `stack` comment to compile and run the program as opposed to just using `runghc`. 
Can you provide errors?
Async/await is similar to do notation, but it is more powerful in some ways (await can appear in expression context) and less powerful in some ways (continuation can only be invoked once, so you can't express the list monad with async/await).
Is the concept of return type polymorphism equivalent to typeclasses being open by default?
Does Hackett support variadic functions? Uncurried functions? Functions whose parameter is a single tuple? `apply` as a function (like Python's `*` in argument lists)?
soon! GHC 8.0.2 isn't officially released yet... :-)
This is simply **impossible** with C++, and that's a good thing...
&gt; Yes. MonadReader has two canonical instances, one for ReaderT r and one for (-&gt;) r. That's awesome! I wasn't aware of that. Thanks for posting! :)
&gt; These are pretty mature and robust libraries, so I'm surprised I can't find any such cases. Stability-wise, yeah. But in terms of developer ergonomics I've found accelerate sometimes lacking. And the CUDA backend didn't work with Windows which was a pain since drivers for linux are often lacking. Also it's one thing to have nice easy 8-core computations but so long as distributed computing etc. is relevant we'll need tools for that as well. 
Futures are continuations, when the answer type of the continuation is an action in a concurrency monad. This means that they can expose a richer API than a plain continuation monad. (Eg, a select operator.)
dude, I'm sorry no one got your pun
I might be wrong. However, there seem to be pretty limited places where memory could actually leak in that function: 1. Bindings not being garbage collected, of which there is only one. 2. Tail call optimization not kicking in. In the latter case, I'd expect the program to run out of stack. Since OP didn't say anything about out of stack errors, I figured it was more likely the first option.
I don't understand this as well. The "Definitional interpreters" paper also uses the CPS transformation IIRC, which would correspond to the Cont monad. What is the point of having the Identity monad here?
&gt; 90% of the time it doesn't matter either way. This is the biggest part of it to me. It's just negligible almost all of the time, and the benefits of laziness are slightly less negligible than the benefits of strictness.
FWIW, this `f &gt;=&gt; g = \a -&gt; f a &gt;&gt;= \b -&gt; g b` is less concise than this `f &gt;=&gt; g = \a -&gt; f a &gt;&gt;= g`, and are fundamentally the same thing (up to strictness). This `x &gt;&gt;&gt;= f = f x` is just the `&amp;` operator from `Data.Function`, and it has nothing to do with Monads. (&amp;) :: a -&gt; (a -&gt; b) -&gt; b a &amp; f = f a --- *(Warning: incoming crash course on category theory)* Ultimately, you're approaching the concept of a Kleisli category. A category is any composition operator with an `id`. The simplest instance is the one we all know and love, `(.)` and `id`. But it's not the only one. Let's define a typeclass for it to help us talk about this. import Prelude hiding ((.), id) class Category cat where id :: cat a a (.) :: cat b c -&gt; cat a b -&gt; cat a c instance Category (-&gt;) where -- id :: (-&gt;) a a -- id :: a -&gt; a id = \a -&gt; a -- (.) :: (-&gt;) b c -&gt; (-&gt;) a b -&gt; (-&gt;) a c -- (.) :: (b -&gt; c) -&gt; (a -&gt; b) -&gt; (a -&gt; c) f . g = \a -&gt; f (g a) *This class is defined in `Control.Category`* There are laws to this class. `f . id == id . f == f`, and `f . (g . h) = (f . g) . h`. I think those are trivially obeyed above. This class ultimately just gives us a way to abstract the concept of composition. Composition is not limited to simple function composition. If you reverse the arguments of the `&gt;=&gt;` operator (aka the `&lt;=&lt;` operator), you get another instance. newtype Kleisli m a b = Kleisli { runKleisli :: a -&gt; m b } instance Monad m =&gt; Category (Kleisli m) where -- id :: Kleisli m a a -- id :: a -&gt; m a id = Kleisli return -- (.) :: Kleisli m b c -&gt; Kleisli m a b -&gt; Kleisli m a c -- (.) :: (b -&gt; m c) -&gt; (a -&gt; m b) -&gt; (a -&gt; m c) Kleisli f . Kleisli g = Kleisli $ \a -&gt; g a &gt;&gt;= f *The `Kleisli` type can be found in `Control.Arrow`, where there is an additional abstraction known as `Arrow`* This notion of composition is found in a variety of ways and can be very powerful. This may be why you felt your ideas could be generalized
It can definitely complicate the FFI story. Interoperating with libraries that weren't written with your fancy-pants green threading system in mind becomes a rather more complicated affair. _Everything_ that would block needs to have some kind of special handling or you get situations where you run out of threads. Worth it? Totally. But it does make for a much more complicated story. Also things like wait-free data structures become more complicated to work with, as most of the techniques that make lock-free data structures wait-free require you to know in advance the number of workers. The total number of threads in such systems tends to fluctuate up and down a great deal. With some work you can tie those resources to the OS threads, but everything you do in that vein is a custom one-off design.
Not very familiar with Java generics, doesn't this require that the caller provide the same type for both arguments, just like in the Haskell case? static &lt;A extends Ord&gt; bool lessThanOrEqual(A a1, A a2) { So if you do `OrdUtil.lessThanOrEqual(x, y)`, both `x` and `y` need to be of type `A`. They can't be of two different types that happen to extend `Ord`. For that, you'd need: static &lt;A extends Ord, B extends Ord&gt; bool lessThanOrEqual(A a1, B b2) { which maps directly to how you'd do it in Haskell: compare :: (Ord a, Ord b) =&gt; a -&gt; b -&gt; Ordering If I'm right, then why does he say "The Java version allows any two objects to be passed, provided they implement the Ord interface"? And if I'm wrong — how does that even make sense? There's just one `A`.
Regarding what you said about FFI (which I don't really have much experience with), doesn't the scheduler have some sort of preemption built in such that if code inside some green thread is blocking, it'll switch to executing another green thread? Does FFI inside a green thread somehow negate that? To condense all that, I think my question is "why does FFI require special handling if the userspace scheduler has some form of preemption?"
Not really, it should be `Maybe f a`.
The overuse of the letter *f* Once you get used to *f* as a function, it's incredibly confusing that it gets reused when functors are being introduced. Just as you got used to *f* representing *sin* or *sqrt* or *inc*, you look at functor and all of a sudden *f* represents Maybe or [], which of course (!) are parameterized types. Obviously, when you're expert, it's clear from the context, but not when you're starting. So instead of class functor f where fmap :: (a -&gt; b) -&gt; f a -&gt; f b write class functor pt where fmap :: (a -&gt; b) -&gt; pt a -&gt; pt b
&gt; Thanks for the quick response! Implemented foldl' and definitely learned a lot from that, thanks! From what I understand, foldl' forces the current value being folded/reduced over to be computed instead of being kept as a lazy, to be computed value (and I guess, what a thunk means), right? Exactly! &gt; Also, will make sure to check out to check out the vector library. Are vectors better because they're a lot like arrays in a C-based programming language in that they provide sequential memory access? Yes, `Data.Vector.Unboxed` provides an unboxed vector: just a block of memory with a bunch of doubles neatly packed into it. However, this does mean that you are slightly more limited in the sorts of elements you can work with (namely types with `Unbox` instances). Note that this is not the case of `Data.Vector`, which you can think of as an array of pointers to normal (lifted) Haskell values.
I've used the `tagged` and more generally `proxy` before for other things, but generally avoid them (unless there's a real requirement) as it makes onboarding new people onto a codebase more difficult (especially if they are coming from another language as is usually the case). I generally want to avoid more advanced type-foo unless it's absolutely required or makes things much safer as, in my opinion, it increases the maintenance load long term. Moreover, things that aren't "accessed by typeclass" (like Container's `Map`) are exactly the problem. One solution is to make an `IsMap` instance to derive but I'd be more hesitant to use it given that the approach is not guaranteed to be zero-weight at runtime; for example, an inlining rule doesn't fire (which is hard to spot). What I'm basically saying is that the unwrapping and type specification in this is redundant noise that there should be a way to do without: lookupMyVal :: MyKey -&gt; Map1 -&gt; Maybe MyVal lookupMyVal k (MyMap m) = Map.lookup k m This, of course, would look a hell of a lot like `type` instead of `newtype` but guess that's what I want. For things x,y,z make the `newtype` behave like `type` but otherwise behave like newtype. As for `ClassyPrelude`, it would need an `IsMap` instance to work.
There is no good reason for `(&gt;&gt;=)` other than how similar it looks to `do` notation in cases like this: ma &gt;&gt;= \a -&gt; mb &gt;&gt;= \b -&gt; return ... In fact, the reversed version `(=&lt;&lt;) = flip (&gt;&gt;=)` ends up behaving much like `$`, which people use much more often than `&amp;`. Using `&amp;` is so uncommon (outside of `lens`), that I wouldn't say there's much value in these obscure ways that you're writing these functions. There's no reason to do `\a -&gt; f a &amp; \b -&gt; g b` when you could just do `\a -&gt; g $ f a`, which is akin to doing `\a -&gt; g =&lt;&lt; f a` instead of `\a -&gt; f a &gt;&gt;= \b -&gt; g b`. Furthermore, I'll stress that `x &gt;&gt;= \a -&gt; g a` is unnecessary when you can just do `x &gt;&gt;= g`, to simplify the whole thing (same principle applies to `&amp;`) There are a number of advantages to `(=&lt;&lt;)` aesthetically over `(&gt;&gt;=)`. My favorite is that it lines up nicer with the types of the monad hierarchy of functions fmap :: (a -&gt; b) -&gt; f a -&gt; f b (&lt;*&gt;) :: f (a -&gt; b) -&gt; f a -&gt; f b (=&lt;&lt;) :: (a -&gt; m b) -&gt; m a -&gt; m b (&gt;&gt;=) :: m a -&gt; (a -&gt; m b) -&gt; m b -- does not line up at all
When you are using intero, what is managing your dependencies? Nix or stack?
Nix.
You can do it this way: lookupMyVal :: (Coercible map (Map.Map k v), Ord k) =&gt; k -&gt; map -&gt; Maybe v lookupMyVal k cmap = Map.lookup k (coerce cmap) But you lose the type safety for the `v`. Still, the `IsMap` seems to me quite a good solution - as the derived class is mostly a noop, does it really happen that it doesn't get inlined and eliminated?
Yea we ran into this problem as well. It seems [to be a bug in the `tls` package](https://github.com/vincenthz/hs-tls/issues/152), and it looks like it was fixed in `tls-1.3.9`. If you're using `stack`, you can apply this fix by adding that version to your `extra-deps`.
A value of type `Monad m =&gt; m (Maybe ())` doesn't know what monad `m` is going to be. So it can't ask that monad to do anything unsafe, since it doesn't know that the monad will be able to do such things. GHCi just chooses that whenever you enter a command that returns a value of type `Monad m =&gt; m ...`, it will default `m` to `IO` and run the action. It's just some awkward top level behavior of GHCi that's very useful for testing out code. The fact is, if the action has type `Monad m =&gt; m ...`, it can't know that `m` will be `IO`, so it can't use that fact to do anything unsafe.
So you can easily create a stream called "users" and post all events for all users to it, but what you really want is a stream per user, so event store lets you listen to one stream and emit events to a partition in another stream, so eg. you would create the "users" stream with all user related events as above, but then you could have a projection that's listening to that stream and partitioning them into substreams of the form "users/&lt;username&gt;" (eg. "users/john", "users/dave" or perhaps "users/&lt;userId&gt;" so maybe "users/105", "users/106", etc) and then you can make a "user details" type of projection off of the partitioned users stream and the "user details" projection will be run on each substream automatically. This blog post gives more detail: http://codeofrob.com/entries/re-partitioning-streams-in-the-event-store-for-better-projections.html His whole series is linked from http://docs.geteventstore.com/ .
&gt; does it really happen that it doesn't get inlined and eliminated? Yep, in practice even on `O2` it may not get inlined. The larger point is how hard it is to figure out if it was (e.g. dump the core). With `{#- PRAGMA inline ...` it probably will inline/boil away but technically this just creates a rewrite-rule that may or may not fire (though usually does). My point is that I adore `newtype` because I get safety without losing performance (it's not even a question of "oh, maybe it's not inlining the type change")... I'd just like to be able to say "loosen up the type safety for this set of functions". I really don't want to move to a method that increases the surface area of performance related things I need to investigate should the need arise. I'd prefer stick with wasting typeing-seconds to unwrap `fooMember k (MyMap m) = Map.member k m` vs my having to possibly waste hours/days tracking down a performance issue that turns out to be an inline issue. All that being said, it's a "princess and the pee" class problem.
I'm sorry but I'm really struggling to understand what you're getting at here and in the SO question &gt; I think lambdas are making the expressions more confusing. Well yes, `f &gt;=&gt; g` is more confusing than `\a -&gt; f a &gt;&gt;= \b -&gt; g b`; that's why `(&gt;=&gt;)` was defined. Are you aware that what you posted is just the [definition of `&gt;=&gt;`](http://hackage.haskell.org/package/base-4.9.0.0/docs/src/Control.Monad.html#%3E%3D%3E)? When you expand a function it tends to get more complicated. So lambdas are language constructs, all the other things you mention, (`&gt;&gt;=`, `&gt;=&gt;`, `.`, etc.) are just haskell functions whose source you can inspect etc. No idea if that helps or is obvious to you. Possibly annoying questions to follow: &gt; I'm prepared to think mistakes were made. What do you mean? By whom? &gt; However do notation is highly analogous to imperative programming, and it maps directly to bind notation why do you say "however"? &gt; This suggests the choice of notation with do was very intentional, and even insightful. ...yes? I mean it was certainly designed to be a nicer syntax than the one it desugars to. `do` notation still "introduces a mess of floating bound variables one has to keep track of" though.
I noticed that you have `import System.IO (getLine, readFile)` and then later down you have `fileLocation &lt;- System.IO.getLine`. I'm actually surprised this works, you can just call the function `getLine` directly, or use `import qualified` to not clutter the root namespace. I suspect what you're trying to do here, is keep track of what module each of these functions comes from while you're still learning. In that case you could also consider using something like `import qualified System.IO as SysIO (getLine, readFile)` I'm also curious why you have chosen the `let .. in` style over the more common `... where ...` style?
That is not the same. class X implements Ord&lt;X&gt; { int compare(X b) { return 0; } } class Y implements Ord&lt;X&gt; { int compare(X b) { return 0; } } `interface Ord&lt;T extends Ord&lt;T&gt;&gt;` does not ensure that `T` be the same type as the type implementing the interface.
&gt; calling a function that takes a tuple would look something like `(foo '(a b c))`? Probably, yeah. I haven’t decided on tuple syntax yet but something like that seems reasonable. &gt; Hackett doesn't support `apply` as a function, but it should be able to support `apply` as a macro, right? Well, it depends on what you mean by `apply`. The traditional Scheme definition of `apply` doesn’t make sense, since lists are homogenous and variadic. However, you could write an `apply` that works with tuples instead of lists, which would basically just be a generic `uncurry`. &gt; By "Hackett doesn't support variadic functions", you mean implicitly "any more than Haskell does", right? I guess? But Haskell doesn’t really support variadic functions at all by my definition of “variadic function”.
Hi! I actually just started implementing qualified `import qualified`a day ago! I read that using the regular imports allows you to use a function from a module, whether fully qualified or not, but `import qualified` forces you to fully qualify it. Also, I think the `let .. in` style was my accent showing from an experience I had with F# (and OCaml) experimentation from a class I took about a year ago and along with my experience in doing functional stuff with JS, it felt very natural to connect it to the concept of local variables (although it's not a variable at all). Thank you so much for the great insight!
Just did! Thanks for the share!
Just wanna say that the application form is really well done, none of that "5 years of full stack web dev experience", just what is really needed. Probably that's because not a lot *is* needed, but still
 /private/var/folders/qm/zjkfdzrn2wx40dvq6y3s8zrm0000gn/T/stack33059/ghc-mod-5.6.0.0/Language/Haskell/GhcMod/Target.hs:497:20: error: • Couldn't match expected type ‘ghc-boot-th-8.0.2:GHC.LanguageExtensions.Type.Extension’ with actual type ‘Extension’ NB: ‘Extension’ is defined in ‘GHC.LanguageExtensions.Type’ in package ‘ghc-boot-th-8.0.1’ ‘ghc-boot-th-8.0.2:GHC.LanguageExtensions.Type.Extension’ is defined in ‘GHC.LanguageExtensions.Type’ in package ‘ghc-boot-th-8.0.2’ • In the first argument of ‘xopt’, namely ‘TemplateHaskell’ In the first argument of ‘(||)’, namely ‘TemplateHaskell `xopt` df’ In the expression: TemplateHaskell `xopt` df || QuasiQuotes `xopt` df || PatternSynonyms `xopt` df /private/var/folders/qm/zjkfdzrn2wx40dvq6y3s8zrm0000gn/T/stack33059/ghc-mod-5.6.0.0/Language/Haskell/GhcMod/Target.hs:498:20: error: • Couldn't match expected type ‘ghc-boot-th-8.0.2:GHC.LanguageExtensions.Type.Extension’ with actual type ‘Extension’ NB: ‘Extension’ is defined in ‘GHC.LanguageExtensions.Type’ in package ‘ghc-boot-th-8.0.1’ ‘ghc-boot-th-8.0.2:GHC.LanguageExtensions.Type.Extension’ is defined in ‘GHC.LanguageExtensions.Type’ in package ‘ghc-boot-th-8.0.2’ • In the first argument of ‘xopt’, namely ‘QuasiQuotes’ In the first argument of ‘(||)’, namely ‘QuasiQuotes `xopt` df’ In the second argument of ‘(||)’, namely ‘QuasiQuotes `xopt` df || PatternSynonyms `xopt` df’ /private/var/folders/qm/zjkfdzrn2wx40dvq6y3s8zrm0000gn/T/stack33059/ghc-mod-5.6.0.0/Language/Haskell/GhcMod/Target.hs:499:20: error: • Couldn't match expected type ‘ghc-boot-th-8.0.2:GHC.LanguageExtensions.Type.Extension’ with actual type ‘Extension’ NB: ‘Extension’ is defined in ‘GHC.LanguageExtensions.Type’ in package ‘ghc-boot-th-8.0.1’ ‘ghc-boot-th-8.0.2:GHC.LanguageExtensions.Type.Extension’ is defined in ‘GHC.LanguageExtensions.Type’ in package ‘ghc-boot-th-8.0.2’ • In the first argument of ‘xopt’, namely ‘PatternSynonyms’ In the second argument of ‘(||)’, namely ‘PatternSynonyms `xopt` df’ In the second argument of ‘(||)’, namely ‘QuasiQuotes `xopt` df || PatternSynonyms `xopt` df’
&gt; There is no good reason for (&gt;&gt;=) other than how similar it looks to do notation My impression is that `&gt;&gt;=` came first, and `do`-notation was devised later. Perhaps someone who was around for that can clarify. Also, `x &gt;&gt;= \a -&gt; long_bit_of_code` puts the bound value (`a`) right next to the expression it came from, and requires fewer parentheses than `(\a -&gt; long_bit_of_code) =&lt;&lt; x`.
&gt; Haskell doesn’t really support variadic functions at all by my definition of “variadic function”. Is that including the `printf` trick? (Discussed in more detail [here](https://stackoverflow.com/questions/7828072/how-does-haskell-printf-work) and [here](http://cs-syd.eu/posts/2016-04-09-typesafe-polyvariadic-functions-in-haskell.html), taken to extremes [here](http://okmij.org/ftp/Haskell/polyvariadic.html)).
Right. Once you have a good sense of what's going on, it's by far the most convenient choice for behavior of the interpreter. Even before you have a sense of what's going on, when you're just wanting to see values and execute actions, it's convenient. When you're trying to *build* a sense of what's going on, it's really painful though.
&gt; Also, `x &gt;&gt;= \a -&gt; long_bit_of_code` puts the bound value (`a`) right next to the expression it came from, and requires fewer parentheses than `(\a -&gt; long_bit_of_code) =&lt;&lt; x`. Sure, but I find that generally to be a pretty rare occurrence. I'm usually either using `do` notation, or the left hand side of `=&lt;&lt;` isn't a lambda.
It's a little unclear from the hiring description; is this a full-time, part-time, or freelance position?
Nanoseconds to schedule well-done green threads in my personal experience. Memory overhead is for stacks. This varies but is manageable.
&gt; clarity on what is actually syntax (see questions like "help me understand the craaaazy syntax in `foo $^^ bar ..**.. blort`") Most of these people probably come from languages where the set of operators is immutable; in a lot of these languages, their semantics is immutable as well. So the idea that operators are just normal functions, and you can make one with whatever name you like, is pretty foreign.
Return type polymorphism is what makes it interesting though. Without it, classes like monoid and monad are unnatural to use.
NO (upvotes only)
I think I need to sleep on this. But I'm going to ask a small question to see if I understand. In `(&gt;&gt;=) :: m a -&gt; (a -&gt; m b) -&gt; m b` instead of operating on the first input as "an element" I want to operate on an arbitrary function to it. This leads to fish `(&gt;=&gt;)`. That is, there is an isomorphism between these representations given by Yoneda and Kan extensions?
&gt; Enterprisey skills &gt; &gt; - Can use Stack 
I think I understand the question but I'm not sure what is YES and what is NO ? Could you clarify please ?
**Nobody implemented it yet**, but it's known as type directed name resolution (TDNR). [There is some information on it](https://wiki.haskell.org/TypeDirectedNameResolution) but it's pretty controversial. Edit: **Nobody implemented it yet**
Side note: Shouldn't you make people both upvote their preferred answer *and* downvote the other?
Note that asking people to "upvote if you agree" is a violation of reddit's rules and can lead to a ban.
Maybe edit these into the actual yes/no comments, to compensate for Reddit moving things around.
No, that only doubles the difference (i.e. doesn't change the relative result) and could bury the losing one under these other comments.
and *git* 
Ehh, this isn't the case that's meant to cover (karma/attention farming). "Makeshift poll" posts like this are really fairly common. In fact, if you buy gold, you sometimes get to participate in _official_ "name the next server" polls which function exactly like this.
This kind of poll is not allowed as it falls under “upvote if you agree.”
It would've been a tiny bit unfair to require 5 years of full stack web dev experience, given that when I myself started working on the project, I had spent half a day trying to ~~learn~~ google up enough Javascript to manage to save contents of a single `input` tag in an IORef (see the second commit: https://github.com/aelve/guide/commit/2cc514a85f8a2fca20951749d92590007e903422)
Who cares about ~100 Reddit karma?
Kwang: Calling arbitrary monads "lazy" or "strict" is confusing. There are many ways in which a monad can be "lazy" or "strict". I don't think either Eval or Strict State would have provided enough strictness for your interpreter: &gt; flip Control.Monad.State.Strict.runState () $ do x &lt;- return undefined; return 1 (1,()) &gt; Control.Parallel.Strategies.runEval $ do x &lt;- return undefined; return 1 1 This is why in my [previous comment](https://kseo.github.io//posts/2017-01-05-implementing-a-call-by-value-interpreter-in-haskell.html#comment-3085021298), I referred to a specific strict identity monad; the one which actually forces the wrapped value: &gt; Control.Monad.StrictIdentity.runStrictIdentity $ do x &lt;- return undefined; return 1 *** Exception: Prelude.undefined BTW, I was surprised when I didn't find my comment [here](https://kseo.github.io/posts/2017-01-05-implementing-a-call-by-value-interpreter-in-haskell.html). It appears that you use inconsistent URLs (with single and double slash), and that confuses disqus. There's a disqus variable ("canonical url") that you can set to avoid that. 
Upholding the rules of reddit is more important than making friends with random people on the internet, the only connection to which is that they don't want me to uphold the rules of reddit.
Just skimming through your code, the actions are the parts that jump out at me. It seems awkward to define a separate action for each direction. Why not make a Direction data type in which you can define all the directions: data Direction = Up | Down | Left | Right and then just have the constructor as data Action = ... | Move Direction | ... This would allow you to easily extend direction to add diagonal movement as well as looking cleaner. Also, you might not need to have Coords be a new type. You could just declare it as a type synonym for (int, int). This allows you to put in a tuple of ints into any function with a signature Coords. This might be useful sometime?
In my opinion, I would much prefer a way to specify "non zero" value rather than a `Maybe` which you would need to propagate all the way up in your code.
[notzero](https://hackage.haskell.org/package/notzero)
That's a good point about the movement; I added the bomb+move stuff and really should clean that up! As for the Coords I did it on purpose to force myself to lean on the type system... not that I'm likely to get another tuple of Ints confused with what ought to be coordinates I suppose...
If this affects the proper operation of your program in any normal use case, you should add the constraint `&gt;= 1.3.9` to the `tls` entry in the `build-depends` field of your cabal file. That way, you are correctly informing the world that your program might not work correctly with earlier versions of tls. And that will work for both `cabal` and `stack`.
This is not a reimplementation of IO, just a free monad approach for a restricted effectful language, it's still 100% dependent on IO to do anything. A good introduction by Gabriel Gonzalez to this approach is [here](http://www.haskellforall.com/2012/06/you-could-have-invented-free-monads.html).
It's one line to take a provided default-safe division and make yourself a safe variant. Why not default safe?
I see your point. Each monad has varying degree of strictness and so I can't call arbitrary monads "lazy" or "strict". This is really tricky :) Thanks for the comment. I've just set the Disqus configuration variable. I will migrate your comment to the canonical url.
Isn't that doubly redundant busywork though? For Stack you still need the explicit `extra-deps` definition in your stack.yaml or a resolver which has the right version in it already. How do you make sure the `build-depends` version bounds don't bitrot?
I'll take the fact that you are moderator of /r/diepartei as a hint that you are trolling. Here is a [nice piece of history of trolling Haskell](https://gist.github.com/quchen/5280339).
Fantastic! Good luck with this effort, I hope it succeeds.
Sounds like another ACME package
If by overloaded numeric literals, you mean the current desugaring of integer literals to fromInteger (_ :: Integer) (`fromInteger` is a member of the `Num` class), then `0 :: NonZero` type checks just fine - and you'll get an exception at runtime. I think /u/chrisdoner has argued this before: overloaded literals of all stripes should be resolved at compile time.
A type like `a -&gt; a -&gt; Maybe a` actually seems a bit misleading compared to `a -&gt; NonZero a -&gt; a`, because whether you get a result depends entirely on the second argument.
No, the most import place to put it is in the cabal file, which describes the relationship of your code with its dependencies. This is critical semantic information about the code which only the author knows, and should share with the world. Both cabal and stack use that information, and future smarter tools will use this information in even better ways. Whereas the `extra-deps` entry expresses the fact that the current stackage snapshot you are using doesn't contain a newer version that you need. It's not really about your code or its dependencies - it's about stackage. And it won't be needed anymore once stackage is updated.
https://prime.haskell.org/wiki/Defaulting describes the limitations of the current type defaulting rules. Your type class always fails because defaults cannot be applied to user-defined classes. 
Yes, this is how you enable the GHCi's extended default rules in GHC.
I also wanted to add a trick question &gt; [ ] Do you ever, like, write comments? At least sometimes? but decided that it was a bit too much.
I mean, all this does is generate a typeclass and some instances. What it lets you do is avoid the open world assumption but covering every possible case. The cases where you'd want to use something like this are probably pretty limited but I have ran into this situation a couple of times. When it would be nice to just have two definitions of a function that get properly selected based on the context.
You can have (-1) be a function by defining a `Num` instance for functions (but you don't really want to do that). Otherwise, this doesn't help you since you can't overload existing names. As for stackage, I can't give you an eta.
In case you want to do an impromptu poll quickly, you can use doodle instead of reddit's upvotes, which I'm pretty sure aren't 1 vote = 1 point: http://doodle.com/poll/gkwacducagzbgf9y NB: doodle is also killer for figuring out when it's best to schedule something amongst a group of friends.
I sometimes use infix functions in anonymous functions to avoid flip: (`elem` [foo, baz, bar]) It is of course just a tool to increase readability and should be used where it would make it simpler, clearer to read the code. 
You could probably also do it in the old type of Haskell, where `main`'s type was `[Response] -&gt; [Request]`.
Its also really simple! However, I'm having problems with evaluating my monadic transformer stack, and having exceptions being thrown without using 'unsafePerformIO' to read spec files. If I can get this: https://hackage.haskell.org/package/hspec-expectations-lifted to work, the argument to stay with HSpec will be strong for me.
 encoderApodization :: !(Maybe (NonEmpty ApodizationFunction)) Isn't this pretty much the same as encoderApodization :: ![ApodizationFunction] , disregarding possible bottoms like `Just undefined`?
Are you not trying to solve a problem which doesn't exists ? I mean if people really need a total division it's easy enough to write your own `/? :: Num a =&gt; a -&gt; a -&gt; a` or stick it in a module people which need it can use it. Unlike other language Haskell makes it really easy to add new operator like this. However, I use `*` `/` lots in trivial programs and what bother me a lot is to have to convert integers to use them in conjunction with double so I sometimes end up defining `x *. y = fromIntegral x * fromIntegral y`.
I don't really like any of the existing libraries for testing and wrote EasyTest: https://github.com/unisonweb/unison/blob/master/yaks/easytest/README.markdown It is a somewhat opinionated replacement for QuickCheck, HUnit, etc. Unreleased currently (incubating in a larger project) but will do a release at some point. So far it's been pretty great, but then I am biased of course. 😀
Used hspec a lot, without considering all alternatives. It is nice, but after reading comparisons and looking at hspec vs tasty as test organizers, I recently decided to switch to tasty/hunit. We have people who don't know Haskell, with whom I have used our test suite to help them contribute a bit. I don't like how much hand waving I have to do about the spectree writer monad with them. I have minimalist tendencies and don't see that much value in rspec syntactic sugar. Nested test groups are good enough relative to having "describe", "context." Comparing third party contributions, tasty and hspec are pretty close, with hspec having a few extra libraries for wai, aeson. There are more discussions and tutorials for hspec. EDIT: Tasty's pattern matchers are slightly richer - https://github.com/hspec/hspec/blob/master/hspec-core/src/Test/Hspec/Core/Util.hs#L70 vs http://documentup.com/feuerbach/tasty#patterns ... Tasty provides restricted single path component matches. withResource seems easier to understand initially than before, around, etc. If I wanted non-technical product managers to be able to skim my test code, I would use hspec to use the syntactic sugar. This isn't a goal of mine. 
One advantage of infix functions is you save some parenthesis example f a `div` g b vs div (f a) (g b) Depending on the name of the function one is easier to read than the other.
Nice. I might switch over to this, once it gets out of beta.
Also, in a prior discussion people pointed out that they liked doctest, but usually have helper functions in their testing code which is messy/hard with doctest. If someone comes up with a nice approach to importing testing helper functions for use in doctest simply while isolating those test helper functions from the actual code under test, that might be appealing. EDIT: I am experimenting with using hspec (or hunit) like doctest. So you import the module into your main code, but only use the Expectation type (shouldBe, etc) inside of functions like functionBeingTested_docTest. At the same time, create a module TestUtil.hs in your main code, which the doctest like fragments can use. Then in your test project, import functionBeingTested_docTest into your test specs, like `it "doctest examples" $ functionBeingTest_docTest `
IME these kinds of tricks are absolutely necessary if you want to provide a nice API to an OO library. At least until type directed name resolution and OverloadedRecordFields are in.
Divide by zero exceptions were common when we were building client facing dashboards at work. If I were to rewrite them in Haskell I would definitely try out something to guarantee no divide by zero.
Setting aside ancient history, I've gone from `tasty` to `hspec` recently. I'm pretty wildly biased against the `*spec` style, but I gave it a chance anyway and actually do find it to provide a rather helpful structure for writing tests and descriptions. I like `doctest` in theory, but find it quite awkward in practice due to sometimes excessive clutter in the source file, and the contortions needed to set up the data, helpers, etc you need to write the perspicuous examples you want to highlight. My current pondering is if linking to `hspec` tests from haddocks around the real functions could be made slick.
I have a library for (financial) accounting. The Prelude number types are useful as a base for building but not for widespread use throughout the library. I have many numeric types: * `Positive` - positive integers * `NonNegative` - integers 0 or greater * `NonZero` - integers positive or negative, but not zero * `Exponential` - has an integral coefficient (usually one of the above types, or just `Integer`) and an exponent which is a `NonNegative` This means I don't use the `Num` operators. I could try to come up with a slick typeclass hierarchy but so far that hasn't been worth the effort, and I don't have the mathematical expertise to come up with something that is theoretically coherent (not that the Prelude numbers are theoretically coherent, though.) My code has improved greatly since I stopped trying to wedge the Prelude number types into whatever I was doing. So at this point I don't care much what happens to `/` because my use of it is buried in code I don't have to touch or see much. 
That's an interesting way to put it, the sentence readability makes alot of sense to me. Thanks!
I could see how that might same some space depending on how the functions being called need to associate. Thanks ! 
I just wish the parameters were flipped when called infix for grammatical correctness `preposition object verb` subject becomes `verb preposition object`. For example consider `filter with isOdd` and `with isOdd filter` and `traverse ala Sum` and `ala Sum traverse`. Also and less persuasively. `verb object subject` to `subject verb object`. For example `remove "header" message` and `message remove "header"` - although not strictly grammatical. `x isBiggerThan y` to `isBiggerThan y x`
Well, for me it was just a lack of nice/easy function like "average" and whatnot. Obviously you can use a fold but those would be nice. And the documentation can be unclear in some ways - e.g. one problem I only figured out reading a PR on github. 
"Can use Spock or some other web framework" - That's a good benchmark! ;-)
One thing that comes to mind is custom command line options.
https://ro-che.info/articles/2013-12-20-tasty-custom-options.html Unfortunately I couldn't find a better example than this years old blogpost and there isn't an example of usage nor an example of --help. :-(
At work (Ambiata) we just use QuickCheck, mainly because its trivially easy to load a single test module into GHCi and run the tests for just that module. This "test a single module" capability is not really possible with Hspec. For my own stuff I use `Hspec` (usually with a high proportion of property tests). I prefer the concise and coloured output that Hspec gives. 
And all should have some means of checking at compile time, though of course that's technically more complicated.
Thanks
c.f. "Whenever I'm dereferencing I've persuaded myself that the pointer isn't null and points at what I intend, but representing that information in the type system is more trouble than it's worth." I have no trouble persuading myself that this is the case. I have some trouble knowing when my other changes might threaten to make it *not* the case. Granted, `a -&gt; a -&gt; Maybe a` doesn't do very much for this. `Nonzero` is probably a better approach, and refinement types are even more attractive once we get them working well.
I'm not sure I understand what you mean. If you're talking about a computer algebra system (CAS) that can do symbolic manipulation, then sure, those are great tools! They are complementary to the approach I've taken, in which functions and values are used as the primitive concepts. In other words, my focus has been on enabling students to use mathematical concepts as the building blocks to create things; as opposed to using technology as the building block to create mathematics. That is not because I think symbolic manipulation and the mechanics of mathematics are unimportant -- they are not! -- but rather because I think they are sufficiently taught by existing math education. That said, as always in education, there is room for many perspectives and ways of learning. Writing code to manipulate expressions can definitely be educational.
I agree, however in practice, except when you are asking for user input, the number you divide by corresponds to something in the real worlds and is rarely null. When it's not the case, then zero appears enough so that you have to check it (but you know it).
Yeah. It's what I want 99% of the time (I'm not embedding Test inside of something else), and its the simplest possible API for this common case... keep the friction low! The constructor for Test is there if you want to run the tests in some more interesting way... A problem with returning the output separately is you can't do streaming output as your tests finish... unless you return an effectful stream of some sort (overkill IMO), or take some callbacks (also overkill and complicated to use). It can be done, but I think it's nicer to just expose the constructor that lets other people play with this if they really need to.
Could you add a link to the talk? &gt; how types should be self-documenting I agree that self-documenting code is better than not-self-documenting code, but it's hardly controversial :) As for comments, * in my project I try to copy [GHC's “notes” mechanism](https://ghc.haskell.org/trac/ghc/wiki/Commentary/CodingStyle#Commentsinthesourcecode) * I think there are some pretty good reasons why self-documenting code won't ever be enough and comments would still be useful, but they're also pretty vague and I'm not ready to articulate them (yet). Anyway, I totally agree with “that means both self-documenting code and comments”.
Not a benchmark! The project is written in Spock, so “some other web framework” is actually a _worse_ thing than Spock for my purposes. Originally they were two separate tickboxes (“can use Spock” and “or can use some other web framework”), but I merged them because – hopefully – for someone who has managed to figure out how to use Yesod or Snap, learning Spock wouldn't take much time.
It wasn't that clear to me, thanks for pointing out. So when you say, Monad remains abstract, that means in GHC it will still convert to the correct use case? For instance, if I expect stream and will try to apply stream's method, will it work? And do I need to specify the expected interface to ensure the conversion? I am sorry it that sounds n00b question, which is this obviously the case ;)
So the value will be later coerced by somehow "knowing" the context? Is there a good simple example of such coercion? 
thanks, this is exactly what I'm looking for, a great tutorial on something I've never heard of!
I tick off a whole bunch of those boxes, but I'm working on my own Haskell ecosystem website :( ( or :) ? )
shouldn't this have `(pure . Just)`? Or is my internal type checker letting me down
Now we need `(:?=)` for `findWithDefault`. /jk
Do you have some examples?
&gt; Plug space leaks in Data.Map.Lazy.fromAscList and Data.Map.Lazy.fromDescList by manually inlining constant functions. This shouldn't be necessary. This is the sort of thing compilers are for. Why was it necessary?
hspec, QuickCheck. The staples of any Haskell code I write, and it goes a long way. Very interested to see what others use of course.
I default to using tasty-quickcheck. I recently found https://github.com/lwm/tasty-discover which I want to try on my next project.
Thank you for the post. I saw some code snippets with this extension but never really care to understand the new syntax. This is crystal clear now :)
My website [justapplause.com](https://www.justapplause.com) uses web push protocol to send push notifications to users on latest versions of chrome and firefox after they sign up. I haven't been able to create a demo on my [personal webpage](http://www.sarthakbagaria.com) yet since it requires SSL/TLS certificate and service worker installed on the website. A quick google search gave [this](https://gauntface.github.io/simple-push-demo/) nicely working demo of web-push.
Yes, thanks. Fixed. This was a pasting issue - I've been away from linux for too long.
 &gt;&gt;&gt; import qualified Streaming.Prelude as S &gt;&gt;&gt; import qualified Control.Foldl as L &gt;&gt;&gt; import Data.Set &gt;&gt;&gt; L.purely S.fold_ L.set $ S.read $ S.takeWhile (/= "quit") S.stdinLn :: IO (Set Int) 3 10 -1 quit fromList [-1,3,10]
Thank you for the quick reply - I will definitely try to understand this once I have a fresh head, but could this be done in a more beginner-friendly way? My knowledge is very low and currently readable syntax for me is something like this: main :: IO () main = do putStr "input: " input &lt;- getLine let toInt = (read input):: Int if input == "exit" then putStrLn "print the array somehow" else main 
Split up the code in 3 functions this time, where inputHandler is similar to the function you wrote (so you can see the similarities). As far as formatting goes, prefix your code lines with 4 spaces for them to be formatted nicely on reddit.
Geany is great. I use it with [ghccheck](https://github.com/emilaxelsson/ghccheck/) for quick type checking.
You don't save the inputs, you simply pass them on as an argument. import Data.List (sort) readNumber :: [Int] -&gt; IO [Int] readNumber numbers = do line &lt;- getLine case line of "exit" -&gt; pure numbers _ -&gt; case reads line of [(number, "")] -&gt; readNumber (number : numbers) _ -&gt; readNumber numbers main :: IO () main = do numbers &lt;- readNumber [] print (sort numbers)
Tasty is so nice, that's what I use as well.
As already mentioned, "Correct by Construction" types are nice if you can manage to find such a representation. A common approach to enforcing more, arbitrary invariants is to write functions which act as "smart constructors", and use these instead of a datatype's normal constructors. For example, say we want a datatype containing a string `s` which conforms to some regex, along with two non-negative numbers `x` and `y` which should add together to give the length of `s`. That pretty difficult to enforce via types, but we can do it with a function: -- Define a type to store our values, although it doesn't enforce our invariants data Foo = MkFoo String Int Int -- Write a function which *does* enforce our invariants mkFoo :: String -&gt; Int -&gt; Int -&gt; Either String Foo mkFoo s x y | not (matchesOurRegex s) = Left (show s ++ " doesn't match regex") mkFoo s x y | length s != x + y = Left "Length doesn't match up" mkFoo s x y | x &lt; 0 || y &lt; 0 = Left "Negative value not allowed" mkFoo s x y = Right (MkFoo s x y) Now we can use `mkFoo` instead of `MkFoo` in *all* of our code, to ensure that we always have valid values (or else have access to a suitable error message). Whilst the `Parser` type does let us represent errors like this, we wouldn't really want it to appear in places which have nothing to do with parsing; using `Either` (or `Maybe`, if you don't care about attaching a message) keeps things nice and general. Whislt the presence of `Either`/`Maybe` can make programming more annoying than using a straightforward `Foo` value, we can usually write most of our code using plain `Foo` values, and afterwards use higher-order functions involving `Functor`, `Applicative`, `Monad` and `do` notation to take care of the error propagation. In other words, there's no need to pattern-match on `Left`/`Right` over and over and over (which can be a common frustration when these sorts of ideas crop up in OO languages). If you want to *enforce* your invariants, you can define your datatypes in a module which exports the type and smart constructors, but doesn't export the normal data constructors, e.g. in my example I could do `module Bar (Foo(), mkFoo) where ...`. That prevents any other modules from using `MkFoo`, they have to use `mkFoo` instead. Unfortunately this also prevents people from using `MkFoo` in patterns, so we should provide functions like `getS`, `getX`, `getY`, etc. if necessary (hopefully with better names!). As an aside: I don't like restricting other modules *completely*, so I tend to write definitions like this in a module called something like `MyThing.Foo.Internal` which exports everything; then I'll write another module `MyThing.Foo` which imports from `.Internal` but doesn't export the constructor. This way, the invariants will be enforced when "used as intended", but those few people who might need to fiddle with the internals are free to do so, but it's up to them to maintain (or give up on) the invariants.
Compilers are far worse than humans at optimization. If there is perceived net benefit to performance above what compilers can attain, humans need to do the optimization. For data structures implementation, the balance is skewed towards manual optimization. This should be obvious enough. There may well be a particular compiler bug or cheaply corrigible oversight that prevents a piece of program from being optimized, and our case in `containers` could be such. But: in general I don't know of any compiler which enables us to write naive code for data structures implementation, when the implementation is for a commonly used library.
For context, here's the fix in question: https://github.com/haskell/containers/commit/79b6aaa75d5a8d6d60466e530de1da6ce6c2f9f6 Where `fromAscList` is using `fromAscListWithKey`, for which the implementation is: https://github.com/haskell/containers/blob/c027280354d6c13044ec6d8b0f2a5aa6de77867b/Data/Map/Internal.hs#L3447
I would avoid calling the `main` function directly (as you do with the `else main` at the end), since it's putting unnecessary constraints on your recursion: i.e. your recursive call is forced to have type `IO ()`, because that's the type that GHC expects `main` to have, and you're using the same value for both. Try renaming your definition, and making a new `main` which references it. For example: myLoop = do putStr "input: " input &lt;- getLine let toInt = (read input):: Int if input == "exit" then putStrLn "print the array somehow" else myLoop main :: IO () main = do something with myLoop here Now you're free to change `myLoop` however you like, including its type, since GHC no longer cares if it has type `IO ()`. In particular, you can turn it into something that works more nicely with recursion, like a function (e.g. `myLoop x = ...`)
Can you not constrain it to `MonadIO`, thus generalising the run functions a bit?
Is that true even if you've turned on `AllowAmbiguousTypes`?
Let's try tackling this from a different point of view. What does it mean to "save" stuff in a list in Haskell? *(FYI, lists and arrays are different, and lists are the norm in Haskell)*. Haskell is an immutable language, so you can't just stuff a new value into an existing data structure. Instead, when we want to insert data into a data structure, we take the old one, and we create a new one that's a lot like the old one, except that we added another element while we were creating it. So what's the type of the function we need to write for this purpose? prepend :: Int -&gt; [Int] -&gt; [Int] `prepend` is going to take one element, and an old list, and create a new list that's the same as the old one, but with that element existing at the beginning. Linked lists are *really* good at this, because you don't even have to recreate the old list; you can just make a linked list that links back to the old list. This is done with the "(:)" operator. prepend x xs = x : xs Since this function is really just a wrapper around `(:)`, I'll just use `(:)` instead of it. So that's how you modify a list. Now your recursive code has to know what list to modify. As I said, you don't modify a list in place. You have to pass it along. So when the line isn't `"exit"`, you need to parse the number from the line with `read`, then do the loop again but with the new element prepended to the old list. loop :: [Int] -&gt; IO () loop xs = do line &lt;- getLine if line == "exit" then putStrLn (show (sort xs)) else loop ((read line) : xs)
Here's a more general tip for converting imperative thinking into functional thinking. The code you might want to write in Python is: # initialize state, loop condition condition = True numbers = [] # collect input while condition: print("Enter a number: ") input = readline() try: number = int(input) numbers.push(number) rescue ValueError: pass condition = input != "quit" # Print outputs: output_list = sorted(numbers) for n in output_list: print(n) We have a `numbers` array that we're using as mutable state, and a mutable variable `condition` to determine whether or not we need to keep looping. Let's break these into their own problems, solve them independently, and use the two solutions to solve the bigger problem. # Looping So you want to do a `while` loop in Haskell. Haskell doesn't have loops, and instead uses recursion. Basically, we write out what we want to happen in a loop iteration. Then we test our condition. If the condition is true, we call the function again. getInputs :: IO () getInputs = do input &lt;- readLine case input of "quit" -&gt; pure () x -&gt; getInputs We test the input. If it is `"quit"`, then we evaluate to `pure ()`. Otherwise, we recurse, getting more inputs. # Mutable State We can mimic mutable state by taking the state as a parameter to the function, and passing a new state value when we recurse. So let's pass our collected inputs into the looping function. getInputs2 :: IO () getInputs2 = helper [] helper :: [String] -&gt; IO () helper pastInputs = do input &lt;- readLine case input of "quit" -&gt; pure () x -&gt; helper (x : pastInputs) We start off our function by calling the helper function with the *initial* state: an empty list. When we recurse, we add the new input to the front of the `pastInputs` list. We're still not returning anything, though. Since we have access to the `pastInputs` list, we can return that instead of `()`. So the first iteration that works is: getInputs3 :: IO [String] getInputs3 = helper [] helper :: [String] -&gt; IO [String] helper pastInputs = do input &lt;- readLine case input of "quit" -&gt; pure pastInputs x -&gt; helper (x : pastInputs) When `input` is `"quit"`, we return the inputs that have been passed in so far. Unfortunately, this returns the list in reverse order, so Prelude&gt; getInputs3 "hello" "world" "foobar" "quit" ["foobar", "world", "hello"] is our output.
So it basically makes the lib user's job easier, but there's redundancy in its implementation — or maybe the code was generated?
Thank you for this context. Wouldn't a 'SPECIALIZE' pragma for `fromAscListWithKey` with `(\_ a _ -&gt;)` work?
Cool stuff! Two possible optimizations: * In `pfCell` you use `(!!)`, which has linear complexity. Since you're using it within a loop, that makes the loop at least quadratic. If you wanted, you could improve performance by using one of `zipWith` family of functions, something like: pairTerm i strandi k strandk = boltz (baseEnergy strandi strandk) * (if k-i &gt; 2 then arr ! (i+1, k-1) else 1) * (if j-k &gt; 1 then arr ! (k+1, j) else 1) pairTerms = zipWith (pairTerm i strandi) [i+1..j] strandks strandi:strandks = drop i strand You can push this even further by doing a `zipWith` in the definition of `arr` in `partitionFunction` as well: len = length strand arr = array ((0,0), (len - 1, len - 1)) $ do (i, strandis) &lt;- zip [0..] (tails strand) j &lt;- [i..len-1] return ((i,j), pfCell i j strandis arr) pfCell i j strandis arr | i == j = 1 | otherwise = sum pairTerms + slipTerm where slipTerm = arr ! (i + 1, j) pairTerm i strandi k strandk = boltz (baseEnergy strandi strandk) * (if k-i &gt; 2 then arr ! (i+1, k-1) else 1) * (if j-k &gt; 1 then arr ! (k+1, j) else 1) pairTerms = zipWith (pairTerm i strandi) [i+1..j] strandks strandi : strandks = strandis * If you find yourself using `sampleStructure` on the same input multiple times, the [alias method](https://en.wikipedia.org/wiki/Alias_method) is a pretty cool way of doing O(1) weighted selection (after a O(n log n) startup cost). That said, I have no idea if or where your code's truly spending time working, so these may both be premature optimizations. And both of these would almost certainly be harder to follow (especially for someone new to Haskell), so if the goal of your code is didactic, then these changes would be counterproductive. As it is, your post is really easy to read and follow! 
... Why does that matter?
Accidentally clicked 'delete'. Thanks for the responses.
There is this too: http://www.scs.stanford.edu/16wi-cs240h/slides/
In addition to the rest of the advice in this thread, you could use `whileJust` from the `monad-loops` package to simplify things a bit: import Control.Monad.Loops (whileJust) main :: IO () main = do whileJust handleInput return &gt;&gt;= putStrLn . show where handleInput = do putStr "input: " value &lt;- getLine return $ case value of "exit" -&gt; Nothing x -&gt; Just x 
Assuming this is your blog, could you use a monospace font (preferably `monospace` so the user's browser setting is respected) for the code instead of a proportional font?
Thanks, yep; I'm imagining that I could use that library, and then write `toJson` Aeson instances.
This is definitely the solution that I like the most and that resonates more with me. Using patterns + guards makes the whole thing even nicer I believe. I already started to refactor some portions of my code and it works like a charm. Thanks a lot!
Another solution without Maybe and without case. main = do result &lt;- loop [] use result use = mapM_ $ putStrLn . show loop accum = do line &lt;- getLine process accum line process accum line | line == "exit" = return accum | otherwise = loop (accum ++ (return . read) line :: [Int]) Note: Poor error handling. Sorting could happen in "use". Edit: Formatting Please comment and criticize!
Nice, I was always baffled by how tertiary structure was determined, this seconardy structure is a nice start.
It's an extension for making point*-free* code pretty. It's not point*less*. ;)
Great writeup, thanks. I never knew I was interested in RNA folding!
hey, thanks for the share!
Thanks for the spending the time to write this up! I assume of the runtime is spent filling the array, which should be O(n^3 ) though if I am not doing the indexing correctly it could be far worse than that. I will try this out.
That breaks down for any `IO` action of type `a -&gt; IO b` because then you don't have a good way to enforce that the action is fed an `a` before trying to read off the `b` that it emits
Is there a good reason for using `Char` to represent RNA bases instead of a bespoke enum `data Base = A | U | G | C`?
Well, yes. You'd want to have something which could actually enforce invariants. Such as: data Main a = Done a | Do Request (Response -&gt; Main a) It doesn't rely on lazy lists anymore, doesn't get out of sync, and is composable - trivially so, even.
Sorry, I meant code examples of using your library. A simple Yesod or Spock demo site or something like that.
All optimizations in all compilers that I know of were written by a human, and can be applied manually
You gave the answer to me, but it still took me WAY too much to do this. :X import Data.List import Text.Read main :: IO () main = do inputs &lt;- getUserInputs putStrLn ("sorted sequence " ++ show (sort inputs)) parseInput :: String -&gt; Maybe Int parseInput input = if input == "exit" then Nothing else (readMaybe input):: Maybe Int getUserInputs :: IO [Int] getUserInputs = do putStr "input:" input &lt;- getLine case parseInput input of Nothing -&gt; return [] Just anInt -&gt; do moreinputs &lt;- getUserInputs return (anInt : moreinputs)
You're welcome.
`return []` is returned when `exit` is parsed. Every time a number was parsed before, `anInt` was prepended to the list of parsed inputs. Try to step through the code by hand for some example inputs: 1 2 exit 1: When parsing the first input, `1`, we enter the `Just anInt`-case. `anInt` has the value `1` and `getUserInputs` is called again. 2: This is the first recursive call of `getUserInputs`, it reads the input `2`, enters the `Just anInt`-case again and this time `anInt` has the value `2`. `getUserInputs` is called again. 3: The third recursive call. This time `exit` is read, we enter the `Nothing`-case and the function returns an `IO []`. 2: Now the second call of the function can continue, prepending its `anInt` to the returned `[]`. It returns `IO [2]`. 1: Finally the top-level `getUserInputs` can continue and prepend its `1` to the list returned by the recursive calls. It returns `IO [1,2]` Does this help? About the second half of your post: The last line of the `Just`-case would never be reached if there wasn't the `Nothing`-case which does not recursively call itself again. The recursion loop continues until the `Nothing`-case is entered. The function is supposed to keep reading user input, until `exit` is entered. You see the correspondence?
As far as I know, you can't use `SPECIALIZE` like that.
Or have a look at [the more recent intro](http://www.atamo.com/articles/free-monads-wont-detox-your-colon/)
Another option, if we committed to targeting GHC exclusively, would be to make a primitive `fromAscListWith#` take a function producing `(# v #)`. This would certainly enhance code reuse, but we're not planning to do that anytime soon.
Oh, I see it now. I think I've clicked link to a function docs and it led me to one of the old named modules and 404. Edit: or maybe it was [this](http://hackage.haskell.org/package/containers-0.5.9.1/docs/Data-Map-Strict.html#g:12) link 
That's a proper documentation bug. Whoops! I'll try to fix it soon. Edit: bug fixed. Thanks.
I started using `(# #)` and `#` in general for this reason. Even with experience getting all the bang patterns right in some functions was too onerous. Just knowing it's correct by construction is much better.
I don't really understand the data structure: data Structure = Paired Char Char (Structure, Structure) | Slip Char Structure | None deriving Show What does it try to express? I don't understand the (Structure, Structure) part. It looks like a tree, but the branches will not be separated, there are a lot of junctions. Why not using a simple list of bases, with a reference to another base when there is a bond? data Structure = [(Char, Maybe Int)] 
Thanks, it really becomes clearer with GADT syntax.
Naw, I get what you mean. I started off in C so I've had a healthy distaste for languages that fixate on layout. It takes a bit of time to get used to, but Haskell's layout is worth it - you can see the 'structure' of code much easier. 
Eh, I don't think it's that necessary. I learned lambda calculus after learning basic Haskell and I just thought it was the same thing, just had different ways of representing (and not that different really.)
OK, yeah, this is what I usually do... I just thought, maybe it's not necessary after all. Thanks!
It may not always good to share subexpressions. It's a tradeoff between CPU and memory. The subexpression takes time to calculate and memory to store. If it takes a lot of memory to store but is fast to calculate, it may be better to calculate it twice but I don't know how the compiler can tell.
&gt; but I don't know how the compiler can tell. Generally? By solving the halting problem :) It could try to measure, at runtime, how long it took to actually execute, and how much it had allocated to do so. It's a bit more nuanced than what was said though, the memory use isn't really the size of the resulting expression - but its size relative to the input - if the input may be GC'd once we store the result. If you have a `sum :: SomeBigData -&gt; Int` and you use it multiple times, you likely want to memoize it more than some other `Int` when it lets you release `SomeBigData`. Giving the programmer full control of this is good enough though and so much simpler.
If you pattern match the empty list, you might as well pattern match the nonempty list. case xs of [] -&gt; ... _ -&gt; let x = head xs in ... -- Why do this (x:xs') -&gt; ... -- When you could do this? It's just a lot clearer, less error prone, and more sound. Plus, the compiler can give you exhaustiveness warnings, but it won't warn you about improperly used partial functions.
They are partial functions because they don't cover the entire set of values they accept, as opposed to total functions. There is no relationship here with curried functions since we aren't talking about partial application. If for some reason that isn't clear take a look at [the first 2 diagrams on the wikipedia page](https://en.wikipedia.org/wiki/Partial_function). Now, you don't want to use those functions because they are not safe and may cause failure with "bad input". The reason why they are still in `Prelude` is because they were left in there that so beginners like you can write simple list programs without having to deal with things like Maybe, Either, etc. It's enough for you to forget one time to pattern match against the empty list before using any of those functions for your program to have the runtime exception feature built in :) Also if you're already pattern matching on the empty list there isn't much typing you're saving yourself from just using the safe alternatives. Not always, since I'm not aware of there being in base safe alternatives for other than `head`, which has quite the unfortunate name `listToMaybe`, so you'll end up writing your own safe version in a few instances.
This leads into [Notions of Computations as Monoids](https://arxiv.org/abs/1406.4823), which has quickly become one of my favorite papers, especially in tandem with [this paper](https://lirias.kuleuven.be/bitstream/123456789/499951/1/main.pdf) that explores Cayley representations to improve efficiency of not just monoids, but also near-semirings.
&gt; Common Subexpression Elimination in a Lazy Functional Language pdf : https://kar.kent.ac.uk/21455/1/Common_Subexpression_Elimination_in_a_Lazy.pdf
You could write your function using [`memoFix`](https://hackage.haskell.org/package/memoize-0.8.1/docs/Data-Function-Memoize.html#v:memoFix).
Indeed, Dan does touch on that in the article too: &gt; I don't know if free arrows are anywhere near as useful as free monads, but I hope I've successfully illustrated one application. Note that because arrow composition is essentially list concatenation it may be more efficient to use a version of Hughes lists. This is what the Cayley representation is about in the monoid notions paper. But it's easier to see the naive list version first.
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/programming] [An introduction to data processing using Haskell and the Grakn knowledge graph (x-post from \/r\/haskell)](https://np.reddit.com/r/programming/comments/5n5hwq/an_introduction_to_data_processing_using_haskell/) [](#footer)*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))* [](#bot)
I don't want to spoil it, but I also remember being stuck on the first steps for a very long time, back when. So here goes. Looking at `sign`, I notice the result is a `b`. Writing up the arguments by a shorthand denoting them by their types: sign ab cd da c = ... I need `...` to be a `b` for the signature to match, and I can only get this by applying the `ab` to "something," since it has type `(a -&gt; b)` sign ab cd da c = ab ... I don't have an `a` for `ab` to transform to a `b`, but I can get one by aplying `da` to "something" sign ab cd da c = ab (da ...) etc. I hope this helps.
In my opinion, one of the major selling points of functional programming, and Haskell in particular, is that your functions are side-effect free. Throwing an exception and exiting the program is a pretty nasty side-effect. It effectively means every time you call 'head' or 'tail', you need to analyze the whole program to make sure the inputs are always valid. However, if you only use total functions (functions which always compute a value for all inputs), you never need to do this analysis, you'll know with 100% confidence that it can't fail.
Here's the exact error message GHC gives you when you call `head` on an empty list: *** Exception: Prelude.head: empty list To use a metaphor from the OO world, every time you use a partial function, you have the equivalent of a null pointer exception waiting to happen. Six months later when you see the above error, you will have no clue where to look. The only thing you know is it has something to do with calling `head`. You probably called `head` in many different places and you have no idea which one is the culprit. It's an awful feeling, the same feeling I used to get when I got a null pointer exception in my Java programs. The best approach is to not use these functions. Use one of the suite of non-partial versions [defined in Conotrol.Error.Safe](http://hackage.haskell.org/package/errors-2.1.3/docs/Control-Error-Safe.html). Very occasionally I encounter a situation where I just don't want to handle the error in that spot. In that case, a nice trick I've learned is that instead of calling `head`, you should use a partial pattern match like this: foo [x] = ... or where [x] = ... or do [x] &lt;- ... This is still not good. We'd much rather remove the potential for it to crash altogether. But it is VASTLY better than head in real-world programs because when this fails at runtime GHC will tell you the exact location in your code where the pattern match failed.
Can't pattern match the `last` item. I suppose I shall reverse the list and then work on it in that case?
The answers so far aren't bad, but I think they don't yet convey a key point of my development process. I don't find it hard to make sure I meet the preconditions of a partial function. But one of the places where the types help a lot is when I revisit the code later. *Have* I actually checked that the list is non-empty here? If I'm passing around a non-empty list, then clearly yes! If I'm passing around a regular list, I have to go and check. In truth, if the check is a line apart (and likely to stay that way) it's not likely to bite you. That's most relevant for init/last, though - for head/tail you might as well move the grab of the piece you need into the pattern match (it often reads better, and you'll get exhaustiveness checking).
The key realization here is that in Haskell it is impossible to construct a value for an unconstrained type variable out of thin air. (Well, `undefined` lets you do this, but it should be fairly obvious that that's not an acceptable answer to an exercise like this.) To solve these kinds of problems it's really useful to think about positive and negative position. They are defined as follows: Positive position: An output, or something that this function has to create Negative position: An input, or something that this function is given The job of the function is to create the thing that is its return value. And since Haskell is a pure language, the only things that it can use to do this are the things in negative position. Let's look at some type signatures with this idea in mind: foo :: a -&gt; b foo :: - -&gt; + This is fairly obvious from the above explanation. Now a slightly more complex example: bar :: (a -&gt; b) -&gt; c In this example, the thing in the negative position is itself a function `a -&gt; b`. Someone else supplies `bar` with a function. But in order for `bar` to use that function, it has to supply it with an `a`. This means that `bar` has to create an `a'...i.e. `a` is actually in the positive position. Since the whole thing is negated it switches: bar :: (a -&gt; b) -&gt; c bar :: (+ -&gt; -) -&gt; + So signs flip back and forth like we would expect from math. Now let's look at your example: sign :: (+ -&gt; -) -&gt; (+ -&gt; -) -&gt; (+ -&gt; -) -&gt; - -&gt; + sign :: (a -&gt; b) -&gt; (c -&gt; d) -&gt; (d -&gt; a) -&gt; c -&gt; b With this calculus for figure out the positive/negative position for everything in the type signature you can get a better idea for the flow of data in your function and piece together the puzzle to return the thing that you need to return. We see that the `b` return value is a positive, so we need to find another place in our function signature where there is a `b` that is negative. Then ask yourself, "how do I get that thing?" Continue repeating this reasoning until you have supplied all the positive values you need.
The thing is, "things are garbage collected immediately" a lot of the time.
Also, *parts* of the produced expression may be able to be freed at various times relative to the forcing of other parts of the produced expression. Not a simple thing at all.
Java and Rust both started out with green threads but later dropped them. Java's reasons are briefly explained [here](http://docs.oracle.com/cd/E19455-01/806-3461/6jck06gqe/index.html), Rust's reasons are explained in detail [here](https://github.com/aturon/rfcs/blob/remove-runtime/active/0000-remove-runtime.md).
If it's not pure, then it's nothing like Haskell. Why does everyone ignore Haskell's most important features?
I agree, however we don't have "side effect free" just for the sake of it. What we want is referential transparency, which is still ok with partial function. Given the same input, your function always crashes (or not).
It might be more clear if it were `Paired Char Structure Char Structure`, which preserves the order of the sequence in the type. So one `Structure` for AGUG would be `Paired 'A' (Slip 'G' None) 'U' (Slip 'G' None)`. That way, it looks like a parse tree for the sequence.
For sure. With a NPE at least Java will usually tell you exactly what line of code it happened on. With exceptions in Haskell you get no position information. Although according to https://wiki.haskell.org/Debugging it looks like GHC has some support for stack traces now?
Essentially this is saying, just like functors give rise to free monads, strong profunctors give rise to free arrows. Am I right? 
Creator of Glance here, happy to answer any questions.
Sure. But unintentionally crashing is something I try to avoid. Sometimes, crashing is the best thing to do (bad configuration files, out of memory, etc...), but I want to be explicit about that. Crashing by mistake should never happen in a well-constructed program. With only total functions, you have that guarantee.
Think of an infinite list for instance. Maybe both use sites just take a bunch of elements from the front and perform a fold, which could be reduced to an imperative loop. But if you share the expression, now each list element generated during the first pass will stay in the memory.
This is similar to diagrams generated by my compiler [(you can see an example here)](http://hackerfoo.com/posts/poprc-runtime-operation-part3.html) to visualize the internal graph representation used for reduction.
For interactivity, an editor is a lot of work, but there are some very nice features that would be easier, like navigating by expanding and folding sections (ie function names &lt;-&gt; function bodies) or even just tooltips with helpful information like types.
The GADT syntax is a little bit misleading, since it's not a GADT (meaning the constructors don't induce any type equalities). It's not a problem as long as you aware that GADT syntax is only used to pretty up existentially quantified type variables. Also it'd be nice if we could write data FreeA p a b = PureP (a -&gt; b) | exists x. p a x :- FreeA p x b
Thank you very much for explaining! I get it now.
Hi, This looks wonderful. As a colorblind person, would it be possible to optionally add patterns in addition to colors to make things clearer ?
I wonder if this is because of the complexity of many of your projects, the high barrier to entry scaring away those inexperienced who would try to "fix" it. I think this will apply to Haskell as a whole, so it's a good "hack" I'd be interested to see a case study of this method applied in different communities
That package could profit a lot from a few small examples in the haddocks.
Or wrap they can wrap their function with `memo`: nut' = memo nut from the `MemoTrie` package: https://hackage.haskell.org/package/MemoTrie-0.6.7/docs/Data-MemoTrie.html#v:memo
I like this one, if you have spare time and know of any examples other than functions and variables, or can refer to another resource that makes comparisons like this, please post.
I really like the style of the graphs. Someone should wirte a real-time interpreter which interprets haskcore expressions and animates the graph embedding with a spring force algorithm. Projected on a video projector this would make a great audio/visual live -performance.
I'd love to hear your thoughts on visualizing polymorphism and abstract data types. Both are pretty critical for abstraction and modular programming and are something that other visual languages don't seem to do well.
I have tried to write a small Yesod app demonstrating the use of the library. The code is [here](https://gist.github.com/sarthakbagaria/c08c0d7b84e1165760bb429a4064cfff). I haven't had the time to test it. It may not work at the moment but I hope it will give an idea of how the library can be used. I will try to complete the example and put it the main repo/readme file when I get some time.
Mostly just derp.
Similar to lambdageek - could you imagine visualizing dependent types? I know they're not part of Haskell yet, but I'm curious what your thoughts are.
I'm not getting the same result with `-ddump-simpl`, actually what I'm seeing are just a bunch of ellipsis, that's why I've used `-ddump-ds` /tmp λ. stack ghc -- -ddump-simpl -dsuppress-all -O2 test.hs Run from outside a project, using implicit global project config Using resolver: lts-7.15 from implicit global project's config file: /home/x/.stack/global-project/stack.yaml [1 of 1] Compiling Main ( test.hs, test.o ) ==================== Tidy Core ==================== Result size of Tidy Core = {terms: 22, types: 31, coercions: 9} -- RHS size: {terms: 4, types: 7, coercions: 0} main1 main1 = \ s_aUy -&gt; (# s_aUy, () #) -- RHS size: {terms: 1, types: 0, coercions: 3} main main = main1 `cast` ... -- RHS size: {terms: 2, types: 1, coercions: 3} main2 main2 = runMainIO1 (main1 `cast` ...) -- RHS size: {terms: 1, types: 0, coercions: 3} main main = main2 `cast` ... -- RHS size: {terms: 2, types: 0, coercions: 0} $trModule2 $trModule2 = TrNameS "main"# -- RHS size: {terms: 2, types: 0, coercions: 0} $trModule1 $trModule1 = TrNameS "Main"# -- RHS size: {terms: 3, types: 0, coercions: 0} $trModule $trModule = Module $trModule2 $trModule1 For reference lts-7.15 has ghc 8.0.1
The only other thing I can think of to add to the list is insisting on reasoning in applicative order. Or, another related idea is: denotational semantics are actually useful, which surprises a lot of people, but it also takes some effort to truly appreciate, so I'm not sure if that counts as something that people can just be told to save them pain in learning.
One of the [todos](https://github.com/rgleichman/glance/blob/master/todo.md) is to use different line styles (dashed, dotted, etc.) in addition to the randomized colors. Something similar could be done for nested icon borders. Is this similar to what you have in mind?
I think the default should be what it is right now. You can have a special purpose numeric package for banking or any number critical application that does total divisions and such.
Yes absolutely. As a rule of thumb, if you're relying solely on colors to discriminate information, you're leaving a significant part of the (mostly male) population utterly confused !
For reference, here’s a fantabulous talk he did about 4-dimensional geometry at 33c3: https://www.youtube.com/watch?v=ct0_g1amEpw
Btw, this is the smallest I could make the example program: main = let inf = inf &gt;&gt; inf in inf &gt;&gt; inf Compiled without optimizations, it will eat up all your memory. Compiled with optimizations, it'll run in constant space. (Tested with ghc-7.10.3. Maybe later versions of GHC can get it right even without optimizations.)
The reason you're not seeing the other definitions is because GHC optimized away everything else away as dead code. GHC deduced at compile time that your program is equivalent to `return ()`. That's why you need to instead define a module which exports the pure values so that GHC knows that it needs to keep them around
This sounds similar to Pieter Hintjens's philosophy and [C4](https://rfc.zeromq.org/spec:42/C4/) more formally. (Indeed, I even see a hat tip to Hintjens at the bottom)
You can do that using a sum type `Table a = Table1 (HashMap k a)| Table2 (Map k a)` and a smart constructor `makeTable :: Int -&gt; [(k,a)] -&gt; Table` that creates it accordingly.
No IDE/tool currently picks up a function's documentation which means we only get the types in the auto complete lists.
In some sense the nature of the kind of code that I write helps. When you have an underlying mathematical truth you can actually answer whether or not a chunk of code is "correct" in a way that is otherwise quite difficult.
Many warts of this kind are collected in [the `haskell-issues` repo](https://github.com/aelve/haskell-issues/issues).
This reminds me of the bash fork bomb: :(){ :|:&amp; };:
I'm on a brand new team at Target that was started from the ground up using Haskell. Here's my view: * Hiring the initial developers went really well, and we still get inbound queries now even though we aren't hiring more Haskellers at the moment. Lots of high quality candidates. * We have one highly experienced data engineer who got to writing Haskell code quickly, but I can't generalize much from that—I'm sure he wouldn't have had trouble picking up pretty much anything. We're also teaching Haskell to some people with operations research backgrounds but little programming experience, but they haven't had to use it yet so I don't know how well it's going. * There have certainly been libraries we didn't have—for example, we're just starting to write our own bindings to Google's [or-tools] package. Writing bindings to a C library is quite doable. We also had issues talking to Hive which we had to work around. We're writing a lot of our own optimization code and we haven't needed anything too exotic so far. The library situation is definitely not *ideal*, but it's totally workable. [or-tools]: https://developers.google.com/optimization/
&gt; What kind of hiring experiences have you had? Apparently this comes up a lot, and I've heard that the quality of applicants tends to be high. Has anyone had any bad experiences here? If you're a huge company and need to hire a thousand people, then Haskell might not be for you. But if your needs are in the realm of a few dozen or less, you'll be fine. And the applicants you get will be MUCH better on average than what you would get if you were not using Haskell. Here's a comment I wrote a few months ago talking about this: https://www.reddit.com/r/haskell/comments/54umkh/haskell_for_large_codebases_looking_for_first/d85cy1e/ &gt; For groups that hired non-Haskellers, or Haskell-interested imperative devs, how long did it take to bootstrap them into your environment? I've been helping a colleague learn Haskell, so am convinced it's not an insurmountable task, but am curious to hear from people with a larger sample size. I don't have direct experience with this, but I have worked with someone who learned Haskell on the job before I joined the company. I think I came in when they had around a year or two of Haskell experience, and I thought they were great. Having an experienced Haskell programmer around to answer questions can GREATLY speed up this process. Assuming you have that I think a lot of it boils down to personal aptitude and motivation. Check back with me in a year or two though. I currently work with a number of people who are at various stages of learning Haskell. &gt; Depending on the kind of software you are building, have you had any issues with finding the libraries you needed? Have you found yourselves writing missing niches? You absolutely need to be willing to fill in missing libraries. But this has improved a LOT in the past 7 or 8 years, and it's going to continue improving more. There are more than 10,000 packages on hackage. Odds are very good that the majority of libraries you'll need for common tasks already exist. More important than writing missing libraries is you definitely need to be willing to fix problems and help maintain existing libraries. The community is still small relatively speaking, so we need all the help we can get. 
Hey, I think there's a bug in your code, or at least it's something that looks like one: (roll, newgen) = randomR (0, arr ! (i, j)) gen -- if roll &gt;= cumulative probability switch canidates, else not checkCase current candidate = if roll &gt;= (fst candidate) then (snd candidate) newgen else current struct = foldl' checkCase None allCases What did you intend to do here? It at least looks like a weird way of specifying what you're doing. I'm pretty sure it's equivalent to: (roll, newgen) = randomR (0, arr ! (i, j)) gen struct = last . (None :) . map (\x -&gt; snd x newgen) $ filter ((roll &gt;=) . fst) allCases Which is going to be more efficient, since this way you're not forcing a whole bunch of things that you're then throwing away, but I suspect you didn't want to throw them away instead. I don't actually understand anything you are doing though, so that's just a hunch. EDIT: Actually this would be cleaner: (roll, newgen) = randomR (0, arr ! (i, j)) gen struct = last (const None : [second | (first, second) &lt;- allCases, first &gt;= roll]) newgen
* For offline documentation, I use `stack build --haddock --haddock-deps` to ensure `haddock` is run over all dependencies. And we also have `stack hoogle` for offline searching. * "Docs should have to build" is probably not a good constraint. There are packages that need some extra config (like 3rd-party deps, or switching to ghcjs) to build, and currently Hackage does not provide means for such config. Allowing package uploaders to manually upload docs is a more pragmatic choice.
&gt; it seems to me as though it is unlikely that forcing a tuple constructor is ever going to help avoid thunk buildup It does help. I faced an issue recently where thunk buildup due to tuple constructor was accounting for a few gigabytes of memory. But that being said, I'm not sure why the functor instance for (,) a is not using lazy pattern matching. If there is a good reason for keeping things strict, that should probably be documented next to the functor instance. It's odd that the functor instance does strict pattern matching while the bifunctor instance doesn't.
New wart: this repo isn't owned by the `haskell` GitHub user and is thus basically impossible to find
[Here's the `Functor` instance for tuples](https://hackage.haskell.org/package/base-4.9.0.0/docs/src/GHC.Base.html#line-648) instance Functor ((,) a) where fmap f (x,y) = (x, f y) It's not strict on the elements of the tuple. It's strict on the *structure* of the tuple, because there's no way to map the function over the second element without doing so.
Awesome! Be sure to add that link to your repo README.md :)
I believe he was referring to the structure not being lazily matched. See https://wiki.haskell.org/Lazy_pattern_match
Would there really be any benefit this was as opposed to wrapping an optimized C library?
Yes, `undefined` throws an exception as well.
Your `forMl_` works well for Maybe but not for anything else, so you may just as well use the specialized `forM_Maybe` (which doesn't need `pure ()` for the `Just` case at all), as Michael suggests. import Data.List forMl_ :: (Foldable t, Applicative f) =&gt; t a -&gt; (a -&gt; f ()) -&gt; f () forMl_ as f = foldl' (\act a -&gt; act *&gt; f a) (pure ()) as main = forMl_ [1..10^6] do_stuff {-# NOINLINE do_stuff #-} do_stuff :: Int -&gt; IO () do_stuff _ = return () 56,687,200 bytes allocated in the heap 87,785,608 bytes copied during GC 23,300,576 bytes maximum residency (6 sample(s)) 378,520 bytes maximum slop 47 MB total memory in use (0 MB lost due to fragmentation) What you really need here is not a left fold[*], but a right fold which can detect the last element of a container and make a tail call at that point. Like this: forM__ :: (Monad m, Traversable t) =&gt; t a -&gt; (a -&gt; m ()) -&gt; m () forM__ a f = let l = toList a b = (False &lt;$ tail l) ++ [True] in foldr (\(x,b) r -&gt; if b then f x else f x &gt;&gt; r) (return ()) (zip l b) [*] except for snoc-like (left-associated) containers, where you do need a left fold
It is different — because my version also tail-calls `&gt;&gt;`. Just try the example program above (the one doing `forMl_ [1..10^6] do_stuff`) with your version and mine, and see the difference in the memory footprint.
Like /u/roche says, I don't think specialization pragmas are the silver bullet; for much the same reason that inlining isn't the silver bullet. I think that what really needs to happen here is that we need to rely less on the fact that `IO` (kinda sorta in theory but actually not really) fits the monad interface, and instead compile it differently. Ignoring all the chicanery about `realWorld` tokens, and ignoring the very real effects that primitive functions can have, if we just look at the guts of what `IO` says it does on the tin, it's very special. It's special because it contains exactly one element, so we don't need to worry about serializing the simulation of nondeterminism (as with the list monad or `LogicT`). It's also special because it doesn't need to keep any memos on the side (like the `Writer` monad, etc), those are stored implicitly throughout the state of the registers, CPU modes, heat of the universe, etc. It's also special because `IO` statements have a built-in representation on the machine: they're stack frames! And so on. There are a whole bunch of things that make `IO` special and unique, even if we completely ignore side effects. The monad interface doesn't give us enough laws to really take advantage of all that specialness, which is why when we humans look at the code we see why it "shouldn't" take up so much memory (we're implicitly using those extra equations that come from the special nature of `IO`) but then GHC balks. This whole class of space leaks around monadic loops is a recurring one and has been for some time. The clever folks working on GHC manage to push it around and make it show up in different places; but ultimately I think we really need to separate out very-special monads like `IO` (and `ST`, `Identity`, etc) and take to heart the fact that they're different from the majority of monads.
Results: benchmarking strict time 1.117 ms (1.077 ms .. 1.145 ms) 0.995 R² (0.993 R² .. 0.997 R²) mean 1.120 ms (1.103 ms .. 1.136 ms) std dev 58.77 μs (53.60 μs .. 65.93 μs) variance introduced by outliers: 42% (moderately inflated) benchmarking lazy time 2.318 ms (2.227 ms .. 2.417 ms) 0.990 R² (0.984 R² .. 0.998 R²) mean 2.315 ms (2.280 ms .. 2.356 ms) std dev 133.1 μs (108.3 μs .. 164.3 μs) variance introduced by outliers: 40% (moderately inflated) Code: module Main where import Prelude import Data.List import Criterion.Main main = defaultMain [ bench "strict" $ nf subjectS [1 .. 10 ^ 5] , bench "lazy" $ nf subjectL [1 .. 10 ^ 5] ] subjectL :: [Int] -&gt; Int subjectL = sum . mapL (+ 1) mapL :: (a -&gt; b) -&gt; [a] -&gt; [b] mapL f (x : xs) = f x : mapL f xs mapL _ [] = [] subjectS :: [Int] -&gt; Int subjectS = sumS . mapS (+ 1) {-# RULES "foldlS/mapS"[1] forall step init mapping list. foldlS step init (mapS mapping list) = foldlS (\l r -&gt; step l (mapping r)) init list #-} {-# NOINLINE[~2] mapS #-} mapS :: (a -&gt; b) -&gt; [a] -&gt; [b] mapS f (x : xs) = y `seq` ys `seq` y : ys where y = f x ys = mapS f xs mapS _ [] = [] {-# INLINE sumS #-} sumS :: Num a =&gt; [a] -&gt; a sumS = foldlS (+) 0 {-# NOINLINE[~2] foldlS #-} foldlS :: (b -&gt; a -&gt; b) -&gt; b -&gt; [a] -&gt; b foldlS = foldl' Your quote: &gt; When you say a bunch of things that are naive and wrong and say them without any doubt as though you are absolutely right it will start to get on people's nerves.
I do actually look at all the code that gets committed, and pushing things up to hackage is still kept down to a handful of people.
But the problem isn't specific to IO. What about this program? import Control.Monad.State import Control.Exception main :: IO () main = evaluate $ evalState worker () {-# NOINLINE worker #-} worker :: (Monad m) =&gt; m () worker = let loop = poll &gt;&gt; loop in loop poll :: (Monad m) =&gt; m a poll = return () &gt;&gt; poll 
Still, it's pretty tough to imagine a system without the empty list. What would it be? Empty lists at least allow the human brain be able to even write a program.
@ /u/erikd link is broken, has a stray 'd' in it.
But what about `Bifunctor`? `bimap id id /= id` there
&gt; Prefer property based testing over tests with a limited set of examples. May I urge you to **please** contribute to [this discussion](https://github.com/nick8325/quickcheck/issues/139) over at Quickcheck's Github repo? The community is in dire need of tutorials/articles/documentation around how to use Quickcheck (or other property based testing libraries) in real-life scenarios.
Thanks. Fixed now.
 &gt;* certain re-exported functions don’t appear in Haddock (hits especially preludes like protolude) This happens when re-exporting modules. When functions are explicitly re-exported haddock generates docs, but not when a whole module is. 
That discussion is about test stateful testing. Both I (in my person haskell hacking) and the team I work with try as much as possible to separate pure and impure code so that we can avoid stateful testing as much as possible. As soon as you remove state, property based testing becomes much easier. When we do need to test stateful stuff it usually ens up as testing with a limited set of examples. 
Additionally they are sometimes useful in scenarios where you are sure lists aren't empty. For example, 'group'/' groupBy' return lists of lists, where the inner lists are never empty, so you can write 'map head . group' and to get a list of single elements. A safe 'head' that returns 'Maybe' would make this a lot more tedious. But cases like these are few and far between. 
At Ambiata we use https://github.com/ambiata/mafia which makes using git submodules easy. It automatically checks out and discovers and cabal files, making them available as source deps. About 150 packages are managed in separate repos using this method.
intero kind of took it's place. Ironically, it currently doesn't even have type signatures in the autocomplete list.
&gt; If you're a huge company and need to hire a thousand people, then Haskell might not be for you. Personally, my untested assumption would be that a given project would take more developers to do in Java than in Haskell. My assumption is based on reports [like this one](https://web.archive.org/web/20160304022149/http://namcookanalytics.com/wp-content/uploads/2013/07/Function-Points-as-a-Universal-Software-Metric2013.pdf) (prog languages start around table 14.*) and others that show project cost is directly related to LoC (or "function points", in this paper). Haskell is more expressive than Java so I would expect to require less developers to do the same project. Does your experience bear that out at all?
I wrote some stuff about that once, maybe it's helpful to you. &lt;https://jaspervdj.be/posts/2015-03-13-practical-testing-in-haskell.html&gt;.
What if Haskell had a way to prevent sharing. If there were an alternative to `let` expressions, which inline the definition and locally suppress CSE optimizations, we could write mean = def xs = [1..1000000] in sum xs / length xs which could translate to mean = sum [1..1000000] / length [1..1000000] where the usual version with `let` is a commonly known space leak. Could the issue with monadic loops be solved with more fine grained control about which expressions get shared? And is this compatible with the rest of the language?
Writing it in Idris sounds like a good idea and it probably will reveal holes in Wayland when you start fleshing out the to be verified protocol.
I really, really hope that's not true. If the only viable "IDE" for Haskell is Emacs/Vi you're massively limiting the amount of people who will ever give the language a serious look.
intero is a separate thing that can be integrated in other editors. There's a partial implementation for VS Code for example
Or maybe do. Because commit access isn't the same thing as being able to put something into production without anybody reviewing it. Granting commit access could make contributors *more* likely to sanitize inputs because, as the article says, it alleviates concerns about the risk of code not being merged. Any commit can be reverted, in the worst case.
I don't understand purpose of this comment. The OP used phrase “partial functions (not curried functions)”, which suggests that he thinks that partial functions have something to do with currying. I explain here the terms. I assumed that he/she might got this idea by confusing “partial application” and “partial function”. 
Tests shouldn't touch a database. This is bad (or at least controversial) practice in OOP and FP. 
I just found this : http://blog.higher-order.com/blog/2016/04/02/a-comonad-of-graph-decompositions/ . It's in Scala however..
Are you referring to using mocks instead? What about integration tests? Edit: thanks phone for not helping.
I'm curious where you are getting these exercises. It looks like something from the riddle section of a newspaper.
But you had to do a whole bunch of bullshit to get the performance you lost back. Whereas without all that bullshit the lazy version is much faster. Which is my point. Rewrite rules will SOMETIMES save your ass, but in any complex situation they will probably fail which will destroy your performance and perhaps lead to nontermination if you ever use infinite structures. My simple example is a much much better indicator of the issues with strictness. You should not have to rely on rewrite rules for composibility of functions and adequate performance.
I love destroyallsoftware.com and I'll watch the video when I get into work, but my question is what about when you actually want to test something at a more "zoomed out" level then a unit such as integration tests ... surely they would use the db in some way (not that you are trying to directly test the db code)
Is there a "freer" arrow `FreerA` where we have instance Arrow (FreerA p) ... without any constraints on `p`?
I'm not sure I agree, but I'd like to see more discussion because I'm not sure I fully understand what you are saying. For instance, I've been writing these Servant APIs lately and they're mostly just thin interactions with the database. Thus, I can abstract out the db-interacting stuff from the actual handlers and there's usually nothing left. (I actually have no clue how to write tests for these APIs, by the way) I'd be curious to see what you mean: do you have examples of how you'd structure a web application with tests and the kinds of things you would test and those you wouldn't?
In addition to stichbury's reply: OWL is not well-suited for graph-structures. Because of its formal foundations and computational limitations it is in fact a more natural language for managing tree-shaped data instead. OWL also makes it hard to help validate consistency of data and ensure it is well-structured, and this is what knowledge graph applications require. We'll share with you more of a complete summary on our blog/website in the coming week(s)! Thanks!
What? More insults instead of apologies? I'm so surprised.
Is this analogous? x = let inf = inf ++ inf in inf ++ inf I certainly wouldn't be surprised if that ate all my memory!
I'm a little bit confused about the boundaries of your task. Are you possibly looking for something like [webdriver](https://hackage.haskell.org/package/webdriver), which will spin up a browser, perform actions, and assert that certain properties are met in the DOM? If you're not looking for something this high-level, I would suggest testing your JavaScript with JavaScript. Mocha and Jest are both good options in this space.
Thanks for the reply! So your ontology language is subject to a closed-world assumption like SHACL? 
This is the relevant github repository: https://github.com/typelead/eta And this is the blog post from the co-founder of Typelead (the company behind Eta) discussing about it: https://blog.typelead.com/https-blog-typelead-com-introducing-typelead-14c4a1bf2df#.lucxdd5ck
I think i got the easy one, but the second one?? what's this v ? sign :: (a -&gt; b) -&gt; (c -&gt; d) -&gt; (d -&gt; a) -&gt; c -&gt; b sign f1 f2 f3 x = f1 (f3 (f2 x)) sign2 :: ((a -&gt; b) -&gt; c -&gt; b -&gt; d) -&gt; ((v -&gt; c -&gt; b -&gt; d) -&gt; q) -&gt; (a -&gt; b) -&gt; q 
I'm afraid JS is the best I can do: https://cokmett.github.io/cokmett/
A quick Google search for "arbitrary instance markup" led me to this which might be relevant: http://hackage.haskell.org/package/blaze-markup-0.6.3.0/src/tests/Text/Blaze/Tests.hs
On one hand, you want your builds to be reproducible, on the other hand you want them to be flexible. That's what `stack` and `nix` are for: You have a `.cabal` file with min and max bounds (and a version range as broad as possible), and you have a `stack.yaml` to pin down a LTS, i.e. a set of versions, for reproducible builds. So: Outdated advice. Use bounds in your `.cabal file` and offer a `stack.yaml` (or several, vor different LTSs) for reproducibility. EDIT: Just realized that the blog refers to package management in general, not Haskell in particular. Hence, read: Outdated advice for the Haskell ecosystem.
This might help: * http://eta-lang.org/docs/html/faq.html#how-is-eta-different-from-frege * https://github.com/typelead/eta/issues/3
I don't understand why they used a different name for the language. To be considered Haskell, all it needs to do is be compliant with Haskell2010, which they have already gone beyond. EDIT: typos
Yep, the discussion in the issue was especially informative, thanks.
Imagine a number system without zero! /s
You are taking on too much responsibility if you think fallout from that approach is on the maintainer instead of the deployer.
I'd really like to have a tool that suggests/includes imports based on my project's dependencies list and missing identifiers that could be found in at least one of the libraries in the deps list.
&gt; The database accessing code should be so thin it's not worth testing. I sometimes end up having to write SQL directly instead of letting Haskell libraries do the "obvious" queries for me. The only way to test that my SQL is correct is to execute it against a real DB. It's not a big deal to have the test operate on an isolated, fresh database and wipe it in between tests. I've gotten a lot of value testing APIs this way so I disagree heavily with "never use a DB in test."
Probably to avoid confusion. Right now Haskell is just about synonymous with GHC. Having a different name means in theory you can google for questions specifically related to their implementation without it being confused with something about GHC. That said I kind of wish they had picked something other than Eta since that's a bit ambiguous.
OMG I've been looking for such function and couldn't find it and it also solved all the flickering! Thanks a ton!
&gt; Does your experience bear that out at all? Yes, I do think that Haskell allows you to do more with fewer developers. How many fewer I couldn't possibly say, and depends largely on the specifics of the project involved. But I already factored that into my above statement. Some projects aren't so much about creativity as a huge volume of grunt work. And in projects like that, if you needed 1000 Java programmers, you probably won't be able to do it with fewer than 500 Haskell programmers. You can't build the great pyramid with 50 insanely productive stone movers. You bump into the laws of physics and sheer scale very quickly. So if you need to hire 500 Haskell developers in a short amount of time...you're simply not going to be able to. The community isn't that big yet, and the logistics don't work out. But in the majority of situations you're likely to encounter I think the upper bound on the number of Haskell developers you can find is high enough so that it likely won't be an issue for you. The company I work for has hired something on the order of a dozen Haskell developers in the last 6 months or so, while maintaining a very high bar. We could have hired significantly more if we were willing to lower our bar a bit.