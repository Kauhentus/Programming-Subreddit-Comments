It may not be sexy, but date/time validation is something I've used QC-like property testing in other languages for, to great effect (and found myself wishing I had direct access to QC). Date/times are notoriously difficult to get right and QC takes a lot of the boilerplate out of testing them. Within Haskell, I've used it for validation of transactional queues (managing workflows, that kind of thing). But, there's a lot to be said for just using it on general function development, simply to catch edge cases you hadn't thought of. Anytime you're dealing with lists, there's usually an opportunity for some level of QC testing to save yourself some future pain.
Testing compiler optimisations with quickcheck sounds like an awesome idea. 
I wonder if I'm ever able to hear that :D 
The `Char8` name is misleading. Specifically, that's decoding for the latin1 character encoding and is almost never what you want.
So, what would be the situations where the existential typeclass provides something that cannot be done more simply with a record-of-functions/explicit dictionary approach? The one likely example I've run into is that [existential typeclass + `Typeable` constraint gives you OOP-style downcasts](https://gist.github.com/sacundim/8511f98d6173d8d46533). But even in that case I would hope that the pattern could be abstracted into a more generic type (which might require `ConstraintKinds`; ~~I haven't tried it yet~~ **edit:** gee, that wasn't hard at all).
Woops, sorry for going silent on you. Your reply made me realise that the questions I had were probably right in the book I just started reading: Real World Haskell. It turned out to be quite difficult to even formulate the problem I was having, hence I told myself to just continue reading. We're two weeks later now and I've finished my initial surface read. I'm just so determined to get good at this language, that I'm going to reread it in-depth, without skipping the exercises this time! &amp;lt;g&amp;gt; I did continue to fiddle with Htoml, and indeed, the examples did help. The question I'm stuck with at this moment is - I'm getting datatypes like Node, which are returned by Text.Toml. I'm able to get a proper value using e.g. M.lookup. But I'm still unsure on how, in general, I should work with datatypes that are introduced by modules / returned by their functions. They seem to encourage the use of modules which I don't yet recognize. And any actual informed attempts I think I make tend to turn into silly trial-and-error iterations way too fast at this point. I did manage to turn a value into an ordinary Text; by case matching on Just (NTValue (VString a)), I was able to extract the Text value from a. But obviously these are all just a beginner's challenges. I just had to share - I didn't want to leave your kindness unanswered. \^ht *edit: words*
Agreed, I often keep around a naive implementation to test against. Unfortunately, since one of the main reasons to use a more advanced algorithm is to avoid the naive one's bad time complexity, this technique doesn't always play well with Quickcheck. But I still like to write unit tests against the naive algorithm on manually constructed small inputs in those cases.
Works for me. Anyway, here it goes: &gt; &gt; Yes! Our plan for GHC, dating back to the dawn of the Haskell Platform, &gt; was this: ... &gt; &gt; I still like that plan! &gt; Concrete proposal based on that and the other fine input in the responses: &gt; *Simultaneous Release:* Since it is organizationally impractical to have one release, let's have GHC and Platform release at the same moment. That is, GHC HQ would keep a release in "RC" until HP was ready. By the same token, HP team commits to tracking GHC from RC1, and aiming to hit ready for release within a week of GHC being ready. Both go "release" in the same announcement. *In fact, let's version HP with the same number as GHC!* &gt; *Pare the Platform Back:* Bring down the number of packages in the Platform, focusing on the things that everyone needs, like text and vector, etc. I reckon that about 1/3 of the packages should go. *And, make that stuff be the latest it can be at each release. *The OpenGL stuff is a hard one, since it is large, but a very big painful build if you need it. Perhaps we need server/non-server versions of the platform - but only if we can get them out on the same day. &gt; *Make sure the Platform Installers are Complete:* I don't know Windows, but if this means adding MSYS, great.. let's do it. The Mac installer has a version switching script and supports multiple installed versions already, but people didn't seem to know. There needs to be more documentation. &gt; *Make Updating the Packages in Cabal 'work':* I'm unclear on the right technical path here, but we need away for Cabal to understand that a) it can't update the stuff tied to GHC, b) it *can* update the other stuff installed from the platform, but as a cohesive set, c) it can easily (and optionally) do (b) in just a sandbox, or in the global(-ish) install. &gt; *One Web Site:* Drop the separate Platform website. Incorporate it into the lovely new Haskell one. Put all the documentation there. &gt; This certainly has implications for how we choose what is in the platform, and how we position those packages. In particular, packages in the past have been stuck at older versions because of the requirement that the transitive dependencies be added to the platform, with the support guarantee that implies. I think we'd have to change that: There are packages in the platform, like attoparsec, packages that are there because they are dependencies, like scientific, but who knows if they will be there next time! &gt; Now, normally I'm the crazy, ranty stability guy. But, I'm thinking this: It is better to have clear release points that package developers can test against, then to have the current slidey scale of different stability guarntees, on different release schedules that we have now. And, to be honest, I realize that the Haskell community "hath spoken" recently on the issue and prefers things to evolve even if the APIs change... &gt; I think we can do this if all the great volunteer talent in our community steps up. Shall we?
While I applaud the attempt to rethink the HP it seems strange to me that it's proposed GHC releases shall be artificially dragged out just so they can be announced simultaneously with the HP release. Why put such a millstone around GHC's neck?
is there a guide to installing ghc on OSX? I installed 7.8 with home brew, but I doubt a formula will be out for a while.
I concur, but really, the proposal is essentially to speed up the platform, not to slow down GHC. &gt; HP team commits to **tracking GHC from RC1**, and aiming to hit **ready for release within a week of GHC being ready**
Totality is necessary to make your proofs trustworthy (i.e actually proofs)
Depends on your point of view I suppose... I think the blog format is easier to read and draws the reader in more. It *is* literate haskell, so you can just copy and paste the whole blog post into a .lhs file and compile it. The source markdown is also available on the GitHub page for the site, though I admit the average reader is unlikely to want to clone my whole website just so they can tinker with the code! I suppose I could include a link to the source for just that file in any literate posts? That might make them easier to download and work with. I might try doing that -- thanks for the suggestion!
So, how about this elephant: in what way does Stackage LTS, with MinGHC covering the Windows issue, not solve everything Platform intends to? (I can't help it: [Relevant XKCD](http://imgs.xkcd.com/comics/standards.png).) But there's certainly an argument to be had for the official marriage of GHC and Platform; centralization helps people find things. So, rather than attempt to hammer nails into Platform's coffin... has anyone directly invited /u/snoyberg and /u/ndmitchell to participate in its planning? It seems a terrible waste to duplicate all that effort.
&gt; But I still like to write unit tests against the naive algorithm on manually constructed small inputs in those cases. Smallcheck, and just do a small (exhausive) limit. Then, a couple of manually constructed long checks using HUnit or similar. I don't remember seeing this in any of the Haskell frameworks I've just barely scanned, but TestNG (in the JVM world) allows you to tag long-running and very-fast tests then do runs based on those tags. (Say, only quick tests after each compile; exclude long-running for a standard test run; include long-standing for something no human as to wait for (CI), or even on a less intensive schedule (once a week instead of post-push, and only branch heads in CI).)
Better idea: Write them provably correct [in Coq](http://www.megacz.com/berkeley/coq-in-ghc/). Pity it doesn't appear to be maintained.
A little background, we already have Aeson-Schema[1], but it only covers Draft 3 of the JSON Schema spec. It's also a more ambitious project, including JSON schema --&gt; haskell code generation. This library doesn't do any code generation. I also tried to make it as modular as possible. It doesn't hardcode any validators (though it does provide a set of defaults that cover Draft 4). This makes it easy for users to add their own validators. It should also make it easy to support Draft 5 when it comes out. [1] https://hackage.haskell.org/package/aeson-schema
Specifically, not ascii text, but latin1 text
You can find this type in the `transformers` library, where it's named `Lift`: http://hackage.haskell.org/package/transformers-0.4.3.0/docs/Control-Applicative-Lift.html
Ah, OK, so the idea behind SmallCheck is the "everything up to a maximum depth" thing? Whereas QuickCheck may generate arbitrarily long sequences of inputs? (I can see advantages to both, but if you *really* want exhaustive checks, then I guess there's no alternative.)
How much do you need to trust that `String -&gt; Type` function that's part of `printf`? I'd rather be worried about logic mistakes, not non-termination. That is, yes you're both right and we need totality, OTOH totality is overkill when it comes to the "macros on steroids" aspect of dependent types.
"nothing speeds up the learning process more than making something useful" I could not agree more. 
One thing that's unique about the Platform is that the libraries selected into it are in some sense committed to a greater level of stability.
PointerErr should really be two types: data PointerFormatError = InvalidFirstChar | UnescapedTilde data IndexingError = ObjectLookupFailed | ArrayIndexInvalid | ArrayElemNotFound | UnindexableValue I really shouldn't have to handle the first two errors if I'm handed a `JsonPointer`, rather than a `Text`. I didn't go back to the spec, but is this really correct? vv -&gt; do unless (null ps) $ Left UnindexableValue You still haven't used `tok` an as index, which seems wrong. I was also going to suggest a conversion frim a JsonPointer to a Prism, but I don't think you can do that (or as the weaker Iso) without adding a dependency, so we can just add a separate small library for that.
Could we also have the packages installed via the platform come with profiling enabled?
Why is there a gratuitous "Show /r/haskell:" in the title?
Saw other people doing it, I thought it was standard practice...
isn't there an extra instance dictionary created or something though?
So far, the responses to this post have been: [Simpsons Did It](http://en.wikipedia.org/wiki/Simpsons_Already_Did_It). Instead, I want to thank the author for sharing this cool definition.
We might have misunderstood each other...I meant that if your two streams are independent *and you never have any reason to combine them*, then there's no point to combine them conceptually. It looks like you're talking about streams that you eventually do want to combine. Your batch construction can already be implemented by auto primitivites and auto combinators, actually --- if you had a1 :: Auto' Foo Baz You can turn it into an `Auto' [Foo] [Baz]` using `accelOverList` accelOverList a1 :: Auto' [Foo] [Baz] And you can do the same thing with `a2` a2 :: Auto' Bar Baz accelOverList a2 = Auto' [Bar] [Baz] You have a couple of options now....you can use `(|||)` to turn it into a processor that takes either batches of Foo or batches of Bar at a time...whatever comes next/first: accelOverList a1 ||| accelOverList a2 :: Auto' (Either [Foo] [Bar]) [Baz] Or you can do your original idea and run them over a tuple with `(***)` accelOverList a1 *** accelOverList a2 :: Auto' ([Foo], [Bar]) ([Baz], [Baz]) -- or, to combine in the end, fmap (uncurry (++)) (accelOverList a1 *** accelOverList a2) :: Auto' ([Foo], [Bar]) [Baz] Which is the exactly the type of your `Processor`. In this way, you can define `a1` and `a2` while reasoning about how the `Auto` reacts to individual `Foo`/`Bar` inputs and the individual `Baz`'s they output...and then wrap it all together in a `Processor` on microbatches. Is this the sort of concept you were thinking of?
Thanks for the kindness. I've liked all the replies so far. They brought up a bunch of libraries to dig through, which is always fun.
I guess it's something that comes from HN, and is a way to grab attention: it implies that it is something new, something that the submitter has developed and asks for feedback. I don't think there is something wrong with the practice myself. 
&gt; the Platform (mostly?) includes canonical, "blessed" packages. ...and a few of those in the platform are obsolete and/or inadequate and should, in my not so humble opinion, not be the canonical recommended packages at all!
Well, I'm dubious this will magically speed up the HP. If I look at https://www.haskell.org/platform/changelog.html the last 4 HP releases were - 2014.2.0.0 + GHC 7.8.3 (though GHC 7.8.4 is latest rls!) - 2013.2.0.0 + GHC 7.6.2 (though GHC 7.6.3 is latest rls!) - 2012.4.0.0 + GHC 7.4.2 - 2012.2.0.0 + GHC 7.4.1 So, paradoxically, GHC releases occur more often than the HP. Why can't the HP simply fixup its release-schedules without affecting the GHC release schedule which seems to have already enough of its own problems to keep releasing on time... anyone remember GHC 7.8.1? And even GHC 7.10.1 slipped already by one month from its original target in February, and has already missed its latest planned 7.10.1 release last week... PS: I may be biased, but I really don't care about the HP as I don't use it. I even advise anyone who asks me against using it (if I *really* have to recommend something "enterprisey" I rather point to Stackage - which I don't use either). I just want to be able to get GHC releases rather sooner than later.
The problem described there is real, but the solution of never even mentioning good editors to newbies is too extreme. I stand by my advice.
Using `Text.IO` is usually not recommended in the context of a network protocol, because then the behavior of the program changes depending on the default locale in effect on the computer where it is running. So even when you have text, it is best to treat it as binary bytes on the wire, and use the codecs in `Text.Encoding` or in the [text-icu](http://hackage.haskell.org/package/text-icu) library.
Whether you use lazy or strict byte strings, there will be some chunking involved. So be very careful with your unicode. The chunks will sometimes break at places that are awkward for unicode. The unicode near the end of a chunk will sometimes not be interpreted properly, or even be invalid, without the bytes near the beginning of the next chunk.
I personally think HP tries to do too much, and shoots itself in the foot because of it. The problem that HP is really solving is "Haskell on Windows". The problem of a stable set of base packages has been solved by Stackage. So that means the only problem left is "Haskell on Windows", which has been solved by MinGHC. In other words, HP tried to do too much, probably became too big to maintain because of it, and as a result failed. The community (a.k.a. /u/snoyberg, who seems to be a whole community on his own) has solved the problem in a better way.
HTML parsing libraries that have accompanying lenses/traversals/folds/prisms defined for them almost make extracting data from HTML fun! One of them is my own but they all offer a similar interface, you should definitely try them out. And it's an interesting way to approach lenses if you've never used them -- which reminds me I still have to write an intro to lenses from that angle =)
Stackage is fantastic, but it does *not* in any way solve the problem that Haskell Platform solves. Stackage gets a very large set of packages to build together. HP provides a small set of packages that not only build together, but are recommended, stable, well maintained packages that are widely regarded in the community. It relieves users from the burden of having to research thousands of packages on Hackage/Stackage in the usual case where they don't necessarily need the best or the latest, but just something reasonable for a common task. EDIT: I agree with you that HP shoots itself in the foot by trying to do too much.
I'm inclined to agree -- what I meant was that this is a legitimate use of `fail` from the library user's perspective because `fail` is one of the 'official' error handling mechanisms (rather than the usual `fail = error`). Whether or not this is the best API design ever seen in Haskell land (probably not) is a different question.
I can find hexpat-lens - is there another one I should be awere of? Also, what's your own library?
I too wonder why `2014.2.0.0` wasn't updated with security/bugfixes. `text-1.1` (as shipped with the latest HP) contains some really nasty bugs that were fixed already last year (see [changelog](http://hackage.haskell.org/package/text/changelog)) 
Do you *really* recommend e.g. [`HTTP`](http://hackage.haskell.org/package/HTTP) for everyday use?
In my opinion, Stackage LTS and MinGHC do not have anything to do with Haskell Platform. There are two main goals of Haskell Platform: * Provide a set of recommended libraries for common tasks. (Like Python's concept of "batteries included".) * Provide a web page with a standard, easy, one-click way to install Haskell on any popular platform. There used to be an additional goal of actually developing the installation packages for the various platforms as part of the work on Haskell Platform. That actually made up the lion's share of the work that needed to be done on the Platform. It was an important service to the community, since there really weren't any other good options. But that time has past, for whatever reasons, and the community has moved on. The Haskell Platform is still important for the reasons I listed above, and should now focus on its core contribution to the community.
Just this past weekend, I started using hjsonschema to build tests for a couchdb client I'm working on. It has made verifying the output of calls roughly 600% easier.
&gt; ...and a few of those in the platform are obsolete and/or inadequate and should, in my not so humble opinion, not be the canonical recommended packages at all! In my opinion, that's what the HP process is all about. That discussion should be ongoing, with wide participation, and a clear way to reach a timely consensus for each release. We should not be sidetracked and discourage participation by focusing so much of our effort on developing and maintaining installer software. There are other projects that do that.
&gt; HP provides a small set of packages that not only build together, but are recommended, stable, well maintained packages that are widely regarded in the community. OK, but this can just be done with a smaller Stackage set.
Because a new platform release currently requires a huge amount of work. According to current protocols, a new platform can't go out until there are fully tested installers for all platforms. And there just weren't enough people working on it. Mark has worked very hard on upgrading the automation to vastly reduce the effort required for a platform release, but that in itself is a huge effort without enough people working on it. That's why I think the right way to reboot the platform is not a slight slimming down of the installer building process. It is dumping that altogether, and focusing on the core value that HP provides: a recommended set of basic packages. And a *recommended* installer for each platform, taken from other projects.
I was writing a similar tool (in python though) and I don't think it's built-in any language. Either someone wrote a library, or you end up writing yours, calling foreign code: external program such as `lsblk` or kernel device list bindings.
The discussion is not about whether the Haskell Platform needs to be updated. It does. The discussion is about how.
As far as I know, the complete list: - [xml-html-conduit-lens](http://hackage.haskell.org/package/xml-html-conduit-lens) - [hexpat-lens](http://hackage.haskell.org/package/hexpat-lens) - [taggy-lens](http://hackage.haskell.org/package/taggy-lens) which is my own (co-authored with Vikram Verma)
&gt; Woops, sorry for going silent on you. No worries. &gt; I did continue to fiddle with Htoml, and indeed, the examples did help. Good. &gt; The question I'm stuck with at this moment is - I'm getting datatypes like Node, which are returned by Text.Toml. I'm able to get a proper value using e.g. M.lookup. But I'm still unsure on how, in general, I should work with datatypes that are introduced by modules / returned by their functions. They seem to encourage the use of modules which I don't yet recognize. You could do 3 things: 1. Patter match your way out. For example in `Just (NTValue (VString a))` you can match for the `a` with `match Just (NTValue (VString a)) = Just a`, and `match Just _ = Nothing`, and `match Nothing = Nothing`. 2. Use lenses. This is a more advanced topic. 3. Call `ToJSON` on the beast and use your Aeson-fu to query the structure. &gt; I did manage to turn a value into an ordinary Text; by case matching on Just (NTValue (VString a)), I was able to extract the Text value from a. Did you use pattern matching at some point? &gt; But obviously these are all just a beginner's challenges. I just had to share - I didn't want to leave your kindness unanswered. ^ht Again: no worries. Good luck with learning Haskell. It's a fun ride, and it is not at all impossible. Just mainly getting the hang of it, and then you are back to programming business as usual, just a lot less bugs. :)
This is a highly OS + config dependant operation. We have servers that have RAID drives - how you you expect that to be represented? There's others that use storage on the SAN - what sort of a response are you expecting to get in this case? A LUN, WWA, HBA, Zone? It's not clear to me how to even answer the question in that case. Granted, these situations are a fair distance from what you are intending, but that they exist shows that your model is incomplete. Which is why there's not really a simple and complete way of doing it in Haskell - because there's not a simple and complete way of doing it. You're probably going to have to make some assumptions to simplify the case, and then delve into the OS specific functionally to request the data you want. If you are on linux, and are happy to ignore things like LVM, you can get a long way by checking /proc/diskstats (may already be a wrapper somewhere), and from that getting it down to the number of physical volumes. On the other hand, with LVM group, you can get the 'same' logical partition split over multiple physical drives, and working back through that split to ensure you only have one scanner running per physical drive is tricky - and highly specific to LVM. If you only have LVM groups that are spread over multiple physical disks, that's going to make that a difficult constraint to meet, without some kernel level work.
My solution would be not to even try. Provide the required options with safe defaults and leave it to the sysadmin to configure the scanner properly.
[ANN] means it's done, not feedback requested
Hi there, I'm guessing from the title that you're already familiar with the existence of the [Write Yourself a Scheme in 48 Hours](http://en.wikibooks.org/wiki/Write_Yourself_a_Scheme_in_48_Hours) wiki book. The question you are asking is quite broad and it is hard to answer because I can't know exactly what your set of skills is. I'd suggest following the wiki book tutorial and if you run into problems ask in /r/haskellquestions or on the #haskell IRC channel about the specific problem. Wish you lot's of fun with your Haskell scheme :)
The title is presupposing errata before any attempt to dig into the material. By and large, both are still fantastic references. My suggestion is just to start hacking on it. Come here and ask specific questions. What you learn from both references will absolutely be applicable. 
How you know if a code is tested without reading the code, understand it , going to the tests, understand them, think in corner cases, verify that the test pass trough the corner cases? There is no other way. Otherwise if you consider a code tested when the number of lines trough which the test pass is 90% then you are too naive. Test coverage is simply an statistical lie. There is no way to know if a program has been well tested except doing the above analysis. And, if that is true, then you are forced to be in my side!!. because if you have to understand the code and the tests to verify that the code is well tested, you MUST demand a readable, and elegant, and understandable coding style to begin with, since otherwise you will NOT understand the code and you will NEVER verify that the code is well tested.
Even with system specific calls, you'll have a hard time. First you have to figure out what filesystem a file belongs to, then you can determine the block device that filesystem is on. After that, you have to determine the type of block device and use type-specific techniques for determining any underlying block devices or, in the case of many HW-RAID drivers, underlying "physical" devices that aren't directly exposed to the OS. For example, on my *desktop*, I have in the past as 6 drives in attached to an HW-RAID card. I then put all of them into a single "RAID Set", and carved off 2 "RAID Volumes" out of that set. Then, I used LVM on top of that to turn one of the volumes into 6-8 block devices, and 2-3 different filesystems across those. The physical disk was 4 (or 5) layers separated from the file and some files were one multiple (or all) the physical disks. Assuming you can even *find* all the layers, then you have to map the block device offsets that the filesystem *might* expose through each layer. If you don't map back the specific blocks, you'll always think a file is on all disks in a RAID-5, when small files are generally "on" only 2 disks. Best to just provide support for a user-specified limit to parallelism and default to the number of drives (MS Windows) for filesystems (unique f_fsid and/or st_dev values; for Unix-like) encountered during the scan.
Yep, give a -j option like make has and let the user sort it out. You've no idea that parallelizing via drive amounts is a good idea either. Lets say you have raid 1+0 of 2 and 2 drives how are you going to know that 4 is the correct amount of reads you should do? Then lets say you have a raid 6 of 4 drives, same question. The only way to know the best amount would be to let the user specify. That and you could end up impacting performance negatively, maybe someone wants to run only one thread/process to negate impact. The best possible solution would be to implement something like what dsync did for i/o performance. That is implement your own scheduler to monitor performance and tune up/down as appropriate. http://www.cs.cmu.edu/~dga/papers/dsync-usenix2008.pdf
&gt; How you know if a code is tested without reading the code, understand it , going to the tests, understand them, think in corner cases, verify that the test pass trough the corner cases? Well, for starters I can reasonably confidently say that if you haven't run a commonly available tool that has a proven track record of finding bugs in projects then your code *isn't* well tested.
We could probably get input from other package management tools that use git to store their package metadata. I know Homebrew and Cocoapods both do that, and I imagine others as well. One lesson to take from Cocoapods, for example, is to not store all the packages at the top level of the git repo. They used to do this and it made the [Github page](https://github.com/Cocoapods/Specs) for the repo extremely slow (impeding you from doing things like reading the README or getting the clone URL).
I read it around a year ago, and I don't remember having any problems. It's definitely not comparable to RWH wrt needing updates.
Pretty much this, Haskell deployment should be based off the successes of Haskell in Linux and Linux-like ecosystems, rather than try to maintain a successful modular approach (package manager) and a historically *alright* monolithic approach (platform).
I'm learning from the Wikibook itself! :)
Artificial neural network. ...maybe not the right context.
Using git just for storage and transfer of .cabal files seems like huge overkill. It means introducing git as a dependency for cabal-install for no good reason. Maybe a more lightweight solution, like librsync, or something like Debian's pdiffs would be a better fit. How do package managers for other programming languages handle this?
Thanks for catching the `UnindexableValue` mistake. That was definitely wrong. I would break `PointerErr` into two types, but then you can no longer do: `jsonPointer somePointer &gt;&gt;= resolvePointer someData` It might be worth it to break `PointerErr` up anyway, but is there a way to do so and still have `jsonPointer` and `resolvePointer` compose nicely?
A CSS selector is usually well understood in web development (`attr{href}` is an extension but doesn't seem too bad). It becomes ugly when the selector is too big or complicated, in those cases Scapel may feel better. Anyway, I expect to trade some of the conciseness of CSS selectors to have more structure (like reusing common prefixes, etc) and type safety, without relying on string manipulation. But simple cases should remain compact IMO.
The platform is made up of old packages, old is not the same as stable. Stable also includes a process for critical bug and security fixes. 
&gt; but is there a way to do so and still have jsonPointer and resolvePointer compose nicely? Yes. I would state though that this may not be the best goal. Sure, it looks nice if this is the only package you are using, but as soon as you have *any other* failure case that you need to pre-, post-, or intra-compose with those two operations, some effort will be required to "lift" them to a larger set of error conditions, or convert `Left`s to `throwIO` or whatever. I would also expect `resolvePointer` to be used more often than `jsonPointer` (otherwise, why have separate functions at all?) so their direct composition may not be the best case to optimize. I strongly encourage total programming with Haskell and having the **type** of a function reflect errors it can't possibly throw causes me to write handlers for "impossible" cases in order to ensure totality. You could use something like [mongoDB](https://hackage.haskell.org/package/mongoDB-0.9.5/docs/Control-Monad-Throw.html) uses. Have `jsonPointer` have a `Throw PointerFormatError m` constraint and have `resolvePointer` have a `Throw IndexingError m` constraint. Their composition will have both constraints. You could eschew the MPTC w/o FunDeps and have a slightly less clean composition, too. Something like: `liftFmtErr (jsonPointer somePointer) &gt;&gt;= liftIxErr . resolvePointer someData`. [`withExceptT`](https://hackage.haskell.org/package/mtl-2.2.1/docs/Control-Monad-Except.html) would be the canonical way to lift errors to a larger set of error conditions. It could also be used in combination with `join . fmap (either throwIO return)` to turn errors into exceptions.
There's even a blog post explaining a pure-haskell implementation of `git clone`: http://stefan.saasen.me/articles/git-clone-in-haskell-from-the-bottom-up/
Sweet! With https://wiki.haskell.org/Haskell_program_coverage you should be able to do http://en.wikipedia.org/wiki/Concolic_testing .
Thank you for yet another valuable contribution to the community, Microsoft Research! Time to learn more about sbv so I can make use of this :)
Agreed. I think the focus on the installers is what is causing so many people to say "we don't need the Haskell Platform anymore." I do agree with Mark that we still need the Platform. Just not for the installers. Cutting back on the number of packages will not significantly change that; it will mostly just cut back on the value that Haskell Platform provides in other directions, which is where it is really needed. It's true that the Haskell Platform installer concept would still add value. But a successful installer project for all platforms is a huge effort. There is other lower-hanging fruit for the Platform.
I agree that building Haskell Platform on top of a Stackage set might very well be the best way to go. But that is a technical detail we haven't gotten up to yet.
heh, thanks, I think I accidentally two paragraphs. 
SBV is fantastic! 
Why can't the compiler normalize those two types at compile-time, and iff this normalization terminates, consider these two types equal? I agree with /u/barsoap and do not understand why every dependently-typed language has to be obsessed with proving termination. 95% of code that powers the modern world is *run* without any termination proofs, so what's the danger in trying to *compile* something without them? As far as I see, the only mandatory difference between a typechecker for a dependently-typed language and the GHC typechecker is that the former might fall into an infinite loop. But that is no big deal in practice: GHC typechecker, despite its "totality" (strong normalization) crashes for some pathological types anyway. And we all live with it!
Well, glad we agree testing is important. The bugs, by the way, aren't related to the size of the ADT. They're flipped parameter order to composeSubst, and an unnecessary extra application of substs at the end (performance bug). High-level testing is great, but detecting low-level bugs in high-level testing is: * Much more difficult. The behaviors encountered are very much removed from the actual bugs. * Less likely: edge cases may be very rare in the context of the whole system, and hard to test. They may be easily reached in smaller tests. 
Can somebody ELI5 what a theorem solver is and does? How could I apply this to a businessey project written in haskell?
Businessy people developing in C and Java you have control statements like IF(). SMT allows you to solve which assignments of variables you need to get a control statement to evaluate True so you can gain access to that block of code. Combine that with a coverage build like gcov and you can make the computer search for unit test cases that get you to a line in your code. Why write simple unit tests when you can ask for them and grab coffee?
Doesn't http://hackage.haskell.org/api provide means to extend Hackage with 3rd party add-on services?
QuickCheck basically throws randomness at your code and tests for properties that you have defined. An SMT solver could actually reason about the code itself.
Cool! Write yourself a scheme is on wikibooks now, btw: http://en.wikibooks.org/wiki/Write_Yourself_a_Scheme_in_48_Hours So you should be able to make your edits inline to the book and contribute them directly :-)
I agree that those things are not solved for by the Platform.
Awesome project! Seems like it could be a great resource for intermediate and advanced topics. I'm not sure how you're planning to organize this—are the videos going to be globally accessible, or will you require a registration? If it's the former, it would be good to have each video on its own page so that it could be linked to. With the current page, I hit the back button after watching a video, which did not do the right thing.
Exactly. Providing a binary installation for Windows, Linux and OS X will lower the barrier for new users. This will always be relevant for the community. If it's too hard to include as many packages as the current platform, that's OK. But we need at least enough to enable new users to run programs normally covered in introductory courses. Over time we can add more packages based on community consensus.
OH BABY
&gt; Wasn't there also discussion of using an SMT solver for type resolution also? You mean stuff like LiquidHaskell?
Stable means unchanging, not bug free or having a process for making changes. Debian stable currently has more (known) bugs than Debian testing, but it's still more *stable* because it is a known quantity. I think we want some standard of support(ability?) from the HP, but we shouldn't conflate that unnecessarily with stable packages.
BTW, code generation can be turned off. This makes for faster type checking if you've got an compile-on-file-change workflow. cabal build &lt;stuff&gt; --ghc-options=-fno-code 
It should handle malformed html well. It's built on top of tag soup which represents html as a list of tags, so as long as the portion of the document that you are interested in is locally well formed it does not matter what the rest of the document contains.
Good point. We could even use a hash fragment and have a sort of single page app if need be. I'll add that to our issues queue. Also if you want to record a video of your own feel free to drop by the office. The math study room is all set up. :)
Basing Hackage on git would substantially lower the barrier to entry for setting up a private server. If you use git already then there's nothing to setup. If you have a GitHub account for your organization then you also have authentication and user access for free. No need to build it from scratch inside Hackage server. And of course git is not the only choice. It might be possible to build a backend independent layer to make it work with Mercurial or Darcs.
can you give a summary? it's a 20min video. like what plugin does it use, what were the hard parts in setup, does that plugin support quasi quotes / GHC extensions, do you get jump to definitions? etc thanks!
Ya, it was way longer than I anticipated. High level (I should write a blog): Using Yosemite 10.10.2 and IntelliJ 14.1 intellij-haskell: https://github.com/rikvdkleij/intellij-haskell ghc-mod doesn't support cabal 1.22 so we need 1.20.x Set up Haskell using (not the one directly linked to on ghcformacosx.com) https://github.com/ghcformacosx/ghc-dot-app/releases/tag/v7.8.3-r1 cabal install ghc-mod hlint haskell-docs Open intellij, install intellij-haskell. Go to the plugin settings and point to hlint, ghc-mod, etc which should be in .cabal/bin #testing it out git clone https://github.com/bitemyapp/bloodhound (or any project) cd bloodhound cabal sandbox init cabal install --only-dependencies cabal build mkdir lib cd lib cabal get ghc-prim http-client http-types containers hspec text time aeson semigroups vector In Intellij: File &gt; New Project &gt; point to location of project (it won't overwrite anything) &gt; Haskell Module &gt; point to /Applications/ghc-7.8.3.app/Contents/bin Project structure &gt; Modules &gt; click lib and set as source directories Profit 
Glad you think it's helpful. I used haskell-vim-now which was great, but still made things harder for me as someone not very good with vim (yet). 
To go even further, disciple is using an ILP solver for its fusing/flow stuff, resulting in most ridiculously efficient loops, because it can fuse filters. Generally speaking, algorithmic sledgehammers are indeed a nice thing.
Oh yeah, I think I might. I did a few short talks at the Hacker Dojo a while back which could make for nice videos.
You are forgetting my original point: the binary windows GHC is nowadays covered by MinGHC. HP is really two in one: a binary windows GHC and a curated set of packages. It is hurting HP by doing both these things. 
How is an unchanging set of packages useful if you have to throw the whole set out completely the moment a critical bug or security hole is discovered in any one of the packages included in it? You certainly can not use something like that on any production system you have to support. 
Hackage is still not distributed, has next to no security, no integrity and is extremely overkill.
thanks :) I'll revise my opinion of this and anywhere where I've suggested otherwise and try to clear up any misunderstandings. I did know about `mvc`, but always thought that it was a games/GUI library that took advantage of pipes. But I guess that's the same as saying that pipes can be used with mvc to build games and GUI's. 
the day has finally come :) thank you to all contributors who made this all possible :) relevant comic: https://ro-che.info/ccc/21
Thanks all for your comments, you are right, it is OS specific and it is pretty complicated also in othe details, and RAID and in particular LVM add to that. My understanding is that a RAID system to the OS is a single physical drive, and thats how I'd adress it in the script. So I guess the main problem is rather an unfortunate choice of terms, i.e. neither "logical" nor "physical drive" describe it well. More so than RAID, i find LVM difficult to handle and I have no clue yet how I'd deal with it. BTW, as mentioned I'm doing this primarily as an exercise to brush up my haskell, to use it for my own limited needs and not to present a solution that is so general that it is suitable for all possible kinds of setups. I'll end up using HSH with my previous bash solution starting out with the the output of "df $FILE" and cut away the partition number...
How much until stackage start providing 7.10 nightlies?
Also, note that `mvc` could be easily be forked to work with `Auto`. There's nothing really `pipes`-specific about the overall architecture.
There are still a [number of packages](https://github.com/fpco/stackage/issues/378#issuecomment-82971435) that are incompatible with 7.10, but hopefully now that the official release is out people will start getting those updated more quickly. I'll continue to run these builds regularly and ping people. I'd optimistically put the timeline at 1 month till everything's fixed, pessimistically at 4 months.
Some of my colleagues at FP Complete are happily using SublimeText.
Thanks for all your (and fpco) work!
&gt; when can we expect OSX binaries (sans HP)? The OSX GHC bindists are kindly provided by Mark (/u/MtnViewMark) who verifies they work on several OSX releases before publication which takes a bit of time.
Hi, I'm one of the two SublimeHaskell maintainers. I'm using Sublime at FP Complete, and have been using it in multi-hundred-module projects at [Google](https://code.google.com/p/ganeti/) and [Tsuru Capital](http://www.tsurucapital.com/en/), as do some of my co-workers in each company. So to the answer in general is, yes, it is used for large-scale Haskell development. Now specifically, fancy editor integration breaks down in large scale. Emacs and SublimeHaskell both like to use existing tools like ghc-mod, cabal, ghc-mod. However, these tools start breaking down a bit for large setups: ghc-mod (the non-caching version) takes very long to typecheck multiple hundreds of modules, and its interactive component (as well as ghci and hdevtools) consume massive amounts of memory once you load &gt; 300 modules. I would say that SublimeHaskell's `cabal` integration is likely the most useful one for large projects. All these tools have problems with multi-package setups, though; cabal cannot currently build multiple dependent packages in a source tree with great convenience. In general, our direction for SublimeHaskell is to move as much as possible of the Haskell related logic _out_ of the editor plugin and into Haskell code (hopefully cross-editor tools), keeping the plugin lean, the amount of Python small, and the whole thing maintainable.
Ugh, looks like today's Github instability is playing havoc with our blog post updating, the servers can't pull the most recent content. I guess considering Github a stable part of our infrastructure may have been overstating things a bit :(. Here's the content: * * * We're happy to announce the availability of [MinGHC for GHC 7.10](https://s3.amazonaws.com/download.fpcomplete.com/minghc/minghc-7.10.1-i386.exe). MinGHC is a minimal GHC installer for Windows, consisting of: * GHC itself * cabal-install * MSYS, which is necessary for building some packages such as network MinGHC came out of some conversation Neil Mitchell and I had at ICFP last year about pain on the Windows platform, and consists of a lot of good code by Neil, and some hacky attempts by me at understanding Windows installer processes. While this project predates the [Commercial Haskell SIG](http://commercialhaskell.com/) by a few months, it's essentially the first project undertaken by the group. While MinGHC is relatively young, it's also a minimalistic product, simply repackaging and serving upstream components unmodified. Additionally, it has become the recommended installer by [quite](https://www.haskell.org/downloads/windows) a [few](http://www.stackage.org/install#windows) [sites](https://github.com/bitemyapp/learnhaskell#windows), and therefore has already received significant testing. Also, due to its minimal nature, it's easy to pop out new releases of MinGHC. For example, this current release was made less than two hours after GHC itself was released! While this project is stable, additional testers and contributors are certainly welcome. Please check out [the Github project page](https://github.com/fpco/minghc) for more information.
yay :) Now we wait for the archlinux release! :D I've got a feeling that releases keep coming quicker and quicker. Is this true? 
That's mostly a kind of liability disclaimer... we are obligated to warn you =)
And also thank you for introducing me to this awesome web comic :3
It sadly rarely gets updated :(
Author here. I'm looking for some feedback on this tutorial (series), which I based on [Go by Example](https://gobyexample.com) after stumbling upon it looking for a Go primer. I found its clear style and simple examples pretty helpful, and wondered how they'd look in Haskell. I'm going to finish off a few more chapters in the next few days, but my thorniest problem right now is [this](https://github.com/eightyeight/haskell-simple-concurrency/blob/master/src/tutorial.md#well-actually): I'm forking a lot of threads, and not killing them manually, but even if I do, they won't kill any child threads. `slave-threads` solves this, but I don't want to introduce a dependency. Thoughts? Exception-safety is also something I know very little about, and assume I'm doing atrociously at. Planning to look at that once I've done a bit more writing. Pull-requests and issues are welcome!
I'll build and test this weekend.
 void (sequence (map printMessage [1..3])) could be mapM_ printMessage [1..3]
I really like "Add switcher to PATH" option, as excessively cluttered PATHs have caused me problems in the past.
&gt; I'm forking a lot of threads, and not killing them manually Maybe have them die naturally? In at least one place you have threads [BlockedIndefinitelyOnMVar](http://hackage.haskell.org/package/base-4.8.0.0/docs/Control-Exception.html#t:BlockedIndefinitelyOnMVar) for no good reason. They could use tryTakeMVar and quit if they don't win, for example. I think that's a better way to approach things than to kill threads anyway. The only threads you should kill are those that perform a task repeatedly, not one-shot threads. Even for repeating threads, usually having a "suicide directive" ([tryTakeMVar](https://hackage.haskell.org/package/base-4.8.0.0/docs/Control-Concurrent-MVar.html#v:tryTakeMVar) in the "main" (not necessarily top-level) loop, maybe?) is usually better than throwing an asynchronous exception in them.
Typeable for everything? Well, there's goes parametricity. :P
Well, you still can't use any Typeable methods without having a context. (I hope!)
It was exactly that, I had a 1.18 version that I added manually. Thanks!
@[danharaj](http://www.reddit.com/user/danharaj) Fair enough – Don't get me wrong though, you probably have some kind of workflow, isn't it?
Also bare [STM](http://hackage.haskell.org/package/stm) when Async is not enough for any reason. [pipes-concurrency](http://hackage.haskell.org/package/pipes-concurrency) may also come in handy. And [parallel](http://hackage.haskell.org/package/parallel) for when you need parallelism rather than concurrency, i.e. you want your program to run faster while performing one task in a deterministic way.
Great job, guys! This is a huge step forward.
It's also very loud
why do you lose parametricity?
Excellent write-up! I was amused to realize that the "conflict sets" we use in our solver correspond to the `Explanation` element of a DPLL(T) algorithm. As I understand it, the `Explanation` element is the key element distinguishing DPLL(T) from plain old DPLL, which is to say distinguishing the algorithm for SMT solvers from SAT solvers. [1] So, while we have talked about calling out to an SMT solver to do our constraint solving in the past, it turns out that, effectively, we have already implemented a (specialized) one! [1] https://www.cs.upc.edu/~oliveras/espai/papers/dpllt.pdf 
SPJ seems to believe that a good portion of "cabal hell" can be resolved: &gt; People mean different things by "cabal hell", but the inability to &gt; simultaneously install multiple versions of the same package, &gt; compiled against different dependencies &gt; is certainly one of them, and I think it is the one that Yitzchak is referring to here. &gt; &gt; But some time now GHC has allowed multiple versions of the same package (compiled against different dependencies) to be installed simultaneously. So all we need to do is to fix Cabal to allow it too, and thereby kill of a huge class of cabal-hell problems at one blow. It's not clear to me from the thread what the definitive story is other than it's close and a nix-style solution is favored. If what SPJ wrote is true, is it possible to get a quick point-fix before the release of a nix-style cabal? ref: https://mail.haskell.org/pipermail/ghc-devs/2015-March/008602.html
*If* no `Typeable a` constraint was required, it would mean that you can do non-uniform things to values of a universally quantified type. Parametricity is dependent on the fact that the only thing you can do to a value of a universally quantified type is return it unchanged. I'm not sure if it would have any practical effects, though. But, I believe the constraint will still be required.
Thanks for this blog post! A few days ago I was trying to learn about coinduction, after hearing in the [type theory podcast about Idris](http://typetheorypodcast.com/2014/09/episode-2-edwin-brady-on-idris/) that induction together with coinduction restores turing completeness, and was left dissatisfied after reading the [wikipedia article](https://en.wikipedia.org/wiki/Coinduction). Your post made things a lot clearer. :)
maybe "multiple instances of the same package version built against different dependencies" http://www.reddit.com/r/haskell/comments/30h8jx/qualified_goals_in_the_cabal_solver_includes_a/cpspzhe
https://downloads.haskell.org/~ghc/7.10.1/docs/html/users_guide/separate-compilation.html#module-signatures
I'd use `Void` because it's in an argument position, so it's easier for your callers to produce a value of a specific empty type than to write generic code which is able to produce all of them. Conversely, when using an uninhabited type in a return position, I recommend using a universally-quantified variable because it will be easier for your callers to instantiate that type to the uninhabited (or even inhabited!) type they need. That is, I'd use `runRight` and `right'`. right :: a -&gt; Either Void a right' :: a -&gt; Either b a
&gt; easier for your callers to produce a value of a specific empty type than to write generic code which is able to produce all of them The point of `Void` is that it has no values, no? data Void :: *
Another option which I sometimes use (not always applicable obviously): runRight :: (forall a. Either a b) -&gt; b 
A slight difference between runRight :: Either Void b -&gt; b runRight' :: Either (forall a. a) b -&gt; b and runRight'' :: (forall a. Either a b) -&gt; b is the scope of `a`: λ let semiImp :: Either a (a, Int) ; semiImp = Right (undefined, 5) λ :t semiImp semiImp :: Either a (a, Int) λ :t runRight semiImp runRight semiImp :: (Void, Int) λ :t runRight' semiImp runRight' semiImp :: (forall a. a, Int) λ :t runRight'' semiImp &lt;interactive&gt;:1:12: Couldn't match type ‘b’ with ‘(a, Int)’ because type variable ‘a’ would escape its scope This (rigid, skolem) type variable is bound by a type expected by the context: Either a b at &lt;interactive&gt;:1:1-18 Expected type: Either a b Actual type: Either a (a, Int) In the first argument of ‘runRight''’, namely ‘semiImp’ In the expression: runRight'' semiImp I rather think this is a point in `runRight''`'s favor. 
Would you recommend against reading OP's tutorial, then? I'm a Haskell neophyte (but familiar with Go).
Having packageKey instead of packageId as before 
I know. I was trying to avoid introducing combinators early on, but maybe doing so will be more useful than heaping on a bunch of functions. `mapM_` is one of my least favourite combinators (fairly noisy) but I may modify this to `void . forM`. EDIT: I think I've settled on `Data.Foldable.for_`. It's simple, semantic and doesn't require the word Monad anywhere :p.
If you want to learn Haskell's way of doing things read Simon Marlow's [Parallel and Concurrent Programming in Haskell](http://community.haskell.org/%7Esimonmar/pcph/).
Wow, that seems fairly obvious in retrospect. One reason for killing threads manually would be to stop doing some expensive computation if, for example, another computation has pre-empted it in a select, but it's certainly not a huge deal. And of course there's the question of threads which for some reason loop infinitely, but that's probably a bug in any case :p.
This is actually a cabal bug. See https://github.com/haskell/cabal/issues/2438. 
read it, but then use `async`, `stm` and `stm-chans`
I'm trying to keep as light as possible on libraries, so that all the examples can run with just `runhaskell` atop either `base` or the HP. I think currently I'm using only what's in `base`, and to bring in `async` would require the HP, AFAICT. Also, until pipes are the ubiquitous method for doing concurrency in Haskell, I'd be apprehensive of comparing them to goroutines.
 Either (forall a. a) b requires `ImpredicativeTypes`, which basically don't work, so the decision is mostly made for you. A related question is what to accept or return in an API to indicate uninhabitedness. My usual advice is to "consume" `Void`, but return an unrestricted `a`. [] :: [a] Nothing :: Maybe a traverse (const Nothing) :: Traversable f =&gt; f b -&gt; Maybe (f a) Now `a` being a polymorphically unrestricted argument, it can be instantiated to `Void` without walking the structure at all, and the field can't occur (covariantly) in `f`.
So, we're running into that wall where using the things built into the language is never recommended in production, haha. I set out to translate the examples using only what I could find in `base` or the HP. I'm no longer sure if that's a goal I should be sticking to, based on the feedback here! `race` looks useful, and my `select` certainly has issues. I think I'll definitely start linking to relevant parts of PCPH as well as GbE, and will probably add a section recommending some of the libraries presented here, if I don't decide to actually make use of them.
&gt; I set out to translate the examples using only what I could find in base or the HP. Ah. That's not a bad goal. Dipping into other libraries runs the risk of other libraries changing incompatibly or going unmaintained. That said, PCPH and async are written and maintained by /u/simonmar, so I think your risk is minimal. It might just be enough to mention that limitation, it is mostly a pedagogical limitation, and point the reader at some good libraries in the final article of the series. Also, I don't get to use Haskell in production, yet, so *no one* should depend on my advice for what "production Haskell" looks like. I'm more of an dedicated enthusiast; I've actually written more Idris over the past month than I have Haskell.
I just added a paragraph mentioning that goal explicitly, but I think I definitely will make use of `async`, and actually read PCPH properly to make sure I'm not reinventing the wheel completely. Thanks for the recommendations! As for the comment about 'in production', I was just looking at the benchmarks of unagi (and other libraries) versus Chan T_T.
But now in the case we have made bounds more restrictive, the old packages still @#$@ with our install plans because the solver will backtrack and try them instead!
"does option pricing in Haskell"? I guess I don't know what "option pricing" is. Although, I think it has something to do with stock prices? What is "option pricing" and where do Monte Carlo techniques come into it? Does this library simulate the stock market or attempt to predict future prices?
Don't we have a deprecation or even black-list mechanism that would prevent `cabal-install` from trying a package version marked in that way? I know folks did think about it before adding the revisions mechanism, but requesting the cabal file at a particular URL and getting different answers on different days still seems like a non-starter to me. I am totally okay with `cabal-install` getting some help to avoid trouble, but that should be a back alley transaction between hackage and `cabal-install`. That this leaks into the most accessible way to query package metadata is unfortunate.
ok, but then I don't understand what you mean by "mutable URLs on hackage"...
Perhaps I'm misunderstanding the revision mechanism then. When a package such as [http://hackage.haskell.org/package/arithmoi-0.4.1.1/arithmoi.cabal](arithmoi-0.4.1.1) was released, would someone requesting that URL get a .cabal file with an "x-revision: 2" line in it?
That should be handled by a deprecation flag.
I don't see how you can be concerned about both those scenarios simultaneously. If the bad version is somehow dangerous, then breaking a previously-valid install plan is good. If you want to continue allowing build plans involving the bad version, then falling back to it in the situation where the fixed version doesn't result in a valid build plan seems okay, too.
Saved to my reading list :)
what the text doesn't state very cleary is that version-deprecation and preferred-versions are the very same mechanism, and `cabal-install` can't distinguish between those two; so version-deprecation is in fact version-non-preferring
The package listings pages aren't versioned, though! The problem is saying that this is version X.Y.Z of some resource, then changing it. I'd also complain if you offered a hackage listings page that was time stamped, but then you edited it without updating the time stamp.
**[Interest intensifies]**
An "option" in this setting is the "right to purchase/sell a stock at a certain price at a given future date" (or sometimes at any point _until_ that future date). Obviously, options have prices, since people buy and sell them. But there are _too many options_ in that for any given stock you can vary both price and date to get a _whole matrix_ of options. The idea is that this whole matrix should be able to be determined in a uniform way by a small number of things. The central ones are 1) The price of the stock now, 2) the time until a given future date, 3) the difference between the price now and the price the option is set at, and 4) a magic number called "volatility" which captures how much we expect the stock price to "jump around" (either up or down) in the near future. [The other elements that go into this have to do with interest, dividends, and other ways the world is "less ideal" than the assumptions in the basic formula]. It turns out that only under _very strong_ assumptions do we have a "closed form" mathematical expression of that desired sort. Instead, we typically give probability distributions for the way in which certain events will unfold (in particular, but not only, the movement of the stock as randomly determined by its volatility) and "monte carlo" a whole bunch of different scenarios, and then average their resultant price to get the "true" price. That's the sort of thing this library is doing. (edit: btw to the OP, this library looks like it's off to a strong start! thank you for sharing it with us)
Perfectionism. 
Yes, that's a great paper, and I was very slightly influenced by it, but at the end of the day the way the ContingentClaim type worked was driven by the needs of the Monte Carlo engine. I'm still thinking about how to move it more towards the approach outlined in that paper.
&gt; We Restrict the Logic to ensure that all our VC queries fall within the decidable fragment. Does this also mean that it doesn't really matter which SMT solver you pick (modulo speed)?
This will help us understand. &gt; Lean is not a standard SMT solver; it can be used as an automatic prover like SMT solvers, but it can also be used as a proof assistant. The Lean kernel is based on dependent type theory, and is implemented in two layers. http://www.cs.nyu.edu/~barrett/pubs/BdMF15.pdf
Thanks. I got it all working. [Here's the thread](http://www.reddit.com/r/whowouldwin/comments/30ilvv/crowdsourced_character_power_sorting/) if you're interested.
In theory, yes, we should work with any SMT solver that supports SMT-LIB2. Though some of our nicer examples require the theory of finite sets, which is not yet standard. In practice, we test LiquidHaskell against Z3 and CVC4.
Yeah, first thing: do a cabal install alex, the included one is broken. It tries to load files from the cabal directory on the build computer. 
&gt; [Z3] is available for free for non-commercial uses — you can even download the source — but it is not “open source” in the full sense of the word (i.e., OSI compatible) and is not free to use for business purposes. Can we get an errata to this section? &gt; This post is licensed under a Creative Commons Attribution-NonCommercial-NoDerivs 3.0 Unported License. EDIT: Because of [the license](http://creativecommons.org/licenses/by-nc-nd/3.0/), we'll need to get separate permissions from Tikhon Jelvis to modify it, or wait until he publishes an updated version. Now the problem isn't that Z3 is proprietary it's that the introductory article is! Gah! I wish everyone would just quit using half-open (NoDerivs / NonCommercial) licenses.
Currently version blacklisting does next to nothing. You can do it, and cabal will still happily pick that version in an install plan. Operationally the current state of things has reduced the number of issues I get filed on me from bad install plans to basically zero. This does come at the cost that the content at the stable URL we have for the cabal file changes. The latter makes me uncomfortable, but while I don't like the mechanism and would prefer to get my bounds right in the first place rather than rely on it, it has pretty much solved the overly-permissive upper bound problem.
I appreciate that it addresses that problem, and I'm certainly not arguing that the right solution is that you have to get it right the first time. I'm arguing that adding a second versioning scheme to a versioned resource is beyond a code smell. The build plan solver is a real asset, and its job is certainly difficult. Having a blacklist is a completely reasonable thing to help it work with a persistent resource. Monkey patching like this is a shortcut that just causes problems elsewhere, and is a strategy that we should know better than to rely on.
yes, that's what I did in a sandbox. But when I moved the executables, the same problem appeared. So, I just built them in the user space. Also, you have to remove the alex and happy from MinGHC because of how the PATH is modified.
Its an IBM machine, use it in winter to warm yourself up. They're like a space heater. Loud yes but they get the chill out of you fast. (honestly not joking, at one job I used to go through the datacenter in mid winter and warm up behind them, it worked perfectly)
Let's get Chris further up the list!
The google project nh2_ links to, ganeti, definitely has a bunch of Haskell code. 
"Type variable would escape its scope" is one of those errors that we need to teach more people how to read. It seems scary, but is actually very informative.
Aside from the obvious * unsafePerformIO * unsafeCoerce We still don't quite have parametericity thanks to the likes of * seq * catch 
Not just Agda and Coq! There's a mizar-style syntax for proof terms too. On first inspection I like some of the module system niceties (the `variable` syntax feels very natural to me), and I like that it is a small core with most features introduced by elaboration. I think the notion is that it is supposed to be both a tool and also a component that can fit into larger systems more comfortably than e.g. Coq. And unlike Agda and even moreso Idris, it is focused entirely on being a prover/checker and not at all on program extraction. I like the idea that this is built for people to just "reach for" when they need it in a sense -- but I feel it will need some effort more along the lines of the Verifiable C toolchain or the equivalent for hardware/chip specs if it is to make strong inroads in industry. (i.e. i sense the goal as more to take coq/agda techniques into ACL territory than to compete with coq and agda in the areas they are already strong).
&gt;following 0 I see you are a very social fellow!
The ordering is "number of reddit_haskell followers who follow this account." Seems pretty straightforward to me.
&gt; seq Used to be in it's own typeclass, though it doesn't affect fast and lose reasoning, and that's really all we get when we don't distinguish between least fixed points and greatest fixed points.
&gt; That this leaks into the most accessible way to query package metadata is unfortunate. While this can be considered a questionable design it can still be changed. The one thing that was needed for the large installed `cabal-install`user-base to retroactively benefit from `.cabal` editing is only to have the latest `.cabal` revision used in the index-tarball. The unversioned `.cabal` URL doesn't actually have to be pointing to the latest revision (otoh it *may* be confusing if it didn't, as `cabal-install` would use a different revision than the one you get presented via the WebUI by default). So are you only complaining about the default `.cabal` URL mutating (which could be easily changed w/o breaking the installed/deployed `cabal-install` user-base - otoh I'm not sure whom this would benefit, as the rev0 `.cabal` is available under an alternative URL as well), or do you have a problem with the content-mutating `00-index.tar`-URL as well?
I'm genuinely curious what platforms you're talking about, and why &gt; generating native code, either manually or via LLVM, is impossible and/or prohibited And why, otoh, compiling via C is possible. To answer your question, if you go with jhc, it'll take you a lot of work to get packages to compile. The basic packages, like vector or bytestring, need to be ported, because jhc lacks ghc's primops. (But maybe they have been ported already, idk) Higher-level packages will often fail because they rely on ghc extensions. It is often possible to rewrite the code without the extensions, but it is work nevertheless.
Point taken. I opened an issue on github: https://github.com/haskell/haddock/issues/376
/r/dataisbeautiful would love a graph or something, I believe.
This doesn't seem to cover [abstract refinement types](http://goto.ucsd.edu/~rjhala/papers/abstract_refinement_types.pdf), are they beyond the scope of the introduction?
I don't know, but it's very good for code review and diffs...
Lean was pitched to me by a recent friend as "a new language with a syntax aimed to make it easier for new people to learn, write, and understand the things you'd normally do in Coq" Not sure how accurate that is but I liked what I read. The tutorial is nice.
Isn't the sample scala code here basically acting like "listT done wrong" as seen in transformers? You could just as easily model listT correctly in scala, using flatMap and proper stream concatenation. The difference is that Haskell takes away "cheating side effects" and thus *requires* more thoughtful effect management. With its powerful type system and library support of this notion, Haskell can help prevent you from doing it the "wrong way".
Marlow! &lt;shakes fist&gt; :p
Refinement types are cool because, well, they allow you to *refine* existing types. We can simply layer refinement types on top of regular Haskell, in order to express and verify stronger guarantees about Haskell code. The runtime is untouched. Dependent types, as I understand it, are simply more powerful than refinement types. You can make use of proof objects at runtime. This means you can basically prove more things; you aren't limited to the smt solver for constructing proofs. So dependent types are "better" at proving more things. But refinement types are "better" at proving things automatically, and "better" at layering on top of an existing type system. Someone please correct me if I'm wrong.
Yes, all packages need recompilation after a major version update
I prefer the term "m-actions", but I worry about how ivory tower that might sound.
`async` is the [bee's knees](http://www.grambarcelona.com/wp-content/uploads/2012/04/BEESknees_lo.jpg) 
So what's the definition of pure and impure? I have no problem saying that *within the state monad* `put` is an impure operation whereas `get` is pure.
That's coming soon, I promise :) I was going to write about it but then we found some exciting new things we could do with abstract refinements so I'm just waiting for the dust to settle a bit before adding those chapters.
&gt; Now, if you paid attention on your Theoretical Computer Science classes And this is why Haskell has difficulty breaking into mainstream. :P
Abstract refinement types are very exciting, looking forward to it.
Refinement types are irrelevant, therefore they are utterly useless at runtime. "Irrelevant" means that there is at most a single value of the type up to definitional equality. This is a key property that allows us to employ automatic search in the first place. If there were multiple different values, then it would make a computational difference which one we choose, and we couldn't just let a solver loose on it, because the semantics could change depending on which proof the solver happened upon.
Thank you!
Regarding the effect of priting, I'd encourage you do try doing tests that verify that you're not printing any sensitive data (e.g. credit card numbers) to log files in a non-monadic language (e.g. Java, Scala or Go or whatever)... you'll probably discover that it's almost always absurdly difficult to "capture" the log output (and be *sure* that you've caught it) because of the lack of explicit modeling.
We needed a code review tool and GitHub is inadequate for us in a few annoying ways. Overall, Phabricator is quite nice, however, and very extensible (which is a bonus).
I agree with this. I was just trying to avoid repeated use of the word "side effects", but I think you are right and it is better to think of everything in Haskell as pure, including side effects.
I don't really have any assumptions about what URLs `cabal-install` accesses (that's not to say I do know precisely how `cabal-install` interacts with hackage), my comment was a complaint about what is done on hackage with the specific example of the URLs for individual `.cabal` files. I do not know why those files are changed in-place, nor do I know if there are other resources that claim to provide a particular version of a package, yet whose contents change over time. My position is that something that has a version number or time stamp should not have its content changed without the version number or time stamp changing. Is that ambiguous in some way that I am just missing?
I was only half-serious, heh. And yeah, most programmers likely know about those particular topics, so it was a pretty bad example to pick on. But really, I was referring to the assumption that everyone with an interest in Haskell *must* have a formal education in CS. Not even "should" -- it's just taken for granted that you wouldn't even be here otherwise. It's a cloistered mindset and, even though it's not really intended to shut anyone out, it shapes the discussion and documentation in ways that can often make it self-fulfilling.
Thanks for the clarification.
Do you know if it could (easily) be extended to work with Tasty?
I have some plans to make it do a lot more. I will definitely take a look. Would you be able to open an issue on Github?
Awesome, was looking for something like this for our Yesod projects. How does it map source to test files, is it by convention? I'm currently using the convention of e.g. src/Handler/Foo.hs has corresponding tests at test/Handler/FooSpec.hs . Would that work, or do I need to rename things?
Ah interesting, thanks for clarifying. Might be not a good fit for Yesod integration tests then, since the test file wouldn't really be including the handler source, as most tests just throw HTTP requests at a certain route.
http://en.wikipedia.org/wiki/ACL2 Probably the most "industrially used" prover--for hardware and chip things, etc. Not really based on curry-howard or anything -- just a very strong ability to search over rewrites, etc.
GHC has a `-fdefer-type-errors` flag precisely for quick development. But it doesn't really speed up compilation in itself, because Haskell's type features are such that code cannot be correctly compiled without knowing what the types *should* be. Instead it turns type mismatches into fake coercions that give an error at runtime. That means you can still test code that is partially buggy, which can reduce the *number* of compilations you need.
I hope i remember when off the phone ;). Meanwhile if found this on HN https://news.ycombinator.com/item?id=9283478 A code test link being used at Facebook, a nice optimization for their CI system.
I wonder if /u/ocharles will hear this summons! Could be that he is too busy right now to publish the errata.
this. this is what the world was missing
That looks very interesting. I still like running at least the related tests. One feature that I'm planning to add is to run all the tests when you quit Arion. 
To be honest, this sounds like a question that's better suited for StackOverflow. 
Didn't see that, thanks for the clarification. Is the 4-6 months window based on an estimate of how long it would take maintainers to provide 7.10-friendly versions of their packages?
Everything powered by Haskell! :)
That's a great paper, but there is no pricing algorithm which works in that generality. PDE models are only practical in low dimensions (2, maybe 3). Monte Carlo is very general but it will still blow up when you need to observe future implied volatility, for example. The standard solution for that is Longstaff-Schwartz, which I don't think can be implemented in the required generality (also you can iterate the problem and blow up quite a few more times...) 
Oh okay I remember reading about `-fdefer-type-errors` at some point, never really tried it myself. 
How long can the delay be? Btw, I know a doctor who has a doorbell like this. (A packaged hardware solution, almost certainly not powered by Haskell.) It enables the doctor to respond when someone knocks at the door with a medical emergency even when he is not home. This doorbell also includes an "intercom" feature, which actually connects through to the doctor's cell phone.
&gt; Surely most programmers must know about Turing machines and the halting problem? Do you think most programmers have studied computer science? I am doubtful.
How does this compare with [operational](http://hackage.haskell.org/package/operational)? Operational seems to have very similar goals: "writing web applications in a sequential style, programming games with a uniform interface for human and AI players and easy replay capababilities, implementing fast parser monads, designing monadic DSLs, etc." It is a classic, well-documented, very mature library, written by [Heinrich Apfelmus](http://apfelmus.nfshost.com/about.html). And how about [MonadPrompt](http://hackage.haskell.org/package/MonadPrompt)? It also seems to have similar goals.
I didn't see any import of `ListT` at all, so I assumed he was referring to the standard one from transformers. I didn't know that `import Pipes` brings a different, non-standard (but apparently better) `ListT` into scope. Perhaps that should have been made more clear, and a link provided to the `ListT` being used.
But the function does change the state of the CPU mannnn
Only in some models of Haskell's type theory.
I'm not sure how the CDN in front of haskell. org affects hackage, but I don't think it currently allows `cabal` to work properly when hackage is down.
It's an improvement, but I still don't think I've seen an explanation for why we need to modify versioned content without bumping versions. I've asked a few times, but I can do so again: Why is the preferred versions mechanism insufficient to help cabal avoid bad build plans? If bad versions are dangerous, then they should be blacklisted. If they don't result in valid build plans, then the preferred, updated version should be chosen by the solver. Again: we are mutating versioned data without changing the version specifically to support an apparent shortcoming of one of our tools. This makes the data repository itself less useful for any potential consumer other than `cabal`. 
Careful. We don't kindly to you big city "operational semantics" types in these parts, son. :)
I updated http://ghcformacosx.github.io/ with Mark's latest 7.10.1 build last night.
Really? Cause the smiley face kinda makes it seem like you do.
Is there a good way of detecting / catching when it tries to fetch a deleted comment? Right now it just throws an APIError.
I still consider it impure. It's usually a good idea to avoid implementing logic that results in results with an `IO a` type. Do notation makes shooting yourself in the foot too easy. We'd be in a better shape overall if do notation was never introduced.
Why my packages have been commented out? # GHC 7.6 # "Alberto G. Corona &lt;agocorona@gmail.com&gt;": # - RefSerialize # - TCache # - Workflow # - MFlow 
that looks like it should work properly for most things i plan to add a bunch of utility functions for things (there's already `pipes-reddit` if you fancy using `pipes`) but i need to actually pin down the API properly first
Most of the time the delay between pressing the button is about 2-3 seconds. Every so often (twice in the last few months) it appears a notification falls into the abyss for a while. My intention is to augment it with something that will actually make a sound directly.
You may not like IO (and for the record neither do I), but that doesn't make it impure.
That whole period is confusing in the history because of the switch to the YAML config file (which has drastically simplified things for me, and I hope for contributors as well). I can't seem to find an issue where I described what was going on, so apologies for the poor communication. I think the curation team has figured out more regular communication patterns to follow since then to make life easier for everyone.
I'm secretly hoping LTS 2.0 has bumped the major version number because it's designed for MinGHC for GHC 7.10. Do I win the prize of 7.10 this week?
Standard internet for "the preceding statement should be taken with a grain of salt" given this is generally a serious sub and English might not be the first language of everyone here.
Why not expand the name LTS more frequently into LongTermSupport on web pages and blog posts etc, for folk who aren't familiar with Ubuntu Long Term Support as a namesource. As far as I've spotted, the TLA is only expanded [here](https://www.fpcomplete.com/blog/2014/12/backporting-bug-fixes).
Sounds good, will do on Mon, thanks for taking this in consideration.
Lol I know, I leave out my /s tag often.
Really loving the aesthetics on the site
Nope, this bump was planned since LTS 1 was announced.
Thanks for the heads-up, that's good (if disappointing) information to have.
Looks premature, https://github.com/fpco/ide-backend is not visible
Oops, I knew I forgot to do something this morning. Should be fixed now.
I really do like the simplicity. Even very basic usage of Refinement types already adds great benefit! These kind of things make me very happy to be a Haskell developer saving me both time, money, and effort.
Yes indeed, thanks. Look forward to investigating this, I would love to have a common backend for all the IDEs
Hooray!!
Thanks! I'm really excited about about both of them. GHCJS+React in the wild baby... Will the code for the "inspection tool" also be released as opensource?
Yes, once it's ready. It's only a prototype for now.
Funny that this pops up just after people were talking about the lack of plugins for ST3 :P
Do you know what Conal is talking about when he writes this? &gt;Before monadic IO, there was a lot of vibrant and imaginative work on functional I/O. 
Thank you so much! However, I'm a bit confused by the number of IDE backends out there now... It would be great if the authors could write up some comparisons and perhaps start a conversation about working together or filling gaps in each others' tooling. I hate to think that all of this work from various open source contributors could be going to waste. We're too small of a community to fragment on tooling. For example, this person has been pretty quiet about their work in progress, but when I look at https://github.com/mvoidex/hsdev I see that it has received 340 commits over the past two years.
What about , also make Default a superclass of Monoid ? So that, `def`, `empty`, `mempty` etc ... can be unified.
Wouldn't we just be generalising `&lt;&gt;` from `Monoid` to `Semigroup`? (Similiar to how AMP allows some operations to be generalised from `Monad` to `Applicative`) Right now, in `base-4.8:Data.Monoid`: (&lt;&gt;) :: Monoid m =&gt; m -&gt; m -&gt; m while `semigroups:Data.Semigroup` has: (&lt;&gt;) :: Semigroup a =&gt; a -&gt; a -&gt; a and if `Semigroup` ~~implies~~*is-implied-by* `Monoid` at some point in the future, `&lt;&gt;` would continue to work in existing code, wouldn't it? 
I don't think that'll make it through the gates as it has no laws, like [point](https://hackage.haskell.org/package/pointed-4.2/docs/Data-Pointed.html). I think it is preferred to have one big class of methods with some documentation to state their laws, over separate classes with some documentation to state their laws. I'm not sure why.
Before making this open source release, I started an email thread with maintainers of related projects to see if there's some way we can start collaborating. I hope we're able to move that forward.
To do this without subtly breaking existing code, we'd have to issue a warning about using the current `Monoid` instance in 7.12, then remove the instance in 7.14 en then introduce the new instance in 7.16. Instead we could also improve working with `First` (or `Alt Maybe`) using pattern synonyms: pattern Success a = First (Just a) pattern Failure = First Nothing 
I know it's hard to turn the ship around. Success has not been avoided and the cost is considerable. Your "instead" doesn't help with either of the situations I outlined above. It helps only at close quarters, and in a way which is probably below the Fairbairn Threshold. The point about getting the Monoid instance right is to get type-directed aggregations working in accordance with the semantic purpose the type describes. (Yes, types describe semantic purposes, not just data representations.) So I guess nothing is going to happen. I wonder how we might design a programming language to manage the impact of library evolution. At the moment, we have no good way to say "I have noticed this change and updated my code, so stop warning me.". Let's take this example. Suppose we had a way to make existing code which uses the Data.Monoid instance for Monoid (Maybe a) splutter noisily or even break: some kind of instance damnation. The fix would be to include an option which in some way acknowledges the change. More broadly, it's worth thinking about what information we would need to put into our code if we wanted to improve the quality of advice the compiler can give us when (for it's surely not "if") the library changes.
That's great, I really think that kind of tooling around Haskell has the potential to be extremely helpful. It would be amazing to have a nice cooperative ecosystem develop around ST3, emacs and vim.
What's wrong with newtype Parser x = Parse (String -&gt; First (x, String)) deriving Monoid
&gt; Monoid implies semigroup, not the other way around. You're totally right... D'oh! I was thinking of `class Semigroup a =&gt; Monoid a where` and messed up when translating that to an English sentence... :-)
So hm, how does this compare to using `haskell-mode`, `flycheck-haskell` and `ghc-mod`? Is it meant to replace (eventually) all of them?
It is a very good idea
This project provides the core library which tools like `haskell-mode` et al may use. By itself, it doesn't provide any end-user functionality. In the blog post we have a video of an [ide-backend-client](https://github.com/edsko/ide-backend-client) plugin for emacs, which uses this library.
And now `Foldable` has leaked all over the `Prelude`. *sigh*
Who wants to shed bikes about the new syntax (#)?
This has the breakage problems of both AMP and FTP. Like AMP, it would break existing `Monoid` instances that don't provide the newly required superclass instance. And like FTP, it generalizes the type of an existing function, causing breakage wherever the more specific type is required for type inference.
Have you guys seen this: http://nikita-volkov.github.io/record/ Here's the [reddit](http://www.reddit.com/r/haskell/comments/2svayz/i_think_ive_nailed_it_ive_solved_the_records/) discussion.
While I agree with the AMP-like issues, I'm not convinced about the FTP comparison, as the generalisation in this case is really more like going from polymorphic `forall a . Monad a =&gt; a`s to `forall a . Applicative a =&gt; a`s, rather than going from `[a]` to `forall t . Foldable t =&gt; t a` which introduces a new `forall`
If anyone understood `Foldable` it would presumably have laws.
&gt; another important piece of the jigsaw is some way to refer to fields that may belong to multiple datatypes. http://m.youtube.com/watch?v=P3ALwKeSEYs
So much awesome! 
Indeed. And it's certainly not the end of the world; we'll live with it, and everything will be fine. I'm still allowed to sigh, though.
Algorave -- Music brought to you by people that looked at existing raves and thought: "This really needs more Haskell."
Yeah! # is too much overloaded already. Let’s use something different! ♯ is nice. 
Yes, I spent quite a lot of time participating in that discussion. ;-) The short story is that part 1 (Records with duplicate field labels) is orthogonal to the `record` library, while part 2 (Polymorphism over record fields) fits with it neatly: one should be able to use the new overloaded label syntax to access `record`-style anonymous records.
Blargh, new syntax. Didn't we have a good proposal around that didn't introduce any new syntax?
How to proceed: I'd like to have http://hackage.haskell.org/package/lattices and http://hackage.haskell.org/package/edit-distance on Stackage (maybe it's too late for LTS 2 though). The maintainer seems to be the same as of `ansi-wl-pprint` (https://mail.haskell.org/pipermail/libraries/2015-March/025352.html related?). In fact I could (help to) maintain both, if help is needed.
If we're going to use unicode, I'm sure we can find something more amusing, such as ☞ or ☢... ☺ More seriously, I'm open to better suggestions, but I suspect we should restrict ourselves to ASCII, and that makes it hard to find spare syntax. One proposal was a prefix @, but IMHO it would be a shame to steal that from type application.
That is unfortunate and I apologize for that. The code resided in one of my code directories and I thought that it was done by me, so I put it on my Github. Comparing the two files, I see that indeed I have changed it a bit, but I could have just issued a PR. I will delete the repo and issue a PR. Thanks for that.
People who have practical experience with polymorphic record fields (e.g. Purescript users), where you can have type signatures like this: getId :: r { personId :: Int } =&gt; r -&gt; Int Has that been helpful to you? I think this `personId` example is the first time this made sense to me, since I can imagine database records all being created by one team and following strict naming conventions. But for other records, won't there be a bunch of cases where the name doesn't exactly match, so you don't get to take advantage of the polymorphism (e.g. one record with the field called `ip`, another called `ipAddress`, another called `ipAddr`)? Additionally, won't writing functions that rely on row polymorphism prevent you from easily calling the `getId` function when all you have is an `Int` (I can imagine that having type safety benefits, but the Persistent approach where `UserId` would be its own type seems preferable to that situation)?
In "A History of Haskell: Being Lazy With Class", section 7 "Monads and input/output" they talk about two other alternatives to IO: streams and continuations. 
Yes. I'd put it the other way around. I'm extremely suspicious of `Alternative` instances which say anything other than instance (...) =&gt; Alternative ... where empty = mempty (&lt;|&gt;) = (&lt;&gt;) And wouldn't we like quantified constraints `(forall x. Monoid (f x))`?
Main links: www.functor.se/careers www.twitter.com/functorcareers Also our LinkedIn ads, but apply by email please: https://se.linkedin.com/jobs2/view/19207355 https://se.linkedin.com/jobs2/view/10989030 Functor AB provides a culture of excellence where talents can excel in very challenging software development with our world-leading technologies and with state-of-the-art practices.
Yeah, pretty much. I have a feeling we could achieve something very similar right now with pattern synonyms. That might be the best we can do.
Ah, good to know about the bug. I'll upgrade. Replacing the type families with type classes required `UndecidableInstances`, but it compiles, thanks!
Could we *gasp* use a dot to indicate member selection?
Then sigh *democratically!*
I'm pretty sure the dot has been discussed at length already, and the conclusion was that it would cause too many conflicts as `foo.bar` (with no whitespace) is already commonly used to compose functions, e.g. in lens :(
I have been building one... But as always the problem is time, and I've been especially busy the past couple of months. On a related note, does anyone know of any haskell to python compilers? 
Wait, where is `#` overloaded outside of GHC internals?
This was my first thought as well.
Remember CPP: #if #else #endif #define. All use front-hash syntax. Meanwhile -XMagicHash uses final hash and because of that causes no problem.
Default allows ones, to produce a value from nothing, without having to know anything about the type. So I can do things like foo :: (Default a, Default b) =&gt; (a, b) and also give a kind of canonical random value ;-) 
Well, yeah, but having a value-out-of-thin-air without knowing what can I do with that value always seemed pretty useless to me.
I don't think "studying computer science" really means anything right now. I mean, I partially envy people who had access to decent CS education, but dare to say I learned more from reading [PFPL](http://www.cs.cmu.edu/~rwh/plbook/book.pdf) than if I came to CS course at my university for five years worth of bashing over the head with object-oriented methodologies. 
Ah, I assumed it would be in Data.Traversable.
I guess it might be doable, but you'd probably have to use a weird subset of the base library and use some IO wrapped memory shenanigans.
Generic deriving for products of other Default instances is nice. That said, I've been shying away from the Default class for a few years now.
…should've guessed it'd be `lens`. :p
Just in case you're not already familiar with it: I use and love evil-mode to VIMify my emacs.
&gt; At the moment, we have no good way to say "I have noticed this change and updated my code, so stop warning me. Interesting challenge! Here's a crazy idea. How about using version ranges? We already specify something like build-depends: base &gt;=4.8 &amp;&amp; &lt;4.9 to tell cabal that we claim that our code will work with those versions. Suppose base-4.9 changes the semantic of `Monoid Maybe`. If we were all super careful about reading release notes, telling cabal that we know about the change would be as simple as specifying a broader version range: build-depends: base &gt;=4.8 &amp;&amp; &lt;4.10 But of course, people don't read release notes, so we'd end up with lots of subtly broken packages which have incremented their version bounds without understanding the implications. So for breaking changes, I suggest that the above should be *rejected* by cabal, because you are indicating to cabal that you believe that the same code will work with both versions, something which cabal knows is likely not to be true. So instead, you'd have to explicitly specify that your code cleverly supports the two different semantics by specifying two disjoint ranges: build-depends: (base &gt;=4.8 &amp;&amp; &lt;4.9) || (&gt;= 4.9 &amp;&amp; &lt; 4.10) Now you're being explicit about the fact that you know about the breaking change. An important feature of this proposal is that unlike obvious solutions like adding an IKnowAboutMonoidMaybe pragma, you wouldn't have to add more and more such pragmas over time. If you start a new project, you simply write a range which begins at the current version, as always: build-depends: base &gt;=4.9 &amp;&amp; &lt;4.10 And since this codebase has only ever seen a version of base in which `Monoid Maybe` is the same as `Alternative Maybe`, that's what the code expects, and there is nothing extra to write to tell the compiler that you know that it once was different.
But that usage can be reasonably considered obsolete as of 7.10, since now `(&amp;) = flip ($)` in base.
I do feel that row polymorphism (and similar systems) is the elephant in the room. Although the blog post pointedly decided not to discuss it, so I have to give the author that.
1. ImplicitParams? 2. Won't changing field accessor syntax break many more modules?
Oh this is very cool to see several implementations. A bit off topic: is there a good tutorial for casual compile-to-FPGA and running on (e.g.) the cheap dev boards out there? I have one I wanted to play with, but got into a rut downloading several gigabytes of Xilinx tools... felt like I may be going down the wrong path.
Also, lens already does a pretty good job as a library for selectors
There's no escaping (that I know of) of that several gigabyte Xilinx package. But at least once you have it installed, [kansas-lava-shake](https://github.com/gergoerdi/kansas-lava-shake) tries to alleviate some of the pain in using it. See [this example](https://github.com/gergoerdi/chip8-papilio/blob/master/src/Build.hs) to get some idea of how you can integrate it into your Kansas Lava workflow.
I like the colour of that paint (although I don't know how much stuff it would potentially break). Are you on the libraries@ mailing list at all?
 A system? Please add some details on what is covered.
I also feel put-off by introducing new syntax. Can someone comment in more detail why the hash is needed, and this proposal wouldn't be feasible without it? I think leaving it off looks fairly intuitive.
Oh, that's a very nice addition.
ghcformacosx already has 7.10.1 available.
Yes, I didn't want to spend too many pixels on the type system issues (for once!). The [wiki page](https://ghc.haskell.org/trac/ghc/wiki/Records/OverloadedRecordFields/Redesign#Overloadedrecordfields) has the full story, but the basic idea is that `r { personId :: Int }` is syntactic sugar for a pair of constraints `(HasField "personId" r, FieldType "personId" r ~ Int)`. Together these represent the fact that a field `personId :: Int` belongs to the type `r`; they don't say anything about other fields in the type. Row polymorphism is great for implementing anonymous records, but it's not obvious how to fit it into the existing Haskell records system. The goal here is to make Haskell records slightly easier to work with, not to introduce a completely different system.
git with signed commits is a distributed secure ledger where there is no possibilities for: - versions appearing and disappearing without trace. - a server lying about the current state of HEAD (for any significant time, as it will stall) - administrators doing whatever they want - missing signatures on sha2 of packages. So hackage doesn't have a useful security architecture, while it has a lot of useless features, like a web interface, user management, search functionality etc. None of that is essential to secure distribution of a ledger/consensus for what packages exist. None of that is needed since the ledger itself should do user management (have a directory of users' keys). No web UI is necessary, and it should be external to the core of the system. It only hinders getting real security.
In general, I doubt that it will be all that useful to expose type signatures like that in APIs (without some form of anonymous records), though database systems might be an exception. Rather, I view `r { x :: t }` constraints as a technique to get field name overloading to be resolved in a predictable way using type inference.
Ah, that makes more sense. So it's more or less an alternative ghc-mod. I'll probably give it a try once it has company and flycheck support.
How do folks here follow along with twitter? I never remember to go there. Are you live-streaming it into your google glasses?
It's really a question of complexity: a bit of syntax vs. a radically different way to resolve identifier names in some circumstances. Having the syntax gives us a simple design for "overloaded labels" like `#personId` that is orthogonal to the record system extensions. Here's a concrete example: suppose you are using anonymous records in the style of the [record library](http://nikita-volkov.github.io/record/). Then `#personId` can just work as a selector for those records, regardless of datatypes in scope, but if you want `personId` to function as a selector, then you either need to introduce a dummy datatype with a field of that name, or have yet another way to bring field labels into scope. 
Testing for regressions by building dependent packages is a great idea! I assume you also run their test suites?
&gt; Salary, compensation, and other benefits is subject to negotiation and the qualifications of the applicant. This is the standard with job applications, and I think it's rather bad. Not only does that mean you have to do a lot of work upfront to even know whether it's the salary range you're interested in, but furthermore negotiating salaries tends to deepen the wage inequalities among workers (such as lower average salaries for women, see the [psychology studies](https://hbr.org/2014/06/why-women-dont-negotiate-their-job-offers/) demonstrating that women negotiate less on average, and that when they do they are often badly perceived as a result). In an ideal world, entrance salaries should be fixed (and announced) based on the job requirements, and later evolution should be determined by the actual work done -- and not you (un)ability to sweet-talk the manager.
More like a very simple algebraic expression type bundled with almost no functionality, and what is implemented is overly complicated and frankly, ugly. I don't like project announcements with empty github projects, and this does not have much more content.
They are not a particularly friendly bunch. Been there. I continue to use brew, but contributing back is a real PITA.
The part1 seems to be some kind of type directed name resolution (different from the TDNR proposal though.) Can the same be done for all identifiers, not only record fields?
You could also have a pragma and have cabal take into account only pragmas that affect your version range. That way the information would be listed clearly in your .cabal file and you wouldn't have the infinite pragma growth problem either.
In what way, and from what perspective? Feedback always welcome.
have a look at [ATS](http://www.ats-lang.org/). 
Yes that makes sense.
Can you please clarify the experience requirements a bit more? I think that you probably want someone with a couple of years of industry experience and not entry level. Job descriptions like yours always confuse me, because they mention many subjects I've touched while studying, but I think I'm not qualified because I lack "real" experience.
Bassline isn't clean and distinguishable (and it's important for music which is advertised as techno), and the sounds are too simplistic (mostly sawtooth waves with minor tweaks).
So this tries to replace the concept of a function name with a hash that "describes" the function in question, which is a key into a global repository. Why not encode in the hash * the type of the function and * the (e.g. `QuickCheck`) laws that it is supposed to satisfy and get the "best" (best performing, most compact, depending on your optimization settings) equivalent function from that global repo?
been a fan of yours for a long while and was thrilled to see you on computer club (the poborsk release was fantastic). congratulations &amp; thx for putting together such a neat release
I don't understand your reasons here. People shouldn't have such tight upper bounds that a patch-level change to a dependency requires any change. The main argument for immutable versions is just so that the concept of versions has some meaning. I'd say it is necessary but not sufficient for repeatable builds. If we can't get our hands on specific versions of code, we can't have repeatable builds. Thankfully the proposal here will mean that we can at least hold on to a git hash for a version of the metadata repository until the community starts advocating `git push -f` as the expedient solution to cabal limitations :P
The fact that linking is typically _so much more expensive_ than compiling, and the decreased possibilities for large-scale analysis available to the compiler would suggest to me that this is a nonstarter.
There is no lack of mathematicians who are great communicators, one skill does not preclude the other!
Correct me if I am wrong, but wouldn't a smarter way of combining the fragments solve that problem? I see no reason why there couldn't be a way of putting everything into a single module instead of twenty. It is of course harder, but not impossible.
Aren't you assuming a lot about the behavior of GHC? I think this is supposed to be a proof of concept. With compiler support I would think a fragment-based system would perform better due to not having to deal with dead code.
For some reason everyone completely abandons all of the arguments about how the syntax of a language is important and shapes the way you program in it when we start talking about field access in records. Accessing fields in Haskell records is disgustingly bad. /u/tibbe and others have made some tongue in cheek comments about using `foo.bar` and the like, but the fact is that that is a *superior* way to do it. It's easily readable and it's intuitive to anyone who's looked at code from any other mainstream language in the last 20 years. The current solutions that involve template Haskell messes are basically just ridiculous. It's like arguing that function composition is easy in C++, all you have to do is write a bunch of ugly macros and template code! Easy peasy. Everyone should be satisfied. I'm all for having a small, simple language, but not if it means that basic, common use-cases aren't supported. If we wanted that we could just write everything in scheme.
There are no versions of a slice. The dependencies reflect the entire static call graph. You are right that types are not sufficient to describe an implementation and I believe nothing is except the implementation itself. My plan is to have explicit updates that replace certain slices with other slices. You can then tag updates as "non-breaking" and you can semi-automatically apply such updates depending on your policy.
Thanks, always interesting to learn what other people hear. This is how I like it to sound though, and I'm not sure why you're hearing mostly sawtooth waves.
The hash encodes the entire static call graph of the function. The idea is that you can not observe a difference between two functions with the same hash. You could have other, weaker, useful notions of equivalence. My plan is to have explicit updates that replace certain slices with other slices. It is hard to define what is "best" for everyone. I'd rather start with something predictable and try and build heuristics that suggest updates on top of it.
I would go with 'name since template haskell already steals that syntax
Actually part 1 doesn't really extend the type system in any interesting ways at all, which is sort of the point. Apart from some cleverness about disambiguating record updates, it essentially lifts the "maximum of one field with any given name" rule without changing how name resolution works in expressions. I suppose part 2, the `#foo` syntax, is a bit like a limited form of TDNR. It is a bit more general than record fields, though I haven't explained the full generality in the blog post. You can think of `#foo` as an overloaded identifier, which will typically refer to a record selector when used at function type, but may have other interpretations at other types.
I'd suggest that the comment shouldn't have to be at the very top, but anywhere in the file. Possibly even within &lt;!-- --&gt; tags. This would make it easier for people rendering html pages using something like hakyll that already require some information at the top of the file. Putting the notewithin html comments would make the whole thing transparent to readers of the webpage.
Doesn't this bring new meaning to "the global namespace"? I always considered having anything in the global namespace to be a negative, but sometimes worthwhile for practical reasons. This sounds like a bad thing to me. I've worked a bit with both Python and PHP and one thing I really favor about Python is their namespace practices. I can look at the imports at the top of a source code file (and let's face it, files and file systems will be around for a while) and know what I should expect to see in the code. I've been happy to see Haskell following similar practices. On the other hand, the PHP code I have worked with basically had a "require everything" at the top of the file and it was anything goes from there, and I found the code much harder to work with because of this.
Author here: Yup, "very gentle" was indeed the intent. The post was aimed at people with only basic Haskell knowledge, so there's plenty of complexity that's elided or left out. I thought about submitting it here, but I decided it was a little too basic for the audience. Feedback appreciated though. 
Nothing wrong with basicness, especially when it's such a good introductory piece. I'm probably not the only Haskeller who's slightly obsessed with the question of how to introduce people to the language. I particularly like the way you introduce the `HTTPMethod` type *after* a series of "stringly-typed" variants; it seems like that would make a lot of sense to people coming from Python.
But the actual composition issue is solved by the lens libraries. The old extension proposal was just about making the syntactic sugar that is record data declarations work more easily with the many lens options we already have.
I've thought about trying that a couple of times, but I fear that small incompatibilities and needing different plugins for things will make me switch back and/or make it too much effort. But perhaps I should just try it. 
This isn't, in general, an idea that's without merit or promise: Joe Armstrong, creator of Erlang, [recently discussed a similar idea](http://erlang.org/pipermail/erlang-questions/2011-May/058768.html) (which spawned a worthwhile [Lambda-the-Ultimate discussion](http://lambda-the-ultimate.org/node/5079)) and David Barbour has been working on a similar [wiki-based development environment](https://awelonblue.wordpress.com/2014/09/29/introducing-wikilon/) for his language Awelon. People have even experimented with languages that have [RDF descriptions of all their components](https://github.com/trith/trith), for a radically more expressive way of linking code fragments together. This is, however, not a good idea _for Haskell_. Haskell is a language that already exists, with extant users and extant codebases and extant language infrastructure. What you're proposing is literally _throwing all that away for an experimental namespacing/packaging system_. It's a system that'd be worth trying, to learn what advantages and disadvantages it might have in practice! But it'd be counterproductive to try to rebuild the Haskell ecosystem around this idea.
Hmm, I agree with that. Thanks for the links!
No, as far as I'm aware. But it would not be hard to add, and probably should be.
You might try installing `nix` via homebrew and `ghc` via `nix`. Not I have done it because my mac is borked, and I can't install nix on it (but I'll probably do that when I get a new one) ;-) 
Short as in, "List" being a top level namespace. With an `using` statement it would be just `head` for you. It stops the cabal hell for the reasons I stated, 1. your code only breaks if a particular function that you are using directly has a breaking change, not anymore if a "module" that you are using contains a function that has a breaking change. 2. if you don't allow breaking changes to functions (which is practical, as opposite to not allowing breaking changes to whole modules) then it is extinguished. 
&gt; https://github.com/Gabriel439/Haskell-Annah-Library/blob/master/Prelude/ASCIIString Well, it's certainly something. `fold x` is declaring that x is the fold for the type being declared?
The reason you don't see any other constructors for strings is that the compiler has built-in syntactic sugar for them. If you enter a string literal into a program it will desugar it to a lambda expression, and the compiler will also detect any lambda expressions that are strings and resugar them. For example, you can concatenate two strings by just writing: $ annah #concat "123" "456" ∀(S : *) → ∀(N : S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → ∀(C : S → S) → S "123456" That converts `"123"` and `"456"` into lambda expressions, concatenates them in lambda calculus, and then detects the result is a string and resugars it. The only reason `ASCIIString` is in the Prelude is so that you can assign a convenient short-hand type to these strings. The compiler also desugars and resugars numbers as lambda-expressions encoding binary numerals. So you can write: $ annah #times 120398 345809 ∀(Bin : *) → ∀(Zero : Bin) → ∀(One : (∀(Bin_ : *) → ∀(Nil_ : Bin_) → ∀(Zero_ : Bin_ → Bin_) → ∀(One_ : Bin_ → Bin_) → Bin_) → Bin) → Bin 41634711982 You're intuition about `fold` is correct: it's just generating the fold for the type being declared (which is just the identity function).
So one thing I'm considering is that Annah will resugar code using imports, too, so that if Annah sees that long type signature for a string, it will replace it with `#ASCIIString`. Similarly, if Annah saw a Church-encoded False, it would replace it with `#False`. The non-trivial part of this is deciding how Annah chooses which files to use for resugaring expressions. This won't be generalized to non-ASCII strings. Keep in mind that what you are interacting with is the compile-time layer of Annah (i.e. things that get computed just once when the program is first compiled). There will be a separate run-time layer that will wrap a Haskell FFI and that will support more efficient runtime computations like machine integers and unicode strings. Folds are just the identity function. You don't need them if you are importing an external expression since the fold is just the identity function. For example, you can actually delete the `#if` from the `and` example and it will still work. The only time you need them is if you are trying to fold a function within an expression that a data type definition scopes over. For example, if you write: type Bool data True data False fold if in e ... then within the expression `e` you will need to use the fold `if` to fold any boolean value. This is because within `e` the data and type constructors are abstract and the fold unwraps the abstraction. From the perspective of the expression `e` the fold actually does not appear to be the identity function but actually appears to have the type: Bool -&gt; forall (Bool : *) -&gt; forall (True : Bool) -&gt; forall (False : Bool) -&gt; Bool If you import external expressions they are not abstract and from that perspective the `fold` just looks like the identity function. So the reason I don't provide a `foldr` file is that it's actually not necessary at all (and I could similarly delete the `if` file, which is also not necessary). I've already built a free monad AST in Annah. Here's an example of `IO` encoded in Annah using a free monad. Here I'm using an abstract `String` type just to make the normalized output more readable: \(String : *) -&gt; given a : * type IO data PutStrLn (str : String) IO data GetLine (k : String -&gt; IO) data Pure (r : a) fold foldIO in let getLine : IO String = GetLine String (Pure String) let putStrLn (str : String) : IO {1} = PutStrLn {1} str (Pure {1} &lt;1&gt;) let (&gt;&gt;=) (a : *) (b : *) (m : IO a) (f : a -&gt; IO b) : IO b = foldIO a m (IO b) (PutStrLn b) (GetLine b) (\(r : a) -&gt; foldIO b (f r) (IO b) (PutStrLn b) (GetLine b) (Pure b)) -- getLine &gt;&gt;= putStrLn in (&gt;&gt;=) String {1} getLine putStrLn If you normalize that, you get the syntax tree that closes over all dependencies it needs: $ annah &lt; io ∀(String : *) → ∀(IO : *) → ∀(PutStrLn : ∀(str : String) → IO → IO) → ∀(GetLine : ∀(k : String → IO) → IO) → ∀(Pure : ∀(r : {1}) → IO) → IO λ(String : *) → λ(IO : *) → λ(PutStrLn : ∀(str : String) → IO → IO) → λ(GetLine : ∀(k : String → IO) → IO) → λ(Pure : ∀(r : {1}) → IO) → GetLine (λString → PutStrLn _ (Pure &lt;1&gt;)) It will get nicer once I add support for `do` notation to Annah. The type system Annah uses is the calculus of constructions (i.e. polymorphism + higher-kinded types + dependent types).
Many tiny modules don't lead to fast recompilations as long as you need to link things :-(
That's a very good point!
This. Also what happens when someone changes `head` in the global namespace ? head [] = unsafePerformIO (deleteAllFiles &gt;&gt; error "lol") head (x:xs) = unsafePerformIO (rootComputer &gt;&gt; return x) What if it's offline ?
&gt; That is, today, if we have a module with 500 functions, one of which is buggy and barely used, unfortunately fixing that function will cause every library depending on that module to break, even if most don't use that function. That's not the cause of cabal hell. Breaking changes are usually about refactoring stuff, which bring changes to those 500 functions. Then you get a version bump, and only half of the libraries on Hackage are updated. That's cabal hell. I don't see how a function only system would help, except if those functions are immutable. Now you can't fix them at all !
Yes. Folds are how you pattern match on types. When you fold a type it takes one continuation for every type and data constructor. A Boehm-Berarducci encoding of a type is to literally encode it as its own fold, which is why all these folds are identity functions. The Boehm-Berarducci encoding of a type is just a higher-order function that takes one continuation for every type constructor and one continuation for every data constructor. The reason for the extra argument is that when you write a data declaration like: given a : * type List data Cons a List data Nil fold foldr in ... ... any `given` parameter (like the `a : *`) becomes an argument for every type constructor, every data constructor, and the fold within that data declaration. So in the above example the `List`, `Cons`, `Nil` and `foldr` will all take an additional initial parameter `a` of type `*`.
Done. Very minor edit. I included the word "consistent" as my impression is that this is a major benefit.
Thanks for that link. I think that suffices my need to see some actual code. :) Its sounds like your music consists mostly of sequenced samples; or do I miss the filters/effects and synths? 
Not an april fool's joke: you can compile and run the executable if you don't believe me. It's an ordinary Haskell project hosted [here](https://github.com/Gabriel439/Haskell-Annah-Library). I'm not familiar with Awelon Blue. I did some (very) cursory reading and it seems like the abstract virtual machine appears a little bit similar to the rough sketch I have in mind for how to implement Annah's runtime layer (not the same as Annah's compile-time layer, which is what my parent comment is discussing). The basic idea is that in Annah the programmer never really makes RPC calls. You just "host" `IO` expressions on network endpoints that others can import directly within their programs using the same hashtag syntax, and under the hood if you sequence multiple imported `IO` expressions that compiles to a bunch of network calls threading data throughout each network endpoint in order, running the `IO` actions on their respective machines, and eventually returning the final result back to the original client. The compile-time layer of Annah is basically a higher-level abstraction than what Awelon's abstract virtual machine is doing. Annah is presenting a more denotational API to distributed computation: you identify a network endpoint with a purely functional expression "representing" what that network endpoint actually "means" (i.e. this endpoint "is" the `True` constructor or that endpoint "is" the `List` type), whereas the abstract virtual machine of Awelon seems to be a lower-level implementation detail of how distribute all these RPC calls. However, I could be totally wrong and if you see more similarities between Annah and Awelon Blue please let me know.
Edit: how is `unsafePerformIO` even allowed? What is stopping someone to have it midway through a seemingly pure function on Hackage? You know that defeats the whole "I know if a function is pure it is not sending missiles" talk.
You could consider the "global namespace" as having state, the function definitions and their names would be that state. It would be a crowded space. I consider having lots of symbols available in a single chunk of code (a file probably) to be a bad thing and an indication of poorly designed code. With the current Haskell practices, if I have 100+ imports in one of my source files, it will be a definite code smell, and would suggest to me that the code in that file is not modular or well organized. It is a mental burden to be working with advanced mathematics, then in the next line be working with a GUI framework, then in the next line be working with who-knows-what-else. When line after line after line uses a few of many millions of symbols, the cognitive overhead will be huge. Perhaps a good way of illustrating the problems is to suggest the best course I can imagine of implementing something like this in Haskell. I do not intend the following to be snarky, thought I think it might end up sounding that way because it will be unconventional and will have some obvious problems (in my opinion). If I were to attempt something like this in Haskell: I would create a new Haskell library called "Everything" with the goal that one day people could start doing "import Everything" at the top of their Haskell source files and no other imports. Once the library became widely used the Haskell community might adopt it as the new Prelude and add the fetch-on-demand features you mentioned to GHC. You would also want to create a novel Wikipedia-like documentation and collaboration tool to help with the development of this library (I have no problem with this collaboration tool, it sounds good to me). The library will be a comprehensive collection of all useful functions and values. (Wow, that is quite a claim!) It will have to be contributed to by many many developers. All developers will have to share the same namespace. The developers will have egos, there will be flame wars. It will be an "append only library". Existing functions will be retained indefinitely, but deprecated when they are no longer optimal. Users of the library can update to the latest version without worrying about breaks, there will be no more cabal hell. Except, existing functions cannot be changed without breaking code. Even if you fix small bugs in existing functions, there will be production code that depended on those bugs as a feature, and that code will break. You cannot expect every function and value in this vast library to have comprehensive automated tests. The automated tests will not save us, changes to existing functions will result in broken code. We will have to rely on the "using" keyword you suggested to help with the fact that List.head happened to have a bug in it and now we're all using List.head2. We cannot expect every function to be bug free before being accepted into this library. Instead of files and imports, we'll have symbols and "usings", and a very crowded namespace. This language and ecosystem will run on operating systems that still follow more traditional versioning and distribution schemes. My code will still break when I have OpenGL 4.2.1 installed instead of OpenGL 4.2.2. And what of linked libraries? Will Everything contain bindings for OpenGL 4.2.1, and OpenGL 4.2.2 and OpenGL 4.2.3? This will not be sustainable. Again, sorry for being snarky. I've done a brain dump of what I think about when trying to picture a real life implementation of your ideas (as I understand them). You did at least get me to step back and ask myself "are namespaces really the best we can do?", I don't know, maybe there are other patterns to help with the problems I have mentioned above, but I haven't seen how what your suggesting deals with these issues.
What about OpenGL stuff? How do we shrink that down?
The time/space complexity needs to be included in the description.
I'm feeling this way about too many things these days.
Also note that OpenGL wasn't considered to be used directly. Always use a middleware for graphics. Thanks to Haskell community efforts, some ugly API's has been abstracted to very useful libs. 
I see what you are visualizing, but I guess it is wrong. Maybe, if you notice that: MyProg.main = List.foldl Nat.+ Nat.0 (List.cons Nat.0 (List.cons Nat.1 (List.cons Nat.2 List.nil)) Would be translated to: myProg.hs module MyProg where import qualified List as List (foldl, cons, nil) import qualified Nat as Nat (+, 0, 1, 2) main = List.foldl Nat.+ Nat.0 (List.cons Nat.0 (List.cons Nat.1 (List.cons Nat.2 List.nil)) You could see it is not about "everything has access to everything, global mess, doom", but actually the opposite: everything only has access to that it has uses! The only difference is that the way we tell the compiler what it uses is by just using it, instead of having a separate statement on another line. Writing "List.head" is essentially **the same thing** as writing `import qualified List as List (head); List.head". Or, in other words: that "global namespace thing" you are so afraid of already exists and its name is "installed libraries". It just takes an extra line, but, in some sense, everything you have on the PC is already available to you. I also see your concern about "head2, head3" and I'm absolutely positive that will not be a problem in practice because functions are reviewed by so many people. Since the whole codebase is centralized, it wouldn't be hard to design an automated code rewriting tool that recovers a lost namespace once the community requests (for example, swapping the names of `head2` to `head` on all existing definitions). And the "egos", "flame war" and so on issues you are pointing are the exact same issues that made so many people wrongly assume Wikipedia could never work. But it does, surprisingly well. Stop visualizing it from a software engineering perspective and imagine an actual Wikipedia of code, with reviewers, moderators, editors, diffs and so on. To be honest, I'm posting it like this is an idea I just had but I'm actually working on it (for another language) since months and it is doing really well. I really wish Haskellers noticed how awesome it is.
Also, no, that is not state at all! State is information that changes. That is just static information. A lot of information is still not state. Wikipedia is, again, the perfect example. It describes very, very complex things, including a lot of code. But, regardless of having 100mb+ of text, it has no state at all. That is why a small group of moderators is perfectly capable of dealing with the whole. 
Actually, Derek writes in to let me know that atomizing modules so that each unit of modularity contains only a single term or type is actually one of the core ideas behind MixML (https://www.mpi-sws.org/~rossberg/mixml/).
 using List import Data.List What is the difference?
By namespacing it. You use some sort of `Internal` namespace -- MyNamespace/Internal/ADT.hs data ADT = Constructor1 | Constructor2 And then use it to define the user-facing bits. -- MyNamespace/smartConstructor.hs smartConstructor = MyNamespace.Internal.ADT.Constructor This doesn't truly hide it from people. You just encourage people to use the safe API provided by MyNamespace.
I suggest trying to find a way to automatically bring everything on Hackage into "the global wiki." We already have namespaces. Just prepend the appropriate `import qualified`. Don't reinvent the wheel with `List.head`. We already have `Data.List.head`. Sure, it's longer, but it's what we already have. If you want this idea to take off, it will need to play nice with the existing ecosystem. Another thought is that QuickCheck isn't necessarily sufficient. Just allow an arbitrary test suite to be attached to any given symbol or group of symbols.
This is just an explanation of the div combinator I posted last month. I'm posting this just so it can help someone in a future, but I'm still unsure if this is ontopic here, so, if there is any problem, please just downvote. Thanks.
Nice! I didn't realize git handled symlinks, let alone github.
FWIW the formula has been updated and the Bottle is now available.
I find Default useful, but I'm also wary of it because of this instance: Default r =&gt; Default (e -&gt; r) I had code where I thought def was supplying a default value, but I had lost a parameter, and it was actually supplying a default function! That took some time to figure out, and I could see this bug sneaking in, in a way the type checker can't help with.
You should look into spacemacs.
I'm curious as to why there's the conversion and comparison of the character in lower case ?
Off topic, but I enjoyed the [Primer](http://en.wikipedia.org/wiki/Primer_%28film%29) reference: &gt; If this description was too abrupt, here is a diagram with a fuller description of the workflow: [http://bit.ly/15IIGac](http://bit.ly/15IIGac)
&gt; I thought about submitting it here, but I decided it was a little too basic for the audience. Bad thinking, IMHO. This sub could use a more even balance of beginner, intermediate and advanced material. Also, many advanced folks would be interested in this, either because (a) they know somebody just getting started that they'd forward it to, or (b) they might have useful suggestions.
I agree.
I was thinking of something explicitly mentioning the breaking change, cmake has something similar I believe. I was thinking about a new cabal file field where you can list one constant per breaking change. 
I think you are right indeed. I remember now trying to use the Nix installer.
&gt; I've wondered what I'd do if a maintainer become unresponsive [Here](https://ro-che.info/articles/2015-02-09-dealing-with-broken-packages) are your options.
Except that doesn't mention the trustees at all :) Hopefully they can in many cases fix things, preventing you from having to fork or take over a package.
If it is called "cabal hell", just rename/replace cabal with a new one. Well, one reason to cause this "hell", in my opinion, is this install process just installs too many packages. Yes, the packages in ghc are so few. If the probability of installing each dependency are P1, P2,...,Pn. The probability of full success is P1*P2*...*Pn. It means almost impossible. Besides, each install process depends on the version of package already installed and then the os itself. Can't there be binaries(pre-compiled ones) provided? Then we can actually *UPDATE* packages. I am sorry to trouble again.
You're not taking typeclass instances into account. They are inherently antimodular, and if an orphan instance leaks into that global database, every other package can accidentally break.
HTTP allows other methods. It shouldn't be `Unknown`, it should be `Other String`. You're already failing on WebDAV, which uses, for example, `PROPFIND`.
They can't (and don't) as of now, hence the post doesn't mention them. Hopefully this proposal will change it.
Cool. I did not know that you were allowed to mark an operator as non-associative. Thanks for the lesson.
:D "enforce immutability, but allow retroactive mutation" Time travel definitely seems like a good approach here.
[One year ago today...](http://www.reddit.com/r/haskell/comments/1bfojn/functor_is_now_a_superclass_of_monad_in_ghc_head/)
They can (with the help of admins) and did, [here](https://github.com/haskell-infra/hackage-trustees/issues/4).
I don't get why this is funny... it's in fact already an indirect super-class thanks to the AMP, isn't it?
That would be... interesting, because a Control.Monad isn't a Data.Monoid. An instance of Control.Monad is a monoid object in the monoidal category of Hask Endofunctors. An instance of Data.Monoid is a monoid object in the monoidal category of Sets. So they're pretty closely related, in that they're both monoid(al object)s in some category. But you can't munge one into the other.
Sadly, no: https://hackage.haskell.org/package/base-4.8.0.0/docs/Prelude.html#t:Monad
In case the two-column format bothers you, I'll just leave this here: javascript:void($('.twoColumn').removeClass('twoColumn').css('text-align', 'left')) Visiting that "URL" after loading the page should remove the two-column effect. It's a hack, but it works for me (Chrome, Firefox).
What's the type of `frac`?
I'm excited to see how you achieve the high-performance part. Do pipes/conduits/io-streams play a part? Will concurrency primitives and channels be involved? Please continue the series.
I think one of the first things to do is to do a quick grep over Hackage to see how many packages would be impacted. Unfortunately, my pure Haskell grep bindings don't yet support enough regex syntax to do something so practical as to search for all characters in the whitespace class. Accepting PRs!
I wrote something that may help. http://hackage.haskell.org/package/charset-0.3.7/docs/Data-CharSet-Unicode-Category.html#v:space
Can we really be sure that this represents all the right characters we're interested in? I mean, I was really expecting something called `whitespace` and without more informative types it's difficult to tell them all apart. As a matter of sanity I like to alpha convert all Haddocks to greek letters before I really dig in.
&gt; On any other day of the year, you may want to stop reading this proposal after the first section and consider it seriously. On the topic of not indenting code too much to the right, I really dislike how the following (and similar constructs) don't work due to the parser expecting let-bindings. let x = if cond then stuff else otherStuff
Okay, I think I've got it but I just need to fill in the following import System.Process import System.IO.Unsafe pureGrep :: CharSet -&gt; [String] pureGrep cs = lines (unsafePerformIO (hGetContents hout)) where name = proc "grep" [regexOf cs] (_, Just hout, _, _) = unsafePerformIO (createProcess name) regexOf = {- ... -}
&gt; On any other day of the year, you may want to stop reading this proposal after the first section and consider it seriously. Given how we got AMP within a year of the last April Fool's joke, this appears to be the new, sanctioned way of making genuine GHC proposals. 
You almost got me. I hate April 1st.
It's not technically part of the language. Of course, similar things are possible with foreign imports and they are part of the language. GHC provides it as an escape hatch, for when the *programmer* can prove it is safe, but the type system / language / compiler can't express that proof. /u/edwardkmett (and other) have used it very selectively and carefully in the past, and I expect it (or something like Idris' `believe_me`) will be with us log into the future.
You're not the only one… Plus, even with a fixed font having to re-indent a whole block makes diffs bigger than necessary, which is a big deal for a few people for some reason. (Also, it's kind of a hassle.)
It would also be nice if you could put the in for a let on the same indentation level let x = foo y = bar in x ++ y instead of let x = foo y = bar in x ++ y 
If that was one year ago today why does it say "2 years ago"?
Looks pure to me. I mean look at the type!
This talk builds up the Frames library by starting with a GADT used to define Vinyl records, then applies the same techniques to several iterations of a design for building embedded domain specific languages (EDSLs) in Haskell. It was a long talk, but the audience was very helpful in covering so much material. The code in the talk is fully self-contained, and the larger language built using the technique shown is on the way out the door, but not quite available yet. As always, feel free to reach out if you want to chat about any of these things.
Oh well... this is the Haskell 1.4 story is repeating itself... maybe we can try again in 10 years
That works.
We need ghc-fmt! It will make haskell even more mainstream, considering that "go ended indent wars with go-fmt" is one of the main benefits that language users bring up.
Agreed! Perhaps we could convince the maintainer/s of [stylish-haskell](https://github.com/jaspervdj/stylish-haskell) to work on this, but according to them: &gt; The goal is not to format all of the code in a file, since I find those kind of tools often "get in the way".
1) I'd suggest putting that in your state (the `s` type), and then you can use `getState`, `putState`, and `modifyState`. Then, in your state, you might have something like `Maybe User`, for which you write an `ensureUser` function, which fetches the user if it's `Nothing` in the state, otherwise simply returning the 'cached' value. 2) Each state is per-request, in the sense that these are Haskell values. Only when using something mutable like an MVar will changes in one requests 'state' reflect in another. If you just want to vary something over the lifecycle of a request, do that just like you would with the State monad. 3) We haven't done any formal (criterion) benchmarking, but that's on the short-term roadmap. Informally, it seems 'fast enough'.
What is the haskell 1.4 story ?
Haskell 1.4 is the language we deserve, but not the one we needed back then. So we got Haskell 98 instead. Because we could take it.
Thank you, dream-fairy!
TBH Microsoft as a whole leaves me in a cognitive dissonance. On one hand, Microsoft Research, Z3, F*, contributions to GHC, etc etc. On the other, Windows 8, Office-with-ribbons...
This kind of an instance seems to make much more sense with Monoid, but more for (&lt;&gt;) than mempty. So perhaps it should be defined for Semigroup, not for Default? The perennial problem of the letter vs. the spirit of the law, I guess :-/
look at the name!
Well, that escalated quickly.
With a slight change of target language you could have an "imperative perl".
WHAT?!? I would never have guessed!
Not ambitious enough. I think the warning switches should actually just modify the source when compiling. I mean the compiler already knows *exactly* what was meant by the code and there's no room for ambiguity, so... y'know. (Forgive me if I missed mention of such a feature. The post was quite dense. :))
Before "everyone" [1] is just hacking away on this over the weekend... Should we maybe start a coordinated effort to get this done efficiently? [1] sometimes I get the feeling that Vim is not really used much in the Haskell community. 
I think that phrase is code for "and there are corner cases we can't handle". Luckily ghc-exactprint / HaRe should be able to do whole file changes, as soon as the new features for 7.10.1 are worked in.
April fools?
Clearly there is some room for improvement in the handling of this proposal in subsequent releases.
Does today's date have anything to do with this announcement? :)
This seems incredibly elaborate for an April Fools. You win!:)
This is really neat. It feels like you establish an initial encoding for the *frontend* and a final one for the *backend*. And the middle end (as an impedance matcher) has all term inspection capability before pushing things downstream (or just be lazy and use the canonical translator). Is this related to superoptimization / supercompilation somehow?
Oh don't worry; most of the comments were posted "1 year ago".
The final encoding is actually the front end, too, as it lets us use plain Haskell bindings for lambdas, and lets us avoid introducing any overhead should the desired backend be `Identity` (or its moral equivalent). I'm not certain what branch of optimization research this rightly belongs to. I call it partial evaluation, but I recall Alec suggesting a more specific name. Unfortunately I do not recall *what* name he suggested, and it probably didn't get picked up on the video. It is not yet supercompilation, but I believe that is entirely possible and pursuing that is indeed among my plans. 
One (or two) years ago Tekmo made a post with this exact title. This was pre-AMP but there had been a lot of stirring about getting `Applicative m =&gt; Monad m` for real. With no actual consensus or proposal or motion it was sort of leaving the entire community in suspense, though. Tekmo's post probably made 60% of the entire Haskell population do a double take. Probably even some GHC maintainers. It was just a well timed, slightly cruel April Fool's joke. I resurrected it this year as a monument to completion of the AMP. It's no joke today.
I count maybe 4 that say they actually use Haskell in some capacity.
That's more than I expected!
I think he saide "peephole optimizer". Also, are the slides for this available somewhere? And the sample code? I really want to play with this.
https://ghc.haskell.org/trac/haskell-prime/wiki/DoAndIfThenElse
I can post that tomorrow. What would you like? I wrote the code in Haskell, edited as Org, exported to PDF by way of LaTeX and Beamer, and tied it all together in Keynote.
I just started a library for quant finance and would love some help. https://github.com/boundedvariation/quantfin
Sounds good to me. The sooner we get this pushed through, the sooner we can start using whitespace characters in operator names.
Well, if we want to type the `frac` exactly as it is in OP, then we can see that the arity of the `g` tuple constructor depends on the divisor number, so we definitely need dependent types for that. On the other hand I'm pretty sure that it can be rewritten using non-dependent Church lists (probably yielding a bit more complicated solution).
Semigroup is 10x as important as Monoid for statistical / "big data" processing. Map-reduce is based on Semigroup, not Monoid. The Scala machine learning packages use Semigroup all over the place. I hope this gets cleaned up in Haskell. 
The objections to this in the past has been that it doesn't extend so obviously to MPTC. But personally I'd be willing to say it can only be used when the defined superclass methods are unambiguous. I might go ahead and implement this in our compiler if I have a little time. 
This will degrade the quality of haskell source code, because it is not greppable, as mentioned in the article. I think going in this direction is a slippery slope, and this particular refactoring is close to be fixable using perl and a regexp. I think a better way forward for open source code at least is to do these large-scale refactorings in systems such as Stackage. A system like Stackage can be seen as a single repository of (most) open source code, and a process for doing refactorings like this across all code can be created. Given a large repo of source code, specifying refactorings using HaRe or similar can be worthwhile, something it might not be for each individual developer. HaRe refactoring definitions can then be reused on private repositories. In general, I think the eco-system will move faster by working towards a "single repository" and "continuous delivery" by expanding on the stackage idea with additional tooling and better feedback cycles. If the process is improved, this whole problem domain should become much smaller and we get better software as a result. 
What do you mean by "single repository" exactly? I was under the impression that Hackage is our primary repository, and that Stackage (which not everybody wants to use - me included, albeit I realise it may be useful to others) just provides one (or a few) cross-cuts through the multi-dimension package/version configuration space, effectively bypassing Cabal's sophisticated solver and all the extensive interpackage `build-depends` version constraint meta-data [soon to be curated](https://pay.reddit.com/r/haskell/comments/30wue5/hackage_trustee_proposal_curating_the_hackage/) on Hackage... Also, I don't think we should base decisions regarding implementation and adoption of language-features on their effect on Stackage (which is not even our community's canonical package repository)
I'm glad FP Complete is picking up on this and seems to be willing to do the grunt work to the benefit of the community. https://www.fpcomplete.com/blog/2015/03/composable-community-infrastructure 
[This very accessible paper](http://www.lexifi.com/files/resources/MLFiPaper.pdf) describes an Embedded Domain Specific Language (EDSL) for financial contracts, it always seemed an interesting topic. There are also several [financial packages](https://hackage.haskell.org/packages/#cat:Finance) on Hackage. Perhaps you can contact the authors and ask whether they had some future work in mind?
&gt; In order to be really useful, we'd want to use this without a LANGUAGE pragma. At least not in the instance definition, but perhaps we should still require some annotation on the class definition?
I use it with 7.8, and I think it (or at least the fork I use) works with 7.10 as well. It doesn't use cabal at all, as far as I know, although it does find packages in sandboxes. But I can't use it for my day job (using cabal-dev sandboxes) so I have limited experience with it, basically only hobby projects and small experiments.
{-# LANGUAGE Java #-} ? 
Go for it. Even the crude text mangling version of DefaultSuperclassInstances that [SHE](https://personal.cis.strath.ac.uk/conor.mcbride/pub/she/superclass.html) provides was worth the trouble. My [proposal](https://ghc.haskell.org/trac/ghc/wiki/IntrinsicSuperclasses) is on the table but going nowhere. I'd write something like class Semigroup s where mappend :: s -&gt; s -&gt; s class (instance Semigroup m) =&gt; Monoid m where mempty :: m and then it's flagged in the header of the `Monoid` declaration that instances of `Semigroup` will be generated by default. Correspondingly, `Monoid` instances which define `mempty` and `mappend` are correctly split. If you want `&lt;&gt;` as is, you can say class Semigroup s where (&lt;&gt;) :: s -&gt; s -&gt; s class (instance Semigroup m) =&gt; Monoid m where mempty :: m mappend :: m -&gt; m -&gt; m (&lt;&gt;) = mappend so that the `Semigroup` instances generated from `Monoid` instances have the correct default definitions. Of course, when you build a hierarchy with these things, you get into the situation where there may be multiple candidates for the default whateveritis. The majority of the complexity in the proposal is the laborious investigation of these corner cases, followed by the attempt to turn "if there's an obvious best choice, take it, otherwise let the user decide" into something more definitively mechanistic. You can definitely pay less to get less, but if the objective is to ensure that progress breaks as little as possible, that takes more care.
Minor detail, thanks to [`return = pure`](http://git.haskell.org/ghc.git/commit/a741e69a230eb6d6e3373ad1fbe53c73b5f95077), you'd only need to define `pure` (and not `return`), i.e. instance Monad MyId where fmap f (MyId x) = MyId (f x) pure = MyId MyId f &lt;*&gt; MyId x = MyId (f x) MyId x &gt;&gt;= f = f x At some point we will probably [warn about `return` being overridden](https://ghc.haskell.org/trac/ghc/ticket/10071), and at some later point in the distant future (but before the next Haskell Report) move `return` out of `Monad` to complete the AMP.
I guess https://github.com/haskell/hackage-server/pulls are welcome :-) While /u/dcoutts is the maintainer and hence has the actual say in this, I'd be curious how FPC would go about modifying `hackage-server` to accomplish the outlined goal.
Personally I feel that custom infix operators can make it really hard to read code. Several public libraries use three or four of them, and holding the definitions for them in your head all the time can be confusing. A short, textual function name like 'mulInc' would be better if you were writing a public library. Nice tutorial though!
At the definition site you'd definitely need a language-pragma to allow this. And if you had to use a language-pragma at the definition-site then many language-pragmas don't require it at the use-site anymore (see also discussion in [#9838](https://ghc.haskell.org/trac/ghc/ticket/9838))
Good tutorial but not relevant here.
&gt; But I feel like I'm so, so far from actually making an application with Haskell. You don't *feel* like you could, but have you tried? Where do you get stuck? It would be easier for us if there was a concrete obstacle we could help you with. &gt; When I look at code for applications that actually do exist, I can't understand any of it. It's harder to understand other people's code than our own, in any language. Again, it would be easier if you had a concrete piece of code you needed help with.
Since I posted this [two weeks ago](https://www.reddit.com/r/haskell/comments/2zpd2e/i_created_a_web_app_front_end_to_the_pointfree/), a lot has changed. I switched to WebSockets, added permalinks, and used a more memorable URL. 
I meant `mappend`. fixed 
Very nice app, thanks! The URL still isn't so memorable for me, though. And it doesn't come up in Google with any of the most obvious searches; that will change soon I hope. EDIT: Can we get pointfree.haskell.org for this? To be fair, you should probably put a link to the pointfree library on the site if we do that.
I hadn't considered SEO. There's not any text on the page that would make it do well in Google searches. Maybe I should add a short explanation somewhere. 
It's even funnier that this post was so successful that it's being downvoted into oblivion.
It isn't. =)
One thing that always bugs me about `pointfree` is that it does't also tell me how I could do better by swapping the order of arguments, or anyway offer that to me as an option. Thus if I write blunt0 x g f h = f (h x) (g x) it says blunt0 = ap ((.) . flip . (flip .) . flip (.) . flip id) (flip id) Sometimes I have a clear idea of the order of arguments I want, but sometimes not. So swapping order there is also blunt1 g f h x = f (h x) (g x) blunt1 = flip (flip . liftM2) and still better, of course blunt2 f h g x = f (h x) (g x) blunt2 = liftM2 There isn't any deep theoretical meaning to this request, obviously. 
With this release, I just want to point out that two web frameworks, Happstack and now Airship, have a principle maintainer in Chicago, IL :)
Some SemiGroups are not Monoids, Monoid can not be a super-class of SemiGroup for that reason. What you want should be possible but use a different syntax.
&gt; I think it's only getting used for user state, right? We [also use](https://github.com/helium/airship/blob/4bcb6e96ada413780ed3634f2c6670148cd58973/src/Airship/Types.hs#L136-L140) the State from RWST for keeping track of response metadata, like the response headers and body. 
In the light of recent announcement from fp complete on opensourcing their IDE backend, are there any plans to integrate it into haskell-mode? 
Ah, my misunderstanding. Thank you!
Very awesome'est. Too bad I can only give you one "constructive" point :) I understand that it is possible for certain packages to block others. Is that info is visible somewhere?
+1 Barring God-like intellect, this really is the only way.
I also recommend reading this School of Haskell [STM tutorial](https://www.fpcomplete.com/school/advanced-haskell/beautiful-concurrency) written by Simon Peyton Jones based on [this paper](http://research.microsoft.com/pubs/74063/beautiful.pdf) of the same name.
No, this is great. We need more stories like this
Doing refactorings in Stackage would only work if all the code you care about is on Stackage. In my case, at least, that is very much not true and likely won't be any time soon, regardless of how nice that would be, which means that even running a simple script over every relevant repository would be significant amounts of work. Additionally, as long as additional tooling is on the table, then we could easily solve any searchability problems posed by Default Superclass Instances. A somewhat smarter `haskgrep` which was aware of Haskell syntax and semantics could search source files for the location of `instance Semigroup t` and report that those methods were implicitly defined in the declaration for `instance Monoid t`.
I think you've confused the _default_ in the name with the `Default` typeclass, which is an unrelated proposition. Think about things this way: every `Applicative` instance is a `Functor`, but not every `Functor` is `Applicative`. Consequently, the relationship between the _typeclasses_ should look like this: class Functor f where {- stuff -} -- because an Applicative is necessarily a Functor class Functor f =&gt; Applicative f where {- stuff -} On the other hand, if I have an `Applicative` instance, then I can easily create a `Functor` instance by defining `fmap f x` as `pure f &lt;*&gt; x`. In that sense, actually declaring _both_ `Functor` and `Applicative` is more work than we might otherwise need, because we'd like to tell the compiler, "If I ever define an `Applicative` instance without having a `Functor` instance aready, then you can figure out what the `Functor` needs to be by doing [something]." In the proposal I linked to, that `Applicative` declaration would look like class Functor f =&gt; Applicative f where pure :: a -&gt; f a (&lt;*&gt;) :: f (a -&gt; b) -&gt; f a -&gt; f b -- a default definition of `Functor` in terms of -- the `Applicative` typeclass instance Functor f where fmap f x = pure f &lt;*&gt; x That's why they're called _default_ superclass instances: because if a superclass hasn't been defined, then we can derive its definition from a subclass declaration instead. A variation is to let instances group all their declarations into one block. Obviously, if I'm defining a `Monad` instance, I'm gonna need to have an `Applicative` and a `Functor` instance as well. Maybe we could just declare them all in a single `instance Monad` block, rather than having them in their own blocks for `Functor`, `Applicative`, and `Monad`. The real advantage of _that_, though, is that if any of those classes were split apart into smaller classes later on—for example, if we wanted to put `pure` and `&lt;*&gt;` into two different classes—then our _instance declarations don't need to change at all_. Because all those methods are still present somewhere in the superclass hierarchy, and we're allowing defining of superclass methods, then we can start refactoring typeclasses _without extra work on the part of users_.
I like to start with something that does nothing, but works. Then I make it do one thing, then the next thing. The only question is the "order" in which it does one and then the next thing. In imperative languages often the "first" thing is to tell it how to take args, set itself up, very top down. In a functional setting I often prefer to give "here is a structure that contains all the things necessary to solve the problem, and I have hardcoded it" and to write the function that "does the work" first, or often the set of functions. That way my program is structured more like a "little library" for the meat of the problem, wrapped in a little component that actually "drives" it.
I agree and I m not saying a semigroup is a monoid. I'm saying given a Monoid (for all monoid) I can create automatically is semigroup instance. There is no notion here of superclass or subclass but only instanciation and constraint.
When Haskell becomes mainstream, I wouldn't be a Hipster anymore. :o
The problem with this approach is that you can then take away people's chance to define a semigroup instance different from the monoid instance. There are obviously many choices for some types. That is why GHC does not usually allow these kinds of instances as far as I understand. A better way would really be to make this the default implementation and allow the user to overwrite this. You can do that today with DefaultSignatures as far as I know.
I would probably use [`stm-delay`](https://hackage.haskell.org/package/stm-delay-0.1.1.1/docs/Control-Concurrent-STM-Delay.html)
The redesigned [OverLoadedRecordFields (new proposal)](https://ghc.haskell.org/trac/ghc/wiki/Records/OverloadedRecordFields/Redesign) seems to be exactly about this. As a side note, row type polymorphism (aka structural typing) makes this possible - it's used in (my very own) [Infernu](https://github.com/sinelaw/infernu/blob/master/docs/type-system.md) to **safely** support this JS idiom.
A really straightforward example would be something like data Resource s m r = Resource { ... , resourceExists :: Handler s m (Maybe r) , deleteResource :: r -&gt; Handler s m Bool , lastModified :: r -&gt; Handler s m (Maybe UTCTime) ... } and then ultimately existentially quantify over the resource type variable `r`.
MultiParameter TypeClasses
While I think jumping in and doing your own app is probably best, you could move on to problems from [codewars](http://www.codewars.com/about) and the like.
With that, there's some risk that an element will be read from one of the channels and then be eaten by the timeout exception, so while it's mostly "if and only if", in reality it's "only if". Even better: timeout &lt;- registerDelay 4000000 mresult &lt;- atomically ( (Just &lt;$&gt; ( readTChan chan1 `orElse` readTChan chan2 `orElse` readTChan chan3 )) `orElse` (pure Nothing &lt;* (check =&lt;&lt; readTVar timeout))) 
 traverse :: (Traversable t, Applicative f) =&gt; (a -&gt; f b) -&gt; t a -&gt; f (t b) (&lt;-&gt;) :: Functor f =&gt; f (a -&gt; b) -&gt; a -&gt; f b For applicative `f` and trivial `t`, they only differ in the first argument. Of course, "traverse for a trivial Traversable" sounds a little oxymoronic. Much closer is `&lt;*&gt;` from applicative: &lt;*&gt; :: Applicative f =&gt; f (a -&gt; b) -&gt; f a -&gt; f b &lt;-&gt; :: Functor f =&gt; f (a -&gt; b) -&gt; a -&gt; f b
OK. I found a simple expression for `&lt;-&gt;` in terms of Applicative: fs &lt;-&gt; e = fs &lt;*&gt; pure e
I'm not sure how this is related to Haskell. But the more algebra people know, the better.
I don't really know if it's widely used. But the function is already defined in lens as (??).
Those are [language pragmas](https://downloads.haskell.org/~ghc/7.10.1/docs/html/users_guide/pragmas.html), they tell the compiler to use features that are not part of the Haskell standard.
The /r/haskell is one of the places that definitely appreciate it the most. But, totally see your point. 
*Total tangent, but I think it's nifty.* One thing that isn't mentioned in this post is that this is an awesome use case for parametricity! In this post we have a bunch of new representations for our types but how do we prove that they're not "too big"? That is to say, we want to show that if something has the type `Bool` (and doesn't do something naughty) that it's actually true or false. Developing this with mathematical rigor is kind of hard (and really cool) but the gist is easy enough to argue for. Since `a` is universally quantified over, to produce it we have to use one of the supplied arguments. This argument is exactly what parametricity is all about: that things which are universally quantified over must be treated parametrically.
New user here too, Hrothen has already answered the question, I have a related one please: aren't those language pragmas a bit of a code smell? I see them included in just about every Haskell project, but pragmas seem like they should be a rarity (since the standard should cover most development needs). It's an indication that the Haskell standard isn't keeping up with how people are using the language. Perhaps people just use the pragmas for convenience features, or I'm just getting something wrong.
&gt; What conventions do you use to name them? I try to make them unique across the package, and tend to avoid overlaps with base. &gt; Does anyone have good conventions for a functional language? I think appropriate names has less to do with what values are first-class and more to do with scoping / namespacing rules. So, really the OO guidelines are broadly applicable. I generally err on the side of having a too-long name. If someone wants to short alias because they are using the value often, it's easy for them to have a (non-exported?) short binding. That said, I try to keep the domain in mind and avoid repeating the same semantic information across too many names. Again, if you lift my functions out of their natural domain, it's easy enough to rebind them to something longer that makes the extra semantic information explicit. I hate "systems Hungarian" with a passion, and avoid it at all costs. &gt; how much to use typeclasses purely for naming purposes? I don't believe this to be good practice, I like my typeclasses to have laws and only one (obvious/natural) implementation per type. That said, if I had something very record heavy I might end up using the Has&lt;FieldName&gt; pattern, and mixing in enough OverloadedRecordFields to solve my problems. But in my [only published package](https://hackage.haskell.org/package/cryptsy-api), I didn't think it was too record-heavy so I just did psuedo-Hungarian prefixes to disambiguate things. Another solution is Vinyl or HRecord instead of records. It's different and I haven't yet used it myself, but I hear tell it can be quite effective.
I understand your intuition for code smell here, but I doubt think it's warranted. These are features that change the language itself, and sometimes in unsound ways. Some, like TypeScopedVariables (type annotations for terms in the definitions) I think are fine and should be included by default (though I might be missing something). Others, like OverlappingInstances, are definitely unsound and only for advanced use cases, and it's very important that readers of your code know that's what you're doing.
There are really a bunch that are pretty much unanimously beloved by everyone. The only reason they aren't in the standard is because its easy to make a change to GHC than it is to create a new standard.
Aren't those imports a bit of a code smell? I see them included in just about every Haskell project, but imports seem like they should be a rarity (since the standard prelude should cover most development needs). It's an indication that the Haskell standard library isn't keeping up with how people are using the language. Perhaps people just use the imports for convenience features, or I'm just getting something wrong.
Any reason Haskell couldn't adopt row type polymorphism? I've used it a bit in Purescript and it seemed pretty useful.
It's the same with any language, the functions that other programmers will use most often should have short, intuitive names, the functions that are more esoteric can have longer, more descriptive names. I think usually control types (Monad transformers, Categories, and Arrows, etc.) and numerical types (e.g. characters, or integers modulo some prime) should always be unqualified. However container types (Strings/Text, Lists, Vectors, Arrays, Maps, Trees) should always be imported qualified. Data types for very specific things, like nodes in an Abstract Syntax Tree, should be unqualified and have long, descriptive names. Sometimes I write a module for a container type with the expectation that people will import it qualified. When I do this, I keep the names very short and simple, and I never declare any infix operators, because using a qualified infix operator looks something like `before Infix.&lt;+&gt; after` which is really ugly. If I declare a data type similar to a well known data type in the containers package, I use the exact same names (e.g. union, intersection, difference) as the analogues from the containers package. So in your code you can write `Map.union` and `Trie.union`, the names are easy to remember and qualified so they don't collide. import Control.Applicative import Control.Monad import qualified Data.List as List import qualified Data.Map as Map import qualified Data.Text as Strict import qualified Data.Text.Lazy as Lazy import qualified Data.Text.Lazy.IO as Lazy sp :: Strict.Text sp = Strict.singleton ' ' toPair :: Lazy.Text -&gt; [(Strict.Text, [Strict.Text])] toPair x = do x &lt;- pure $ Lazy.toStrict &lt;$&gt; Lazy.words x guard (not $ null x) return (Strict.toLower $ head x, tail x) main :: IO () main = Lazy.getContents &gt;&gt;= mapM_ (Lazy.putStrLn . Lazy.fromChunks . List.intersperse sp . uncurry (:)) . Map.assocs . Map.fromListWith (++) . concatMap toPair . Lazy.lines I think this is a good example of how qualifying names can make working with data types very clear, even when you mix strict and lazy Text strings. The only point of confusion may come from the fact that the names `mapM_` and `concatMap` control structures are similar to the name for the `Map` data type. But this should not be a problem for experienced programmers. And in the main program, since no one will be importing or using these functions, I make the names nice and short so they are easier to type.
Lists of language pragmas can get unwidely, but let's see the issue from another angle. In Haskell, it is often idiomatic not to reach for the "big guns" if a more restricted solution suffices. For example, a function of type **a -&gt; IO Bool** can do way more things than a function **a -&gt; Bool**. So the latter should be preferred unless the former is actually necessary. Similarly, using Applicatives instead of Monads is often recommended when the structure of your computation is "simple" enough. Applicatives are less powerful, but you gain in analizability and understandability, precisely because they can do less things. One way to see language pragmas is as an application of this principle at the language features level. By looking at the pragmas, people reading your module can see what advanced language features will *not* be used in it, and adjust their expectations accordingly. 
I have to disagree. Bundling several extensions under one date-limited non-descriptive name is a recipe for unintelligibility. Does common extensions 2016 include ImplicitZygohistomorphicPrepromorphisms or was that only introduced in common extensions 2018? I think explicit naming and fine-grained control is much clearer, and we should stick to that until the Haskell standard is updated. I never liked the catch-all Glasgow exts exactly because that's a meaningless name from a programming point of view. 
The trouble is it's very hard for beginners to tell which is which. I think it would be good for the community if some of the least controversial extensions from types 2 and particularly 1 could be formally made part of the core language.
I'd suggest that if you find you ever need to activate NoMonomorphismRestriction that the better solution to the problem is to add a top-level type signature.
is this a Scotty fork?
Thanks man, thats what I've been waiting for.
Very cool. good luck! One question though, How does Spock compare with other Haskell web frameworks (such as Scotty, Happstack, Snap or Yesod)? Why and when should I choose Spock over other frameworks?
I don't believe you can 'market' excellence in any academic sense. People have tried this in the past, and it resulted in new-age quantum mysticism, the Mozart Effect, and a stupid pop-celebritization of academic figures. Einstein talked funny at 5 years old! he was sort of religious which is why i believe in jesus too! he had pretty legs on imgur omg!! dat hair tho! Literally everything except the reason Einstein is great. You can 'market' Haskell as being 'secure' and 'safe' and 'reliable', but you can market anything that way. A shiny, high-DPI lock icon will mean more than any attempt to explain type theory and referential transparency, even though the pretty icon isn't protecting anyone from anything. The reason Haskell is great is because its systematic faithfulness to mathematical abstractions, and as soon as you start talking about math, the masses leave the room. They were only there for the beautiful lock icon and the funny hair.
&gt; module Main where{import List;import System; import Data.HashTable as H;(???????)=(concat );(??????)(???)(????)=((groupBy)(???)(????)) ;(??????????????????????)(????)=((?????????? )((tail).(???????))((????????????????????)(( ??????)(?????????????????????)(????))));(??) =([' ']);(??????????????)=((hashString));(?) =((&gt;&gt;=));(???????????????????????)([((???)), (????)])=((?????????????)(???))?(\(?????)-&gt;( (????????????????)(==)(??????????????))?(\(( ???))-&gt;((??????????????????)(??????????????? )(???)(?????))&gt;&gt;((?????????????????)(???))?( \((?????))-&gt;((((???????????????????)((????)) ((??????????????????????))((?????))))))));(( ???????????????????????))(??)=(????????????) ("usage f dic out");(?????????????????????)( (???),(??????))((????),(???????????????????? ))=((???)==(????));(?????????????????)(???)= (toList)(???);(????????????????????)(????)=( ((??????????)(((??????????)(snd)))((????)))) ;(??????????????????)(???????????????)(???)( (?????))=(((mapM)(((???????????????)(???)))( (lines)(?????))));(???????????????????)(???? )(???????????????????????)(?????)=(????????? )(????)((unlines)((???????????????????????)( ?????)));(????????????????)(???)((????))=((( new)(???)(????)));(main)=((???????????)?(((\ (???)-&gt;((???????????????????????)(???)))))); (???????????????)(???)(????)=((????????)(??? )((sort)(????))((??)++(????)));(???????????) =(getArgs);(????????????)(???)=((((print))(( ???))));(??????????)(???)(????)=(((map)(???) (????)));(????????)((???))(????)(?????)=(((( H.insert))((???))(????)(?????)));(?????????) ((???))((????))=(((writeFile)(???)((????)))) ;(?????????????)(???)=(((readFile)((???))))} My eyes!
Rank n types is technically undecidable isn't it?
&gt; It's an indication that the Haskell standard isn't keeping up with how people are using the language. The last two standards were Haskell 2010 and Haskell 98. Most people want to take advantage of new research and features in less than a decade. 
"Wow, this file is really popular! Some tools might be unavailable until the crowd clears." -- by Google docs
Excelent
You can't _infer_ all types with Rank N Types because you lose the Principle Typings property. But that just means that sometimes users will need to describe a function with an explicit signature. Checking itself is still decidable. 
&gt; . But that just means that sometimes users will need to describe a function with an explicit signature It can be worse than this, sometimes the compiler will give up if it doesn't meet certain criterion. For example, `runST $ ...` is very common and the compiler actually has some sort of special rule that handles this case, but if you manipulate it a bit (I can't describe exactly how because I forget how exactly I ran into this issue) the compiler will give up. So yea, I guess its decidable but not always complete. And in this case there was no way to annotate with an explicit signature.
I was wondering what the correct table should be, so I wrote [a SmallCheck test](https://gist.github.com/gelisam/f279f148bba92edd224c) to figure it out: | I A B C D --+---------- I | I A B C D A | A D C B I B | B C D I A C | C I A D B D | D B I A C (A * A) * A = B A * (A * A) = I 
Their [last post here](http://www.reddit.com/r/haskell/comments/2zqyes/we_released_a_video_library_about_functors/) got a whopping 73 upvotes, I think they earned the right to post their non-Haskell videos here :)
Any code which extensively uses a Haskell library you [don't understand](https://github.com/ekmett/category-extras/tree/master/Control/Morphism) tends to be somewhat obfuscated anyways. (I shouldn't single out that package, I am sure recursion-schemes is a lovely package, and I am just too lazy to work out why)
For the first case I have been finding type holes really helpful lately.
I love [this slide](https://docs.google.com/a/fausak.me/presentation/d/1a4GvI0dbL8sfAlnTUwVxhq4_j-QiDlz02_t0XZJXnzY/preview?sle=true#slide=id.ga91f3f66a_1104): &gt; Haskell crushes imprecision of thought
It's not stagnant, but it's slow, for at least two reasons: * There is a lot of discussion over each addition to the standard. * The extension mechanism is so good that people generally prefer to just use them rather than spending time arguing on a standards committee. The latest standard is 2010. Before that it was Haskell98, which is still a great language. In between there was a standards committee that got bogged down. A bunch of people stepped in and have devoted a lot of time to improving the procedures without sacrificing quality or adding churn. I'm really impressed with the way they have done it and reap the benefits daily. 
I wish this import Data.Text (Text) and qlf as T , Data.Vector (Vector) , Foo.Bar hiding baz , {- ... -} was legal Haskell, though.
Which chapter of which Bob Harper book? HoTT?
The entire section entitled "Equational Reasoning" in PFPL is about logical relations. Chapter 48 is about equational reasoning in System F (eg parametricity).
Right. The straw man code for doing this is available [here](https://github.com/acowley/MetaPragma). I'm keen to retain the visibility of language pragmas in source files, but as the list of pragmas gets longer and longer, they become a problem. There are so many names to look up if you are like the OP, which effectively hides the controversial extensions among the non-controversial ones. This is like using a long license agreement that nobody will read! Next, look at those names, it's a disaster! There is no consistency in how language pragmas are named, and the names alone are only suggestive of their impact to experienced users. Another point here is that I don't want to *hide* the definitions of these meta-pragmas, but instead use them as one level of indirection. I'd like to see a conservative core that most people use, then each organization can also have their own set of pragmas. That these are *not* baked into the compiler means that things can change without grand bureaucratic effort.
Added to my list, thanks!
I like how the speaker has a curious attitude throughout, e.g., with the Latin square for a group with 4 elements "Will this work? I dunno, lets try it and find out". It's the correct attitude for math, and learning in general. It probably would've been worth mentioning that a group is really *a group of symmetries* (of some object) where a symmetry is an "undo-able transformation" that leaves the object invariant...like permuting labels, or rotating the plane while leaving an equilateral triangle invariant, or... When you begin thinking about elements of a group as morphisms, the law of composition as the composition of these morphisms, then the peculiar theorems of group theory become a lot more "natural". (Fun fact: a group can be thought of naturally in the category with a single object, where all the morphisms are automorphisms.)
&gt; I'd like to see such an argument made I think all we have right now is argument by example. Like many people who are about this particular topic, I work this way daily and work with lots of analysts at other companies that also work this way. There are very very few people even trying to use Haskell for these kinds of workflows and then a small percentage again that are satisfied by it. I would switch in a heartbeat if it were even feasible, but it's not right now. To me the biggest hurdle is that Haskell's major strengths around type safety and reliability just flat out *do not matter* when you are throwing numbers around in a repl and plotting things randomly. If you try to run a broken expression you find out immediately and can just pick up where you left off and try again. So in exchange for an unwieldy API that requires much more mental overhead than just typing `eig(rand(100, 100))` you gain confidence that the code won't crash under a bunch of hypothetical inputs that you are absolutely never going to run into because you're about to throw this code away. For once you really *do* want an unstructured playground where you can rely on keeping everything relevant in your head. So far the only answer I've seen is to call into languages like Julia and Python (which in turn calls into C and Fortran). If that's the case, it's hard to justify not just using those languages directly. I *can* see myself using Haskell to build longer-lived applications which are backed by numerical analysis in some way, but that's not what "data scientists" are trying to do and building GUI applications in Haskell is about 10 million times worse than other "general purpose" languages, so that's kind of moot as well.
&gt; … but Haskell is the only major language whose type &gt; system can verify the lack of side effects. &gt; `factorial :: Int -&gt; Int` does not wipe the DB Unless it uses `unsafePerformIO` or `accursedUnutterablePerformIO`, of course. But hey, you're *selling* something, so it's OK to gloss over some things, right?
That example isn't actually allowed by RankNTypes alone, just by the special casing. You can tell because `id runST ...` doesn't work. Making sense of `runST $ ...` requires instantiating the type variable `a` in the type `(a-&gt;b) -&gt; a -&gt; b)` of `$` with an entire quantified type like `forall s . ST s r`, which isn't allowed without something like `ImpredicativeTypes` (but that doesn't seem to help either, maybe it just applies to data types?).
Thanks so much for this post. I hit this case and would have just added the language extension instead of making the type more specific. Much appreciated.
This was my first thought when I saw this at the talk. I'm still waiting for the language which dooms itself by completely forbidding backdoors :3 
 {-# LANGUAGE Safe #-}
Exercism looks incredibly interesting!
Yes!
The correct version would be something like &gt;`factorial :: Int -&gt; Int` does not wipe the DB ... unless you have deliberately irresponsible programmers on your team. I think the omission of the last part is at worst a white lie. First off, those functions are clearly marked as unsafe. The name alone should tell anyone with common sense, that they are to be avoided. Second, if any kind of code review with an other Haskell programmer happens, the alarm bells will go off as soon as one of them appears on screen, unless the reviewer is equally misguided. Third, everyone who learns Haskell and at any point in time voices his curiosity about those functions will get told to not use them unless he really absolutely knows what he is doing. So yeah, it is ok to gloss over those things. If I am completely honest, bringing up unsafe functions to me is like asking someone who presents a new gun designed for safety whether the gun will prevent one from deliberately shooting in ones own foot. You likely will be able to do so because you can remove the safety features manually via tinkering or because they are already removable, but why the hell would you deliberately shoot in your own foot?
&gt; other I see, thank you for reply. 
The amount of disdain from programmers with only a passing interest in Haskell is simply amazing to me. 
It's not about responsible or irresponsible programmers (are authors of the `bytestring` package deliberately irresponsible?), it's about the claim &gt; type system can verify the lack of side effects It can't. At least not without "Safe Haskell", which is far from synonymous with Haskell.
&gt; it doesn't feel like it makes me better at actual programming, just better at problems. The a lot of problems in actual programming, too. Are you thinking you might want more design instruction / experience? I know small competitions like codewars programs don't exactly encourage modularity and maintainability. Try [aosa](http://aosabook.org/en/index.html) for case studies. For experience, I suggest scratching your own itch or improving the Haskell ecosystem.
I'd still want to be able to dd / cat it somewhere else for analysis if (e.g.) my liblogmemheap.so.2 is refusing to link on this system.
&gt; I'm still waiting for the language which dooms itself by completely forbidding backdoors Agreed. In fact, I'm trying to learn enough, and experiment enough, to write a *systems* language that does that. Hopefully, I'll be at least as good at avoiding success at Haskell (assuming I ever get done; I don't get to spend much time on it.)
lens signatures are obscene, but I've found that understanding the signature that GHC gives me and "contracting" it to the right alias, for all my hand written exported lenses, has taught me a lot about lens and what a well designed library it is.
This has a few good points in it, but I'm not sure I agree with the overall technical tone in these slides. Businesses are - for good reason! - very risk averse when it comes to anything outside of their core business. Your boss probably already understands that you find Haskell very exciting, would be happier working with it and could possibly even be a little bit more productive (?). On the other hand, from your boss's perspective it's a huge unfamiliar distraction with very little precedence in industry. Can we hire people? Do they all work at banks? Are we going to be rebuilding rails from scratch? How has this thing existed for 20 years without becoming mainstream? Those are the sorts of questions you need to address. The best way to move Haskell forward is to build something important like [opaleye](https://github.com/tomjaguarpaw/haskell-opaleye), [wreq](http://www.serpentine.com/wreq/), [haskellonheroku](https://haskellonheroku.com/), [pipes](https://github.com/Gabriel439/Haskell-Pipes-Library), [cloud haskell](http://haskell-distributed.github.io/) or [yesod](http://www.yesodweb.com/) and then sell *that* instead; write some amazing documentation and tell as many people as you can... And then probably try very, very hard to get a job at one of these newfangled startups that use Haskell :)
Honestly, who cares? If you are building a startup in Haskell, you don't have to answer to anybody about your language choice.
I'm a huge fan of Spock. I've used it in a couple of projects, and I've been very happy with it.
People fear what they do not understand. Fucking monads! How do they work?
Sure. But it can be bundled with a certain "pack", and TypeFamilies with another.
I agree. And the disdain for correctness in general is even more scary.
Welp, time to move to a [different language then](http://i.imgur.com/H27SUx5.png) 
As long as most files have at most ten language pragmas and closer to five enabled on average I really don't see a need to add the semantic burden of multiple packs to each programmer's mind.
Harsh or not, it's only mildly funny, and it doesn't hurt, so it's no good satire. What does hurt, OTOH, is compassion and pity for the ignorance of the author, should he be serious about it.
&gt; That way lies ~~PHP~~ insanity. Disclaimer: I write PHP code for a living.
It's not harsh at all! The sign merely says that once you discover Haskell, there's no going back.
Yes Austin.
Coq has no backdoors as far as I'm aware. In fact, I don't believe it has any doors at all.
https://twitter.com/aisamanra/status/579040253169668096
wat
You're absolutely right, sorry if it was confusing. This post was a bit of a brain dump. I could have used `()`, but I wanted to be clear that the value shouldn't be used, so the first word that popped into my head was bottom. You could in fact replace it with a real bottom and add a type to prevent it from ever being entered, e.g. bottom = bottom fromJust :: Just a -&gt; a fromJust p = p bottom id 
Author of slides here. I agree that that slide was a weak point. I considered getting rid of it, but (as you may notice) I back the claims up (see: slide 35) in subsequent slides. I think that more is gained than lost by the whole set (29 to 35) so I kept it in. I didn't intend these to be interpreted as precise or mathematical graphs because productivity is impossible to measure (the idea that it can be measured brought us that "Agile"/Scrum shit that I hate more than the worst-designed programming language). I wanted to convey "Haskell is optimized for long-term productivity, which justifies it being harder to learn than many languages". The other failure, on my part, was that I conflated behavior with language. I had a "good" curve for *Haskell*, an "okay" curve for "Brand X" (e.g. Python or Ruby as typically practiced) and a "bad" curve for Enterprise Java. Those are all reasonable claims, but I was comparing a *language* (Haskell) to a *behavior* (Enterprise Java). It's not fair to compare "Haskell, the language" against "Java, as practiced by mediocre programmers in hobbled environments" because to do so assumes that a person or company using Haskell can't possibly stagnate (which, although the odds *favor* using Haskell, is probably untrue). 
Well, [someone](https://www.google.com/maps/@43.035882,-83.720566,3a,15y,307.83h,81.89t/data=!3m4!1e1!3m2!1srl_IOWKLTh9A96OCljVktQ!2e0) seems a little more optimistic about it (though this is not a very nice neighborhood... something like 1 in 10 houses occupied when I visited).
Being optomized for long-term programmer productivity was, incidentally, part of how people sold Perl. Sure it's hard to learn and confusing at first, but oh when you have Perl down you are so productive. So selling it on that point you should consider that the person listening might remember exactly this being said about Perl way back when.
Where is that? The sign looks American but, well, Thüringen is a bit of a distance from there.
Haskell prevents you from *unintentionally* introducing side effects in pure code. That's a big benefit worth advertising, because even well-meaning and disciplined programmers introduce side effects; they cannot realistically audit all code they transitively depend on for the presence of side effects without the aid of a type-checker.
Cabal can already do this in the .cabal file. It turns out that when given the option, Haskell programmers tend to prefer listing extensions in source files. Probably because there isn't a huge cost to that list in terms of readability, and it's easier to maintain the syntactic extensions near where they are used.
Yeah, as far as I can tell nobody really cared about ruby until RoR took off.
I consider the phrase "Incoherent Instances" to be a warning in and of itself. But a further warning when the condition occurs wouldn't hurt. I've occasionally wished for warnings about unused language pragmas in general.
I try to avoid "import qualified ..." Instead, I generally import all names explicitly, and frequently use imports like import Data.List as List (map) import Data.Map as Map (map) if only one type of map is used it can be used unqualified, and the imports can be referenced for which one. If more than one are imported, you can say List.map or Map.map, and the compiler will signal an error if you don't.
I'm guessing here, but I think it might be that GHC cannot actually know about future instances that haven't been declared yet -- so it would be hard to warn *consistently* without whole-program compilation. (This wouldn't preclude warning about cases where it *does* know about multiple instances, but I think it might devalue such a warning if it weren't consistently triggered.)
&gt; There is a lot of discussion over each addition to the standard. You wouldn't happen to have a link to this discussion would you? I think I visited the Haskell' Trac page about a month ago and saw literally 0 activity. That sounds pretty stagnant to me. Community members talking about different extensions doesn't count as "lots of discussion" to me if a) those members aren't part of the committee that's supposed to be discussing standards, and b) that conversation isn't visible. 
&gt; a sigil such as $ or % How fitting that perl invokes the occult.
I actually like our current policy. It allows package authors to communicate more information in their version numbers. In the Semver convention, there isn't really a good way to communicate the difference between a very large API change and a relatively small but still incompatible API change. Our system allows you to do that. For the former, you can bump B and keep A constant. For the latter you can bump A.
I don't follow the standards discussion much, but I'm sure there's a public list. The libraries list is a good example of this kind of discussion, though: https://mail.haskell.org/pipermail/libraries/ There are a multitude of haskell mailing lists for various specialties. https://wiki.haskell.org/Mailing_lists
&gt; I try to avoid "import qualified ..." I might use it too often. It's generally my default, since some packages recommend it (Data.Map, e.g.) and the longer name can give more clarification. So, I don't always think of the times when it's just be simpler to drop "qualified".
Could you make this a front end to lambdabot?
https://github.com/ivanperez-keera/haskanoid
Personally, I prefer Semver: - It's consistent with *pretty much what everyone else does*. - It's easier to remember 3 numbers than 4, and also less stuff to type and say. - The meaning of `A` vs `B` was never explicitly mentioned in the Haskell PVP. Using `A` vs `B` to indicate large vs small API changes, while sensible, is therefore not necessarily true everywhere. (There's also the question of what is considered a "large" change? Also, are periodic small changes necessarily better than large but rarer changes?) - Having a single number `A'` might have a psychological effect of making the author more cautious about API breakages (totally speculative!). Perhaps one could have an exception for versions below `1.0.0`, which not obey any rules because they are typically highly experimental libraries that change rapidly.
Here are a couple of resources: * https://wiki.haskell.org/Error_reporting_strategies * https://www.fpcomplete.com/user/bartosz/basics-of-haskell/10_Error_Handling * http://www.haskellforall.com/2012/07/errors-10-simplified-error-handling.html (I.e. have a look at [either](http://hackage.haskell.org/package/either) and [errors](http://hackage.haskell.org/package/errors) which is built on top of it) [note](http://hackage.haskell.org/package/errors-1.4.7/docs/Control-Error-Util.html#v:note) turns a `Maybe` into an `Either` and `Either` is also a monad, so you can use do notation with it. It's not a great idea to use `error` (you can read it as "crash"). EDIT: ~~Unfortunately it takes a little bit of effort to get familiar with `EitherT`, but I'd say it's worth it.~~ (In this case it looks like you don't need `EitherT`)
What am I suppose to gather from this? It's an SDL+OpenGL app that runs as-is on a Linux desktop, With a *hint* in the readme that they have it running on android, it doesn't use any Android APIs, or even include how to cross compile it for android.
I'm aware of the .cabal file option. I just think it would be nicer to be able to bundle things up once in a library, and then explicitly say which bundle you're using the same as with library imports (which would as you say result in the info being in the source file).
Yes, I think you are correct in that instances can be created later (ie. in other modules) that might cause it to pick an arbitrary instance. Hence GHC would not be able to know in general if an instance will be picked arbitrarily where a function is defined. However, when the function is called, eventually an instance will be picked, so GHC could produce a warning there.
No, both A and B are for breaking changes (update, removal). C is for changes that can be protected against (e.g. with explicit or qualified imports) such as additions. D is for bugfixes, documentation changes etc. 
Do you have a suggestion how to do this without breaking everything? If not I'd say it's a non starter. 
Oh yes. I would write it like that, but I'd like to have `LANGUAGE NoMultilineLet` and be able to write: let x = if &lt;long stuff&gt; then &lt;long stuff&gt; else &lt;long stuff&gt; 
The Wiki could be updated to mention the transfer and the packages adopting the Semver policy would specify that clearly on their front-pages on Hackage.
Personally I'm eagerly waiting for react-native for android. They've already released it for iOS. My plan is to use react-native, which is JS, and then a haskell-&gt;JS compiler (ghcjs or haste probably). But then I don't know react so maybe I'm naive :-)
On a side note, I added -XSafe flags to the modules above, and they still seem to work. I am not sure if I am using them correctly, but if I am, then it would mean that Safe code is not guaranteed to be deterministic [Determinism is claimed by the GHC user's guide](https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/safe-haskell.html#safe-language): &gt;The Safe Haskell *safe* language guarantees the following properties: &gt; &gt;* Referential transparency — Functions in the safe language are deterministic, evaluating them will not cause any side effects. [...] I don't think this behavior is a bug since it seems to be allowed by the Safe restrictions on overlapping instances: &gt; In the safe language dialect we restrict the following features: &gt; &gt;[...] &gt; &gt; * *OverlappingInstances* — This extension can be used to violate semantic consistency, because malicious code could redefine a type instance (by containing a more specific instance definition) in a way that changes the behaviour of code importing the untrusted module. The extension is not disabled for a module M compiled with -XSafe but restricted. While M can define overlapping instance declarations, they can only overlap other instance declaration defined in M. If in a module N that imports M, at a call site that uses a type-class function there is a choice of which instance to use (i.e. an overlap) and the most specific instances is from M, then all the other choices must also be from M. If not, a compilation error will occur. A simple way to think of this is a same origin policy for overlapping instances defined in Safe compiled modules.
What if we declare PVP as Semver with an extra version number prefixed? Semver: X.Y.Z New PVP: M.X.Y.Z Where X, Y and Z are according to the Semver spec, and the "M" is merely a "marketing" number. Something rather arbitrary that a package author may choose to increase for what ever purpose he sees fit. See /u/mightybyte's comment below; he argues that it is nice we can show "small" and "large" changes in our version numbers -- that small-large distinguishment is exactly what I call arbitrary. Next we can try to convince Tom Preston to make this "marketing" number (optionally) part of the Semver standard and we'll all be more/less in the same camp :) edit: small touch-ups 
Which is what you should expect when working with software that has dependencies on incomplete libraries. It just encourages people to put out a version 1.X.X (which I see as a good thing).
Unfortunately Haskell is the wrong tree in the wrong forest. There isn't a good story for Haskell &lt;-&gt; Java interop, so examples you might hear about use the NDK instead. This doesn't make sense for any apps other than games. 
You need our current 2-dimensional major-version-component scheme to allow two independent API generations/epochs to co-exist (think `parsec-2.b.c.d` and `parsec-3.b.c.d`). Otherwise you couldn't update the old 2.x branch of `parsec` anymore with backward-incompat API changes
&gt; The aforementioned incompatibility forms another issue: it is a potential reason of confusion for the newcomers, who've previously applied Semver in other languages. Well, then just imagine the huge mess causing endless confusion trying to figure out at which point a package has switched to the "new" versioning policy (or whether that package has adopted it at all yet). The current PVP scheme is well established, and there's a lot of deployed tooling that assumes the current PVP-scheme to be adhered to. You should come up with very well-thought migration-scheme, before we can start seriously considering changing the PVP
As I mentioned already, `M` can also be used to keep apart distinct API generations/epochs under the same package-name.
That's terrible... :-( Now everytime you include a build-dependency, you'd have to check Hackage whether the version range you're currently developing against is before/after/across the semver-switch in order to decide which upper bound to set on the respective `build-depends` entry.
Why doesn't it make sense? That's the only way you can enforce that no two versions of the same package are simultaneously used in the same install plan. Is there any actual downside? 
&gt; rather than just "some" as you want us to believe to give more weight to your argument No need to nitpick. Let's stay constructive. That reference had neither any significance for the argument nor any of such intention. Besides a difference between the definitions of "most" and "some" without any real stats can only be subjective.
This should be a top level reply. It is by far the best reason to retain the current scheme. 
And how would you split the package? Into one named parsec2 and one named parsec3? That wouldn't really be any better than what we have now.
It is a lost battle, unless you are up to redo the whole UI yourself in OpenGL/SDL, as well as, writing JNI wrappers or Haskell modules that duplicate Android frameworks. I have been using C++ for common business code between Android and WP8 for my hobby mobile projects. Recently I decided that the NDK isn't worth the pain for hobby projects and will be focus on Android Java land instead. 
Let the one with the higher number be parsec, since that is the one people would naturally pick if they don't know about the distinction and therefore is what most people do think of as it being parsec. Rename the other one to something cool, maybe alluding to its origin or not. Also, I feel that I should note that I am not an expert in terms of managing packages. To me it is simply weird to have two different things under one name, and splitting seems to be a natural way to resolve this. There may be reasons to keep things under one name which I am not aware of.
Curry gives you something fluid, uncurry gives you the meat back
A function is "curried" if it accepts only one argument at a time. Read the identifier `curry` as a verb; it's a function that transforms an uncurried function into a curried one. `uncurry` is just the reverse.
Why don't we actually have our tools understand the meaning of versions? step #1, optional: Tools can tell you whether you have a major or minor compatibility change (Elm already does this) step #2: have a cabal field `versioning-policy` whose values can be `PVP` or `SemVer`. step #3: There are different possibilities here that need more thought. But it is now possible to talk about a PVP in terms of SemVer if you like.
Actually I meant that the current parsec package stays the way it is and all the stuff from parsec2 gets moved into its own new package. The older versions of parsec2 would still be in the parsec package, but the new ones would not. It's not optimal, but it at least sounds like a usable solution to me.
You might consider [Frege](http://www.frege-lang.org/), a Haskelly language that targets Java. As others mentioned, you might consider using React-Native with Haste, PureScript, Elm, or Idris. Personally, I'm curious about the viability of using the same basic architecture as React-Native, but with (something like) GHC, Idris or [Pure11](https://github.com/andyarvanitis/pure11), instead of a JS engine. I think there are many of us hoping to see a better story emerge for GHC on ARM; maybe it's time for a consortium focused on mobile tooling for Haskell? Edit: Idris has a Java codegen, but I don't know what kind of state it's in.
Thanks for the reply!
Some of them, anyway :)
Haskell is named after Haskell B. Curry Functions in Haskell are curried - multiple argument functions are done using (-&gt;) rather than (,) So, "curry" is the one that makes functions Haskelly.
This is pretty much how I use it. A.B.C.D A = could break me somewhere, or I want to signal that the world has very much changed B = could break someone else somewhere C = someone could want to detect this with MIN_VERSION_foo D = compatibility fixes. The PVP is semantic versioning with an extra prefixed digit.
Since last year or so, the PVP was revised to consider additional instances added for a type or class you define in the package to be a minor update (non-orphans). Therefore if you supply an orphan instance, then you should depend on a particular minor version of the packages that supply the type and class. (or just whichever one depends on the other, if they have a dependency relationship.)
Curry converts coupled function?
"By default, functions in Haskell are curried." If you see a function that looks like a function in Python or Java (it has parentheses - takes a pair) then you can apply curry to it to get back to default Haskell. That's my mnemonic.
Lots and lots and lots and bucketloads, and seriously, a huge amount of German immigrants have established a mostly quiet, but surprisingly wide-spread cultural background noise in America.
There are certainly a handful of extensions which should unquestionably be added in the next Haskell Report revision. What worries me about some forms of this proposal (particularly, the idea of defining new pragmas in libraries - or even worse, individual config files) is that it incentivizes people to adopted language variants in cliques. We're already starting to reach a point where people are publishing competing ideas of standard libraries. Let's not do that with the language, too. A community consensus-building process may be painful, but it also gets people basically on the same page. That said, I do think there's room to mitigate the long pragma lists somewhat, by defining *meaningful* higher level extension names for sets of extensions that fit together cohesively. But the inclusion of years and version numbers in the name is a bad sign there...
elcric_circle is right that if you decide to use Haskell for this you would definitely get support from the community. However, making an Android chat app using material design will be much, much easier using a Java-compatible language, which unfortunately Haskell is not right now. If your goal is to solve Haskell on Android then go for it; lots of us would be very grateful. If your goal is to build the app, though, use Java or another JVM language. You said you're an amateur programmer; depending on your comfort level, Java might be the best choice if only for the many Android examples you'll find in that language.
Well, we are talking about a situation which is best dealt with by not getting there in the first place. No solution is perfect.
No solution is perfect, that is why authors of packages sometimes need to rewrite them once they gain more experience with the problem domain. Your solution of old versions sharing a name with the new incompatible rewrite while new updates compatible with the old API get a new name seems to solve nothing at all though. In general I do not see what the big deal is about the extra digit in the version number. 
When I first learned about the PVP I immediately wondered why it wasn't just SemVer. This caused me some amount of pain. But then that pain resided within the minute and I've not felt it since. I think that regardless of which versioning scheme is better, the delta on value is tiny while the implementation pathway would... not be tiny.
Agreed, I've always considered the first component like an so-version ahead of a semver. So, if glibc-2.16 (which is installed as libc.so.6) were on hackage it's version number would be 6.2.16.0. It's mostly prevented us from having to have package *names* with numeric suffixes, unlike some similar services.
I have looked at ATS, although not deeply enough to determine if it completely forbids backdoors. I probably should spend some more time on it, but ISTR not liking the syntax and tooling nearly as much as Idris. In my language, I'd also like to include higher inductive types and the univalence axiom. I think I actually came up with a potential practical use of HIT earlier this year, though I can't recall it now. That convinced me that I really do want HoTT as an underlying theory instead of anything else.
I'd be open to changing things if there was a big gain, but I actually like having the marketing digit available. We are perhaps more sensitive than most languages when it comes to breaking changes, even though that breakage may be unlikely to affect a user. This means that many major bumps are uneventful, so it's nice to be able highlight those that are much more likely to be eventful. All that said, the way we all stick with zero versions is unfortunate. I'd support an effort to bump things up to at least 1.X.Y. This would be most easily done in concert with a GHC release as most maintainers have to bump things at that point anyway.
Almost all Haskell functions are curried...they don't take pairs. So to make it take pairs, you uncurry. curry is much rarer because pretty much everything is pre-curried for you or presented in curried form. 
Well Tikhon has sort of written why he writes for Quora [here](http://www.quora.com/Tikhon-Jelvis/Stuff/Why-I-Write?share=1). I feel his various answers related to Haskell and PL topics makes a good impression about the Haskell land there.
&gt; Login with one of the social accounts below Hmm... No thanks.
Is this useful? https://hackage.haskell.org/package/acid-state
Thanks!
Comment starts with {- don't they ? So technically pragma are embedded within comments, no ? 
Datomic is a database that does not mutate data. 
[**@mfikes**](https://twitter.com/mfikes/) &gt; [2015-04-04 19:47 UTC](https://twitter.com/mfikes/status/584442088160366592) &gt; Pic of first time getting React Native to work from ClojureScript, using Om and Ambly. :) [[Attached pic]](http://pbs.twimg.com/media/CBxa-3qWYAAyldb.png) [[Imgur rehost]](http://i.imgur.com/zmnSOJY.png) ---- ^This ^message ^was ^created ^by ^a ^bot [^[Contact ^creator]](http://www.np.reddit.com/message/compose/?to=jasie3k&amp;amp;subject=TweetsInCommentsBot)[^[Source ^code]](https://github.com/janpetryk/reddit-bot) 
I don't know if it is possible, but maybe you can use [Fay](https://github.com/faylang/fay/wiki)/[PureScript](http://www.purescript.org/)/etc, compile the code to JavaScript and then use [Cordova](https://cordova.apache.org/) for the app?
How does that interact with the "1.0.0 is the first official API" policy? Do you shift it over to M? Or should you start from 1.0.0.1 and 1.1.0.0 is the first stable release? Or is marketing releases just zero indexed, and keep 0.1.0.0 as the "official stable release"?
I don't know of an implementation; but I have thought about this a bit. A graph could be stored as a set of node pairs. Inserting or removing members from a set takes log(n) time. Or the graph could be represented as a map from nodes to nodes to make traversal a bit faster.
According to the post I responded to they are independent.
No problemo :)
&gt; What do you mean by "taking a snapshot" in this context? Why do you think it would take log(n)? By taking a snapshot I mean providing version control functionality for graphs. E.g. a user1 has some data, which is a mindmap-like structure, i.e. a graph. Another user, user 2, can fork that data and modify it to include user2's data. So now user1 still can query his data, but user2 can also query his data. Two versions of the same data co-exist together without occupying twice as much space on a disk and in memory (but, of course, for very small amounts of data the spine will be almost the same size as the data itself). By log(n) in this case I meant how much space it costs to create a snapshot (I assume it will be just the size of the spine?). For the same of an example, let's say that queries will be simple like "friends of Rob" (where relationships are e.g. "Rob -friend-&gt; Joe") or "which supplements increase nerve growth factor" (Lion's mane --increases--&gt; NGF). "What is 'lion's mane'? " ([lion's mane --is--&gt; fungi, lion's mane --is--&gt; supplement] . A user2 can disagree with some piece of data and might want to maintain their own version of that graph.
It solves mainly one problem: Not breaking anything. You depend on a version before the split? Nothing happens, because those are still at the same place. You want to use the new version of the split off package? Well, you have to change your dependency to the new package and probably do some other name-related adjustments. Also, the big deal is not having an extra digit. It is that two different things are under the same name. The original argument was, that the current pvp does support that while semantic versioning doesn't. My counter argument basically was that you don't want to do that, because it doesn't make sense because -as already mentioned- it means that you have two different things under one name and what you actually need is to split/fork the package, not an extra digit. (I didn't say "fork" but that's what I meant with splitting.)
I see Bifunctor and Biapplicative in that package.. why doesn't it have a "Bimonad" too?
I'm pretty sure "bimonad" does not make sense as a construct. What would the equivalent of `&gt;&gt;=` be?
Our goal at [Slant](http://www.slant.co/) could reasonably be stated as "get rid of Quora's last reason to exist."
Am I the only one who got the joke? Heh.
I prefer M.0.Y.Z to be unofficial APIs, for any M, so I get a "restart" when I bump the marketing number. That said, I'm also comfortable with that only being true for M=0, so that both 0.1.0.0 and 1.0.0.0 are official APIs.
My mistake. Sorry!
I must have old information then. Sorry!
The feature set is still growing, and some of the new stuff doesn't have front-end access yet. For instance, we can add "stats" ([this topic has them](http://www.slant.co/topics/103)) for one sort of "neutral" attribute. Also, option descriptions can help fill in neutral data, though preventing them from just being used as a dumping ground for ad copy has been an occasional annoyance. For things which might be a pro or a con depending on how you look at them, we actually encourage just adding both. Subjectivity is fine. Users can endorse individual pros and cons, independently of what they think of the option as a whole, which helps get a feel for what's a major point and what's just nitpicking. Another thing that's still back-end-only is topic-level descriptions, [example here](http://www.slant.co/topics/341). One of the things I think these will turn out being good for is establishing baseline assumptions, to avoid people adding "pros" that are common to everything that fits the topic. We do have a [public forum](http://meta.slant.co/) where stuff like this is discussed, if you'd like to get a better look behind the scenes. (It's linked down in the page footers on the site, but really needs to be more prominent.)
Its like twitch but for coding. Very very cool. but if you didnt wanna sign up there is an option to view as a guest :P
just as monadic programs correspond to a CPS translation with a single continuation, bimonadic programs may perhaps be treated as a CPS translation that has TWO continuations, a "success" and "failure" contination?
1. Neil recommends "automatically applied" upper bounds (see post) that are actually more restrictive than the upper bounds recommended in the PVP. 1. Neil does not recommend changing the numbering scheme at all, which is what this reddit post is about. 1. Again, the upper bounds are not what this post is about.
A better type might be: bibind :: (forall r. a -&gt; m c r) -&gt; (forall l. b -&gt; m l d) -&gt; m a b -&gt; m c d Or perhaps, the simpler: bibind :: (a -&gt; m c r) -&gt; (b -&gt; m l d) -&gt; m a b -&gt; m c d I'm not sure yet what the analogous laws should be, exactly. The first type of 'bibind' is more restrictive, saying that the two functions must be totally polymorphic over their second and first parameters of 'm', respectively. The second type says that the functions may result in any type, but the parameters 'l' and 'r' are thrown away to make the resulting type 'm c d'. 
this. google wants you to use java. if you try using something else, you will experience pain. 
Sometimes you *need* a major-version bump as updating build-depends or making a version compatible with a newer GHC can force you into an not 100% back-ward compatible API change. And for that you need that extra dimension the PVP gives you for major versions. Otherwise you'd be forced to either violate/bend the PVP contract and hope it doesn't trip anyone up, or give up keeping the old API branch maintained. Other cases I can think of where the "marketing" component of the major-version is useful is if a package name (because it was a name too good not to take) is taken over to be used for a totally different new package. But luckily, this is something that happens rather seldom...
Wouldn't ghcjs runtime be a too much of a burden for smartphone batteries? 
&gt; Sometimes you need a major-version bump as updating build-depends or making a version compatible with a newer GHC can force you into an not 100% back-ward compatible API change. And for that you need that extra dimension the PVP gives you for major versions. Otherwise you'd be forced to either violate/bend the PVP contract and hope it doesn't trip anyone up, or give up keeping the old API branch maintained. While I agree with this, I want to outline that besides such case being very unlikely, it is an outdated piece of software, which we're talking about. The maintenance is destined to eventually stop for it. I think there can't be a clearer point in time for it than when a broken GHC-compatibility appears.
Rather ironically, * http://hackage.haskell.org/package/parsec2 * http://hackage.haskell.org/package/parsec3 Why? I suspect, because of [this](http://www.reddit.com/r/haskell/comments/31e3jj/lets_update_our_versioning_policy_to_bind_closer/cq0zpf2).
Your argument for changing the PVP seems to mostly break down to 1. the aesthetics of "0."-prefixed versions having a *"beta" feeling*, 2. that the PVP doesn't specify how non-API changes (bugfixes/cleanups) ought to be handled, and finally 3. that the PVP is not the same as SemVer and thus may confuse those who are used to SemVer ---- - As to 1., this is a rather subjective feeling (which I don't share at all), and more of a cultural difference between various (sub)communities. Some even use year-numbers/dates for versioning (I wonder where the `HTTP`-package versioning fits into btw). As the scale of the version numbers is subject to political/competition/marketing reasons, I don't think it's the place of the PVP to mandate anything here, unless you want package authors to get harassed for not following the particular school-of-thought now encoded in the PVP. - As to 2., the PVP simply just defines the first 3 components, but doesn't forbid any additional components: &gt; A package version number should have the form A.B.C, and may optionally have any number of additional components, for example 2.1.0.4 (in this case, A=2, B=1, C=0). **This policy defines the meaning of the first three components A-C, the other components can be used in any way the package maintainer sees fit.** I think it's actually a good idea, to have the PVP specify only as little as needed, and leave enough freedoms to make creative use of the non-covered version components (c.f. `transformers-compat-0.4.0.{2,3,4}`) to exploit our powerful Cabal solver, and/or to give headroom for maintaining separate generations of an API (for example, afaik, /u/edwardk uses the first component in his package versions to denote his different artistic phases, but this still gives him "major-version headroom" to push out maintenance releases w/o being blocked from releasing major-version bumps if he wanted to) - And finally, 3. is a questionable argument IMHO (c.f. "Haskell is not Java, hence it is a potential confusion for newcomers who've previously programmed in Java"). Afaik, the PVP predates SemVer by a couple of years, but it happens to share the core ideas with it. The difference is mostly on a superficial and aesthetic level (one-component vs two-component major-version) and the `0.`-version signalling semantics. Maybe all we need is a "PVP for SemVer-users" TLDR-section (which would probably just comprise one or two sentences) on the PVP wiki page to bring SemVer-used newcomers up to speed with the PVP.
My mnemonic is to check their type in GHCi...
using TCache and RefSerialize for serialization would do the trick, I think. http://hackage.haskell.org/package/TCache http://hackage.haskell.org/package/RefSerialize
I wish people would stop saying that total languages aren't turing complete. It's just as inaccurate as saying that purely functional languages are free of effects. https://personal.cis.strath.ac.uk/conor.mcbride/pub/Totality.pdf
Unintentionally funny is the best kind of funny! :)
Oh my god that page is hilarious! As I'm reading it, I'm thinking about the whole "monad is just a monoid in the category of endofunctors."
`ghc-mod` isn't compatible yet with GHC 7.10 [according to this opened issue](https://github.com/kazu-yamamoto/ghc-mod/issues/437). And nice step-by-step. Thank you for sharing it!
I didn't need to set `CHCP 65001` for `λ` to work this time. But I have had trouble before. Right now my console displays font Lucida Console, so it seems to work fine.
I recommend using MSYS2. It gives you Pacman, a package manager from ArchLinux, and a large repo of native packages to use on windows. It is also actively maintained. 
Awesome, exactly what I have been waiting for! Keep up the good work!
It seems that a good first step is giving A and B proper definitions. Based on what people have said in this thread major.ghc-compatibility-updates.minor.patch seems to cover the important bases. This accomplishes two goals: it fixes the "marketing" problem of having a lot of packages at 0.X.X and also makes it easier to deal with GHC incompatibilities which seem to be more common these days.
I haven't thought to hard about it, but why wouldn't the combined bimonad work? class Biapplicative m =&gt; Bimonad m where return :: a -&gt; b -&gt; m a b bind :: (a -&gt; b -&gt; m c d) -&gt; (m a b -&gt; m c d) Maybe this demands too much since it may as well be `WrappedBimonad m a b ~ m (a, b)`?
This whole bifunctor business is likely to prove a bit of a dead end. Its two-ness is a bit hardwired, isn't it? Refunctor your bifactor! It's a functor between indexed structures of kind `(Bool -&gt; *) -&gt; (() -&gt; *)`, which can be treated as one example of `(i -&gt; *) -&gt; (j -&gt; *)`, a notion which accommodates lots of GADTs and is closed under lots of awesome structure. Our tools for working with these things are a bit kludgy *for now*, but there's room for improvement.
Your terminology is a little off. Updating an element in an immutable tree typically requires re-creating the O(log(n)) ancestor nodes for the changing element. The [git distributed revision control system](http://en.wikipedia.org/wiki/Git_%28software%29) is fundamentally based on immutable tree structures. Instead of relying on pointers and in-memory data structures, git relies on smart use of content hashes and efficient hash-indexed file layouts. When a single file 3 directories deep is modified, a new tree is created that has only 5 new nodes allocated (for the three new directories, the new file, and a new top-level "commit" node). Log-structured filesystems and immutable databases (like /u/richhickey 's [Datomic](http://www.datomic.com/) also operate on similar principles. 
I for one, am extremely happy that I learned Perl at one point. It is super productive for one liners.
So what's the problem with Frege? Is it too slow, or...?
hard to follow for the likes of me ;) but you're saying that since (\*,\*) is "isomorphic" to (Bool -&gt; \*) and (\*) is "isomorphic" to (() -&gt; \*), and we "uncurry" the kind of Bifunctor into ((\*,\*) -&gt; \*), and finally we substitute the above in, and get ((Bool -&gt; \*) -&gt; (() -&gt; \*)). but... that's at the kind level,not the type level, so it's not like those "indexed" monads I've read about right? AND I'm lost haha. edited: added backslashes to the Asterix, which were showing up as parentheses
I'm saying that the kind * -&gt; * -&gt; * is indeed isomorphic to the kind (Bool -&gt; *) -&gt; (() -&gt; *) by reasoning along the lines you suggest, which amounts to high-school algebra for exponentiation. a -&gt; b works a lot like the exponentiation b^a Currying/uncurrying is (a, b) -&gt; c = a -&gt; b -&gt; c -- c^(b*a) = (c^b)^a and we also have the corresponding rule for the unit of multiplication () -&gt; x = x -- x^1 = x That gives * -&gt; * -&gt; * = (*, *) -&gt; * -- a pair of * is * squared = (Bool -&gt; *) -&gt; * = (Bool -&gt; *) -&gt; (() -&gt; *) That's the kind of a type-level operation taking indexed types to indexed types. Such a thing can indeed be a functor and even a monad, but not in the category of types-and-functions. First find the right notion of arrow. type x :-&gt; y = forall v. x v -&gt; y v -- here x, y :: i -&gt; *, and v :: i And now take class FunctorIx (f :: (i -&gt; *) -&gt; (j -&gt; *)) where mapIx :: (x :-&gt; y) -&gt; (f x :-&gt; f y) Note that the index kind for "elements" doesn't have to be the same as for "structures". For our example, we're saying "there are two kinds of element inside one kind of structure", which is just what a `Bifunctor` gives you. However, if element and structure indices do have the same kind, then your `FunctorIx` is an endofunctor, which might be an instance of... class (FunctorIx m) =&gt; MonadIx (m :: (i -&gt; *) -&gt; (i -&gt; *)) where returnIx :: x :-&gt; m x joinIx :: m (m x) :-&gt; m x These are but one of things people have dug up which are some sort of indexed take on monads. Correspondingly, I'm not sure whether it's what you're referring to or not. You'll also find Atkey etc working with `* -&gt; i -&gt; i -&gt; *` and Orchard etc working with `* -&gt; m -&gt; *` for some monoid `m`: for both of these, the notion of "element" is still given by a type in `*`, but the idea of a monad has been replaced with something fancier. Here, the notion of "element" has been indexed by `i`. One way to get your head around that is to think of the kind `i` as describing *states* of the system we're talking to. A type of kind `(i -&gt; *)` represents a *condition* on states: condition `x` holds in state `i` exactly when you have a value in `x i`. The notion of monad over these things is exactly the standard categorical definition, but instantiated to `i -&gt; *` and `:-&gt;` instead of the `*` and `-&gt;` familiar in Haskell. A `MonadIx` is an operator which maps *post*-conditions to *pre*-conditions from which they can be achieved. Read `m x` as meaning "we can reach `x`". Looking more closely at returnIx :: x :-&gt; m x -- we can reach x from x joinIx :: m (m x) :-&gt; m x -- if we can reach a state from which we can reach x, then we can reach x That amounts to saying "there's a command which does nothing and there's a semicolon which joins commands in sequence". So it's not new or freaky. It's Good Old Fashioned Hoare Logic, dragged through the Curry-Howard Isomorphism.
I would add to this that we don't need a special `error` or `undefined` value to write a term that has every type, because: 1. Haskell allows for unrestricted reference cycles in definitions; 2. Type assignment will give many such expressions an unrestricted type variable. Simple example: loop x = loop x Let's assume that `x :: a`. Since `loop` is being used as a function applied to `x`, then its type must be `loop :: a -&gt; b` for some `b`. There are no ocurrences of `loop` and `x` that contradict this or further restrict it. Therefore, `loop :: a -&gt; b` is the most general type.
In a continuous integration system, by definition, all code compiles and all tests pass on the master branch. If someone want to do a transformation (say split Monoid into Monoid + Semigroup), then that is done on the whole system so the invariant is kept (all code compiles and all tests pass). If someone wants to change the blaze-builder version, then that is done on the system as a whole. Snoyberg is trying to do this in Stackage by kindly trying to get people to create official releases that keep the Stackage invariant (it compiles and tests pass given the packages in Stackage). If Snoyberg ignored the package maintainers in order to create faster iteration cycles, then this would not be a problem. The reason he is not doing that is because he is too polite, and probably FP Complete don't want to be the ones "forking" other people's packages. However, the inevitable path forward, in order to create faster iteration cycles, and deal with maintainers that don't care about keeping the Stackage invariant (thus the package maintainers don't want their packages to be composable with the ones in Stackage), is to keep patch sets for those packages, ignore the official releases, and provide alternative releases that keep the global invariant, and composes nicely. The issue linked to above is Snoyberg trying to get a official release out. In a continuous integration system, the integration tree, thus the metadata (and the patch sets) that are in the master git tree (stackage) is what is released as a global artifact, for every commit, so the official releases can be mostly ignored. I expect a platform for hosting "patch sets" to packages like snap-*, or patch-sets for moving to a Monad/Semigrup split (using HaRe) will be announced by someone, and not only as patch sets, but as forks of most of hackage. The move towards a single repo style of development is required for enterprises that don't have time for inefficient coding practices and need to compose large number of packages flawlessly, and need to be able to move quickly at the same time. 
The ghcjs runtime is a fraction (&lt;10%) of the size of javascript downloaded on G+ or any similarly javascript-heavy, engineering-smart site. The size of the runtime won't be a problem. Maybe the optimization level or something like that will be a problem, but not the runtime size.
Fractionally. :) The core dev team is still small and contains only one Haskeller (not me, I'm on content/community) so the priority has been to keep everything accessible to everyone in case emergency help is needed. Pretty sure we do have a bit of live Haskell on the back-end, though, and I know it's a long-term goal to expand that.
Very nice to hear from you. Please keep us up to date regarding your progress, especially your use of Haskell. Would love to hear about your experience on the tallship, that sounds like it was fun. In fact all of your travels would make very interesting discussions. Not many can say they learned Haskell in the Scottish Highlands!
But G+ and gmail do not use javascript html frontend on smartphones. They have native apps. Besides i did not say anything about the size of ghcjs. Just that it will add additional burden to cpu and thus burn more battery juice. 
I think there are essentially two reasonable ways to have `Biapplicative` and `Bimonad`, one which has an instances for `(,)` and one which has an instances for `Either`. Yours is the one that works for `(,)`with `instance Bimonad (,) where return = (,); bind = uncurry`.
It's basically like `&gt;&gt;=` for the `(-&gt;) Int` Monad or for `Reader Int`. You build up a compitation that requires an "enviromment"... you leave a "hole" waiting for an Int... and at the end, you give an Int to fill in. Every Int gives you a different result. In fact you can say that a Reader Int is like an array... every Int index, you get a new value. Every new value comes from applying the same function... but with a different source index. Here, it's the same. When you bind, you get a V3 x y z result. The first field is what you get from "only considering the first fields" the entire time. The second field is what you get when "only considering the second fields" the whole time. the third field is what you get when you "only consider the third field" the whole time. do x &lt;- V3 1 2 3 y &lt;- V3 x (x+1) (x-1) z &lt;- V3 x y (x + y) return (x + y + z) To get the first field of the result... only consider the first field the entire time. x is 1, y is 1 (x), and z is 1. so you get 3 at the end. For the second field, x is 2, y is 3, z is 3 (y). So the result is 8. For the third, x is 3, y is 2, and z is 5 (x + y). So the result is 10. V3 3 8 10 This is useful for when you want to basically chain a computation involving vectors and you want to represent the result of each component as the same parallel function applied to everything.... but with different sources of data each. It also conveniently gives you a diagonal with `join`. Also note that the Applicative instance derived from liftM2 and ap gives you "zippy" functionality like ZipList. 
(reddit was formatting (\*) as () without "\\". I didn't do too well with algebra proofs but I can do basic equational reasoning correctly!)
Oh, this helps! In the original code, there is no place where the resulting value of the type V3 is visibly constructed, and it confuses me.
Hi - backend engineer/resident haskeller, here. Our backend is currently CoffeeScript and SQL/plpgsql - CoffeeScript is used on the front-end, so it's understood by everyone on the team, and it lets us deploy to Heroku easily, but still has a functional-friendly syntax that makes EDSLs and monads relatively pain-free. That said, plpgsql has some severe limitations on abstraction, we'll likely be moving to Docker+AWS soon, and it doesn't make much sense to pass up on Haskell's features when server code is full of lifts, fmaps, and kleisli arrows anyway. There's a good chance we'll either migrate the server to Haskell, compile to SQL from Haskell (or to plv8 from Haskell/purescript/other), or both, in the very near future.
I think you make a lot of good points here. &gt; [...] he writes instances so that other people don't and wind up with orphans This is one of those practical concerns I do my best to avoid involving myself in :) But I just wanted to make an idle point here that I hope The Next Haskell finds a tidier solution to the typeclass architecture, regarding this and other issues. One idea I've had (and I'm sure others have as well) is that it would be interesting to explore language designs where the choice of instance is done on per-block, a per-module, or per-package basis. &gt; V3 (V3 a) is a vector space only and exactly when a imbues V3 a with field structure. I was thinking of when this invariant might hold. The idea that a vectorspace over a field is itself a field is exactly to say that the thing is a field extension. The oddity here is that being constrained to "`V3`", you limit yourself to field extensions of degree 3, such as adjoining the cube roots of 2 to the rationals. But heaven forbid you ask for the square roots instead! :P &gt; [...] there's often a tension between the categorical abstract algebra and the "practical" abstract algebra I feel the same way. Traditional algebra, like much of classical mathematics, appeals heavily to non-constructive tools. Without a computable fundamental theorem of algebra, we are kinda stuck with pretending floats form a field. I sometimes wonder if we should be looking for cute ways to parametrize over our "faking it". Things like the numeric tower (the `Num` class) or unverified monads in Haskell or Idris are social contracts rather than technical ones. But I wonder if it would be possible to quantitatively analyze how close these structures are to their classical ideals in some progressive fashion. I'm done speculating now :7)
Looks like it's been updated here as well: https://github.com/fpco/minghc/blob/master/README.md
Thanks for the tip. I will give that a try next time I need to go through this again.
You should always only use the MinGW included in GHC. Using a separate one causes problems (well, I am sorry but I don't remember exactly which). There's a ticket to [update the MinGW distributed with GHC](https://ghc.haskell.org/trac/ghc/ticket/9218) already and it'll go for 7.12.1. There are incompatibilities you can read about in the ticket.
Well, there's a reason why [Haskell LTS 2.0](https://www.fpcomplete.com/blog/2015/04/announcing-lts-2) continues with 7.8.4. Hopefully the packages will be updated by August in time for LTS to switch to 7.10.
Absolutely. There's a common idiom in the US: "the right tool for the job." Haskell, in this instance, is the absolute wrong tool. If your goal is an Android app, I strongly suggest picking a language that natively targets the JVM. Edit: Look into Scala/Clojure!
I am going to riff on the answer given by /u/mstksg a bit. Consider the data type data Axis = X | Y | Z Then `V3 a` is isomorphic to `Axis -&gt; a` from :: (Axis -&gt; a) -&gt; V3 a from f = V3 (f X) (f Y) (f Z) to :: V3 a -&gt; (Axis -&gt; a) to (V3 x y z) = access where access X = x access Y = y access Z = z Then this monad instance is isomorphic to the standard Reader Monad instance for `(Axis -&gt;)`: instance Monad (-&gt;) Axis where return x = \axis -&gt; x vector &gt;&gt;= f = \axis -&gt; f (vector axis) axis
It's totally fair to generalize this argument to types like `Vector :: Nat -&gt; * -&gt; *`. Any kind of field extension you like :) 
I think you may be confusing the three indented lines beneath the main bind definition as a single pattern, when they are in fact three separate patterns belonging to the definition's 'where' clause. Each one stands separate from the others and binds one of the three variables needed for the right hand side of the main definition.
I agree that analogies should not be at the front of each concept of the learning material, but rather mentioned in passing. Analogies are fine and can help, but it should not be assumed that the reader knows about other techniques (unless necessary). They can become tiresome even for an experienced programmer. It has been difficult to come by learning material for (functional) languages that do *not* assume the reader not to be already familiar with FP and not wanting everything explained as to a Java programmer.
Sharing is good. I often ask, but I cannot follow the answers.
Can you follow this, though? Is it readable? That tends to be a good litmus test for any new notation :)
For one thing, validEmail needs to run a validity check on all string inputs at runtime, whereas with a dependent type system you would be able to avoid some runtime checks by proving validity at compile time.
I've had a similar experience learning to program in functional languages. My programming language progression has been: high-school math, javascript, erlang, university math, and now haskell. I tried C++ last summer, even wrote a cool little sorting program, but it was very frustrating. Trivial programs in functional code were much more involved in imperative code due to the manual memory management. I agree with you regarding human languages and thinking in the target language. The best progress that I've made in learning Haskell was simply looking at manageable chunks of code and trying them in `ghci`. My ideal tutorial would have minimal commentary, lots of small examples with output, and a long discussion/summary at the end. Analogies can go into the discussion, AFTER the tutorial. 
A main point of [Scratch](https://en.wikipedia.org/wiki/Scratch_\(programming_language\)) is that it addresses this problem by giving children something fun to create that incorporates programming. I think robots can also be a good motivator for beginner programmers (although testing is usually bit slower).
I think the core point you miss here is that you're not prevented from writing an incorrect *validation function*. `validEmail str = Right str` fits the `String -&gt; Result Error Email` type, after all. Also, the only thing we know about the resulting string is that it's valid, but we don't know why it's valid or what it means to be valid. Usually, we include such information in documentation and comments, but we can do better with dependent types. As a simple example, I might want to validate binary string literals in Agda. First, I explain what "valid" means: BinaryLit : List Char → Set BinaryLit str = All (λ c → c ≡ '0' ⊎ c ≡ '1') str `All` is a standard library type. A value of `BinaryLit str` is a linked list that contains a proof for each character of `str` that the character is either '0' or '1'. If I have a value of `BinaryLit str`, then `str` must be a valid binary literal. Now, the actual validation might vary depending on the exact library and abstractions one likes to use. I personally like to use a typeclass for decidable proofs. Since equality for characters is decidable, and disjunction of decidable types is decidable, and `All Prop` is decidable when `Prop` is decidable, (and I have all these instances already defined), `BinaryLit str` is decidable too. -- type synonym Error : Set Error = String -- note : postfix "⁇" is a method of the Decidable type class. -- "A ⁇" returns either a value of A or a proof that there is no such value validate : (str : List Char) → BinaryLit str ⊎ Error validate str with (BinaryLit str) ⁇ ... | yes p = inj₁ p ... | no ¬p = inj₂ "invalid literal" Of course, without the typeclass magic (or with a more complicated notion of validity) one would have to write a validation function manually. But the key point here is that we're obligated to write a validation function which is correct with respect to our definition of correctness. Whether our definition of correctness is correct, well, that's another story... But at least to we're able to even talk about our ideas precisely, no matter how nonsensical they might be. 
This is like `[a,b,c] &gt;&gt;= f` i.e. apply `f` to each element and concatenate the results, but the `V3 a` type must contain exactly three `a`'s instead of *&gt;=0* `a`'s like in the `[a]` type, and instead of concatenating it picks the corresponding element back out for each of `a`, `b` and `c`.
&gt; how could you possibly validate or invalidate something which doesn't exist yet? That logic cannot possibly disappear at runtime. Supposing you write `validEmail "foo@bar" :: Maybe Email`, you now have to handle a case which will never happen, despite knowing as a programmer right now that this is correct. Dependent types can let you run your validation function at compile-time on the string `chrisdone@gmail.com`. Thus the logic disappeared at runtime. Simplifying with a type like: data NameChar = F | O data DomainChar = B | A | R | M U data NonEmpty a = Cons a [a] data Email = Email (NonEmpty NameChar) (NoneEmpty DomainChar) That's a grammar for a very limited set of emails. Then I can construct `Email (Cons F [O,O]) (Cons B [A,R])` for `"foo@bar"` and know that it is a valid construction based on the constraints set in `NameChar` and `DomainChar`. I can also provide a `validEmail :: String -&gt; Maybe Email` type for runtime construction. But I had the option to have a compile-time construct too. Where dependent types come in in this example is that I can't, in regular Haskell, go from a string literal `[Char]` to the `Email` type without introducing partiality, because there isn't enough type information in `Char` to use type functions, and I don't have the ability to run code at compile time (without template-haskell). A dependent type system would let me define something like `parseEmail :: (str :: [Char]) -&gt; toType (fromStr str)` where `toType` returns the type `()` if the string is invalid, and returns the type `Email` if valid, so that `parseEmail "foo@bar" :: Email` whereas `parseEmail "" :: ()`. Note how the type of `parseEmail` depends on the value of `str`. See [printf in Idris](https://youtu.be/fVBck2Zngjo?t=8m51s) for a more compelling example.
What trips me up is that I don't quite see the motivation for this. Why would I use this notation? In which situations does it produce clearer code? You should probably lead with that. Will be much interested to understand the definitions and such when I know why I do it!
[Not quite...](http://en.m.wikipedia.org/wiki/Biff_Tannen)
Non-mobile: [Not quite...](http://en.wikipedia.org/wiki/Biff_Tannen) ^That's ^why ^I'm ^here, ^I ^don't ^judge ^you. ^PM ^/u/xl0 ^if ^I'm ^causing ^any ^trouble. [^WUT?](https://github.com/xl0/LittleHelperRobot/wiki/What's-this-all-about%3F)
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**Biff Tannen**](https://en.wikipedia.org/wiki/Biff%20Tannen): [](#sfw) --- &gt; &gt;__Biff Howard Tannen__ is a [fictional character](https://en.wikipedia.org/wiki/Fictional_character) in the [*Back to the Future* trilogy](https://en.wikipedia.org/wiki/Back_to_the_Future_trilogy). [Thomas F. Wilson](https://en.wikipedia.org/wiki/Thomas_F._Wilson) plays Biff in all three films as well as the [Universal Studios](https://en.wikipedia.org/wiki/Universal_Studios) ride, and voiced the character in the [animated series](https://en.wikipedia.org/wiki/Back_to_the_Future:_The_Animated_Series). &gt;Biff is a tall, arrogant, violent bully who obtains what he wants by intimidating others into doing his work for him, or by cheating. He and his family members have a tendency to misuse [idioms](https://en.wikipedia.org/wiki/Idiom) in a way that makes them appear foolish and comical despite their intention to insult or intimidate. His favorite insult is "butthead". &gt;==== &gt;[**Image**](https://i.imgur.com/pReFSrb.jpg) [^(i)](https://en.wikipedia.org/wiki/File:BiffTannenBackToTheFuture1985.jpg) --- ^Interesting: [^Town ^bully](https://en.wikipedia.org/wiki/Town_bully) ^| [^Thomas ^F. ^Wilson](https://en.wikipedia.org/wiki/Thomas_F._Wilson) ^| [^List ^of ^Back ^to ^the ^Future ^characters](https://en.wikipedia.org/wiki/List_of_Back_to_the_Future_characters) ^| [^Super ^Back ^to ^the ^Future ^II](https://en.wikipedia.org/wiki/Super_Back_to_the_Future_II) ^Parent ^commenter ^can [^toggle ^NSFW](/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+cq2uopi) ^or[](#or) [^delete](/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+cq2uopi)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
1. Note that I dont require applicative. However: Applicative f =&gt; x &lt;-&gt; f = f &lt;*&gt; pure x 2. I used Lists mainly to simplify exposition. You could use any other container of functions, though the (!!!) lookup function would have to be generalized. I don't have any real-life examples yet; I came up with this while playing around with the language.
I don't have the answers to your questions. Personally, I find the pattern described above interesting. However, I haven't had the chance to use it in real-life code, yet. Maybe, someone might look at this and see a way to re-write their code in a clearer way. Maybe not. It's a coin flip. This isn't a functional pearl like e.g. the [Applicative Paper](http://www.soi.city.ac.uk/~ross/papers/Applicative.pdf) was.
`curry const` and `curry . const` are pretty complicated to grok. I finally get `curry const`, but still haven't quite figured out `curry . const`.
Just use lens? I kid, I kid. 
Nice, this makes sense to me: as I understand, your ValidEmail type is recursive, carrying a state variable with NameState being present until At, then DomainState being validated after that, and at the end, if it's all ok so far, return Email type. The thing that scares me is that this looks like it's... explosive in compiler computational resources, but I suppose that's another can of worms entirely.
Yeah, I think that solves it for me. Thanks.
&gt; I didn't understand why we need that library when we have ncurses Because GPL
I was always interested in programming as a kid, but I soon gave up because all the books I could find were about doing math and manipulating strings. Then one day I found Processing (the Java with macros for graphics things) and within half a year I was learning about design patterns and Eclipse because my code got cluttered and I wanted to make ever more fancier graphics. A bit later I was learning about efficient data structures because my computer couldn't keep up with the fanciness. 
What's the problem exactly with hscurses? What did you fix?
I'd like it if the errors knew about syntax, so for `case 2 of` it would expect (`-&gt;` or `|` followed by an expression), or for `x = = y`, expected an expression after `=`.
Yes, the token list is informational, though somewhat superficial. Each possible token could be leading the parser to a different production rule, but not all production rules are interesting, and some of them are too low-level for the user to even know about. Perhaps this can be remedied using additional annotations in Happy.
How I understand for different text colors in one window I must use wattr_on and wattr_off functions instead wattr_set, because wattr_set enables attributes for all window. We have `wAttrOn :: Window -&gt; Int -&gt; IO ()`. but attributes in hscurses hidden in Attr type. I can't cast Attr, so I need fix wAttrOn signature Also hscurses haven't function like `Pair -&gt; Attr`, so I can't enable only color attribute I am guided by this howto http://tldp.org/HOWTO/NCURSES-Programming-HOWTO/color.html
&gt; I blame the teacher for putting you in the wrong class (or perhaps for designing the class incorrectly). Definitely. GitHub may be awesome, but non-programmers probably don't know about it. If you are targeting them, either explain it first or don't make it a requirement. (Git and GitHub can be introduced without source code; recipes, poetry, or prose can also be placed in text files and source controlled.) A lot of non-programmers will have very little experience at the command-line. I don't think it's reasonable to completely remove the CLI from trying to teach someone programming. First, it's just too useful, and there's no many of our tools that have an inferior or non-existant GUI. Because of this, you need to be *very* explicit with any CLI tasks you require and it might be useful to have a short lesson on the shell. You don't have to go into bash programming at all, but breaking a command line down into executable, argument, option, option-argument parts; talking just a moment about shell special characters and quoting; and maybe even going into the semi-standard format of the summary part of a man page can go a long way toward making your student less apprehensive at the CLI. Both can be postponed if you want to jump right in. There's one or two Haskell REPLs available in the browser. FPComplete's School of Haskell allows experimentation in the code snippets and live execution. Several "kata" sites like [cyber-dojo](http://cyber-dojo.org/) support Haskell, an require nothing other than a web browser. But, eventually, the course and your students will outgrow those tools, and you'll need to provide them the CLI and Git skills you can't even remember operating without.
I learned the command-line before I ever learned programming. But, then again, my other option was either nothing at all or Windows 2.x. I didn't learn version control (back then, it wasn't Git, it was CVS and Subversion) until college, and you can definitely program without them, but they make sharing code with other programmers easier. If the teacher required Github for collaboration (or whatever), they should have covered some basic git tasks.
This is not correct. wAttrSet does exactly what you want. wAttrOn turns a given attribute on without touching all other attributes. wAttrSet sets a given attribute and colour and unsets all other attributes. (Note there is wAttrOff but no wAttrUnset). Colour is not an attribute, it is totally separate. Attributes are things like bold and dim. Pair represents a screen colour. I have no idea why wattrOn uses Int and wAttrSet uses Attr, but Attr is an instance of Num and so you can convert Int to Attr. I don't know why one would need to though. I have never used wAttrOn and friends, only wAttrSet. I don't know how to use wAttrSet, or why I would want to. There are no examples I could find of its use. You can peruse hscurses-fish-ex package for an easy example of colour manipulation with hscurses. It uses wAttrSet and works perfectly.
Sorry, I meant that the definition does not necessarily require `f` to be applicative -- any old Functor will do. By all means, restrict to `Applicative f` and use the applicative definition. It's a free country :) I'm a big fan of Applicatives, as can be seen from my mimicking of Applicative notation.
Remined me of an article that always stuck in my head and finally had to go hunt down, about why many people fail basic programming tests; it's not because they aren't intelligent, it's because they "refuse to deal with meaninglessness". "Formal logical proofs, and therefore programs – formal logical proofs that particular computations are possible, expressed in a formal system called a programming language – are utterly meaningless. To write a computer program you have to come to terms with this, to accept that whatever you might want the program to mean, the machine will blindly follow its meaningless rules and come to some meaningless conclusion. In the test the consistent group showed a pre-acceptance of this fact: they are capable of seeing mathematical calculation problems in terms of rules, and can follow those rules wheresoever they may lead. The inconsistent group, on the other hand, looks for meaning where it is not. The blank group knows that it is looking at meaninglessness, and refuses to deal with it" http://blog.codinghorror.com/separating-programming-sheep-from-non-programming-goats/ http://www.eis.mdx.ac.uk/research/PhDArea/saeed/paper1.pdf
Strange, my test code with wAttrSet print all text with one color, maybe I made a mistake. I'll look to hscurses-fish-ex later, thank you.
My mistake, it uses attrSet (no w), but the idea is the same.
I think error messages in general might become one of Haskell's biggest hurdle to adoption. The more generic the language/(standard)libraries get, the more errors become something that need serious Google'ing before I understand them.
&gt; that's just a systems-language thing For the record, not all of them: http://www.rust-lang.org ;)
I've heard that some of the ideal use cases of languages like Idris are in creating DSL's. Think of it as a meta-language that is particularly suited to creating other languages or protocols, since it can enforce all sorts of guarantees.
Will put it up this week. Just adding some docs and making sure the haddock stuff looks sane.
As a side note: I think the community possibly undervalues the importance of supporting both architectures actively. I, for one, build client-side apps that must run natively on both architectures on our end-users' machines. Until I have both, I have neither! ;)
Thanks for sharing an interesting and relevant read.
Personally, I'm also more in favor of the PVP. But being consistent with everyone else is *more* than just an appeal to popularity: it's a matter of *standardization*. Even if the more popular choice is worse, the advantages of working well with other tools and communities and having other programmers understand the system immediately can outweigh any technical downsides. It's like when Apple comes out with a new port that has some advantages over USB: on the one hand, it's technically superior; on the other, it makes it hard to interact with the rest of the world. It's rarely clear which option is better in any specific case.
This sounds great to me. That's how I've always thought about it. The real problem seems to be that, at least compared to Semver proper, the PVP is poorly marketed. People aren't clear on exactly how it works. I'm not sure how to deal with this, but part of it is just coming up with a simpler, more concise description. Comparing to Semver is a great approach. It would also be good to make the versioning conventions clearer in Hackage, especially when you're uploading a package. I'm not sure exactly how, but it feels like what we're doing now is sub-optimal and could be improved.
Yup, I know. I am absolutely certain they will get better and I regret not having the time to help. I didn't mean to single out Elm as hopeless, I just needed a quick example of what might happen if your starting point is "short and sweet".
For my case specifically, he did cover basic git tasks (he had to; he was teaching me remotely). But I still had to learn them. The fact that you have a teacher guiding you doesn't mean there is no work involved in learning, and if I hadn't had a teacher guiding me, there are few Haskell materials (I'd argue none, but the Wikibook is making a noble attempt) that engage someone with that little programming knowledge . I'm at a comfortable level with git now, I think. I no longer weep tears of blood every time there's a merge conflict.
This exactly. I actually think one of the main issues is the wall of text. You get a lot of information and it is kind of an overload. Some more tasteful use of white space etc. could possibly improve this situation.
Ermine would actually be a good option for a proficient Haskeller, here. It's Haskell like, but has a good FFI to the JVM. However, it's not yet ready for prime-time. There's two versions of the Ermine implementation: a legacy Scala compiler, and a shiny new Haskell compiler that's still only 3/4ths done. While the legacy Scala implementation is currently used in production, I'd recommend waiting until the Haskell version is done. 
I should have mentioned that myself. My bad!
You've got it right. This is partly why I started on the [book](http://haskellbook.com/) - nothing was written for people that hadn't programmed before. The author of this post is my co-author, I think they came on board after this was written. I've got [a post talking about the problems with different resources](http://bitemyapp.com/posts/2014-12-31-functional-education.html) and I do note that among all resources, Thompson's book is the one best suited to those that haven't programmed before.
&gt; I no longer weep tears of blood Next time, see a doctor instead of a programmer. ;)
Well, the whole transaction has to commit, so if something is read from the channel, it should be in the TVar, or I am missing something.
Yeah, there are some super handy messages, especially ones that suggest "did you mean this?" or "try enabling this" are very handy indeed. But it's worth recognizing that there are useless errors that GHC outputs too. It's hard to recall which ones, but once in a while I look at an error, knowing what the real problem is and think "well that is completely misleading". Other times, especially in the presence of more exotic extensions like type families, a compile error triggers n other inference errors related to type-classes that are completely unrelated to the *real problem* that's 20 error messages down below. I think better messages/UX is probably the more productive approach that teaching people divination from unhelpful messages. Lennart Augustsson did some work on the custom Haskell compiler they use at work with a small tweak (but big improvement) on error provenance. See here: https://www.youtube.com/watch?v=rdVqQUOvxSU I expect there're a number of low-hanging fruit or re-thinking of messages to be had. Consider the benefits gained by typed holes, which list the types of related bindings. That's a thing GHC has never done. Why? Because nobody with the chops thought to do it. But it's there in typed holes now. 
On the other hand GHC could really be improved in some major ways as far as e.g. type synonym use in error messages goes. I would expect even just using String instead of [Char] in a lot of them would help the newcomer immensely. Similar things could be said about more generic kind of code and errors where the fact that this is generic is irrelevant (I am looking at you Num and OverloadedStrings). Perhaps GHC could make more of an effort to distinguish errors related to argument order and number from those that are about the actual argument type in these cases (better handling of the 'I am telling you you called bla with too many arguments but really you just forgot the do keyword' situation would help a lot here too).
The types in `linear` all instantiate such a class, drawn from my `adjunctions` package: http://hackage.haskell.org/package/adjunctions-4.2/docs/Data-Functor-Rep.html In fact, the original design of linear was based on the notion that we can look at `(forall a. Lens' (f a) a)` as a canonical choice of `Log f` for any representable functor `f`. We since relented a bit, when it became clear that working with such a choice of representation for `V n a` was actively getting in folks way, and switched to more liberal schemes.
If the OP doesn't understand it, saying "GPL" probably won't help. The GPL imposes restrictions on derivative works--including, in this case, programs using GPL libraries--that are unpalatable to many commercial developers.
In my opinion, learning how to become a better philosopher is helping me to bend the light to be able to more adequately understand Haskell functional programming languages. A big thing of mine, which still is, since the curve to implementation is confusing at times. Is being able to learn about isomorphism but now being confused to develop logic &gt;&gt; in that sense I am starting off with a paired-programmer logic to get insight into how a skilled developer composes his declarations. Hence why I might read more than actually code at first. Anyway briefly put, I might go on codewars to see if I can learn more during work ...
An interesting experiment would be to expose the compilation process via an interactive browser. When an error is encountered, you could browse the runtime state of the compile process (suitably simplified for human consumption), such as the inferred typing rules in a particular context or the state of the parser where the syntax error occurred. Part of the error message problem is trying to determine at the error point just which information that is available there will be actually useful to help the user discern the error. If you give too much information, quite a lot of it turns out to be irrelevant and it makes it hard to find the real problem. If you give too little, then it's just not helpful at all and you can't even dig for it. But if you could *interact* with the compiler at the error point, the user could perhaps direct the compiler to give just the useful information, or at least all the information possible could be accessible without up-front information overload.
Wow, thanks so much for doing this. I was going to do this myself (not so much for ghc, but for some of my projects where I use happy), and now I don't have to! Did you submit a patch to happy's upstream?
I've actually thought about this a lot. Appropriately enough, I wrote out my thoughts [on Quora](https://www.quora.com/Why-do-some-people-choose-to-use-Quora-over-writing-a-blog/answer/Tikhon-Jelvis). The main reason is that it's fun and easy. I get instant feedback and a what is essentially a source of writing prompts on any subject I care about. It's something I do to unwind. I actually write on a bunch of those other sites too, but they have qualities which make them worse than Quora for some things. And since I retain copyright on all my work there, everything possible I wrote is available on a CC-BY-SA license, so it's still accessible for various uses. One particular advantage, a pretty important one, is that Quora has a distinct audience from everything else I use. It actually reaches people who don't care much about functional programming off-hand but are in a good position to learn. I know a bunch of younger Berkeley students, for example, who won't see anything on /r/haskell or SO but do read my stuff on Quora. This has a ripple effect because now they're more likely to talk about it with other students, have workshops in their clubs (which I've actually seen) and so on. I write on [StackOverflow](http://stackoverflow.com/users/286871/tikhon-jelvis) too, but it's actually hard to find a good questions to answer! Ditto for related StackExchange sites like programmers and Emacs. Reddit and Hacker News are good too, and I use both, but they're too transient in practice. (HN more than here, but it's true for both.) For example, a year ago I wrote up how the reverse state monad works in an HN comment and while *technically* it's more accessible, nobody's going to read it any more. I recently moved the same content over to Quora, and there, at least, it will still be read next year. Comments and discussions here are great, but they don't have much staying power. I have a [blog](jelv.is/blog), and I'm even writing something for it right now, but it's an entirely different experience. It takes way more time and mental effort to write anything, and I try to maintain a higher standard for blog articles, so I publish way less. That's more a matter of concerted effort over a few weeks than something I do in my down time, and it turns out I'm pretty lazy. Originally I was apprehensive about joining Quora for those same reasons, but the practical advantages ended up outweighing my concerns. Something with similar experience and audience to Quora would be great, but I'm not sure it really exists at the moment.
Going further, given a representable functor we can derive the `point` method for `Pointed` (and the `pure` method for `Applicative`) class Pointed p where point :: a -&gt; p a default point :: Representable p =&gt; a -&gt; p a point = tabulate . const instance Pointed V3 which gives ghci&gt; point 'a' :: V3 _ V3 'a' 'a' 'a' Not sure how useful this is but hey :) **Edit:** With GHC 7.10 you can also use `DeriveAnyClass`, very cool: {-# LANGUAGE DeriveAnyClass #-} data V3 a = V3 !a !a !a deriving (Functor, Pointed)
Well, then you must also say that Haskell is free of effects (aside from nontermination and a couple of other invisible effects)
The real type error is below a lot of junk all the time. :(
The lazy pattern match makes your fmap a lot less efficient. 
&gt; `fmap id _|_` = `_|_` with the lazy pattern match because of the strict fields. EDIT: heres why: the strict fields mean that V3 (f a) (f b) (f c) = (f a) `seq` (f b) `seq` (f c) `seq` (V3 (f a) (f b) (f c)) while fmap f ~(V3 a b c) = V3 (f a) (f b) (f c) is syntactic sugar for fmap f v = let (V3 a b c) = v in V3 (f a) (f b) (f c) so we get fmap id _|_ = let (V3 a b c) = _|_ in (id a) `seq` (id b) `seq` (id c) `seq` (V3 (id a) (id b) (id c)) = let (V3 a b c) = _|_ in a `seq` b `seq` c `seq` (V3 a b c) = let (V3 a b c) = _|_ in _|_ `seq` b `seq` c `seq` (V3 a b c) = let (V3 a b c) = _|_ in _|_ = _|_ 
You need a backslash to escape the underscores: \_|\_ (⊥) **Edit:** Fixed it before I could suggest it :)
I thought ncurses wasn't actually GPL, but is under the GNU banner with an agreement to keep an MIT style license?
I have no idea. My reference was exactly based on the fact that I tried to use [haskell ncurses package, and that is GPL-3](http://hackage.haskell.org/package/ncurses). I have instinctively assumed that the underlying library is GPLed and the maintainer reused the same license. Apparently it uses some [MIT-X11 "derived" license](http://invisible-island.net/ncurses/ncurses-license.html#issues_freer)
I just came across [this](http://dl.acm.org/citation.cfm?doid=1159876.1159887), which does something similar for type errors.
Given a representable functor you can derive a lot more than `point`. You can derive `return`, `(&gt;&gt;=)`, `mfix`, `ask`, `local`, `distribute`... if the representation/logarithm is a monoid then you can also derive a comonad instance. After all a representable functor is isomorphic to (-&gt;) (Log f) -- so all the instances we have there are candidates to transfer.
&gt; "smart constructors" The world would be a much better place -- and I'm talking the "there would be no more wars" kind of better -- if we stopped didn't label things as "smart". Smart phones, smart TVs, smart cars. Let's fish for a more vibrant word next time.
Be my guest.
Isn't that law only valid in propositional logic, not constructivist logic?
Ah I see.
This happens a lot, but it's all in the first error in the list. It's just that the individual error could be worded better.
I'd be interested in an example if you have one.
For those further interested, the Logic chapter of Software Foundations (http://www.cis.upenn.edu/~bcpierce/sf/current/index.html) asks for proof of the same double implication, in addition to three more.
I know right? It is so [hot'n cold](https://www.youtube.com/watch?v=kTHNpusq654).
Thanks for the detailed answer. &gt; It takes way more time and mental effort to write anything, and I try to maintain a higher standard for blog articles, so I publish way less. Have you thought of getting a less-prepared companion to your blog? Edward Yang has both [a blog](http://blog.ezyang.com/) with a relatively low frequency, and [a log](http://ezyang.tumblr.com/) with less-refined posts that is more frequently updated. In particular, making actual use of the CC-BY-SA by quoting/sharing/saving your idea-liberating Quora/Reddit/HN posts on a log would make a lot of sense, would increase your readership (I would certainly subscribe to "All things Tikhon") and increase its lifetime (even Quora may vanish someday). The requirement is to get the publication effort low enough that you both in practice (are there ways someone else could help with that?). 
I highly doubt anyone coming from other languages will associate the + operator when used on strings with maths. It is more likely they will associate it with their favorite language's string concatenation. 
I was informed that in Julia language, `*` is for string concatenation, and `^` for repeat strings multiple times. They called it a `design`, because they think the strings form a `group`. Yes, a `group` without inverse element.
Just found this site http://packdeps.haskellers.com/reverse , it will be helpfull for libraries clients search. :)
Am I doing it wrong? :P lem_callCC :: LEM -&gt; Peirce lem_callCC lem = undefined how about now? :P lem_callCC :: LEM -&gt; Peirce lem_callCC lem = lem_callCC lem 
Relatedly, in Python `+` on strings mean concat, while `*` with a string and a number means string-replication...
Passing next-possible-tokens from happy to GHC is a *great* idea! Please make sure GHC HQ hears about this - could you post a link to the GHC devs list please? SPJ is constantly begging people for constructive well-specified ideas for improving the readability of GHC error messages without reducing their information content or making them less robust. So I think he (and others at GHC HQ) will be thrilled about this. Obviously there is a lot more to be done in this direction, as you point out, but it's a great start. I would vote for already including this in GHC, even with the rough spots. A few quick fixes that could help with a few of the points you raise: * Replace things like "CONID QCONID PREFIXQCONSYM" with something more human-readable, like "a constructor". * If there are too many possibilities, just say something like "just about anything except '='".
Here's a solution in Agda I wrote some time ago; don't read if you want to solve it yourself. :P https://gist.github.com/vituscze/7fd9ae3dedbc81b7fa6b
Since most often errors should be resolved in a top to bottom order, I would like it if the error messages where printed in reverse order. Then we would not have to scroll to find the first error.
Yeah, pretty much. I'm comfortable with saying that Haskell is free of *side* effects, since that's part of purity. Not sure what it means for a language to be "free of effects" altogether...
*sheds tear*
I must say that this API would really confuse me. I would expect things with `Num` to be, well, numeric :) I'm assuming things like `(-)` and `negate` are `undefined`/`error`? That would be another reason for me not to like this: I can never be sure I can safely pass your type to a function expecting `Num`, since it's not really numeric, it just likes to have the operators.
That's my point. Haskell is just as much free of effects _altogether_ as Agda is turing-incomplete.
Haskell does not have an official semiring class and `Num` is the closest thing to it. Even if we did create an alternative `Semiring` class and standardized on that it still wouldn't have support for numeric literals like `Num` does.
The real instance is actually not string-specific: instance Monoid a =&gt; Num (Shell a) The `Num` instance for `Shell Text` uses whatever the `Monoid` instance for `a` is when multiplying. This is in fact the only semiring instance for `Shell` that satisfies the semiring laws. For example, this instance actually is not a valid semiring: instance Num a =&gt; Num (Shell a) where fromInteger n = pure (fromInteger n) (+) = liftA2 (+) (*) = liftA2 (*) ... because it doesn't satisfy the distributivity law.
I wrote some simple shader generation for my graphics framework. Depending on your needs, it might be easier than you think. https://github.com/bananu7/Hate/blob/master/src/Hate/Graphics/Shader.hs
Awesome, congrats!
Great job. Out of curiosity, what is this Haskell library mostly used for? That is, what are you doing that is better implemented as a Haskell program rather than using emacs's `org-mode` for querying and manipulating org files?
Take a look at [LambdaCube3d](https://lambdacube3d.wordpress.com/).
This talk gives one of the best descriptions I've ever seen of the motivations for why FRP (Functional Reactive Programming) is useful. All the other explanations I've seen are too much in the weeds, but this is clear, concise, and understandable. This part 1 video goes through practical examples and gives a nice intro for what it's like to use the library. He even walks through integrating a live tweet stream right into the presentation. It's pretty fun to see the audience tweets appear on-screen in real time.
Can't answer for the op but I wrote a similar library in a small webservice used to display/comment org files.
I'm not sure that gets you out of jail completely. Say you want to write a program that does the following: 1) Ask the user to type a definition of an Agda function f :: Unit -&gt; Int. 2) If f doesn't typecheck, exit. 3) Evaluate and print f(). 4) While evaluating, don't do any IO actions. It seems to me that you can't write that program in Agda, and that's one reason why Agda is said to be Turing incomplete. All Turing complete languages can do that.
It may be a bit socially unconventional but if the operators are well behaved as in some algebraic object (groupoid, monoid, etc.), it's just a matter of taste and background. But OP's greater point is the well-behaved aspect of mathematical operators.
I understand. The way I see it, there are two really compelling benefits of Haskell: * Safety, due to the strong and static type system * Equational reasoning, thanks to the fact that Haskell is purely functional However, the `Num` class is an unfortunate case of where we have to pick between (syntactically pleasing) equational reasoning and type safety. Generally, when I pick between the two I err on the side of equational reasoning mainly because equational reasoning is pretty unique to Haskell, whereas strong and static type systems are not.
Good point about the slides; I'll add that. Behaviors and Events (the two fundamental signals in Reflex) are not monadic or arrowized; most of the primitive functions for manipulating them are pure functions. There are two primitives, sample and hold, that are monadic, because they need to know the "current time" (in FRP system; not the system time) at which they are executed. I go into this stuff more in Part 2; see, e.g., https://youtu.be/3qfc9XFVo2c?t=486
I wanted to have org scheduling data in Haskell types for playing with Haskell-to-js rendering of project timelines. I'm not a long-time orgmode user, but it looks very cool and working on the parser was a good way to dig in. Now to learn me some keyboard shortcuts... If anyone else is making figures from org data I'd love to hear how you do it. Thanks for the mention, /u/pspringmeyer
Brilliant, thanks for working it out.
So by cheating with fix all I did was creating a bottom value. I thought so.
I highly recommend Dan Piponi's [Adventures in Classic Land](https://wiki.haskell.org/wikiupload/1/14/TMR-Issue6.pdf)
I have some JSON data in an Org file that I link into a Haskell source block with noweb. Then I go via aeson, lens, and Chart to a figure. It's kind of hairy. I need to revisit the setup to see if it can be made easier to setup for others.
I thought this might be of interest to some people here. The README goes over some of the design decisions we made while implementing this. The pure signal function approach is probably the one way in which halogen differs from other similar libraries. 
My library [FWGL](https://github.com/ziocroc/FWGL) includes an EDSL based on advanced type system extensions like DataKinds, but it's rather incomplete. You can modify and use it for some very basic (but safe) shaders. https://github.com/ziocroc/FWGL/tree/master/fwgl/FWGL/Shader https://hackage.haskell.org/package/fwgl-0.1.1.0/docs/FWGL-Shader.html
It may be useful in "yi" (editor in Haskell) or in "PanDoc".
[My solutions, in haskell and newbie coq](https://gist.github.com/agrif/2a870f3655b71615a9b7). Spoilers, of course! I opted to also prove LEM &lt;=&gt; DNE to complete the triangle, and `dne_lem` ended up being my favorite.
very interesting. The resulting code seems far more clean than other FRPs
&gt; Yes, a group without inverse element. That's bad. That's really bad. That's almost as bad as what Haskellers do to category theory T__T
is a "source block" an emacs thing? sounds interesting.
This reminds me a little bit of "immediate mode guis" from back in my C/C++ days. E.g. https://github.com/ocornut/imgui
Can someone explain why Void is defined as "newtype Void = Void Void" rather than just "data Void"? It seems like the latter should allow a simpler implementation of "absurd", by using an empty pattern match. I don't understand Edward's [current implementation](https://hackage.haskell.org/package/void-0.6/docs/src/Data-Void.html#absurd) at all, why does it have to use seq, etc.
yeah. I tend to be on the "moar sugar" side of things, but in this case, wouldn't: &gt; |+| and &gt; |*| be both safe and readable and arithmetic-intuition-invoking? maybe I've just gotten used to mentally squinting when reading "bracketing" operators, and those look really weird to someone who hasn't read any haskell.
&gt; This is a plus for modularity (but not for composability Could you elaborate on your understanding of the difference between these two concepts? Are they not two sides of the same coin?
Hey, thanks for checking it out! Widgets in reflex-dom can be composed just like you mentioned - just use (&gt;&gt;), from Control.Monad! This will lay them out sequentially in the DOM. You can also use (&gt;&gt;=), of course, which will let you make use of the results of other widgets, and you can use RecursiveDo to pass results upwards in the DOM. For example, to create a textarea widget, you can write `textArea def`. This creates the widget, and also returns its value and associated events. So, you can create two textAreas like this: `textArea def &gt;&gt; textArea def`. Of course, usually, you'll want to make use of the value of a text area. So, you can write: `textArea def &gt;&gt;= dynText . value`. This will create the text area, then compose it with a dynamically updating text area showing its value. All of these individual components are widgets (`MonadWidget t m =&gt; m a`), just like reflex-dom users themselves write. You can check out this particular example, and ones that build on it, at https://obsidian.systems/reflex-nyhug/#/step-13 For my take on TodoMVC, check out https://github.com/ryantrinkle/reflex-todomvc/blob/master/src/Main.hs I hope that makes things a bit clearer! Let me know if you have any other questions.
It's an org-mode thing, so, yes, it's an emacs things. It means that I have a file that has a lot of stuff in it. One thing it has is a part that goes like this: #+NAME: mydata #+BEGIN_SRC js someJSON #+END_SRC Then, elsewhere in that file, I have a part that goes like this, #+BEGIN_SRC haskell :noweb yes json :: String json =[str| &lt;&lt;mydata&gt;&gt; |] main :: IO () main = doStuffWithJSON json #+END_SRC I can then load that Haskell block into GHCi with a helper (that unfortunately is not part of haskell-mode due to a lack of consensus as to how to do it), and magic happens. It's a neat way to have a notebook-style file with all kinds of related data, code, and artifacts (e.g. images) in it.
modular is to enable everithing in a single place. A single piece of source code for example. Then you can plug different modules by calling one to the methods of the other. composability is to use a single plug to mix two elements and produce an element of the same type and so on. Web applications that use "separation of concern", that is separate CSS HTML and Javascript are not modular. objects in OOP are modular, since one object is self contained. It could call methods of other objects. But this is not composability. I have no single plug to combine two objects to create a third object. I have no combinator to mix two reflex widgets and produce a third that sum the behaviours and renderings of the two so that a third get the result of the two without regard of what it is inside (but I may be assuming too much in this case and be very wrong). To do that, the third widget has to know the identifiers used in the other two in order to do something with them, so you need to plug things by means of identifiers. That is not composable. They can be made adjacent, but to create a more complex behaviour than mere adjacency you must do some seaming with identifiers. In hplayground I can compose like this: do r &lt;- w1 &lt;|&gt; w2 w3 r where w1 and w2 may be from simple input boxes to complex dynamic widgets with nested combinators inside. w3 don´t care about that. It simply expect a value, and this value is all that he need. w3 don´t need to know any internal identifier in w1 or w2. it does not need to know if the result comes from either one of the other. So a widget in hplayground composes with any other (as long as they typecheck) and may make use of others widgets behaviours without loosing this composability.
I wonder what are those 6 packages which use NullaryTypeClasses?
I see. Another possibly dumb question: if Void were defined as a datatype with no constructors, and absurd were defined as an empty pattern match, would it force its argument or not?
Are this numbers from LANGUAGE pragmas only or do they contain extensions that are only defined in cabal files? 
 fwgl-0.1.0.3/FWGL/Backend/GLES.hs fwgl-0.1.0.3/FWGL/Backend/IO.hs fwgl-glfw-0.1.0.3/FWGL/Backend/GLFW/GL20.hs fwgl-glfw-0.1.0.3/FWGL/Backend/OpenGL/GL20.hs fwgl-javascript-0.1.0.2/FWGL/Backend/JavaScript.hs partial-0.1.0.0/src/Control/Partial.hs 
Sure! In reflex-dom, a "widget" is anything with a type signature like this: `myWidget :: MonadWidget t m =&gt; m a` Of course, widgets can take arguments, just like any other monadic function in Haskell. The return type `a` can be whatever the author wants; if it's based on any Events or Behaviors from the DOM, those will make use of the phantom type parameter `t`, which identifies the FRP timeline. I've got a much more in-depth example in the README at https://github.com/ryantrinkle/try-reflex
Moreover, since it uses identifiers of textArea, and get the value of a textArea, a and b are not arbitrary identifiers of any kind of widget, but a precise type of DOM element. That means that this code is not arbitrarily composable. I mean, i can not change textArea for a more complex widget,since the third line expect precisely the identifier of a textarea or at least some identifier that admit a value method.
I see. the monad return identifiers, not values. That is great and make things a lot more intuitive is some aspects, specially for people accustomed to the DOM management. It is not so alien, but this make it less composable and it makes the use of alternative and applicative operators less obvious, and does not allow for some dynamic behaviours. 
But please correct me If I´m wrong, but when you return (value a) you only get the first value as you said, it does not give successive values. to obtain that succession of value events you need some reactive call using the textarea (or whatever widget) identifier?
Wow, that's really cool. Again, (regarding the other posts (because I'm dumb)) I'm having trouble construction the shaders. I see in your hackage page you've shown an example, but I'm having trouble understand it. Do you perhaps have a simpler one depicting phong/gourad shading with a texture and colour? That's as complex as I'd need, since I just want to include some basic ones, and maybe a few others such as shadow mapping. Thanks, I appreciate it! 
correct, That is more or less what I wanted to say. it hasn't to be a DOM element, but the third widget has to know how to get the value of each different widget. If the widget changes form one to other different, the receiving widget has to change his code too. There may be some homogeneity in the interfaces that may avoid this problem and make widgets that depend on other widgets more independent in a OOP-like style. sorry for mentioning OOP.
The video is quite comprehensible, it may be the first FRP demo I've seen that isn't mired in nomenclature I don't understand, builds up from small components but doesn't bore me with unrealistic counters or sliders, and has some real world products written in it. 10/10 talk, count me interested.
Why is `mapDyn` monadic instead of pure? Does `mapDyn` add elements to the generated DOM?
Both [John Carmack](http://functionaltalks.org/2013/08/26/john-carmack-thoughts-on-haskell/) and [Eric S. Raymond](http://esr.ibiblio.org/?p=1796) recommend it as a way to [make you a better programmer](http://dubhrosa.blogspot.co.uk/2012/12/lessons-learning-haskell.html). I'm not sure why you think there needs to be *one particular task* that Haskell is *irreplaceable* at. If, instead, it is just *a little bit better* at *most* things without being *too much worse* at a *few* things, it's could still be very profitable to use. I wouldn't recommend it for embedded work, or across a heterogeneous cluster, but for about everything in between Haskell might be appropriate.
Good question! Eventually, `mapDyn` will be replaced with `fmap`. It's not doing anything that affects the DOM. However, in order to be efficient, `mapDyn` needs to use `hold`, which is monadic. In order to write `Functor (Dynamic t)`, I'll need to introduce a new primitive that provides a weaker form of `hold` as a pure function. This is one of my top priorities for the near future.
despite its many issues, i still find haskell a nice artistic diversion from the pathologically practical tools people use otherwise. haskell has a style. there's some "form" you can appreciate beyond the "function" meeting the requirements. haskell is not boring. haskell lets interesting people have interesting discussions about programming. 
This looks really cool! But the desire to use it is forcing me to contend with something that's been bothering me. I've always been suspicious of generating html in code because it's hard to separate design and functionality. After making something work, I usually go back and try to make it look nice. This often means adding extra spans and divs, css classes and ids, etc. It seems very ugly/painful to add these directly in the code as opposed to an html template. For instance, it seems a shame that information about fontawesome is cluttering up the Twitter client example in the talk. Yet this seems to be a common approach these days. I'd be interested to hear what people think of the tradeoffs. 
what are its many issues? 
What do Haskellers do to category theory?
&gt; no way to use "bold" typeface to this day I just checked – `__bold__` works for me. Or did you mean something else?
&gt; even better – `cabal build` I don't think it'd be a good idea to extend the compilation times with documentation generation. There should be a dedicated target for just that: the compilation. `cabal build` is that target. You have `cabal install` for doing everything at once.
I don't care if it's actually markdown. That was just an illustrative example. The point is markdown is popular because it strives to feel natural to people whereas haddock is not if people keep messing up the basics as the article points out. 
Oh, right. I meant `cabal install`, yes.
&gt; That was just an illustrative example Okay, but I still wanted to share the link, for other readers if not for you. (I guess a lot of people want specifically Markdown, after all.)
If anyone has the power/knowledge/time to do this, let us use [reStructuredText](http://docutils.sourceforge.net/docs/user/rst/quickstart.html) &amp; [Sphinx](http://sphinx-doc.org) please! (I also love [readTheDocs](http://rtfd.org)) Maybe I'll put `write a Haskell domain for Sphinx` on my `things to do in case I'm actually immortal` list.... 
I have my own markup which is similar to haddock, except that both modules and symbols use single quotes (so 'Module' links to the module, and 'Module.function' to the function inside). Also, if the thing inside is not actually the name of a module or a function, it emits it literally with quotes and no linking. The result is I never have problems with quotes. I've been meaning for a long time to see if I could add the "don't make a link unless it's valid" feature to haddock, especially for double quotes. Personally I'd also like to remove the special treatment of /s, but that's just me.
Could you please also make a breakdown per package instead of per file? I.e. count all uses of an extension within one package as one.
Nice! I came up with something pretty similar, but played with it to try to make the results somewhat self-explanatory. https://gist.github.com/cdsmith/9af82fde6549b69537ec
Why does this webpage hijack my ctrl-k keyboard shortcut? Or any keyboard shortcuts, for that matter?
Honestly, we should just fix some of these. Who needs italics? @ should be verbatim; and teach Haddock to understand how to parse things.
The main thing to consider when comparing these two options is that ghcjs gives you semantics you can reason about that very closely match GHC's. It'll probably lose by a fair bit on small benchmarks and code size, because it is trying to achieve a much harder goal, but the moment you actually need to use a finalizer and have it work properly, it starts to pay off.
&gt; Haskell has features that I could have leveraged to encode the length information in the types themselves, This would be dependent types which are not directly provided by Haskell. Or am I missing something?
Good point. Thanks.
By parse, do you mean automatically hyperlinking identifiers in code blocks? Is that a commonly used feature? It's the only reason I can think for code blocks not being verbatim.
Sorry if this is addressed in the videos, I'm not done with them yet... One difference with immediate mode guis is that layout is coupled to ordering and nesting of elements in HTML. (immediate mode guis I've played with have been absolute positioned) * How do you deal with a dependency on an element in a forward position in the DOM? E.g. &lt;span&gt; display the value in the text area &lt;/span&gt; ... &lt;textarea&gt; * ...or alternatively, later on in a nested position. E.g. &lt;div&gt; &lt;div&gt; &lt;span&gt; display the value in the textarea &lt;/span&gt; &lt;input&gt; &lt;/div&gt; &lt;div&gt; &lt;span&gt; display the value in the input &lt;/span&gt; &lt;textarea&gt; &lt;/div&gt; &lt;/div&gt;
You can use [bird tracks for verbatim](https://www.haskell.org/haddock/doc/html/ch03s08.html#idm140354810780208).
Since they're not verbatim, you can also do things like hyperlink the type names inside of a code block, which is used to great effect in modules such as [Pipes.Tutorial](https://hackage.haskell.org/package/pipes-4.1.5/docs/Pipes-Tutorial.html).
Can we please stop reposting this?
I use haskell professionally for web development. My setup: * Ubuntu * GHC (I don't bother with haskell platform) * Cabal sandboxes * Yesod (with scaffolding) + persistent * Postgresql (There are many reasons to prefer postgres to mysql. One that is particular to this setup is that persistent-mysql is not as well maintained as persistent-postgresql, causing things like the auto-migrations to break sometimes. Theoretically, persistent is supposed to work with many backends, but people who work on it seem to care the most about postgres.) * nginx for reverse proxy and possibly SSL termination.
I'm confused. Is the parent post sarcastic? /s
Go with Debian if you are fine with older software that doesn't change frequently. (Although, a new version is coming out quite soon.) Go with Ubuntu if you want newer software; you will be required to upgrade most of the system every 6 months. (apt-get install apache2-mpm-prefork) as root on either should give you a working webserver, though with minimal content. The haskell-platform seems to be regularly broken on Ubuntu, but cabal and ghc are generally simple to install on either.
The project that we recently ported to use reflex is forcing us to grapple with this issue. Before, all our markup was in templates. That was a big advantage for us because it allowed our design team to be almost completely decoupled from the Haskell development team (for the same reasons you point out). This is also why I'm the author of [heist](http://hackage.haskell.org/package/heist) and why we use it instead of things like blaze/lucid/shakespeare/etc. I still think that as soon as a project gets large enough to have dedicated developers and designers this runtime template mode of doing things is more efficient. But for front-end apps with a substantial interactive component I think reflex tips the scale for me. With reflex, in 9 days with a team of three people, we were able to completely rewrite a 5800 line web GUI in 3000 lines. As far as maintenance goes, the jury is still out but it seems like the reflex version is easier to work with and much less buggy than the haste version. So I think that for rich interactive GUIs it's definitely worth the tradeoff.
Seems like it hijacks "k" and "j" (based on hjkl navigation) and doesn't check for modifier keys.
Very cool. If the author is reading this, I would suggest taking a look at what Chris Smith did with [Haskell for Kids](https://cdsmith.wordpress.com/2011/08/16/haskell-for-kids-week-1/). It's a great way to approach the math in a really tangible way. Now, a few years down the line he has improved the infrastructure a bit and it is available at [codeworld.info](http://codeworld.info/).
Just a quick remark, your `takePrf` can be refactored with [`state`](http://hackage.haskell.org/package/mtl-2.2.1/docs/Control-Monad-State-Class.html#v:state) (from `Control.Monad.State`): takePrf :: Int -&gt; PrfStream BS.ByteString takePrf = fmap L.toStrict . (state .) L.splitAt . fromIntegral -- takePrf n = fmap L.toStrict . state . L.splitAt $ fromIntegral n 
I used a Bitnami stack for an instance of the bug tracker Redmine at work once. It was great at first, really simple, but a huge pain once I started wanting to do things with it. For example, upgrading a particular part of the stack, replicating the production environment for development (eg, a plugin, or a backup script), troubleshooting issues. Unfortunately, I think it's inevitable that you'll have to acquaint yourself with every part of your stack to some degree, and it will probably save you time and effort in the long run to start from a clean Ubuntu VM, and learn how to install and configure each component. Luckily, ghc, cabal, postgres, mysql, nginx, and apache are all fairly straightforward to install on Ubuntu. I recommend hvr's GHC and Cabal packages: https://launchpad.net/~hvr/+archive/ubuntu/ghc
Here's a modified version of the demo with a moving directional light: https://gist.github.com/ziocroc/7a7c2f36247e64382d7c You can try it [here](http://ziocroc.github.io/lig/)
base now includes [`Data.Void`](http://hackage.haskell.org/package/base-4.8.0.0/docs/Data-Void.html) which uses an empty type as allowed by Haskell 2010.
Ah, ok. That makes a lot more sense, now. I think I was mainly confused by the ":-" symbols and stuff. Weirdly enough, my default example is rotating suzannes with a box in the middle! Thanks! I'm giving bananu7's code a try though, since it seems a bit easier to use, although I think yours is more extensible it seems. I'll let you know if I'd like to use it, though, since it does seem really nice to use! 
Oh my god, can you please be my dad, too?
Author of the post is a mother.
To be fair, I expected the author to be the ~~mother~~ father until I was in the last paragraph. Damn stereotypes.
&gt; That's almost as bad as what Haskellers do to category theory T__T These sorts of smug comments are uncool and unappreciated. I see it as a kind of "drive by" snideness that serves mainly to annoy others and make the speaker feel superior. I hope that was not your intent but it is definitely how it comes across to me. OTOH, if you wanted to have a real discussion about how you see CT being used/misused in Haskell, then perhaps you could usefully collect your thoughts into a blog post or something... I'm sure some people might find that interesting. :)
That's great news.. Forgive my ignorance, I have 2 more questions: (1) Is it possible to embed Purescript as a library into a Haskell program (the logical choice for the server-side)? And (2) is there already an attempt to type-check the connection between a Haskell server-side and a Purescript client-side?
I use an Ubuntu derivative with GHC/etc installed as described in the download section of haskell.org. I can totally recommend that for your use case as it is most well documented and broadly used. Other options that are quite well supported are NixOS and Arch, but they build on some concepts that are slightly harder grasp then the equivalents in Ubuntu; and they have small communities compared to Ubuntu-and-co. The derivative I use is NetrunnerOS: Ubuntu uses Unity/GNOME as a desktop, Netrunner uses KDE with a bunch of sane defaults (like Ad/Flash blockers installed by default). Since you primarily use Windows, you might not want to use a desktop on top of your Linux at all. In that case the Ubuntu server edition might be best. You can setup a shared $HOME folder with Virtual box, and simply use your favorite editor in Windows to edit source files in there. To setup and run things you can log in (SSH) to your Linux with Putty. If you develop websites you can use a browser from Windows to access it. The last two points (SSH/web) probably need some setting up on the Virtual box side (a NAT table) Since setting up the NAT table is a bit cumbersome, you might want to use the Linux desktop after all. I can also recommend to get used an editor like Emacs or Vi, because they currently have the best Haskell tooling. Good luck!
Or more precisely: http://www.cis.upenn.edu/~bcpierce/sf/current/Logic.html#lab224 advanced, optional, 5 stars; yet so satisfying after you make a type checker happy :) 
See the first theorem in Andrej Bauer's answer [here](http://cstheory.stackexchange.com/questions/24986/a-total-language-that-only-a-turing-complete-language-can-interpret/24994#24994), saying "If a language can interpret itself then it is not total". My comment was essentially a reformulation of that.
You can definitely embed purescript as a library. That's what the Try PureScript website does in fact. The purescript compiler is available as a library [on Hackage](http://hackage.haskell.org/package/purescript). There have been some attempts to generate serializers on each side of a Haskell/PureScript server/client split. You might find [this search](https://github.com/search?l=Haskell&amp;o=desc&amp;q=purescript&amp;ref=cmdform&amp;s=updated&amp;type=Repositories) useful.
Hi, I'm the author, and I will check those out. Thank you! I kind of wanted to see how it would go without jumping into an immediate project, but that looks like fun.
Seconded. It's obviously the way forward long-term and appeals for a lot of the same reasons as Haskell (declarative, pure functions, all your dependency errors upfront, etc). There's even a REPL! The main problem is just it needs more people packaging things and documenting things. It's definitely my favorite OS, just know that you'll have to learn to package things yourself. Think "Arch, but crazy stable and not as well documented".
This is very easy: haste, if it's supposed to run it in the real world. Ghcjs spits out *mega*bytes of js for a hello world. Also, purescript is close enough to Haskell both syntactically and in spirit. 
&gt; The main thing to consider when comparing these two options is that ghcjs gives you semantics you can reason about that very closely match GHC's. And how is haste worse? Both ghcjs and haste generate code from ghc's STG. 
I feel your pain :)) I myself currently monitoring the haskell-reactjs scene and the number of disjointed efforts in that arena is mind boggling: ---- ghcjs: [blaze-react](https://github.com/meiersi/blaze-react) [reflex](https://github.com/ryantrinkle/reflex) [francium](https://github.com/ocharles/Francium) [oHm](https://github.com/boothead/oHm) [react-haskell](https://github.com/joelburget/react-haskell) FP Complete showcased their client side app for IDE backend with ghcjs which most likely does use some sort of reactjs like framework. ----- Haste: ~~[react-haskell](https://github.com/joelburget/react-haskell)~~ (They switched to ghcjs recently) [Hom](https://github.com/arianvp/Hom) ----- Elm: [elm-html](https://github.com/evancz/elm-html) ---- purescript: [halogen](https://github.com/slamdata/purescript-halogen) [purescript-react](https://github.com/purescript-contrib/purescript-react) ------- As you see quite a lot of different attempts. I only included the ones that are either direct bindings to reactjs or in the spirit of reactjs. I'm sure there are many more frameworks utilizing FRP libs. Very hard to make the right choice. I do not want to end up as i did years ago choosing [hunchentoot](http://weitz.de/hunchentoot/) for one of my projects LOL 
Do you mean you expected the author to be the father until the last paragraph?
I use Kubuntu and getting things 99% there is easy enough as a little script # PPA with versions of GHC and cabal more recent than official repos sudo add-apt-repository -y ppa:hvr/ghc sudo apt-get update sudo apt-get install -y pkg-config build-essential autoconf automake `# core dev stuff` \ zlibc zlib1g-dev libreadline6{,-dbg,-dev} libyaml-dev libxml2-dev libxslt-dev `# dev libs` \ zsh xterm zip ack-grep terminator tmux yakuake `# productivity` \ git gitk gitg mercurial darcs nginx libnotify-bin `# ditto` \ postgresql-9.4{,-dbg,-ip4r} postgresql-server-dev-9.4 postgresql-{client,contrib}-9.4 libpq-dev `# db stuff ` \ ghc-7.8.4 cabal-install-1.20 `# from PPA` chsh -s $(which zsh) $(whoami) export PATH="$PATH:/opt/ghc/7.8.4/bin:/opt/cabal/1.20/bin:.cabal-sandbox/bin:$HOME/.cabal/bin" cat &lt;&lt;TOZSHRC &gt;&gt; ~/.zshrc # Adds PPA-installed GHC and Cabal to path export PATH="$PATH:/opt/ghc/7.8.4/bin:/opt/cabal/1.20/bin:.cabal-sandbox/bin:$HOME/.cabal/bin" TOZSHRC cabal update &amp;&amp; \ cabal install alex doctest ghc-mod happy 'hi==0.0.8.2' hlint hoogle pointfree pointful stylish-haskell &amp;&amp; \ hoogle data Replace the postgres stuff with mysql as appropriate, I'm not familiar with those packages. It doesn't install yesod globally, but you should do that in a sandbox anyways. mkdir -p myproject/src cd myproject cabal init cabal sandbox init # then edit the project's cabal file # and `cabal install --dependencies-only` EDIT: Whoops, didn't take note of the Haskell LTS desire… I don't know about installing LTS Haskell, but the above is pretty close.
What is the problem you have, that is solved by this?
[documentation on hackage](https://hackage.haskell.org/package/stitch) made this a couple days ago for a personal project when i didnt want to mess around with clay i love clay and its definitely a great library but i often feel like it's too *big* and i occasionally find it limiting, so i made stitch as a very very flexible alternative there are no custom combinators for CSS properties or type-checked CSS measurements or Prelude conflicts, just a bunch of key-value pairs and a lot of text mangling it also includes a basic compressing printer if you're that way inclined, and i hope to port Sass' ampersand operator some time in the next couple days (which is honestly the best thing Sass ever made)
I guess you could concatenate error messages from your Lefts or something like that.
So, the use in GLES is pretty much the same as described in [24 days of Hackage](https://ocharles.org.uk/blog/posts/2014-12-10-nullary-type-classes.html). And the use in partial is the same as described in the [extension's documentation](https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/type-class-extensions.html#nullary-type-classes). But your comment about "poor man's Backpack!" got me thinking. 
I'm not questioning the boy's reasoning (which is quite good, given that he had no prior experience with functional languages). What bothers me is why you didn't explain him how it *really* works. When I was learning FP, one thing that kept me from understanding currying is that I, too, assumed that `a -&gt; a -&gt; a` meant `(a -&gt; a) -&gt; a`, and it made no sense. The notation can be very confusing. &gt; By "second a" he means (1+) 2. So his reasoning is probably that `a -&gt; a -&gt; a` means that we have got an `a`, then we turn it into another `a`, and then we turn it into the final `a`. So `(+) 1 2` becomes `(1+) 2`, and it then becomes `(1+2)` or three. ~~That is *not* how it works.~~ *(edit)* That is not what the type signature means. The `a -&gt; a -&gt; a` looks like a chain of transformations, but it's not, it's a function type. So you showed him the formal notation, but didn't explain what it really means. All I'm saying is that incorrect understanding is worse than no undertanding at all and you should clarify things to him as soon as you can. Please, treat all of the above as only my opinion and I don't want to sound rude or offensive.
Yes, exactly that.
&gt; Maybe you want to see all the errors and only proceed if everything succeeds? Yes. That's what I'm doing. Merging my errors together so I see them all, but if there are no errors I get the sum of my results.
Thank you! I hope you do. I didn't start learning programming until recently, and it's fun--wish I'd learned sooner.
I've recently released the [cmark](https://hackage.haskell.org/package/cmark) library, which offers fast, accurate CommonMark conversions (in both directions), and depends only on base, text, and bytestring. This would be a good starting point for haddock integration. It parses CommonMark into a Haskell structure that can be manipulated and transformed into whatever underlying structure Haddock uses. 
It was snide. But I didn't say it to feel smug. It's a long-standing frustration I have with the Haskell community. There's a lot of lip-service paid to category theory, but it's usually lacking in substance. For instance, we talk a lot about functors and categories -- the first because they are visible in the language, the second because it's in the name. But coming from the Haskell School of Categories (of which I'm an alumnus), it's not entirely clear what a natural transformation is good for. We learn that it is something like polymorphism. But that's only conceptual scaffolding, and under closer scrutiny, the analogy breaks down. In the 6 or so years I've been following Haskell, I haven't seen more than this one example given. Surely we can do better motivating something that is so important. But while we merely struggle with natural transformations, the community is almost silent on adjunctions. MacLane's slogan was "adjunctions are everywhere". And they are. But it wasn't until I started studying categories in mathematical contexts that it became apparent. I'm sure there are dozens of really great examples in programming contexts (parsers and prettyprinters, I believe). But we don't talk about them. Co/Limits and universal properties tend to get underdiscussed a bit as well. I'm not sure I have seen much Haskell code that makes use of any universal properties in an overt way. I would be interested to see if anyone knew of any. Some notions (especially applicatives and monads) are just contorted a bit, making it hard to see why such a notion is a natural thing to define. I think everyone is familiar with the pitfalls of the monad pedagogy. Applicatives are conceptually easier, but in both cases, the algebraic laws are jumbled due to the form they are presented to us. Applicatives, as [Edward Yang explains here](http://blog.ezyang.com/2012/08/applicative-functors/), are really monoidal functors. In their Haskell implementation, that means that they distribute over tuples (and record types). For instance, `[(a,b)]`, which we might write as `List (a, b)` for clarity, distributes, and we get `(List a, List b)`, or in Haskell, `([a], [b])`. The laws we get for equational reasoning become the monoid laws. I have been meaning to type up a blog post regarding the analogous issue for monads. Conceptually, monads are not about sequencing, as the `(&gt;&gt;=)` function suggests it might be. They are about flattening of structure. (It just happens that in the case of state- or IO-like monads, this flattening process *is* sequencing). All this is not to say there aren't good efforts being made, nor that there aren't people who *do* understand category theory here. I enjoy [Bartosz Milewski](http://bartoszmilewski.com/2014/10/28/category-theory-for-programmers-the-preface/)'s series. I love to listen to people on \#haskell-blah talk about the subject. But I believe that the Haskell community needs to figure out what its relationship to category theory is. We need a better story to tell and a better curriculum for those who are interested in learning it. 
The statement about the function being either addition or multiplication is a wonderful way of realizing the power of restricted parametricity :O
Yeah, I accept the point about coinductive interpreters (i.e. interpreting a program requires you make an unbounded number of calls to an effect handler). &gt; all of the Agda compiler's algorithms ought to provably terminate You can write a self-compiler in Agda, but not a self-interpreter. The reason is that, even though an interpreter of Agda programs does indeed provably terminate for all valid Agda progams, that fact can't be proven in Agda itself. You need a stronger system to prove that, at least Agda + Con(Agda). That's pretty much Gödel's second incompleteness theorem, which says that if Agda is consistent, it can't prove Con(Agda). The above applies to all languages whose type systems correspond to consistent logics. So for example Haskell is off the hook. You can write a Haskell self-interpreter just fine, because Haskell's type system corresponds to an inconsistent logic which can prove any statement, including Con(Haskell) :-)
Maybe for some applications, but certainly not for all.
That does look like it behaves similar. I'll keep playing with that. 
There's a practice on reddit of putting "/s" in a comment (usually at the end) if it is sarcastic. Your happened to have a "/s" in it for a different reason. I was just making a poor joke, I guess. :(
Waterfall v. Agile is a false dichotomy. They're both subcases of organizations where business drives and engineering is just treated as an annoying cost center that exists to implement business peoples' ideas (and that always takes too long and too much time). Therefore, they both (Waterfall and Agile) suck. An engineer-driven organization is going to be better. Here's a take-down of Agile: http://www.quora.com/Why-do-some-developers-at-strong-companies-like-Google-consider-Agile-development-to-be-nonsense/answer/Michael-O-Church
Speak for yourself. Most of the engineers where I work happen to be women at the moment.
There is also the nifty opaleye and postgREST libraries that only works with postgres at the moment. I'd definitely prefer postgres over mysql for Haskell development.
Here's how most of our Yesod devs work: * Make sure your server is running keter. * Install Postgresql and GHC on your Windows box. Make sure you have a quite recent version of cabal (even if you are using the Haskell Platform). * Install the same version of Linux that your server is running in a VirtualBox VM on your Windows box. Install the same version of GHC in the Linux VM as you have on Windows. * Develop and test your Yesod app totally on Windows, using `yesod devel`, your favorite editor for Haskell (you'd be surprised what people choose for that), and your favorite browser(s) for testing. Do everything inside cabal sandboxes. * When you're ready to deploy to the server, push your current code to version control, pull in the Linux VM, and run `yesod keter` there. I'm sure this can be shortened and streamlined in many ways - suggestions welcome - but this is how we work day-to-day and it works fine. Disclaimer: Personally I am using a Mac so my workflow is slightly different, but everyone else works as above.
Taking the opposite approach, we could introduce a pre-processor that allows Haskell programs to be written using syntax constructs from Edwin Brady's [whitespace](http://compsoc.dur.ac.uk/whitespace/) programming language (first released 2003-04-01).
&gt; When I was learning FP, one thing that kept me from understanding currying is that I, too, assumed that a -&gt; a -&gt; a meant (a -&gt; a) -&gt; a, and it made no sense. The notation can be very confusing. Why would you consider F(S^2, S) to be naturally isomorphic to F(F(S, S), S) (where F(A, B) is the "set" of functions from A to B, just making up some notation)? you can immediately just look at the first two values and see that in the first case there is no functional dependency from the first two the second but there is in the third.
Thanks for sharing. Maybe it is good to have the Clay comparison in your readme as well, it could really help someone trying to select a lib like this. &gt; which is honestly the best thing Sass ever made Agreed.
Because I was 14 (or so) and I didn't know what is "naturally isomorphic" or "functional dependency", and I only had some limited experience with imperative languages (including C++ and C#), so I decided to try something different and started reading the Programming F# book. And in the very first chapter the author instead of a proper explanation of how currying works just said something like this: `a -&gt; b -&gt; c` means a function from `a` and `b` to `c`. And I thought to myself: "What a weird notation", because I unintentionally parsed it in my mind as `(a -&gt; b) -&gt; c`, without understanding it or thinking about isomorphisms. After that I lost my interest in functional programming until a year later, when I decided to give Haskell a go (because it promised very strong static typing, and I wanted that after some C++ nightmare-ish debugging). And LYAH explains currying very clearly.
Perhaps I can use this to implement Org syntax for interactive editing in literate Idris files. That would be really fun!
What a cute read! really enjoyed it. I wish my parents taught me programming... meh I didn't end up that bad after all. Still a programmer :)
I'm the one who did colors for Idris output, including errors. In general, I'm really jealous of GHC error messages - most of them are still far ahead of our error messages in helpfulness and specificity. But the color feature does a bit more than you mentioned, and I think that GHC could profitably adopt it. Idris's pretty-printer has a notion of _semantic annotations_ that are used to generate things like the coloring. But we can do a lot more than that as well! If I'm at the console, I'm basically stuck with colors, but in Emacs or any other future environment that someone wants to implement, compiler output is sent as pairs of strings and annotated regions, where the annotations are fairly declarative in nature. The editor or IDE can then choose how to display them. In an Idris error message in Emacs, you get semantic coloring of names, tooltips with documentation strings and type annotations, commands for showing or hiding implicit arguments to terms, and the ability to normalize a term in-place in the error message or show its fully elaborated representation in the core language. I've got a [blog post](http://www.davidchristiansen.dk/2014/09/06/pretty-printing-idris/) that explains how it's implemented. Trevor Elliott at Galois has since implemented the same feature for the `pretty` library, which is (as far as I know) API-compatible with the GHC pretty-printer, so it should be possible to adopt a similar approach. In GHC, it would be nice to be able to do things like see Haddock summaries, expand or contract type synonyms, see _which_ names are type synonyms either by color or in a tooltip, and similar things. This should be achievable without major changes to the code, based on my (limited) knowledge of the implementation.
That was a pretty poorly formulated comment, but the "99%" remark reminded me of a Haskell hackathon I attended not long ago with around 100 attendees, of whom I think exactly one was female. So, like, there's something to the notion that it's "not only stereotypes."
You mean one that is positive even in: `(-1) mod 2` ?
react-haskell switched over to ghcjs just recently.
Ah, of course. Connecting it to the Gödel result made it click. Although, I suspect this is false: &gt; You can write a self-compiler in Agda, but not a self-interpreter. Because an Agda typechecker has to be able to evaluate Agda programs.
I think even the requirement of matching dev and server environments is not so important. I use https://halcyon.sh to build binaries on the server rather than uploading binaries. It has worked well so far.
IIRC, it was implementation-dependant in C89, but from C99 on, its behaviour is standardized.
I didn't know about bake, the continuous build system until I just saw this video. I've been using mercurial commit hooks to reject change sets that fail to build or test but it's nice to have the extra intelligence of bake which Neil mentions in the video.
My father started teaching me, because I expressed interest, nearly 30 years ago. I still can't get enough of it.
Hey! Yeah, the codeworld.info site is mine. I'm very interested in what you're doing! I've taught using the CodeWorld web site for three different classes at two schools now, to ages 11-13. But it's great to see others' perspectives, too! CodeWorld copies a *lot* from the gloss library, so like gloss, it's focused on getting something visual on the screen quickly and easily... but still in a purely functional way. Your use of GHCi has advantages, too, in a different way; but I haven't yet seen the light on how to build a great interactive shell for CodeWorld. If you are interested, let me know, and we can talk more about it.
You can use [Haskell for shell scripting](http://www.haskellforall.com/2015/01/use-haskell-for-shell-scripting.html). I advocate this approach to learning Haskell because you get the highest payoff for the least investment in the language. The [tutorial I wrote](http://hackage.haskell.org/package/turtle-1.0.2/docs/Turtle-Tutorial.html) explicitly targets people who have absolutely zero Haskell programming experience.
I'm using 6.0.0a too.
It's very elegant, but I've had a devil of a time actually using it for a parser. [Here](https://hackage.haskell.org/package/syntax) is a package based on lenses that implements a Syntax class. Among other things it has an implementation of indentation.
Now it's the era of Haskell machine? :-D
Failed to reproduce here. Linux [host] 3.2.0-4-686-pae #1 SMP Debian 3.2.65-1+deb7u2 i686 GNU/Linux The Glorious Glasgow Haskell Compilation System, version 7.10.1
Huh, that's really interesting. Is there a video anywhere?
Yeah, I just installed it the other day and I'm really enjoying it so far. Nix just feels great to use, but the documentation can be a little rough. Do you know if there are any decent packaging tutorials? Something that goes beyond nix expressions and actually shows how to package a configuration so that it is similar to nixpkgs?
Wow! That works sounds awesome! IMHO GHCJS is really an excellent addition to the Haskell ecosystem! Thanks a lot for all the hard work.
You should probably watch Edward Kmett's talk Typeclasses vs. The World if you are thinking about alternatives to the current system and its advantages and disadvantages. It covers a lot of the alternatives and why they won't work.
I believe what your suggesting is [exactly what Idris supports](http://docs.idris-lang.org/en/latest/tutorial/classes.html#named-instances). Maybe it's worth having a play there and seeing how things "feel"?
Trying to understand how it all works (nix channels, certain conventions used in nix expressions) has had me wanting to give up as well. However, there is no other Distro or tool that lets me declaratively and, modulo channel/expression-set, define a referentially transparent way to share project environments. There's, Docker has some of these features but at a huge cost to comprehension (as opposed to "some PATH manipulation and a bunch of stuff stored in /nix/store). Running the OS has been a challenge for me too, I've seen others start later than me and pick it up faster. But the benefits and opportunity are worth is, as a community, investing (volunteer) time into it, as /u/rezb1t mentioned. Hitting walls like the OP has have become (hard) learning experiences. I'd say: don't give up, the end goal is worth it! (edits: bad autocorrect is strong with this one)
Wait, why is that?
Pardon if this is a naive question, but why must the isomorphisms be partial in both directions? I understand why it must be partial in one direction, parsing but why printing?
Could you elaborate? My understanding is that essentially the only traversables are containers and I think of both `(,) String` and `Maybe` as containers. 
Always positive is best for doing modular arithmetic, since it always maps dividends to the canonical element of the modular equivalence class. So, for example `mod x 5 == mod y 5` (in Haskell syntax) always true when x ≡ y mod |5| (in Math notation). Also always-positive makes proving the uniqueness property in the Division Theorem a little more straight-forward. &gt; haskell mod gets must of the 4 positive Haskell's `mod` gets exactly 2 of the 4 positive; the result always matches the sign of the divisor. Haskell's `rem` gets exactly 2 of the 4 positive; the result always matches the sign of the dividend. Neither option is any closer to always-positive than the other. `rem` is what C uses as `%` these days.
If most people are fine with `cabal install`-ing unsigned packages sent over an unauthenticated connection, they won't mind this either.
I'm not the OP, but since I can reproduce it on my machine, I might as well answer: &gt; can you reproduce this with `it ^ (2^9)` Yes. &gt; `foldl (^) it (replicate 9 2)` Yes. &gt; QuickCheck No, I ran it a bunch of times and the tests always pass, but that doesn't mean anything because let k = it ^ (2^9) in k == k does not crash. It seems to be the act of printing the string which crashes it? let k = show (it ^ (2^9)) in k == k Indeed, as the above does crash. Tweaking your quickcheck expression in the obvious way, the tests still pass, but the run much more slowly so it will take a while until I can claim to have run them "a bunch of time". *edit*: passed 100000 tests.
Its not my article, i just thought it would be interesting to discuss.
Its not my article, i just thought it would be interesting to discuss. BTW: I posted it here, because the NixOS said, it is also about Haskell.
Because it's dependently typed. I could write a whole agda program and then evaluate it on the type level if I wanted. E.g, to typecheck: foo : AgdaProgram input == 3 foo = refl requires evaluating `AgdaProgram`
Also cannot reproduce: $ uname -srvmo Linux 3.19.2-1-ARCH #1 SMP PREEMPT Wed Mar 18 16:21:02 CET 2015 x86_64 GNU/Linux $ ghci --version The Glorious Glasgow Haskell Compilation System, version 7.8.4
&gt; Most of the engineers where I work happen to be women at the moment. Do they change genders frequently?
I don't know what you are trying to do either but just in case `ExceptT` does concatenate the left messages for you.
Replicated on GHCi 7.8.4 on Yosemite.
How can it be an isomorphism if it is partial?
It can be an isomorphism on subsets, which granted is not an isomorphism in the usual sense.
All distros have 'only so much man power'.. the problem with NixOS is that you need MUCH more man power to maintain the same set of packages as other distros.
I figured it was to avoid all the cases that would be caused by allowing a complete generalization.
I wonder what is the future of named instances in Idris by the way, as just by coincidence today I've stumbled upon this comment http://www.reddit.com/r/haskell/comments/2aw6vl/idris_0914_released_with_updated_javascript/ciztkn4 from some time ago.
I think of this as more like an "industrial-strength" version of Thermite, but the underlying ideas aren't exactly the same, and of course, one uses React and the other uses virtual-dom. I'm going to continue to work on Thermite though, since I think it definitely has its place.
~~Do you have a link to the talk?~~ Found it: https://www.youtube.com/watch?v=hIZxTQP1ifo
My thinking is that it would be more appropriate to use improper prisms rather than partial isomorphisms, at least for parse-print and decode-encode pairs.
Her.
&gt; Every type class should have at least one more law than its super class. The (implicit) super class of all base type classes has no laws, thus forcing every base type class to have at least one law. Ooh. *Sexy*.
It shows that you aren't a grandmaster in Ehrenfeucht's game. Casual!
Done about 15 times too, no problems in Yosemite GHC 7.10.1.
I'm pretty close, so it ended up being $4 shipping. Its a flat rate I think no matter how many of these you buy though. It'll show it on the checkout once you've put in your shipping info.
Could you elaborate on that? I think I get the [literate idris in an org buffer](https://github.com/idris-lang/Idris-dev/wiki/Egg-%236:-Improved-Support-For-Literate-Programming#org-mode) part. ( orgmode-parse doest't recognize code blocks yet, but we could add that ). What does interactive editing in literate idris mean? Having inferior-idris work in emacs on .org buffers with idris blocks?
How ever you view the talk, it makes sense for everyone arguing about the topic to watch it first to save needless repetition of the same old ideas. 
Yeah. For instance, it's useful/informative for everyone to know that for the kind of thing Ed wants, “Modular Type Classes” à la Chakravarty, Dryer, Harper are probably not sufficient (and thence, Canonical Structures probably wouldn't work for him either). My realization was that there is a trade-off between Haskell-style type classes and modularity, and that the two are entirely incompatible. I hesitate to put words in Ed's mouth, but I think that he finds the former so incredibly useful and liberating that he is willing to sacrifice the latter, even though he'd prefer to have both. My preference is to wear the hairshirt, and get modularity at the expense of having to prove a number of theorems manually which previously came for free from the structure of my code (via type class coherence). In the end, each person must decide for themselves.
I know nothing about the Pusher protocol, but the standard approach to boilerplate encoding/decoding problems these days is to use GHC.Generics. It doesn't always fit (typically when your data structures don't resemble the wire protocol), but it kills all the boilerplate when it does.
&gt; I'm surprised this submission is being down-voted. I'm not, it seem that people on this channel downvote any (even constructive) criticism about their beloved language/platform, which is a bit disappointing, because it doesn't help to go forward.
Yeah it seems to get better a few minutes in as well.
Their payment mechanism is through Paypal at least, so that is SSL at least.