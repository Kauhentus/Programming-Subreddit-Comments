It is a mistake to think the network delay or the GC will protect you from timing attacks. The attacker can simply perform multiple interactions and remove these factors. What is needed is a constant processing time for each part of the protocol. It would be good to wrap the Timing Black-Box around all the parts that could potentially leak timing information. Maybe it could be configured into different modes depending on the level of protection vs performance required in case it makes it too slow for some people. It might also help test whether timing attacks are possible by collecting statistics. Edit: Thinking about it, it is the network packet response time that needs to be constant, or at least it should have timing unrelated to any potentially useful information. Obviously it only needs to be constant within particular sub-types not across evey kind of packet. 
&gt; No custom monads **This is an outrage*****!***
nested where clauses :/
Would a `let .. in` be preferred instead of the second `where`?
yea. also removefirstitem is just tail. however the stuff defined in where clauses is in scope for the guards, using a let means the conditions there would have to be moved inside the let scope in an if-then expression
Let me ask a broader question: what's the purpose of putting the upper bounds on a package? I take it to be as a guide to Cabal for which versions of packages to try and compile with. For example, if I write a foo package that depends on bar, and I've tested with bar 0.1, then I put a "&lt; 0.2" constraint so that Cabal doesn't try to use version 0.2 instead. But does this really apply to base (or any other bootlib)? If I rely on base 4.4, and a new GHC with base 4.5 rolls out, Cabal isn't going to try and build with the old base, it's just going to die. In other words, I see the following three scenarios: * Include upper version bound: my package is guaranteed not to work with the new GHC. * Skip the upper version bound. * Package happens to work. * Package doesn't work. So besides a few dashed hopes and dreams when the compile fails later in the process, it seems like it's better to skip the upper version bound. Note, however, that this does *not* apply to arbitrary packages, where the upper bound can in fact guide Cabal to make a better choice. Maybe the best choice is to have Cabal ignore version bounds that it has no control over, I'm not sure.
There is one free variable (x) in the expression.
what about the other direction? say I have a library `foo` which in version 0.1 works for `base &lt;= 4.3.*` and in version 0.2 it works for `base == 4.4.*`; if I request 'foo' with GHC 7.0 I get `foo-0.1` and with GHC 7.2 I'd get `foo-0.2`, right?
removeFirstItem is not just a tail; it actually does a lookup on a given list item and returns a new list without that. &gt; however the stuff defined in where clauses is in scope for the guards, using a let means the conditions there would have to be moved inside the let scope in an if-then expression Well that is something I didn't knew.
You should really use cabal to build your package. As it stands now, if I want to build your package, I have to guess the dependencies and install them by hand.
then it can be just a call to filter, no? wrapping it in another function seems unnecessary. unless you actually need to get the call to ID and yea, you have to be careful about scoping with let vs where and guard/ifelses etc etc etc etc
I can't just call filter because `i` may be `[]` at some point at runtime and `head i` will throw an exception when that is the case. `i` is the item I want to filter out, so that when `i` is `[]` I still want to get back my original list (that is the reason for the `id` function in there) :)
You can look at my [original version](https://gist.github.com/1175855/59d2876b6af44f1b9cb68810434234b34b72994b), maybe that will make it a bit clearer 
Thanks, I'll at least have a lot to learn from. Also I taught that your `lengthSize` function could easily take the following form: lengthSize resize = ([0, -1, 1, 0, 2, 4, 6, 8, 10, 12, 14] ++ repeat 0) !! (length resize)
First time I hear about WAI. I'll look into it.
Works on my machine Jokes aside, I'll upload the cabal file ASAP.
Yes, this event is aimed at industry/business. The biggest share of the money goes to the venue for room hire and catering. Other costs include speaker travel expenses, insurance, registration staff, badging, eventbrite fees, graphic design. The taxman takes 20% of the Â£132 figure quoted above. We're not planning to make a profit on this event. If it's successful we'll run it again, grow the event and look for (paying) sponsors to subsidize it.
is it GHC-7.2 based?
It could, but if you do this: &gt;lengthSize = ([0, -1, 1, 0, 2, 4, 6, 8, 10, 12, 14] ++ repeat 0) !! 10000 haskell has to find the 10000 index of a list and for that it has to first create it, which is slower than my function. BTW, nice thinking, using laziness like that.
Yes, new in the sense that it's not been run before in this form, rather than new in the sense of being 'unique'. The emphasis is on hands-on learning for programmers.
We don't provide GHC in it, so you can use the GHC version you need, more or less. But it's tested with the Haskell Platform, primarily, which is still 7.0. Report any build error with GHC 7.2 , if any, and we can look into it.
We're hoping that the two keynotes and one of the case studies will be filmed for later broadcast. We're speaking to a possible partner to do this for us for free.
I'll keep my fingers crossed. I can't make it to the UK but I'd love to watch some lectures/keynotes online.
I was actually assuming we where talking about small numbers given the specific case being detailed for only a small set :)
Ah, sure. I was just mentioning FRP since you asked for further developments. :)
in that case you should be using find and remove, see Data.List to figure out something a bit more idiomatic
You're right, but I don't like leaving things to chance.
The author is unaware that -- just as for every heart-beat in Yesenin-Volpin's childhood there was another, later one -- so for every anecdote about Yesenin-Volpin's life, there is another, even more astounding one. 
Wow, just loaded it up. Installed without a hitch, and it seems to work quite well so far (initial impressions). This is outstanding! I am not a huge fan of IDEs in general, but there are not many functional languages with such a great environment. Haskell has such a great ecosystem with Hoogle, cabal, etc... It is great to see that power brought to the IDE. Thanks for the hard work.
It's not so bad as you might imagine. If you're looking at index 10,000, that means you already traversed a list of length 10,000 to compute the size, so asymptotically you haven't lost anything. Anyway, you can recover from this problem by doing something like this: lengthSize = last . zipWith const [0, -1, 1, 0, 2, 4, 6, 8, 10, 12, 14, 0] . (undefined:) This runs in O(1) time (unlike your current definition) and sports the compactness of metaultralurker's idea.
Yes, but unless I've misunderstood the expression large parts of it are constant. For example: sum (replicate 3 (2 * 8) ++ replicate 3 8) Or have I got the expression wrong? I'm fairly new to haskell.
Thank you for trying it out!
I'm having trouble installing it on OSX - seems to be some sort of dependency problem with cabal (ghc version 7.0.3 This is what I get if I try it from source: &lt;command line&gt;: cannot satisfy -package-id Cabal-1.10.1.0-705074891db944af83ea4aa7a09bae14: Cabal-1.10.1.0-705074891db944af83ea4aa7a09bae14 is shadowed by package Cabal-1.10.1.0-49678efb6bfc399545e2b61629b900e2 (use -v for more information) cabal: Error: some packages failed to install: scion-0.1.0.10 failed during the building phase. The exception was: ExitFailure 1 Any ideas?
You seem to be implying that there are infinitely many Yesenin-Volpin anecdotes. I remain skeptical.
Your error message is not good, you seem to have several version of Cabal 1.10.1.0 floating about. Look at ghc-pkg output and try to remove one version so you only have one...
How do I remove it? Could you link me to some docs - sorry, I'm new to haskell..
There's a ghc-pkg tool that lets you "unregister" packages, you may have one version in the global package database (common to all users), and one in the user specific database, that seem to be causing problems. If you're really new to Haskell, I recommend just installing the Haskell Platform, that should get you going. If you have the Haskell Platform, EclipseFP should install fine, downloading all the things it needs on top of it. The Haskell Platform is a set of tools and libraries that all play nicely together, there is a version for OSX.
I've really tried to get my head around this and haven't succeeded. I can't figure out how to use the current code from ghci. cabal-dev ghci... then what? Otherwise I'd try this: If I figured out how to represent an infinite list, would it result in JavaScript that halts? 
You almost certainly want to be using some form of recursion for this.
In FP land (actually this is relevant for any kind of paradigm) it is better to work in smaller steps. You want to make a string easier to read by adding whitespace (spaces and line breaks) and indentation. So let's start with this: easierToRead s = indent (addWhitespace s) Adding line breaks can be pretty similar to what you did: addWhitespace s = concatMap withWhitespace s where withWhitespace '=' = " = " withWhitespace '[' = " [\n" withWhitespace ']' = "\n]\n" withWhitespace c = [c] Indentation then becomes a matter of keeping track of the level and putting spaces after new lines. indent s = indentAt 0 s where indentAt level ('[':cs) = '[' : indentAt (level+1) cs indentAt level (']':cs) = ']' : indentAt (level-1) cs indentAt level ('\n':cs) = ('\n' : indentation level) ++ indentAt level cs indentation 0 = "" indentation n = " " : indentation (n-1) That's pretty much it (modulo typos and brainos). Notice it don't use any particularly complex feature and it could use the libraries a bit more instead of doing so much with helper functions, but the general idea is breaking the problem in smaller pieces and composing them together. We shoved the state to the only part interested in it (i.e. indentation) and made it very simple to change and use the state, so simply using it as an additional parameter is enough and understandable.
If that's because you are confident there were e.g. finitely heartbeats in Yesenin-Volpin's childhood that's okay, since Yeseninism-Volpinism also entails that proposition. He just (just??) rejects a negative account of either finitude or infinitude in terms of the other. There was *obviously* no end on the heartbeats of his childhood, and thus infinitely (or in-finitely) many of them (right??); but there were also finitely many, since they were certainly fewer than `maxBound::Int`, indeed less than a billion and we can approach this from below by typing `[1..maxBound::Int]` in ghci, and reading quickly (right ??). The natural ground for denying that there were infinitely many heartbeats in the childhood of Yessenin-Volpin is that the heartbeats of his adulthood came later, but Cantor has cured us of that vulgar prejudice. Only a few distinctions akin to Cantor's are needed here (no??). See the famously astounding discussion in Dummett, Wang's Paradox [scribd](http://www.scribd.com/doc/63837360) So *surely* it is only the claim about an order of successively *definitely more astonishing anecdotes* that could really occasion doubt. :) 
This is awesome and I fully intend to go over it until I understand it later. Thanks!
In codeIndex.hs, let v = (a,b,c,d,e,f) where a = z !! 0 b = z !! 1 c = z !! 2 d = z !! 3 e = z !! 4 f = z !! 5 return v You can write that as: return (a,b,c,d,e,f) where a:b:c:d:e:f:_ = z In codeView.hs, checkInputFile :: [a] -&gt; Bool checkInputFile file | len == 24 = True | len == 25 = True | otherwise = False where len = length file You can write that as: checkInputFile :: [a] -&gt; Bool checkInputFile file = length file `elem` [24,25] You can rewrite removeEx, removeEx2, dropPercentage, dropPixelage using filter and isAllNumbers using map. checkDatabase returns 0 or 1; consider using booleans: return (a /= []). You have some functions defined in multiple files, better write them in a single file and use import. Install hlint (cabal install hlint) and use "hlint filename" to get suggestions.
**Spoilers!** More fun if you do it yourself... I defined `bottom` as bottom c f p@(x, y, z) | y &lt; -0.5 = c (f p) | otherwise = c p I defined `middle` as middle c f = c `cube` left `top` right `bottom` right One possible solution for the final Rubik checkerboard is display $ standard `middle` left . left `cube` up . right .down `middle` right . right `cube` up `middle` left . left `cube` down . left
I'm getting an install error on osx. &gt;Cannot complete the install because one or more required items could not be found. &gt;Software currently installed: FP: Haskell support for Eclipse 2.1.0 (net.sf.eclipsefp.haskell.feature.group 2.1.0) &gt;Missing requirement: Haskell Plug-in Debug Core 2.1.0 (net.sf.eclipsefp.haskell.debug.core 2.1.0) requires 'bundle org.eclipse.jdt.core 0.0.0' but it could not be found &gt;Cannot satisfy dependency: &gt;From: Haskell Plug-in Debug UI 2.1.0 (net.sf.eclipsefp.haskell.debug.ui 2.1.0) &gt;To: bundle net.sf.eclipsefp.haskell.debug.core 0.0.0 &gt;Cannot satisfy dependency: &gt;From: FP: Haskell support for Eclipse 2.1.0 (net.sf.eclipsefp.haskell.feature.group 2.1.0) &gt;To: net.sf.eclipsefp.haskell.debug.ui 2.0.3 
in case anybody was interested I ended up with this: import System.Environment main = do (inputString:_) &lt;- getArgs putStrLn $ easierToRead inputString easierToRead :: String -&gt; String easierToRead s = indent (addWhitespace s) addWhitespace :: String -&gt; String addWhitespace s = concatMap withWhitespace s where withWhitespace '=' = " = " withWhitespace '[' = " [\n" withWhitespace ']' = "\n]\n" withWhitespace ',' = ",\n" withWhitespace ' ' = [] withWhitespace c = [c] indent :: String -&gt; String indent s = indentAt 0 s where indentAt level [] = [] indentAt level (',':'\n':' ':'[':cs) = ", [" ++ indentAt (level+1) cs indentAt level ('[':'\n':'\n':']':cs) = "[]" ++ indentAt level ('\n':cs) indentAt level ('\n':'\n':cs) = indentAt level ('\n':cs) indentAt level ('\n':']':cs) = '\n' : indentation (level-1) ++ ']' : indentAt (level-1) cs indentAt level ('[':cs) = '[' : indentAt (level+1) cs indentAt level (']':cs) = ']' : indentAt (level-1) cs indentAt level ('\n':cs) = ('\n' : indentation level) ++ indentAt level cs indentAt level (c:cs) = c : indentAt level cs indentation n = replicate (n*4) ' '
This must be the best documented Haskell-related project ever: * http://eclipsefp.github.com/features.html * http://eclipsefp.github.com/install.html Congrats, this is really impressive.
So how do I enable the syntax highlighting?
I have the Haskell Platform installed... Should I just get rid of ~/.cabal and start from scratch?
Interesting. Nonstandard analysis (in the form of Internal Set Theory) has something similar for every infinite set. For example, in the set of natural numbers there is a finite number of standard natural numbers (and every "nonstandard natural number" is greater than it, and thus nonstandard natural numbers function as "infinite integers" or "unbounded integers"). However, every uniquely definable natural number is standard. So you can never actually obtain a nonstandard natural number, you can just reason about them. I say that this similar to this anecdote because it seems that although every number from 2^1 to 2^100 is finite and therefore constructible, clearly at some point it becomes way too large to count. I guess this is still different in that we can still reason about nonstandard natural numbers, even though we can't construct one.
This is great. Seriously. Even if it's not the best code in the world, it's still an excellent example of doing something *simple* and demonstrating what can be done in Haskell from a less complex perspective. Bookmarked and will read through the code later with a coffee :D
Does 1/2 exist?
I really do want to get back to this and other functional graphics &amp; GUI work. I keep seeing *almost* solutions for the broken status of cross-platform, ghci-friendly GUI &amp; graphics libraries. And now I have a functional-&gt;GPU compiler that generates insanely fast 2D &amp; 3D rendering code to replace the functional images implementation I showed in the video.
That's right. Would anyone like to help?
&gt; Composable GUIs are a two-edged sword: they are extremely simple to compose, but they are also quite limited in what they can do. Yes, sort of like pure functional/denotative programming, where we can't really *do* anything at all. I like the severe constraint as a spur to creative thinking. In contrast, imperative models let us *do* all kinds of things but without much sense of what it *is* we're doing. Which is fine in other situations.
Great as usual!. I may have missed something but it would be great to see how the kids did with the last "organization" topic. 
Ah, yes... the results don't look very much different from how they've looked before, though. That was the goal! They all were able to revise their programs and choose better variable names and get some more logical grouping of shapes together.
The semantics of Haskell are such that function calls are, by themselves, stateless. In cases where one would maintain a state, the state has to be explicitly passed through function calls. However, what facilities like the State monad allow one to do is to pass such a state around in a syntactically transparent way that does not force one to have to constantly add an extra input parameter to every stated call and latch updated states to output values. The composition functions for monads can hide a lot of these mechanics, particularly when used in the context of "do" blocks. As is the case in the examples given by other, for simple forms of what one would think of as state, sometimes it is reasonable to just add to each function as inputs the parameters that condition its behavior: functionWithState :: A -&gt; B functionWithState a = ... using state s of type S must be done as functionWithoutState :: S -&gt; A -&gt; B functionWithoutState s a = ... using state s as passed in If the state needs to be updated, than one must do the following functionWithoutState :: S -&gt; A -&gt; (S, B) functionWithoutState s a = (... updating state ..., ... using state ...) If two such functions use state and they are composed functionWithStateY . functionWithStateX using, updating, and passing on state. This must be changed to something such as functionWithoutStateY r b where (r, b) = functionWithoutStateX s a where *r* is the updated version of *s*. This stated composition can certainly be handled more elegantly by using a State monad, which allows one to replace the above with something such as functionWithMonadicStateX a &gt;&gt;= functionWithMonadicStateY or do b &lt;- functionWithMonadicStateX a c &lt;- functionWithMonadicStateY b return c There is a little to this, but it is merely a generalization of what is done in the above example. 
I had the same message. In my case, I'd installed the C++ subset of Eclipse. I went back to Help-&gt;Install New Software, picked "The Eclipse Project Updates" from the "work with:" dropdown, and grabbed some more bits. Notably the SDK, and and Platform SDK. I've no idea what that means, but it seems to have done the trick. The hint is in the error. "&lt;rhubarb&gt; Haskell Plug-in Debug &lt;rhubarb&gt; requires 'bundle org.ECLIPSE.jdt.core" &lt;rhubarb&gt;". 
This is really great. I think I'll be ditching GEdit for this.
so.. do you need parent permission forms to teach monads? 
Yes? The question was evidently about the natural numbers though, i.e., `data Nat = Z | S Nat` No 'constructivist' view is well stated as pertaining to 'existence'; Ysenin-Volpin clearly understood Friedman's questions as meaning something like: does the expression ` (S (S Z)) ^ S(S(S(S Z)))` reduce to an element of Nat -- or does it rather 'diverge'; naturally he took twice as long to answer each successive query. Note that Aronson's account, as one would expect, is a falsification of Friedman's, which is itself maybe a bit crude. Thus, e.g., Aronson thinks Y-V would deny the 'infinite,' when of course his view produces an amusing superabundance of infinite totalities.
Sure it wasn't about the positive rationals? data PRat = U | S PRat | I PRat
Excited about using this. Could see myself transitioning from vim if I can find a half decent vim emulator for eclipse.
I have no intention of teaching monads. This isn't really a Haskell class, so much as a class that just happens to use Haskell. The language is nice in that it's functional -- which is important because that does a much better job of preparing kids for algebra and other high school math. And it's nice because it has familiar syntax in lots of places where, for example, Scheme would not. The named data types and constructors are also an improvement over Scheme... the more naming you can do, the better, IMO, especially for new programmers. But one of the nice things about teaching with the gloss library is that you completely sidestep any IO at all, and even most uses of type classes, and just get a simple language of functional expressions.
Yes, see p. 4 of Friedman's notes.
Many thanks to Manuel for coming up to Brisbane to deliver this awesome talk. Also thanks to [functional io](http://functional.io/) for sponsoring it. It was a great night.
Ah, yes I had the c++ subset too. Thank you. edit: well, not totally working. &gt;cabal: Error: some packages failed to install: &gt;scion-browser-0.1 failed during the building phase. The exception was: &gt;ExitFailure 1
@ostochast: Very nice, thanks for posting this! @g__: &gt; You can write that as: return (a,b,c,d,e,f) where a:b:c:d:e:f:_ = z It's not a great idea to mix `do`, `let`, and `where` like that. The scoping rules start to get complicated. In fact, based on the indentation, I think `g__` is making a mistake about how this scopes. The `where` clause in g__'s version does **not** apply to the `return`. It applies to the entire `do` block. That code might work, but if so, it's just accidental. To my eyes, this would look simpler: let a:b:c:d:e:f:_ = z return (a, b, c, d, e, f) That said, it's almost never right to use a 5-tuple. You probably want to define a data type for that.
This is wonderful. I had no idea ultrafinitism was a thing. Does anyone make a kind of epistemological argument against the existence of very large numbers? I.e. Basically that numbers bigger than we can comprehend cannot be said to exist? I've been reading the amazing "Everything and More" by David Foster Wallace. It's really been incredibly enlightening to read. All about the history of math and the coming to terms with new mathematical objects, and all the really hairy metaphysical questions at the core of all of it. 
Great stuff! Do you mean to say that the drawing area on the gloss-web web site is 501 by 501 in size? If it's 500 by 500, then it's not true that the x and y coordinates can range from -250 to 250. Obviously, the magic in this kind of project is choosing what **not** to talk about. It takes a lot of care to tip-toe around the concept of "one-off error" without mentioning it. I'm not sure what is the right thing to do here.
Yep, I got that too. I worked around it by manually running Cabal on all the missing dependencies, or forcing a rebuild for those that were present but seemed not to satisfy Scion-Browser. What a PITA. Here's my command history. Note that I has to go through several cycles of restarting Eclipse and letting it run to failure before I got the entire list of things done. Is this my opportunity to say something cutting about type safety? Glasgow is only an hours drive away. Perhaps I should go and tell them in person! 458 cabal update 459 cabal install uniplate 460 cabal -v install uniplate 461 cabal -v install syb 462 cabal -v install uniplate 463 cabal install unordered-containers 464 cabal install hashtable 465 cabal install hashable 466 cabal install text 467 cabal install deepseq 468 cabal --reinstall install deepseq 469 cabal install text 470 cabal install hashable 471 cabal install unordered-containers 472 cabal install scion 474 cabal install hslogger 475 cabal install mtl 476 cabal --reinstall install mtl 477 cabal install transformers 478 cabal --reinstall install transformers 479 cabal --reinstall install mtl 480 cabal install hslogger 481 cabal install network 482 cabal install parsec 483 cabal install parsec --reinstall 484 cabal install network 485 cabal install hslogger 486 cabal install scion-browser 487 cabal install missingh 488 cabal install hunit 489 cabal install missingh 490 cabal install regex-compat 491 cabal install regex-posix 492 cabal install regex-base 493 cabal install regex-base --reinstall 494 cabal install regex-posix 495 cabal install regex-compat 496 cabal install missingh 497 cabal install scion-browser 498 cabal install zlib 499 cabal install zlib --reinstall 500 cabal install quickcheck 501 history | grep cabal
Please check the Eclipse error log and the output in the scion-server console. Report any findings to the Help forum: https://sourceforge.net/projects/eclipsefp/forums/forum/371922
Sorry, not sure. I don't have a Mac... One solution would certainly be to wipe the Haskell Platform and reinstall, but you're probably better off asking on the haskell-cafe mailing list, the community is friendly and some OSX users will in all likelihood help you.
Someone should just to get Conal working on GUIs again! (I'm not the right someone though)
 $ cabal update Downloading the latest package list from hackage.haskell.org $ cabal install scion-browser cabal: There is no package named 'scion-browser'. You may need to run 'cabal update' to get the latest list of available packages. What is going on?
Well, the key is that it's not measured in integers. So the point (0,0) isn't necessarily talking about a specific pixel, but rather to a point on the coordinate plane. So I think it's correct to talk in terms of the distance being 500 by 500, and the coordinates ranging from -250 to 250. In integer pixel terms, the 250 and -250 coordinates are, in a sense, half off the screen.
Much appreciated you beautiful bastard. Shell pastable: cabal update cabal install uniplate cabal -v install uniplate cabal -v install syb cabal -v install uniplate cabal install unordered-containers cabal install hashtable cabal install hashable cabal install text cabal install deepseq cabal --reinstall install deepseq cabal install text cabal install hashable cabal install unordered-containers cabal install scion cabal install hslogger cabal install mtl cabal --reinstall install mtl cabal install transformers cabal --reinstall install transformers cabal --reinstall install mtl cabal install hslogger cabal install network cabal install parsec cabal install parsec --reinstall cabal install network cabal install hslogger cabal install scion-browser cabal install missingh cabal install hunit cabal install missingh cabal install regex-compat cabal install regex-posix cabal install regex-base cabal install regex-base --reinstall cabal install regex-posix cabal install regex-compat cabal install missingh cabal install scion-browser cabal install zlib cabal install zlib --reinstall cabal install quickcheck 
The install is completely broken. Started from the JEE version. Clean install. Then installed Haskell platform. Then gotthis error: &gt;cabal: Error: some packages failed to install: scion-browser-0.1 failed during the building phase. The exception was: ExitFailure 1 This wasn't ready for release.
Of course, Scion should build on ghc7. It should build with the GHC bundled with the latest Haskell Platform. I think it doesn't build on 7.2 because some dependent libraries are not building on it yet. What errors are you getting? Post them on the EclipseFP sourceforge Forum.
Interesting, but also disappointing. What I love about Haskell is that a good program is simply a minimal list of things that are true. When you have written enough equalities to express your original problem in terms of library functions, your program is done. A program is a proof. The result is beautiful and timeless. The fact that the resulting proof can be "executed" by a computer seems a happy accident. When you complicate your program in anticipation of runtime efficiency, you violate that beauty, and you risk that timelessness. To make such changes is to assume that what is inefficient today will also be inefficient tomorrow. Partial function application and liberal use of `seq` make programs harder to understand. Simple proofs become complicated jerry-rigs. You stop focusing on the *what*, and start worrying about the *how*, and that way procedural madness lies. I realize that this is an impractical dream. In the real world, such compromises are unavoidable, and the optimization shown in this article will be useful. Programmers make such trade-offs in every language, but in Haskell they are especially painful to accept.
The examples weren't great. f x y = sqrt x + y result = let f_1 = f 1 in f_1 2 + f_1 4 will also compute sqrt 1 only once, but it is clearer what f is doing.
I'd see it differently. It's great to be able to have a single language in which we can both write beautiful code and also write programs that are fast. When thinking about performance then thinking about evaluation is essential: the number of times things are evaluated (sharing) and the order (lazyness, seq etc). It's fine to ignore evaluation and just think about what is being calculated, but as soon as you care about time and space performance then you have to think about evaluation. This is not a bad thing! If you don't care, you don't have to care.
In fact, I must have missed the point of the article, because the use of currying removes the need to return explicit lambda functions. Doesn't it?
For every elegant and transparent functional algorithm that is slow there exists an equivalent inelegant and opaque algorithm that is faster. -- The Functional Programmer's Book of Proverbs
Scion doesn't build with 7.2 because of MissingH. I've been told it's fixed in the last version, but it's not uploaded to hackage yet.
Thanks for the plug. BTW, my last name ends in two "t"s.
That will call sqrt twice.
OK, I checked and you're right. In GHCi: let fsqrt x = unsafePerformIO (print "done" &gt;&gt; return (sqrt x)) let f x y = fsqrt x + y let result = f_1 2 + f_1 4 where f_1 = f 1 let g x = let sqrt_x = fsqrt x in \y -&gt; sqrt_x + y let result2 = let g_1 = g 1 in g_1 2 + g_1 4 evaluating we get: &gt; result "done" "done" 8.0 &gt; result2 "done" 8.0 I guess it really is tricky. I read "Assume no advanced optimisations - these often break down on larger examples," but didn't think I was assuming anything fancy. I'd be interested to hear why my example at the top of this thread evaluates f_1 twice from someone who knows more about this sort of thing.
That's the compiler's job. If haskell can't optimize this simple case of pure functions, then well.. there's some low hanging fruit right there (?).
&gt; When thinking about performance then thinking about evaluation is essential: the number of times things are evaluated (sharing) and the order (lazyness, seq etc). When I think of considering how many times things are evaluated in my code, I'll often do something like the example in 'step 2': result = let f_1 = f 1 in f_1 2 + f_1 4 In this way I am both not repeating myself (DRY) and (in my mind) expressing something about how many times things should be evaluated. However when Neil Mitchell says that in order for the above to be more efficient we have to define 'f' as: f x = let sqrt_x = sqrt x in \y -&gt; sqrt_x + y rather than: f x y = sqrt x + y ...it seems like that flies in the face of what would expect from haskell's semantics and the equivalence of curried / partially-applied functions. f x y = x + y f x = (x +) f = (+) Why can't the compiler optimize these pure functions into a cascade of let / lambda expressions?
Well, how many times does sqrt get called in this example? #include&lt;stdio.h&gt; #include&lt;math.h&gt; float f (float x, float y) { return sqrt(x) + y; } int main () { printf("%f\n", f(2,3) + f(2,4)); return 0; } On the other hand, the variant that computes the square root only once might look vaguely like this: #include&lt;stdlib.h&gt; #include&lt;stdio.h&gt; #include&lt;math.h&gt; struct g_closure { float (*pf)(struct g_closure*, float); float sqrt_x }; float g_pf (struct g_closure * closure, float y) { return closure-&gt;sqrt_x + y; } struct g_closure * g (float x) { struct g_closure * result = malloc(sizeof(struct g_closure)); result-&gt;pf = &amp;g_pf; result-&gt;sqrt_x = sqrt(x); return result; } float apply_g_closure (struct g_closure * closure, float y) { return (*(closure-&gt;pf))(closure, y); } int main () { struct g_closure * closure = g(2); printf("%f\n", apply_g_closure(closure, 3) + apply_g_closure(closure, 4)); return 0; } The optimizations being referred to is let-floating, and there is a good chance that you'll see that sqrt is only computed once in your example if you compile the code `-O`. Unfortunately, I don't know of a reliable and nice way of ensuring re-computation.
By the way, you can just use Debug.Trace to do that unsafe stuff more cleanly in future :) It's basically `unsafePerformIO` of `putStrLn` or `print` in a simple wrapper.
&gt;The verbose name-spacing required is an in-your-face, glaring weakness telling you there is something wrong with Haskell. This issue has been solved in almost every modern programming languages, and there are plenty of possible solutions available to Haskell. *facepalm*
&gt; it seems like that flies in the face of what would expect from haskell's semantics and the equivalence of curried / partially-applied functions. I don't know why you think this. Of course these things are equal (in the sense of giving the same answer) but the different ways of writing it give you different sharing properties during evaluation. One is not obviously universally better than the other. Sometimes you can make things go faster by sharing more, sometimes by sharing less. You get to decide based on your knowledge of what you're doing. GHC deliberately does not do many transformations that change sharing properites, precisely because it can make things slower in some cases (e.g. retaining stuff in memory you were not expecting).
A couple comments, which I'll make here because I have the feeling more people will see them and correct mistakes I've made... &gt; There is also a new attempt to give Haskell a more reliable interface than just version numbers. I think of this as focused on preventing installations that fail to compile, whereas I view dependency hell as mostly being about failing to configure and start the installation. I think that's not accurate. One of the goals of this new interface should be to say something like "I depend on the existence of the following exported symbols and types. If they are there for some version of the package, then let it satisfy this dependency". There does need to be a mechanism to still specify lower (and even upper, though hopefully not often) bounds by version number in case there are known bugs in some versions... but I'd hope that if this package specification language is adopted, we'd move *away* from the expectation that people would put upper bounds on their `Build-depends` fields. That would solve a good number of these problems. There's still a very tough problem about whether this can be done cleanly without a cure-worse-than-the-disease problem of needing to write out very verbose and detailed specifications of things you depend on, which the compiler could have inferred for you... &gt; Template Haskell (TH) that performs IO does not automatically recompile itself when it should. I wrote a GHC feature request related to this some time ago, at http://hackage.haskell.org/trac/ghc/ticket/4900 It's not as magical as what the article is suggesting, but it would at least let you obtain the desired behavior (by adding a pragma).
The reason fsqrt is applied twice can be understood based on the lambda form of the functions f and f_1. f = \x -&gt; \y -&gt; fsqrt x + y f_1 = (\x -&gt; \y -&gt; fsqrt x + y) 1 which simplifies to f_1 = \y -&gt; fsqrt 1 + y The compiler may be reluctant to simplify further by evaluating fsqrt 1 at this point. Some functions will blow up or fail to terminate, and it's a pretty fancy job to apply a function and watch out for those possibilities. Maybe the result depends on the target computer's math library. In any case the compiler is free to not optimize this to f_1 = \y -&gt; 1.0 + y
Type directed name resolution is a big mistake, in my opinion. Other languages that "solve" the namespace problem do so at the expense of type-inference. When you repeatedly state the type of your records, sure the compiler can easily namespace accesses in that type. Haskell has name-directed type resolution (type inference). If it also gets generalized type-directed name resolution, the two nicely-distinct simple phases are meshed into one complicated phase. EDIT: Reddit discussion makes me retract my statement: I now think it may be a net positive if done well.
You do have to care when the API becomes bigger/uglier for performance reasons, though. Perhaps there should be a convention to separate the beautiful (but perhaps slower) API from the fast one.
The problem is that there's no clear optimum here. It's a time/space trade-off, and without some runtime profiling or a "Sufficiently Smart Compiler (TM)", it is not possible to determine (in the general case) which is more optimal. It also depends on things like the scarcity of memory space, cache miss costs, computation costs, etc which vary from hardware to hardware.
And then there's the file-location package for even nicer traces (as well as traceutils, which I released just before I heard of file-location..)
Greg Weber here- I wrote the article, although Michael's perspective is similar. Thanks a lot for the GHC ticket. I will add my thoughts to it. I don't think it is going quite far enough in that one would have to indicate a file in 2 places- I would really like a solution where the file can be indicated in just one place. I am not expecting GHC to necessarily implement a loadQQ function, but just some primitive that would allow us to write a loadQQ function. Glad to know that module improvements could help solve dependency hell. Although I do worry that we could end up in the dangerous position of expecting it to solve all our problems and stall efforts on other fronts, particularly if it takes a long time to implement or the solution ends up not being one that is easy for the programmer to work with.
Here's an argument by reduction that type-directed name resolution doesn't interfere with type inference. For every field name 'foo' used in a record type, write a type class: class HasFoo r f | r -&gt; f and for each record that uses that name, write an instance: instance HasFoo RecordType FieldType Clearly this can be type-checked, since it's valid Haskell with MPTCs and fundeps. Now, after type inference, if there are any types with HasFoo still in their context, generate an error. Voila, you've got valid type inference with type-directed name resolution. There are good reasons this wouldn't be an ideal implementation, I suppose... but it demonstrates that type-directed name resolution doesn't conflict with type inference.
This will lead to ambiguous types all over the place -- you're going to need to give up on a lot of automatic inference and specify type annotations everywhere. So you'll just be trading the namespace problem for lots of manual annotations.
The TDNR proposal will only add the TDNR step to the locations in the program where the programmer chooses to use it. If it is in fact disadvantageous in this regard then I would expect it to be used mostly just for records.
The slides are available as well: http://justtesting.org/video-and-slides-of-data-parallelism-in-haske
&gt; I don't think it is going quite far enough in that one would have to indicate a file in 2 places Sure, the alternative of course would be that TH's Q monad would get something like "addDependentFile" that would write info into the result (presumably the hi file?) indicating that this relationship exists. I'm not an expert on GHC's current build system, so I'm not sure if it reads the .hi or .o files when making the build plan, or just the source file imports. If it does the latter, then exposing that in TH does seem like the better way to go.
I'm not convinced that ambiguity will be a very big problem. You might have to throw in some tweaks -- for example, add something like the monomorphism restriction for HasFoo contexts, so refuse to generalize them even if you can. But then you'll have one of: (a) a term that's ambiguous because it's unused or exported without a top-level type signature, *and* only uses these type-directed operations, (b) a term that is used with precisely one type, so will be assigned that type, or (c) a term that's actually used with multiple types, and you can complain about ambiguity. Of course, one would need notation for *intentionally* exporting a type-dependent name, but this is rather the whole point, so it can't reasonably be counted as ambiguity.
I imagine that every time you use: foo . bar . foo $ x You're going to get ambiguous types at the "intermediate" positions, at least if `foo` and `bar` belong to multiple records (And the type of `x` is unknown).
as per my comments below, according to the [TDNR proposal](http://hackage.haskell.org/trac/haskell-prime/wiki/TypeDirectedNameResolution) there would be no TDNR being used in that code. Only when the programmer uses the TDNR dot operator.
I can't find it now, but this reminds me of a post I just saw a few days ago representing integers as the difference of natural numbers. Anyhow, very nice presentation. Thank you.
I recently installed the vrapper plugin. I haven't used it much, so I can't comment too much, but it seems like the best approach. You get code completion in insert mode and normal vim like behavior outside. It's not actually vim, so some features are missing, but that also seems to mean there are less bugs. I'm not an expert on this though, especially since my vim-fu is weak, there may be some advanced vim features that are missing. Link: http://vrapper.sourceforge.net/home/
I was getting this error too (on Windows). For me the problem was some arbitrary precision library: Loading package double-conversion-0.2.0.0 ... can't load .so/.DLL for: stdc++ which failed. This was fixed by following the faq here (third question): http://eclipsefp.github.com/faq.html Or here: https://github.com/mailrank/blaze-textual#readme Basically you reinstall the library using the haskell version (which is aparently 10x slower and so not default). Bugs in ghc or something. It worked like a dream after that, my only real issue is that the hoogle search feature has to wait for the database to load every time I open eclipseFP (around half a minute), and that if I try to use it before it's been loaded it can cause the entire eclipse to hang. Feels like the program is waiting for the hoogle database to populate before returning control of eclipse to me, which seems wrong somehow. 
Right, given any commutative and associative operation on a set (it has to be cancellative too, if you don't want to lose some elements in the process), you can always complete it as a group, by considering equivalence classes of formal differences, where (a - b) ~ (c - d) whenever there exists a k such that a+d+k = b+c+k. This is the so-called universal group or Grothendieck group. If you consider addition on naturals, then the universal group is the integers, and its elements are equivalence classes of formal differences of naturals. Similarly, if you then consider non-zero integers with multiplication, the universal group is the non-zero rational numbers, whose elements are equivalence classes of formal quotients of integers. This here is somewhat different though (and cool!), in that there are unique representations for every rational.
I'd like to note that this release is **truly fantastic**. Had to dig myself through the great new [FAQ](http://eclipsefp.github.com/faq.html) and [JPMoresau's hints](http://jpmoresmau.blogspot.com/2011/08/hoogle-command-line-on-windows.html) -- all "issues" were definitely not in the scope/range of the EclipseFP authors but should generally be pushed to the respective projects and e.g. Haskell Platform installer (to get e.g. the MinGW wget/gzip/tar; I had to remap my PATH because I had a wget from node.js not working for EclipseFP).
Thanks, I got it sorted in the end (ghc-pgk unregister worked) but some other difficulties came up: I'm not sure if this is a bug or just user error but I want to point eclipse at an existing package called netwire which has a netwire.cabal file and its source code at the top level instead of in src/. I couldn't seem to get it to work, and therefore eclipseFP was pretty much unusable because it didn't pick up any of the language directives in the cabal file. BTW - I'll cross post all this to the haskell-cafe as you suggest
Regarding cabal's limitations and dependency hell, one solution is to use cross-language packaging. Large companies like Google and Amazon use in-house package managers, and open source distributors of course rely on apt, yum, ports, etc. There are also open source package managers that don't need root permissions. Nix is a good example: http://nixos.org/nix/ See an overview of Google's approach at http://google-engtools.blogspot.com/2011/08/build-in-cloud-how-build-system-works.html
Nice exposition. I automatically reach for pairs to represent rationals, so this is a handy reminder that there are other ways.
Please don't prefix your record selectors with the data type name. I think most people don't these days, though some older packages still do. Instead, put your records in different modules. If you need to use a single one, you can import unqualified. If you have to use multiple with clashing selectors, import them qualified. This way you can even choose a shorter, less verbose way to access them. Of course if the selectors have the same name because they access the same kind of 'thing', the correct thing to do might be to create a type class for it. However, that leads to more complexity, which might not be what you want for an API.
Unfortunately, reliably lazy addition and multiplication aren't possible with this representation. Here's why: - Consider an expression of the form `S x * R y`, which works out to mean `(1 + x) / (1 + y)`. In the case when `x` equals `y`, everything cancels out and the result is simply `U`. Since every number is known to have a unique representation, and the result isn't guaranteed to have `S` or `R` as the outermost constructor, we can't be lazy here. - Consider an expression of the form `R (S x) + R (R y)`, which works out to mean `1 / (2 + x) + (1 + y) / (2 + y)`. In the case when `x` equals `y`, we can add directly to get `(2 + x) / (2 + x)` which again cancels to give simply `U`. The worst case scenario is when you have infinite terms that *are* equal, since it can't know that and will keep looking to see if they ever differ. Nor is this behavior "reasonable"; while `zero * infinity` is guaranteed to diverge immediately upon even cursory inspection, and perhaps rightly so, `(1 + phi) * phi` is certainly not problematic. For both operations, only slight differences at the innermost constructor can give vastly different results. This shouldn't be too surprising, because the size of the representation corresponds poorly to the size of the number, but is proportional to the size of the numerator and denominator in reduced form. So `30/31` is very, very different from `31/31`. To demonstrate, using a condensed notation: - `8 / 9` = `{RRSSSSSS}` - `1 / 9` = `{RSSSSSSS}` - `1 / 10` = `{RSSSSSSSS}` Some addition: - `8 / 9 + 1 / 9` = `{}` - `8 / 9 + 1 / 10` = `{RRSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS}` 
Isn't the easiest solution to the "record problem" just to state that records introduce a new namespace, where all record constructors can be qualified? What am I missing here?
Use the [sourceforge help forum](http://sourceforge.net/projects/eclipsefp/forums/forum/371922) instead to report problems.
Because of the functional dependency, if x has an unambiguous type, then so does `foo x`. And by extension also `bar . foo $ x` and `foo . bar . foo $ x`. On the other hand, if `x` is unknown, you can still write down the type. With associated types (because they are nicer to read), the type of the expression would be: &gt; f x = foo . bar . foo $ x &gt; f :: (HasFoo a, HasBar (FooField a), HasFoo (BarField (FooField a))) &gt; =&gt; a -&gt; FooField (BarField (FooField a)))
Tyr42, I don't know what's going on*. I only know how to make it go away. See above. *I do have a hint of a suspicion. I've got xcode 4.1. There is an update, 4.1.1. Apple tells me I only needed it if I had an error during the installation of 4.1. Perhaps that's related. Other than that, its &lt;invisible &amp;&amp; heavily armed&gt; turtles all the way down.
Ah, OK, they're real numbers. That comes with its own set of subtle problems, of course.
Since no one has mentioned it, I'll point out the obvious fact that these are just finite continued fractions. So the well-know theorems about how to calculate with them might help with some of the difficulties that have been mentioned. (They also provide much shorter proofs for the basic facts in the original post.)
I really liked [this comment's solution](http://www.yesodweb.com/blog/2011/09/limitations-of-haskell#comment-302612555) to the namespace problem.
Don't think of the simple example with the sqrt, think of the second complex example with the Table, and with usage all over the place. How can the compiler know if building the table is an expensive one-time case, or if it is something that should be thrown away as quickly as possible?
Yes, you can still get "right-to-left" inference, but you pretty much lose out left-to-right inference, which is one thing I really like about Haskell programming. That type can never be resolved, though, because the result type does not help infer the input types here.
&gt; Yes, you can still get "right-to-left" inference, but you pretty much lose out left-to-right inference, which is one thing I really like about Haskell programming. This situation is no worse than for the `show` function. Given the result type of `show x` it is also impossible to infer the type of the argument. 
I agree that placing records in different modules is better in most cases, but lets be clear that in any object-oriented language and most other modern languages one would not need to artificially place record definitions in different modules and use qualified namespaces whenever selecting a field.
This is possible in ocaml, which has the same issue of record fields escaping record scope. It is still verbose and unwieldy. Perhaps it's better than what haskell has, but I wouldn't consider it a full solution - using records in ocaml is still annoying.
But if you had something like: module Record where data Record = Record { a :: Int } And then used Record.a instead of a it wouldn't escape. Or am I missing something?
Java would not require a programmer to go to these lengths.
I don't follow?
We discussed doing exactly the same thing for request-local state in Snap's new component system for a while, but ended up deciding on a more traditional state-monad approach instead, since we didn't need the persistence stuff. Our application of the idea was to allocate the keys once at initialization time, and then use the same keys in multiple maps: one per HTTP request. I'd definitely stick with the simpler unsafeCoerce implementation, though.
I think this is pretty encouraging - of the six limitations described, two are being actively worked on (stack traces and dependency hell), one is fairly straightforward (TH reloading), one has various solutions and some improvements are available now (code reloading). That leaves two: type error messages and record field name clashing. Like everyone else I have no idea what to do about type errors. For receord fields I completely agree this is an important issue. It's also on my list of "important things to work on sometime". The activation energy is quite high though: there's a lot of previous work on record systems, and it's not clear what combination of features would offer the most bang for buck. Everyone wants different features, and if you miss any out then someone will be disappointed, but at some point someone has to make a decision. But we want it to be a *good* decision...
And the opposite of the `read` function... A lot of the ambiguous types errors I remember getting from GHC indeed involved polymorphic functions like show. However `read` and `show` at least have one of their types fixed. If all record fields introduced polymorphic arguments *and* results, I think it would make ambiguous types significantly more common than even `show` and `read`. After this discussion, though, I'm no longer convinced that this would be a net loss. Manually annotating these ambiguous types may be a smaller price to pay than making all record accessors unambiguous by name.
Hmm I don't use haskell, but from what I read, I guess this would be something like: Record.a x where x is the record? In ocaml the syntax would be x.Record.a. In either case you have to specify the field, the record, and a distinguisher. Modules aren't too bad, they can be aliased, they can be opened, but it's still not as convenient as x.a or a x. The consequence is that records become just a bit more heavyweight, where they're already heavyweight enough as it is. In ocaml there are objects, and with an object you can do this: x#a, and it will infer the type of x to be &lt; a: &lt;inferred type&gt;; .. &gt; - i.e. an object/record with at least one method/field a of inferred type. however, there's a performance cost to access these fields, and the object syntax is also pretty heavyweight when all that's desired is a simple record. This is not so much a theoretical problem as it is an annoyance in practice. And perhaps other programmers don't find it as annoying as I do. But it annoys me enough that I find myself renaming fields to avoid putting them in separate modules, or avoid records where they'd otherwise be useful because accessing fields takes up too much of my screen. Not all the time, of course - it is a reasonable fallback. One thing is, that it's not unusual to find yourself wanting to access a record in a module nested in a module - and then your syntax is x.Module.Record.a or Module.Record.a x. Whereas in typical imperative/OO languages, it'd be x.a, no matter where the record was defined. The other issue with records is that they can't be defined at point of use - they must be predeclared. This means that in sequencing of small transformations of data, records are too unwieldy to use, which means the programmer (i.e. me) defaults to tuples, which can lose some readability if the amount of data is more than 2 or 3 fields. Again, objects in ocaml don't have this problem, but they still maintain their performance and construction verbosity problems. If a solution can maintain performance whilst providing this functionality, it would be useful. I believe standard ML has anonymous record functionality, but I haven't tried it.
Okay, thanks. I'm pretty new to the concept of sharing I guess. Is this behavior of 'let' part of the language spec, or just something that GHC does? Is there some place I can go to get a handle on how this all works? It seems like there should be some more explicit way of suggesting to the compiler that some data should stick around, like we have with seq. But maybe I just have to educate myself a bit more.
Here's a small idea for type errors: would it be possible to have a pragma to set a canonical type alias? For example, in blaze-html we have: data HtmlM a type Html = HtmlM () It can be very confusing for someone who doesn't know the internals of blaze-html to see "HtmlM ()" in their error messages. Then there are more complicated ones; in hamlet we have: type HtmlUrl url = (url -&gt; Text) -&gt; Html Just a small thought.
There is a very simple solution for the "Error messages from Complex types" problem: don't use overly complicated types. No one will ever prevent you from using some complicated type hackery deep inside your library to ensure some invariant or to ensure reusability, but just try to keep them away from your API. A great way to reduce complexity is to avoid type classes. For example, the HTTP package has a type class to parametrize over the result type of responses. This type class makes the documentation of the API far less readable, makes errors more complicated and has only three instances: String and strict + lazy ByteStrings. Why not always use lazy ByteStrings internally and provide some custom accessors like 'decodeAsString', 'decodeAsText', etc. My experience is that introducing type classes for me as a programmer is always very easy, but understanding why some other author uses them is way harder. In my opinion type classes should have a far more limited scope when it comes API design.
I didn't completely understand your response since I haven't specifically studied this specific subject yet (this semester actually) but one thing I wanted to point out is that in this representation, you get duplicate elements, but then I was thinking if we were to implement it in a dependently typed language (Coq or Agda), you could index the type by a proof obligation that the number is normalised. That way "normal integers" do have unique representations because they come with a proof that they are normalised. On the other hand, this introduces a bit of boiler plate code in functions manipulating them but I was thinking with a bit of cleverness that could easily be hidden, especially with something like Agda's new [instance arguments](http://people.cs.kuleuven.be/~dominique.devriese/agda-non-canonical-implicits/). In the meantime, I'll be reading up on the things you mentioned. Thanks.
shift and reset aren't really needed. If you inline them the make_zipper function can be simplified: import Data.Traversable import Control.Monad.Trans.Cont data Zipper t a = ZDone (t a) | Z a (Maybe a -&gt; Zipper t a) make_zipper :: Traversable t =&gt; t a -&gt; Zipper t a make_zipper t = runCont (traverse (\a -&gt; cont (\k -&gt; Z a (k . maybe a id))) t) ZDone Note that traverse also works instead of mapM, i.e. we only need the applicative instance of Cont. (Which is provided by the transformers package).
See the [new ThreadScope homepage](http://haskell.org/haskellwiki/ThreadScope) on Haskell wiki
Has anyone actually met Oleg in real life? I like to think he's a modern, CS version of [Bourbaki](http://en.wikipedia.org/wiki/Nicolas_Bourbaki). 
In Java, I could do: class Foo { Bar bar; } and then later: Foo foo = new Foo(); foo.bar.baz(); No extra namespacing required.
Sorry, what does injectivity mean in this case?
&gt; Injectivity of type constructors means that a type constructor T (defined by a data declaration) gives distinct types when applied to distinct arguments. Contrapositively: T a ~ T b implies a ~ b Agda (the proof language) actually used to have an optional feature that enabled this, but it was found to be anti-classical, meaning you can prove things that are not true in classical logic.
Interesting, okay, thank you. 
In my book, that looks like induction. Or, rather, the axiom that all possible Ts are closed under inductive type equality, which my intuition assures me is useful for general sanity, but then I'm just a random hacker.
But how is this different from enforcing methods must be tied to a class for example? Especially when the language enforces one class per file? If anything I see the Haskell system as more flexible.
Many of us have met him. He is a very colorful character. =)
We seem to have come to similar conclusions- WAI basically does exactly what you are suggesting for the HTTP example, and that works very well for that use case. But we can't achieve the levels of abstraction we want to achieve by completely abandoning more complex types- we have to walk a line. This is something that seems learned through experience and isn't always taught that way. In fact, I think one can easily get the opposite impressions (at least with respect to using polymorphism). But the issue is even more nuanced- we have realized there are good and bad forms of polymorphism (for an API). For example, our templates have to be converted to widgets. A template is not specific to Yesod, and there are different template types depending on what substitutions are required in the template. Widgets are a Yesod specific construct that knows how all of the html,css,and javascript templates should be combined and placed on the page or otherwise sent to the client. We used to have one hamlet template function that could return different types that were all Widget instances. Now we have different functions for each of the different return types. However, even though the return type is explicit for the hamlet functions, the returned value is a member of a type class ToWidget so that the user can call toWidget on the returned value.
There is an alternate presentation of shift and reset using `Cont`. It looks like this: `reset` takes a computation with control effects and delimits it so that no effects can escape. This means it can yield a pure value, so: reset :: Cont a a -&gt; a reset m = runCont m id `shift` captures a continuation, but the continuation it captures is a pure function that has no control effects. Further, it delimits the function that it passes the continuation to, so no control effects can pass out of it, either. This gives the type: shift :: ((a -&gt; r) -&gt; r) -&gt; Cont r a shift = cont Using this formulation, your `make_zipper` is using `shift` just like Oleg's, and the only difference is that you used: runCont e f instead of: reset $ f &lt;$&gt; e but those are of course equivalent. The delimited continuations Oleg is using are almost the same as this (same reset), but the shift does not capture all the invariants it could using the type.
First of all, not all OO languages force one class per file- but Haskell does force one Record per file (to be sure to avoid clashes). OO language: class Record {a::String} class Clash {a::String} r = Record.new("ONE") clash = Clash.new("CLASH") r.a clash.a Haskell module Clash -- separate file data Clash = Clash {a::String} end module Record -- separate file import qualified Clash data Record = Record {a::String} -- this is fine r = Record {a = "ONE"} a r -- Haskell gets whipped by OO :) clash = Clash.Clash { a = "CLASH" } Clash.a clash 
see my comment to neitz below. It isn't just the constructor, but every field access that must be namespaced.
My unenc function terminates for all positive rational numbers, and I believe you'll find it exactly mirrors the enc function. import Data.Ratio data PRat = U | S PRat | R PRat deriving (Eq, Ord, Show) enc :: Fractional a =&gt; PRat -&gt; a enc U = 1 enc (S x) = 1 + enc x enc (R x) = 1 / ( 1 + enc x ) unenc :: Rational -&gt; PRat unenc 1 = U unenc x | x &gt; 1 = S $ unenc (x - 1) | x &gt; 0 = R $ unenc (n' % d') where d = denominator x n = numerator x d' = n n' = d - n failures :: Integer -&gt; [Rational] failures lim = [ n % d | n &lt;- [1..lim], d &lt;- [1..lim], enc (unenc (n % d)) /= (n % d)] 
Keep in mind that the T type might not use its type parameter, and the type could even be empty. Equality of types is also a fairly complicated issue, in general, and there isn't always a clear right answer.
Actually, it turned out that I didn't need the persistence stuff either. But the conceptual simplification brought by a first-class store was extremely useful and it felt quite liberating to eradicate a whole lot of IORefs.
The high activation energy is why I tend to favor TDNR as an immediate and easier way to have a good-enough solution to records right now that won't stop anyone from implementing a better long term record solution. I would like to see even greater experimentation with compiler pragmas/support for custom error messages than just what Michael is suggesting. In Yesod we have our own build command that calls out to cabal build- mostly to support external templates and code re-loading (which is really just automatic re-building). I want to integrate a service that will automatically send compiler error messages to a website. That site gives you easy access to a url to send to others for help, like hpaste. But solutions to the errors can also be reported (perhaps somewhat automatically also). That way it is possible for the build command to suggest solutions for a compiler error you receive if a similar one is found that already has a solution. Other than solutions, it would also give some interesting insight as to what problems are most often being encountered.
The scion and scion-browser packages are bundled with EclipseFP and not uploaded to hackage. Look inside the .metadata/plugins folder for the extracted source code. This is to ensure that the Java and the Haskell code are in sync, but of course these packages could/should be available from hackage too.
care to report the errors you got before that line to the Help forum? We can try to work out a solution...
The update of the Hoogle database should not cause Eclipse to hang, we've tried to make it run asynchronously in an Eclipse job, but...
Damn you Russell! *shakes fist*
 Reading available packages... Resolving dependencies... In order, the following would be installed: scion-browser-0.1 (new package) Configuring scion-browser-0.1... Warning: 'hs-source-dirs: test' directory does not exist. Dependency Cabal ==1.10.1.0: using Cabal-1.10.1.0 Dependency HTTP ==4000.1.1: using HTTP-4000.1.1 Dependency MissingH ==1.1.0.3: using MissingH-1.1.0.3 Dependency aeson ==0.3.2.11: using aeson-0.3.2.11 Dependency attoparsec ==0.9.1.2: using attoparsec-0.9.1.2 Dependency base ==4.3.1.0: using base-4.3.1.0 Dependency bytestring ==0.9.1.10: using bytestring-0.9.1.10 Dependency cereal ==0.3.3.0: using cereal-0.3.3.0 Dependency containers ==0.4.0.0: using containers-0.4.0.0 Dependency deepseq ==1.1.0.2: using deepseq-1.1.0.2 Dependency derive ==2.5.4: using derive-2.5.4 Dependency directory ==1.1.0.0: using directory-1.1.0.0 Dependency filepath ==1.2.0.0: using filepath-1.2.0.0 Dependency ghc ==7.0.3: using ghc-7.0.3 Dependency ghc-paths ==0.1.0.8: using ghc-paths-0.1.0.8 Dependency haskeline ==0.6.4.3: using haskeline-0.6.4.3 Dependency haskell-src-exts ==1.11.1: using haskell-src-exts-1.11.1 Dependency mtl ==2.0.1.0: using mtl-2.0.1.0 Dependency parallel-io ==0.3.2: using parallel-io-0.3.2 Dependency parsec ==3.1.1: using parsec-3.1.1 Dependency process ==1.0.1.5: using process-1.0.1.5 Dependency tar ==0.3.1.0: using tar-0.3.1.0 Dependency text ==0.11.1.5: using text-0.11.1.5 Dependency utf8-string ==0.3.7: using utf8-string-0.3.7 Dependency zlib ==0.5.3.1: using zlib-0.5.3.1 Using Cabal-1.10.1.0 compiled by ghc-7.0 Using compiler: ghc-7.0.3 Using install prefix: C:\Users\username\AppData\Roaming\cabal Binaries installed in: C:\Users\username\AppData\Roaming\cabal\bin Libraries installed in: C:\Users\username\AppData\Roaming\cabal\scion-browser-0.1\ghc-7.0.3 Private binaries installed in: C:\Users\username\AppData\Roaming\cabal\scion-browser-0.1 Data files installed in: C:\Users\username\AppData\Roaming\cabal\scion-browser-0.1 Documentation installed in: C:\Users\username\AppData\Roaming\cabal\doc\scion-browser-0.1 Using alex version 2.3.5 found on system at: C:\Program Files (x86)\Haskell Platform\2011.2.0.1\lib\extralibs\bin\alex.exe Using ar found on system at: C:\Program Files (x86)\Haskell Platform\2011.2.0.1\mingw\bin\ar.exe No c2hs found No cpphs found No ffihugs found Using gcc version 4.5.0 found on system at: C:\Program Files (x86)\Haskell Platform\2011.2.0.1\mingw\bin\gcc.exe Using ghc version 7.0.3 found on system at: C:\Program Files (x86)\Haskell Platform\2011.2.0.1\bin\ghc.exe Using ghc-pkg version 7.0.3 found on system at: C:\Program Files (x86)\Haskell Platform\2011.2.0.1\bin\ghc-pkg.exe No greencard found Using haddock version 2.9.2 found on system at: C:\Program Files (x86)\Haskell Platform\2011.2.0.1\bin\haddock.exe Using happy version 1.18.6 found on system at: C:\Program Files (x86)\Haskell Platform\2011.2.0.1\lib\extralibs\bin\happy.exe No hmake found Using hsc2hs version 0.67 found on system at: C:\Program Files (x86)\Haskell Platform\2011.2.0.1\bin\hsc2hs.exe No hscolour found No hugs found No jhc found Using ld found on system at: C:\Program Files (x86)\Haskell Platform\2011.2.0.1\mingw\bin\ld.exe No lhc found No lhc-pkg found No nhc98 found No pkg-config found Using ranlib found on system at: C:\MinGW\bin\ranlib.exe Using strip found on system at: C:\MinGW\bin\strip.exe No tar found No uhc found Creating dist\build (and its parents) Creating dist\build\autogen (and its parents) Preprocessing library scion-browser-0.1... Preprocessing executables for scion-browser-0.1... Preprocessing test suites for scion-browser-0.1... Building scion-browser-0.1... Building library... Creating dist\build (and its parents) C:\Program Files (x86)\Haskell Platform\2011.2.0.1\bin\ghc.exe --make -package-name scion-browser-0.1 -hide-all-packages -fbuilding-cabal-package -i -idist\build -isrc -idist\build\autogen -Idist\build\autogen -Idist\build -optP-include -optPdist\build\autogen\cabal_macros.h -odir dist\build -hidir dist\build -stubdir dist\build -package-id Cabal-1.10.1.0-55f781465ee9f32289755ad706c71f0f -package-id HTTP-4000.1.1-fb2f8cf8c7cbbc826ae834d203203d50 -package-id MissingH-1.1.0.3-f3c72f39dd967a6c9ca8a7cdd1ccc970 -package-id aeson-0.3.2.11-70be5cfd6b182a8f975c1e291d6a8dc0 -package-id attoparsec-0.9.1.2-e531b388d7d9753927e91355fc0c4e64 -package-id base-4.3.1.0-f520cd232cc386346843c4a12b63f44b -package-id bytestring-0.9.1.10-cd85f14e02463c02ba4c77d7adcdb54f -package-id cereal-0.3.3.0-81dac2655ecff64c5fc69671490df093 -package-id containers-0.4.0.0-18deac99a132f04751d862b77aab136e -package-id deepseq-1.1.0.2-09b3aed0c4982bbc6569c668100876fa -package-id derive-2.5.4-248be7e3458381daf8d7d99666d84cad -package-id directory-1.1.0.0-3a2367d72569467a8af8a231656ff1b8 -package-id filepath-1.2.0.0-f132e9f7703da4e20a47ff2b9acf1ea1 -package-id ghc-7.0.3-fc75cf67c86ba2c0d64b07024a18d3b4 -package-id ghc-paths-0.1.0.8-c46da8d4888c8620aaa9bb9968da3c38 -package-id haskeline-0.6.4.3-d9ce216ce91763ff70f5a6f31be7b6df -package-id haskell-src-exts-1.11.1-0c57f18e2d403c3c65ecd6ded277f223 -package-id mtl-2.0.1.0-fb4d1695269b74308c1517c06b76e6e8 -package-id parallel-io-0.3.2-ccf4a53a58035b42192f0769ae4b2fa9 -package-id parsec-3.1.1-9da59b00c02e52f9f62f3c87c7e0e2ec -package-id process-1.0.1.5-b3dded8e54a2e13d22af410bdcfafff4 -package-id tar-0.3.1.0-8e2eb74010dd0c2e35925d09c3062dac -package-id text-0.11.1.5-9cb8ded0b18a7122d0250edf8f673a6f -package-id utf8-string-0.3.7-dcbcb8de655cdfe1c4c28c094f8c6799 -package-id zlib-0.5.3.1-5fbdf714525b76e0e601c2ffb25f2044 -O -rtsopts -Wall -fno-warn-unused-do-bind -fno-warn-orphans -threaded -XHaskell98 Scion.Browser Scion.Browser.Query Scion.Browser.Build Scion.Hoogle Scion.Browser.Instances.Json Scion.Browser.Instances.NFData Scion.Browser.Instances.Serialize Scion.Browser.Parser.Documentable Scion.Browser.Parser.Internal Scion.Browser.FileUtil Scion.Browser.Parser Scion.Browser.TempFile Scion.Browser.Types Scion.Browser.Util Scion.Hoogle.Instances.Json Scion.Hoogle.Parser Scion.Hoogle.Types Scion.Hoogle.Util Scion.Packages src\Scion\Browser.hs:14:8: Could not find module `Data.Serialize': There are files missing in the `cereal-0.3.3.0' package, try running 'ghc-pkg check'. Use -v to see a list of the files searched for. cabal.exe: Error: some packages failed to install: scion-browser-0.1 failed during the building phase. The exception was: ExitFailure 1 Before doing this, I deleted the cabal folder. Uninstalled Haskell Platform. Downloaded Haskell Platform again, and then reinstalled. Deleted eclipse folder. Copied JEE eclipse from archive to C:\eclipse. Ran eclipse. Added update for eclipsefp, and then ran the update. Restarted, and got the error message. That error message can be fixed by running cabal install cereal --reinstall, but then I get another message for something else, and so I would end up having to do this a couple hundred times. Also, the previous release of eclipsefp worked without any problems.
Yeah, I tried that. It just ends up in a seemingly infinite regress of reinstalling dependencies.
wow... must have hit a nerve.
I love the images. anyway: http://hackage.haskell.org/package/djinn
damn, I want his autograph on the HList paper...
Very approachable and helpful too, which might seem strange to the people who take part in that silly lionisation thing. 
I have some ideas how error messages could be improved, but there's one huge problem. A statement like "error messages are bad" is extremely subjective. It would be nice if you could start a wiki page with examples of error messages and a short description of what the actual problem was. It should be possible to sort these into different classes of errors and then look into addressing one class of error at a time (or devise strategies that would work for multiple errors).
see my comment below to simonmar for a way we could try to address errors this in a more objective and systematic way. I think the compiler error messages are often as good as they can be. What I would like to see is a way for the programmer to somehow be able to provide additional specific error information. This seems like a hard problem to solve, but seems like a great problem for a language like Haskell with a strong connection to academia.
I'll just leave this here, https://lists.chalmers.se/pipermail/agda/2010/001526.html
You should get 10 frames per second. It's throttled intentionally to save bandwidth by the server. If your internet connection is bad, though, it could end up lower.
The previous version of EclipseFP didn't have scion-browser, which is a new component. I've never seen that error message "there are files missing in the xxx package". There seem to be an infinite number of ways installing an Haskell package can fail... &lt;sigh&gt; We have tested installing EclipseFP (before release) on a clean install of the Haskell Platform on Windows and it installed fine... It did install cereal-0.3.3.0 and had no issue with it.
It could be proved by induction, if `T` were a constructor. However, that would require `*` to be defined by induction, which it arguably isn't, and of course leads to the same paradox in this situation.
Yeah, it'd be the ordinary thing to do. The hoogle search works fine after it's loaded the database, it's only if I make a query while it's loading it that the hang will occur. As far as bugs go it's not enough to stop me using eclipseFP, since I don't really have a need for hoogle within the first 30 seconds of opening up the program. The feature set you've provided is really great, I'm really grateful that you've provided what appears to be a stable enough IDE for Haskell, we've been lacking one since now, in my opinion.
Being able to reason about types like that is very important if you want to get into actual interesting proofs at any point. Djinn can help with simple ones but it won't work for anything actually remotely difficult to prove. Or even things involving higher-rank types in Haskell :)
Pardon my cluelessness, but is this 'inconsistency' more alarming, from a specifically Haskell point of view, than the more familiar data False where absurd :: False absurd = error "I am false" ? It seems his procedure can be extended to yield a divergent term at any type, e.g. it goes through if you substitute `Bool` for his `False`. In that case you of course get that familiar pleasant warm feeling when you ask `ghci` which `Bool` you've defined as `absurd :: Bool` -- the same one you get with, e.g. `fix not` -- rather than an error saying there's no `Show` instance for `False.` In Oleg's case there is the obvious difference that we get a divergent term in a type where there's nothing to 'converge' to -- thus one unlike `Bool` -- but my stupidity is preventing me from being surprised by this; the whole Haskell mechanism sort of leads one to expect it, no?
I'd written something a while ago that seems related to this (if only distantly). It generates functions from a target type signature and a universe of other functions. For example: Lets specify the universe as the following three constructs. a :: Int -&gt; Int a x = (x + 1) s :: Int -&gt; Int s x = (x - 1) zero :: Int zero = 0 And lets make the following type signature our target: target :: Int The program would generate (in this case) an infinite stream of functions using `a`, `s` and `zero` that match the target. Here's an example of the output: a zero s zero a (a zero) a (s zero) s (a zero) s (s zero) a (a (a zero)) a (a (s zero)) ... I've been hoping to expand this program out to include the ability to parse as Haskell module and work with something other than fixed types. The mostly literate code is here: https://github.com/sw17ch/Volcano/blob/master/stash/search.lhs. The rest of the github project is my half-assed attempt to modularize everything and make it into something more usable--currently none of it does anything. I _also_ intend to eventually make a way for the user to specify tests that the resulting functions must pass in order to be considered in the final set. In this way, I was hoping to generate functions from a specification rather than writing the specification and then writing the function. EDIT: Fixed syntax error in definition of `s`.
That's a nive idea, but I'm not sure how applicable it would be. Since GHC doesn't (seem to) try to unify types any further once it meets an impossibility, the types in error messages are often (from my experience) much more polymorphic than you'd expect. This looks like it would prevent a lot of "canonical synonyms" from kicking in. In your example, rather than "(url -&gt; Text) -&gt; Html" error messages will often mention something like "(url -&gt; Text) -&gt; t" or "t -&gt; Html", which do not match the HtmlUrl pattern.
I don't think it's alarming. There are already half a dozen ways to write infinite loops in Haskell, so one more doesn't bother me. I made the same statement back when this stuff was going on on the coq/agda/haskell mailing lists, and Oleg said something to the effect that it'd be nice to not just add more and more ways to sneak bottom in the back door, but I don't think this is particularly damaging, since Haskell isn't aiming to be strongly normalizing. Of course, if you wanted to build a Haskell-alike total language, you'd have to remember all these subtle bugs in the way Haskell (or, extensions thereof) does things, which I suppose is a motivation. The more bottoms you can sneak in this way, the more you have to rip out and rethink, and the less generally applicable work on these combinations of Haskell extensions is.
TDNR? EDIT: Type Directed Name Resolution http://hackage.haskell.org/trac/haskell-prime/wiki/TypeDirectedNameResolution
&gt; `s = (- 1)` Unfortunately, this does not mean what you think it means.
*sigh* I got bit by it again. Edited original post. Thank you.
You may be interested in `subtract 1`.
Even in the situation where a Djinn-like tool cannot hope to solve all problems outright, it might teach us something about how to structure the presentation of proofs. The parts that Djinn can't get for itself are the parts we need to write down. Lambda-calculus is great as a checkable evidence language for proofs, but as a source language for proofs it's dreadful. I'd be very happy to give a high-level proof tree, refining goals to subgoals (stating propositions, not proof terms), and giving key choices of strategy (e.g., which induction) and witnesses (instantiating forall nontrivially), but using a Djinn-like algorithm at each node of that tree to synthesize the actual lambda-term.
for the sake of this post, not really. it doesn't add anything helpful.
Your post didn't hit a nerve, your post didn't hit anything. I have no idea what you were trying to say. I assume it was trying to be snarky, but even that's an assumption, I have no idea what snark was intended.
I really hope your editor supports some kind of macros. On X11 you can do it globally for your entire UI.
I found a solution: * Delete C:\Users\username\AppData\Roaming\ghc * Run ghc-pkg recache * Run eclipse, and wait for double-precision error, and then close eclipse * Run cabal install blaze-textual --user -fnative --reinstall * Run cabal install aeson --user --reinstall * Restart eclipse The only problem is hoogle doesn't seem to be able to download its database.
curiously, `containers` even has `base &gt;=4.2 &amp;&amp; &lt;6, array -any` as its dependancies...
I'm watching this and I right away think of Quicksilver (OS X) and Kupfer (Linux/GNOME) as examples of making composability available to an extent.
 uncurry :: (a -&gt; b -&gt; c) -&gt; (a,b) -&gt; c Apart from that, no.
I can't believe that while you responded I created a function that had the exact same signature and did the same thing :) paramFromTuple f (a, b) = f a b
Its actually not that unusual.
It happens more often than you'd think, which is why it's so handy that you can [use Hoogle to search by type signature](http://www.haskell.org/hoogle/?hoogle=%28a+-%3E+b+-%3E+c%29+-%3E+%28a%2Cb%29+-%3E+c).
For me it is the first time it happened; and that means I'm actually progressing a bit in Haskell :D
kudos!
It's called the standard library for a reason :) But yep, Hoogle is really useful.
Well, it's more useful than most things GHC spits out.
You sure you need that operation?
I found that it very helpful to sit down and read the Prelude docs from beginning to end to see what was already implemented. It took me waaaay too long to clue into this seemingly obvious tip.
Depends. Do you care if you block?
I felt like I just witnessed a miracle.
Should I consider the first argument to `step` to be in seconds?
Then doing isEmptyChan is pointless because some other thread might take all the items out of the Chan in between when you check if it is empty and you actually do the read. This is why using isEmptyChan makes no sense for Chan.
btw, fun a b = a b is the same as ($), i.e. ($) a b = fun a b and thus, your hypothetical \(a,b) -&gt; fun (fromTuple (a, b)) should be the same as uncurry ($) :: (a -&gt; b, a) -&gt; b
unless you have only one thread that reads, and you don't want that thread to block
Reading from a chan never throws an exception, readChan will succeed as soon as there's at least one item in the chan. You need to read the documentation for Chan instead of making assumptions about how it behaves. 
Just something fun, since `($)` is just `id` with a specialized type, we also have uncurry id :: (a -&gt; b, a) -&gt; b
Also, for example in `saveFile`, code that looks like: liftIO a liftIO b liftIO c can be written as: liftIO $ do a b c 
And they say C++ skills don't transfer to Haskell...
Since a number of people seem to have found this useful, I thought I'd submit it here in the hopes of helping other frustrated OS X users (and anyone else wanting to get GHC 7.2 up and running from scratch), and flagging the issues for maintainers.
Indeed. When you read from an empty Chan, the thread will sit and wait until some other thread writes to the Chan. Then the reading thread (or some other thread that is waiting on the same chan) will wake up, read the just written value and carry on.
But it's pointlesser*!*
On this same note, how did he get so good/prolific at this stuff? I mean he has clearly been doing it for years and is very interested in it, but still. Who is Oleg Kiselyov? 
Why put it on Github instead of the Haskell wiki?
The BlockedIndefinitelyOnMVar exception will get thrown if the GC determines that a thread is blocked on an MVar that no one else has a reference to. But there's not really a way to catch it and continue in the same thread.
It's just some personal notes on what I had to do to get things up and runningâthe thought of putting it on the Haskell wiki never occurred to me. Hopefully it won't be required for too long, and I don't want to contribute to wiki bitrot.
Yes, seconds, as a floating point number. The server throttles frame rate to about 10 fps, so it'll usually be in the vacinity of 0.1 unless you have a slow internet connection.
Yep, the error reporting on hoogle failures is not great yet. Does hoogle data work on your machine if you launch it from a console? You may need to install additional utilities on windows (see my [blog entry](http://jpmoresmau.blogspot.com/2011/08/hoogle-command-line-on-windows.html)).
The functional thing will start to click once you struggle through a few problems like the one you posted about. Also I'd direct you to stackoverflow.com, which is a better place for questions and lots of smart nice haskellers ready to answer! For fun, here's one way you could do it foo = foldr [] (\a as-&gt; a : filter (&gt;a) as) and my apologies if the above does not compile. EDIT: Just... ignore me
Try starting out by zipping the list with its own tail, for example: ghci&gt; let xs = [3,1,5,3,6,8,6,5] ghci&gt; zip xs (tail xs) [(3,1),(1,5),(5,3),(3,6),(6,8),(8,6),(6,5)] Now these pairs are possible to compare. We could use a list comprehension like this: ghci&gt; [y | (x,y) &lt;- zip xs (tail xs), x &lt; y] [5,6,8] Which gives the numbers which are greater than their predecessor. Unfortunately, 3 isn't there because it doesn't have a predecessor. However, we can just always put the first element of the list into the result: ascents [] = [] ascents (v:vs) = v : [y | (x,y) &lt;- zip (v:vs) vs, x &lt; y] after which we get: ghci&gt; ascents xs [3,5,6,8] This is what I initially took from what you described, but note that it won't always produce an ascending list, if that's what you were after: ghci&gt; ascents [5,1,2] [5,2] It's also possible that what you wanted was an ascending list consisting of those elements strictly larger than all those seen previously in the list (and not just their own predecessors). There are a few decent ways to accomplish this. We can get a list of the largest element seen so far with: ghci&gt; scanl1 max [3,1,5,3,6,8,6,5] [3,3,5,5,6,8,8,8] and then remove duplicates from that: ghci&gt; group . scanl1 max $ [3,1,5,3,6,8,6,5] [[3,3],[5,5],[6],[8,8,8]] ghci&gt; map head . group . scanl1 max $ [3,1,5,3,6,8,6,5] [3,5,6,8] Or, to be yet more clever, we can use groupBy. It produces nonempty groups which concatenated together give the original list, such that the comparison of the head of each group with each member of the tail succeeds, and such that the list of lengths of the groups is lexicographically maximal (it's greedy about adding things to groups as it goes left to right). Aside: Note that the Report somewhat foolishly neglects to specify the behaviour of groupBy when applied to something that isn't an equivalence relation. But the obvious implementation of it, the one which the Report provides, and the one in every implementation of Haskell that I know about, will do the right thing. So, we can form groups where the first item of the group is larger or equal to all the other members of the group, and then just take the first element of each: ghci&gt; groupBy (&gt;=) $ [3,1,5,3,6,8,6,5] [[3,1],[5,3],[6],[8,6,5]] ghci&gt; map head . groupBy (&gt;=) $ [3,1,5,3,6,8,6,5] [3,5,6,8] Or as pointed out by zuserm below, we can use nubBy to get the same result. Unfortunately, the parameters to the function passed to nubBy are backwards at the moment(!) so we have to use nubBy (&lt;=) rather than nubBy (&gt;=) in ghci: ghci&gt; nubBy (&lt;=) [3,1,5,3,6,8,6,5] [3,5,6,8] You would expect that nubBy apply the comparison operator to the elements of the list in the order in which they appear in the list, but GHC's current implementation doesn't do this. I consider this a bug, myself, though the Report again says explicitly that the behaviour of nubBy for functions that aren't equivalence relations is not defined (which is silly, because it's useful).
Thanks so much for you help! I had only just skimmed over zip because I didn't think to use tupples.
A great place to get help is the #haskell irc channel on irc.freenode.org. Great people and always someone around to help out! As for your question: What you want to do is a pairwise compare of all elements with their predecessor/successor. I am not quite sure whether you want to compare list elements in the original list or after removing elements. filter' xs = take 1 xs ++ (map snd . filter (uncurry (&lt;)) $ zip xs (drop 1 xs)) The first element always fulfills your specification. then you build pairs of adjoining elements, filter them for successor being greater than its predecessor, and throw away the comparation element. If, however, you want to reapply your rules to the new list with an element removed it could look like this: filter' [] = [] filter' (x:xs) = x:filter' (filter (&gt;x) xs) 
You have the arguments to `foldr` the wrong way round. :) A slight improvement to this would be to allow a user-definable predicate and then define the desired function in terms of that. dropMatch :: (a -&gt; a -&gt; Bool) -&gt; [a] -&gt; [a] dropMatch p = foldr (\x xs -&gt; x : filter (p x) xs) [] dropLeq :: Ord a =&gt; [a] -&gt; [a] dropLeq = dropMatch (&lt;=) 
When you have a Haskell function that you want to write, it's always worth searching to see if it's been written before. Haskell has two really nifty search engines Hoogle and Hayoo which let you search by type. Your function has type Ord a =&gt; [a] -&gt; [a] but that doesn't describe what you want very well, and indeed searching for Ord a =&gt; [a] -&gt; [a] doesn't turn up anything useful. There is however an obvious generalisation of your function, what if rather than using (&lt;) we pass in a predicate instead? Then we get (a -&gt; a -&gt; Bool) -&gt; [a] -&gt; [a] Searching for that you only one exact match, nubBy, which happens to be exactly what you want. &gt; :m + Data.List &gt; nubBy (&lt;) [3,1,5,3,6,8,6,5] [3,5,6,8] and you can click the click the source button on the haddock and see how it's implemented while you're there. edit: "not greater than" should of course be (&lt;=) not (&lt;)
I think part of your problem comes from not specifying it well enough. Specifically, you don't handle the case of the first item of the list the same way in your specification and your example. You specify that elements must be greater than their predecessor, something that simply *cannot* ever be true for the first item in a list. So to resolve this, let's write it this way: "I would like a function that returns the first element of a list, as well as any further item whose value is greater than its predecessor." So really we have two functions. One, we need to take the first element no matter what, so we have: filter' (x:xs) = x : undefined filter' [] = [] Now we need to fill in the "undefined". We can use a recursive function to do this pretty easily. We really want a function that takes three things. The previous element, the current element, and the rest of the list. What it will do is examine the previous and current, determine whether or not to use the current element, and then process the remainder of the list. filter'' (p:c:rest) = if c &gt; p then c : filter'' (c:rest) else filter'' (c:rest) This definition works because if we do `filter'' [1,2]` the pattern match is equivalent to `filter'' (1:2:[])`. This is all well and good, but what happens when we get to the end of the list and we do something like, `filter'' [5]`? Well that means we've "run out" of current items. So we treat the one-element list case like this: filter'' [p] = [] So now we've handled 2 or more items in a list, and we just wrote the case for 1 item left, but what about the last one? Of course, the empty filter'' should also return an empty list: filter'' [] = [] Finally, we put that in our `filter'` function: filter' (x:xs) = x : filter'' (x:xs) filter' [] = [] filter'' (p:c:rest) = if c &gt; p then c : filter'' (c:rest) else filter'' (c:rest) filter'' [p] = [] filter'' [] = [] We can hide the definition of `filter''` using `where`: filter' (x:xs) = x : filter'' (x:xs) where filter'' (p:c:rest) = if c &gt; p then c : filter'' (c:rest) else filter'' (c:rest) filter'' [p] = [] filter'' [] = [] filter' [] = [] A very, very slight optimization can be made in terms of lines of code by replacing `filter'' [p]` and `filter'' []` with `filter'' _ = []`. **Here's your homework assignment in exchange for this help**: write a version of `filter''` that uses list and tuple manipulation functions to do this. Hint: Use `zip` and `tail`.
You can also think recursively to solve this. It won't be a one-liner, it might be slower than one, but it is probably easier to understand. filter' [] = [] -- nothing to do on the empty list filter' (x:[]) = x:[] -- nothing to do on a list with one value filter' (x:y:ls) | x &lt; y = x:y:(filter' ls) -- keep the two values and recurse | otherwise = (filter' ls) -- skip y and recurse EDIT : fixed a mistake -- on the last line, the recursion has to be called on x:ls rather than ls EDIT2 : ... or not (after re-reading your question) EDIT3 : the problem in your reasoning was to try comparing a value with its predecessor, when you can simply compare with the successor - which is basically the same.
No problem! Note that I added to my response as well, in case what you were after is actually something a little different. (Unfortunately, there seem to be lots of ways to get that same result in the example you provided. :)
Does not work even when the arguments to foldr are swapped: pathological :: [Integer] pathological = [5,4,3,2,1,2,3,4,5] filterBerryman :: Ord a =&gt; [a] -&gt; [a] filterBerryman = foldr (\x xs -&gt; x : filter (&gt; x) xs) [] test = do putStrLn . (++) "jberryman: " . show $ filterBerryman pathological &gt; test jberryman: [5] 
This does not work either: pathological :: [Integer] pathological = [5,4,3,2,1,2,3,4,5] filterBeast :: Ord a =&gt; [a] -&gt; [a] filterBeast = dropMatch (&lt;=) where dropMatch :: (a -&gt; a -&gt; Bool) -&gt; [a] -&gt; [a] dropMatch p = foldr (\x xs -&gt; x : filter (p x) xs) [] test = do putStrLn . (++) " beastaugh: " . show $ filterBeast pathological &gt; test beastaugh: [5,5] 
Nice one liner! (I should note that of all the solutions I tested on this page, yours and cgibbard's are the only ones I think matched the spec he offered.)
I don't think this is right either (by my read of the problem): import Data.List pathological :: [Integer] pathological = [5,4,3,2,1,2,3,4,5] filterZuserm xs = nubBy (&lt;) xs test = do putStrLn . (++) " zuserm: " . show $ filterZuserm pathological zuserm: [5,5] This could be the solution but I do not think it is. 
Your post is so much more clever than mine, and produces perhaps the most concise solution to the problem. Well written! **Edit! Disaster has struck!** **Edit 2: I am a moron! And for some reason on re-reading cgibbard's post I neglected his first solution, which produces the result I thought was most likely the correct one.** Nonetheless, here is an example of the groupBy solution in the second half producing a different result: pathological :: [Integer] pathological = [5,4,3,2,1,2,3,4,5] filtercgibbard = map head . groupBy (&gt;=) test = do putStrLn . (++) " cgibbard: " . show $ filtercgibbard pathological &gt; test [5] This is because the `groupBy` function only tests for when the comparison fails relative to the head of the list. Thus, descending and ascending runs end up being grouped into the same group. Specifically, the groups that form are (is) `[[5,4,3,2,1,2,3,4,5]]`. Taking the `map head` of this returns `[5]`.
That *might* be what is desired anyway. The problem description is a little ambiguous.
It isn't the same, because if you compare with the successor you then have a special case at the end of a list. As I said, simple case analysis (like you did with your original function) suggests the problem is poorly posed. Finally, it looks like your function doesn't do the right thing. It looks like it only shows increasing sequences: pathological :: [Integer] pathological = [5,4,3,2,1,2,3,4,5] filterJiyu [] = [] -- nothing to do on the empty list filterJiyu (x:[]) = x:[] -- nothing to do on a list with one value filterJiyu (x:y:ls) | x &lt; y = x:y:(filterJiyu ls) -- keep the two values and recurse | otherwise = (filterJiyu ls) -- skip y and recurse test = do putStrLn . (++) " jiyunatori:" . show $ filterJiyu pathological &gt; test jiyunatori:[1,2,3,4,5]
You could change it to filterZuserm xs = nubBy (&lt;=) xs And it removes duplicates.
Hah, good point, thanks.
Please note my post below, his solution is not, strictly speaking, correct. For certain definitions of correct. His solution may very well be precisely what you want, but does not seem to coincide with the problem specification or example you gave.
For me it's clear that the intended result is: filter' [5,4, 3, 2, 1, 2, 3, 4, 5] = [5, 2, 3, 4, 5] 
Absolutely. I'll clarify my post, I don't *think* it's what he wants. But it could be!
I do not believe that is the desired result of the function, but at this point the problem posed and the specification are ambiguous. Many people have submitted answers, several of which are unique in their result when called with the argument `pathological`.
Maybe this is actually what is desired though. He didn't make it clear whether he meant the preceding element in the original list, or the preceding element in his constructed list. To be safe, I did both versions.
I thought I made it clear that my two versions of the function had different meanings. My 'ascents' probably is what you think is the solution to the problem.
You, sir, are correct. Not even just technically correct, but all around warm and fuzzy correct. Sorry about the mistake!
I've corrected my post, sorry about that!
I'm glad you posted that link. I cannot stress how important this is """Higher level BUILD files do not magically fix this, but they do provide a better construct for dealing with the problem. BUILD targets list all inputs and dependences (outputs are implicit to the rule type and well defined). This means that ***we can more easily reason about exactly what each action should have access to, which means we can do things like sandbox action execution.*** The higher level rules in BUILD files provide a way to enforce better discipline in declaring inputs and dependencies, which indirectly enables greater parallelism.""" * When you have sandboxed builds, you can define the environment. * When you can define the environment, you can have repeatable builds (mind you, the environment that template haskell has access to must be controlled). * When you have repeatable builds, then you can cache. * When you can cache, then you can build all of hackage and keep it up to date without problems. That's a wonderful place to be. For haskellers it should be easy to see that compilations should be a pure function, only depending on a tightly controlled environment, and the input source files. This property is more important than anything, because with repeatable builds, you *know* whether something on hackage will build, because a robot checked it for you. There is no point in downloading (or uploading) versions that don't build. There is no point in providing dependencies that don't build etc.
This is awesome. The only thing that wasn't immediately clear in the fist part is what ariella0 does, as opposed to charlie0 which is easy to grok even without going to the 2nd part. Also *Squarea* is a weird name (I first parsed it as "square a"). But this is bikeshedding.
Related: http://okmij.org/ftp/Haskell/Mr-S-P.lhs
That doesn't quite work. However, [Data.List.**Ordered**.nub](http://hackage.haskell.org/packages/archive/data-ordlist/0.4.4/doc/html/Data-List-Ordered.html#v:nub) does exactly what I believe the original poster is asking for.
It can be translated with google-translate (which is integrated with the Chromium browser for instance) See also http://permalink.gmane.org/gmane.comp.lang.haskell.glasgow.user/20610 for a related thread in glashow-haskell-users
It might be worth pointing out that Coq's `intuition` tactic does something like what pigworker describes. It uses the same proof calculus as Djinn to generate a proof "skeleton", and takes another tactic to handle all the leaves where it gets stuck.
Umm, yeah... take that, other language communities. Can your programming language play rugby? Didn't think so.
You have the wrong Haskell, Ken.
I hope I'm not the only one searching for "Jonny Python - Linebacker"
Well - time to get rid of some: flip (-) I thought I was past my "reimplement prelude" stage. 
What do you need exactly? Something like GLFW? I remember having some issues with GLFW back when I was using it for OpenGL stuff in Haskell, but I did eventually get it working just fine. But I mean functionality wise - is that what you are looking for? Or are you looking for full on native GUI toolkit support? Cocoa, win32, gtk...
In my experience, you never get past that stage. :)
 filter' (x : y : more) | x &lt; y = x : filter' (y : more) filter' (x : y : more) = filter' (x : more) filter' xs = xs 
I looked into that, and started wondering how advanced a scenario one could make. Something like Mr P knows the area of the rectangle Mr S knows the circumference Mr H knows the length of the diagonal Mr P says "I know that Mr D does not know whether Mr S knows that I don't know" ...and the others state that now they know.
Allows you to use gloss from GHCi, without needing to restart GHCi. Tested on Windows 7 SP1 64-bit, and OS X 10.6.8. You need to pass ghci the following startup flags if you're using OS X: '-fno-ghci-sandbox -framework Carbon' Cabal install defaults to using GLFW-b for building gloss, but you can pass cabal-install the '-f GLFW' flag to use GLFW instead of GLFW-b.
For the curious, I (hello from my at work alt!) screwed up by assuming that the area would monotonically increase along with the perimeter. A counterexample, (2,6,7) has a smaller area than (2,6,6) but a larger perimeter. So, in `havingSquarea`, when I used `takeWhile (check (&lt;= s + a^4))`, I assumed that as soon as I found a triangle (a,b,b) with squarea larger than s, that I could stop, when I should have realized that there still might be triangles (a,b,c) c &gt; b, with squarea less or equal than s. The fix for this would be to takeWhile either (a,b,b) has squarea lte s, or (a,b,b + a - 1) does. (I think. I haven't proved yet).
I think you are right. Once you pick a and b, squarea is a fourth degree(?) function of c. There is a zero when c = a + b, but we want an actual triangle, so one candidate is c = a + b - 1. The next zero is c = b - a, but we restrict to c &gt;= b, so the other candidate is c = b. The first derivative of squarea with respect to c is (unless I made a mistake) s'_c = -4c(c^2 - a^2 - b^2) So the area is actually maximized when it's a right triangle!
To use gui libraries from ghci you need to pass a special flag to ghci on startup, otherwise you run the risk of having issues with thread local storage. I can provide more details if you're interested.
Very cool! I thought about doing that port some time ago, but thought it looked like too much work. You are braver than I!
There's a guy on here who's currently teaching kids using the Gloss library, I bet this will be of great interest to him
beautiful
Do I need more than the following code: [http://wxhaskell.sourceforge.net/download/EnableGUI.hs](http://wxhaskell.sourceforge.net/download/EnableGUI.hs) ? If so, then I am indeed interested in those special flags. Could you provide details by adding an 'issue' on the github page?
Agda basically does this. Records are nested modules with a type and getter functions defined.
That would be me, and it would be of interest indeed. So far I've avoided installing gloss locally on kids' computers and instead just used the web version I put together... but when we get to animations, that may change!
That's the correct answer according to my interpretation of the OP's question.
I don't know what went wrong earlier, but GLFW-b doesn't crash on me anymore on OS X, so that's cool: everything works on OS X 10.6.8 and Windows 7 SP1 64-bit. I did however finish porting gloss to GLFW (and pushed the changes to my github repo), although cabal defaults to GLFW-b. You can choose to build install gloss with GLFW instead of GLFW-b by: cabal install -f GLFW The earlier referenced 'EnableGUI' is automatically build and used when building/using the library on OS X. Do note that you indeed need to pass ghci 2 special flags for everything to work properly: ghci -fno-ghci-sandbox -framework Carbon I assume you can set those flags in your .ghci file.
Really wish GHC would get on the boat or that people developing this would contribute the necessary infrastructure to Haskell proper. Nonetheless, nice going!
I've been working on this as well, but mainly focusing on the visual representation of the results, and ability for the user to be directly involved in the search process. The latter is necessary because for any practical uses, the space is pretty explosive.
How does it work around lack of tail call support? (Or does it not follow Java's method model?)
You don't need tail calls as often in haskell, because of laziness. For example, map is sometimes more efficient without tail-calls.
shouldn't circular convolution be implemented using FFT (so it's n*log(n)?)
You mean have GHC compile to java byte code?
Yes. I realise that it's a lot of work, but I would imagine that it would benefit others a lot more that way.
Hm, this looks really cool, but it's not exactly encouraging that there's been no real activity on the project since mid-May.
One way to get started more easily might be to write a Haskell runtime as an interpreter in Java, perhaps. As in, write an STG machine to execute GHC Core instead of targeting JVM byte code directly. There would be a performance cost of course, but it might help bootstrap things.
How does Frege differ from scala or clojure, which both aim to be haskell (or lisp)-like...
Well compared to Clojure, the Hello World program will not need 3 kB of parentheses.... But seriously, Scala and Clojure are just very different languages from Haskell while Frege tries to be a proper alternative implementation of it.
This. Clojure and Scala do not aim to replicate Haskell syntax. Frege does. Also, Frege is lazy by default, while Clojure/Scala are eager.
Who is paying this person, and how can I make them pay this person more?
How does this deal with http://homepages.inf.ed.ac.uk/wadler/topics/garbage-collection.html?
The only difference that really looks like a big deal to me is that the function composition operator is a thick dot instead of (.) How do you type the thick dot?
I'm still struggling to decide if laziness is a good thing or not.
Like this: â (seriously, that depends on your operating system, code editor, &amp;c.)
Laziness allows you to structure a datatype to match its computation, and vice versa. With laziness, an algorithm can be written as a search within an infinite data structure. The same algorithm may be doable without laziness, but it would essentially consist of explicitly re-implementing laziness.
Easiest way, copy and paste from camccann's comment
but the question is if lazy-by-default is necessary. What about opt-in laziness a la Scala/Clojure?
[GHC FAQ: Why isn't GHC available for .NET or on the JVM? ](http://www.haskell.org/haskellwiki/GHC:FAQ#Why_isn.27t_GHC_available_for_.NET_or_on_the_JVM.3F)
I'm aware that there are issues that make it difficult, but given that frege seems to have a good start on these issues, I don't think it's unreasonable to suggest that the developer put his effort into making ghc support the jvm instead of developing an entirely new compiler.
They're both open source projects. Give it a shot.
I'm not against open source projects or something, I just would rather have GHC *everywhere* along with all of the libraries that it already has.
Go for it. Start with LambdaVM.
I understand what you're getting at, that if I want it so bad that I should do it myself. Sadly, I only have so much time to work on projects to contribute to the Haskell community, and this Java isn't my top priority anyways. However, I don't see how that precludes me from giving my opinion.
why? the world already has a bajillion languages for the jvm. i don't give a poop about the jvm nor do lots of other people. slowing down ghc development to meet this non-goal seems like a terrible idea
I'm proposing that the guy that made this compiler in the first place contributes his efforts to GHC instead of writing a whole new compiler. I fail to see how that would slow down the other GHC developers that much.
I'm not saying you shouldn't have nor express an opinion; i'm pointing out that the reason this stuff hasn't happened is close to home. Those who want it on the JVM don't have the time for it, and those who have the time and expertise don't see the JVM as a priority. Until that situation changes, it's not likely to happen.
As an alternative to modifying GHC to emit java bytecode, would it be possible to modify LLVM instead? If it can be done, it seems like it would be more useful to more people (i.e. users of other languages that also use LLVM). (For my part, I'm content with native compilation, but I could see how JVM support might be a nice way to get broader hardware support.)
There's a blog post from a couple of weeks ago: http://fregepl.blogspot.com/2011/08/frege-runs-with-jdk7.html
The real problem isn't getting the Haskell language to run on the JVM, it's more that the RTS is difficult to port (it's mostly written in C I believe). If you want to go and rewrite the RTS in Java or something else, then go for it.
 CTRL-ALT-u 2 2 1 8 ENTER
And you can use its package version as an approximation.
What does this mean for haskell?
The irony, of course, is that the package defines `tau` in terms of `pi`.
More precisely Ï in terms of pi; tau in terms of Ï.
Unless any major hype in the magnitude that Clojure or Scala has seen kicks in, it's just yet another interesting language compiler experiment IMHO As I understand it, Frege does not seem to care for 100% Haskell compatibility, so this might actually hinder its take-off (as most existing Haskell libraries won't work there). Maybe it's a bit comparable to CPython vs Jython: Jython is just a niche platform which only a relatively small group (compared to the CPython community) actually cares about. It's nice to have, but alas it's not 100% compatible w/ CPython, and means that the huge amounts of libraries existing for CPython simply don't work w/ Jython (at least not all), leaving Jython just as way to write high-level glue-code glueing together Java libraries for programmers which prefer Python syntax over alternatives such as Scala/Clojure/Groovy/... 
map f (x:xs) = (f x) : map f xs map f [] = [] But you still need tail calls sometimes, just not every time.
Well now we know what the ultimate question of life, the universe, and everything is!
I still don't see what people like so much about tau. It's just a tradeoff of which terms you're going to have factors in. With pi, you have no factor in area, with tau you have no factor in circumference. It's a silly movement with no good purpose and no different than [standards wars.](http://xkcd.com/927/)
This package should be submitted as a contestant to The Most Frivolous Programming of 2011.
Opt-in laziness in Scala stinks. Don't know about Clojure.
First, it is laziness *by default*. All languages allow encoding of eager/lazy evaluation. Haskell makes it easier to encode lazy than eager, but both are expressible. In a pure language, it is somewhat important to have built-in support for laziness, because otherwise, you'd have to implement it via composition of effects -- and then the result, which is semantically pure, would seem effectful (as it was composed from effects). So it makes sense to make it a first-class pure operation that hides the effects under a pure cover. 
That's a hard problem to answer because Haskell is really about the most general purpose of general purpose languages. You aren't going to find Haskell's equivalent of Rails... a single library or use case that motivates a large community to move to Haskell. I wrote a longer version of that at http://cdsmith.wordpress.com/2011/03/13/haskells-niche-hard-problems/ That said, there are some single applications or libraries that can make Haskell appealing in some specific situations. I'll list a few, but I'll forget more, so maybe others can fill in the gaps. * Software transactional memory. Haskell was basically the first language in which this was implemented in a usable way. It's mature and solid, as opposed to the experimental attempts to add it in more imperative languages. * Parsing: Parser generators exist in lots of environments, but for construction of ad hoc parsers in embedded DSLs, it's hard to beat parsec and a host of similar libraries (attoparsec, etc) in Haskell. * Web applications: we have two competing philosophies here... see Snap for an approach to building web applications based on lightweight combinators. It's nice and refreshing in the sense that it's non-disruptive not easy to add, e.g., a web server and REST web service interface to an existing application, but it doesn't build too much magic for you; you're still in control of what's going on, not conceding your design over to a framework layer. Or, look at Yesod for a radically statically-checked approach to web programming, using a lot of Haskell extensions to extend static error reporting to the template language and more. * Data Parallel Haskell is still in development, but is an awesome example of the kinds of things that are feasible in Haskell, but unthinkable in other languages. It lets you write your program in a way that's nice for programming, with nested data structures and such, and then completely restructures your code, turning it sort of inside-out at compile-time to turn it into bulk data parallel operations on large arrays, in order to get better parallelism properties.
I recently bought [Pearls of Functional Algorithm Design](http://www.amazon.com/Pearls-Functional-Algorithm-Design-Richard/dp/0521513383) by Richard Bird and I can't recommend this enough if you are interested in functional design. Although this is not always an easy read as some of the algorithms can be quite complex and non intuitive, what really shines is the approach to arrive at such elegant solutions. The recipe is almost always the same: start with a non optimal solution that will act as your specification and then through the application of different techniques and mathematical reasoning, you arrive at the optimal solution.
You can also do [x | x &lt;- [2..], all ((/=0) . mod x) [2..x - 1]] which is 40 characters ignoring (non-significant) spaces I prefer filter (\x -&gt; all ((/=0) . mod x) [2..x `div` 2]) [2..] however ... EDIT: ah, the linked discussion has this already.
I think that is a perfect summary.
* More tutorials like Land of Lisp * More standardization (Lisp, but which Lisp? CL but which CL? Scheme but which Scheme?) * Far less smugness in #lisp
I second that sentiment: Haskell excels at helping you to boldly program where no one has programmed before. For instance, software transactional memory (STM) is simple and works very well in Haskell (GHC specifically), but other languages struggle with it or have even given up trying to implement it. This is all because of Haskell's superb static type system. A more modest example would be my own [library](http://www.haskell.org/haskellwiki/Reactive-banana/Examples) for functional reactive programming (FRP), a paradigm which was pretty much [born][2] in Haskell. [2]: http://conal.net/papers/icfp97/
It provides all the feeling of superior pedanticism without the burdensome requirement of real knowledge.
Arrows are a killer application for xml handling. See HXT. It took me a few lines of clear code to parse xml output of ADP Tax service. I shudder to think what i'd have to go through if i did it in java. 
That and it also provides a lot of tidier formulas. Okay, so you have the formula for the area of a circle, as well as, uhm, well there's *probably* something else. I guess.
Certainly the examples provided by the Tau advocates are compelling; they're undeniably more elegant. But voraciously advocating tau is like using a slightly different syntax for Haskell and advocating it rabidly over the default syntax because it's a bit cleaner. Maybe it is, but your work isn't interoperable, and you just demonstrate your ignorance of non-superficial issues. And if anyone thinks using tau rather than pi will help get someone into mathematics who otherwise wouldn't, I don't even know what to say to them.
I agree with the others. Haskell is better understood as a general purpose programming language, which means that you can pick whatever real-world application that takes your fancy. That said -- there is the universal principal that it is better to progressively expand the scale of tasks that you attempt in a (relatively) unfamiliar programming framework.
Okay, then, name another formula besides area. Excluding ones that are obviously the "half-way" case of a more general form. By the way, what's the formula for the volume or surface area of a sphere? No, it's not an even trade-off at all. The meaning of pi is inherently geometric, because it defines (and is uniquely defined by, up to any rational multiple) the metric of euclidean space, via the trigonometric functions (which relate angles and distances). The rational multiples of pi thus correspond to rotations, with 2 * pi giving a full rotation. So any time you have a formula that involves some multiple of a rotation, it is almost always going to include 2 * pi because that's the *identity*. There's no argument, at all, in any way, against 2 * pi as the more fundamental and useful value. It objectively is. The only argument in favor of pi is sticking with the status quo because, oh well, sure it's dumb but everyone is used to it. This is actually a valid reason for not changing things on a whim (why isn't `Functor` a superclass of `Monad`, again?), but at least be honest about it instead of presenting it as a blatantly false equivalence.
So, would you argue that the `fclabels` package is a bad idea, because it's not directly interoperable with the built-in record syntax, and using it thus demonstrates an ignorance of non-superficial issues?
If fclabels' advantages were just a matter of superficial notation, sure. Lenses have a deeper semantic value than tau.
What? The semantic value of tau is fundamental to euclidean geometry. The semantics of tau practically *are* euclidean geometry, in fact. On the other hand, there is no clear semantics for pi that doesn't reduce to meaning "half of tau". I'm not sure why you think lenses are more meaningful than the essential concept of an entire field of mathematics. In fact, if I was worried about interoperability in a Haskell package doing any sort of geometric computation, I would sacrifice something like `fclabels` (non-portable, with a dependency on TH!) long before I would worry about not defining 2 * pi as a constant, precisely because the former is a mere syntactic convenience, while the latter is deeply tied to the meaning of what the code is doing.
You can replace `tau` everywhere with `2*pi` without losing much abstraction, elegance, or much anything at all. The same cannot be said for inlining all of the functions defined on lenses. Sure, fclabels is non-portable; I use data-lens instead. But that's a detail irrelevant to functional lenses in the abstract.
The "differences to haskell" document suggests you can use &lt;~ instead of the dot.
Dear god this sums up my issues with tau absolutely perfectly. Ever notice that most of the people who push Tau aren't very accomplished mathematicians (or even utilizers of math) in the first place? (Cue George Carlin saying "Why is it that most of the people who are against abortion are people you wouldn't want to fuck in the first place?")
I felt the same until seeing the geometric interpretation of Euler's identity (namely that half a turn gives -1). Of all the arguments given, that one is by far the most convincing. The other arguments about integration and limits are just notational arguments IMO; though it's interesting that pi doesn't have similar notational arguments supporting it, ne?
Much of the progress in mathematics can be attributed to such arguments about notation. The question to ask is: why? Ironically, noting that history shows that progress advances rapidly after notational changes isn't a good argument. Naturally, we don't keep historical records of all the notational variants that didn't cause major change. No, the reason why is because arguments over syntax are *fundamentally* arguments about semantics. Every syntax has an inherent semantics that arises from the structural properties of the notation. Even in programming language disputes, the arguments are not arguments without substance. What are the fundamental primitives of computation? Are they mathematical expressions?; or are they subsequences of computational action? The invention of block-structured programming radically altered the face of computer science, but fundamentally it was just a change of notation. We argue about the superiority of functional languages over procedural ones, but again this is "merely" an issue of syntax. 
&gt; For instance, software transactional memory (STM) is simple and works very well in Haskell (GHC specifically), but other languages struggle with it or have even given up trying to implement it. This is all because of Haskell's superb static type system. It's working fine for Clojure as well. Are you sure static typing is key here? Clojure is dynamic, and I think their shared preference for immutability might be more important STM-wise?
&gt; I felt the same until seeing the geometric interpretation of Euler's identity (namely that half a turn gives -1). Of all the arguments given, that one is by far the most convincing. Of course, this only works if you assume a flat, euclidean geometry. In a spherical or hyperbolic space, there *is* no constant ratio between a circle's radius and circumference, and with other metrics the value may be different, if it's even well-defined at all. Euler's identity is nothing more than an elegant way of relating the geometric interpretation of complex numbers to the geometry of 2D euclidean space, via an identity between a full rotation in each. I'm actually not sure what a *non*-geometric interpretation of Euler's identity would even be.
&gt; Ever notice that most of the people who push Tau aren't very accomplished mathematicians (or even utilizers of math) in the first place? No. In fact, I've noticed that most of the people who promote it are precisely people who would use math on regular basis. Aside from mathematicians, it seems to be most popular among physicists, electrical engineers, and computer scientists. The most common response from serious mathematicians seems to be "well, obviously 2*pi is more meaningful" and then they shrug and carry on. For instance, take comments like this: &gt; It may be that 2*pi*i is an even more fundamental constant than 2*pi or pi. It is, after all, the generator of log(1). The fact that so many formulae involving pi^n depend on the parity of n is another clue in this regard. Yeah, clearly not an accomplished mathematician [speaking there](http://blog.computationalcomplexity.org/2007/08/is-pi-defined-in-best-way.html?showComment=1186584000000#c971095637512882599). What I *have* noticed is that the people who *complain* about tau haven't actually thought about what the math means, because if you do it's pretty obvious that tau makes more sense (whether or not one cares enough to change existing notation). I mean, for crying out loud, I didn't care that much to begin with, but it's really hard *not* to support an idea when it's *fundamentally correct* and provokes so much smug, uninformed opposition.
&gt; You can replace tau everywhere with 2*pi without losing much abstraction, elegance, or much anything at all. True. In fact, you can replace any meaningful, named value with an expression that evaluates to it without losing *much*... or you could just use the meaningful value directly instead of obfuscating the semantics to no benefit. Having clear notation *matters*. &gt; Sure, fclabels is non-portable; I use data-lens instead. But that's a detail irrelevant to functional lenses in the abstract. And whether other people use pi is just as irrelevant to the semantics of 2 * pi, which are significantly more useful than just pi.
No, I agree with the philosophy. I do think tau is more meaningful. I disagree with the amount of effort and pressure that people put on it. The idea that someone would add another library dependency to pull in a constant with a factor of two in front is ludicrous. Terry Tao gave the really typical reaction of "Yeah it probably is better" but I'd bet he really doesn't care THAT much as far as notation goes. It'd be as if a bunch of people spent a ton of time petitioning another letter to the alphabet for the sound of th because it's used so often and makes it's own unique sound, under the guise of it being a profound linguistics issue. Did I also mention that tau is loaded as shit as a letter? Time constants, surface integral measure, wick rotated time parameter, lifetimes, torque. Fuck it.
&gt; Or we can even use type synonyms as constraint synonyms: &gt; type Stringy a = (Show a, Read a) I'm liking this already.
Nice! Thanks, I must've missed that. `&lt;~` is a clever and intuitive replacement.
Well, I tend to assume that most of the evangelism is thoroughly tongue-in-cheek. And while I wouldn't necessarily add a library dependency, I don't see any reason not to just define it in my own code when I need it, because 2 * pi comes up often enough that I've pretty consistently ended up defining it as a constant *anyway* to tidy up the code... so I might as well use the silly name for it. &gt; Did I also mention that tau is loaded as shit as a letter? Time constants, surface integral measure, wick rotated time parameter, lifetimes, torque. Fuck it. Since obviously no other letters are ridiculously overloaded... there are at least a few other ways that pi is used already, and tau at least seems no *worse*. The lack of structured namespaces in mathematics is a much larger problem.
Genuine question, since I don't know Clojure at all: how does it avoid the problem of non-rollback-able actions in a transaction? E.g. the proverbial launch-nukes amongst the "safe" memory accesses?
This is my take on the matter: http://www.reddit.com/r/haskell/comments/hezgk/haskell_singularity_approaching/c1uwd75
What's the benefit of Arrows here, over Category+Applicative? In the case of HXT, perhaps it could also be monadic? Or does it use the Arrow/Applicative restriction to optimize the processing?
I want to upvote this with every account I've ever created! (I'm not actually going to, but I'm tempted)
Some of the evangelism isn't... I mean, I recall a product that was not unlike erector sets, (only for building larger things) that was funded on kickstarter, and the owner of the product stamped all the angle measurements of the pieces on them in tau radians. C'mon...
My impression is that it deals with the problem through a combination of saying "hey, don't do that, ok?" and the language being designed from the ground up to make doing things the right way *at least* as easy as doing them the wrong way. Clojure's philosophy about state, mutability, and side effects is actually very similar to Haskell's; what the latter formalizes in the type system, the former does based on convention, metaprogramming, and common sense. The reason why STM falls apart in almost every other language is that they neither encourage a pure functional style nor provide a rigorous means for effect tracking.
Basically. The STM.NET project failed for example, not because of static types, but mutability. And also probably because of design. But because mutability is pervasive in .NET, STM suddenly becomes drastically more expensive, because you have to basically track every read and write so you can keep it part of the rollback log in case you need to retry. Everything essentially becomes part of the transaction so it can be reverted (their design involved annotating things that weren't safe to revert I think, which basically means everything else is wrapped in `unsafeIOToSTM`.) In Haskell, mutability is not the default. So you don't need any special machinery to handle that. Also, our STM actions are typed, so the actual places you *do* need to keep around transactions and logs to undo things is inside `STM`. There is otherwise no special overhead. Same with clojure (although you don't have types.) Overall I really like STM. People say it performs worse than locks, but honestly, if you want to increase performance by doing multiple threads, you want to *avoid* sharing memory and locking as much as possible. Sharing memory across CPUs is expensive as hell. STM should be used for the places where you want to synchronize state, but it should be infrequent no matter what the model if you want it to perform well. STM is just safer by default.
Yessssssss. I absolutely *loved* this idea (or a [prior version of it](http://hackage.haskell.org/trac/ghc/wiki/KindFact), at least) the moment I heard it--it takes a whole host of minor annoyances and bits of awkwardness, unifies the underlying concepts, and then solves them all in one fell swoop. Beautiful.
Aha --- so the enforcement is a moral one, as opposed to a legal one? This is something on which reasonable people can disagree :-)
Speaking as someone who tries to use Haskell in anger, I would say that it is still in a phase of growth. The foundations are probably quite solid --- certainly more so than language with more market-share, but there are idioms which are still missing and corners to smooth. For instance, the desire to have high performance web-servers pushed the new IO manager. The need to have efficient and easy to reason about stream processing led to iteratees/enumeratees. Things like Yesod and DPH are raising new questions about best-practices in novel directions. From the point of view of writing a "usable real-world application", this means that there isn't really a perfect set of libraries, methodologies, etc. which we can point at and unambiguously say "this is the Way". My own learning experience has been to have a problem that needs solving, and deciding to solve it with Haskell, in that order. No need to decide to "do something with Haskell" then go looking for a problem --- old proverbs about hammers and nail, etc. Having a concrete problem focuses your mind, and also helps you to really appreciate the thinking behind the solution (when you find it). If you can, discuss various things here, and on things like stackoverflow (where many Haskellers lurk), and maybe your experiences will feed back into the overall language direction!
Well, putting explicit angle measurements on something like that at all seems kind of silly, especially in radians, since I'd expect it to be aimed at people who might not be terribly familiar with trigonometry or whatnot, and irrational constants are not exactly the most user-friendly idea ever. On the other hand, labeling things using simple fractions of a complete circle would be clear and easy for almost anyone to understand, wouldn't you say?
This is definitely one of the most awesome extensions to date.
Well, what stops people from using `unsafePerformIO` inside an `STM` transaction? If memory serves me, Clojure includes some means of ensuring that impure code isn't used inside a transaction, on pain of preemptive runtime errors, much like GHC forbids using `atomically` inside of `unsafePerformIO`. It's pretty much all moral enforcement in the end; the question is how much, and in which ways, you ask the computer to help you.
If it's engineering, they should be using degrees. Using tau radians clearly shows some intent to push this on people.
It's worth noting that the STM.NET project was trying to do something much harder, i.e., actually integrating STM fully into the reality of how .NET is used. They could have done something easier, similar to how GHC or Clojure implement STM, but the result would have been badly crippled due to not interoperating well with existing .NET code. On the other hand, the combination of laziness and purity, and actually taking them seriously, make it possible to reasonably *add* something like STM to Haskell and integrate it with the existing language, which is the *truly* remarkable thing. Clojure's STM works well because Clojure deliberately did things right from the beginning; it's failed in other languages because bolting it on after the fact was intractable. In Haskell it essentially *was* bolted on after the fact, and everything just works.
Much better said. Thanks!
I meant arrows as a tool to build a DSL language. Nice and easy to read syntax to express succinctly series of transformations of xml tree. 
There's a couple of gotchas, but its fairly core to the language. If you want your code to be lazy, you wrap your data structure producing code in a single function call, and that's that. It's not perfect, but it works fairly well for the language and doesn't break the lisp-based syntax.
If they had put "pi radians" instead, would you feel they were pushing their views on people?
I have this weird tendency to attribute immutability to the type system. :-) My reasoning is that the type `(-&gt;)` denotes pure functions.
HXT doesn't actually make good use of arrows; it should have been monads. The telltale sign is that you never use `first` or `***`, only some `$$` combinator (forgot the name) for partial application. That said, I do think most of the HXT combinators are quite slick, but it would be easier to use and understand if the unfortunate arrows were dropped.
Recently Haskell started to really resemble to the language in my head. (It's shame it doesn't work on OSX 10.5 anymore...)
oh no, forget this madness for once.
The trick with using a type tail :: Stream a -&gt; Later (Stream a) to enforce causality is neat. Other than that, I fail to understand how this improves upon a simple-minded stream-based model like my own [reactive-banana model](https://github.com/HeinrichApfelmus/reactive-banana/blob/master/reactive-banana/src/Reactive/Banana/Model.hs). In particular, I can reuse Haskell's semantics and don't have to invent my own language and type system. 
I agree --- which is why I said it's a topic on which reasonable people can disagree! Laws come from morals, and morals often pre-empt laws, etc., etc. Can I be cheeky and ask how Clojure deals with effects in general? I can't seem to find a decent article stating how it works (i.e. I can find out how to print things to screen, but no idea how it is sequenced) --- it's supposed to be lazily evaluated, but yet there's no monads! How is this achieved?
Nothing stopped people from doing IO via streams prior to Haskell '98. But monads opens up a nice syntactical world which is nice. I think of this as a tentative first step, so that one day we can say "we've understood GUIs" and specialise a syntax for that too. Academic work like this is just that --- academic; they have no idea what programming in the large would be like with this (and they point that out themselves) and they have no clear idea of how to hook it in a sensible way to existing GUI toolkits, which deal with a different model of concurrent, asynchronous events (things that you have dealt with, or at least are trying to).
Ah, I meant the semantics in particular. Synchronous streams are very simple from a Haskell point of view, so I don't quite understand why they are doing elaborate correctness proofs. But I guess that's because they want to show that their `Later` (or "â¢") type does preserve causality, which is a worthy goal. A more interesting proof would be to show that their system is complete, i.e. that *every* causal stream operation can be expressed in terms of their primitives. That's where "causality by API restriction" does poorly. Concerning the GUI code, I don't see anything that reactive-banana can't handle. I use the `NetworkDescription` monad instead of uniqueness types and my remaining difficulties (dynamic event switching and event generation like in the [Wave.hs](https://github.com/HeinrichApfelmus/reactive-banana/blob/master/reactive-banana-wx/src/Wave.hs) example) are beyond their scope.
Piecing together what I've read from Clojure users, Clojure allows you to annotate code with an "io" macro which will raise a runtime error if you try to run it inside a transaction. That seems to be enough safety for the standards of a dynamic language.
Clojure is a strict language! It offers a lazy sequence abstraction, but that's it. So effects in Clojure are handled the same way as in any Lisp, the differences being that the standard libraries are written with immutability in mind (for example, they offer the full usual range of persistent data structures), and the "default" mutable references are transactional.
This is great news!
Aha --- that's what I'd assumed, then I read that it is lazy. Clearly I misunderstood "available" and "by default". All good.
I wonder why Frege is not Haskell? As it is, Frege has too many little differences to make porting Haskell code easy. Many of the differences seem totally gratuitous. 
Doesn't work on OS X 10.5? I would be surprised if this is true.
Well, it probably works if I compile it myself. It is not officially supported since GHC 7.0. And compiling GHC is not my favourite hobby either. (Also for example the network library did not compile on either OSX or Windows last time I tried).
ImplicitParams is generally considered to be a mistake. I've even heard talk about removing the extension from GHC.
Yeah, I have an old MacBook still running 10.5 and have been sad about this as well. Compiling GHC on an underpowered laptop I rarely use isn't high on my list of things to do, nor is upgrading its operating system. I've pretty much given up on doing anything serious in Haskell in OS X because of this.
I'm using an old MacBook as well. Yes, you have to compile it yourself, but that takes maybe 2 hours. It also allows me to install it into ~/local rather than somewhere system wide.
Yes, this is *absolutely* why I'm not going to ICFP. Just can't keep up otherwise.
Which I'll probably do at some point. But 2 hours is about how much I use the laptop in a month on average, so it just seems like more hassle than it's worth. On the occasions when I need the laptop (travelling or whatnot) I just hack on small stuff.
The same, except I use the machine often and I didn't give up Haskell on OSX, but I miss the fabulous new developments.
Very intresting. Do you have an example of haskell xml library that in your opinion has a good flowing syntax and is both powerful and easy to use/understand ? 
Unfortunately not, HXT is probably still the slickest; it's just that a few fundamental interface changes would make it even slicker.
&gt;They can detect file changes with inotify on Linux, whereas we use polling in Yesod. How hard would it be to do conditional compilation: inotify for linux, polling for windows ? It looks to me that this approach would greatly benefit at least half (maybe more) users of yesod, yet still be easier to implement than some complete rewrite.
...wait... there won't be any internet connectivity at ICFP!? :-/
...do you know a little bit more about why it is considered a mistake?
It's not implicit (shows up in types), and there are some odd semantic issues.
I know why *I* consider it to be a mistake. That is because making any kind of really helpful use of it would require writing large bits of your program with no type annotations at all -- otherwise you're just moving one boilerplate to another -- which is by and large a bad practice anyway, and that to the extent you are making some kind of systematic use of the feature, you are leaking out what should be fundamentally local concerns by using these special magic let-bound variable names. I can see how it's mildly convenient in a few situations, but it doesn't seem to make up for the complexity of adding a new alternative kind of parameter passing and abusing words like `let` and `where` by giving them new meanings in terms of passing parameters. You might look into the ((-&gt;) r) (sometimes called "lightweight reader") monad as an alternative in cases where implicit parameters seem appealing.
What is step-indexing?
What if were possible to tell the compiler to infer just the implicit-parameter constraints of a type annotation?
When did anyone mention a re-write? It sounds like you are pretty passionate about this- we could use your help.
I think there are certainly a lot of good arguments for partial type signatures in GHC... but I hadn't thought about them with respect to implicit parameters. Mostly I'm thinking of session types and similar things. I guess I still think that, if 99% of the Haskell programming community has never come across a need for implicit parameters, then I'd consider there to be a cost to the feature, just in terms of a lost opportunity to say "What are you doing differently, such that you're running into problems that the whole Haskell has by and large not run into?"
I don't know about that. It's very often not obvious that some Problem A could be neatly solved by applying Feature B in a way that nobody had thought to try before, only to find that a few years later Problem A is the *only* thing Feature B is used for, since it turned out to not work very well for solving the Problem Z that motivated it in the first place. That said, solutions like those are usually preceded by clever hacks and cute examples that demonstrate the techniques needed, and I don't think I've seen much of anything like that with implicit parameters. In my own experience, being someone who's rarely met a GHC extension he didn't like, I've found implicit params to be neither implicit enough to use as a workhorse in place of juggling reader monads, nor first-class enough for egregious metaprogramming and type-level trickery.
I use the feature pretty regularly. 
Java is a punching-bag I can do the same in less lines of code using css/xpath which is a DSL specifically designed for xml, is more succinct, and much easier to understand. selectors with something like Ruby's Nokogiri. But actually I do one better and meta-program xml parsers in a completely declarative way using the [sax-machine](https://github.com/pauldix/sax-machine) library. And I created a fork that does this in constant space (although it is still pretty slow because it is Ruby, even though it is using libxml2 underneath). If this is the best example we can come up with we are in trouble :)
So do I, I'd be annoyed to see it go.
For anyone unfamiliar with him and his work, Haskell Curry was a mathematician and logician who made numerous contributions to human knowledge, particularly in areas related to (what would eventually become) the field of computer science. Among other things, he: - Spent years researching and exploring the subject of [writing programs in pointless style](http://en.wikipedia.org/wiki/Combinatory_logic). - Was an early proponent of non-strict semantics, by [finding ways to work with diverging terms within a larger framework](http://books.google.com/books?id=zNAln8UYGvQC&amp;lpg=PA10&amp;dq=run%20away%20from%20paradox%20curry&amp;pg=PA10#v=onepage&amp;q&amp;f=false). - Worked on building [the foundation of modern theorem provers](http://en.wikipedia.org/wiki/Curry%E2%80%93Howard_correspondence) like Coq and Agda. - Invented one of the first type inference algorithms. See also [another biography of Curry here](http://www-history.mcs.st-andrews.ac.uk/Biographies/Curry.html) for more information about him and his life.
[My simple, work-in-progress Snake game](https://github.com/MatrixFrog/gloss-snake) 
Could you provide some usage example how/where ImplicitParams helps you?
&gt; the side effects As long as we keep them safely confined to the IO monad :)
Nice! Someone is working on a nicer front end including the ability to share your programs and vote up programs written by others... so keep an eye out (but maybe not for a month or two; no idea what his time frame is)
I almost always write my code without annotations, and then let vim insert the type for me. It might be boilerplate, but it's not boilerplate I have to write.
I guess 99.9% of the Haskell community is just content with using Reader to do the same thing.
cool idea
Can you hint how big the overhead of using Thrift is?
I used implicit parameters in a project long ago and it worked quite well. The main problem is the fact that they show up in the types, but that could be fixed with some kind of IDE support or partial type signatures. I stopped using them and haven't really missed them that much though.
I'm really enjoying this series. By now I already knew to upvote this before I even read it, but now I wish I had even more upvote points. Thanks for sharing!
I often wonder if "pointless style" was the best choice of words ;D
There will be, but we will all be writing in Japanese.
I've never laughed at a joke so niche that wasn't with friends I know physically. E: referring to the first comic in the series, that is.
So he's eleventy-one today then.
Conversation with my logic professor circa 2007: Me: [shows him an implementation of the Ackermann function, which was under discussion in class that day] Professor: What language is that? Me: It's Haskell. Professor: Why's it called Haskell? Me: For Haskell Curry. Professor: Oh yeah, Doctor Curry! I dated his daughter!
Mathematical concepts are often named after satire. Consider something as basic as irrational numbers. Or imaginary numbers. It's pretty funny, so why not?
Of course not, it has side effects!
If it helps clarify things, the term arrived in Haskell via category theory, which acquired it from topology, in which setting the tongue-in-cheek meaning is rather more straightforward: It's about applying topological concepts to abstract spaces without directly mentioning the geometric points that may exist (or may not, since some pointless topological spaces can't be expressed in terms of sets of points). Mapping back to category theory "points" can be used to mean "atomic things inside an object of the category", which may or may not meaningfully exist. If the objects of the category resemble sets the points generally resemble elements of those sets. Mapping back to Haskell, the "points" are individual values of a type. Combinatory logic, as invented by SchÃ¶nfinkel and developed extensively by Curry and others, was a completely unrelated (of course) attempt to simplify the semantics of logical expressions by eliminating quantifiers, and therefore named variables (in amusing contrast, the lambda calculus sought similar goals by *emphasizing* the role of variable binding). It is a coincidence (but of the distinctly mathematical sort that often isn't coincidental at all) that the relationship between combinatory logic and lambda calculus is analogous to that of pointless vs. point set topology, which is why I enjoy humorously applying the latter term to Curry's work.
Excellent work. Most of the comments on libraries echo my experience.
It's been surprisingly minimal so far, using little RAM or CPU. This is on a very lightly loaded server, so YMMV.
Did you name one of the fields `indizes` on purpose, or was it a typo? I'd expect either `indices` or `indexes`. 
Note that companies wanting to see progress on the Cabal/Hackage issue can join the [Industrial Haskell Group as associate members](http://industry.haskell.org/partners). The IHG is currently funding work on cabal including a new [dependency solver](http://haskell.org/haskellwiki/HaskellImplementorsWorkshop/2011/Loeh), and we have told the IHG members that integrating cabal-dev features into cabal is on the TODO list. The more companies in the IHG the more time can be spent on these infrastructure issues and the quicker we can improve things.
The code is really nice to read. Maybe you could make sure the examples are always correct, possibly using [doctest](http://hackage.haskell.org/package/doctest). I think the first example has a mistake: &gt; project (IIsbn _) x = [isbn x] This sould be [IIsbn $ isbn x] Do you have some data about how it compares with IxSet?
I would love to see Haskell [GObject introspection](http://www.haskell.org/haskellwiki/GObjectIntrospection) become stable. Right now it seems to be too much in conflict with gtk2hs.
If he's 111 then who is Frodo?
I think it's a semi-typo. In German it is written with a z, but I'll fix that with the next release.
Thanks! 
Thank you. Didn't know doctest before, but will have a look at it and review the example once more. My comparisons with IxSet are quite vague. I have a production setup with about 100k records and 10 indices. I guess I saved 30% memory, but I'm not sure, because I changed several things when I migrated to HiggsSet. Performance increased massively, because IxSet was only useful with compound indices. Filtering by several criteria in sequence caused rebuilding of all indices and re-balancing the resulting trees.
I suppose you could let the usual academic genealogy stand in for family relationships. In that case Curry was formally a student of [Hilbert](http://genealogy.math.ndsu.nodak.edu/id.php?id=7298) but apparently worked mostly with [Bernays](http://genealogy.math.ndsu.nodak.edu/id.php?id=7863). Either way... uh, take your pick, I guess, there's plenty of family tree to choose from.
According to the Mathematics Genealogy Project, [Saunders Mac Lane](http://genealogy.math.ndsu.nodak.edu/id.php?id=834) is his advisorial nephew. Seems fitting.
Thanks!
On the topic of hackage quality, in an HN discussion about STM I wanted to link to the reverse dependencies on STM in Hackage, but I couldn't find that. Is there anywhere to find that now in a linkable form?
On the topic of space reasoning, and optimization in general, one of the ideas I've been kicking around in my head is the ability to label functions with something like a claim that says "this runs in constant space", and cause the compiler to throw an error at compilation time if the annotation is false. It's a young and ill-formed idea. Thoughts? (One of the second-order advantages is that it provides a way to learn about what violates the constraints in general; right now there is simply no feedback of any kind when you've written a space leak until your program basically crashes. I think the feedback may be more important in the long run than the actual functionality.)
Have you been in touch with Ben about this? He'd be the ultimate decision maker when it comes to merging this into a future version of gloss.
I don't see why they should be in conflict, on the contrary gobject inspection is the right way to do gtk2hs code generation these days. We only used this old method because that's all that used to be available. I'd be very happy to see my gtk2hs code generator rewritten to be based on gobject introspection (we've proposed it as a GSoC project a couple times). I'm very glad to see people are making progress on this.
Ah, I should have guessed. I'm Dutch, so I should recognize German.
I've contacted him a minute ago, will update this page with updates once I know more.
Two problems: 1. With lazy evaluation, reasoning about space is hard precisely because the space usage of a function depends on the context. In other words, you can't actually put a label "this runs in constant space" on values, it's not compositional, that's kind of the point. 2. Space usage is likely undecidable, there will be false positives.
Clojure has an easy way to add lazy evaluation in select areas... if you mix that with effectful code, you get unpredictable timing and ordering of effects, much like you do with unsafePerformIO in Haskell.
I have a question for the submitter. Do you still beat your wife?
Wait a second, Haskell doesn't cut it because it doesn't quite aproach the speed of well-optimized C, so you suggest Python instead? And if all you know is C then of course you will write a C program much faster than a program in any other language. But that hasn't anything to do with how well-suited that language is for anything. You should state more precisely what exactly it is you are asking, because one could mistake you for a troll.
&gt; If you are trying to write a program that runs on multiple cores, you obviously care about speed. Haskell is fast, but it is still slower than C. So you're losing speed by using Haskell. Huh? Why use something slow like C? Just hand-optimize your assembly instead of relying on the compiler to do it for you. &gt; If you care about how long it takes to write the program, you still wouldn't use Haskell. I'm guessing C is easier than Haskell for the average programmer. Or you could use Python / Ruby / Node. In my experience, getting a finished and working program done in Haskell is faster than in any of those languages you mentioned, except in cases where a specific, very well-designed library already exists for a given task. Of course, it's easy to use C libraries from Haskell, so that's mostly a wash. Also, concurrency and parallelism are very different problems, using Python or C in a functional manner ranges from painful to impossible, trading off execution speed for development speed is not a binary choice, and the "average programmer" only exists in poorly-thought out arguments. I recommend not getting information about Haskell from people who don't know anything about Haskell or, I suspect, much of anything about programming in general.
You're conflating concurrency and parallelism. Concurrency tends to have latency requirements, not pure performance requirements, and preemptive scheduling does good things for this (not to mention all of the simplifying effects from this: I think it would be faster to write a highly concurrent program in Haskell than it is in any of the languages you cited.) As for parallelism, yes, it's a very specialized domain and the compiler does have to do a bit of heavy lifting. What things like DPH let you do is solve classes of parallelizable problems without gouging your eyes out.
I'm not sure if [that fallacy](http://en.wikipedia.org/wiki/Loaded_question) really applies here, or if the OP is just very misguided and uninformed.
&gt; If you care about how long it takes to write the program, you still wouldn't use Haskell. I'm guessing C is easier than Haskell for the average programmer. If, by that, you mean a programmer that knows C but doesn't know Haskell, then... it depends on the scope of the program, since you'd of course have to learn Haskell first to write the Haskell one. But for someone who knows both, the idea that it would be faster to write something in C than Haskell is, frankly, ridiculous.
You have the "ease of writing programs" part completely backwards. In general Haskell can be considered somewhat on par with Python and Ruby in terms of productivity, but when it comes to writing parallel programs it smokes them completely. Compared to Haskell, the languages you mention are *crippled* when it comes to parallelism, and they make even the most basic things ridiculously complicated and error-prone. Of course, you need to learn Haskell before programming in it becomes easy, but that's an obvious initial investment. After all, the reason you know Python/C/whatever is because you learned them at some point. The point on performance is more interesting. In my perception, Haskell's sweet spot is that it makes it completely trivial to write parallel algorithms with an "okay" base sequential time (typically within a factor 10 of C) and a "reasonable" parallel speedup. Of course this isn't that interesting if you believe that parallelism is about extremely performance-critical applications where programmer time is no object. But this - or so haskellers believe - is the mindset of the past: if you imagine programming a 100-core machine, you'll realize that getting reasonable parallelism will be a necessary part of programming even in applications where performance is only one concern among many. That means the future doesn't want parallelism to be optimal, it wants it to be easy - preferably as easy as sequential programming. This is what Haskell offers. (I should add that it's also possible to use Haskell for the old performance-critical guru-style parallel programming, with very good results. But it's not qualitatively easier than in some other languages, and I wouldn't recommend someone learn Haskell just for that.)
yup big agree on libraries last week i was considering using haskell for some integration with git. the haskell git libs seem well written but very incomplete, and i couldn't tell what was or wasn't provided meanwhile in perl land there were modules to automate and interract with anything git-wise, and everything i found was well documented i'm not trying to pick on the authors of the haskell git libs, this was just one example and i'd love it if haddock had a strongly-encouraged/required "example" section. type signatures are not documentation on their own, humans need some context
&gt; With lazy evaluation, reasoning about space is hard precisely because the space usage of a function depends on the context. In other words, you can't actually put a label "this runs in constant space" on values, it's not compositional, that's kind of the point. Is that really the case? Seems to me you could say the same thing about static types, and the necessity of excluding some programs that would be correct, but can't be made to type check. Consider a trivial case, like `map`--there are obvious ways in which the space usage caused by `map` *is* actually compositional, in similar manners to how map itself is compositional in what it does. Yes, it's a lot to keep track of, which is exactly why it would be nice to get the compiler to help us do so. &gt; Space usage is likely undecidable, there will be false positives. Again, the same argument applies to static types. You'd be accepting false positives in order to aggressively eliminate the true positives. The obvious conclusion of the above, of course, is that "annotations" are probably the wrong way to think about it. If you're going to do this sort of thing, do it *right*, and make resource tracking part of the type system.
&gt; The feedback that mentioned Cabal specifically was complaining about how cabal handles dependency problems and versioning. This. I have recently (just yesterday and today) been wrestling with cabal-install, trying to install both the latest Yesod release as well as Euterpea. Also had some struggles getting cabal-install in my userspace for my university account (the computers have ghc but not cabal-install). Let me just say, there was a lot more heartache and struggle than there should have been. I love Haskell to death, I'd choose it anyday over any other language, but good grief, I never had this kind of trouble with pip or ruby gems.
Type signatures actually are documentation on their own, when attached to a function with a clear name, when the type is very generic, and when you're sufficiently familiar with Haskell to read the types easily. There are quite a few functions in the standard libraries that qualify. Consider `concat :: [[a]] -&gt; [a]`. With a type that generic there are very few things it could be doing at all, only a handful that are remotely sensible, and pretty much one that makes sense given the name. Type signatures are not at *all* sufficient documentation for mostly monomorphic functions taking multiple types that are specific to a non-core library, this goes double for anything implementing a complex algorithm, and triple for anything using FFI bindings.
Sorry to hear that :( Out of my own curiosity, have you tried using cabal-dev? I would be interested to see if (in your opinion) it makes it as smooth as pip or ruby gems. If it does help, I hope that knowing people will be soon working on integrating cabal-dev's features into cabal (dcoutts reply) is reassuring.
I'm definitely going to look into it rather soon. Haven't tried it yet.
Do you need to be using the 'Main' package, rather than 'Hello'?
Nope. See new edits.
But it should be someone who is 33 when he is 111. 
Dear downvoters: wtf? This question, though posed from a somewhat "Haskell is inferior" point of view, brings up valuable discussion about why OP's assumptions are wrong. Please upvote to encourage this discussion.
Still same problem with Euterpea on Windows: it hangs compiling the last file, Euterpa/Audio/Render.hs :( I'm guessing this is an issue with Euterpea, though.
If it could be made to work as part of the type system I'd have no objection, but at the moment it seems to me that it would involve annotating something that involves information that is currently not exposed to the type system at all. Further examination may or may not invalidate that. It does involve a lot of nasty annotations, worse than INLINE has ever produced, it's just... I think the core idea I'm grappling for here is that there should be _something_ that prevents massive space leaks from passing unexamined. I freely admit that ideas go, this one is acting like a slippery fish in that every time I try grab it it wriggles away, but I can still hope someone more skilled than I can spear something useful from the pool. As an intermediate Haskell user at best, is there really nothing more I can do (in practice[1]) than wait to step on a landmine, watch my mental model of Haskell get blown apart, then try to reassemble something from the carnage? Quite possibly, and if so, _c'est la vie_, but I thought it at least worth asking the question. \[1]: In theory, of course, I could just learn the way Haskell works directly and up front, but in practice I'm going to learn from the running model, not the raw theory. I know the raw theory as well as I'm going to without more direct experience and still have a hard time guessing whether or not I'm writing leaks.
&gt; If it could be made to work as part of the type system I'd have no objection, but at the moment it seems to me that it would involve annotating something that involves information that is currently not exposed to the type system at all. Further examination may or may not invalidate that. Right. What I'm talking about would not in any way be a simple extension of Haskell, unfortunately. &gt; It does involve a lot of nasty annotations, worse than INLINE has ever produced, it's just... I think the core idea I'm grappling for here is that there should be something that prevents massive space leaks from passing unexamined. I freely admit that ideas go, this one is acting like a slippery fish in that every time I try grab it it wriggles away, but I can still hope someone more skilled than I can spear something useful from the pool. As an intermediate Haskell user at best, is there really nothing more I can do (in practice[1]) than wait to step on a landmine, watch my mental model of Haskell get blown apart, then try to reassemble something from the carnage? Quite possibly, and if so, c'est la vie, but I thought it at least worth asking the question. Well, with the caveat that I'm far from an expert, it seems to me that there are relatively few ways that *massive* space leaks can be introduced, for some suitable definition of "massive". So that's something that could be tackled both with tool support (say, something that could examine code and make some rough predictions about what the behavior at runtime will look like, possibly aided by annotations or interactive prompting about what you expect to happen) and reference material (e.g. guides on how to spot potential problem points, general idioms for getting the kind of behavior you want, etc.). More broadly, something that would be less drastic than trying to describe bounds on resource use but still cut to the heart of a lot of questions would be an explicit distinction between recursive and corecursive fragments of a program. I suspect that would still be very difficult to integrate with Haskell as it stands, though.
Are you using this with acid-state? Is that what it is designed for? Is index building a blocking operation? If so, what happens when the server restarts- do you have to wait for indexes to be rebuilt? Is there a way to replicate this across machines?
Hear, hear. Thank you for putting this together. A real public service.
I mostly use it when trying to figure out types inside crazily complicated local expressions. If I don't know what I should put in for some sub-expression, I can put in (?foo x y z) in for the sub-expression in question and get back a type error that shows the type that the expression (and the x y and z arguments) are constrained to by the surrounding code. This can be used interactively much like programming with 'holes' in Agda or a proof assistant as you work towards a goal.
&gt; On the topic of space reasoning, and optimization in general, one of the ideas I've been kicking around in my head is the ability to label functions with something like a claim that says "this runs in constant space", and cause the compiler to throw an error at compilation time if the annotation is false. http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.117.224&amp;rep=rep1&amp;type=pdf
Small complaint, but the link to your github page from hackage is broken (should be a fully qualified link)
What's that, you say? Complexity analysis of algorithms!? In the type system!? Oh yeah, they already did it in Agda. In 2008: http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.66.2327 :-)
http://i.imgur.com/tILWv.jpg - in honor
come on, the point of documentation is to ENCOURAGE people to use the code. every hackage package with haddock docs should have some examples. if it wasn't for haddock autogenerating the type signatures, they probably wouldn't be there either no, i stick to my point, we need more human-friendly docs
Ah, I see. Good luck with it then!
I think so. I had the same problem on Linux.
I recall reading somewhere that Haskell's internals were particularly ill-suited for running on the JVM, performance-wise? Something to do with taking far too much time for heap allocations, perhaps? Nice /r/doctorwho reference in the last bit though, can't sneak that by me :-P
Well the Haskell code style encourages non-descriptive variable names, things like foo foo' foo'' and other nonsense. I think the lack of comments, the lack of documentation, the strive for terseness, and the code style are all connected somehow. 
&gt;&gt; the space usage of a function depends on the context &gt; Is that really the case? Well, consider the function \n -&gt; foldl' (+) 0 $ [1..n] Its space usage is `O(size of input) = O(1)` (the input is an integer). However, the space usage of the function \n -&gt; [1..n] is `O(argument)` and the space usage of foldl' (+) 0 is difficult to quantify. Probably `O(1)` is the space usage of a function is meant to be the "space usage not counting the size of the argument". Looks quite tricky to formalize to me, I know of only a single attempt in the literature to do something like this. &gt;&gt; Space usage is likely undecidable, there will be false positives. &gt; Again, the same argument applies to static types. You'd be accepting false positives in order to aggressively eliminate the true positives. Sure. But the trick about Haskell's type systems is that it's quite lightweight and the type of an expression *is* (mostly) context free, i.e. there's always a most general type. In contrast, space annotations are likely very complicated, even for simple problems, and it's not clear whether they are worth the extra effort. To get a feeling for the problem, one may want to try a simpler problem first, namely strictness annotations and analysis. They are quite complicated in full generality, so we currently only use them in the most simplest cases (a `!` here and there).
Thank you. I already got email regarding this. It will be fixed with the next release.
I think the bar really is set with Ruby libraries and the 'example' documentation that comes with them. If you peruse a few Ruby libs on Github, the README files are pretty good with "How do I do X" and so on. However the Ruby reference documentation for libs is notoriously bad in comparison to Haskell So if Haskell could have the robustness of good reference documentation combined with some decent example work, then I think we'd be there
Yep, it's a known problem. Have a look at Euterpea's README.txt.
Correct, I designed it to be used with acid-state. Index building happens on the insertion of every single element and *fromList* as the default way of import is nothing else than a *foldl' insert empty*. And yes, this blocks. Non-blocking indexing is an interesting idea, but the implications are severe (loosing ACID for example). &gt; If so, what happens when the server restarts? This depends on your instance of SafeCopy. I just use *toList/fromList* what means that the index is rebuild when the server restarts. Technically it is possible to store the indices to disk as well, but: * Is it really faster restore an index from disk than recalculating it? * Avoid redundancy * My favourite point: You are free to add/delete an index in the code without the necessity of changing your SafeCopy instance and data types.
Dear drb226 - I really should add a "how to get started" guide, it seems. Sorry, but it's all work in progress currently. Maybe you also want to check out the Sudoku solver from the source distribution in directory examples/ for a something more complex program. What does irritate me is the 14sec it took for compiling. Should be much much faster, although I have seen unusual long compiler startup time just after rebooting. 
I've only been doing Haskell for 2 or 3 months, and it's nice to see that most of my current frustrations are shared by the community, and not just do to my noobness.
In general, by returning lazy values (kind of Futures), instead of actual results. But often one knows that the tail call will not itself call a chain of other functions. In those cases, the call can and will be done right away. It's not that a JVM has no stack, you know.
this has been forwarded through the appropriate channels ;)
Be assured that the low activity in the summer months is caused by - well, the summer. The project will be developed further steadily, but considering that I have a job, a family and a dog it may be not as fast as one might wish.
Well, it's too easy to propose what other people should do. "I just mowed the lawn." - "What??? I propose you contribute to GHC instead."
True, 100% Haskell compatibility is not an objective. GHC development is very active and it has so many experimental, though useful and widely used features that it would be hard to keep up. And even if Frege were 100% Haskell 2010 compatible, most of Haskell code would be not portable because it uses this or that GHC extension or even C code. (That being said, it is quite easy to port something. I took the liberty to adapt the source code of QuickCheck, it was done in half an hour.) To make this clear, Frege is not even remotely thought as competing with Haskell. Rather, look at it as a preprocessor for the Java compiler that happens to allow you to write pure functional Java code in a Haskellish syntax. AFAICS there is no other tool or language that would allow this at this time. Of ourse, if one doesn't want or need the JVM, one should use Haskell in the first place. No question about that.
Thanks very much for the effort, that was a joy to read. I wished there was a central place for organizing such issues. Like a task force keeping track of what needs to be done and what efforts are on the way. Maybe around https://github.com/haskell or https://www.haskell.org I have the (probably wrong) impression that we have those surveys every year, but in between little is done to address these issues.
The interesting thing seems to be that they can provide stream operations that would let you construct uncausal streams if they didn't have the type system to restrict their use. I'm not sure whether the modal part could be embedded into Haskell, but trying to embed linear types is a huge pain.
Yup. I guess that the modal part can probably be modeled as an applicative functor or monad. Their use of linear types is not so bad. It's equivalent to a monadic interface, which is basically what my reactive-banana library does. In fact, I prefer the monadic interface because it makes clear when values are shared and when they are duplicated: \mx -&gt; do x &lt;- mx return (e1 x, e2 x) vs \mx -&gt; liftM2 (,) (e1 &lt;$&gt; mx) (e2 &lt;$&gt; mx) 
That's not a tail call?
No. The last call is a call to (:). I might have forgotten some parens... (f x) : (map f xs)
I'm still mystified why Frege differs from Haskell in minor details that might as well have been like Haskell. 
Unfortunately, there is not a general answer. There a 4 cases that come to mind: a) too hard/impossible to implement like the layout rule that inserts a } when there is a syntax error on a place where } would be correct. Can't see how to do this in a yacc parser, where the layout is done in the scanner. b) The base decision that all primitive types should be inherited from Java is responsible for things like true/false instead of True/False, Strings that are no char lists, etc. c) can be implemented later (i.e. some missing things like arithmetic sequences) d) It's more natural for java programmers. (Like /* comments)
I don't want to have to work to write lazy code :)
Ah, well then. If it's been done in Agda it should be easy to retrofit onto Haskell, right? :P
PEG is more general than regex and already has a good haskell implementation ( http://hackage.haskell.org/package/frisby ).
Ah, SchÃ¶nfinkel's combinator basis, written in Church's notation, I see. ;] Curry's original presentation actually used the BCKW combinators as a basis. K is the same, while the others each do individual pieces of the compound operation that S performs. Taken as a programming language, I'd say that BCKW is actually nicer to use, and it's much simpler to implement S using B, C, and W than it is to implement B, C, and W in the SKI basis. Anyway, the BCKW basis is defined as: b :: (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c b x y z = x (y z) -- function composition, (.) or fmap on the ((-&gt;) e) functor c :: (a -&gt; b -&gt; c) -&gt; b -&gt; a -&gt; c c x y z = x z y -- flip k :: a -&gt; b -&gt; a k x y = x -- const, or pure on the ((-&gt;) e) applicative w :: (a -&gt; a -&gt; b) -&gt; a -&gt; b w x y = x y y -- join on the ((-&gt;) e) monad Do notice the complete definition of the reader monad contained therein!
With `she`, I wouldn't be shocked if you could!
&gt; Looks quite tricky to formalize to me, I know of only a single attempt in the literature to do something like this. I'm not sure if it's necessarily tricky to formalize, or just tricky to formalize via the standard formalisms. :] Big O notation, like many things, carries some implicit presumption of finite recursion, so of *course* it's going to stumble when applied to things like `[1..n]`. In fact, while it's certainly correct to say that `[1..n]` is O(n), that's useless to the point of being actively misleading. Consider how you'd describe the space usage of `[0..]`, then compare that to how you'd actually use such a value in a program. Not helpful at all! A workable formalization of space usage in Haskell would have to, at minimum, account for the properties of lazy data structures and the strictness of specific functions, for the simple reason that the space usage of something containing thunks depends on how it's used later. In the case of `foldl' (+) 0`, the strictness of the fold and of addition means that the accumulator will always be forced, while the list elements will be forced one at a time. So as a first approximation, `foldl' (+) 0` has a time complexity that's the sum of the time complexity of the input list's elements, and a space complexity that's the *maximum* of the space complexity of the input list's elements. On the other hand, `(+)` alone has space and time complexity as the sum of its arguments. The strict fold forces this at each step, which reduces the space to a series of constant values, but `foldl (+) 0` doesn't force anything, so it simply returns a single thunk whose time *and* space complexity are the sum of the complexities of the input list elements. The *allocation* complexity can also be recovered from the above in a fairly obvious way. What's notably unaccounted for is *sharing*. Whether that's something that can be left aside or that needs to be included directly, I'm not sure. Also, it's not really a coincidence that a sum function has a lot of sums in the resulting complexity expression--lazy complexity expressions are often going to look like functions with a structure similar to the functions they describe, and a useful formalism should make those expressions no less compositional and context free than the function itself is, as applied to the values it operates on.
a) It's not too difficult. You can to use error productions that interact with the lexer. I don't remember the details since it was 20 years ago that I did it. But certainly possible with Yacc. b) I understand the difference in what String is. I don't understand the syntactic difference with true/false, that's so easy to fix. c) Fair enough. d) Bad idea, IMO. You're not going to win over the Java programmers because the comments looks the same. And they will make porting Haskell a pain. 
It doesn't. It can't, because a Java program has no way to influence the garbage collector of the JVM.
Well, to paraphrase McBride, she probably works very well for complexity analysis, in the case that you need some complexity to analyze...
Wow, looks really cool. Thanks for pointing. But.. why are they not popular? Is the constant factor big? Or are people simply not informed?
a) Maybe, but in our case lexer and yacc parser are completely decoupled for reasons that are hard to explain in brief. This is not going to change anytime soon. b) If we want to make use of java methods, then we need to pass and receive booleans (among others). Hence, we have the choice of 1) making Bool abstract 2) making Bool an algebraic datatype with True and False as constructors like in Haskell and convert to/from java boolean each time 3) fiddle around with two Boolean types. Option 3 is completely unacceptable, IMHO. 2 would be workable, but not orthogonal to Int, Char, Float, etc., i.e. boolean would need special care in every corner of the compiler. Not at all to my liking. d) You may have a point here. Yet, from my experience I found that porting is not that big an issue. (I've ported QuickCheck, Wadlers Pretty Printing library and some solutions from the great language shootout and project Euler. Granted, the latter ones were only small in size and had almost no comments.) Consider also, as I wrote in another thread: &gt; And even if Frege were 100% Haskell 2010 compatible, most of Haskell code would be not portable because it uses this or that GHC extension or even C code.
D'oh! I always forget about READMEs Still had some trouble with the provided instructions. `runhaskell Setup.hs configure --disable-optimization` told me that I was missing dependencies, even though when I ran `cabal install x` for each of them it said I already had the library installed. Fortunately, after some experimentation, I found that simply adding this line to Render.hs fixed the issue: {-# OPTIONS_GHC -O0 #-} That's Oh-Zero, meaning optimizations disabled for that file. Running `cabal install` worked like a charm after this small adjustment. Of course, then I went to play an example song, but some of the examples use n+1 patterns, which apparently fail to load in the latest ghci à² _à²  fortunately this was easy to track down and fix manually. And the example actually worked! At long last.
Yeah, I figured. That seems like an argument against languages like this on the JVM altogether doesn't it?
a) The interaction is very minor. All the the parser needs to do is to manipulate the token stream. b) Bool is a type known to the compiler and part of the Prelude that should not be changed. It's easy for the compiler to recognize Bool, True, False, and pattern matching on Bool and convert those to the appropriate Java compatible data&amp;code. I've just done this for a platform where the types Bool, Maybe, Either have special representations and they need to be treated totally differently in the compiler. That doesn't mean that the user of the language needs to know. (BTW, I don't see how the Frege Bool type can be exactly the Java Bool type anyway. Since Frege is lazy the Bool type (just as all other types) has an extra bottom element in the domain that you need to account for. So you'll need to box all types.) d) This just feels like you're doing your best to upset Haskell users, which I think are a much more likely group of people to try Frege rather than Java people. BTW, I think Haskell on the JVM is an excellent goal. 
I don't think so. Wadlers Paper was written 1987. Do you know yet of what magnitude our space problems have been then? Yes, some space may be leaked (= garbage collected later than theoretical possible) but whether this gets even noticed is another question. And even if so, all is not lost. The transformations described in Wadlers paper could be implemented and, instead of modifying the garbage collector one could see to it that selection functions like fst and snd do their work immediately whenever the argument is already a tuple and not just a thunk. 
Ok, I now see how frisby (or PEG in general?) is weaker. In terms of regexes, it has only possessive quantifiers. Well, if I do the same for regex-applicative, the problem I describe will vanish as well. I'm not sure I understand all the implications of this decision. Anyway, it's good to know about PEG.
Sure, all java types are held boxed, whenever the need arises. But there are only two cases: either it's a reference type (there a generic box class will do fine) or it's a java primitive type. It's really no big deal and if the very pressing need arised I would simply replace references to Prelude.Bool.True with the literal true and Prelude.Bool.False with the literal false during name resolution. I am not at all interested in upsetting anybody. Yet, indeed my idea was that Frege would be most interesting to bright Java programmers (at least when a substantial portion of the JavaAPI is ported). After all, Haskell programmers already have the best language of the world, why the hell should they even care? Of course, I may well err in this respect. Time will tell. &gt; BTW, I think Haskell on the JVM is an excellent goal. I see forward to it and stand ready to dump frege in the trash bin then. Up to this day, it may serve some purpose, perhaps. The more so as this project is just in its beginning. It can take any direction, and should there be a user base that demands certain things there is no resaon not to realize it, AFAICS.
tl;dr on the telescope: &gt; It is scheduled dynamically to match project needs to the available weather.
Please don't use full Perl semantics; at least [omit backreferences](http://swtch.com/~rsc/regexp/regexp1.html).
Did no one else notice that he attended Harvard at 15-16? What a boss! "Curry was born on September 12, 1900, in Millis, Massachusetts, to Samuel Silas Curry and Anna Baright Curry, who ran a school for elocution. He entered Harvard University in 1916 to study medicine but switched to mathematics before graduating in 1920."
When I said I think Haskell on the JVM is a good goal, I also meant that I think Frege is the best option for this so far. Perhaps you could add a --haskell flag to the compiler to make it more compatible for those of us who prefer Haskell to some slight variation on Haskell.
As the happstack maintainer, I just wanted to say awesome! I have not examined the code yet. But I approve of any attempts to improve on IxSet. While IxSet is not that bad.. it is also not that awesome either :) 
That's putting it too strongly. The only straightforward mutable variables in the language are the transactional Refs. A define produces a Var, which begins as a global immutable variable, but can be given a mutable thread local value with the block-scoped "binding" construct. So, a Var which has not been explicitly made mutable already in the surrounding code can't leak effects out of a transaction, and no Var can leak effects between threads. Agents are a funny kind of mutable variable which can only be updated by sending an update function to be asychronously applied "later". Transactions buffer agent sends until commit, so that doesn't leak effects either. The only things that really leak state are Atoms, the "yes, I really want to program with manual CAS" variables, and the FFI (most of the standard library functions are instrumented to fail in a transaction, which also avoids visible global effects).
`n+k` patterns are no longer Haskell, as of last year.
"Perl semantics" is not about backreferences, it's about using left-biased choice and greedy repetition. http://www.haskell.org/haskellwiki/Regular_expressions#.28apple.7Corange.29
Back references have been around at least since Unix V6, probably earlier.
Do you consider the comments and true/false as the most irritating matters? More objections in decreasing order of relevance? 
Thank you :-) As you may remember from our conversation in #happstack I used IxSet before and generally it worked for me. The new library is of course heavily inspired by IxSet. The only drawback was that issue with index rebuilding. At first I didn't have a look at the code and thought it were feasible to filter by several indices in sequence. Problems arised when my bunch of data exceeded the 10k records. Long story short: I would have liked to simply improve IxSet or write a drop-in replacement, but I couldn't come up with one. I also read the discussion from one year ago and the attempts with kd-trees. In the end I just started hacking my own solution, since I really needed one. So this is just another proposal. Next week I want to work out some benchmarks and tests. Probably I should test it against IxSet to avoid a deterioration to the current state-of-the-art. Btw: The question arised above: Have you considered making acid-state transparent over network? This is feature I already think about for weeks.
This. I've been developing in Ruby for maybe five years, and I've always appreciated the community's efforts to make getting up and running with a new library or tool easy, but the lack of in-depth reference documentation means I end up reading the source code far too often. Which isn't to say that one shouldn't read the source code to the libraries one uses! Quite the reverse. But it shouldn't be one's first resort.
Another related/semi off-topic question: Does anybody know if acid-state works on Windows?
Since I've not used Frege I can't really tell. :) But anything that is different from Haskell when it could have been the same would irritate me (as a Haskell user).
Ah, that's even more thorough than I realized. I knew Clojure strongly encouraged immutability but those are some significant hoops to jump through in order to do things the wrong way. :] Thank you for the clarification.
I find it particularly useful for working with big trees, for example ASTs. PLs work and similar is particularly easy in Haskell - although doing it naively can lead to lots of boilerplate - generic programming libraries, techniques such as zippers, and the type system to enforce various properties make it quite useful and excellent.
Out of curiosity, why did you use the literate style? There doesn't seem to be much text in there (no more than a normal commented piece of work).
I have a few questions regarding Frege's runtime. - Do you have a mechanism for removing indirections (due to updates)? - Do you support [selector thunks](http://hackage.haskell.org/trac/ghc/wiki/Commentary/Rts/Storage/HeapObjects?redirectedfrom=Commentary/Rts/HeapObjects#Selectorthunks)? - Does every thunk update require a lock or CAS?
Hrm, I'm not sure I'm convinced. But I found the paper I meant: [Possibilities and Limitations of Call-by-Need Space Improvement][1]. :-) I think the problem you're getting at is that the usage of an expression is not a single number (= "maximum space while evaluation it"), but a function from contexts ("how much do I force") to numbers. I'm not sure whether this assignment is *compositional*, i.e. whether the "space function" of an expression only depends on the "space functions" of its subexpressions. Concerning sharing, it's important; the following well known space leak is solely due to sharing: average xs = sum xs / length xs [1]: http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.8.4097
If you can read Java, you can look it up here http://code.google.com/p/frege/source/browse/frege/rt/Unknown.java Short answers to your question: Yes, evaluation of a lazy value caches the result which is some real data, so repeated evaluations will get back the value immediately. No, there are no selector thunks. Yes, in anticipation of parallel executions, every update is synchronized. 
&gt; I think the problem you're getting at is that the usage of an expression is not a single number (= "maximum space while evaluation it"), but a function from contexts ("how much do I force") to numbers. I'm not sure whether this assignment is compositional, i.e. whether the "space function" of an expression only depends on the "space functions" of its subexpressions. I would certainly hope it would be compositional, given pure functions! To imagine otherwise seems somewhat worrying. Now, it could certainly produce a more complicated function. In the worst case scenario, we'd effectively have "dependent complexity" if the value resulting from one subexpression determines the space usage of another in a non-trivial way. But we can still approximate upper bounds, and furthermore those are exactly the cases that actually *are* difficult to reason about. If a formalism handles the simple cases easily and draws attention to the difficult cases by working poorly, that's already a win because now you know where to focus your attention. &gt; Concerning sharing, it's important; the following well known space leak is solely due to sharing: &gt; &gt; average xs = sum xs / length xs Oh, it's absolutely important. What I'm not sure of is whether it needs to be represented directly in the resource complexity formalism, or if it's something that can be floated out and only considered at the meta level, as would normally be the case for e.g. preconditions on values. In particular, I'm not convinced that the issue of sharing isn't mostly orthogonal--starting from a clean laziness-aware approach of the sort I've suggested, the effects of sharing seem like something that could be analyzed quasi-locally, e.g. applied to a single value and its connections (which may be highly non-local in the program structure, but localized in the resource complexity expression). The paper you linked to is interesting, but seems to be tackling both laziness and sharing simultaneously. What I propose is separating laziness (which would then also apply to call-by-name, not that anyone uses that) from sharing (which might then also apply to uses of memoization in a call-by-value setting!) and considering them independently before building a combined approach. I even have a few rough ideas of how one might begin on this, but to be honest I find computational complexity one of the most crushingly dull topics in computer science so I'm not terribly enthusiastic about spending a lot of time on it... :T
I hope you're taking these criticisms in a friendly spirit, since this seems like an exciting project. That said, I don't think that handling both java and Haskell style comments would be that hard, and perhaps it would be an acceptable choice? In my experience, the closer two things are, the more people want to try to resolve the remaining differences, a number of small outstanding differences are harder to remember than just switching mental modes entirely. My guess is that if you start to acquire a user base (and I hope you do), then there will be a gradual and consistent pressure to whittle down at the differences. That said, starting out with some quirks for ease-of-implementation purposes and to get something working out the door, and iterating towards more polish isn't a bad plan, whether or not its fully intentional.
I know that when I was first interested in picking up Haskell I was doing lots of JVM based stuff, and quite interested in a Haskell for the JVM. And I think you underestimate, perhaps, the number of weekend/evening/hobbyist Haskell devs who would be excited by a JVM-based language they could have a shot at integrating into the dev ecosystem of their day job.
&gt; I hope you're taking these criticisms in a friendly spirit Absolutely. I think I can tell when one is basically constructive and when one just wants to spray bad karma :) &gt; handling both java and Haskell style comments would be that hard Technically, no. But I want to avoid clumsiness. I am almost convinced now to implement Haskell style comments. After all, if some Java only person really tries frege, he's got to learn so much new that the comment syntax won't make a difference, would it? &gt; In my experience, the closer two things are, the more people want to try to resolve the remaining differences, a number of small outstanding differences are harder to remember than just switching mental modes entirely. I couldn't say it better! Yesterday I thought: I wonder if people would criticize that "Frege is not Haskell" if I had choosen an entirely different syntax, terminology, etc. &gt; That said, starting out with some quirks for ease-of-implementation ... Thank you for the helpful comment. Yet, at some point people will have to realize that frege is *still not* haskell, even if there would be a remarkable set of source files that happen to compile on both systems. 
Sure, but they were popularised by Perl, and are certainly part of the full "Perl regexp semantics".
Sure; it's just that when people talk of Perl regexps they usually mean to include backreferences.
I thought GHC might maintain some legacy support for them a bit longer, though. Guess not.
Doesn't it? You may need to enable them as an extension now, using a compiler flag or pragma. But I'd expect GHC to continue supporting H98 as defined in the report.
GHC prefers to stick to the latest standard by default; you can use `-XHaskell98` or even just `-XNPlusKPatterns`. Or at the top of a source file: {-# LANGUAGE Haskell98 #-} or {-# LANGUAGE NPlusKPatterns #-}
I completely agree. There are lots of would-be functional programmers in industry who are constrained to work with the JVM platform but not necessarily Java. Many are turning to Clojure and Scala, but I speak for myself and others in saying that we would much rather something Haskell-like for the JVM (ideally as Haskell-like as possible, which then gets you many things for free, for example an emacs mode). I would love to contribute to this project, perhaps starting with a REPL, just need to find some time!
I'm very excited by Frege, but I have a very simple and stupid question. How is "Frege" pronounced? Thanks!
Ironically, I was just reading that "Nearly all serious accidents in which software has been involved in the past twenty years can be traced to requirements flaws, not coding errors." Thus "simply trying to get the software `correct' in terms of accurately implementing the requirements will not make it safer."
On the other hand, aggressively encoding requirements directly in a machine-checked way can be surprisingly effective at finding bugs in the specification. If you have two components and the only way to connect them requires an obviously impossible function, that's a good sign that at least one of them has a design flaw.
I wish there was a good systems-level functional programming language. Ideally, it should: * Probably use strict evaluation by default * Be able to control memory representation exactly * Handle mutating variables well, with zero copying * Be able to cast/reinterpret bits of data with zero copying * Be extremely safe from any errors, including null pointer exceptions, buffer overflows, and division by zero * Have all the other nice features that any modern programming language has over C, such as fast build times and no preprocessor. Habit seems like the best fit for this, but so far they've released a paper describing the language and nothing else. DDC also looks promising. I just wish there was a good functional C replacement.
What's the advantage of using this over straight LaTeX? How is it differentiated from pandoc?
Great write up. Congrats on getting the grant!
You can generate LaTeX code automatically! For example, when I wanted to make in LaTeX a big commutative diagram with periodic structure, HaTeX saved me from big amount of ugly LaTeX code. (:
Is it also possible that specifications themselves will improve if those who create the specs realize that there are better language features available that can enforce the spec in the first place? In other words the specification process itself may have some dependency (at least in the minds of those making the spec) on knowledge of language features. Can knowledge of Haskell up the game of spec writers? Is there a culture of mediocrity in spec writing that can be fixed/helped by knowledge of Haskell?
What do you mean by 'transparent over network'. The next step in acid-state is to add support for multimaster replication..
There another nice thing about Clojure, which is that you get a lot done whilst staying in the shallow end of the language. There's no need for Java style code, crud etc., to get into agents or STM etc., though it can be a better way to do things. I built a relatively high performance object server and interfaced it to .net using nothing much more than maps, a SQL library to talk to oracle and some socket code. I'll freely admit that it was likely 3 or 5 times more code than a star could get, but I blasted it out quite fast and it worked well. Personally, I'm somewhere in the area between the drones and stars. Yeah, I took one look at the bug list for scala, and decided if I couldn't understand the bug reports, it wasn't for me. 
A REPL would be great! Anybody who has to contribute something is, of course, very welcome.
Go to translate.google.de and select "Von: English" and "Nach: Deutsch", then type in "Frege". Then click the button that reads the word to you. The german pronounciation you will hear is correct. (With the english one, I am not so sure ...)
There is also [Hex](http://luispedro.org/software/hex), it hopes to become a reimplementation of Tex in Haskell. Hex produces DVI files from Tex code. It is different from HaTeX. HaTeX helps automating the process of creating LaTeX code. IMO
I also generate LaTeX code from Haskel by way of XML. As part of the contracting work that I do, I produce very large test scripts that are exported in a variety of formats, including PDF. I use Haskell to generate and randomize various sets of tests are divide them across multiple platforms in an attempt to keep coverage fairly even.
Clearly it should be `4i(ln(1-i) - ln(1+i))`.
I used frisby for lazy "infinite" parsing back in 2007 and was very happy with it. My parser definitions actually came out much cleaner than the equivalent in Parsec, but I did miss Parsec's friendly error messages though. All the above having been said, I'm not a very informed person so ymmv.
I have to say, hackage and `cabal install &lt;package&gt;` don't encourage reading READMEs. I hope hackage 2.0 will show any README (and perhaps INSTALL) files in the package github-style, so that not everything has to be crammed into the .cabal description.
Wrapping it in a single function call is not encoding laziness (call-by-need), it is encoding call-by-name. Unless that function is memoized. There are also some more nuanced Parallelism concerns (what happens if two threads demand a value from the same thunk? A good laziness implementation can block on another thread computing the value, and do other useful things in the meantime).
Its not a function call, strictly speaking. It's hard to describe exactly what happens because I'm only familiar with the usage details and not the implementation details. The lazy-seq call (a call provided directly by the interpreter) tells the interpreter to only compute the head of the sequence, and only when asked for. The other approach is via the delay and force macros which operate on individual statements (by which I mean anything that evaluates to a value, which ideally speaking is the whole language, though I want to say there are a couple of exceptions). In both cases, the thread locality is determined by the programmer, and depends on how the values are bound. Vars are thread local, and so evaluating a value in one thread does not (should not, I haven't tested this myself) produce its value in another thread. The other mutable data types have different concurrency behavior. But in general, you have to specify that you want a value to be lazily evaluated across threads. You get it easily, but not for free. I'm going to experiment with this, and I'll get back to you with more details.
Oh, sorry, I thought you meant wrapping your value in a function wrapper, where you (probably) meant calling a function that does the lazy suspension for you, properly.
If the kids get this lesson well, they'll be well on their way to mastering Haskell and mathematics. Maybe it should be week 6 and 7 too. :p
It will be week 6, because there's no school for.most of next week for my own class... as for week 7, I definitely want to do animations, which are just Float -&gt; Picture... so it's more of the same anyway, but in a slightly different way, and they are very eager to get there
Awesome. Functions are the fundamental idea of modern mathematics and programming. Embracing them is the first big step of learning math.
I'm excited to do one of these, even though it won't actually play any sound: http://wheelof.com/whitney/
Glad to read my work is helpful. (:
Correct. HaTeX only helps you to work with the LaTeX code. It does **not** compile that code to DVI or PDF formats.
Type directed name resolution looks wonderful, and for more reasons than this problem alone. I realize I've been craving it.
It's used in the WordNet package, and I found its use to be somewhat confusing.
The problem is that TDNR is a big proposal, and not unanimously accepted. I don't think I'm the only one who's reticent to signing a blank check for TDNR simply in order to solve the particular problem of record names. For the record, I also don't think record field name resolution is such an important problem (first-class fields are much more important). Rather, it's because it's not that important that the fact that we haven't solved it yet in whatever way was easiest is such an eyesore. I'd like it if people stopped trying to piggyback big design decisions on this, so we could get it solved quickly. Personally, I'm in favor of the simplest option, which is to allow prefixing the field name by the type name ("Record.field"). If there's a module by the same name, just raise an ambiguity error. If we really want the compiler to resolve fields for us, then something like a "Has_foo" typeclass is more conservative and therefore preferable.
&gt; Personally, I'm in favor of the simplest option, which is to allow prefixing by the filed name by the type name ("Record.field"). If there's a module by the same name, just raise an ambiguity error. It may be off topic, but this is how it is done in [Frege](http://code.google.com/p/frege/). In addition, *r.f* is allowed and means *T.f r* if r is known to be of any type who's type constructor is T.
Yea, that's correct.
To feed the debate, here are a few concrete proposals I can think of. My personal favourites are P1 and C2. **== Proposal P1 ==** (P for prefix) A field of a record type can optionally be prefixed by the name (that is, the name of the primary type constructor) of the record. For example: data Record a = Rec {x :: a ; y :: a} foo r = Record.x r + Record.y r -- equivalent to x r + y r If there are several record types in scope with "Record" as their primary constructor, or if there's also an imported module by that name, then an error is raised at compile-time. **== Proposal P2 ==** Same as P1, plus: if a type synonym T is defined as type T {...} = R {...} where R is the primary constructor of a record type (or, transitively, another type synonym satisfying the same property), then R's fields can also be prefixed with T. This gives a way of renaming record types to avoid clashes. **== Proposal C1 ==** (C for class) For every valid field identifier "foo", there exists a type "T" such that a class "Has_foo" is already defined as follows: class Has_foo r where foo :: r -&gt; T set_foo :: T -&gt; r -&gt; r try_foo :: r -&gt; Maybe T -- for variant records Every time a record type with a field "foo" is defined, it is made an instance of "Has_foo". The record update syntax "r { foo = a }" also imposes a "Has_foo" constraint on r. If, at some point, there are two record types in scope whose fields foo do not always have the same type T, a compile error is raised. This proposal doesn't require any type system extensions, but the constraint that all fields with the same name have the same type is a little heavy and looks like it will have messy corner cases. Still, it does remain strictly more permissive than the current system. EDIT: actually, this proposal is no good because it doesn't allow the type of fields to vary according to the record's parameters, as in "data R a = R { foo :: a }". Is there a way this can be fixed while remaining in conservative type-land? **== Proposal C2 ==** For every valid field identifier "foo", a class "Has_foo" is already defined as follows: class Has_foo r a | r -&gt; a where foo :: r -&gt; a set_foo :: a -&gt; r -&gt; r try_foo :: r -&gt; Maybe a As in C1, every record with a field "foo" is automatically made an instance of "Has_foo", and record update syntax also carries the corresponding "Has_x" constraint. This proposal requires multi-parameter type classes and fundeps (a formulation using type families is of course also possible, at the cost of a rather ugly "Type_foo" associated type). On the other hand, it allows full freedom in reusing any field names between any records, and is still backwards-compatible in the sense that any programs which compile in the current system should still work (bar the monomorphism restriction). **== Proposal C3 ==** A class "Field" is already defined as follows: class Field f r a | f r -&gt; a where get :: f -&gt; r -&gt; a set :: f -&gt; a -&gt; r -&gt; r try :: f -&gt; r -&gt; Maybe a and for every valid field name foo, there already is a definition data Field_foo = Field_foo Then, a definition such as data Record a = Rec { x :: a, y :: a } generates the following implicit declarations: instance Field Field_x (Record a) a ... instance Field Field_y (Record a) a ... A modicum of syntactic sugar, such as ".x" for "Field_x", would probably also be welcome. This proposal even allows us to write "generic field access patterns" that are polymorphic in the particular field name, but on the other hand it requires MPTC and fundeps (or TFs), isn't backwards-compatible (there's no "foo" accessor anymore), and implies some new syntax. **==========** These are off the top of my head, so please forgive any oversights or inconsistencies. These proposals are in competition with the TDNR proposal on [The Haskell' Wiki](http://hackage.haskell.org/trac/haskell-prime/wiki/TypeDirectedNameResolution), and a possible sub-modules proposal which I don't think anyone has formulated yet. What do you think? Preferences? Arguments? Did I overlook any interesting solutions?
This is related to type inference: int f(struct foo x) { return x.y; } The fact I had to tell the compiler `x` is a `struct foo` is what makes the name resolution of `x.y` so easy. In effect, in C, you're almost always indirectly fully-qualifying every record field name you use. In Haskell: f x = y x What is the type of `f` and `x`? 
Any thoughts on how well that works in practice?
I can't directly help in providing a good solution, but I have been reading a lot about this problem and I have seen it's been bugging people for years (too bad for a language with so much power built in). I can only suggest you to fix a deadline for a new proposal and go select the one with the highest customer feedback. I vote for TDNR.
Right, it seems like TDNR would not be compatible with first-class records. &gt; Personally, I'm in favor of the simplest option, which is to allow prefixing the field name by the type name ("Record.field"). If there's a module by the same name, just raise an ambiguity error. I wonder if there is a way to make the idea of module and the data type namespaces you suggest more coherent. Not sure what I mean exactly...
&gt; I also don't think record field name resolution &gt; is such an important problem Just about everyone who uses Haskell for real work at their job runs into this problem. (And yes, that is a quickly growing population.) It is important. I agree with you that: &gt; first-class fields are much more important But as you say, &gt; I'd like it if people stopped trying to piggyback &gt; big design decisions on this, so we could get it solved quickly.
Am I the only one who reads "the record problem" as "we have no proper *extensible* records"? I think fclabels et all already do a very good job at fixing record selection.
Some points for discussion... (of course I know the answers to these, but they might be things that you want to discuss, or want to think about for when one of your students asks -- though perhaps not explicitly -- or things you want to use as jumping-off points for future lessons). We can take stars and make them awesome, we can take elephants and make them cute, but why can't we take our pre-existing elephants and make them tailStickingUp? Because tailStickingUp doesn't satisfy your assumptions -- but it might be useful to discuss this. Also, one of your assumptions is "Given any thing, we know how to make it awesome." What is a thing? Can we make an awesome 2, or an awesome awesome (on a tangent, we *can* make an `awesome (awesome elephant)`, or even `pictures $ iterate awesome elephant` -- some truncation needed). This leads to a discussion about types.
Hmmm ... by what measure? For me very well. But this is extremely subjective as I myself designed and implemented it that way. A weak point is without doubt that sometimes one needs to annotate certain bindings because of the left right bias of the type inference. Where *left right* refers of course to the code the type checker sees **after** desugaring, reordering of let clauses in dependency order, etc. 
I personally use Haskell every day for my day job and do not consider this to be a problem. It never prevented me from doing my job, it never even prevented me from doing my job in an elegant way. Keep your modules small and your namespaces tight and this shouldn't be that big of problem. But, if most people think it is a problem, feel free to solve it. My preference would go out to be able to have multiple modules per file. That would somehow naturally solve the record label problem, right?
Sorry, you're right, the title is a bit misleading. I should have said the Records Namespace problem. This issue is different from what [fclabels](http://hackage.haskell.org/package/fclabels) is solving, and also different from the extensible records problem.
Subjective experience is the only measure I was looking for, yes. :] I know that I've designed and implemented things for my own use that turned out differently in practice than I expected, both positively and negatively. The approach seems straightforward. After using it, did anything about it strike you as working better than expected, or impact how you structured your code? How awkward is the occasional need for annotation?
Something along the lines of fclabels et al should be built in to the language. That's not to say that that we shouldn't solve the problems of field name resolution and extensible records: but if we introduce new syntax to solve these problems, we should take the opportunity to make lenses part of that new syntax.
&gt; I wonder if there is a way to make the idea of module and the data type namespaces you suggest more coherent. Not sure what I mean exactly... One obvious solution here would be to uniformly extend `where` clauses to allow exporting identifiers with an optional namespace prefix determined by the definition the `where` clause is attached to. Functions would probably have to be disqualified as a special case due to ambiguity from multiple patterns and having lowercase identifiers (which aren't valid for namespaces). This would make modules the trivial case (named `where` clauses with no body), give namespaces to data types defined in GADT style, *and* extend to type classes as well. Getting the syntax right to be minimally invasive while still having the existing forms as special cases would be tricky, though.
Some *trunk*ation needed?
Seriously though, good points. IIRC, type signatures haven't been used so far, and a student asking these kinds of questions might give cdsmith a reason to talk about them, or maybe even to get into higher order functions!
&gt; After using it, did anything about it strike you as working better than expected, or impact how you structured your code? I noticed that I tend to order function arguments in "OO-style", so that the "main" data structure is the first argument. This is so because those field labels are only a special case: in Frege one can write arbitrary functions in the namespace of a data type, i.e. every **data** may have a **where { defs; ... }** clause much like type classes and the type constructor acts as a reference to the type's namespace. Constructor fields, then, are syntactic sugar that will be transformed to selector functions in the namespace of the type.) &gt; How awkward is the occasional need for annotation? Not that much (especially if one tends to annotate ones functions anyway). But I still feel the need to enhance on it because it runs counter to the normal order of things: namely that the type of an expression is deduced from it appearing as some function argument and not the other way around. Also, it is annoying, because plain old HM type inference should work without annotations. So, at least the cases where the type of a binding v could be finally inferred independently of occurence of that v in an expression like v.f should be ok. Like, for example foo v = v.done &amp;&amp; isOk v isOk :: T -&gt; Bool Here it is clear that v :: T and yet type inference complains about v.done not resolvable. If one exchanges the order of the conditions, it works. Of course, to do that the right way makes the type checker slightly more complicated, hence I did not do it yet. 
The . is a record-access operator in C. While C uniformly requires that you specify the types of arguments, it's not necessary in this case to decide what to do; if you were to add a record-access operator to Haskell, type interference would work just fine on f x = x . y 
Further, the "y x" idiom used in Haskell is problematic because: - it's a barrier to understanding to everyone coming from a normal programming language, since they all use x.y or x-&gt;y - it pollutes the global namespace with accessor functions for specific record types 
What if multiple types have a record named `y`? It's simple in C because of *monomorphism*, not type inference. In Haskell, we'd need to either extend the type system (to express ad-hoc "has a record by this name with this type" constraints), implicitly lift all records to type classes (which is a complicated step down the road to structural subtyping), or make unqualified record access an error on polymorphic types (limited, but covers a lot of the actual scenarios).
&gt; I personally use Haskell every day for my day job and do not consider this to be a problem. OK, I'm sure this depends on the particular problems spaces you happen to work in. &gt; Keep your modules small and your namespaces tight and &gt; this shouldn't be that big of problem. This is an issue of scale. What if doing that results in hundreds more modules and namespaces to manage? Or even thousands? All in separate files, in the current system. It's quite common in real-life applications for the density of data fields in an application to be very high. btw glad to hear you're using Haskell successfully at work!
We've talked informally about types, but right, we've been avoiding type signatures. Rest assured, students are seeing them, in their error messages, but rather than making it a topic as yet, I'm letting them become intuitively familiar with what they mean. After animations come simulations, where students will define their own data types to represent a state of the world, and write a "step" function that changes the world as time passes. That provides a great setting for a more systematic introduction of types and simple data structures.
http://www.thepimanifesto.com/ for a list of formulae that work better with pi. I personally still prefer tau, and I think it would genuinely have made my maths education and introduction to radians easier. I'm a little perplexed at the volume of vitriol it seems to create though. It's interesting to think about but I'm not about to claim that anyone who disagrees is a moron and/or charlatan.
This is a problem in Haskell because Haskell does not have syntax for record access. It's not a problem in C because C does. If you access a record in something, C knows it's a struct. If Haskell can say (+ 1) :: (Num a) =&gt; a -&gt; a I don't see why it couldn't be extended to say something like (. someField) :: (Record { someField a, ... } b) =&gt; b -&gt; a [inventing new syntax here, hope it's clear]. 
&gt; It's quite common in real-life applications for the density of data fields in an application to be very high. This probably depends heavily on the application, and to a lesser extent on one's preference in idioms. If one was working mostly with large, structured collections of small, simple data, and preferred using an idiomatic style with [a more Backus/Iverson aesthetic](http://en.wikipedia.org/wiki/J_%28programming_language%29) it'd be easy to solve the problem by *not using named fields at all*, and not really missing them either.
I would call it the *shut up and take someone else's money* approach.
Yes, that would work, but let's be clear: you're not just inventing new syntax, you're inventing a fundamentally new form of type constraint. Something like this could be encoded already with minor difficulty, a lot of boilerplate, and a royal flush of type-hackery-enabling GHC extensions--but would still likely fall short of the ideal.
Well, sorta. It's new to Haskell, but it's been the norm in conventional programming languages for what, 25 years? 
No, it hasn't. The much simpler monomorphic version is the norm in languages with extremely limited type systems. This could be added to Haskell pretty easily, and would work just fine with strictly monomorphic types. The polymorphic record constraint in your example is another matter entirely. I don't know if *any* existing language does that. It might work out to be equivalent to structural subtyping, in which case... I think OCaml would qualify as already supporting it.
Even with monomorphism, if you have to infer the monomorphic argument type from the record access expression, it's more problematic than if all types have to be specified as in C. That's why I claim it is also related to type inference. But I agree monomorphism is the main thing. I see it as a trade-off between specifying the record via annotating the type of record values, vs. specifying the record via giving fully-unique names when using field accessors. If you use polymorphic record systems (e.g: the HasFoo approach), you can do better than either. But I wouldn't say that Haskell is strictly worse than conventional languages in this aspect, because in a different aspect it is better (it can infer the monomorphic record type from the uniquely-named field accessor that I use).
Well, x.y and x-&gt;y are notations that are already taken, and Haskell's notation is so different already, that I don't think it would be beneficial to try and emulate this particular bit of syntax. Whether it populates or pollutes the global namespace is a question of your expectations, I guess :-)
I think we're talking about different "it"s. The "it" I'm talking about is straightforward syntax for record access that doesn't pollute the global namespace. If you want that polymorphically, there's the C++ approach (where you have to specify the type) and the Ruby and Python duck-typing approach (where "type-checking" is done at run-time). I think what you're saying doesn't exist yet is the combination of that with Haskell's rather nifty compile-time type inference behavior, and indeed I haven't seen that combination yet. What I'm getting at is that, in a world in which other programming languages exist and most users are coming at this from the perspective of "I need to write software to do X and work with existing tools Y, Z, ..." rather than "I love Haskell and want to do X in Haskell", this is *Haskell*'s problem, not users' problem. 
Indeed!
I use Haskell for real work all the time, and I run into this problem, but I don't care. I just throw in a prefix on all my record fields, and the problem is solved. I'm much more keen to get first-class lenses in place without TH kludges... and my concern about this type-directed name resolution stuff would be that it's so unimportant, and mucking with the record system in a different way might interfere with that. That said, I like sfvisser's proposal of multiple modules per file. It would require a bit of thought with regard to GHC's policy of resolving module names to file names to pull in dependencies, but if all the modules were required to be "Foo.Bar" where Foo is a common module name prefix, and the convention were to name the file Foo.hs, then that would give at least a small finite number of places for GHC to look, and it could give an ambiguity error if a module is defined in multiple places.
&gt; we should take the opportunity to make lenses part of that new syntax Or, at the very least, we should make sure that the TDNR / namespace solution is compatible with a solid way forward on getting lenses in the language.
yeah... I'd rather have something like fclabels and associated syntactic sugar than the "do" syntactic sugar for monads
Since you've stepped on that particular landmine, I should mention that I don't think the Haskell community is particularly interested in mimicking every detail of what mainstream languages do well. Many of Haskell's strengths are direct consequences of the fact that it wasn't designed by starting from the mainstream status quo and simply adding features to it. In the same way, we welcome newcomers and are eager to share all the fun Haskell gives us, and we won't give up on that fun to satisfy people who only see what the language takes away and not what it offers. So, the record problems are problems to the extent that they annoy Haskellers, and what someone coming from a mainstream language would expect is not (at least in my humble opinion) particularly important. Since the solutions other languages have developed for these problems aren't directly applicable to Haskell either, "language X already solves it" is irrelevant to us.
&gt; The "it" I'm talking about is straightforward syntax for record access that doesn't pollute the global namespace. In Haskell, this is called "pattern matching". Honestly, I'm not convinced that encouraging the use of named field accessors is a good idea at all. Based on how often people coming to Haskell from other languages seem to use `head` and `tail` instead of pattern matching on lists, I expect that record access will be misused more often than not. As far as I'm concerned, making it easier for people familiar with other languages to use record accessors would, all else equal, be an argument *against* any proposal. I'd rather encourage people to learn better ways of solving problems.
&gt;it pollutes the global namespace with accessor functions for specific record types I agree that this is a problem. It forces everyone to use their own flavour of resolution for "obvious" accessor names (e.g. unFoo or trailing underscore) which hurts clarity as it's another local convention that needs to be understood.
As someone who's been passionate about functional programming for about 12 years now, I really hate hearing this, because it supports what a lot of people say about functional languages: that they're never going to be real, useful languages, because their maintainers and advocates are too in love with peculiar notions of elegance and don't respond to real-world use cases. Records are a great case in point: record data is the *norm* when you are working with data sets of any size. Haskell has the best syntax for the things it has syntax for, and type inference and laziness are great. I want Haskell to be the norm, but it's going to remain a toy language as long as an experienced programmer coming from another language hits a barrier like this the first time they try to implement a real project. 
&gt; That said, I like sfvisser's proposal of multiple modules per file. Haskell '98 says that recursive modules are allowed, and I think it implicitly it allowed multiple modules per file (it's been a while since I checked up on this). The most important thing here is for GHC to **STOP USING MODULES AS A UNIT OF COMPILATION**! My modules are for logically organizing my functions and this coopting of modules as a unit of compilation really cramps my style.
How does pattern matching address global namespace pollution? Given that approximately all data in all databases everywhere, and all data exchanged over the wire anywhere, are stored and transferred in record format, what better alternative to record accessors are you proposing? 
It's peripheral to this discussion what the syntax should be, but there should be a syntax. I think the expectation that I be able to ask for a field in a record by its existing name is not unreasonable. Global namespace pollution means I can't do that. For example, say I have a 97 different types of record that each have a field named "id"---now my Haskell program is a maze of qualified names, or my Haskell records look nothing like the records in the database or the C program. It's important to be able to interchange data in simple, intelligible ways even if it wasn't created in your language of choice. 
If there must be differences, it would be nice to have a hs2fr tool to automatically convert (a subset of?) Haskell code to Frege. And then perhaps have this preprocessor built into a compiler. I wonder how difficult this would be.
I'm pretty sure that if you actually worked through most of those and figured out what the semantics of using pi in the formula is, the majority would be clear examples of things implicitly built from considering a half-way rotation, or defined by reflection in a plane. The former is only meaningful in terms of a full rotation (i.e., tau), while the latter is mathematically less elegant in multiple ways and probably still needs the concept of a full rotation to make sense. Obviously there's not a big difference in which notation is used, which is why mathematicians just seem to shrug about it. What really annoys me about the whole thing is just how terrible the attempts at arguing in favor of pi are; full of blind symbol manipulation with no understanding of where the structure of the formulas comes from. Sites like that go a long way to convincing me that making the semantics of the notation more explicit really *is* important, and that it's worth deliberately using things like tau for that reason alone. Which is roughly the argument that winterkoninkje makes in other comments here.
Does that mean that things that work now, like `ghci Main.hs` and relying on GHCi to find all the other files that `Main.hs` depends on, will break? I'm in agreement in theory with multiple modules per file, but I don't think it's a good trade-off to break the existing automatic dependency stuff that GHC gives you. I don't consider "you should be using a Cabal file" to be good enough, either... if I'm throwing together a quick example and loading it into GHCi, I'm not interested in typing `cabal init --no-comments` and then editing the resulting file to explicitly list the packages I'm using. Hence my suggestion about a compromise that lets GHC still do that part for you.
It avoids global namespace pollution by not requiring superfluous names just to do anything. The easiest way to solve problems is not creating them in the first place. Not to say that better namespace management wouldn't be useful, but record accessors aren't *that* important when most interesting data types shouldn't use them anyway. &gt; Given that approximately all data in all databases everywhere, and all data exchanged over the wire anywhere, are stored and transferred in record format, what better alternative to record accessors are you proposing? What do serialization formats have to do with the representation of live data?
&gt; What do serialization formats have to do with the representation of live data? Now you're just being silly. 1. Having to transform every piece of data you access to bring it into conformity with your language's quirks is ridiculous. Nobody but a language fanatic would put up with it. 2. Given that you're going to do that, you still want good syntax at the transformation stage.
If you're waiting for Haskell to get popular, you're going to be in for a long wait. It was only a quarter tongue-in-cheek when SPJ said that the main goal of Haskell was to avoid becoming mainstream. Make no mistake: the people driving the language development are academics, and they have some abstract things they'd like to sort out --- the fact that this turns out to be remarkably useful is completely coincidental to them. Of course, anyone is welcome to try and fork the language and get a stable platform, and someday someone might; but I'd guess that time is still a long way off.
&gt;Something along the lines of fclabels et al should be built in to the language. I'm not sure I agree with that. To raise another issue, it seems to me there are at least a few reasonable ways in re to what could be done about records, some more and some less ambitious, and a lot of any such functionality would be implementable as a library, as multiple current record-related libraries demonstrate. I'm not too happy when some special syntax of haskell is tied to a particular implementation. Another example where this badness bites that comes to mind are lists - in most applications ppl could be better off using the element-strict adaptive lists dons made (https://donsbot.wordpress.com/2009/10/11/self-optimizing-data-structures-using-types-to-make-lists-faster/), yet one can't use the familiar list syntax with these. While this is a different, and lesser, issue, I'd rather not introduce new examples of it. And the issue of proper records is particularly unsuited for a final decision locked into the compiler given various suggestions, libraries and approaches already in the wild. Whatever is adopted as default, I'd wish it were possible to use XRebindableSyntax and some library, to make record syntax mean something different. I gather the old GHC trac page on records suggested proceeding in this manner: http://hackage.haskell.org/trac/ghc/wiki/ExtensibleRecords &gt;There are a lot of design decisions listed on this page, some of which inspire strongly-help opinions. It is clear that discussion will not resolve these, and we don't seem to have a lot of examples to help clarify matters. The deadlock is unchanged: we still have too many good ideas. &gt;I believe the way forward is to implement several of the possible systems, and release them for feedback. ... Libraries could then implement mkUnderlyingRecord, underlyingEmptyRecord, MkUnderlyingRecord, UnderlyingEmptyRecord, viewUnderlyingRecord and viewUnderlyingEmptyRecord in whatever way is best. What do you think? but this suggestion too has just rotted in the trac wiki for the last 2 years or more.
Definitely agree on the latter. I really hate having to screw around with clumsy build system nonsense just to write some code and run it, especially when the build system requires me to explicitly repeat things that are already explicit in the code. Using Cabal (or any build system, in my experience) creates more problems than it solves up until the point where you're dealing with a very large project or one that needs to build automatically in lots of places. A lot of code never needs that.
Regarding global namespace pollution: can you please *show me* how pattern matching helps? If I have a record with a named field in it, the namespace is already polluted because I wrote a record constructor.
Record syntax does have factual advantages over positional constructor arguments. First, it makes data definitions more easily extensible. As long as you only access the values of a record using record syntax, you can easily add more fields to it without modifying current code. You can also modify or remove current fields and only need to adapt the code that actually accesses them. This makes records very useful for things like application-wide configuration data. Then there's the usual argument that giving names to fields helps you remember what is what. It's true that records aren't used that much in Haskell, but there are still cases where you want to gather a lot of data in a single value, and you only care about bits of it at a time. This is what they're for.
&gt; Having to transform every piece of data you access to bring it into conformity with your language's quirks is ridiculous. Nobody but a language fanatic would put up with it. Which is why nobody anywhere uses ORMs, right? Phrasing things in terms of "language fanatics" isn't really helping your case here, by the way. I don't think I'm a fanatic for wanting data to be structured appropriately for the way it's actually used. Record syntax and namespacing is one of the worst aspects of Haskell as it stands, certainly. But it's also just *not that important* overall. I'm interested in things that will help me use the language to write working programs to solve actual problems. Adding stuff that encourages people to write bad, clumsy code instead of learning Haskell idioms for better ways to do it is not helpful or useful in any way. If that keeps people who are more interested in what's familiar than what's useful from learning Haskell, eh, not my problem. My goal is practicality, not evangelism. &gt; Given that you're going to do that, you still want good syntax at the transformation stage. Yes, and I doubt records are ever appropriate for doing so.
C3 with syntactic sugar (and better yet, eventually a *kind* for field names), seems like an excellent idea.
Right, see, that's a good description of some of the places where it is *is* useful. On the other hand... &gt; As long as you only access the values of a record using record syntax, you can easily add more fields to it without modifying current code. ...but you can't add new constructors without creating a whole bunch of inexhaustive patterns. Record syntax is basically only useful for types that look like big tuples, and lets you attach names to each position in the tuple. Also, keep in mind that "modifying" large records has a nontrivial cost in rebuilding the new version, so it's also only useful for types that you construct rarely and access often.
I'm not sure a new kind is necessary. Some desugaring, plus the new `Constraint` kind, should get you most or all of what's needed, I think.
By... not giving things named fields in the first place, unless they're actually useful? A lot of types don't benefit from named fields, and in many cases (e.g., sum types) they're actively harmful.
This starts to remind me of the Python solution to this problem. "Namespaces are one honking great idea -- let's do more of those!"
The modules version is simply unsuitable unless GHC allows more than 1 level of recursive modules. I wrote a library that mimics the messaging in the BeAPI using the pseudo-OO pattern of putting each object into its own module. Unfortunately, the many cross references required at least 2 levels of recursion when processing modules, which GHC did not support at the time (and I believe does not at this time). The recommended solution was to pile everything into a single module. Any solution that requires splitting types into different modules increases the likelihood of running into the recursion problem, which will probably be solved by renaming the functions. So, at best, it will be a partial solution whose limits will become most clear well into a project when it is more expensive to rename everything.
&gt; iText ... So I have to pipe to a Java program and write the code in Java. This is very interesting to me. I use iText too. And i'd love to do it from haskell. Could you please share some details on how exactly do you "pipe to a Java program" ? Do you mean unix pipe and just simply running a command line java program ? or communicating via tcp sockets/http ? 
&gt; I want Haskell to be the norm, but it's going to remain a toy language as long as an experienced programmer coming from another language hits a barrier like this the first time they try to implement a real project. I have trained myself to the point where Haskell is the default and every other imperative language simply looks alien to me. It's a very refreshing perspective, and I could easily go on rants about the many barriers that other languages have ("they don't have first class functions, no pattern matching, stupid side effects, no laziness, etc. etc.") compared to my default.
I ran [hlint](http://community.haskell.org/~ndm/hlint/) on your code, and it came up with a few suggestions. For example, instead of (toUpper $ head input) == 'Y' || (toUpper $ head input) == 'N' you can use (toUpper $ head input) `elem` ['Y', 'N'] You should run it yourself; it's very helpful when learning Haskell.
Use hlint to get automatic suggestions. Type "cabal install hlint" and then "hlint filename". hlint is also integrated with hpaste.org, so you can paste the code there. Here are some other tips: * `length input &gt; 0` is better written as `input /= []` or `not (null input)`. * getInteger and getYesNo are very similar. You can factor the similarities into a higher-order function: getValue :: String -&gt; (String -&gt; Maybe a) -&gt; IO a and then use it like: getInteger = getValue "Please enter a valid integer" $ \input -&gt; if input /= [] &amp;&amp; all isDigit input then Just (read input :: Integer) else Nothing * This isOrdered' relation (x:y:ys) | y `relation` x = isOrdered' relation (y:ys) | otherwise = False is better written as "y \`relation\` x &amp;&amp; isOrdered' relation (y:ys)". Or even: isOrdered' f [] = True isOrdered' f x = and $ zipWith f (tail x) x This version also aborts early if a disorder is found. * You can implement printTree as: printTree = mapM_ print . listOfTree A more verbose way is: printTree x = sequence_ (map print (listOfTree x)) This means that you map the "print" function on listOfTree, and then execute the prints in sequence. This does the same as your printTree function but with less repetition. The general abstraction is called "Foldable". 
Are there any obvious problems with just removing automagic creation of accessor functions (apart from backwards compatibility)? Isn't pattern matching good enough as the default case?
There's no reason Haskell should try to be unfamiliar. But from the start, it has never aimed at being 'the norm', but at doing things right (the motto is 'avoid success at all costs', after all - since doing things right is a research problem, and compatibility concerns would impair change). Given how a lot of it is quite alien, whether a particular surface syntax that's decided on is familiar or not should be a minor point anyway, and the tiniest among numerous barriers a new programmer would encounter when tying it.. Now that said, I'm not sure that there is anything wrong with how records look like in other languages (and judging by TDNR proposal, neither does SPJ). Except that the damned dot is so (over)used in haskell for other things, that it may be a bad idea to use it here too. 
Well, it would break the extra-automagic creation of lenses with TH that `fclabels` does, which is pretty much superior to the built-in record syntax in almost every way. :]
What really gets me is the clumsiness of working with imperative actions as first class entities, if that's even possible at all. Really, how are you supposed to get basic structured imperative programming done in any of those languages? :]
I was assuming that record access wouldn't even contribute to type inference--they certainly don't in C, after all. :] The types would have to be inferred from the rest of the context, then checked against record names afterwards. Either a mismatch or an ambiguity would then produce a compile error.
Does this (admittedly strange) behavior really surprise people? I thought it was a reasonably well-known stumbling block with fundeps. Also, anyone able to shed light on sclv's results?
Awesome, thanks! I had no idea what hlint was until today. Also apparently there's a dedicated pasting site for Haskell (hpaste.org); TIL many things.
Really helpful, thanks! I'll be sure to get hlint... in fact, I had no idea what cabal was until today.
Programs are written mainly for people to ~~read~~ rewrite, and only incidentally for computers to run. The problem isn't that Haskell doesn't run on the jvm, the problem is that none of my coworkers can read Haskell/Frege.
I am afraid: as difficult as any other compiler. But, as I expressed in my blog post, I am quite decided to remove the more superfluous (i.e. lexical) differences. Yet, there remain difficult cases. For example, take field labels. In Haskell they just map to global selector functions. In Frege, they are scoped in the namespace associated with the type, so you can't just write: myfield x but instead: x.myfield or even: T.myfield x -- where T is the type constructor of x's type 
&gt; That said, I like sfvisser's proposal of multiple modules per file. Namespacing sounds like the natural way to disambiguate names, yes. But, even then, wouldn't it still be syntactically quite heavy to explicitly create modules for each record? And wouldn't it be sensible that all records are so namespaced by default? If so, and records were taken to imply separate modules imported unqualified if possible, wouldn't one get in effect the first option, "Optionally use the type name. "? Perhaps this special case can be implemented quicker than allowing multiple modules per file generally, yet be more practical to use. (though I guess it still would be nice if it were compatible w XDisambiguateRecordFileds, for something TDNR-like)
ah, nice. I didn't know about zipWith
Reminds me of this issue... http://hackage.haskell.org/trac/ghc/wiki/Records
Well, if they can read Java, there's an easy remedy: make them read **and understand** the java code Frege generates. I promise you, they'll read Haskellish programs within a few week as if they never read anything else. :)
I think I get what you're saying. So for instance, one could have two classes with the same method names, but they would have to be used qualified if there was ambiguity, like 'Class1.method someClass1'? When you mentioned GADTs it occurred to me that maybe the syntax there could support polymorphic records data Term a where Lit { val :: Int } :: (Has_val t)=&gt; t ... class Has_Val t where val :: t -&gt; Int erm... Not sure if there's anything there or not.
That's not quite how it works unfortunately. If people have trouble understanding the code I've written they'll demand that I rewrite it in a way that's understandable, using known idioms. It's very hard to teach people new things if they don't want to learn. Also, it would only work for new code, or you'd have to make the compiler work both ways. It's been 6 months since I worked on new code. Don't get me wrong, Frege looks like an awesome language, fills in the lazy functional lanauge on the jvm niche nicely and I'm certainly going to be paying attention to it, but if the original source language wasn't an issue I would've switched to Ruby, Clojure, Scala or one of the myrriad of other JVM languages that aren't Java years ago.
&gt; I think I get what you're saying. So for instance, one could have two classes with the same method names, but they would have to be used qualified if there was ambiguity, like 'Class1.method someClass1'? I was suggesting that if `Class1` and `Class2` both have a function `foo`, then when both classes are in scope directly you'd need to refer to `Class1.foo` or `Class2.foo`, in exactly the same way you currently need to qualify conflicting imports from other modules. Conflicting field names for data types would likewise be handled that way. Basically, this is just treating type classes as records of their operations (i.e., the implicit dictionary), and modules as a degenerate case of both (i.e., a class/data type with no constructors, only a namespace). An obvious further generalization would be to give data types a full inner scope that allows arbitrary definitions with named fields being implicitly placed there, meaning that this: data Pair a b where Pair { fst :: a, snd :: b } :: Pair a b ...would simply desugar into this: data Pair a b where Pair :: a -&gt; b -&gt; Pair a b fst (Pair x _) = x snd (Pair _ y) = y So if you wanted to add something else to the inner scope, you could: data Pair a b where ... swap (Pair x y) = Pair y x And so on.
Sorry, didn't want to annoy you. My comment was, of course, entirely sarcastic (and self ironic in regard of the "beauty" of the generated java code).
The reason for a new kind is, basically, so that you don't have `Bool` and `Int` as potential instances in the `Field` class. You don't *have* to enforce that at all, but it would be nice... One could just cheat and have another class, purely for constraint purposes, such as `IsFieldLabel` with an instance for labels and nothing else. Still circumventable, but at least a bit more enforcement...
+1 sounds like a great combination for this problem. (and roughly equivalent to having records imply a module for themselves, plus XDisambiguateRecordFileds). 
I like circumventable, actually. Just make sure the stuff needed to do so is only exported by some sort of `GHC.InternalStuff.FullOf.MagicHashes` module and leave it at that.
I'm fairly new to haskell and have a genetics background, but I'm trying to write a bioinformatics module. I just want to check that I understand the problem, because I think it's something I've run into in my module. I've got a few data structures like this: data SNPSite = SNPSite { ss_consensus_position :: BioSeq.Offset , ss_five_prime_seq :: BioSeq.SeqData , ss_allele_1 :: String , ss_allele_2 :: String , ss_three_prime_seq :: BioSeq.SeqData , ss_quality :: BioSeq.Qual } deriving (Show, Eq) -- | GENOTYPE and COLUMNGENOTYPE blocks data SNPGenotype = SNPGenotype { sg_consensus_position :: BioSeq.Offset , sg_read_position :: BioSeq.Offset , sg_read_name :: FilePath , sg_allele_1 :: String , sg_allele_2 :: String , sg_score :: BioSeq.Qual , sg_direction :: Maybe String -- optional , sg_peak_coordinate :: Maybe Int -- optional } deriving (Show, Eq) There are a lot of data types that have common records like "consensus_position" and "allele_1." I thought it was a bit of a hack-ish solution to just add a prefix to all the records, but is this the sort of problem that others are having as well? Am I following anything close to "best practices?"
The syntax for Open Quark is a lot less clean than Frege or Haskell, which seems to be the main issue that keeps it from gaining popularity.
I would hope that Frege creators learn from unfortunate mistakes of previous attempts. Deviating too much from haskell syntax will most likely doom project to fail. 
Slides for this paper are available [here](http://parametricity.net/dropbox/yield-slides.subc.pdf) Here's the abstract for a forthcoming talk by the same authors: &gt; **Yield, the control operator: applications and a conjecture** &gt; &gt; *Roshan P. JAMES, Amr SABRY* &gt; &gt; In previous work, âYield: Mainstream delimited continuationsâ (TPDC 2011), we presented a generalized version of the yield control operator that was distilled from studying yield operators of various programming languages. In this brief abstract, we extend that presentation to establish the connection of yield with dynamic binding, dynamic scope and generalized stack inspection in the spirit of Kiselyov et al (SIGPLAN Not. 2006), we outline a lightweight workflow infrastructure in the spirit of Lu and Gannon (eScience 2008) and we provide a yield monad transformer that allows yield to be composed with other effects. &gt; &gt; Finally, we pose a question of considerable theoretical interest: do delimited continuations expressed using yield in combination with session types shed light on answer-type polymorphism? &gt; &gt; To be presented at Continuation Workshop 2011, colocated w/ ICFP 2011 (Saturday, Sep 24, 2011)
Ouch! No, you're doing fine. That's a nice example of the kind of scenario where this problem hurts the most. That said, there *are* other (possibly tidier) approaches to working around it that may or may not apply in your case, but this is definitely something that should be easier than it is.
pattern matching means you have to declare your record fields at a different spot than where you actually want to use it. Even with record puns and any other (imaginable) GHC extension that is still second-rate to what I can do in any OO language.
A frege-to-haskell preprocessor could probably handle that case rather easily. haskell-to-frege may be more difficult, requiring you to inspect any possible invocations of field label functions. This is, interestingly enough, related to that other discussion about Haskell namespacing.
&gt; Click here to see a sample of the CAL language *click* &gt; 404 Not Found à² _à²  btw "click here" is an abomination, never ever use it unless your target audience is the elderly.
Comparing to the empty list requires an Eq instance on the list item type, which "null" does not.
It is very un-Haskelly to make a type like SBTree be monomorphic to Rational (i.e: hard-coded type). It is nicer to generalize it to: data SBTree a = Empty | Node a (SBTree a) (...) That way you also know that the SBTree doesn't really care what's inside it, which reduces some of the mental burden when reading. You should use record syntax for the node constructor. Whenever you find yourself writing a comment, e.g: -- SBTree Node has 4 parameters: node value (Rational), left child, right child, and parent (in that order) that you can express in code, instead, better express it in code. The record field names are exactly the content of your comment, so you can happily delete it. `nearestAncestor` should of course be polymorphic to `a`, rather than hard-coded to rationals. That's all for this round :-)
There's the [generator](http://hackage.haskell.org/packages/archive/generator/0.5.4/doc/html/Control-Monad-Generator.html) pages that implements yield as a means to generate a `ListT`.
In all seriousness, this is how I feel about most aspects of the language. For most things you encounter in a traditional academic CS environment, especially math and algorithms that don't require any I/O or structured data, Haskell is a revelation. Sadly, the cumbersome record syntax is a nuisance and a barrier to entry, and the awkwardness around I/O is fatal to use of Haskell in my field. 
A huge amount of data is out there with named fields, whether you like it or not. If Haskell can't handle it, it is Haskell, not the data, that gets dropped on the floor. Examples of record data with named fields: - everything in any SQL database anywhere - every filled-in form everywhere, on the web or off it - every packet on the internet - most binary wire formats for data - most of what XML is used for Are you going to "fix" all of that? Is it all just going to go away? 
&gt;Something like this could be encoded already with minor difficulty, a lot of boilerplate, and a royal flush of type-hackery-enabling GHC extensions--but would still likely fall short of the ideal. creating has-predicate classes involves a lot of boilerplate and falls short of the ideal? I thought all more ambitious record systems did class predicates. I really do hope the compiler needn't learn a whole knew kind of constraints just for records, but that these would be a matter a library could do w/o much problems (but yes, w mptc+tyfuns).
so basically this is the has-predicate, with record types. Sounds similar to the system SPJ suggested some 8Â½ years ago, https://research.microsoft.com/en-us/um/people/simonpj/Haskell/records.html Imho fairly heavy-handed an approach; special syntax, special predicates, record types etc, for not that great a gain vs simpler solutions. In particular, sounds like something that would have little chance of becoming part of the standard itself (like hugs's TRex) any time soon, plus its tantalizing that what's hard-coded here (predicates) might be implemented by a library when the language supports more type hackery (namely, http://hackage.haskell.org/trac/haskell-prime/wiki/MultiParamTypeClassesDilemma) So I'd rather have a simpler option as a default, so it doesn't go far from the standard and is easier for other compilers, but that the syntax can be hijacked by a more ambitious system that supports things like that. I think its perfectly clear to everyone that the current system is not good - even when implemented, it was just intended as a patch until something better is agreed upon. But it so happens that there are many ways to do records, and none are obviously the right way, even after all these years, so nothing gets done.
As far as I understand type directed name resolution would make this possible as well (no need to prefix the Data.Map.map function, and it's in perfect harmony with Prelude.map). If so, I think it has a couple of benefits. import Data.Map mapList = map (*2) [1..10] mapList = map (*2) $ fromList [('a',1), ('b', 2), ('c', 3)] 
What makes you think that record types with named fields are the best way to represent all, or even most, of those, in code working with them? In many cases I wouldn't want to use records for those even *with* nicer syntax, because it would be a bad design. And no, I still don't care about evangelism. Enough people use Haskell that it's not going away anytime soon, and if other people don't want to use it because it won't let them hammer screws in the way they want to, I really, honestly don't care. Their loss.
But why not make a class and use that? class Id a where getId :: a -&gt; Int data Person = Person { personId :: Int, personName :: String } instance Id Person where getId = personId data Car = Car { carId :: Int, carName :: String } instance Id Car where getId = carId getId (Person 5 "Slim Shady") getId (Car 101 "The mystery van")
Of course, you can already do that with `fmap`. :]
\[\](/rage)
I recently had this problem when parsing some json data of some match results. I found that my data source had a lot of duplicate field names in their objects, likely because it was generated from an OO language. For example, a Match has a name, a Match has a Location which has a name, and a Match has players which each have names. There were a few other problem fields, and I ended up just using some quick regular expressions to add prefixes to fields based on data type. It does feel like a hack, and I'm looking forward to a better solution.
Doing it today would require boilerplate, possibly generated using Template Haskell. Making it part of the language would turn the boilerplate into another desugaring step, which is fine. The falling short I expect is mostly in terms of stuff not being first-class entities. That's tolerable, but still irritating.
Depending on what you're doing with the data, in a case like that you're often better off leaving it in a more ad-hoc format, at least initially. Make a simple type with cases for each possible type of value, then shove everything into something like a `Map String JsonValue`. That saves you from also dealing with the type errors and missing fields that are likely to crop up in that kind of data, without having to muck about with `Maybe` everywhere and error handling and whatever other nonsense. If you have a limited set of field names and don't mind throwing out any unexpected fields that appear, you can use an enumerated type as the `Map` key instead of arbitrary `String`s, as well.
&gt; ..Yet unknown and forgotten It was submitted here [in march](http://www.reddit.com/r/haskell/comments/g7vcj/open_quark_on_github_haskell_like_lang_on_the_jvm/)
1. IDs can be of any type, so a -&gt; Int won't work. 2. Because, if you do that to accommodate every field, that's an insane amount of boilerplate. I used "id" as an example of a field that is guaranteed to create lots collisions in the global namespace, but any human-meaningful field name is going to create collisions. Are you really going to make a class for each of them? I could imagine that this would be how the compiler implemented fields---just autocreate a class every time a field is used. That's halfway to duck-typing, though ;)
I think not as the proposal stands: &gt;TDNR is driven by a new syntactic form "a.f" which, for lack of a better term, we call a "TDNR invocation". That is what makes the type-directed name resolution spring into action. Otherwise it is inactive. However I'm not too happy w having a magic dot, firstly because its yet another kind of dot, secondly because its magic (that only works as reverse application) - yet do like having some kind of tipe-directed disambiguation, as convenient shorthand, in principle. (well, and it should be compatible w lenses/labels, not just for getter functions, which its not if it must be reverse application.)
The obvious way to work with a data structure is the right way, unless there's a strong reason to use a non-obvious way. Other people are going to be reading your code. Let's iterate through the use cases here. What would you use to replace records for (off the top of my head): - genome sequence data - financial transactions - web server logs - internet packets - file status information The reason Haskell needs to interact with records is not "in order to be popular, Haskell has to interact with data formats created by idiots"; it's "records are a natural way to store data of common types that human beings need, and Haskell would be much more useful if it could deal sensibly with these types". 
hm, but a syntactic sugar that desugars into mptc + tyfuns by default? Wouldn't it be better to say just make the labels built-in, as per http://hackage.haskell.org/trac/haskell-prime/wiki/FirstClassLabels , to support this and other record system there mentioned, and something more modest as default desugaring of any special syntax?
I'm certainly not qualified to comment on what the best way to implement records is---I'm a user of languages, not a designer. But I seem to have found myself having to argue with camccann that having usable records is both possible and valuable, and that's the first proof-of-concept that came to mind. 
Similarly: {-# LANGUAGE FlexibleContexts #-} module Main where f :: Num String =&gt; String f = "Hello" where x = 1+"hi" main = print "Hi!" Compiles into an executable successfully, despite `f` requiring the non-sensical Num String instance.
Sigh. Using named fields isn't obvious at all for a lot of that stuff and I have no idea why you think it is. External data formats still don't necessarily have anything to do with internal representation and I don't see why they would. Those examples are all extremely vague, too much so to suggest a representation off the top of my head, and to be quite honest going through and working out a bunch of concrete examples just to prove a point *vastly* exceeds the amount of effort I'm interested in spending here. Frankly I doubt I'd ever be able to convince you anyway. You already have your preferred one-size-fits-all approach (which you've never actually justified as being a good idea, just asserted to be the natural default), so I'll leave you to it.
 import module Bar where data Baz = Quux
Of course having a record system that was less terrible would be both possible and valuable. It's one of the weakest parts of Haskell's design, certainly, and I'd love to see it fixed. The only point of disagreement is how frequently useful records are for general day-to-day use, and my contention that bolting on an only slightly-less-bad record system (e.g., what many mainstream languages have) would serve mostly to encourage people new to Haskell to use bad designs by misapplying idioms from other languages while not actually helping me get anything useful done. Doesn't change that I'd still love to see a good solution designed and implemented.
yeah, it should be like C2 or C3 proposals in a previous comment class Id a | r -&gt; a where id :: r -&gt; a ... instance Id Person Int ... instance Id Car Int ... or data Id=Id instance Field Id Person Int ... instance Field Id Car Int ... presuming a library defined class Field: class Field f r a | f r -&gt; a where get :: f -&gt; r -&gt; a .. and perhaps something like r .# f = get f r Presuming there were some namespacing to allow equally named fields in the same module (or a fixed prefixing scheme a programmer must follow), a library could do all this the way fclabels do it; a single template haskell invocation to derive (generate) these definitions from a record. so you can today make a smallish library that would allow you to, for eg write : data Person = Person { person_id :: Int, person_name :: String } data Car = Car { car_id :: Int, car_name :: String } mkLabels [''Person, ''Car] john = Person 5 "John" car = Car 10 "whatever" id john id car or, presuming r # f = f r john # id car # id or presuming the C3 proposal, say john .# id car .# id etc. such a library was at least suggested not too long ago - not sure if there's something on hackage or not. https://intoverflow.wordpress.com/2010/05/09/polymorphic-first-class-labels/ requires multiparameter type classes + functional dependencies or type families though. In any case, I'd consider this to mean that nothing more than namespacing should be implemented by the compiler for this kind of thing. Well, and maybe, http://hackage.haskell.org/trac/haskell-prime/wiki/FirstClassLabels
Also, it looked relatively orphaned/unsupported from the start, somewhat quirky, and with some semantic choices that pushed it fairly far afield from Haskell in terms of behavior.
I'm not saying "records are the Right Way", I'm saying records are how all of those things are currently handled when they're handled at scale, and they're the obvious and familiar way to handle those things, so if we're not going to support them in a way similar to how other languages support them, an alternative has to (1) actually exist and (2) be a clear improvement. 
I find it interesting and amusing that the authors say at the beginning that `yield` is probably popular due to its inclusion in Ruby. Of all the languages that have had such an idea, they pick one that grabbed it very late in its development cycle and mainly just because the idea had done so well in Python.
Hehe, very true. That's one reason why the only imperative language that I touch with a ten-foot pole is CoffeeScript, and only because Haskell can't be used in the browser yet.
&gt; Sadly, the cumbersome record syntax is a nuisance and a barrier to entry, and the awkwardness around I/O is fatal to use of Haskell in my field. Works for me. What's your field? 
I figured since the input was highly structured (I believe it's a dump of C or Java data structures) so I wanted to have the relations clearly defined in the parsing. However, it is a good point that I am probably over-specifying it. I do sort of wish I could specify a chain of indexes on the JsonValues and get back a Maybe T. I wonder if that's possible. I'll have to read the documentation. It would make it easier to pull out just the values I really want to use.
On reflection, I think you're right: if -XRebindableSyntax allowed programmers to have lenses for their records without explicitly calling a template haskell function, that would be good enough, *and* it would avoid the delay and aggro of officially sanctioning a particular lens implementation. 
Apart from all lexical and syntactical matters: there remains the fact that only those modules could be converted that do not use the foreign function interface directly or indirectly. For, one would have to port the foreign C interface to a java native interface in the hs2fr case, and it is doubtful if that will work out to be 100% equivalent. In the fr2hs case, matters are even worse, I have no idea if it is even possible at all to call java code from C code. To illustrate my point, have look at the [Swing example](http://code.google.com/p/frege/source/browse/examples/SwingExamples.fr) which implements 2 examples from the Java Swing Tutorial. It would be a matter of a few minutes to make this a *syntactically valid* Haskell, and yet it would not compile. 
All good points, but to nitpick... &gt; isOrdered' f [] = True &gt; isOrdered' f x = and $ zipWith f (tail x) x The first line is redundant, as `tail [] = []` and `and [] = True`.
Wow, this works really well! Why haven't I heard of this trick before! I've added :set -XImplicitParams to .ghci now so I can always use it when needed. (I hope that this doesn't have any adverse side effect.)
And then comes Data.Set...
We will probably want to fix the module system anyway, at some point. If we lean on it more, that will just be more incentive to fix it. Alternatively, we could include fixes for the module system in with this proposal.
To complete the story: i :: a -&gt; a i = w k -- ask withReader :: (a -&gt; b) -&gt; (b -&gt; c) -&gt; a -&gt; c withReader = c b s :: (a -&gt; b -&gt; c) -&gt; (a -&gt; b) -&gt; (a -&gt; c) s = b w . b . c -- &lt;*&gt; on the ((-&gt;) e) applicative I looked for a more intuitive implementation of S in terms of BCKW, but couldn't find one.
No, tail of the empty list is an error. But drop 1 [] == [].
Oh good grief, yes. *\*blushes\**
You might find https://github.com/MedeaMelana/JsonGrammar useful, it allows you to use your own names for object properties with a nice syntax.
But it is old. http://quarkframework.blogspot.com/
Note that the paper has a number of small errors in the code. I'm translating from Haskell to OCaml (it really is a 1-1 job so far), that's why I'm detecting them. For instance, the first `return` definition is incorrect, as is the definition for `same`.
I've occasionally been tempted to take a bunch of what I've learned from Haskell and build a straight-up imperative language that has most of the same benefits, plus extra stuff that would be useful in that context. But then I realize that nobody would want to use the resulting language and forget about the whole thing.
I can deal with the pain of transition to something better but I'm not fond of causing pain in the hopes that there will be a transition in the future. I see the module version as a trap for the unaware and something that the knowledgeable will simply avoid. This would lead to a situation where the ones most able to fix the issue will feel the least pressure to fix it. Those who are caught by it might spend many hours restructuring their programs, as I did, simply because it's the fastest and most practical solution. The extra pain, rather than sending us to the doctor, teaches us to walk with a limp.
I agree that record syntax is basically for labeled tuples. However, I think you underestimate how drastically and near-inescapably important labeled tuples are to vast quantities of real world code, and not just CRUD code, but across the board. Let's not forget, there are only two hard problems -- caches, and naming things!
A more interesting type from a CS standpoint doesn't need them. A "more interesting type" from a domain specific standpoint could be that you've extended the fields in a record with four more pieces of information, all represented as doubles.
I generally agree with you, but think you're really a space cadet on this discussion. One reason external data formats tie to internal representation is that we need to *read* those formats before we can do anything else to them. And to the extent that we read those formats, we need the easiest-possible-to-use internal representation of those formats. And in many cases, a labeled tuple type, if done right, is powerful enough that we *don't* need to do anything beyond that. When you're programming math, physics, numerics, various other pure code or models or simulations, then these issues seem pretty small. When you're interacting with large amounts of real-world-data *already in* a labeled tuple format, or data which *naturally* fits in that format, then the story is very different. Maybe you could step back a minute and ask "why are relational databases so popular? Is it related to the fact that their labeled tuple format is a decent, flexible, fit for large amounts of real-world data?"
By chain of indexes, I assume you mean looking up a property you expect to be another map, then looking something up in that, &amp;c., out to some final value, and getting `Nothing` if at any point the property you ask for is missing or not the type you expected. Would something [like this](http://hpaste.org/51420) help? That's the sort of thing I'd use for semi-structured data, or hierarchical data from an outside source that I wasn't using enough to bother parsing fully.
Yes, I realize it's common. Many bad ideas are. Based on *my own experience working on real world code*, things that actually should be labelled tuples are pretty rare compared to things that are forced into that model because people don't know of, or don't have access to, anything better.
Here's a link to the DSS page. https://science.nrao.edu/facilities/gbt/schedules/dynamic Thanks for sharing, I'm one of the developers on this project. Actually, the DSS is not entirely written in Haskell. Our core scheduling algorithms are written in Haskell (over 10,000 lines), the remaining interfaces and web services are written in Python (Django) and GWT.
In my experience, most domain specific types are either ad-hoc structures better treated as dictionaries, hierarchical structures better handled with simple pattern matching, or complex structures containing multiple sum types that embed domain knowledge as possible configurations. All of those are exactly the sort of thing you don't want to use plain records for. Extending a record with four doubles sounds more like a *lack* of domain-specific types than anything else. What do doubles even mean? Unless you're designing a floating point processor, probably nothing.
I'm wondering how much pain it would cause to scrap the syntactic "setter" functionality of records, so that they would be plain functions. I know I don't ever use the setting functionality, but I don't know how useful that is to others.
What hath God wrought?
I see an interesting update on the wiki page - basically a somewhat fleshed out version of proposal C3 someone enumerated in this discussion. I'm wondering though, about possible cons of implementing polymorphic selection &amp; update in the manner there described - by supporting a built-in class Label, and labels promoted on the type level as its members with global scope. Looking at the http://hackage.haskell.org/trac/haskell-prime/wiki/FirstClassLabels page, I see there are other approaches. For eg,, beefing up the module system to support sharing constraints (though I hope some syntax not requiring a new keyword would be possible, unlike that proposal, but whatever). If the only reason any of that proposal is outside userland (apart from desugaring, which could largely be done by TH anyway), is because of problems when importing these into other modules, perhaps we shouldn't work around this by some built-in system specifically for labels, but fix this aspect of modules. However I have no idea how heavy-weight such a feature would be, or how frequently useful it is apart from this specific usecase. So, how do the two compare? EDIT: also wasn't there a way of emulating the features of ML, specifically mentioning sharing constraints? I can't really understand all of it, but: http://okmij.org/ftp/Haskell/TranslucentAppFunctor.lhs . can this inspire some type hackery that avoids the need for any language extension for these?
&gt; I generally agree with you, but think you're really a space cadet on this discussion. One reason external data formats tie to internal representation is that we need to read those formats before we can do anything else to them. And to the extent that we read those formats, we need the easiest-possible-to-use internal representation of those formats. External formats exist to describe data, internal representations exist for using data. It's nice if these overlap, but to the extent that they don't that's what parsers and such exist for. &gt; When you're interacting with large amounts of real-world-data already in a labeled tuple format, or data which naturally fits in that format, then the story is very different. Which is of course why you never find real-world data in formats that don't match labeled tuples, such as XML. Honestly, I think some of the "non-real-world" examples you gave are probably *more* likely to be suited to a labeled tuple approach than most "real-world" problems are. &gt; Maybe you could step back a minute and ask "why are relational databases so popular? Is it related to the fact that their labeled tuple format is a decent, flexible, fit for large amounts of real-world data?" Oh, come on. Have you ever actually *used* relational databases in the real world, with real world mainstream languages? Have you seen what ORMs are used for? The whole thing is a gigantic train wreck caused *exactly* by people trying to cram one kind of data (i.e., objects) into another structure that it doesn't fit. Why do you think the whole NoSQL thing became popular? The impedance mismatch between OOP and labelled tuples (i.e., SQL) is *an order of magnitude worse* than dealing with Haskell's records! Maybe you could step back a minute and ask "what's the best way to *actually* solve real-world problems, rather than just imitating the same mistakes everyone else makes?" If wanting *practical solutions to actual problems* makes me a space cadet then okay, I guess I'm fine with that.
I work in large-scale IT. A common example of something I'd like to do is a parallel fold over a large distributed filesystem in which individual FS operations are slow but you can do thousands of them in parallel. My colleagues and I did this with shell pipelines fed to Perl, or Ruby and Python if we were feeling exotic. I set aside the parallelism to create a toy problem to get started with: just a working, if slow, fold over a filesystem that is general enough to be used for the "du" command or similar statistics. I spent a lot of time trying to get it to work on my own as an exercise to learn Haskell I/O, and then asked for help here and elsewhere when what I wrote sucked; I got a lot of bad answers, suggesting that most folks in the Haskell community just don't touch this kind of problem, and a couple of good and thoughtful answers that resulted in a much more cumbersome, verbose, boilerplate-y program than I was able to write in a few minutes in Ruby, and which still had an awful memory profile. (The lazy IO version was also not great with memory, but it had the advantage of being simple and readable, as long as I started by predefining short aliases for all the (unsafePerformIO . functionIWantTheResultsFrom pairs). But if you tell a Haskeller you're using lazy IO, 50% look at you like you're dirty, 30% look at you with pity, and 20% say "yeah, you kind of have to if you're doing anything real".) Fundamentally this is just a graph-traversal problem where the graph isn't a pregenerated object in RAM, so the conclusion I took away from this is that Haskell makes solving typical computer-sciencey problems *at scale* harder rather than easier. What's sad is that the scale in question wasn't even the scale of the organization (1000 machines, 10 PB, 1 billion files) but just a test implementation that would fit on a single machine; and meanwhile I've left that job and I'm working on systems a few orders of magnitude bigger, swimming in a sea of C++. 
It's incredibly useful if you're masochistic enough to actually use record syntax directly. Otherwise, it's just a poor imitation of what you get from using, e.g., `fclabels`.
...wow. And I thought my QQ'er to use Agda from Template Haskell was terrifying.
Is there a reason that [using JNI](http://en.wikipedia.org/wiki/Java_Native_Interface) wouldn't work? Would be difficult and painful to do automatically, I expect, but seems like it should be possible.
I wonder how Frege's Java output compares to what GHC produces when compiling via C. :]
I don't say it's impossible, but I guess that it will be hard to get it 100% equivalent. AFAIK one must obey one thing or another when JNI-ing (but, honestly, I never did it myself until now). The whole memory management thing will most likely be a nightmare. Can you name a popular (or just mildly complex) C-Library that was not initially intended to be used from Java, that is used via JNI? 
Well, at least in Java we don't have preprocessor macros ..... 'nuff said!
If f's arguments are swapped, then indeed it can be written in one line: isOrdered' f x = and $ zipWith f x (tail x)
XML does actually tend to match nested, labeled, tuples. As does JSON. But more to the point, yes, ORMs are horrible very often, especially when used for *objects*. But when you use databases for sane things (and there are many, many, sane things) then labeled tuples are a natural fit. I work with practical haskell code for real world problems every day. I work with large databases every day, where the data *makes sense* to be in a database, and where a view of the data as a bag of tuples makes absolute sense. For my purposes, which I would imagine are not uncommon, then better records seem eminently sane. One should also take note that positional matching is fine for up to maybe 4 or 5 things -- after that it becomes increasingly difficult to deal with. When we're passing around rows with 20+ pieces of data, matching that information out by position (especially when we only need part of it) can be noisy, ugly, and painful. I'm not arguing for an ORM. I'm arguing that labeling makes tuples more powerful, especially large tuples, and I'm arguing that tuples, which is just to say product types, are a pretty fundamental thing across a range of domains.
I think the problem here is that you're generalizing your experience too broadly -- in the code you've work with, in your domain, you haven't found much need for labelled tuples, and you've found abuse of existing record/db systems fine. That doesn't mean that other people, on reddit and elsewhere, working on different codebases in different domains may not have radically different needs, different conclusions drawn from different experiences, etc. It's fine for you to say that you don't find this feature important, and that you've seen it misused. But to then tell all sorts of other developers, at least *some* of whom must have *some* idea what they're talking about, what *they* need... It's not a good look.
At the moment, acid-state does not work on Windows. This is because the code to safely log transactions to disk has to work at a fairly low-level in order to ensure everything is flushed properly, etc. Adding Windows support is a TODO item, but the first step is finding someone motivated enough to do it (aka, someone who uses Windows). There are only a handful of functions that need to be modified, and it is mostly just replacing openFd, fdWriteBuf,etc, with the windows equivalents. 
Looks like this is the JS that it transforms to: http://ect.bell-labs.com/who/ajeffrey/agda-frp-js/jAgda.FRP.JS.Demo.Clock.js
Well, at least future papers will be able to include a citation for that fact.
This was an idea that came up a few weeks ago on Google+, and I'm going to go ahead and start planning it. The idea is to use a combination of hangouts, darcs/git, and a wiki to coordinate a hacathon without travel. Want to try it? Fill out this form and tell me when you're available.
Agda isn't really the first language I would think of in order to put a running clock in the browser...but...ok...cool.
&gt; XML does actually tend to match nested, labeled, tuples. As does JSON. But more to the point, yes, ORMs are horrible very often, especially when used for objects. They match hierarchical data roughly to the extent that the hierarchy and values are predetermined. XML in general is a poor match; specific DTDs may match very well. Objects are doubly problematic because of adding inheritance to the mix, which is why ORMs are intrinsically broken. &gt; I work with practical haskell code for real world problems every day. I work with large databases every day, where the data makes sense to be in a database, and where a view of the data as a bag of tuples makes absolute sense. For my purposes, which I would imagine are not uncommon, then better records seem eminently sane. Large sets of well-structured relational data (as opposed to stuff wedged into a DB that doesn't belong there) are not uncommon, but I doubt they're extremely common, either. Of course, relational data also tends to be very poorly served by most implementations of labelled tuples, which are far too rigid for the power the relational algebra gives you. I actually have a toy implementation somewhere of pure relational algebra using anonymous recursive tuples indexed by singleton type tag names *a la* HList, with various forms of indexed joins. It was really fun, but slow as crap. :T Having a saner way to do things like that would be awesome. It's kind of funny, usually when I'm bashing ORMs it's because of people trying to impose OO design onto naturally relational data. Treating a DB as a dumb persistence layer and making it impossible to do meaningful queries is a long-standing pet peeve of mine. &gt; One should also take note that positional matching is fine for up to maybe 4 or 5 things -- after that it becomes increasingly difficult to deal with. When we're passing around rows with 20+ pieces of data, matching that information out by position (especially when we only need part of it) can be noisy, ugly, and painful. Yes, no argument there. Positional matching alone is very often a bad idea, and symptomatic of ignoring structure that should be present. Naming the fields is the right thing to do when that's the structure you want, e.g. rows of relational data. Naming the fields instead of applying some other structure that makes more sense is a kludge.
&gt; I think the problem here is that you're generalizing your experience too broadly Perhaps, but I'd say the same to you. :] Hard to reconcile that kind of disagreement, unfortunately. &gt; It's fine for you to say that you don't find this feature important, and that you've seen it misused. But to then tell all sorts of other developers, at least some of whom must have some idea what they're talking about, what they need... It's not a good look. I've never said it's not a useful feature or that *nobody* needs it. Obviously, you spend a lot of time working with exactly the kind of data where it's most unequivocally the correct approach, and cases like yours are why I'd like to see the situation improved, rather than simply throwing out record syntax altogether and being done with it (as has occasionally happened with features that really weren't useful at all). My contention is merely that simply making records *easier* to use in Haskell, rather than *more useful*, will result mostly in them being misused because of seemingly familiar idioms, rather than significantly helping people who really do need them to be more productive.
No idea. I'm not very in touch with either the Java or C world these days, though, so I'm not at all the person to ask.
OK, thanks for your input! I considered parametric polymorphism, but thought it would make this implementation more difficult, because of the need for typing "SBTree Rational" everywhere instead of "SBTree". Indeed the generateTree method can only work with an SBTree of type Rational, because the mediant function (which takes two Rational arguments) is used to generate the values of child nodes. Now I know better.
Is `zipWith` specified to have that evaluation order, or is it permitted to use any evaluation order it likes? The documentation I have installed doesn't mention evaluation order or strictness.
Well, `(&lt;*&gt;)` for `((-&gt;) e)` is `(=&lt;&lt;) . flip`, so I think the simplest implementation is `(join .) . fmap . flip`, which... appears to be exactly what you have there. :]
Hangouts only allow up to 10 participants, don't they? Is the plan that everyone starts a hangout and people just join every now and then? I don't think you can have a hangout open and join another one.
The plan is a hangout for each project... yes, they are limited to 10 people, but typically you have several different groups at a hackathon anyway, not everyone on the same project. Maybe it'll be a disaster, but in any case it's easy to give it a shot and find out!
Sorry for being late to the party, but this seems so insanely useful so I had to comment on it. This (and TDNR) will solve so many real world problems. I can't wait for 7.4!
If you want your program to be verifiably correct, going via Agda would be much more pleasant than the almost impossible task of verifying JS.
Eh, just get that little factoid into a Wikipedia article and cite this paper. Then it'll *really* be true! I recall hearing an anecdote roughly along these lines: An unsupported fact was carelessly placed in a news article from a generally respectable source. This was then used to justify including the fact into a relevant Wikipedia article. Based on that, the information was then repeated elsewhere. Some time thereafter, when the original source caught the oversight in fact checking, a quick review found several other citations, which satisfied them that the information was correct. Fittingly enough, I don't remember where I saw this and thus can't cite my information. :]
I'm interested. How long do you think it would go for?
[An update](http://permalink.gmane.org/gmane.comp.lang.agda/3128): &gt; Agda now knows where your (HTML5 geolocation compatible) browser is [...] I just came back from a walk round the block with my Android phone, and watched my coordinates update themselves.
I see. Certainly an interesting idea. Let's see how it works out.
I'm expecting it to be three days, Friday through Sunday, and basically pick your own schedule (though we'll encourage people that post projects to post their time zone and discuss hours). So you'd come when you can, and take a look at the current projects on the wiki, and either join one or post a new one.
Nowhere near as terrifying as it could be.
It's OK to make a type synonym like: `type SBTreeR = SBTree Rational`. And to have some functions that work on more specific instances of general types.
yield has always been integral to the Ruby programming language at least since people have actually used it- I am not sure where you are getting your information from. If Ruby didn't have yield (or some other similar facility) I don't think anyone would be using Ruby today. The Ruby yield is fundamentally different (as yields go) from Python's and not a copy of Python, which certainly didn't invent the concept. Python's yield is an external iterator, which the authors note is more powerful. Ruby 1.9 added Enumerators (which have a yield *method*, not the keyword) which are very similar to python's external yield. Ruby's internal yield is well integrated with the language and goes together with its blocks that were made a central part of the language. As the authors note, an internal yield is easier to use for the common case. 
I'm a bit confused w.r.t to Ruby's `yield` feature; it looks rather different (seems pulled inside out) to what is provided by Python's coroutine-like `yield` facility. For Python's `yield` see: * http://www.dabeaz.com/generators/ * http://www.dabeaz.com/coroutines/ * http://yieldprolog.sourceforge.net/ 
Ah, my mistake then... I remember the discussion among Python people when yield was added in Ruby 1.9, but I wasn't aware that was the *second* yield concept added to the language. Of course neither of these languages invented the idea, but it's certainly the case that Python popularized it at least among people I've worked with. Maybe there are other circles in which it looks the other way around.
I've posted this link because I think this also applies to Haskell (at a different level, I would say).
Ouch. Most of the time I really dislike the rhetoric of "most developers just aren't smart enough to use ___". It's almost always a dishonest argument used by people who who want to justify their own opinions by how stupid *other* people are. But that's pretty clearly not the case here. And it's hard to argue with the sheer quantity of his direct experience. That's... really depressing, actually. Dang.
Well, it doesn't need to implement laziness.
Rubyists don't call 1.9 yield, they call it Enumerators. It isn't actually a language feature. Enumerators are built on the new language feature called fibers (light weight threads under the programmer's control). I know nothing about Python circles. I just have the (possibly wrong) impression that yield is more central to Ruby than to Python, or at least that yield and blocks together are. But many more people know Python than Ruby, so probably more know about yield from Python.
This is the second of 2 honest posts on the subject by a big Scala promoter (who is also a Haskell fan). Note that some people in the Scala community disagree with him- they think teams can start writing better Java with Scala right away even if not taking full advantage of the FP in Scala, and do so without contention. Of course one can draw parallels to Haskell. Both are hard to learn largely due to the choice of advanced type systems, which is fairly independent of paradigms. Haskell is harder because it adds the purity pardigm. It is also harder for the more typical programmer used to OO. Haskell is hard to bring into a team because of the pure FP learning curve, whereas Scala has the Java/OO backdoor. But this article points out that it is not easy to get every one to write and maintain good Scala in a team for that same reason. If a team attempted Haskell they would have a true pass/fail test and be less likely to end up in limbo. Does this give Haskell a good selling point: that it is hard to write bad Haskell? I have seen bad Haskell- code littered with IO or passing large amounts of arguments and configuration state. But that is very rare and re-factoring even that is usually at least very safe and sometimes straightforward. So the damage the novice can do is limited. But instead of limiting power with the intent of preventing a programmer from doing things, Haskell gives you a great deal of power- just in a way that greatly prevents it from being abused- the novice can eventually harness it.
I think the reason why these Java programmers are to "dumb/unmotivated" to switch to a better paradigm is largely because they have been programming Java in a Java culture for a decade and have the common human trait of being resistant to change, not that they are inherently stupid.
Yeah. I've seen human-created Javascript that was worse than that. Actually, I've probably written worse Javascript than that myself. :T
Yeah, I do think there's a large aspect of people living up *or* down to the expectations placed on them. But still... articles like this make it hard to convince myself that's all that's going on. :[
Perhaps Scala should adopt the Haskell mantra: avoid success at all costs! Keep it research-driven for as long as possible, I say.
I'm hopeful. If it was as cut-and-dry as hard languages == less industry adoption, how do you explain the popularity of C and C++ today? Haskell seems to be a tough sell in my town (St.Louis runs on the JVM) but Clojure seems to be doing pretty [well](http://www.revelytix.com/content/careers). Scala is featured largely at this year's [Strange Loop conference](https://thestrangeloop.com/schedule) at which Bryan O'Sullivan is giving a Haskell workshop.
I think copumpkin was the first person I saw use it really heavily. I just adopted the style from him.
C got popular because 1) Unix was written in C, so everyone who got Unix got a C compiler with it, and 2) C is *fast*, while still being portable. C++ got popular by being a more convenient version of C, while still being just as fast and portable. Scala doesn't have these advantages. Oracle doesn't ship it with the JVM or the JDK. It's just as portable as Java, while not being orders of magnitude faster.
That is not the mission of Scala. 
Obviously, or else the suggestion would be moot.
I think it will. Thank you. I'll have to look at that when I have some spare time this weekend.
Thanks for the reference. I'll take a look.
I did not read the article, as it is late at night. I just want to make one argument to rebut what you say: &gt; And it's hard to argue with the sheer quantity of his direct experience. It is very easy to argue against 'direct experience' no matter how much he has. Direct experience is anecdotal, and thus is useless to the claim you cite: &gt; most developers just aren't smart enough to use ___ This claim, or a more useful generic would be far better answered by actual testing and statistical analysis. 
Re: "it is hard to write bad Haskell ... the damage the novice can do is limited". I used to share this kind of optimistic belief. Sadly I don't any more (after several years of Haskell development which has included several encounters with novice code). Note, that's not to say it's not hard_er_ to write bade code in Haskell than in many other languages (indeed I think Haskell is about as good as it gets right now) - just that purity and types can only go so far - they can't substitute for experience. (I strongly suspect the story would be similar with full dependent typing - unless the novice is just coding to types provided to them).
Does that mean the clock always shows the correct time?
&gt; I spent a lot of time trying to get it to work on my own as an exercise to learn Haskell I/O Well, trying to learn Haskell by starting with I/O doesn't work very well. It may work for Python or Ruby, but not for Haskell, the language is too diffrent. You simply need a certain level of expertise to do advanced I/O problems in Haskell, that's just how it is. (One indication of expertise would be that you know the Prelude and the Control.Monad.* instances by heart, for example.) Easy I/O problems (reading a file, traversing a directory) are still easy, though. &gt; unsafePerformIO . functionIWantTheResultsFrom Lazy I/O does not require `unsafePerformIO` in any way. Nor is the FUD around it accurate, you just need a good understanding of lazy evaluation (it's *lazy* I/O, after all) if you want to understand its resource usage. &gt; Fundamentally this is just a graph-traversal problem where the graph isn't a pregenerated object in RAM, so the conclusion I took away from this is that Haskell makes solving typical computer-sciencey problems at scale harder rather than easier. Well, that's not my experience, neither with file system traversals, nor with the typical computer-sciency problem, which often happens to be parsing. 
Hah, well, I could change my system clock, so no. :) Really depends what it is you want to prove. My point was that reasoning about JS is fiendishâeven the simplest operation, assignment, can't be reasoned about simply (thanks to things like the with statement).
Well I wouldn't go that far. But it does make a case for not putting that much weight into avoiding certain more elegant design decisions, just to keep it closer to Java, since those who are resistant to learn it won't bother anyway. For instance, Scala's stuck with the "/" operator for integer and real division, which are completely different operations, rather then using "div". Again there will be a bunch of "(3/4)*x"-like errors going unnoticed. 
&gt; C++ got popular by being a more convenient version of C, while still being just as fast and portable. &gt; Scala doesn't have these advantages. Yes it does. Scala is a more convenient version of Java, and if you use it like that it's just as fast and portable. But it can be so much more, and you can gradually learn the more interesting bits.
thanks. you should post the actual blog rather than a re-publisher next time.
Consider yourself lucky. Indianapolis is MS land.
It seems to me that a strategy suggestion can (A) reinforce a direction under consideration, (B) suggest an improved direction, or (C) suggest a radical replacement of strategy. None of these is moot; all are valid; but I meant to imply that your suggestion was Type C and that this is perhaps uncalled for. Sorry if I was too concise. :-)
[I'll just leave this here.](http://hackage.haskell.org/packages/archive/CC-delcont/0.2/doc/html/Control-Monad-CC-Cursor.html) :)
Pollack talks about adoption failure of Scala from the bottom (java programmers). Well this is familiar to all of us. Any FP language has the same problems (haskell, lisp, erlang). It's an obvious fact that is not really interesting to discuss here. But as i [commented](http://www.reddit.com/r/programming/comments/kf2xi/scala_use_is_less_good_than_java_use_for_at_least/c2jvyhm) on this article earlier, scala is rejected alot also from FP crowd, people coming from haskell and lisp. I do not like scala (for reasons i outlined in my comment) and prefer clojure when working on JVM. [Michael Snoyberg](http://www.reddit.com/r/haskell/comments/jqlll/simplicity_in_clojure_i_guess_for_haskellers_this/c2ehxek) also did not like scala when he tried it a few times. So it would be interesting to conduct a poll amongst haskellers who tried scala or who are using scala currently, how it fares with them. And what are the reasons they like / dislike scala. And what are the viable FP alternatives on JVM for them. 
I wonder if Scala's association with Java is a negative from a management perspective. Managers would have to wonder why they should bother with another language on the same VM given that they can get what they want done with their existing staff skills. Decision makers may see Scala as unnecessarily mucking things up. Perhaps Haskell is the clean break that's needed in some organizations.
Simon Peyton Jones has written a [second book][1] that is very similar to this. If I remember correctly, he also included the .tex source to it, so it might be possible to turn it into .epub. [1]: http://research.microsoft.com/en-us/um/people/simonpj/papers/pj-lester-book/
All hackathons need a buffet area to hang out at too during hacking breaks.
Sorry, I didn't notice. The original article is [here](http://goodstuff.im/scala-use-is-less-good-than-java-use-for-at-l). The author is David Pollak.
Cool! I had to implement Chord in my distributed systems class, but it was in C++.
Definitely interested; as an undergrad I don't have a lot of cash to travel, but I could definitely devote a weekend to some Haskell hacking. imho we should do this sort of thing on at *least* a quarterly basis, if not monthly.
&gt; Does this give Haskell a good selling point: that it is hard to write bad Haskell? I have seen bad Haskell- code littered with IO or passing large amounts of arguments and configuration state. But that is very rare and re-factoring even that is usually at least very safe and sometimes straightforward. I'm surprised that's all you've seen. The two very distinctive features of Haskell, aside from being high order are, * Purity * Laziness In my experience, and non-surprisingly, all the bad haskell I've seen is related with either of these. On the *purity* side pretty much what you've mentioned. But another very common one are functions with tons of sub functions, so that they can use certain variables globally. Quite frankly even I don't have any amazing solutions for this one. 'implicit parameters' did offer a nice way of writing these sub-functions independently but to the best of my knowledge they are deprecated. Then *laziness* is completely disregarded by some of people. Many algorithms that would be much more elegantly expressed by means of consumers and producers - in typical origami style with folds and unfolds - are implemented in an iterative fashion, using recursion. This code tends to be much harder to test and adapt - E.g. You want to check the number of steps performed, ops now you need an extra argument in your recursion. Or if you're "smarter" you make it "global" in typical imperative programming style, but of course using a state monad. 
&gt; a somewhat-disturbing number of the tools we are interested in are in Haskell I lol'd. But really, this is great that they are seriously considering gitit.
If only we knew what's in a manager's head. Nevertheless, I think that Scala can be seen as a the *continuous* path towards integration of the FP practice, with all the advantages that FP may bring to the organization. Faced with the fact that Java is too old to adopt contemporary ideas (e.g. no closure yet, inappropriate models for concurrency) and that they will eventually be too-much-behind (it's 2011, you have to evolve with the rest of the world), many organizations are wondering what will be the "next Java". I think that today Scala offers the most viable solution for the ones who own a big Java code base and many JVM-based solutions. You can feel this in the design of Scala with all the compromises, a dÃ©jÃ  vu of C++ making Odersky a modern Stroustrup. My company is in this position (i.e. in what language are we going to code in 5 years?). It is out of the question to abandon 10 years of coding efforts to go somewhere else. So this Haskell *clean break* would be killer for us (I really love Haskell but I'm being realistic here). Nevertheless, it is clear that we may benefit from using FP and a part of our management is already sold to that idea. But who said that 100% of the code need to be in a single language? As long as legacy code can inter-operate with the new one (the JVM may be the key for that), everything is fine. What will probably happen is that at some point, we will conduct some pilot projects using some JVM-based languages and we will eventually write most of the new code in the most suitable language for our task. The *clean break* would definitively be an option if we were to start all over again.
 ghci&gt; "not being smart enough" /= "being stupid" True I consider myself a moderately smart programmer, my career made me use Java much more than everything else combined together. While I have the feeling Scala is something I can master, I don't have any problem admitting that I often struggle with Haskell and I don't think I will ever be a Haskell guru. 
I'm a long time C and C++ coder and have been learning/playing with Haskell for a couple of years now. My perspective is as a language user, not an implementer. I don't even pretend to understand how compilers work. Of the three proposals (Better name spacing (BNS), type directed name resolution (TDNR), nonextensible records with polymorphic selection and update (RPSU) I think I prefer RPSU best. My main argument against BNS is that it is too burdensome for the user. Like others, I avoid name clashes with prefixes. To minimize typing, I abbreviate them. data Employee = Employee {eID :: Int, eName :: String, ...} data Group = Group (gID :: Int, gName :: String ...} fn = eName fred gn = gName qa Disambiguation by data types would work fn = Employee.name fred gn = Group.name qa but that would require me to type a lot more than I do today. Using file-local modules with qualified names might allow me save some typing, but I'm not sure specifying modules and their abbreviated names would be worth the hassle. On first reading, TDRM seemed like a great solution. I would be able to type fn = fred.name gn = group.name which seems perfectly "natural" and "obvious". But upon further reflection, it doesn't "feel" like the Haskell way. When first learning Haskell, it took me a little while to get comfortable with the notion that extracting the field from a record is really just calling a function, whose name is the "name of the field". But TDRM seems to turn this idea on its head. In fn = fred.name (.name) is a new kind of function invocation, and unlike other Haskell function calls, the function name comes after the argument. To me, it seems like TDNR is trying too hard to make a function call not look like a function call. I think TDNR would feel more "Haskell-ish" if it were invoked with something that looked more like fn = name# fred I didn't see any downsides with RPSU. Its solves the namespace issue and its use fn = name fred gn = name group still looks and feels like Haskell. Finally, I would also suggest that any solution for this limited namespace issue not preclude the ability to implement first class record types in Haskell some day. I would also hope that performance is considered. If a data type consists of all unboxed fields, it would be good to be able to access them as quickly as possible. 
The Prelude definition is considered the reference implementation that defines the function.
&gt; I consider myself a moderately smart programmer, my career made me use Java much more than everything else combined together. While I have the feeling Scala is something I can master, I don't have any problem admitting that I often struggle with Haskell and I don't think I will ever be a Haskell guru. See, this is where I'm more optimistic--perhaps too optimistic, but I don't think that's the case. I think that in an environment where Scala, or Haskell, was widely used--and more importantly, that the benefits offered were assumed to be the norm, not something unusual--you'd be just as comfortable with that as you are with Java now. In other words, I don't think the problem is that Scala or Haskell is too hard for most people, I think the industry as a whole just has standards that are too low, and that mainstream programmers--almost by definition!--don't deviate much from that norm, and that it's not reasonable to expect otherwise.
Haskell, leading the world in syntactically-valid emoticons.
Awesome! Let's see how the first one goes, and then talk about when to schedule the future versions. I'm skeptical that monthly would be sustainable for a lot of people who already are devoting lots of time to other parts of the community (for example, it wouldn't have worked for the next few weeks because of ICFP and ensuing recovery time)
Is Scala an almost-superset of Java, in the way that C++ is an almost-superset of C? Do most Java programs compile with scalac? Because from all the things about Scala I've read, it seems like a different language that runs on the JVM and can use the same types as Java.
And now please obfuscate a Mandelbrot renderer into the operators and their parameters.
I found your thoughts on the subject interesting, so I'm now going to quibble over them. :] &gt; that would require me to type a lot more than I do today. An improved namespace approach would, ideally, be designed to improve namespace management across the board and only incidentally make record names less of a hassle. As such, I'd expect that something similar to how modules work now would exist as well, so you'd be able to do something like this: data Employee = Employee { id :: Int, name :: String, ...} qualified type Employee as Emp fn = Emp.name fred That's completely hypothetical syntax I just made up on the spot, of course, and is based on my own preference for a more far-reaching enhancement to namespaces, but any namespace-based solution would have to provide *something* along those lines, for exactly the reason you pointed out. For that matter, simple type synonyms as they currently exist would probably work about like my example above, at the cost of introducing a spurious synonym only used for qualifying names. &gt; unlike other Haskell function calls, the function name comes after the argument. This isn't quite true, in a way that may or may not bolster your point, heh. It's trivial to write a "reversed function application" function, of course, and making it an infix operator creates the same effect as your examples. For instance, defining `(|&gt;)` as `flip ($)`, you can write `fred |&gt; name` and such. As it turns out, there's already one particular operator that does a slightly modified version of function application in that order, which is used all over the place in the standard libraries. Compare: (|&gt;) :: a -&gt; (a -&gt; b) -&gt; b (&gt;&gt;=) :: (Monad m) :: m a -&gt; (a -&gt; m b) -&gt; m b Hmmm. &gt; I didn't see any downsides with RPSU. Well... I'm not an expert on such issues, but I would guess that the main downside is that it's the "biggest" proposal of the lot, in terms of implementation complexity, and possibly also in terms of changes to the language spec. This is particularly the case if first-class record fields are also a goal, because that means hiding the changes from the user isn't an option. GHC probably has a lot of what would be needed in place, but there are reasons many GHC extensions aren't just dumped right into the core language spec. TDNR is a more drastic departure in some ways, but I expect in a more self-contained way.
Note that it was just a quick hack to demonstrate the point. Beware of bugs, as I've only type-checked the code, not tested it. ;] If you have any questions, I'd be happy to help.
It seems WinGHCi is really not enjoying this. It can parse and run the code, but as soon as you make a mistake in editing it, it chokes on printing the operator. Even `:t` doesn't work. *Main&gt; :t (â¯Â°â¡Â°) (*** Exception: &lt;stdout&gt;: hPutChar: invalid argument (character is not in the code page) 
Ehhh. Lazy I/O is intrinsically crippled for certain kinds of problems for exactly the reasons that it's convenient for others. You give up control over how some input is performed, in order to weld the results to an incremental pure function. As long as you can maintain the illusion that input just happens magically, you're fine, but you lose a lot of composability there in order to retain the composability of (pre-existing) pure functions. It's great for a broad class of (simple, common) problems, but it's a dead-end otherwise. The most popular alternative is iteratees, which certainly work well, but screwing around with low-level state machines is a pain when you're just doing simple stuff. And I think it's silly that "advanced I/O" requires as much expertise as it does, because I can't see any reason why it *should*. Honestly, most I/O-oriented problems in Haskell can be solved just fine with regular, boring old non-lazy IO, a few helpful idioms, and actually taking a higher-order functional approach to IO from the ground up, instead of trying to bolt imperative, stateful approaches onto functional code after the fact.
I seem to recall WinGHCi having known problems with Unicode and code page settings. :T Might work in GHCi from a terminal window.
In any other language community, I'd assume this is a joke...
Actually, that second book covers only two thirds of the book the OP is interested in: &gt; **Relationship to** *The implementation of functional programming languages* &gt; &gt; An earlier book by one of us, [Peyton Jones 1987], covers similar material to this one, but in a less practically oriented style. Our intention is that a student should be able to follow a course on functional-language implementations using the present book alone, without reference to the other. &gt; &gt;The scope of this book is somewhat more modest, corresponding to Parts 2 and 3 of [Peyton Jones 1987]. Part 1 of the latter, which discusses how a high-level functional language can be translated into a core language, is not covered here at all.
That will result in a pretty big list of 'o', 'w', and '!'s.
IMHO Scala should be regarded as something completely different from Java (i.e. no semi-superset relation as opposed to stuff like Groovy for instance), the only common denominator is the JVM and interoperability with the Java type conventions. 
That's what I thought, and it means that Scala isn't a more convenient version of Java.
The drama! à²¥_à²¥
I with agree you from a logical/reason based standpoint. However, when you intrude on Java, and can only offer what's perceived as an incremental change with a ton o' learning pain; it's going to be a hard sell. That's where a clean break can help, especially if new projects are on the horizon that aren't necessarily tied to the old.
~.^ declared and not used!
Then again, I think many people would say the same about C++ vs. C. Admittedly, it's mostly C programmers who would say that. But still.
I cut his part at the last minute. He kept forgetting his lines, so we put him in charge of lighting.
As the guy was just hit by a thrown table, going "ow!" a lot seems to be in order.
Your criteria for this are extremely narrow. If I added some type inference and first-class functions to Java, and decided that instead of T v one would write v : T for variable type annotations, then it would invalidate every Java program, and so you would say it is not a more convenient version of Java. There are very few things, if any, in Java that do not have a straight forward analogue in Scala. The difference is fairly superficial syntax. The keyword `trait` instead of `interface` for instance. Once you learn the translation, there's nothing stopping you from writing programs in Scala the exact way you'd write them in Java, with a few Scala bonuses. I doubt even the smuggest lisp weenie would suggest that the average Java programmer is incapable of learning even a slightly different syntax for Java. So the syntactic superset criterion for being "a more convenient Java" doesn't strike me as being very compelling.
&gt;which seems perfectly "natural" and "obvious". But upon further reflection, it doesn't "feel" like the Haskell way. When first learning Haskell, it took me a little while to get comfortable with the notion that extracting the field from a record is really just calling a function, whose name is the "name of the field". But TDRM seems to turn this idea on its head. In &gt;fn = fred.name &gt;(.name) is a new kind of function invocation, and unlike other Haskell function calls, the function name comes after the argument. To me, it seems like TDNR is trying too hard to make a function call not look like a function call. I think TDNR would feel more "Haskell-ish" if it were invoked with something that looked more like &gt;fn = name# fred I had the same feeling. Still I was looking over the mailinglist, and found this discussion we're having is happening on the ghc-users list too. http://www.haskell.org/pipermail/glasgow-haskell-users/2011-September/020897.html And there simon seems to be saying TDNR needn't touch record syntax - in fact internally it sounds damn similar to RPSU: http://www.haskell.org/pipermail/glasgow-haskell-users/2011-September/020904.html &gt;&gt;For example, why shouldn't: &gt;&gt; &gt; &gt;x f = f.x &gt; &gt; &gt;&gt; be a reasonable function? &gt;Yes, it would, and of course any impl of TDNR would need an internal constraint similar to your Select ... class Select (rec :: *) (fld :: String) where type ResTy rec fld:: * get :: rec -&gt; ResTy rec fld &gt;... &gt;And now we desugar f.x &gt;as get @T @"x" f &gt;where the "@" stuff is type application, because get's type is ambiguous: get :: forall rec fld. Select rec fld =&gt; rec -&gt; ResTy rec fld ---- Btw an interesting type - no 'undefined' label argument here, but some soon-to-be-implemented type application stuff? I wonder if @T is necessary, since that looks like its coming from nowhere and its just a type of that record, f - why isn't just the argument f enough? Because instead of special dot syntax, this could otherwise be a function right? And before that, we had a kind String, for type-level strings, possible when the new kind system becomes stable, cool - though I guess he's just writing the ideal code, and it would be doable as a class like in RPSU while this isn't in place. in any case, pretty similar to RPSU, where its: select :: r -&gt; n -&gt; Field r n and l1 = select r (undefined ::''l1'') ...apparently the dot thing was to enable the API to be more easily interactively explored, and should work for any functions rather than just records for that reason. I wonder when do these implicit instances get generated? In any case, if it were used just for the records, then it would be equivalent also in offering polymorphic update: &gt;It's a little unclear what operations should be in class Select. 'get' certainly, but I propose *not* set, because it doesn't make sense for getIndex and friends. So that would mean you could write a polymorphic update: f v = f { x = v } &gt;Restricting to record fields only would, I suppose, allow polymorphic update, by adding a 'set' method to Select. the reply msg is also interesting. Seeing its possible internals I support TDNR, provided it offers polymorphic update (I gather from the reply msg this needn't necessarily imply limiting TDNR for records only, but unsure whether it should be so limited or not), and hopefully not use the damned dot (or at least let it be something, anything else but a dot) ;) 
&gt;but I would guess that the main downside is that it's the "biggest" proposal of the lot, in terms of implementation complexity, and possibly also in terms of changes to the language spec. This is particularly the case if first-class record fields are also a goal, because that means hiding the changes from the user isn't an option. actually the guy who made that proposal said in the mailinglist: http://www.haskell.org/pipermail/glasgow-haskell-users/2011-September/020912.html &gt;This has the advantage that the extension to Haskell is fairly small, and it's compatible with existing user code, but if we later decide we want extensible records, we need only add a type function to order Label lexicographically. also, it appears that internally TDNR would/(could?) work in quite a similar manner to RPSU; w labels as type-level strings, (I presume optionally) a new kind String, type application notation instead of an actual field argument - but implementation and syntax details aside, the same thing ;P just to add, I also think namespacing everything would be a great thing irrespectively, and could answer TDNR's wikipage question of: &gt;The problem is that you could then only refer to x and y using TDNR, and I rather dislike that; I would prefer TDNR to be a convenient abbreviation for something one could write more verbosely. If you like, what is their "original name"? 
&gt; Your criteria for this are extremely narrow. Yes they are. Most every C program is a C++ program, which means most every C programmer is a C++ programmer already. This makes it trivial to slowly add C++ features to a large C program. In contrast, adding Scala features to a large Java program involves rewriting the entire relevant class files in Scala, since it has a different syntax. I'm not saying that Java programmers are incapable of learning Scala, I'm saying they haven't done it, and that makes it harder to transition.
I'm quite comfortable with Haskell; I've been using it for fun for years, for example for Project Euler. The project in question was specifically for learning to use Haskell to do I/O in the kinds of problems I was already facing. You're frankly sounding like the first class of respondents I mentioned in my previous comment: dismissive, usually because they haven't actually thought about the problem in question. Please, try traversing a filesystem using lazy IO but not "unsafe" functions. If you can do it, I would sincerely appreciate seeing the results. Next, consider a fold over a filesystem using proper monadic IO with a signature like this one: foldFS :: (FilePath -&gt; ResultT -&gt; IO ResultT) -&gt; FilePath -&gt; IO ResultT If you can write it in a way that is concise, readable, and uses bounded memory like a shell -&gt; Perl pipeline would, I promise to be impressed. As a test, feed it a function that will print all the filenames as it encounters them ("find") and then report the total file sizes ("du"). (You can report file sizes rather than block counts, since the Haskell standard library doesn't expose that member of struct stat.) 
As someone who develops both haskell web apps and lisp (clojure, sbcl) web apps, as well as java, dotnet, asp, php and python for years, i can tell you that nothing comes close to lisp in terms of quick prototyping. If this is your main goal, then pick up any modern lisp, Arc, clojure (my favorite), Racket, sbcl and you will be fine. I would personally go with clojure because of the mature platform (JVM) and tons of stable, production ready, excellently documented libraries for practically all kind of work you could think of. Take a look at compojure and ring web frameworks. If you still want to work with haskell (i do :)), it is also an excellent choice for web applications. Between Yesod, Snap and Happstack you will definitely find something to your liking. 
Well, I wouldn't try to write a network client with lazy I/O, because the order of I/O matters a lot there. As for Iteratees, they can't do anything that lazy I/O can't do as well; their only advantage is that they offer a guarantee (within limits) that you don't leak the file contents. Concerning the expertise, I just meant that one should have a good understanding of higher-order functions and "type-driven programming"; beginners usually struggle a lot with that. I think that knowing the Prelude and the monad instances by heart is a good litmus test for that. (The main learning block with `&gt;&gt;=` is that it's a higher-order function with a seemingly weird type.)
Why not `act 1 = ...` instead of `act_1 = ...`? :-)
That must have been a **very** big table for it to elicit 285600 times "ow!"
Heh, well, it wasn't so bad until I introduced a couple of additional lines of dialogue. ;)
Same thing happens for cmd.exe and PowerShell in Windows.
good idea =)
Great reply. Thank you. But I've never used Java, that seems like it rules out Clojure. (I don't really know anything about Clojure; that's just my assumption.)
What impression does the following `foldFS` function make on you? import Data.Functor import System.Directory import System.FilePath import System.Posix.Files foldFS :: (FilePath -&gt; a -&gt; IO a) -&gt; a -&gt; FilePath -&gt; IO a foldFS f acc x = go acc [x] where go acc [] = return acc go acc (x:xs) = do b &lt;- doesDirectoryExist x if b then do ys &lt;- getProperDirectoryContents x go acc (map (x &lt;/&gt;) ys ++ xs) else do acc &lt;- f x $! acc go acc xs getProperDirectoryContents x = filter (`notElem` [".",".."]) &lt;$&gt; getDirectoryContents x listall = foldFS (\f _ -&gt; putStrLn f) () totalsize = foldFS (\f acc -&gt; do s &lt;- getFileStatus f; return (acc + fileSize s)) 0 test f = f =&lt;&lt; getCurrentDirectory It doesn't use bounded memory, but that's because every depth-first traversal uses memory proportional to the path length. This version uses a bit more memory than that because it caches directory listings on the path. By the way, Bryan O'Sullivan (author of Real World Haskell) has written a nice [library for file system traversals and such][1]. [1]: http://hackage.haskell.org/package/filemanip
I see you didn't list javascript. For prototyping, it's hard to imagine how another existing language would make it much faster as it can be done with jQuery currently. What exactly makes Clojure so good that "nothing comes close in terms of quick prototyping"? (I'm familiar with Java, Scala, .NET webapps)
Clojure is a (well-designed) Lisp that runs on the JVM. It has very little to do with Java, but you can of course call Java libraries. http://learn-clojure.com/clojure_tutorials.html Arc is (IMHO) a toy language, or rather it's a tiny macro language on top of Scheme. If you're more interested in Scheme, then I recommend a look at [Racket](http://racket-lang.org/) (formerly known as PLT Scheme).
&gt; It is very easy to argue against 'direct experience' no matter how much he has. More particularly, one should be dubious of quantity of direct experiences. One's direct experience suffers from extreme selection bias. The only time quantity is useful is if it has/can be demonstrated that the samples come from enough different groups to cover the space, or that the space is homogenous enough that sampling from one group will suffice.
Paul Graham was operating before the advent of easy to re-use code - everyone was writing code from scratch, particularly in the web space. Today how fast you can create a site is depenedent almost entirely on leveraging existing libraries. Unfortunately there just aren't that many in the web space for Haskell or lisp when compared to languages like Ruby or Python that you can still move quickly with. I have programmed a lot of web apps in Ruby. I would use Sinatra or maybe Rails. Personally I like client-heavy apps though and end up spending most of my time in javascript using Coffeescript + Backbone.js on a prototype now and am looking to learn better tooling in that area. Going client heavy also means that it is easier to switch to Haskell for the backend later when out of the prototype stage. 
anyone tried using the [iterIO](http://hackage.haskell.org/package/iterIO) library yet?
He probably couldn't even read them with those screwed up eyes. 
Such concept might be useful, and I am sure sociologists have means to pull useful data out. But it is also possible to have everyone selecting the same bias... I wonder how they would handle that.
I agreed with your first point about leveraging existing libraries, then found myself flummoxed your second point -- in my experience there are *plenty* of libraries in the Haskell web space -- certainly enough to lean on for rapid prototyping of almost any sort. I'm curious what sorts of things you feel are missing vis a vis Ruby or Python?
Why is this strange? You've written a function that works **if** you provide it with a `Num String` instance. But since you never use it, the instance is never needed.
Haskell has no shortage of platform-level stuff for effective web development, but I suspect where it's lacking (in comparison) is additional libraries on top of those to provide chunks of pre-built, largely self-contained functionality that can be integrated easily into a larger site. For serious web development where you'd be doing a lot of adjusting and customization anyway this isn't really a problem but for extremely rapid prototyping it's nice to be able to drop in something that's close enough and move on. Glancing just at Yesod stuff (since it's easy to look for, and I imagine the others are similar) on Hackage it looks like most of the *really* obvious stuff is there, but not too much beyond that. I suspect that most such libraries begin life when someone can't find one, writes their own, then uploads it somewhere as open source in hopes of getting other people to improve it. So it should be a largely self-correcting problem as people keep doing web development in Haskell.
&gt; mature platform (JVM) and tons of stable, production ready, excellently documented libraries A myth which almost everybody in the field holds as evident.
While it's true that on the code level, they're different syntactically, so a java programmer is not "automatically" a scala programmer, interop is designed to be almost 100% trivial. Also, the concepts from java map directly into slightly different scala syntax. So you can easily start throwing .scala files into your .java project and reference all your existing java functionality. You can inherit from java classes/interfaces, call java methods, have nearly seamless conversion between java types and scala types, and so on, in both directions. We do this at my job on a large java codebase and it works well.
I use JQuery extensively in my everyday work. It's the best thing that happened to javascript. But, - I use Javascript only as a client side tool. The OP obviously was asking for server side web framework. - quick prototyping in lisp is a product of a synergy of an incredibly malleable language (macros), repl, and killer IDE (slime). The way slime allows me slice and dice code, move around, copy/paste, transpose s-exps is unmatchable to anything i used in my 20 years career. (VS, Eclipse, Netbeans, Delphi, IDEA). Add to it appropos, fuzzy completion, jumping to and from function definitions, hot update. None of it is available for javascript. - any change in javascript requires reloading the entire page. In Lisp with slime you can just update in place changed function or even a global variable without unloading application. And yes, i know about FireBug :) It's not the same. - error reporting in javascript is abysmal. I spent uncountable hours trying to find out what did i do wrong with some jquery library or plugin (jqGrid arrrh). Javascript program either works or defiantly silent, to the point that you have to waterboard it to get any answers. This definitely does not help with quick prototyping.
In Haskell98, all missing instances are found immediately at the use site. I did not expect this to change with extensions on. But I agree that in the "type-class restrictions as parameters" world view, it makes perfect sense. 
Haven't used it yet, but I read the docs, which are incredible.
The JVM is certainly mature, but I don't think much of the libraries.
Check `hGetEncoding stdin` in a cmd.exe GHCi. (You don't need to check with PowerShell; it uses the same cmd.exe terminal.)
Personally, I'm happy to think of type class constraints either as extra arguments to the function, or as logical assertions. It's the way the two perspectives collide due to the instance arguments being implicitly found by GHC when the result is used that makes me scratch my head. I hadn't realized this could happen with flexible contexts in general. It's a lot easier to do it unintentionally with fundeps.
He was asking for server side webframework but because a prototyping perspective was brought into discussion, I don't see why Javascript should be left unmentioned. In my current project, I used jQuery for the prototyping. I also code a lot in Javascript in hobby projects because it is so fast to edit (quick reload), and expressive to manipulate DOM. I do have noticed the problems with debugging. For me, it is the biggest concern and waste of time. Though, I don't use 3rd party libraries unless they are widely used or otherwise simple. So I don't get the downvotes, as I do javascripting professionally for what was discussed, and I didn't know asking honest question is not even a neutral thing. The most interesting part I got from your reply is the IDE, Slime. It is good to know Clojure's ecosystem has evolved to have such good an editor. Structural refactoring is what I mostly miss when I use Webstorm. Thank you for your reply.
Why Sinatra &gt; Rails?
Slime is a common lisp ide in emacs. But it can be easily adopted to work with practically any lisp, which clojure programmers did.
With respect to prototyping: If you are experienced with Rails then it is a better option. And there may be some Rails specific plugins you need for your prototype that would make it a better option. I recommend Sinatra to newcomers because it is simpler and should be easier to learn. From the non-protyping angle, there is a gem called sinatra-synchrony which drops in and lets you have async IO.
actually, Haskell has a big shortage of even the most basic web development libraries. I have been working on Yesod for the last year to remedy that. We are close the point where there aren't any gaping holes (advanced form validation and model validation is all that comes to mind there), but the Haskell libraries available are often limited in their functionality. For example, with the latest release of Persistent we finally have a decent "ORM". However, we are still hurt by the records issue, and the SQL part of the ORM has very limited join functionality. The MongoDB part of Persistent doesn't yet fully support embedded objects. This is a problem for both prototyping and "serious web development". I think Yesod is finally to the point of having enough basic supporting libraries that we can say it is a serious contender with alternatives for "serious web development". The main reason is due to Haskell's strong-typing, speed, and async IO, none of which I care greatly about for a prototype.
Although if possible the [specialize pragma](http://www.haskell.org/ghc/docs/latest/html/users_guide/pragmas.html#specialize-pragma) is preferred.
Wow, they are great. If only other hackage libs had docs half this good.
Yes--to be clear, I was very much speaking in the present tense there. A lot of the groundwork exists (as far as I know), but many of the higher-level components taken for granted elsewhere may be absent or incomplete, and I would expect a lot of "plumbing" is missing as well. I've certainly noticed the improvement over the last couple years (which apparently is due in part to your efforts--so, cheers!). And what I meant by "serious web development" there is that, on a large enough project which wasn't heavily reliant on anything completely unavailable, I'd bite the bullet and accept the cost of implementing the missing components and building supporting systems as I went, in order to keep the other benefits of using Haskell.
I've never used Rails. Thanks for the advice. I don't think I understand the sinatra-synchrony gem.
hungry for more? [part two](http://biosimilarity.blogspot.com/2011/08/i-am-not-monad-i-am-free-algebra-pt-2.html) and [part three](http://biosimilarity.blogspot.com/2011/08/i-am-not-monad-i-am-free-algebra-pt-3.html)
I prefer C3, but I wonder whether that would work with unboxed types. E.g., data Record = Record { x :: Int#, y :: Char# }
Nice. A few thoughts: 1. Do you know about `hlint`? I ran it and it gave me a couple of things that, without looking at the code in context, looked like pretty good ideas. 1. It might make more sense for `htodo init` to put the db in `~/` or `%appdata%` or some central location like that, instead of the current directory, at least by default. 1. It would also be nice to have a default priority so that you can just enter the text of the item and then press enter twice. 1. When you mark an item as "done", is it deleted from the db, or is there a way to show the items that are done? The help doesn't seem to say one way or the other.
This confuses me... I am at the same time familiar with basically all of the ideas expressed here, and yet thoroughly confused as to what the articles are trying to say. It seems the author is deliberately avoiding concrete examples or definitions, and may or may not have made a bet with someone regarding how many words from category theory they can fit into three blog posts without actually stating a conclusion. Heck, I'd settle for a description of why the first article seems to think that the sets List[G] and G\* are different from each other. For example, I can assign no meaning at all to "from a certain vantage we can treat List[G] as a concrete model of G\*". Assuming that List[G] is supposed to refer to a finite list type, they are *the* *same* *set*, defined with two different notations. (If List[G] is supposed to be Haskell lists, I still don't see how the wording from the article makes sense; one is a subset of the other, consisting of those elements of finite length. I am not sure how you'd call that a model.)
Nice , I was thinking you should add the &lt;&gt;&lt; and &gt;&lt;&gt; operators to the header of your blog. :-)
Well, if I had to guess... So, if `[G]` is the *same* as G\*, I would expect that `data GStar = GNil | GCons G GStar` is also the same as G\*, and transitively `[G]` as well. Yet for some reason GHC seems to think that `[G]` and `GStar` are not, in fact, the same thing. Perhaps that's what he means there? Most programming languages do not, unfortunately, give us the luxury of working directly with the platonic ideals of algebraic structures, only specific implementations of them that are (alas) not automatically identical to other representations, even if they are equivalent by a unique isomorphism.
I'm still missing how that's the same distinction given in the article. The relationship described there was a very asymmetric one: G\* was the "real" thing, and List[G] was the model or representation. The claim is made that this is the same as the relationship between Monoid and Monad. That certainly seems rather false to me; in one case you've, okay if not the same type then isomorphic ones. In the other case, you have very much non-isomorphic ideas... admittedly related ideas, but I have no idea how contemplating the relationship between G\* and List[G] is supposed to clarify that relationship.
You might check this Stack Overflow question. http://stackoverflow.com/questions/5645168/comparing-haskells-snap-and-yesod-web-frameworks/ To add to what I wrote there I think the following characterization is fairly accurate and objective. The big idea behind Yesod is to see how far type safety can take the construction of web apps--most notably, type safe URLs. To accomplish this, the authors chose to use a substantial amount of Template Haskell quasiquoting to create DSLs for defining templates, routes, data model, etc. Snap chose not to go quite so far with TH and type safety in an attempt to avoid some of the perceived difficulty that is frequently associated with TH. Snap and Happstack have very similar web server APIs, but in at least a couple notable cases Snap trades off some power and generalization in favor of simpler interfaces. High test coverage and good documentation have also been other things we've focused on with Snap. Snap also uses a newer iteratee-based web server while Happstack uses lazy IO (although I believe they are planning to change that sometime in the future). I'll again reiterate what I said in the other post that the distinctions I've mentioned here are not absolute. If you wanted to use type safe URLs with Snap or Happstack you could certainly do so. In fact, the web-routes package that I believe Yesod uses for it's type safe routing was originally developed for Happstack. 
But... a monad is just a monoid in a category of endofunctors! Yeah, okay, got me there, really not sure what he's getting at. Oh well, I tried. 
Very nice, this represents a lot of work. It is a valuable contribution to our ecosystem. Thanks!
 evolve n pool = last . take n $ iterate (&gt;&gt;= compete) (return pool) 
Yeah, handwritten Typeable instances need to be banned. 
This looks like folding on natural number. If there's fold function on natural number, I prefer use it.
It's easier just to import `Unsafe.Coerce`.
Interesting choice, since a command-line to-do list manager is a standard example in a popular Haskell textbook.
Thanks! Unfortunately, I can't test the changes since I upgraded to Mac OS X 10.7 Lion. $ make ghc --make -O2 -fforce-recomp hellogenetics.hs -package haskell98 -package random-extras -package random-fu [1 of 2] Compiling Genetics ( Genetics.hs, Genetics.o ) [2 of 2] Compiling Main ( hellogenetics.hs, hellogenetics.o ) Linking hellogenetics ... ld: warning: could not create compact unwind for _ffi_call_unix64: does not use RBP or RSP based frame ld: warning: PIE disabled. Absolute addressing (perhaps -mdynamic-no-pic) not allowed in code signed PIE, but used in ___gmpn_modexact_1c_odd from /Library/Frameworks/GHC.framework/Versions/7.0.3-x86_64/usr/lib/ghc-7.0.3/integer-gmp-0.2.0.3/libHSinteger-gmp-0.2.0.3.a(mode1o.o). To fix this warning, don't compile with -mdynamic-no-pic or link with -Wl,-no_pie
Really? Which one? Not RWH right? I have read that book and I do not remember a todo list application.
Check out todotxt for some other potential ideas.
*Learn You a Haskell for Great Good.* [Here](http://learnyouahaskell.com/input-and-output) is the section containing the example program.
Yes, but Unsafe.Coerce is helpfully labelled as an unsafe operation whereas Dynamic gave me more of an impression of safety. 
 1. Yes I do know about hlint and I was supposed to have run it before I made this reddit post but I forgot to after I had made some minor changes. I should make it a default git pre-commit hook to prevent myself from committing bad code. Thanks for the reminder. 1. It actually already does this just type in "htodo -u". I think that in the future there will be a configuration file with whichever you want the default to be. That way you can specify what htodo init does as its default action. I think that there are valid cases for it to do either so I will leave it at the users discretion. 1. That is an excellent idea. Done and committed! 1. No it is not deleted from the database, the value of a state flag merely changes to the "done" state. I don't like deleting items that people may want later. However there are no filters implemented yet, so my preferred method of showing done items is not there yet, however, I just committed some new code and you can type in "htodo --done" or the longer "htodo show --done" and it will show the done items too. I realised that you should still be able to see them but as it stands the inability to properly show the items that you want is a bug. Thanks for the great response and just for having a look at what I have done!
That is cool, I will have a look at what they are doing and maybe get some good inspiration for future direction.
Oh good stuff, does the example ever get more complicated though? It is still an odd co-incidence, since I have not read LYAH yet.
It doesn't seem like it would be terribly hard, either module Dynamic(Typeable(),typeOf, ...) where class Typeable a where unsafeTypeOf :: a -&gt; TypeRep typeOf = unsafeTypeOf
How about expressing it as repeated kleisli composition? evolve n = iterate (compete &gt;=&gt;) return !! n
 evolve n pool = foldM (const . compete) pool [1..n]
Those all seem to be warnings. Are you sure it didn't actually create your binary?
I asked about this [a month ago](http://www.reddit.com/r/haskell/comments/jaa8t/eta_on_the_haskell_platform_2011400_release/). The response was that they were waiting for GHC 7.2.1, but that's been out for a while now. I don't mind the delay, but an update on when we can expect it would be nice.
interesting solution...would you care to explain it since programming with arrows is probably less familiar for some folks (including me)
So where is the use site?
yep, and an ubuntu package would ultra nice.
Can someone explain to me the construction of the tensor product (and monoid expoential) for the Eilenberg-Moore category of a commutative monads is? It should be an algebra over the coequalizer of something.
Just like plain functions are arrows, functions of type `Monad m =&gt; a -&gt; m b` are also arrows, known as kleisli arrows, although I didn't really use their "arrowness" in my example. The composition operator for plain functions is `(.) :: (b -&gt; c) -&gt; (a -&gt; b) -&gt; (a -&gt; c)`. Similarly, kleisli arrows can be composed using `(&lt;=&lt;) :: Monad m =&gt; (b -&gt; m c) -&gt; (a -&gt; m b) -&gt; (a -&gt; m c)`. In my example, I used the flipped version of this operator, which is simply (&gt;=&gt;) :: Monad m =&gt; (a -&gt; m b) -&gt; (b -&gt; m c) -&gt; (a -&gt; m c) f &gt;=&gt; g = \x -&gt; f x &gt;&gt;= g So it's a lot like composing plain functions, only with the side effects of a monad. In my solution here, i repeatedly compose the `compete` function with itself, so you get compete &gt;=&gt; compete &gt;=&gt; ... &gt;=&gt; return which, by the definition of `(&gt;=&gt;)`, is just \x0 -&gt; compete x0 &gt;&gt;= \x1 -&gt; compete x1 &gt;&gt;= ... &gt;&gt;= \xn -&gt; return xn which can be simplified to \x0 -&gt; compete x0 &gt;&gt;= compete &gt;&gt;= ... &gt;&gt;= return
Something more point-free: evolve n = (!! n) . iterate (&gt;&gt;= compete) . return 
I think you have an off-by-one error here. It should be `last . take (n+1)`, or just `(!! n)`.
I don't think we should wait for 7.2 as it's a preview release. We should release with 7.0.4 and then make the next release with 7.4. We should try to get back to a regular schedule.
I don't want to discourage you, I too wanted to do a todo list in haskell :) But, shouldn't we be using a scripting language to do this instead of a heavy language like Haskell (using the right tool for the right job :) ). It has 1430 (cat $(git ls-files | grep -vE 'READ|LICE|.gitignore') | wc -l) lines to implement a todo list which can be done very easily using a *bash* script, in far fewer lines of code: I have been using a bash script to maintain list of todos and syncing it with Dropbox. You can check it out here: https://gist.github.com/1226371 . Also, I've the following line in my xmobarrc, so I know which task I am working on :) , Run Com "/home/minhajuddin/.scripts/s" ["peek"] "slotter" 600 Hope someone finds this helpful P.S: I know your main motivations was to learn Haskell, but, if we learn it by doing something which we shouldn't be using it for, I don't think that's a good learning experience.
And hsgtd. It's unmaintained, though.
Is GHC still able to derive instances if you do that?
Consider Sequence[G]. Sequence[G] is also a representation of G*. If we restrict ourselves to finite strings, then Sequence[G] is iso to List[G], but we wouldn't say that Sequence and List are the same, and it wouldn't make sense to say that Sequence[G] is a model of List[G] or the converse.
I actually think that's precisely where he's going to go. Except G* isn't "real" in any special sense -- its a theory -- the representation of it is the concrete real thing we're going to work with. But we can specify a relationship between the theory and a given set that models that theory... So not all monoids are monads, but all monads are iso to particular monoids.
Didn't think about that. Maybe deriving Typeable could become more magic?
If you're not too annoyed at me from the previous argument, I'm curious what your thoughts were on the "abstract recursion scheme" style of approach suggested in the comments on [the related post you made a while back](http://www.reddit.com/r/haskell/comments/cs54i/how_would_you_write_du_in_haskell/), by which I mean either [winterkoninkje's deep magic](http://www.reddit.com/r/haskell/comments/cs54i/how_would_you_write_du_in_haskell/c0uvqqo) or more specialized forms with similar structure, such as [jmillikin's revised example](http://www.reddit.com/r/haskell/comments/cs54i/how_would_you_write_du_in_haskell/c0uuhlw), [blackh's version based on ListT](http://www.reddit.com/r/haskell/comments/cs54i/how_would_you_write_du_in_haskell/c0uueh7), or even [frud's condensed example with specialized functions inlined](http://www.reddit.com/r/haskell/comments/cs54i/how_would_you_write_du_in_haskell/c0uu85w). Something on the general spectrum of those approaches (avoiding direct recursion, handing off single-step IO functions to a generic recursive driver) seems to be the closest to what you wanted, but most of your replies were roughly "oh, that does look interesting, I'll have to think about this". I was rather disappointed at the time that you never elaborated further.
The fold on Nat is not explicitly defined in Haskell (after all, Nat isn't either) but `fold f c n` can be written `iterate f c !! n`.
&gt; In Haskell98, all missing instances are found immediately at the use site. Isn't that exactly what's happening here? You have no use site of `f` and therefore no instance is searched for. EDIT: oh, I see what you're saying: `(+)` is a use site of `Num`. This still seems fine to me: `f :: Num a =&gt; a` means "if `a` is a `Num` then `f` is an `a`" and `f :: Num String =&gt; String` means "if `String` is a `Num` then `f` is a `String`"
Line 107, your problem with `putStr` is that stdout is line-buffered by default. `import System.IO` at the top, and do a `hFlush stdout` after the `putStr` line.
So you're saying the author views "G\*" as the abstract idea of a sequence without any idea of its structure, and List[G] (and Sequence[G]) as implementations as a particular algebraic data type? That makes sense then. It's an unusual notation for me, as I mostly see G\* used as a synonym for List[G]. It's also still unclear how that's viewed as the exact same relationship one finds between a monoid and a monad.
G\* and [G] are not the same thing because G\* includes expressions with all associations of the free monoid operator, whereas [G] only includes the right-associated normal forms. This is not just a technical quibble because when we're talking about G\* we assume that certain things which are possible are in fact easy; for example, concatenation. However, we cannot directly concatenate two lists, we must copy one onto the front of the other. And yet, if we chose a different representation of the free monoid, like Data.Sequence, then concatenation would become easy/uniform. Just because there's an isomorphism between G\* and [G] does not mean they are the same thing. The type of expressions for Boolean algebras and the type of CNF expressions for Boolean algebras aren't the same thing for much the same reason. We don't go around saying of arbitrary Boolean expressions "that's CNF" just because it's possible to convert it into something which is in fact CNF. The state of being in CNF means that there are certain properties which hold and which affect other issues like time complexity. Just because we can convert all formulae into normal form does not mean those properties hold of the original formula.
IIRC, 7.4 is expected in October. It would be unusual to release the platform in late September with 7.0.4 and then wait another 6 months for a major release.
You don't need to know Java to use Clojure. According to [this SO answer](http://stackoverflow.com/questions/2609571/learning-clojure-what-should-i-know-about-java-and-more/2609646#2609646), only `java.lang` and `java.io` are really necessary to know. On the bonus side you _may_ easily use whatever Java library if you need it. For more see [Learning Clojure, what should I know about Java and more](http://stackoverflow.com/q/2609571/25450).
So by G\* you mean a type data Star g = Id | Gen g | Times !(Star g) !(Star g) and an instance instance Eq g =&gt; Eq (Star g) where a == b = toList a == toList b where toList Id = [] toList (Gen g) = [g] toList (Times a b) = toList a ++ toList b ? I think I preferred slcv's answer, which implied that G\* just means the abstract idea of a finite sequence, without any particular data structure in mind. Then it makes no sense at all to say "when we're talking about G\* we assume that certain things which are possible are in fact easy", because I don't know what "easy" means for operations on an abstract idea.
Not only that, but the "monoids" in which a monad is one in a category of endofunctors are a much, much more general idea than the monoids he's introduced to this point. That is, monoids (as objects in a monoidal category) are not the same concept as monoids (as algebraic structures on a set). Of course the latter is an *example* of the former.
I think the language being used here is roughly that which comes from model theory, but in a sort of not precise way: http://plato.stanford.edu/entries/model-theory/ This style of description can get confusing, especially when we're not being super formal with it, because what is being modeled and what is being interpreted tend to get confounded.
"Num String =&gt;" makes sense in a library compilation. But in whole program compilation, it could have been made a type error...
I remember that discussion! I thought that winterkoninkje's solution was one of a few that were fairly performant, while also being pretty concise. I don't seem to recall any discussion regarding them having bad memory profiles, at least. In any case, the main (only) issue seemed to be not anything about Haskell per-se, but the lack of a low level primitive to step through a directory's file listing rather than reading it in at once-ish.
On the other hand, monads are a very specific example of that general idea. In particular, if you take a one-object subcategory of Cat, retaining appropriate morphisms, wouldn't a monad coincide with the standard category theory notion of a monoid as a category with one object? Modulo any tedious boilerplate required to get the definitions to line up, since I don't recall the specifics.
"Bounded" memory is a requirement. I don't mean "worst-case O(1)", but rather: - worst-case O(n), since the worst case is a filesystem that looks like a linked list - in practice O(log n), since real filesystems look more like trees - in practice O(1), since operating systems no longer bother with piddling memory allocations and the first one will be enough to hold the longest path you encounter, even on seriously big filesystems. By contrast, a naive Haskell implementation tends to be O(n) or worse. Run your program over a filesystem that contains millions of files. If its memory usage grows steadily until it fills up your RAM and start swapping, it's wrong. Compare the memory profile against find / -printf "%s %p\n" | perl -lpe '$t += $1 if s/^(\d+)\s+//; END { print $t }' 
Ahh, thanks.
[GHC 7.4 will enter feature freeze at the end of October,](http://www.haskell.org/pipermail/cvs-ghc/2011-September/065590.html) so we can probably expect the final release around December.
Personally i think type checking URLs and HTML is fantastic, just don't like how it's done, thru TH... I would prefer much more other a specialized small parser/compiler outside GHC doing the job... IMO TH is a big hindrance.
not possible, they're were all taken at [django/python conf](http://blog.leahculver.com/2009/03/good-times-at-pycon-2009.html).
If `G*` contains distinct elements for `g1(g2g3)` and `(g1g2)g3`, then it is not the free monoid over `G` (not a monoid at all), which is what he defines `G*` to be.
There are loads of places Robert could take a todo application if he's interested in fleshing it out, for example: * a curses or graphical ui so you can review items more easily * integration with web todo applications like ToodleDo (this is potentially handy for smartphone support) * integration with your mail (imagine teaching your mailer to link a todo item with a mail so that when you look at one you see the other) * integration with your calendar So lightweight may not be the way to go. 
What you mean is what I described: my code is intended to use O(maximal path length) memory. Note that solely for simplicity, I have opted to use a tad more than that, namely O(maximal size of FIFO queue for depth-first search). The difference is minuscule except for pathological cases, and you'd have to dig deep into the documentation for the `find` utility to figure out whether it has the former or the latter bound. You may want to test the code yourself; I for one am confident in my ability to write leak-free programs in Haskell. Not sure where the notion comes from that it might be hard or impossible. 
Actually, I think you'll find many of the Scala community disagree with him because he tends to be quite dishonest (and it trickles through in this article) which has pissed quite a few people off. Maybe things have changed, but some people hold a grudge as it turns out (I hear about it occasionally). Many have also recently become disillusioned to "Lift". My personal, and usually private (I am compelled by your genuine comments), opinion that he is under-qualified to discuss the topic at hand and I choked on the introduction of the reassurance to readers of his credentials. &gt; I have written more lines of Scala code (more than 250K) than almost anyone on the planet. Er no, sorry. I could list my credentials too, and it might even be persuasive for some, but I don't think all that matters. It would look dicky at best right? The best I can say about this article, being intimate with the background, is that it is well-intentioned. PS: Why is this in our haskell?
&gt; It has 1430 (cat $(git ls-files | grep -vE 'READ|LICE|.gitignore') | wc -l) lines to implement a todo list ...of which nearly a third are import statements, blank lines, type signatures, and various other trivial things that harmlessly inflate linecount, and another couple hundred are UI niceties (helpful messages, error handling, &amp;c.) that your bash script lacks. Using SQLite instead of plain files adds another couple hundred lines as well, between the schema files and DB code. So that's really a *very* misleading line count. It's also written in a style that isn't particularly compact, which has the benefit of being more approachable, but it would be a good bit shorter written in the terse style that I prefer.
Why not have `Unsafe.Typeable` with the class and `unsafeTypeOf`, then have `Data.Typeable` export `Unsafe.Typeable.Typeable` but not `unsafeTypeOf`? `deriving Typeable` would then refer to `Unsafe.Typeable.Typeable` without importing it (not sure if GHC can do this?). That way, it's impossible to write a program that "goes wrong" in this sense without someone importing a module that starts with "Unsafe".
Please give an example implementation of `evolve` that uses foldl. As I said, it looks like it should be foldl, but I have no idea how to implement it as such.
It should not be implemented using foldl. Left-nesting of (&gt;&gt;=) is usually bad (by which I mean inefficient). 
One also wonders if help is needed with something in particular. Do they have a public bug tracker?
[Here you go.](http://hackage.haskell.org/package/pony)
Made me laugh
No, I'm also meaning the abstract type of finite sequences. But, when dealing with abstract sequences like these we *do* often assume that certain things are easy. Often times we assume that we have access to the nondeterministic inverse of concatenation. We also often assume we have snoc, and its inverse, because it's "just a sequence" so of course you can add/remove things from either end. But if our thinking about sequences assumes any of these things, then we're not thinking about [G]. For abstract ideas I'd say that "easy" has to do with how we assume we can manipulate those ideas. When writing out functions or proofs in mathematics we often make certain assumptions in order to simplify the structure of the proof. But, if we want to port that into programs, we have to either have a concrete presentation which has easy operations to match the ones of the abstract theory; or else we have to restructure the proof in order to match the presentation--- in which case I would argue that the two are not, indeed, the same proof.
[Not quite the same connotation](http://i.imgur.com/b1wHd.png)
No. No. No. No. No.
I meant to a while back. So I tried to cabal install it then cabal-dev on a clone of the current git repo. Cabal and Cabal-dev on the current git clone give an error. Error at [hpaste](http://hpaste.org/51549)
&gt; PS: Why is this in our haskell? I wanted to launch the debate on whether or not Haskell is an inappropriate language for some programmers. After 10 years of Java, I find it hard to believe that the Java programmer mass will easily move to something else. For Scala, I'm not sure. But I fear the Haskell is out-of-reach for most of us. It takes too long to assimilate all FP notions, a new language, a new toolset (ghc, hackage, cabal, etc), purity, non-strictness, monads, arrows, iteratees, etc. If you've been doing Haskell since 1998, you saw all this coming progressively and maybe you don't realize how long it can be for a Java programmer to get where you are. There's a limit to what you can learn on your free time when you have 2 kids and a full-time job. If universities would use Haskell as a learning tool, things could eventually change.
Manually-written Typeable instances are unsafe.. And a `data` around an Int is not the same representation as an Int (though a `newtype` is).
&gt;Yeah, handwritten Typeable instances need to be banned. The thing is, if you want to write Haskell2010 standard-compliant code rather than using a GHC extension then at the moment you have no other choice.
Still, do you think Haskell is a better choice for writing this kind of app?
I fear that 7.0.4 is going to be a non-starter on Mac OS X 10.7 (Lion). I've tried just about everything, and consistent with several others who have tried, it just won't work and isn't buildable. Note that the 7.0.2 and the current HP seem to work fine. We could do a 7.0.4 based HP for Mac for 10.6 (Snow Leopard) and earlier, but would be a bit bizarre to not support 10.7. Is 7.0.4 all the much different than 7.0.2? Perhaps we shouldn't bother with an HP update for now.
Why does this existttttttt?!!! I would have gone more towards [another package...](http://hackage.haskell.org/package/on-a-horse-0.1) 
It's not right solution. GHC cannot derive Typeable instance for data type, if it have parameters with kind other than *.
I will bet $100,000,000 any time on my own imagination, and Haskell is currently the best tool to translate my imagination into code.
Yes.
I think a direct link to the article would be practical: http://prog21.dadgum.com/13.html I think the tool-chain etc of Haskell is mature enough for the kind of stuff he mentioned, yes. A lot of the article is about optimization.
Does it have a hard realtime requirement? No, I would not. Otherwise, probably, yeah. And I'm not even that good at Haskell!
Why would you not use Haskell if you had realtime requirements? Realtime systems are not really about speed, but about being able to reason about the system well enough to prove that certain performance guarantees can be met.
I feel that this point sums it up: &gt;If you had $100,000,000 none of these are an issue, as you can easily afford to hire top developers to address any and all of them. At it's core Haskell is a wonderful language, but the ecosystem could use some love. Being able to employ quality developers to work full time on tools like cabal would go a long way. And more resources (such as technical writers) thrown at key libs would be fantastic.
http://hackage.haskell.org/package/atom guarantees deterministic execution time and constant memory C-code.
Your code is fine. Yes, you could rewrite it to use `foldM`. But please, *please* don't use any of the fancier suggested versions. If I didn't knew what they were supposed to do it would take me way longer to understand what's going on.
Seems like it might be hard to reason about some of the stuff the runtime does for you. In most code all you need to know is that it's "fast enough", but as you say hard realtime isn't about speed so much as absolute predictability.
I think the budget should not be taken literately, it's more like "would you choose X when you agree to finish a project which is very very important and must not fail or be late".
There's no arrows at work here; well, Kleisli arrows, but those aren't related to the `Control.Arrow.Arrow` typeclass (although they are an instance).
I reported this a few hours ago; a fixed version is [on Hackage already](http://hackage.haskell.org/package/iterIO).
For a quick hack, familiarity with the language matters more than anything else. For an application intended to have ongoing development and more features, Haskell works at least as well as anything else. Is Haskell the best choice for the application as-is? Maybe, maybe not. But I'd certainly rather have a clean, structured Haskell codebase to build on for further development than a pile of ad-hoc scripts.
I understood it as something similar to that, but with the caveat that you are backed by boundless resources. I think this is important as this prevents "brick walls" of, say, a compiler bug or a missing library - you can just contract those problems away.