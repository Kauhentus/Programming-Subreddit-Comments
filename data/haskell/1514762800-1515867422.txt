I would reconsider whether having the build self contained within a Haskell build process is worth the effort; whether such a setup will evolve well as cabal changes with new versions. It might be easier to distribute through Github, treating the project as multi-language project with relatively independent Go and Haskell build steps, using shell or makefile glue at the top.... If you express everything as a cabal custom build, very few potential users will know how to modify the build quickly. If you use shell scripts or makefiles with common build setups for Go and Haskell, more people can read and contribute instantly.
Very useful answer thank you. If Phantom isn't maintained, could it mean that some JS isn't interpreted correctly? 
That alone makes me glad Cardano is a thing. I'm hoping a bunch of the Haskellers working on Cardano invest some of their profits and learnings into the Haskell community. :)
We use [`webdriver`](https://hackage.haskell.org/package/webdriver) at work. It works very nicely.
You could try the popular [Purely Functional Data Structures](https://www.cs.cmu.edu/~rwh/theses/okasaki.pdf) paper by Chris Okasaki.
Thanks for the advice, but this makes me recall the days when I must install a huge .NET framework on Windows just for a small app of a few MBs....so no, I'm not going the other way around!
thank you very much, just made it through the first chapter, this looks like a great resource :)
Haskell's built-in list type is actually a singly linked list. Remember that functional data structures have to be immutable, so when inserting, updating or deleting an element, you have to create (almost entirely) new copy. Inserting at the head of a linked list is pretty easy, just create a new "list node" and have the "next node" be the old head. If you want to insert in the middle of the list you have to traverse the list until you get to where you want to insert. And because we can't modify the existing list, you have to create a copy of each node as you go, until you get to the insert point. Try to figure out some of the functions for the build-in list works (map, foldl, foldr, filter) before you build your own.
A little bit of advertising: if anyone is seeking to drive `puppeteer` from Haskell, take a look at [`nodejs-interop`](http://github.com/TerrorJack/nodejs-interop) and see if it helps for your use case :)
About as dangerous as a bunch of tensors.
Thanks, will take a look at it !
I actually found your repo when I started reading the book. Unfortunately, the book I was given was the first edition, originally made to be used in conjuction with Miranda, and it is the one I'm solving in my repo.
Thank you and a happy new year!!!
Facebook's Sigm is another one :) Seller Lab's Feedback Genius is another, but that's much less known :)
I wish you the same!
 `Client`/`Server` type wrappers that cover lots of handy IO actions but cannot be sent over the network to the wrong party. Auth&amp;Auth at the type level. I'm leaving out things I think are obvious like distinguishing encrypted data from not. This one's tough, but a client security combinator set to allow only authorised parties to be sent authorised data.
I could not find a proper CCM mode for AES that worked well on hackage. I couldn't really find any good authenticated encryption.
Pure cipher block chaining.
[removed]
Haskell beginner here! How did you find the book? I have been following the cis194 lectures and would love to have another perspective.
So I would suggest defining them the way Haskell itself defines them: data List a = Nil | Cons a (List a) So now it is trivial to add an element to the front (`Cons :: a -&gt; List a -&gt; List a` does that), and removing an element from the front is also trivial by pattern matching on `Cons`. The specific operations you want, inserting, updating, and deleting, are generally not the operations you want to be doing on a linked list, what with them all being `O(n)`. But I will show you `insert` anyway so you can get the general idea of what an operation on a linked list might look like: insert :: Int -&gt; a -&gt; List a -&gt; List a insert 0 v xs = Cons v xs insert n v (Cons x xs) = Cons x (insert (n - 1) v xs) insert _ _ _ = error "Invalid index" It's also worth noting that the build in lists in Haskell `[a]`, are in fact linked lists.
Prove your implementation's correctness?
Sodium doesn't have ccm. The main AES implementations do include gcm for AE.
I'm not aware of any fast sorting algorithm already implemented in Accelerate that you could use (at least, that radix sort example I wrote is just a toy), so you are probably better off finding something faster in C and using that directly. (my plan was always to write FFI bindings to use such implementations directly from Accelerate, but I have yet to find the time for that)
I never saw your earlier post... sorry you ran into so many problems with `Repa` and `Accelerate` \\:
Thank you so much for your clarification! Maybe this is something I could contribute. I'm doing a PhD currently more towards the end. If you have any pointer where to look for FFI bindings (perhaps a C++ vector library?) this would be something to think about.
There's [this talk by Edwin Brady about modeling client/server communication in the type system](https://www.youtube.com/watch?v=IQO9N0Y8tcM). Of course it's in Idris and uses dependent types, but still, it's a good attack at the core problem. There should be plenty of room to expand from where he leaves off.
There's also a book, but I'm not sure how much more it has than the paper. 
It is probably worth pointing out that cardano currently has the 5th largest cryptocurrency market cap, at 18.5 billion dollars - https://coinmarketcap.com/coins/ . Good work IOHK, clearly investors are digging it :)
Here's a gist with a sketch of how that would look: https://gist.github.com/kcsongor/596945b60736f21d00ccd77e6fb06728 The idea is that we can tag the type parameters by introducing a wrapper newtype P (i :: Nat) a = P a And then instead of working with the original type p a b we do the instance selection on p (P 1 a) (P 0 b) If `p a b` and `p (P 1 a) (P 0 b)` are Coercible (which in most cases they will be), we are good to go.
You should have a look at [futhark](https://futhark-lang.org). It's a pure functional language for generating gpu code. You can then run the generated code from haskell.
As for whether `Generic1` would allow me to derive more, I don't think it would, at least not with this technique. At least, It's not clear to me what it would give
The example is what I've got from google image search, you can browse the [opengameart](https://opengameart.org/) site to find more.
That said, it is just a Windows feature you enable. So it's easy to get and official.
Linked lists in imperative languages have mutations, so they wouldn't make much sense in FP since it's declarative in paradigm. The building blocks of FP algorithms are best described by Category Theory as Bartosz explains in his lectures. Categories, Functors, Monads, etc., are the structures and algorithms of FP. 
I can't speak for the first edition (although I've heard it's good), but the second edition is great as long as you keep in mind it is introduction to *functional programming*, not Haskell. What I mean is, you most likely won't come out being productive in Haskell if you already aren't, there are better books and resources for that. The book is more on the theoretical side. For example, you learn the duality theorems which relate foldr to foldl, and prove them, which is certainly interesting and arguably useful, but not something essential. The book also assumes or expects quite a bit from the reader, although the preface states "no knowledge of computers or experience writing programs is assumed". All in all, it's great as a reference for some topics, and I definitely recommend it, but probably not as a first book, and certainly not as an introduction to Haskell.
HINT: A demonstration of the *symplectic* method
As I've discovered in the past few days, you can do much of the cool stuff in Idris in Haskell if you're willing to use `singletons` everywhere. 
`ansi-terminal` is very nice and has windows support. It's low-level though, so you'd need to build on top of that.
Hi, sorry for the late reply. I downloaded the book from here: https://usi-pl.github.io/lc/sp-2015/doc/Bird_Wadler.%20Introduction%20to%20Functional%20Programming.1ed.pdf Be mindful of, as I said, that the book is from 1998 and was meant for Miranda.
[`Xterm.js`](https://xtermjs.org/) looks good to me. I'm gonna fire up a web server and open a browser window. Everyone has browsers, don't they!
So.. is there a standard module that deals with Date/Time? Which one is the standard/recommended one in the community?
Libsodium has [AES-GCM](https://download.libsodium.org/libsodium/content/secret-key_cryptography/aes-256-gcm.html) but it's tricky to use securely so it's a very low priority and would only belong as an extra in the `Unsafe` API. On the other hand, if what you care about is authenticated encryption in general, instead of the specific three-letter abbreviation, Libsodium has excellent support for authenticated encryption [with](https://download.libsodium.org/libsodium/content/secret-key_cryptography/aead.html) or [without](https://download.libsodium.org/libsodium/content/secret-key_cryptography/authenticated_encryption.html) associated data, and making this API as user-friendly and powerful as possible is definitely going to be a major priority. As an extra, I'm seriously considering making the encryption "transparent" to Haskell datatypes so that any serializeable data can be encrypted, decrypted, authenticated etc., and that ciphertexts, signatures etc. would use phantom types to remember what type of data they come from.
This is definitely something I'm interested in exploring; the core of what I've written so far already includes a lot of dependent typing for convenience, and since we have this amazing type system available it would be a shame not to use it to the fullest extent of what is possible.
I am using `time` Date.Time for my first project and it has rather detailed and scattered but OK API. I have seen `thyme` as a rather speedier alternative, but haven't had problem with `time`'s performance ... yet :)
I haven't used nix personally and can't recommend it without first hand experience. And the reason why I haven't used it because it's not supported on Mac OSX (last I checked - has that changed?)
The approach of hiding everything behind cabal has worked okay for Haskell libs needing a c library like the yaml library and most database libraries, but I am not sure how it will fair for any language ecosystem that cabal doesn't explicitly recognize. It generally feels like you are in the murky land between a Haskell package and an operating system package, but more people use stack/cabal alone rather than as tools that retrieve their dependencies from nix or their distro. Nixpkgs generally tries to be the simpler, integrated approach to building an operating system package, but it is limited by which languages have active communities within it. The ideal for what you are building would probably be two nix packages. One package that bundles the go lib, and one haskell package that referenced the go package as a dependency. But it would be annoying having to figure out doing this from scratch as the first project. I wonder if there any haskell packages outside of hackage that people have already packaged for nix, which needed a go dependency. Alternatively, anything in nixpkgs that involves a second language using a go lib. 
Yeah my post didn't get that much attention at the time and some people even downvoted. To be fair, the topic was rather unconventional and the gist I wrote was freakishly long. My gripe was more with `Repa` back then, and when I retried with `Accelerate` and got runtime errors about unsupported typeclass instances I kind of just flipped the table and decided I was done with this topic for now. Now that you have chimed in I would of course be interested in your opinion, but sadly the post is already archived.
Perhaps it's fine to keep the last instance, and make it `OVERLAPPABLE`. Concrete instances will presumably look like this, where `a1`, `a2` are like `s` in `State s` (not covariant variables). instance ... =&gt; NFunctor (MyCon a1 a2 ...) where ... And since `MyCon a1 a2 ...` is more specific than `f a`, that instance will be picked over the overlappable `NFunctor (f a)`. So that is fine as long as we work with concrete functors. Functor-polymorphic functions are also quite common. In those cases, `NFunctor f` does not subsume `NFunctor (f a)` (at least, not according to GHC's rules for overlapping instances whenever there is some other instance more specific than `f a`). But we may still carry on as if that instance did not exist. In terms of implementing functor-polymorphic functions, the situation is actually not worse than `Functor` vs `Bifunctor`, whereas in terms of using them, `NFunctors` are still more general, code that is polymorphic over n-functors will work with any m-functor for `m ≥ n`.
&gt; Small nitpick that I found: in your first proof by induction, you never explicitly mentioned that it was a proof by induction on the range i..j. Excellent point, which nicely illustrates why proof "by induction (on X)" is a subtle notion. The point I'm trying to emphasize here though is that the thing the _proof_ is "inducting on" is just the thing that ensures that the _program_ "terminates", namely the metric `j - i`. Will update, thanks!
Uhh... Not quite sure I follow. Category theory is really good at giving a name to similarities between different structures and thus tends to be a good inspiration for abstractions (functor, monad, etc) but data structures themselves don't come from category theory. Linked lists are linked lists. They happen to commonly be implemented in imperative languages using mutation internally but often (especially in oop APIs) they tend to be thought of as closer to immutable with "atomic operations". In functional languages linked lists tend to be implemented in a declarative way as befitting their inherently recursive structure. Specifically, using a sum type and with operations defined using pattern matching. There's not really anything about a declarative paradigm that would make it less conductive to "data structures" and more conductive to "category theory"; there's no real correlation there. I suppose some of this fake tension might come about because Haskell tends to mix a lot of things together. It is a functional language, a strongly typed language with almost dependently typed strength, and a language whose ecosystem takes copious (although very loose and vague) inspiration from category theory when designing libraries. If one is interested in functional programming and it's algorithms and data structures, the books "purely functional data structures" and "pearls of functional algorithm design" are must reads. If one is interested at understanding why, if you squint really hard, you might see why someone would want to call our monoid typeclass a "monoid", Bartosz's book is a solid introduction.
An alternative response, running with your objective of hackage/stackage distributable. I read a bit about postgresql-libpq which is has to deal with a somewhat related issue for a large user base. It uses a cabal custom build, but that essentially calls out to some globally installed executable (pg-config or a more generalized version). The executable is in charge of providing the paths needed when building. Essentially, someone has to install postgres first, which will also install pg-config. In your case, you could require that users install the go library globally first as well as a shell script your require. Your shell shell script can act similarly to pg-config, providing paths needed to cabal custom build at build time. (Try reading through the commit history on postgresql-libpq's setup.hs, to see how it evolved) For postgresql-libpq, it appears as though both stackage and hackage CI build servers install libpq globally. I don't think hackage and stackage should rush to install the go dependency and compiler to their CI servers, until you have a wider use base. To play nicely there, I would provide a cabal flag to enable a build mode that either use cpp to conditionally stub out your external calls or forces the cabal custom build to use a mock c library. If you add a test-suite, I would have the test-suite only exercise pure Haskell functions that don't use the API directly or use it mocked.
I know nothing about cryptography and blockchain, but I know [this article](http://www.michaelburge.us/2017/08/17/rolling-your-own-blockchain.html) exists which might be helpful.
Preventing user-provided nonces has both pros and cons. If users can't provide the nonce then you are preventing many possible uses of the library - making a strictly weaker library but tailored to be easy and/or safe for particular uses. Such an API could even be built on top of more powerful but lower level Sodium binding. I'm not sure I agree with the notion that some crytptographic primitives should be placed under an `Unsafe` module hierarchy (reading issue #40). It strikes me as questionable to generalize about primitives and declare some set of them "usually safe" while others are "harder to use safely". I'd rather see people who need a safe primitive not talk about ciphers and hashes at all but instead about a secure communication channel and/or key exchange, which could be built in many ways in part with the tooling discussed here. So to recap a few of my views: * Some goals you identify are orthogonal to my needs and desires from bindings to a cryptographic library. * The community could use bindings, or improvements to existing works, with more rigor and more review. * Libraries with higher level functionalities, which could be built on a lower level binding, could provide a safer interface for developers. For many possible needs a suitable library does not currently exist on Hackage.
I sometimes write [here](https://medium.com/@danidiaz) but not that often. By the way, I put together [a repo](https://github.com/danidiaz/lens-aeson-examples) with some more `lens-aeson` examples. Perhaps the easiest way of reading them is by compiling and generating the haddocs.
&gt; or in a monadic way in Haskell You don't want to do that. Monadic parsers are more expressive than EBNF. For example, you can express that an XML tag consists of a `&lt;`, a tag name, a `&gt;`, some nested tags, a `&lt;`, a `/`, that _same_ tag, and a `&gt;`; whereas in EBNF you'd have to state that the closing tag accepts any tag name, and then you'd have to check that the opening and closing tags are the same in a post-processing step, after the parsing is done. But this extra power comes at a cost. Sequential composition says that given a parser for values of type `a` and a parser for values of type `b`, I can parse an `(a, b)` pair by first parsing the `a`, and then parsing the `b`. Monadic composition says that given a parser for values of type `a` and a way to choose a parser of type `b` based on the `a` value parsed by the first parser, we can parse a `b` by first parsing the `a`, and then running the chosen `b` parser. For example, we can write a parser which accepts an `a` followed by any alphanumeric string, and then based on that string we can construct a parser for the rest of the XML node, including a part which only accepts that same alphanumeric string. The cost is that the grammar is opaque: we have no way to know what the language accepted by the second parser will look like until we give it the `a` parsed by the first parser. That's fine if we are running the parser on a particular input, because we'll successfully parse the `a` and then we'll have an `a` with which to choose the second parser, but when generating code, you don't have an `a` yet, you have code which will parse an `a` at runtime, and so you can't convert the `a -&gt; Parser b` into a form you can examine and turn into the rest of the generated code. So for your project, it makes a lot more sense to choose a more limited grammar DSL such as antlr's, or one of the Haskell parser combinator libraries which produce a concrete grammar instead of a parser, as discussed in [a recent post](https://www.reddit.com/r/haskell/comments/7mnbya/parsing_combinators_representation/). One you have a concrete grammar, you can generate the corresponding code in the language of your choice. If those languages support any kind of parser combinator EDSL, including a monadic one, generating that code should be quite straightforward, since those EDSLs are designed to look like grammars. If you want to target a language which does _not_ have such a library, then... I advise writing one, otherwise your task is going to be a lot more difficult. Tools like antlr perform a lot of transformations on that grammar in order to turn it into code, so if antlr or a similar tool supports all the languages you need, great; if not, adding an extra target language is probably going to be a lot of work. That being said, I think the reason antlr's generated code looks so different from the original grammar is because it turns the grammar into an NFA and then optimizes it, so if parsing speed is not a concern, you might be able to get away with generating a simple recursive-descent parser.
It's *not* "just a Windows feature". Sure, the kernel of the system is tiny, and yes, it's easy to get, but that's just setup. The actual system is an entire Linux distribution (using unmodified distribution files is one of the selling points of WSL!). [Some research shows that using Ubuntu costs 600MB](https://superuser.com/questions/1201269/what-size-does-basic-bash-wsl-subsystem-on-windows-occupy). You can probably make a significant reduction by using a smaller distribution, but that's still a lot of hassle.
With WSL, wasn't there a large amount of work put in to make them more similar? I can't imagine trying to hack glue code capable of smoothing that over into the kernel.
[members](https://hackage.haskell.org/package/lens-aeson-1.0.2/docs/Data-Aeson-Lens.html#v:members) is an indexed traversal, that means you can compose it with ['withIndex'](https://hackage.haskell.org/package/lens-4.15.4/docs/Control-Lens-Internal-Indexed.html#v:withIndex) and visit pairs `(type,entry for the type)`. Would that be a start? For example, `members.withIndex.alongside id (key "code"._String)` should get you all the type/code combinations. (I haven't tested it though.)
I tried that also, but I think the best idea is to put the `withIndex` at the end, and preserve the index information until them by using [(.&lt;)](https://hackage.haskell.org/package/lens-4.15.4/docs/Control-Lens-Indexed.html#v:-60-.). For example, this should return all the (type,code) combinations: v^..key "result".(members&lt;.values&lt;.key "code").withIndex
I'm not sure what the current best is, but I would start with boost or even just the standard C++ sort; I assume they would be pretty good. Different implementations might perform better for your particular use case though. If you do some experimentation/benchmarking of different implementations, I'd be interested in what you find.
i had a hard time getting used to the notation, but i've figured it out thanks for all your guys help. i mean push and pop i guess on this type of stack that haskell generates. the (a list) item, that is used to like contain the rest of the stack was one of the key ideas i couldn't see how to implement. i thought that immutability meant that i couldn't like update the structure once it is defined, and thought that would mean when i pushed something into it i would need to like define how to make the new object. as for the operations on the linked list i meant push not insert and either delete (take the first or last elements). my language was sloppy.
The accelerate error at least is a consequence of me trying to adhere to the standard Haskell type class hierarchy. This is occasionally convenient to have (e.g. your use of `P.sum` over a list of embedded language terms is possible because we overload the standard `Num` typeclass) but also not quite malleable enough to be completely overloaded (e.g. `fromEnum :: a -&gt; Int`, but we really need an `Exp Int` here because this should be an embedded language int, not a real int, so we can't really implement `Enum`, but then `Integral` depends on it, so...). I'm becoming increasingly convinced that I should give up this metaprogramming convenience to avoid errors such as the one you ran into. Your initial question, how to move smoothly between the "thinking in loop nests" vs. "thinking in collective operations" style, is an interesting one. I think I just kind of got used to thinking in this different style and forgot that it was part of the learning curve? I agree this could be better though...
Regarding cryptography in the more general sense, you might want to have a look at sites that build on exercises, like the [cryptopals crypto challenges](https://cryptopals.com/), for instance.
Also phantom arguments.
The product is very interesting judging by their features page, which was also really nice to navigate.
[BNFC](http://bnfc.digitalgrammars.com) let's you write a grammar and generate parsers in multiple langs, though not all the ones you mention have backends.
Cryptonite is a crypto-primitive kitchen sink. It is good to use at the lower layer but one should always have something more higher level. Issues like nounce reuse etc are not handled by cryptonite which can compromise implemented protocols.
Thanks Gershom for the update and the email alerting me to the possible impact on Stack. Copying my response from a question on the mailing list: As far as I can tell, Stack is _not_ affected by this, since—although it uses the same hackage-security library as cabal-install—it follows a different codepath outside of hackage-security for downloading tarballs. I'm not 100% certain Stack is immune, however, so if someone notices a problem, please report it.
Indeed it bit me too a few weeks ago, but I'm using ekmett's lib now.
That works, thanks! Though that's sorcery to me.
Is by any chance related to the problem that the [tar](https://hackage.haskell.org/package/tar) library still does not support the modern tar file format ("pax", or "POSIX.1-2001") that in recent years has become the standard across all platforms?
This is a mutable reddit comment. It is false that reddit comments are mutable, to allow the ability to fix mistakes that were made.
You have to keep track of state (whether the mouse is down or up) in order to decide whether to draw a line segment when a `mousemove` event happens. The FRP way to keep track of state are `Behavior`s. &gt; or just use register-callback style and get the work done? This works, so I don't see why not. :-) 
So should I use accumE/accumB? Also I thought FRP is all about using Event and Behavior instead of IO... Looks like I'm wrong :) BTW I had open a github issue on essentially this problem in threepenny, I think we can talk on there for better record tracking purpose...
So the author disagrees with custom setup scripts which could also do any IO action? Perhaps a more constructive approach would be to suggest a mechanism which allows old packages to continue building as new updates are added to hackage. It seems the intent behind this is to suggest to stop modifying package descriptions but without offering another mechanism to fix the problem. 
It's worth noting that mutability of source packages is older than Hackage revisions -- it can also be achieved by simply using the `TemplateHaskell` language extension. I've written `TemplateHaskell` code that does HTTP requests and I'm not proud of it.
Overlapping instances *in any form*, `ImpredicativeTypes` (because it's always broken), `ExplicitForAll` (because it doesn't do anything useful and can confuse people into thinking `ScopedTypeVariables` is on). I really don't like the way `DefaultSignatures` fits into the language, but it's quite useful sometimes and there's nothing else that can do what it does.
&gt; ...crypto-primitive kitchen sink. It is good to use at the lower layer... Yes, and that's also exactly what libsodium is. From the [home page](https://download.libsodium.org/doc/): &gt; ...for encryption, decryption, signatures, password hashing and more... Its goal is to provide all of the core operations needed to build higher-level cryptographic tools. So - you are building something higher level, on top of this? Are you building it on top of [saltine](https://hackage.haskell.org/package/saltine)? The package name "libsodium" is a bit confusing to me. It sounds like the name of a libsodium binding, i.e., an alternative to saltine, not something higher-level built on top of libsodium. (And the package name "saltine" is confusing for the opposite reason, but it has been around for several years now.)
I am not OP so I am not the one who is writing the binding. I am the developer of `raaz` which has its own set of hand coded libraries. All I am saying is that just saying we have `cryptonite` and therefore we do not need other crypto libraries does not make much sense. In particular, I believe there should be some experimentation given some of the possibilities namely 1. Strong types 2. Liquid Haskell 3. Dependent typed Haskell 4. Linear types (particularly for encoding session types).
I found a way to generalize this solution so it supports contravariant and invariant positions, see if you can find it before I find the time to write about my solution in my next blog post :)
I'd be interested in how to do this in Haskell, but I'm somewhat familiar with FRP using JS, so maybe it helps or not. FRP is basically working with streams. You define streams of input, and map/bind them to outputs. The IO should be at the start and at the end, with everything in between only transforming streams of IO. For a drawing app, there would be a stream for each type of user interaction, namely a left click stream, a right click stream, keydown, etc. You then define an update function that refreshes the screen when its provided with new information to render. The streams are bound to events at initialize, such that any IO triggers a pipeline of transformations on that initial input and a new frame to be rendered to the screen.
I don't think the main point is that this happens to open up yet another pathway for weird packages to mutate themselves. The point is that this emphasizes how with package revisions, even normal packages are not immutable. Those who are opposed to package revisions feel that the map from package and version number to the corresponding set of build artifacts should be a pure function. If any of the artifacts change, including the cabal file, then version number must also change. Build tools can then rely on this for creating reproducible builds. Those in favor of package revisions see the cabal file as machine-readable metadata about the package-version pair, with precise semantics. If the metadata is wrong, it should be fixed. Uploading a new version with correct metadata isn't good enough, because older versions will still be considered and used by some build tools. And if your build tool really does need immutable metadata even when wrong, you can still get that. Details [here](https://github.com/haskell-infra/hackage-trustees/blob/master/revisions-information.md). From a theoretical point of view, I lean towards the second point of view. But in practice, we have to be careful and only make changes to Hackage (both in the way it works in general, and in revisions of specific packages) in a way that all of our popular build tools continue to work reliably. So far, I think people have been pretty responsible. YMMV, but I have not yet experienced any problems due to package revisions.
I am not qualified to answer your questions, but if you need more inspiration on "Complete and Easy" I literally just submitted [my Master's Thesis](https://github.com/adamschoenemann/clofrp) on a Haskell-like language which uses "Complete and Easy" inference on a system with polymorphism and type-constructors (along with some other stuff). Its nowhere near what you're going for here, especially it doesn't have GADTs or even any coverage checking whatsoever, but you might find it useful nonetheless.
&gt; attempting to get an edit star Freakin' millenials need a star for everything ;)
This (reflex demo)[https://github.com/dc25/mouseTraceHaskell] comes to mind. Not exactly what you are asking for, but it may be instructive.
Oh this is what we're doing today? We're gonna fight?
Thanks for your lengthy answer. Seems like Antlr is really worth a shot and I can keep investigating if I am not happy with it. BNFC is also interesting, from what others have said, but I'll probably only try it once I decide I _need_ Haskell output. 
I like the insight - I've had similar, but it needs different terminology. I'd say it's not really about the "strength" with which types are enforced, but about the precision to which you've specified them. That said, I don't have a suggestion...
[removed]
Here's a concrete proposal: https://www.yesodweb.com/blog/2015/09/true-root-pvp-debate. tl;dr: external, append-only database of trusted successful build reports to guide the dependency solver.
What's wrong with the idea that the revision is a part of the version? AFAIK, packages are still immutable as long as you're specific about revisions.
I remember once working on 2D graphics in Haskell, and I kept on writing buggy code that confused the X and Y coordinates. To solve the problem I created `newtypes` for X-distance, Y-distance, Width, and Height. The wrapping and unwrapping of values was tedious at the lower level interface, but it was worth the trouble. I never had a dimensionality bug make it to runtime ever again after that.
Reminds me of dogelang. Yes, it's a real thing. http://pyos.github.io/dg/
Thanks a lot for the prompt reply. I'll keep that in mind. Where would you place the book : *Category Theory for Programmers*? Just found the book and contents section looks kinda daunting!
Thanks a lot for the link!
The proposal is * Only specify package dependencies, no package versions * Let a builder try to build a package with every possible combination of dependencies in order to determine which versions it actually works with. I don't think that this approach scales very well at all. If you have 100 dependencies and 10 versions of each you need to build 10^100 different combinations of packages in general. 
No problem, happy new year!
Seems a bit of a stretch to call it weak because it can't catch 100% of type errors at runtime. I think strong and weak are relative terms, and it helps when comparing languages, but perhaps not in the general case. And someone correct me if I'm wrong but I think the Haskell type system is a different beast compared to imperative languages that use operational semantics and OOP. In the latter, there is a distinction between classes and objects, and also native types, something that Haskell abstracts away with denotational semantics and category theory, giving the programmer several "layers" of control over types, as oppose to the more one-dimensional types of OOP, which seems more appropriate to measure using a strong vs. weak scale. 
Looks nice! How does it compare with this package? https://hackage.haskell.org/package/threads-supervisor
Out of curiosity, why did you want to decrease usage of `ResourceT` - based functions in the first place?
I don't remember proposing testing every single combination. The complexity you mention isn't specific to my proposal; _any_ solution that claims it needs to test every single combination would be susceptible to this. I'd instead recommend testing specific combinations (as I mention, Stackage Nightly is such a collection, and the matrix builder is another), and then providing that information. Tooling is free to glean from that whatever it wants. For example, if I've tested against `foo-1.0, bar-1.0` and `foo-1.1, bar-1.1`, then logical conclusions are: * `foo &gt;= 1.0 &amp;&amp; &lt; 1.2, bar &gt;= 1.0 &amp;&amp; &lt; 1.2` * Something more complicated, indicating that `foo-1.0` and `bar-1.1` are not guaranteed to work together Nonetheless, I was simply responding to the comment "but without offering another mechanism to fix the problem." This is one such mechanism that has been discussed, I don't think Taylor need mention in every place he discussed revisions that there are other proposals out there.
Michael Snoyman recommended reflex when my coworker asked him directly about this a few months. I'm currently building a frontend with [haskell-gi](https://github.com/haskell-gi/haskell-gi) (specifically [gi-gtk](https://hackage.haskell.org/package/gi-gtk-3.0.18) ) wrapped in reflex
I don't intend to call it weak, though I didn't say so explicitly. My statement that "there's no such thing as a strongly typed language" was meant to imply that there's also no such thing as a weakly typed language. Instead, each program you write will be somewhere on the strong/weak spectrum, but Haskell makes it easier to push in the strong direction on that spectrum.
That's a perfect example of what I was getting at, thank you! Getting the extra safety cost you some overhead, but it was (at least in your opinion it seems) worth it. I'm also willing to wager that Haskell's specific approach to newtypes made this technique easier to use than in other popular languages, making you more likely to do it.
I agree that there's probably a better dichotomy than strong and weak, but I'm sticking with the commonly used terms out there. Precision does sound closer to the real idea, I agree.
&gt; After all, at the OS level, a file descriptor is just an int, and tells you nothing about whether it's read mode, write mode, or even a pointer to some random address in memory. &gt; Instead, our goal in writing strongly typed programs is to contain as much of the weak typing to helper functions as possible, and expose a strongly typed interface for most of our program. By using withSourceFile instead of withBinaryFile, I now have just one place in my code I need to check the logic of using ReadMode correctly, instead of dozens. You can see this a lot in Haskell bindings to C libraries. I wrote a binding to the ODBC C library recently and designed the API to rule out the kind of segfaults and race conditions I experienced with HDBC-ODBC.
Optimistic evaluation is cute, but IMO is too unpredictable to be useful for programming. Imagine a call to foldl... it might either stack overflow / use lots of stack space or behave like foldl’ depending on how long the operation takes, which is not even deterministic. That plus the fact that it’s complicated to implement means I don’t think it’s a good choice.
I think I see what you mean, that in memory you will have data structures regardless of the programming paradigm. However, I think that category theory is not just an inspiration for some FP interfaces. CT is the math of abstract functions, and how these abstract functions compose. CT not only abstracts FP interfaces, imho I think it abstracts the patterns of computation itself! Now even though Haskell compiles to mutable data structures, the idea behind denotational semantics is to use math such as CT to provide not just syntax and interfaces, but also a way to reason about your code... &gt; Specifically, using a sum type and with operations defined using pattern matching. ...such that instead of using a linked list you instead think about it as a sum type all along. Don't get me wrong though, there are plenty of reasons to implement all of those data structures that are typically done with mutations, maybe as an exercise or perhaps for performance, but I will point out that Linked lists are named after how they work... in other words operational semantics, something that denotational semantics in Haskell aims to abstract away (isn't haskell List literally a linked list under the hood?). Lastly, I would like to reference the [Curry, Howard, Lambek correspondance](https://wiki.haskell.org/Curry-Howard-Lambek_correspondence), because it suggests that using CT for FP is more than just a correlation. 
(This seems related to the Rustic notion of providing safe APIs to `unsafe` implementations.)
Nothing to do with the content, but this website is cancer: https://i.imgur.com/bsK9zW0.jpg Note how not a single piece of actual content is visible
I've found this kind of technique hugely helpful even in C (where a single-element struct is basically a newtype). I gave a talk about this that deserves to be turned into a blog post...
&gt; What resources would you recommend for implementing a call-by-need language? * The Implementation of Functional Languages * Implementing Functional Languages: a tutorial There is also the [GHC Reading List](https://ghc.haskell.org/trac/ghc/wiki/ReadingList) which has good links although not all will be relevant to you. **** &gt; This could also potentially help us get some more interesting Haskell interop than just going through the C FFI. How would that work? Wouldn't that require binary compatibility with GHC's runtime/calling convention? Which sounds like it would require a ridiculous amount of work for a small payoff.
The idea is that there exist "indexed" versions of optics (indexed Folds, indexed Traversals...) that can supply the index of each visited element to special consuming functions like [ifor](https://hackage.haskell.org/package/lens-4.15.4/docs/doc-index-I.html). `values` and `members` from lens-aeson are both idexed (by the array position and the object key, respectively). Indexed optics can be used directly as nonindexed ones, if you don't care about the indices. Now, when you compose an indexed optic with a nonindexed one using plain vanilla `.`, the index information is lost. The composition is not indexed. But you can preserve the index information using the special operator `&lt;.`. In fact, if you compose two indexed optics (for two levels of objects for example) you can preserve both indices by using `&lt;.&gt;` operator. The index of the compositiion becomes the pair of the original indices. This lets you accumulate index information as you move downwards a data structure, only to "make it appear" by using `withIndex` or related function. 
[removed]
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [reflex-frp/reflex-platform/.../**project-development.md** (develop → bdc94c6)](https://github.com/reflex-frp/reflex-platform/blob/bdc94c605bf72f1a65cbd12075fbb661e28b24ea/docs/project-development.md) ---- [^delete](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply ds3af0h.)
I was mainly curious. I am experimenting with some operators inspired by lenses, but the underlying representation is record-based syntax with get/set or modify-like operations. edit: The reason I was using the records as representation is because I find it easier to reason about what is going on.
&gt; The wrapping and unwrapping of values was tedious I wonder if there is a way to not have to do this as much? Some syntactic sugar or something else... 
Awesome! If there's any way I can help out, let me know :)
Mind you, you can do the same thing by making a custom Setup script or template-haskell or autoconf-based cabal build or whatever fetch arbitrary code from anywhere you like.
You could separate data and codata, and have a different evaluation strategy for each!
I agree. Instead of apparering professional, it feels like I accidentally clicked a link in a spam mail. 
Thanks! The capataz library has the same goal as threads-supervisor, but there are a few differences... The features threads-supervisor offers that are not currently on capataz are: * Bounded supervisor queue, capataz only uses Unbounded queues at the moment * Notification to parent supervisors As a side not, I was reading through the code, and there may be some rough edges in regards to masking and async exceptions On the other hand capataz provides: * AllForOne supervisor restart strategy * Permanent, Transient and Temporal worker restart strategy * Callbacks for termination, completion and failure on supervised routines * More detailed telemetry * Code is a bit more documented Thanks for checking it out 
Would that mean `data` would have strict fields as well? Because if so a performant`map` over a finite list is impossible. There are very good reasons for non strict evaluation of `data`.
I wrote/am writing an open source "cryptocurrency" in haskell, complete with verbose, informative readme, and a 2 hour long video presentation @ boston haskell: https://github.com/tdietert/nanocoin
If I may address the content then, I find blogs like this a bit annoying in that it makes a pretty banal observation: &gt; Languages like Haskell are not some piece of magic that will eliminate bugs. You need to follow through with discipline in using the languages well if you want to get the benefits of features like strong typing Only the incredulously naive couldn't possibly believe that. Yes, magic doesn't exist even in programming languages and one must learn how to use Haskell to take advantage of it. Doesn't that apply to any programming language "weak" or "strong" or any discipline? It's too general that it renders itself meaningless. I think the paper is probably what this blog should aspire to be about in style and content: [Elementary Strong Functional Programming](https://pdfs.semanticscholar.org/b60b/1c2e49ec6f574f220f162c8fdc81b2831830.pdf). It also has precision and insights that most blogs lack such as this one. NB: fpcomplete.com is a site I generally avoid simply because of all that dynamic spam. It's annoying and nothing on that site is worth that level of annoyance. edited for clarity.
Redux (on top of React.js) is probably the most popular FRP in JS. It uses a state-action-model architecture (actions being like events), which is basically event-behavior. But yeah, pretty much anything in JS you have to try and look past the ugliness to make it practical. I've been meaning to try FRP in Haskell, for both back and front-end, and it seems the only major downside is performance due to the way streams get processed with a lot of duplicate messaging, although I don't know if this happens in Haskell libs. I'm trying to be language-agnostic in regards to FRP. I started reading on [push-pull FRP](http://conal.net/papers/push-pull-frp/), which is supposedly a way to solve this and speed it up.
I absolutely love that book! It is a bit old but in all fairness almost everything you'll learn from it can be used in Haskell. The only outdated chapter is the one about interactive programs.
&gt; With the exception of the Typescript version of the Sodium library, I don't know of any Event-and-Behavior style FRP libraries for JS. There is [purescript-behaviors](https://github.com/paf31/purescript-behaviors/), which is still a bit of a work in progress, but definitely usable.
To be frank I don't get the hostility. There's an ad, which you can close, or avoid using something like NoScript. Then there's a blog post the content of which you might find interesting or not, but you don't have to pay for it and it doesn't advertise any product. Why you choose to act like you're offended by this, and why this is the most upvoted comment is beyond me.
In some cases, you can use `-XGeneralizedNewtypeDeriving` to derive typeclass instances of the underlying type, like `Num`, `Monoid`, `IsString` and the like. You might also be able to use classy lenses if the underlying type is something for which a lot of newtype wrappers exist. If you use newtypes with phantom type variables or GADTs, you might be able to reduce the pain by writing a few combinators that work for all `MyNewtype a`.
It's a myth that functional programmers believe that strong typing solves all bugs which is perpetuated in many discussion threads. If you care about popularizing functional programming it is good to counter these myths. A blog post like this might help to do so.
Very nice! I'll have to keep an eye on that. Although if we're going that far away from JS-the-language, my current (perhaps biased) recommendation would be `reflex` :)
or strict StateT
Yeah I thought as much, and I know I'm not helping by replying with a comment that isn't exactly on-topic or constructive either. I just don't get why people take these things sooo seriously (and why anybody in 2018 still uses a browser without add-ons) :)
IIRC, neither [strict WriterT](https://stackoverflow.com/a/47839335/1364288) nor strict `StateT` are actually strict in the accumulator. An example: import Data.Monoid import Control.Monad.Trans.State.Strict import Control.Exception main :: IO () main = do let (x,_) = flip runState mempty $ do modify $ mappend $ Sum (1::Float) modify $ mappend $ (throw $ AssertionFailed "oops") modify $ mappend $ (error "oops") modify $ mappend $ undefined modify $ mappend $ (let z = z in z) return True print x Runs fine: *Main&gt; :main True 
Fair enough; this blog is for the incredulously naive.
You're annoyed by reading blogs about things you already know? Not everything *you* think is obvious is necessarily obvious to other people. You would be surprised just how many people fall under the ""incredulously naive"" category. To many developers, programming languages are a kind of magic, and it is natural to extend a level of mysticism to Haskell. Just because you are fortunate enough to already know what the blog is saying doesn't mean we should deprive others of the opportunity.
Okay, good point. TIL that strict StateT isn't as strict as I thought.
Actually not really. Although stream fusion makes my argument even stronger. It's because lazy `map` takes `O(1)` stack space due to guarded recursion, but strict `map` takes `O(n)` space due to the lack of any form of tail call recursion. You need either mutation or laziness to efficiently map over a linked list. You can try it yourself in Haskell. Make a linked list with strict fields and map over it. Then do the same with a lazy linked list.
I'm a little familiar with Redux, and I don't think it's very much like Event-and-Behavior FRP at all. Behaviors seem to be tricky to understand until you use them. I've seen three or four event handling libraries pop up on this subreddit that were reactions against FRP, and two of them had authors who were convinced that there wasn't any mechanism for accumulating / dealing with state in FRP systems (and had strange thoughts about what Behaviors did). I think this kind of FRP needs more documentation and examples. The other thing that is key in Event-and-Behavior FRP systems is the denotational design. Every piece has a meaning, and when you compose the pieces / programs, the meaning composes as well. If you're after some language agnostic materials, I highly recommend the Manning book on Functional Reactive Programming. It uses Java with the Sodium library (Event-and-Behavior FRP, built with the denotation in mind) for most of the book, with bits of JS libraries in there to compare and contrast. There are quite a few different versions of Sodium for different languages as well. There are wins from the denotational design aspect of these libraries that I wasn't even properly aware of until this book pointed out some of the pitfalls that other libraries with the FRP label on them run into with even small programs. The book does have a few sidebars about how there are far fewer things to be disciplined about if you are using a language like Haskell though :) On that front, the Sodium Haskell library was deprecated in favour of `reactive-banana` once it became clear they had effectively converged. I'm a fan of `reactive-banana` (which is great), but I'm spending most of my FRP-related time with `reflex` at the moment (which is excellent), which can be used for the front and back end. I'm currently chipping away at a pile of libraries to make various backend things easier, so again, I'm a bit biased there :)
No, I'm annoyed at the banality of the blog post. Banality isn't the same as knowing or not knowing something on my part. For example "Elementary Strong Functional Programming" is probably obvious to a lot of functional programmers but it's still insightful. It's making a constructive case for a form of FP not trying to disabuse someone of an irrational belief. It makes for much interesting reading to a general audience.
Sadly, I am looking for the specific three letter abbreviation. I want to implement a Haskell wrapper around [SJCL](http://bitwiseshiftleft.github.io/sjcl/), and the default AES mode is CCM :/
Yup, its even worse on my screen. Closed the article again.
Anything wrong with using an accumulator?
I would use 'strict-by-default' because it's *easier to implement*. It also interacts better with contemporary programming languages better. Ref: Idris, whose JS backend is blazingly fast, and PureScript, where the generated code is damn readable JS
So far I enjoy it too. It honestly it doesn't seem outdated so far, just very in depth. How is the interactive programs chapter outdated? I haven't gotten to it yet.
&gt; rough edges in regards to masking to be clear, in threads-supervisor (or in capataz)?
In threads-supervisor, capataz unmask exceptions in particular places to avoid unexpected behavior
Have you used FRP for logic but not for any GUI? i.e. just the backend, not any frontend. e.g. a simple server that gets some http requests, can process stdin, watches a file, and may be toggled on/off; it keeps some state, and maybe debounces. the point being that it processes several different events, and some continuous quantities ("behaviors", i guess) too. what i want is: to write some ad-hoc manual hooks, like channels or callbacks (not specialized to say DOM) and register them to provide some source of events. I've tried a fold-style thing, using pipes, Cont stuff, and just plain IO. They're all fine, but I kinda hope that re-using reflex-generic methods (like ) can clean things up. I've skimmed through a few reflex-dom, but it seemed more involved than I was expecting. Like, is there an example of a "reflex-io", which provides only two primitives, wrapping stdin and stdout? 
Oh, just found reflex-brick, which uses something called reflex-host, and has a good blog post. https://github.com/lspitzner/bricki-reflex/blob/master/src/Brick/ReflexMain.hs http://hexagoxel.de/postsforpublish/posts/2017-10-30-brick-plus-reflex.html https://github.com/bennofs/reflex-host/blob/master/src/Reflex/Host/App/Internal.hs These might be what I was looking for. 
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [bennofs/reflex-host/.../**Internal.hs** (master → ab72c16)](https://github.com/bennofs/reflex-host/blob/ab72c16077ab3bcc1c1e81312aac1090e64b97d4/src/Reflex/Host/App/Internal.hs) * [lspitzner/bricki-reflex/.../**ReflexMain.hs** (master → b97ccef)](https://github.com/lspitzner/bricki-reflex/blob/b97ccef05f10bfbdac5ff56e1167af9ef62c99b1/src/Brick/ReflexMain.hs) ---- [^delete](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply ds3ozrz.)
&gt; If you have 100 dependencies and 10 versions of each you need to build 10^100 different combinations of packages in general. The approach I have taken in [cabal-rangefinder](https://github.com/gelisam/cabal-rangefinder#readme) is to make sure the package builds with the latest version of every dependency, and then to figure out how far back I can stretch the lower bounds for each dependency. I check each dependency independently, and I binary-search the available versions to figure out the breaking point, so it would only take 100*log(10) builds to find all those lower-bounds. It's pretty easy to come up with situations in which one of my underlying assumptions are wrong, but it would work much better if the metadata was accurate: * the assumption that there is a single breaking point is wrong if one version in the middle was uploaded by mistake and then a fixed version was uploaded immediately afterwards, but if I could rely on hackage's metadata on [which versions are deprecated](https://hackage.haskell.org/packages/preferred), I could simply skip those versions. * when asking to build one of the dependencies at a specific version, I let cabal choose which version of all the other dependencies to pick, but if the metadata is incorrect, cabal might pick wrong and I'd get a false negative for that specific version of that specific dependency. So I think I have a pretty good system for generating good version bounds _given_ accurate metadata for the dependencies, so it's a virtuous cycle. The problem is that a lot of the metadata is incorrect (or at least it was when I last played with cabal-rangefinder, it's been years now), so I'm firmly in the "if the metadata is wrong, it should be fixed" clan.
&gt; incredulously naive That word doesn't mean what you want it to. You mean "incredibly".
I've used it for simple servers so far, mostly to try to test my ideas for libraries for reflex on the backend. I think you're probably best off not looking in `reflex-dom` and just looking in `reflex`, although it can take a little time to get across the various typeclasses. I've got some examples of gui-less hosts [here](https://github.com/dalaing/reflex-host-examples), and you can debounce things with the [Reflex.Time](https://github.com/reflex-frp/reflex/blob/develop/src/Reflex/Time.hs#L200) module. The `reflex-host` library is a good option if you don't want to get your hands dirty with the host internals, and I've also got [something](https://github.com/dalaing/reflex-basic-host) like `reflex-host` but with less bells and whistles, that I've been using to build libraries for using `reflex` with sockets / websockets / servant servers. I need to sort out some issues there and probably write something for working with servant clients, and then I'm going to see if I can write a Raft server that uses a slightly restricted `reflex` network as the distributed state machine. At some point I want to look at what it would take to put together a parallel version of the `reflex` spider. At the moment these backend examples are either single threaded or a are parallelized manually - which doesn't feel like it's a lot of work, but having it done automatically, correctly, and with some confidence that the right performance choices are being made would make me happier.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [reflex-frp/reflex/.../**Time.hs#L200** (develop → f78a73e)](https://github.com/reflex-frp/reflex/blob/f78a73ef7300846f4b81f34df0d17d75cb64476d/src/Reflex/Time.hs#L200) ---- [^delete](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply ds3qgi1.)
Can you give some exact code to show how that would work? As I'd be extremely surprised if you could get something performant regardless of accumulators and what not.
https://shivindap.wordpress.com/tag/accumulator-passing-style/
What about go to definition for non-repo code?
Oh, there is also [most-behave](https://github.com/briancavalier/most-behave), which adds behaviors to most.js in the same way as in `purescript-behaviors`, as sampling functions on top of events. If you're stuck using JS event libraries for whatever reason, one of these two options might be good. Otherwise, I agree, go reflex :)
Thank you! This is exactly what I needed. 
Some code bases not mentioned yet (but you are probably aware of): HKIO's Cardano and Haskoin. There are quite some papers in this field.
Yea for the most part, there's almost no difference between lazy and strict `StateT s m` when `m` is bind-strict. Lazy monads like `Identity` are where the difference comes up
"All you have to do is &lt;some set of steps&gt;" is a classic misguided justification for bad UX design. If most people find it inconvenient (albeit possible) to use the site, then it probably needs work on the usability front. The fact that this is the most upvoted comment is very good evidence that most people find the site inconvenient (albeit possible) to use. Also, saying "this is hard to use" is not the same thing as saying "I'm offended by this."
I've talked to quite a few people who were very interested in something `reflex`-like but for purescript. The questions have always been around whether `reflex` is relying on laziness in a way that makes a lazy language a requirement. I think the Sodium Typescript library is a good counterpoint to that, although it looks like they had to do a lot of work to make the memory / GC behave. Are there any plans to add something like `Dynamic`s to `purescript-behaviors`? That's one of the things that is lacking from `Sodium`, and it's a pretty nice way to being able to get to something like a virtual-dom approach in terms of DOM manipulation, except where you can elide the diff / patch steps :)
The last paragraph pertains to the digression into continuation-passing style. See: &gt;The APS implementation worked well even for large values of n as the stack-size required for the program is now a constant
&gt; informative readme That's an understatement. Read like a well written tech book! Kudos. (tips hat)
WriterT only leaks space if `mappend x y` is actually smaller when evaluated than when unevaluated, right? E.g. evaluating the spine of `x ++ y` doesn't actually use appreciably less space than leaving it unevaluated; whereas 1 + 2 obviously uses one more Int than 3.
Half of this, probably, can be replaced with haskell-ide-engine.
Ah fair enough, but that still doesn't actually change anything. Factorial is not equivalent to mapping over a strict linked list, the former has many different well known performant `O(1)` stack space solutions, whereas the latter does not. Honestly all this beating around the bush just isn't useful, if it's possible then I would request that you implement it. I have tried myself and could not manage it. If it is such a huge task that you think it is possible but don't want to spend the time, then that alone is already a very good argument for sometimes evaluating data non-strictly. A few more examples for benefits of non-strict evaluation of `data` are knot-tying (think dynamic programming) and simple and flat `where` clauses. I mean honestly if you look through any post about the benefits of non-strict evaluation (I know Edward Kmett made a great one a while back), the vast majority apply to `data` and not just `codata`.
&gt; If it is such a huge task that you think it is possible but don't want to spend the time I don't have a Haskell environment set up on my machine, and setting one up is indeed a huge task.
If you implement it I will run it for you. Here's a few for reference: map' :: (a -&gt; b) -&gt; List' a -&gt; List' b map' _ Nil' = Nil' map' f (Cons' a as) = Cons' (f a) (map' f as) Takes up a lot of stack space, significantly slower than lazy implementation. map' :: (a -&gt; b) -&gt; List' a -&gt; List' b map' f xs = rmap' Nil' id $ rmap' Nil' f xs where rmap' a f (Cons' x xs) = rmap' (Cons' (f x) a) f xs rmap' a _ Nil' = a Tail recursive, so `O(1)` stack space, but requires multiple passes and thus significantly slower than lazy implementation. The above is basically just the first implementation but with a hand rolled stack. The main issue is that the first thing you must produce in the final result is `Cons' (f &lt;last_element&gt;) Nil'`, but you can't produce that until you are at the end of the input list. So at the point where you produce the above you are necessarily sitting on `n - 1` elements that are NOT in the final result list, and instead are stored in some other way such as on the stack or in a reversed list.
Honestly I'd be fine with `OverlappingInstances` if `Overlappable` was always required (and the type system doesn't violate the open world assumption like it currently does), and if overlapping orphans were forbidden. To elaborate on the `Overlappable` part, and why it's necessary for coherence. module ShowInList where showInList :: Show a =&gt; a -&gt; String showInList x = show [x] **** module Foo where import ShowInList data Foo = Foo deriving Show instance {-# Overlaps #-} Show [Foo] where show _ = "Foo List" **** showInList Foo == "[Foo]" showInList Foo == show [Foo] show [Foo] == "[Foo]" show [Foo] == "Foo List" So we have managed to violate referential transparency, as replacing equals for equals gave us a completely different result. Now currently even adding `{-# Overlappable #-}` to `Show a =&gt; Show [a]` wouldn't have fixed the issue, as for whatever reason GHC's current type checker violates the open world assumption when it comes to overlapping instances. However if this issue is fixed, then we are in much better shape. We either don't have `{-# Overlappable #-}` on `Show a =&gt; Show [a]`, in which case everything is fine as the above overlapping instance is impossible to write. Or we DO have it, and then when we try to write the problematic `showInList` function as above we will get: Overlapping instances for Show [a] arising from a use of ‘show’ Matching instances: instance Show a =&gt; Show [a] -- Defined in ‘GHC.Show’ instance [overlap ok] [safe] Show [Foo] The exact same thing that we would have gotten if we had defined `showInList` in `module Foo`. We would thus appropriately have to write `showInList` as follows: showInList :: Show [a] =&gt; a -&gt; String showInList x = show [x] Which would have given `"Foo List"` both times above, and would be perfectly coherent and sound.
Unfortunately that's still on my to-read list, so I can't say, but I've heard good things about it, and I liked the few [Bartosz videos](https://www.youtube.com/playlist?list=PLbgaMIhjbmEnaH_LTkxLI7FMa2HsnawM_) I got to see. 
Yes, Accelerate currently does not have mutable arrays for exactly that reason. If we could establish a story for how they should be handled, then we could look into adding them. Those are good suggestions regarding documentation, I'll add something along those lines. Thanks!
Lazy data is useful, so making data strict and codata non strict is conflating strict/nonstrict with data/codata when they don't actually line up that way entirely.
I figure that when you need lazy data, you can encode it using explicit thunks. Any reason why not?
Doing so is cumbersome. Why not just support both properly?
Because languages should have minimal cores? (That the data/codata distinction indeed belongs in a minimal core is a matter of opinion, but IMO it's definitely meaningful.)
The nix people are going to be very, very happy about this package
Laziness pretty much has to be in the core so one can make all the relevant optimizations. 
I have a working setup of dante using ElvishJerricco 's reflex-project-skeleton using the below code in dotspacemacs/user-config. But I realized that I dont use dante at all, instead I still use local hoogle, ghcid and project wide grep (&lt;space&gt;+*) I have disabled the fly-check-mode as it degrades the performance of emacs. (use-package dante :ensure t :after haskell-mode :commands 'dante-mode :init (add-hook 'haskell-mode-hook 'dante-mode)) ;; (add-hook 'haskell-mode-hook 'flycheck-mode)) (setq dante-repl-command-line '("/path/to/reflex-project-skeleton/cabal" "new-repl")) (evil-leader/set-key-for-mode 'haskell-mode "x" 'xref-find-definitions "a" 'dante-type-at "z" 'dante-info) ) 
This is definitely true. It's almost all irrelevant to most people though. So just reading the README should put anyone on a happy enough path. I think it should be improved, but I don't think it's impenetrable.
You should definitely check out vim plug. It's my favorite plugin manager for neovim by a very large margin. You might also want to check out fzf (fuzzy finder). I use it in my shell a lot and, combined with ripgrep, it's amazing. I think ALE is the newest hotness vs neomake but both work well :) Generally, I'd probably start off a "Haskell vim setup" with the 100% Haskell part first, then detail your personal config stuff towards the end (or even separately), but that just personally how I like to organize posts.
I have no idea why this is getting downvoted. The second point is quite important I think.
The title made me think this was a security notice and that there was some sort of hackage exploit.
This isn't violating referential transparency as far as I can tell.
I mean it violates the principal of substituting equals for equals, which is a pretty core part about referential transparency. At the very least it violates coherence, so regardless of exact terminology overlapping instances is currently not safe to use.
Right, exactly like `kgid_t`. Not like `gid_t`, which is simply a (couple layers of) `typedef` (`type` alias, in Haskell terms). I think this is what you were saying; I'm being explicit just in case :)
Oh I absolutely agree that nix is the right tool for this problem. I wish it were more prevalent but for now I still want to support people that don't use nix.
Are you suggesting have the go compiler, or the .a and .so files produced by the go compiler for my project, distributed as an OS package? The former is certainly doable, but then I still need a custom Setup.hs to invoke the go compiler. The latter seems like a huge amount of work, for I would need to create packages for the various package managers and get them included in various Linux distributions' package repositories, or distribute the package as source and ask the user to build it manually. Again, if the whole world (or at least the Haskell world) is using Nix, it would be almost trivial. That link you provided is super interesting - I was wondering what the build environment was like on stackage. Tangentially, it also got me thinking, how does stackage prevent malicious code in custom setups from causing harm? A super locked-down docker container + some firewall rules maybe?
Yes, although `WriterT` still appends `mempty` for every action that doesn't explicitly `tell`, so we could have a computation with very few `tell`s that still leaks.
I don't think your first point has any consequence on the space usage I described. That's more of a time complexity thing
Maybe I haven't looked into that. But keep in mind the GHC Api isn't stable across versions so it's **at least** still additional maintenance work. As a user using the FFI with tooling that creates Haskell wrappers around it would probably be good compromise. But don't take my word for it and make sure to do your research.
btw, `memo-trie` is used within `vector-space`, so these optimizations will be very useful.
In what situation would an overlappable instance be usable if not whenever there is no more specific matching instance in scope (which is unsound with respect to the open world assumption)?
Referential transparency just means that you can freely substitute the name for the definition without changing semantics. What you're doing is just substituting the wrong definition. It's like saying that name shadowing violates transparency. That being said, I agree that adding/removing an import shouldn't change the behavior of a program.
It's only unsound in the presence of orphans. So you can simply forbid overlappable orphan instances, and also forbid orphan instances that overlap.
Fair point. List is bad example anyway (remember beginners reading these ramblings!), as it has lazy spine, so `State` strict in state won't be much different from lazy state. IIRC I have used only strict(we) writer with some kind of `Builder` as an accumulator. I probably forget something.
&gt; What you're doing is just substituting the wrong definition. No I'm not... what are you talking about? showInList Foo == "[Foo]" But if you substitute in the definition of `showInList`: showInList x = show [x] To get: show [Foo] You get a different result: show [Foo] = "Foo List"
There's also the [newtype](https://hackage.haskell.org/package/newtype) machinery
I see. That makes some sense.
Oh wow you're right! The `showInList` definition's context `Show a` forces a instance resolution to pick an instance before the type is fully specified. I mean, I get how it works but it would certainly cause a lot of headache to have to debug this.
Does the compiler eliminate parts of the tree construction when the program is expressed as a hylomorphism? My thinking is that you only need to keep track of the current score, the starting number, and the pool. We often see fusion for lists but it would be awesome to see GHC fuse the tree construction and destruction here. It would show off the lovely feature of Haskell that one can reason about the simplest steps of a problem, let category theory spin up a full solution, and let the compiler spit out an optimised one!
Note that since you know you only need the maximum score, you can simplify the algebra. maxScoreAlg :: Algebra TreeF Int maxScoreAlg (NodeF n []) = n maxScoreAlg (NodeF n lst) = 2*n + maximum lst
This is an interesting way to think about the problem. The straightforward way is to think of it as graph problem, and use the Dijkstra A* algorithm (modified to maximize instead of minimize) to solve it.
I nominate aeson (https://github.com/bos/aeson), which is used in literally every Haskell web project ever.
Very nice. By the way, the same functionality is also available in [Control.Lens.Wrapped](https://www.stackage.org/haddock/lts-9.11/lens-4.15.4/Control-Lens-Wrapped.html).
Can you elaborate? What sort of static analysis did you have in mind? Just strictness analysis or something else?
&gt; This library... provides ways to make threads reliable in situations where the usage of async or forkIO would give you the same outcome. The outcome is **never** the same for async and forkIO. And the difference is completely orthogonal to whether the threads are long-running or short-running, or to the number of threads at play. The most important difference between forkIO and async is who is responsible for handling exceptions. With forkIO, the child thread is responsible to handle all exceptions, because the parent will never see exceptions that happen in the thread. With async, responsibility is shared: exceptions not handled by the child thread can be handled by the parent thread. You almost always want the async behavior. Handling all possible exceptions is a big responsibility. It's complex, there are many different kinds of possible exceptions that generally involve various levels of application logic. You normally do not want to be forced to duplicate that complexity across multiple threads. The case where you can't use async is when the child thread will outlive the parent thread. Then you are forced to add the complexity of doing exception handling in the child thread.
Super cool. Could anyone throw a dart at the performance-board of defining these high-level recursion schemes vs a couple mutually-recursive combinators? I'm inclined to start using this in place of the traditional combinators but useually performance is a consideration in these cases. 
I tried spaceneovim and liked it ! But I don't use it regularly since I'm using vscode more and more and Haskell is not my working language. https://github.com/Tehnix/spaceneovim
I never use GC-based finalization in pipes-concurrency, it is a bad idea. I always use explicit finalization and it seems to work well.
I gave a talk at my company about how we can use `newtype` in C++ to make the code more robust. However, the overhead of new struct creation and the lake of automatic deriving of behaviors based on the behaviors of the internal object makes this rather painful to use in C++ at a point were people prefer using unwrapped types and std::tuple.
I think (and hope) you have misunderstood the proposal. It is not mandating "no package versions". While the data about which dependency version choice cause this package to build or fail to build is important, it is far from the only information expressed by dependency bounds in the cabal file. The proposal is to establish a standard way to collect and express data specifically about build successes or failures for various combinations of dependency versions. Build tools could then combine this information with the information expressed inside the cabal file. It sounds like a great proposal to me.
Thanks to Bartosz for making his code available. We can see that the call `(_, chains) = hylo chainAlg coalg (0, pool)` at the end gets simplified to this Core (`-O2`): Bridge.bestChain_$s$shylo = \ sc_s7fy -&gt; case sc_s7fy of _ { (ww1_s73G, ww2_s73L) -&gt; case ww1_s73G of _ { GHC.Types.I# ww4_s73J -&gt; case Bridge.$wcoalg ww4_s73J ww2_s73L of _ { (# ww6_s74U, ww7_s74V #) -&gt; case Bridge.$wchainAlg ww6_s74U (map (Bridge.hylo_$shylo chainAlg coalg) ww7_s74V) of _ { (# ww9_s74Y, ww10_s74Z #) -&gt; (ww9_s74Y, ww10_s74Z) } } } } where the interesting parts are: - the parameter `sc_s7fy` will be substituted with the pair `(0, pool)`; the result `(ww9_s74Y, ww10_s74Z)` corresponds to the pattern `(_, chains)` (this is to help see where we are) - `Bridge.$wcoalg` and`Bridge.$wchainAlg` are optimized versions of `coalg` and `chainAlg` where the input and output are unboxed (`(# ..., ... #)` is an unboxed pair which takes no space other than its contents); in particular, the output of the former is fed almost directly to the input of the latter, without allocating an intermediate `NodeF`. However, this optimization happens only at the root of the tree. The recursive call `Bridge.hylo_$shylo` is different: Bridge.hylo_$shylo = \ @ a_X2yC @ b_X2yE f_a2iv g_a2iw eta_B1 -&gt; f_a2iv (case g_a2iw eta_B1 of _ { NodeF a1_a2xS a2_a2xT -&gt; Bridge.NodeF a1_a2xS (map (Bridge.hylo_$shylo f_a2iv g_a2iw) a2_a2xT) }) Without going into the details, we can see there are some `NodeF` constructors in the way here. The problem is in the recursive definition hylo f g = f . fmap (hylo f g) . g The compiler doesn't recognize that the recursive call uses the same arguments. That is not a generally applicable optimization; sometimes it makes programs slower. But in this case it helps, and we can do it manually: hylo f g = h where h = f . fmap h . g In fact that is [how recursion-schemes defines `hylo`](https://hackage.haskell.org/package/recursion-schemes-5.0.2/docs/src/Data-Functor-Foldable.html#hylo) (Bartosz wrote a standalone version for simplicity). Now the body of `hylo f g` can be optimized all at once. `Bridge.$wh` is `h` above where `f` and `g` are replaced with `chainAlg` and `coalg`. We see that the `NodeF` got unboxed again, its contents being passed directly between `Bridge.$wcoalg` and `Bridge.$wchainAlg`. The recursive call is hidden in `h_r7ln` which calls back to `Bridge.$wh`. (Note the consistent naming scheme with the `$w` prefix, although I couldn't explain what that means exactly.) Bridge.$wh = \ ww_s74B ww1_s74D -&gt; case Bridge.$wcoalg ww_s74B ww1_s74D of _ { (# ww3_s74V, ww4_s74W #) -&gt; Bridge.$wchainAlg ww3_s74V (map h_r7ln ww4_s74W) } h_r7ln = \ w_s74v -&gt; case w_s74v of _ { (ww1_s74y, ww2_s74D) -&gt; case ww1_s74y of _ { GHC.Types.I# ww4_s74B -&gt; case Bridge.$wh ww4_s74B ww2_s74D of _ { (# ww6_s753, ww7_s754 #) -&gt; (ww6_s753, ww7_s754) } } } So a small tweak, or using the recursion-schemes library, makes the tree construction fuse away. We are still listing all combinations explicitly, and it would take more effort to fuse `maximum` and `map score` with `hylo`, but that seems at least feasible.
How do you count the cardinality of: "strictly fewer programs terminate"? :) Q4. I personally like the STG paper and the book on implementing functional languages.[I've collected references I like against the simplexhc repo](https://github.com/bollu/simplexhc-cpp#references) You seem knowledgable about Q5. What's the general strategy when they force a bottom? How do they "fix" this? 
Yes, Haskell's approach I think does make it much easier, especially if you use generalized newtype deriving to derive the instance for `Num`. I didn't like unwrapping the integer inside of the newtype, but writing an explicit conversion from an `XDist` or `YDist` to an `Int` with an explicit conversion using `xDistToInt` or `yDistToInt` did make the code easier to understand, if making it a little cluttered. A better way to do strongly-typed graphics would be to use the `linear` and `diagrams` package, although it does have a bit of a steeper learning curve.
For the record, I have somewhat given up on being able to write compilers for lazy languages that are as fast as C. for JIT, I still have hope. My rough reasons is along the lines of: - Current hardware is optimized for strictness - Traditional compiler optimizations rely on strictness. I am not aware of any "interesting" compiler optimizations that can exploit laziness - demand analysis (analyzing what parts of a lazy program can be evaluated strictly) is literally a fight with turing completeness.
There is and there is ongoing work, but fundamentally the console host is very different from a pty.
Pinging /u/edwardkmett :)
Thanks for the explanation and effort understanding the core! Now that you mention it (and I think about it), I realise that getting rid of the tree constructors and the chain constructors are two separate optimisations. As /u/Gurkenglas points out, there's an algebra to extract the max score directly from the tree, so if GHC can fuse away NodeF constructors with that it would give great solution. Nevertheless, it's an interesting problem to work out whether one can coax GHC into finding that type of solution on its own with small modifications to Bartosz' code. People complain that performance is hard to reason about in Haskell, but it is *fun*!
It's based on the same console host.
In the Rust Libz Blitz was there a requirement that each library be improved exactly once?
I'm not au fait with the situation but proc-do notation for arrows - this seems an area that could do with some speeding up. The rewrite rules seem to be fairly slow. If anyone had any pointers on where to look/what to do I certainly wouldn't mind looking.
I usually reach for `streaming` these days for my streaming needs, which also has a concurrency package: http://hackage.haskell.org/package/streaming-concurrency. I've had luck with this.
Hunting Hy and lo.
Is there a similarly elegant solution for part 2 of the problem?
Thanks for the explanation of differences between `async` and `forkIO`, if anything it motivates me to go through its source code in depth to understand it's error handling better. &gt; The outcome is never the same for async and forkIO. With this line, I guess I was referring to the outcome in _best_ case scenario where there are no errors, and the only thing I would do with the returned `Async ()` value is `cancel` it. Come to think about it, this is silly, so I'll remove it from the `README`. &gt; And the difference is completely orthogonal to whether the threads are long-running or short-running, or to the number of threads at play. I guess this comment was a response to this: &gt; async fits the bill perfectly for small operations that happen concurrently, not necessarily for long living threads. Yes, `async` may deal with long-living threads, however, `async` _will not_ automatically restart those long living threads in case of failure. I can foresee some combinators to enhance an `async` with retry capabilities for sure, but any approach that I can come up with mentally would not be something similar to a supervision tree. Paraphrasing Simon Marlow, the beauty about Haskell Concurrency is that we have different specific tools for various use cases; Capataz is not looking to cover all use cases of `async`, it tries to cover the ones that I've seen only covered reliably in the `distribute-process` library. Again, thanks for your candid feedback. 
Why not just have a way for downstream too tell upstream that it's since and won't need any further input? 
Strict inclusion: All programs from the traditional arguments-first order terminate and then some more. =) Ennals' thesis evaluates to a dynamic depth speculatively. If it runs out of fuel for speculation it packages up the work done so far as the thunk in a form that can be resumed. It then charges the let binding site for the construction of that extra work. During GC if it collects that thunk unevaluated it is considered 'bad' work. If on the other hand it gets used, its considered 'good' work. The ratio between bad and good work is used to eventually turn off speculation completely at let binding sites for which speculation isn't paying off. This means that in the limit, as there are a finite number of let binding sites in your program, your program has the same asymptotic behavior as without speculation as nothing ever turns speculation back on for a given site. The tricks are updating the Gc to track this extra cost/blame info, tracking a good global speculative execution depth using other heuristics from the paper, etc. To answer your actual question, bottoms simply because deep speculation which then become thunks frozen part way through. That speculation depth limit is what saves you. As a data point to show that more simplistic approaches fall apart, the "Eager Haskell" folks also did a form of speculative evaluation, but then just chunked on fixed size boundaries. e.g. You went and did 50 things, lets package it up like we did above. Unfortunately their performance results were worse than naive lazy-style evaluation, where the optimistic evaluation stuff was better.
&gt; Are you suggesting have ... the .a and .so files produced by the go compiler for my project, distributed as an OS package? That would be the ultimate solution, but I was just proposing manual instructions for installing the go library from source: - Ensure go compiler is installed - Clone/build/install the desired go library to specific location. I am assuming it is a packaged go library you need? (I have never used go) - Clone, install a specific helper shell script on the path, which behaves like pg_config Then, automate (and mock for CI) the Haskell steps: - cabal custom build which calls out to your "pg_config"-like helper to get paths needed - provide a cabal build flag that triggers different custom build behavior and conditionally compiles without referencing the go compiled c library 
I used `webdriver` to scrape Uber's website to document trips and collect screenshot evidence.
I’ve recently started contributing to the `containers` package, and when familiarizing myself with the code I was reminded how overwhelming the APIs are for core data structures. **Feedback I’m looking for** 1. Is this useful? 2. Is this the right place for this? (part of the containers package, linked from the top of the Hackage page) 3. Do you have any suggestions for content, organization, anything at all? This isn’t meant to replace the haddocks (which are a fantastic API reference!), and this isn’t an excuse to not make the haddocks better. The goal is to provide a canonical place for beginners (or anyone really) to go to get an introduction to the data structures that the package provides and the most common functions. It should be enough to get someone started using them, and if they need a specific function they can use Hoogle or the API docs. **Some Backstory** When I was first learning Haskell I got to the point after I knew the basic syntax where I asked the question (as I assume most people do) “where are the data structures in the standard library?”. I was inevitably met with the reply of “go read the [containers hackage page](http://hackage.haskell.org/package/containers)”. So, I head over there, as I’m sure many others have, to discover a pretty complicated module structure (I had just started learning about module). I eventually find my way to `Data.Map.Strict` to be confronted with 125 (yes, 125!!!!) public API functions. At this point I give up, go search for “haskell containers tutorial” and find...nothing useful. When you’re first starting (or even if you just haven’t used it for a while) it’s nice to have a walkthrough of what to import and how to use some common functions. If we want to build the community we need to make it easy for newcomers to do the simple stuff, like use basic data structures, and I don’t think telling them “read the API reference” is an appropriate response. **Why ReadTheDocs?** - Good browsability and search (nice sidebar, search box) - Restructured text is great - Easy to edit (link directly to GitHub where it can be edited in place) - Easy to publish changes (just commit to the repo and ReadTheDocs picks up the changes) - Kinda pretty - Used by a number of other Haskell projects (Cabal, Stack, Servant, Haddock to name a few) 
I use [hasktags](https://hackage.haskell.org/package/hasktags), and I used to use [codex](https://hackage.haskell.org/package/codex) for dependencies. I haven't used codex in a while, so I don't know if that works anymore. Intero also has a rudimentary jump to definition thing, but I think it's local code only.
Thanks I was having a hard time to actually use it
I just realized this was posted by /u/snoyjerk and not /u/snoyberg. I'm going to add this to the Github issue linked in the blog post.
Raaz is experimental (as of now) and has a long way to go. It is meant to be a lower level library on which other protocols are supposed to be built. However it rethinks some of the basics mainly to give a safer interface even when sticking to the lowest interface. Consider for example the case of randomness. I believe `cryptonite`'s interface expose too many things to the user, in particular various entropy sources. This I believe is not the right design. In contrast, raaz does not provide the user any ways tweak the entropy sources. This limited interface is is one of the most important defence against incorrect use of randomness. Similarly we have a very different approach to locked memory. It is not possible to reconcile this with `cryptonite`'s interface. So it is not often possible to maintain compatibility. 
I think any way you slice it, you'll just reinvent the same trade-offs as you have already with finalizers. I'd need to see a concrete implementation proposal though to be sure.
I nominate `containers`.
TIL. Thanks!
I used `fasttags` for this purpose mostly. It is fast enough I can run it over my whole working directory structure without noticing a delay. 
&gt; Are there any plans to add something like Dynamics to purescript-behaviors? I didn't have any, but I'm interested. Can you explain the benefit of `Dynamic` to me? I understand they are like pairs of behaviors and events, but why does it give more functionality than just that behavior sampled at that event? Do you use the behavior in other ways?
The main use I've had for finalizers has been explaining how conduit and pipes differ. 
I did some playing around and think that you can indeed make this approach work. If I got it right, then you could transform the tutorial example example :: IO () example = do (output, input) &lt;- spawn (bounded 1 :: Buffer Int) as &lt;- forM [1..3] $ \i -&gt; async $ do runEffect $ fromInput input &gt;-&gt; P.take 2 &gt;-&gt; worker i performGC a &lt;- async $ do runEffect $ each [1..] &gt;-&gt; P.chain print &gt;-&gt; toOutput output performGC mapM_ wait (a:as) into something like example :: IO () example = do (output, input, seal) &lt;- spawn' (bounded 1 :: Buffer Int) as &lt;- forM [1..3] $ \i -&gt; async $ do (runEffect $ fromInput input &gt;-&gt; P.take 2 &gt;-&gt; worker i) `finally` atomically seal a &lt;- async $ do (runEffect $ each [1..] &gt;-&gt; P.chain print &gt;-&gt; toOutput output) `finally` atomically seal mapM_ wait (a:as) But you do have to watch out for exception safety, since omitting the call to `seal` will again trigger an error. For larger examples, a bracketed approach is certainly safer, but I don't know if all the tutorial examples can be squeezed into this scheme. I guess depending on your usage scenario this might not be a limitation actually. I *think* I read somewhere that you can still get deadlocks with `withSpawn` and judging from the haddocks, `withBuffer` seems to get more praise. It's a shame you can't make it fit with `Managed`.
I had a look at your suggestion, and the `streaming` library seems to be really nice. I *especially* like that you don't have to use obscure composition operators, something that I didn't quite like about `stm-conduit`. A good reminder that you shouldn't look for packages on stackage alone :/
There'll always be leftovers...
&gt; Intero also has a rudimentary jump to definition thing, but I think it's local code only. Anybody knows how to configure it so those local jumps work properly? Sometimes `gd` jumps to the import statement; sometimes it jumps to the definition in a separate file; sometimes it opens a useless dialog which asks me to choose between the definition and the type-signature one line above it, and the dialog doesn't close when I choose either, unlike most similar dialogs; sometimes it asks me if I want to "keep current list of tags also"; sometimes it hangs for a long time, presumably because it's trying to index something large. I do have a relatively large multi-package project, large enough that I never do `stack ghci`, only `stack ghci &lt;target&gt;`, maybe intero is trying to do the equivalent of `stack ghci` behind the scenes?
Streamly (https://hackage.haskell.org/package/streamly) is exactly what you should be using for Concurrency + Streaming in 2018 :-) It is a streaming library designed with concurrency ingrained and not added as an afterthought. Take a look at the README for a quick overview and there is a comprehensive tutorial as well in the haddocks.
I think you're being _really kind_ to the Haddocks by calling them "fantastic". The quality varies wildly - the documentation on Data.Map is actually OK (though as you point out overwhelming), but too many other APIs subscribe to the wrong-headed notion that providing a type signature, standing alone, is adequate documentation. It isn't, not by a long shot. TLDR: This is a great idea. Prioritize introduction of APIs by focusing on most common use cases, and provide lots of examples. In my experience, a pervasive lack of good examples is the biggest weakness with most haddocks.
If you like streaming then you will like https://hackage.haskell.org/package/streamly as well. The streaming API interfaces of streamly are quite similar to streaming, the difference is that streamly is built for concurrency from ground up.
`fasttags`
 Use first class continuations (no async-wait) , use forkIO to watch for sources of input and bind the resulting with a first class continuation. compose these forked threads with &lt;|&gt; for parallel processing or with &lt;*&gt; to join two parallel proceses. Add thread pooling and monitoring. Use state to store a stack of resources to be closed and perhaps exception handlers to be called when some event happens. Alternatively, you can use `transient` Bottom line: It is really difficult to make streaming, events, threading, resource management, exceptions and thread monitoring orthogonal (to name some some of the effects that you will need in real life). A library should consider all these effects at the core. The idea of a streaming library being useful is not realist outside of some particular cases. Often, What is needed is a library with some or many of these effects included. But when stm-conduit-threaded-async-frp-except-monitoring would appear, it will be really useful but... will we call it an "streaming" library? 
Haha, yeah "fantastic" is pretty generous description, I must have been in a really good mood when writing that sentence :P and thanks for the feedback, glad to hear this is heading in the right direction. I have some changes lined up to improve the haddocks for containers as well that I'm hoping to finish in the coming weeks.
https://hackage.haskell.org/package/fast-tags
There are some open issues in Tasty, but it generally provides the features I want for test organization and I like it a lot. Try to be aware of the open issues and stay within typical use cases and examples to save yourself time.
fwiw, my introduction to `containers` was actually in the ["modules" chapter of *Learn You a Haskell*](http://learnyouahaskell.com/modules). Kind of hidden, but it did give a good rundown of the essentials in `containers` that was enough for me to immediately start using it.
Containers has had a ton of work done on the core structures for performance already. Things like that are probably poor candidates for this...
I'm pretty sure GHC already turns foldl into foldl' with one of the optimization levels.
&gt; I personally find its hard to reach the same levels of abstraction in a strict language. What about a strict language like [Spiral](https://github.com/mrakgr/The-Spiral-Language) with intensional polymorphism and first class staging? I made it myself and would love some feedback as I hadn't had any substantial criticism nor usage of it by other people of it so I am a bit lonely writing the documentation just for myself. Its type system is more powerful than Haskell's and though it does not have parametric polymorphism, it does first class types. When I started work on the type system for Spiral instead of going for unification, I decided to go for abstract interpretation as it seemed more powerful. As I worked on the type system, it became a partial evaluator since I kept putting in more and more functionality in it and apart from low level stuff like register allocation it does most of the the stuff a JIT would while exposing it as polymorphism. This gives it C levels of speed or at least the potential for it once excessive inlining has been wrung out. On a parser benchmark against F# it has been 18x faster than it. It also has a Cuda backend and the beginning of a ML library. I hadn't ever heard of a language with the same features as Spiral. Currently the documentation for it in need of a editor, but it should be readable nonetheless. It is also currently work in progress, but it has around 150 pages.
I don't recall CCM very well, but I believe you should be able to build CCM out of the CBC MAC and CTR implementations available in may libraries including crypto-api. No, this is not an ideal situation.
I do have some interest in the notion of first class staging. Notably, the necessity modality from Pfenning's old work on modal type theory provides a very simple foundation for staged computation. Benton and later Neel Krishnaswami's work on handling linear logic provides an even nicer foundation than Pfenning's work in some ways. Notably, they show that you can split the sort of necessity modality that is used for staging into two parts. One gives you necessity / the embedding of linear logic terms into your intuitionistic fragment. Teased apart you get the ! modality from linear logic and Eugenio Moggi's monad from the computational lambda calculus, which motivated the design of IO in Haskell from the same adjunction. This has been the approach I've been taking towards staged computation. That said, _just_ having staged computation seems kinda weak from a software engineering perspective when it comes to putting code together. There are some things that, say, MetaOcaml has demonstrated being really good at, but I'm not sure I'm willing to give up everything else we do in Haskell to be just a little bit better at that. It seems to be a thing I also want, rather than a thing I want instead.
The tutorial at haskell-lang.org is more of an example driven walk through of Set and Map together, the docs here are an introduction to the common API functions with examples invocations, along with more structured organization (content broken down by module, which is I think a better structure for this specific type of doc). Currently there's a "Short Example" section at the top of each page, and I think it could benefit by having a longer example at the bottom, something akin to what haskell-lang has. I'll reach out to the authors and see how they feel about folding some of the content into here (or vice versa).
This is really nice, thanks! However, I must say, this (and haskell-lang.org tutorials) are the sort of thing that I *really* wish were in more appropriate locations, like wikis or haddocks. The Pipes library does this excellent thing where there's an entire tutorial module with nothing but haddocks. I think this ought to become standard practice, instead of having tons of different tutorials in tons of different places.
That makes sense. The tutorials are indeed structured rather differently. I do suspect it might make sense to fold some of theirs into yours. Going the other way around seems a bit odd.
Frankly, I think discoverability matters an order of magnitude more than any of the issues you described. If our standardized doc tool is insufficient, we should work on fixing it instead of avoiding it.
Possibly a longer example in map, where you convert ```[(a,b)] -&gt; Map a (Set b)``` 
In an ideal world yes, Haddock would be used for API documentation as well as user guides, but from what I've read it would require a huge amount of effort to make Haddock a good tool for writing this type of content. In its current form, a lot of people choose anything but Haddock for writing guides (see [this](https://www.reddit.com/r/haskell/comments/7heo7k/reflecting_on_haskell_in_2017_blog_pot_by_stephen/dqqz7yp/) discussion for example, and as an interesting data point Haddock doesn't even use Haddock for its user guide). Could Haddock be updated to be as nice as (or better than) ReadTheDocs? Absolutely, but that's going to take a lot of work and its not staffed. If at some point in the future Haddock becomes a feasible alternative then there could be a community effort to move the docs there. I care strongly about good documentation and there's a tool available right now that addresses our needs, so why not use it?
&gt; Could Haddock be updated to be as nice as (or better than) ReadTheDocs? ReadTheDocs docs are either RestructuredText or markdown. Haddock is easily as nice as markdown already - but it will never be a standard documentation language. The solution is for Haddock (the library) to allow haddocks (the docs) to be expressed in reStructuredText, and expose the extra semantics you need for haddocks to be appropriate *.hs documents. It's not too far away from this now and a much easier task than turning Haddock into a general semantic doc language. 
I understand that, but it seems to be that the reality is people just don't _like_to write Haddocks because they're a pain to use. And I completely agree that discoverability is incredibly important, so I think we need a stop-gap: one option would be to add a `Tutorial` module to every package, but its up to the author whether they want to write Haddocks there or simply link to a ReadTheDocs tutorial. The one small downside I see with the `Tutorial` module approach is the unpredictability of the url, you need to first go to hackage.haskell.org/pkg-name/docs and then from there click the Tutorials module, instead of going to haskell-pkg-name.readthedocs.io. It also has the issue of SEO not being very good for it because the links change every version.
The hylomorphism would stay the same for part 2, you'd just need a different implementation of `score` that would calculate the length and strength. E.g. score = (length &amp;&amp;&amp; sum) . fmap score1
To be clear: So you are either fully open to be hacked. Or you get the security fix and then get the performance penalty. Todays estimates range form 17% to 30% performance loss. Some "Gamers" said that framerate is not affected. But Network, SSD, DB access times will suffer.
Disclaimer: I'm a frequent user and contributor of Reflex, so I'm biased. * **paradigm?** Miso is basically the elm architecture. Reflex is pretty hard-core FRP. Miso is more simplistic and forces you to follow some commendable design patterns. Reflex basically gives you event and behaviors and tells you to find design patterns yourself. So the Reflex paradigm is more powerful and less restrictive while Miso is much harder to write yourself into a spaghetti corner with. In my opinion, Reflex pays off here, as its paradigm is much more composable / reusable, and its drawbacks are easily made up for by Haskell being so easy to refactor with. A little documentation on good design patterns in Reflex would go a long way here. * **their internal concepts, e.g. do they use a virtual-DOM?** Miso uses virtual dom, Reflex does not. Reflex just changes individual UI elements when your events or behaviors change, which very often just does what you want out of the box. But sometimes this requires some finesse on the user's part, which definitely makes Reflex harder to reason about. But the advantage is that you're much closer to the DOM without losing much of the abstraction. * **communities?** AFAICT, Reflex has a much more active community and is much more well tested. * **learn-curve?** miso is much simpler than Reflex and easier to pick up. * **server-side rendering?** Both support server side rendering, but miso's is definitely more polished. It takes a bit of know-how to use Reflex's static renderer. * **speed?** Last I checked, Reflex has better performance. Of course this is contingent on that occasional finesse I mentioned earlier. But Reflex is *especially* faster if you're interested in mobile. Reflex can build "hybrid" native apps (i.e. they use a webview as the UI, with native interop), except that the Haskell code is actually cross compiled to the native CPU, making it *much much* faster. To put it in perspective, these native apps tend to be faster on mobile phones than desktops running the JS versions. * **documentation?** Reflex is sorely lacking in documentation. Can't say for miso. * **other aspects?** Since I've used Reflex *a lot* more than miso, this section is going to be pretty biased from me. [reflex-platform](https://github.com/reflex-frp/reflex-platform) provides a really optimal, stable, and easy-to-use environment for building GHCJS and mobile apps with Haskell. This isn't technically tied to reflex, and you could use miso with it just as well, but `reflex-platform` is definitely focused on Reflex (especially Reflex mobile apps). Reflex also has some much more well defined abstractions that allow it to do some pretty magical stuff. This isn't super relevant to most people, but I've seen some cool things implemented because of this. The entire FRP network and DOM framework can be transformed for some pretty crazy custom behavior. This is how the static renderer was built in less than a week.
This is the `foldr`-based generalization of `listFoldrMapM`. foldrMapM :: (Foldable t, Monoid m, Applicative f) =&gt; (a -&gt; f m) -&gt; t a -&gt; f m foldrMapM f = foldr (\x leftM -&gt; liftA2 mappend leftM (f x)) (pure mempty) That works for your example but it doesn't seem very natural to run the continuation `leftM` corresponding to the tail before the action for the head `x`. 
You may be right, but I had an hour to poke at it recently and got the impression there might be some low-hanging fruit there, but maybe relatively few 2x improvements which is what this calls for.
I suggest all nominated packages must either have an existing benchmark (ex. can `cabal new-bench` or `stack bench`) or the nomination must point to such a framework (in a fork, PR, gist, etc). If you want people to efficiently flash-mob something then we best not be duplicative.
Nice, thanks for sharing this.
Thanks for the recommendations.
Thank you! You are right that it seems unnatural to run the competition for the tail before the one for the head, but this is also what foldrM from Data.Foldable does. There might be a variant of foldrM (and similarly of foldrMapM) that performs the monadic effects in the order that one would intuitively expect, but I have never seen such a function discussed.
One thing to add to the `Seq` section would be the patterns that are now exported as of the latest release. e.g. case listObject of [] -&gt; ... x : xs -&gt; ... vs. case seqObject of Empty -&gt; ... x :&lt;| xs -&gt; ... Effectively do the same thing from a logic standpoint, but one uses lists, the other uses sequences.
I'm using now, working great for most of the haskell specific stuff in the post (linting, autocomplete, renaming, etc..) except: * Intero (mainly the REPL functionality) * Vim-hoogle (should be replaceable with a binding to hover in hie) * bindings for running the pointfree/pointful utils on a selection. And some that aren't really needed when running hie: * vim-hindent * vim-stylishask * vim-brittany (I've forked vim-hindent to run Brittany instead)
&gt; paradigm? `Miso` is intended to be pragmatic and is based on the `Elm` architecture. &gt; their internal concepts, e.g. do they use a virtual-DOM? `Miso` has a virtual DOM, it performs diffing and patching simultaneously. It can be mocked for testing, and can eventually be used to diff other kinds of trees (like an `iOS` `UIView` hierarchy tree, `Android` `XML` tree, etc.). &gt; communities? `Reflex` is older and has a bigger community. `Miso` is relatively new onto the scene. &gt; learn-curve? `miso` is basically `Haskell98`. The only type-level knowledge required is when you want to use `servant` for routing and/or pre-rendering. &gt; server-side rendering? Yes, see https://github.com/FPtje/miso-isomorphic-example for an example. Credit to /u/fptje . &gt; speed? I'd say they'd be equivalent when running in the browser. When it comes to DOM manipulation, `miso` might be faster judging by these benchmarks, https://medium.com/@saurabhnanda/benchmarks-fp-languages-libraries-for-front-end-development-a11af0542f7e . But javascript benchmarks aren't always accurate, and are usually contrived to be optimal for a specific scenario, so caveat emptor. On mobile, if you're using a cross-compiled `reflex` application done via `jsaddle`, it would probably load faster than viewing a website in a mobile browser that was built with `miso`. &gt; documentation? Not too good for `miso`, but you can pretty much read `Elm`'s and it will be directly transferrable. There are a lot of examples in the `README.md` as well. other aspects? `Miso` can simulate pure components using monad transformers. Example here: https://github.com/FPtje/miso-component-example
&gt; On mobile, if you're using a cross-compiled reflex application done via jsaddle, it would probably load faster than viewing a website in a mobile browser that was built with miso. Not just load time. GHCJS is a serious runtime performance bottleneck. It's *good enough* on desktop, but I wouldn't recommend using GHCJS for anything nontrivial on mobile at all. Cross compiled Haskell, on the other hand, is actually really fast on mobiles. So interactive apps see a really substantial performance improvement when cross compiled.
Time to benchmark this...
I'm planning to add a section at the bottom of each page with more thorough examples, I'll be sure to include something like this there. Its a little bit more involved than simply a `Map.fromList`so I'm hesitant to include it in the construction section. An implementation off the top of my head would be: ``` foldr (uncurry (Map.insertWith Set.union)) Map.empty . map (fmap Set.singleton) ``` but there's probably a simpler implementation.
I'd agree. The performance bottlenecks in `miso` aren't due to the virtual-dom diffing (they should be), but rather seem to be related to language interoperability (JS -&gt; Haskell -&gt; JS), especially when handling events. Cross-compilation and the ability to access native APIs is ideal, but being relegated to a WebView precludes you from accessing native components (i.e. `UIView`) afaik. While speed might be better, is there a way to allow users to access these native components? 
They're pretty useful for doing things like DOM updates when you have your `Event`s running with push-based FRP and your `Behavior`s running with pull-based FRP. If you just had a pull-based `Behavior` managing your DOM state, you'd have to poll it for updates, and if you just had the `Event` you can't do the fun `Behavior`-based state management stuff. With `Dynamic`s you have the pair of them together, and they stay synchronized by construction, so you can use the `Event`s under the hood to update your DOM and the `Behavior`s for the usual FRP state management. If you push your `Dynamic`s down as far as you can (and use a helper function to deduplicate updates if you break them apart) it can really help to minimize DOM updates. There's also some pretty good ideas in the `reflex` collection management APIs that mean that if you have a widget for dealing with items like `Dynamic v -&gt; m ()` you can build something that efficiently manages the updates for a `Dynamic (Map k v)` with pretty low effort the user. On top of all of that, there is some FRP folklore I picked up on from various places (some reactive-banana haddocks and examples amongst them) around creating `Event`/`Behavior` pairs that are sychnronized, just because that's occasionally useful. I think it's nice to see some of those common things given library support, just as a way of spreading those ideas.
Sounds a bit similar to [exercism.io](http://exercism.io/).
`transient` is kind of the odd man out here in that it isn't often (ever?) included in the comparison between streaming libraries and it seems to want to offer *way* more. That this library isn't widely adopted I can kind of understand, since it promises to be a "revolutionary" way of doing everything differently and furthermore, to quote from the [wiki](https://github.com/transient-haskell/transient/wiki): &gt; Scared by the complexity of the Haskell language? Don't worry, Transient is designed to make you productive from the beginning, since it does things for you. Anyone with a little knowledge of Haskell can use it. Now I'm scared by the complexity of the Transient library! Not to step on someone's toes, but while the Haskell tower of abstraction may be high, I have always appreciated that at least they build up on top of one another. When learning something new, you can make use of things you (and other people!) know and use already. Usually this involves trying to reimplement a (maybe slightly simplified) version, or at least staring long and hard at the types. With `transient` I get the feeling you have to buy-in into doing things "the transient way from now on". While there might be benefits to that approach, I kind of have to be motivated to break with old habits. Particularly, the first few examples in the [tutorial](https://github.com/transient-haskell/transient/wiki/Transient-tutorial) can be done with stuff from `base`, and the tutorial is *very* long, so the proposed merits are not clear from the beginning. If I had actually implemented all the examples it would probably have taken me two hours until I got to the section [Programmer defined State](https://github.com/transient-haskell/transient/wiki/Transient-tutorial#programmer-defined-state). Here the author makes some claims which I can not in good conscience agree with. In particular (check the link if you believe I am taking things out of context): * "Session state is handled like the state in a state monad (so state is pure)" * "The session data management uses the type of the data to discriminate among them (&lt;-- various instances of `Typeable`). But the semantic is the same." * "There is no global state at all." He then proceeds to provide the following example (modified for clarity): data Person = Person{name :: String, age :: Int} deriving Typeable main= keep $ do setData $ Person "Alberto" 55 processPerson processPerson= do Person name age &lt;- getSData :: Typeable a =&gt; TransIO a liftIO $ print (name, age) Notice how * We didn't have to initialize the State in `main`. We didn't have to "run" the transformer, nor did we have to get a "new" reference. That is, we could have forgotten to do so. * We can only get and set State in `TransIO`, which conveniently happens to implement `MonadIO`. To quote the tutorial: "What happens if there is no such data? The computation simply stops." In my opinion, this can only be considered "pure" if you consider `IO` as pure, since `getLine` always returns the same `IO` action. Likewise, if inside `processPerson`I have no way of telling if the state is initialized or not, it might just as well be global. To be fair, looking at the Haddocks I *do* see functions like getData :: (MonadState EventF m, Typeable a) =&gt; m (Maybe a) the type of which makes way more sense to me. But it isn't mentioned in the tutorial. Looking at the issue list I see things like [exceptions silently being dropped](https://github.com/transient-haskell/transient/issues/61). While I do see some nice ideas here, and a lot of good intentions, I think this library is not for me.
It's not *so* bad. Sure, in Haskell there's always five ways to do it, but it's easy to refactor or write a conversion function while relying on laziness for example. You have to get used to just pick *some* reasonable choice to get going, usually there isn't a "best" one. And this wasn't really what I was asking for either, I merely looked at two highly-regarded libraries and noticed they seem to have some issues. The ones I encountered were known for several years, so I asked myself: "Why isn't anybody talking about this? Have people moved on? Surely *some* people must stream concurrently!"
That's certainly my plan ;) Progress is slow going. Not really any farther than [a couple of weeks ago](https://www.reddit.com/r/haskell/comments/7kkfqg/what_are_you_working_on/drf2a8w/?st=jbztdrxg&amp;sh=3b105bef).
If you're a masochist, it's a great choice.
The typeclass definitions are a little dated. For instance, `Monad` isn't `Applicative` and `Monoid` isn't `Semigroup`.
There does not have to be a webview. If you don't use jsaddle, you can just use the C FFI and use the native UI however you please
Good link!
Where can I find more information about the impure part? It sounds pretty obviously bad to me that speculative execution would be bad for code that is performing side effects. Got any links?
That's true too.
Couldn't this a least be mitigated by putting large glaring links to your docs in the Haddocks? I think every package should have Haddocks regardless, but if you want to create a long tutorial, etc. a link should suffice in many cases. Heck you could even have a Tutorial module like Pipes and then have a link there!
Using `stack-static`: `Copying from /home/runkleisli/.stack/snapshots/x86_64-linux-tinfo6-nopie/lts-9.6/8.0.2/bin/cabal to /home/runkleisli/.local/bin/cabal Copied executables to /home/runkleisli/.local/bin: - cabal Warning: Installation path /home/runkleisli/.local/bin not found on the PATH environment variable.` Setting the PATH was an aspect of OS X configuration that changed from version to version, had no official source on the correct way to do this that was kept up to date, with users reposting older information so that dates of relevance got confused, and which could lead to failed builds well down the road if not done correctly. So I mistrust this option.
That's true, but it does mean old versions of haddocks will point at incorrect versions of tutorials. Though I suppose this can be mitigated by having different versions of the tutorials for every hackage release or something.
Now try to be a Python beginner and do anything concurrently at all! ;)
ReadTheDocs has first class support for versioned docs based on the tags in your git repo, see the [stack docs](https://docs.haskellstack.org/en/stable/README/) for an example. There's a version selector on the bottom left.
Seems promising, I'll follow up with you on this.
It's pretty rude to insult the documentation of a package on the basis that some *unrelated* package is poorly documented. The main weaknesses in the documentation of `containers` are, I believe: 1. Functions are not always grouped and ordered ideally, and the documentation sometimes misses good search terms. These problems can make it hard to find the right function without reading through the documentation for lots of other functions. It can also be hard to see at a glance which functions are really central to the idea of a data structure and which are special-purpose extras. 2. The `Data.Graph` documentation is pretty sparse, and tosses around terms like "topologically sorted" without explanation. To be honest, I don't personally know who (if anyone) uses this, or what they use it for, or what makes any of it important, so I'm not the best one to fix the problem. Pull requests will be accepted gratefully. 3. Not all class instances are documented as they should be. I'll get that taken care of as soon as I can.
As one more data point for comparison: [Minesweepers written using: Elm, Reflex, Miso](https://dc25.github.io/myBlog/2017/11/26/minesweepers-written-using-elm-reflex-miso.html)
No offense, but I'm having a pretty hard time taking a recommendation seriously when it's clearly based on less than a few days' experience with any of the frameworks. Do you consider yourself an authority on the matter?
Am I the only one who finds RST verbose and generally mysterious?
I'm sorry about being that unclear, but to me peterb12´s comment reads like he wanted to point out the poor state of the docs of packages other than `containers` that would only provide type signatures.
&gt; Your session has timed out. Please go back to the article page and click the PDF link again. I get this msg when I access the link of Lennart's Paper : (
Updated the link.
this is a great idea (and good documentation effort too) ! Have you written any applications on top of it already? I can't wait to break free of akka (and Scala altogether) at work, and this seems to be a good substitute. 
No offense taken. Which part would you like me to substantiate. I've made two claims: - Miso is easy to get started with - Don't recommend GHCJS for serious production use-cases
The latter. I find it hard to believe that a few days' experience is enough to come to that conclusion.
That doesn't make sense. Do you expect maintainers of *any* production ready project to have zero problems with their project? Facebook doesn't recognize any problems with react-native? Linus Torvalds is 100% happy with the Linux kernel? No, maintainers always have problems with their projects; that's why maintenance is required.
I'm not trying to be in a credentials contest with /u/dmjio or /u/ElvishJerricco, but _you_ seem to be framing the conversation in that way. &gt; testimony based on a few days' experience This is probably incorrect. I have evaluated GHCJS multiple times over a span of 3-6 months over the past 2 years. While not as deep as the other two gentlemen, my experience is definitely more than a "few days'" worth. Reasons why I, *personally*, have a negative outlook on GHCJS: * Second-class citizen in the Haskell ecosystem - it can't be treated as "just another" library/project. There's also this issue about GHCJS probably never getting first-class status because WebAssembly is the way forward (/u/ElvishJerricco had written extensively about this). * Issues with TH &amp; lenses -- any large project will end-up using these * JS size * Load-time * Perf * Dev UX is worse than GHC (but has probably improved since the last time I looked) I'd happily retract my negative outlook, if GHCJS (or any other UI framework using it), can put a "X is proudly built with GHCJS" on their repo homepages (where X is a "serious" production app). If people are succeeding with GHCJS *despite* the problems that I've mentioned above, then my point is automatically invalidated. And I have absolutely no problem with that. In fact, I'll start looking at a migration strategy from Angular =&gt; GHCJS myself. May I ask you why _you_ have a positive outlook on GHCJS? Do you have any success story to share? 
Yeah, I got exactly the same impression.
You are right, I phrased my thoughts wrong. What I mean is that if somebody like /u/ElvishJerricco who is an expert in Haskell has to apply finesse to get acceptable performance with a library then your average "does not publish libraries" Haskell user will likely have a hard time in a production setting. The fact that he is extremely familiar with the library makes this point stronger? And that is only after you take into account all of the other issues that other people mention. As things currently stand, I do not feel like starting a new web project and using reflex-dom+ghcjs for frontend. Of course, there is the fact that I am still a beginner in Haskell (vaguely "understand" monads, still thinking that applicative makes more sense for a lot of problems). btw. No offense /u/ElvishJerricco I am very excited and really appreciate all the stuff you do for Haskell ecosystem, thanks :) 
I'll cut in here. &gt; Second-class citizen in the Haskell ecosystem Though I agree that it's second-class, I don't agree that this is a deal-breaking problem. We use tools every day that are second-class. Almost every library anyone uses is second-class. Even Stack is *kind of* second-class, as it's not actively considered in the GHC dev process (we've seen a couple of Stack bugs crop up recently because of this). &gt; Issues with TH &amp; lenses -- any large project will end-up using these This hasn't been a major issue in practice. We tend to define lenses in modules that don't get in the way of the typical dev cycle very often; TH is kinda slow on GHCJS, but honestly the bigger compile time problem by far is the JS linker in GHCJS; I'm eager to try out some of the new generics and labels based lenses anyway; and finally, the GHC-based dev workflows possible in GHCJS elide TH problems almost entirely (contingent on your framework supporting this, which I don't think miso does). &gt; - JS size &gt; - Load-time &gt; - Perf Runtime perf is one of my two main problems with GHCJS. But it's been *good enough* on desktop; it's mobile where it presents a serious problem. WebAssembly should hopefully help a lot in both cases though, and the plan is to make it easy to migrate from GHCJS to wasm. &gt; Dev UX is worse than GHC (but has probably improved since the last time I looked) I have been working fairly actively on this. I'd say it's almost on par with GHC-based dev UX if you're willing to use Nix. &gt; If people are succeeding with GHCJS *despite* the problems that I've mentioned above, then my point is automatically invalidated. There are plenty of people succeeding with GHCJS. We use GHCJS at work (Takt Inc) to serve fortune 500 companies and have been pretty happy with it. Obsidian Systems uses GHCJS+Reflex with almost all of their clients, and they've got quite a few. I'm aware of one or two other consulting firms with multiple clients depending on GHCJS. And I'm sure there are quite a few companies that I *don't* know about. I've got one pretty good example whose app is out there that I think will impress you, but I'm not sure if they're quite public enough for me to share yet; I'll get back to you on that after I ask about it. &gt; May I ask you why you have a positive outlook on GHCJS? Do you have any success story to share? I've used GHCJS in production at two different companies by now, and have generally found Reflex to be a remarkably productive experience, giving me the ability to develop features and refactors at alarming rates. Other than performance, the problems with GHCJS seem to be superficial or irrelevant compared to the enormous gains you get by sharing code between the frontend and the backend, having access to the entire Hackage ecosystem, getting Haskell's powerful abstractions and runtime model, and more. And I'll stress that code-sharing advantage, because I think people underestimate *the order of magnitude* of benefit it gives you. You know how we use type systems to rule out entire classes of bugs and make refactoring a breeze? Code-sharing has a similar effect.
Great news actually, thanks for letting me know!
Yea I'm not currently making an argument for or against GHCJS. Just trying to point out that having written a few benchmarks is not enough to base a recommendation on I don't think.
&gt; Almost every library anyone uses is second-class. Even Stack is kind of second-class, as it's not actively considered in the GHC dev process (we've seen a couple of Stack bugs crop up recently because of this). This *is* a problem. I hold my position that certain core components should be elevated to "first class" (or "blessed") status within the Haskell ecosystem. It will be *much* better for overall Haskell adoption if this happens. I guess it's a philosophical difference and can't be settled over a Reddit debate. &gt; There are plenty of people succeeding with GHCJS. My I request you to please raise a PR to the GHCJS repo and make these successes visible? A blog post about how you're using GHCJS in production would also help (unless you've already done that and I haven't come across it). Try googling for "who's using (elm|ghcjs|purescript|react|angular|vuejs) in production" to understand where I'm coming from. &gt; I've used GHCJS in production at two different companies by now, and have generally found Reflex to be a remarkably productive experience, giving me the ability to develop features and refactors at alarming rates. Other than performance, the problems with GHCJS seem to be superficial or irrelevant compared to the enormous gains you get by sharing code between the frontend and the backend, having access to the entire Hackage ecosystem, getting Haskell's powerful abstractions and runtime model, and more. And I'll stress that code-sharing advantage, because I think people underestimate the order of magnitude of benefit it gives you. You know how we use type systems to rule out entire classes of bugs and make refactoring a breeze? Code-sharing has a similar effect. I'm not going to debate with you on this point, because this is precisely what I've been looking for. We *thought* we could achieve this with a Typescript frontend and Haskell backend, but that isn't going too well. We are about to do a PoC in Purescript next. Unfortunately for me, GHCJS doesn't inspire that confidence. If given the choice between "messy, but will surely work - here are others who are using it" vs "probably awesome - but god knows what's going on here", I'll always choose the former.
&gt; If I may ask, what exactly did your "evaluation" entail that amounts to 3-6months of experience? _Evaluating in gaps of 3-6 months_ - not claiming to have 3-6 months experience! There are two open-source trails (apart from the throw-away projects that don't exist now): - https://github.com/vacationlabs/haskell-webapps/tree/master/UI - https://github.com/vacationlabs/js-framework-benchmark/tree/master/miso-non-official-v0.4.0.0-non-keyed
It might be possible to make this work, but I burned days trying to do so. The difficulty of debugging the resultant deadlocks made me give up on the library in the end. It was a shame because the API felt right.
At the risk of being told "whoosh": leftovers and detecting upstream finalization are IMO much larger differentiators, which is why finalizers is the one that's on the chopping block.
Thank you very much! I'll check it out
This is more interesting than I first thought. We have a way for downstream to detect upstream termination via “await” returning an Either. So we could have “yield” do the exact same thing for upstream to detect downstream termination, and allow it to clean itself up appropriately.
Thanks. Took me quite a while to figure that out properly. IIRC there's still some issues with `cabal repl` and/or `stack repl` though. There's a PR for it.
Isolation is a thing you can prove formally, they did it for sel4, which guarantees *very* strong isolation. ...if, and only if, the semantics of the hardware can be relied upon...
&gt; run the competition for the tail before the one for the head Mobileo for "continuation"? 
Is there any documentation on how to build mobile apps with Reflex?
&gt; I'm fairly sure the blackboard font is actually handwriting. It's either a handwriting font or someone with extremely regular handwriting. You can see certain letters look identical - the 's's in "We want to prove things in their most general form to allow reuse" are all identical for example. There does seem to be variations of each letter though.
&gt; Miso is basically the elm architecture. Reflex is pretty hard-core FRP. Miso is more simplistic and forces you to follow some commendable design patterns. Reflex basically gives you event and behaviors and tells you to find design patterns yourself. So the Reflex paradigm is more powerful and less restrictive while Miso is much harder to write yourself into a spaghetti corner with. In my opinion, Reflex pays off here, as its paradigm is much more composable / reusable, and its drawbacks are easily made up for by Haskell being so easy to refactor with. A little documentation on good design patterns in Reflex would go a long way here. Curious: would it be possible to write some kind of framework on top of Reflex to support the elm architecture? Then allow you to break out of it if needed? That way you could get the best of both worlds - consistency, simplicity and mindlessness most of the time, and a high level of expressiveness when you really need it.
&gt; Obsidian Systems uses GHCJS+Reflex with almost all of their clients, and they've got quite a few. I believe [wrinkl](https://www.crunchbase.com/organization/wrinkl) is one of them (see [reddit post](https://www.reddit.com/r/haskell/comments/5ebj8q/haskell_opportunities_at_wrinkl_inc_in_nyc/)). They have a Slack-like UI using GHCJS.
Is there a nice way to skip tasty's spec? In JS World usually if u have `testGroup ...` to run a group, you'd have `only $ testGroup ...` that would skip all other tests. It is nice feature when you are developing, and don't want to comment out other test groups. Is something like that in Tasty?
Not a showstopper, but one of the things that makes Haskell difficult to start with. The paradigm, non-strict evaluation and language is different enough from what many are used to to make it a very steep learning curve. Add GHC extensions, category theory, advanced type-level stuff in use by popular libraries on top of the myriad of choices in Hackage, and you're facing a tough climb. I'm 1.5 years in learning Haskell (self-study on evenings), and I'm still a long way from understanding important and much used parts of the language (+extensions) and libraries.
That doesn't sound like it would be too hard. The hard part would be either creating vdom or avoiding the need for it
The [project](https://github.com/reflex-frp/reflex-platform/blob/develop/docs/project-development.md) docs touch on this.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [reflex-frp/reflex-platform/.../**project-development.md** (develop → 23862b7)](https://github.com/reflex-frp/reflex-platform/blob/23862b7390a2f95b18084473780dd364ea97872b/docs/project-development.md) ---- [^delete](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply ds6qrow.)
I'm not sure what the state is with miso, but it's pretty good with reflex. Reflex supports building with GHC, so you can just enter a Nix shell with GHC instead of GHCJS and use any tools you'd typically use with GHC. Can't say it makes sense to use Stack though, so for editor integration, you'll want to use dante (which works like a charm)
&gt; A mix of libraries that do the same would have probably a lot more complexity. I think that might be true, and maybe it's a matter of taste if you want an all-encompassing framework or want to use separate libraries. I hope I didn't come off as a complete asshole in my previous comment. You're completely right in that the issues I saw have workarounds which you could / should use. It's merely that there are some chosen defaults at work here which I don't like. Other people might. &gt; I guess that the state is pure as is in the case of a monadic stack which include state and IO. the state of the monad transformer is pure besides that the stack can perform IO also. If I find myself in `StateT s IO`, I can use the functions get :: Monad m =&gt; StateT s m s put :: Monad m =&gt; s -&gt; StateT s m () which make no assumption about the underlying monad. Hence I *know* that I won't accidentally launch missiles by using them. This is clearly not the case for a `getSData` that lives in `TransIO` and kills my thread. I was half-serious when I meant that you *could* consider `IO` pure, since after all evaluation is not the same as execution, if you think there is some practical merit in thinking about things this way. But in my opinion, the author's claim that "the semantics are the same" is not true. And for a *beginner*, the life of which is supposed to be made easier by this library, things might just be all the more confusing. Anyways, this is just a *personal* opinion after checking out the library. I didn't say it's inherently *bad* and in fact I upvoted your recommendation since it is an honest option. I am just not quite convinced at this time.
Admittedly that is still one of the things I ponder about, but regardless it would involve network communication, so I see the problems as nicely split between the - client side: needs a way to hide that it might do a synchronous network call, and - server side: needs to run the function it gets and return the result back. One way might be to look at CloudHaskell and serialising whatever needs offloading, placing a routing interface in front on the server part that takes the function to call (just a string) and the serialised arguments. I mean, it's certainly not an unsolvable problem, it's just a matter of what way is less tedious - there's a lot of material on mobile offloading, but all in imperative languages, which is why I'm interested in how to do it in a pure language.
Just to put a voice out there: I built a 10kloc CRM in Reflex+GHCJS for a client in about 2 months. They are very happy with it and I'm also quite pleased with the result. Code reusability is incredible. I was able to implement multiple features in almost no time flat because I could just pull in Hackage. I will admit that performance on mobile is by far the greatest down side. I have not tried the static renderer yet, which would be the next step in improving that. EDIT: To be clear, using any Haskell-based web front end is going to be more "painful" initially because the ecosystem is just a lot smaller. You have to be willing to pull things along a bit on your own. But I think it pays for itself rather quickly.
This seems to be related to [Cloud Haskell](https://haskell-distributed.github.io/tutorials/1ch.html). However, I don't understand why you're so insistent on trying to hide IO, what would you do if you had network issues? If you're doing IO, especially IO that could and (likely, will) fail, it seems like you shouldn't attempt to hide that behind `unsafePerformIO`. 
Maybe "the semantics of X are the same as Y" has to be made more concrete then. When I read that, it appeared to me that this is supposed to mean "the semantics of using the functions I'm about to show you are the same as you'd expect by some `Monad m =&gt; StateT Person m`". You're right that I can *choose* to make `get` and `put` do unorthodox stuff, but that's a deliberate choice then. Usually people build monads out of transformers. At least, a function living in `Monad m =&gt; MonadState s m` can't decide to add it's own missile launching code (bar using `unsafePerformIO` of course). You're right that type signatures aren't expressive enough to guarantee everything we want. Still I would expect some "common sense" laws to hold. For instance, I would expect things like get &gt;&gt;= put = return (), though this is not an "official" law, but something I *believe* most Haskellers would agree with. Maybe `getSData` doesn't kill my thread, I haven't read enough about `transient` to be correct on a detailed technical level. Still, it's *weird*.
re: generic-lens stuff do Generically derived instances feel fast enough on ghcjs, when they're compensating for template Haskell by working harder?
What do you mean by working harder? I'm told they produce exactly the same code as TH lenses after optimization.
Haskell Haskell Haskell!
Just that using less template-haskell requires more default-via-generics instances, e.g. FromJSON. 
Just that using less template-haskell requires more default-via-generics instances, e.g. FromJSON. 
Well with generic-lens specifically, this is fine, since it's the same runtime perf and better compile time. I've heard JSON suffers in runtime perf when deriving via generics, but I haven't seen a benchmark showing this and you don't *have* to use it just because you used generic-lens
**&lt;HASKELL APPEARS BEFORE YOU&gt;** **- WHAT IS YOUR REQUEST?**, he says.
((.)$(.)) ((.).(.)) flip . (. flip id) foldr (+) 0
Uh o_0 I had /never/ heard of this one before. (It just appeared last month in fact)
I really wasn't expecting Haskell to pop up. A couple of days ago there was a C reference in a chant: &gt; “Ent mayne, par an the seas arg sea, char twin stars arg thee, par an thesis bracketa…” &gt; &gt; It felt good to speak the true tongue once more, the one that bent the mind, and drove daemon cultists to caffeine and insomnia, pouring over ancient texts time and again, to ensure that they had the commands inlaid correctly. Also the dungeon core IDs can be decoded to ascii.
If you want restrictions on what can be done, you can just create a newtype wrapper around IO.
Interesting, reminds me of Haxl by spj ( https://code.facebook.com/posts/745068642270222/fighting-spam-with-haskell/ )
Thank you for the detailed explanation! I think I need to go back and try Reflex some more for it all to sink in, but it makes sense conceptually. Could you just please give an example of "the fun Behavior-based state management stuff"? I'm not sure what you mean there.
You could use MTL style / free monad techniques to separate your desired effect from it's implementation. Additionally, I'd say if you have a logic about which node your code runs on leaking into all your application, maybe that indicates an architectural problem? 
I guess I projected too much on `capataz`. Will try it out soon anyway, thanks for the clarifications!
I apologize if this is a trivial question, but why does Const have a monoid constraint in its Applicative witness? I think I understand its usefulness, but how did it came about?
The fun stuff I'm talking about is mostly to do with the `Applicative` instance. It's pretty nice being able to build up `bAdmin :: Behavior t AdminOptions` from an admin page, and `bPrefs :: Behavior t Prefs` in another page, and then pass `bContext = Context &lt;$&gt; bAdmin &lt;*&gt; bPrefs` down through your app, pulling bits out with `fmap` as you go if/when you want to limit the scope of what your widgets can access. That gets particularly fun with `Traversable` - if you had an app for interfacing with reddit, going from `subredditPrefs :: Map (Behavior t Prefs)` to `bPrefs = Prefs &lt;$&gt; bMainPrefs &lt;*&gt; sequence subredditPrefs` is something that is handy fairly often. Normally I'm doing this with `Dynamic` instead of `Behavior`, so I'd pull things out with `fmap` and `holdUniqDyn`. If you rig things just right, it means the `Event` that fires when you change a setting would be wired up to tiny pieces of JS hooked into the DOM that would cause the minimal updates to text / attributes pretty promptly. I've got a series of posts on `reflex` [here](https://blog.qfpl.io/posts/reflex/basics/introduction/) if that is helpful, although I've got a few more to go yet, on component design and collection management.
With Applicative operating over the second type parameter, pure :: b -&gt; Const a b needs to be able to create values of type a (and does so with mempty). It’s a similar story for &lt;*&gt;.
A couple of complex examples would be better before drawing any conclusion in your article; the current ones are too simple.
There's time, tz, tz-data, and timing-convenience that you'll have to use in tandem for extensive time related operations. 
[Does surveymonkey pay per view?](https://www.reddit.com/user/qazakian/overview/)
Blogspam?
(presumably riffing off [Hastur](http://kinginyellow.wikia.com/wiki/Hastur) and *The King in Yellow*.)
I might be wrong one this, but I think because those functions are so generic, the solution that type checks is the only correct one. But point taken, I'll make some tests for those problems anyways! I was also thinking of trying to set up exercises for some popular libraries like aeson or wreq.
Whenever I read anything about vim setup for haskell, it always has a bunch of features that I don't think I would want, but never has any of the things I do want, or do have but as a homebrew semi-reliable hack. For instance, for me ^p works fine for completion, and I still prefer it even after experience with IDEs (it's faster, works when there are syntax errors, and I often want to complete inside comments). I tried ctrlp.vim, but... using buffers and tab completion just seems faster, since it's limited to my "active set." For the ghci stuff, it seems faster and more conveient to just run ghci in the next window. I'd like a case expand or an "extract function/lemma" ala Idris, but hopefully faster than in Idris, which is so slow I tend to just type it out instead. Meanwhile, what I would like, but no one seems to have is: Better find tags ^] logic: Try searching including dots, then look for 'import ..Y as X' and rewrite a X.something as Y.something and search, and then try without dots. Peek at a tag: Look up a tag, and display its destination line in a temporary box, plus/minus several context lines. String formatting: Automatic formatting for \ continued or "unlines" style string literals. E.g. have 'gq' on "abc\ \ def " or [ "abc" , "def" ] wrap to 'textwidth'. Someday I'll get around to implementing thse, if vim can even support that, but meanwhile does anyone have anything like any of these? Also I have keystrokes that swap around things, for instance ^n swaps the current word with with the next element in a comma, space or -&gt; separated list, dependent on context. It understands parens, so I can juggle argument lists around. I use this a lot but it seems like other people's vim configs don't include doodads like this? I ask because maybe they do, and if they do, maybe they do it better?
Unfortunately the study is locked behind a pricey paywall: "The report is available for individual purchase ($499 USD)." 
Do you have any ideas or examples I could link to?
It appears that for whatever reason it worked this time, and yes the curl also works on its own. I appreciate the support, I'll let you know if I run into any further issues!
I don't really see the point you're trying to make. Nobody is using Java for 'mathematical computing'. 
Given the fact that the composition of writer and reader are reversed and there isn't a distributive law to be had to collapse things down here, I'm going to go out on a limb and say that I predict you'll be able to find a counter-example that shows you are breaking the laws. In general these parts tend to only fit together in a law abiding manner one way.
What does Request/response mean ? Not able to find any related sources in Conduit docs also.
I would say that's a bit of an exaggeration. One does not always need to write highly optimized numerical code. For example, I know many people that have a REPL open at all times to do some symbolical computations or quickly prototype an idea. I can not imagine doing that in Java. The "operational burden" is just too high to a point where the maths is hardly recognizable. Performance is not always the main concern. Sometimes productivity and expressiveness are more important. 
Maybe "mathematical computing" was a misnomer. Is there a better term for it? The idea was to compare the imperative and the functional programming models for translating mathematical functions into code and I think that the functional model is a better fit here. Simply because of the way you express yourself in that model.
AFAIR, TAPL provides an example for typing `fix` with the help of mu in the otherwise strongly normalizing STLC. Not really an answer to your question, but the consequences of that might obviate finding an answer to your question.
Great to hear it! Maybe it was a transient network issue; I've seen those from time to time. All this stuff is hosted on S3/CloudFront, but it's really not as reliable as I would like.
I thought this was going to be about an emacs mode adopting Agda's colour scheme to highlight unsolved metavariables (aka things where you need to explicitly pass implicit arguments).
Sounds like startup speak for "we're not making a new market so we're less likely to die!"
Monad laws are testable as long as the type constructor has "testable equality". https://gist.github.com/Lysxia/ce18a428d2f1b60cabbcd7d9efa67941 No counterexample found so far.
I don't know too much about QuickCheck, so I don't really know what I'm looking at, but the fact that you ARE able to quick check monadlaws is really cool! I did try to look into that, but what I found required an eq instance for the monad, something which isn't possible due to the function. I could have good use of this sort of wrangling with QuickCheck in the future. I'll look into how it works.
Note that the composition of the `m -&gt; a` component doesn't use the second component, and the second one only uses the first applied to `mempty`. If we pull that element out, we get data Prescient' m a = Prescient (m -&gt; a) (m, a) - `Prescient' m` is a monad: [the `Product`](https://hackage.haskell.org/package/base-4.10.1.0/docs/Data-Functor-Product.html) of the reader and writer monads. - there is an injection inj :: Monoid m =&gt; Prescient m a -&gt; Prescient' m a inj (Prescient f m) = Prescient' f (m, f mempty) - `inj` is a bind-return-homomorphism (we don't yet know that `Prescient m` is a monad, but we can check that it commutes with bind and return). From that we can deduce the monad laws without having to write them in terms of the implementation of `(&gt;&gt;=)` and `return` (the checks are all in the second point above), and thus `Prescient` is a monad.
But this DOESN'T mean Prescient is isomorphic to Prescient', right? You have an injection from Prescient to Prescient', but you can't transform Prescient' to Prescient without loss of information. Keeping in mind that you've now shown that Prescient is a monad, does that mean that there still some interest to be had about it?
The main idea for testing with types containing functions is very simple! Although functions are potentially infinite objects, a counterexample is witnessed by a finite (and often small) subset of inputs: - two functions are not equal if they disagree on some input, so we can hope to find a counterexample by just trying a random sample of inputs, I call that *testable equality*, here denoted by `(=?)` (as opposed to *decidable equality* represented by `Eq`) and it extends to just about anything (even `IO` if we can duplicate the world...); - since the part of the domain that matters is finite, we can shrink a counterexample function to a piecewise constant function for which there is an easily showable description. 
Depends on what 'disruption' means. If the current situation, whatever the context is, is seen as ossified, dysfunctional or at least suboptimal (e.g. legacy spaghetti code or corrupt/unjust/inefficient laws and institutions), disruption offers the chance that things can change and improve (by refactoring, reform, revolution or otherwise challenging the existing way). If the current situation is seen as acceptable and workable, then nothing needs fixing and disruption is seen as fixing what is not broken entailing unnecessary risk and damage. They didn't explicate what exactly they want to disrupt, though one can imagine the possibilities. If it's just about 'disrupting' your contemporary competitors with ideas very similar to yours, then it's just plain old power struggle.
That would be a great chant to drop into one of those science fiction stories about how humanity has lost their ability to control technology, but they still depend on it. Or as part of Asimov's Foundation trilogy, when the First Foundation is running nuclear reactors for the stars around them that the other cultures can only understand through the lens of religion.
&gt; The original idea for the article came up after seeing the implementation of a convex hull algorithm in both, Java and Haskell. E.g compare this with that. But that's another straw-man argument - it's not a like-for-like comparison. The Java code is pre-Java8, written is a very verbose "old-school" style. It could easily be rewritten far more concisely - I can rewrite the Kotlin example in Java and it takes ~60lines.
I tried my hands at proving the instances lawful (using Isabelle/HOL), but it is still missing the associativity monad law, for which I unfortunately don't have the time right now. Anybody proficient with Isabelle, feel free to continue on it (or clean it up, as I'm kinda rusty): http://downthetypehole.de/paste/s47PuSqK
FYI this job pays £31K/year.
Holy tamales! I recall someone posted slides here where one of them said, paraphrasing: the state of the software industry is that sellers don't know what they're selling and buyers don't know what they're buying. This "study" seems to fit that sentiment exactly.
The "change the world" mantra regarding any startup is clearly bullshit. The eating is just as distasteful.
I don't know. Just looked up a [convex hull implementation in Kotlin](https://github.com/bmaslakov/kotlin-algorithm-club/tree/master/src/main/io/uuddlrlrba/ktalgs/geometry/convexhull). Not impressed. There's just so much non-problem related code in there that I still prefer the Haskell version. You might say that this is not a like-for-like comparsion either, but then I would love to see your Kotlin version, too. ;-)
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [bmaslakov/kotlin-algorithm-club/.../**convexhull** (master → be9300c)](https://github.com/bmaslakov/kotlin-algorithm-club/tree/be9300c5d305f2fc3a94a5f8bd8caa29908e31d8/src/main/io/uuddlrlrba/ktalgs/geometry/convexhull) ---- [^delete](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply ds87tov.)
Indeed they are not isomorphic, but the injective homomorphism means that if you use it to lift all `Prescient` actions to `Prescient'` and only use monadic operations otherwise, then there is no information created that would be lost in the trip back `Prescient' -&gt; Prescient`. In terms of novelty, it seems similar to "time-travelling" abstractions like `tardis` and `monad-chrono`. There is probably a small algorithmic problem where it is a perfect fit, but I can't find much to say about it. One perhaps interesting remark is that if we derive this structure from a monad product, just the addition of `runPrescient` results in this nontrivial behavior.
**Disruptive innovation** Disruptive innovation is a term in the field of business administration which refers to an innovation that creates a new market and value network and eventually disrupts an existing market and value network, displacing established market leading firms, products, and alliances. The term was defined and first analyzed by the American scholar Clayton M. Christensen and his collaborators beginning in 1995, and has been called the most influential business idea of the early 21st century. Not all innovations are disruptive, even if they are revolutionary. For example, the first automobiles in the late 19th century were not a disruptive innovation, because early automobiles were expensive luxury items that did not disrupt the market for horse-drawn vehicles. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/haskell/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^| [^Donate](https://www.reddit.com/r/WikiTextBot/wiki/donate) ^] ^Downvote ^to ^remove ^| ^v0.28
Absolutely -- I was mostly commenting that I'd never used finalizers myself.
Wow streamly looks awesome. And I thought the streaming space had been explored. 
flip (foldr (flip (.) . (*)) id . enumFromTo 1) 1
At the very least it's something you could use to help your case with management.
You need a good definitive book for Haskell. Just telling people to read the docs isn't going to cut it. I love Programming in Haskell by Graham Hutton but it does focus less on real-world stuff. The Haskell Book by Allen and Moronuki is a good book for beginners but unfortunately is almost as big as the C++ specification! A good compromise between the two that's at most 400 odd pages would definitely push beginners to explore Haskell more and not start and give up soon thereafter (as I suspect a lot of them do, I myself have done post Graham Hutton's book). Lipovaca's LYAHFGG is cute, but not really well-balanced or well-paced, and the cuteness becomes grating after some time. I wager the average serious beginner does not want to be treated like an imbecile. In this regard, I would say that Jim Blandy's book, "Programming in Rust" is a perfect example of how to structure a book for beginners to intermediate developers.
Thanks for putting this together, I've been looking for a guide like this! I've been using a poor man's version of this using sandboxes: some-project-using-primitive$ cabal sandbox init some-project-using-primitive$ cd .. &amp;&amp; git clone https://github.com/haskell/primitive.git &amp;&amp; cd some-project-using-primitive some-project-using-primitive$ cabal sandbox add-source ../primitive This is _much_ better.
More than advertising, we need better documentation. I've been mulling over doing something like [Haskell package Attack](https://www.reddit.com/r/haskell/comments/7nq4kl/haskell_package_attack_january_2018/), but for documentation, but keep scaring myself out of it. Idea would be that we would finally start writing better docs for packages in Haskell instead of simply talking about how documentation is missing. As a bonus, beginners could contribute, rather than needing to be an intermediate Haskeller. Briefly comparing the two videos, i'm not sure if having multiple of that type of video would add any value, unless perhaps each targeted a different audience. Something like "Why Haskell as a (Developer|Employer|Investor)" type of thing.
How could you make different color areas mix? I deem that to be one of the most interesting aspects in watercolor paintings.
I think your problem may be that ghc-mod and ghc 8.2 don't play well together. At least that was the case 3 months ago. See this prior comment: https://www.reddit.com/r/haskell/comments/7octd9/setup_haskell_environment_on_arch/#form-t3_7octd9d7t
So I’m learning Haskell now. I came from clojure and am learning it so I can speed up my side project. I love Haskell so far but I have a couple of gripes:: 1. The docs are all over the place and frequently use academic papers as how to manuals. 2. The vocabulary isn’t yet integrated with other programming languages. Think: monads are like __ in this other programming language. Break it down to something dead stupid and half correct and then work them up to a full understanding. 3. Make the docs free. No more “I like book X” 4. Ok so this one is just dumb but I still haven’t gotten used to the camel casing function names and many of the function names in the libraries are non intuitive. Maybe explain the code design guidelines in a way that convinces me that it’s better than what I’ve used in the past. Other than those minor issues, I’m happy with it so far!
Yeah... But the arch packages only have 8.2. Should I just install stack as static package and install ghc and cabal with stack?
Haha, believe it or not, I wholeheartedly agree with you and was actually aware of the [dangers of the word "we"](https://www.youtube.com/watch?v=o4lOeu-P_2Y#t=18m56s) when I used it above. When I say "we", don't take it too seriously my friends :) 
I am not a stack expert, but you can install versions of stack that will support earlier versions of ghc. They install their own local version of ghc which is sandboxed from your system wide installation. This way you can keep your arch 8.2 version, but you will have to either read their pages ( https://www.stackage.org/ - which shows lts versions for older ghcs) or wait for a stack user to log on and advise you on how to let ghc-mod find your stack installed version and use it. Good luck. And don't feel stupid, getting a haskell environment set up is hard if you don't have an expert sitting in the room helping you. I don't, and I feel your pain. 
From a beginners (naive) perspective: I love Haskell and from a certain point of view it's by far the coolest language I've encountered. I still don't have any clue what to do with it though.. All beginner material I've seen introduces you to purely abstract concepts without any context. After three months of university I now know how the compiler evaluates my code, how to fold containers and about monad laws. But give me any simple programming task and if it wasn't purely abstract problem solving I wouldn't know how to do it in Haskell. In most other languages you will start working on simple real world examples very soon, leading you to more complex applications with actual use. I'm pretty sure I won't stop learning and eventually end up using Haskell, but the existing tutorials and material (I also have a book) definitely makes it very hard.
Exactly; I was commenting generally about the state of the Haddocks, not necessarily containers specifically. If you want an example you can almost pick at random, but I think the Data.Vector documentation is a pretty good example of inadequate documentation. Very few examples, most functions are documented by a type signature, and a bare sentence reiterating the function name and mentioning its run-time. 
I think the following should work: 1. Download the stack binary. Put it in `~/.local/bin` 2. You may have to do what is described in [this link](https://github.com/commercialhaskell/stack/issues/2712#issuecomment-347702380) 3. Install the global ghc, if you so desire, using `stack setup` 4. (Optional) Use `stack path --bin-path`(IIRC) to adjust `$PATH` so that you have ghc available without using `stack ghc`. I've not tried this with the latest ghc, but I've talked to a few people that had problems with the latest(including a ghc panic), so maybe adjust the global `stack.yaml` to use an older snapshot that uses previous ghc. The downside of this is that neither stack nor ghc are managed by your package manager. This isn't an issue in practice, however, since it's very easy to remove both. Just remove `~/.stack`. Also, it makes sense to think of the compiler as a dependency of your project, so, from that perspective, having a global ghc to use for all your projects doesn't make sense.
Please no. Buzzword marketing is not a good long-term strategy. This isn't what anyone wants to hear, but [the way you advertise groups discriminates between people based on their dispensation, resulting in massively divergent group dynamics](https://www.pitt.edu/~bertsch/Carnahan.pdf). Don't get me wrong - I want new people too, but I want new people who are interested in our ideas, not people who came because they saw an ad full of buzzwords to sell to a corporate bureaucrat.
Disrupting labor rights, extremely innovative
Good to hear! I hope all of this gets fixed soon since the haskell story on arch is just a massive shitfest, and has been for a while. If you're running into more errors, don't hesitate to post them. 
java 8 pseudo-fp is okay... too bad my last place was stuck on java 6. 
Speaking of Hastur... https://1d4chan.org/wiki/Old_Man_Henderson
We don't need to but that doesn't mean we shouldn't.
Agreed that we do not need to sell Haskell, but we do want Haskell’s adoption to grow. The question is: in what ways can we contribute to Haskell’s growth and adoption? Creating videos, blog posts, etc. highlighting the benefits of Haskell certainly won’t hurt. Personally, I think that the biggest thing Haskell has going against it is the perception that it is mainly a language for teaching concepts but not viable in practice. I think the best way to combat that is to build interesting applications with Haskell.
Recently started learning Rust and am so impressed at the freely available online learning material. In particular the open source book on GitHub: https://doc.rust-lang.org/book/ https://github.com/rust-lang/book
As a Haskeller who feels comfortable with the basics but wants to take it to the next level, I would love a cookbook style resource. Simple short examples of how to achieve common tasks. The Rust equivalent: https://rustbyexample.com Which is one of a few freely available learning resources listed officially under "Learning Rust": https://www.rust-lang.org/en-US/documentation.html
Agreed. There really isn't any "dead stupid" explanation of monads, because it is a fairly sophisticated concept that doesn't have a clean analog in most other languages.
Seems the antithesis of _"Avoid success at all costs"_.
Scratched out a proof of the associativity law but it's kind of tedious to rewrite out all the reductions. `join . fmap join` and `join . join` both reduce to (I flipped the monoidal value and the function cuz it was convenient at the time): ``` getM :: Prescient m a -&gt; m getM (Prescient m _) = m getF :: Prescient m a -&gt; m -&gt; a getF (Prescient _ f) = f doublejoin (Prescient m f) = Prescient (m &lt;&gt; getM (f mempty) &lt;&gt; getM (getF (f mempty) mempty)) (\x -&gt; getF (getF (f x) x) x) ```
This is really great info! Thanks! &gt; The usual cause is that patches in the head.hackage repo apply to unmodified tarballs but the nix build process tries to apply patches after patching the cabal file to take into account package revisions. It would probably be a good idea for "the nix build process" (I'm guessing `sel.callHackage` in particular) to be extended to specify the exact revision you want, rather than just pulling the latest revision. The mutability introduced via revisions doesn't play nice with the "purely functional" notion of Nix.
Yup, seems to be the case.
This seems to line up with the behavior of how https://github.com/ekmett/streams/blob/master/src/Data/Stream/Infinite/Functional/Zipper.hs#L130 compares with https://github.com/ekmett/streams/blob/master/src/Data/Stream/Infinite.hs
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [ekmett/streams/.../**Infinite.hs** (master → 4a49079)](https://github.com/ekmett/streams/blob/4a49079269e418ad37e7b72b3f2c1424e0c6edd8/src/Data/Stream/Infinite.hs) * [ekmett/streams/.../**Zipper.hs#L130** (master → 4a49079)](https://github.com/ekmett/streams/blob/4a49079269e418ad37e7b72b3f2c1424e0c6edd8/src/Data/Stream/Infinite/Functional/Zipper.hs#L130) ---- 
Interestingly, assuming that the `(&gt;&gt;=)` associativity and `(&lt;*&gt;) = ap` results check out, this gives a `Monad` for `Store` that is compatible with the `Applicative` I already export in the `comonad` package. If I had to guess why it seems to work, I'd say that the normal way of distributing (,) m over (-&gt;) n needs a comonoid on one or the other, which we always have, whereas here we need a monoid, and are given one. That needs quite a bit more rigor before I'm comfortable with it, though.
If you want to write mathematics, write mathematics. You wouldn't be implementing it on a computer if you wanted to write mathematics.
If you want to write mathematics, write mathematics. You wouldn't be implementing it on a computer if you wanted to write mathematics.
May I show you a few examples of applications you can build with Haskell? Very simple ones: - [JSON EDSL](https://gist.github.com/soupi/c7c94a45d006bc70f3b896f327ea47a3) - [File reader, a bit similar to `less`](https://gist.github.com/soupi/199a16be6e2071c3b724) - [Compiler from S-Expr language to JS](https://gist.github.com/soupi/d4ff0727ccb739045fad6cdf533ca7dd) and the accompanying [tutorial](https://gilmi.me/post/2016/10/14/lisp-to-js) - [Stack language interpreter](https://gist.github.com/soupi/86effe7a8dd5554555eeef8ec357d9b9) Bigger ones: - [Image album server](https://github.com/soupi/imgs) - [Static blog generator](https://github.com/soupi/hen) - [Dynamic blog using the filesystem](https://github.com/soupi/hablog) - I use this for [my blog](https://gilmi.me) - [A self hosted website for announcing group events](https://github.com/soupi/gathering) - You can see it in use [here](https://gathering.purescript.org) - [A CHIP-8 emulator](https://github.com/soupi/chip-8) - [A toy chat server](https://github.com/soupi/msg) - [An interpreter](https://github.com/soupi/pureli) - [A compiler](https://github.com/soupi/nyanpasu) (this is still very much a work in progress) 
getM and getF are 'pos' and 'peek' respectively from Store.
it says starting salary.
When people really want a free resource on Haskell I refer them to the [Haskell wikibook](https://en.wikibooks.org/wiki/Haskell).
&gt; Nobody is working on Idris full time, and we don’t have the resources at the moment to polish the system on our own. Last I heard no one was working full time on GHC either.
On the unrelated note: perhaps switching to a different IDE tooling would be better? Intero or HIE. 
Really? I didn't know that. Do you think Idris could eventually become production-ready even without full-time workers?
I don't know any Idris so I can't tell. However, PureScript did become production ready without full-time workers.
Curious what is untrue about favouritism of mentioned languages by companies which fund their development and/or build their infrastructure upon ? What is untrue about questioning generalizing "we" as with no representative value because quite unfounded ?
[removed]
Thanks a lot, I will definitely look into that!
No, we need more and better libraries and documents.
Also, there's [this list](https://www.reddit.com/r/haskell/comments/6uoys8/where_are_the_simple_lets_build_with_haskell_posts/) of project oriented tutorials using Haskell, and [1](https://joyofhaskell.com/), [2](https://lorepub.com/product/cookbook), [3](https://intermediatehaskell.com/) intermediate-topics oriented haskell books in the making. Hopefully the situation will get better soon enough!
I've been looking at [Control.Lens.Zoom](https://hackage.haskell.org/package/lens-4.15.4/docs/Control-Lens-Zoom.html) and was wondering how I could use something similar with a prism, where we want to generalise some `MonadReader (Maybe LocalSumtype) m` into `MonadReader GlobalSumType m` where I have some `Prism' GlobalSumType LocalSumType`
So how does that starting salary relate to the final salary?
You can call me rude, or you can maybe introspect as to why so many people seem to be quietly agreeing with me. It's no skin off my back either way. 
I spend a lot of my time writing haskell using `singletons` nowadays, this has made me wonder if I should just use idris, it might be easier.
It probably comes down to funding. Cryptokitties became popular because people thought they could make money with it. In contrast, you know for sure you will get no financial return from paying someone to improve Idris. If Idris launched an ICO to fund development, it would probably fail miserably, because it can't be monetized. If someone found a way to make a market in which you could get financial return from investing in a project that is good for humanity, Idris would probably be funded very quickly.
I mean, if this is a plaintive `why`, ala 'why is there still world hunger?', I sympathize. But if you're actually asking, the answer is extremely simple: Of the humans with the money available to invest in Idris development, they : a) Are unaware Idris exists b) Do not believe they will see a return on their investment c) likely believe their money will be of greater benefit to humanity being donated to an actual charity of some kind. B and C are pretty much indefensible points. For B, There is nothing anyone has accomplished so far in the history of computing that definitively proves anything Idris introduces increases programmer productivity in a way that results in higher profit margins for their patrons. Suggests, sure, proves, no. It's not a sound financial investment. For C, really, honestly, a 'better' programming language will help nobody substantially save for programmers, who, in case you have somehow missed this, are really quite well off lately. Assuming someone has spare capital in the order of hundreds of thousands, if not millions, of USD to fund full time work from one or more experienced developers for a few years, and somehow B and C are not issues for them, maybe, just maybe, they can throw some capital towards Idris development, but this seems extremely unlikely... Unless of course you write some CryptoKitty breeding programs in Idris and they become popular somehow, at which point, all of these problems will be magically solved.
It is more of a plaintive `why`, yes (thanks for the new word). But it also has been a while since we had a thread to talk about Idris on /r/haskell, and that's a way to get more people involved, I hope! I agree with everything that you said. But, on funding, I think that if we made a crowdfunding for Idris development we'd actually get a sufficient quantity to have some full-time devs, there seems to be enough interest on this community, even if small. But I'm not sure we'd find the right people to do it... most of those capable of collaborating with Idris substantially are, as you said, already well off, working in something unrelated, sadly. &gt; Unless of course you write some CryptoKitty breeding programs in Idris and they become popular somehow, at which point, all of these problems will be magically solved. Now you're giving me ideas.
You won the thread for today
I've experimented with this since I've posted this. I don't think it's possible to somehow tell the current monoid value during writing. However, it's possible to indicate to Prescient whether or not it is currently reading or writing by changing (m -&gt; a) to (Maybe m -&gt; a) data Prescient m a = Prescient (Maybe m -&gt; a) m -- Functor and Applicative instances are the same. instance Monoid m =&gt; Monad (Prescient m) where Prescient f m1 &gt;&gt;= b = Prescient (\m -&gt; let Prescient g _ = b (f m) in g m) (let Prescient _ m2 = b (f Nothing) in m1 &lt;&gt; m2) runPrescient :: Prescient m a -&gt; a runPrescient (Prescient f m) = f (Just m) --write is the same. -- Returns Just v in the reading stage, and Nothing in the write stage. scry :: Monoid m =&gt; Prescient m (Maybe m) scry = Prescient id mempty This does, however, cause Prescient to lose its equivalence with Store. I've also tried my hand at creating a monad transformer version of Prescient, but I without any luck. This is what I've done so far: ``` data PrescientT s m a = PrescientT (Maybe s -&gt; m a) (m s) instance Functor m =&gt; Functor (PrescientT s m) where fmap f (PrescientT g s) = PrescientT ((fmap . fmap) f g) s instance (Monoid s, Applicative m) =&gt; Applicative (PrescientT s m) where pure a = PrescientT (const (pure a)) (pure mempty) PrescientT f s1 &lt;*&gt; PrescientT a s2 = PrescientT (\s -&gt; f s &lt;*&gt; f s) (liftA2 (&lt;&gt;) s1 s2) -- Breaks ap == (&lt;*&gt;), probably among other things. instance (Monoid s, Monad m) =&gt; Monad (PrescientT s m) where PrescientT f s1 &gt;&gt;= b = PrescientT (\s -&gt; f s &gt;&gt;= ($ s) . getFunc . b) (f Nothing &gt;&gt;= liftA2 mappend s1 . getInput . b) ```
&gt; Idris is too hard for most programmers to learn No way. Some of its features are definitely complicated, and most people would have a very hard time coming to grips with them. I know I do. I could definitely use some good documentation on Elaboration Reflection for starters. Most of the functionality that matters for writing day to day software? Much easier than writing robust, maintainable software in Java, PHP, JavaScript and so on.
kickstarter?
There are already robust tools that do what Idris does, and more. See Coq, Isabelle, Agda, ATS
Chicken and egg problem. Without an ecosystem, the infrastructure is nothing more than potential. Build an ecosystem worth using, and people capable of improving infrastructure are more likely to be motivated to help. Your post has motivated me to set aside some time on the weekend to work in an Idris frontend library I've been slowly building as a side project. And there's certainly plenty of low hanging fruit to work on, OP ;) Hit me up if you need ideas.
Keep growing the github repos of example full applications - CLI, web and much more - showing a mixture of patterns and architectures. I think there are plenty of library examples already. Continue to grow companies who build their product with Haskell. Commercial Haskell projects are the ones where people will have to keep answering questions of how to organize larger applications for long term maintenance and growth. I think there is beginner reading material, there are books and articles showing how to use various important libraries. I see larger examples of increasingly complex applications as the frontier, as beginners can fork and imitate those. The examples will keep someone engaged over time and thinking about how to build such applications at work, beyond their first hobby project or two.
Better evangelism would be nice. If Fun Fun Function kept doing Haskell (https://www.youtube.com/channel/UCO1cgjhGzsSYb1rsB4bFe4Q) or someone made a Haskell course for egghead.io that shared some practical examples of building things in Haskell it'd be great (here's how to write a web socket server in conduit, etc). I like the adage, "show, don't tell." Although as a newcomer still learning Haskell I feel like I have to switch between theory and practice quite a lot more than other languages. I bought haskellbook.com a while ago and have worked through the early chapters which is great. And I tried my hand at the adventofcode challenges last year in Haskell but was constantly frustrated by hitting walls with features I hadn't even used yet that seem quite fundamental. So it's back to reading more theory and trying again... one needs quite a lot of perseverance. At the same time I think having a video series with someone knowledgeable and enthusiastic would go a long way. A short, simple series on a single library or practical example to illustrate a concept or bit of Haskell theory would make adopting Haskell for more experienced programmers and newcomers alike more enjoyable IMO. Plus, it's great for programmers from other languages to see our enthusiasm and joy! If they can see what a pleasure it is to program in Haskell they might get envious or curious.
&gt; That makes no sense to me. Sometimes I wonder if humanity has forgotten the &gt; notion of priority? Most things of humanity does not make sense. Religion, politics, misguided priorities like fighting the so called "terrorism" but not the terror on roads (Indian context), cutting large swaths of forests for "development" and this is my favorit assuming that people are "rational". Compared to that using COBOL looks insanely rational. 
I thought Ben and another person were essentially full time GHC developers (they work at Well Typed but 90% of their job seems to get specifically GHC). I'd imagine there's a few part time contributers paid to do some work on GHC at other Haskell companies as well? I'd be surprised if there wasn't, honestly. Idris has even less industry usage than Haskell, so it makes sense that no company has yet sunk enough resources into it to justify the manpower.
This is the only reason why. People learned php back in the day because you could make cool non static websites with it. Ruby on rails came a few years later and got people learning ruby. People learned assembly then c then c++ for building games, and now some learn c# for unity. People learned js to make interactive client side features on websites. People learned R for mathematics Right now what is the draw to Haskell? It's cool. Aside that it's pretty hard as there is no "killer framework" based on it that will draw crowds into learning it, and many people learn it out of curiosity on what a theoretical academic framework would look like as an implementation. I know there's lots of things written out there in Haskell but right now it's not "famous" for anything besides its paradigm.
I would also add a quick and dirty option, when the services have already been implemented without forethought about testability - - start the application as an external process - wreq or http-client or process (using curl) to drive the service and capture responses - aeson parsers and other matchers combined with some test suite organization like tasty
&gt; The idea of "monads" is obnoxiously abstract. It's usually enough to think of `&gt;&gt;=` as the shove operator, and `ma &gt;&gt;= \a -&gt; ...` is just shoving `ma` into the `\a -&gt; ...` function. It's quite intuitive. The problem is you gotta go through the nasty theory at least once in your life before you become satisfied with the shove intuition. Maybe there is no solution to this problem. *Class, I hate to break it to you, but to become a Haskeller you just gotta bite the bullet. No way around it.* &gt; LYAH is a free read online. I frequently hear people say it's not that great, but it was my first Haskell book so it has a special place in my heart. +1
The real meaning of that motto is "Avoid (success at all costs)", which is a good motto, but the interpretation of "(Avoid success) at all costs" is 100% a joke.
Having tried to introduce Haskell to my workplace, Haskell needs more success stories. People still call it "esoteric" and only a "research" language. As an example; Go programming language has almost no technical merits, but they have many success stories (Go, Docker, tons of popular projects on GitHub) so people have a much easier time trying to convince their management&amp;peers. So my current thinking is that we should be more vocal about using Haskell in production, even if the thing we are doing is trivial and not worth to talk about. Sadly, from the decision-maker perspective "We wrote our CRUD app in Haskell, it was fast, easy and cheap" sounds much more attractive than "See what GHC can do". 
Did you look into Agda? I think it is as production-ready as Haskell. The ecosystem is not as mature as many other programming languages, but you can inject Agda very easily and reliably into Haskell making it in my eyes production-ready.
A lot of stuff became production-ready without full-time work. GCC comes to mind. What of Idris do you think that is not production-ready? Is it buggy? Not enough library? No satisfactory FFI? Did you look at similar languages like Agda and Coq?
That's not really how production-ready things work. It's just a natural consequence of human nature that if someone isn't dedicated full time to something, it never gets done. 
Does anyone know how to apply these techniques to Python? It seems so easy to generate images and generate polygons and lines etc. in Haskell when I briefly looked at the example in the link. Anyone know how I could do the same in Python?
All valid points in my opinion as well!
Thank you so much! It is exactly what I was looking for!! I'll look into it further in the morning.
As much as I'd love to see something like this happen, I strongly recommend you don't pursue this. Many people have tried (including me), and they've created things that they like/use, but that nobody else will use. I see a language like Haskell one day being useful as a foundation for probabalistic programming, but the more I use Haskell, the more its warts just make me feel like I may as well be using Python and get the nice numeric programming ecosystem. Even if all your main program logic is in Haskell, there's very few instances where you'd need Haskell ML code. It's 99% of the time easier to use the existing Python ecosystem. Also, probability/stats is perfectly amenable to a categorical foundation. Besides the mathoverflow link you provide, you should look into the Giry monad. This is the foundation for most probabalistic programming.
This looks a lot like `Store` comonad. https://hackage.haskell.org/package/comonad-4.0/docs/Control-Comonad-Store.html It can be turned into monad automatically by applying `Co` to it, since `Comonad w =&gt; Monad (Co w)`. I dunno if it detriments efficiency.
Well what I would like to see is not to use Haskell to replace the current ecosystem , but to provide something that make machine learning more amenable for functional programmers to understand. I would like to look into the giry monad later 
IOHK is big on cryptocurrencies and big on Haskell. I hope they make big cash and pump it back into dependent types research. so maybe cryptokittens will once finance our compilers? Who knows. I'm just speculating. But at least they're pumping research money in similar projects (formally verified LLVM as an example)
I think that there is not mystery in machine learning from the formal point of view. AFAIK. ML algorithms are pure functions of many zillion of variables. [This course](https://www.youtube.com/watch?v=mbyG85GZ0PI) among other things describe the functions implied in some ML algorithms The thing that destroy the purity of such algorithms are the effects needed for managing, selecting, filtering, analyzing and computing efficiently such big amount of parameters. 
That is how it works already. The `all-cabal-hashes` argument says precisely which version of hackage to use with `callHackage`. The package revision is also taken into account in the package description so there is no mutability due to revisions. The problem is because the patches in `head.hackage` are intended to be applied to the unrevised tarballs but the nix patch phase happens after the cabal file has been patched. A slight impedance which has to be worked around.
Agda is total, in that context meaning provably terminating, so it's either inconsistent (depending on the language extensions) or not Turing complete. One of the reasons I switched to learning Idris from Coq was because Idris lets you write your own tactics in the language. Coq now has mtac which is a tactics monad similar to Idris' Elab, but if you're looking to understand how and when to write tactics to handle the appropriate aspects of a proof, the proof of the Odd Order Theorem and such are already written with custom SSReflect-based tactics, meaning OCaml, not Coq. * ATS is barely older than Idris, so "there are already" doesn't really apply. * Is ATS production-ready? I've seen some really good benchmarks. * Does ATS have a system for working with OpenGL or Vulcan? Idris has a WebGL library. The efficiency could be damned, and any language that has such a library is closer to actual production needs, given the way people treat processing demands. It's valuable to be able to actually consider graphics programming in a language long before it's realistic. * Does ATS let you write your own backends? Else, not the same targets - using Idris to write pragmatic Javascipt is an important use case, considering it means proof libraries are shared between web developers and everyone else. Isabelle I remember having impossible to understand error messages. The documentation for Isar is also ineffable, it too rarely reaches the point of saying what something does or is used for in a proof or attempt to connect it to natural language proof theory. So I didn't find it very accessible. That said, Elab in Idris isn't well-documented either, and it's completely missing from the tutorial, even though it made the old proof scripts deprecated. I've forgotten a bunch of the research I did when deciding not to use it further. It's powerful for proofs, in practice. Is it powerful for programming too?
Woops.
I'm investigating this some myself. Currently, I'm working through Conal Eliot's old posts on differentiation. I'm basically following the same path he did - start with investigating automatic differentiation, then realize there's a lot of concepts I don't understand as well as I thought I did. It's a fascinating series of posts.
You might want to look into Jared Tobin's [blog](https://jtobin.io/) and [code](https://github.com/jtobin), as well as [Dominic Steinitz's blog](https://idontgetoutmuch.wordpress.com/) to mention just these two. There indeed is a bunch of libraries out there already, feel free to just go ahead and write your own (whether it should then be published is up to you). You can attack the whole thing from about a dozen different angles.
&gt; For C, really, honestly, a 'better' programming language will help nobody substantially save for programmers, who, in case you have somehow missed this, are really quite well off lately. I find perspective extremely shortsighted. Assuming longer-term continuation of technological human civilization, formal languages for software/law/mathematics/communication/etc. would be an extremely important and far reaching thing affecting the vast majority of all human activity. The welfare of 2018 programmers is irrelevant in comparison. While it's true that funding a random development project in software technology would likely be almost useless, because such projects tend to be so, it would be misguided to dismiss evidence of the importance of e. g. homotopy type theory from superficial reasoning about "software technology" reference class.
Co (Store s) is State s. This is a different beast.
This would go a long way with me as someone who leads two software development teams. I can't just tell my teams, "We're using Haskell now!" It would need significant justification and motivation to convince our developers. That comes from hearing about success stories of production systems and popular open source projects written in Haskell. Stakeholders still need to be convinced as well with stories about productivity, ROI, and risk management.
Wouldn't you just pay the project founder (who appears to be Edwin Brady) or else let him designate someone?
I was doing some research and it sounds like 31K GBP is probably doable for a single person for but for a family of four would be rather slim. What could be the reason that this globally, distributed company doing blockchain research can't afford to attract more experienced developers with attractive salaries?
I am super excited by DependentHaskell, it means we can use all the libraries/bindings/tooling that already exist in the haskell ecosystem and get dependent types when you want them.
You could just use the imagemagic bindings for python, they worked well enough 12 years ago when I used them last; and I imagine they still do.
Technically it's non-strict, which only discourages working normal business hours.
i just pm'd you with some thoughts. but also check out https://gitter.im/dataHaskell/Lobby
more like a long way away from compensation for east/west coast USA OR what i've seen for smart eastern european contractors working remote for US based companies. when the GBP was 1.5-2x the USD, and if this was a straight up first research postdoc out of grad school, this would be a decent rate. Its not clear to me if this is the case. :) 
There are a lot of comments below about commercial development. Having gotten budget to enhance languages before I don't think they are quite on point. Here is how I would put it. Programming languages to get popular start by becoming popular in a niche not met or not met nearly as well by other languages and then expanding into nearby niches. Once you own a niche you can get commercial investment from people who care about that niche. Almost no one is in the language business. There are companies like Microsoft, Micro Focus, Franz...) but those are rare and they are very choosy. So languages by themselves mostly stagnate. To get funding they need a niche. Forget about all the other issues, what niche is Idris currently a viable contender for? Let's assume Idris was going to go after something like being a contract specification and conversion language for RESTful web-services. That is try and beat things like SOAP. I'm not even sure Idris type system works for this. Let's take a basic example from insurance "a valid member ID is an social security number or a registered synthetic identifier for an existent American who is a family member of someone with a valid subscriber ID". Idris' type system would let me create an member ID from base things like Vector to make a 9 digit number that's been checked and that "resolves to subscriber" is true and ... But then how is that going to be better than SOAP? Try this exercise and come up with a niche where Idris in 2018 is better than almost all more mainstream existing languages. That really is the first question. Then comes question 2. Will the existing Idris community tolerate the language starting to evolve towards better supporting this niche? Given Idris focus on being Haskell with better types I'd really suspect that Idris is better suited for academic and hobbyist work at this point not commercial. The niche it has is computer scientists. Computer scientists mostly don't need the enormous amount of work required to get good performance. Compound that further. Idris is cutting edge in terms of design. Most likely no one knows how to make a fast Idris. And no one knows how much it would cost to and how long it would take to learn how to make a fast Idris. 
Wow insane. Terrific work! I think you are may have just invented an entirely new concept in computer science. It is almost a pity you aren't an academic. 
I feel the most confidence writing a json parser with aeson, against third party json. So, possibly a combination of aeson + wreq + servant + a caching library could provide a nice combination for writing local web services that wrap third party services...It's not a killer application, but it could get haskell up to sidekick status on many projects. I think Elm could be another growth area, where people are looking for anything but JS, that doesn't require tons of technologies to be hacked together. If Elm gets to a large marketshare, anybody even mildly curious would start experimenting with Haskell more. Elm provides a low barrier to entry to realizing the benefits of using stricter types for data. 
I find it more interesting to make academics site blog posts.
There is `magnify` for working with Reader. It works out of the box with any fold, so you can use it on a prism.
I can give you back an IO action in an IO action. I can take IO actions as pure arguments uninterpreted. I've been known to use IO (IO ()), usually with newtype noise to give back an "action" that can be used to cancel the process of listening to some event stream. Polluting all arrows with arbitrary effects isn't free, if I have to work polymorphically with arbitrary "effects" for things like `map`, now you have to care about what order those effects happen, and `map` ceases to be a canonical construction that is uniquely defined for a given data type. There become many different maps for the different orders you might visit the nodes, like there are many potentially different versions of `traverse` for a given type.
(.) can compose them, yes. But you can also implement them using just things that are provided by `base` (nay, really, just the Prelude). This makes it much easier for users to adopt lenses than the older style of lens I used to use in the `data-lens` package. This made it possible for folks to sprinkle a lens or two into their libraries without incurring any dependencies at all, which rather greatly increased their adoption.
Well, in that case, the problem is easier!
My main interest in Haskell these days is using it as a tool to teach higher maths. If you have some concrete ideas on how Haskell could be used to teach machine learning, I'd love to hear them.
Have a look at the GitHub link, it tells about basic statistics from random variables to expectations and covariance 
I wish I could use Agda, but I'm absolutely underwhelmed with the fact it can only be used properly on a single editor, EMACs. I'm a VIM user, so I'm not going to basically re-learn text editing (given that I'm very proficient on VIM) just to use a language. IMO that's an awful design choice, just for the sake of beauty. Just why?
I'm assuming you're working under a definition of “red tape” something like, “excessive bureaucracy”. Then, how do you define bureaucracy? My dictionary simply defines bureaucracy as “excessively complicated administrative procedure” and my thesaurus lists red tape as a synonym for bureaucracy.
Finally discovered what the issue was. It was due to an interaction between ghc-mod and nix. The solution was to create a wrapper script for ghc-mod like so: #! /nix/store/fi3mbd2ml4pbgzyasrlnp0wyy6qi48fh-bash-4.4-p5/bin/bash -e export NIX_GHC="/nix/store/rvaj3lh0pdhgm1pkbzgy4vhvgzm2ls7z-ghc-8.0.2-with-packages/bin/ghc" export NIX_GHCPKG="/nix/store/rvaj3lh0pdhgm1pkbzgy4vhvgzm2ls7z-ghc-8.0.2-with-packages/bin/ghc-pkg" export NIX_GHC_DOCDIR="/nix/store/rvaj3lh0pdhgm1pkbzgy4vhvgzm2ls7z-ghc-8.0.2-with-packages/share/doc/ghc/html" export NIX_GHC_LIBDIR="/nix/store/rvaj3lh0pdhgm1pkbzgy4vhvgzm2ls7z-ghc-8.0.2-with-packages/lib/ghc-8.0.2" exec "/home/miguel/.nix-profile/bin/ghc-mod" "$@" and point the atom ghc-mod package to that. Now atom works fine with single haskell shebang script files like so: #! /usr/bin/env nix-shell #!nix-shell --pure -i runghc -p "haskell.packages.ghc802.ghcWithPackages (pkgs: with pkgs; [turtle foldl text system-filepath])" #!nix-shell -I nixpkgs=/home/miguel/.nixpkgs/nixpkgs-channels-4aab5c57987af2dbf8b93fe30d5859a4a56d1aca {-# LANGUAGE OverloadedStrings #-} {-# LANGUAGE RankNTypes #-} module Main where import Turtle main :: IO () main = view $ ls "." 
Another VIMmer here. I recently broke down and learned the basics of Emacs in order to play with Coq and Proof General. I just hope it doesn't sap and impurify all my [precious muscle memory](http://www.imdb.com/title/tt0057012/quotes/qt0454442).
Crap. The fact that it is a different applicative is really awkward, because it means that to get the `ComonadApply` you'd need to use another type than `State s` -- one that just exists for that instance.
Perhaps more clear but less stylish, I think I have a counterexample for ComonadApply: example1 :: State String (String -&gt; String) example1 = do v &lt;- gets sets "final state" pure $ \v' -&gt; v' &lt;&gt; "final value (from initial state " &lt;&gt; v &lt;&gt; ")" example2 :: State String String example2 = do v &lt;- gets sets "final state2" pure $ "final value2 (from initial state " &lt;&gt; v &lt;&gt; ")" leftSide :: String leftSide = extract example1 (extract example2) rightSide :: String rightSide = extract (example1 &lt;*&gt; example2) law :: Bool law = leftSide == rightSide What's more is as far as I can see, the definition of `duplicate` isn't even relevant to my counterexample (I can comment the definition out completely, the code still runs and returns `False`). But I may be doing something wrong somewhere, as I have very little understanding of comonads.
It's fine, it's the law that doesn't mention `duplicate`.
When proving a negation, a single counter-example is all you need.
I never expected my work to cause this kind of result! This is extremely exciting! I don't really have anything to say about the state comonad you've suggested; I don't know enough about comonads. I'm only an amateur, and the Store monad instance I discovered was really just a stroke of luck. I wasn't even aware Store existed. When it comes to Store's utility as a monad, I do still wonder if there's room for improvement. There's a small variation I described [in this comment](https://www.reddit.com/r/haskell/comments/7oav51/i_made_a_monad_that_i_havent_seen_before_and_i/ds96viw/) that may be more useful. Perhaps the variation breaks the monad laws; I haven't really tested it.
There is 'Evil' for Emacs, which lets you use most of the basic vim commands. Also, take a look at this: [agda-vim](https://github.com/derekelkins/agda-vim).
It's an interesting distinction, since the current Applicative for `State s` has a sequential computational dependency on the various state pieces. My guess is the other Applicative instance is doing the more "Applicative-ish" (in my opinion) thing of computing the two terms independently, and then combining their resulting states using the monoid - meaning there's no sequential computational dependency there. This changes the monad instance too, of course. And probably gives a different transformer. Who knows, maybe both of these interpretations of State turn out to be useful?
Maybe you were wrong to decree that `(&lt;@&gt;) === (&lt;.&gt;)`?
Can I actually use Agda to write JS/native programs that are pretty much as fast as native?
I'm not quite sure how you do it - I decided to try to deploy a wai server incide the lambda, so that when the environment isn't restarted, the init time can be skipped. Also, as I am using lambda as a 'computation engine' (rather as an endpoint for S3 events), I have added some timing statistics to find out where is the bottleneck.
What you have is not what I had in mind. For example: type ProbSpace a = [a] --discrete probability space as list.Here a is the parameterized event type I don't want to be restriced to discrete measures. I want to work in arbitrary measures. It's possible in Haskell, but it's not pretty, and Haskell's type system just gets in the way. Idris is a bit better, but the compiler leaves a lot to be desired. Also, what you have is actually quite misleading. For example, your `Num` instance for `RandVar` doesn't incorporate dependence of the random variables, and so will lead to wrong results sometimes. Fixing a problem like this in a satisfactory way is quite tricky with Haskell's limitted type system.
I didn't find a simple way to do this but I fully expect you can write a `newtype` with a phantom `[Nat]` (or `[(Symbol, Nat)]`) for (named) weights allowing you to derive it directly {-# Language DerivingVia #-} data MyType ... deriving Generic deriving Arbitrary via (MyType `WithWeights` [1, 4, 4]) and deriving Arbitrary via (MyType `WithNamedWeights` [(1, "OneThing"), (4, "TwoThings"), (4, "ThreeThings")])
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/barbados] [Cryptocurrencies and Haskell Course with IOHK in Barbados - Application &amp; Scholarship](https://www.reddit.com/r/Barbados/comments/7ominu/cryptocurrencies_and_haskell_course_with_iohk_in/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
Nah. Its just that as you pile more and more constraints on a type, eventually one of them will usually give.
Um, if you're using the Applicative of ReaderT s (Writer s) -- there is a pretty obvious Monad at hand. ;)
Very glad to see the use of `InstanceSigs` and `TypeApplications` Cool introduction to *generic-random*
Update: Looked at existing monad proofs in Isabelle, and it turned out to be pretty simple: http://downthetypehole.de/paste/ULScRHsU (other proofs could probably be shortened as well). All your instances are lawful.
Oh dear. In my defense, I was actually hacking on the Applicative instance for State s directly try to get my example to work, but still... I think I should go to bed now.
You are mistaken, and I made a mistake. Coq supports writing software with dependent types, but Isabelle does not.
On the other hand, these functions get confusing when the combining function is not commutative. At that point, perhaps an explicit fold becomes clearer. The only `fromList` functions that really *need* to be part of the library API are the ones that assume ascending keys. These get to play tricks to cut the time from linearithmic to linear. All the rest are purely for convenience.
I wonder if trying to work around this problem is even worth the trouble for such a relatively small project. Should we just wait for the hardware to get fixed over the next few years?
[removed]
Could I ask what you use them so often for? I fell in love with Idris a few months ago and have been looking for examples of dependent typing in the wild now.
I mean to get to this book someday (https://mitpress.mit.edu/books/introduction-computational-learning-theory), possibly this year. I have no idea what I will do when I start reading this book. My main interests in Theoretical CS, approximation algorithms, and related. 
Some formalization has been done recently in Lean (I theorem prover based on dependent type theory): Talk: https://www.youtube.com/watch?v=-A1tVNTHUFw Paper: https://arxiv.org/abs/1706.08605
[LLVM](https://downloads.haskell.org/~ghc/8.2.2/docs/html/users_guide/codegens.html#llvm-code-generator-fllvm) is probably the way to go. I think retpolines are being implemented in LLVM?
&gt; That said, I find it more satisfying to make academics cite blog posts. A bit of schadenfreude :) Funny. Worth pointing out people in the real world do know stuff. And at the same time kinda evil :) Citations are their lifeblood. 
Not a popular opinion, but I think this kind of tests are the one that are more reliable and allows you to refactor more fearlessly.
I'm starting to think `Free`/`Freer` monads might be the exact approach I want. It'll also provide a nice way to have a place to route functions through on the server side, via a serverInterpreter, while the clientInterpreter would be aware of when to offload on each action it is interpreting. Thanks! :)
As a fellow beginner, I think you're spot on with this.
`DerivingVia` looks so nice! There may be a few different ways to do the `custom` thing with `via`; one I like is to attach values to types/symbols with a typeclass class Named name ty | name -&gt; ty where named :: ty data Custom instance Named Custom (GenList '[Maybe Integer]) where named :: GenList '[Maybe Integer] named = ...
That's in theory though, right? I still need unicode to use other people's code.
Yeah I guess that's true. There is no Agda coding style guide afaik but I can see why most people would consider it bad style. Imho this is very unfortunate since Unicode did improve my Agda coding a lot. My humble suggestion would be, if I were you I'd try configuring vim to work with unicode. I know that Idris community doesn't like unicode (which is why Idris doesn't have unicode support) but I think it does improve readability quite a bit. Anyway, it's your decision.
I'm of two minds with regards to ghc and side-channel attacks. On the one hand, I suspect that there are a ton of side-channel attacks that GHC compiled code is vulnerable to in general, in that it is really not possible to do timing-attack-resistant stuff with it. On the other, I also suspect that there are a ton of attacks that just via pure obscurity, GHC compiled code won't get hit with -- if you aren't intimate with STG, the generated asm looks like its been run through a weird obfuscater anyway... about the only stuff that looks relatively straight-line is in tight inner loops that have been deliberately written to give pretty asm to begin with.
Neat! So how can we think about the utility of the `duplicate` function on `State`? If we think of the degenerate case where `State` is only used as `Reader` it seems to give us a `local`. So it lets us sort of experiment in a bunch of possible worlds with the given state, with the condition those worlds be "reachable" from ours by a monoidal action. The update monad question is a good one that seems relevant to this...
Easy solution. Use spacemacs. It is very beginner friendly (similar to atom or sublime) and it uses VIM keybindings instead of emac ones (you choose this at the start of installation). 
Anyway I'd still love to see an up-to-date comparison between Idris and Agda, you're all assuming here that Agda is superior to Idris in some aspects, but I don't know it!
/u/doofin , why don't you ping us in the chat room, I'm sure there'll be others interested in this : https://gitter.im/dataHaskell/Lobby 
Very cool! For the Store monad I had an immediate relative offset intuition in a coordinate system sense. After looking at the concrete zipper example that seems to kinda fit? I am much less clear about the store comonad. My general comonad intuition comes down to "act as if we are running a state computation for each possible state" but I am not sure how accurate that is. 
I think this is a quantity issue. And we need some flagships companies using and advertising Haskell, like Facebook. 
Their lifeblood is *being cited*. Having to cite a blog post is only mildly embarrassing.
Is there also a `Cozoom` for prisms and `Store`? If so, what's it mean?
They state that their approach is not limited to whole program optimization, it was just what they implemented, so they could have access to "maximum information".
No worries! What I mean is earlier up in the tutorial we define `newtype Word = Word (Unsigned 64) deriving Show` so wouldn't they be equivalent? Once again I'm a noob here, so I'm guessing this?
Another approach is encoding open kinds (see **4.3.1** of [Promoting Functions to Type Families in Haskell](http://www.research.ed.ac.uk/portal/files/28356495/eisenberg_stolarek_promotion.pdf)) data TYPES :: [Type] -&gt; Type type Gens tys = TYPES tys -&gt; Type allowing us to define data Custom :: Gens '[Maybe Integer] class Named (with :: Gens tys) where named :: GenList tys instance Named Custom where named :: GenList '[Maybe Integer] named = fmap Just arbitrary :@ Nil which I discussed in a few posts before Christmas ([Encoding Overlapping, Extensible Isomorphisms](https://www.reddit.com/r/haskell/comments/7j90mr/blog_trick_encoding_overlapping_extensible/), [Witnessing Monoid actions + Semigroup / Monoid / Applicative homomorphisms](https://www.reddit.com/r/haskell/comments/7jx8wc/blog_witnessing_monoid_actions_semigroup_monoid/))
Eventually you'd want to be able to specify them separately of each other, so one type-level tag for `fmap Just arbitrary` can be reused in different `GenList`s
http://hackage.haskell.org/package/bittrex
How many? tbh after reading more than ten of those i'm not particularly excited about reading more. &gt; And we need some flagships companies using and advertising Haskell, like Facebook I don't think this is a very achievable goal unless you work at a big company or know someone who works in a big company and can push Haskell there, and then publish an article about it.
I really support this. I feel like the best way to make a developer interested in Haskell is writing a cool and useful application in it. And also make it easy for them to get started.
There's an obvious solution then. Rebrand Idris as "Idris for Block Chain"! ;)
My thread from last week mentioned optimistic evaluation, and there were some nice answers there you might like. https://www.reddit.com/r/haskell/comments/7nmcrj/what_evaluation_strategy_to_use_for_a_new/
I love Haskell so I am biased. I can tell you I would need much more than this list to be a little bit impressed ;-)
First, you need to answer the question: do you want to implement true DSL as a new syntax (like `HTML` or `XML` or `SQL`) or you want to implement EDSL (embedded DSL) so users can use your functions directly inside Haskell code?
Sorry should have been more specific, an EDSL is what I am going for. It will just be some bindings for the API.
Our execution model is so weird that custom attacks would almost certainly have to be developed... which means more people learning Haskell, yay! Also, all pure code is completely immune, only branches in impure code could be targetted because this is a *timing* attack and pure code *is not aware of time.*
Nice list. Btw I have myself recently started following the futhark code base, you mention it is written in C++. Isn't the compiler written in Haskell?
The Bryan O'Sullivan suite of tools comes highly recommended. [`wreq`](http://www.serpentine.com/wreq/), `aeson` and `bytestring`. `wreq` supports streaming response bodies as well. For hashing the URL, `SHA` worked well, but `cryptonite` would have been fine too. https://github.com/dmjio/bittrex
This list was not meant to impress anyone. It was meant to show a few applications that can be written by a lone junior developer who isn't a Haskell master, and that can be read by other intermediate Haskellers to learn a bit more about Haskell. If you want to be impressed, check out: - [postgrest](https://github.com/begriffs/postgrest) - [Project:m36](https://github.com/agentm/project-m36) - [PureScript Compiler](https://github.com/purescript/purescript) and other compilers written in Haskell, there are *a lot* - [xmonad](https://github.com/xmonad/xmonad) - [Haxl](https://github.com/facebook/Haxl) - [Stack](https://github.com/commercialhaskell/stack) - [wire-server](https://github.com/wireapp/wire-server) - [Servant](https://haskell-servant.github.io/) - [matterhorn-client](https://github.com/matterhorn-chat/matterhorn) and if you'll allow me to add some purescript to the list - [cubbit](https://github.com/aratama/cubbit)
Isn't that sort of a chicken/egg problem? If companies aren't using because it's not production ready, then no companies will invest in it to make it production ready.
Good question. For me, its mostly just interesting that there is another comonad on offer. There is a rather small pool of comonads and comonad transformers available for exploration in Haskell. It also means that I had a serious mistake in my understanding of the tooling available to find them. So now I have a new trick to use when trying to recover the unit laws, etc. I'm quite curious about what this comonad (and the monad for Store) mean in practice and what the generalizations mentioned at the end of the article might do.
Yeah that's how it works.
What does it mean that you use abstract interpretation instead of unification? Could you explain this to a layman, please?
If you have the time would it be possible to have documentation (perhaps in the README) as to why each file is necessary and what it does. I personally like to understand every file I have checked into git for my projects, and I am assuming there are a decent number of other people who feel the same way. So if reflex-project-skeleton is the best way to get started with Haskell on the front-end and the back-end (which seems to be the case) then it would be really nice to have that information available to me. I still do have some hesitations with using a reflex focused project as a base for general purpose web dev, but it seems that is the state of the Haskell web ecosystem at this time, and it isn't a huge deal for now. Thanks so much for all of your help!
&gt; Also, probability/stats is perfectly amenable to a categorical foundation. I beg to differ. The Giry monad is nice and all, but the thing you really want isn't a formalization for a single measure in isolation but rather a formalization of the whole category of measures (or measurable spaces, or whatever). More particularly, we want that category to be "nice" (e.g., having limits, being CCC, etc)— but alas, there is no such category. There are plenty of candidate categories, but none of them are nice to work with, because they're all deficient in some way. This isn't for lack of trying: the problems these categories encounter are fundamental to measure theory, much as the similar issues are fundamental to categories of topological spaces. (IMO, the thing we really need to make progress in this area is not searching for a new category but rather searching for a new logic. The fact that getting a CCC is impossible indicates that the lambda calculus is a poor internal logic for reasoning about measures. So rather than trying to shoehorn the category into giving us a lambda calculus, we should instead be asking what a calculus would look like which accurately reflects the category we have.)
&gt;A monoid is the generalized structure underlying concepts like the natural numbers: We can add two natural numbers, but we can't (in general) subtract them, as there are no negative numbers. IMHO, the ideal example of a monoid is strings with concatenation. This is because strings are the "free" (e.g. most natural) monoid over the character type, and programmers are already acquainted with both strings and chars.
Yea clarifying all the basics is next on my todo list for the docs. There's definitely been a lot of confusion around how Nix gets dependencies and how `project` differs from `./try-reflex`
For me, I like the more elegant approaches mentioned. I would only use this as a first pass, until one had time to refactor the code for testability. Having to start up a service for tests or cleaning a db in-between tests feels cumbersome, and more likely for people to avoid keeping the tests up to date.
&gt; There is a technicality here, that Haskell also has an intermediate between functor and monad called "applicative". As I understand it, this does not have a clear category theoretical analogue. I'm not sure why it exits, but I believe it has been added into the hierarchy after the fact. As far as I understand, this was added for programmer convenience, essentially. When computational structures start to be considered, the distinction between applicative and monad start to become interesting. An applicative is essentially a monad that doesn't have any "choice" or "knowledge". For example, the monad instance of Maybe must, by necessity, short circuit on a Nothing value. However, the applicative instance of Maybe must, by necessity(?), blindly continue. This makes Applicatives a great choice for parsers. An applicative must also not rely on previous input or context (which is why it's not allowed to short circuit; it can't "know" the last result was a Nothing). This lack of dependence means that a chain of applicatives can always be executed in any order and in parallel; this feature is what motivated Facebook to develop Applicative Do notation in order to easily and 'automagically' parallelize code-paths. (Hopefully I didn't butcher that explanation too badly)
[removed]
[removed]
[removed]
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/purescript] [\[ASK\] Favorite math topics\/books you'd recommend to write better, more rigorous software in Haskell &amp; PureScript?](https://www.reddit.com/r/purescript/comments/7owczx/ask_favorite_math_topicsbooks_youd_recommend_to/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
 filter (\x -&gt; sum x &lt; 21) . subsequences $ [4,4,4,4,4,4,4,4,4,16] Sorry no REPL on me, but there is often no need for a fancy data structure with solving problems like this in Haskell.
I agree :) I bring up free monoids later in the article, as an example of a functor, but you're correct that it makes for a good motivational example. I was always a bit dissatisfied with the natural numbers as an example because they also have more structure (they have multiplication and both addition and multiplication are commutative, they have a form of division, they have a total order,…) and it becomes harder to disentangle properties specific to the natural numbers from the concept. That being said, the free monoid over an alphabet *also* has more structure to it - for starters, it is also totally ordered (inherited from the order on the alphabet). In that sense, the most "general" - and in a sense simplest - monoid are functions f:X-&gt;X, for some set X. Any monoid can be viewed like this and it both hides existing structure and does not naturally give rise to new structure. So, in a way, it forces you to *only* work with it as a monoid.
Thanks for the explanation :) I don't *really* understand, though, to be honest. The implementation of `&lt;*&gt;` for `Maybe` seems straight-forward, but I'm not really sure how it leads to what you say about short-circuiting (or not) and relying (or not). Maybe (heh) I should just have a look at how `Applicative` is used in practice to get that, though :)
What do you mean that the applicative instance of Maybe must continue.
I miss the mention of monoidal categories, without which the business of plugging functor composition into diagrams is rather sudden and vague. More tangentially, /u/ekmett has pointed out before that the "the" isn't very appropriate in "the category of endofunctors", because there are choices other than composition for tensoring (e. g. functor products/coproducts). So I would rephrase the thing as "monads are just monoids in a monoidal category of endofunctors, tensored with functor composition".
Here's one way to look at it. I'll just put the relevant type signatures here first. `(&lt;*&gt;) :: Applicative f =&gt; f (a -&gt; b) -&gt; f a -&gt; f b` `(&gt;&gt;=) :: Monad m =&gt; m a -&gt; (a -&gt; m b) -&gt; m b` The monad can make choices whether to continue or abort inside the `(a -&gt; m b)`, while in the applicative case the `f (a -&gt; b)` is _inside_ the context of the computation, and cannot get out in any meaningful way so it has to "continue". More generally, `(&gt;&gt;=)` is context aware, and can base further computations on current state, while `(&lt;*&gt;)` can only combine computations. There is a great post on SO discussing this [here]([https://stackoverflow.com/questions/23342184/difference-between-monad-and-applicative-in-haskell)
&gt; I miss the mention of monoidal categories, without which the business of plugging functor composition into diagrams is rather sudden and vague. Well, the reason is, that I don't know about it :) And it's also, IMO, a style-question. Obviously, I can't (and shouldn't) write a text-book on category theory, so I *had* to omit a lot of stuff. And to me, personally, I found explanations that don't assume too much extra knowledge but also don't go too much into notational details much more satisfying/helpful. Like, I know that *I* get frustrated, if an explanation starts with "please first read this book on $topic, so I can use all the lingo that makes the explanation elegant". But duly noted, thanks for the feedback :)
It's hard to comment unless you paste your actual modified `nix` files. 
Thanks!
It has been proven time and again that "security by obscurity" is useless. Unless perhaps you're a government or large organization capable of enforcing significant secrecy mechanisms, but we're not that. The complexity and eccentric style of our generated object code only makes it that much harder to figure out what we need to do to protect against this attack. It's already quite complicated even for the simplest assembly code. We have to decide either to fix this, or to make a public decision to remain vulnerable. Whether the latter is viable depends on the details of the vulnerability. I'm not knowledgeable enough about this to comment. It does seem like people are pretty upset about it though. On the other hand, I'm not sure we should jump into the water and commit ourselves quite so quickly as Google did. There are discussions that imply that reptolines may not completely solve the problem. There are multiple variants of the attack, and multiple variants and approaches to mitigation. We really have to understand this well before we commit to something.
Great, thanks!
One thing I don't like about this is that the equality we use to talk about this mathematically isn't actually the equality used by the type checker. The equality we use when talking about these things mathematically is merely an isomorphism, so `Maybe (Identity a)` is in this world the same as `Maybe a`. But obviously the Haskell type checker uses a definitional equality, not an "up-to-isomorphism" equality, which means that `Maybe (Identity a)` is actually not the same thing as `Maybe a`. So in some sense we use these math words to describe what we're doing in our language, and from an external perspective, it's true, but from an *internal* perspective (from "inside" the language), it's actually not true. Haskell does not actually respect the way that we talk about it.
An easy way to think about [abstract interpretation](https://www.quora.com/What-is-the-relation-between-partial-evaluation-constant-propagation-and-abstract-interpretation) is to take its name literally - as interpretation over abstract values (such as types) rather than just concrete ones. [Unification](http://okmij.org/ftp/Computation/FLOLAC/lecture.pdf) can be though as one of the particular ways of doing abstract interpretation. Unification works top to bottom and does so by resolving equality constraints, while my the typing scheme in my own language propagates types forward only. This makes it more powerful as the evaluator knows the type of everything at all time. The disadvantage of this is that those terms not on the execution path do not get typechecked. ...Hmmm, so saying I am using abstract interpretation instead of unification might not have been the best way of putting it, but I do not think I've ever seen a name for the kind of typing scheme that Spiral does. Maybe calling it forward propagation might be good, but I felt that this would have been too weak - C uses that thing and Spiral is worlds apart from it in expressiveness. Maybe something like staged forward propagation would be good?
The problem is the change in the relationship between `==` and `ErrorCall`. But I guess this is also an argument for catching exceptions by type when their values don't matter.
&gt; Internally Maybe (Identity a) and Maybe a are indistiguishable Sure they are! example1 :: Maybe (Identity ()) example1 = pure (pure ()) example2 :: Maybe () example2 = example1 Haskell rejects this code, yet our endofunctor monoid (under functor composition) clearly says that `Maybe . Identity = Maybe`. The type checker is using a different equality than we are when we talk about this "monoid in a category of endofunctors," hence my saying from "inside" the language, from the type checker's perspective, this is *not* true. Making it true requires constructing an isomorphism -- which in this case is not merely computational work, it's *human* work that I must bash out on my keyboard!
What do you mean when you say it propagates types forward? Can you give an example that contrasts with propagating types from the top down?
Just to be sure, did you add it as a dependency to the correct module and restart your shell? You can run `ghc-pkg` to see what your Nix code has come up with for dependencies.
It's unfortunate that patter synonyms have very same syntax as constructors. But as such they still very convenient feature. The issue you are referring to can be prevented by either: 1. declare only unidirectional synonyms 2. make sure "==" for values works same as using patterns Here was probably some compatibility issues which prevented doing so
I think the unfortunate part here is that pattern synonyms, when used as terms (e.g. a bidirectional synonym), have the exact same syntax as constructor applications. So constructor application syntax is now overloaded and we have to keep that in mind and think about possible extensions of a constructor application when reasoning about the code. As an example, if `ErrorCall "hello"` had a special syntax here, indicating it's a pattern synonym rather than a constructor application, it would not be a problem. (OTOH this would be against one of the uses of pattern synonyms which is to enable backwards-compatible refactors)
I downvoted this because self-aware monad tutorials are still monad tutorials, and monad tutorials are morally wrong.
Will do. Thanks a bunch for making this man! 
So assuming the card values are 1..10 (aces being 1), this seems to do that: ``` counts :: [Int] counts = [4,4,4,4,4,4,4,4,4,16] values :: [Int] values = [1..10] -- Compute number of different hands given [(count,value)] cards and maximum allowed sum genUpTo :: [(Int, Int)] -&gt; Int -&gt; Int genUpTo [] _ = 0 genUpTo _ 0 = 0 genUpTo [(n, v)] mx = min n (mx `div` v) genUpTo ((n, v):rest) mx = sum [ 1 + genUpTo rest (mx - cnt * v) | cnt &lt;- [1..n], cnt * v &lt;= mx ] + genUpTo rest mx main :: IO () main = print $ genUpTo (zip counts values) 21 ```
I'm looking at http://adventofcode.com/2017/day/7 and would like to try to solve the problem in a specific way. I've defined ``` data Prog a = Prog { name :: String, weight :: Int, holding :: [a] } deriving (Show, Eq, Ord) ``` Parsing the structure gives me `Prog String`. In this case "holding" is a set of strings pointing at the keys of children. I want to take that structure and rewrite it to `Fix Prog` (or, possibly more reasonably, `Fix Prog`). Is there an elegant way of achieving this? 
I chuckled :) FWIW, to me this exercise was an excellent hook to learn about type theory and also have a closer look at Haskell specifically. So, As an FP-agnostic, I feel like I can justify writing a monad tutorial before the FP-gods (should it turn out they do exist) by explaining that it shouldn't matter what path I choose to powerful types, as long as it works for me ;)
Thanks for that, I was able to get something to work by using `to` with `Magnify`.
It might be good to show an example. Here is how unification works in F#, Haskell and the other MLs. let f a b = let c = a + b c + 3 f 1 2 At the start of typechecking pass, it knows nothing about the program so in the course of tying, it goes top to bottom. let f a b = When it first encounters a new variable since it does not know its type it assigns it to metavariable. Hence it would assign `f`, `a` and `b` to fresh metavariables in its type environment. Now it has in its type environment that: f: `'a -&gt; 'b -&gt; 'c` a: `'a` b: `'b` Then it goes to the next line. let c = a + b Since it knows that `+` is of type `forall a. 'a -&gt; 'a -&gt; 'a`, it tries to unify `a` and `b`. This is done by substituting the `'b` for `'a` in their types. The opposite would also be fine. f: `'a -&gt; 'a -&gt; 'c` a: `'a` b: `'a` It also adds `c` to the type enviroment with the type 'a'. Then it moves to typecheck the next line. c + 3 Here it has to unify c and 3. Since 3 is of type `int32` (in F#), from that it can infer that `'a` is `int32`. After the substitution is done the type environment looks like this. f: `int32 -&gt; int32 -&gt; 'c` a: `int32` b: `int32` c: `int32` Since `int32` is also the return type it can infer that `'c` is `int32`. f 1 2 On the last line it just checks that the arguments match up. Staged forward propagation is different. inl f a b = inl c = a + b c + 3 f 1 2 If you typechecked the above in Spiral, what you would get is: 6L When Spiral first encounters a function it does no typechecking. It just carries it in its enviroment like a piece of syntax wrapped in lambda. f 1 2 What would trigger typechecking is actually calling the function. And once that is done Spiral knows more than just the type of the variable – it knows what the exact literals `a` and `b` are in this case. So it adds them at compile time. inl f a b = inl c = a + b c + 3 f (dyn 1) (dyn 2) You can push them to runtime using `dyn`. Here `a` and `b` would be an abstract variables of type `int64`. Once you type them this is what would come out. let (var_0: int64) = 1L let (var_1: int64) = 2L let (var_2: int64) = (var_0 + var_1) (var_2 + 3L) Hope that makes clear the essential difference between Spiral's intensional type system and ML's extensional one. F# for example does nothing with the information that 1 is a literal during type checking apart from assigning it a type. You can find more information on how HM type inference is done in the paper I linked to. Spiral on the other hand propagates that information in its exact form. This is both highly useful and desirable in a programming language as it allows to integrate typing and optimization and unlocks new levels of polymorphism that not dependantly typed languages offer. Having done an intensional type system I now feel that the mainstream languages are going too far in the other direction and the PL research community has missed an entire branch of programming languages in their fervor for abstraction. If you are interested in learning more I suggest going through the tutorials for Spiral and leaving me some feedback. Also PR me any typos that you find if you do so please. 
I have not encountered Free monads but I am familiar with transformers. Out of interest why are you not a fan of the Free monad approach? 
VERY COOL!
Most ideas are taken from this recent blog post: * https://markkarpov.com/post/free-monad-considered-harmful.html In short, they have worse performance in general more complicated. I understand `Free` monads well enough, but still sometimes struggling to understand things. I much more prefer to work with functions like this one: * https://github.com/haskell/mtl/blob/master/Control/Monad/State/Class.hs#L92 rather than this one: * https://github.com/lexi-lambda/freer-simple/blob/888465fd063c3939e087ab482fb4d11795b133f8/src/Control/Monad/Freer/Internal.hs#L215 My experience says me that it's harder to fuck up with and easier to reason about more simple approaches in average. So, unless you don't truly need true power of Extensible Effects and the performance is not a problem for you and you can't wait for `-XDependentHaskell` I think you don't need `Free` monads :)
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [haskell/mtl/.../**Class.hs#L92** (master → 5be11c9)](https://github.com/haskell/mtl/blob/5be11c94442a04d86f58e4b3aa528372c3486ad6/Control/Monad/State/Class.hs#L92) ---- 
For my solution, I preprocessed the [Prog a] into a map, then traversed the map to find the root element, and used that to build up the tree with the `ana` combinator. You can find my solution at https://github.com/Solonarv/adventofcode-2017/tree/master/Day7
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [Solonarv/adventofcode-2017/.../**Day7** (master → a4dbf18)](https://github.com/Solonarv/adventofcode-2017/tree/a4dbf1862782dbcdaf447c0b1bb62980ef4d1eb1/Day7) ---- [^delete](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dsd3kdx.)
You're quite welcome; I'm glad it's helpful!
I think you may have butchered it a little, no offense! The applicative instance for `Maybe` does not continue, it short circuits. The key thing is that the values inside the applicative (the `a -&gt; b`, the `a` and the `b`) cannot depend on or effect the state in any way. They essentially only react completely purely with each other, while the applicative does all the "applicative stuff" around them. 
Congratulations, and very interesting work too!
&gt; I wager the average serious beginner does not want to be treated like an imbecile. I agree completely. In that regard I enjoyed "Thinking Functionally with Haskell" by Richard Bird, although it too doesn't focus on real-world stuff, for which I have high hopes for https://intermediatehaskell.com by Vlad and Artyom.
Damn, I wish I saw this post earlier :'(
nope, ubuntu
Wow! Thank you for the gilding, kind sir! :D
I don't see the reason why Haskell code would necessary be IO bound. I agree with the second point, it's of course much better to patch kernel and services code first. But I still don't think that people are never going to write haskell programs or services that handles secrets (keys, passwords, ...). With Spectre variant 2 any other process can attack a vulnerable haskell process and get a dump of its memory.
This is called `Succs` in [successors](https://hackage.haskell.org/package/successors-0.1.0.1/docs/Control-Applicative-Successors.html#t:Succs) (where shrinking was also mentioned as an application), and `hedgehog` has something similar with a whole tree.
&gt; Hey, that was a pretty good read. (Btw i think the newlines in your code blocks got mangled - might want to edit that). I intended them to be like earlier, but maybe this is better? &gt; I think what you're describing is depth-first vs breadth-first. Would it make sense to base the distinction on that? This definitely occurred to me as well. It is well known in optimization that the search order can have great influence on the speed of the program. And I do think that type inference (or propagation) is a kind of search. But ultimately extensional and intensional type systems are not necessarily duals of each other - they measure fundamentally different things and do so in different ways. The difference is not just the search order, but also granularity of abstraction. The fact that [intensional type systems](https://arxiv.org/abs/1712.09302) have very little presence is a reflection of math in general - most of it is of extensional variety, and intensional type systems have received little formalization as they tend to be ad-hoc. I looked widely for material on intensional polymorphism, but I could find very little. What I have in Spiral is 95% my own work. &gt; (and the naming too - it's probably a good idea to reuse applicable naming rather than come up with new concepts to learn that are homeomorphic to existing ones) I know that Spiral join points can also be called fix points, but I am sort of making up the terminology as I go along and hoping for the best. If you feel like there is a better way to explain or name things don't hesitate to inform me. Also don't assume that I can grook math - like in that paper on semantics of intensionality that I just plugged. :) &gt; I would like to check out Spiral at some point. Do you have a place where you post about it, say Twitter, so that I could get reminded of its existence periodically after my current workload is over? No, the Github repo is the only thing right now. I'll probably post about it on the F# and the programming languages subs once I make more progress. I intend to add new documentation every six months announce it when it is done. In the meantime I'll stay in shadows. This is Spiral's first release so I've been publicly talking about it a lot. I hope that Spiral's ideas spread into other statically typed languages, but popularity is not my goal for the language itself. I made it in order to replace the current crop of big ML frameworks like Tensorflow with a library integrated directly in the language, and will use it to do shadowy things. Languages never become popular purely on their own merits anyway so I do not want spend effort constantly representing it.
The elegant way is called `loeb :: Functor f =&gt; f (f a -&gt; a) -&gt; f a`. `f` is `Map String`, `a` is `Fix Prog`. The argument `f (f a -&gt; a)` is a map that, for every `String` key, explains how to construct the corresponding `Fix Prog` given the whole map of `Fix Prog` (that you are in the middle of defining, but everything will be fine if you're not too strict) (i.e., `Map String (Fix Prog) -&gt; Fix Prog`, i.e., `f a -&gt; a`). I assume you have a `Map String (Prog String)` so all you need to get to `Map String (Map String (Fix Prog) -&gt; Fix Prog)` is a function ??? :: Prog String -&gt; Map String (Fix Prog) -&gt; Fix Prog
I think it would be great if you could make some simple examples comparing how one would code some things in Spiral vs Haskell vs C and what features of spiral are being used there. Then, laypeople can understand the point of Spiral and spread the word. Readily accessible take-aways are important for that reason.
Good catch... Can't believe I missed that. Thanks.
This is enormously cool and exactly the sort of thing I was looking for. I'm guessing it won't help me find the root, but you can't have everything.
It bothers me that this issue is still open: https://github.com/idris-lang/Idris-dev/issues/3839
The mess there is that they don't extend to transformers at first glance. Given that we don't have a 'State' and 'Store' version that isn't a transformer I'm somewhat loathe to add the instances to the type synonyms or without properly understanding the constraints the transformed 'm' would have to avoid having to implement them as FlexibleInstances.
Tutorials are full of those examples. Most of it is code and half of the examples would be impossible in any language other than Spiral. The most notable thing is [the tensor](https://github.com/mrakgr/The-Spiral-Language#5-tensors-and-structural-reflection).
I agree that’s a terrifying issue, idris seems completely unsound. 
I tried to explain how to create a well-designed, normalized database with Persistent. I'm writing this as I learn the ins-and-outs of Persistent. Any feedback/criticism is appreciated :) (I think I need to explain 3NF better, but I don't totally understand it yet...)
No, this isn't just the product of Identity and List. if that were the case the definition for pure would be `pure a = Once a [a]`. There is also the fact that the getDefault fields get involved with the list in liftA2.
Oh man this is amazing, Thank you for turning me on to this. Makes me want to write a little RPG or roguelike in Haskell... Might be a good opportunity to start learning to use brick and dive into Lens a little bit.
idk, but try cabal configure first, and use cabal build (old build). 
ah you're right, my bad
Dang, that's really scary.
**Logic**: - [Programming in the 1990s](http://www.springer.com/gp/book/9780387973821) Will introduce you to modelling discrete systems using first-order logic, predicate calculus, and a smattering of set theory. It's written for the _working_ computer scientist and so the pedagogy is focused on pragmatism. - [A Logical Approach to Discrete Math](http://www.springer.com/gp/book/9780387941158) This is a more fundamental book focused on the pure maths and introduces you to a system of proof that has many similarities to that shown in the first book. **Fundamental Maths**: - [Introduction to Graph Theory](https://www.amazon.com/Introduction-Graph-Theory-Dover-Mathematics/dp/0486678709/ref=zg_bs_13938_1?_encoding=UTF8&amp;psc=1&amp;refRID=5B340WJFHW26XTZVXM54) Despite it's title this book actually introduces you to the basic concepts of proofs using set theory to start. You work your way up to proofs in graph theory using those foundations. What I like about it is that you start by writing proofs and not learning rote formula without any context. It's written for liberal arts majors or those traumatized by mainstream math curricula. I recommend it as a refresher into mathematical thinking.
I think it's a quantity issue. I've probably read all of the success stories, failures, and everything I can come across. I'm comparing this to Javascript which has so much content that I'd probably be sitting here until the heat death of the universe before I could finish it all. Frequency is an important factor in getting people to pay attention to ideas. I think we could use more success stories, more tutorials, more widely used tools, meetups, conferences, etc, etc.
I'm on my phone so won't go through the whole thing right now. But to answer the first part you are stuck on: You can represent your card count array with a simple card count list, where each index represents a different card: cardCounts = [4, 4, 4, 4, 4, 4, 4, 4, 4, 16] Now to get each one decremented once you can do: applyToEach _ [] = [] applyToEach f (x : xs) = (f x : xs) : ((x :) &lt;$&gt; applyToEach f xs) As follows: applyToEach (subtract 1) cardCounts [[3, 4, 4, 4, 4, 4, 4, 4, 4, 16], [4, 3, 4, 4 ... 
Type safety is for cowards. Real heroes hope for the best and never return emails. 
It'd be fun to do this with backpack.
Ah! Is that the dual of [`Lift`](https://hackage.haskell.org/package/transformers-0.5.5.0/docs/Control-Applicative-Lift.html#t:Lift)?
I think you're making a good point, but I'm having trouble understanding it. &gt; the thing you really want isn't a formalization for a single measure in isolation but rather a formalization of the whole category of measures (or measurable spaces, or whatever). Can you be more specific about what this would buy us in terms of programming languages? &gt; There are plenty of candidate categories, but none of them are nice to work with, because they're all deficient in some way. Do you have any references? As a concrete example of my claim to categorification of statistics, see [McCullah's What is a Statistical Model?](https://projecteuclid.org/euclid.aos/1035844977) in the 2002 Annals of Statistics. Of course, statisticians have widely not adopted this way of thinking, but my sense is that's mostly because statisticians just don't like categories. &gt;IMO, the thing we really need to make progress in this area is not searching for a new category but rather searching for a new logic. The fact that getting a CCC is impossible indicates that the lambda calculus is a poor internal logic for reasoning about measures. So rather than trying to shoehorn the category into giving us a lambda calculus, we should instead be asking what a calculus would look like which accurately reflects the category we have. This is the part I'm most confused about what you're talking about. I'm not familiar with internal logics, or how it would make a categorified concept more useful as a programming language. Do you have any examples of other fields that have used this concept in this way?
`Once` behaves like a polynomial space modulo `x^2`: `pure` has no linear term, multiplication of `a+bx` with `c+dx` yields `ac+(ad+bc)x+bdx^2`, the latter summand of which is discarded. We can thus derive some variants: -- polynomials modulo x^3 data Twice a = Twice a [a] [a] instance Applicative Twice where pure x = Twice x [] [] liftA2 op (Twice a b c) (Twice d e f) = Twice (op a d) ((op a &lt;$&gt; e) ++ (op &lt;$&gt; b &lt;*&gt; pure e)) ((op a &lt;$&gt; f) ++ (op &lt;$&gt; b &lt;*&gt; e) ++ (op &lt;$&gt; c &lt;*&gt; pure d)) -- polynomials data Poly a = Poly [[a]] instance Applicative Poly where pure x = [[x]] liftA2 op x = (.) Poly $ flip foldr [] $ \y -&gt; zipWith (++) (op &lt;$&gt; x &lt;*&gt; pure y) . ([] :) -- polynomials modulo x^2 - 1 data Parity a = Parity [a] [a] instance Applicative Parity where pure x = Parity [x] [] liftA2 op (Parity a b) (Parity c d) = Parity ((op &lt;$&gt; a &lt;*&gt; c) ++ (op &lt;$&gt; b &lt;*&gt; d)) ((op &lt;$&gt; a &lt;*&gt; d) ++ (op &lt;$&gt; b &lt;*&gt; c))
I don't think that what you say here matches with your previous comment. `Nothing &lt;*&gt; x` and `Nothing &gt;&gt;= f` both must necessarily short-circuit: there is no other possibility. The distinction is when presented with Just values: `Just f &lt;*&gt; Just x` must produce some Just value (specifically `Just (f x)`), whereas `&gt;&gt;=` is more powerful. `Just x &gt;&gt;= f` may produce either a Just or a Nothing, at the discretion of `f`. I like to think of it more like: an applicative computation has its "context" or "structure" fixed, derivable only from the context of the values passed to it (the `f` in `f a`). In the case of Maybe, the context is whether those values are all Just, or whether some of them are Nothing. A monadic computation is more powerful: it may use its "content" (the `a` in `m a`) to determine the shape of its "context" (the `m` in `m a`). In the case of Maybe, this means that a function we bind to may choose to abort processing by changing `Just x` into `Nothing`.
I think LogicT and all related libraries are more related to constraint programming than to logic programming . They lack the double matching necessary for programming with rules. So they don't fit with what is needed in this case. In the other side, there are implementations of Prolog in Haskell as EDSLs.
However function is something very general. Something that is more akin to extraction of information from a container of data is a comonad. So multiple steps of learning/recognition could be modelled by a comonadic chain. Maybe?
A point to keep in mind is you would have had the same issue if a new constructor had been added and utilized by `error`
When there is both an Applicative and a Monad instance for a given type, they must agree. mf :: Maybe (Int -&gt; Int) mf = Just inc mg :: Maybe (Int -&gt; Int) mg = Nothing mn :: Maybe Int mn = Just 3 -- the following two expressions must be equivalent -- (though the execution of the applicative may be optimized) mg &lt;*&gt; (mf &lt;*&gt; mn) do g &lt;- mg -- this is where it short circuits, because mg = Nothing f &lt;- mf n &lt;- mn let n' = f n let n'' = g n' return n'' The applicative expression does not really "continue on" any more than the monadic expression. All it has to do is look at `mg` and see that it is Nothing in order to bail out of the computation with Nothing as the result.
Ouch. Well, I tried to search for a precedent. It's hard to find types from its functionality... Did you know this from the beginning, or is there an search strategy? 
I have been working on [`pkgdbgc`](https://github.com/lspitzner/pkgdbgc), a utility that performs garbage collection on cabal new-build "store"s (the global package database). A first (or second, as we think more per-component now :) reverse-dependency of `cabal-plan`. (It is underdocumented at this stage, of course. You can basically register roots in the garbage-collection sense in a separate, global register; e.g. build (dist-newstyle) dirs, and then automatically get rid of any packages in the global db not reachable from any known roots. The interesting bit in the dist-newstyle directory is exactly the `plan.json`, so this is where `cabal-plan` comes into play.
Now refactor this as a type indexed by polynomials! Another generalization is power series/combinatorial species.
&gt; whenever the codomain of g is the domain of f - there should also be a composed arrow¹ g∘f:X→Z Just wanted to point out the typo, got your domain and your codomain mixed up there.
We use them at my work often enough and for the same tasks you would use dependent types in idris for - any time you need a type at the value level or vice versa. (De)serialization is a big part of it. Idris reads much easier though. Dependent types are really messy in Haskell comparatively.
[removed]
&gt; Can you be more specific about what this would buy us in terms of programming languages? Ignoring measures for a moment, let's just think about lambda calculus. If we only have a formalization for one (class of) type, then we only know how that one type works, we don't know anything about what other types might exist nor about how the various types interact. For example, only having a semantics of `a -&gt; b` in isolation can't teach us that we have isomorphisms like `(a -&gt; b -&gt; c) ~ (b -&gt; a -&gt; c)` or `((a,b) -&gt; c) ~ (a -&gt; b -&gt; c)`. Whereas, if we have the entire category of types, then we can start to look at these sorts of interactions. And by analyzing that category and proving things about it we can discover that certain types must exist or must behave in a certain way. For example, what do CCCs buy us for LC? Well, one things CCCs have is categorical products; so that means LC is going to have all the (finite) ways of tupling things together— even if we're only dealing with STLC and so can't give a single polymorphic datatype for capturing all those tuples, still we know they must all exist. Another thing CCCs have is categorical exponentials; in effect, this means LC can freely equivocate between functions-as-procedures (aka: morphisms) vs functions-as-data (aka: exponentials), because we can demonstrate an isomorphism between procedures and data. Indeed, this closedness is the crucial property that makes LC (and CCCs) so special. Now let's turn our attention back to measures/measure spaces/measurable spaces. We do indeed have a canonical way of taking the [product](https://en.wikipedia.org/wiki/Product_measure) of two measures, so that means tupling makes sense, with some caveats. N.B., when I say "tupling makes sense" I mean that both the construction of tupled values, as well as taking projections from tuples (via marginalizing over the other components) make sense. If we switch to quantum probability (or similar) then that would no longer be true, since the projections don't work quite right. However, do beware that the product measure may not count as a categorical product, depending on how you set the category up. One issue here is that categorical products must be essentially unique, yet the product measure is only unique when the component measures are σ-finite. (σ-finiteness is required [for lots of nice things](https://en.wikipedia.org/wiki/Fubini%27s_theorem) though, so restricting yourself to σ-finite measures is almost surely desirable.) Another issue is that the measure-theoretic product doesn't preserve all measure-theoretic properties. The canonical example being that it doesn't preserve completeness; thus, a product of Lebesgue measures is not the Lesbegue measure of products. So either you allow non-complete measures in the category, which means your language gets tuples exactly à la lambda calculus but these algebraically/syntactically nice tuples aren't measure-theoretically/semantically nice. Or you forbid non-complete measures and instead take the completion of the product measure, which makes your language's tuples semantically/measure-theoretically nice but completion is non-algebraic so you end up with some weird tuples that can't show up in traditional lambda calculus. There are other details to bear in mind about product measures, but I think you get the point. As a point of reference, note that these sorts of issues about products also show up in lazy lambda calculi. In domain theory (which gives the semantics for lazy LC) we have two different notions of "product": one where we get eta-expansion `⊥ = (⊥,⊥)`, and the other where we get strictness `(e,⊥) = ⊥ = (⊥,f)`. Haskell's tuples are a compromise between these two, which is why they lack the nice properties of either. But the real issue is that no matter how you set up your category of measure(able) spaces, you don't have exponentials. (Unless you pick a really boring category where all the spaces are finite.) This means we can't freely equivocate between functions-as-procedures vs functions-as-data; thus, (in general) we don't get higher-order functions and we don't get lambdas/closures. Sure, *some* measurable spaces `A` and `B` allow us to convert the hom-set `A -&gt; B` into a measurable space `B^A`; the problem is that *not all* spaces can do that. More particularly, the problem is that whether it's doable is determined by semantic matters rather than algebraic matters; which means we can't use the usual sort of syntax-directed type system to decide whether a particular lambda/closure is allowed or not. The force of this is two-fold. First, it means your language needs to make a hard distinction between procedures vs closures. Second, because there's some obscure non-syntax-directed mechanism for deciding whether a given closure is allowed or not, it's going to be kinda inscrutable and obnoxious for users to program in. &gt; Do you have any references? Alas, there isn't a really good reference for a lot of this stuff. I can give references for the bits and pieces, but there isn't really any good reference that brings it all together. &gt; This is the part I'm most confused about what you're talking about. I'm not familiar with internal logics, or how it would make a categorified concept more useful as a programming language. Do you have any examples of other fields that have used this concept in this way? Depending how much category theory and type theory background you have, you can check out [the n-lab description of internal logics](https://ncatlab.org/nlab/show/internal+logic) as a way to get started. The tl;dr is that given some (class of) category `C`, the internal logic of `C` is a calculus that captures all the crucial properties of the category. That is, rather than thinking about commuting diagrams and co/limits and such, we can instead use the internal logic as a way of playing all our usual algebraic symbol-pushing games and be comfortable in the fact that whatever the internal logic does the category does too. In a way it's kinda the opposite of denotational semantics. In denotational semantics we start with some syntax/calculus and try to find a semantic model that "does all the same things", whereas internal logics start with the semantics/denotation and try to find the syntax/calculus. Re other fields using it, the place to start is topos theory. Or, it's probably easier to look at logoi rather than topoi (i.e., conceptually easier; alas, it's harder to get literature on logoi than on topoi). Topoi are all about what it means to be a "nice place to work with spaces", whereas logoi focus on logics rather than spaces. The key thing here is they both establish what the heck quantifiers are all about. That is, we often have logics with quantifiers like `∃`/`∑` or `∀`/`∏`, but what the heck do those mean? Topos theory gives a good idea of what they mean and where they come from, and this has been used to apply those quantifiers to categories that happen to be topoi/logoi but which were never previously thought about as having quantifiers like that. Similar things show up with the categorification of modal logics, linear logics, etc. Basically anywhere you get a quantifier or modality. [Dan Licata](http://dlicata.web.wesleyan.edu/pubs.html) gave a great talk at this past FSCD about categorical modalities. I don't recall him using the term "internal logic", but that's definitely the angle he was coming from imo.
Are you familiar with Hakaru [\[code\]](https://github.com/hakaru-dev/hakaru) [\[site\]](https://hakaru-dev.github.io)? We've been working quite hard on this line of thought :)
Though I haven't had much time to work on it of late, you may want to check out [bytestring-trie](http://hackage.haskell.org/package/bytestring-trie) which includes a number of optimizations folks typically miss when making trie data structures.
Oh yeah for sure, I've seen the `singletons` library. Brilliant workaround for the lack of built in dependent typing, but also a bit painful to use for sure. I didn't mean like what function do dependent types serve, I was asking more about instances where they came into play that aren't often seen in the existing literature. We've all seen `Vect` a million times, just looking for other practical, perhaps even domain specific uses.
Nice! What was your usage, which doesn't seem like QuickCheck as preexistent version or mine version? For performance, I think Logic is more appropriate in place of []. `fmap` and `append` are both O(1). If you don't need to inspect the intermediate structure, it's faster than Seq.
I don't compile GHCs myself. My desktop is Ubuntu so I simply use https://launchpad.net/~hvr/+archive/ubuntu/ghc AFAIK you have to do some patching to build older GHC on newer distributions, ping /u/hvr For macOS, we made a script, `install-on-macos.py` https://haskell.futurice.com/ to install 7.10.3-8.2.2 versions. Older GHC need very-untrivial patcing (bumping `unix`) to work on newer macOS For Windows, I *would* try https://chocolatey.org/packages/ghc but I don't know how well it works (whether you can have multiple GHC for example)
Indeed it is, thank you for the correction :)
you don't need any higher mathematics for this stuff -- TAPL is already a basic introduction to type theory that introduces all the core notations and ideas. You may also enjoy harper's pfpl (https://www.cs.cmu.edu/~rwh/pfpl/2nded.pdf) and at a more advanced level (but again, directly accessible if you read carefully and aren't afraid to google a bit here and there) pfenning's class notes on constructive logic: https://www.cs.cmu.edu/~fp/courses/15317-f09/schedule.html 
This isn't an easy question to answer. The mathematics required to understand TAPL is all basically covered in the second chapter. It's not really taught in high school at all, so that chapter is your go-to source. But it does take a while to make sense of if you're new to abstract mathematics, and you sometimes don't have the space to do that in one chapter of a book. You have a couple other options: This content is sometimes covered in university Computer Science departments under the name "Discrete Mathematics", usually a class for freshmen or sophomores. There are lots of textbooks for that, though I don't know a particularly good one. It's taught in a few Mathematics departments under a title like "Introduction to Proofs" or similar. [Here](https://books.google.com/books/about/Proofs_and_Fundamentals.html?id=QJ_537n8zKYC) is a textbook designed for such a course. Both of these are overkill for understanding Pierce, but they do give you more room to grapple with the ideas. If you haven't studied math since high school, the other thing that might hold you back is knowing *how* to read mathematics. You don't read it like a novel. It is a common mistake to read all the words in a math book, and then feel dumb because the ideas didn't make sense. In fact, you want to do things a bit differently: - When reading for the first time, focus on the *definitions* and *examples* (and counter-examples or non-examples: examples of things that don't meet a definition). These are what give you an understanding of what you're basically doing. - When it starts to become hard to follow, go back through, and this time look for any mathematical arguments (proofs) or example computations, and try to work them out in detail, until you could explain them to a stranger yourself without just reciting words from the book. - Do the exercises, and set a standard for yourself that you will understand what you did. You don't need an answer key: if you're not sure whether you have the correct answer or not, then you are guessing too much. Check all of those guesses, and understand why they must be right (or, perhaps, why they were wrong) before you accept them. This manner of reading takes many times longer than reading a book straight through word by word, but that's what mathematics is all about. So first off, maybe give that a try for the second chapter of Pierce's book. They augment if you need more help.
Any reason for that `{-# INCOHERENT #-}` pragma? The second instance is already more specific than the first and in my experience instance resolution sometimes tends to pick an incoherent instance even when there are more specific instances available which is probably not what you want.
probably the best discrete mathematics book is the graham, knuth (yes, that one!), patashnik book "Concrete Mathematics". you don't need a word of it to understand type theory, but it's a great read.
Interesting. I've only ever used hspec (and a bit of HUnit on top of which hspec is built), I don't know much about tasty. Could someone who has experience with using both hspec and tasty say how these two compare?
It's to allow situations like the second one above where the monads are still abstract. liftSub :: MonadSub b m =&gt; b a -&gt; MaybeT m a `MonadSub b b` unifies with `MonadSub b (MaybeT m)` (because `b` could be equal to `MaybeT m`) but does not match it (because `b` is not necessarily equal to `MaybeT m`), so if it weren't marked incoherent the search would fail. (c.f. [the last step in the summary of instance resolution in the GHC user manual](https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/glasgow_exts.html#overlapping-instances)) It's true that most of the time instances should not be marked incoherent, but in this particular situation it seems to do something sensible. Note that, given only `MonadSub b b` and `MonadSub b n =&gt; MonadSub b (t n)`, there is actually at most one possible derivation of `MonadSub b m` for any ground types `b` and `m`. That observation is independent of the overlap annotations, which affect how the compiler actually selects instances. Moreover, there is no unexpected behavior: when there is an instance, all it does is `lift` repeatedly from `b` to `m`, that seems benign enough. Thus this is safe in a formal sense because the worst that can happen is that the compiler fails to find an instance: it will not let a program go wrong. This is still not a perfect solution because formal safety has blind spots. Here are more cons to add to my previous list: - the error messages are insane; - there is potentially an infinite loop at compile time (we can pick `MonadSub b m` which results in a new `MonadSub b n` constraint, ad infinitum; GHC has a depth limit of course); - complexity increases the potential for a confused user with a deadline to proceed with random trial and error until an incorrect program compiles.
Adding to what /u/phadej already said, it's also worth mentioning /u/erikd's [`jenga` tool](https://github.com/erikd/jenga) if you're interested in using Stackage snapshots with cabal. Also note that `cabal new-build` supports GHCs back till GHC 7.0 (while old-build even supports GHC 6.x), whereas ["Stack will almost certainly fail with GHC below version 7.8"](https://github.com/commercialhaskell/stack/blob/98c51cafb038bf09b58d5607956fb199d3660735/src/Stack/Setup.hs#L383).
&gt; A first (or second, as we think more per-component now :) reverse-dependency of `cabal-plan`. If you only count packages on Hackage... I've got local scripts/tools which use it as well, and the implementation of http://matrix.hackage.haskell.org heavily relies upon `lib:cabal-plan` as well... :-)
TIL from #haskell about ghc binaries from /u/merijn. So I think I am switching
Of course, if there is a nicer way of doing this, this would also be very interesting to look at. Essentially, what we want is having a general API which is not specialized to a specific monad stack in the presence of callbacks whose monad is not fixed in our API either.
Tasty is just a way to organize your tests, really. You can use tasty with hpsec, hunit, quickcheck, smallcheck and more. The adapater packages are aptly named `tasty-hspec`, `tasty-hunit`, etc.
Interestingly, you also get a comonad out of it. I *believe* the laws hold data Succs f a = Succs a (f a) deriving Functor instance Alternative f =&gt; Applicative (Succs f) where pure a = Succs a empty (&lt;*&gt;) = ap instance Alternative f =&gt; Monad (Succs f) where Succs a as &gt;&gt;= k = let Succs b bs = k a bs' = extract . k &lt;$&gt; as in Succs b (bs &lt;|&gt; bs') instance Alternative f =&gt; Comonad (Succs f) where extract (Succs a _) = a duplicate x@(Succs _ as) = Succs x (fmap pure as) instance Alternative f =&gt; ComonadApply (Succs f) 
Absolutely. I suspect that in a case like this, it might actually be breaking less if it was made a breaking change, changing the constructor and getting a compile-time failure.
https://www.reddit.com/r/haskell/comments/7lb2zx/proposal_monthly_package_attack/ds8nu90/ nominates [vector-algorithms](https://hackage.haskell.org/package/vector-algorithms)
Makes sense. &gt; I guess the only real answer at the moment is "hardened apps only live on hardened appliances, where hardened appliances never run untrusted code..." Even that is not an answer anymore. A significant amount of software is run as cloud services, where other people's code is running on the same hardware as yours, at random. Yet users still do expect reasonable protection. It's impractical to say "don't use the cloud unless it's OK for everything you do to be publicly visible".
Any info when Intermediate Haskell book will be realesed? 
There's also [1haskellADay](https://github.com/geophf/1HaskellADay)
The web page gives "Error establishing a database connection". I appreciate the irony but don't think it's intentional... EDIT: OK, it's up again!
[removed]
And of course you can even write (fmap . fmap) as (fmap fmap fmap).
I blame wordpress for that...
My main issue with pattern synonyms is that I can't have separate signatures for matching and construction, which forces me to take redundant combinator names.
Good idea! I raised it at https://github.com/ndmitchell/hlint/issues/422
Hey, I don't have an answer for you yet, but a question: Why does it say in your `HttpExceptionRequest` `host = "api.github.com"` but in your curl you go to `s3.amazonaws.com`? Also, are you around right now so you could jump on some form of live chat? I would very much like to see this failing and debug it in action.
For you or anyone debugging this: I recently got a PR merged to the Network package that prints the parameters passed to getAddrInfo, so that you can go straight to that function and hopefully see why it’s failing. You’d need to build with the master version of network to get it, though, so it may not be worth the extra hassle. https://github.com/haskell/network/pull/289
That is very useful, thanks!
Thanks for the detailed writeup. &gt; Ignoring measures for a moment, let's just think about lambda calculus... All the advantages of having a CCC make sense to me. &gt; Now let's turn our attention back to measures/measure spaces/measurable spaces.... I agree that there's a number of problems with the existing measure theory formulation of probability, but I don't think that is in practice a problem for programming languages. My sense is that all these problems are rooted in the fact that the traditional formulation of the real numbers is much more complicated than mathematicians of the early 1900s thought before measure theory was properly formulated. I don't think this is in practice a problem because: 1) In practice, we don't need to work with the real numbers. We will be doing some sort of numerical integration method, and these numeric integration methods don't really make sense for all the pathalogical cases. No practitioner really cares if they can integrate a function over the rationals, for example. I'd love to see some cool examples where these sorts of pathalogical cases could have a plausible use programmatically, but my sense is that there's only toy examples. 2) There are alternatives to measure theory that (I believe) avoid all the obstacles you've mentioned. For example, the [Radically Elementary Probability and Statistics](http://www.stat.umn.edu/geyer/nsa/) uses only 'finite' sample spaces, and thus doesn't have any of the pathologies induced by the real numbers. Finite is in scare quotes because they use a non-standard model of finite numbers which is closer in an intuitive sense to "real numbers plus infinities and infinitesimals". Everything useful (such as central limit theorems) that can be done in measure theory can be done in this non-standard analysis, and all the simplifying properties of finite spaces still hold. &gt; Second, because there's some obscure non-syntax-directed mechanism for deciding whether a given closure is allowed or not, it's going to be kinda inscrutable and obnoxious for users to program in. Programmers in a PPL will have much more difficult issues than this to worry about. Since the language performs integration with some numeric algorithm, programmers will have to worry about the details of how the algorithm works, which means they will always be analyzing non-trivial properties not present in the syntax of their code. &gt; Dan Licata gave a great talk at this past FSCD about categorical modalities. I don't recall him using the term "internal logic", but that's definitely the angle he was coming from imo. Thanks for the reference. After a quick reading of the paper, I think I get the general idea of what you mean, although the paper's definitely above my comfort level in this field.
They say "_It will be released at the beginning of 2018_". Beyond that I have no idea; you should just ask the authors. Personally I am anxious to see it released in time!
Yes, I think the link works with newer Haddock versions (pointing to the explanation and examples right below the entry for the `Seq` type itself), but I'll have to double check. `Data.Sequence` does have a few fusion rules, but it doesn't have a full fusion framework like lists do. Just basics like `fmap f . fmap g = fmap (f . g)`, `traverse f . fmap g = traverse (f . g)`, `fmap f . reverse = reverse . fmap f = mapReverse f`, and `f &lt;$&gt; xs &lt;*&gt; ys = liftA2 xs ys`. The general idea is that we have simple rules that sometimes help and never hurt; we try to avoid intrusive ones that can potentially cause trouble. Do you think I should document these rules in the Haddocks, or would that just intimidate people more than it helps them?
Thanks for the feedback so far; I look forward to reading more later. Do you know how to tell Haddock to qualify a particular occurrence of a name?
I'm pretty sure GHC now supports PIE.
FWIW, I'm guessing you didn't have the dependency in your cabal file. The project nix-shells won't put a dependency in your shells just because you listed it in `overrides`. It figures out which dependencies to provide by looking at the cabal files. The `overrides` field just lets you override how to build dependencies if it needs them.
oh -- you're right. not any released version, but coming in 8.4: https://ghc.haskell.org/trac/ghc/ticket/13702
Mathematics never made any sense to me in college. But then when I started doing my PhD I started reading Coq'Art, and it all started to click. I'm a programmer at heart, not a mathematician. What the hell is induction? But recursion, recursion is simple. Turns out, they're the same. Curry-Howard isomorphism, orgasmicly good, to cite a memorable quote from SPJ :) Coq'Art is a great book, the pace is not too high, the exercises are good, and at the end of it you'll both understand dependent types and have a great intuition for constructive mathematics, and know how to use a theorem prover to boot. Cannot recommend it highly enough.
Computer science in general hasn't been around for all that long, and it'd be incredibly difficult to provide an objective, 100 year assessment on the impact of a particular programming paradigm. So, sure, it's still certainly possible that the long term implications of discoveries in this arena could be of great benefit. And it is not my intent to devalue the effort anyone puts into this arena. But when it comes down to a purely charitable contribution to human welfare, I would hope that priority would generally be given to those in need - And we are certainly not wanting for lack of genuinely productive charitable causes in need of contribution.
See also [semantic editor combinators](http://conal.net/blog/posts/semantic-editor-combinators) for more of this sort of thing.
Brilliant! Thanks! 
&gt; Do you know how to tell Haddock to qualify a particular occurrence of a name? I'm not sure there is such functionality. I've been using `@"Data.List".'Data.List.filter'@` thus far (`"` for module links, fully qualified `'` for linking to the right `filter`, `@` for monospaced module links).
https://github.com/fpco/stackage/issues/3185
Dependency changes are the entire point of revisions.
You'll be glad to hear that Matt Renaud has come up with a bunch of good examples for his `containers` tutorial documentation. We are planning to add many of those to the Haddocks, and probably add even more. My only concern about having lots of examples is that doing so reduces the number of functions that appear on one screen. Since most of the `containers` modules are rather large, I'm a bit worried that expanding the documentation will make them even harder to navigate. Edit: It looks like the Haddock "synopsis" panel is supposed to help with this, but unfortunately it appears to be ~~almost useless for a module with many exports, and not so great for a module with even one very long type signature.~~ somewhat unintuitive for modules with many exports or with long type signatures. Edit to edit: it *was* almost useless, but it seems to have been improved.
Not all dependency changes are created equal. I don't see how tightening the lower bound fits into https://github.com/haskell-infra/hackage-trustees/blob/master/revisions-information.md#why-do-we-have-revisions
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [haskell-infra/hackage-trustees/.../**revisions-information.md#why-do-we-have-revisions** (master → d2867a1)](https://github.com/haskell-infra/hackage-trustees/blob/d2867a11612dcecff2d79f941c3744d570597f1c/revisions-information.md#why-do-we-have-revisions) ---- [^delete](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dsfwe47.)
This would be a case of: &gt; a build-plan turns out to be incorrect, and we wish to tighten bounds to exclude incompatible versions of dependencies Seems like some `tagged` version between 0.7.3 and 0.8.5 broke the package. So I doubt it's for "no good reason." I'm personally not such a fan of revisions. I'd rather see the fourth version component bumped, since this is an allowable way to update old versions of a package. But there's a ton of nuance to the issue that I admittedly don't fully understand.
If something *broke* then wouldn't *newer* versions have to be disallowed (= upper bounds tightened or «The Lagging Way» described in https://github.com/haskell-infra/hackage-trustees/blob/master/cookbook.md#the-lagging-way) instead of older ones as in the revision bump in question? I simply think in this case revisions were misused, and Hackage tooling shouldn't allow that, does it make sense?
I'm indeed quite glad to hear more examples are inbound. [This is what you are referring to](https://m-renaud-haskell-containers.readthedocs.io/en/docs/sequence.html), right? I quite like this indeed, having skimmed a bit through it just now. To address your concern: I myself am not worried about "too much documentation" at all, and here is why. I already use 4 superior ways of navigating a package's docs in addition to plain old scrolling up and down: - Ctrl+F to search on the page and jump to a relevant section. - Click on internal haddock links that link to related links (cf.) when the docs contain these. IIRC this haddock syntax allows for internal linking: `@localBinding@`. - Look at [the index page that contains all the bindings](https://hackage.haskell.org/package/containers-0.5.10.2/docs/doc-index-All.html) and use Ctrl+F to find. - Use the [table of contents](https://hackage.haskell.org/package/containers-0.5.10.2/docs/Data-Sequence.html#g:1) to jump to sections when they are meaningfully defined.
We ran into this particular issue due to semigroupoids as well. A couple thoughts: - I see the utility of revisions when not using a frozen dependency snapshot; the fact that it hasn't bitten us before now (afaict) is probably evidence that this is an important tool and worth the risk - it seems like there's not quite adequate guidance as to the pitfalls of various changes you might make in a revision, but being able to see that old stackage LTS snapshots continued to resolve would be really useful as a test - I can't really see why `stack` wouldn't just ignore revisions (though I'm sure this has been discussed before); it seems as though it should be on curators of stackage to revise a snapshot version if it's found e.g. that a package has a Bad Bug, or is in some way logically incompatible with another package. Maybe these failures are the mechanism by which stackage curators/maintainers learn about just those sorts of issues.
That could make more sense. But FWIW, it's impossible to tighten constraints at all without breaking at least *some* theoretical build. If someone had been using semigroupoids with tagged-0.8.5, then your suggestion would break their build. Not saying I know what should have been done in this case; just that it's a nontrivial issue.
Heh, I was wondering why this seemed really basic until I checked the date on it. It's aged pretty well!
Yes, those are the ones.
&gt; just ignore revisions Seems like the right approach to me.
cf https://github.com/commercialhaskell/stack/issues/3723 and https://www.snoyman.com/blog/2017/04/stackages-no-revisions-field i too don't see why a snapshot doesn't explicitly pin down revisions at the time of snapshot.
Ah yea the default reflex backend on macOS depends on WKWebView, which I believe is newer than Yosemite. The jsaddle-warp backend should work, and i wonder if jsaddle-webkit2gtk would work...
You can think of Applicative as a higher-kinded Monoid in a way, and Const's Monoid constraint is a witness to that. `pure` is like `mempty`, and `&lt;*&gt;` is like `mappend`. You can really look at `Const` as a way of lifting a Monoid to its "dumb Applicative": pure :: Monoid w =&gt; a -&gt; Const w a (&lt;*&gt;) :: Monoid w =&gt; Const w (a -&gt; b) -&gt; Const w a -&gt; Const w b Look at those type signatures and "forget" the second parameter of Const, and you get basically: pure :: Monoid w =&gt; {w} -- the parameter is ignored pure = mempty (&lt;*&gt;) :: Monoid w =&gt; {w} -&gt; {w} -&gt; {w} (&lt;*&gt;) = mappend
Yes, that's probably better. I'm not sure how much I want to use pattern synonyms in the Haddocks, though. They certainly improve the user experience considerably, but `containers` is historically a "portable" package. To the best of my knowledge, pattern synonyms are not at all portable as yet, and they aren't even particularly well-established within the GHC ecosystem. I'm open to being convinced to use them liberally in the documentation anyway. Of course, if they show up in a similar form in other Haskell variants (PureScript, Frege, whatever else), that will pretty much automatically change my mind.
Can you give an example of why you would want this? I personally think that if `x` matches a constructor or pattern synonym `Y`, then `Y` and `x` should be equal, which is a much stronger requirement than just that they must be the same type. So without further information I would probably be against such separate signatures.
`ToJSON a` vs. `FromJSON a` Using `lens-aeson` today I could package pattern JSON :: (FromJSON a, ToJSON a, AsJSON t) =&gt; () =&gt; a -&gt; t pattern JSON a &lt;- (preview _JSON -&gt; Just a) where JSON a = _JSON # a But without the ability to have different constraints, I have to take 2-3 names: One for a 'JSON' pattern that just works, but needs too many constraints, one for a function that just does construction, because it isn't a pattern, and another pattern that just does matching. I'd rather just have one pattern with two signatures. One for matching and one for construction so that I can take precisely one name for one nominal purpose. I don't want a little bit of syntactic convenience to cost me to have to pass completely unnecessary dictionaries to no end but prescriptivism. In my BDD code I have to maintain just such a function `bdd`: https://github.com/ekmett/coda/blob/9c1902f860df5b62dda1b0b33df1fb14928279b0/lib/bdd/Data/BDD.hs#L158 despite it being redundant with the pattern `BDD`: https://github.com/ekmett/coda/blob/9c1902f860df5b62dda1b0b33df1fb14928279b0/lib/bdd/Data/BDD.hs#L179 and I have to also supply `BDD_` for when I just need the matching side. https://github.com/ekmett/coda/blob/9c1902f860df5b62dda1b0b33df1fb14928279b0/lib/bdd/Data/BDD.hs#L179 There are plenty of these situations where you have a pattern you can always match on, but can only construct with a little more "oomph".
Ah yes, I momentarily forgot about that fake definition.
I see hackage revisions as mostly for dependency changes and it is an important, useful, tool for making the ecosystem function. Stack borrowing from Hackage is good but such borrowing needs to be done with awareness of this aspect - as jberryman mentioned, you can just ignore new revisions.
I generally try to avoid complicated build plans, full of disjoint ranges, largely due to old bugs in the backtracking behavior of cabal that probably aren't relevant anymore, so whenever I find a minor version that is broken that I can't work around, I'll often advance the floor or ceiling depending on if its a new or old version of the package. Another scenario that arises is that a lot of times the lower bound given is because folks assume they only need a given set of features. e.g. if I don't need anything particularly new out of `base`, so I'll set it to go back to some older version, these assumptions are sometimes flawed. 99% of the time I get these right, but when hard evidence is presented that such a build can fail, I'll often update the package with a revision. Builds should fail at dependency resolution time rather than try and fail after dependency resolution has fixed the build plan. Should I be perfect and check everything rigidly on every version of every compiler with every version of every dependency? Sure. It'd be nice. But I'm not that disciplined, and the combinatorial explosion is horrendous already even for what testing I do. Finally, This ensures the largest class of versions work with my packages. Other times. I can work on an old version of package x or an old version of y but both both, or I can work with old versions of x and y together, and new versions of x and y together, but when I mix those I break, and these interactions may not be obvious. I've run into all of these scenarios in the wild. 
I agree this is suboptimal and there isn't currently a testing procedure available to check to see if a revision will break any stackage releases. Without such a tool, you'll keep seeing this sort of thing from time to time. &gt; I appreciate that LTS is over 3 years old Keep in mind the support window for "LTS" releases in stackage is ~3-4 months, not multiple years like LTS in Debian. A 3 year old LTS release has literally reached the moral equivalent of Debian old-old-old-old-old-old-old-old-old-old-old-old-old-stable ;) As a stopgap, you can force `--allow-older=tagged` to allow the plan.
Ah that is a good point I didn't think of. I was thinking of truly different non-overlapping type signatures, which I should not have limited myself too. I agree that in the situations you have listed it would be much better to have that ability. 
You're gonna need to either use a `let` or pass in all the required context as parameters.
Snapshots do pin down revisions - that is described directly in the blogpost you linked to: &gt; When a Stackage build runs, until now it would always take the most recent revision of a .cabal file and use that for bounds checking (and other activities). Then, when creating a snapshot, it would include a hash of the revision of the .cabal file it used. That hash is in turn used by Stack to ensure that—when building a package from a snapshot—it uses the same revision for reproducibility. I haven't dug up when revisions were added to snapshots, but I think lts 3 was too early. Old snapshots were not updated, because, unlike hackage packages, they are not mutable. See also https://hackage.haskell.org/package/acme-mutable-package Also, I don't see the relevance of #3723, that is a detail of how stack behaves for version constraints, and how the code should be structured for them. Nothing to do with package revisions.
Btw, why do you want to do this translation?
You can put both the modify and the read into a single combined helper?
As far as I know Idris introduces linearity trough data types instead of function arrows.
This is pretty cool! Will try using it in my scripts.
Ok, that clears it up -- this is just an artifact of using a particularly old snapshot then.
Any lts-6 and onward, and nightlies roughly around 2016-05-25 and onward all specify hackage revisions, so that they are not affected by these mutations. This was not retroactive, though. Perhaps stackage snapshots that don't have revision information should not pay attention to any revisions. Could attempt to figure it out based on the date of the snapshot publication and the dates of the revisions. Seems complicated. The workaround for this is simple just add `allow-newer: true` to your `stack.yaml`. This is suggested by the plan construction. I'm quite surprised that you got that configuration error, actually. For `stack build --resolver lts-3.22 semigroupoids` I get ``` Error: While constructing the build plan, the following exceptions were encountered: In the dependencies for semigroupoids-5.0.0.4: tagged-0.8.2 from stack configuration does not match &gt;=0.8.5 &amp;&amp; &lt;1 (latest matching version is 0.8.5) needed since semigroupoids is a build target. Some potential ways to resolve this: * Recommended action: try adding the following to your extra-deps in /home/mgsloan/.stack/global-project/stack.yaml: - tagged-0.8.5 * Set 'allow-newer: true' to ignore all version constraints and build anyway. * You may also want to try using the 'stack solver' command. Plan construction failed. ``` Both of the suggested resolutions fix the issue.
Maybe because you were supposed to come here? https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/index.html
That's not what I asked. I know how to find the information I need elsewhere (e.g. https://linux.die.net/man/1/ghc), thank you very much. What I asked is if I can have "man ghc" to work as with any other program. 
Very lucid explanation!
&gt; nightlies roughly around 2016-05-25 and onward all specify hackage revisions I see, so basically this was "fixed" in Stackage. Thanks for the tip, looks like the problem is not as bad as I thought.
&gt; we do not want to entirely ignore version bounds Sure. But why not use `--index-state`, so that the version bounds (and other package metadata) will be immutable from the POV of stackage?
You can blame me for this. We did similar `tagged` lower bump for `comonad`. See https://github.com/ekmett/comonad/issues/41#issuecomment-356257142 There I remembered to check that revisions shouldn't affect any Stackage LTS release (that was easy as lower-bound was lower than any `tagged` version in any LTS release). With `semigroupoids` it was different. The issue with semigroups-5.1 release was first reported on Jul 2016: https://github.com/ekmett/semigroupoids/issues/48, as it was with GHC HEAD (before 8.2), no revision was done, and only master fixed. Recently I was verifying `lens`'s lower bounds, and cabal new-build picked up older `semigroupoids` when I forced `tagged` to be `0.7.3` (previous lower bound): http://ircbrowse.net/browse/ghc?id=234152&amp;timestamp=1515256140#t1515256140 So Ryan did revisions for affected versions with `tagged &gt;= 0.8.5`. Another option would be to restrict upper bound `base &lt;4.11`, but that would made package versions completely uninstallable on GHC-8.2. *Not a clear choice*. The aim to make as precise revisions as possible, but for now we have not so sharp tools. It's not **yet** possible to *add conditional bounds*, i.e. if impl(ghc &gt;= 8.2) build-depends: tagged &gt;= 0.8.5 which would been enough to fix 8.2 build issues, and not invalidate Stackage snapshots. You can help by making a pull request allowing such revision in https://github.com/hackage-trustees/hackage-cli/blob/master/src/Distribution/Server/Util/CabalRevisions.hs (the same file is in hackage, but it's easier to develop in `hackage-cli`). The spec is *accept new conditional (i.e. `if`) sections, where condition doesn't depend on automatic flags (i.e. is static), only build-depends are specified, with only dependencies package unconditionally depends on*. 
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [hackage-trustees/hackage-cli/.../**CabalRevisions.hs** (master → 12663bb)](https://github.com/hackage-trustees/hackage-cli/blob/12663bb8743eb262d6bf4a61cf5705680147a7c1/src/Distribution/Server/Util/CabalRevisions.hs) ---- 
Doesn't that prevent extra-deps from bumping packages to versions newer than the index?
https://github.com/ghcjs/ghcjs/issues/602#issuecomment-352821037
But does AI make humans half as dangerous?
"Transpiler" is not even a word.
It is a word, since it's used by people. (I do agree that it is an unnecessary word that seems to imply compilers are magic. But it's still a word.)
I want to use the generated code from Ott in Haskell: http://www.cl.cam.ac.uk/~pes20/ott/
I always liked it, because it implies that it's an intermediate tool, that you're translating from source Language to source language. But I understand that all compilers are just translators.
Note that this comment was made 3 weeks before luite published this branch yesterday. I suspect that at least a few bugs have been fixed since then altough luite mentioned yesterday that there are still a few failing tests.
The problem of zipping two trees reminds me of [unification(-fd)](https://hackage.haskell.org/package/unification-fd).
Hadn't even thought of that! I think you could probably slot a unification program into a zip-fold quite easily. Might give it a go later today
Your programs pair up the elements of those tree types, not the nodes. It's a subtle difference, but in general if you're working with an AST you won't have elements per se. `Plated` thinks of trees not as polymorphic containers of values but as monomorphic containers of subtrees.
You might enjoy `Data.Align` and `Data.These` from the [`these`](https://hackage.haskell.org/package/these) package for handling the subtrees which are not shared in both trees. Also nicely allows for the trees to be parametrized over different types when being zipped.
That shit will be solved when file name and file content would be 1-1. Then reproductibility will be trivial. I think IPFS will be the solution for a lot of software problems. https://github.com/whyrusleeping/gx
I prefer: data ProcessStatus = ShouldTerminate | IsFinished deriving (Eq, Ord, Show, Read, Generic, Typeable)
Perhaps a Fedora subreddit is a better place to ask? GHC certainly has a man page on other distros I tried. But I don't know who's responsible for the fedora ghc package.
Or Warhammer 40k.
I believe GHC does not distribute man pages. There is a man page on [die.net](https://linux.die.net/man/1/ghc) and like many man pages it was written for the Debian GNU/Linux system. You can copy it from Debian, or file a bug with Fedora, or file a bug with GHC to try to get them to distribute it. 
Problem there is that then you'll just run into the same issue but with extra-deps. i.e. an extra-dep you specified could be "mutated" by a revision.
I remember trying to do a similar thing using [recursion schemes](https://hackage.haskell.org/package/recursion-schemes). The `fermo` fold could be written like this: fermo :: (Recursive f, Recursive g) =&gt; (forall a. Base f (a -&gt; c) -&gt; Base g a -&gt; c) -&gt; f -&gt; g -&gt; c fermo alg = cata (\x -&gt; alg x . project) The `forall` there isn't necessary, but I think it confines the fold to the kind of thing you're talking about. To find (for instance) if 2 trees are equal, you might do this: {-# LANGUAGE RankNTypes #-} {-# LANGUAGE TemplateHaskell #-} {-# LANGUAGE KindSignatures #-} {-# LANGUAGE TypeFamilies #-} {-# LANGUAGE DeriveFunctor #-} {-# LANGUAGE DeriveFoldable #-} {-# LANGUAGE DeriveTraversable #-} {-# LANGUAGE LambdaCase #-} {-# LANGUAGE FlexibleContexts #-} import Data.Functor.Foldable import Data.Functor.Foldable.TH fermo :: (Recursive f, Recursive g) =&gt; (forall a. Base f (a -&gt; c) -&gt; Base g a -&gt; c) -&gt; f -&gt; g -&gt; c fermo alg = cata (\x -&gt; alg x . project) data Tree a = Leaf | Node a (Tree a) (Tree a) makeBaseFunctor ''Tree equal :: Eq a =&gt; Tree a -&gt; Tree a -&gt; Bool equal = fermo alg where alg LeafF LeafF = True alg (NodeF x xl xr) (NodeF y yl yr) = x == y &amp;&amp; xl yl &amp;&amp; xr yr alg _ _ = False I think this is a reasonable definition for "rebuilding the tree" also: zipo :: (Recursive f, Recursive g, Corecursive h) =&gt; (forall a b. Base f (a -&gt; b) -&gt; Base g a -&gt; Base h b) -&gt; f -&gt; g -&gt; h zipo alg = fermo (\x y -&gt; embed (alg x y)) (again, the `forall` is unnecessary) zipTree :: (a -&gt; b -&gt; c) -&gt; Tree a -&gt; Tree b -&gt; Tree c zipTree f = zipo $ curry $ \case (LeafF,_) -&gt; LeafF (_,LeafF) -&gt; LeafF (NodeF x xl xr,NodeF y yl yr) -&gt; NodeF (f x y) (xl yl) (xr yr) Given that the algebra here is basically `&lt;*&gt;`, you could probably make that explicit: zipRec :: (Recursive f, Corecursive f, Applicative (Base f)) =&gt; f -&gt; f -&gt; f zipRec = zipo (&lt;*&gt;) Of course these require much more boilerplate than the plated solution, but I think they're interesting nonetheless. 
I have a hard time following the exact details of your question, but here are a few facts about error handling which seem relevant. First, please don't use `SomeException`, it will catch _all_ exceptions, including asynchronous exceptions, and you don't want that because it breaks functions like [`timeout`](http://hackage.haskell.org/package/base-4.10.1.0/docs/System-Timeout.html#v:timeout) and [`cancel`](http://hackage.haskell.org/package/async-2.1.1.1/docs/Control-Concurrent-Async.html#v:cancel). Instead, error-handling primitives are typically polymorphic in `e`, so you can choose which exceptions you'll catch: &gt;&gt;&gt; import Control.Exception &gt;&gt;&gt; :t try try :: Exception e =&gt; IO a -&gt; IO (Either e a) For example, if I instantiate `e` to `ArithException`, I'll catch division by zero exceptions but not `undefined` exceptions: &gt;&gt;&gt; try (evaluate (42 `div` 0)) :: IO (Either ArithException Int) Left divide by zero &gt;&gt;&gt; try (evaluate undefined) :: IO (Either ArithException Int) *** Exception: Prelude.undefined ... Whereas if I instantiate `e` to `ErrorCall`, I'll catch `undefined` exceptions but not division by zero exceptions: &gt;&gt;&gt; try (evaluate (42 `div` 0)) :: IO (Either ErrorCall Int) *** Exception: divide by zero &gt;&gt;&gt; try (evaluate undefined) :: IO (Either ErrorCall Int) Left Prelude.undefined ... Note that the `Show` instances for exceptions are a bit confusing: &gt;&gt;&gt; show DivideByZero "divide by zero" &gt;&gt;&gt; show (ErrorCallWithLocation "undefined" "Main.hs:1:1") "undefined\nMain.hs:1:1" So above, the `***` lines indicate that the exception was not caught by the `try` and is instead being printed by `ghci`, whereas the `Left` lines indicate that the exception has been caught. If the `Show` instances were more reasonable, we'd see this instead: &gt;&gt;&gt; try (evaluate (42 `div` 0)) :: IO (Either ArithException Int) Left DivideByZerl Anyway, how did I figure out that `ArithException` and `ErrorCall` were the types I had to catch? Well, that's the one time when I think catching `SomeException` makes sense. `SomeException` is an existential wrapper around an exception of an unknown type, so we can only perform actions which work on every exception. And [`Exception` has `Typeable` as a superclass](http://hackage.haskell.org/package/base-4.10.1.0/docs/Control-Exception-Base.html#t:Exception), so one of the few things we can do is ask what its type is using [`typeOf`](http://hackage.haskell.org/package/base-4.10.1.0/docs/Data-Typeable.html#v:typeOf): &gt;&gt;&gt; import Data.Typeable &gt;&gt;&gt; Left (SomeException e) &lt;- try (evaluate (42 `div` 0)) :: IO (Either SomeException Int) &gt;&gt;&gt; typeOf e ArithException
make sure your function is using it as an STRef and not as the actual value. can you post it?
Does it suffer from being out of date? Its over a decade old, as someone with no experience in Coq I'm just wondering if it's still relevant for Coq or more for its pedagogy 
then from a compilers perspective, any language that is translated from source syntax to the source syntax of the compiler (IR) is "transpiled" ? 
Hey, that's me! The docs have now been published at [haskell-containers.readthedocs.io](https://haskell-containers.readthedocs.io) and are linked to from the haddock package description and the github repo now. I'll be sending out an updated post announcing the new location of the docs and hope to put together a template for other packages to use if they wish (as well as the haddock autolinker extension I wrote to use for containers).
You can also put examples in collapsible sections. This is done in [Data.Maybe](https://hackage.haskell.org/package/base-4.10.1.0/docs/Data-Maybe.html#v:maybe), but I've never seen it done anywhere else. It seems like a good solution to providing examples without making the docs awful for users who already know (or who can intuit by analogy) what the functions do.
I'm interested in the company but not the position. But I suppose the vagueness was intentional?
I'll try to get in touch with the maintainer, thanks :-)
Thanks! I'll ask the maintainer if there's any reason not to have them, let's see if I can contribute otherwise...
You wouldn’t happend to be hosting a summer internship by any chance? Sounds very interesting!
This seems like it would blow up in any situation where you actually go to *use* the value you bind out of the monad. This is avoided in the `log` handler by tossing the argument out, but if you wanted to write, eg: foo k = do x &lt;- k.get "hello" if x == Just "world" then ... If you're unable to use a value that you bind out of the monad with this inspection, then you might as well just use applicative.
It's not rare, but it's mostly bad. Type synonyms I sometimes approve of: 1. Ones used purely for backwards compatibility: `type Typeable1 = Typeable`. 2. Ones with `forall`s in them: type Lens s t a b = forall f. Functor f =&gt; (a -&gt; f b) -&gt; s -&gt; f t type f ~&gt; g = forall a. f a -&gt; g a 3. Ones that are just used internally and briefly: type CT x y z w = (...,...,...,...) class CT x y z w =&gt; C x y z w instance CT x y z w =&gt; C x y z w 4. Ones that wrap type families: type Reverse xs = Reverse' xs '[] type family Reverse' xs acc where Reverse' '[] acc = acc Reverse' (x ': xs) acc = Reverse' xs (x ': acc)
&gt; Any tips? I've been bitten by using the [default encoding](https://www.snoyman.com/blog/2016/12/beware-of-readfile) when reading text / strings. Nowadays I use stuff like pipes-text with a [explicit encoding](https://hackage.haskell.org/package/pipes-text-0.0.2.5/docs/Pipes-Text-Encoding.html) (utf-8 typically).
source distributions and corresponding stack compiler config setup?
seems like `gx` is shorter to write, but strictly less powerful than `nix`.
Agreed, a summer internship would be very cool. Finding an interesting internship for the summer seems to be hard tho.
[removed]
Yes. I write just about every "bash script" in haskell. One tip is to have a quick way of opening a project and the main file where *all* of your most used imports are there. 
+1 on the summer internship!
In the past, I've thought about doing Haxl-style analysis via `unsafeInterleaveIO`, which seems similar to the OP here. The idea was to use thunks to represent unstarted jobs. When such a thunk is evaluated, start all the jobs that have been queued so far. Basically just trading invisible Applicative hacks for invisible laziness hacks.
It's also about how fast it is to get the tools installed and running, when I consider Haskell for scripts. Either I would want to use nix or stack and avoid having to compile before running.
Thanks!
No, not quite. It's a pragmatics issue, not a semantic one. You'd never code in IR. In my mind, a transpiler takes a programming language to another programming language.
I would also be very interested in information about a summer internship!
Python seems more approachable for the persons I share scripts with. But, I have written scripts in Haskell. The main problem I had was trying to "shell out" to cygwin, which didn't work because the library I was using "shelled out" to cmd.exe when running on MS Windows. Also, you still have to compile the script while you are working on it to get the most out of Haskell's type-checking, which is not in my normal script editing flow, exactly.
if i'd never code in javascript, then i'd compile to javascript rather than transpile to it? :D
 - ` bidirectional [pattern synonyms](#patterns) Empty, :&lt;|, and :|&gt;.` doesn't render correctly. - I'd like to have a short comment that answers: "should I use in instead of a list"? or "what do I have to worry about when replacing `[]` with `Seq` (in terms of memory usage probably, as well as performance on small sequences possibly, or other issues). - Is there a alternative interface I should use if I want to generalize over both Data.List and lists and Data.Sequence? - Link to at least 3 alternatives and how they compare to help the user make an *informed* decision (the lack of cross-links in hackage is my biggest issue with the docs). 
"Discrete Mathematics using a computer" showed that Haskell is a good human readable notation for doing mathematics, not just for talking to a computer. (The book is available as a PDF online. It's a good one.)
Even the original Design Patterns book was mostly how to work around old Java's limitations in both typesystem and lacking first class functions. (I think Peter Norvig had a piece about how those patterns become invisible in Python or so.)
PSA: Sorry for the broken links. Downloads should work now.
I don't really have knowledge to add about it at this time but I've wondered this before, and how it possibly wouldn't make a good fit for scripting. Assuming that by script you just mean a small program that's likely not meant for a lot of people. I don't know what Python, Ruby etc could possibly even have on Haskell for scripting, other than HS has a steeper learning curve. It's pretty much the winner for conciseness, and there's nothing you can't do in it. 
This seems to be a common sticking point for people...
Yep. That's why I recently clarified it in the document. Not sure what else to do about it
I’ve used Conduit to create a lot of one-off scripts to munge large amounts of data. Works great. optparse-applycative is great as well. Anything nontrivial command line parsing and I use it over stdlib. 
So, there are two different ways of *using* values. There is data flow and control flow. Yours is an example of control flow, but first I'll get into data flow since `prog3` is an example of that in the post. I think you are mismatching handlers which expect no control flow with computations which do have control flow. Resulting, indeed, in a likely crash. **Data flow** is when we pass the result of an operation into the parameters of the next operation. This means that the actual data that goes through the computation is dependent on the result of operations, but the actual operations which occur are static. If we have data flow, but no control flow, we can consider the computation an arrow computation. A simple illustration of data flow is: x &lt;- k.get "Cats" k.put "Dogs" x Which is what happens in `prog3`. Since all operations are static and none of the key values depend on any operations, we are easily able to create a handler which retrieves all touched locations. Also, since this computation doesn't comply to the demands of applicative, we are not able to inspect it with applicative inspection. So *I don't fully agree when you say we might as well just use applicative*. However in Purescript, due to strictness, we might get into some problems when we introduce a function `f` which operates on `x`: x &lt;- k.get "Cats" k.put "Dogs" (f x) I don't see an easy way of getting around this. But in Haskell it should still be fine. **Control flow** is when the operations that are occurring are dependent on previous values of operations. A simple illustration of control flow is your example, but for the sake of the next paragraph I will introduce an operation `bool` which returns an `f Boolean`: b &lt;- k.bool if b then do x &lt;- k.get "Cats" pure x else do k.put "Dogs" 22 pure x It is not possible anymore to inspect this computation and get a set of all locations which will be touched. However, we can get a set of all locations which are *possibly* touched. We can create a handler which actually passes the value for the first requested `k.bool`, which we can run for both `true` and `false` and aggregate the results. So we can mix passing actual values when they are needed and passing dummy `undef` values when we know they are not needed. This is similar to creating mocking handlers which return predetermined values in for example testing code.
Umm, you welcome international applicants but don’t sponsor visas? Are you looking for remote workers or am I missing something regarding Swedish work laws?
Looks like for [internships under three months, you can just apply for a visitor's permit](https://www.migrationsverket.se/English/Private-individuals/Visiting-Sweden/Volunteering-or-internship.html).
I’m guessing they’re thinking about non-swedish EU citizens, since they are international, but don’t need a work visa to move jobs internally in the EU.
I think if you look (use your favorite search machine) for the names of the papers you often find them elsewhere too (often on the sites of the authors or their departments). Of course I seldom look for state-of-the-art stuff so I might be just lucky with the older/mainstream stuff.
This is like the blog post I've always wanted to find out this topic. Thanks a lot for this elaborate answer!
At my former place of employment I had to run checks for compliance with our policy for permissible files on some of the organizations computers. To do that I needed to accurately identify filetypes and I knew the file command used libmagic to id files by content instead of extension. Unfortunately file isn't recursive by nature, so I tried for a while to write a bash script to make it recursive and just never succeeded. A few years later I was able to write a Python script to make use of libmagic and wrote my own crawling code for the filepaths given. It worked alright, but eventually I wasn't sure what libmagic I was using and there were two different Python packages by the same name, and I had to cart around a bunch of files to use it. After learning Haskell, we needed a working version of the utility and my Python version broke after a few months, so I decided to do it in Haskell. It took 2 hours to write it up in Haskell compared to the days I spent working on Python. It's just one executable now that I can move over easily on a flash drive and run as needed. I feel badly I never learned how to do complex scripts in Bash, but honestly I think Haskell is a much better substitute for that. It's probably not the best example of scripting, but this is how I did it: [https://github.com/flounders/fileChecker](https://github.com/flounders/fileChecker)
Literally Haskell was created because some scientists wanted better tool to do their research. So Haskell from the get go was developed in tandem with research and scientific papers.
It's a fair question, I don't know the answer. I suspect some things may have changed, but that most of the book is still fine. 
Typechecking as a service is easy to set up. What's blocking you?
Yes it was. However, I'm not sure "scientific" is, or should be, synonimous with "paywalled".
I've even heard of some very sophisticated and illegal search engines that can get papers from behind paywalls. But even if these rumours were true, it's still not a long-term solution. In the end, the criminals that steal those papers will get arrested and the wealthy that charge for access will get more wealth.
It's not particularly sophisticated, and if this hypothetical person was a hypothetical woman in hypothetically Russia who hypothetically wanted to use hypothetical university proxy logins to hypothetically liberate hypothetically stupidly paywalled content, the hypothetical she could probably, hypothetically, operate more or less hypothetically indefinitely since, hypothetically, her hypothetical country doesn't give a hypothetical damn about hypothetical intellectual property laws, except where said laws benefit the hypothetical kleptocrat in hypothetical charge of it all. Since said hypothetical kleptocrat effectively elected the hypothetical, rumored, and purported "leader" of the only country that truly thinks paywalls around science aren't, hypothetically speaking, nonsense, I don't think much hypothetical jail time will come from it.
Hmmm, _serialised_ `parrow`. Sounds like a nice approach to restrict embarrassingly parallel execution :-)
Is this unique to Haskell? Do other programming languages have a higher rate of unpawalled research papers? Or is your issue more with the fact that research papers constitute such a large percentage of Haskell development?
It would be awesome if you could present this work at [Regensburg Haskell](https://www.meetup.com/de-DE/Regensburg-Haskell-Meetup/). Any chance to make this happen?
Doesn't Nix do all that?
I was not talking about *stealing* something - I find most papers on public (and AFAIK perfectly legal) sources (official homepages of the researchers or their department) more or less what people even link on hackage (example: https://hackage.haskell.org/package/extensible-effects links to http://okmij.org/ftp/Haskell/extensible/exteff.pdf which comes with this comment "Unless specified otherwise, all the code and the documentation on this site is in public domain" - so I think it's ok to read this right?) 
This is what I used to think before I found some papers from the recent Haskell Symposium to be ungoogleable.Now I think that, if someone's making a profit selling access, those same people would go some lengths to remove publicly available sources or, rather, prevent them from appearing by legally binding the speakers not to publish elsewhere.
To be fair to bash, there is the `find` command specifically to traverse directory trees and process entries with commands that do not have built-in recursion.
The latter, yes. My theory is that, if what gives power to Haskell are modern abstractions, then removing modern abstractions from open access would damage that power.
Right, specifying a date should be required. But we can't do that in the short term, because it would invalidate existing `stack.yaml`s.
It doesn't necessarily have to be the *oldest* revision, just a *fixed* revision. Stackage should have a way of specifying a revision other than the oldest. If unspecified, the default should be the first revision, to guarantee immutability. (similar to what I wrote elsewhere in this thread regarding `extra-deps` in `stack.yaml`.
Tragically, that's the problem with being born in the Land of the Free.
&gt; if what gives power to Haskell are modern abstractions, [Don't worry about people stealing an idea. If it's original, you will have to ram it down their throats.](https://en.wikiquote.org/wiki/Howard_H._Aiken) I don't think that the Haskell community withholds information on purpose.
I'm gathering info on a draft for "Scripting in Haskell" here in a repo https://github.com/k-bx/haskell-scripting , I encourage everyone to propose more things, I'll later find a day to dedicate it to actually put the content there. Scripting in Haskell is indeed a great thing, becoming more and more accessible over time, but there is a variety of ways you can do that, some of which involve more eDSL knowledge (Turtle), some are better for one-off tasks (just `stack script` with no dependencies). It's a good and big topic which needs a cookbook.
- FWIW https://pacmpl.acm.org/ will accumulate quite a lot of Haskell related stuff (ICFP + POPL) and it's free to read. - A lot of authors use https://www.acm.org/publications/authors/acm-author-izer-service (you just have to find a link, which is difficult at times). - https://github.com/gasche collects a lot of papers from past conferences, many have links to *some* versio - And last and not least: http://www.sigplan.org/OpenTOC/ a lot of papers.
I use scholar.google.com to find papers and can't remember ever failing to find PDFs for Haskell-related papers.
Development of the language isn't private per se, only some publishing options are. Oxford University Press, for example, the publisher of JFP, does have an optional open-access policy. However computer scientists seem to be in general very keen to publish preprints on Arxiv or their personal pages.
Have you checked if preprints are on the [arXiv](https://arxiv.org/)?
Can you elaborate? 
Any comparisons with [shakespeare](https://hackage.haskell.org/package/shakespeare)?
This looks like a wonderful middle ground! I wonder why this is not done more often.
For better or worse, Hamlet uses its own template syntax that you use with a QuasiQuoter or `[sxi]?hamletFile`. `nice-html` doesn't use any TH. Also worth mentioning is Heist. It also has its own template syntax and machinery for dealing with directories of templates, hot-reloading them etc. It also has "compiled" templates. They use a `DList` of *things* (`RuntimeSplices` or plain, static strings, that I assume are concatenated in the same was as `nice-html` during compilation). I used the compiled interface a few years ago and it worked pretty nicely, but IMO it was quite difficult. I'll add benchmarks for them both...
I found two downsides to turtle: 1) it relies on deprecated system-filepath (which is actually kinda awkward to use) 2) The underlying model is bit hard to understand. Ie. you can't just "do haskell" with it. Each to their ow, though. 
This needs to be higher!
Huh? If you have to ram it down their throats then putting it behind a paywall hardly seems likely to be effective.
To be honest, access to DL of ACM is so cheap I wouldn't even consider it a paywall.
I found a temporary solution, I just added explicit revision of *semigroups* package to `extra-deps:` section of `stack.yaml`: `- semigroupoids-5.0.0.4@rev:1`.
&gt; how shall we chat? Probably the easiest way is if you ping me on Freenode IRC (user `nh2[m]`) or Matrix (user `nh2`, which I am testing currently). We can then hop into a screenshare.
As shown in [this comment](https://www.reddit.com/r/haskell/comments/7pmpru/how_does_it_happen_that_the_the_majority_of_the/dsikikt/) most papers are accessible. I haven't seen any evidence at all that the Haskell community specifically puts papers behind pay walls. On the other hand the [ideas in FP are around for ages](https://www.cs.kent.ac.uk/people/staff/dat/miranda/whyfp90.pdf), [people are arguing to use FP instead of](http://www.thocp.net/biographies/papers/backus_turingaward_lecture.pdf), [say java](http://chrisdone.com/posts/dijkstra-haskell-java) for ages and published [papers showing prototyping benefits of Haskell](http://haskell.cs.yale.edu/wp-content/uploads/2011/03/HaskellVsAda-NSWC.pdf). Coming up with something like "modern abstractions" that would "damage power" is just plain bullshit. People are just to lazy to learn the basics.
Depends, when are you planning a meetup?
I am just saying that the quote seems to imply the exactly opposite of what you are trying to imply with it. If it is already hard to get people even to learn the basics then that is not really saying anything in favor of making it even harder by introducing more barriers. 
A performance question:I'm looking at maximumBy in Data.Foldable. I've got a series of "a"s that have "b"s associated with them. a -&gt; b is, however, expensive. If I implement (a -&gt; a -&gt; Ordering) in terms of this projection followed by the application of Ord b, can I expect the "b" for the maximum "a" to be computed only once or should I write a custom fold for this?
If they make it to the paywalled version, then yes. The purpose of the preprint is to get wider review, so if it goes on to be published you can be confident it is mostly the same. If there is some critical flaw or omission, it usually does not go on to publish.
With the exception of the final layout (and perhaps copy-editing), yes.
Academic publishing is completely broken. The entire model, especially the review process. Academics do the hard parts for free (reviewing, and most of the typesetting) and publishers charge insane amounts for serving pdf files. Publishers "add value" by making horrible websites that embed that pdf in weird and broken ways and other useless crap. See arXiv.org for a glimpse of the future, and for access to many papers for free! Many academics (notably Tim Gowers) have written about this and campaigned for improvement.
His name was Aaron Swartz.
In addition to the tings pointed out by /u/phadej , it is common (but not universal) practice in math and computer science for authors to put a "draft version" of their paper on their website. Usually, the primary difference between the draft version and the published version is formatting; there are sometimes some minor corrections or clarifications. Since the paywalled version is the version that can be verified as being peer-reviewed, and is almost always better-indexed (*some* of the excessive money that publishers charge is actually used to add value), this is what's referenced, and so this is usually the top search result. You can usually use google scholar to find "other versions", or you can look for it on the author's webpage. This takes more work, of course, but if this is a problem, it sounds like you find it worthwhile to pay someone to do the work of collecting, archiving, and indexing academic work.
Thanks for your interest in the course. Unfortunately, the application closed at the end of December. We will be announcing a new intake later on in the year. Follow @InputOutputHK for the latest news on new courses!
Thanks for your interest in the course. Unfortunately, the application closed at the end of December. We will be announcing a new intake later on in the year. Follow @InputOutputHK for the latest news on new courses!
You can do some mathematics in Haskell just fine. There's no universal notation for all of math for all people.
No, Aaron was the guy who downloaded articles off PACER and *didn't* get tried for distributing them (as the documents were, in fact, public property), and then downloaded articles off JSTOR, *didn't* make them available to anyone, and got tried for that, leading to his suicide. Land of the Free, y'know. Her name is Alexandra Elbakyan.
I was taking modern abstractions to be the principal selling point of Haskell. On the other hand, I find it hard to take on faith that the results of year 2017 in research are insignificant when compared to the results of year 1990.
Alexandra Elbakyan is cool too.
I wasn't aware of that, but that is good to know and I'll look into it further. I still have one Python project that I'm maintaining and that will come in handy.
That's just one of many. Here are some others I found with a quick search: * [shelly](https://hackage.haskell.org/package/shelly) * [hesh](https://hackage.haskell.org/package/hesh) * [hell](https://hackage.haskell.org/package/hell), using [shell-conduit](https://hackage.haskell.org/package/shell-conduit) as its backend. * [HSH](https://hackage.haskell.org/package/HSH) * [hsshellscript](https://hackage.haskell.org/package/hsshellscript) * [shellmate](https://hackage.haskell.org/package/shellmate) Some related tools: * [shqq](https://hackage.haskell.org/package/shqq) is a quasi-quoter that allows you to embed shell commands inside Haskell code, with Haskell values interpolated into the commands. * [shell-monad](https://hackage.haskell.org/package/shell-monad) allows you to write shell commands in Haskell. The output is POSIX-compatible shell script code.
I wish I had known about the exec option in find. I didn't think it would let you run arbitrary commands as I never read through the whole man page. That would reduce what I have been doing down to less than 5 lines more than likely. Thanks for letting me know.
I strongly disagree. It's not about the cost. It's about the discoverability. There are tons of people out there who might benefit from papers hidden behind paywalls. If they weren't behind paywalls, the information would be vastly more discoverable--indeed, just a google search away. But because they are behind paywalls (no matter how cheap) you have to intentionally look there. This dramatically reduces the discoverability and therefore the number of technological advances that could have been had by people needing exactly that knowledge but unable to find it because they didn't know it was there.
This is my first time writing Haskell, so feedback on how to improve my code is welcome.
Have you considered supporting hsx2hs to allow HTMLish syntax in the source files? It would not add any dependencies to your project, you just need to export a few functions that have the names and arguments that hsx2hs uses. And you'd still be able to use the plain Haskell combinators as well
Nice would be benching as well against type-of-html I'm sure that it's quite a bit faster...
What about pandoc?
Often you end up googling them even with the paywall, because the title and abstract are accessible anyway -- not to mention which a lot of literature discovery comes from following citations rather than random googling.
I don't see much of a discoverability problem here, abstracts are public and discoverable. Also, stating that it reduces the number of advances dramatically, without any supporting data, is not very meaningful.
No blockers at this time.
Link?
I don't get why you think it's ridiculous. If it's difficult to get novel ideas to spread, then the paywall just adds more difficulty. Like I said. The availability of materials for beginners is irrelevant. (But by the way, RWH is full of examples that don't even compile with anywhere near recent GHC, so it's a poor example.)
Hmm. Applicative has static data and control flow. Monad has dynamic data and control flow. Is there an abstraction with static control flow but dynamic data flow?
(as a nix noob): should i wait a few days for this to reach nixpkgs and get cached? relatedly, if you're familiar with reflex-platform, it uses a particular older compiler version, right?
Just a fetish of mine: [asciinema](https://asciinema.org/) is great for making videos of the terminal.
[Yes, arrow.](https://dl.acm.org/citation.cfm?id=2633636)
As phischu said, that abstraction is indeed arrow (the linked paper is also where i learned the terminology from). But the example of: x &lt;- k.get "Cats" k.put "Dogs" x doesn't nicely fit arrow: if we want to inspect the touched locations, we want to consider the location values as static, but the other parameter of the operation is dynamic. In the standard arrow all data flow must considered dynamic.
what about it?
A preprint is usually revised before publication, and it depends on the author whether s/he keeps the version on the arXiv up to date.
I doubt it can happen in January. Mid-February perhaps.
Seconded, I'd like to see this too.
People have already pointed out `Arrow`, but there's an interesting alternative discussed in [this thread](https://www.reddit.com/r/haskell/comments/6g83px/stm_collisions/). To my understanding, `Key` and `Cage` (discussed in the other thread) give you this same static-control-flow dynamic-data-flow abstraction.
Back in the day when the Haskell Symposium was the Haskell Workshop papers were usually put up for downloading on that year's Workshop website. I rather miss those days, there was a lot of exciting work happening. I was mostly using Scheme at the time and I was definitely envious of the Haskell world where it seemed all the action was. Hopefully the papers themselves are still available.
Hypothetically? Yes.
http://docs.python-guide.org/en/latest/shipping/freezing/ 
&gt; When a builder builds a building, they can build out of bricks or out of straw or out of bananas or out of steel girders… And it makes a difference what you build out of, how ambitious your building can be and how likely it is to fall down. I'd like to see this building that's built out of bananas.
&gt; Do other programming languages have a higher rate of unpaywalled research papers? Other languages don't do work in the form of research papers at all. It is a unique characteristic of Haskell that it has both interesting research work being done in it, and people using it at industrial scales very hard. (I say "interesting" research work to contrast with the things that are occasionally published in other communities that mostly revolve around "Given all the decisions already made by some language design, how do we make it run fast?" IIRC Java has some GC papers on that topic, for instance. There's also the "Given the already-existing design of some language, how can we uncomfortably wedge some academic idea into it?", which produces no results of value because no one will ever pick it up and use it. Haskell is the only community I know that has a functioning feedback loop between industry and academia.)
object-oriented programming &gt; functional,,,
Hey, did you ever figure out how to go about this? I find myself in a similar situation.
This is part shameless plug, part honest recommendation. :) A lot (all?) of the math done in (for example) `Main.hs` could be avoided by using Brick to do your interface drawing: https://github.com/jtdaugherty/brick Very cool project!
At the bottom of the module header? The bottom of the module seems an odd place for it. Can you have a peek at the rules in `Data.Sequence.Internal` and let me know which you think should be discussed, and how?
Pretty nice, gave myself a tut through the keybindings and they feel quite natural. I'll see if it helps me stay organized through the day. One thing I'd like is some control over the colors. The default palette is pretty dim looking on my terminal (black background), so it's a little hard to read.
I thought [he didn't have a PhD](https://youtu.be/re96UgMk6GQ?t=57m50s). Did he receive an honors doctorate or is it a courtesy title?
&gt; In 2013, he received an honorary doctorate from the University of Glasgow. https://en.wikipedia.org/wiki/Simon_Peyton_Jones#Awards_and_honours
At least three alternatives could be tricky. There's been some interesting work on variants that perform better in most common cases (increasing the branching factor, using transients), but I don't know if any of those experiments have been published on Hackage. It might make sense to compare to lists, vectors, and perhaps arrays; would you like to open a PR? There are some alternative interfaces, and it might be worth linking. `Control.Lens.Cons`, `Control.Lens.At` (already mentioned briefly), and Snoyman's sequence class would all be worth linking.
I like it. I'd love to see the ability to add due-dates and priorities as well.
Portability is the main potential concern. I'd hate to deprecate the Haskell 2010 way and close off the API to non-GHC implementations. For example, I imagine it could be useful to get `containers` working under Frege, which I don't think has pattern synonyms. Compatibility with Purescript seems like too big a reach, unfortunately.
Will do today along with the other ones...
(Sorry, (but) (it's (very)) (obviously LISP))
Nope. I have nothing against it though. Is there a reference implementation somewhere?
We’re so incredibly lucky to have him as a community - great researcher, very pleasant person, and incredibly inspiring.
That SIGPLAN site is the best place to look for new papers (2014 onwards). It has all the official published versions of papers from the most relevant conferences for Haskell: POPL, PLDI, ICFP and the Haskell Symposium. It's convenient to have all those papers in one place.
The publishing model in most CS sub-fields is different—it's based around conferences run by a professional society (the ACM) rather than journals run by for-profit companies. In practice it also means that I've never encountered a CS paper I couldn't easily find on Google without needing to pay anything.
I'm not sure data-flow without control flow is useful at all. Any data not influencing control-flow in some way is basically dead and could be left out.
I second the idea of not catching `error`. Any function that is not monotonic with regards to definedness makes me extremely nervous. And it is trivial to right one when you inspect `error`. If you want to assert that an "error" or "exception" occurs use real exceptions with well defined and fully specified semantics, such as `Either`, `Maybe`, `ExceptT` etc. Since IIRC `try (error "foo" == error "bar")` is undefined behavior with regards to whether you get back "foo" or "bar", in practice it typically is going to be "foo" with GHC, but that is absolutely not even close to guaranteed.
I just wouldn't publish it to hackage unless you are serious about maintaining it over time.
I did a lot of thinking on this. We might approach a closed pool for the first round, but I hope to open up the internship to an open pool for the second round. I definitely look forward to engaging with the community more over time.
I did a lot of thinking on this. We might approach a closed pool for the first round since we probably need somewhat experienced candidates to refactor our way out of the current state of things, but I hope to open up the internship to an open pool for the second round. I definitely look forward to engaging with the community more over time.
One of my favorite things about this 'joke' definition of monoids is that, if you understand it, you instantly understand what comonads are, as well -- just comonoids in the category of endofunctors :)
Super cool! This has similar benefits to QuasiQuoter based templates I'm guessing, but more flexible as you can compile these templates during runtime.
The differentiating point, in my usage, is whether or not the output is intended as human readable. I think that's an incredibly significant distinction, both from a use perspective, and a design perspective, that is absolutely worth calling out by using a different name.
Not unprecedented for people to get inspired to do it - https://github.com/devonhollowood/search-algorithms#readme
I wonder if dropping transformers would also improve performance? Are there benchmarks in current no-transformers branch? 
Yup. I've converted most of the bash and Python scripts I use for backups into Haskell (mostly using Shelly). All going well, I'm only likely to look at or change these scripts once or twice a year - if that - so I really, *really* want what each function does to be clear when I come back to it, and to make sure a change I'm making doesn't break something.
Oh no, I'll have to refactor my `HandlerT App (ST s)` functions!
What is a good design to provide an array interface to a `[[a]]` I want to be able to take slice, ie take a part of `[[a]]` assuming it to be concatenated list, but still maintain the original nested structure. splice (1,3) [[1,2],[3,4,5]] should be [[2],[3,4]]
The best design is to not use linked lists and use vectors or arrays instead. 
Yeah that's a fair point. Some discussion of their relationship would be good though.
It's probably easiest if I just send you a pull request. 
Do you have an example of improvements to error messages?
 benchmarking 10/blaze time 86.42 μs (86.19 μs .. 86.68 μs) 1.000 R² (1.000 R² .. 1.000 R²) mean 86.11 μs (85.94 μs .. 86.32 μs) std dev 663.6 ns (523.4 ns .. 847.2 ns) benchmarking 10/nice time 31.86 μs (31.82 μs .. 31.91 μs) 1.000 R² (1.000 R² .. 1.000 R²) mean 31.81 μs (31.77 μs .. 31.86 μs) std dev 153.5 ns (111.3 ns .. 230.7 ns) benchmarking 10/lucid time 56.89 μs (56.72 μs .. 57.11 μs) 1.000 R² (1.000 R² .. 1.000 R²) mean 56.77 μs (56.62 μs .. 56.97 μs) std dev 576.2 ns (472.3 ns .. 695.5 ns) benchmarking 10/hamlet time 25.48 μs (25.41 μs .. 25.58 μs) 1.000 R² (1.000 R² .. 1.000 R²) mean 25.60 μs (25.52 μs .. 25.73 μs) std dev 364.0 ns (274.5 ns .. 488.8 ns) benchmarking 100/blaze time 717.5 μs (712.6 μs .. 722.5 μs) 1.000 R² (1.000 R² .. 1.000 R²) mean 710.2 μs (707.7 μs .. 713.2 μs) std dev 8.869 μs (6.879 μs .. 11.11 μs) benchmarking 100/nice time 316.3 μs (315.5 μs .. 317.2 μs) 1.000 R² (1.000 R² .. 1.000 R²) mean 316.3 μs (315.7 μs .. 317.9 μs) std dev 3.082 μs (1.903 μs .. 5.745 μs) benchmarking 100/lucid time 476.4 μs (474.8 μs .. 478.6 μs) 1.000 R² (1.000 R² .. 1.000 R²) mean 475.1 μs (474.1 μs .. 477.0 μs) std dev 4.662 μs (3.225 μs .. 7.254 μs) benchmarking 100/hamlet time 233.4 μs (232.8 μs .. 234.1 μs) 1.000 R² (1.000 R² .. 1.000 R²) mean 233.3 μs (232.9 μs .. 233.8 μs) std dev 1.635 μs (1.143 μs .. 2.231 μs) benchmarking 1000/blaze time 6.822 ms (6.799 ms .. 6.845 ms) 1.000 R² (1.000 R² .. 1.000 R²) mean 6.823 ms (6.810 ms .. 6.842 ms) std dev 45.97 μs (31.75 μs .. 64.87 μs) benchmarking 1000/nice time 3.149 ms (3.140 ms .. 3.159 ms) 1.000 R² (1.000 R² .. 1.000 R²) mean 3.160 ms (3.154 ms .. 3.168 ms) std dev 22.48 μs (16.92 μs .. 31.56 μs) benchmarking 1000/lucid time 4.778 ms (4.758 ms .. 4.801 ms) 1.000 R² (1.000 R² .. 1.000 R²) mean 4.758 ms (4.748 ms .. 4.772 ms) std dev 35.74 μs (28.35 μs .. 44.78 μs) benchmarking 1000/hamlet time 2.306 ms (2.300 ms .. 2.311 ms) 1.000 R² (1.000 R² .. 1.000 R²) mean 2.310 ms (2.305 ms .. 2.317 ms) std dev 19.79 μs (14.24 μs .. 27.74 μs) Somehow Hamlet is the fastest of them all! Even though it uses Blaze internally. That's weird. Oh well; `nice-html` is still nice for other reasons :-).
It hopefully should make no difference, since everything is newtype wrappers that should disappear at runtime. It's possible that we're missing some optimizations like inlining, but GHC tends to handle ReaderT pretty well.
I want to believe.
&gt; They forget to upload their papers, they are too lazy to learn about long-term archiving (arxiv.org) unless it is made mandatory by their institutions. I think this happens more often in computer science, because it's a conference-dominated field. In mathematics, by contrast, all important work is distributed by arxiv. Authors commonly keep arxiv updated with the publication status of the paper. When the paper is *actually* published, the day comes and goes with little fanfare, because the whole surrounding research community has been not only reading it, but advancing the research and submitting their own papers building on it, for a couple years.
Yeah, `Data.Typeable` helped(!), since it was an `ErrorCall` that had the `show` contents of a `ConnectionError` inside of it, so it *only* looked like what I expected. :/ Second, it turns out that it was also thrown lazily – it avoided the catcher, because it wasn't fully evalued, so I had to add `evaluate =&lt;&lt;` in the body of the catcher. (`merijn` figured this out on IRC) Both problems probably arose due to `telegram-api`, `servant` or their combination.
How does changing WidgetT's representation impact performance? Do you have any benchmarks? I'm skeptical of dropping transformers. Adding an additional type for the special case of the master site increases complexity. Is the removal of the type variable that much of an improvement? We already have the `Handler` type alias that hides it in most situations. Dropping transformers also precludes the possibility of swapping out `IO` with something else in the future. Maybe this isn't feasible, but you could imagine using a different monad for testing or for guaranteeing security (ex, `LIO`). 
I almost switched to using Brick, then thought I should try to do it myself. I do want to redo the UI code though, and there are some more complex interactions I want to add, so I may well switch to Brick. Thanks for all your work on Vty. I don't think I'd have got anywhere without it.
Thanks, I'll take a look. The GIF recorder I'm using is very slow/buggy.
Due dates are definitely high on my feature list. Don't know whether to add priorities specifically or whether to use a more general feature like tags.
I'm going to add a config file soon. The default colours look great with Solarized, but I tried them in the default terminal setup and they didn't look great.
I didn't know this. These legal waters are so murky! Even though you speak with confidence, I have a hard time taking on faith that one cannot sell the right to distribute one's paper away from themselves. Can you give me some links so I may know more?
I don't think you parsed that sentence properly, since `very` modifies `obviously`... 🙄
Cool! I'm glad you found it useful! Vty has changed hands a lot over the years. I'm the maintainer these days.
u/tikhonjelvis says the papers from Haskell Simposium are still officially being made accessible, elsewhere in comments.
Another way to access papers is by simply sending the authors an email. Many academics are happy to help out and like it when someone shows interest. 
Thanks! These are great suggestions — Introduction to Graph theory in particular looks interesting.
Yes. From the site: &gt; ...enabling visitors to download the definitive version of the contents from the ACM Digital Library at no charge. It's convenient because it has all the papers linked in one place and makes sure you have the official version of the paper from the conference. Unfortunately it only goes back through 2014, but it seems to have every paper from most SIGPLAN conferences—certainly the ones that are likely to have Haskell papers. As an additional note, the ACM [copyright policy](https://www.acm.org/publications/policies/copyright-policy#permissions) allows authors to distribute the paper for free, only restricting commercial use. Most paper authors host copies of all their papers on their websites. This is less discoverable, but it means you can almost definitely find free, fully legal copies to download for the vast majority of CS papers over the last 20 or 30 years.
Yes, I totally agree, paywalls are bad. This is my opinion, most likely the opinion of everybody else in the Haskell community and the opinion of most scientists. Publisher like paywalls, scientists don’t. Go to your local university and ask a random professor if he likes paywalls. The answer will most likely be no. Is the reason for FP ideas not being widely adopted that knowledge is guarded by some elite community? No this is not the reason. As numerous people in this thread have pointed out the information is all there, on arxiv and in preprints. Speaking of “making ideas available to the general public”. [Simon Peyton Jones is working hard on it.](https://youtu.be/Ia55clAtdMs).
The ACM has still, for the longest time, stolen copyright from authors. This is unethical and not justified by they independent charitable action. Giving a cheap membership to students to access documents acquired through coercion rather than mutual consent is nothing to be proud of.
That's because you supply it a mock for put that doesn't inspect x at all. Imagine a real implementation that doesn't do anything with x. That would make x dead.
I can't recommend brick highly enough. @jtdaugherty is not exaggerating that it will handle most of the display for you with very little ceremony. Nice project! More of us should share our side projects...
What innatelogic said above is also true of ACM-run conferences. &gt; Academics do the hard parts for free (reviewing, and most of the typesetting) True. &gt; publishers charge insane amounts for serving pdf files I consider this true: - charging *any* price for a document acquired for free whose production was funded by public money is not ok - currently the APC asked by the ACM are $900, which is not as bad as others but still horrible for doing nothing at all -- PACM got a shared deal at $400 and amortizes it through their budget and conference subscriptions, but that is still way too much -- and again justified by no valuable work. &gt; Publishers "add value" by making horrible websites Yep, OpenTOC is horrible to use -- this is the reason why almost no one is aware that it exists. For reference, I was told that the price that ETAPS is paying Springer for Open-Access publication is lower than what ACM makes SIGPLAN pay for PACMPL. So the difference between evil for-profit editors and professional societies is not as clear as you would like -- with respect to their impact on *my* research budget funded by taxpayers. 
The point is that the handler k will most likely have various implementations. For example one to `IO` which interacts with a database, one to `State` to test locally, etc... These do actually use `x`. There is no 'one intended' implementation, so `x` is an important part of the description of the computation. The point I wanted to highlight with my post is that we can also consider other possible implementation of handlers: inspecting handlers, but they need not be limited to applicative computations. These indeed specifically ignore certain flow in the computation, because we want to obtain a specific static analysis from it.
Valid points, but this means in order to use this trick you have to factor everything that actually uses a monadic result into such handlers.
I do a lot of my scripting with Haskell and the turtle package. It always takes a bit longer to get the script done than in bash because types are more specific, but it's more robust and enjoyable than bash scripting. I really like the interface of turtle, specially the Shell type. I use nix for making the packages available to the script. My scripts usually look like this: #! /usr/bin/env nix-shell #!nix-shell --pure -i runghc -p "haskell.packages.ghc802.ghcWithPackages (pkgs: with pkgs; [turtle foldl text system-filepath])" #!nix-shell -I nixpkgs=/home/miguel/.nixpkgs/nixpkgs-channels-4aab5c57987af2dbf8b93fe30d5859a4a56d1aca {-# LANGUAGE OverloadedStrings #-} {-# LANGUAGE RankNTypes #-} module Main where import Turtle main :: IO () main = view $ ls "."
How did you guys decide upon establishing an internship?
Working on tools for programmers has the benefit that you are essentially doing the work programmers normally do, except at the "meta" level. That is, a programmer's job is normally to automate busywork for other fields, but someone who works on compilers/languages/tooling/formal methods is automating the busywork _of programming_, which is a much more powerful lever. I don't think it's controversial to say that a large part of our current and future economic growth is dependent on productivity increases from automation, so really the only question here is whether economic growth is an unalloyed good. Personally, I think the potential increase in existential risk resulting from technological advancement is a large enough concern that we can't be confident that technological progress / economic growth is an unalloyed good, but at the same time I think that the only such risk I am capable of working towards mitigating (AI intelligence explosion ala Bostrom) will probably be best mitigated via formal methods or something like it. I'm assuming in the above argument that the amount of things we have to automate (and the maintenance work associated with this automation) is sufficiently large that automating the work of automating things is worthwhile. This seems relatively well-supported by the available evidence, though I'm not really taking into account the tradeoffs associated with our tools (e.g.: if there are some developers who are incapable of learning dependent type theory, the increased cost of developers who can has to be weighed against their assumably higher productivity).
[Was it The Monad Challenges?](http://mightybyte.github.io/monad-challenges/)
Yup! That's it. Thank you so much!
Don't forget: turn this comment into a blog post
Is the udemy course any good for someone who knows programming and has read SICP but doesn't know haskell? or would starting with the docs be better?
Oooh this looks nice. I'll definitely give it a try once I have some time to integrate it into my existing to-do workflow (read: incomprehensible mess of scripts).
That's a good point: Brick doesn't support Windows (because Vty doesn't support Windows).
[removed]
Tags would probably be even better, then you can just define a priority tag.
I see there are several. Is any of them worth spending time on for a beginner/beginner-intermediate? Which one is best? Thanks
 staticPut :: String -&gt; Arr Val ()
Has anyone read the Udemy terms and conditions? I wouldn't click Agree if they gave you free courses. http://learnyouahaskell.com/ is good enough. http://book.realworldhaskell.org/read/ also has some content but it might be a bit dated.
Cool, thanks!
&gt;Has anyone read the Udemy terms and conditions? I wouldn't click Agree if they gave you free courses. I quickly skimmed over their terms and conditions and they really are bad... 
You are right, my statement was not really correct. We can specifically model arrow operations where we separate the static inputs from the dynamic inputs. But the distinction if an input is dynamic or static needs to be made upfront.
You are right, my statement was not really correct. We can specifically model arrow operations where we separate the static inputs from the dynamic inputs. But the distinction if an input is dynamic or static needs to be made upfront.
A thought that's been resonating with me a lot recently is [to prefer monad morphisms over monad transformers](https://twitter.com/mattoflambda/status/951923926233374720). In practical terms, this means having foo :: WebAppHandler a bar :: SomeOtherContext a embed :: SomeOtherContet a -&gt; WebAppHandler a rather than trying to stack effects together.
What's bad about them?
Udemy only shows me the german version, so I'm not quoting it, as this is an english forum. But one paragraph says that by using Udemy, you grant them the right to use your name, photographs and voice for advertisement purposes and that you waive your right for privacy and publication rights (if that makes sense in english).
Hmm. During ghcjs-boot I get this. &gt;[5 of 5] Compiling GHC.Integer.GMP.Internals ( &gt;src/GHC/Integer/GMP/Internals.hs, &gt;dist/build/GHC/Integer/GMP/Internals.js_o ) &gt;jsbits/ghcjsbn.js: openBinaryFile: does not exist (No such file or &gt;directory) &gt;Failed to install integer-gmp-1.0.1.0 
+1 from me on this sentiment
The correct way to write it that is *clearly* `(but Sorry (very (obviously (is it LISP))))`…
I tried a second time, this time after a fresh ghc install. Different error. This is using the sandbox script. """ [1 of 1] Compiling Main ( pkg/ghc-prim/dist/setup/setup.hs, pkg/ghc-prim/dist/setup/Main.o ) pkg/ghc-prim/dist/setup/setup.hs:19:1: warning: [-Wdeprecations] Module ‘System.Cmd’ is deprecated: Use "System.Process" instead | 19 | import System.Cmd | ^^^^^^^^^^^^^^^^^ Linking ./pkg/ghc-prim/dist/setup/setup ... Configuring ghc-prim-0.5.1.1... Failed to install ghc-prim-0.5.1.1 ../../data/Prim.hs: copyFile:atomicCopyFileContents:withReplacementFile:copyFileToHandle:openBinaryFile: does not exist (No such file or directory)cabal: Error: some packages failed to install:""" 
I'm going to be in Delhi in February. Great to hear there is a community!
Great! Please report back, I'd like to know what issues (or not) you encountered!
As for modularizing reflex-platform. I guess the question I should ask is what parts are specifically tied to reflex[-dom]? Since clearly reflex-platform can be used for non-reflex libraries, what exactly would the downsides be of removing every direct reference to reflex or reflex-dom? From there it should be easier to figure out what's necessary to have some sort of `ghcjs-platform` (that perhaps `reflex-platform` is an extension of).
We’ll have a meetup in February as well. Hope to see you there!
Shoot! Well there’s always next time. Let me know when you’ll be around next and we’ll try scheduling a meetup on those dates.
&gt; Since the paywalled version is the version that can be verified as being peer-reviewed In programming language conferences, this is not true. The version that was reviewed had annexes containing important content (such as: proof arguments for the claimed results in the paper), and the version that is paywalled does not have them. It is therefore strictly inferior to a "preprint" published by the authors. and is almost always better-indexed (some of the excessive money that publishers charge is actually used to add value), this is what's referenced, and so this is usually the top search result. &gt; (some of the excessive money that publishers charge is actually used to add value) Can you be more precise, what uses are you talking about? &gt; This takes more work, of course, but if this is a problem, it sounds like you find it worthwhile to pay someone to do the work of collecting, archiving, and indexing academic work. I think that it is normal to pay a fair price for some of the tedious work that goes into "publishing" -- namely checking that all author-provided versions build correctly, possibly buying DOIs for paper, and indexing them in the various bases that managers are using to judge journal quality. That fair price should be substantially lower than what most editors (ACM included) charge for it; LIPIcs charges 60€ for this, which sounds reasonable -- as an author I would be happy to pay this out of my research budget.
any updates?
You can do this with both stack and [nix](https://nixos.org/nix/manual/#ssec-nix-shell-shebang). Dependencies are downloaded during the first run and your script's compiled. The nix shebang is significantly slower than the one for stack. On a small script with nix I get ~6s first run, ~3s from then on.
This may be just a tiny pointer in the right direction, but does SmallCheck have a similar function available to QuickCheck's `counterexample`? And if it does, do you think you could attach the desired diff that way upon failure?
Your code looks good: I found it very readable. The code also displays "good style" (explicit export lists, lots of total functions, etc). You seem to be comfortable with functional idioms; if this is your first time doing FP, well done! Have you considered using `deriving Show` here? https://github.com/FabianGeiselhart/Tetris/blob/master/src/Tetris/Color.hs
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [FabianGeiselhart/Tetris/.../**Color.hs** (master → 3b14747)](https://github.com/FabianGeiselhart/Tetris/blob/3b147477d2460781c605506a5f5e7c5138e584e3/src/Tetris/Color.hs) ---- [^delete](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dsm9sfe.)
&gt; Have you considered using deriving Show here? I can see why I should do this. Thanks for the Tipp! &gt; if this is your first time doing FP, well done! Yes it is.
A very minor hing. main :: IO () main = do args &lt;- getArgs UI.main (read $ args !! 0) (read $ args !! 1) I would probably just use pattern-matching: main :: IO () main = do x : y : [] &lt;- getArgs UI.main (read x) (read y) 
Let me know when you know more :).
Okay. Thanks.
So the current ideas are to look at conference co-located workshops. One idea would be to look at ICFP co-located workshops such as IFL.
Another thing (and this is quite debatable). In the `Tetris` module, you have shiftPiece :: (Int -&gt; Int) -&gt; (Int -&gt; Int) -&gt; Field -&gt; Either (Field, Fail) Field It is a bit difficult to know which is the function for rows and which is the funtion for columns, just by looking at the signature. Now, you could just state that in a comment. I would perhaps create some some small auxiliary type like data RowsCols a = RowsCols { rows :: a, cols :: a } that made things more explicit when passing "some stuff for rows, along with similar stuff for columns". You could use it in createField :: Int -&gt; Int -&gt; IO Field as well.
&gt; bit difficult to know which is the function for rows and which is the funtion for columns I mixed them up very often. So I'll try to integrate that into my code. Thanks again.
True in this case, but of course in general, this will only work if `x` and `y` have the same type, right?
Yes, that's right.
Both only work if `x` and `y` have the same type b/c lists can't have elements with different types.
Actually `UI.main (read x) (read y)` will support different types for each argument to `main`. `x` and `y` must be the same type, but each `read` can have a different type result.
First advice: * https://markkarpov.com/post/free-monad-considered-harmful.html
The main thing that's reflex-specific is the try-reflex script. It grabs a bunch of reflex-related libraries and puts them in scope. There's also a ton of performance tweaks and such, which I would call reflex-inspired, in the sense that we've validated that they make our reflex-dom apps faster, but I think they're all either good or harmless in the context of other libraries. You might want to try using /u/ElvishJerricco's awesome [project system](https://github.com/reflex-frp/reflex-platform/blob/develop/docs/project-development.md), which just puts whatever libs you need in scope. On the whole, there really is not that much reflex-specific stuff, simply because there's not much reflex-specific *needed*. The vast majority of the work in reflex-platform is for cross-compilation to JS, Android, and iOS.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [reflex-frp/reflex-platform/.../**project-development.md** (develop → d020b13)](https://github.com/reflex-frp/reflex-platform/blob/d020b133263de862b61b5950a3a2887f6533bfaa/docs/project-development.md) ---- [^delete](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dsmga3m.)
How long did it take you to quote this project?
It's mine. I can give you a signature using the GPG Key the commits are signed with.
&gt; unboxed MVars what? have you tried storable vectors, llvm, disabling boundschecks flag? nevertheless, vectors can't be as performant as arrays in other languages because they int64 indexed and not really arrays, they "slices" 
Certainly nice to hear multiple viewpoints. One thing though, is that I feel most of his points are alleviated with `Freer`, as one commenter also points out; performance, composability, does away with functors, etc. I do feel I have a use for the `Free`/`Freer` style of structuring the code, since just slapping things into `IO` is *way* too coarse of granularity for me. I need to discern between multiple types of `IO` and other actions, and extensible effects via `Freer` seems most fitting to this, by building up a AST via the GADTs. 
Ah interesting. In that case what are your thoughts on basically officially making it a `ghcjs-platform` and breaking try-reflex off into a separate project? And yes I think I will be using the project system for the projects I am working on! Thanks!
I don't have any recent benchmarks. Way back when I did some benchmarks and chose the current lazy WriterT implementation because it was (surprisingly to me) the fastest, but the differences were negligible. Once you take into account the I/O being performed by a typical request handler, the various overheads are basically irrelevant. &gt; Adding an additional type for the special case of the master site increases complexity. Can you clarify what you mean by this? &gt; Is the removal of the type variable that much of an improvement? We already have the `Handler` type alias that hides it in most situations. The issue is that the full form of the type still appears in type errors. I'm not claiming this is a _massive_ win (I hope I made that clear in the post), but a small gain. &gt; Dropping transformers also precludes the possibility of swapping out IO with something else in the future. Maybe this isn't feasible, but you could imagine using a different monad for testing or for guaranteeing security (ex, LIO). If no one has done anything like that until now, I highly doubt it will happen in the future. In any event, I would recommend a very different approach to achieving that if desired: define a `MonadLimitedHandler` or some such typeclass that only provides the operations desired, and do not pull into scope a `MonadIO` instance. Because so many operations in `HandlerT` currently interact with an `IORef`, I don't see much of an alternative.
With the free monad for a functor you typically have a few different options based on whether you CPS things, and/or whether you make the catenable list of continuations an explicit thing. 1.) The naive free monad has the wrong cost for left associated binds. 2.) The CPS'd free monad (hit with Codensity) is "bigger" than the original free monad under the hood, but has cheap left associated binds in exchange for paying full price every time you go to inspect even one layer of the result. You can shrink-wrap this down to just the right size using a slightly different encoding, but its still about adding continuation passing. 3.) You can drop a "Reflection without Remorse" style catenable list of continuations in the middle to get the benefit of both cheap left associated binds and cheap inspection, but cheap here is merely asymptotic. The _constant_ factors are actually quite high. 4.) Finally, you can also CPS the things you shove in the catenable list of continuations. Then to get an effect system you can layer an open sum type on top, and/or separate the "request" and "response" types used by your effect system. You can also start just from an arbitrary type constructor of kind * -&gt; *, by first building a functor with Coyoneda, (the free functor given a type constructor) and then using it to build the free monad. The construction in chapter 2.4 of the freer paper is just this. This is the same thing the "operational" monad does. If you compare `Free (Coyoneda f)` and operational you'll just find an inlining of the Coyoneda structure. This construction is still "free" but its free relative to a different forgetful functor. You may choose to implement it using any of the approaches above, but `Free (Coyoneda f)`comes at a cost, each inspection of the layer of `f` in `Coyoneda f` pays full price. Relative to the naive free monad, multiple inspections will be expensive. One can play some games here and keep track of the current type existentially, etc, making this a bit more ambiguous, but really I can only give you `Coyoneda f` layers rather than `f` layers if I want to have the same asymptotics. Every one of those constructions above is available for this. The paper on freer monads uses this "operational" monad with a "naive" free monad encoding at first in chapter 2.5 and then in 3.1 switches to a reflection without remorse encoding of the continuation. So its a reflection without remorse free monad holding (Coyoneda f) where f is an open sum type of effects. But as I mentioned above, Coyoneda is kinda expensive if you're going to use it multiple times. The `freer` package on hackage tweaks the formula from 3.1 in the paper a bit, as it manages a queue like that section, but basically throws careful management of extraction from the queue away, building one with O(1) append, but with merely ephemerally amortized unconsing in pursuit of cheaper constant factors. This means it has an explicit queue like option 3 above, but it implements that queue in a way that tries to kill constant factors in exchange for worse asymptotics on inspection as multiple inspections pay full price. But in some ways this is a price you've already paid, with Coyoneda in the mix, multiple inspections are already expensive. This means you shouldn't use it for things like adding a layer of effects to, say, handle name capture in a syntax tree at a given binder where you will be looking at the syntax tree with a bunch of rewrite rules or something. Each inspection pays full price. So it isn't without cost. (Sad, given the name.) You can use freer when you want an effect system with cheap left associated binds and will be inspecting the result once, most if not all of the time.
You also might be interested in this: * https://github.com/lexi-lambda/freer-simple
Ah yeah that is a good point. How about `multi-platform-platform` ;). To be honest I can't think of a great name, perhaps it would be worth opening a github issue to discuss it?
No, no, sorry, that was an autocorrect error on my phone. How long did it take you to **complete** the project? 