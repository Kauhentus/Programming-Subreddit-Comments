Thank you!
I am already doing this using a Python script. https://github.com/sras/ghci-remote I have recorded a screen cast where I use it with neovim editor [here](https://www.youtube.com/watch?v=cwUzDjgaI1c)
I'm not sure what you are trying to do with `Fix`. Can you provide the actual definition of `data Matrix ...`? To explain why I don't get it: My guess is your `Matrix m n k` represents a matrix, which has `m` rows, `n` columns of entries of type `k`. e.g. `Matrix 3 2 Int` is a 3x2 Matrix which has entries of type `Int`. But then, nesting like `Matrix m n (Matrix p q k)` is a matrix which has matrix entries, something like [block matrix](https://en.wikipedia.org/wiki/Block_matrix), not a matrix multiplication. To express matrix multiplication as data, I would use `ExistentialQuantification`. {-# LANGUAGE ExistentialQuantification #-} data Matrix m n k data MatrixExpr m n k = Leaf (Matrix m n k) | forall p. Mult (MatrixExpr m p k) (MatrixExpr p n k)
If I understand everything correctly, problems are coming with [unsafePerformIO](http://hackage.haskell.org/package/base-4.11.1.0/docs/System-IO-Unsafe.html#v:unsafePerformIO) If you inline an `unsafePerformIO`, then the action may be performed more times than it should be ! An example can be found here: https://gentoohaskell.wordpress.com/2014/06/16/unsafeperformio-and-missing-noinline/ 
The situation in Rust is similar. Attoparsec does not provide streaming — it buffers everything into memory. For streaming, the best option is likely to write a parser by hand. Go has libraries that could be used as inspiration.
I think if you are not on the latest ghc, then this will result in a linker error if you have modules that uses template Haskell [1] [1] https://ghc.haskell.org/trac/ghc/ticket/8025
As a general principle, I try to avoid guards as much as possible. countIterations f r = fromIntegral . length . takeWhile (not r) . iterate f Some datatypes will also help. The type of `foo` gives me absolutely no suggestion about what it might do.
[http://gilmi.me/blog/post/2015/02/25/after-lyah](http://gilmi.me/blog/post/2015/02/25/after-lyah)
Consider [ghcid](https://github.com/ndmitchell/ghcid), which reloads ghci when a source file changes. It has editor integrations that might get you what you want.
Mirrors my experiences. I had to parse a large quantity of archived emails and gave up trying to do it in haskell. Ended up using python. For another task, I'm currently using mime for emails coming from a dedicated source. It works well enough, but the emails are small (very). Thanks for writing this up and assuring me that I am not crazy for not having succeeded at this. 
And with vim key bindings!!! 
I would have thought attoparsec parses lazy bytestring incrementally but it's on the user to limit backtracking so old chunks can be gc'ed? 
I mean, if that's your definition of fun and you don't mind spending a weekend (or potentially more) on that, know yourself out. But from a pure practical point of view I don't think you're going to gain much, if anything.
Is there any way to import labels from github and use them for kanban columns instead of github projects? Some other folks use this approach (waffle.io) 
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [ndmitchell/ghcid/.../**README.md** (master → aa9c244)](https://github.com/ndmitchell/ghcid/blob/aa9c244811c50a362a71dd618e1a4c1fc49142a5/README.md) ---- [^delete](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply e3jb4d9.)
Issue labels?
Thanks!
It looks like [it was added in `ghc-8.6.1-alpha2`](https://github.com/ghc/ghc/commit/e0b44e2eccd4053852b6c4c3de75a714301ec080#diff-0d92703213de86fca20f780beaef4f31).
Should be possible. I'll add it to the list
Omg wow, I love this. I was just thinking I could use something like this for my Haskell project, this is so lovely. Thanks!
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [silkapp/multipart/.../**Multipart.hs#L113** (master → 1d78640)](https://github.com/silkapp/multipart/blob/1d78640335f4b84b188491a496479518fc3afdcd/Network/Multipart.hs#L113) * [obsidiansystems/multipart/.../**Multipart.hs#L114** (split-at → 9a649e0)](https://github.com/obsidiansystems/multipart/blob/9a649e061ec64b27823327fa5b4432c33a7a3167/Network/Multipart.hs#L114) ---- 
Attoparsec _does_ stream the input (input can be partial and fed incrementally), and not the output. Yes, it also does append the input until it has completed for backtracking purposes, but the input should be small and limited and roughly 1:1 with the token that you'll return. Meanwhile, Conduit provides the input/output streaming interface. Integration between the two is provided in [Data.Conduit.Attoparsec](http://hackage.haskell.org/package/conduit-extra-1.3.0/docs/Data-Conduit-Attoparsec.html), which, by the way, is one of the most pleasant network protocol parsing experiences around, in any language. It's a marriage made in heaven. You use attoparsec to parse small units of syntax and stream the resulting tokens. In the case of email, these would be the mime headers, mime separators, etc. For body of email parts, those would be going through conduit directly. This is how [xml-conduit](http://hackage.haskell.org/package/xml-conduit-1.8.0/docs/Text-XML-Stream-Parse.html) works, it combines attoparsec and conduit together. I also parsed the TDS (SQL Server) protocol using attoparsec and conduit. Without attoparsec, you'd be manually doing the same bookkeeping of buffers and offsets and leftovers in Conduit.
If provides a lazy interface, but it's discouraged. All branching backtracks, so all input is kept.
Is there a way to set up a WIP limit?
Is that a limit to the number of tasks in a specific list?
Go ahead! See if there is a Haskell counter part for the [Pexpect Python library](https://pexpect.readthedocs.io/en/stable/) that I am using to control the running ghci instance. I had faced some issues when I handled it using my own code that made me ultimately switch to Pexpect. Also, one thing I have found is that for big projects, reading individual lines from the output slows down the ghci process considerably. So I just capture the output from prompt to prompt. You might want to change the ghci prompt to something that is unique and will not normally be part of the ghci output. Good luck.
Thanks. I'm pretty sure there is a form of expect for haskell.
[It seems](https://github.com/yesodweb/persistent/issues/514) that the correct syntax to use is bar UTCTime "sqltype=TIMESTAMP(0) with time zone"
For you graph, you'll probably want to use: http://hackage.haskell.org/package/fgl
(I posted this at the very end of last month and didn't get any responses) When I call `waitToSetLock fd (WriteLock, AbsoluteSeek, 0, 0)` from the unix package, my process waits, but also uses 300% CPU and all my fans start spinning up. If I move the `waitToSetLock` call into a forked thread (works with both forkIO and forkOS), the process sits nicely at ~0.3% CPU. What could be causing this? lockPwd f = bracket main recover (\_ -&gt; f) where mode = unionFileModes ownerReadMode ownerWriteMode main = do fd &lt;- openFd "/etc/.pwd.lock" WriteOnly (Just mode) defaultFileFlags putStrLn "waiting to set lock" waitToSetLock fd (WriteLock, AbsoluteSeek, 0, 0) putStrLn "got lock" return fd recover = flip setLock (Unlock, AbsoluteSeek, 0, 0) 
I really think we (Haskell Community) should build something (like email parsing) together.
Please put this to GHC's docs :D
Yes, exactly. Kanban is all about limiting the work in progress, as it is based on the Theory of Constraints. The reasons are very interesting, and have to do with Toyota System and Lean. The six rules that define Kanban are 1. Visualize the workflow 2. Limit WIP 3. Manage Flow 4. Make Process Policies Explicit 5. Improve Collaboratively (using models &amp; the scientific method) http://www.djaa.com/principles-kanban-method-0 This page gives some insights on why limiting the work in progress is so important in Kanban. Limiting the WIP is in fact one of the Kanban's core principles, so much that the association of Kanban proponents is even called Limited WIP Society (https://limitedwipsociety.ning.com) I believe it is legit to say that a board that declares to be a Kanban Board should implement at least a visualization of the work in progress limit.
I am currently reading "Parallel and Concurrent Programming in Haskell". There is a function `waitAny` that performs a list of Async and returns the first one to complete. waitAny :: [Async a] -&gt; IO a waitAny as = do m &lt;- newEmptyMVar let forkwait a = forkIO $ do r &lt;- try (wait a); putMVar m r mapM_ forkwait as wait (Async m) What bothers me is that this function creates a lot of threads that are blocked on putMVar indefinitely. Is it ok to do that?
A ghc thread is a green thread, meaning it uses far less resources than an OS thread. If it was doing `forkOS` that could be expensive.
As an aside. Regardless of what your application does, one should ignore timezones up until times need to be displayed to users (and even then you can make the UX choice to display UTC times to your users). You'll save yourself a lot of lamenting and gnashing of teeth if you just store your times as UTC/Zulu (without timezone). 
I'm not using timezones in the db, I'm just using Persistent's UTCTime field. Weirdly enough it's stored with a timezone anyways.
That's why I prefer my type systems without implicit coercion.
I don't mind having a lot of threads. I just do not understand what happens to them, because only one of them finishes. If there are N elements in the list, there are N-1 threads that are blocked on \`putMVar m r\`. But for some reason, I don't get BlockedIndefinitelyOnMVar error, or something similar.
Thank you for responding. &gt; Can you open an issue on https://github.com/reflex-frp/reflex-platform/issues with logs showing what happens when you try? That doesn't sound like any error I'm familiar with. Well, sadly I can't. I tried to undo what script did (to try install it again) based on some comment (which suggested deleting /nix and something from user) and now the script crashes instantly. Not sure I should be reporting it since I obviously broke it. I understand what Nix does, but it seems quite weird if in order to setup simple GHCJS project it is required to install a second OS. I just wanted to try writing JS in Haskell, not have bulletproof VM-like setup for servers. &gt; This is ignoring all the system dependencies. NodeJS has 17MB. I was not counting system dependencies for Stack and Nix too. They are free to use what I have already installed (so many dev packages). &gt; But yea, GHC a and GHCJS are also very large installations. Compared to JS world and even Java one (!), GHCJS is very heavy, at least the Nix way. It seems to me, that for beginners, hobbyists who want to try it, it's feels too hard to setup. But it might be just me - I might be doing something terribly wrong...
`ghcid` exists (written in Haskell), it's the only reliable "IDE"-like thing I've found, since its only expectation is a working repl. I'd try that before writing your own :-)
try using `stack`'s "script" feature: #!/usr/bin/env stack -- stack --resolver lts-12.4 script {-# LANGUAGE OverloadedStrings #-} import qualified Data.ByteString.Lazy.Char8 as L8 main :: IO () main = do ... https://haskell-lang.org/tutorial/stack-script
&gt; Not sure I should be reporting it since I obviously broke it. Please do. I, for one, would be happy to help :) &gt; I understand what Nix does, but it seems quite weird if in order to setup simple GHCJS project it is required to install a second OS. I just wanted to try writing JS in Haskell, not have bulletproof VM-like setup for servers. Heh, yea I understand that. You may have more luck with the ghc-8.2 and ghc-8.4 branches of GHCJS. Most likely, you can just do `stack build` on those branches to get it built. Not sure about installing from that though... &gt; I was not counting system dependencies for Stack and Nix too. Both of them choose not to when it's reasonable (with wildly differing definitions of "reasonable" :P), largely because linux distros aren't as stable as e.g. the JVM when it comes to providing a consistent environment. &gt; It seems to me, that for beginners, hobbyists who want to try it, it's feels too hard to setup. In an ideal world, it really is just as simple as curl https://nixos.org/nix/install | sh nix-env -iA nixpkgs.haskell.compiler.ghcjs Which isn't really more complicated from a user's perspective than NPM. But Nix is considerably more niche than NPM, so there's sometimes more gotcha's, unfortunately. I know people don't like to hear that disk space is cheap, but... it kind of is. Anyway, I'd recommend at least giving the non-reflex-platform Nix installation a try. You'll miss out on all the great stuff reflex-platform provides, but you could at least get going with relatively little effort.
Thanks a lot, i hope i get something done with this path.
How does this actually look for an actual Github project? Does anyone have screencaps/ videos?
&gt; Is there some guide/script which can quickly setup GHCJS project without requiring more than 5GB of space and complicated manual configuration? If you're on Ubuntu, you may want to give https://launchpad.net/~hvr/+archive/ubuntu/ghcjs a try; basically it's just sudo apt-add-repository ppa:hvr/ghc sudo apt-add-repository ppa:hvr/ghcjs sudo apt update sudo apt install ghcjs-8.2 cabal-install-2.4 PATH=$PATH:/opt/ghcjs/8.2/bin That quickly sets up a basic GHCJS 8.2 development environment and should be well below 5GiB ...and then follow http://hackage-ghcjs-overlay.nomeata.de/ (and if you run into problems, feel free to ask me)
Thanks for writing this amazing resource! You've already linked to a ton of other posts/guides, but I wanted to share one more: [Alexis King's opinionated guide to Haskell in 2018](https://lexi-lambda.github.io/blog/2018/02/10/an-opinionated-guide-to-haskell-in-2018/#any-flavor-you-like).
&gt; Please do. I, for one, would be happy to help :) Alright, you have convinced me :). &gt; Heh, yea I understand that. You may have more luck with the ghc-8.2 and ghc-8.4 branches of GHCJS. Most likely, you can just do stack build on those branches to get it built. Not sure about installing from that though... My GHC version (7 line) literally differs in one patch version. Though it could work... &gt; I know people don't like to hear that disk space is cheap, but... it kind of is. Yeah, but I have `/` (along with other dev tools) on not so big ssd. Sadly I have encountered quite a lot of application having issues with symlinks (most guilty are JetBrain IDEs - navigating to "non-project" files with symlink translated, failing to see changes in such files, so they are not detect when committing and so on).
I'm interested in eventually doing something similar. Do you have a writeup describing the email setup you ended up with? 
Have you considered using purescript? &gt; http://www.purescript.org/ 
Bookmarked :) Very useful as quick reference.
I have filled a ticket on reflex platform, I will first try it that way (it seems like the most common way of using GHCJS). If I fail to set it up, I'll try your approach :). Thank you for your response, I know that it probably seems trivial from your POV, but for a complete beginner those first steps are really hard and so far quite frustrating. I was ready to use something else (TypeScript or LiveScript), but you guys gave me hope :D.
`unsafePerformIO` is now defined with the `runRW#` primitive, which isn't inlined until after all Core-to-Core transformations are done in order to prevent the problems described here.
Side note: `Data.Set.size` is O(1), but `Data.IntSet.size` is O(n), so automatically replacing it isn't automatically the win you may think it is.
I was looking at it, but if I understand it correctly, it is not Haskell, but a new language, right? I am still quite a Haskell beginner (well, eternal Haskell noob, trying to learn on and off for literally years on toy projects) and I am not sure if I should be combining similar, yet different languages. I fear I would end up mixing those two constantly. Also I am not a fan of bower (working as a JS dev and was quite happy when everything on front-end moved to one package manager - npm). If I fail at setting up GHCJS, I might give PureScript a shot. At least it is almost Haskell, not almost JS (I was considering using TypeScript, but while it's not a bad language, when I have a choice I usually prefer more concise languages).
I've heard of people mounting /nix on a separate disk, but I have no idea how to do it. That's a really cool way to offload Nix's disk usage though.
Absolutely monumental work! I'll have to sticky this for a week or two so it can gain more exposure. 
It's a new language heavily inspired by Haskell. You can also avoid bower by using psc-package. I understand your intent in choosing GHCJS. Purescript can always be your plan-b. 
Can anyone give me an idea of how Haskell ranks in respect to readability, expressiveness, and compactness. Basically how much boilerplate / verbage is needed to accomplish a given task, how readable is the solution (assume well written), and how well does the language facilitate the expression of ideas and concepts. I've been looking at Rosetta Code and in many examples it seems to use a relatively low amount of lines. Basically is Haskell a good language measured by these three axis? Which of these does Haskell excel at compared to mainstream languages?
I had to write a [really complicated streaming mime parser](https://github.com/snapframework/snap-core/blob/master/src/Snap/Internal/Util/FileUploads.hs) plus a fiendishly tricky [streaming Boyer-Moore-Horspool implementation](https://github.com/snapframework/io-streams/blob/master/src/System/IO/Streams/Internal/Search.hs) to handle HTTP uploads for Snap. The various MIME specs are completely insane :(
Sorry for my vagueness. I'll use your definition for Matrix. I'm then trying to fold over what you call a block matrix to perform multiplication. I'm aware my type matching is absolutely definitely wrong, because I'm unsure myself how I'm supposed to define this. Optimistically the goal is understandable but please let me know if not. cata alg = alg . map (cat alg) . unFix alg :: Num a =&gt; Fix (Matrix m p) -&gt; Matrix p n [[a]] alg (Fx (Mult (mat_1 mat_2))) = mat1 * mat 2 -- multiply both matrices alg (Fx (Leaf (Matrix m n k)) = k -- k :: Num a =&gt; [[a]] 
Small corrections: `(not . r)` and `genericLength`.
Ah, I have missed this you are right ! Here the size is combined with the creation of the set, so `Data.intSet` is faster :)
sweet, can't wait to see it. (do you mind making a repo, even if it's empty/ not pushed to, so i can follow it?)
lmk if it works. i can help today a bit (with any dependency bulls\*\*t).
True, O(n log n) vs whatever `fromList` is for `IntSet`.
Did you take a look at mbox btw (https://hackage.haskell.org/package/mbox-0.3.4/docs/Data-MBox.html)? Main problem being that it doesn't do anything with MIME. but for efficient streaming over raw textual bodies it should work well...
I agree with hvr that it would be good if beginners could get started with GHCJS and reflex without having to learn about Nix. Once you have learned about Nix it no longer feels like a big deal, but it is for beginners, I fear.
I feel, that better to have 3 (possible compatible, and tied) libraries for each use case -- decomposing(parsing), composing and modify. Because each task require bit different api (although they can, and should have some common core) 
Oh man! Thank you so much for this! 
I think it's worth noting that one doesn't *have* to use Nix for the whole of Haskell development. You *can* just use it for the system package management aspect; i.e. you can use it just to install the compiler, and not for all the Haskell deps. This can dramatically simplify the process for the typical user. When it is as simple as installing Nix and running `nix-env -iA nixpkgs.haskell.compiler.ghcjs`, I think the complaints about nix's complexity are far less valid.
I mentioned it on the comments of the gist, but we're working on a new parser: https://github.com/purebred-mua/purebred-email which we're using in our WIP text based e-mail client. It can already used for parsing e-mails, but has still a few TODO items. Any help welcome.
This looks interesting, but I fail to see how this helps profile memory usage. The amount of data displayed corresponds to memory usage, but I imagine using this on non-toy programs would be impractical, am I missing something?
Oh, that's good to know. But you still need to get reflex and ghcjs-base somehow.
What is the eager interface vs. the eager interface? Can the eager interface parse non-trivial grammars (such as JSON)?
Sure! A few things jumped out at me as issues with this code; these are issues that I've typically seen with translating OO approaches to functional syntax, rather than functional idioms. You're looking at ways to update finite state machines, so let's use the modified code as a starting point. I’ve also added an instance signature. class FSM s e m | m -&gt; s e where update :: m -&gt; s -&gt; e -&gt; m data State = Stopped | Started data Event = Start | Stop data Obj = Obj { state :: State } instance FSM State Event Obj where update :: Obj -&gt; State -&gt; Event -&gt; Obj update obj Stopped Start = obj { state = Started } update obj Started Stop = obj { state = Stopped } --- The first thing that looks like idiomatic OO is the typeclass. This you state is part of your goal. &gt; I am aiming to describe a generic FSM typeclass that would fit any new type with its own states and events. That is _usually_ not idiomatic haskell. For example, when implemented with the multi param typeclass and fundeps you’re duplicating your enforcement of State. You may wish to parameterise that in the data type. data Obj stateType = Obj { state :: stateType } Hmm, what about the event? data Obj eventType stateType = Obj { state :: stateType } So now our instance looks like update :: Obj Event State -&gt; State -&gt; Event -&gt; Obj Event State That’s interesting — all the values are paremeterized by the `Obj` type. What if we didn’t fix the event or state types? update :: Obj eventType stateType -&gt; stateType -&gt; eventType -&gt; Obj eventType stateType This function and data type now link the Obj type to the State and Event types. You no longer need the typeclass to do that. The typeclass now only gives you an overloaded function name, to which I'll return later. --- Another problem is the instance you have defined. Removing the typeclass (for now) we have update :: Obj Event State -&gt; State -&gt; Event -&gt; Obj Event State update obj Stopped Start = obj { state = Started } update obj Started Stop = obj { state = Stopped } You don't have cases for the following update obj Stopped Stop = ? update obj Started Start = ? I suspect I know what you intend to do in these situations, but the typeclass based design sort of hides it. --- If I were writing this code, I’d do things differently still. Look at that update code again. The `Obj` is irrelevant! You pass it through unchanged. How about using this function instead? updateState :: Event -&gt; State -&gt; State updateState Start Stopped = Started updateState Stop Started = Stopped updateState Stop Stopped = Stopped updateState Start Started = Started The actual logic in this case is a simple state transformation. There are advantages to writing it in this way. - It is clearer - It is more focused on what is actually changing - It plays nicely with higher order functions, in particularly `scanr` and `foldr` — switch to `State -&gt; Event -&gt; State` for the `foldl` variety. Actually, this may be overspecified. How about this? updateState :: Event -&gt; State -&gt; State updateState Start _ = Started updateState Stop _ = Stopped You want a function that transforms values of type `Obj Event State`, though. updateObj :: Event -&gt; Obj -&gt; Obj updateObj ev obj = obj { state = updateState ev (state obj) } That’s a little ugly. updateObj :: Event -&gt; Obj -&gt; Obj updateObj ev Obj{state} = Obj { state = updateState ev state } Or with lenses, import Control.Lens data Obj eventType stateType = Obj { _state :: stateType } state :: Obj eventType stateType `Lens’` stateType state = _implementation elided_ updateObj :: Event -&gt; Obj eventType stateType -&gt; Obj eventType stateType updateObj = over state . updateState In this case the state transformation is separated from the 'Obj'. It isn’t clear from this example why you have the 'Obj'; it certainly isn’t needed for this simple case. --- Do we really need the typeclass for overloading? {-# LANGUAGE NamedFieldPuns #-} data FSM eventType stateType = FSM { _state :: stateType , _update :: eventType -&gt; stateType -&gt; stateType } obj :: State -- ^ The initial state -&gt; FSM Event State obj _state = FSM { _state , _update = updateState } This gives us an _update function of type _update :: FSM eventType stateType -&gt; eventType -&gt; stateType -&gt; stateType but we can define a function update :: eventType -&gt; FSM eventType stateType -&gt; FSM eventType stateType update e o = over state (_update o e) o --- My final code looks like this import Control.Lens data State = Stopped | Started data Event = Start | Stop updateState :: Event -&gt; State -&gt; State updateState Start _ = Started updateState Stop _ = Stopped data FSM eventType stateType = FSM { _state :: stateType , _update :: eventType -&gt; stateType -&gt; stateType } state :: FSM eventType stateType `Lens’` stateType state = _implementation elided_ type Obj = FSM Event State obj :: State -- ^ The initial state -&gt; FSM Event State obj _state = FSM { _state , _update = updateState } update :: eventType -&gt; FSM eventType stateType -&gt; FSM eventType stateType update e o = o &amp; state %~ _update o e The data type FSM encodes all I need, with no typeclasses. My state transformations are explicit functions, with no extraeneous components. I still have an overloaded `update` function. Rather than instantiating `type members` of a class you can define a data type and use polymorphism to handle different varieties. --- There's more advanced stuff, too — for example, using type families to reduce the number of type parameters. {-# LANGUAGE TypeFamilies, DataKinds #-} data FSMKind = FSMType type DefaultFSMType = FSMType type family StateF (f :: FSMKind) :: * where type instance StateF 'DefaultFSMType = State type family EventF (f :: FSMKind) :: * type instance EventF 'DefaultFSMType = Event data FSM f = FSM { _state :: StateF f , _update :: EventF f -&gt; StateF f -&gt; StateF f } type Obj = FSM 'DefaultFSMType Here you’re linking the State and Event types using the `FSMKind` type. You can create new values with type synonyms (as when they are promoted to types, they are treated separately by the compiler). Also I’ve never used data families, but I believe you could change the type family declarations to the following data family StateF (f :: FSMKind) :: * where data instance StateF 'DefaultFSMType = Stopped | Started data family EventF (f :: FSMKind) :: * data instance EventF 'DefaultFSMType = Stop | Start
Your list of length 6 has 5 items in it.
We did have a look at it in our recent project, but since we were already going to be parsing the headers elsewhere (we do need to deal with MIME types, different character sets, and attachments), we ended up pretty much just splitting the mbox on the From lines, and deferring the rest until later for that part.
that looks great. i will give it a try.
https://pdfs.semanticscholar.org/8f32/3d970cdaf89836a88fd0b3f6e35fd3df2551.pdf This is a very neat small paper where the author participated in a government project to see how effective different languages were for prototyping. Haskell came out ahead in most categories. It's old (1994), but I think their points for haskell are still accurate. He gets into discussing the results in page 8, but the whole thing is only 15 pages and it's interesting.
I've written about this a good deal: http://reasonablypolymorphic.com/blog/higher-kinded-data/
Ha ha, my fault! I'm fixing it...Thank you 
I would approach the problem as follows. Bundle the most promising Haskell implementation with the best implementation available across all ecosytems. The library will always attempt to parse with Haskell implementation first, but fallback to other implementation if there is an exception. Accumulate exceptions periodically, add to regression test suite, improve Haskell library. Repeat for six months until there are no more exceptions. Then remove the fallback mechanism.
3 libraries seems excessive. I was just planning to have three modules and a set of shared core types.
This is my vote for Elm as well. It's much easier to program than Haskell (although much more verbose). And you have easy access to GUI. Plus there is an escape hatch to just use popular Javascript when you really need a feature but cannot be bothered to implement it yourself. 
Think there is a mis-typed example under GADTs ``` data RestrictedMaybe a where JustInt :: Int -&gt; Maybe Int JustString :: String -&gt; Maybe String Nothing :: RestrictedMaybe a ```
A few thoughts. First of all, when you say "a game" - do you mean "a game I can play around with, just as a tool for learning"? Or do you mean "a game I can package up and sell on the indy game market, or as a mobile application"? Because these are very different questions! If you mean the first, then a few people mentioned my project, CodeWorld. I think this is, far and away, the best way to learn the basics of functional programming while making fun visual art, animations, and games -- just so long as you don't expect to build something that looks professional! CodeWorld games run on a single square HTML canvas, and they allow only a set of basic geometry primitives and combinators for drawing. There are no built-in widgets or controls. There's no sound. You can't even import images! This is by design: building pictures by transforming and composing geometry primitives is a powerful way to learn functional programming, but uploading PNGs you drew in Photoshop... is not. So if you're mainly interested in learning functional programming, and think making a game sounds like a fun tool to do so, by all means, try CodeWorld. It's great! But if you want to make a REAL game of professional (even indy-game) quality, then you'll need to look elsewhere. I don't know exactly where that is. I seem to recall Manuel Chakravarty was working on bindings to some game-related API at one point, but it was specific to Apple so I didn't pay that much attention. Reflex Platform is also an attractive way to build graphics programs that run in both web and mobile settings. Miso is another technology that I've heard good things about, and is specific to web front-ends. There are also bindings to stuff like SDL and OpenGL and Gtk, if you want to get your hands more dirty (but be careful; if you start out there, you're more likely to miss the point of functional programming and end up writing C-like callback spaghetti, just in Haskell syntax.) FRP is definitely an interesting idea for writing a game; but no, it's not necessary. You can write event loops in Haskell just as well as in any other language. Reflex is built on FRP, so you'd end up using FRP is you did it that way. Miso, on the other hand, is a simpler system based on just functions that modify a model. (If you've ever played with Elm or Redux, Miso is very similar.) CodeWorld is even simpler than Miso, and also works with plain functions that handle the game state. &gt; I figured that I could make a loop function that would be passing around the state of the program in the loop and then recursively calling it, but what benefit would this provide over using global variables in a game? Ah, that is the key question! Many people here (including myself) would love to tell you all about why explicit state is better than implicit state, but in the end, I think it is something that often has to be experienced to really sink in. By being explicit about state, you can reason equationally, and get coherent meanings for the expressions in your code. Since you have one type for your state, you can enforce type invariants, instead of just having a bunch of variables that don't know about each other. It turns out to be a whole different way of approaching solving the problem. So my advice is, just try it, give it a shot, and expect it to be very different and take some time to learn. Even if you hate it, you'll have learned something.
You could look up HaskellRank on youtube, he does easy HackerRank problems in haskell. 
Depends on how you use matrices in your recursive type. For example, it's possible to define `Fix` version of `Expr k`: data Expr k where Scalar :: k -&gt; Expr k Mat :: (KnownNat m, KnownNat n) =&gt; Matrix m n k -&gt; Expr k Mult :: Expr k -&gt; Expr k -&gt; Expr k Add :: Expr k -&gt; Expr k -&gt; Expr k data ExprF k r where ScalarF :: k -&gt; ExprF k r MatF :: (KnownNat m, KnownNat n) =&gt; Matrix m n k -&gt; ExprF k r MultF :: r -&gt; r -&gt; ExprF k r AddF :: r -&gt; r -&gt; ExprF k r -- Fix (ExprF k) ~ Expr k cata :: (ExprF k a -&gt; a) -&gt; Expr k -&gt; a And not possible for the following version, because it recurses MatExpr while changing parameters. data MatExpr (k :: *) (m :: Nat) (n :: Nat) where Zero :: MatExpr k m n Dense :: Matrix m n k -&gt; MatExpr k m n Add :: MatExpr k m n -&gt; MatExpr k m n -&gt; MatExpr k m n Mult :: (KnownNat p) =&gt; MatExpr k m p -&gt; MatExpr k p n -&gt; MatExpr k m n Note that this is not special because of using "dependent type". [See an example took from Wikipedia](https://en.wikipedia.org/wiki/Polymorphic_recursion): data Nested a = a :&lt;: (Nested [a]) | Epsilon It is not possible `Fix f` for some f be isomorphic to `Nested a`. 
**Polymorphic recursion** In computer science, polymorphic recursion (also referred to as Milner–Mycroft typability or the Milner–Mycroft calculus) refers to a recursive parametrically polymorphic function where the type parameter changes with each recursive invocation made, instead of staying constant. Type inference for polymorphic recursion is equivalent to semi-unification and therefore undecidable and requires the use of a semi-algorithm or programmer supplied type annotations. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/haskell/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
**[Polymorphic recursion](https://en.wikipedia.org/wiki/Polymorphic_recursion)** &gt;In computer science, polymorphic recursion refers to a recursive parametrically polymorphic function where the type parameter changes with each recursive invocation made, instead of staying constant. Type inference for polymorphic recursion is equivalent to semi-unification and therefore undecidable and requires the use of a semi-algorithm or programmer supplied type annotations. ***** ^[About](https://www.reddit.com/user/ultimatewikibot/comments/90r969/about) ^| ^[Leave](https://reddit.com/message/compose?to=ultimatewikibot&amp;subject=Blacklist&amp;message=Me) ^[me](https://reddit.com/message/compose?to=ultimatewikibot&amp;subject=Blacklist&amp;message=Me) ^[alone](https://reddit.com/message/compose?to=ultimatewikibot&amp;subject=Blacklist&amp;message=Me) 
This is [TraversableB](http://hackage.haskell.org/package/barbies-0.1.3.1/docs/Data-Barbie.html#t:TraversableB) from `barbies`
I've been looking at mbox for importing my gmail account which is a couple gig .mbox file. I'll let you know how it goes.
No write up yet, but I'll make one once I have the setup finished.
From my experience of go-gmime binding, high-level composing, and processing mail can looks too differently. And hard to make reasonable easy API.
I hacked togeter a Servant server that exposes ghci through a REST api here: https://github.com/danidiaz/resty-repl It's very crude but it sort of works. I didn't wrote a client—tested it with curls—but perhaps it could be derived from Servant's API definition.
I wrote an article a few years ago about the Reader monad that might be helpful. You can find it [here.](http://nadineloveshenry.com/haskell/readerMonad.html)
Shimmer looks like an anonymous project without a repository not homepage. [http://hackage.haskell.org/package/shimmer](http://hackage.haskell.org/package/shimmer) Shimmer also rhymes with scammer. I'd avoid this lambda machine!
The only way I could wrap my head around `Reader` was to implement it myself. Start by creating a newtype for a function with a single unapplied argument and write the Functor and Monad instances for it. Then implement the `ask` function. newtype Reader r a = Reader (r -&gt; a) -- OR -- data Reader r a = Reader { runReader :: (r -&gt; a) }
Why does `liftM :: Monad m =&gt; (a -&gt; b) -&gt; m a -&gt; m b` have the `Monad m` constraint? Why not just `Functor m`? Why does it even exist, why not just use `fmap`?
Historical baggage and backwards compatibility. `Functor` was not a superclass of `Monad` before GHC 7.10. Today `liftM` is redundant along with a number of functions, like `mapM`, `sequence`, etc.
So different that you need different types to represent the same message? Had I finished the library, I would have imagined a few different modules that provide interfaces. The `.Compose` module would provide high-level combinators like `replyTo`, `to`, `cc`, and other bits that create a new message. There might be another module `.Extract` that has functions to search and extract information from a message. This would include things like saving mime attachments, or extracting information from headers, such as who it was sent to, who sent it, etc. In both this modules, you'd be calling functions to do the work, rather than looking directly at the types. In the core would be the underlying types that represent email messages along with the parsers and pretty printers. I can see how the answer to my question at the top could be yes. If you are trying to modify an email message and just add a header -- you don't need (or want to) parse the entire email and then pretty-print it again -- you only want to insert a header. Especially if bits of the email are malformed. You could still insert new headers even if bits of the body are not correctly formed. And the types needed to extract content from an incoming email message may be different from the types best suited to creating and sending an email in streaming fashion. When extracting information, you often ignore many of the headers -- so why even bother parsing them? That said, I think there are still reasons to have shared types for more atomic things such as email addresses. 
Or the algebraic-graphs package. 
It's also worth mentioning the upcoming `[No]StarIsType` and the interaction this has with `TypeOperators`. Basically, just be cautious about using `*` as a type operator. https://ghc.haskell.org/trac/ghc/wiki/Migration/8.6#StarIsType
The User's Guide mentions that `OverloadedStrings` may eventually be superseded by `OverloadedLists`. However, the classification probably confuses more than it helps, so I've changed it to 'stable'. (My criteria for what's stable and what isn't are not very consistent.)
Good point; I've moved `DeriveGeneric` to the basic track. More generally, there are a bunch of extensions (`BangPatterns`, `EmptyCase` and `EmptyDataDeriving`, etc.) where I'm unsure which track they belong to. Opinions welcome.
Oh dear, that was very wrong. Thank you!
That's a good addition, thank you! I've added Alexis' guide to the 'further reading' section.
https://github.com/f4814/ghci-daemon I have a rough prototype on my disk. I'll probably push a first version today or tomorrow.
Unfortunately, Executable named pacman-key not found on path
Funny task, which I tried to solve few years ago was a DSN reports parsing. I take a testsuite from perl library, and tried to re-implement it (in go, not in haskell, although I used combinators and query DSL). So about extractions -- I see two good high level use-cases: DSN parsers (excract reason from delivery report), and implement a fancy DSL to build fast procmail/maildrop replacements.
Reader is used to make some data structure available to some context. `greeter :: Reader String String` means "greeter is value that produces a String (2nd param) when given a String (1st)" To produce the String from greeter, you give it a String by doing this: `runReader greeter "Nikki"` In this case, `ask` just brings the String "Nikki" into scope, making it available to the rest of `do` block. 
Ah, that's a bit of an annoying corner case; thanks for pointing it out. I've added a note to the guide.
I'm still skeptical of this. Granted, I don't teach people Haskell basics, but I've never seen * be a serious source of confusion. Plus, it seems like the kind of confusion that is cleared up in a matter of moments. Outdating so much literature and code seems much more likely to be an actual impedance. That said, this does improve the language, and the best time to make big changes is always yesterday, so if this doesn't result in nontrivial problems, I'm all for it.
You should look at ghc-mod - it does exactly this using the GHC API, though the interface is probably not what you are looking for.
&gt; It seems like the kind of confusion Nice.
I used ABAP once for my civilian service for about a month. The language made a really awful impression on me. This was in a hospital and surprisingly those guys didn't know that they could use relational joins. In a way I'm not surprised. But I don't remember any specifics so take my opinion with a grain of salt.
I really don’t buy this. [LYAH had a little section about kinds and `*`.](http://learnyouahaskell.com/making-our-own-types-and-typeclasses#kinds-and-some-type-foo) It was extremely approachable and I’ve understood it ever since. Easier to understand than other concepts in LYAH even! So I don’t think `*` is any sort of barrier to entry. Removing it feels like a desire for purity at backwards compatibility’s sake..
( &gt; Do unmatched parentheses bother you at all?) Oh, ah yes, yes. Yes they do. It's not as bad as a missing close bracket, but it still feels terribly lopsided. Good guide, well-written.
It seems that the time windows to make this default is 12 releases/6 to 7 years, which should give plenty of time for learning resources to catch up.
&gt; another example of Germany’s digital failure I mean, I live there. And I wanna be mad at the person who wrote this, but they are SO DAMN RIGHT.
&gt; What's the point of having an input if you don't handle it? The extent to which an input can be "handled" depends on the context, from which you derive the assumptions you can hold about it, and the operations you are allowed to perform on it. There's no absolute notion of "handling an input". &gt; IMO it should be ok to match against anything that doesn't go beyond the type restriction, and since the restriction here is "no restriction", I should be able to match against anything I want. It is not true that there is "no restriction". In fact, it is the opposite. `s` is arbitrary, so the amount of information you have about it is near to zero. This imposes *a lot* of restriction on the operations that can be performed on its elements. If you pattern match on elements of `s` using constructors of `State`, you are relying on the assumption that `s = State`. Since such an equation does not hold, the program is ill-typed.
You can just think of ask as a getter function that returns the value "stored" in reader. So the function in your example is using reader to store someone's name. Then you just "get" the name (which is of type string) and append it to a greeting message. Not a perfect intuition, but it's a perfectly usable one.
&gt; Removing it feels like a desire for purity at backwards compatibility’s expense.. And that's bad? 
&gt; Documentation and books go out of date all the time. I mean, that doesn't mean that it's *good* that they do. It should still be avoided if reasonable.
When the value is absurdly low and the breakage is high, yes.
It definitely can be. Depends on the case, but we shouldn't pretend that it's just *always* worth breaking compatibility to purify the language. For instance, we probably can't ever swap `:` and `::`, despite that most haskellers seem to wish the language had started that way.
I'd like to point out that the motivation section contains 4 points. Ignore the one about confusion if you don't find it compelling, and there's are still 3 more reasons to make the change.
Let's say LYAH explains \`\*\` well enough. In fact, I learned about kinds from LYAH and personally, I personally, I had no problem with this syntax. However, this is only 1/4 of the motivation section. Other reasons to make the change are lexical consistency, paving the road for term/type parser unification, resolving the conflict with \`TypeOperators\`, etc.
The value is not very low. It would enable us to unify terms and types (see point #2 of the [motivation](https://github.com/ghc-proposals/ghc-proposals/blob/master/proposals/0030-remove-star-kind.rst#motivation))
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [ghc-proposals/ghc-proposals/.../**0030-remove-star-kind.rst#motivation** (master → afce35f)](https://github.com/ghc-proposals/ghc-proposals/blob/afce35fe8c10577913f43f14bd3e34bddb20503f/proposals/0030-remove-star-kind.rst#motivation) ---- [^delete](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply e3m0unf.)
`BangPatterns` is definitely "commonly used". My hunch would be to put it on the basic track.
Pretty sure `newtype` accepts record syntax because I had the same journey of trying to understand `StateT` by implementing my own `StackT` (without wrapping `StateT`).
&gt;The situation in Rust is similar. Looks like OCaml is not different either. I've managed to find one library that looks promising [https://github.com/janestreet/email\_message](https://github.com/janestreet/email_message) but it still suffers from the same problems: * zero documentation (not a single line of docs, only tests provide some examples). * no streaming support (at least can't see it in the interface).
Shimmer repository [https://github.com/discus-lang/shimmer](https://github.com/discus-lang/shimmer)
Personally, I would use explicit callbacks: the user provides an asynchronous stream and asynchronous callbacks, and the library’s job is to fire the callbacks at the appropriate times.
You should add the git repo to the shimmer cabal file though :)
Anyone who has been around Haskell for a while would know the name of the shimmer author Ben Lippmeier. Ben has co-auhored academic papers on Haskell related topics and has contributed to GHC, Data Parallel Haskell, Repa, Gloss and so much more.
Yet another failure of the open-closed principle.
Wait, so the plan is to replace `*` which is obviously different and people can look up with `Type` which looks like a regular type name and people have to just know is special?
Link is 404 for me. 
&gt;You'll save yourself a lot of lamenting and gnashing of teeth if you just store your times as UTC/Zulu Well it depends on which time zones your users are in. See [http://www.creativedeletion.com/2015/01/28/falsehoods-programmers-date-time-zones.html](http://www.creativedeletion.com/2015/01/28/falsehoods-programmers-date-time-zones.html) &gt;If I have a timestamp for a future event, I can convert it to UTC, store it as UTC along with the time zone and be sure that I can reliably convert it back to the correct “wall time” in the future &gt; &gt;If time zone rules change in the mean time for the time zone in question, converting back from UTC to “local time” might produce a different result.
Thank you, this is an incredibly useful and thorough resource, though I would actively discourage users from using cabal's default-extensions field. It is hellish when trying to use ghci. IMO, all haskell source should be able to build with just GHC alone, and default-extensions gets in the way of that, since most people using default-extensions don't write them in their modules. Not only that, but you can't look at a module's source and see which extensions are enabled in that module. I don't mind it being mentioned but I would rather it be noted that it's not a good thing.
Why? All this will do (as far as I can tell) is break a huge amount of existing code with little benefit (aside maybe consistency across code with dependent types).
&gt; Why bother dealing with extra syntax in the parser, printer and docs, for something which doesn’t add any benefit Aside from the things you just mentioned? Stability of documentation and books seems pretty significant, especially when you consider that one of the chief complaints about Haskell (usually from the same people complaining it's not "beginner-friendly" enough) is the relative dearth of resources. 
The value is pretty massive. This little wart makes GHC unnecessarily complex and is a major stumbling block in the implementation of `-XDependentTypes`. One major benefit of `-XDependentTypes` which is sometimes overlooked is that *it will greatly simplify the Haskell language's type programming facilities*. A great slew of extensions which add up to A Poor Man's Wonky Dependent Type System will be obviated. The special treatment of `*` would make this simplification process awkward as hell and probably produce a pretty messy result. It took me an embarrassingly long time to figure out what the point of (closed) `-XTypeFamilies` was. Well, they're simply functions at the type level! With `-XDependentTypes` if you *mean* a function at the type-level then you just *write* a function at the type level. `-XKindSignatures`? **Gone!** `-XDataKinds`? **Gone!** `-XPolyKinds`? **Gone!** These, and a whole bunch of others, will still exist as extensions, but they'll be thin slices from the `-XDependentTypes` pie, instead of the chaotic smorgasbord we have now. And that's A Good Thing. 
I'm pretty sure it does too, because I just tried it in GHCi.
I was seriously confused by * when I first saw it and it is impossible to google for.
And to me using `*` instead of `Type` is way more understandable. If `Type` is the *type* of types, why is it not of type `Type`? It is, after all, a type. Haskell isn't a good dependently typed language. Dependently typed languages exist. Go use them. They're basically Haskell with dependent typing. They can call into Haskell code if you need to. But they aren't Haskell and Haskell isn't them. Haskell should be focusing on non-dependently-typed things, IMO. There's a huge amount of design space outside of dependent types.
The cop-out -- if you can justify it based on your users' competency levels -- is to just display UTC and be done with it. Ain't nobody got time fo dealing with DST bullshit... 
&gt; And to me using `*` instead of `Type` is way more understandable. If `Type` is the _type_ of types, why is it not of type `Type` ? It is, after all, a type. It is! Prelude Data.Kind&gt; :k Type Type :: Type 
&gt; Haskell isn't a good dependently typed language. We're working on it! &gt; Dependently typed languages exist. Go use them. They're basically Haskell with dependent typing. They're all different. Name one lazy language that is lazy, dependently typed, and allows matching on types (Haskell allows it in type classes and type families). I don't know a single language other than Haskell that has data families. Are there other dependently typed languages with parametric dependent quantification (`forall` must not be a special case of `pi`)? Etc. If there was a language that is "basically Haskell with dependent typing" I'd be using that. So far I tried Idris, Coq, Agda, and none of them are. &gt; Haskell should be focusing on non-dependently-typed things, IMO. There are different parties that want different things from Haskell. I want dependent types, you don't — that's alright. For the things that you personally want, why don't you write a thesis (like Richard Eisenberg did on DependentHaskell), then a multitude of proposals, then start implementing them, and so on. Haskell is a research language, — put in some effort and you can steer the wheel in the direction you want.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [ghc-proposals/ghc-proposals/.../**0030-remove-star-kind.rst#motivation** (master → afce35f)](https://github.com/ghc-proposals/ghc-proposals/blob/afce35fe8c10577913f43f14bd3e34bddb20503f/proposals/0030-remove-star-kind.rst#motivation) ---- [^delete](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply e3mzjm2.)
Idris is almost identical to Haskell, I thought. Laziness is made optional but that's probably good, given the primary thing that causes significant bugs in Haskell programmes is also the one big thing that doesn't get annotated in types: laziness. &gt;There are different parties that want different things from Haskell. I want dependent types, you don't — that's alright. For the things that you personally want, why don't you write a thesis (like Richard Eisenberg did on DependentHaskell), then a multitude of proposals, then start implementing them, and so on. Haskell is a research language, — put in some effort and you can steer the wheel in the direction you want. Look I don't disagree with the idea that Haskell should be a testbed of language features rather than trying to be a 'working man's language' or whatever, but I still think that the entirety of type-level programming is incredibly convoluted.
Yes, I know. That's because Haskell doesn't aim to be a proof assistant — we don't have a termination checker, we do have exceptions, and since 8.0 we have `Type :: Type`. Adding `Type :: Type` is brilliant because it makes everything simpler and has no downsides for Haskell because the type safety of Haskell comes from consistency of its coercion language.
What is that reason, if not consistency, that Haskell doesn't have?
I wanted to give a chance to the libraries like `beam` and `squeal` but I realized that they can't handle all cases even for a medium-level web-application. They introduce a lot of type-level tricks but unfortunately they work only for simple DB schemas. And they are not beginner friendly. 1. Since you need to define data type for table in advance, you can't use `WITH` construction from SQL to define anonymous tables. 2. You need to show raw SQL query each time you change it. When you work with huge tables, you need to call `EXPLAIN ANALYZE` and see how performant it is. This requires more steps with some DSL for SQL queries. 3. Databases like PostgreSQL contain a lot of complicated features. Like support for JSON operators and dynamic generation of columns based on SQL query. Even with Haskell type-system this might become a challenge to express in types. It just becomes very difficult to express some SQL queries when you work with DSL sometimes. It's much easier to play with SQL query in `psql` REPL, analyze it and then just copy-paste to Haskell code instead of rewriting to some DSL.
That it makes the language inconsistent!
You know that another thing that makes a language inconsistent is unrestricted recursion? ``` loop :: a loop = loop ``` Are you proposing a termination checker for Haskell, or do you perhaps not care about consistency as a logic?
&gt;Are you proposing a termination checker for Haskell, I think adding a termination checker to the language would be a HELL of a lot more useful than adding dependent typing. So yeah, call it one of the many things that could be explored in the design space of Haskell that is being sidelined by what feels, as a casual user of the language, as the primary focus of the developers of the language: dependent typing. Maybe that's wrong, but that's how it feels. I don't contribute to the language, I don't use it in the software industry, and maybe the dependently typed people are just loud rather than actually directing the language, but it feels like the entirety of the discussion around the development of the language is about dependent types.
These sound more like opportunities than problems ;)
I've added a screencap: https://taskell.app/#importing-github-projects
I think I've been using "Kanban" very loosely in that case! In my mind it was just a list of tasks that you move around. I'll have a look into it.
So we should understand these extensions will be phased out over a comparable timeline as `*` , that is 6 or 7 years ?
1. I'm a bit surprised to hear that. Opaleye has no problem with anonymous record types (one can just use tuples) and it's hard for me to imagine that Beam has not solved this problem. 2. It would be trivial to add a function to Opaleye that issues an \`EXPLAIN ANALYZE\` for any query (I'm happy to take a PR) and I can't imagine it would be difficult for Beam to support that either. 3. JSON operators are no problem for Opaleye. In principle it can support dynamic columns but I've never bothered adding that since no one has expresse an interest. I'm sure Beam either does support these or could support them if its users needed them.
Absolutely agree. I'm astonished to see so much obsession with dependent types when there's loads of low hanging fruit we can pick at the value level. Your examples of extensible records/variants and first-class modules are both great. On the other hand, if dependent types don't complicate value level programming then I won't oppose them.
Nobody said they were mutually exclusive. Two things don't have to be mutually exclusive for one to be worked on to the exclusion of the other.
You're case seems rather exceptional, if you just want to do the common things I think beam is pretty good. I wish they'd make working with the migrations easier though.
I am using `Servant` to serve an API. I want to use `servant-js` to generate the Javascript code for querying the api. However I have added authentication through `servant-auth-server` (JWT Tokens) and I am unsure how to use the Javascript generation functions. Do I need to extend the current `servant-js` capabilities or is there a way to pass in headers?
That's what \*I\* said! :-)
I'd be interested to know at what point of learning Opaleye you get stuck in "type gymnastics".
Maybe a mix of persistent, postgresql-simple, and one of beam et al. Start with persistent for any table. Then escape to postgresql-simple when you want to write anything beyond get by id queries, and upgrade pieces to beam when feel like.
I don't think they will be phased out. But rather be enabled as part of `-XDependentTypes` similar to `-XExplicitForall` gets enabled as part of `-XScopedTypeVariables`
&gt; Do you have benchmarks comparing this to Data.ByteString.Char8? No benchmarks unfortunately. AsciiString is obviously gonna lose in performance though due to the overhead of 7-bit cross-byte alignment. OTOH ByteString loses in memory footprint, occupying 8/7 as much memory for ASCII data. Hence the abstractions provide different tradeoffs and thus serve different purposes. &gt; Your deferred-folds package looks very interesting BTW. Thanks. It is an ongoing research.
A company failing is usually (always?) a human problem. A Haskell company failing doesn't mean that Haskell is the problem. A PHP company becoming a wild success doesn't mean that PHP was a good technology choice. The technical problem is a big one, but the human problem is much bigger. And you'll only get a chance to reap the benefits of the technical choice if you solve the human problem.
Yeah that package looks great
Opaleye is more on the weird-syntax-everywhere end of the spectrum for me. The [tutorial](https://github.com/tomjaguarpaw/haskell-opaleye/blob/master/Doc/Tutorial/TutorialBasic.lhs) starts off making sense but the mode of operation/choice made just isn't/aren't what I was expecting, for example: &gt; Opaleye assumes that a Postgres database already exists. Currently there is no support for creating databases or tables, though these features may be added later according to demand. No supporting of creating tables? I mean I can probably just run a raw query, but this seems like a weird thing to leave out. &gt; Opaleye can use user defined types such as record types in queries. This is a little bit further down, but it strikes me as odd that this isn't the obvious *primary* usecase. I have my domain objects, and I want to basically adapt them to a persistence layer, not necessarily build an algebra around `PgColumn` types -- I'd rather introduce those when I need to. For a simplistic example, a library that expected you to implement a multi parameter type class like `HasColumns m` which exposed a method `columnsNames :: m -&gt; [ColumnName]` which could then be put together when deriving a generic way to create an insert statement for a given `m` would be so much more straightforward to me. This is where I start to feel like maybe I'm just not comfortable enough with what opaleye wants to do, as in what the benefit of writing it in that way is. Also I find: &gt; youngPeople :: Query (Column SqlText, Column SqlInt4, Column SqlText) &gt; youngPeople = proc () -&gt; do &gt; row@(_, age, _) &lt;- personQuery -&lt; () &gt; restrict -&lt; age .&lt;= 18 &gt; &gt; returnA -&lt; row really hard to read 
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [tomjaguarpaw/haskell-opaleye/.../**TutorialBasic.lhs** (master → aad1ca2)](https://github.com/tomjaguarpaw/haskell-opaleye/blob/aad1ca2f249d9059748b7536fdabe40819c960a3/Doc/Tutorial/TutorialBasic.lhs) ---- 
Thanks for the feedback! 1) Acid-world includes a VCache-like persistence strategy (it only works well with keyed maps). With the acid-world implementation the spine of the data structure is kept in memory, while values are only pulled in from the persistence layer when called for. I've put together a working test for indexed sets (ixset-typed) as well - so you can run queries on an indexed set while only keeping the indexes in memory rather than the whole structure. This means you can address much larger data set than in traditional acid-state. 2) I think automated checkpoints should solve that (it would be fairly trivial to support this, including deleting old unneeded checkpoints and event logs) 3) A global lock for updates seems like a good thing potentially? I'm not sure how avoidable it is, without implementing merges. A global lock for reading doesn't seem essential though - in my current implementation there is a global lock but I think I'd get rid of that. I'm not sure that acid-state has a global lock for reading actually? I agree that a strong migration story is essential. I have a few ideas in that direction but not 100% sure about them yet. What you say about acid-state, though, is not entirely true - I've personally been using it in production with a state of a few GB for years, and have gone through dozens of schema migrations without ever losing any data.
I use persistent (with esqueleto when needed) a good amount. Maybe these qualify as too much type hackery for you, but it's pretty straightforward to figure out what the actual queries are. You can avoid the template haskell if you want and define all the datatypes/class instances yourself. I think using the template haskell is much more convenient though. 
I've been waiting for something exactly like this. I'm super excited to test it out.
Haha don't get too excited yet, there is a fair amount more work to do before it is ready for serious use! Out of curiosity, what is attractive about this as opposed to acid-state?
I think esqueleto is the closest to what I envision -- I will give it a try again in the near future. Also I want to point out that I didn't mean that TemplateHaskell was not convenient, I just didn't want to expose myself to too much stuff that I didn't understand at the time. It is absolutely more convenient than writing tons of boilerplate but I just want to minimize spooky action.
That's a good feature list! I'm going to bring this up at IOHK and see if it would be feasible to test this out, as we currently use `acid-state` and face many of the problems that your library solves. I am especially excited about multiple state segments and alternate persistence for huge data stores -- our newest design for the acid-state database is to use a record of `IxSet`s from `ixset-typed` to provide SQL-ish querying on "primary keys," and being able to only load a "table" (or even, part of a table by index!) would be absolutely fantastic. That `acid-world` has a test suite is also very nice. I've been extremely skeptical of `acid-state` since implementing the very first tests for it, especially after reading [about how well `sqlite` is tested](https://www.sqlite.org/testing.html). Unfortunately, it doesn't seem to cover my main problems with using acid-state: - Debugging. It's incredibly difficult to debug an acid-state database file in the presence of a bad migration. The error messages are incredibly bad and provide almost no context about what happened, and there's no way to get a "partial" restore along with a good, actionable error message on the event that failed to read/parse. I have lost about two weeks of productivity at IOHK due to migration-related programming bugs. - Actually being ACID. `acid-state` uses `bracket` for it's file backed storage mechanism, which ignores OS signals, which means you need to install signal handlers yourself to ensure you get a non-corrupted event log if the user sends a SIGKILL for whatever reason during an `Update`. ([relevant GitHub issue](https://github.com/acid-state/acid-state/issues/79)) The partial write detection and correction would be helpful for this, but I'd still be concerned about the bullet-proof-ness of this vs SQLite or some other more well-worn software. - Lack of an offline event viewer. This is partially because you have a user-defined serialization and deserialization formats, so you can't actually provide this. One thing that happens in a real world app is that the database gets into a weird state, and in any other database, there's a console or GUI you can check into that'll show you the SQL tables or document stores or whatever. `acid-state` cannot have this, and `acid-world` could only have it if it required a consistent serialization format that *doesn't* require knowledge of the user's datatype definitions and instances. This would also require it to be human-readable, which is an onerous requirement for a serialization format wrt compressed size and encoding/decoding performance.
Special support for semigroup data structures wrt parallel and distributed updates would be useful. 
Looks interesting. One question, though, why not name the module `Data.AsciiString` to fit `Data.ByteString` or `Data.Text`?
I called this `FTraversable`: https://www.benjamin.pizza/posts/2017-12-15-functor-functors.html
Also, there's this issue in `acid-state` that forced company to create fork and fix it in fork since maintainers decided to not accept changes: * https://github.com/acid-state/acid-state/issues/79
&gt; In the next release (or 8.5 years in), the -XStarIsType extension may be removed from GHC to simplify the internals. Seems too generous to me. Will it be easy to maintain it so long? Would many people use libraries that can not get a syntactic update for &gt;4 years?
https://hackage.haskell.org/package/base-4.8.1.0/docs/Control-Concurrent.html#v:forkIO &gt; The newly created thread has an exception handler that discards the exceptions BlockedIndefinitelyOnMVar, BlockedIndefinitelyOnSTM, and ThreadKilled, and passes all other exceptions to the uncaught exception handler. So the threads are likely receiving BlockedIndefinitelyOnMVar but are discarding it. 
Maybe I can give you my schema and example of SQL query and you could help me to rewrite it into `squeal`?
Issue #79 is exactly what caused me to implement "Partial write detection and correction" - in acid-world events are only considered to be persisted if their event id has also been written to a separate file, so that on restore it is possible to tell the difference between partial writes and corruption. (This is an idea floated in that issue thread, but I think implementing it in acid-state would have taken a bit of rewriting).
Thanks!
Absolutely! Please open an issue.
Ah, so the issue you might run into with trying to do that is that, whatever the persistent layer is (SQL, acid-state etc) at some point the type you are interested in has to be serialised and deserialised. Can you point me to the type you are trying to store? 
Please take another look at [Squeal](https://github.com/morphismtech/squeal). While the type trickery it uses may look intimidating, Squeal aims to provide familiar syntax that generates predictable SQL. Think of it as a way to learn Haskell's type-level capabilities in a setting you already are familiar with. With Squeal you can use your own domain models, just give them `SOP.Generic` and `SOP.HasDatatypeInfo` instances and you can populate your models using Squeal queries. Squeal also supports DDL statements and migrations.
Counterpoint: `cabal repl` and equivalents exist, and I'd rather not have 20 lines of the same extensions at the top of every module.
To what extent does the average haskeller still endorse "avoiding success at all costs"?
I said that knowing this, and I rather would for readability. I can see why one might not (more lines), but it seems overall less correct
Good point about backwards compatible, but couldn't that be kept with a preprocesor flag?
I'm curious about why `opaleye` doesn't use a "record-of-functors" represention, like `vinyl`, instead of a unrestricted type variable for every single column. I tried it a few years ago, and the types just got too big and messy. this shrinks the number of type parameters to a constant one or two, and reuses record-generalizations of typeclasses.Like `RecApplicative` for `Applicative`, where you can "validate" a record-of-maybes (nullability) into maybe-a-record, with `rsequence`. See: * http://reasonablypolymorphic.com/blog/higher-kinded-data/ * http://hackage.haskell.org/package/vinyl-0.9.0/docs/Data-Vinyl-Core.html#v:rtraverse * https://hackage.haskell.org/package/composite-opaleye * http://hackage.haskell.org/package/generic-lens
For myself, I want to write raw SQL queries and manage my schema outside of Haskell. But I also want better validation than I get from postgresql-simple. I'm not sure this is a reasonable thing to want, but I've heard that typed racket can do it, with queries type-checked against the database at compile time.
"record-of-functors" is probably a misnomer. concretely, what i mean is: data User f = User { identifier :: f Int , name :: f (Maybe Text) } type User_Postresql = User PG type User_Haskell = User Identity type User_Query = User ??? newtype PG a = PG (PGTypeOf a) type family PGTypeOf a where PGTypeOf Int = PGInt PGTypeOf (Maybe a) = PGNullable (PGTypeOf a) ... or something like that. to be clear, I'm not saying this is better or possible, since expressively representing SQL in the Haskell type-system might require the flexibility of more type variables (for nullability, aggregation, intermediary computations during queries perhaps, etc). 
Firstly, many thanks, this kind of feedback is exactly what I was hoping for! Ok, great! It sounds like what I've called \`CacheState\` would be usable for you, and it does exactly what you describe with \`ixset-typed\` - it keeps a fully queryable \`IxSet ixs (CValIxs ixs v)\` in memory, and inside the update / query monads provides an api for interacting with it, including running any arbitrary transformation. So this allows exactly what you mention - you can issue a query that, say, restricts the IxSet to only those values you are interested, and then only that part of the IxSet will be deferenced into actual values. There is an example [here](https://github.com/matchwood/acid-world/blob/develop/test/Test.hs#L488) . Yeah my plan would be to aim for exhaustiveness with the tests - they are by no means complete at the moment! In particular I plan to have a set of tests that try to break the persistence layer by throwing asynchronous exceptions at random moments during crucial parts of the code. * Debugging - agreed, I think we can do better than acid-state at least. I plan to introduce a standalone log, which should help (i.e. it will actually tell you what was happening and at what stage something went wrong). And one of the nice things about having events serialised as a uniform container is that it is possible to examine some parts of them even when other parts won't deserialise (i.e. all events have a date and a uuid, as well as the event specific arguments). This means it would be possible to give an error message like "The event arguments for the event "insertUser", issued at 2018-08-05..., with uuid 123-.. could not be deserialised". Same for checkpoints - each segment is stored in a separate file, so you will get an error like "Could not deserialise segment named "Users" - the parser error was: ". * This is something that hopefully we will be able to solve! My idea so far is to model persistence in such a way that at some point a single atomic file system action will take place, and the success or failure of the persistence is indicated by that action (e.g. renaming a file). My checkpointing process currently does something similar to that, but this will be an area where I really want to focus on getting it right (hopefully with help and input with those more knowledgeable in this area!). I am aiming to make this bulletproof as far as possible - I want it to be that at every possible stage it should be possible to kill the process in any way imaginable and still have the state restore. * Yeah I've had a think about this before. That is one of the motivations for providing a JSON backend - the JSON backend does actually produce human readable files (they are not actually JSON, but a stream of JSON values that would be easy to write an offline parser for). As for storage, segment files (checkpoints) are gzipped (configurable), so I'm not too worried about that, but I don't think Aeson's general performance will match CBOR. Similarly with a db backend - if you use postgres or something then you can obviously inspect the events and segment state directly in the database. I was thinking earlier today though about a more general solution for the deserialisation problem. It would require some effort, but at some level every parser is simply some combination of primitives. So all that is really necessary for an offline event viewer is to define some intermediate type for every deserialisation strategy (like \`Value\` for JSON) that encapsulates every primitive serialisation type available. And then you would be able to write a tool that could generically read acid-world files, even in the absence of their specific types, with the same limitations as JSON or databases in general provide - you might see an event as something like "Event "insertUser" &lt;date&gt; &lt;uuid&gt; EventArgs \[CBORInt 1, CBORUtf8Text "Haskell", CBORUtf8Text "Curry"\]" or whatever (probably more convoluted than this, but you get the idea).
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [matchwood/acid-world/.../**Test.hs#L488** (develop → e8784e6)](https://github.com/matchwood/acid-world/blob/e8784e67d2214269d035cac166a9ba410ecf2ec6/test/Test.hs#L488) ---- [^delete](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply e3nk7dq.)
Yeah, that's a great abstraction. But the name Unfold is not precise. I remember Haskell thrift bindings package use FoldList, which is more precise IMHO. But anyway thanks for extracting this and put into shape!
This really belongs in acme =)
Would you mind expanding a little on that requirement?
You're looking at this from the perspective of performance, whilst the issue is in the tradeoff between performance and memory footprint. Some applications need performant manipulation of characters, whilst others need efficient memory usage. E.g., I've developed this package, while working on an application, where I have to store millions of URIs in memory and do lookups, without any character manipulation. URIs are ASCII. It also saves space in the serialised form BTW.
I think that is essentially what squeal is reaching for, except slightly higher in safety than you want. Maybe opaleye could expose a training wheels version, similar to what happstack-lite was a long time back. Maybe it wouldn't be a direct upgrade to opaleye for users who outgrew it, but it could serve a conceptual training ground. Something where you have the query monad, but try to remove anything else. esqueleto is also close to what you are describing.
After some searching and hacking I appear to have found a solution. Creating the following instance for `Auth` does the trick. instance forall lang ftype api auths v. ( HasForeign lang ftype api , HasForeignType lang ftype T.Text ) =&gt; HasForeign lang ftype (Auth auths v :&gt; api) where type Foreign ftype (Auth auths v :&gt; api) = Foreign ftype api foreignFor lang Proxy Proxy subR = foreignFor lang Proxy (Proxy :: Proxy api) req where req = subR {_reqHeaders = HeaderArg arg : _reqHeaders subR} arg = Arg { _argName = PathSegment "Authorization" , _argType = typeFor lang (Proxy :: Proxy ftype) (Proxy :: Proxy T.Text) } Now I'm pretty new to type level programming so a lot of this is going over my head, I just found a very similar solution online and fought with compiler until the types matched up. The only issue with this solution is the fact that it produces: `Authorization: &lt;key&gt;` instead of the desired `Authorization: Bearer &lt;key&gt;` I cannot figure out what to change to achieve my desired result. My current solution is to just pass `Bearer &lt;&gt; key` to the javascript functions, but this doesn't feel right. Any help toward a solution to this/ helping my understanding of the type level stuff would be greatly appreciated.
&gt; I’ve developed this package, while working on an application, where I have to store millions of URIs in memory and do lookups, without any character manipulation. URIs are ASCII. How are the URIs generated? If they’re read from a file, couldn’t you just memory map the file? Even if that isn’t an option, how much of a difference will saving approx. 10% of your memory usage make? Plus, millions of URIs is on the order of MBs, will saving a few MBs really make or break your application? If so, I’d question the wisdom of running a garbage-collected like Haskell on such a memory-constrained system. &gt; It also saves space in the serialised form BTW. Yes, but then normal applications (vi, less, etc) can no longer read or write the file. Since you are already requiring computation when reading or writing, you might as well use a traditional compression method at that point. 
I also find stack a bit much for small projects. I like to use `runhaskell` to run small script like programs. It comes with the standard distribution of the haskell platform. For example, you could make a file called Hello.hs and, add the following: main = do putStrLn "Hello. What's you name?" name &lt;- getLine putStrLn (name ++ " is such a lovely name.") Then just type `runhaskell Hello.hs` in a terminal and, it will run the program like a script.
&gt; Changing the software necessitated reassessing almost every process at the company, insiders say. But Lidl’s management was not prepared to do that. If you're not changing your processes, what's the point of trying to build a new system at all? Unless the old system was a catastrophe from a technical perspective then changing your processes is the entire point isn't it? 
Because we already have tons of flags and nobody is particularly interested in this change. Would also bring up questions like "what's the m for?". If you really want the functor constraint you can always just use fmap anyway. 
Beam author here. Automigration is an optional part of beam. I don’t use it, because I think automigration is a terrible idea, but I’ll admit it’s useful for testing and prototyping. The preferred beam way of doing migration is to write out the migrations yourself, which gives you explicit control over the order of commands
I don't think you should underplay just how very much clearer "Type" is as a kind, compared to the rather enigmatic "*". Maybe :: * -&gt; * "What now?" Maybe :: Type -&gt; Type "Ah - Maybe is a function from types to types." it just all reads so much more intuitively, and to be fair, when you're learning Type level programming, clarity is a blessing.
This is nothing personal, and I’m sure you’ve worked hard building this library, but to be honest, I don’t think this solves any problem better than an existing solution. You might think that’s “destructive”, but I think projects that don’t fill a niche are also destructive. It makes it harder for new developers to find the best solution for their problem. You say the ecosystem lacks an alternative to this project, but I don’t think any ecosystem has an equivalent library. The closest I can think of is VarInt from Google Protobufs, but that solves a different problem (and doesn’t have the alignment issues your library does). Any community *should* be grateful for people who contribute (and maintain!) useful, open source libraries. However, instead of being indignant at my (perceived) lack of appreciation, I think you should revisit your problem, determine if the problem this library solves is actually a problem that needs solving, and, if so, determine if there is an existing library that solves it better. I don’t mean any offense to you personally, I just disagree with the design decision you’ve made here, and I think that vocalizing it might be helpful to others who see the post (and maybe even you). I’m genuinely sorry if you find that destructive.
No
The latest Opaleye does support that, though I'm phasing it in slowly so as to not give users used to the other style a heart attack https://github.com/tomjaguarpaw/haskell-opaleye/blob/master/Doc/Tutorial/TutorialBasicTypeFamilies.lhs#L138
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [tomjaguarpaw/haskell-opaleye/.../**TutorialBasicTypeFamilies.lhs#L138** (master → aad1ca2)](https://github.com/tomjaguarpaw/haskell-opaleye/blob/aad1ca2f249d9059748b7536fdabe40819c960a3/Doc/Tutorial/TutorialBasicTypeFamilies.lhs#L138) ---- 
I find it interesting how you can give pretty definitions with the reader monad: average = div &lt;$&gt; sum &lt;*&gt; length
SPJ wrote a paper on OOP style overloading in Haskell to mimic subtyping etc. Here: https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/overloading_conf.pdf
I don't think Haskell really needs OO. You can get the same functionality easily by other means. I mean interfaces are already supported through type classes. If you want methods you can use the state monad. If you want inheritable fields you can use lenses and the state monad. And if you want to model inheritance you can also use type families, possibly in conjunction with a type class. (Thats what the eta language does to some extent)
Is it besser now? import Data.Array import Data.Graph import Data.List type Distr = Array Int Int emptyDistr :: Distr emptyDistr = ... --add a pebble to a given distribution in all possible places addAPebb :: Distr -&gt; [Distr] addAPebb = ... --check whether a distribution contains another contains :: Distr -&gt; Distr -&gt; Bool contains = ... --check whether a vertex is reachable from a distribution reaches :: Graph -&gt; Vertex -&gt; Distr -&gt; Bool reaches = ... --count how often a function has to be performed until the result satisfies some condition localPebbleNumber :: Graph -&gt; Vertex -&gt; Int localPebbleNumber g v = length $ takeWhile (not . null . snd) $ iterate ( \(goodDistrs, badDistrs) -&gt; let (newGoodDistrs, newBadDistrs) = partition (reaches g v) $ filter ( \distr -&gt; not $ or $ map (contains distr) goodDistrs ) $ nub $ concat $ map addAPebb badDistrs in (goodDistrs++newGoodDistrs,newBadDistrs) ) ([], [emptyDistr])
Make sure you're a member of the `docker` group. If you've just added yourself, you'll need to log out and then log back in.
I ended up turning off my computer and turning it on again, so I don't think that is an issue. I also could run ```docker run hello-world``` without ```sudo``` so I believe that means everything is fine with the docker permissions.
I know my post was basically a complaint, but I want to make sure that you know I absolutely appreciate and value your work -- thanks so much for making Beam! I'm glad to hear your stance on auto-migration -- I had too much of a snap reaction to seeing that in the docs and assumed that was the primary method of migrations. Again, I want to thank you for making the library, Beam was the easiest to follow that I've seen so far -- the tutorial made it super clear the relation between the domain models and the tables, along with including how to query them. I really do think I will likely start using beam once I can find some time to get used to it properly.
I will! I think I'm going to take a weekend and take a closer look at all these libraries, it's the only way to get past. Yesterday I was going through [a guide to GHC's Extensions](https://limperg.de/ghc-extensions/) to try to start understanding the bits that make me uncomfortable but it's more of Squeal's syntax that makes me uncomfortable... Like this: getUsers = select (#u ! #name `As` #userName :* #e ! #email `As` #userEmail :* Nil) ( from (table (#users `As` #u) &amp; innerJoin (table (#emails `As` #e)) (#u ! #id .== #e ! #user_id)) )
\`(:\*)\`, \`(&amp;)\` and \`(!)\` are all just operators at the value level. \`(:\*)\` is a tupling operator that generates lists, it's a "heterogeneous" version of the usual \`(:)\` operator. \`(&amp;)\` is reverse function application \`x &amp; f = f x\`; in the context of Squeal it let's you chain together modifier clauses. And, \`(!)\` is Squeal's version of qualified aliasing, i.e. the \`.\` operator from SQL.
Thanks for doing this. It's very useful, and I wish that a resource like this had existed when I was learning haskell. After a cursory review, one thing I would place in a different category is `PackageImports`. I would have it in Miscellaneous instead of Questionable. It is uncommon for this extension to be necessary, but I have had situations where, in an application with a lot of dependencies, two different library authors used the same module name. I make crucial use of this extension in my `primitive-checked` package, which shims out everything in the `primitive` package while retaining the same module names.
woot!
Wrong thread? Did you mean to post this in https://www.reddit.com/r/haskell/comments/94a8za/a_guide_to_ghcs_extensions/ ? :)
Sure, but you have to admit it's still a mouthful.
I'm assuming you're asking for advice so here is mine. By no means I'm a good Haskell programmer so take my advice with a grain of salt. in src/Data/Deck/Deck.hs The use of list monad (or applicative) in `deck` is really good. I really like it. You will need to either pass a value that is randomly generated to the `shuffle` function or make it `Deck -&gt; IO Deck`, otherwise you wouldn't be able to randomly shuffle the deck. There are two functions `dealHand` and `dealCommunity`. Since you're already pattern matching the list, why not just: dealHand :: Deck -&gt; (Deck, (Card, Card)) dealHand = make . cards where make (x:y:r) = (uncards r, (x, y)) Also these two functions won't be complete (that is, there is a chance that it will throw exception at runtime) thus you might want to return `Maybe (Deck, (Card, Card))` to denote there is not enough cards. Although if my understanding of poker is right, it should never run out of cards. In src/Hand/Unigraph.hs, the part `(\x -&gt; (length x == n))` can be written as `(== n) . length`. I'm not sure if it is better, though. In src/Hand/Hand.hs, the `twoOfAKind`, `threeOfAKind` and `fourOfAKind` are extremely similar, maybe you can write a function where you pass the different parts? You can still define `?OfAKind` that only fill the different parts. `same` function didn't check for empty list, while I understand the list will never be empty, it still could be written in an safer way: same :: Eq a =&gt; [a] -&gt; Bool same xs = null xs || all (== head xs) (tail xs) That's all I can give. overall the code is very clear and readable. 
Thanks for the detailed explanation -- I will make sure to refer to this comment when I give Squeal another try
I'm imagining rapper grillz with operator symbols.
First of all, sorry for taking so long to reply -- I had left a tab with your post open back when you submitted it, but only got back to it now. Thanks for the edit! I have just approved it. Approval is not quite as critical as it might seem as first: except for the Wikijunior books, Wikibooks is configured to display the latest version of pages, regardless of whether the have been reviewed -- for regular books such as WYAS the reviewing system is mainly an early warning system for edits. By the way, feel free to get in touch about issues like this involving WYAS or the Haskell Wikibook, either via a PM here or through [my talk page](https://en.wikibooks.org/wiki/User_talk:Duplode). I try to keep an eye on edits to the Haskell Wikibook, and I probably should begin to do the same with WYAS.
What's an example of an arrow `T -&gt; C` ?
yes I think it's a good place to start (your `~/..local/bin/stack`) - has you docker-user (or some of it's group) access to it? (not sure if read and execute access is enough) it's probably enough to run the container with your user and inspect stuff from within the container: /usr/bin/docker run --user ssoss -ti -v /home/ssoss/.local/bin/stack:/tmp/stack sha256:093b06222f1f8b2c25252d43c0e3242e475121ae135dd6d0ccdef5a2cc8c494b /bin/bash (see: https://docs.docker.com/engine/reference/run/#user)
Type classes like Eq :: Type -&gt; Constraint Show :: Type -&gt; Constraint .. could be written `Eq :: TC` and `Show ::TC`
"Lesser memory footprint is useless, only character operations performance and readability matter." - this is your argument. You deny the possibility of there being applications, which don't need character operations or readability, and would benefit from lesser memory and space consumption. Then you take this highly subjective and ignorant argument, and apply it with effort to prove someone's work pointless, and now even "destructive". I can't see anything constructive about your actions. In fact they have all the properties of trolling.
Hmm. Here's my honest opinion. As awesome as the idea of acid-state is, there's one disqualifying reason that I can't see taking it seriously in a production setting. And that is that state written in acid-state is only accessible using acid-state. That means only one programming language (even if it's a great one!). And it means all of your data management has to be done from scratch. Business dashboards? Data cleanup scripts? Replication? Anomaly detection and alerting? Legal compliance for GDPR? You name it, and you're now building it from scratch, when pseudo-off-the-shelf products could do a bunch of this with only minor integration work, if your data were in any system that exposes it with a common API or protocol. (Here, "off the shelf" could mean commercial, or open-source, or in-house systems developed and maintained by an unrelated team.) My read of the above is that you're not really solving this. You've got a database backend, but it looks like that's only an option if you're okay with treating the database as an opaque blob that can't be queried except by your Haskell server. Is that correct?
Agree. I really would like to tell a few anecdotes but I think the article already gives a good impression of the general situation here.
I suspect they just wanted a more efficient implementation of the same processes.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [purebred-mua/purebred-email/.../**Internal.hs#L114-L143** (master → a602876)](https://github.com/purebred-mua/purebred-email/blob/a6028762ad0ae38ebc1a530497e3bcec8adb217a/src/Data/RFC5322/Internal.hs#L114-L143) ---- 
Oh, sure :) I was still half asleep when I asked it
As a long-time acid-state user I think this is great, as I have experienced some of these problems myself. However, in defence of acid-state, it is certainly possible to write "production"\* software using acid-state. I have a project that's been using acid-state since it was called "happstack-state", my oldest clients have data from that time that's been thru *a lot* of migrations. For me the greatest advantages are that you can write your db code in haskell and that your program becomes "configuration-less". That said, I have experienced some of the issued that acid-world aims to address: * Partial writes (happened a few times because of disk full, solved by manually shaving of bytes at the end) * Memory spikes, to the point of being OOM-killed (I had a branch at one point read the events lazily, but finally solved it by reducing the size of my transactions that where way to large) Someone mentioned migration of large records, it is annoying, but not a big problem really, just a bit ugly. Other problems I've had: * SafeCopy changed serialization format at one time, many years ago, caused some headache migrating * Be careful if you use datatypes defined in external libraries in your state, because i they change you will have to deal with migrating that stuff. And as there's no way to have two different versions of a package in your project, you need to invent some tricks. * You can't really rely on Exceptions for error handling inside transactions, because of the lazy nature of things this may wreck your state. * I think a common pattern is to have a transformer stack inside your transactions (Reader for carrying around commonly used parameters and Either to convey errors). You then pass in what you want in your Reader as a parameter to each transaction. This means the size of a transaction is at least the size of what you put in your Reader. Now, if some fool where to carelessly use some huge datastructure in the Reader...
What's also true is that not every module requires the same set of extensions. There are parsing modules, and code generation modules, etc. So I agree with /u/chessai 's readability argument, because when the concerns in a project are separated the extensions at the top help the reader guess immediately what's going on. 
This package is really cool ! I've been doing stuff like that manually (or using TH). :-)
I like the way you've modeled events.
I tried all the suggestions, except for the strict fields as I don't see why it would help my program. The way I use \`Config\` is in \`config :: IO Config\`. Since it's in an IO, is it correct to assume the fields will be calculated all when I do \`c &lt;- config\`? I have tried to add an unused field and to populate it with a non-existing environment variable. I already see the exception when I do \`c &lt;- config\` in \`ghci\`.
Well, you probably *shouldn't* be carpet-bombing the subreddit if you aren't getting answers. But I'll let this one through, this one time. I also wanna know the answer. 
I understand your concerns for sure, but I think that acid-world can help with at least some of these issues. The database backend isn't an 'opaque blob', it is a normal looking database (at present without foreign keys, but that might be possible to implement). A database in acid-world would be expected to look something like *Event - UUID &lt;uuid&gt; - Date &lt;timestamptz&gt; - EventName &lt;text&gt; - EventArgs &lt;json&gt; *User - ID - FirstName &lt;text&gt; - [...] [..] Or whatever fits the structure of your data. The only part of this that would be defined by acid-world would be the schema for events (and there is nothing to stop users from writing their own backend with a different schema). The database backend at present is very much at a POC stage, but I think any direction it went in would be with this sort of structure - the whole point of providing a database (and, to some extent, a JSON backend) is to address the concerns you bring up. In another [reply thread](https://www.reddit.com/r/haskell/comments/94rxip/feedback_request_on_poc_acidworld_a_potential/e3nk62t) I floated some possible solutions to providing some generic tooling for the file system backend as well. Of course if you manage the backend data independently of acid-world that you will have to careful that you don't break anything, but that is always the case I think - pretty much any non-trivial program models some logic and relationship that is not captured at the database level I think (arbitrary validation for example).
For the random-part I would just supply a partially applied function with randomness: foo &lt;- randomRs (1,52) &lt;$&gt; newStdGen let shuffle = shuffleRand foo With http://hackage.haskell.org/package/random-1.1/docs/System-Random.html#v:randomRs and shuffleRand :: [Int] -&gt; Deck -&gt; Deck So you do not throw io around everywhere, but have an infinite amount of randoms :)
I agree entirely, I've been using acid-state in production for years as well! That's one of the reasons I was so reluctant to move on from something like it, but there are some things (like the single root type) that are just annoying to deal with (in one application I have a root `DB` type with 140 fields) and to address even that part of it required basically a complete rewrite, so I figured why not try to broaden things a little more! Of the issues you raise I hope acid-world should help with partial writes, memory spikes, and exceptions for error handling (this last one might be a bit tricky to get right, but I want to make the update monads capable of throwing exceptions, with MonadThrow maybe). The other issues are, I think, things you have to address regardless of the persistence layer. If somebody changes the `FromField` deserialisations in `postgresql-simple`, for example, then you equally have some headaches! And haha your last point is familiar to me as well... I think the only solution there is to give us 'fools' some more comprehensive documentation and guidelines, and hope that we read it! 
Agreed. And I've seen this mindset far too often at Tech. initiatives led by business people. I fear that the mindset of a typical business-person is quite conservative when it comes to processes. There's huge revenue at stake – why change it if it has worked so far? As an engineer, I'd never want to work in a team in charge of keeping a dying 30-year old monolith alive. But then again, one can just imagine the sheer amount of complexity with the supply chain and so forth that Lidl has to deal with – being a retail giant at that scale.
Look at apecs and sdl for starters. There are some Vulcan bindings if you’re ambitious. And in that latter case you’ll want to learn a bit about BangPatterns, minimal allocations, and general GC behavior to keep the gc pauses from affecting UX. 
Interesting stuff. I've been working on [adding multiple serialisation strategies to acid-state](https://github.com/acid-state/acid-state/pull/96), attempting to address some of the same concerns in the other direction. It's nice to have some story for modular state segments and composable updates. One consequence of using `Symbol` to identify segments/events though is that you end up needing orphan instances, and two libraries might use the same name for different things. Have you thought about making it possible to use data types instead?
Postgresql-typed, but I don't like the live DB connection at compile time
When you use `ask` in `greeter` `m` gets substituted for `Reader String` and `r` gets substituted for `String` so `ask` inside `greeter` has type `Reader String String`. `do name &lt;- ask; return ("hello, " ++ name ++ "!")` is [syntactic sugar](https://en.wikibooks.org/wiki/Haskell/Syntactic_sugar#Do_notation) for `ask &gt;&gt;= (\name -&gt; return ("hello, " ++ name ++ "!")`. Lets see types `(&gt;&gt;=) :: Monad m =&gt; m a -&gt; (a -&gt; m b) -&gt; m b`. First argument to `&gt;&gt;=` is `ask :: Reader String String`. So you have `(ask &gt;&gt;=) :: (String -&gt; Reader String b) -&gt; Reader String b`. You have `return :: Monad m =&gt; a -&gt; m a` so `(\name -&gt; return ("hello, " ++ name ++ "!") :: String -&gt; Reader String String` therefor entire `ask &gt;&gt;= (\name -&gt; return ("hello, " ++ name ++ "!") :: Reader String String`.
Hey, mirpa, just a quick heads-up: **therefor** is actually spelled **therefore**. You can remember it by **ends with -fore**. Have a nice day! ^^^^The ^^^^parent ^^^^commenter ^^^^can ^^^^reply ^^^^with ^^^^'delete' ^^^^to ^^^^delete ^^^^this ^^^^comment.
&gt;Laziness is made optional No, it is not Where is no laziness in idris. Try this: \`\`\` fibs : Stream Integer fibs = 1 :: 1 :: zipWith (+) fibs (drop 1 fibs) \&gt; take 40 fibs \`\`\` (Inf, Lazy - isn't laziness (call by need), it's call by name)
Used to be a PITA, I've done it recently and it was fine. My preference is still Linux, but for not directly Haskell related reasons.
I don’t think a smaller memory footprint is useless, I just don’t think a fixed 12% reduction will solve anyone’s problem. What prompted you to write this library? Was your application swapping or running out of memory? If so, and if your library fixed those problems, I’ll shut up. However, I’m guessing that wasn’t the case. I think you saw that your application had a ton of ASCII data in memory and got nervous. But if you don’t have any symptoms, how can you know if this is the cure? &gt; You deny the possibility of there being applications, which don’t need character operations or readability, and would benefit from lesser memory and space consumption. My question to you is: benefit in what way? Using less memory only matters if you don’t have extra memory. If there are no memory errors, then why try to reduce memory usage? &gt; I can’t see anything constructive about your actions. In fact they have all the properties of trolling. Again, I’m sorry you feel that way. I can assure you I’m not trying to troll you. I understand that you are proud of this library and that it must suck to have someone show up and criticize it.
I'm not proud. It's not about me. It is not criticism if your argument is subjective or ignorant. There's also nothing constructive in discouraging people of doing useful things.
I like this a lot. My biggest gripe with type level programming in Haskell is the (what feels like) arbitrary restrictions on what you can do. 'First class' type families is an exciting prospect to me, I already can see how I can apply this at $work.
Looks cool, although also would be nice to see postgres backend in own library, to reduce dependency list of core package.
&gt; It is not criticism if your argument is subjective or ignorant. I only know as much about your use case as you’ve told me. If my argument is invalid because of some information Im missing, then enlighten me. If you have some objective facts about the library, please share them I’d love this discussion to be based around objective measurements, but you dont seem to have any. &gt; There’s also nothing constructive in discouraging people of doing useful things. Believe ot not, but I’ve tried to be tactful here. But, to be blunt, I have to say it: I don’t think your library is useful. You keep acting like open sourcing your software is some noble deed and any criticism goes against the spirit of the community, but the fact is that open sourcing a library is only as helpful as the library is good. The premise of your library is flawed, and, despite all your complaints about the subjectivity of my arguments, you are yet to mount a fact-based defense of your own library.
Not sure how that happened, but yes, I posted this in the wrong place.
I'm totally not an expert, but I failed to reproduce the said behavior. Maybe writing your runtime environment will help more knowledgeable people. [Gist](https://gist.github.com/viercc/f6f66f49c2ccc696e72bfdd879f4d2df) My environment: $ cat /proc/version Linux version 4.15.0-24-generic (buildd@lgw01-amd64-056) (gcc version 7.3.0 (Ubuntu 7.3.0-16ubuntu3)) #26-Ubuntu SMP Wed Jun 13 08:44:47 UTC 2018 
I could not reproduce the problem on my machine (macOS 10.13.1, apfs filesystem) with the `bracket` version: both the process which successfully acquires the lock and the process which patiently waits for the lock use ~0% CPU. Then I tried the `forkOS` version, and at first it failed with `RTS doesn't support multiple OS threads (use ghc -threaded when linking)`. I then recompiled with `-threaded`, and this time I could reproduce the problem: sometimes one of the two processes uses ~300%, sometimes it doesn't! I then went back and compiled the `bracket` version with `-threaded`, and this time one of the two processes uses ~300% every time. So the problem is definitely related to the threaded runtime. To simplify my test process, I then tried to run two copies of the `bracket` version inside two `forkIO` threads instead of inside two separate process, but this time the result was even worse: the two threads both acquire the lock, neither waits for the other! Same with `forkOS`. At least `forkProcess` behaves sanely, that is, one of the two forked processes blocks and uses 300% CPU, the same as when I run `myTestProgram &amp; myTestProgram` in the shell. After a few more tests in which I sometimes killed my processes with Ctrl-C, I got into a state in which I couldn't kill my processes with Ctrl-C anymore, presumably because some previous run acquired but didn't release the lock, so every process was stuck in a C call. That made me realize that the `bracket` wasn't buying me anything, so I replaced it with a regular `do` block. The 300% CPU remained, so masking is not involved in the issue. I then tried varying the number of capabilities using `+RTS -N1` up to `+RTS -N4`. The result: +RTS -N1 ~0% +RTS -N2 ~0% +RTS -N3 ~200% +RTS -N4 ~300% Weird, why didn't I get ~100% with `+RTS -N2`? Maybe there is some locking logic somewhere which works fine when two capabilities race for that lock, but results in all the blocked capabilities spin-locking if more than two capabilities race for the same lock? The other threads aren't doing anything, so which lock might they be attempting to acquire? We have a parallel garbage collector in which all the threads stop what they're doing and participate in the garbage cleaning chore, so there's probably a synchronization point in which the capabilities wait until all the other capabilities have entered the garbage collection state, and similarly when leaving that state. The body I am protecting with the lock is a 4 seconds sleep, so all capabilities are idle for 4 seconds. An [idle GC](https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/runtime_control.html#rts-flag--I%20%E2%9F%A8seconds%E2%9F%A9) will occur after 0.3 seconds. Increasing the idle GC to 1, 2 or 3 seconds still produces the ~300%, whereas increasing it to 5, 6, or 7 eliminates it. Bingo! Googling confirms that the [parallel gc is using spin locks](https://ghc.haskell.org/trac/ghc/ticket/3553). So I think what is happening is that the `waitToSetLock` call makes the current thread unavailable for garbage collection (I don't know if that means the thread is "descheduled" as in the linked issue), which then causes the other threads to spin-lock while waiting for that thread at the next parallel GC. The problem still occurs with the latest release, GHC 8.4.3, and I could not find an existing issue describing the problem, so please [file a ticket](https://ghc.haskell.org/trac/ghc/newticket?type=bug).
I wasn't aware of any uses of `PackageImports`, but your cases make absolute sense. I'll move the extension up as soon as I've got a proper internet connection again. Thanks!
The use of free monads looks interesting to me. I’m used to monad transformers, but this looks a lot neater. I really like that the abilities of the monad are encoded as, essentially, a type level list of abilities, as opposed to monad transformers where each ability is nested in a not-so-obvious way inside another one (e.g. `ReaderT Conf (LoggingT m)` — or is it the other way around?). Are there any gotchas with free monads compared to monad transformers?
In your type class declaration you put `update :: m -&gt; s -&gt; e -&gt; m` where only `m` is a type class parameter, this should imply that when implementing the type class FMS for a type `m` you should implement a _parametric polymorphic function_, i.e. a function which has basically the same definition for every possible choices of `s` and `e`. It seems to me that you wanted to do _ad hoc polymorphism_, that is that you wanted a function whose definition declaration can change for different choices of the type variables `s` and `e`. Type classes are the way to deal with _ad hoc polymorphism_ in haskell, so I think that the best solution to your problem could be to use _multiparameter type classes_, possible using some constraints with your state and error types. I hope this help.
You could use some more modern haskell features to give a nicer interface for bounded existential types. Object '[Eq, Show, Collection] And forward instances like instance (Contains Collection xs) =&gt; Collection (Object xs) where
Just more fiddly to pattern match on mostly. Might also add another layer of pointer indirection.
The things in the talk can be achieved with `mtl` (tagless encoding of the language) too: type EventSourced s a =&gt; forall m. (MonadError ServantErr m, MonadState s m, MonadIO m) =&gt; m a or (you have to keep `m` free): type EventSourced' m s a = (MonadError ServantErr m, MonadState s m, MonadIO m) =&gt; m a --- How you nest the effects might matter: `ExceptT e (StateT s m) =/= StateT e (ExceptT e m)`. Sometimes you want exceptions to unwind state modifications, sometimes you don't, sometimes you don't care (e.g. when you don't `catch`).
You can get overriding by just fixing it for yourself: https://hackage.haskell.org/package/oo-prototypes-0.1.0.0/docs/Data-Prototype.html
And sadly you don't see the full kind signature of classes in Haskell code when in reality you should be exposed to it over and over, as of GHC 8.6 it's simply not supported class Eq :: Type -&gt; Constraint .. I operate on the principle that you should always be able to write signatures out in full (as given by the `:type` and `:kind` commands). Before I adopted this religiously I could trace so much confusion to being unsure about the type or kind of things, not to mention that it allows you to notice patterns that you otherwise would miss, [compare](https://gist.github.com/ekmett/b26363fc0f38777a637d): newtype Y p a b = .. data Nat (p :: Cat i) (q :: Cat j) (f :: i -&gt; j) (g :: i -&gt; j) = .. -- GADT syntax newtype Y :: Cat i -&gt; Cat i data Nat :: Cat i -&gt; Cat j -&gt; Cat (i -&gt; j) An ongoing [GHC proposal for `-XTopLevelKinds`](https://github.com/goldfirere/ghc-proposals/blob/top-level-sigs/proposals/0000-top-level-signatures.rst) lets us write just that type Eq :: Type -&gt; Constraint class Eq a where (==) :: a -&gt; a -&gt; Bool (/=) :: a -&gt; a -&gt; Bool as well as a proposal [`-XTopLevelSignatures`](https://github.com/goldfirere/ghc-proposals/blob/top-level-sigs/proposals/0000-top-level-signatures.rst).
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [goldfirere/ghc-proposals/.../**0000-top-level-signatures.rst** (top-level-sigs → bd26f0a)](https://github.com/goldfirere/ghc-proposals/blob/bd26f0a634bd0c93579b07a1ef2c0d3bc63f8ac7/proposals/0000-top-level-signatures.rst) ---- 
This is not a refactoring though. The goal of refactoring is to improve the inner structure of a piece of code *without* changing its external interfaces. Haskellers complain because this seems like bikeshedding and requires updating every text talking about kinds.
Yeah, similar to how ´RankNTypes` subsumes `Rank2Types`.
Oh my god, this makes me excited. CTE's have been one of the features I've been most wanting in Beam. Actually my most requested feature would probably be data-modifying CTEs. These are extremely helpful when inserting into junction tables, which happens all the time: https://stackoverflow.com/questions/20561254/insert-data-in-3-tables-at-a-time-using-postgres Does beam support this yet? I was actually going to make a github issue for it but now that we're here wanted to check
Sorry, I posted in Hask anything because I wanted to avoid clutter if it was a simple issue. But then it seemed like not a simple issue so I made a thread. Should I have just created a thread first? 
Thank you for the help, and for explaining your debug process! I will file a ticket later today. 
Very cool!
This is very cool. I wonder what the essential differences are between this and my ["HKD pattern and type-level SKI"](http://h2.jaguarpaw.co.uk/posts/hkd-pattern-type-level-ski/). One difference is that `Eval` is not applied recursively, so you have to write Eval (Fst (Eval (Fst '( '(Bool, Int), Float)))) rather than Eval (Fst (Fst '( '(Bool, Int), Float ))) I fear that the former may be too painful to use in practice. One could write extra combinators to help with this, like the SKI version has, but that leads me to wonder what really is the essential difference between the two versions. 
TransformListComps is so nice in data analysis! A simple example: count xs = \[(the x, length x) | x &lt;- xs, then group by x using groupWith\] That group + finalize functionality is really concise when it comes to collecting up a bunch of similar data and then finalizing the grouped data. I use it literally every day at the office. 
An alternative to `PackageImports` is using the [`mixins:`](https://github.com/danidiaz/really-small-backpack-example/tree/master/lesson1-renaming-modules) section of Cabal (&gt;=2) to rename some module dependencies of a component. I prefer it because it doesn't force the code itself to be aware about packages.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [danidiaz/really-small-backpack-example/.../**lesson1-renaming-modules** (master → 3eac68f)](https://github.com/danidiaz/really-small-backpack-example/tree/3eac68f9e34381e2f42402817c7d6f60e319f688/lesson1-renaming-modules) ---- 
Opaleye is 100% a fantastic library -- I really want to make it clear that's not what I was implying. There is lots of hard work that has gone into the library and it shows, pretty sure it's the go-to suggestion for Postgres-only work at a level higher than `postgres-simple`. I *personally* have not mastered the concepts and feel uncomfortable using tools I don't understand enough of. This uncertainty is compounded by the fact that to me it *seems* like I could write a really dumb typeclass-only driven implementation for the kind of SQL boilerplate I want to cut down on (selects with single keys, selects with pagination, inserts, etc).
Have you tried the [reflex-platform](https://github.com/reflex-frp/reflex-platform)? If you follow the instructions there, it should just work.
Looks like the version of reflex-platform you're using is extremely out of date. I would have expected it to still build, but nonetheless I would still recommend following this guide: https://github.com/reflex-frp/reflex-platform/blob/develop/docs/project-development.md. It's much more up to date and useful.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [reflex-frp/reflex-platform/.../**project-development.md** (develop → 0b2e793)](https://github.com/reflex-frp/reflex-platform/blob/0b2e793f6b39d32d7abc4d242f39ca9ac699c9b2/docs/project-development.md) ---- 
Is the plan to make something like Matplotlib for numerical computing? I'd love to have Haskell as my go-to tool for vectorized code and machine learning, if the low-level stuff was handled by mature tooling.
&gt; arbitrary restrictions on what you can do. That's a type system. Type level programming is always a double-edged sword.
Yeah, the present structure is just for this early development phase - if and when released it would be split into a core with separate backends/serialisers.
&gt; Laziness is made optional but that's probably good Maybe. It has some real problems, though. https://gitlab.com/boyd.stephen.smith.jr/idris-okasaki-pfds/blob/master/src/ImplicitQueue.idr is *supposed* to use laziness, to get good amortized bounds. I haven't figured out exactly what's wrong, but the Idris performance makes no sense. (Inserting 1000 elements takes more time than inserting 100 elements 10 times, even though the resulting structure is identical.) Compare with Haskell (that doesn't even need to explicit `Lazy` type), where it performs as expected. (High constant factor, but good scaling.)
Thanks, it took a bit of experimenting to arrive at that. I tried compressing the definition of events even further - in principle I thought it might be possible to write a type family that extracted the function result from a function type but that doesn't really work. Even trying to make assertions ghc about the last type in a type level list seemed to be a no go.
I wholeheartly agree with the proposal, I'm just not looking forward to retraining my brain and rewriting my code to use `Type` instead of `*`. The former should be relatively easy, since I've been spelling it as `Type` in Idris and `Set` in Agda for a while. The later feels daunting, but it's not like I have a popular package on hackage.
As long as there's only one field, yes, it does.
I see the following main difference in points of view, which makes them complementary (and it took me a while to figure it out): defunctionalization (as in my or Richard Eisenberg's blog posts) answers the question "How to represent arbitrary functions in a composable way?" and SKI (as in yours) answers the question "How can we actually compose those functions?", both in the constrained environment of "type-level Haskell" which doesn't feature anonymous functions in particular. In my blog post I show off some monadic combinators but what we can really express with them is not so obvious, precisely because we don't have anonymous functions to use `(&gt;&gt;=)` with. Encoding a calculus like SKI would provide an objective lower bound on the expressiveness of the combinators in this language. SKI is a representation of terms consisting of applications (`S`), constants (`K`), and variables/holes (`I`). The limitation of `Arr` in your blog post is that constants and variables must be type constructors (or partially applied ones), and not type families. So far, the most general way to lift that limitation seems to be defunctionalization.
Does anyone know this history of why `*` was originally chosen over something like `Type`? Was it completely arbitrary? Or was there a good motivation at the time that no longer applies?
Have you seen the C FFI already in the base library? [http://hackage.haskell.org/package/base-4.11.1.0/docs/Foreign.html](http://hackage.haskell.org/package/base-4.11.1.0/docs/Foreign.html)
I'm not sure what you mean by 'being a compiler plugin'. `Coercible` is just a type class whose instances can only be declared by the compiler itself, because the compiler is the only authority that can guarantee that the representation of two types are equal (modulo user-supplied `role` annotations). You could always use `unsafeCoerce` if you're bold, no need for a 'compiler plugin' that postulates `Coercible` instances that morally just use `unsafeCoerce` without any formal justification under the hood. A logically sound dependently-typed language could probably make `Coercible` an open type class with an additional proof obligation, but really these are instances you want to have derived automatically by your compiler or some elaborator/tactic.
What package is \`randomRs\` in? I can't seem to import \`System.Random\`...
Ah nice! I did see discussion about that a little while back but I wasn't sure anything was actually happening (apologies for the couple of places where I've said it wasn't under active development!). By the way, I am not at all opposed to folding this work into acid-state itself, if anyone wanted that, but I think that some of the changes necessary to, eg, support multiple state segments would be so large that it would effectively be a rewrite, without any easy migration pathway. I did think about that issue with `Symbol`. I don't think it's a killer problem - there is uniqueness checking at compile time for event and segment names, and I think it should be possible to fairly easily import and "rename", as it were, segments and events, if they do clash. But it isn't ideal I agree. There were two reasons I went for `Symbol` - firstly was because I wanted to use a constant time access heterogenous keyed data struture, and Vinyl's `AFieldRec`, which uses `Symbol`, was the easiest option. Secondly because in two parts of the implementation it is crucially important that we get a unique serialisable key to represent an event / segment - deserialisation is achieved with a `HashMap Text (Deserialiser ss nn)', and segments are stored in files named with their `symbolVal s`. By using `Symbol` and `symbolVal` and a uniqueness constraint I can guarantee on a type level that every event and segment is represented by a unique `Text`. To avoid orphans we could add `type SegmentKey :: Symbol` and `type EventKey :: Symbol` to segments and events, and use those as keys where necessary, but that still doesn't solve the import problem. Does `Typeable` provide any guarantee that different types' `TypeRep`s will be shown with different strings? If so then that might be a possible solution, as the uniqueness constraint should still work, and the only added overhead in userland is declaring `data InsertUser` or whatever. I would also need an alternative to `AFieldRec` - some constant time keyed heterogenous structure in which the keys could be any `Typeable a =&gt; a`. Which should be doable I think. It would probably have to `unsafeCoerce`, but both `acid-state` and `Vinyl` internally use that anyway so that should be ok.
I believe it may predate type checker plugins. But otherwise I have wondered the same thing as well. I guess one practical problem is that compiler plugins are annoying to use – the whole {-# OPTIONS_GHC -fplugin=Foo.Bar.Baz #-} stuff is not a good user experience (my [proposal #92](https://github.com/ghc-proposals/ghc-proposals/pull/92) might help here a bit). And custom error messages might be easier inside the compiler than with a plugin. So unfortunately, I think the answer is that “type checker plugins are just not good enough”.
see nomeatas comment, I didn't know which part of the plugin-architecture (apparently type-checker plugin) would be needed.
Overall it's good, I have some minor comments: 1. matches = concat $ dropWhile (not . same) $ ... can be extracted to a function since this code appear a lot 2. When you do (take n xs, drop n xs) (like in dealHand and dealCommunity), you should use the splitAt function instead, which does the same thing 3. Using hlint could help you with some style issues. Also, adding -Wall to your compiler can help too 4. The do notation in Table.round can be removed. (let doesn't need do) 5. speaking of the Table.round function, you could use the state monad to avoid the `d'` temp variable (this one is more advanced)
It was 14th in 2014... Too bad it dropped like that. 
I think similiar to the prelude, one could always activate the plugin per default (and maybe disable it with some pragmas). But in theory the whole functionality of Coercible could be implemented using type-checker plugins?
Yes, it is mainly the state thing I am unsure of
Well, not the role annotations. Or at least not with that syntax; maybe using `ANN` pragmas. But solving `Coercible` constraints should be possible. Certainly after [this change to the API](https://ghc.haskell.org/trac/ghc/ticket/14691) (should be in 8.6). But, and I think this is what u/sgraf812 alluded to: `Coercible` just exposes features present at Core (coercions), and a type checker plugin could not safely expose more than what’s possible in Core.
That's interesting. Would you care to share your experiences in this discussion: https://github.com/ghc-proposals/ghc-proposals/pull/157
The 2014 ranking of Haskell according to this https://spectrum.ieee.org/static/interactive-the-top-programming-languages#index was 25.
I've seen this kind of comment lot, especially when discussing Haskell vs Lisp and it intrigues me - it's highlighting an area of ignorance in my understanding of programming. I don't understand it - I feel like every time I've used dynamically typed languages I've felt like when it comes down to it they have a notional type at runtime that has surprised me in a way that a typed language could have warned me about at compile time. "Undefined is not a function" is an example, huge numbers of unit tests in Python that end up forcing types is another. Duck typing always felt like somewhat of a pretence. I say that as someone who learnt to program with dynamically typed languages so I'm not being some sort of Haskell snob, indeed I feel that I'm quite a newbie when it comes to Haskell. So what am I missing? Where are these situations that I can express in a dynamically typed language but can't in Haskell? I guess I'm missing something obvious but feel blind to it :(
The `random` package.
ok, thanks! I was more interested in the functionality of generating Coercible-instances.
Jupyter already has a Haskell kernel, just saying.
I wrote something similar a while ago. I found a way to abstract the rules with something I call a `Matcher`. It's very similar to a naive parser. This way I was able to write the rules very concise. [In case you are interested to see another approach.](https://github.com/muesli4/poker/blob/7ac612adf93f4d1425feaa38d5d185c965a43b16/src/Poker/Card.hs#L105)
Ah, I think I see what you're saying. Correct me if I'm wrong but, I think you want to marshal persistent data structures to and from C functions. If that were the case I'd definitely try to avoid dropping down into C. I think I'd try to convert a C function like `void func (int* data, int num)` to a Haskell function of `func :: List[Vector Int] -&gt; List[Vector Int]`. That would seem pretty ideal to me and, be totally doable in Haskell using closures.
Great, thank you. I removed all my .nix files and used the default.nix from that guide. I don't know why my reflex-platform was so out of date, but I checked out the most recent commit and committed the change. It builds my frontend now!
I don't get the jobs rating. I've never saw any haskell opening in my region but I see plenty of Visual Basic/Delphi Am I living in the wrong country?
Thanks! 😊
I didn't say **typed** programming. I said type-*level* programming. That is, writing computations that spit out types. The tension is always this: The more expressive your type computations, the less you can reason about them, but the more descriptive they can be about your value level programming. Haskell98 isn't much more sophisticated than simply typed lambda calculus *at the type level*. These days, with the right extensions and techniques, you can write untyped lambda calculus at the type level. That is to say, your types can be arbitrarily complex. That's not necessarily a good thing.
Yes and no. The ultimate goal to make a Haskell interpreter with output that is both graphical and interactive. Numerical computing and machine learning would be just one of many use cases. Right now, the graphical part works. However, I have not spent any time making it pretty, it's raw HTML at the moment. For instance, I'd be happy if someone were to hook the [Chart](https://hackage.haskell.org/package/Chart) library into the HyperHaskell ecosystem. It should be possible via diagrams already, but an instance of `Display` would be great too. If you do, please make a pull request to [hyper-extra](https://hackage.haskell.org/package/hyper-extra). Let's put it this way: I'm not in the business of designing a plotting library. I'm in the business of making the visual representation of Haskell values more functional. 
I'm not sure exactly how to do it on the Haskell side, but I would keep most stat in some sort of opaque (to users) context object, and have every function take a pointer to said context object. Dereferencing the pointer should put all of your exported functions in IO, but that doesn't prevent you from having a larger body of "pure" code also.
\#6 when limited to just embedded work! Seems a bit odd to me.
Thank you for taking the time to read my project so thoroughly and give such helpful feedback. It was very helpful :D.
I see.. lol. How do I import it?
This involves the interplay between type systems and semantics. One of the catchphrases is "well-typed terms do not get stuck". This program: True + 4 is "stuck", since it isn't a value (like `True` or `4`), and there are no evaluation rules that will move it towards becoming a value. Most type systems are conservative, which means that they will sometimes reject valid programs in order to make sure that they will always reject invalid programs. Consider this program: if (length [1,2,3] == 0) then 7 else False It will fail to typecheck, because the "then" and "else" branches have different types. The whole expression could be treated as having the type `Bool` though since it is always going to return `False`. Depending on where you are standing you might view this as an arbitrary restriction. If the type system knew about constant expressions then we could perhaps add type rules that would allow us to have different types for the different branches in those cases. There are all kinds of tradeoffs there, both in terms of coming up with the type system to support that,and in the ergonomics for the users of the type system. 
Books have editions and libraries have versions Until Haskell is officially mainstream we should embrace change, because we won't become mainstream by preserving the status quo
Take a look to this post https://www.fpcomplete.com/blog/2017/12/building-haskell-apps-with-docker. Right now we are building our images with the native integration, using ``` stack image container ``` 
It seems to be reliant on typeclasses (`Arg`, `Vary`)...in what way is this practically different from how *quickcheck* does things? What if we wanted to sample a function `Double -&gt; Double` that is, say, always decreasing, or one that is monotonic?
I actually did exactly that, but when I downloaded a Fedora 28 image, it would crash when I would run ```dnf update``` or ```dnf install &lt;package&gt;```, so I thought this would be easier since it is 100% built into ```stack```
I'm not an expert on this topic, but from the post I think that you can even build it from Windows using ```stack image container```. But using Ubuntu as the base image.
&gt; Unfortunately, IHaskell has a reputation for being difficult to install. This seems like a great project, especially the Windows support, but it would be nice if some of this effort went into making IHaskell easier to install instead of starting from scratch. I'm also aware of [haskell.do](https://github.com/theam/haskell-do) which targets the same niche.
`This app ranks the popularity of dozens of programming languages... `
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [NixOS/nixpkgs/.../**lib.nix** (master → 308d780)](https://github.com/NixOS/nixpkgs/blob/308d7809d36abd8d61c3d61ed260c3df771979f2/pkgs/development/haskell-modules/lib.nix) ---- 
Ahh! I set up the stack.yaml, Dockerfile, and Makefile as suggested in the last section of the article, and I _still_ got the same error! ```docker build -t skyb0rg/myapp-base .``` worked fine (I used Dockerfile not Dockerfile.base) But when I got to ```stack build```, I got Received ExitFailure 1 when running Raw command: /usr/bin/docker run -v /home/ssoss/.local/bin/stack:/tmp/stack sha256:093b06222f1f8b2c25252d43c0e3242e475121ae135dd6d0ccdef5a2cc8c494b /tmp/stack --version So I don't know what to do now
Be aware that for building your project inside a container, you need to add this line to stack.yaml ``` docker: enable: true ``` 
Not sure about implementing `Coercible` as a compiler plugin, but it's totally possible to add `Coercible` suggestions to code with GHC source plugins! * https://github.com/mpickering/hashtag-coerce
Nice plugin, but shouldn't this rather be rewrite rules that just do use coerce?
At present it only handles the most absurdly simple form of the file - one with 'packages:' only, and no globs. Googling for nix info is gives results with a very low signal to noise ratio, but as far as I can tell there's no particular support in nix for glob expansion, so the globs would have to be converted to regular expressions and run through, say, `filterSource`. I suspect the right answer is to build an external tool ala cabal2nix, or to build the functionality into cabal2nix itself, because the nix language isn't particularly good for parsing, either. I'm reluctant to throw a pull request into nixpkgs while there's so many caveats on the usage.
just a hunch: do you use SElinux? Try to run `sestatus` if this fails you probably don't - but if it says yes then maybe this could help you: https://stackoverflow.com/questions/24288616/permission-denied-on-accessing-host-directory-in-docker there are additional security/policy settings in place beyond *nix basisc
I'm not sure, actually. I always had an impression that rewrite rules are kinda fragile and not reliable. And we probably need to use `inspection-testing` tool (which is great, btw, thank you, /u/nomeata !). Writing `coerce` manually guarantees that performance won't suffer in any cases.
I just tried `stack templates`, saw `ghcjs` and give it a shot. Well, it setup nothing. Ticket - https://github.com/commercialhaskell/stack-templates/issues/131. If you have some working tutorial to make ghcjs running with stack that would be awesome (I kinda like stack, so far only downside was it is a bit space hungry).
THANK YOU SO MUCH I do use SELinux, and I never knew that it could cause an issue like that Thanks again, I've been stuck on this for days
Is it a coincidence that there are Haskell companies called "Lumi" and "Lumiguide"?
Can this be generalized to `fnWith :: CoGen a -&gt; (a -&gt; Gen b) -&gt; Gen (Fn a b)`, where the output range can depend on the input value? That generalization would seem useful but I have no idea what for.
Some of the state is not necessary for the user to access, and therefore I want to keep it within the framework.
Haskell has always been a shining light... 🤔
I tried Purescript for a front-end recently. The biggest pain point for me by far was that I couldn't get deriving JSON to work, so I ended up just writing all the (de)serialization by hand. There some rough edges, but ultimately, it did live up to my expectations of being "Haskell with a runtime designed for the browser." In particular, it does not have the obscenely low abstraction ceiling of something like Elm. I was able to use free, transformers, rank 2 types, etc., without any issues.
If you *really* need to unsafely create global mutable state, a common pattern with GHC Haskell is: stateRef :: IORef StateType stateRef = unsafePerformIO (newIORef initialState) {-# NOINLINE state #-} The state will be created the first time it’s accessed, and shared thereafter; you can access it with the functions from `Data.IORef`. The `NOINLINE` pragma is important to prevent multiple unshared copies from being created inadvertently if the call is inlined into multiple locations. However, you should try to avoid this if possible. For example, can you store this “framework state” in one of the state parameters you already have, but only expose part of it publicly? 
Historical note: it was with functional dependencies instead of type families, but Oleg Kiselyov used a "single function" style of type level programming for much of his classic hlist work, as in [here](https://mail.haskell.org/pipermail/haskell-cafe/2011-February/089184.html).
Isn't your second program simply coercing an int to a bool? At some point it had to pick a type to make it a valid program. I've been bitten by coercion enough times that I don't feel that it's a helpful tool in my programming. I see that it sort-of allows more "valid" programs but only really though an additional magic processing step.
This looks very cool, especially the fact that it's windows compatible. As a Haskell enthusiast who is fairly deep into the windows ecosystem (my day job is as an F# developer), I often have trouble getting Haskell libs to work on windows, so this is very much appreciated. I'll definitely try building on my windows home machine as soon as I get home and see if I can help out.
This isn't a coercion - but it also isn't strictly Haskell, since Haskell doesn't have the kind of type system you'd need for this to work. That program will always return a `Bool` no matter what you do, and a type system that dealt with constant expressions could deal with that without having to do any kind of coercion. The main point though was that in Haskell it is ruled out by the type system but is not stuck. The evaluation rules would proceed from if (length [1,2,3] == 0) then 7 else False to if False then 7 else False to False every time. So even though "well typed terms do not get stuck" is in play, we have a type system that rules out some terms that don't get stuck, which makes it a conservative type system. That does simplify writing and reasoning about the type system though, so it's not something that too many people lose sleep over.
What do you mean by "couldn't get deriving JSON to work"? I wrote a library for doing this for all normal JSON structures and wrote a tutorial on how to use generics for working with ADTs https://purescript-simple-json.readthedocs.io/en/latest/
There is a link to IRC channel in hie readme somewhere closer to the end. I had completion working in that setup with ncm not sure if it works now
Ok, I see what you mean about this not being coercion since that part of the program is not evaluated. I can't see why that's an appealing characteristic for a language though as the opposite of what you said is true for those languages in this case: they are hard to reason about.
I think Argonaut was the one I was trying to use, since it looked like it was trying to be compatible with Aeson, but it looked like there was a major change to the way Purescript does generic deriving recently and Argonaut hasn't caught up. I'm sure it all makes sense if you've been following along and know how things have been progressing, but for me as someone jumping in cold, it was all quite confusing, and I needed results immediately so I just hand-wrote the JSON stuff myself. But ya, I'll check yours out and see if it works for me.
That would be great! The GUI app and the Haskell parts are fairly independent from each other, though. So, any trouble with installing Haskell libraries will likely stay that way. But it would be very helpful if I could add a binary package for the GUI parts [to the release][1]. This requires playing around with [electron-package][2]. The `Makeful` currently only has a target `pkg-darwin`. [1]: https://github.com/HeinrichApfelmus/hyper-haskell/releases/tag/v0.2.1.0 [2]: https://github.com/electron-userland/electron-packager
IHaskell requires Jupyter, which requires Python. I don't want to spend my time figuring out install plans for Python. This way, it's at least time well spent. Oh, right, I forgot about [haskell.do](http://haskell.do). I'll add it.
I remember [this comment](https://www.reddit.com/r/haskell/comments/3hlck0/planned_change_to_ghc_merging_types_and_kinds/cu8twew) from three years ago and I am very happy that they've managed to do it in time.
At that point, couldn't the SKI combinators be symbols as well? type family A (sym :: k ~&gt; l) (x :: k) :: l data S (f :: k ~&gt; (l -&gt; m)) (g :: k ~&gt; l) :: k ~&gt; m data K (a :: l) :: k ~&gt; l data I :: k ~&gt; k type instance A (S f g) x = (A f x) (A g x) type instance A (K a) _ = a type instance A I x = x type instance A HT ('C '(h, _, _)) = h -- etc. and `S` could be further split into data TyCon :: (k -&gt; l) -&gt; (k ~&gt; l) data S2 :: (k ~&gt; (l ~&gt; m)) -&gt; (k ~&gt; l) -&gt; k ~&gt; m type instance A (TyCon f) x = f x type instance A (S2 f g) x = A (A f x) (A g x)
Do I understand correctly that your interface files contain all definitions of functions too? &gt;In the current version, when some module A re-exports declarations for module B, those declarations are also added to module A's interface file. Does this mean the source code for those declarations is available too?
This is awesome. Nixpkgs should definitely have something like this. I will say, though, that I rather don't like the idea of parsers written in Nix, or any complex algorithms for that matter. It's untyped and slow. Maybe it would be better to write a simple Haskell98 program for converting cabal.project into json? This would have the added benefit of being useful elsewhere, though the drawback of using import-from-derivation (unless something like [nix-plugins](https://github.com/shlevy/nix-plugins) ever becomes standardized).
Wow, thank you. For the record, yes, this seems to work for `Prelude`. You can write `mixins: base hiding (Prelude)` and then define your own and it Just Works!
They effectively can't (safely) access it through a opaque data structure. Also using a context object makes it slightly easier to support threaded operation. In this case the context type would just be a typedef for `void*` anyway. You wouldn't have a struct definiton anywhere in any C code. (The type of the context argument would be `Context*` = `void**`).
What about it?
&gt; It is simplest to just say that PureScript was the unique solution to the following set of constraints ... supporting things like sum types, row polymorphism, type classes and higher-kinded types. I thought this was kind of funny, almost like the constraints were picked after the language was. AFAIK, row polymorphism isn't really supported even in Haskell yet. Purescript is the only thing that satisfies these constraints. 
Awesome! If you've managed to create a binary of the GUI parts with electron-packager, could you zip them and upload the .zip somewhere? Then I could add it to the release. That would be nice!
Of course! I'll do it as soon as I get home, I have it packaged and ready (and make-able)
Of course! I'll do it as soon as I get home, I have it packaged and ready (and make-able)
Well, the are reliable when they work :) And nothing wrong with writing `coerce` manually, if you want to be explicit. But we really should make the compiler do that work for you whenever we can, for the benefit of those who don’t know, or don’t want to think about, `coerce`. 
As which-witch-is-which [mentions](https://www.reddit.com/r/haskell/comments/94a8za/a_guide_to_ghcs_extensions/e3r78rd/), yes. In fact, it seems it is even possible to make `Prelude` a Backpack signature that can be instantiated in several ways.
Apologies for any confusion! I realise that there is a lot of material and it is hard to compress it into something README sized... The part of acid world that addresses larger than memory data (what I've called `CacheState`) is (at present) a sort of standalone persistence strategy that only shares some of the structure with the rest of acid world. So acid world actually provides two different models of persistence - 1) A more flexible alternative to acid-state (multiple serialisers, backends, multiple state segments etc) but all with the same limitation that the total data has to fit into memory 2) `CacheState`, which is an LMDB backed persistence strategy for keyed maps (this is the part that is a bit like VCache). Ideally this could be folded into the first model, but I'm not sure if that is a worthwhile idea or not. There are many advantages to an event sourced system, like acid-world, like replaying of events. Also, a pure state based acid layer is inevitably easy and faster to use than something like VCache if you are dealing with smaller data sets / have a lot of RAM. Having said that, I did consider using VCache but I decided against it for a few reasons. As far as I can tell there is very little infrastructure around it at all. I have not been able to find any tutorials or any libraries building on it. Your experience report is one of the few things that ever seems to have been written about using it, and to be honest, reading your experience report was one of the things that put me off. It seemed like you had to put in a lot of investigation to get reasonable performance for a simple test case. So partly I am just outsourcing my judgement to the community in general - if VCache really was an easy win and a great replacement for acid-state then why isn't anyone using it? As for maintenance, VCache by its nature is a complex and ambitious solution, heavily reliant on some fairly involved logic relating to garbage collection and so forth. I don't want to invest a lot of time into building infrastructure around a solution that I don't understand the guts of, and that the community seems to have entirely ignored, for whatever reason. I am willing to be persuaded otherwise, but it will take a fair amount of convincing!
I kinda had the same suspicion. Nonetheless I really enjoyed the article.
A useful addition to this document might also be that PureScript has instance chains now instead of overlapping instances.
&gt; you are yet to mount a fact-based defense of your own library. AsciiString occupies 7/8 memory compared to ByteString - fact. It makes it useful for applications, which don't need to manipulate strings, but only store them in memory - fact. I've mentioned and explained that already. &gt; you dont seem to have any. Simply denying facts, which you've been doing so far, does not make them non existent. &gt; Believe ot not, but I’ve tried to be tactful here. But, to be blunt, I have to say it: I don’t think your library is useful. You've been tactful enough to mention that in your every comment starting from the first one. 
&gt; you are yet to mount a fact-based defense of your own library. AsciiString occupies 7/8 memory compared to ByteString - fact. It makes it useful for applications, which don't need to manipulate strings, but only store them in memory - fact. &gt; you dont seem to have any. I've mentioned and explained these already. Simply denying them, which you've been doing so far, does not make them non existent. &gt; Believe ot not, but I’ve tried to be tactful here. But, to be blunt, I have to say it: I don’t think your library is useful. You've been tactful enough to mention that in your every comment starting from the first one. 
Indeed. [Instance chains paper](http://homepages.inf.ed.ac.uk/jmorri14/pubs/morris-icfp2010-instances.pdf).
The [Types.md](https://github.com/purescript/documentation/blob/master/language/Types.md) in the [documentation directory](https://github.com/purescript/documentation/blob/master/language/) is also quite interesting.
It's a pretty messed up list to be fair. VHDL and Verilog are not embedded programming languages, neither is Arduino. HTML is not a programming language at all. Delphi, Scheme and Scala are mobile languages? Haskell is only listed for Enterprise and Embedded, where I'm pretty sure it's used for web pretty heavily in the back-end.
Except maybe for type classes, I can see relatively strong domain-modeling reasons to want row types and higher-kinded types. It's not a coincidence that Ur/Web, a language design for web programming, has those features and other advanced features, as do Links, Opalang, etc. (Type classes tend to be more of a convenience feature, even though they do make a qualitative difference in the overall feel of the language. Flexible qualified types (constraints, not just type classes) can enable fairly flexible EDSLs, so I can see how that would also matter on the web.)
Surely Eta is a Haskell, not a dialect. Anyway, applying the term "dialect" to Purescript seems to be consistent with standard usage: &gt; it is not an actual variety of the "standard language" ... but rather a separate, independently evolved but often distantly related language https://en.wikipedia.org/wiki/Dialect
**Dialect** The term dialect (from Latin dialectus, dialectos, from the Ancient Greek word διάλεκτος, diálektos, "discourse", from διά, diá, "through" and λέγω, légō, "I speak") is used in two distinct ways to refer to two different types of linguistic phenomena: One usage refers to a variety of a language that is a characteristic of a particular group of the language's speakers. Under this definition, the dialects or varieties of a particular language are closely related and, despite their differences, are most often largely mutually intelligible, especially if close to one another on the dialect continuum. The term is applied most often to regional speech patterns, but a dialect may also be defined by other factors, such as social class or ethnicity. A dialect that is associated with a particular social class can be termed a sociolect, a dialect that is associated with a particular ethnic group can be termed as ethnolect, and a regional dialect may be termed a regiolect. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/haskell/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
&gt; AsciiString occupies 7/8 memory compared to ByteString - fact. It makes it useful for applications, which don’t need to manipulate strings, but only store them in memory - fact. Ugh. I feel like we’re talking in circles, so I’m going to give this one more try: you are operating under the assumption that reduced memory usage is always good. That’s not true. Reduced memory usage is good when your system is low on memory and swapping. If you can reduce your memory usage and avoid swapping, you will have much better performance, since it is very slow to use swap space. You don’t have that problem, however. Additionally, a 12.5% reduction likely won’t be enough to tip the scales from “swapping” to “not swapping”. Desktop OSes don’t really run out of memory anymore (i.e. `malloc` will never return null). Embedded systems can have this problem, but that doesn’t apply here, since Haskell doesn’t support embedded systems. Reduced memory usage can improve cache performance, since you are able to fit more data into each cache line, reducing cache missed and the frequency of cache fetches. However, because you are using data that is 7 bits per unit, you will always have data that spans a cache line boundary (usually at both the beginning and end of the cache line, but always on at least one). This completely negates these benefits and then some. A modern OS will always use all of your available RAM, even if it’s just to cache some data (evicting that data when an application needs more memory). That’s because unused RAM is no good to anyone. So using less RAM for the sake of it doesn’t make this library useful. If your only argument is “it’s slightly more memory efficient, and that’s good for some reason”, then there’s nothing more for us to discuss.
Interesting, I've never heard of a language being described as "a Haskell" (in the same way one might say "a Lisp"). I think it's quite accurate, in this case. FWIW Eta describes itself as a "Haskell dialect": https://github.com/typelead/eta
one of the minor but really useful differences is the Symbol.Cons class. it's hella good, y'all
It's more like I had these constraints in mind when I designed PureScript initially, because I knew I wanted to use it for Real World Stuff then, and nothing has really come along to replace it for me. None of these things would be a deal breaker on its own, probably. For example, lacking row polymorphism wouldn't immediately disqualify a language for me, but the lack of sum types certainly would, and if you don't have type classes, you'd better have something to replace it with. Ur/Web is one interesting language that I'll probably seriously consider next time I have to choose a front-end language, actually.
Ok, so first of all, I love this. However, the following was my very first interaction which gave me a good chuckle and a haskell-sigh: ``` 1+1 ``` ``` &lt;interactive&gt;:2:1: error: • Overlapping instances for Hyper.Internal.DisplayIO a0 arising from a use of ‘displayIO’ Matching instances: instance Display a =&gt; Hyper.Internal.DisplayIO a -- Defined in ‘Hyper.Internal’ instance Display a =&gt; Hyper.Internal.DisplayIO (IO a) -- Defined in ‘Hyper.Internal’ (The choice depends on the instantiation of ‘a0’ To pick the first instance above, use IncoherentInstances when compiling the other instance declarations) • In the expression: displayIO (let e_111 = 1 + 1 in e_111) In an equation for ‘e_111111111’: e_111111111 = displayIO (let e_111 = 1 + 1 in e_111) In the expression: (let e_111111111 = displayIO (let ... in e_111) in e_111111111) :: IO Graphic &lt;interactive&gt;:3:1: error: • Ambiguous type variable ‘a0’ arising from a use of ‘+’ prevents the constraint ‘(Num a0)’ from being solved. Relevant bindings include e_111 :: a0 (bound at &lt;interactive&gt;:2:23) Probable fix: use a type annotation to specify what ‘a0’ should be. These potential instances exist: instance Num Integer -- Defined in ‘GHC.Num’ instance Num Double -- Defined in ‘GHC.Float’ instance Num Float -- Defined in ‘GHC.Float’ ...plus two others ...plus one instance involving out-of-scope types (use -fprint-potential-instances to see them all) • In the expression: 1 + 1 In an equation for ‘e_111’: e_111 = 1 + 1 In the first argument of ‘displayIO’, namely ‘(let e_111 = 1 + 1 in e_111)’ ```
I agree. Any language without lazy evaluation is, in my book, no longer Haskell or a Haskell dialect. It might be appropriate to call it “A Haskell-inspired language” or “A language with Haskell's syntax”.
Purescript is certainly more than “A language with Haskell's Syntax” which implies agreement only on syntax whereas their semantics is also very similar (i.e. the same except lazy evaluation). I know lazy evaluation is a big thing, but semantics also includes things like type systems and purescript shares that with haskell.
&gt; Reduced memory usage is good when your system is low on memory and swapping. If you can reduce your memory usage and avoid swapping, you will have much better performance, since it is very slow to use swap space. True. &gt; You don’t have that problem, however. Additionally, a 12.5% reduction likely won’t be enough to tip the scales from “swapping” to “not swapping”. You see, you've just made two suppositions and then used them deny the benefit that you've just mentioned. Libraries are intended for various scenarios, and when optimizations matter 12.5% sure is a serious number. &gt; Reduced memory usage can improve cache performance, since you are able to fit more data into each cache line, reducing cache missed and the frequency of cache fetches. However, because you are using data that is 7 bits per unit, you will always have data that spans a cache line boundary (usually at both the beginning and end of the cache line, but always on at least one). This completely negates these benefits and then some. Another assumption tied to a specific scenario. Also again you're thinking in units, whilst I keep saying that this library is not intended for per-character access. 
If you're after aeson interop, you could also try \`foreign-generic\` which is what we're using at Lumi. You can tweak the options on the PureScript side to derive decoders and encoders which are compatible with the vast majority of Aeson's options, and fall back to hand written instances as a last resort.
With linear haskell going live in a couple of months/years, would it be possible to add them to a monadic parser combinator library to avoid functions that backtrack? Like if it's a linear arrow, it doesn't backtrack and if it's a normal arrow, it might.
Add the dependency to `poker.cabal` (e.g. after line 19 or 27) and then `import System.Random` where needed.
&gt; semantics also includes things like type systems In the PL literature, “semantics” usually refers exclusively to the runtime behaviour of a program(ming language), ie, specifically _not_ the type system. &gt; purescript shares that with haskell PS’s type system is similar but also rather different to Haskell’s, much like its syntax. I’d describe it as “inspired by” Haskell, in both syntax and type system. (The semantics are actually a lot more like (a subset of) JS, which shouldn’t be surprising since one of PS’s design philosophies is to produce relatively readable JS.)
perfect thanks
I think I've read of people using Scala for Android apps, but yeah, that thing was always crap.
I commented in the past to the effect that they are kind of dialects. Bare in mind even linguists argue about what dialect means, but: * [Haskell as a culture](https://www.reddit.com/r/haskell/comments/79i7h9/why_is_elm_more_popular_than_purescript/dp2ariz/) * [Haskell as a language with many dialects](https://www.reddit.com/r/haskell/comments/79i7h9/why_is_elm_more_popular_than_purescript/dp2gp1d/) To be fair even GHC Haskell is a dialect, most people in practice don't use Haskell 2010. Does adding `Strict` to your language extension list suddenly make your module not Haskell? 
Yes, it does. That doesn't make it bad, just not-Haskell. wget http://pubs.opengroup.org/onlinepubs/9699919799/basedefs/V1_chap02.html#tag_02_02_01 -O - | sed -e 's/POSIX.1-2017/Haskell 2010 Report/g;s/POSIX/Haskell/g' Wish we had a standard and a certification / test suite of the quality of POSIX, UNIX, and the Open Group.
IIRC, PureScript "requires coherence", but also assigns names to implementations to make the generated JS more readable.
One reason FFI should still be standardized, but be optional part of the language (sort of like a standard LANGUAGE pragma). Eta has a great JVM/Java FFI!
Idris and Agda are Haskell-inspired dependently-typed languages. :) I think one of them might even have a JS backend, so it can replace PureScript. ;)
&gt; Does my approach make sense? Yes. &gt; Does it even make sense to create something like this in Haskell or is it just not the right tool? Yes. https://github.com/nh2/call-haskell-from-anything provides concrete examples on how to do this. You don't need the FFI-via-msgpack-serialisation bits if your interface already is a well-defined C API, but the cabal file shows how to make a `.so` Linux shared library and issues like [this one](https://github.com/nh2/call-haskell-from-anything/issues/19#issuecomment-263923769) show how to make it even easier in newer cabal (apparently; haven't tried that yet).
Thanks. I'd missed the split requirement and so couldn't understand how the cache updates would function. It seems that this makes sense where we want random access retrieval but queue/stack like insertion.
Thanks for this. It's a gem. So well reasoned and written. 
Another reason: If you want to implement 'Functor/Applicative/Monad' for some non-trivial data type, you might want to only implement 'Monad'. Then you can just define `fmap = liftM` and `(&lt;*&gt;) = ap`. This works, since `liftM` and `ap` are implemented only in terms of `(&gt;&gt;=)`.
It doesn't follow the spec for C FFI.
[Here](https://passy.svbtle.com/dont-fear-the-reader) is an extremely simple breakdown of how the reader monad is implemented. Puzzling through it yourself can be valuable, but if you're like me, you may learn faster by following along with someone else's implementation and playing with it.
I prefer `fibs = Data.Function.fix $ (0:) . scanl (+) 1`. But, I'm perfectly willing to have `Data.Function.fix` restricted to endofunctors of M-types or something like that which is compatible with strict/total/productive languages.
Is it unheard of to use a dummy value as an argument in another function as a kind of syntactic sugar? I've been writing some code that basically selects some kind of field from a data structure, given a filter to apply first. And it seemed natural for me to express this idea in a kind of sql-ish way, using some "syntactic sugar". A simplified sketch of my code: ``` takeAll :: Reader [(a,b)] c -&gt; () -&gt; (b -&gt; Bool) -&gt; Reader [(a,b)] c takeAll selector _ f = local (filter (f . snd)) selector suchThat :: () suchThat = () keys :: Reader [(a,b)] [a] keys = asks (fmap fst) example :: Reader [(a,b)] c example = takeAll keys suchThat (\val -&gt; val &gt; 5) ``` On the one hand, this seems kind of funky to me, but I also think that it introduces a fairly readable syntax that accurately reflects the computation and provides a nice layer of abstraction. So basically I'm wondering: is this an abuse of the language or somewhat acceptable (provided I include a bit of documentation)
Apparently Haskell is more popular than SQL
Here's what it looks like in PureScript, for reference: &gt; fibs = 1 : 1 : defer \_ -&gt; zipWith (+) fibs (unsafePartial fromJust (tail fibs)) 
I'd be happy to! Just submitted: https://github.com/tathougies/beam/issues/290
At the moment, an interface file for a given module contains types for all exported declarations, but only the core AST for the code in the current module. To do cross module inlining we still need to load the interfaces for the original source modules. It would be easy to also add the core AST for exported functions, but I'm a bit worried about the potentially quadratic space usage. However, each interface file is closed, in the sense that is contains enough information to compile the entire module down to object code. They're more "intermediate files" than "interface files", as they do contain the complete core AST for the module. The plan for bootstrapping is to have the existing compiler drop .di files after some stage, and then 'port each core-to-core pass to the bootstrapped compiler so it reads a .di file and produces another one after applying some set of passes.
Thanks for the response! The main reason I ask is that in my mind, the main advantage of hedgehog over QC is that you always provide generators as first-class values instead of through typeclass/newtype hacks; so instead of here all functions being made using a canonical typeclass, the generators might be passed as first-class values. But, I see now that the benefit hedgehog is offering in this specific library is the shrinking, which is definitely a pretty big deal!
Interestingly in latest versions of purescript they are removing row polymorphism in favor of IO monad in many standard libraries. Turns out riw polymorphism is more complexity than it worth in many cases
Thank you! I will give it a look.
Thank you! I will try to consider how to store the framework state
They are only moving from `Eff` - tracking granular effects to a catch all type. Row polymorphism was not removed. (You can still write `{ x :: Int, y :: Int | e }`)
It's #6 when restricted to the languages that have the embedding tag, not the #6 language for embedded work. Not sure I'm completely on beard with this particular ranking.
Thanks, that seems to be what I was looking for!
Do you happen to have any links to e.g. a GH issue or somethink where I can read more about this? I'd quite interested to read their reasoning.
Just check the type of \`return :: Monad m =&gt; a -&gt; m a\`, so the magic is in the \`return\` method of the \`Monad\` typeclass that lets you inject values into a context without knowing anything about that context.
my enlightenment, was the connection between 1) purely functional 2) types 3) and data when you have pure functions, you \*only\* have data going in and coming out and types all of a sudden become much more useful. It's a virtuous circle, makes things a lot simpler.
That worked a treat. Thank you very much!! Out of interest, how does the `"{Authorization}"` part work? I have never seen this before in Haskell.
e.g. for generating monotonic functions mentioned in the other comment :)
What's stopping GHC from providing them? And how well can you approximate then using OverloadedLabels, type-level maps and other existing tricks?
Idris, but it's also strict by default
Now I don't want to say their methodology might be a bit spurious, but they rank J above OCaml… J, an arcane 2-character glyph language reminiscent of APL, being more well-known and used than OCaml? I'd be surprised (and the ranking by Open Source corroborates this). This could just be that they [used search engines primarily to rank languages](https://spectrum.ieee.org/static/ieee-top-programming-languages-2018-methods), and "J Programming" gets a lot of "Java" and "Javascript" hits.
Great post! This is a fantastic reference of potential stumbling blocks for new Haskellers---something particularly helpful for anyone tutoring. Thanks!
Thank you!
I didn't realise you were the poster of this. If a language I created was one of the available options I'd probably pick it too. Jokes aside, I did like the article and thanks for your work on purescript.
This is the correct answer. Backwards compatibility has nothing to do with it. instance Functor F where fmap = liftM -- perhaps we should call this replaceM f &lt;$ m = m &gt;&gt; pure f instance Applicative F where pure x = ... liftA2 = liftM2 (*&gt;) = (&gt;&gt;) -- We could name this too m &lt;* n = m &gt;&gt;= \mr -&gt; mr &lt;$ n instance Monad F where m &gt;&gt;= f = ... m &gt;&gt; n = ...
&gt; the constructor of IO is not exported out of its module. Is this explained anywhere? I understand that IO a -&gt; a can't be (safely) done, but never quite intuited why.
Using a source plugin to do the analysis seems much more straightforward. Did you consider that?
#haskell on http://functionalprogramming.slack.com/ might be a good bet.
The reason that speaks for me is that it doesn't "make sense": * A function with type `IO a -&gt; a` could be seen as "getting an `a` out of an `IO` container", analogous to a `[a] -&gt; a` function (ignore empty lists for now). * If passed the same argument, it should always get the same answer (referential transparency). * Now tell me, what `String` does `getLine :: IO String` contain, such that your desired function would produce it. Remember, there must be exactly one.
I think Agda also has a JS backend.
If I were to guess, it's because `defer` is not a solution to the problem "I want to be able to write code and have it be lazy without having to think about it."
In this post, I demonstrate how use broadword programming techniques to exploit data-parallelism in the parsing of a CSV-like format. Doing so allows us to parse the text 8-bytes at a time instead of one byte at a time as a traditional parser would.
Great post!
Hiding constructors is a commonly used technique when you don't want your users to do 'unsafe' operations, and to force them to use a specialized interface. &gt; I understand that IO a -&gt; a can't be (safely) done, but never quite intuited why. This is how I see it, but I am not sure if I get your question completely. Say, `IO String` is the signature of a function that reads a String from the hard disk. Now, can you read that string without actually doing the IO operation that reads the string from the Hard disk. No, right. It is this idea, that is modeled by the IO type. So by hiding the constructor of IO, Haskell is encoding this real world behavior in the type system which is "You cannot get an String from an IO String without actually performing the I/O operation." along with the behavior that "If you are calling IO operations from your function then you become an IO function". Not all such arbitrary behavior can be encoded using the type system, but since Haskell's type system is some rich, we can model this specific behavior in its type system. Sum types, Product types, Hiding constructors (and may be more that I am not aware of) are all part of the vocabulary available to the programmer to model such real life behaviors/constraints, that programmer can count the type system to enforce. 
you can achieve the same thing without dummy values with a infix function suchThat = ($) example :: Reader [(a,b)] c example = takeAll keys `suchThat` (\val -&gt; val &gt; 5) 
So if I'm understanding correctly, the IO container is too ambiguous to guarantee referential transparency since such a function can receive `getLine` or `getContents`, etc., which must be handled differently? I'm assuming lists suffer similar ambiguity, and something like `fromMaybe` is doable because there is only `Nothing` or `Just a` to anticipate.
Nope, I was not aware about those, thanks for the heads up. I found the [ghc proposal](https://github.com/ghc-proposals/ghc-proposals/pull/107), the implementation ticket [https://ghc.haskell.org/trac/ghc/ticket/14709](https://ghc.haskell.org/trac/ghc/ticket/14709), and the commit [https://phabricator.haskell.org/D4342](https://phabricator.haskell.org/D4342). However, I failed to find any documentation about those. Would you have a resource to share? It's maybe too soon. If I understand correctly this will come with GHC 8.6.1 right? I'll probably move to this later on.
Oh, Markdown...
You can't catch exceptions outside of IO. `undefined` is the closest thing Haskell has to `null`, evaluating it is an immediate exception. Any value which may fail to exist is basically an immediate crash waiting to happen. So, since `IO a -&gt; a` can't ever prove that `a` exists, it's basically a logic bomb waiting to go off, and is verboten. There are more reasons, (and [technically](http://hackage.haskell.org/package/base-4.11.1.0/docs/System-IO-Unsafe.html)...), but this is a solid, fundamental, practical reason why not to try to evaluate `IO a -&gt; a`.
Can it? checkTrueIsFalse :: 'True ~ 'False =&gt; () checkTrueIsFalse = () To make ghc check that a proposition is true, you need to construct a program which will only type-check if the proposition is true. Unfortunately, your program instead checks: _if_ the proposition was true, would `()` have type `()`? But `()` has type `()` regardless of whether the proposition is true, so your program is not checking anything! Try this instead: -- typechecks checkTrueIsTrue :: ('True ~ 'True =&gt; ()) -&gt; () checkTrueIsTrue x = x -- error: Couldn't match type 'True with 'False checkTrueIsFalse' :: ('True ~ 'False =&gt; ()) -&gt; () checkTrueIsFalse' x = x No we ask if `('True ~ 'False =&gt; ())` and `()` are the same type. Now this only type checks if ghc can discharge the `'True ~ 'False` constraint. That's much better! Unfortunately, ghc is not a very powerful proof checker. For example, if you want to prove that `(||)` is associative, you'd have to check the results for `(True || True) || True ~ True || (True || True)`, for `(True || True) || False ~ True || (True || False)`, etc. If you only ask whether `(x || y) || z ~ x || (y || z)`, ghc won't bother checking all those possibilities, it will just say that the type `(x || y) || z` does not seem to match `x || (y || z)`, even though we know it does.
Can you do `fibs@(_ : tfibs) = 1 : 1 : defer (const zipWith (+) fibs tfibs)` to avoid the partiality noise? It's still not great. Any type (/ family / class) that has Data.Function.fix associated with it?
The `Lazy` class gives a `fix` function in terms of `defer`.
This is a [plugin](https://github.com/mpickering/hashtag-coerce) for detecting opportunities for `coerce`, which may serve nice reference.
Over on /r/haskellquestions, we sometimes get code review requests. Most of them get some answers, though some unfortunately get ignored. To minimise your chances of ending up in the latter category, provide complete code if possible (e.g. a project on Github that people can clone and build) but indicate which part the review should focus on. A generic "tell me what you think" is less likely to be successful than a specific "this part looks hideous, what can I do about it".
No, the point is that a function is a relation that associates an input to a single output according to some rule. Note the word “single”. A relation that associates an input to multiple possible outputs is by definition *not* a function. So, again, If we have `unIO :: IO a -&gt; a`, what single value should `unIO getLine` always return? Of course the question is silly, because we know that stdin can very between runs of our program, so there’s no one single correct answer. This proves, by contradiction, that `unIO` can’t be a function given the mathematical definition of “function”. (Granted `unsafePerformIO` is a thing, it’s just not a proper function, and the whole point of functional programming is to, well, program with functions — therefore such things should be avoided)
What is missing from Typescript for it to qualify as having row polymorphism? It has structural typing which is more or less the same thing.
Excellent point. Thank you.
It was really interesting. I did not quite get the example about GADTs though. The example with NothingString does not seem sensible to me so I don't understand the usage of this extension. Can someone give another example? Also, what's a type hole?
My naive reply would be whatever `String` `getLine` receives, but it sounds like that isn't the case. I suppose I wasn't following the nuance of a stdin from getLine not being the `String` part of `IO String` to be unwrapped from the `IO` container, because my expectation would be the same stdin should return the same `String`. `fromMaybe :: a -&gt; Maybe a -&gt; a` takes a default value to return if it gets `Nothing`. Perhaps `fromJust :: Maybe a -&gt; a` would have been a better example, which just throws an error if it receives `Nothing`. I should not consider these functions?
Can someone help me use thing function? `randomPrime :: (RandomGen g, Integral a, Random a) =&gt; g -&gt; a -&gt; a` This is what I'm doing but I continue getting errors when I tweak it. let gen = mkStdGen 214 let p = randomPrime gen (read "32") No instance for (Integral String) arising from a use of ‘randomPrime’ 
Try https://exercism.io/ -- The Haskell track has a growing list of mentors (and if you're experience, consider joining as a mentor).
I thought basing `Functor` or `Applicative` instances on `Monad` instances was discouraged anyway? At least I thought that's why we have `-Wnoncanonical-monad-instances`.
Your usage of `randomPrime` should type-check, given that there is enough information to resolve randomPrime gen (read "32") :: (Integral a, Random a, Read a) =&gt; a That is, `read` needs to know which type it should interpret `"32"` as. Adding a type signature, something like `read "32" :: Int` should be good. The type error you gave should only arise if you omitted the call to `read`, i.e. if you wrote `randomPrime gen "32"`. Or you omitted some context.
From what I remember of TypeScript, its type system is pretty good at expressing records, but it has a lot of gotchas when it comes to subtyping, so you'll end up with things of type `any` here and there. Also, it has something like extensible records, as you say, but not full row polymorphism, so you can't refer to a row in isolation, or use them to create something like polymorphic variants.
`fromJust` and `unsafePerformIO` do share some similarities.
&gt; My naive reply would be whatever String getLine receives, but it sounds like that isn't the case. That's not a value, that's the description of a process. We asked for a `String` and you gave us an `IO String`.
I completely agree with this. I used acid-state for a significant production application that was up for 5+ years. Based on that experience, I don't plan to use it again for roughly the reasons that /u/cdsmith summarized. I went with acid-state because I was seduced by the allure of transparent scaling. But now, almost 10 years later I've come to believe that is fool's gold. The number of organizations in the world that really need that level of scalability is quite small. The odds of you actually needing it are vanishingly low. And even if you had an oracle and knew you would need it and we did have a technology that could magically scale, you will have to pay more for it. When you're small that price may be too much to pay. In my case, hosting costs were 5-10x more because I chose acid-state and had to keep everything in RAM. If you're trying to bootstrap, that kind of cost difference matters. I suspect there's a fundamental law of the universe here somewhat similar to how fast things have to be small and large things are going to be unavoidably slow: scaling is unavoidably costly. And it's just not worth the cost until you know you need it.
Please post bug reports for Haskell projects on [the issue tracker for that project](https://github.com/haskoin/haskoin/issues), not here. Also, when you write that bug report, you will need to provide a much more detailed explanation of what did not work for you, as there is no way for the maintainers to guess what you mean by "it doesn't work". Presumably, it works just fine for them, so they can't just try and see.
I wish I felt qualified for this ... wrote a 1000+ line Yesod app but haven’t done much embedded stuff.
The only times I've heard about working conditions at Tesla, it was from the factory workers. Would working as a dev there result in an analogously hostile work environment? What are the working hours like? Do I run the risk of getting my salary cut if I become injured or ill?
I have some more material that is nearly ready to produce but keep running into bugs with ghc-8.6.1. Another reference can be the \`graphmod-plugin\` which reimplements \`graphmod\` using a source plugin. 
Im actually a awesome fit for that position, minus the Haskell that im currently learning.
For me the wrapper confusion part is really relatable. I was gonna type here why, but you summed it up perfectly, and also the consequences it has on getting familiar with more advanced constructs.
fromMaybe is of course a function, just not a total one. A function must associate at most one value to each it consumes, but it needn't consume every value.
fromJust is a function, just not total. unsafePerformIO associates a variety of values to the value it consumes: it's not a function.
RAL?
`do` can be used to wrap any expression Prelude&gt; 1 * do 2 + 3 5 With [BlockArguments](https://ghc.haskell.org/trac/ghc/ticket/10843) you will be able to do the while cond do whatever example right out of the box. ;) You still need to use this trick for the rest, though. ;)
Glad to see Haskell being used at Tesla!
Both make your logic inconsistent.
Would also have to ask yourself how you feel about being treated decently when others around you are not.
Workers of the world unite!
Probably not. For a highly-skilled position, you have a lot of negotiating power. A better question is whether the company fanning you with palm leaves is enough to distract you from how they treat the labor that lacks your negotiating power. Still, from the other side, as far as I know Tesla is still not profitable, and as such, it’s not correct to imagine the choice as “treat the workers well or crush them under the boot of capitalism to make the rich richer” (and there definitely are corporations that fit the latter description); currently, it’s more like “overwork the workers or it’s over and there is no Tesla.” Anyway, there are plenty of corporations I have a strong moral opposition to, but Tesla isn’t really one of them. You could certainly do a lot worse as a Haskeller.
I'd rather use newtypes around the arguments I want to label. 
Do you sponsor H1-Bs? 
&gt;Anyway, there are plenty of corporations I have a strong moral opposition to [..] Like Starbucks? ;) 
In Agda, you can define a mixfix operator, e.g. ``` if_then_else_ : {A : Set} -&gt; Bool -&gt; A -&gt; A -&gt; A if true then t else f = t if false then t else f = f ``` or `while_do_` likewise. There are also [syntax declarations](https://agda.readthedocs.io/en/latest/language/syntax-declarations.html) which allow to define custom binding structures.
Do you think software engineers literally work in the factories?
I think it'd confuse me as to what `takeAll` and `suchThat` do – I'd probably incorrectly guess that they're something like parser combinators. Having keyword arguments in Haskell would be nice, but I think that emulating them in this way actually makes it less readable :/ P.S. I thought about doing something like this too at some point ;)
Probably worth applying anyway. The preferred qualifications are a bit of a stretch— *we want you to have written Haskell* *...and JavaScript/HTML* *...and embedded C programs* *...and have worked with build systems* A candidate that has substantive experience in every one of these areas is going to be exceptionally rare and will probably cost more money than they are willing to pay.
For a toy-sized, but still motivating example of GADTs, I'm going to go with the classic [length-indexed lists](https://pastebin.com/E76nm5Bk). I hope it helps display the gist that a) GADTs let you tag types with more information that can be used to check more properties and b) they tend to bring substantial amounts of type-level programming, so some moderation is advised.
For a toy-sized, but still motivating example of GADTs, I'm going to go with the classic [length-indexed lists](https://paste.ee/p/6AU26). I hope they help demonstrate the gist that a) GADTs let you tag types with more information to check more properties with and b) they tend to bring a good amount of type-level programming, so some moderation is advised.
I am sorry, but I do not care enough to go look for a link. Some guidelines the moderators intrusively edit questions along may even be informal.
I've looked at Ur/Web and I really want to like it but I just can't get over the whole writing XML inside of the language thing.
&gt; The preferred qualifications are a bit of a stretch Not really that far fetched. Experience with Javascript, HTML, and Build systems isn't exactly rare. I'd wager the vast majority of developers fall into that category. Embedded C - well, they're presumably writing software for cars, so that's a no brainer. This is a relatively rare skill, but it's not that obscure. Haskell - Well, that's why we're all here. 
I mean.... They don't say how -much- experience of any one of those things. The way I read it, they want an intelligent and proficient polyglot, but not necessarily a god-tier domain expert in all of the above. I'd qualify for 3/4 on that as written, and I'm still mostly just a hobbyist who occasionally convinces his boss to let him write some code.
Hooray for `BlockArguments`
This is really cool! It's nice to see plugins getting more use recently
Individually you will find plenty of people with two or even three of the bullet points. But I'm saying that substantive experience in *all four* is going to be rare. Embedded C and frontend JavaScript are diametrically opposed to one another. Someone who has professional experience writing code that deals with things like event bubbling and closures probably hasn't professionally written any embedded code. This is all not to mention also having written Haskell. And no, I'm not counting your weekend Arduino project that makes LEDs blink as embedded experience.
I've only been at Tesla a few months and I can only speak for my immediate team (which is not the team which is recruiting here, I didn't know a Haskell team at Tesla until after I got my offer). That said, the working conditions have been quite good. I (and most of the team) put in not much over 40 (and there has been no pressure on working hours at all), and people are out working from home when sick with no fuss. 
Re: Tesla hiring college grads and working them hard for poor pay: That may be old information, or only specific teams. My team is definitely looking to hire senior people, and my offer was competitive with my Google/Facebook offers. 
The problem is the devs said they'd be producing 5000 lines of code per month by last December and they're still struggling to put out 1000 per month. They are thinking of taking their AI repository private so they are under less scrutiny to produce software.
If PureScript is not a Haskell dialect then what is?
Not bad. I'm quite fond of this one personally: https://www.youtube.com/watch?v=1S1fISh-pag
I've definitely done this before, but only in hobby projects a long time ago. It's not unique to haskell though, I picked it up from lisp where it's been around probably longer than I have. 
How does one apply?
Nifty. What's the integration story for plugins with cabal-install and stack? I see in the GHC docs that `-fplugin` takes a module name that's a member of a registered package, so does it suffice to just put `build-depends: graphmod-plugin` (and `extra-deps: [graphmod-plugin]` for stack) and then away we go?
&gt; build systems If nothing, there's Shake. A lot of people here have at least played with it, not to mention those running static sites generated by a Shake builder.
I'd say if you have written a 1000+ line program in C, you probably qualify. 
Thanks Saurabh.
Thank you!
Thank you!
It works on new Reddit, but not mobile or old Reddit. Worse yet, I think the new Reddit code button actually uses fences...
&gt; people are out working from home when sick with no fuss. So... no sick leave?
Car factories are unfortunately dangerous places to work. Earlier this year, a worker was [killed](https://www.usatoday.com/story/money/cars/2018/04/03/bmw-auto-worker-dies-head-trauma-s-c-plant-accident/483827002/) at a BMW factory in South Carolina. Tesla gets a lot of attention and scrutiny over pretty much everything, including factory safety, but it’s [not clear](https://www.bloomberg.com/news/articles/2018-06-06/musk-s-tesla-injury-claim-left-work-safety-experts-wanting-more) that Tesla’s factory is any less safe than other factories. In fact, Tesla claims it’s doing better than average. &gt; In June, Musk said Tesla’s 2018 injury rates so far were 6 percent below the average, even as Model 3 production increased. Source: [Bloomberg](https://www.bloomberg.com/news/features/2018-07-12/how-tesla-s-model-3-became-elon-musk-s-version-of-hell). History helps explain why Tesla workers haven’t unionized. The Tesla Fremont factory was formerly the NUMMI factory, a Toyota/GM joint venture. NUMMI’s workers were unionized by the United Auto Workers (UAW), which gained a terrible reputation among auto workers in the Fremont area following the closure of the NUMMI plant. &gt; Anger among workers at the New United Motor Manufacturing Inc. (NUMMI) plant in Fremont, California, toward the United Auto Workers (UAW) exploded at a January 24 meeting discussing the imminent closure of the facility. Nearly 5,000 workers will lose their jobs when the plant, formerly a joint venture of General Motors and Toyota, closes on March 31. Four hundred or so workers were present at the meeting Several attendees captured the eruption on video, which began during comments by UAW Local 2244 Bargaining Chairman Javier Contreras. Contreras was booed, jeered, and interrupted as he attempted to present details of the severance package. At one point an outraged older worker demanded to know “where the hell” the union official had been for the last six months. Contreras burst out, “Shut the f— up, you motherf——!” At that point, furious workers rushed to the front of the room. Contreras and other local UAW personnel were defended by a few union officials. Local union leaders pleaded for calm and called in the police in a bid to control the workers. &gt; &gt;Workers present say that the yelling began because union officials would not allow them to speak. Workers are angry that they have been kept in the dark over UAW negotiations with NUMMI. The video footage reveals that the UAW, on the one hand, and rank-and-file auto workers, on the other, make up two mutually hostile camps. The workers bristle with mistrust and contempt for the union; the UAW officials are defensive and thuglike. The episode exposes the UAW’s role in executing the layoff and wage cut dictates of business—as well as their unmistakable contempt for the workers they nominally represent. Source: [World Socialist Web Site](https://www.wsws.org/en/articles/2010/02/numm-f06.html). For this reason, workers are unlikely to want to join the UAW. The UAW has been unsuccessful so far, despite [spending money](https://www.detroitnews.com/story/opinion/2018/04/23/opinion-uaw-part-anti-tesla-campaign/34178345/) on an effort to unionize Tesla workers. Labour laws prevent management from interfering with workers’ efforts to unionize. There is an [ongoing trial](https://www.bloomberg.com/news/articles/2018-06-11/tesla-accuses-uaw-of-orchestrating-an-anti-elon-musk-infomercial) involving Tesla and the UAW to determine if there was any interference. 
New Reddit truly is a disaster... 😒
&gt; In fact, Tesla claims it’s doing better than average. Why would you quote that and not the claim it aims to counter which is directly above in the article? &gt; A 2017 analysis by Worksafe Inc., a nonprofit, said that serious injuries at Tesla’s plant in 2015 and 2016 were well above industry averages. This seems pretty one-sided...
I'm not 100% sure what you mean by transparent scaling here? In the sense that for acid-state to scale all it needs is more RAM? I'm interested because you seem to have judged acid-state to have a good story for scaling, whereas my main worry about using it has always precisely been that it doesn't scale (as opposed to mainstream databases, where there are established techniques for sharding/replication etc). Also I'm not sure how scaling costs relate to the issue raised by /u/cdsmith. Cdsmith's basic point was that data in acid-state was not accessible outside acid-state. As covered in my reply, this limitation does not necessarily apply to acid-world. I also don't entirely buy the cost argument - I think this is highly dependent on individual circumstances. If acid-state were a little more user friendly then the developer time savings of using it, as opposed to an external database (with all the marshalling, potential security issues, lack of compile time guarantees, schema management etc etc), may well outweigh any additional hosting costs. Projects like beam may mitigate that of course.
Well, perfect timing ;)
Ooh, nice! Hm, I probably don't want to add it as an implicit extension. But since extensions can be specified on a per-worksheet basis, what if it is simply made a default entry in every new worksheet that is being created? It would clutter the interface for novices, but that could be somewhat ameliorated by a documentation search for language extensions. What do you think?
`esqueleto` does a simpler version of this to emulate the sytnax of [case expressions](http://hackage.haskell.org/package/esqueleto-2.5.3/docs/Database-Esqueleto.html#v:case_): case_ [ when_ &lt;condition&gt; then_ &lt;value&gt; , ... ] (else_ &lt;alternative&gt;) `when_` has three arguments. The second argument is of type unit, and `then_` is an alias for `()`
Thanks a lot for the details. I added a note about this in the article.
From a quick glance, I just want to note that there is a [complex number type](http://hackage.haskell.org/package/base-4.11.1.0/docs/Data-Complex.html) in `base`.
You need to also pass additional `ghc-options` such as ``` ghc-options: -fplugin=GraphMod ``` and also run the finaliser yourself if you use cabal. I don't know about stack but it's probably similar. There are some plugins which you always have to run as they affect the output of your program, so enabling these in the explicit fashion is a good idea. Running one-off plugins like `graphmod-plugin` though is less convenient which is why I use nix to do it. 
Can you say more about how this would work? I can see that in principle one could store the events/checkpoints in database tables rather than files, but that doesn't quite correspond to what you have here, in particular for checkpoints. How do you turn an acid-world transaction (i.e. a Haskell function that modifies the User segment) into an atomic database update?
It's fair to say that `acid-state` development has been pretty limited, I think due to a combination of an understandable lack of maintainer time, nervousness around making changes in the absence of a good testsuite, and not wanting to make radical changes to something that is working successfully in production. I'm trying to help with the maintainership load and working on better testing, but time is ever limited... There is definitely a trade-off here between making incremental improvements to `acid-state`, with a reasonable migration pathway for existing users, versus starting a brand new library that can more radically change things and doesn't have to care about backwards compatibility or reliability. I can see the appeal of the latter (it's certainly more fun!) but I suspect it will take quite a lot of work to turn the proof of concept into a robust production-ready library. It'd be great to think further and talk about how many of these improvements can be folded into `acid-state`. In any case, thanks for driving this forward and stimulating interest in (how to improve upon) `acid-state`! On the specific point about the use of `Symbol`, you make a good case for using it in combination with compile-time uniqueness checking. I'm somewhat sceptical about the chances of truly independent libraries wanting to provide segments/events that need to be used together, so I suspect your approach may well be fine. (I think that `Typeable` will let you get unique identifiers (package + module + type name, or possibly fingerprint) but the data migration story would need some thought.)
I should also add that the sheer speed of a solution like acid-state helps a lot as well. To achieve the same sort of performance against a database (in my experience anyway) requires some sort of caching mechanism, which then starts to add to RAM requirements and adds yet another fragile and side-effecting layer to your program.
`else_` is also `?:` from `errors`.
Is the source for this a private communication? Otherwise I'd like to see it.
Ghcjs is too painful to install,so I switched to idris..
Agreed on the trade-off. I was originally thinking of just starting some issue threads on the `acid-state` repo, but I figured that, without some code to back them up, the response would just be "Yeah, that would be nice, but no-one is going to do it..." So I started looking at how to, eg, support multiple state segments in `acid-state`, as the basis for a PR, and realised that it was impossible without a major, backwards incompatible rewrite. The question then became - would that be a good thing? And at the moment I would say not. It would be possible to write a migration between `acid-state` and an `acid-world` style event/segment model, but it would be a considerable amount of work to make it bulletproof enough to force it on users at some point as they moved to the next version of `acid-state` (and would definitely require some work on the part of users). Additionally, I think that taking `acid-world` to production readiness is considerably less work than adding `acid-world` features to `acid-state` along with some semblance of backwards compatibility. The reason for this is precisely the freedom that a new package gives. And I'm struggling to think of an aspect of `acid-state` that can't either be directly lifted into `acid-world`, or improved on, given the compiler features we have to work with in 2018 as opposed to 2011, and the feedback from years of user experience with `acid-state`. And to be honest, there are aspects of `acid-state` even now that in a new library would cause it to be considered not `production ready` - lack of tests, unrecoverable partial writes, memory usage on state restoration, poor/non-existent error messages ("expected bytes" is not very helpful when it refers to a failure somewhere in 1GB file that is unreadable except by your own program) and error handling in the update monad are probably the biggest I'm aware of. If there was agreement to a major rewrite of `acid-state` to (my personal minimum features) support multiple segments and remove the need for a centralised `makeAcidic` then that might be a way forward, but given such a substantial rewrite I'd still question whether it would be quicker to just start from scratch. "I'm somewhat sceptical about the chances of truly independent libraries wanting to provide segments/events that need to be used together". Yeah, same here! But it is hard to predict that sort of thing - as far as I know there isn't an easy way to do this in Haskell at all at the moment. I didn't actually set out specifically to provide that functionality actually, it is just a natural result of the way segments and events had to be structured for it to work at all. Good point about `Typeable` - we can't have it that if someone moves the definition of a type from one module to another then it breaks old state. So I would lean towards `Symbol` - if segment and event renaming is fairly easy to do then there is always an option to work around the issue there, and in any case I think a responsible package author would simply prefix their chosen `Symbol` names appropriately.
No thanks Even disregarding this ridiculous job posting ("Has written a 1000+ line program in Haskell" ???) I would never work for such a company. 
Does your team is a good opprotunity for remote/contractor?
Well firstly I will say that it is entirely possible that my implementation is flawed! The database backend is very much a POC - I wanted to include it just to see whether it would be possible, on a type level, to support it. Anyway, the current (rough and ready) implementation is: 1) An events table. This stores events as they are issued. Each update is a database transaction. There is no logical difference here between appending to a file and adding a row to a table as far as I can tell? 2) Checkpoint tables - each segment has a table. To use the db backend you have to define how each of your segment types translates to a db table. This obviously doesn't work very nicely for some types, but that equally applies to any persistence strategy that is backed by a database. 3) When a checkpoint occurs, all of the state tables are truncated (see below) and then the current state is inserted into them. Any events in the current events table are marked as "Archived" or similar. 4) When state is restored, firstly the checkpoint tables are read back into their segments, and then any events that are not marked "Archived" are run against that state - the same as with the file system backend. Of course this can be substantially improved on, but all I was interested in was whether it was feasible - I'm not sure how much demand there actually is for a db backend like this and I didn't want to spend much time on something that may never be used. To support checkpoints properly you could tag events as they come in with the last checkpoint id, and then when a checkpoint was created it would use a new checkpoint id. That way you would end up being able to restore any specific checkpoint, and know which events came after it. Does this sound reasonable, or am I missing something?
What precedence makes this work right with chained conditions? Like: `if_ a `then_` b `else_` if_ c `then_` d `else_` e
You're welcome! I'm glad it worked. I'm not a Servant poweruser either, and I've learned something from your solution, so I wanted to give back and dug into the source code to see of how `HeaderArg` was handled [here](http://hackage.haskell.org/package/servant-js-0.9.3.2/docs/src/Servant-JS-Internal.html), check the definition of `toJSHeader`. There I've discovered `ReplaceHeaderArg` does what you need. You can see the `{XYZ}` part is handled in an ad-hoc manner.
I'm not actually sure? I've been lucky enough to not get sick yet :). I think the answer is that no, Tesla does not officially have sick leave, but in practice most teams are reasonable about you staying home when you are sick and getting what work you can done. 
My team is all based in Palo Alto, and I don't think we are open to fully remote workers at the moment. You can check our jobs page, we have a number of offices around the world: r/https://www.tesla.com/careers/search#/?department=1, though I think software engineering is mostly focused in the Bay Area.
I search for some examples of [row polymorphism](http://www.typescriptlang.org/play/index.html#src=%2F%2F%20https%3A%2F%2Fbrianmckenna.org%2Fblog%2Frow_polymorphism_isnt_subtyping%0D%0Afunction%20f%3Crho%20extends%20%7B%20a%3A%20number%2C%20b%3A%20number%20%7D%3E(x%3A%20rho)%20%7B%0D%0A%20%20%20%20return%20Object.assign(x%2C%20%7B%20sum%3A%20x.a%20%2B%20x.b%20%7D)%0D%0A%7D%0D%0Aconst%20answer%20%3D%20f(%7B%20a%3A%201%2C%20b%3A%202%2C%20c%3A%20100%20%7D)) and [polymorphic variants](http://www.typescriptlang.org/play/index.html#src=%2F%2F%20https%3A%2F%2Fv1.realworldocaml.org%2Fv1%2Fen%2Fhtml%2Fvariants.html%0D%0Atype%20Basic%20%3D%20string%0D%0Atype%20RGB%20%3D%20%7B%20r%3A%20number%2C%20g%3A%20number%2C%20b%3A%20number%20%7D%0D%0Atype%20Grey%20%3D%20%7B%20i%3A%20number%20%7D%0D%0Atype%20Color%20%3D%20%7B%20basic%3A%20Basic%20%7D%20%7C%20%7B%20rgb%3A%20RGB%20%7D%20%7C%20%7B%20grey%3A%20Grey%20%7D%20%0D%0Afunction%20color_to_int(color%3A%20Color)%20%7B%0D%0A%20%20%20%20if%20('basic'%20in%20color)%20return%200%0D%0A%20%20%20%20if%20('rgb'%20in%20color)%20return%2016%20%2B%20color.rgb.b%20%2B%20color.rgb.g%20*%206%20%2B%20color.rgb.r%20*%2036%0D%0A%20%20%20%20if%20('grey'%20in%20color)%20return%20232%20%2B%20color.grey.i%0D%0A%7D%0D%0Atype%20RGBA%20%3D%20%7B%20r%3A%20number%2C%20g%3A%20number%2C%20b%3A%20number%2C%20a%3A%20number%20%7D%0D%0Afunction%20extended_color_to_int(color%3A%20%7B%20rgba%3A%20RGBA%20%7D%20%7C%20Color)%20%7B%0D%0A%20%20%20%20if%20('rgba'%20in%20color)%20return%20256%20%2B%20color.rgba.a%20%2B%20color.rgba.b%20*%206%20%2B%20color.rgba.g%20*%2036%20%2B%20color.rgba.r%20*%20216%0D%0A%20%20%20%20else%20return%20color_to_int(color)%20%0D%0A%7D) and they seem to work fine in Typescript. 
https://github.com/yav/graphmod/wiki
Well. I tried to apply anyway, and clearly noted in cover that I like to be a remote and other bla-bla ;) Let's look whats happens (but most probably "nothing", lol)
http://hackage.haskell.org/package/ghc-heap-view
I think this is a pure UX question. If your goal is making a primarily interactive environment, I think that ExtendedDefaultRules is vital to interactive Haskell ergonomics. If your goals include matching GHCi behavior, and/or making things as simple as possible for new people, offering an option to disable the implicit extension seems like it makes the most sense, as that both improves signal to noise for new folks, and more closely matches GHCi's behavior. If your goals instead focus on making a more complete Haskell development environment that happens to also include interactivity as a bonus, then this is primarily a question of documenting ExtendedDefaultRules as an option, whether or not you default it to showing up. Any of the available options seem like valid directions for this project. Live results implies interactivity as a focus but doesn't necessitate it. As an example, I have used plenty of editors with tooling extensions that do fine without ExtendedDefaultRules, and I am not mystified or frustrated (welll..... most of the time), as I know I'm interacting with GHC and so I expect GHC behaviors. It just depends on what set of expectations you try to cultivate and reward with your project. Also, since I didn't say it earlier - Nice work! This seems like a cool project that has a lot of potential.
It's even in R5RS Scheme's minimalist `syntax-rules` macro system.
Along with the idiom bracket example, you can look at the `HList` package, which provides similar "syntax" for heterogeneous lists.
&gt; Currently, GHC 7.4 through 7.10 should be supported Is out of date, but ideed 8.0 and 8.2 are the newest supported versions.
Monad transformers are a pain in the arse for newcomers, but a fairly lightweight intermediate subject. The same for lenses, but for very different reasons. My advice: Put off dealing with Mondad Transformers until after you understand why the constructor of `IO` is sealed and agree with the design decision, and until you've built at least a few programs that use say, `Maybe` or `Either` inside of `IO` successfully and what the pain points of that experience were. If you can to the `Maybe`/`Either` excerise without using do notation at all, you're definitely ready for Monad Transformers. Lenses are just a totally different beast. Basically, build some nested getters and setters for record objects first. Understand what typechecks and what doesn't, and why. Then go tackle lenses, BUT, beware of how deeply into their implementation logic you try to go, and don't feel like you have to master all of `lens` at any point. Really, getting intimidated by the sheer SCOPE of what a lens can do is the problem there, not the typical uses of lenses themselves in day-to-day code, which is fairly easy to understand.
What's the best way to get my project on next week's issue?
Here's an interesting take on the subject: [Generic Functional Parallel Algorithms: Scan and FFT](http://conal.net/papers/generic-parallel-functional/generic-parallel-functional.pdf)
That sounds like `git blame`
It's crucial to see that `getLine` is one particular value. Every time you execute your program, `getLine` is always the same value as the last time. What *isn't* the same each time is the line from `stdin` that you would get if you were to carry out the process that `getLine` describes. That might seem a like a contradiction (i.e. if `getLine` is a constant, how can it give different results?), but it most certainly is not. Here's a thought experiment: Suppose you tell me "go fetch my mail from my mailbox, please". So I go and fetch your mail, and as it turns out, there's a letter from Bob. A week later you say again "go fetch my mail from my mailbox, please". This time I return to you with a letter from Alice. In both cases you've dictated precisely the same process -- if you were to express both utterances in code as a string, you could compare both and see that they are precisely equal. However, despite having spoken the same thing twice, the *execution* of those commands resulted in two very different outcomes. So it is with `IO`. Each time you refer to, for example, `getLine`, it's the exact same element from the set of all possible `IO`s, regardless of what happens to be in `stdin`. When you say: &gt; [...] the String part of IO String to be unwrapped from the IO container [...] that implies, wrongly, that some expression `expr :: IO String` has some `String` value embedded therein, which would mean that `expr` as a whole can vary in value. This is false. The source of your confusion probably stems from thinking of the `IO` monad as some type of container. The metaphor can certainly work for some monads (for example, [`Identity`](http://hackage.haskell.org/package/base-4.11.0.0/docs/Data-Functor-Identity.html) is a perfect example), but it will trip you up for others. A better metaphor might be: the IO monad describes a process (in the plain English sense of process), and those descriptions can be chained together ([`&gt;&gt;`](http://hackage.haskell.org/package/base-4.11.1.0/docs/Prelude.html#v:-62--62-)), and we can additionally compose process descriptions by saying that one process can depend on the result of carrying out another described process ([`&gt;&gt;=`](http://hackage.haskell.org/package/base-4.11.1.0/docs/Prelude.html#v:-62--62--61-)). To add to that very last point, here's an example: step1 = (checkIfIAmHungry :: IO Bool) step2 = \hungry -&gt; if hungry then (eatFood :: IO ()) else return () `step1` is a description of the process of determining if I am hungry, and `step2` describes what I should do in the event I am hungry (or not). We can compose the two so that I don't let myself starve: `step1 &gt;&gt;= step2`. Now, it's important to see that nowhere in the expression `step1 &gt;&gt;= step2` have we actually *done* anything -- it's purely a *description* of how something might be done. So `IO` values can't be "unwrapped" to get at a value any more than I can "unwrap" your utterance "go fetch my mail from my mailbox, please". In the latter case, though, what I *could* do is *execute* your demand, walk over to the mailbox, grab your mail, and then hand it back to you; in Haskell (barring `unsafePerformIO` et al), you actually *cannot* directly do the analogous thing and execute your `IO` values -- the only thing you get to do is build up a big IO *describing what you would like done* and name it `main`. When you compile and execute your program, the runtime then does it thing and *finally* executes that big description.
Rhine sounds really interesting. Looking forward to reading the paper.
It’s 404ing for me
Very interesting. Seems like I’m not the only one considering something like this then...
Have you tried just evaluating the variable (is without the `:print`)?
You might want to look at Selda. I used it a lot lately and I'm very happy with it.
http://hackage.haskell.org/package/QuickCheck-2.11.3/docs/Test-QuickCheck-Gen.html#v:suchThat (Positive n) `suchThat` (&gt; length ns)` ? OR http://hackage.haskell.org/package/QuickCheck-2.11.3/docs/Test-QuickCheck-Gen.html#v:elements `elements (take (length ns) [0..])`
 propCheck ns (Positive n) = nthSmallest ns n == sort ns !! (n `rem` length ns) The division will surely take less time than the `sort`.
I personally like "-Wmissing-import-lists", but it would be nice if it only applied to imports from dependencies.
Thanks. Btit i found it quite advanced to have a grasp.
It just 200-ed for me.
Thanks for the recommendation! For those unfamiliar, [`selda` is on hackage](https://hackage.haskell.org/package/selda) and has [an official tutorial](https://selda.link/tutorial/)
Please post bug reports for Haskell projects on [the issue tracker for that project](https://github.com/gleachkr/Carnap/issues), not here.
This GitHub project provides a framework for implementing a number of different logics (propositional, predicate, modal, etc) and calculi and parsers for working with them. Part of it is a Yesod project, but I'm just trying to compile the core part which implements the data types and parsers for the logics (in the directory "Carnap"). I'd like to use a more recent version of ghc than 7.10.3, and later stackage releases than lts-6.30, but compilation doesn't seem to terminate using more recent versions of ghc -- the compiler seems to just use more and more memory without bound, and eventually brings my machine to a standstill. I'd be interested to know if anyone can diagnose the problem... 
I've posted a clarification. I'm not sure whether this is a bug with the project or with ghc, but I thought it might be of interest to people - and I wondered if people might have tips on identifying *where* the non-termination arises. 
I wish you could disable it on a per-import basis. Actually I wish you could disable warnings with more granularity than entire modules. 
It was being used by Tweag (who's also the maintainer) when I was working there. I don't know if it's still being used. I don't think any of the projects it was being used in are going to be open-source ever, however.
Yes, but evaluating the variable my still result in a closure.
GHC has a pretty good convention for this IME: https://ghc.haskell.org/trac/ghc/wiki/Commentary/CodingStyle#Commentsinthesourcecode
Also for unused imports. Since there are new things constantly going into Prelude, it seems like you can't support &gt;1 ghc version without tons of unused import warnings.
Surprises me there's nothing on name shadowing here. Does Haskell really force a warning on variable name shadowing with no way to turn it off? If so, it's kinda funny Haskell is so opinionated here, because there are other languages which take quite the opposite stance: Agda is perfectly fine with name shadowing.
The article does not list all of GHC's warning flags. It mainly concerns itself with the difference between *-Wall* and *-Weverything*. You may be interested in *-Wno-warn-name-shadowing*.
Ah, thanks! Although tbh, as far as I understand, the reason shadow naming is less kosher in Haskell is because everything is mutually defined/recursive by default, whereas in Agda, the termination checker ensures that this is not a problem. This makes it easy in Haskell to accidentally use shadow declarations to make a loop when you didn't intend to, whereas in Agda that's not a make-able mistake.
No worries! I think your explanation makes some sense. I never thought very much about the stance on name shadowing in Haskell. Coming from Common Lisp, it was a bit annoying, but I quickly accepted that "this is the Haskell way" and moved on.
Just in time! Now [`summoner`](https://github.com/kowainik/summoner) adds most of those warnings by default to generated projects :) * https://github.com/kowainik/summoner/pull/116
Sure :) For building *just* the package which does logic implementation, I was able to build straight from the github repo (which uses lts-6.30), and which I forked for convenience. Try: $ git clone git@github.com:phlummox/Carnap.git $ stack build Carnap Building the other components is a bit of a pain, as they require ghcjs, which tends to be very finnicky -- and the config expects you to have postgres, which I have no desire to install. But I got everything working in a Docker image, which you're welcome to use: [github repo](https://github.com/phlummox/carnap-docker-B) and [Docker hub page](https://hub.docker.com/r/phlummox/carnap-docker-b/). Bizarrely, I seem to have traced my problem with the logic component, at least, down to whether one function signature is added or not. In the commit I'm looking at, [622d9fab][https://github.com/phlummox/Carnap/commit/622d9fabd62d58053b5a24c7a9dc7e4723017c94], if Carnap.Calculi.NaturalDeduction.Syntax.isAssumptionLine is given a signature, everything works fine; if it's left off, all recent versions of ghc seem to suck up my computer's entire memory &amp; then start chewing on swap space. 
I'm interested in debuging where your termination issues stem from. So, if I understand this right, everything's fine with `lts-6`/GHC 7.10.3? When you say &gt; Bizarrely, I seem to have traced my problem with the logic component, at least, down to whether one function signature is added or not. In the commit I'm looking at, 622d9fab, if Carnap.Calculi.NaturalDeduction.Syntax.isAssumptionLine is given a signature, everything works fine; if it's left off, all recent versions of ghc seem to suck up my computer's entire memory &amp; then start chewing on swap space. What compiler are you compiling with? Is it a GHCJS or plain GHC component?
As far as I can tell, that shows module imports, not things imported from modules? i.e. it shows that Control.Monad is getting imported, but I don't see a mode which says that `void` is being imported from it.
Just plain ghc. Yup, works fine with lts-6/GHC 7.10.3; if I switch to anything later (e.g. lts-7.0/GHC 8.0.1), it doesn't terminate.
First a nitpick: runtime representation is not defined in the language standard, but depends on implementation. The following applies to GHC. Both \`Dummy\` and \`Options\` are lifted and therefore represented by pointers to structs. The struct contains a pointer to an info-table followed by payload. For more details, see [https://ghc.haskell.org/trac/ghc/wiki/Commentary/Rts/Storage/HeapObjects](https://ghc.haskell.org/trac/ghc/wiki/Commentary/Rts/Storage/HeapObjects)
`graphmod` recently was implemented using upcoming feature from GHC 8.6.1 — Source Plugins. * https://mpickering.github.io/posts/2018-08-09-source-plugin-graphmod.html Maybe you can open issue to the author of this library and discuss possible opportunities there?
Thinking about this more, the only serialiser that actually needs this is `SafeCopy`. `JSON` can be parsed directly and `CBOR` has a [bijection](http://hackage.haskell.org/package/cborg-json) with `JSON`, or can be read with another `CBOR` implementation. My feeling here is that the best approach would be to write a 'SafeCopy' like wrapper for `CBOR` and then use that as the 'standard' approach to serialisation. `CBOR` does support migration internally, but I like the compiler guarantees that `SafeCopy` gives. I don't think this would be particularly difficult to do. And if anyone doesn't care about generic reading of state then they are free to use `SafeCopy` directly.
You guys have pretty much convinced me to learn Haskell. I think I will start today. As a java dev with C++ background I've been drawn to Haskell for a while now. I just often find myself using the more "functional" side of java and the whole concept just seems so clean.
I like the idea of this and it should be fairly straightforward to provide a serialiser that uses it. But I don't think it is really an alternative to something like `SafeCopy`. You can only access `Since` and `Until` fields if you are accessing a `Rec v` with a concrete `v`, like a `Rec V2` (unless I'm missing something?). So presumably when you make a change to a field you have to change every single type reference you have from `Rec V2` to `Rec V3`. In a reasonably sized code base that means updating hundreds of lines of code. Sure, you can find and replace across your files or whatever, but that isn't ideal. The nice thing about `SafeCopy` is that it compartmentalises migration into precisely that area of your program that actually needs to know about it. `versioning` might work very nicely though in contexts where you actually want to support multiple versions externally (e.g. with versioned APIs) so it would be good to support it for use cases like that.
&gt; People usually write functions in curried form so uncurry is more common. And I expect the vast majority of `curry` calls are `uncurry | do some decorations which really want a 1-argument function | curry`.
For some configuration reasons, I can't try GHC 8.0.1. But using lts-9.35 with GHC 8.0.2, I get a type error in Carnap.Core.Examples.ToyLanguages. Using lts-12.5 with GHC 8.4.3, I get the same error. So, my guess is that GHC 7.10.3 was too liberal with some programs (or some language extensions changed) and that this led to a termination bug in GHC 8.0.1 which was fixed in GHC 8.0.2 by properly rejecting the program. Here are the type errors spit out by GHC 8.4.3 for completeness: /data1/graf/code/hs/Carnap/Carnap/src/Carnap/Core/Examples/ToyLanguages.hs:108:35: error: * Couldn't match type `t' with `a' `t' is a rigid type variable bound by a pattern with pattern synonym: :!$: :: forall (b :: (* -&gt; *) -&gt; * -&gt; *) idx. () =&gt; forall t. (Typeable t, Typeable idx) =&gt; Fix (Copula :|: b) (t -&gt; idx) -&gt; Fix (Copula :|: b) t -&gt; Fix (Copula :|: b) idx, in a pattern synonym declaration at src/Carnap/Core/Examples/ToyLanguages.hs:108:28-35 `a' is a rigid type variable bound by the signature for pattern synonym `:!!$:' at src/Carnap/Core/Examples/ToyLanguages.hs:107:20-101 Expected type: ToyLanguage (a -&gt; b) Actual type: Fix (Copula :|: (Predicate BasicProp :|: (Connective BasicConn :|: (Binders BasicQuant :|: (Function BasicTerm :|: EndLang))))) (t -&gt; b) * In the declaration for pattern synonym `:!!$:' * Relevant bindings include y :: Fix (Copula :|: (Predicate BasicProp :|: (Connective BasicConn :|: (Binders BasicQuant :|: (Function BasicTerm :|: EndLang))))) t (bound at src/Carnap/Core/Examples/ToyLanguages.hs:108:35) f :: Fix (Copula :|: (Predicate BasicProp :|: (Connective BasicConn :|: (Binders BasicQuant :|: (Function BasicTerm :|: EndLang))))) (t -&gt; b) (bound at src/Carnap/Core/Examples/ToyLanguages.hs:108:28) | 108 | pattern (:!!$:) f y = f :!$: y | ^ /data1/graf/code/hs/Carnap/Carnap/src/Carnap/Core/Examples/ToyLanguages.hs:120:29: error: * Could not deduce (Typeable ghc-prim-0.5.2.0:GHC.Types.Any) arising from a pattern from the context: Typeable b bound by the inferred types of TBind :: Typeable b =&gt; ToyLanguage b q :: Typeable b =&gt; BasicQuant ((t a5 -&gt; f b1) -&gt; g c) f :: Typeable b =&gt; Fix (Copula :|: (Predicate BasicProp :|: (Connective BasicConn :|: (Binders BasicQuant :|: (Function BasicTerm :|: EndLang))))) t1 -&gt; Fix (Copula :|: (Predicate BasicProp :|: (Connective BasicConn :|: (Binders BasicQuant :|: (Function BasicTerm :|: EndLang))))) t' at src/Carnap/Core/Examples/ToyLanguages.hs:120:9-13 * In the pattern: TQuant q :!!$: LLam f In the declaration for pattern synonym `TBind' | 120 | pattern TBind q f = (TQuant q :!!$: LLam f) | ^^^^^^^^^^^^^^^^^^^^^ /data1/graf/code/hs/Carnap/Carnap/src/Carnap/Core/Examples/ToyLanguages.hs:283:33: error: * Couldn't match type `t' with `a' `t' is a rigid type variable bound by a pattern with pattern synonym: :!$: :: forall (b :: (* -&gt; *) -&gt; * -&gt; *) idx. () =&gt; forall t. (Typeable t, Typeable idx) =&gt; Fix (Copula :|: b) (t -&gt; idx) -&gt; Fix (Copula :|: b) t -&gt; Fix (Copula :|: b) idx, in a pattern synonym declaration at src/Carnap/Core/Examples/ToyLanguages.hs:283:26-33 `a' is a rigid type variable bound by the signature for pattern synonym `:!$$:' at src/Carnap/Core/Examples/ToyLanguages.hs:282:20-104 Expected type: SimpleLambda (a -&gt; b) Actual type: Fix (Copula :|: (Applicators Application :|: (Abstractors Abstraction :|: (Function IntObject :|: (Function SimpleTerm :|: EndLang))))) (t -&gt; b) * In the declaration for pattern synonym `:!$$:' * Relevant bindings include y :: Fix (Copula :|: (Applicators Application :|: (Abstractors Abstraction :|: (Function IntObject :|: (Function SimpleTerm :|: EndLang))))) t (bound at src/Carnap/Core/Examples/ToyLanguages.hs:283:33) f :: Fix (Copula :|: (Applicators Application :|: (Abstractors Abstraction :|: (Function IntObject :|: (Function SimpleTerm :|: EndLang))))) (t -&gt; b) (bound at src/Carnap/Core/Examples/ToyLanguages.hs:283:26) | 283 | pattern (:!$$:) f y = f :!$: y | ^ /data1/graf/code/hs/Carnap/Carnap/src/Carnap/Core/Examples/ToyLanguages.hs:288:26: error: * Could not deduce (Typeable ghc-prim-0.5.2.0:GHC.Types.Any) arising from a pattern from the context: Typeable b bound by the inferred types of SimpAbs :: Typeable b =&gt; SimpleLambda b v :: Typeable b =&gt; String f :: Typeable b =&gt; Fix (Copula :|: (Applicators Application :|: (Abstractors Abstraction :|: (Function IntObject :|: (Function SimpleTerm :|: EndLang))))) t -&gt; Fix (Copula :|: (Applicators Application :|: (Abstractors Abstraction :|: (Function IntObject :|: (Function SimpleTerm :|: EndLang))))) t' at src/Carnap/Core/Examples/ToyLanguages.hs:288:9-15 * In the pattern: (SAbstract (Abs v)) :!$$: LLam f In the declaration for pattern synonym `SimpAbs' | 288 | pattern SimpAbs v f = (SAbstract (Abs v)) :!$$: LLam f | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Well, yes.
Thank you for unpacking that for me in detail. I'm beginning to understand; I need to let it sink in and perhaps work some examples.
I’m the author of the post if anyone has any questions. My open questions from this post: 1. Should people just turn off the monomorphism restriction in their code? 2. Have people found the missed specializations warnings helpful for improving performance? 3. Should projects, and maybe CI systems like Stackage, use the GHC internal consistency linting flags (e.g. -dcore-lint) as a way to find GHC bugs they can report upstream? Also just realized I should check if -Wall implies -Wcompat.
Excited about this lot of papers this year :)
Dfinity has the maximum entropy.
Cheers! I know that ViewPatterns changed in breaking ways going into 8.0.1 - no doubt other ghc extensions did too. So I suspect you're right, and it's due to one of those.
In our source plugin, [`smuggler`](https://github.com/kowainik/smuggler), we use our own [`hash-store`](https://github.com/kowainik/hash-store) package that detects whether something is needed to be changed. It compares hashes of file contents. There was discussion under announcement, why we don't use `pluginRecompile` to detect whether plugin needs to run again: * https://www.reddit.com/r/haskell/comments/90xyb1/ann_smuggler_ghc_source_plugin_to_manage_import/e2virsx/ But I would be happy to see, whether GHC API can be changed to take into account this case as well!
No, I'd expect almost all `curry` calls to be paired with an `uncurry` but not necessarily the other way around: there are many use cases for `uncurry` which don't require re-currying.
For example, I pretty commonly write stuff like `uncurry f &lt;$&gt; zip xs ys`
Hear hear! All the good variable names are already taken.
Also, construction of a tuple or manipulation of an existing tuple nearly always requires less terms than deconstructing or using optics. Therefor, for composition with functions that return a tuple, it's a path of least resistance to modify a function to take an existing tuple - that's a no-brainer. However, if you have a function that 'wants' a tuple already, constructing a tuple is really easy to do if the terms are already in scope, and even if not, it's generally not difficult to compose the construction of said tuple (especially with use of tuple sections). So, even in the presence of a function which is not already curried, `curry` is more likely to introduce confusion than it is to make composition significantly easier, and so it's generally overlooked.
This is very nice!
I'm pretty sure this is the same as `zipWith f xs ys`.
This is great! I’m especially hoping for audits on Text, ByteString, and cryptonite from this series
Announcement blog post requiring 2 click-throughs of bullshit fluff just to get to a god damned PDF? No thanks. 
Haskell on Windows works great for basically everything except for things having to do with the command line. Despite the recent advances in the windows console environment, building interactive CLI programs in windows with Haskell is more or less a no-go for all but the most trivial of usecases. If you want to make some text based UI projects of the sort that are kind of ideal for beginners, use Ubuntu. I started with Haskell on windows and did fine, but I had a strong understanding of computer science basics and a solid command of programming in a few other languages. Your experience as a newcomer to the field may not be as graceful, and if Haskell is really your first language, I'd recommend Ubuntu so that you can have an intermediary step in interactive program complexity between "solves a toy puzzle" and "is an application designed for other people to use."
The classic path is to become a PhD student with a professor that is active in this field. Not sure if that answer applies to your situation, of course.
Stack users can use the following approach to try it out: https://mail.haskell.org/pipermail/haskell-cafe/2018-August/129764.html The stack.yaml was generated from this script: https://gist.github.com/DanBurton/5b7fc5fae1ebfdc5bcaecfbac15d2903
Interesting! Yes, I do think that uncurrying is more popular because there are more curried functions than uncurried functions. So you would more likely need to uncurry a curried function than curry an uncurried function, since the latter is much rarer. However, the empirical ratio bugs me a bit. Uncurry is used only about 8 times more than curry...but i'm sure there are more than eight times as many curried functions as uncurried functions in Haskell. I would suspect there would be at least a dozen times more uncurried functions than curried functions. So this uncurry to curry ratio is a little smaller than I would have expected.
Does this mean haddock's been updated too? I've been meaning to try the new `:doc` command in ghci but it hasn't been working in the alphas
I noticed data is a bit slow to load sometimes. Reference to dynamic content has been removed from Markdown to boost loading of the description.
Thanks for producing these! I'm on Arch Linux, and got the following by trying `stack setup` as-is: ``` colin@yumi ~/c/h/test-new-ghc&gt; stack setup No setup information found for ghc-8.6.0.20180810 on your platform. This probably means a GHC bindist has not yet been added for OS key 'linux64', 'linux64-ncurses6', 'linux64-tinfo6'. Supported versions: ghc-7.8.4, ghc-7.10.1, ghc-7.10.2, ghc-7.10.3, ghc-8.0.1, ghc-8.0.2, ghc-8.2.1, ghc-8.2.2, ghc-8.4.1, ghc-8.4.2, ghc-8.4.3 ``` So I threw a `6` onto the end of the existing `tinfo` entry in the `stack.yaml`, and that seemed to install it properly. The repl worked. I then configured [Aura](https://github.com/aurapm/aura) to build with 8.6, simultaneously bumping the resolver to `lts-12.5`. `allow-newer: true` had to sneak in there too, to massage some upper bounds. I got most of the way through compiling Aura's dep graph of 133 packages. `Cabal`, `stm`, and `memory` failed to compile for me. (This isn't really a call for help, more a report of my experience.)
`NumericUnderscores` is the sleeper hit of this release
What does this provide over a package like generic-lens?
While there was an issue with the alphas that made the user's guide and the html haddocks for the libraries unavailable, the `:doc` command already worked – at least for me. But maybe you're using a different binary? What happens when you run `:doc head` in `ghci`?
Once you enable `DuplicateRecordFields` you need to be able to actually use those fields. Personally I see these classes as the minimum that is needed to make this extension useful. You can't expect all users to do advanced generic programming or incur an external dependency as the only possible way to use records. Moreover there are cases that generic-lens cannot handle but the built-in mechanism can. I can't post examples now because I am on mobile. IIRC it was a problem with type families.
Isn’t literate Haskell (.lhs) essentially CWEB for Haskell?
`DuplicateRecordFields` is already useful right now IMHO, in things like generic deriving ("if the record has a field "x" with type y, provide this default typeclass implementation"). Also with extensions like `NamedFieldPuns` / `RecordWildCards`.
Amazing work, so useful!
Superb answer .. describes PhD work without the politics
I'm curious: what does memory usage for your program look like? Is being able to gracefully handle OOM (perhaps you wrote an accidental space leak) not a concern?
There could - but that's a next step afterwards. You could also imagine extensions to disable generating selector functions. Idea is to add what's needed first, confirm it works as planned, then thinking about would can be removed. 
NixOS users can try the beta using `cachix` and `head.hackage`. https://gist.github.com/mpickering/fd26e9f03d6cb88cbb91b90b6019f3dd
You mean `nix` users ;) It can be used outside of nixos.
For me the main difference is that missing MonadFail instances now breaks compilation.
Can this be run asynchronous with other plugins so GHC can use more cores?
We've finally caught up to Ada, I'm so (genuinely) happy.
Thanks, is there a way to do this : propCheck (NonEmpty ns) = length ns &gt; 1 ==&gt; adjacentElementProduct ns == aep ns without filtering out the fails, i.e. just make it generate lists of &gt; 1 element?
Yep! Here's hoping it gets enabled by default soon. https://downloads.haskell.org/~ghc/8.6.1-beta1/docs/html/users_guide/glasgow_exts.html#numeric-underscores
This is one of these occasions where solving a problem more generally is actually easier than solving it for the special case. Note that this is not going to be the most efficient implementation, but surely one that is understandable. First, let's understand how we can pick a random number. There is the function randomRIO :: (Int,Int) -&gt; IO Int in Random.System we can use to pick a number in a range. This is going to be useful. Now, let's think about how we can shuffle something. We want to take an ordered list and make it unordered, so we want to build a new list from the old one that is randomly ordered. One way to do this is to take a random element from the list, use it as the first element of the new list and repeat this process for the remainder of the list. To implement this, we need the (!!) function that returns the element at a given index. We also need take which returns the first elements of a list and drop which returns the last elements of a list. Using this, we're ready to go. Lets say we are given a list [a] of elements. We want to pick a random index i between 0 and length-1 of the list, choose that element as the first element of our new list and combine it with the shuffled rest of the list. shuffleList :: [a] -&gt; IO [a] shuffleList [] = return [] -- An empty list cannot be shuffled more shuffleList [x] = return [x] -- A list with one element cannot be shuffled more shuffleList as = do i &lt;- randomRIO (0,length as-1) shuffledRest &lt;- shuffleList (take i as ++ drop (i+1) as) return $ (as !! i) : shuffledRest This gives: &gt;&gt;&gt; shuffleList [1..20] [1,14,11,19,20,15,4,9,16,8,10,12,18,2,13,17,6,3,5,7] This process works for any list, so especially also for your list of Cards.
&gt; You can't return a deck in Haskell and have it random. Haskell functions are functions -- same input, same output. If you want to use randomIO there's no way but to return IO Deck. This is a main design feature of Haskell, forcing the programmer to mark "impure" functions as such, by wrapping the result in the IO monad. To use randomness, one either works with IO, or passes around the RNG seed (being very careful not to reuse it twice, otherwise the same "random" numbers will be used twice). This isn't strictly true. There's a function called unsafePerformIO, which has the type signature IO a -&gt; a
Perhaps it's due to the simple nature of the program, but I did not have any issues with memory usage. I just ran a quick profile of the program and it holds steady at ~17MB. I'm told Yampa's use of arrows makes it difficult to write in accidental space leaks. I imagine for more complex programs, space leaks would be more of a concern and would necessitate either unit testing to catch leaks beforehand or (like you mentioned) somehow handle OOM at runtime.
 f’ = fmap (first f) x ?
&gt; Once I wrap the random number from ` to 52 in the monadic context, how do I get it out? You don't. There's no general function `Monad m =&gt; m a -&gt; a`; some monads might provide such a function, but not all of them. Instead you use `fmap`/`liftM` to lift functions to the monadic context or `(&gt;&gt;=)` ("bind") and the like to compose Kliesli arrows. `do` notation is based on `(&gt;&gt;=)` for the most part and may make operating in a monadic context more familiar.
This worked for me. To clarify, the change is: linux64-tinfo: to linux64-tinfo6: 
In case you *just* need to shuffle a list: https://hackage.haskell.org/package/random-shuffle
I'd use [random-shuffle](http://hackage.haskell.org/package/random-shuffle-0.0.4/docs/System-Random-Shuffle.html). 
`import Control.Monad (replicateM, (&gt;=&gt;))` `import Data.List (sortBy)` `import Data.Function (on)` `shuffle, shuffleTwice :: [Card] -&gt; IO [Card]` `shuffle xs = fmap fst . sortBy (compare \`on\` snd) . zip xs &lt;$&gt; replicateM 52 (randomRIO (1,1000))` `shuffleTwice = shuffle &gt;=&gt; shuffle`
This shouldn't be a valid `Arrow`.
I'm not following. Why must `c` be a Monoid?
[http://okmij.org/ftp/Haskell/perfect-shuffle.txt](http://okmij.org/ftp/Haskell/perfect-shuffle.txt) is a really nice lecture about perfect pure shuffling of list in complexity \`O(n log n)\`.
Your method have a quadratic complexity, but works well for a list of cards ;)
A small example: `shuffleM` works in `IO` and uses the global random number generator (the kind of things you'll do in any other language): ```haskell Prelude&gt; import System.Random.Shuffle Prelude System.Random.Shuffle&gt; l &lt;- shuffleM [1..10] Prelude System.Random.Shuffle&gt; l [4,1,5,2,9,3,10,8,7,6] Prelude System.Random.Shuffle&gt; l &lt;- shuffleM [1..10] Prelude System.Random.Shuffle&gt; l [7,4,5,9,6,8,1,10,3,2] ``` If you want something not in `IO` and with a local random number generator (the kind of things you should do): ```haskell Prelude System.Random.Shuffle&gt; import System.Random Prelude System.Random.Shuffle System.Random&gt; shuffle' [1..10] 10 (mkStdGen 0) [4,7,10,8,6,3,2,5,9,1] ``` 
One thing you could do is write an `IORef` in a type checker plugin which you read in `pluginRecompile`. Definitely a bit hacky though, I can't see what precise information could be passed into `pluginRecompile` to get your case to work. 
Idt you can implement `arr`
No, at the moment GHC assumes that all plugins potentially modify the code. Additionally, printing stuff from multiple plugins that run on multiple threads without producing garbage is not-trivial.
I prefer your way more than I describe below, but if you really need, use one of these. Solution 1: Probably overkill, but searching Hackage showed me [quickcheck-combinators](http://hackage.haskell.org/package/quickcheck-combinators-0.0.4/docs/Test-QuickCheck-Combinators.html) package. I have never tried it, but usage will look like: -- You will need the type annotation propCheck :: AtLeast 2 [] Int -&gt; Bool propCheck (AtLeast ns) = ... Solution 2: You can define newtype wrapper similar to `NonEmpty`. newtype AtLeastTwo a = AtLeastTwo [a] deriving (Show, Eq) instance Arbitrary a =&gt; Arbitrary (AtLeastTwo a) where arbitrary = f &lt;$&gt; arbitrary &lt;*&gt; arbitrary &lt;*&gt; arbitrary where f a1 a2 as = AtLeastTwo (a1 : a2 : as) shrink (AtLeastTwo as) = AtLeastTwo &lt;$&gt; filter (\as' -&gt; length as' &gt; 1) (shrink as) propCheck (AtLeastTwo ns) = ...
I would suggest that you work on a project with very advanced people. I also dont know a book. Seeing how others do it helps.
We don't say the "u"-word.
I don't think GHC makes any guarantees that those types will have the same representation.
&gt;I don't think GHC makes any guarantees that those types will have the same representation. Then the natural question is: why not? :)
I'm not saying it isn't do-able :), it certainly is! What I'm saying is that I shouldn't have to feel bad while writing that code because I shouldn't have to write \`unsafeCoerce\` in the first place and give up type safety :)
&gt; Wait, doesn't the fact that you can use GHC generics actually mean that you do have that guarantee? No. Generics work via a function that converts to a canonical type, not by giving you an abstract view of the real runtime representation. Aren't there info tables associated with each data type? Even though I'm sure they're identical for these types, I could imagine GHC thinking up reasons to care about e.g. the addresses of these tables in the future.
&gt; [..] I could imagine GHC thinking up reasons to care about e.g. the addresses of these tables in the future. Could you give an example? Perhaps it is just my lack of imagination but I can't really come up with anything solid...
You *probably* don't want this. It actually messes with the probabilities in every non-trivial game to use something like this instead of shuffling some small number of decks. For example, if you are playing poker with one deck, you have exactly 0 probability to be dealt two queen of spades. If you just generate random cards instead of dealing from a deck you have about a 4% chance of seeing two queen of spades in each 5 card hand. --- Formatting code requires 4 spaces your message source doesn't have any spaces. You can also do `inline code formatting`. You also have to separate code blocks from the other paragraphs with a double line break.
There is no guarantee because you want to leave the compiler as much freedom as possible. For instance, GHC might decide to do some unboxing when the type is not generic. Assuming anything unpromised about representation is lying to the compiler and you’ll be punished. Sooner or later. 
Unless things have changed, the author hasn't maintained that package for about 7 years and made some references to a severe bug. Also the `shuffle` function has a complicated and undocumented precondition on the random numbers input and will crash without it. The only documentation is a paper by Oleg. On the other hand, the paper makes the case that this should theoretically be a better shuffle than those in other languages. In other words, this is a very traditional haskell package from the good old days :)
\`arr\` would just be \`arr f = F (fmap f)\`. See my other comment for a more obvious reason that the laws are broken.
You could achieve this using newtypes and pattern synonyms, I'm pretty sure. That would give you coercions and meaningful pattern matches, would it not?
But I guess in real-life it will be much easier to convert list to an array and then use Fisher–Yates shuffle algorithm. And complexity will be `O(n)`.
No. GHC doesn't generate record selector functions for these fields, let alone `HasField` instances etc.. Since the type system is compositional, it's not enough for there to be some context in which the types make sense, we must be able to assign types to subexpressions.
&gt; Generics work via functions that convert to and from a canonical type Those functions aren't actually used at runtime, are they? It seems like it would be a big performance hit to constantly convert to the inductive/sop definition and back.
How does `ghcid` decide whether to recompile a module? Does it just look at the file modification time?
I'm not sure if it breaks any law, but there is something wrong in this type. Let's take runF (F f) = f f :: IO () -&gt; IO () f x = x &gt;&gt; x g :: IO () -&gt; IO () g x = pure () then, what should `runF (F f *** F g) (print "foo" &gt;&gt; pure ((), ()))` print?
Hmm, yes that is certainly a work-around that checks most boxes! If I'm really nit-picking, I'm not sure if GHC can transitively infer the pattern matches are exhaustive and if it can't then having to add a COMPLETE pragma means your code is now brittle against future change should one of the two types change.
&gt; There is no guarantee because you want to leave the compiler as much freedom as possible. While I can understand why you may want to do that, there are lots of places where we emphasize programmer control - newtypes and coercible themselves being a good example of it. &gt; For instance, GHC might decide to do some unboxing when the type is not generic. That sounds like a reasonable point. What if my example was changed so that both types were polymorphic? Let's say something like data Result e a = Error e | Okay a data Either l r = Left l | Right r Would you agree that we should be allowed to derive Coercible here? &gt; Assuming anything unpromised about representation is lying to the compiler and you’ll be punished. Yes, I get that which is why I'd like to not use unsafeCoerce :). As others have pointed out using PatternSynonyms is a work-around.
&gt;I think that in this domain, "why" is a more important question than "why not". Well the "why" is essentially a combination of 1. The compiler behaviour is "reason-able" - you have the guarantee that the compiler is not doing anything funky behind my back by special casing names of constructors. It doesn't care about names, only shape. 2. (Arguably) Clearer code compared to the pattern synonyms work-around. Of course, I recognize that there is some subjectivity involved here and you may think that these do not form a compelling argument. &gt; This sounds like a pretty strong constraint to place on the compiler, In another comment I gave an example of an Option and an Either type both to which are necessarily isomorphic. Would that qualify as a constraint too in your opinion?
I assume it uses the same logic as `ghci` because `ghcid` just reuses it.
No, I don’t think those should be coercible. The compiler might analyze your program and discover that the argument to `Error` is never used, so it does not need to be represented in memory. Promising coercibility hamstrings the compiler. I agree that something like `coerce` would be nice to have and should possibly be derivable. But it would be a function that might carry a runtime cost. 
You could maybe, as a workaround, define a dummy function in the same module as the synonyms, that matches against each constructor you have a pattern synonym for. That way you'd at least get exhaustiveness warnings *there*, if the type you're wrapping changes. Not an optimal solution, but it's something at least. 
&gt;The compiler might analyze your program and discover that the argument to `Error` is never used, so it does not need to be represented in memory. Are you talking about a hypothetical example here or does GHC actually do this? It sounds like an anti-modular optimization if it works across modules. You change one caller to use the argument and suddenly a bunch of modules which don't depend on it transitively may need to be recompiled.
Cool, extra points if the extensions were links to the respective entries in the docs ;)
You're right, I totally didn't think about probabilities. Do i need 4 spaces at the beginning of each line? I literally add 4 spaces to every line in my editor (spaces, not tabs!) and then copy and paste it and they just go away, double line break also becomes just one.
Yo dawg, I heard you like work-arounds so I put a work-around in your work-around 😂
I believe they *may* sometimes be around at runtime. GHC, to my knowledge, usually does a pretty good job of optimizing most of it away, but I don't think there are any strict guarantees. This is something that you can kind of test for though, using the `inspection-testing` plugin.
I think that it's more important to me that the compiler's externally visible behavior is predictable, and not it's internal behind the scenes behavior. Isn't that the point of the abstraction? As long as, to the external API, data types behave as expected, I shouldn't worry about what the compiler is doing internally. The compiler should be free to do whatever funky stuff it needs to to make my code as fast or efficient as possible. Funkiness obscured by a clean external interface *is* the goal, and I *want* it to do funky things as long as they are not externally observable, and make my code run better :) re: isomorphic types, remember that isomorphic data types do not necessarily have the same representation... and in many cases, you don't *want* them to. Consider two isomorphic types: data Foo = Foo Int Bool data Bar = Bar Bool Int Because these two data types are isomorphic.... do you think we should force GHC to give them the same internal representation? Don't you think that is a bit unreasonable to say "if your data types are isomorphic, they must have the same runtime representation!", which would force `Foo` and `Bar` there to have the same representation? Also consider a third type that is isomorphic to both `Foo` *and* `Bar`: data Baz = Baz1 Int | Baz2 Int Now all three data types are isomorphic (up to bottom), but I don't think you would want them to have the same representation. Isomorphic is not "important enough" to force GHC to bend over backwards and re-encode these types so that they have the same runtime representation.
I was talking about a hypothetical compiler. It’s definitely not a modular optimization. But there are whole program Haskell compilers that could conceivably do these things. 
It must have some additional logic to know when to call `:r` in GHCi. 
This is similar to associated type families: class FSM m where type FSMState m type FSMElem m update :: m -&gt; FSMState m -&gt; FSMUpdate m -&gt; m Basically, we can deduce s and e from m. This and the multi param type classes morally express the same idea although they aren't exactly isomorphic.
For your Foo and Bar example, I'd expect GHC to actually use the same representation if the Int and Bool are unboxed because there's necessarily only one optimal "struct layout" (if there are several ones, then the tie should not be broken based on how the fields are ordered in the definition). It's a different story if they're boxed. &gt; I think that it's more important to me that the compiler's externally visible behavior is predictable, and not it's internal behind the scenes behavior. Isn't that the point of the abstraction? As long as, to the external API, data types behave as expected, I shouldn't worry about what the compiler is doing internally. Hmm, I think this is the point where we differ. I'm of the opinion that one should be able to choose the level(s) of abstraction one wants to operate at :). &gt; Don't you think that is a bit unreasonable to say "if your data types are isomorphic, they must have the same runtime representation!", It certainly is and I made no such claims. (If it feels like I did, then I apologize for that, because I certainly did not intend to come across that way). I was merely talking about the one specific instance where all the constructors line up precisely and I was asking for your thoughts on that case. &gt;Also consider a third type that is isomorphic to both `Foo` *and* `Bar`: &gt; &gt; data Baz = Baz1 Int | Baz2 Int &gt; &gt;Now all three data types are isomorphic (up to bottom), To ignore bottom is to treat everything as unboxed for sum of products vs product of sums. I'd not be surprised if the same representation was used but I'm not saying that it must be. All I was pointing out was it "I'd expect it to work in the most trivially obvious case" ... of course, one can stretch "trivially obvious" to expand to arbitrary sums of products vs products of sums but I'm not doing that.
How many packages are you observing this on? We did a study of Hackage to see how much pain incorporating the MonadFail change so late in the release cycle would inflict. It looked like the change would only affect a few percent if packages so we decided to move ahead. In the future I hope we can have API-breaking changes like this sorted out we before the beta. 
I think it will require a handful of upstream packages to provide the missing instances, then the rest just falls into place.
Interesting. You've designed PureScript but you think to try Ur/Web. Is there something you don't like in PureScript?
That's what we all said.
Good points, thanks! I think I'm leaning towards the "enable `ExtendedDefaultRules` by default and make it show up" option. &gt; Also, since I didn't say it earlier - Nice work! This seems like a cool project that has a lot of potential. Thanks! 😊 Stay tuned. Literally, I'm currently working on a small music project that uses this as an interpreter.
Ah yeah, I think that's what I meant, I couldn't get haddock working with 8.6 so I couldn't test `:doc`
Doesn’t it have an option to run a program with user-provided stdin? You can also write “tests” (just assertions) and run them in main. I had a little test library I wrote I would paste into HackerRank to re-use for tests. 
Does Reddit not have a working source view for comments? I use RES so I've had a source button for years. I don't know what's wrong with you copy and paste, bit yes each line of the code block needs 4 spaces.
For refernce this is the "source" of the last bit of my previous message: Formatting code requires 4 spaces your message source doesn't have any spaces. You can also do `inline code formatting`. You also have to separate code blocks from the other paragraphs with a double line break.
This method works well in practice, but does not guarantee a correct shuffle (i.e. a shuffle where each permutations are equiprobables). To think quickly about why, you have `N = 1000 ^ 52` possibles outcomes for the `replicateM 52 (randomRIO (1, 1000))`. However there is `M = 52!` possibles permutations of the list of `Card`. A necessary condition (but not sufficient) to get a perfect shuffle will be to have `mod N M` == 0 which is indeed not the case: ``` &gt;&gt;&gt; 1000 ^ 52 `mod` (product [1..52]) 39579697475694089862173967067225300578361681026519924736000000000000 ``` Everything involving probability and random numbers should be taken with a lot of care and usually we should rely on peer reviewer libraries which are supposed to do that better than what our intuition is doing. 
This would be great if each extension linked to the latest manual documenting it!
do you have any words to a foregin?
ghcid is great without a doubt. As an additional data point, when coding in Emacs, I'm using [Dante](https://github.com/jyp/dante) because it's been more reliable than ghcid or haskell-ide-engine. That and Flycheck integration just works out of the box and it's not Stack-exclusive like Intero.
Alternately ghc --supported-extensions | rg 'MyExtension' 
It does, if you click on the checkmark boxes.
Why not try compiling your compiler into Javascript with GHCjs?
It actually is. It's simple to use and there's basically no setup required.
Nice trick! One possible extension -- you can avoid having a single data type have all of the "versions" by using a data famly (or injective type family): data family User :: Nat -&gt; Type data instance User 1 = UserV1 { name :: String } data instance User 2 = UserV2 { firstname :: String, lastname :: String } So you can flip the expression problem and be able to define/hide your "old" versions in different modules so they aren't always hanging around a giant `User` data type. One functionality you do lose though is the ability to do dependent pattern matching; with the GADT approach, you can pattern match on `User v` and the pattern matching will deduce what `v` was; however, this isn't a feature with data families.
Nice! Haven't worked with data families before; thanks for the tip!
A fold works by recursion, so on the face of it, no, there's nothing in **Haskell** that you can do with a fold that you couldn't do with explicit recursion. On the other hand, **GHC** can often find opportunities for optimizing a fold that you might not be obvious to you or the compiler when using explicit recursion. Ergonomically, it also means you'll be writing a lot of boilerplate and making it harder for people to read your code in cases where your problem can be expressed as a fold.
Explicit recursion is really the "[GOTO of functional programming](https://en.wikipedia.org/wiki/Considered_harmful)", since it's extremely easy in Haskell to accidentally write yourself into an infinite loop, and it's often very difficult to read and figure out what the code does. Whenever possible we like to use higher-order functions, because they are usually tried-and-true to not inadvertently introduce infinite loops on their own, and it makes it much easier to read code because they encapsulate a known pattern. Consider: foo [] = [] foo (x:xs) = (x*2) : foo xs vs. foo = map (*2) xs In the former, it's very easy to accidentally write an infinite loop when using explicit recursion, and you have to look at it a bit longer to figure out what the code was meant to do. In the second example using a higher-order function, we abstract the design pattern that the explicit recursion was trying to "say"; we also know that map won't introduce any infinite loops on its own if the input list is finite. Same thing with, say: foo [] = 0 foo (x:xs) = x + foo xs vs. foo = foldr (+) 0 In the first, it's extremely easy to introduce infinite loops, and it's also not immediately clear what's going on. In the second, it's clear what's going on, and we know foldr won't inadvertently introduce any infinite loops that weren't already there, and we know the intent of the author. In the end, you might be looking at it the wrong way. Foldl and foldr aren't there to offer any new functionality -- they're there to make it *easier for you to write maintainable and correct code* by eliminating a huge class of potential errors and can help make your code more readable. You don't use them because you *have* to, you use them because they help a lot! :)
Nice read!
What functionality are you giving up? They are specific tools to help you in specific situations. That's like saying you get how multiplication is useful. But you can always just use iterated addition. So you have two options: 1. Use multiplication where you want to multiply, and addition where you don't 2. Be 'consistent' and always use addition for everything But, I think it's fair to say that using both multiplication *and* addition in the same code base isn't too absurd. In fact, it's something that a reader of your code would hope you do :) Using recursion where you can't use higher-order functions and using higher-order functions where you can is just like using a screwdriver where you can't use a hammer and using a hammer when you can. Would you suggest that a carpenter destroy all of their tools and use just one, to be consistent? In the end, we use the best tool for the job in every situation. We don't try to make one tool useful for every situation.
I’ve actually started learning C# after using Haskell for several years. I really enjoy being able to use fp techniques fairly seamlessly in a C-style language. 
Is this what you are talking about? https://github.com/ndmitchell/ghcid It does sound pretty good.
will it build the depends packages? I'm thinking llvm-hs will give it a hard time. I'll try tho, thanks for the suggestion
Using fold and the other higher order functions (map, filter, etc), are very useful for the readability of your code, because they include some of the intention of your code. A function using explicit recursion can do anything. While that sounds good and powerful, it’s also error prone and hard to predict. Using higher order functions can give a more readable structure to the code. Also typically, even complicated recursive functions can be rewritten with higher order functions. Another reason to use higher order functions such as fold, is that they generalise to other data structures. Haskell’s recursive functions typically use pattern matching on lists, but if you don’t want to use a list, what should you do? Other data types such as vectors or arrays don’t open themselves up to explicit recursion, but they have implementations for the higher order functions. So if you write your function with higher order functions it’s easier to switch the data structure as required.
I loved this article, have been thinking about versioning a lot lately. If you are writing a public API or working with mobile you will inevitably usually hit issues around versioning, and I don't think there's a single good answer to them. Personally, I like the approach you outline here a lot. That said, I'm curious why in the article you said: ``` Disclaimer; the system I present probably isn't a great idea in a large production app, but is a fun experiment to learn ``` Why wouldn't you use it in a production app? Is it because it adds too much complexity/strictness/verbosity/....? I can see that, but the alternative is often just major backward compatibility with comes with a host of unreliable issues and complexity, though in the real world sometimes its the most practical solution. Lots of tradeoffs between the two usually. But, if this isn't the solution to versioning an API, what is?
Whoa, that is cool! Unfortunately, I suspect pattern matching would be really handy to have in a codebase with versioning, so this might not be worth it to the GADT approach
How does this compare with this library? * https://github.com/lortabac/versioning
You can still pattern match, but you just can't do dependent pattern matching unfortunately! You can simulate it though with a GADT wrapped, or re-use Data.Functor.Product and singletons: type MatchableUser = Product Sing User This should recover dependent pattern matching
It seems to me that these class morphisms are very much like ML functors. class morphism Enum -&gt; Eq where x == y = fromEnum x == fromEnum y versus functor EnumToEq (M: Enum) : Eq with Eq.t = Enum.t = struct type t = M.t (==) x y = M.fromEnum x == M.fromEnum y end
I yelled about it so often when folks talked about IDE stuff [that I eventually just condensed the rant into a blog post](http://www.parsonsmatt.org/2018/05/19/ghcid_for_the_win.html) and I've been piping up with it whenever I've had the opportunity. I do wish it were more front-and-center in our discussions of tooling.
I'm very new to haskell (and programming in general). I just saw [this comment](https://old.reddit.com/r/haskell/comments/96t8yv/question_about_folding/e431z6r/) about how explicit recursion is bad practice. I'm wondering if someone could show me how to rewrite one of my project euler (problem 24, to be specific) solutions properly. I had thought I was getting comfortable with folds, but I honestly have no idea how to do it with this particular one. For context, the full code is: import Data.List oneMillion :: Int oneMillion = 1000000 - 1 nthPerm :: Int -&gt; [Int] -&gt; [Int] nthPerm n numList = if length(numList) == 1 then numList else nextDigit : nthPerm modulus (delete nextDigit numList) where factorial = product [1..(length(numList) - 1)] modulus = mod n factorial entryNo = div n factorial nextDigit = numList !! entryNo answerList :: [Int] answerList = nthPerm oneMillion [0..9] answer :: Int answer = read . concat . map show $ answerList The nthPerm function is the one I need rewritten. It takes a number and an ordered list of digits, and uses the number to pick out a specific digit (dependent upon the ordering). When it finds the digit it wants, it conses it onto nthPerm applied to the same list with the same ordering, but *minus the digit it decided to extract*. I think the reason I'm struggling is that this function doesn't eat through the list linearly, but instead takes out entries from the middle. Thanks for any help, and I'm welcome to other criticisms as well, I'm sure there's lots of room for improvement.
I'm very new to haskell (and programming in general). I just saw [this comment](https://old.reddit.com/r/haskell/comments/96t8yv/question_about_folding/e431z6r/) about how explicit recursion is bad practice. I'm wondering if someone could show me how to rewrite one of my project euler (problem 24, to be specific) solutions properly. I had thought I was getting comfortable with folds, but I honestly have no idea how to do it with this particular one. For context, the full code is: import Data.List oneMillion :: Int oneMillion = 1000000 - 1 nthPerm :: Int -&gt; [Int] -&gt; [Int] nthPerm n numList = if length(numList) == 1 then numList else nextDigit : nthPerm modulus (delete nextDigit numList) where factorial = product [1..(length(numList) - 1)] modulus = mod n factorial nextDigit = numList !! div n factorial answerList :: [Int] answerList = nthPerm oneMillion [0..9] answer :: Int answer = read . concat . map show $ answerList The nthPerm function is the one I need rewritten. It takes a number and an ordered list of digits, and uses the number to pick out a specific digit (dependent upon the ordering). When it finds the digit it wants, it conses it onto nthPerm applied to the same list with the same ordering, but *minus the digit it decided to extract*. I think the reason I'm struggling is that this function doesn't eat through the list linearly, but instead takes out entries from the middle. Thanks for any help, and I'm welcome to other criticisms as well, I'm sure there's lots of room for improvement.
I really wouldn't overly worry. I tend to mostly stay away from folds, and it doesn't cause any problems. I suggest learning what bits appeal to you and likely you'll grow more comfortable with good over time. 
It's more than O(sqrt(n)), because numbers can get pretty big. For example, addition is thought of as O(1) by people who deal with fixed-width integers like the ones in processors, but it's O(n) on big numbers, so if it was trial addition, it would be sqrt(n) iterations of O(n) time, which is O(n sqrt(n)). Division is much slower than addition.
It may be a bit heavy dependency for just this, but the [combinat package](https://hackage.haskell.org/package/combinat) has an implementation of a uniformly random permutation or "shuffle" (see [Knuth shuffle](https://en.wikipedia.org/wiki/Fisher%E2%80%93Yates_shuffle) for the background): import System.Random import Math.Combinat.Permutations shuffle :: [a] -&gt; IO [a] shuffle xs = do let n = length xs perm &lt;- getStdRandom (randomPermutation n) return $ permuteList perm xs 
**Fisher–Yates shuffle** The Fisher–Yates shuffle is an algorithm for generating a random permutation of a finite sequence—in plain terms, the algorithm shuffles the sequence. The algorithm effectively puts all the elements into a hat; it continually determines the next element by randomly drawing an element from the hat until no elements remain. The algorithm produces an unbiased permutation: every permutation is equally likely. The modern version of the algorithm is efficient: it takes time proportional to the number of items being shuffled and shuffles them in place. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/haskell/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
\`llvm-hs\` depends on the LLVM C++ libraries so you first need to somehow manage to compile these to JS and then get GHCJS to call these functions instead of trying to use the C FFI. While that might be possible in theory, I would expect it to be really difficult in practice (I have no idea if anyone managed to compile LLVM to JS) and hardly worth the effort. If you want to run your compiler in the browser you are probably better off writing some server backend that actually does the compilation and compile your code on the server instead of the client. In 
\`versioning\` provides a \`V\` data-type to represent a version number. In theory you could use it with any encoding, so, for example, you could rewrite this blog post and just replace \`Nat\` with \`V\`. However if you use the encoding proposed in the README --that is if you think in terms of adding/removing fields at each version-- not only it makes for less verbose data-declarations but it also opens the way for automatic upcasting and decoding, which is where the library shines IMHO. I wonder how hard it would be to achieve the same level of automatization with GADTs. One thing that a GADT-based encoding gives you is dependent pattern-matching, but I'm adding singletons for the \`V\` type, so something equivalent should be possible soon. Anyway, good to see that people are looking for type-safe ways of solving the versioning problem. I'm pretty sure there is still a lot to explore.
There is also [https://github.com/typeable/schematic](https://github.com/typeable/schematic), notably the \`Data.Schematic.Migration\` module.
while cool and all, why not something lighter like `atmega/esp32` paired with `hc-05`?
Was the performance investigated at all? Is `hip` able to fuse any operations together when they are composed?
Can't use F# where you work?
Folds can be composable and first-class. Check out the [`foldl`](http://hackage.haskell.org/package/foldl) library. They have many ready-made folds, like for example [`variance`](http://hackage.haskell.org/package/foldl-1.4.3/docs/Control-Foldl.html#v:variance) and [`hashSet`](http://hackage.haskell.org/package/foldl-1.4.3/docs/Control-Foldl.html#v:hashSet).
&gt; I think the reason I'm struggling is that this function doesn't eat through the list linearly Then *don't even bother making it non-explicitly-recursive* on that list. I mean, the imperative equivalent of *forcing* non-explicit-recursion is basically 'You should not use `while` loops. All loops should be `forEach`'. And nowhere in the comment you referred to mentions 'bad practice'. The rest of the thread, in fact, talks about how you should see folds: They are *specific tools* for a *purpose*, not something you just reach for whenever you see lists. And you absolutely shouldn't unconditionally give up explicit recursion. Not even when you see lists. But you said you want some suggestions, and you seem eager to see some non-explicit recursion, so here we go. When writing functions it might be helpful to use a 'pipeline' approach, by determining and separating the intermediate steps. First we can identify that you're actually doing two things in `nthPerm`. First one is to *generate* a list of 'digits' of `n` in a sort of 'factorial base'. That's all the `modulus` values. We can generate that in a separate function: factorialDigits :: Int -&gt; Int -&gt; [Int] factorialDigits num total = unfoldr go (num, total) where go (_n, 0) = Nothing go (n, size) = Just (modulus, (next, size - 1)) where factorial = product [1 .. size - 1] (modulus, next) = n `divMod` factorial [Learn more about `unfoldr` in the docs.](https://hackage.haskell.org/package/base/docs/Data-List.html#v:unfoldr) A direct equivalent without `unfoldr` would be: factorialDigits :: Int -&gt; Int -&gt; [Int] factorialDigits _n 0 = [] factorialDigits n size = modulus : factorialDigits next (size - 1) where factorial = product [1 .. size - 1] (modulus, next) = n `divMod` factorial As you can see, this direct version is about as tidy as the `unfoldr` version. But not too much of an improvement, I would say. Next we can permute the `numList` according to some digits: permute :: [Int] -&gt; [Int] -&gt; [Int] permute digits numList = snd $ mapAccumL go numList digits where go list n = (delete x list, x) where x = list !! n `mapAccumL` has this type: mapAccumL :: Traversable t =&gt; (a -&gt; b -&gt; (a, c)) -&gt; a -&gt; t b -&gt; (a, t c) So we can use it at this type: mapAccumL :: (a -&gt; b -&gt; (a, c)) -&gt; a -&gt; [b] -&gt; (a, [c]) It's like a `map`, but you can pass some value (`a`) from one element to the next, so it 'remembers' things. A weird but appropriate analogy would be a 'recurrent neural network'. A direct version for this is: permute :: [Int] -&gt; [Int] -&gt; [Int] permute [] _list = [] permute (n : ns) list = x : permute ns (delete x list) where x = list !! n `mapAccumL` now has a clear advantage: the traversal and generation of the list is all packaged inside `mapAccumL`, and only the initial conditions and `go` needs to be specified. Time to put them together: nthPerm :: Int -&gt; [Int] -&gt; [Int] nthPerm n numList = permute (factorialDigits n (length numList)) numList And `answer` is still right, so I guess I did everything alright? So was it worth it? I think `permute` was definitely worth it, and `factorialDigits` was less so. The separation of the two was very much worth it, and that was probably the real important part. Using folds encourages you to use simpler functions, and to break up your logic into traversing the entire set of data and manipulating single pieces. In the end, I do hope you learned something; that was *some* typing involved. --- I would also drop here an alternative `factorialDigits`. I don't think I have enough time/energy quota for Reddit to explain this, but this is what I would have written. base :: Int -&gt; [Int] -&gt; [Int] base num bases = snd $ mapAccumL divMod num bases factorialDigits :: Int -&gt; Int -&gt; [Int] factorialDigits num total = reverse (base num [1 .. total]) 
Why? It is no faster to run hlint separately, it would still be faster to do it in GHC itself. When https://github.com/ocharles/hlint-source-plugin/issues/3 is fixed it's definitely faster than `ghc --make &amp;&amp; hlint` as it only requires source to be parsed once. On top of that, you get recompilation avoidance, so you will only hlint changed source code.
Also \`stack build --fast --ghc-options="-j +RTS -A32M -RTS"\` as a default Emacs "haskell-compile" command.
Thanks for the response! I don't know a lot about programming so I got the wrong idea from the comment. I thought I had heard that goto statements were bad practice, but to be honest I don't remember where I got that impression from. I hadn't even considered splitting it up in that way, but that's pretty clever. I haven't seen mapAccumL before, (presumably because I haven't yet reached the stuff on Traversable in the book I'm reading), so I'm going to take a little more time to digest that, but your first version factorialDigits makes sense and I get the general idea. This was really helpful, I appreciate it.
Great post!
Great explanation, although your last example is expanded in the wrong order. It should be `[4, 5] ++ []` in the innermost set of parentheses.
Thanks for the correction. I will fix it now.
The last release with significant implications for dependent haskell was GHC 8.0, which did the following: 1. Removed the restriction on promotion of GADTs. 2. Unified types and kinds. This year (2018), there was a haskell summer of code project for a [dependently typed core replacement](https://summer.haskell.org/news.html#dependently-typed-core-replacement-in-ghc), but I don't know how it turned out. There have also been some GHC proposals: - [Add Dependent Haskell Quantifiers](https://github.com/ghc-proposals/ghc-proposals/pull/102) - [Syntax for Visible Dependent Quantification](https://github.com/ghc-proposals/ghc-proposals/pull/81) - [Partially Applied Type Families](https://github.com/ghc-proposals/ghc-proposals/pull/52) None of these have been accepted (or, to my knowledge, implemented), but they have generated good discussion.
What readings / field of knowledge is needed to understand the underpinnings of Haskell? I am currently in college and just got my minor in mathematics but the material I've seen all looks pretty foreign. I would like to really understand the theoretical foundation of the language to better understand how Haskell works and is different from other languages.
since it should be only a front end for neovim - give it a try - you can steal some of my [https://github.com/epsilonhalbe/dotfiles/blob/master/init.vim](https://github.com/epsilonhalbe/dotfiles/blob/master/init.vim) config which works with stack and haskell-ide-engine (most of the time). I personally prefer the terminal (in combination with tmux) to an electron app.
so cooool. thanks for sharing!
Would marking the `Cell` fields as strict do anything to improve performance? data Cell = Fixed !Data.Word.Word16 | Possible !Data.Word.Word16 It seems like it could eliminate the need to deepSeq and maybe reduce allocation. 
Have you used this on OSX at all? I've got hie integrated with vscode right now and I keep getting these errors about integer-gmp and another about ghc-mod and dynamic linking. One or both causes crashes and odd behavior after awhile but googling has been no help. Thus, I'm thinking about shutting them off and going only hlint + ghcid.
&gt; there's necessarily only one optimal "struct layout" Not true. In many cases there are two; optimize for space or optimize for access time.
Have you red [this](https://typesandkinds.wordpress.com/2016/07/24/dependent-types-in-haskell-progress-report/) from Richard Eisenberg (2016)?
Is there a way of having something like "bind" in the output value of a conduit. Something like: `ConduitT a (ConduitT a b m ()) m () -&gt; ConduitT a b m ()`
Not OSX but I tried it on Linux, it works great. I also had hie and ghc-mod but both were unstable and would randomly stop working or give weird error messages. Also unlike former two this ide uses only ghci just like ghcid so you get great performance and portability (works on GHC 8.0+).
This is also where wasm backend will be useful. clang can already be run on browser: https://tbfleming.github.io/cib/
Thanks!
The text should be more specific that it's counting all package releases, including updates. The year 2015 may have hit the maximum because many packages required updates to conform with AMP and FTP changes. It would be nice to see the statistics for only the *new* package releases. 
You could start with Type and Programming Languages (aka TaPL).
Wow! Thank you this looks like a great starting point.
Wow. Your Syntax highlighted code areas are fancy! Especially for mobile!
&gt; I know there are "better" algorithms but I chose this for practicing.
It’s a bummer that it thinks datakind ticks are the start of quotes though
Mark karpov got tired of similar issues. He solved them by directly using the ghc lexer in his static site generator (See https://github.com/mrkkrp/ghc-syntax-highlighter).
/u/tosskers comment inspired me to try some vector hackery and I got down to 3.4 seconds serial or parallelized under 1. As memory layout I have `Unboxed.MVector RealWorld (DigitSet, Bool)` where DigitSet is Word16. The Word16 storage allows for [bit twiddling](https://github.com/Tarmean/Sudoku/blob/master/library/HiddenSingletonPass.hs#L46) instead of Map which probably accounts for most of the speed difference. Storing the cell markup digit wise (instead of cell wise) and bundling three rows (9*3=27 bits) in a Word32 probably would allow for another 5-10x speedup but I never got around to implementing that.
If anyone has any suggestions (or links) on how to avoid using Incoherent, or how to set this up better for optimization passes, or a better way to inject other morphisms in there, I’d love to know!
Great work! I do appreciate good testing tools. It's amazing what formulating your specifications in code can do.
While the post is great, and I don't have anything against Artyom, I've applied almost 2 weeks ago and no feedback so far.
Crap! Sorry, such things really shouldn't happen. Can you PM me your name and email? 
That's from 2016. Any Updates?
Wait, why have I just heard of *this*? Worked perfectly on OS X for me, where fancier alternatives all had problems.
yep that's what I thought. no surprised. sad tho
I'm personally very excited about a possible future language extension implementing this feature. I have a few questions: 1. Is there a plan for moving this from the prototype implementation into a proper language extension? I found the source code for the implementation at https://github.com/cifasis/ghc-cm, but I don't see any mention of a plan to merge the changes upstream. 2. Could this be implemented as a GHC plugin? My read of the paper is that everything that's being done at the type checking level could be done using the existing type-checker plugins. But all of the syntax to define new morphisms wouldn't be supported and would basically have to be hard-coded into the plugin. [There is a proposal for an extended plugin system](https://github.com/ghc-proposals/ghc-proposals/blob/master/proposals/0017-source-plugins.rst), however, and I'm wondering if the more powerful plugin system would support the syntax necessary for class morphisms?
This series of articles is fantastically helpful. Thanks!
You can do anything at all with `GOTO`. That's kinda the problem -- it's so powerful that it makes it impossible to reason about programs that use it! Explicit recursion has a similar problem. I can do anything at all with it, so it's very powerful, but it also becomes very difficult to reason about the program. With something like `map`, I know a few things immediately: - I will perform the callback once per element (eg `O(n)` in the size of the list) - The structure and ordering of the list won't change - The result of `map f xs` terminates if `xs` terminates. `foldr` and `foldl` have similar advantages: - I will perform the callback once per element in the original structure (eg `O(n)` in the size of the list) - The structure is processed left-to-right consistently - If `k` is lazy, then `foldr k z xs` will also be lazy and can be used even if `xs` is infinite - If `xs` is empty, then `foldr k z xs` is `z`. So we say: I can accomplish this task using one of a few common ways to factor explicit recursion out. We can transform a recursive structure (`map`), we can reduce one (`foldr`, `foldl`), and we can generate one (`unfoldr`, `iterate`, etc). When we go from explicit recursion to one of these more limited ones, we gain similar benefit to thinking about our code as when we go from `GOTO` to using a `for` loop, an `if` statement, etc.
http://mail.haskell.org/pipermail/ghc-devs/2018-August/016101.html
Thanks, it took a few iterations to get the code to where it is now and I am quite happy with it. Seems my Haskell clean code-sense is starting to work :)
Thanks for the update! Here I found few new related features, like: StarIsType and some PolyKind improvements. [https://downloads.haskell.org/\~ghc/8.6.1-beta1/docs/html/users\_guide/8.6.1-notes.html](https://downloads.haskell.org/~ghc/8.6.1-beta1/docs/html/users_guide/8.6.1-notes.html)
I understand. So long time from now.
I obviously didn't try it out on a folder of csv files, but might something along the lines of this work? {-# LANGUAGE OverloadedStrings #-} module Bind where import Conduit import Data.ByteString (ByteString) import qualified Data.ByteString.Lazy as BL import qualified Data.Either as Either import Control.Monad (void) data ZipEntry = ZipEntry data ZipInfo = ZipInfo unZipStream :: (MonadThrow m, PrimMonad m) =&gt; ConduitM ByteString (Either ZipEntry ByteString) m ZipInfo unZipStream = undefined data HasHeader = HasHeader -- Just for clarity type DecodeResult a = Either String [a] -- Approximately cassava's type signature. decode :: HasHeader -&gt; BL.ByteString -&gt; DecodeResult a decode = undefined -- Skip the header and take everything until the next header. take1 :: Monad m =&gt; ConduitT (Either ZipEntry ByteString) o m BL.ByteString take1 = dropWhileC Either.isLeft *&gt; takeWhileC Either.isRight .| mapC (Either.fromRight "") .| sinkLazy takeMany :: (MonadThrow m, PrimMonad m) =&gt; ConduitM ByteString (Either String [a]) m r takeMany = void unZipStream .| takeNext -- ^ I'm too lazy to fiddle around with the 'ZipInfo' result right now. where takeNext = do bs &lt;- take1 yield (decode HasHeader bs) takeNext You obviously still have to tweak things to actually handle the headers somehow and deal with the `ZipInfo` result, the type of which was "in the way".
why is it the project of just one dude? Shouldn't this have wider buy-in and development support from the whole GHC dev team? It should be bigger than one dude and his personal work schedule
oops, fixed
I think I figured out how to deal with the composition problem as well, and edited my other comment.
&gt; Or is that how GHC has always been? Various pet projects of various academics? GHC has always been worker-driven. You do need to get minimal buy-in from one of the core maintainers, but mostly that's about not breaking things. Other than that it's all about who produces patches. Anyone that wants DependentHaskell to come about faster needs to start writing (or funding) patches. And, honestly, the GHC/Haskell community isn't universally enthused about DependentHaskell. I love dependent types, but I think they are of questionable value when all types are inhabited. Lots of people are concerned about type inference, for good reason; type "noise" if what drove industry to languages like Python and JavaScript. They don't want to lose any of the type-inferencing power of non-dependent Haskell, and probably won't use any features that require type annotations. I'd honestly rather more effort went toward producing the next Haskell report than adding more features to GHC. Improved error messages, improved performance, improved IDE integration, etc. are all higher priority that DependentHaskell in various part of the community. And, yes, it's not exactly a zero-sum game. Performance tweaking and revising a new report are vastly different skills, both from each other and from implementing DependentHaskell. But, it's not exactly the biggest community to begin with so when you fracture it you end up with a lot of activities that are one-person projects.
This is awesome
This whole section of the documentation is intended to define the fields: https://www.haskell.org/cabal/users-guide/developing-packages.html#package-properties Also you get most of them semi documented by running `cabal init` and asking for it to document all fields (including those unused in the inited file)
Great write-up!
This is how most FOSS projects work; people all contribute whatever they can or want to. It's not quite like there are "employees" who are assigned jobs to be done; usually people just come on to add a feature or fix a bug that they personally are invested in, and there are some people who stay on regularly to manage logistical things like deployment.
Thank you! I didn't realize property &lt;=&gt; field.
Let me note it was a bit confusing because when I searched for a field name, the actual documentation link is labeled as 3.1 Quickstart, so I didn't think it was relevant, having already looked through 3.1. Section 3.3.2.2 does not appear on the left sidebar at all, until you drill down deeply, and as I aluded to in the other comment, "package properties" doesn't immediately scream out "field documentation" to a noob like me.
&gt; aren't exactly the same I'm afraid I don't understand what you mean. Why is the debug info relevant? You can't use that information at runtime without violating type safety.
I know, right! Ghcid was what kept me with Haskell some time ago when I was getting very frustrated with slow compile times and loose feedback loops in my workflow. I was baffled that I had to search for it so much and no one told me about it beforehand.
The [Haskell Weekly](https://haskellweekly.news/) has new job listings every week :)
I'm would like to solve one problem and work and I'm interested in exploring solution with the help of `acid-world`. It would be really great if `acid-world` would contain some tutorials or simple examples. Like similar directory in `acid-state`: * https://github.com/acid-state/acid-state/tree/master/examples Something like `HelloWorld` or `KeyValue` would be really useful!
I see. Now I understand you concern. However, doesn't type safety guarantee that the underlying value is never inspected? The runtime shouldn't be basing it's decisions on where exactly the pointers in the info table point to unless you're using Data.Dynamic or something like it.
Most, if not all of them are for senior positions 😥.
Ooh. I might need to steal this for my own testing. =)
Hmm... so you *do* expect almost identical info tables. In this case, we'll have to see if GHC developers are willing keep this promise. And until it is made official this would always have to be unsafe.
&gt; The runtime shouldn't be basing it's decisions on where exactly the pointers Perhaps the `:sprint` example or my wording misled you, but the info table has *nothing* to do with runtime typing. The runtime *has* to inspect the info table, otherwise there would be no way to tell that a certain object is a thunk or an evaluated constructor value. And no, `Data.Dynamic` doesn't use this.
An analogy would be the table of virtual functions in C++. Even you lay out your virtual functions in a class `D` exactly as class `C`, there would be no guarantee that casting a `C*` to a `D*` would produce the corresponding vtable for `D`.
An analogy would be the table of virtual functions in C++. Even you lay out your virtual functions in a class `D` exactly as class `C`, there would be no guarantee that forcibly casting a `C*` to a `D*` would produce the corresponding vtable for `D`.
(Sorry for the comment mess-up. I keep hitting 'delete' when I mean 'edit') Perhaps the `:sprint` example or my wording confused you, but: &gt; The runtime shouldn't be basing it's decisions on where exactly the pointers in the info table point to The runtime *has* to, otherwise there would be no way to tell whether an object is a thunk or an evaluated ADT. And `Data.Dynamic` does *not* use this.
There are also entry-level positions for geniuses (:
My understanding is that maintaining type inference that exists is a high priority in the DH development. I also find that in my daily development, there are a lot of things that would be easier with features that are on the road map for DH, even if those features aren't quite dependent types. For example, today I used the `Replicate` type family to save boilerplate when declaring a `'[Double, Double, Double, Double, Double] :: [Type]` (actually, I wrote `Replicate 4 Double ++ '[Double]` to clarify intent, but that's tangential). I wish I could have just used a lifted `replicate` function, but the current machinery for doing so is extremely cumbersome. I'd be happy to discuss further, if you're interested.
:'(
&gt;The runtime *has* to, otherwise there would be no way to tell whether an object is a thunk or an evaluated ADT. But does it need to distinguish between different ADTs if they're of the same shape? I guess it does it if it passing around typeclass dictionaries at runtime.
This is cool! I tried going through your code but it's quite hard to understand. It'd be great if you can write a post about how it works.
Thanks :)
Thanks :)
This doesn't directly answer your question, but I wrote a small library (in Haskell) that will scrape the Problem Sets for a given HackerRank question and generate the boilerplate for a solution and test file. I found that running the specs locally made it easier for me to debug, so perhaps this could relieve some of your pain.
Anduril Industries [looking for Haskellers](https://jobs.lever.co/anduril/12225d20-74be-441a-889e-1bb2e607fae3) to work on hardware interfaces, detection, tracking, and sensor fusion. There'll be lots of profiling and correctness testing work, as well as calling into and being called from C++. Anyone interested should shoot me an email at [travis@anduril.com](mailto:travis@anduril.com) 
Thanks so much for all your work on cross compiling Haskell! Your articles really helped me getting cross compiling Nix infrastructure off the ground at work.
Not too sure about your specific problem, but a lot of the Haskell I write for work is called as a library by C++ programs. If you want the C caller to be able to pass around opaque bits of state, check out StablePtr in base.
But that's not about commutativity, but follows from tue definition of the identity element of a monoid. Maybe happens to be a two-element monoid, ie. one needs to be the identity, or else ot would be a semigroup. (And I'm not mixing up monad or monoid, just refer to the fact that monads are the morphisms of [C,C] for C a monoidal category).
Please do, the more it is used, and the more tests we build up, the more robust the whole LSP ecosystem becomes.
Galois is one of the few companies that makes me wish I could rewind a couple decades and slap some solid sense into my younger self. Due to the high profile level of work you all do I get the distinct impression that non-STEM wielding industrial developers might have a hard time competing to stay out of the circular file, let alone a chance at an interview. Is there a grain of truth to that? Or, depending on the incoming projects and general work load, does Galois look for creative and passionate developers willing to build out functional systems from specs designed by their research peers?
IIRC this is defense industry work for the US government. Mentioning as I didn't see it in the post.
There seems to be no mention of multiparameter type classes in the paper. Admittedly, this feature would probably become very complicated with very little reward when moving to mptc but I would have liked to see a discussion about that from the authors. Anyway, cool paper!
One reason the hiring situation is particularly tricky at a company like Galois is the fact that their interview process involves a gun duel with another prospective candidate.
That isn't the case. It's often true, for reasons others have discussed. But /u/goldfirere mentioned in the above quote: &gt; I have a small army of students who will hopefully be implementing... Some collaboraors and I... For a large open source project, the ideal world is a single clear and deeply involved leader with many enthusiastic volunteers. That is what we have here. But given the scope of this endeavor, it might help to have even more enthusiastic volunteers jump in.
Embedded systems deployed in remote environments, low-latency video processing, radar, I wonder what it could be 🤔
This should work I think, but it comes with a couple of caveats: * Foreign tools querying the database will see the latest checkpoint, not the current data, and they will have to be very careful about editing it. * Won't creating a checkpoint be rather expensive if you have to dump the entire state in a single database transaction? This might well be fine for some applications, of course, but overall I'm not sure you gain much versus writing a separate tool to copy the state into a database for analysis purposes.
I think overlapping instances should be enough here `class IsTup a b | a -&gt; b` `instance {-# OVERLAPPING #-} (c ~ 'True) =&gt; IsTup (a,b) c` `instance (b ~ 'False) =&gt; IsTup a b`
This job at Microsoft Research in Cambridge, UK will involve a substantial component of either Haskell or F#, probably both. &gt;a rather special job coming up, linking mathematics, compilers/proof systems, and machine learning. If you can see the beauty in both functional programming and C++, and have strong mathematical skills, now would be a great time to [send me a CV](https://www.microsoft.com/en-us/research/people/awf/). \-- [https://twitter.com/Awfidius/status/995987190260862976](https://twitter.com/Awfidius/status/995987190260862976) CV should be sent to Andrew Fitzgibbon [https://www.microsoft.com/en-us/research/people/awf/](https://www.microsoft.com/en-us/research/people/awf/).
This is funny, if is any suspicion that the job would be for the state department, people start questioning things, which is good, of course, but if in the job description is clear that it would be about working on some adds injecting alghoritms with who knows what consequences on the society, no ethical dilema, no questioning, no nothing, as long as it commes from the private sector
I work here, and am enjoying it a lot. Happy to answer questions!
I think a basic understanding of functions and function composition should suffice. However, there is a deeper connection between functional programming, type theory and category theory. For the latter aspect, you can't do much worse then this series of blog posts: [here](https://bartoszmilewski.com/2014/10/28/category-theory-for-programmers-the-preface/) Hope this helps. Cheers. 
Glad if it helped. Not so sure what I did for nix though 🧐
Thanks for your reference. I will look at it. 
&gt; I am new to Haskell language and I want to learn it. First thing I want to do is to learn the math which stands behind the whole language. I wouldn't recommend that approach. The best way to learn the maths behind Haskell is to learn Haskell.
&gt; So what is the best way to learn it? Just start to learn the syntax and features of the language? That would work. You should just learn Haskell like you'd like any other language. &gt; why do you think it is not good approach to start learning the theory first? When is it ever a good idea to learn something practical by starting with the theory?
also if you start learning the "theory" you'll probably be really disappointed with Haskell ;)
I agree -- while it's probably a good idea to read at least one introductory text about the syntax and features of the langauge, the best way to learn is to start trying to build things in haskell itself. A few notes: - I personally started with and loved [Learn You A Haskell For Great Good](http://learnyouahaskell.com/), but it's considered terrible outdated now... I still think it's good for the basics at least though. - I recently read [Real World Haskell](http://book.realworldhaskell.org/read/) and it's excellent, more up to date, and has comments - [The easiest to follow introduction to Functors/Applicatives/Monads I've ever seen](http://adit.io/posts/2013-04-17-functors,_applicatives,_and_monads_in_pictures.html) - [A good practical haskell guide](https://seanhess.github.io/2015/08/04/practical-haskell-getting-started.html) - Just use [stack](https://docs.haskellstack.org/en/stable/README/) -- while I'm normally a staunch advocate of starting from the simplest thing, Stack exposes enough of cabal to you to not make it too far away (you'll need a `stack.yaml` *and* a `project.cabal` file)
Thank's a lot for the sources. What do you think of this one guys? [http://haskellbook.com/](http://haskellbook.com/) I've read good feedback on this book. Do you have any experience with it someone?
You're creating a strawman here. There are people that will refuse working for Google, Facebook, etc. They just don't need to ask who's hiring, because these companies put their brand forward when they need to hire. 
How to exploit data-parallelism to build a rank-select bit-string for the CSV format described in RFC4180, taking into account complicating factors such as quotes, escaping, and quoted control characters.
There's a good article about Anduril and what they do [here](https://www.wired.com/story/palmer-luckey-anduril-border-wall/). 
We at [SQream](https://sqream.com) are looking for Haskell developers to work on our in-house SQL compiler. The job is on-site at our R&amp;D center in Tel Aviv. [Here is the full job posting](https://sqream.com/careers/haskell-software-developer/).
I am reading it myself. Since I have already know most stuff in the book from other languages, I still find it interesting enough to read through it. With a study group formed with some of my colleagues, they are reading it with no prior knowledge of Haskell. And they have a quite easy time reading through it. It is quite elaborate in all the topics it explains and makes it easier to follow and thoroughly understand more advanced topics later on. TL;DR: I find it one of best resources out there. 
Sent you my CV 2 weeks ago, no reply so far.
I applied for a summer internship but I haven't heard back yet. I work on compilers and formal verification (polyhedral compilation, LLVM, trying to formalize polyhedral compilation semantics in Coq, etc). Will I get to hear back? :) http://github.com/bollu formalization of polyhedral compilation semantics: https://github.com/bollu/polyir
But why run `ghc --make &amp;&amp; hlint` instead of running them in parallel?
Yes you're right. `Nothing` is the monoidal identity of `Maybe` values. But, regardless of the cause, `Maybe` is still commutative. Since there are only two possible values, the only way Maybe could not be commutative would be if: Just &lt;&gt; Nothing /= Nothing &lt;&gt; Just But as you pointed out we actually have the monoid identity law which proves this false: Just &lt;&gt; Nothing == Nothing == Nothing &lt;&gt; Just
I think I needed incoherent for the instances to fire on uninstantiated type variables. I tried overlapping and the typeclass resolution would halt (unless I misunderstood something). My conclusions was overlapping is enough to separate Int from (a,b) but not a’ from (a,b). Does that make sense?
TH is just a macro language. It reads and writes Haskell code. A feature like this would have two parts: * A quasiquoter that assigns textual names to the arguments of a function. The result of this step is a `Map Text Int`. * A quasiquoter that creates a curried invocation of a function from a list of pre-specified argument values using the textual names. This would first lookup up the textual names in the map created in the first step, then create the curried invocation. To be a bit more concrete, here's a simplified example of how you might do the last step. This function takes as input a function name, and a list of pairs of argument position and value to be used at that position. The output is the code for a curried function invocation. For simplicity, the input and output Haskell code are strings; in real life it would be TH expression AST components, but the code is essentially the same. mkCurry :: String -&gt; [(Int, String)] -&gt; String mkCurry fun args | null vars = unwords defBody | otherwise = unwords $ defHead ++ defBody where defHead = "\\" : vars ++ ["-&gt;"] defBody = fun : mkArgs 0 args vars mkArgs lastArg ((n,val):as) vs = let (h, t) = splitAt (n - lastArg - 1) vs in h ++ val : mkArgs n as t mkArgs _ _ _ = [] vars = take totalVars ['v' : show k | k &lt;- [1..]] totalVars = maybe 0 ((\n -&gt; n - length args) . fst . NE.last) $ nonEmpty args Sample usage: &gt; putStrLn $ mkCurry "foo" [(2,"42"),(3,"72")] \ v1 -&gt; foo v1 42 72 &gt; putStrLn $ mkCurry "foo" [(1,"42"),(2,"72")] foo 42 72 I will again 100% agree that doing this with a type-level computation instead of TH would be much better, in many ways. The OP library is an amazing example of how that might look. But currently it's so awkward and difficult to write even simple code like this at the type level that a TH-based solution is still probably the best way to implement a feature like this in real life.
I don't know why people seem against learning theoretic stuff first. I think it is quite nice to dive into that. It is probably not the quickest way to learning haskell in order to write some apps but if you are interested in maths and so on you may find that journey enjoyable ! I find very interesting and I like Haskell a lot because it is closer to the math. I got interested in Haskell after being interested in the theoretic stuff. Probably some other languages like idris are still closer but Haskell is nice, and there are some cool resources out there! The youtube series of Bartosz Milewski are great fun to study!
Ok, even running them in parallel doesn't address the other points mentioned. At worst, this is as slow as running them in parallel. I still don't see why you say "this is less useful than running it outside GHC". To me, it's more useful having an single stream errors and warnings to fix.
I haven't tried ghcid yet. Recently I've been using \`stack build --fast --file-watch\` which I believe does something similar. Is that right? Can someone comment on differences between using ghcid vs. the above stack command?
Thanks for the reply. The thing is that I like math so that is also one thing why I want to learn also theory behind the Haskell. I will check the YT series of Bartosz. Many people agree that it is good source of information.
It's useful for the case of a large system or ecosystem written in a dialect of Haskell that assumes a core set of extensions across all modules. It avoids errors that are sometimes hard to decipher when you forget to specify one of the core extensions in some module. I personally don't prefer setting things up that way to begin with. But if you come into a project like that, you don't have a choice. And this is quite common. So I think OP is correct for including it.
Awesome stuff. Too bad we may not be able to use this (not using beam in production).
Wow, I’ve been using the Real World Haskell book for the past month to try and learn Haskell and have been very frustrated the whole process due to the mistakes and arguably steal learning curve on the exercises. Knowing that there is an online version with comments is awesome! 
Join us in building the cyberpunk dystopia you dreamed of in your youth!
Does you hire remotes from outside US, or it domestic only?
Summer internships are already most the way though, you should have heard back by now, I'll ask the coordinator.
Oh. This was for next year that I applied :) (2019!)
Yikes. Makes CCTV look tame.
I decided to dump some quotes from /u/goldfirere's [thesis ^[PDF]](https://cs.brynmawr.edu/~rae/papers/2016/thesis/eisenberg-thesis.pdf) about **BAKE** (type checking / inference / elaboration algorithm, translates Dependent Haskell to the new DT intermediate language **PICO**) &gt; The novel algorithm **BAKE** (Chapter 6) performs type inference on the Dependent Haskell surface language, providing typing rules and an elaboration into **PICO**. I am unaware of a similarly careful study of type inference in the context of dependent types. &gt; As elaborated in Chapter 6, Dependent Haskell retains important type inference characteristics that exist in previous versions of Haskell ... In particular, all programs accepted by today’s GHC—including those without type signatures—are also valid in Dependent Haskell. &gt; ## 3.3.2 Backward-compatible type inference &gt; &gt; Working in the context of Haskell gives me a stringent, immovable constraint: my work must be backward compatible. In the new version of GHC that supports dependent types, all current programs must continue to compile. In particular, this means that type inference must remain able to infer all the types it does today, including types for definitions with no top-level annotation. Agda and Idris require a top-level type annotation for every function; Coq uses inference where possible for top-level definitions but is sometimes unpredictable. Furthermore, Haskellers expect the type inference engine to work hard on their behalf; they wish to rarely rely on manual proving techniques. &gt; &gt; The requirement of backward compatibility keeps me honest in my design of type inference—I cannot cheat by asking the user for more information. The technical content of this statement is discussed in Chapter 6 by comparison with the work of ... A further advantage of working in Haskell is that the type inference of Haskell is well studied in the literature. &gt; Even disregarding `let`-generalization, *BAKE* is the first (to my knowledge) thorough treatment of type inference for dependent types. My bidirectional type inference algorithm infers whether or not a pattern match should be treated as a dependent or a traditional match, a feature that could be ported to other languages. &gt; What may be more surprising to the skeptical reader is that a Π-type is inferred, especially if you have already read Chapter 6. However, I maintain that the **BAKE** algorithm in Chapter 6 infers this type. .. Note that the type inference algorithm infers only relevant, visible parameters, but these arguments are indeed relevant and visible. .. As further justification for stating that **BAKE** infers this type, GHC infers a type quite like this today, albeit using [*singletons*](https://hackage.haskell.org/package/singletons) &gt; replicate :: Π n -&gt; a -&gt; Vec a n &gt; replicate Zero _ = Nil &gt; replicate (Succ n') a = a :&gt; replicate n' &gt; &gt; ## 4.4.2 Inferring `Π` &gt; &gt; The discussion of quantifiers in this chapter begs a question: which quantifier is chosen when the user has not written any? The answer: `→`. Despite all of the advances to the type system that come with Dependent Haskell, the non-dependent, relevant, visible, and unmatchable function type, `→`, remains the bedrock. In absence of other information, this is the quantifier that will be used. However, as determined by the type inference process (Chapter 6), an inferred type might still have a `Π` in it. For example, if I declare &gt; &gt; replicate' = replicate &gt; &gt; without giving a type signature to `replicate'`, it should naturally get the same type (which includes a `Π`) as `replicate`. Indeed this is what is delivered by **Bake**, Dependent Haskell’s type inference algorithm.
Thanks! This is primarily intended for exploratory usage; what do you currently use when you need to interactively explore Postgres data? 
Microsoft Research in Cambridge is an amazing place, and Cambridge is one of my favorite cities. I'd apply if I were qualified enough.
If you think the only problem with palantir is that the body count isn’t sufficiently directly attributable.
&gt; My understanding is that maintaining type inference that exists is a high priority in the DH development. Well, somewhat. **Nothing** that currently infers is supposed to break. However, implementing type inference for new features, not so much. It's understandable since there's some not-so-pleasing results about the complexity / decidability of dependent type inference. But, no one is claiming that using the new features won't imply adding some amount of "type noise", and there's parts of the community that prefer inferred types so much they will avoid forms where type annotations are required. &gt; I also find that in my daily development, there are a lot of things that would be easier with features that are on the road map for DH I'm sure there are. I don't doubt they are improvements. I doubt they are of the highest priority for even the majority of the GHC/Haskell users. It's still a small team pushing them forward; and I think they might appreciate additional effort, if DH is *your* priority. I want Idris and Agda style "term inference" / proof search. By picking my types well, I can get the complier to not only accept, but actually generate my programs. In addition, because of totality I can prove invariants about my programs as long as the compiler is correct -- which I have to trust in for any translation of a HLL to binary. I'm not going to get that in Haskell. So, my Haskell code tends to swerve away from those features entirely. I barely use GADTs these days.
Reflex gets used with a strongly integrated UI library (reflex-dom), where you build up your interface along with the logic, and the interface changes as your application state changes. Reactive banana I think has in general much less use because of this. It provides ways which you can hook into existing UI frameworks - for example gtk, but IMO this is not nearly as convenient. Reflex is also much faster (at least it used to be) and more focused on performance than Reactive-banana.
&gt; 1. The current common ways of working with types still work fine unchanged, including type inference. Yes. Modulo some renamings (* -&gt; Type, for example), everything *sound* that we've done with types will still be available. &gt; 1. Some new ways of making types more expressive and precise will become popular - but only those that are just as simple to use and understand, including type inference. &gt; 1. New amazingly powerful techniques, orthogonal to our current uses of types, will become available, such as: Write a specification of requirements for a function using a language at least as powerful and simple as quickcheck+hspec, and have it verified at compile time. We'll see on these; I think requirements specification is a pipe-dream in Haskell, unless we somehow carve out a total subset or figure out other ways to give as least limited totality assurances.
You should hear back soon, another round of reviews is eminent.
I would really recommend reading LYAH as the first book over Real World Haskell -- it's outdated but way more digestable IMO. 
Do you require experience with cryptography and distributed systems? I'm [ok at Haskell](https://sjakobi.github.io/blog/2018/08/14/hi-haddock-3/) but I know virtually nothing about cryptography and distributed systems.
Another point in this design space is `yesod devel`, provided by [yesod-bin](https/hackage.haskell.org/package/yesod-bin). No, it's not just for yesod, you can use it easily for any Haskell project. It has no dependencies on anything specific to yesod. It has features that make it especially nice for web apps. Instead of `ghci`, it runs `stack build` each time you change a file, then it re-runs your application. For web apps, you get a "rebuilding" page in your browser during the rebuild, then your app comes up again in the browser automatically. This is a mature and stable tool, in use by many web devs for years. It would be interesting to hear comparisons with some of these other newer tools for the specific case of web dev.
Haddock in GHC 8.4 is already panicking for me even without your fork when using GADTs and type families .. wondering if it's related to the panicks you're experiencing. https://github.com/VictorCMiraldo/generics-mrsop/issues/33
Shipping minimal GHC! Yes!
Is this supposed to be a metaphor or a joke?
A joke. We don't perform any sort of head-to-head hiring.
I [Haskell Programming from First Principles](https://www.goodreads.com/book/show/25587599-haskell-programming) is probably the best introduction. RWH is showing it's age. It's what I learned from, but -- unless the second edition came out when I wasn't looking -- many of the examples just don't work anymore. LYAH is okay. I think exercises are important though; it's too easy to go through LYAH and get a sense of false understanding.
I think it's probably the best Introduction to Haskell book available. As with any language ecosystem, there's going to be plenty left to learn after you finish, but I think it's a really strong foundation to start with. Parallel and Concurrent Programming with Haskell is a nice second book, if you are at all interested parallelism or concurrency.
For sure I am. I am willing to go through a lot of things. I've got one year to learn this language and write thesis about it then. :-) 
I had the subject on my university which was dealing with Lambda calculus, Lisp and Prolog so at least I've got something to build on. :-D
Lumi (r/https://www.lumi.com) is hiring in Los Angeles or remote (within the USA). Lumi is making packaging simpler for e-commerce brands. We're solving supply chain problems, bringing elegance to the complex systems of pricing, manufacturing, shipping and freight in the packaging industry. Our engineering team is fully remote and oriented towards functional programming. We use Haskell on the backend and PureScript on the front-end. You can read a bit more about our stack here: * [https://medium.com/fuzzy-sharp/purescript-and-haskell-at-lumi-7e8e2b16fb13](https://medium.com/fuzzy-sharp/purescript-and-haskell-at-lumi-7e8e2b16fb13) * [https://medium.com/fuzzy-sharp/migrating-to-postgres-2dc1519a6dc7](https://medium.com/fuzzy-sharp/migrating-to-postgres-2dc1519a6dc7) We've got a current opening for Database Administrator, and frequently posting new jobs here: [https://www.lumi.com/jobs](https://www.lumi.com/jobs)
I've found reactive-banana much more pleasant to use for dorky command-line apps, which is all I write. Here's a piano that segfaults sometimes https://github.com/mitchellwrosen/alicia-banana
I just can't like this company. Looks like such a waste of intelligence. Like, how are these things good to mankind? Actually, do people actually care about mankind? I mean, keep constructing our surveillance/killing robots. Is it a nice hard project? Hell yeah. Is it worth? Not sure. It's as if most engineers are (paradoxically) not smart enough to have this type of critical thinking. I'm sure I'm just overreacting though. Everything is fine.
Indeed. To be precise, they will see the latest checkpoint plus the latest events, which is the same as any external tool would be able to do, regardless of the backend. Creating a checkpoint will definitely be expensive! It is expensive with the FS backend as well of course (and with `acid-state`). On the other hand there is the possibility with a database to track changes within segments and only incrementally update the tables when checkpointing, which is not realistically an option with a file system solution. As to the overall gain, I think the database backend would only really fit a use case where either/and 1) you wanted to use some existing database tools 2) there was external pressure (client/management/deployment etc) to use a specific database In the latter case, using `acid-world` as opposed to a conventional database package would be appealing only in so far as you like the idea of working with events and state monad updates. I would guess that most Haskellers do find this appealing, as it is a simple and elegant abstraction - I haven't come across much criticism of `acid-state` when it comes to the basic concept or interface.
I envision something along the lines of subtyping as in Liquid. With help from the compiler - like, "compile this function - of mine or in a library - as if its type had the following extra parameter". Again with help from the compiler, it should be possible to include totality proofs for most functions. Like I said, a dream. Well beyond the first pass of DH. But in principle, I think it might just be possible.
You think someone Peter Thiel is behind this one too?
Inferred refinements would be neat.
["Anduril’s lead investor is Founders Fund, the VC firm headed by Peter Thiel"](https://www.wired.com/story/palmer-luckey-anduril-border-wall/)
Ok, regarding the other points: \&gt; printing stuff from multiple plugins that run on multiple threads without producing garbage is not-trivial. This \*should\* be trivial by having a thread responsible for printing. I'm unable to use all cores, even when compiling multiple packages in parallel using \`stack\` (yes I know it doesn't do \`ghc --make\`). The inability to use all cores is becoming a bigger and bigger issue. Thus doing parsing twice seems like a lesser evil than depending on serial execution of two plugins. That's why I said "this is less useful than running it outside GHC". Again, the most important issue seems to be the ability to use all available cores, \*not\* saving CPU.
Can't do much better?
We aren't hiring remote developers at this time; access to the hardware is necessary for most tasks. Candidates must be qualified to work in the United States and able to work from our lab in Orange County, CA.
Great post! :-D I'm liking this series and look forward to the next instalment! Can you efficiently add two bit-strings? Maybe you can add in blocks of 64bits (as unsigned ints/Word64) and manually carry overflows?
You'll be surprised with how many things you can do with the Holy Trinity of map, filter, fold.
Yep, that's exactly what I do.
Bad news from me (I am from Europe), but good for someone in OC. Thank you for reply.
do you have a link to a project of yours building an event loop on reactive-banana? I've been working off the link below, but i'd like to try out different FRP frameworks https://github.com/deech/fltkhs-reflex-host/blob/master/src/reflex-host.hs
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [deech/fltkhs-reflex-host/.../**reflex-host.hs** (master → eb3516a)](https://github.com/deech/fltkhs-reflex-host/blob/eb3516ae9edfba8a2f686c14654eefa878d0a332/src/reflex-host.hs) ---- 
What else is on you wishlist after relocatable and minimal ghc distributions? ;-)
Based on my experience: Whatever you do next :-)
In pure functional programming everything is an expression. You are able to abstract a lot more. This is great mostly to reason more easily about code. Code is written to be read. As a community you can share frequently reoccurring patterns and directly know what it means when you see it in code from someone else. Not only will you compact your code once you use folds and other higher-order functions, but it will help you understand code from other people much better and faster.
In particular, it really changed how I thought about programs in Haskell when I realized the wealth of equations that had solutions. For instance, functions aren't the only thing that can be fixed, everything fan be fixed, and the result is a non-trivial term that satisfies the given equation...it's super powerful.
What are the use cases where one might be more suitable than the other?
https://github.com/ghcjs/ghcjs/tree/ghc-8.4 exists, i suppose you should contribute!
Sorry to hijack the topic, but I'm interested if anyone can provide a comprehensive comparison between PureScript and GHCJS as well from a Haskeller's perspective :D
+1
You can watch this video from YOW! conference. The company uses Haskell for full-stack and they use GHCJS a lot. You can also hear benefits of this approach: * https://www.youtube.com/watch?v=riJuXDIUMA0 And here is the platform: * https://github.com/obsidiansystems/obelisk 
I don't. I've only dabbled and I tend to delete things I don't like. Roughly, you call `Control.Event.Handler.newAddHandler`, and it gives you two things: * an `AddHandler a` that you pass to `Reactive.Banana.Frameworks.fromAddHandler` to make your `Event a` (inside the `MomentIO` monad where you build your FRP network), and * a `Handler a` (`~ a -&gt; IO ()`) which you feed in `a`s to make the event fire. To do output, you call `reactimate` on values of type `Event (IO ())`, which performs them when the event fires. Once you've built your network, you call `actuate` on it, which starts it running in a background thread. Then you write your polling loop which listens for input and feeds it into the FRP network.
I actually have a screenshot of code that I wrote in \`reactive-banana\` awhile ago, which illustrates @char2 's points on using \`newAddHandler\` Here goes: [https://imgur.com/a/PG8tfeQ](https://imgur.com/a/PG8tfeQ)
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/Tu3POGR.png** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) 
So basically a network of observable streams and we attach handlers that mutates the DOM thru reflex-dom's API by hand to the network and the handlers will react accordingly to network changes. In other words, just the reactivity implementation without the vdom part built. Did I get it right? What about the other question - is reflex-dom? reflex? specific to browser technology then? (Or at least the usual use case is to build an interactive \*HTML\* page?)
May I ask you to clarify, what does "SPA" stand for?
https://en.wikipedia.org/wiki/Single-page_application
Reflex is not specific to browser technology, there have been other UI embeddings - it's just that the author (Ryan Trinkle) and others use it for web development, so this aspect has received most attention, it's probably the most widely applicable use. There's been a big effort to make it play well with ghcjs. There could easily be a reflex-gtk in a similar style, but it does take quite a lot of work to write a wrapper, something like gtk is not a small API! In the past I've written a prototype widget system with gloss, just simple things like buttons, animations etc. A 'native' reflex UI system would be really neat, maybe Cario based?
Last I heard, there was serious work going on to port GHCJS to [WebAssembly](https://webassembly.org/). What is the status of that?
SQL compiler, optimizer, build system, GPU code generation (thrust/CUDA), SQL client, and more and more....
There's also Phil's recent post talking about using GHC Generics to easily share data types and work with those generically between Haskell and PureScript https://www.reddit.com/r/haskell/comments/955b2t/purescript_and_haskell_at_lumi/, since you can really generically do anything you want with PureScript types (especially with solved row type classes). I see a lot of complaints that Aeson JSON instances don't match any PureScript instances, but I think things people should consider doing is to actually go through making their own encodings on the Haskell side and learn how to use generics-rep and row type classes in PureScript (e.g. https://purescript-simple-json.readthedocs.io/en/latest/generics-rep.html)
From what I have overheard on IRC, they've produced working webassembly binaries the unregistered C backend of ghc, but apparently it produces enormous binaries - so they're currently working on using the LLVM backend directly. /u/elvishjerricco can probably elaborate!
&gt;An operad is an abstract mathematical tool encoding operations on specific mathematical structures. It finds applications in many areas of mathematics and related fields. This snapshot explains the concept of an operad and of an algebra over an operad, with a view towards a conjecture formulated by the mathematician Pierre Deligne. Deligne’s (by now proven) conjecture also gives deep inights into mathematical physics.
Nice! I will try it out.
Last I heard, there was serious work going on to port GHCJS to [WebAssembly](https://webassembly.org/). What is the status of that?
I find the relation to computer algorithms a bit of a stretch. The writer starts with: &gt; Let Pn denote the list of all these functions that take inputs x1, ..., xn of a ﬁxed type (for instance this could be the type “number”) and spit out a single output y = P(x1, ..., xn) of the same type. But that is not very specific to computer algorithms. I have seen this claim before (that operads are useful for compsci). But what uses do operads actually have in compsci? (This is an honest question I have for a long time.)
I've never been a big fan of the library level row type implementations in Haskell so I've never experimented with using them with Opaleye, but there shouldn't be anything preventing you from doing so.
&gt; But does it need to distinguish between different ADTs if they're of the same shape? The docs said nothing, so we can't rely on them being undistinguished. I presented the exact contents of the info table because I thought it would be a useful mention. I guess it lacks way too much explanation than I thought it would..
&gt;questionable value when all types are inhabited. There is a proposal about an extension that allow for unlifted types that could be not-inhabitated: [https://ghc.haskell.org/trac/ghc/wiki/UnliftedDataTypes](https://ghc.haskell.org/trac/ghc/wiki/UnliftedDataTypes)
It's a joke about \[Évariste Galois\]([https://en.wikipedia.org/wiki/%C3%89variste\_Galois](https://en.wikipedia.org/wiki/%C3%89variste_Galois)), the company's namesake—he was an incredibly influential mathematician who died at age 20 (!) in a duel.
**Évariste Galois** Évariste Galois (; French: [evaʁist ɡalwa]; 25 October 1811 – 31 May 1832) was a French mathematician. While still in his teens, he was able to determine a necessary and sufficient condition for a polynomial to be solvable by radicals, thereby solving a problem standing for 350 years. His work laid the foundations for Galois theory and group theory, two major branches of abstract algebra, and the subfield of Galois connections. He died at age 20 from wounds suffered in a duel. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/haskell/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
We at [Wire](https://wire.com) are hiring Haskellers and we especially need people with some devops background so you seem like a good fit! We're located in Berlin and we're making an end-to-end encrypted messenger (like Signal, but we specifically try to push secure messaging to big businesses). Our backend is written in Haskell and everything we do is open-source. I wrote a post about what it's like to work here: [Medium](https://medium.com/@neongreen/wire-is-hiring-a-haskell-developer-and-an-operations-engineer-berlin-51e7f3ed3050). NB: since this vacation got published to Reddit (a couple weeks ago), we've got quite a bit more responses than we expected, so please be patient if you don't hear back from us immediately.
&gt; "Flame of the West" wew
Yea the binaries are massive to the point that I consider it a bug. They're two orders of magnitude too large. So we're exploring both fixing that and moving to the LLVM backend with a shimmed implementation of the GHC calling convention.
Hi all. I'm very new to functional programming, currently reading through Learn You a Haskell and working through the early project euler problems for practice. For problem 10 I wrote this prime sieve of eratosthenes, but for some reason it is taking a super long time to generate the first 2000000 primes. I know its probably not written very elegantly, but the implementation of the algorithm felt okay to me: removezeroes :: (Integral a) =&gt; [a] -&gt; [a] removezeroes [] = [] removezeroes (x:xs) | x == 0 = removezeroes xs | otherwise = x:(removezeroes xs) sieve :: (Integral a) =&gt; [a] -&gt; a -&gt; [a] sieve [] _ = [] sieve (x:xs) end | x == 0 = sieve xs end | x*x &gt; end = x:(removezeroes xs) | otherwise = x:(sieve (sieveparse (x:xs)) end) primesbelow :: (Integral a) =&gt; a -&gt; [a] primesbelow x = sieve [2..x] x sieveparse :: (Integral a) =&gt; [a] -&gt; [a] sieveparse (x:xs) = (removeevery x xs (x-1)) removeevery :: (Integral a) =&gt; a -&gt; [a] -&gt; a -&gt; [a] removeevery _ [] _ = [] removeevery m (x:xs) 0 = 0:(removeevery m xs (m-1)) removeevery m (x:xs) i = x:(removeevery m xs (i-1))
I hope that never happens. \`OverloadedLists\` creates so much ambiguity.
Yea I'll admit documentation is lacking. We're a very small effort so we don't spend a lot of time doing things like documentation when IRC has been good for most of our communication :P We have a wiki at https://github.com/WebGHC/wasm-cross/wiki, but it's not very informative at the moment. We try to keep issues logged at https://github.com/WebGHC/wasm-cross/issues and https://github.com/WebGHC/ghc/issues, but again we're small and communicate largely over IRC so some things aren't logged.
That's a very good point. I didn't think about that.
No worries! Thanks for the repo link, I'm sure that'll be more than enough to nerd snipe me for the next couple weeks.
Operads appear to be some form of "controllable monad". (Though this is the first time I've seen operads). Take `data Nat :: Type`, `Vect :: Type -&gt; Nat -&gt; Type`, and `Fin :: Nat -&gt; Type`, with the usual meanings, plus some of their operations. There's an operad to be formed: -- quasi-standard data Nat = Z | S Nat data Vect :: Type -&gt; Nat -&gt; Type where VNil :: Vect a Z VCons :: a -&gt; Vect a n -&gt; Vect a (S n) data Fin :: Nat -&gt; Type where FZ :: Fin (S n) FS :: Fin n -&gt; Fin (S n) type family (n :: Nat) + (m :: Nat) :: Nat where Z + m = m S n + m = S (n + m) type family Pred (n :: Nat) :: Nat where Pred (S n) = n addComm :: forall n m. n + m :~: m + n addComm = unsafeCoerce Refl fmapVect :: (a -&gt; b) -&gt; Vect a n -&gt; Vect b n fmapVect _ VNil = VNil fmapVect f (VCons x xs) = VCons (f x) (fmapVect f xs) appendVect :: Vect a n -&gt; Vect a m -&gt; Vect a (n + m) appendVect VNil ms = ms appendVect (VCons n ns) ms = VCons n (appendVect ns ms) -- operad for Vect type VectO a = Vect (a -&gt; a) vectOId :: VectO a (S Z) vectOId = VCons id VNil vectOComp :: forall n m a. Fin n -- ^ which position? -&gt; VectO a n -- ^ the thing that we plug into -&gt; VectO a (S m) -- ^ the thing that we plug in -&gt; VectO a (n + m) vectOComp FZ (VCons p ps) qs = case addComm @(Pred n) @m of Refl -&gt; appendVect (fmapVect (p .) qs) ps vectOComp (FS i) (VCons p ps) qs = VCons p (vectOComp i ps qs) (This software is distributed, as-is, with no warranty, blah-blah, and might break some operad law.) Note that every `Vect a n` is a `VectO a n`: toVectO :: Vect a n -&gt; VectO a n toVectO = fmapVect const So, when you do `vectOComp i (toVectO xs) fs`, you essentially pass the `i`th element of `xs` to each of `fs`, get a sequence of elements back, and then insert them in place of the original. Sound familiar? If you do this *everywhere*, at all indices, you get something similar to the `Monad` for `[]`. (Note that the usual `Monad` for `Vect - n` is the same as for `(-&gt;) (Finite n)`.). However, it's "controlled" in that I can choose *not* to do it everywhere, and just give a particular index. Note also how `vectOId` seems to know about `return`. The same should work for binary trees with values at the leaves: data BinV :: Type -&gt; Nat -&gt; Type where Leaf :: a -&gt; BinV a (S Z) Branch :: BinV a l -&gt; BinV a r -&gt; BinV (l + r) The usual `Monad` for `Bin` (without the value counting) replaces each leaf with a new tree. The operad you can build here picks a particular leaf and grows a single new tree there. (Slightly annoying roadblocks include getting back from `VectO a n = Vect (a -&gt; a) n` back to `Vect a n`. `fmapVect fix` should work but that feels hacky. Also, `vectOComp` wants the length of the produced `Vect` before giving you the element, while `(&gt;&gt;=) @[]` allows more flexibility. I guess it's a bit *too* controlled.)
That talk actually made me want to use ghcjs + reflex stack. I feel like this could be the breakthrough people are waiting for. How can I contribute?
Thanks! Your answers make things much clearer! I really appreciate it.
A very simple way would be to represent polynomials as lists of numbers, with each index corresponding to a power and eah element to the coefficient. From there, integration is just a simple list traversal function, and application a fold. I am not sure on how to deal with integration constants. But you may overcome that using another type for coefficients instead of `Double`.
I see how this abstraction can be useful, but I'm not sure how it is better than function from a product of types. This can happen in categories without products, but useful examples do not come to mind easily.
PureScript is strict and targetting straightforward JS means it has more stack limitations. The compiler also does not do any general optimizations like inlining for arbitrary code (it only inlines syntactic binds for `Effect`). Lots of the PS ecosystem is designed to be stack safe, and without optimizations, it will always use more heap and pressure the GC. It's possible to write fairly fast `Effect` code (around 1.5x normal JS) if you write it to take advantage of the optimizations the compiler _does_ perform. I would like to see a whole program optimizer/bundler for PureScript code.
Note that this method won't work at all for transcendental functions, unless you want to represent them by their taylor series (which makes things like multiplication/interpreting complicated expressions more involved).
A lot of people are suggesting that you learn the practical stuff first, then delve into the math: and I agree with them. Much of the math and formalisms that make Haskell great are practically motivated. Type classes are cool and spawned a lot of awesome research, but they fundamentally address a practical concern: ad-hoc polymorphism. This is why people suggest you learn the practice first, you see the motivation and how it works, and then later you can look under covers and see the machinery. If you are interested in something that has deeper mathematical ties, I'd look to something like Gallina (calculus of constructions) or Agda (unified theory of dependent types).
Besides WebGHC, there is [Asterius](https://github.com/tweag/asterius) that has the same goal, but takes a different approach on the task.
As you seem to be aware of the "issues" about writing yet another monad tutorial. What is exactly the aim of your ? Is it written only for yourself as way to try getting a better understanding of Monad or is it just yet another monad tutorial (which, as every other monad tutorial, claims to be different, but is not ...) ? Having said that, I'm not sure about what you maen with The term Monad is somewhat misleading. `A better definition would be that a datatype (newtype) has an instance of the Monad typeclass.` Because a Monad is not a type (kind \`\*\`) but a type constructor (kind \`\* -&gt; \*\`).
This stuff is frankly scary and despicable.
GHCJS has first-class support for lazines (that's the primary difference). It also has the full Haskell ecosystem and you can use the same code on the backend and the frontend. PureScript has some nice things on its end though. Because it's not beholden to the Haskell standard, it tidies up some things like the `String` data type. 
Hi! Thanks for your answer. I edited my question, I might need to integrate some transcendental functions
&gt; But its performance is remarkable considering what it has to work with You can also post-process it with various tools, particularly the closure compiler. 
Here's my favorite Haskell wiki page: https://wiki.haskell.org/Libraries_and_tools/Mathematics I'm sorry that I don't know what to recommend. I really think Haskell needs a robust, typed CAS. I've tried making my own for fun, but every time I end up wanting dependent types 
Thank you for your reply. I will probably start with http://haskellbook.com which has good references and then I will do some little projects to see how it is programming in Haskell and then I will jump to pure theory and paralelism. :-)
Thanks! I'll look into it. From what I know there was a project of creating a binding for SymEngine, but it's dead. However the owner seems to be open to add contributers: https://github.com/symengine/symengine.hs/issues/16#issuecomment-413241182
I got a Research Assistent job in autonomous driving and we haven‘t used these things for while (the lab is older). Pretrained CNNs are surprisingly flexible and can be mixed SVMs when less data is available. Of course some applications, which probably involve very weird images, are still out of reach, but it seems to be very niche to me.
&gt; which, as every other monad tutorial, claims to be different, but is not ... Does it have to be the case? The [original post](https://byorgey.wordpress.com/2009/01/12/abstraction-intuition-and-the-monad-tutorial-fallacy/) was about one particular failure mode, in which the tutorial fails "recognize the critical role that struggling through fundamental details plays in the building of intuition". It even clarifies that "not all monad tutorials are like this". Most of the monad tutorials I have seen recently were pretty bad (I haven't read the OP's yet), but I don't think that's because the very idea of writing monad tutorials is misguided. Saying that it's impossible to explain what monads are gives monads even more undeserved mystique! I think it would be more accurate to say that it is easier to write an unhelpful monad tutorial than to write a helpful one.
Hmm, your target audience is beginners who haven't yet grokked monads, but then you tell them about monad transformers and compiling to core and optimizations... isn't that going to be overwhelming?
You could represent the transcendental functions as part of your ADTs. That should make it easy to integrate them.
Thanks. That's pretty interesting.
Please [don't call it the IO monad](https://blog.jle.im/entry/io-monad-considered-harmful.html). In main = do putStrLn "Hello" putStrLn "World" clearly "the IO Semigroup" is enough: `main = putStrLn "Hello" &lt;&gt; putStrLn "World". IO is just a datatype, of which we sometimes use the "Monad interface", if you will. Furthermore, I believe if people know what "Core" is and what "Dictionaries" are, they will most likely already be familiar with `Functor/Applicative/Monad` anyway. Regarding what the perfect Monad tutorial should look like instead, I th
I reckon reactive-banana would be suitable for creating a control system as well. Something like those in scifi movies with a lot of controls and panels showing real-time graph :D Or maybe a building's electricity billing system - sounds very nice to build with reactive-banana :D What do you think? These are just thoughts from a beginner like me :P
What the hell is this sorcery?
What outputs are you getting? I get this one: ``` ###:c ###:b ###:c ###:b ###:c ###:b ###:c ###:c ###a ``` Perhaps I had published an old version of abstract-algorithm? Could you update to `abstract-algorithm@0.1.26` (I just published it)?
I agree that records and modules should be more similar. It would be nice to have a `Recordtype.field` style accessor, just like the 'into a namespace/module' dot operator.
I think what you want to know is more like "how Haskell is made", which is the programming language theory behind it. I would totally suggest what you are trying to do to anyone that are interested in Haskell. The math behind it is very well-thought and can guide you to a whole new world in computer science. An important thing is, you don't need to understand all the math behind to use it, at the same time, I don't think most people feel like to learn the math despite they are using Haskell, and that's fine. Functional Programming has been an old field in mathematics before the appearance of modern computers, which means you don't actually need to know Haskell to learn it either. It combines Logic, Type Theory and Computation Theory, and became the great power that made Haskell such a powerful language. I'll just throw some names of books here: "Types and Programming Language" is a nice introduction to programming language theory from a programmer's perspective, "Proofs and Types" has more mathematical details and greater picture of the basics of programming language theory like Logic, Lambda Calculus and Type Theory. And "Lectures on Curry-Howard Correspondence" covers all the important formal math proofs behind. And that's, imo, to "Learn Haskell the hard way", but I think learning programming language theory gives me a greater look on what programming languages can and can't do, and it also gives me a full understanding on what Haskell really is. I hope these are what you are looking for.
Is this just an effect of precomputation? I.e. I can compute (smallish) Factorials in O(1) if I'm allowed to precompute a table of Factorials.
Absolutely not at all. There are zero computations or pre-optimizations. The algorithm is actually logarithmic. The magic is on the optimal sharing.
I still get a much longer result if I remove the `::copy` line, so that the last three lines are ::add 8 42b ex_8
I think there has been a proposal for that kind of thing in the past, but people were reluctant to overload `.`. I would've preferred it, since every time I see `#`, I think Fortran's `%` looks prettier...
Seems pretty cool. I don't know if I would be able to code in this language though. Stuff like this: @copy #n ::n #R #xs ::::xs #xs #R :B0 :R xs #xs #R :B1 :R xs #R End R #x x is pretty intimidating.
This isn't just the laziness that makes `head . sort` O(n) when `sort` is O(n lg n) and `head` is O(1), right? It seems like you could even get this sort of weird behavior in GHC, if you had something where printing the head also caused something in the tail to reduce because it was shared, maybe?
It is not really because `copy` returns the entire result, i.e., it doesn't affect the output at all, as opposed to `head`, which returns only one element and thus may avoid doing a lot of computations. I'm not sure if the same behavior can happen with Haskell. That would be an identity function `f : A -&gt; A` that, for a value `x : A`, causes something like `print $ f x` to run much faster than `print x`. 
[removed]
Can you elaborate on what optimal sharing entails? I skimmed the introduction of the article but I couldn't really find an explanation of its ideas
When working with DuplicateRecordFields, (which seems quite usable using a combination of NamedPuns and Generic lenses) this is most welcome to avoid continually shadowing selectors (and avoiding the weird type errors when accidentally using a selector)!
Currently performance of `hip` is sub-optimal and fusing is very much dependent on the vector or repa backends. The good thing is that I am in a process of migrating `hip` to a new very fast array library: https://github.com/lehins/massiv which will drastically improve the performance. That transition will also enrich `hip` with it's own fusion handling. So stay tuned, new version of `hip` will be coming some time later this year ;) As far as performance investigation of algorithms introduced during GSoC, nothing was done at all. In fact, I am sure they are very slow. :) But I do expect that will get fixed during the transition.
I think it's implied that there's an error involved. Since applying copy is semantically equivalent, it's a valid reduction strategy for its arg
If copy x = x semantically, then adding copy is a valid reduction operation. Since the abstract algorithm is optional, it should have already done this. Unless in the definition it's only allowed to shrink expressions.
It does not perform any program transformation. It is optimal in the sense it performs the least amount of beta-reductions to reduce a specific λ-term, but it doesn't magically fix an inherently slow algorithm. The cool observation is that the λ-term `copy(add(N,x))` is actually asymptotically faster than `add(N,x)` - i.e., it can be computed in less beta-reductions!
&gt; Well, let me explain carefully. Just kidding. I have no idea Hahaha, got me there! I was so excited for a second. I can't even understand the [answer](https://stackoverflow.com/a/31724886/2682729) here to one of your SO questions, maybe someone can make more sense of it and come with an explanation?
Sources?
So many questions, but I think the biggest one is likely one you can anticipate: I assume that there's a structure/syntax-directed canonicalization of the interaction graph that will increase sharing of common "substructure" (I'm unsure what the terminology here is, but I hope you get what I'm trying to say). Is this canonicalization cheap to perform? Do we perform a single step of of this process after each reduction in a lock-step manner to make this online? A follow-on: is there any bound on the growth of the size of the expression as we perform this canonicalization? And in the canonical form, is there a single redex at every step that can be taken, or do we still get the same guarantee via confluence?
So wait, does this mean the same function will never be applied to the same argument twice? Doesn't that mean that if there's a chance a function will be applied to the same argument more than once, it'll have to hold the result in memory until it knows it won't happen again? E.g. in a program that's doing a lot of matrix multiplication, doesn't this risk blowing up memory conception, even if the program was intended to run in constant space?
Still, copy doesn't have *negative* complexity, just somehow a *fractional* complexity. Like copy behaves as if it were O(1/n) or more likely O(1/g) where g is some function not of the size of the input but of the "redundancy" of the input.
Sorry, I don't understand your first questions very well. Common canonicalization? Single step of this process (which) after each reduction? Could you rephrase? But perhaps I can answer your question by just explaining how it is done. The λ-term is transformed into a graph. It the λ-term is not in normal form, the graph will have "active pairs", i.e., pairs of node that must be rewritten. Those nodes are, thus, rewritten until there is no active pair left. Each rewrite either deletes the two nodes, or duplicates them, so it is a constant time operation that may grow the graph by at most 256 bits (two nodes). Rewrites are confluent, so you can pick any order and it won't affect the final count. [Here](https://imgur.com/a/DP9rAgm) are some illustrations. Does that help?
Oh, fair enough. `add` behaves as if it had `O(1/n)` complexity on that composition. I was wrong by calling it negative. Although it was just a silly analogy anyway so I'm not sure I should edit the post at this point.
My bad, [this](https://www.amazon.com/Implementation-Functional-Programming-Languages-Theoretical/dp/0521621127) book. I've edited the article now.
Sure. It's at cool and confusing result nonetheless. Thanks for sharing.
I can't recommend [this book](https://www.amazon.com/Implementation-Functional-Programming-Languages-Theoretical/dp/0521621127) enough. It explains those things much better than myself and has all the sources and proofs you might ask for.
But my question is, if that term can be computed in fewer beta reductions, and this algorithm is optimal in the number of beta reductions, and the head normal form of each is equal, then why does this algorithm not produce the faster reduction?
It should be noted that `inc` is an amortized constant operation; naively evaluating `add` (starting at 0) would result in linear performance, not linearithmic. Also, I think your `inc` snippet has B0 and B1 switched. Does the choice of `ex` affect running time? For example, if we use all 1's instead (especially since I think they are backwards) do we see a result that's more-expected (i.e. linear)?
My naive understanding is that this algorithm simply prevents duplicating beta reductions; it does no program transformations to change which beta reductions must be performed. Somehow the `copy` version of this function changes the set of beta reductions that are required. It can't reduce the non-`copy` version to this because the algorithm does not change the set of beta reductions that have to be performed; it just deduplicates the execution. I don't at all understand how it is that Haskell style laziness requires extra duplication though
Could you provide an example where Haskell's laziness requires unnecessary duplication of work? I'm not really wrapping my head around it.
By the way, just wondering. If we had the ability to cause the repeated application of any constant-time operation to have logarithmic complexity, could that be used to solve any problem faster than currently known?
\&gt; I don't at all understand how it is that Haskell style laziness requires extra duplication though Haskell will have duplications for eg, \[ f x, f x \], barring compilation optimizations, or even in the case of \[ f x, f y \], there may be some duplication in the body of f that won't get caught. Basically, this will share anything even in very disparate expressions, whereas haskell will only share an expression given a name (and the same name even, although internal parts might be shared).
I don't remember out of my head, but there are examples on the first chapters of [this book](https://www.amazon.com/Implementation-Functional-Programming-Languages-Theoretical/dp/0521621127). I believe you can find a free PDF in some places...
How are they different? Copy is just the identity function so it could be optimized away as a noop!
I don't understand the argument in &gt; ## 4 Costs and Drawbacks &gt; &gt; This might cause some confusion that record fields can't be accessed by toplevel selectors anymore - however, that shouldn't be too big of an issue, because some library authors already stopped exporting these selectors so they don't have to break downstream software on record changes.
I don't think so. I'm reading a bit about this now, and I don't think the algorithm has anything to do with deduplicating the user's terms; it just disallows the reductions from introducing duplication. I *think* a better example would be if `f` had a constant term in its body that could in theory be let-floated out. With GHC, we rely on program transformations to do this, but it sounds like this algorithm will give the effects of let-floating using just the reduction of interaction combinators? /u/SrPeixinho does that sound at all right?
This may be a stupid question, but what is the purpose of `type`? For instance, the following code compiles without problems, so I don't see the point of using type, other than making the code more readable: type EmailAddress = String type Name = String printEmailAddress :: EmailAddress -&gt; IO () printEmailAddress = putStrLn main :: IO () main = do let myName = "device_nr_zero" :: Name printEmailAddress myName return () 
Could you be more explicit?
Thank you for your complex reply. I really appreciate it. :-)
I thought he was saying it’s about the stuff inside a function, not laziness: whatever is “inside” f, when you call `f x` and `f y`, the “inside” of f will be computed both times.
I haven't read the paper yet to understand the abstract evaluator, but even if the graph mutation is O(1) does the rewrite operation take long as it could have to traverse all the nodes to find which one to rewrite? 
Firstly, the `Applicative` function are useful for simplifying. For example, `liftA2` is: liftA2 :: Applicative f =&gt; (a -&gt; b -&gt; c) -&gt; f a -&gt; f b -&gt; f c liftA2 f x y = f &lt;$&gt; x &lt;*&gt; y Which is similar to this (but a bit more general): liftA2 f x y = do rx &lt;- x ry &lt;- y pure (f rx ry) Which means that: x1 &lt;- fibS (n-1) x2 &lt;- fibS (n-2) let res = x1+x2 Can just be: res &lt;- liftA2 (+) (fibS (n - 1)) (fibS (n - 2)) Some lens-fu might also be handy. For example, calcS can be written like this: calcS :: Int -&gt; State Memo Integer calcS n = do res &lt;- liftA2 (+) (fibS (n - 1)) (fibS (n - 2)) at n &lt;?= res Or even this: (A bit overdone but still quite clear IMHO) calcS :: Int -&gt; State Memo Integer calcS n = liftA2 (+) (fibS (n - 1)) (fibS (n - 2)) &gt;&gt;= (at n &lt;?=) (The [full `lens` package](https://hackage.haskell.org/package/lens) is quite large, but [`microlens-mtl`](https://hackage.haskell.org/package/microlens-mtl) and [`microlens-ghc`](https://hackage.haskell.org/package/microlens-ghc) together provide the functionality needed here.)
Unfortunately, the bookkeeping to maintain the redex sharing takes exponential time: [Optimality and inefficiency: what isn’t a cost model of the lambda calculus? 1996](http://doi.acm.org/10.1145/232627.232639) It won [an ICFP award](http://www.sigplan.org/Awards/ICFP/#2006) for most influential paper in 2006: &gt; Julia Lawall and Harry Mairson’s 1996 ICFP paper “Optimality and inefficiency: What isn’t a cost model of the lambda calculus?” exposed a fundamental problem with proposed algorithms for optimal reduction. Starting with Jean-Jacques Lévy’s seminal work in 1978, the goal of optimal reduction was to correctly normalize lambda-calculus terms without duplicating redexes. Various strategies were subsequently devised to realize optimal reduction, notably the solution of John Lamping at POPL 1990, then simplified and improved by Georges Gonthier, Martín Abadi, and Jean-Jacques Lévy at POPL 1992. Each solution used subtle bookkeeping mechanisms to control sharing. Lawall and Mairson showed that these bookkeeping mechanisms introduced a complexity and inefficiency of their own. They discovered terms that could be normalized in linear time, but whose bookkeeping costs required exponential time. They further showed that Frandsen and Sturtivant’s cost model for lambda-calculus reduction, presented at FPCA 1991, needed to account for the size of intermediate terms, and that optimal-evaluation interpreters were at least exponentially slower than the proposed cost model. Lawall and Mairson concluded that the notion of optimality did not necessarily coincide with that of efficiency. As a consequence, different and possibly optimal evaluation strategies were still needed, as were more realistic cost models. Subsequent work in this area has focused on such cost models, on further analysis of the inherent complexity of optimal reduction, and on relaxing the optimality condition in exchange for lower bookkeeping overhead and greater overall efficiency. [Previous reddit discussion](https://www.reddit.com/r/haskell/comments/2zqtfk/why_isnt_anyone_talking_about_optimal_lambda/) 
Thanks! I've heard of lenses but I'm not familiar with them yet. I will come back to your examples once I start getting to know them more.
Thanks for writing this example. I really like the analogy you make with monads, that's a good point. The roadblock of going from `VectO` to `Vect` is not surprising, isn't that the same for monads? (We generally don't have `m a -&gt; a`). Maybe all you need is a `runVectO` function. Also, this really resonates with me: &gt; The question should be "how can computer science use this abstraction," not "how can we instantiate this abstraction to describe computers". 
When can we have this as a module-level ghc plugin?
Actually, from what I see in that discussion, it seems like the abstract algorithm avoids bookkeeping altogether? Isn't that's what's meant by oracle-less? 
&gt;"Nearly 90 Percent Of People Killed In Recent Drone Strikes Were Not The Target" What's the percentage for strikes where there was a pilot in the cockpit?
Had you asked for numerical integration, I would've recommended [`integration`](http://hackage.haskell.org/package/integration).
I suggest contacting Levent, the package developer, directly. He's been very responsive to my questions and requests in the past.
Consider the following Haskell expression: let f x = (\y -&gt; y) x in f 4 + f 5 The function `f` is used twice. In Haskell the beta-redex inside of `f`, i.e. `(\y -&gt; y) x`, will be reduced twice, but in Absal not.
&gt; I assume that there's a structure/syntax-directed canonicalization of the interaction graph that will increase sharing of common "substructure" As far as I understand, OP never **increases** sharing, they are just very careful not to lose it.
To make it clear: so does the cost of copying terms in absolutely any other evaluation strategy. There is no known algorithm that performs better.
Don't we have this already? Let's define `c n f` by `c 0 f = id` and `c (n + 1) f = f . (c n f)`. Then `c n f x` can be computed in O(log n) time, by the binary multiplication trick: * Note that `c (2n) f = (c n f) . (c n f)` * Form the sequence `c 0 f, c 1 f, c 2 f, c 4 f, c 8 f, ..., c 2^(floor log2 n) f`, by computing `floor (log2 n)` compositions. * Compose the functions `c (2^i) f` from the above list, whenever `i` is a 1 bit of `n`.
I've started reading this book (this optimal evaluation strategy looks very interesting!) but am finding it very hard going (maybe I don't have enough prerequisite knowledge with a undergrad course in lambda calculi/combinatory logic/types). Do you know of any other more informal descriptions of how this works, so I can get the gist before understanding the full formal algorithm?
No, to visit a node you just pop() from a stack. Rewriting includes adding more nodes to visit because they are always on the neighborhoods of the node you are in. It is a truly constant-time operation as evidenced by the fact Absal performs 30m rewrites/s independent of the graph.