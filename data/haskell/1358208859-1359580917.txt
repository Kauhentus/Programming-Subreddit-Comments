here is a question: what is category theory? Modern math uses concepts like "functor" so pervasively that thinking of them as categorical might not be useful. For me "f-algebra" is a categorical notion (you have an endo functor `F`, and object x, and a morphism `f : F x -&gt; F` such that...). So is "initial." How does category theory free type theory even make sense? But, I don't think this is how you see things. Instead, I assume, "functor" and "f-algebra" are such banal concepts that you think one should not try to think about them as "categorical" at all. Calling an f-algebra "a recursive eliminator" or "a fold" is then a quick way of avoiding clogging your mind with that nonsense you got exposed to in CT class. This might be a reasonable perspective. I might argue that knowing computational complexity theory is not very helpful in PL research. But, obviously a lot of PL research requires you to know what "big O" is. The point being that this is so core in CS that it isn't really worth mentioning.
Data.Reify is a very useful package, but as others have pointed out the problem in the blog post could be solved more correctly with mfix or pregenerating the ids. Where Data.Reify does shine is when implementing a deeply embedded EDSL. Then it can be used to Reify an abstract syntax tree into a graph where sharing is recovered thus opening up possibilities for optimization, and for being able to finitely describe self-referential data structures. I recommend reading the actual paper to see where this might be useful, it is quite readable. 
I'd recommend Scotty. It's minimal, easy to grok, and takes 5 minutes to learn. For something more elaborate, perhaps Snap or Happstack. If you want something more type safe via generated code, use Yesod.
Happstack and Snap are in my opinion more Haskell-canonical frameworks. They don't introduce too much magic from the first line of code, like Yesod does (I mean TemplateHaskell, quasi-quotation and so on.) But all three of them are quite easy to start to play with. The main reason I don't recommend Yesod is too much magic that it provides: you either need to understand it (and that's not as easy as it could be) or you do not learn anything and it's a voodoo for you.
RIM + Haskell == 'avoid success at all costs'
I fully agree "initial f-algebra" is definitely a category theory (CT) concept, but it happens to coincide rather neatly with our intuitive type-theoretic understanding of positive datatypes, their fixpoint construction and the derived eliminators. Moreover, I have a reasonable idea how to define a *dependent* eliminator (and everyone that understand how Coq derives its induction principle would be in the same situation), while last time I looked into this the categorical counterpart was reserved to a select few reviewers of Michael Abbott's PhD thesis (ok, I'm lightly trolling here, but you get the point: dependent eliminators in MLTT or CIC are well-understood, their categorical equivalent is research in motion; in particular I would expect research on (dependent) eliminators to predate any link made between them and categorical models). I'm not on a crusade against category theory, and I'm the first willing to research the history of our research topics to point out the interesting and surprising connections between domain (I was surprised, and impressed, when I found out that the use of categories to structure types had been demonstrated in the very paper in which Dana Scott exposed his topological model for the untyped lambda-calculus). I think I would agree that categories, functors and natural transformations are good notions to know when getting into type theory, just as "big O" is important to understand when doing programming (or analysis) even if we don't ask people to become expert in complexity theory or advanced algorithms. I am however surprised by the way people seem to get convinced around here that learning CT will confer them godlike power, and is necessarily the subdomain of math they *have to* learn if they want to become really familiar with Haskell. I'm impressed by the way the Haskell community managed to make functional programming surprisingly sexy to the rest of the programming crowd, but I suspect there are also risks of excessive marketing creating negative reactions. There already is publicity around programming concepts explained [without resorting to Category Theory or Haskell](http://www.ini.cmu.edu/events/2012/10/ini_oct5.html), are we letting dynamic language proponents reap the benefits of doing things right "without the theory"?
A lot of work gets done in CT and then has the CT details filed off before being presented to a general audience. Not everyone who knows and is interested in questions in a given domain is familiar with CT, so it often wouldn't be perspicuous to use CT jargon in explaining the work. This is one of the nice things about CT as a working theory: it's easy to file off the serial numbers.
s/`f : F x -&gt; F`/`f : F x -&gt; x`/
I'm a Haskell beginner, also interested in learning a web development framework. For me, the fact that a book on learning Yesod is freely available pushed me there. http://www.yesodweb.com/book I'm reading through now and enjoying it.
[insert a stupid joke about the monad police executing your code]
For anyone looking to learn about web programming then I would suggest learning *actual* web programming architectures. Which means that Yesod is probably right out. It's cool but does a hell of a lot of magic, learning Yesod is learning Yesod, you learn less about web programming in general. You also want to learn a fairly normal templating system at first. Nothing that embeds HTML in haskell or haskell in HTML or generates HTML with haskell combinators. These approaches are all awful in practice. I would also start with a smaller framework rather than a big fully featured one if your goal is to learn more. I would recommend Scotty (with Hastache or another sane templating system) but Snap is also a decent choice and has nice documentation. 
I agree you don't need category theory to be a good (or great) Haskell programmer. But I do think it's increasingly helpful to hum a few bars if you want to be a good _computer scientist_. The connection between CT and Haskell in particular is just that as a pure, functional language, it's easier to do theoretical computer science with it than e.g. Fortran. So yeah, I'm for breaking the association between being a good Haskell programmer and being a category guru. But on the other hand, knowing a good deal of CT (or order theory, or topology, or linear algebra, or group theory, or etc.) doubtless makes you a better computer scientist. On the other hand, I also think being a good computer scientist ultimately makes you a better programmer, but at least in the Haskell community that's a widely held viewpoint. (Elsewhere... eh, I prefer not to contemplate).
If you want type-safe routing, Happstack with web-routes-boomerang is pretty awesome.
Just plain warp with the simple (formerly wai-lite) package is pretty great too. Hard to get any simpler.
Just plain warp with the simple (formerly wai-routes) package is pretty great too. Hard to get any simpler.
Yes. In fact, Junior 2 (or 1 if we accept n+k patterns) vs. Senior 1 or 2.
Neat. :)
snap is another great option. very minimal and simple to use. good number of extensions, as well.
Also, one nice feature of `happstack-lite` is that it is just a subset of `happstack`. So, when you start feeling like you are out growing it, you can gently expand by simply importing extra functionality from the full `happstack`.
Happstack offers as much (if not more) type safety as Yesod -- through libraries like web-routes and reform. If you want it all bundled up in a single package, you can start with happstack-foundation. 
I have been there, and it's sometimes a long road because, well, because it seems the book is a slightly rushed first draft that incidentally became the first edition. It's good, but it's got some flaws. The Yesod Googlegroup is absolutely indispensable for straightening out some question marks, as well as errata. If you run into problems, you can ask me directly aswell -- I am a pretty meticulous reader and I have read through most of it, and also applied a lot of it. Having said that, it *is* worth the little pain it involves since Yesod haven't got many rivals in the Web MVC world. Absolutely fabulous and improving. I don't think it is for beginners of Haskell, though (I consider myself an intermediate+)
I was looking into Yesod at first, but had the feeling that it would help us too much, thus decreasing the learning opportunity. We also had some doubts about Yesod's heavy use of TemplateHaskell, which we think would also hinder our learning opportunities. Thanks for sharing your opinion 
Thanks for the Scotty tip, I didn't know them yet. Do you know where I can find some more information about Scotty because their website seems really 'lite' and Google doesn't provide much info either. It looks a great platform to start building on for learning about Haskell web programming, but I'm afraid that there is not much information available for beginners, and that the community that can provide help isn't that big. Or Am I wrong in this?
I kind of like this idea. Can you point me to some good documentation for beginners (a tutorial, some nice examples,...)?
The book looks great. But I'm starting to think that Yesod will take too much of the work away from us like I said in the [comment to esmolanka](http://www.reddit.com/r/haskell/comments/16kqe0/recommended_haskell_web_framework_for_beginners/c7x85rx) above. What is your opinion on that?
&gt; You may find that it's easier to just use libraries instead of investing the time in a framework, especially if you're new to web development in general. I'll keep that in mind, thanks! For the javascript problem, we are thinking of using [Fay](http://fay-lang.org/) so we can code everything in Haskell. Don't know yet how do-able this would be.
I'm not sure why jhickner says "formerly wai-routes". Wai-routes is still very much wai-routes! There's a dead simple example on the project page - https://github.com/ajnsit/wai-routes. Here's the full source - https://github.com/ajnsit/wai-routes/blob/master/examples/Example.hs
Yeah, that's a fair point. I guess I was just being picky about your use of the word "invalid" where I might have chosen to say "inexact" or "unsafe". Also, I think I had the "[Fast and Loose Reasoning is Morally Correct](http://www.cs.ox.ac.uk/publications/publication1680-abstract.html)" paper in mind, although I couldn't remember the name of it until just now.
...but the definition of a reasonable person is someone that agrees with me! Argh!
The code demonstrated in the video seems to use an old Parsec API ([Text.ParserCombinators.Parsec][PCP] instead of [Text.Parsec][P]), and it doesn’t seem to use the [Applicative][A] style. Applicative provides [`(*&gt;)`][*&gt;] and [`(&lt;*)`][&lt;*] which are equivalent to the Monad [`(&gt;&gt;)`][&gt;&gt;] and the Pandoc-defined `(&gt;&gt;~)`. `parseA *&gt; parseB &lt;* parseC` (run the three parsers sequentially, result in the result value of `parseB` – mnemonic: the angle brackets point to the parser whose result value gets used) is a common occurrence in parsers. You often don’t need the full power of Monad in parsers, Applicative is mostly enough. ([`(*&gt;)`][*&gt;] and [`(&gt;&gt;)`][&gt;&gt;] are only distinct due to a historical mistake. Ditto for [`fmap`][fmap] and [`liftM`][liftM], [`liftA2`][liftA2] and [`liftM2`][liftM2], [`pure`][pure] and [`return`][return], [`(&lt;*&gt;)`][&lt;*&gt;] and [`ap`][ap] etc.) For more information about the relationship between [Functor][F], [Applicative][A] and [Monad][M], see the [Typeclassopedia](http://www.haskell.org/haskellwiki/Typeclassopedia). As for when you do need the power of Monad, it is when your parser needs to decide which parser to run next based on the result value of a previous parser. [PCP]: http://hackage.haskell.org/packages/archive/parsec/latest/doc/html/Text-ParserCombinators-Parsec.html [P]: http://hackage.haskell.org/packages/archive/parsec/latest/doc/html/Text-Parsec.html [F]: http://hackage.haskell.org/packages/archive/base/latest/doc/html/Data-Functor.html#t:Functor [A]: http://hackage.haskell.org/packages/archive/base/latest/doc/html/Control-Applicative.html#t:Applicative [M]: http://hackage.haskell.org/packages/archive/base/latest/doc/html/Control-Monad.html#t:Monad [&lt;*]: http://hackage.haskell.org/packages/archive/base/latest/doc/html/Control-Applicative.html#v:-60--42- [*&gt;]: http://hackage.haskell.org/packages/archive/base/latest/doc/html/Control-Applicative.html#v:-42--62- [&gt;&gt;]: http://hackage.haskell.org/packages/archive/base/latest/doc/html/Control-Monad.html#v:-62--62- [fmap]: http://hackage.haskell.org/packages/archive/base/latest/doc/html/Data-Functor.html#v:fmap [liftM]: http://hackage.haskell.org/packages/archive/base/latest/doc/html/Control-Monad.html#v:liftM [&lt;*&gt;]: http://hackage.haskell.org/packages/archive/base/latest/doc/html/Control-Applicative.html#v:-60--42--62- [ap]: http://hackage.haskell.org/packages/archive/base/latest/doc/html/Control-Monad.html#v:ap [liftA2]: http://hackage.haskell.org/packages/archive/base/latest/doc/html/Control-Applicative.html#v:liftA2 [liftM2]: http://hackage.haskell.org/packages/archive/base/latest/doc/html/Control-Monad.html#v:liftM2 [pure]: http://hackage.haskell.org/packages/archive/base/latest/doc/html/Control-Applicative.html#v:pure [return]: http://hackage.haskell.org/packages/archive/base/latest/doc/html/Control-Monad.html#v:return
To quote Shapiro (I think): It's easier to do program transformation if you don't have to preserve correctness.
I'm using AngularJS (and coffeescript) with a simple [scotty](http://hackage.haskell.org/package/scotty) backend, with aeson for marshalling between Haskell datatypes and JSON. It works really nice, I have a few projects using it, one is [Agders Projekt](http://github.com/danr/agder). I haven't used snap, but scotty does what I need. For instance, [this is how](https://github.com/danr/agder/blob/master/backend/Main.hs) the route handlers look like.
Thanks, I was vaguely aware of \*&gt; and &lt;\* but I still don't have all of the operators like that at in the front of my mind. Having understood the use of &gt;&gt; and &gt;&gt;~ in Pandoc has made *&gt; and &lt;* make a lot of sense. The way that they point to the value they keep is more intuitive. Are you able to elaborate on your last paragraph? Monad adds in the &gt;&gt;= operator right? But why is that needed when deciding which parser to run?
Indeed, I have said so in my first post: &gt; If you plan to be a Haskell practitioner, you don't need to learn CT at all. If you plan to become a researcher in programming language theory, you may learn some CT along the way (though a lot of people successfully avoid the more theoretical-minded communities and still do extremely interesting and useful stuff), if not to do your own research then at least to understand some extremely obtuse stuff that has been written by people that knew CT too well. But realistically, how much of the people learning Haskell are also planning to become computer science researchers (or research-level industry tinkerers like sigfpe, Oleg, ekmett, Tekmo, etc.)? Maybe you're not doing a service to the large portion of people that would be interested in Haskell as programmers by letting them think that they also need category theory.
&gt; `parseA *&gt; parseB &lt;* parseC` As an aside, in my experience it is best to avoid `(*&gt;)` and write the above as: `id &lt;$ parseA &lt;*&gt; parseB &lt;* parseC`. The reason is that often you want to map a function over the result, and with this style that can always be done in a uniform way. I.e. you always write function &lt;$[&gt;] arg1 &lt;*[&gt;] arg2 &lt;*[&gt;] arg3... where you use `(&lt;$&gt;)` and `(&lt;*&gt;)` if the result of the arg parser is used, and `(&lt;$)` and `(&lt;*)` if it is not used. Even more uniform would be to write pure function &lt;*[&gt;] arg1 &lt;*[&gt;] arg2 &lt;*[&gt;] arg3... which also works with no arguments.
Can I infer from that that if I want to become a computer science researcher (as in my case, I don't want to be just a computer programmer) studying the kind of stuff cited in this reddit would be useful? Edit: But following the title of my question: I can infer too that if one wants to simply learn Haskell these abstract mathematics aren't of that much importance (that's what one understands by reading most of the comments?). Since Haskell is a language so tied to computer science researches those terms come to surface very often, is that right?
Scotty is very minimal, so the example code and the haddock documentation provide all the information you need. It's a bit like studying the Prelude.
`fmap :: (a -&gt; b) -&gt; Parser a -&gt; Parser b` `(&lt;*&gt;) :: Parser (a -&gt; b) -&gt; Parser a -&gt; Parser b` `(=&lt;&lt;) :: (a -&gt; Parser b) -&gt; Parser a -&gt; Parser b` # Functor `fmap :: (a -&gt; b) -&gt; Parser a -&gt; Parser b` `(&lt;$&gt;) = fmap` In the context of parsers, Functor gives you the power to apply some arbitrary function to the result value of a parser. Say, convert a parser of a sequence of digits into a parser of Integer by applying `read` to its result value: `nat :: Parser Integer` `nat = read &lt;$&gt; many1 digit` # Applicative `(&lt;*&gt;) :: Parser (a -&gt; b) -&gt; Parser a -&gt; Parser b` Applicative gives you the additional power of running two parsers sequentially and having the first one result in a function to be applied to the result value of the second one. `negation :: Parser (Integer -&gt; Integer)` `negation = (const negate &lt;$&gt; char '-') &lt;|&gt; pure id` `negation` replaces the result of `char '-'` with the function `negate :: Integer -&gt; Integer`, or if `char '-'` failed, just results in `id` without consuming any input. `int = negation &lt;*&gt; nat` `int` runs `negation` and `nat` sequentially, and applies the result of the former to the result of the latter. Note that `negation` has no power to choose whether to run `nat` or not. It will be run, period. `discardingSecond parseA = (\a b -&gt; a) &lt;$&gt; parseA` replaces the result value of `parseA` with a function that takes an additional “`b`” and throws it away. `discardingSecond parseA &lt;*&gt; parseB` runs `parseA` and `parseB` sequentially and applies the result value of `discardingSecond parseA` to the result value of `parseB`, resulting in only the result value of `parseA` if both parsers succeeded. We just reinvented `(*&gt;)`. `parseA &lt;* parseB = (\a _ -&gt; a) &lt;$&gt; parseA &lt;*&gt; parseB` `parseA *&gt; parseB = (\_ b -&gt; b) &lt;$&gt; parseA &lt;*&gt; parseB` Given that Applicative is more powerful than Functor, a Functor instance could be implemented in terms of an Applicative instance: `f &lt;$&gt; a = pure f &lt;*&gt; a`. On the other hand, not all instances of Functor can be instances of Applicative (how would you implement `pure :: a -&gt; Map k a`?) # Monad `(=&lt;&lt;) :: (a -&gt; Parser b) -&gt; Parser a -&gt; Parser b` Monad gives you the additional power of running a parser and deciding what to run next based on its result value. Say, we want to parse a format that represents a list of strings by a digit representing the length of a string, followed by the string. We’re going to need to parse a digit and then decide how many characters of input to consume based on the result of that parser. `list :: Parser [String]` `list = many item` `item :: Parser String` `item = do { num &lt;- digitToInt &lt;$&gt; digit; replicateM num anyChar }` `λ&gt; parseTest list "3foo7bar baz4quux"` `["foo","bar baz","quux"]` Given that Monad is more powerful than Applicative, an Applicative instance could be implemented in terms of a Monad instance: `fs &lt;*&gt; as = do { f &lt;- fs; a &lt;- as; return (f a) }`. On the other hand, not all instances of Applicative can be instances of Monad (one can’t implement a valid instance of Monad for ZipLists of unrestricted size for instance).
Great explanation thanks! I've got a couple of places where I do fn &lt;$&gt; ( a *&gt; b &lt;*c) so this will make it much cleaner!
This is awesome! This belongs in the documentation attoparsec! It'd be great if more haskell libraries included a narrative style block of haddocks (like [the pipes tutorial](http://hackage.haskell.org/packages/archive/pipes/2.4.0/doc/html/Control-Pipe-Tutorial.html)).
I can only recommend **not to use** ***frameworks*** at all. A framework is philosophically going against the ideals of software design. Because it expects you to link into *it*, and wants “no other gods beside me”. A library is a better choice, as you can plug it into *your* work, and aren’t blocked from attaching other stuff to it either. If it says “framework”, don’t use it. Period.
It depends in which fields of computer science you're interested in, but if it has "programming languages", "type systems" or "logic" in the title, yes, that would definitely be a plus -- although as I said you can do excellent research in programming languages without knowledge of category theory or topology; it's one of the great things about programming languages as a research fields, it spans from very theoretical topics like hardcore proof theory to nearly-humanities topic like [Social factors in language design](http://www.eecs.berkeley.edu/~lmeyerov/#socio), or user-interface topics. &gt; Since Haskell is a language so tied to computer science researches those terms come to surface very often, is that right? My personal point of view is that they don't need to come to surface as often as they do on this reddit for example. I suspect there are a lot of posts of the form "I just understood a concept so let me forge my own explanation to reassure myself" (which is fine, but often more useful for the writer than the readers) that hints at those topics, and less posts of the form "I have a neat results that come from this more advanced theory that I want to explain to you with the maths because they're interesting" -- although sclv and I have by now cited a few examples of this case as well. PS: as a starting point, I would recommend that you find information on the denotational semantics of the simply typed lambda-calculus in terms of cartesian closed categories. It's a nice example that does not require too much categorical baggage (but will still force you to learn some core notions), but it's maybe slightly misleading as it works *too well* compared to similar constructions for more realistic typed calculi.
As far as documentation is concerned, I've always found the Snap docs to be excellent. With *snap init* you can quickly get something up and running. Their Snaplet API allows you to plug and play in a very modular way. If easy for beginners and simple to get going is what you want, Snap is my recommendation.
Wow, that's a great response, thank you. I think the main thing I was missing is that the Parser Monad is not actually doing the parsing, it's just a typeclass that describes how to combine parsers. Is that a fair way to sum it up?
It's become more and more clear to me that maybe this would be the way to go. Do you have a recommendation of a haskell web server and some starting libraries to build on?
The web frameworks are starting to get pretty mature, but Fay is still new and only supports a subset of Haskell. However, that said, I'm still pretty excited about Fay.
I have to disagree with this sentiment. Framework-schmamework. It's just a name. There's no governing body that says if you call your project a framework, then you have to meet this criteria, and if you call it a library then you have to meet that criteria. As I've [written elsewhere](http://stackoverflow.com/questions/5645168/comparing-haskells-snap-and-yesod-web-frameworks/5650715#5650715) much of the functionality used by the big three Haskell web frameworks can be mixed and matched like Evi1M4chine's "libraries". A number of people have argued that Snap and Happstack are more like libraries, and that Yesod is much more of an all-encompassing approach. I would imagine that Haskell web frameworks have used the term framework just because that's what their users are looking for--not because of some specific characteristic. You should make your decisions on concrete issues, not based on random words chosen because of the prevailing cultural context.
Implementations of parsers in Haskell certainly don’t **need** the Monad instance, they could easily provide the equivalent functionality without it. Monad (just like Functor and Applicative… and Foldable and Traversable and Alternative and Monoid and…) simply provides a common interface to certain primitives all kinds of modules would implement anyway along with a bunch of useful helpers based on them for free (say, `replicateM`, used in my post). Whether a parser library implements the actual full functionality of `(&gt;&gt;=)` inside the Monad instance or whether `(&gt;&gt;=)` is just based on some other primitives implemented elsewhere in the library cannot be known without inspecting the code, but in any case, the Monad instance is just a subset of the library. As a side note, [here’s a tiny toy parser combinator module](https://gist.github.com/3966338#file-fooparser-hs) that lacks all kinds of things such as good error messages but can be useful for pedagogical purposes. The other file in the gist implements an s-expr subset parser using it. Also an insight: `Parser i o` just a reimplementation of what `StateT [i] [] o` does out of the box.
If you are going to say something negative like this, then you should at least point people to what you would use instead. edit: ah, forget about it, Its just u/EvilMachine again...
For applicative code, I much prefer the `f &lt;$&gt; arg1 &lt;*&gt; arg2` style, because it means that I don't have to give names to the evaluated arguments. In complicated cases, I would write your example as (\x y z -&gt; f y) &lt;$&gt; arg1 &lt;*&gt; arg2 &lt;*&gt; arg3 Still, it would be nice if there was a do syntax for Applicative. Maybe even switching automatically based on whether monad-like features are used (and if Applicative were a superclass of Monad).
Great news. I for one would be eager to get a lot of the existing conduit-using code migrated over to pipes to see how much this elegant interface pays off in practice.
The answer to this question is somewhat dependent on how far you want to go with your CMS project. People are making it sound like Scotty is much simpler than the others, but it looks to me like it has roughly the same level of complexity as snap-core, happstack-lite, and wai+warp. At first it might seem simpler, but that is because it builds on wai+warp. Simplicity is good, but if you want to make a substantial web app, then you're probably going to want higher level features eventually. For instance, state management. You'll pretty quickly discover that simple Request -&gt; m Response functions need to be augmented with some extra state that you might want to modify during the course of processing a request. That means that you're probably going to want a StateT monad transformer. Ok, that's fairly straightforward. But then as you continue down the road of building a CMS you're going to discover that you want to facilitate the development of pluggable independent components just like other CMSs such as Drupal and others have found. Now you're reimplementing snaplets. In short, the complexity in the bigger frameworks is usually there for a reason. If you don't mind reimplementing snaplets as a way to learn Haskell, then by all means, go for it. But my experience with learning Haskell has been that I benefit hugely from seeing how other more experienced people have done things. If you just stick with scotty since it's the simplest, then you may be missing the chance to learn about aspects of Haskell that other experienced people have used to solve some of the higher level problems. If you intend to take your CMS project very far and you use Scotty, you'll probably find that you're reimplementing things that have already been done by other frameworks.
If I understand correctly, pipes-safe already does that. Normal termination finalizes promptly. The only case where a finalizer is delayed is if you compose it with something else that terminates before the bracket completes. Edit: It also finalizes promptly in the face of exceptions, too. I've updated the announcement post to clarify that.
One base patch is a generic change to the autotools stuff to make a test work in cross-compiling mode. The other is a QNX-specific change to assumptions about the CLK_TCK constant. Two of the patches to GHC core are just passing more flags through to the rest of the build process to make it know it is cross-compiling. The other GHC patch implements a new OS specifier, mostly so that the right linker flag can be used for pthreads (since QNX is slightly nonstandard in that regard).
Now I have another reason to wish for `Rank2Types` in the next standard :)
`trySafeIO` says it preserves exceptions as `Left`, but the return type is `IO e` ?
Happstack will probably teach you more about Haskell than any other option. The crash course is also quite beginner friendly in my experience (it's how I learned Haskell).
Happstack is pretty much a set of libraries. It's one of the reasons I prefer it over others. It still gives you most features of a framework (it's more than plain "warp" for example) and if you want a framework there's happstack-foundation and clckwrks.
Well, Snap up to the point of using snaplets. Snaplets have typical tell tale signs of a framework: they use text-file based configuration with assumptions about directory structure, for example. I don't know how hard-coded those assumptions are, but IMO in a library design such assumptions would be opt-in, not opt-out.
The `e` ends up being an Either because of runEitherK. I can specialize the type in the next release to make that clear.
It's deprecated. :)
Your points respectively don't support your claim, and are incorrect. Using config files is not a "tell tale sign of a framework" at all. It is the tell tale sign of wanting to be able to reload configuration without recompiling the app. Libraries do this too. A framework is defined by the inversion of control, the framework is the app and your code is the library. Also that configuration is done by the snaplets themselves, not the "framework". If you want to write a snaplet that requires a hard coded config passed to its initializer, you are welcome to do so (the ones included with snap are good examples). Heist for example is initialized with a HeistState, session is initialized with configuration options, etc. Second, the only assumption is that snaplets put stuff in the snaplets directory (I haven't checked but there may very well be a lower level initialization function that lets you pick a directory). The directory structure past that is configured, not assumed. For example, the project template created by snap init puts templates into the snaplets/heist/templates directory. You aren't required to use that code, you can write whatever you want, or change the skeleton code to suit your needs. If you want it in snaplets/I_have_an_obsessive_hate_boner_for_snap/ its up to you.
I'm sorry, I meant to say that simple was formerly called _wai-lite_.
There are some examples in the hackage docs: http://hackage.haskell.org/package/simple http://hackage.haskell.org/packages/archive/simple/0.2.0/doc/html/Web-Simple-Router.html
I think 'requires' is a bit strong. But it would certainly be better :) I have a proof-of-concept pipes 3.0 HTTP backend here that just uses StateP and some hand-written (and not entirely compliant) ByteString parsing: http://hub.darcs.net/stepcut/hyperdrive/browse/hyperdrive I'll be integrating the pipes-safe stuff shortly. I'm currently experimenting with how to actually do the parsing -- though the stuff I am investigating in that arena has little to do with pipes itself. 
Thankyou. This is great. Edited out the surprised cursing.
There's a typo in your tutorial under the trySafeIO section: the example uses runSafeIO.
Thanks. I will fix it.
I have a few questions regarding the implementation: 1. When looking at the various runSafe[r]IO and trySafe[r]IO functions, I see a huge amount of code replication. Is there a special reason to not refactor the common parts in helper functions? Performance, maybe? 2. Why is the CheckP class needed? Isn't it possible to implement this solely on top of the Proxy class?
+1000 for the last sentence!
&gt; When looking at the various runSafe[r]IO and trySafe[r]IO functions, I see a huge amount of code replication. Is there a special reason to not refactor the common parts in helper functions? Performance, maybe? I was lazy. I will fix that in the next release. I will probably just define the `run` functions in terms of the `try` functions in the next patch. &gt; Why is the CheckP class needed? Isn't it possible to implement this solely on top of the Proxy class? It is not possible to my knowledge. I settled for adding the `CheckP` type class and just making sure it had a principled theoretical basis that still let users reason about how it behaves.
I'm in favor of collaboration -- though I am current working on a server not a client. Also, I am looking at using an external code generator for the parser rather than combinators. Though.. it is likely to target 'ParseP' -- as that is the sensible type for a pipes parser.
As a newbie I tried combo: * **Happstack** - because its tutorial is very clear and easy to follow: http://www.happstack.com/docs/crashcourse/index.html * **happstack-heist** - because I like the idea of splicing text into HTML, it's also described in tutorial for Happstack * **acid-state** - because it's simply awesome data store, which may be not suitable for your project though and it uses a little bit of magic from TH; described as well in tutorial And it worked pretty well for me and it was really simple to start with. For comparison, at first I couldn't convince myself to Snap, mainly because of lack of such good tutorial as Happstack has (personal opinion only!).
You mean in favour of `RankNTypes`? I'd take either, but have never seen use for more than 2.
Blatant troll. There are more sensible ways to say "sorry, but this reddit is not for this kind of questions, read a Haskell tutorial and then Stack Overflow will give you the answers you need".
I always just use WAI as my basis any then layer whatever libraries make sense on top of that.
Point-free notation on functions with more than 2 arguments usually gives me headaches, it's kind of my limit. I often use in my code a function like: modifyIf cond f x = if cond then f x else x And then I may say: filter p = foldr (\x -&gt; modifyIf (p x) (x:)) [] As concise and more readable, I believe.
I'm getting a 404, on the patched ghc.torrent link.
As a contributor to the AWS library we have been adding support for the services that we want to use. I agree that we need to support more services. Would DynamoDB be an important feature for you?
Disclaimer: I'm the primary contributor to Scotty. Scotty's API is intentionally kept small so it fits on one haddock page: http://hackage.haskell.org/packages/archive/scotty/latest/doc/html/Web-Scotty.html It's probably best to learn by example. There are several small examples included with the package. You can either download the cabal tar file directly, or view them on github: https://github.com/xich/scotty/tree/master/examples All that said, Scotty is essentially wai+warp with a router bolted on top. It is designed to let you get started easily, but does require some knowledge about how the HTTP request/response cycle works, and is geared towards implementing RESTful web services, so it doesn't offer you a lot of the niceties that a state-ful web application might want. Of course, if you are also interested in learning Haskell, you might enjoy implementing these niceties along the way.
If you import other QML/JS files into the main one, it should work fine. BB10 UIs in general have on "main" QML file, but that does not prevent you from having custom widgets defined in other QML files, etc.
Ok, that's what I thought. I was concerned about the custom widgets, eg, trying to get their carousel example working, on one page, then other pages being the standard listview. 
From http://www.haskell.org/ghc/docs/latest/html/users_guide/template-haskell.html : &gt; Note that pattern splices are not supported. See also [bug #1476](http://hackage.haskell.org/trac/ghc/ticket/1476) on ghc. I ran into the issue some time ago, the only solution I found was to build the structure "from hand" (i.e. without quasi quotation).
Nothing major, but: 1. :m can open up the whole top-level of a module, not just the exports (when * is used) 2. :m can be undone, where import is cumulative. http://www.haskell.org/ghc/docs/7.4.1/html/users_guide/interactive-evaluation.html
Don't they run through the monad instance, though?
I found [this thread](http://www.mail-archive.com/haskell-cafe@haskell.org/msg65597.html) helpful.
I did something similar to this, but I also used MetaHDBC for statically checked database access. Perhaps you'd find it interesting. Check out Types.hs and Queries.hs. https://github.com/johnpmayer/deployapp My favorite little snippet was this from Utils.hs: makeJSONHandler :: ToJSON a =&gt; IO a -&gt; Snap () makeJSONHandler query = liftIO query &gt;&gt;= (writeLBS . encode)
I've been playing with Happstack. It's pretty easy especially if you try Happstack-Lite first. Relevant link: http://www.happstack.com/docs/crashcourse/index.html 
Can Haskades work on Blackberry Playbook as well? 
I've been working on both pipes-attoparsec and pipes-network (both original works by pcapriotti), and I'm trying to build a simple HTTP proxy server as I go just as an excuse to actually use both libraries and learn about their shortcomings. It's great that you are working on a more serious HTTP server :) pipes-attoparsec does not currently support interleaved parsing, it just supports being continuously fed full parser inputs from upstream, handling parsing errors, and limiting the input length. There's a lot of documentation, including a tutorial: http://hackage.haskell.org/package/pipes-attoparsec-0.1.0.1 I'm currently working on adding interleaved parsing support using the approach Tekmo suggested: https://github.com/k0001/pipes-attoparsec/blob/interleave/src/Control/Proxy/Trans/Attoparsec.hs In the case of the simple HTTP proxy, an example use case of such interleaved parsing would be reading the HTTP headers to get the values for “Host” and “Content-Length”, and then be able to just stream as many bytes of request body as 'Content-Length' says to 'Host', without such bytes going through a parser. Also, I haven't had a chance yet to actually use pipes-network apart from some simple examples, and probably there are features missing, so I've not yet released it. But here's the working code in case you want to check it: https://github.com/k0001/pipes-network/ 
Aha, I love getting inspiration from Reddit! I had some code that was originally: map (over _1 . sconcat . NonEmpty.fromList) . groupBy ((==) `on` fst) . map splitRow It did the job, but it felt like it was trying too hard. Using `semigroups` and `lens` just to split a list of tuples and group by the first half? Ack. Turns out what I really wanted was: map (fst . head &amp;&amp;&amp; foldMap snd) . groupBy ((==) `on` fst) . map splitRow Not much difference in typing, but I think the first is much easier to understand - no need to understand what a `semigroup` is, nor do I incur the extra dependency. Thanks, tel/dons!
Existentials have more power than the "double negated" encoding? In system f/intuitionist logic? How? newtype Exists f = Exists (forall r. (foral x. f x -&gt; r) -&gt; r) is isomorphic to data Exists' f = foral x. Exists' (f x) and is, in fact, just a simple encoding of its elimination rule. Anyway, it is trivially true that newtype + rank2 gives you the expressive power of rankN (modulo non functorial data types like GADTs). It is just that once you start doing things like using pervasive rank 2 to fake subtyping and yoneda encode all your categories into hask (a la `lens`), rank 3 and higher doesn't seem that weird. Also, GHC doesn't go type inference at rank2 anyways, so you are not giving much up when you enable higher ranks.
There's also Ralf Hinze's [genuine case of a rank-3 type](http://www.cs.tufts.edu/~nr/cs257/archive/simon-peyton-jones/higher-rank.ps): type Map f g = forall a b. (a-&gt;b) -&gt; f a -&gt; g b newtype Hfix h a = H (h (Hfix h) a) hmap :: forall h i. (forall f g. Map f g -&gt; Map (h f) (i g)) -&gt; Map (Hfix h) (Hfix i)
Which is the semigroup stuff? is that `sconcat`?
I used [Snap.Extras.JSON.writeJSON](http://hackage.haskell.org/packages/archive/snap-extras/0.1.7/doc/html/Snap-Extras-JSON.html) for that. This also automatically sets the correct content type "application/json" in the response -- some JavaScript libraries need this or will fail parsing AJAX queries.
`sconcat` and `First` are from `semigroups`.
: ) I used to avoid using arrow combinators in "normal" code, but since Hlint suggests their use automatically I ended up using them extensively in a tuple-heavy but non-arrowy exercise. After that I just see them all the time and think the simplicity is well worth introducing the arrow concept at least on functions. Glad I/dons could inspire an &amp;&amp;&amp;. (ps. am I the only one who mentally pronounces &amp;&amp;&amp; as "ananand"?)
It should, but I have not tested that at all yet.
I hope this is not the last API change. So many things to improve :)
Bravo Jasper! Thanks for making things simpler.
Libraries have to deprecate and evolve, otherwise somebody else will fork them and do it for them.
I'm curious about the performance of this example. I ran the example with cabal-dev/bin/angularjs-todo --no-access-log and then hit it with ab -c 40 -n 10000 http://localhost:8000/ And I saw ~900 requests per second. Now this is running on a four core machine (not a great one), but is much lower than I expected. since it uses all four cores, this is 250 per second. For a (bad) comparison, the [tornado hello world](http://www.tornadoweb.org/) gets 5,000 requests per second on a single core. Now, I know you're not putting this up to benchmark but the impressive snap benchmarks made me think the performance would be better than this. Can you suggest how I could profile why it is so slow?
&gt; The important Compiler type has been changed from Arrow to Monad: this makes it much easier to write custom compilers, as most Haskellers are more familiar with monads. **Awesome!!!**
See [this post](http://snapframework.com/blog/2012/12/9/heist-0.10-released) for more detailed information about this.
There's a tension though. If everything breaks all the time people might tire of Haskell and use something more stable.
This will be my excuse for not learning about arrows.
If you ignore the difference between Node a [] and Leaf a, that Tree type is equivalent to Cofree []
Thanks, I'll check out the cellular automata article, though it's certainly not very practical :) I think [this](http://hackage.haskell.org/package/streams) is the package, using Comonad for streams. Too bad Hackage doesn't track reverse dependencies. I wonder if it's actually used by any other package.
I think a more appropriate term is probably "fanout" but I like "ananand".
You can use http://packdeps.haskellers.com/ for reverse dependencies.
This is so scary. Today I decided "you know what, I need to spend some time grokking comonads" and downloaded a bunch of papers on the train. Then just now, I think - I bet reddit is going to right about comonads today. AND LO AND BEHOLD. Have we reached the singularity?
And conversely - did we lose anything moving to Monad?
Good question. Originally I went with Arrow because I wanted to do stuff that I thought wasn't possible using Monad (because Monad is not restrictive enough for some dependency tracking things). It was only after a lot of messing around that I figured out there was a way to do this using Monads, but, in order to make this work, a lot of information needs to be persisted in between builds (i.e. save it as files in the `_cache` directory). E.g. a `Pattern` in Hakyll is (more or less) a predicate on a file path. These can be constructed e.g. using the `IsString` instance: foo/*.markdown And in previous versions, also using the following constructor: predicate :: (FilePath -&gt; Bool) -&gt; Pattern Obviously we can't persist the latter -- so this is one of the few features that I had to drop. This example is one of the few cons that came with Hakyll 4. But I think that overall, it's definitely an improvement.
Any state involved in your web application (sessions) shouldn't be stored in your web application's memory, HTTP is stateless for a reason and you want to keep it that way as much as possible. It would be reasonable to reinvent the wheel here as a learning exercise in the way you suggest but doesn't strike me as the right wheel to reinvent at the beginning. That's why all of these simple examples use WAI, you can just use some WAI middleware to handle your sessions and not have to worry about it. It depends on how much of this is about getting that blog finished and how much is about learning how to do web dev. Pluggable independent components are useful if you want to build a CMS platform that many people use and build plugins for or if you are a web agency that has to build 3 new websites a month that are mostly the same. Even in this case I think they are an awful architecture and a cancer infecting today's web framework ecosystem. One thing they are good for is building a CMS that a non-technical person can change and modify. Since this person wants to learn *in haskell* then I don't think that applies at all. If you are writing one site and will work on it for a while then pluggable components are a horrible tradeoff, the standard loosely coupled and well modularized coding practices that apply to any software project are a much better style to adopt. Snaplets are a nice implementation of a shitty model for web application code reuse, as well as being extraordinarily dated in an era where most modern sites are moving to client side interfaces that interact with REST api's.
It's too bad you are getting downvoted for this. You are absolutely correct. I do find it's often a pragmatic choice to use a bundled WAI-&gt;routing-&gt;function that returns text "framework" like scotty or wai-lite or the like but heavy duty frameworks are a cancer on the web developer ecosystem.
Nope. This is completely wrong. There is a very very simple distinction between frameworks and libraries and it applies (and has for decades now) to pretty much all software, not just web stuff. If the 3rd party code controls the flow of data through the program and your code registers handlers or follows a naming convention or implements a specific interface in order to be called then you are writing code inside a framework. If your code controls the flow of data through the program, calling 3rd party code when appropriate then you are using libraries. If you draw out a dependency diagram or anything outlining the different system layers and your code lives "inside" the 3rd party code then that's a framework. That's where the name comes from. There are many frameworks that have pieces that can be used as libraries of course, and that's the *entire* point of abstraction layers like WAI, but you are confusing someone trying to learn about web dev by misinforming them about terms with standard industry definitions. 
Since the primary dev of Scotty already responded I can't add much. I will add that if you are learning now then you can be ready for where all web development is quickly moving: Write all your functionality as a stateless REST API and have your interface javascript communicate with it. That does mean writing a lot more javascript (or whatever you use that compiles to javascript, but if you don't know javascript debugging that might be very very painful) but if you go the old school way then you might find that by the time you learn all this stuff you are pretty out of date. Stateless API's appeal to me and should appeal to anyone who likes Haskell *and* HTTP!
There's Data.Traversable.for = flip traverse, which when restricted to Traversable [] and Applicative Identity is just flip map. Please don't put a different 'for' into the Prelude kthxbai.
Perhaps I could humbly recommend my own blog post, which builds on that classic Lambda the Ultimate link and shows how to implement simple probabilistic cellular automata using comonads and comonad transformers: http://productivedetour.blogspot.com.es/2012/12/evaluating-probabilistic-cellular.html http://productivedetour.blogspot.com.es/2012/12/evaluating-probabilistic-cellular_31.html 
At a glance, it looks like his example of (=&gt;&gt; shortest) is inefficient. I suspect that the work on each subtree is not being shared. This anippet: allShortest (Node x xs) = let ys = map allShortest xs in Node (x + minimum (map coreturnTree ys)) ys allShortest leaf = leaf should only compute each subtree once.
Tekmo, anytime you'd like any help whatsoever with python drop me a line! Your writing has been absolute gold for learning Haskell! What are you trying to build out of interest?
I think this wraps your standard-process pretty nicely: http://bob.ippoli.to/archives/2013/01/11/getting-started-with-haskell/ So is this the "best way" to go right now?
so I managed to install cabal-dev on windows but if I try to "cabal-dev install --enable-tests --force-reinstalls" on windows it will fail on "time-1.4.0.2" yet again (caused by a ./configure): Resolving dependencies... Configuring time-1.4.0.2... cabal.exe: The package has a './configure' script. This requires a Unix compatibility toolchain such as MinGW+MSYS or Cygwin. Failed to install time-1.4.0.2 cabal.exe: Error: some packages failed to install: time-1.4.0.2 failed during the configure step. The exception was: ExitFailure 1 :( ... guess I have to get MinGW+MSYS ...
cellular automata, lenses can be seen as the use of indexed comonad coalgebras, image filters, simultaneous play games, semantics for dataflow programming, i use them in scala because monad transformers don't work well in scala and every comonad gives rise to a monad transformer, so I can explicitly plumb the extra bits of state or what have you around that I need comonadically, attaching logging to individual structures rather than the application as a whole, redecorating trees, comonadic zippers, lens 3.8 has a bunch of uses of corepresentation by a left-adjoint and every left-adjoint of a functor from Hask -&gt; Hask is a comonad, so extract has a well defined meaning there.. The trick with comonads is that unlike monads, one comonad probably won't be what you structure your entire program around. I use lots of little comonads. E.g. a Snaplet in the snap web framework forms a comonad. Its not a particularly useful comonad, but that it is a comonad gives you a couple extra laws and the easy ability to rewrap a snaplet in its configuration after you edit it using that configuration. I similarly have a comonad for dealing with retaining names from the source in the locally nameless representation used in my 'bound' package. There are lots of names in a source program, hence lots of these little comonads running around. I have other places in our compiler here at S&amp;P Capital IQ where I replace my free variables with comonads that know something extra, such as their type rather than build up an explicit environment, knowing I can extract to throw away the extra information when I'm done.
I have other things built on top of streams, but nothing I've pushed to hackage.
do be honest: never did anything like it - can you explain how you do this? I have ubuntu running in a VM so compiling the code in linux is not really a problem - BUT I want windows-executables at the end of the day and this is a problem (isn't it?)
Wow! Just when I though I had me head around Free! Maybe you could talk about this stuff at the next Hoodlums? :-)
As part of my MSc thesis (not coincidentally as a student for one of the authors of your cited papers) I tried to implement a finger tree this way. Encoding the structural invariants into the GADT and using them in the algebras was... hard. Very hard. Lots of boilerplate datatypes are needed to make things work. I'm still wondering if this will be easier in a dependently typed language. [my attempt at a finger tree](https://github.com/sebastiaanvisser/islay/blob/master/src/Container/FingerTree/Abstract.hs)
Nice code. Regarding dependant types, I too wonder how much easier it would be. I'd like to try it in Edwin Brady's Idris if I get some free time. Even without dependent types, language support such as idiom brackets and macros for instance generation would help a lot. EDIT: Just occurred to me that kind polymorphism in the latest GHCs would be a useful feature here too.
I was hoping to do a London HUG talk on recursion schemes in general, if that ever transpires, I could try and talk a bit about this stuff nearer the end. 
YES PLEASE! I'm a newbie to recursion schemes, but I find them increasingly fascinating. I would love to hear you talk about this :)
A few more references that are helping me on my quest to grok this stuff: * [Data types à la carte - Wouter Swierstra](http://www.cs.uu.nl/wiki/pub/GP/Schedule/WoutElsinghorst.pdf) - introduces Expr as fixed points of functors, though not higher order. This might be a good read before you dive in with this article and climb the abstraction ladder higher. * [Functional Programming with Bananas, Lenses, Envelopes and Barbed Wire](http://research.microsoft.com/~emeijer/Papers/fpca91.pdf) - this was my first introduction to recursion schemes. I found it hard to get my head around the syntax, but if you stick with it, it's not so bad. * [`recursion-schemes`](http://hackage.haskell.org/packages/archive/recursion-schemes/3.0.0.1/doc/html/Data-Functor-Foldable.html) - a Haskell library for recursion schemes in the above paper. Again, another way to play with recursion schemes.
I feel like the only programmer who cares about Mac+Windows+Linux support.
Thanks! In this case I have two ways that I have to use Python. The first is that I have a structural search engine for proteins that interfaces with a PyMOL client, which is a molecular graphics program written partly in Python. I've already finished the PyMOL plugin to communicate with my server, so I'm good there. The frustrating part is distributing this PyMOL plugin for this program and I'm trying to devise simple installers. There are some things that complicate it: 1. My client plugin depends on the `pika` library, and for a while I had trouble figuring out a reliable way to distribute it. On ubuntu 12.04 and debian testing it is not so bad because now they have it packaged up. On Mac it is also okay because as far as I can tell most macs have `python` and `easy_install` so they can just run a few commands to install it. However, windows is a bit more difficult. I have a colleague working on that and yesterday he seemed to have found some program that converts python code into installers but I haven't had a chance to test it. Before he figured that out yesterday, installing `pika` on windows was very difficult. 2. This `PyMOL` program isn't a good Python citizen and basically keeps its own Python module hierarchy (I don't know if that's the right term) on OSX and Windows when you install it, and my client plugin has to go in those non-standard locations. It also uses different versions of Python from platform to platform although they generally range from 2.5 to 2.7. I think we're about to solve most of these problems now, but the main questions I have are: * Where do you look up what Python versions and installers come pre-installed on various versions of OSX? * What is the simplest way to distribute Python code on windows for non-technical users? * How do you properly google for Python help or documentation? Like, whenever I want to find the documentation for some Haskell package, I just type "hackage &lt;packagename&gt;" into google and that works 99% of the time. Is there an equivalent trick for Python? More language-based questions I have are: * Is there a mature STM implementation on Python? The closest I can find is queues which I can use to simulate MVars, and otherwise use locks/events/semaphores. * I find myself wanting sum types all the time. Unfortunately, my colleague understandably doesn't follow the church encoding equivalent of them in Python, so I'm reluctant to use them in my modifications of his code. Is there some Python workaround for the lack of sum types? Those are the main questions that are fresh on my mind. If I can think of any others I will let you know.
&gt; packages that reinstalled: binary bytestring regex-base regex-posix.... [a long list] It sounds to me like the dependencies of cabal-dev (or some package it depends on) just need to be bumped. You should complain to the maintainers of cabal-dev and let them figure out whether it's their fault or one of their dependencies' fault and then get it fixed up. Have you tried installing cabal-dev from their repository (wherever that is)? It's possible that whatever dependency problems there are have already been fixed.
First of all, the example is using heist 0.10, not 0.1. Second, you don't just automatically get the performance improvement by upgrading. The new version provides the ability to get faster performance, but depending on how you use heist, that may require varying amounts of effort.
When I say "state" I'm not talking about state that remains across requests. I'm talking about state that your application needs to keep during the course of processing a single request. It's completely different from what you are talking about here, and it facilitates REST api's quite nicely.
This sounds like a problem with `cabal-dev` and it sounds remarkably similar to [this problem](http://www.reddit.com/r/haskell/comments/137156/this_is_unacceptable_cabal/c71cx7n) raised here not too long ago. I linked to a comment that describes how to easily fix the problem. Heck, I still haven't successfully installed `cabal-dev` even on Linux because it keeps conflicting with my copy of the Haskell platform. I'll conclude with some general comments: * Even if this particular problem is `cabal-dev`'s fault, Haskell development still sucks on windows. It kills me that Haskell is otherwise perfect for distribution, producing statically linked compiled executables ready to run, yet then proceeds to shoot itself in the foot by making it difficult to compile on the most popular operating system. * You don't necessarily need to use `cabal-dev`. I certainly don't (which is not necessarily a good thing, though). However, then you have to learn how to reset your package database if cabal gets trapped in a dependency local minimum (i.e. delete `~/.ghc/&lt;arch_ghcversion&gt;/`. Alternatively, you can develop using `cabal-dev` once you use the fix in the link I gave you, but then your friend can skip it and just use `cabal` if all they want to do is compile the test executable you gave them.
Can you elaborate on simultaneous play games?
This has been posted a few times. And mostly been downvoted because it's full of shit.
Please, everyone stop posting Douglas Crockford, and particularly not this video. The guy is misinformed at best.
You forgot the `DeriveFunctor` extension.
I'll look on my mac when I get home, I think it's in /Library/Framework/ or something. However, the way I work with python on the mac is to install python with homebrew and use virtualenvs. Python is used for so many system level things on both linux and OSX that I like to leave the system python alone as far as possible. I don't know if this is a viable approach for you (perhaps not if you're distributing to users). The best way that I know of for distributing python on windows is py2exe [1] and there's py2app for OSX [2]. I've used [1] and it was fine, never used [2] though! I'd start here: http://www.python.org/doc/ for python docs, the index in sphinx is pretty good, but really google (or maybe duck duck go) is your only friend here. There's no search by types in the python world. One thing that is absolutely indispensable for developing in python though is ipython [3]. I live in ipython! You import your, or any other module create and instance of what you need to know about (or get a reference to it's class) and do: $ ipython Python 2.6.6 (r266:84292, Mar 25 2011, 19:24:58) Type "copyright", "credits" or "license" for more information. IPython 0.13 -- An enhanced Interactive Python. ? -&gt; Introduction and overview of IPython's features. %quickref -&gt; Quick reference. help -&gt; Python's own help system. object? -&gt; Details about 'object', use 'object??' for extra details. In [1]: s = "test" In [2]: s? Type: str String Form:test Length: 4 Docstring: str(object) -&gt; string Return a nice string representation of the object. If the argument is a string, the return value is the same object. In [3]: s.&lt;TAB&gt; s.capitalize s.count s.encode s.expandtabs s.format s.isalnum s.isdigit s.isspace s.isupper s.ljust s.lstrip s.replace s.rindex s.rpartition s.rstrip s.splitlines s.strip s.title s.upper s.center s.decode s.endswith s.find s.index s.isalpha s.islower s.istitle s.join s.lower s.partition s.rfind s.rjust s.rsplit s.split s.startswith s.swapcase s.translate s.zfil &lt;obj&gt;? gives the documentation (think :t) and &lt;obj&gt;?? gives the actual implementation from the source code (kinda like :i, but actually shows the source) I think there's some sort of STM implementation in pypi, it won't be anywhere near as good as what's available in Haskell I shouldn't think :(. No sum types I'm afraid, I might be able to suggest an alternative pythonic approach if you have bit more detail? [1] http://www.py2exe.org/ [2] http://svn.pythonmac.org/py2app/py2app/trunk/doc/index.html [3] http://ipython.org/
&gt; if I try to "cabal-dev install --enable-tests --force-reinstalls" on windows it will fail Why would you `--force-reinstalls`? What project are you running `cabal-dev install` on? Can you post its `.cabal` file? &gt; :( ... guess I have to get MinGW+MSYS ... Perhaps. I normally have this on my Windows systems anyway, so maybe I didn't realize it was a dependency for packages like `time`. 
I feel like there is a slight misunderstanding/vagueness in the presentation: the techniques demonstrated here (in the first part, I'm not talking about the heterogeneous equality) are about computing fixpoints at the higher kind `*-&gt;*` rather than the usual `*`. It is not directly about GADTs, and in fact it is rather orthogonal to the GADT mechanics. In a dependently typed language, you have an important distinction between the *parameters* of an inductive type and the *arguments* of its constructors. Witness the difference between the two following Coq definitions: Inductive List (A : Type) : Type := | Nil : List A | Cons : A -&gt; List A -&gt; List A. Inductive FullTree : Type -&gt; Type := | Leaf : forall A, A -&gt; FullTree A | Node : forall A, FullTree (A * A) -&gt; FullTree A. In the first case, A is a parameter fixed once and forall in this datatype definition, and the recursive datatype itself is computed as a fixpoint in `Type`. In the second example, we have a fixpoint in `Type -&gt; Type` that makes a non-trivial use of its type argument, by changing it between the argument and the return type of the `Node` constructor. The reason why we sometimes make this distinction in dependent type theories is that parameters are *simple*; they don't move, they're easy to handle theoretically. On the contrary, general type arguments that may be instantiated differently in the constructor are more powerful (they give rise to a more general elimination principle), and therefore source of more theoretical difficulties. What is demonstrated in this blog post is how to compute a fixpoint in higher kinds, that is for datatype of the second form. The fact that the example is done on a GADT rather than, say, a non-regular recursive datatype as `FullTree` above is not in fact relevant to the technical development following. 
Really? According to the link, the video was made on the 15th and published on the 16th of this month. In other words, yesterday. **edit:** Mr. Crockford has given a lecture with the same title at two (or three) other places (YUIConf?), which have been posted in r/haskell in the past. And indeed, downvoting (and objections from people I know to be trustworthy) has occurred. Nevertheless, because this is a new video given at a new venue, he may have changed or improved his lecture so perhaps this objection does not apply. I don't have the patience to watch a bad lecture twice to decide. Downvote retracted; upvote awarded. Good day sir! 
Doug Crockford has given that same talk multiple times. This is one instance of it.
It isn't a fully fleshed out example, but in theory if you have two or more players that have to make a decision, based on a current board state and then merge the results, you have something very much like a comonad for the game. This is similar to the cellular automata example, where the 'each player updates his state' function plays the role of the 'w a -&gt; b' CoKleisli arrow.
I understand that computing fixed points of a higher kind have many applications other than GADTs and I did provide links to two examples (nested/non-regular data types and generic schemes for mutual recursion). Perhaps this could have been pointed out earlier on in the post. The HFunctor definition and discussion is loosely based on the paper "Foundations for Structured Programming with GADTs" by Johann and Ghani, which I also reference in the post.
I simultaneously edited my post and saw this comment. I agree, based on title alone (though it seems there is a Q&amp;A session at the end - perhaps he gets called out). Is there a way to put a little warning box beside the link saying something like *misleading content*. In the same way people warn about gore or whatever in the main reddit? 
I was just curious because I had a similar idea recently and I wanted to compare notes. It sounds a little bit similar to the idea I had, where each game entity was basically a Cofree comonad, where extract gave its current value, and the context was how it would interact with other entities.
That really helps a lot, especially the stuff on ipython. I always feel blind when I'm reverse engineering python code because I have no idea what attributes are instantiated or what types anything has. For sum types, the most common scenario is needing a Maybe. It irks me to use a reserved "false" value like the empty string. The second most common scenario is wanting to return an algebraic data type of results (i.e. I'm decoding a network message which is essentially encoded as a tagged union).
Ten minutes in, I was curious why there is so much distaste for this video on r/haskell. Then I saw the [`Interstate` example][interstate]: new Interform('text') .moveTo(100, 100) .setSize(400, 32) .moveInside() .setBgColor('pink') .select() .setZIndex(20000) .on('escapekey', 'erase'); `unit` is obvious (`new Interform`), but where is `bind`? It confuses me how he makes the connection that `Interform` is a monad without showing `bind`. His [ADsafe example][adsafe] makes even less sense in this regard. He conflates `bind` with function application by introducing a third argument to `bind` (the argument list). JavaScript has a mechanism for partial function application which Crockford's target audience will understand (`Function.prototype.bind`). His feature introduces [a mess of code][arguments-in-bind]: func.apply(undefined, [value].concat(Array.prototype.slice.apply(args || []))) This being 'necessary' would scare me away from monads in JavaScript if I didn't know better. Granted, he does explain that things are better in ES6 (a next revision of JavaScript's standard), but I shouldn't have to wait until a language update before using the monad pattern. I like how Crockford explains that [`unit.method` is monadic][unit-method]. This example has the same problem as the `Interform` and ADsafe examples; `bind` is missing. `unit.method` has nothing to do with monads, and is just for convenience, but Crockford doesn't explain that well enough. [`unit.lift`][unit-lift] (`fmap` or `liftM` in Haskell) should be implemented as a library function, as its implementation can be derived from `unit` and `bind`. I think it confuses readers more to mix `liftM` and adding a method on an object and putting it inside an implementation. Crockford also makes no connection between `unit.lift` and `Array.prototype.map`, which I find really insightful. Why is the monad in the [`unit.lift` with `alert` example][unit-lift-alert] called `ajax`? Crockford does not explain the downsides of the `Maybe` monad (e.g. interacting with other pieces of code) or reasons why you would want to observe `null`/`Nothing` (e.g. error handling). Monads are not magic and the `Maybe` monad does not solve all your `null` reference issues. (It will probably make your code harder to debug, even if JavaScript had native support for the `Maybe` monad.) Promises *can* be monadic. However, he talks about promises by explaining what problem promises solve and how they solve it instead of explaining the monadic nature of promises. He takes seven minutes make the connection *in a talk about monads*. I think explaining the `Either` monad (for error handling) would be much more useful before explaining something as complicated as a promise monad. The fact that none of the questions asked were about monads (except the `do` notation question, which I think Crockford thinks is only relevant to promises) shows that Crockford did not explain monads well enough to develop any understanding in his viewers. [interstate]: http://www.youtube.com/watch?v=b0EF0VTs9Dc#t=1129s [adsafe]: http://www.youtube.com/watch?v=b0EF0VTs9Dc#t=1212s [arguments-in-bind]: http://www.youtube.com/watch?v=b0EF0VTs9Dc#t=1306s [unit-method]: http://www.youtube.com/watch?v=b0EF0VTs9Dc#t=1349s [unit-lift]: http://www.youtube.com/watch?v=b0EF0VTs9Dc#t=1391s [unit-lift-alert]: http://www.youtube.com/watch?v=b0EF0VTs9Dc#t=1465s 
yes - I had to reset from time to time (although I find it indeed easier to uninstall/reinstall the complete platform, after trying ghc-pkg,...) but it's still annoying - I even installed MinGW but even with it I cannot sucessfully compile the few packages I want - guess that means just another reset :( It's like: test YESOD = wipe a few days later: hmm wanna try fay ... well WIPE and so on I now have a Linux-VM prepared with platform and cabal-dev and I'm honestly thinking about just resetting the hole WM everytime I wanna test another thing - but real world development with Haskell? ... sorry but who would give you money for spending half of your time setting up just the env.? It's really a shame - no wonder you have to use shit like Javascript to earn your living - at least JS's tools are working and you can get things done :(
yes - I can get "cabal install" cabal-dev from the git-sources - and it works - but even with cabal-dev I get errors and no I don't think it's a problem with cabal-dev I think one of the problems is that the platform seems to use a (the?) Win32 package that depends on older versions of time(I think) - but I have to check on my other computer tomorrow - I don't wanna fry my install on this one by "messing around" :P
... but maybe you can help me - most of the problems seem to come from me trying to use the bmp - package (see I only want to write some kind of image ... a task so easy in ever other lang. I know and just right frustrating with haskell it seems) - can you point me to a package that manages to write bmp/png/jpg/giv/svg/whatever easily?
interessting - I was hoping for a platform that packs "everything you need" - from some web"framework" to UI and System stuff - just like say the JRE or the .net framework - I don't mind "current packages" - I want a reasonable stable environment I can work against
I'm not a fan of this talk, but I'd be interested in hearing other people's thoughts on it. Here are some of mine: 1. He made the classic mistake of calling the objects "monads". 2. I completely agree that one doesn't *need* to learn Haskell to learn monads (though it helps given the current state of affairs - to use his analogy, it's like only Spanish speaking people currently know how to make burritos properly). However, the idea that you don't need to understand the types is completely bogus. If you don't understand the types, then you don't actually understand monads. He even provides evidence for this when immediately after saying you don't need to understand the types, the very first thing he does is start explaining about the types. He even uses pseudo-JavaScript (ie: JS with type annotations) starting around 12:03. 4. His `MONAD` function seemed to be pointless - why not just have `unit` be top-level? it also seemed to use the term "monad" in a different sense from the rest of his talk - it doesn't return what he calls a monad, so why is it called "MONAD"? Later he actually does stuff in `MONAD`, but that's also when he added extra parameters to bind. At this point I stopped watching, as I got the distinct impression that he had no idea what he was doing. Also, and this is a bit OT, but his whole attitude during the talk really put me off. For example, I thought the context coloring idea was pretty neat, but the "I'm a grownup" comment was childish. There were numerous other things like this in the video. "JavaScript is leading the way" made me throw up in my mouth a little. His implication that he invented the idea of chaining methods was kind of laughable, too.
Might consider putting a link to the second part at the end of the first...
This is a kind notice to politely inform you that your blog broke my alt-left arrow, which normally functions as a shortcut for the back button. Some javascript contraband, but nothing to get in a twist over.
Your name looks familiar to me!
Odd, I'm using a standard Blogger template.
Seems that we have a duplicate identifier!
I'm not compelled that this is the correct distinction. Non uniform types might still be encodable using parameters. For example, we could imagine a type theory that would allow Inductive FullTree (A : Type) : Type := | Leaf : A -&gt; FullTree A | Node : FullTree (A * A) -&gt; FullTree A. actually, that might be legal coq. I don't have an installation handy. The point is that the while general arguments can vary with constructors, non uniform instantiation concerns only the legal forms of recursion. So, while non uniform parameters preserve isomorphism, general arguments do not.
Hm, you're right, that's valid Coq. I'll need to think about this more. More generally, I'm a bit perplexed by the question of what should be seen as general inductive definitions, and what should be explained through type equality constraints, and I'm not quite sure there is a clear answer anywhere in existing research. 
I approved the post since it was made in good faith, but just so you know, this is the *third time* a link to this has been submitted. You probably didn't notice the others because they got downvoted to oblivion, which goes a long ways to telling you what other people's thoughts on it are.
If it makes you feel better, things will get much better with `cabal` soon. The main `cabal` repository already merged in support for sandboxed builds, which obsoletes `cabal-dev`, so once that makes it into the platform your problem will be solved. Also, since you are a Windows Haskell developer, you should consider contributing towards making Windows a better development environment since you have a lot of first-hand experience. Every little bit helps.
And same home country! :)
Thanks. I actually looked for prior submissions, but must have missed them. I'm pretty sure I didn't get a warning about a duplicate link either, but now that I think about it, three are many URLs that lead to the same YouTube video, and Reddit probably doesn't do any normalization.
Yeah Windows development is entirely frustrating. But, I'm not sure if going piecemeal like that is the way to go. May only make it tougher for people to use stuff you write especially if you aren't targeting the libraries in Haskell Platform.
Ah I see, then it's just like any other haskell program isn't it? That type of state handling should be second nature to a haskell programmer.
I feel your pain. My primary development platform is Windows, and when I'm trying to get a project done I spend inordinate amounts of time trying to wrestle the platform. It almost doesn't matter the language if that's language primary platform is Posix based. I don't think installing from source for any language is ever really pain free. I think that's is probably why so many Enterprise apps are written for the JVM, only one environment (usually) to wrestle. I don't know what the solution is, would it be a specially derived version of cabal? wabal, focused on windows? How I've alleviated some of my pain is to have 32 and 64 bit common versions of MinGW. That and pkg-config could help at least to get stuff installed. But binary distribution could require some interesting solutions. As a thought does anyone know how the Mono guys resolve the differences? 
If you have to run the code inside the monad then you are still locked into the framework unless I'm misunderstanding. Can I just take my pick of my preferred routing library, templating library and form handling library and have them all work with WAI requests with a few lines of glue code? Or do I have to worry about implicit state passing being done by those monads? I fairly certain that this is the case. I agree that haskell makes it wonderful to code frameworky stuff *as if* you were not inside a framework compared to other languages but that doesn't mean you get the important benefits of not being in a framework. The big haskell frameworks in particular have also done a better than average job of making their components reusable from what I've seen, but those framework monads remove boilerplate by assuming control of various pieces of your state and flow so they can pass them along to the template or response objects. 
As phillipjf points out, your "index" example isn't. Nested data types and non-regular recursion are theoretically interesting in their own right, but they're not what indices are about in dependent type theories. The indices of dependent types are exactly what GADTs give you, e.g.: Inductive List (A:Type) : Nat -&gt; Type := | Nil : List A 0 | Cons : forall {n}, List A n -&gt; List A (S n) The big difference between parameters and indices is that, when someone hands you some arbitrary value `x : List A n`, if you perform case analysis on `x` then (a) you gain no extra information about `A` because it's parametric, but (b) knowing what constructor built `x` *does* give you new information about `n` because the index reflects the *value*--- conversely, if you can learn new information about `n` then this constrains the possible values of `x`, thereby allowing you to ignore certain branches of case analysis[1]. So, parameters carry no information, whereas indices are what the dependency in dependent type theories is all about. [1] If you're in Agda or a similarly smart language. Coq is bloody stupid however: you need to manually pass in an equality proof in order to get information out of the case analysis, and then you need to manually construct the contradiction in order to eliminate absurd branches.
In constructive/intuitionistic logics in general `(exists x. P x)` is stronger than `~(forall x. ~P x)`. Just because it's absurd to think nothing has a given property, that doesn't actually let you get your hands on something with the property. This is related to the fact that `(exists x. ~P x)` is stronger than `~(forall x. P x)`[1]. Again, just because not everything has a given property, that doesn't mean we can actually get our hands on a concrete counterexample. This is the one of De Morgan's four laws which fails in the intuitionistic/constructive setting. To make things more concrete, try implementing the following two functions: mapExists :: (forall a. f a -&gt; g a) -&gt; Exists f -&gt; Exists g mapNotAllNot :: (forall a. f a -&gt; g a) -&gt; NotAllNot f -&gt; NotAllNot g [1] Assuming you're not a Russian constructivist or otherwise accept the generalized Markov property. If you like GMP, then of course these two are of equal strength. But even still, that only gives you that `(exists x. P x)` entails `(exists x. ~~P x)`. (N.B., `(exists x. P x)` entails `~~(exists x. P x)` regardless of your feelings about GMP.)
I would love to - but to be honest: I don't feel like competent enough to really help, as I'm still trying to learn about Haskell (sure: I got the basics down - but all those fancy extension-stuff, all the little things around cabal, ghc - FFI, etc. ... well I'm still a novice there) BUT: I would love to contribute if I really knew how! For example AFAIK there where projects to bridge Haskell to .net - as there is mono and it's moving in the right direction maybe we can get some Haskell-.net/Mono FFI working - this would reduce most of the pain *I* have with Haskell (meaning: no/few support/packages for your everyday tasks like nice UI tools, simple things like outputing a bitmap, etc. that are at the core of .net and Mono) So if you can point me to some source/community/whatever I would be willing to invest time helping out the haskell-community here or elsewhere as I still hope to use Haskell in the real-world some day (even F# is not half as potent as Haskell IMHO and that's the best functional real-world-language I know of)
Here you go (had to include the bmp&gt;1.2.3.2 or cabal would go and resolve the newest one and then complain thet 1.2.3.1 is missing - see it as a hack as I don't know whats going on). You can find the cabal and the project here: https://github.com/CarstenKoenig/RayTracer/blob/master/RayTracer.cabal Right now it will not do a thing and even a test fails as I just somewhat demotivated by the current problems I have with cabal right now (had a break before/over the holidays and just came back this week to try setting up the project on a new PC and blowing in my face while it did work fine before)
Not all contributions have to be code, and usually the best way beginners can contribute to the community is to share their learning experiences while they are still fresh on their minds. For example, if you do finally get `cabal-dev` working on windows, then you could write up a blog post on how to reliably set it up on Windows and submit it to /r/haskell. People love that kind of thing. Similarly, if you use a library and you found it unclear at first and then finally figured it out, you could contribute documentation to that library so that others don't have the same problem. This is actually something beginners are better at than experts, because experts have already internalized all the flaws of various libraries so we forget about them and what it was like to first overcome them.
He says that he ignores side effects; doesn't undefinedness count as a side effect? (E.g. in Agda one can define a "partiality monad")
&gt; If you have to run the code inside the monad then you are still locked into the framework unless I'm misunderstanding. Sure you're locked in to something, but the point of lock-in is very small and given that, it's as if your code is controlling flow. &gt; Can I just take my pick of my preferred routing library, templating library and form handling library and have them all work with WAI requests with a few lines of glue code? If said routing, templating, and form handling are well-designed, then yes you can--and in fact, we do. Heist is a Snap project, but it's not at all tied to Snap. There's even a [happstack-heist package](http://hackage.haskell.org/package/happstack-heist). The same is true of web-routes (a routing library). There are glue packages for happstack and wai. The happstack glue package is 80 lines of code including comments. It would be a similar amount of code to make one for Snap. And finally (by now it shouldn't be a surprise) digestive-functors is an example of a great form handling package that is completely framework-agnostic. It has glue packages for snap and happstack which are similar in size to the web-routes glue packages. &gt; Or do I have to worry about implicit state passing being done by those monads? I fairly certain that this is the case. No, in the above cases you don't. Haskell makes this possible with things like monad transformers, purity, higher order functions, and maybe type some classes here and there. &gt; I agree that haskell makes it wonderful to code frameworky stuff as if you were not inside a framework compared to other languages...but those framework monads remove boilerplate by assuming control of various pieces of your state and flow This is pretty much my point. Yes, maybe they do assume control, but the above examples show that it's not hard to jump out and call libraries (including APIs provided directly by the frameworks) to accomplish things in a composable way that is easy to maintain.
Yes, a lot of it will be second nature for experienced Haskellers. But Snaplets handle the plumbing for you and do so in a way that makes things more composable than you'd probably get with the obvious approach. It also has the added benefit of organizing things that would have been more ad hoc.
Thanks. I actually link to that sigfpe post from mine, but due to a dumb mistake I stated that the blog was Lambda the Ultimate instead of A Neighbourhood of Infinity. It's corrected now.
Sorry, I didn't consider trying different versions of `bmp` too see if they had this problem. Good catch! I personally use the Haskell Platform with cabal-dev on OS X, Linux, and Windows, and have only experienced minor portability pains. Most packages are well-behaved and portable, and I'm usually able to easily diagnose issues which arrise otherwise. You won't get the bleeding edge (I *really* would like lambda-case), but at least it's easily for anyone to set up. 
um... exists :: f a -&gt; NotAllNot f exists x = NotAllNot $ \f -&gt; f x mapNotAllNot :: (forall a. f a -&gt; g a) -&gt; NotAllNot f -&gt; NotAllNot g mapNotAllNot f (NotAllNot g) = g (exists . f) I think you are confused about the encoding. The reason I used "double negated" in quotes is that this not the correct encoding, but that the system f encoding is isomorphic to existentials (Thing I have long thought was a well known theorem: System F with impredicative universals and implication can faithfully encode all intutionistic second order connectives). `~(forall x. ~P x)` is not the same thing as your `NotAllNot` type. It would translate into newtype BadNegate = BadNegated ((forall x. P x -&gt; (forall r. r)) -&gt; (forall r. r)) which is not at all what you want. The type you want uses *the same r*. I believe that existentials alone are not enough, but I am highly confident that System-F with universals is complete. Note, `exists` is the introduction rule for existentials, while as I said earlier, the newtype is essentially just a wrapper over the elimination rule. One way you could interpret the meaning of the word "constructivists" is that "introduction and elimination rules are all you've got." This does not hold in classical logic: you have no way of proving `(A\/~A)` from just intro/elimination rules--at least not so long as you stick to the natural deduction perspective. You can make classicality into structure (mostly) in the sequent calculus, but you still have something that looks like an axiom: `A =&gt; A`. Another good example of the *same r* thing is the following isomorphism holds in any reasonable intuitionistic logic a = forall r. (a -&gt; r) -&gt; r in Haskeller, "codensity of identity is identity". This looks a whole lot like double negation elimination, but its not. Because it is the same r, and it is univerally quantified, it is easy to get a value out (just pass it the identity function). Proving that it is really an isomorphism is a bit of work since you need to use parametricity, but it holds. As to your footnote: when I have my constructivist hat on, I will only accept a version of the markov principle that is admissible. This turns out to be a relatively easy thing to come up with. But, note that this is not what is going on here, and is even irrelevant to your example code. Given mapExists :: (forall a. f a -&gt; g a) -&gt; Exists f -&gt; Exists g is is trivial to construct the proof that `forall f. ((exists x. f x) -&gt; (exists x. ~~(f x)))` doubleNegate a = \f -&gt; f a theorem = mapExists doubleNegate so then the question is if `mapExists` is valid. Obviously it is: mapExists f e = case e of Exists (fx) -&gt; Exists (f fx)
In strict languages it might be more natural to regard non-termination as an effect, but in lazy ones it works better to view it as one of the possible values of the domain. 
&gt; If you're in Agda or a similarly smart language. Coq is bloody stupid however: you need to manually pass in an equality proof in order to get information out of the case analysis, and then you need to manually construct the contradiction in order to eliminate absurd branches. Which I think is the right way to do, at least for non-linearly-occuring indices. Other you get into too much trouble knowing which kind of axioms you're implicitly assuming on equality.
&gt; At my place of work, this technique has been used to great effect to write a rich set of standardised recursion schemes that work over many custom data types. Could you give an example of a custom data type and of a use case? (Unless the 'Example: A tracing evaluator' is already one of your use cases) Plus what are the advantages of that technique when compared to making Expr (\*) traversable (unless it's impossible and then I'm missing something) and using a product of applicative functors to process it? **EDIT:** Sorry, that was stupid ;) (*) The original Expr, before the HFix refinement: data Expr :: * -&gt; * where Const :: Int -&gt; Expr Int Add :: Expr Int -&gt; Expr Int -&gt; Expr Int Mul :: Expr Int -&gt; Expr Int -&gt; Expr Int Cond :: Expr Bool -&gt; Expr a -&gt; Expr a -&gt; Expr a IsEq :: Expr Int -&gt; Expr Int -&gt; Expr Bool
This kind of talk makes me wonder something: have you ever found really useful (*) to use monads in another language than Haskell? (*) Useful = Gain &gt; Encumbrance Personal experience: monads (+ transformers) in a dynamically typed language (Python for me) implies some hard time (esp. to debug).
You don't need Happstack for that either - I happily use it with Snap.
Something along this is always touted as the go-to example for factoring out induction from datatypes, and the factored-out version is very basic and straightforward. But in any real-life case, you have several, mutually inductive datatypes -- e.g. you might have `Expr`s containing `Ty`s and `Decl`s containing `Expr`s and `Ty`s, and so on. How does this technique apply to these cases? Do you have one type parameter for each member of this mutually-inductive family? Doesn't that get unwieldy pretty fast?
You won't be able to write even a Functor instance for the above GADT. But of course, using HFunctor, you can go on to define higher-order analogues of the above classes you mention, e.g. HTraversable. Our custom data types are mostly language syntax trees, something GADTs are a good fit for. EDIT: updated the blog post to mention the above.
&gt; Nested data types and non-regular recursion are theoretically interesting in their own right, but they're not what indices are about in dependent type theories. No, that's *exactly* what they are. An indexed data type is just a nice syntax for a fixed point at higher kind. The list you describe could be encoded in a primitive type theory as: List = fun (A: Type) -&gt; fix L : nat -&gt; Type. fun (n: nat) -&gt; [ Nil of n = 0 | Cons of exists m:nat. n = s(m) * A * L m ] 
Uhh, you changed the type of `Tree` from one blog post to the next. It turns out that the "obvious" definitions for `return` and `join` for your previous post does actually form a monad. You are right that the modified type does present some complications though.
It's a popular "misconception" that C is fast because it's close to the hardware, and usually forget the decades of work on the miraculous optimizations that C compilers perform. Reminding people that the language is not the implementation doesn't hurt.
I thought applicative was more general than Monad? Monad implies Applicative via (return, ap), no?
There are people who incorrectly (ab)use C as a macro language for assembler, and I think this is in part where the misconception comes from. I've heard it argued time and time again that you can "predict" what the compiler will emit with C. It's true for very specific cases (i.e. if you stick with the same compiler version and microarchitecture) but I wouldn't call it an advantage of C itself, because the standard makes no such guarantees. I've even seen the devs of GCC and Clang say that this is bad practice, and that their tools are not designed with this in mind. I don't think C is a bad language per se, but I think it gets chosen for the wrong reasons more often than people would like to admit.
Thanks :). It wasn't actually needed in the associated literal haskell file.
It actually sounds like there has been much more work done to make the different pieces of these frameworks inter-operable than I was aware of. That's fantastic. All my experience still tells me this is approaching the right place from the wrong direction as it puts the onus on the framework developer to *not* lock their users in by doing extra work. I will admit that you will never get much traction for your project if it's not a framework that removes boilerplate and has a really simple hello world tutorial, even though the non-beginners will have to fight their way out of those handcuffs to get actual work done if they aren't making CRUD pages. I think my experiences are fairly typical in that any type of framework level boilerplate removal would only save me a small amount of time when I'm spinning up a new website, and since I'm not working at a web agency or as a consultant doing small business websites that's not a problem I care about at all. I want to use well tested and nicely designed libraries to solve problems I have while adding features to a modern website that doesn't just do CRUD operations on a database table. Maybe I'm wrong that this is typical but the success of "micro" frameworks in the Ruby/Python world at the moment and the move away from scaffolding/boilerplate removal frameworks as well as the early steps to move away from WSGI/Rake/WAI completely in favour of HTTP seems to suggest that I'm not the only one who is convinced that frameworks are the wrong abstraction layer for professionals.
You should write up something about that and link some code examples, I'd be interested in seeing it.
You even get string literals and an operator. "hello" &lt;&gt; "world" :: Builder and actually, without that type annotation the expression is polymorphic and works for `Text`, `String`, text's `Builder`, plain `ByteString`... :)
I love it, it is fast, pretty accurate and after completion it has even a karaoke like interface, but instead of lyrics you have chords, genius.
I tried it on [this song](http://chordify.net/chords/amiga-protracker-23-jogeir-liljedahl-guitar-slinger-snowwie88) and as far as I can see the result isn't meaningful. Too bad, I've wondered what those chords were for a long time!
I thought the post will be about "extracting chord sequences from MP3's using HarmTrace" in Haskell.
http://ismir2012.ismir.net/event/papers/295-ismir-2012.pdf (this is a great paper, it never occurred to me that transcribing chords might be much easier than individual instruments' fundamental tones. also Chap 17 http://haskell.cs.yale.edu/wp-content/uploads/2013/01/HSoM1.pdf
As the time goes by, I got more and more disenchanted about posts like this. Not because the intent is not noble (thanks for toying with the example, it's always fun trying to do this sort of optimization), but just because, from my experience, they say everything and nothing. Probably part of the comments would be "you can tweak this and your C code would be 2x faster" or "tweak that and your Haskell code will fly", without adding much to the table. At the end of the day, I think everyone knows that abstraction and lazyness comes with a price, both in terms of reasoning effort that of performance. But what I love of Haskell is that we don't have just ONE way to do things. Probably there should be only ONE obvious way to do things, but I love the grain to which I can tradeoff readability/performance to my own needs :) My 2 cents! A.
Well, obviously, yes.
Ah, there was a typo in the last post, thanks for letting me know! I'll fix that. The "Tree" type I presented in the other post actually wasn't even much of a tree with the typo; it didn't hold any information at all except at the Leaves.
I don't want to keep beating a dead horse, but it seems like we're making good progress here, so I'll continue. It seems to me that you're still looking at this from the wrong direction. &gt; It actually sounds like there has been much more work done to make the different pieces of these frameworks inter-operable than I was aware of. That's fantastic. I don't think the work was spent on interoperability. I think it was spent on implementing components and, because of the unique power of Haskell, interoperability ended up being easy. &gt; I think my experiences are fairly typical in that any type of framework level boilerplate removal would only save me a small amount of time when I'm spinning up a new website You're vastly underestimating things here. By your definition, if you go even to the low level of using only warp or snap-server directly, you're still using a framework. To not use a framework, you'd have to make a bunch of network calls yourself. It would take a lot of effort to do this stuff yourself and still get the same level of performance and efficient memory usage that you get from using warp or snap. It just so happens that for these kinds of applications, the boilerplate that you want to avoid is exactly the kind of boilerplate that inversion of control allows you to eliminate. &gt; Maybe I'm wrong that this is typical but the success of "micro" frameworks in the Ruby/Python world You keep falling back to imperative languages and extrapolating experience with frameworks in those languages to Haskell. But I'm saying that shifting to Haskell completely changes the game. The "frameworks are bad" maxim applies to languages that are bad at allowing you to abstract control flow. Haskell is not one of those languages. 
Great to see a physicist here ;-) I am also a physicist, too. As for your question, I think the following reference may help you although it's rather too old. http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.41.1967 I think you can approach to this problem from having N-dim lattice (simply array, why not) and then constructing 2d lattice with context (meaning all the relevant near-point information) out of the original lattice and reducing down to numbers (energy in Ising model) and sum up. That's basically expressed in terms of Functor, Foldable, and Traversable. 
Note that having neighbor information for every point in lattice is not inefficient or duplication of information at all because of sharing. 
I don't know if it is the best solution but something like a map of coordinate tuples to whatever data you need to store per site sounds like a good solution to get a persistent (non-ephemeral I mean) data structure to represent the lattice you mention. For updating you could in theory use lenses (see the lens package on Hackage) but that is probably overkill here.
&gt; abstraction and lazyness comes with a price, both in terms of reasoning effort that of performance The point of both abstraction and laziness is to make reasoning easier, not harder. The price is paid in the predictability of performance, but even that can be overcome with a cost model.
I found that document quite interesting... Do you happen to know more papers applying Haskell to physics problems you'd recommend to physicists?
Why not just compile with `gcc` or `clang` with optimizations disabled? Is unoptimized C really that slow?
Even with `-O0`, gcc and clang still produce faster code than tcc.
I believe OCaml has GADTs now. Scala has a poorly-worken broken approximation of GADTs as well. If you want languages which support something more advanced than GADTs, try Agda or Idris or similar.
OCaml has GADTs since 4.0 yes.
in fact, i don't know many of them. haskell had not been very favorable for numerical calculation because physics (and scientific) problems often need maximum performance out of metal rather than beauty of the approach. I think for high performance computing, generative approach using haskell is practically useful. One interesting paper I can mention is http://www.cse.unsw.edu.au/~chak/papers/polymer.pdf (although this is not a physics paper but chemistry)
Found the following thread had relevant links on the efficiency of purely functional algorithms : http://stackoverflow.com/questions/1990464/efficiency-of-purely-functional-programming
Idris, Agda and Coq have inductive families (as geezusfreeek noted, “super-awesome GADTs”), but no deriving. Idris and Coq have type classes; Agda has “instance arguments”, which can serve the same purpose in some cases. Epigram 2, if it ever solidifies into existence, will be built on a closed type theory that is amenable to first-class generic programming (which will allow things like `deriving`). The way this works is by encoding your types into a universe; then, you can free write generic programs over the “codes” for those types. I hope that I’ve not misrepresented how this works! You might be interested to take a look at [The Gentle Art of Levitation](http://cs.ioc.ee/~james/papers/levitation.pdf) which describes this work.
I think that is almost impossible to write a efficient monte carlo method in a pure functional code since you will need to mutate a lot your system state, but I would love to be proved wrong. The best thing that I [found](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.124.7455) was a haskell code that generate a C code. I tried a lot of different methods in Haskell (using arrays, maps, trees, zippers...) and in the end the fastest code was a [mutable one](https://github.com/aivuk/Simple-Ising/blob/master/ising.hs). 
I'm a computational biophysicist and as much as I love haskell, I find it difficult to apply to my work. Do you find haskell useful in your work? Automatic code generation seems really exciting, but I doubt I have the CS chops to hang.
Awesome idea, if it worked really well it would be fantastic. I haven't found a song which it works really well on yet.
Automatic recognition of chords is hard, in general, but we have a [page with featured examples](http://chordify.net/chords) that work quite ok.
I was talking performancewise: lazyness makes more difficult to spot space leak and most of the times this affect also performance :)
+1 for dimensional, very good package!
This is really cool! There are more and more lazy functional language to JavaScript compilers out there. Mostly Haskell, now also Idris. I wonder if there are some best practices for efficiently encoding thunks and partial application or maybe even threading. Do you want to use closures or fake closures using objects? It almost seems to be there might be a need for a single efficient intermediate lazy machine (plus maybe ffi) as a JavaScript library that lazy languages can target. This way we can optimize at this compilation step at one place and multiple implementations can benefit.
[There's an entire PhD thesis on it -- PDF link.](http://research.microsoft.com/en-us/um/people/akenn/units/programminglanguagesanddimensions.pdf)
&gt; Generalized algebraic data types are one of my absolute favorite features of Haskell. Could you tell us why? I.e. what you're using them for, for instance. Their sole application I know about for now is abstract syntax trees (for EDSLs).
Ouch: http://www.reddit.com/r/programming/comments/16ttqn/when_haskell_is_faster_than_c/ Not quite the same reactions from the folks of /r/programming.
Agda also has a JS backend, I think?
A JS equivalent of the [epic](http://hackage.haskell.org/package/epic-0.9.3) package!
GADTs subsume phantom types allowing their programming to encode invariants. E.g. red-black trees. Also see hoopl.
I think it got a good reaction here because the thesis (which I think is true) is one that people in this subreddit want to hear. It did poorly in /r/programming because it didn't back up that thesis very well. I had the second most upvoted comment in the other thread and I complained that the C code was not, as claimed, written in the most straightforward manner and that the more straightforward manner would probably have been faster. The top comment in the other thread also complains about how poorly the arguments are supported. I think there are definitely situations where a straightforward implementation in Haskell will beat a straightforward implementation in C, but the article does a very poor job of demonstrating this.
Also check out [unittyped](http://hackage.haskell.org/package/unittyped) which has the advantage over dimensional of having a more flexible unit system and is supposed to be easier to define new units in.
[Real World Haskell](http://book.realworldhaskell.org/) : [Chapter 13](http://book.realworldhaskell.org/read/data-structures.html#data.num) [Compile-time "units of measurements" for Haskell](http://code.google.com/p/units-of-measurement/) : [Example](http://code.google.com/p/units-of-measurement/source/browse/trunk/Data/Units/Example.hs) [Adam Gundry](https://personal.cis.strath.ac.uk/adam.gundry/units-of-measure/) [Physical_units](http://www.haskell.org/haskellwiki/Physical_units) (wiki)
Looks good. Also by Brady I see and used by Idris.
I don't want to be the *Correction Guy* but as far as I know recent versions of Idris do not use Epic any more.
If I grep to the code I at least find a function called 'epic'. Maybe it is inlined into the package. Edit: Indeed, not used anymore. compileEpic f t = fail "Epic backend disabled
/r/programming is just as guilty of only being discriminating when it suits their purposes. Witness the "Unreasonable effectiveness of C" claiming C was a high level language getting hundreds of upvotes.
There is [agda-frp-js](https://github.com/agda/agda-frp-js/). [These people](http://research.microsoft.com/en-us/projects/fstar/) seem to be doing insteresting related stuff, too.
Example RBTree with GADTs: https://github.com/yairchu/red-black-tree/blob/master/RedBlackTree.hs#L27 Example AVLTree with GADTs: https://github.com/yairchu/red-black-tree/blob/master/AvlTree.hs#L15
I'd actually be sort of more interested in a "core" with multiple backends -- LLVM, JavaScript, etc. That way we could target compilers at that core and get web compilation automatically. I suppose one could be built on top of the other...
Welcome to typeclass hell.
I'm in high energy physics as well. I use haskell for data analysis. It work way better then C++ &amp; ROOT. When you have to change you program over and over C++ quickly devolves into unmaintanable mess. Same happens to haskell but it's much easier to clean it up and extract reusable pieces. The only problem is lack of libraries. I had to write a lot. Also haskell is useful for not so high performance computing. When you can trade some performance degradation for less efforts in writing code and have library support it works quite nice. 
GADTs aren't just for ASTs, they're for general programming, like structs and enums. GADTs do the heavy lifting of defining new data structures. They're concise, robust, and incredibly expressive. If you add `deriving (Eq, Ord, Show, Read)` to your data structure, Haskell automatically figures out how to compare, sort, print, and parse the data structure. It's generics on crack.
I think you miss my point. I wasn't suggesting this *exact* algorithm could be parallelized. I was suggesting that one can make algorithms that compute the same thing and do use parallelism. Furthermore, there does exist a naive way to parallelize this sort of thing, although I don't know if the performance payoff will be worth it. Split your lattice in to N sublattices. For each "boundary" beween two lattices, introduce a mutex. Spawn some independent threads that go off and mutate various sublattices in parallel, taking randomized locations off a single queue (such that no two threads share the same location at once). For boundary sites, guard your actions by a mutex. This is again a slightly different algo than that described, but one whose properties will be very similar, and which does lend itself to parallelization, various functional techniques, etc. I could imagine using a little more thought and doing something "optimistic" with phases, if that makes any sense, to produce a lockless version of the above algo.
Or you could use [Frink](http://en.wikipedia.org/wiki/Frink). If you are just looking to add dimensions to your calculations, this is a pretty sweet tool, and probably easier than using Haskell for this (though maybe less fun than writing a tool for Haskell that does it).
http://lambda-the-ultimate.org/node/4600 http://www.reddit.com/r/haskell/comments/13t795/hackage_unittyped01_an_extendable_library_for/ http://www.reddit.com/r/haskell/comments/yf277/ask_rhaskell_whats_the_state_of_units_of_measure/ ------------------ I can imagine a patent examiner saying "this is incomprehensible, if I grant it nobody will notice" and now its Oracle IP: http://www.google.com/patents/us7530051
&gt; If you add deriving (Eq, Ord, Show, Read) to your data structure But you need StandaloneDeriving for that: data Foo a where X :: (Show a) =&gt; a -&gt; Foo Int deriving instance Show (Foo a) it works. But this doesn't: data Plop a where X :: (Show a) =&gt; a -&gt; Foo Int deriving (Show) Weird?
Yes. Thank you clarification. I suspectthat in many cases payoff doesn't worth the effort. In many cases process level parallelism is available. For example one may need to run simlations for several parameters' values or run simulation several times etc. Unless you aren't contrained by memory (e.g. very big lattice) easiest thing is to fire several processes.
Not all Monte Carlo simulations require mutation. Consider random walks of a graph which can be lazily generated. I once did something like this as part of a UCT implementation, where after each random walk I would update the path it took in operations linear in the length of the path, the same amount of operations it would have taken using mutation.
&gt; This particular algorithm is not parallelizable at all. This is not necessarily true. The most simple way I can think of off the top of my head (which may even be decently fast if the array is large in proportion to the number of threads you use and your gains from the parallelism outweigh what you lose in boxing and indirection) is to just use an array of `TVar`s and use one thread per core to work on it. The trick to parallelizing this is to note that each step doesn't depend on the *entire* previous state.
Not licensed as BSD. License: LGPL-2.1 
My understanding is that there is some manual mugging involved at that point, but I held off on commenting because I have only used it on demo projects. Dont you need a full fledged CAS system for the units if you want easy new types and ease of equivalence?
But that's not GADT syntax. In GADT syntax, that would be: data Trool where TriTrue :: Trool TriFalse :: Trool TriMeh :: Trool Everything you've said so far seems to be referring to standard Haskell-98 ADTs. If that's the question, you can find those in many more languages - Scala, for one.
Okay, so for your information, these are called *ADTs* (algebraic data types), not *GADTs* (_generalized_ ADTs). Okay, I understand better your original point of _**ADTs**_ being one of the selling points of Haskell. (which application range goes, of course, _far beyond_ just abstract trees) And indeed, GADTs declaration looks a lot like class and methods declaration. (That's the point: being able to specify exactly the types of the data constructors)
great stuff! Thanks a lot
Isn't that what laziness does, turn effects into values? E.g., in (undefined + undefined), which bottom is called? There's a monad in play that sequences them, something like Either (exception) a
&gt; wherein the program is written in an object-oriented language. Use Haskell, and I think you're okay. In fact, since the thesis linked in ky3's comment is from 1996 and the patent claim is from 2005, I wouldn't be surprised if that language is in there specifically to distinguish it from prior art in functional languages.
Idris aims to do this, though not yet in a particularly usable way. There are several intermediate stages which a code generator can be hooked up to. The C backend uses a first order functional language, for example. This is what I'd like to drop back into epic, eventually, so that anything Idris can generate, your epic enabled language can generate too. I don't have time to do this at the moment, but if anyone else has the enthusiasm I could try to explain what would be necessary...
You seem to be using Parsec as a Char parser (ie it works on the level of individual Chars) rather than a token parser. You should have a look a Parsec's [Token Parser](http://hackage.haskell.org/packages/archive/parsec/latest/doc/html/Text-Parsec-Token.html) which allows you to supply a separate lexer to parse the stream into separate tokens and have your parser work on tokens. A lexer would completely remove the need for things like your stringGS function. 
Using `Applicative` instead of `Monad` can clean up your parsers significantly. Checkout `&lt;*&gt;`, `&lt;*`, `*&gt;`, `&lt;$&gt;`, `&lt;$` and friends. Monad syntax with `msum` for alternatives isn't that bad either: myParser = msum [ try $ do thing0 thing1 , try $ do thing2 thing3 , do thing4 thing5 ] 
this is certainly overlapped with "contract" idea. http://hackage.haskell.org/trac/ghc/wiki/Commentary/Contracts can anyone tell difference and what's going on? this kind of things are really cool!
http://skillsmatter.com/podcast/scala/front-end-language-features contract is shown at around 42:00 in SPJ's lecture.
The first thing I made when learning Haskell was a 1-D Ising model very similar to what you describe, the source code here: http://hpaste.org/64529. Immediately after, I addressed obvious performance concerns and put everything into Control.Monad.Random. It doesn't address your questions, I used a list because design goals were to generalize it to an arbitrary Potts model with support for additional kinds of steps (as opposed to Metropolis-Hastings). If you're curious, I can send later versions of the code, which provides support for graphical output, Replica-exchange steps, and arbitrarily-defined Hamiltonians (using pattern-matching).
I wonder how much these dimensional libraries could be rewritten, [now that we have the Nat kind and promoted numeric literals](http://www.haskell.org/ghc/docs/latest/html/users_guide/promotion.html#promoted-literals).
This is the big one! I can't tell you how many parsers I have compacted (in a good way) using Applicative.
&gt; It's generics on crack. Incidentally there's a thing in Haskell we call "data-type generic programming" usually just called "generics" (but not really much related to Java generics) which makes it possible to make many custom classes derivable too. This pretty much requires algebraic types to be doable. [Here](http://hackage.haskell.org/packages/archive/web-routes/0.27.2/doc/html/Web-Routes-PathInfo.html#t:PathInfo) is an example of this. I think it's pretty cool. As for your original question, taking into consideration that you really meant algebraic types and not really GADTs per se... Lots of Haskell-like languages have algebraic types, but it might be more interesting that a language like [Rust](http://www.rust-lang.org/) has them, seeing as Rust is only vaguely similar to Haskell in certain respects. It also has some early support for deriving what it calls traits, but currently only two such traits can be derived if I remember correctly. It is also interesting to study [what makes algebraic types, well, algebraic](http://www.scs.stanford.edu/11au-cs240h/notes/zipper.html).
My understanding is that type systems provide a limited, constrained, logic that is less likely to be abused or misleading and is also quicker to write. That’s the value of a type system like Haskell’s over a complete solver. I think.
Although a lot of the top comments were critical of that assumption, and the article in general. There has always been a strange divide between the opinion of people upvoting comments and people upvoting articles on major reddits. I agree though, there is no useful definition of "high level language" that would admit c as a member.
Agda has a type system that's lets you phrase most logical statements you'd care about in it. The problem in this case is that most logical theories can't be solved automatically. This automatic approach will be okay with certain kinds of arithmetic and certain logical statements, but huge swaths of the automatic theorem proving landscape are probably impossible to do automatically, and many other parts are unbelievably inefficient. The theory of real-closed fields, for example, is decidable, but the decision procedure is one of the few algorithms out there with NONELEMENTARY complexity (i.e., really fucking slow.) Most SMT solvers have a hybrid between real deciders and heuristics, as I understand it, but the solver is sometimes going to say "I dunno" and then you're going to want a stronger type system to fill in the proofs it can't figure out by hand. This is cool stuff but I doubt it'll replace theorem provers anytime soon. 
Here's a dubious version of `literal` done sfvissers sort of way for comparison http://hpaste.org/80998
^This^ - the use of try in an post called "Parsec Patterns" is problematic when try is so often an anti pattern.
We see know why Coyoneda f is isomorphic to f, but not why Free (Coyoneda f) is isomorphic to operational's Program f.
`Monoid`: how do we combine the relation functions? Conjunction makes more sense to me than disjunction, but is that the only way? `Functor`: imagine that `Sieve` is defined as `data Sieve a = Sieve (a -&gt; Bool) [a]`. Then to turn a `Sieve a` into a `Sieve b`, we need an `(a -&gt; b)` to turn the `[a]` into a `[b]`, and a `(b -&gt; a)` to turn the `(a -&gt; Bool)` into a `(b -&gt; Bool)`. `fmap` gives us only the first of these: `Sieve` cannot be made a `Functor` (and thus also not a `Monad`, `Traversable`, etc). instance Foldable Sieve where foldMap f sieve = foldMap f (Data.Sieve.toList sieve) -- a more efficient version may be available
Thanks for the feedback. Parsing tokens would certainly avoid the need for that and could probably speed things up quite a bit. I should probably rewrite ImplcitCAD's interpreter to use them. That said, it wouldn't be as viable in all languages -- if the language is ceases to be regular at the token level, a tokenizer seems kind of silly. Also, I kind of aesthetically dislike splitting parsing up into a lexer and parser.
&gt; Also, I kind of aesthetically dislike splitting parsing up into a lexer and parser. I suggest getting over that then, because that separation makes a *lot* of sense and produces a much more elegant parser. The only time I don't use it, is for tiny languages that I would normally use a regular expression for.
As I said, I don't have any deep expertise in Parsec. I've just personally written a bunch of stuff in it and wanted to share some tricks I came up with. I apologize if I'm presumptuous in describing these as patterns. Could you explain why use of try is an anti-pattern?
Thanks! I've been trying to use applicative more. I haven't internalized it the same way I have monads, yet. As for msum, that certainly works. There's also a Parsec specific function, choice, which takes a list of parsers and does the same thing. But I like the combinator syntax better...
`choice` does not do backtracking. `choice` is just combining multiple parsers with `(&lt;|&gt;)` - it's implemented as a fold with that operator, in fact.
Nice, I like how you align the infix operator (like `&lt;?&gt;`) with the `do` keyword. do parse1 parse2 &lt;?&gt; "parser name" This can also be done with case.
Isn't this bit relatively straightforward and can be seen by formally manipulating the definitions (unlike the isomorphism between `Coyoneda f` and `f`)? Something like this: `Program f` is (or used to be) defined as the following GADT: data Program f b where Return :: b -&gt; Program f b (:&gt;&gt;=) :: f a -&gt; (a -&gt; Program f b) -&gt; Program f b which is isomorphic to data Program f b = Return b | forall a. Bind (f a) (a -&gt; Program f b) which is isomorphic to data Program f b = Return b | Bind (Coyoneda f (Program f b)) Do you see now that `Program f` is isomorphic to `Free (Coyoneda f)`? (I should have probably used different type / constructor names, but I hope it's clear what I mean here). For me, the co-Yoneda Lemma was the key insight. I used to think that one could reason about GADTs in the same way as about plain ADTs, but this is not so. Because of implicit existential quantification, GADTs allow to encode relations that ADTs can't express. In particular, I now understand why the definition of free applicative functor using GADTs actually works (although I didn't check all details).
The beauty of Haskell semantics is that it doesn't matter which bottom is called. All bottoms are equal, from a semantical perspective. You can only observe the difference using the IO monad.
This is very clever, it catches many bugs I inserted while playing with the demo. One limitation though is it doesn't track the constraints of values based on tests using new values derived from that value. Eg the function: {-@ f :: Int -&gt; Int @-} f x | abz x == 0 = 3 | otherwise = 3 `divide` x Doesn't verify, even though it is perfectly safe. This is because it can't deduce that `abz x == 0` being `false` implies `x /= 0`. I'm not sure how important such a feature would be though. It isn't like it can ever mark all valid programs valid without solving the halting problem. I couldn't get a invalid program to be marked as `valid` though, which is a more important test.
I definitely agree. I often think people base upvotes almost exclusively on the title. /r/haskell liked this title, /r/programming didn't.
Do you happen know if there is some book or something like that that goes more into detail of what logical theories correspond to current type systems?
&gt; Because of implicit existential quantification, Implicit existential quantification and implicit type equality constraints.
Oh, that's very straightforward. Nice!
I'd just read up on the Curry-Howard correspondence. I don't have a particular book to recommend, but the wikipedia article explains it decently I think. The idea is that all type systems (within reason) correspond to some form of logic. If you want a reference tome about the automated proving stuff I mentioned, I'd imagine [this book](http://www.cl.cam.ac.uk/~jrh13/) by John Harrison would be an ideal read. I haven't read it myself, but I've played with a lot of his stuff and he knows what he's talking about.
I agree that best practice is always to use cabal-dev. Make sure to use the git version of cabal-dev, though. For some reason, the cabal-dev developers (at galois) have ceased updating the cabal-dev package on hackage, so by now it is completely bitrotted. Could this be your problem?
Oh, never do that! The cabal-dev on hackage is unsupported and usuable. Use only the git version of cabal-dev.
Hah. GHC defines which one is called, and it's observable; the fact that the "Haskell 2010 Language Specification" or whatever doesn't define it is irrelevant.
Posting at the top level, as a summary: Almost all of the pain reported in this thread seems to come from people trying to use the hackage version of cabal-dev. It doesn't work. Please use the git version. And please join in to the collective plea of the haskell community to the cabal-dev developers at galois to upload to hackage much, much more frequently. It was a fantastic contribution to the community to release it, but please, please upload. It takes literally seconds. There appears to be a misconception that uploading to hackage implies some kind of "release". It does not. It is just a way to make it easier for people to use your code. The Haskell Platform, and other more recent efforts, provide means of defining releases.
Not sure this is what was meant by "antipattern", but Parsec is at its most efficient when parsing grammars that do not require arbitrary look-ahead--that is, grammars that do not require the use of "try". "try" makes Parsec slower and more memory-hungry, though I have not been able to get an answer as to exactly how much slower and more memory-hungry. It might be an inconsequential difference in many applications. That said, you can often rewrite your parsers to eliminate the use of "try". For example, to parse either "abc" or "adef", you have try (string "abc") &lt;|&gt; string "adef" when you could rewrite your parsers to eliminate the try: abc_or_adef :: Parser (Either Abc Adef) abc_or_adef = do _ &lt;- char 'a' ((string "bc" &gt;&gt; return (Left Abc)) &lt;|&gt; (string "def" &gt;&gt; return (Right Adef))) This is called "left-factoring." Here is an example with applicatives instead: abc_or_adef = char 'a' *&gt; (abc &lt;|&gt; adef) where abc = Left Abc &lt;$ string "bc" adef = Right Adef &lt;$ string "def" (hopefully that stuff is right; untested; please correct me if I am wrong...) You cannot always left factor, as not all grammars allow it. If you're lucky enough to be writing your own grammar you can try to write it so that it is a "predictive" grammar that will not need "try".
Oh dear, an epic fail as it were. (sorry, somebody had to make that joke).
Alex is probably the most widely used lexer in the Haskell world. 
It's important to make a distinction here: Haskell is very good at **inferring** the type of an expression. That inference mechanism takes your program and generates **constraints** which can then be solved by a certain solver (doing equational reasoning). Why that isn't handled by a SAT solver is probably more related to convenience than any fundamental theoretical reason. However once all Haskell types are inferred, **checking** that a program is well-typed is rather straightforward (at least in the absence of more advanced features). What is different in the case of refinement types is that checking the type of a program itself requires some **theorem proving**, and this is what we want to delegate to SMT or SAT solvers. This is a relatively new approach in the sense that these types can be used to express complex invariants instead of mere sanity checks as is the case in most languages (though the gap is growing thinner).
[Omega](http://omega.googlecode.com) was one of the first to have them, among other goodies.
 data Program f b where Return :: b -&gt; Program f b (:&gt;&gt;=) :: f a -&gt; (a -&gt; Program f b) -&gt; Program f b This is not Program but ProgramView. Maybe the conversion that's made from Program to ProgramView is similar to the effect of Coyoneda on the free monad. But curiously, that's not how works the 'improve' method in 'kan-extensions' work: it uses the usual Codensity method. Is ```Free (Coyoneda f)``` isomorphic to ```Codensity (Free f)```? (I _roughly_ know how the Codensity improvement works (notably its analogy with lists vs. DLists concatenation), by right-associating the &gt;&gt;='s of Free, but I lack the theoretical background necessary to put it into words.) **EDIT:** 'free' package also provides an 'improve' function (Control.Monad.Free.Church.improve), that works by using the Free datatype church-encoding (called here F) and turning it into a Free. Is F isomorphic to both aforementioned datatypes? **EDIT2:** Okay, Edward Kmett explained this here: http://comonad.com/reader/2011/free-monads-for-less-2 Or I'm mixing things up and those are different kinds of improvements?
The HALO paper http://research.microsoft.com/en-us/um/people/simonpj/papers/verify/hcc-popl.pdf has a nice description of how liquid types relate to contracts, in short with contracts the goal is to use essentially the same language (here, haskell) to write specifications, while with refinements, we use a combination of refinement predicates and the type system to write specs. Contracts can be more expressive, and in some ways easier to write as specifications are just haskell functions. OTOH, checking and inference are more complicated, since one has less exploitable structure in the specifications. Also, yes, we hope, someday, to eventually have this included with ghc/haskell-platform but that will still require a lot of work...
Why use the C ternary syntax? Wouldn't it be better to use the more Haskelly if/then/else?
Way ahead of you, if you check the old Idris source :). (As a historical note, the name for Epic arose before that meme. I originally called it "ESC" for "Epigram Supercombinator Compiler" but then James McKinna said, "but it's Epigram to C, surely it should be called Epic!" which was helpful of him. Of course it should...)
Cool, would it be fair to suggest that it works best on songs with a strong baseline?
That is great, thanks for the explanation!
&gt; Operational is designed to offer both a O(1) (&gt;&gt;=) bind operation* and O(1) view function. The Free monad has to choose between them. As I understand it, the O(1) view is brought by ```Codensity (Free f)```, or the curch-encoding of Free (*), or ```Free (Coyoneda f)```, i.e. every technique that fuses the fmaps and ensures right-association of the binds (&gt;&gt;=). Do you say that these removes the ability to have O(1) binds? Because that's the first time I hear of this drawback, but maybe I was misunderstanding the implications. (*) F in 'free' package (Control.Monad.Free.Church)
Where does ghc define this? Last I talked to Simon PJ about this he was very keen on it not being specified and not being observable (as am I). Also, you have it the wrong way around. What one implementation does is irrelevant, it's the standard that matters. :)
If "addressing an abstract architecture" is what constitutes a high-level language, the term becomes somewhat meaningless because you could just say non-assembly language instead. Having the abstraction level being a floating scale rather than a set requirement makes so much more sense. If all programming languages I know fall somewhere on a scale with regards to the level of abstraction, C and Java would both end up on the lower half, language-wise. Therefore, I call them low-level languages. (Or preferably lower-level languages, to indicate that I'm actually comparing them to something else.)
Perhaps I'm being dense, but I don't think this refutes the point I was making. Certainly GADTs are recursive, and not regularly so, but they're not doing the same thing as, e.g., rose trees. With non-regular recursion, we're just saying that for each data constructor the recursive uses are allowed to be instantiated differently than the constructor's result. Performing case analysis on a value of non-regular recursive type does not provide any information about that value's type. You're obfuscating that difference by explicitly packing up a type equality proof as an argument to the constructors, but this is begging the question because it is precisely the fact that the type of type-equality proofs exists which gives you GADTs--- and type equality is not at all necessary for non-regular recursion.
How so? Coq chose to have stupid case analysis because they wanted to fall back on OCaml's case analysis as an implementation detail. Working around this gross limitation requires a mind-boggling as well as mind-numbing amount of boilerplate. Yours is the first claim I've heard that this situation is, in fact, desirable. What sort of trouble is it you have in mind?
&gt; mapNotAllNot :: (forall a. f a -&gt; g a) -&gt; NotAllNot f -&gt; NotAllNot g &gt; mapNotAllNot f (NotAllNot g) = g (exists . f) Hmm, last time I tried showing that to GHC it complained about existentials escaping...
Hmm, it should type check since you are just using the definitional catamorphism. Since writing this I have realized that there is actually another term with the same type mapNotAllNot f (NotAllNot g) = NotAllNot $ \cont -&gt; g (cont . f) I suspect these can be shown to be existentially equal with the use of "free theorems" but I have not done it. They are not trivially equal, and I find that interesting. 
If you don't want to dip into Alex, you can always just use Parsec. I.e., use Parsec to convert `[Char]` into `[Token]`, and then use Parsec to convert `[Token]` into `AbstractSyntax`. That's essentially what things like the `stringGS` function are doing, they're just doing it more implicitly. 
`try` is an antipattern because all too often people sprinkle it everywhere until the parser works. Sorta like how people just enable whatever extension GHC suggests, rather than actually thinking about whether the want or need the extra power. That said, the non-backtracking definition of `(&lt;|&gt;)` is extremely silly. There are performance reasons for wanting such a thing, but that should be relegated to a different operator name ---e.g., one that doesn't look symmetric--- since it is not what people expect out of `(&lt;|&gt;)`. Also, this is one of the places where applicative (i.e., non-monadic) parsers shine: because of their restricted power, applicative parsers can automatically left-factor for you (at least for the obvious cases like a choice of many string literals), which means you don't need to ugly up your grammar with such optimizations.
To expand a little on godofpumpkins' advice, I highly recommend [Sørensen and Urzyczyn](http://www.amazon.com/Lectures-Curry-Howard-Isomorphism-Foundations-Mathematics/dp/0444520775) as a place to get started on thinking seriously about the Curry--Howard correspondence. You can get the basic idea from Wikipedia and numerous other places online, but they're all a bit soft on the details and technicalities (e.g., when is the correspondence truly an isomorphism and when is it not?). In contrast, Sørensen and Urzyczyn are good about both drawing the connections and also showing where/why they break. They also cover some classical logics, which is uncommon in most discussions about Curry--Howard. Another good place to get started is to read up on the Barendregt cube. PTSes are interesting in their own right, but perhaps more importantly the Barendregt cube gives a nice initial map of the territory. The map is incomplete to be sure, but it hits the high points and can help you get situated. That said, most presentations of the Barendregt cube only focus on the lambda calculus side of things; you'll have to make note of the correspondences yourself.
well, maybe it's not spelled out anywhere, but if I evaluate error "a" + "error "b" I think I'll get either a or b, and it will consistently be one of those.
Thanks, this seems to be exactly the kind of stuff I was looking for! And I had never heard of the lambda cube before - if anything I would have expected a name like that to come from the [time cube](http://www.timecube.com/) guy :D
Specifically: * Get actors working in GHC 7 * Host the code on GitHub * Add snippet examples in the docs * Add full standalone examples, including the proper import declarations.
See also: Cloud Haskell 
I actually like actor-style approaches for some problems, but I have to agree that the existing `actor` package doesn't seem very appealing. I'd rather start from scratch, or at least I would if I had the time for starting a project like that...
&gt; There are a bunch of common components that are necessary or useful to write web applications...A "web framework" tightly couples these components I think I demonstrated above that in Haskell these components are not tightly coupled. &gt; So yes, the power of haskell allows to write a better web framework, but that is the wrong layer to optimize. There is much more power in loosely coupled components hanging on a standard request/response object like WAI. The components I mentioned previously are even more loosely coupled than this because they don't even depend on a standard request/response object. Haskell actually doesn't even have a standard. There are three main frameworks, and each has its own request/response types. But we still have fabulous interoperability between components written for different frameworks.
Never said it does. Neither does msum. Note how sfvisser uses "try $"
&gt; This is not Program but ProgramView. No, the type `ProgramView` isn't recursive. I agree that the definition of `Program` I wrote is not how `Program` is defined in `operational`. I was under the false impression that it used to be defined so at some point. However, not everything is lost, I think. The type `Program f` is essentially the type of monadic expressions, i.e., formal expressions constructed out of values of type `f a` for some `a` using the operations `return` and `(&gt;&gt;=)`. Many of these expressions should denote the same value due to the monad laws and should thus be considered equal. This is similar to how one constructs free algebraic structures in universal algebra: one takes all formal expressions, or terms, obtained from a given set using the operations of the structure in question, and introduces an equivalence relation on the set of terms that captures the laws the operations of the structure have to satisfy. Quotients of types modulo equivalence relations are known to be hard to deal with in programming languages. The common approach is to keep the type on which the equivalence relation is defined abstract, and to provide only ways to inspect values of that type that identify equivalent values (i.e., using the interface provided by the type one cannot distinguish two different representatives of the same equivalence class). That's what happens in `operational`, I think. The type `Program` is kept abstract and can only be inspected using views, which are defined so that equivalent values produce equivalent views. The type I wrote is up to notation the type `NF` from the [documentation](http://heinrichapfelmus.github.com/operational/Documentation.html) of `operational`. It's the type of normal forms of monadic expressions. There is a function `normalize :: Program f a -&gt; NF f a` such that two expressions `e1` and `e2` of type `Program f a` are equivalent modulo the monad laws if and only if `normalize e1 == normalize e2`. The function `normalize` picks a canonical representative from each equivalence class, and therefore, the type `NF f a` is isomorphic to the quotient of `Program f a` modulo the monad laws. The type `NF f` is isomorphic to `Free (Coyoneda f)` as discussed above.
I'm not sure what you mean about "falling back on OCaml's case analysis", could you be more explicit? I'm pretty sure the pattern-matching of Coq is internally compiled into a very simple form of case analysis because that is what the kernel supports -- and arguably supporting something simple and well-understood makes the underlying theory more robust. There are different aspects to the question of how to handle pattern-matching on indices. Some of user convenience (and I would agree that Coq could do more by elaborating itself the more advanced arguments in the *subset* of cases where it is reasonably easy to do so; in fact [Pierre Boutillier is working on that](http://www.pps.univ-paris-diderot.fr/~pboutill/files/AIPA_Boutillier.pdf), other of metatheory and future-proofness; that's where I think being careful (to the risk of less convenient) is good. I'm currently confident that things accepted by Coq's handling of pattern will stay correct with our more refined understanding of equality and inductive types (observational equality, higher inductives, whatever) in the next years, while Agda's method is more fragile. See [this agda-list discussion](https://lists.chalmers.se/pipermail/agda/2012/004843.html) for example. Note that, as some discussions above have shown, I'm shooting a bit above my comfort zone here. Apologies in advance for any mistakes.
You can get it to compile in GHC 7.6 within 5 minutes (I just did). Steps: 1) cabal unpack actor 2) remove haskell98 dependency from the cabal file 3) fix imports 4) change `throwTo c1 (AsyncException ThreadKilled)` to `throwTo c1 ThreadKilled` 5) cabal install
When converting [Char] to [Token], how do you keep around the location information, so that errors in the second pass get the same kind of reports as the first pass?
That was really interesting, thanks!
At least for UnitTyped, I can say that using the type-nats git branch of GHC (master doesn't have a solver for Nat types yet, making it rather useless in practice), some complicated examples had a speedup of more than 30x. :)
In this simple case, yes. But there are cases were strictness analysis can change which bottom you get. So, e.g., the compiler can convert a call to error to a loop and v.v. And that's ok, because the program cannot internally observe such changes; they can only be observed in the IO world. 
It's the other way round. `Codensity` brings O(1) bind, but has to lose O(1) view. It's exactly the same situation as with lists and difference lists. Lists have expensive concatenation, while difference lists have a hard time inspecting the first element.
the resulting, somewhat diverging HN thread: https://news.ycombinator.com/item?id=5090717
Also look into POMDP (BDI is pretty outdated) and agentspeak, which can be extended out of the BDI arena.
Are you aware of the -XRebindableSyntax extension? It allows you to override (&gt;&gt;=), (&gt;&gt;) and fail.
I was thinking about this a while back, trying to include Arrow notation as well, and try to clean up arrow notation a bit, getting rid of `proc`, by having more syntactic overloading between the two. The idea is a to bind the syntax to a new typeclass, rather than specifically to Monad or Arrow. In the end, I found that my understanding of all relevant concepts was inadequate for the task.
I wasn't!
Awesome! Can we push the changes upstream?
Use a free monad or operational. Then do notation creates a syntax tree that you can freely interpret to fold using the operators of your choice.
Ahh! I tried `do x; y where (&gt;&gt;) = …` _first_, but wasn't aware that `-XRebindableSyntax` was capable of doing that, thought it was only allowing `if` rebinding. Nice one! That makes things interesting.
It's not yesod, but the fay ide has a bootstrap front end: https://github.com/faylang/fay-server What do you mean when you say integrate? Is there anything you need over and above producing html with the correct structure and classes?
Yesod 1.x comes with bootstrap bundled by default. See https://github.com/yesodweb/yesod/wiki/Changelog
Funny, I just finished reading your free monads post, and now you're talking about it to me. I'll be like that Sixth Sense kid in a few days, _I… I see free monads_. I think free monads could work for this. Maybe. Can you show an example of how to do [the list example](http://www.reddit.com/r/haskell/comments/16zrsp/generalizing_do_notation/c80wmfr)?
So this covers all my cases: or = do mzero mzero Just 123 where (&gt;&gt;) = (&lt;|&gt;) list = do 1 2 3 [] where (&gt;&gt;) = (:) mo = do "abc" "def" where (&gt;&gt;) = (&lt;&gt;) With the exception of the `ap` case for `(&lt;*&gt;)`, which you can get away with if you reverse the order: ap = do pure 3 pure 2 (,,) &lt;$&gt; pure 1 where (&gt;&gt;) = flip (&lt;*&gt;) But that's a little… odd. Still, I'd probably prefer it to writing out the `&lt;*&gt;` operators.
Just something basic. I was just very surprised at the lack of examples when I googled 'yesod bootstrap'
The list example is not well-typed, because in `do` notation you would expect that in: do m1 m2 ... that `m1` and `m2` have the same type (aside from the return value), but they can't if you can prepend `m1` onto `m2`. However, a slight variation on your proposal would work, using `Free ((,) a`: elem :: a -&gt; Free ((,) a) () elem a = liftF (a, ()) list = do elem 1 elem 2 elem 4 ... that generates: Free (1, Free (2, Free (4, Pure ()))) ... and you can convert that to a list: toList :: Free ((,) a) r -&gt; [a] toList (Pure _) = [] toList (Free (a, x)) = a:toList x toList list = [1, 2, 4]
Yesod is a web framework, while Bootstrap is a collection of static client-side stylesheets and javascript libraries. Integrating them should amount to serving the static content. Should be be able to write a resource line along the lines of: /bootstrap/js StaticR Static getStatic /bootstrap/css StaticR Static getStatic
That is rather nifty that you can rebind (&gt;&gt;) local to the do statement.
That last one can be written as: ap = do pure 3 pure 2 pure 1 pure (,,) where (&gt;&gt;) = flip (&lt;*&gt;)
I thought the `proc` notation allows a programmer to use arrows in a "pointful" manner, just like `do` does. If that's true, then `do` is just a special form of `proc`: all monads are also arrows (via `Kleisli`).
Yes, that is my understanding too.
Good point. Nice.
Hm, this kind of negates the point of using do-notation. I don't need free monads to do that, I can just use tell. list = do elem 1 elem 2 elem 3 where elem = tell . return But that didn't save me any syntactical overhead at all. In fact it added some. The list example *is* well-typed, that's why it compiles. That's the point of rebindable syntax, that you're not restricted to the Monad type-class and its type constraints.
If the list example is well-typed, then what happens if you write: do r &lt;- 1 r -- or 'f r' in the general case
This would be interesting, since it would allow code in the same module to use, say, ido (for indexed monads) and do, whereas `RebindableSyntax` requires local rebinding for that. Though I can't say I find any of these specific examples compelling or particularly easy to read.
Well, the `1` is wrong. I left `&gt;&gt;=` as `concatMap`, so it should be a list.
Definitely would be better --- thanks for the suggestion!
&gt; tweaking it took to make xmonad a viable real-world program What tweaking? It went live 4 weeks after we started, and has been remarkably stable for 5 years now...
I do not have a good overview of IPA based systems and their advantages and disadvantages. &gt; So, what does the agent model bring to the party? Multi-Agent Systems (MAS) are distributed platforms hosting autonomous software agents. A common characteristic of MAS's is the model in which agents interact. Agents have beliefs, and the only way they manipulate the beliefs of another agents is with agent message passing. Agents have mailboxes, and they expect messages of specific types, often called "performatives", such as request, accept, reject, ask etc... The above description does not clear up where I would use agents over actors or vise versa. Could you provide a concret example of the agent based solution vs the equivalent actor based one. Or point me to one on the web. I read the following and they did not clear this up for me. http://en.wikipedia.org/wiki/Java_Agent_Development_Framework http://en.wikipedia.org/wiki/FIPA &gt; Agents have beliefs, and the only way they manipulate the beliefs of another agents is with agent message passing. Additionally with out more description this sounds like you could be passing around Bayesian nets/evidence or sending simple messages like you normally see in actor based examples.
The wiki article on [BDI](http://en.wikipedia.org/wiki/Belief-Desire-Intention_software_model) spells out some of what I was looking for.
as someone who always checks out the comments on the haskell articles on HN, I don't think it's accurate to say that haskell has received a lot of "fanboying" there. People tend to upvote random haskell-related articles, usually written by beginners on their impressions of the language, and then other people complain loudly and bitterly about how we're all such fanboys, peppered with a good amount of sneering about words they've heard like "monads" and "functors". It's all unbelievably boring and we'd all do well to let people write whatever blog posts they like and get on with our lives. 
This is overkill, just add it to your apps static directory and call touch Settings/StaticFiles.hs Then in your defaultLayout in Foundation.hs reference it with: pc &lt;- widgetToPageContent $ do addStylesheet $ StaticR css_bootstrap_css this will work so long as you added it to static/css/boostrap.css 
&gt; It's exactly the same situation as with lists and difference lists. Lists have expensive concatenation, while difference lists have a hard time inspecting the first element. For this we have Data.Sequence. (Yes, I remember now having seen that operational was to free what Seqs were to lists) I guess the other optimizations (namely ```Free (Coyoneda f)``` and the church-encoding ```F```) serve the same purpose, i.e. making O(1) bind (by accumulating fmaps, like DLists accumulate concatenations) but loosing O(1) view. One thing, though: the optimization (the 'improve' function in 'free' package) is not simply using ```F``` instead of ```Free```, it's about using ```F``` when building and _then_ turning ```F``` into ```Free``` at the last moment (when it's time to view). Does that give you the best of both worlds (like operational) or is there still something I miss?
The problem is that real programs don't fit within a blog post. However, I'm making an effort to find time to blog about snippets of my own work, which is entirely in Haskell.
No, I'm not obfuscating -- I'm *deobfuscating*. Indexed types are an example of a recursive type of higher kind. Rose trees are a fixed point at kind `* -&gt; *`, and length-indexed vectors are a fixed point at kind `Nat -&gt; *`. So `Vec : Nat -&gt; *`, and this is why `Vec 5` may have different inhabitants than `Vec 0`. The equality proofs just make visible a desugaring step in the compilation pipeline. (The main alternative is to define a type-level function which computes on the natural number index, and Edwin Brady can probably tell you a story about the cases when you can optimize the first version into the other.) 
&gt; This article is in response to an earlier one comparing Haskell and C, which made the claim that Haskell beats out C when it comes to speed. Blatant lie. The point of the original article was to show that Haskell _could_ beat out C when it comes to speed _in certain situations_ [*]. The article was specific in the end that people wanting performance badly enough and having enough time _should directly invest in C_. It's a shame to twist the truth thusly. People, there's a point where too much simplification creates _lies_. And for God's sake, they should stop saying nonsensical stuff like "language X is faster than Y". **Programs** are fast, **compilers** are optimized to generate fast code! Compare what is comparable. A shame, because the rest of the article is a quite interesting reading about examples of micro-optimization techniques. [\*] **EDIT**: To downvoters: I'm not saying the original article was doing it right. It had its fair share of poorly-backed up statements. I'm just setting the record straight regarding its _intent_.
I totally agree. Success (or not) stories like those of Warp or XMonad are far more tale-telling and lesson-teaching.
No but there are somewhere on the web slides describing the architecture of XMonad. Not very long to read and quite easy to grasp if I remember well, and showing that a simple architecture can be the backbone to some real-life use cases.
&gt; People tend to upvote random haskell-related articles, usually written by beginners on their impressions of the language I think we had enough of those posts. Too much information builds noise.
Was that compile time or run time? I assume there wouldn't be an impact on run time, since it's all static anyway?
I'm not sure if free theorems would help prove the equivalence, but it's a standard CPS distinction. The original version performs the work of `g` eagerly, whereas the new version pushes `g` onto the continuation. (Whether we view `f` as being pushed onto the continuation depends on whether we're in call-by-name or call-by-need; the latter version can be guaranteed by using a let-binding to explicitly float `f x` out of the continuation's lambda.) If we (a) didn't have bottoms, and (b) performed evaluation under lambdas, then they'd be the same. Still, it is interesting that they're not trivially equal.
&gt; hanging on a standard request/response object like WAI &gt; they don't even depend on a standard request/response object. Haskell actually doesn't even have a standard Are you kidding me? Fine. Replace every mention of "request/response object" with whatever term you feel describes WAI. &gt; The components I mentioned previously are even more loosely coupled than this because they don't even depend on a standard request/response object How would I write a routing component that would interoperate with all the haskell frameworks? Can you write such a component without either the author or the users knowing the implementation details of the frameworks you want to support or even which frameworks exist? That's the level of loose coupling I'm talking about and I have either been completely misusing and misunderstanding Snap or this isn't possible and I'm right.
But this isomorphism fails when the type constructor is not a functor. For instance, this basic example in operational: data Log o r where Log :: o -&gt; Log () In this case, `Log o` is not a functor, so `Free (Log o)` is not a monad.
Ignore the downvoters. Easier to throw a rock than make something with it.
You wouldn't receive any confirmation, but you would receive a bounce message if something went wrong. The list archive is here http://www.haskell.org/pipermail/beginners/2013-January/thread.html so you can check if your message showed up.
Compile time. Run time there is very little performance penalty anyway, dimension-aware multiplication and division is optimized to just * and /. Only coercions (and therefore addition and subtraction) have some impact, as they need to do a multiplication and a division every time.
&gt;The above description does not clear up where I would use agents over actors or vise versa. Could you provide a concret example of the agent based solution vs the equivalent actor based one. Or point me to one on the web. I read the following and they did not clear this up for me. Take a look at a canonical auction example. The implementation of a book-trading scenario in Jade is in Sections 2 and 4.4 of: http://jade.tilab.com/doc/tutorials/JADEProgramming-Tutorial-for-beginners.pdf Perhaps the biggest distinction, is would be something I'd like to see built as part of a Haskell based MAS system, is message structure. In CloudHaskell, you can send any serialized types you like to other processes. Application level types are arbitrary, whereas the FIPA standard sets out ACL message structure. Jade comes with a GUI to send dummy messages, which might help explain the message structure: http://jade.tilab.com/doc/tools/DummyAgent/images/dummy_agent.gif The ACL message structure specification is spelled out in full: http://www.fipa.org/specs/fipa00061/XC00061D.pdf In the absence of any higher level clever BDI/POMDP reasoning, standardizing messages is what I'd like to see. Note also, that ACL messages contain important delivery meta-data. One does *not* say 'send msg target', but simply 'send msg'. Within the message itself is information about which agents should receive the message, and indeed to which agent the receiver should reply to with the response. It is up to the agent platform to do the necessary message exchange work (which would sensibly be implemented as an agent).
Thanks very much I had looked at the archive but it wasn't updated as far as I could tell, I will check back when I'm at a computer, It might be the case that I tried to post using an email address not signed up to the mailing list, if that might cause a problem ? Thanks for your help 
In that case I would expect you should get a bounce message but I'm not certain
There's a great paper that explains monad transformers, too: [Monad Transformers - Step by Step](http://www.cs.virginia.edu/~wh5a/personal/Transformers.pdf)
I couldn't get `where` to work reliably when I was going back and forth from one use of `(&gt;&gt;)` to another; there is also the trouble that one can't write `where ((&gt;&gt;),(&gt;&gt;=)) = (&gt;&gt;),(&gt;&gt;=)` to go back to a 'real' monad. This is an extremely debauched version of a parser from a couple days ago: {-#LANGUAGE RebindableSyntax,CPP#-} import Text.ParserCombinators.Parsec import Control.Applicative ((*&gt;),(&lt;*),(&lt;$)) import Prelude #define parsing let ((&gt;&gt;),(&gt;&gt;=)) = ((&gt;&gt;&gt;&gt;),(&gt;&gt;&gt;=)) #define attempts let x &gt;&gt; y = try x &lt;|&gt; y literal :: GenParser Char st Expr literal = do attempts do parsing b &lt;- do attempts True &lt;$ string "true" False &lt;$ string "false" OBool// b &lt;?&gt; "boolean" do attempts do parsing a &lt;- many1 digit &lt;* char '.' b &lt;- many digit ONum// (read (a ++ "." ++ b) :: Double) do parsing a &lt;- many1 digit ONum// (read a :: Double) &lt;?&gt; "number" do parsing string "\"" strlit &lt;- many $ do attempts '\"' &lt;$ string "\\\"" '\n' &lt;$ string "\\n" noneOf "\"\n" string "\"" OString// strlit &lt;?&gt; "string" &lt;?&gt; "literal" infixr 1 // f // x = return (f x) data Expr = ONum Double | OString String | OBool Bool (&gt;&gt;&gt;=) :: GenParser a b c -&gt; (c -&gt; GenParser a b d) -&gt; GenParser a b d (&gt;&gt;&gt;&gt;) :: GenParser a b c -&gt; GenParser a b d -&gt; GenParser a b d ((&gt;&gt;&gt;=),(&gt;&gt;&gt;&gt;)) = ((&gt;&gt;=),(&gt;&gt;)) 
Very cool. I wonder if this could work well with so called "Hybrid Type Checking" to make it more flexible in cases where it can't verify a constraint (assuming there are cases like that ). 
He uses the notion of simplicity that relates to ease of learning for a particular audience. If he went through the Kolmogorov simplicity path, he'd find himself, again, at the lambda calculus...
In theory, this scheduler should be only slightly slower than the current scheduler when you're not using priorities (pay as you go.) It's not there yet, but I don't think there is any theoretical barrier to getting it that fast.
This is neat. One comment - on your list comprehension slide, I think it's worth saying that list comprehension [ x | x &lt;- s, p x ] is equivalent to mathematical set builder notation { x | x ∈ s, p(x) } rather than the universal quantification that you currently have.
I feel if simplicity takes him to eliminate first class functions but keep subtyping in his language we have a very different understanding of simplicity. From what is in this post I see his language end up with a lot of boilerplate and very few ways of abstracting away things into libraries.
It _did_ receive a lot of fanboying, about two years ago. I know, because it's why I started learning Haskell. (I find it ironic in hindsight; one of the things I now know having learned Haskell is that a great deal of the fanboying was factually inaccurate. Ah well, worked for me I guess.) Now it has toned down, but the haters are still around. I actually consider this a positive for Haskell; it's a sign that we're coming out of the trough of disillusionment and headed up the slope of enlightenment in the [hype cycle](http://en.wikipedia.org/wiki/Hype_cycle).
Ha, perhaps you're right. Maybe my objection is that HN folks want to ascribe this fanboyism to the "haskell community" when they're really talking more about a subset of HN users. I dunno.
Exciting stuff. I would love to be able to adjust thread priorities for a network of Chan producers/consumers. Good luck! 
As soon as you introduce priorities you run into priority inversion problems. There must be a better way to specify system behaviour. For instance, specifying timing/latency constraints directly. Priorities are too far removed from the actual domain requirements.
Ah! I've been under the wrong impression about WAI then, I thought it had wider adoption (apparently based on yesod's naming choice), the lack of documentation around it should have been a clue. Routing was perhaps a bad example as it's easy, and often preferable, to not implement routing with a middleware type pattern. But what about the more canonical examples of middleware? Like gzipping a response or some other action that requires access to request headers or parameters?
I'm not very familiar with schedulers. Can you explain how giving constraints instead of priorities would help prevent priority inversion - or in this case, how no task with a less strict constraint would prevent a task with stricter constraints from running?
I call it “then”. And “sequence” also works, when “then” is inconvenient in English conversation.
See the [Earliest Deadline First](http://en.wikipedia.org/wiki/Earliest_deadline_first_scheduling) scheduling algorithm. Basically, if EDF can't schedule your tasks such that all deadlines are met, then your schedule is literally impossible to achieve: &gt; EDF is an optimal scheduling algorithm on preemptive uniprocessors, in the following sense: if a collection of independent jobs, each characterized by an arrival time, an execution requirement, and a deadline, can be scheduled (by any algorithm) such that all the jobs complete by their deadlines, the EDF will schedule this collection of jobs such that they all complete by their deadlines.
See also: [pronunciation guide on Haskellwiki](http://www.haskell.org/haskellwiki/Pronunciation).
Yes, SPJ is worried about this too. What I'm curious is how much of this we can defer to building priority-sensitive concurrency primitives above the RTS, and how simple we can keep the RTS implementation. For example, stride scheduling supports ticket transfer, which can be used to temporarily boost low priority processes which are causing problems for other folks. I don't think timing/latency constraints are very practical for a system that is also using stop-the-world GC; at the least, it will also be somewhat removed from domain requirements as well. (As an added note, one thing I'm quite interested in is using priority for *isolation* purposes, e.g. in conjunction with SafeHaskell)
Hmm, so what would be an example of a closure that isn't identical to partial function application? 
That's a misleading page. A closure is really an implementation technique for first class functions. partial application is something you *do* to a function, a closure is something that *exists* at a low level to represent a function.
I tend to think of (&gt;&gt;) as 'no bind'.
I would be interested in seeing this used to make unamb more fair when used recursively.
I've always thought it would be mildly interesting to use haskell-src-exts and grammatical framework to to parse haskell source files and write them back out as spoken english. So for example: f :: Int -&gt; Int might be something like: f has the type Int to Int. Of course, there are a lot of possible ways to read that.. which is one of the reasons for using grammatical framework ;) 
A function that encloses a variable x and takes an argument y, and returns x incremented by y - that is, the enclosed value changes when the function is called. Note that this isn't a pure function. I'd type out the definition in lisp but I'm on my phone. 
Anonymous classes in Java. (See [my comment](http://www.reddit.com/r/haskell/comments/172w75/whats_the_difference_between_closures_and_partial/c81r7go) above.)
A closure is a function wrapped up with some free variables. Partial application is one way to obtain a closure.
Yes, that is exactly the kind of use-case that I'm thinking of!
I'd like to hear or see the lecture as well. Is there any other media?
A partial application might give you a closure. Closures are an implementation technique, partial application is a language concept. 
Yes, in fact the "lAssert" (or liquidAssert) example illustrates how one can do a kind of hybrid check. In essence, the "liquidAssert" in the example STATICALLY VERIFIES (by requiring that the precondition holds, assert :: {v:Bool | (Prop v)} -&gt; a -&gt; a but you could drop the precondition, and instead write: assume :: p:Bool -&gt; a -&gt; {v:a | (Prop p) } assume p x | p = x | otherwise = error "yikes, runtime check failed!" which does NOT enforce that "p" holds but instead allows you to assume it in the output. This, in essence, gives you a hybrid check where at run-time the property "p" is checked to hold, and where the verifier can optimisitically assume, at static verification time, that "p" holds (of the value returned by the call to "assume".) Does that help? Thanks!
As I understand it, EDF only works well in situations where the total load is less than 100% CPU. It's used in real time systems where overall load is kept low, but work needs to be scheduled to guarantee response times; once load increases, it does a poor job of sharing contended resources fairly. Haskell threads are frequently used for parallel processing, i.e. to ensure that enough work is generated to keep the CPU fully loaded. I don't think EDF would be much use to Haskell; garbage collection already makes it less than ideal fit for real time systems.
I realize you are going towards more of an introduction to the style, but trimAllArtistCredits = runIdentity . traverse (Identity . trimArtistCredit) can be replaced with trimAllAritistCredits = fmap trimArtistCredit
I was expecting this to end with something like: trimArtistCredits = last._acJoinPhrase %~ rtrim where last = taking 1 . backwards $ traverse I couldn't find `last` in the lens library.
http://hackage.haskell.org/packages/archive/lens/3.8.1/doc/html/Control-Lens-Cons.html#v:_last
Here's a shameless plug for the HLearn library I'm working on, and the function [parallel](http://hackage.haskell.org/packages/archive/HLearn-algebra/0.1.1.0/doc/html/HLearn-Algebra-Functions.html). It takes any computation over a semigroup and makes it utilize multiple cores in the most efficient way possible (i.e. a fan-in reduction). I would imagine lots of language processing operations could be expressed as a semigroup. Maybe it will be useful to you, maybe not. At least looking at the source can give you some ideas on how to use Control.Parallel.Strategies.
Language processing: could that be NLP or lex/yacc/code gen-type stuff? Haskell covers both pretty well, just need clarify.
Wow, what school do you go to that has a class with Haskell in it?
I also like the [orc](http://hackage.haskell.org/package/orc) library for high level concurrent orchestration.
Ah I am mostly NLP-focused, thanks for pointing that out. I do a bit of the flex/bison parser building stuff but it's more of a hobby than anything else. 
If nothing else, the source looks like a great place to start learning (my goal is to write something of my own with cluster computing in mind, so this is actually quite good- thanks). Thanks for the reply. 
Thanks. If you have any questions, please ask. The source isn't nearly as well documented as it could be at this point. BTW, in theory, the parallel function would work perfectly well over a networked cluster. I just haven't gotten around to writing that version of the function yet.
I look forward to poking around in it that's what she said
&gt; Ah! I've been under the wrong impression about WAI then, I thought it had wider adoption I'm sure that was the intent, but that's not the reality. There are a few other projects using it, but IMO the Haskell web ecosystem is too young to have meaningful standards. Snap was the first server to come out using iteratees to implement a fast robust server that had constant space guarantees. Warp+wai followed awhile later. Then after awhile, warp switched to using conduits. Recently there has been a lot of discussion and innovation in streaming libraries like conduits, pipes, etc. It's very much an open problem and the jury is still out on which one will become the most widely used. &gt; But what about the more canonical examples of middleware? Like gzipping a response or some other action that requires access to request headers or parameters? I'd say use the same approach used by all the other examples I've mentioned: factor the problem into something that doesn't depend on a request/response object. Then it's just a little glue code to get it to work with your framework of choice.
"Everything is a function" is false.
I have worked a fair bit with `distributed-process` and I think at this point it just is not worth it unless you really are using multiple computers. Think of it more as equivalent to MPI--not something meant for most users. Hopefully this will change, but we still have a ways to go. On the other hand `async` is fantastic. For simple IO concurrency `async` is so easy it is stunning. People should use `async` in IO code by default--if you want it to scale, and have easy to read code, use async.
http://idontgetoutmuch.wordpress.com/2012/04/01/solving-a-partial-differential-equation-comonadically/
For NLP it would seem that you've hit pay dirt with Haskell: http://nlpwp.org/book/
So one could do that in Haskell to create an internal state?
Scatological endeavors with a scalpel. I commend that, I suppose. Personally I prefer to [look at PHP from a distance, if at all](http://www.youtube.com/watch?v=nnun8y7r8_U).
Oxford at least used to teach Haskell (I forget whether as the first programming language). Cambridge and Edinburgh use SML as the first language (somewhat ironically).
It can, but it doesn't really help provide the understanding for the jump we're about to make. I think if I were going to mention that, I should first mention that: map trimArtistCredit ≡ fmap trimArtistCredit And then that `fmapDefault` shows that fmap f ≡ runIdentity . traverse (Identity . f)
I'm trying to show how people can invent the general principles themselves, and the post felt long enough without showing yet more ways to do it :)
I went to De Montfort University, we had a brilliant Lecturer for Haskell and FP. Learnt a lot from him.
That is a pleasant reading ! 
a fine introduction to netwire, delightfully written
I thought it was hilarious and, perhaps because of your style, instructive in seeing your thought process. Thanks for this. :-)
I start thinking that Lenses are like iPhone's apps: There is one for everything.
In Copenhagen we use SML as the first language. Then it goes something like Java, C/assembler, SQL, Python, Haskell, Erlang, Prolog.
Useful for the [Inception monad!](http://donsbot.wordpress.com/2010/12/05/control-monad-inception/)
My favourite part: &gt; Okay, so reactive-banana or netwire. Seems like installing netwire will be faster, because typing cabal install netwire is faster than cabal install reactive-banana. Nice read.
That depends. I think xmonad core might fit into a Steve Yegge blog post
This is spam!
Well you could use the state monad to create something behaving that way, but functions must be pure in haskell
Woah nice, thanks!
I love the emphasis on testing starting to creep into Haskell. What I'd really like now are some solid resources on designing for testable impure/monadic code. Testing pure functions couldn't be easier, but a lot of my programs end up pushing a lot of the error-prone operations out into IO. I've yet to figure out a good way to test this code to my satisfaction. I haven't found any libraries for mocking/stubbing external resources in Haskell for the most part. For instance, Ruby has the libraries VCR and WebMock which can stub out external HTTP calls so that your test code does not have to rely on some external service or spin one up itself for each test. Maybe this would be a good topic to an intermediate Haskell book.
&gt; a lot of my programs end up pushing a lot of the error-prone operations out into IO. You may want to have a look at the IOSpec package. There are several other ways to "mock" monads/functions in Haskell: - use a typeclass: all the operations that should be mockable should go in a class. You use only that class in your code and then make several instanciations ("back-ends"): one for testing (e.g.: a Writer monad that logs calls to verify them afterwards), one for real execution (IO). This is useful is there is an obvious way to assiociate each back-end to one type. - use the package 'operational' or free monads: express the set of your operations as a datatype (GADT for operational), you get a monad for free out of it, use it in your application, and can then build several interpreters that will progressively "unwind" code expressed in that monad (idem, one interpreter for testing, one for real IO...). This is less straightforward and implies more data-construction/deconstruction but offers a dynamicity that the typeclass option does not provide (possibility to create interpreters online, i.e. while your app runs, for instance). - **EDIT:** use ImplicitParams (see my comment below). Note that I never used the second option myself for testing purposes (I used it for other things, like modeling the language of the operations that an agent in a game can execute).
&gt; I love the emphasis on testing *starting to creep into* Haskell. Haskell has actually *led* the state of the art in testing for well over a decade. Other languages are only beginning to catch up.
QuickCheck is the most awesome.
You're thinking only of QuickCheck or of other testing techniques?
Updating a grid sounds like a natural fit for the costate comonad (although sadly I can't find many references): * http://blog.sigfpe.com/2006/12/evaluating-cellular-automata-is.html * http://idontgetoutmuch.wordpress.com/2012/04/01/solving-a-partial-differential-equation-comonadically/ It might also be worth checking out repa which was designed for solving problems requiring parallel updates to arrays: * http://www.haskell.org/haskellwiki/Numeric_Haskell:_A_Repa_Tutorial 
I'd be rather interested to read from you on the ins and outs of using QuickCheck well. I find that a lot of times for pure code I can manage to encode invariants into the type system, which makes QuickCheck less useful, or in the case that I have effectful code I have a hard time coming up with the invariants for those effects that I expect to hold true.
While I think QuickCheck is a fabulous tool, in the projects I work on I find unit tests much more common and useful than QuickCheck tests. It seems to me that much of the real world code that is out there is not very amenable to nicely defined properties that can be tested with QuickCheck. I would imagine this is why people tend to be more familiar with unit tests.
There are many other testing approaches, generally inspired by QuickCheck, that have come from the Haskell academic community.
I think you've got the arrow of causality pointing in a wonky direction. People are more familiar with unit tests mostly because they haven't heard of QuickCheck at all. The closest approximation that's moderately well known is fuzz testing, which is far less useful. I don't agree with your comment about unit tests being more useful than QuickCheck tests, though I don't quite disagree with it either. Unit tests are often cheaper to write, though of course QuickCheck subsumes them (i.e. you can write unit tests using the QC library). But in my experience it's not a big deal to write good, useful QC tests that exercise real-world impure code... once you've trained yourself into the approach. Where I can unambiguously agree: it's certainly easier to test pure code than impure, and having code be well-factored is even more important for testability (regardless of technological choice).
Have you read the corresponding LYAH chapter about State? It's usually a much better entry point than RWH on the things covered by both. http://learnyouahaskell.com/for-a-few-monads-more#state
In addition to the above resources, I've been working on a POS tagger which I'll release to Hackage "any time now". There are a few bits I want to iron out before release; but let me know if you're interested.
Seconded. It often* seems like QuickCheck has a bit of a square peg/round hole when I try to use it -- I would be really interested to see how others use it. *Not always, by any means, but often enough to make me wonder how others are applying it in cases that I've found awkward. 
Typically you need to think a bit more rigorously about your program. If it's hard to write QuickCheck properties, it's even harder to prove correct. Proving a program correct usually revolves around _deriving_ the program from the specification, as well as a proof of correctness. So, yes, sometimes writing QC properties is hard, you should design your program around the QC properties, not the other way around.
I have found no significant advantage for untyped languages over Haskell-style statically typed languages, and I found several major disadvantages. I reason about my programs _denotationally_, not _operationally_. I don't want to think about what my program is _doing_, just what it _means_. This is (next-to) impossible without a type system to help me along. Other things like referential transparency and so on all help to make this easier. The "scribble, smudge and smear" approach to programming is one that I am very bad at. I prefer to sit down and carefully determine a solution to my problem. Sometimes I will appear to do nothing for hours on end as I mull over exactly how I should structure my code. I think that this is not the norm for programmers, but it's why I find Haskell natural to program in. Speaking more generally, I think the reasons companies like Facebook use untyped languages is entirely cultural, and there's absolutely no reason why a company couldn't be equally successful using static types. I also think that Paul Graham is arguing against static types like in languages like Java which tend to be very unexpressive and, due to having to write all of them explicitly, are barriers to easy refactoring. Haskell's types are helpful, Java's types tend to just get in the way. (BTW, YouTube is now completely written in Java and C++)
At the very small scale, i.e. exploration and playing around with definitions in the REPL, I don't find static typing (in haskell) feels like a straight-jacket. Type inference means you don't have to type most types and if you're not getting the types to match up then your run would mostly fail even if it wasn't caught by a type checker[0]. At the large scale, i.e. either large complex systems or those I expect to require frequent maintenance/extension over a long life, I find static typing essential to ensure high quality software. If there is a cost involved in meeting this then it's one well worth paying for well engineered code, and one that large scale systems in dynamic languages also have to bear (often in the form of unit tests) but without the same backup from the computer. (Why do the worrying myself when I can get a compiler to do it for me.) There may well be a middle ground where thinking about types slows you down but the software is not long-lived enough to care about solid engineering. Its not a one that I've personally felt, but then I often find I sit down and think about software problems with real paper and pencil [1] rather than the metaphorical one in one of Paul Graham's smudgeable programming languages. One issue is that you never know when that exploration is going to become something that does something useful, followed by something that people start to rely on, and before you know it it's the core of a large system that you wish had been well engineered after all. There are countless examples of software starting out this way. To be honest, I'd rather be using a language which backs me up as the software grows from the start. [0] In haskell, the biggest holdup for me is often getting the functions I want in scope. i.e. the effort of getting imports right. I don't think this is better or worse than in typical dynamic languages. I'm sure it could be fixed for me with better tooling. [1] Actually, in recent years pencil and paper has increasingly been replaced with digital replacements, but it remains exploration through notes and scribbles rather than any sort of runnable code.
Sadly it's written in Go, which is a badly designed language, and types very much do get in the way there.
I disagree. It is not an advanced type system, that's for sure, but the types do not get in the way.
The type system is less expressive and more cumbersome than that of Java, and only a little earlier I was arguing against Java's type system as being cumbersome. Do you disagree with this statement as well?
Very much so. Couldn't disagree more. I really doubt you've actually [tried Go](http://tour.golang.org).
Actually, QuickCheck-style testing is supported by at least some mainstream-ish unit testing frameworks. While unfamiliarity may still be a factor, I suspect the bigger obstacle is that real codebases written in imperative languages tend to be built in a way that's antithetical to clean QuickCheck-style tests. For that matter, often with pre-existing codebases you're lucky if regular unit testing is tractable, never mind anything nicer.
I may have been misinformed, but I was told by Googlers when I worked there that YouTube was being rewritten largely in Java.
I find that it is quite the opposite. Haskell types tend to guide me directly toward the code I intend to write without allowing me to get lost in the weeds. If I don't actually know what my goal is, the types at least tell me when I'm really being stupid. If I didn't have a history years ago of using dynamically typed languages, I wouldn't understand why one would prefer to delay the discovery of blatant bugs in his code. My own experiences show that this can be chalked up to mere ignorance about what static types can do, or at least that was the case for me.
In many programming languages I often end up feeling limited in my explorations. The consistency of their design is limited to that of the language creator. The languages that I have tried which are more mathematical in nature or try to follow laws are not limited by the consistency and knowledge of their author, at the very least not as much. This means in languages that follow laws it is easier to explore what is possible through trail and error and intuition built from experience, because they are not as limited by the authors forethought, the limits are those of the laws used which generally are greater then what any author could implement piece meal.
I completely agree about the advantages of static typing in the very small and large scales you mentioned. I've had trouble exploring Python's standard library in the REPL ("okay, but what type should this undocumented argument be?") and I've been through large project refactorings at work (in Java) where the compiler was a great help. My doubt is more about the middle ground you mentioned. The initial growth phase when the code base is still not very large and code is added and removed rapidly to experiment with new features. Paul Graham mentions the pencil and paper design in the essay: &gt; For example, I was taught in college that one ought to figure out a program completely on paper before even going near a computer. I found that I did not program this way. I found that I liked to program sitting in front of a computer, not a piece of paper. Worse still, instead of patiently writing out a complete program and assuring myself it was correct, I tended to just spew out code that was hopelessly broken, and gradually beat it into shape. Debugging, I was taught, was a kind of final pass where you caught typos and oversights. The way I worked, it seemed like programming consisted of debugging. &gt; For a long time I felt bad about this, just as I once felt bad that I didn't hold my pencil the way they taught me to in elementary school. If I had only looked over at the other makers, the painters or the architects, I would have realized that there was a name for what I was doing: sketching. As far as I can tell, the way they taught me to program in college was all wrong. You should figure out programs as you're writing them, just as writers and painters and architects do. I'm not sure his advice is applicable for everyone or in every situation, but it's interesting.
I see. I always thought the backend was in Java and C++ with Python for the front end, but I'm not sure about the proportions or current evolution.
Java type's system is cumbersome and needlessly verbose. Go's is not. It is terse and expressive enough to be useful, i.e. provide compile-time sanity checks at the same level of e.g. Java. Sure, this is subjective to some degree, but claiming that Go is more cumbersome than Java is completely ridiculous to me, and shows that you have, at best, a superficial understanding/background of/in Go, or that you spent 10 minutes looking at it when it was first announced as a prototype internally, years ago. I don't know of a better way to demonstrate this than to ask you to take [the tour](http://tour.golang.org/) -- within the first couple of pages you'll see how Go provides similar guarantees at compile-time whilst being very terse. Basic type inferencing via := prevents having to redeclare types when they're already known (from a function's type signature, or a `make(T)`.) Type signatures contain the information they need to, and nothing more (e.g. a function that takes 3 strings can be defined as `func foo(a, b, c string) { ... }` I invite you to explain how this is not less cumbersome than Java, a language that's notorious for having just about the most (needlessly) verbose type system out there.
That post seems very interesting, thank you (the link in the submission appears to be broken, here is the [new one](http://happstack.com/clck/view-page-slug/15/happstack-fay-acid-state-shared-datatypes-are-awesome))! Shared datatypes are the most interesting thing about node.js for me, but I'd prefer to avoid using Javascript outside the browser. Haskell + Fay is certainly a much more appealing idea. :) 
Go is explicitly typed, like Java. It has _no_ parametric polymorphism, _unlike_ java, which make its type system strictly _less_ expressive. It, like Java, has _no sum types_, which is a terrible omission that can only be emulated with interfaces, which are _extensible_ when they shouldn't be. With Java, we have the ability to seal classes from extension using `final` to remedy this, whereas making a sealed interface in Go requires hacks. Java has a notion of mutability control in the type system, also with `final`. Go has no such control, which is appalling for a "concurrent" language. Go has no statically checked exception mechanism -- Java's is, while primitive, at least present. Note: I'm not talking about verbosity, I'm talking about expressivity. Verbosity is less of a problem in Go than in Java, I'll grant, but Go's type system is still monstrously cumbersome.
Pleasant from beginning to end. :)
I wouldn't consider the State monad giving you 'mutability' as much as 'shadowing'. edit: but yes good explanation
In Málaga, Spain, we learn Haskell both in Mathematics and Computer Science as core courses.
I don't have a lot of experience with dynamically typed languages, but I'd guess it's not so much about preferring to delay bug discovery as opposed to avoiding the burden that strong static typing *may* cause, especially during the fast-growth, high-experimentation phase of a project. One example is the case I mentioned in the OP about needing to add IO to a previously pure part of the code in a Haskell project. Do you have experience in situations like these?
Do you feel the mathematical approach provides enough advantages in naturally "messy" environments such as web development as well?
&gt; Testing pure functions couldn't be easier, but a lot of my programs end up pushing a lot of the error-prone operations out into IO. While testing code involving IO actions is still going to be necessary in places, it's possible you might not need as much code in IO as you think--in particular, keep in mind that any code polymorphic in choice of `Monad` instance is actually pure. To illustrate, consider the basic structure of the following code: guessNumber :: Int -&gt; IO () guessNumber n = do putStrLn "Guess the number!" guessLoop n putStrLn "All done." guessLoop :: Int -&gt; IO () guessLoop n = do putStr "&gt; " i &lt;- readLn when (i &lt; n) $ putStrLn "Too low!" when (i &gt; n) $ putStrLn "Too high!" if i == n then putStrLn "Good work!" else guessLoop n This is a toy example, but the straightforward imperative style is representative of how a lot of real programs can be written; this is awkward to test, particularly in real-world scenarios that involve more than console input/output. With some refactoring though, the situation changes. Start by abstracting out the recursion: guessNumber :: Int -&gt; IO () guessNumber n = do putStrLn "Guess the number!" runLoop (loopStep n) (== n) (putStrLn "Good work!") putStrLn "All done." runLoop :: (Monad m) =&gt; m a -&gt; (a -&gt; Bool) -&gt; m b -&gt; m b runLoop step check done = do i &lt;- step if check i then done else runLoop step check done loopStep :: Int -&gt; IO Int loopStep n = do putStr "&gt; " i &lt;- readLn when (i &lt; n) $ putStrLn "Too low!" when (i &gt; n) $ putStrLn "Too high!" return i The `runLoop` function is completely pure--just specialize it to the identity monad, or whatever other monad you like. You can even take things a step further: runGuessLoop :: Int -&gt; IO () runGuessLoop n = runLoop (loopStep (putStr "&gt; " &gt;&gt; readLn) putStrLn n) (== n) (putStrLn "Good work!") runLoop :: (Monad m) =&gt; m a -&gt; (a -&gt; Bool) -&gt; m b -&gt; m b runLoop step check done = do i &lt;- step if check i then done else runLoop step check done loopStep :: (Monad m, Ord a) =&gt; m a -&gt; (String -&gt; m ()) -&gt; a -&gt; m a loopStep prompt write n = do i &lt;- prompt when (i &lt; n) $ write "Too low!" when (i &gt; n) $ write "Too high!" return i Essentially the entire program is contained in the combination of `runLoop` and `loopStep`; where `runGuessLoop` gives the original functionality, you could just as easily feed in other things, e.g. run it in a `State` monad with a pre-set script of "user input" to pull from. Of course, the code has gotten noticeably larger in all this. On the other hand, you're also more likely to end up with reusable code this way. The `runLoop` function, for instance, is very generic and could easily be moved into a library (say, something like [the `monad-loops` package](http://hackage.haskell.org/package/monad-loops)). Another useful technique, if you have a clearly-defined set of `IO` primitives needed, is to bundle them up inside the environment of a `ReaderT` and thereby make almost your entire application parametric in the choice of monad--all you have to do is run it with a different environment and suddenly all your IO-heavy logic is running in a `State` monad instead, or something along those lines.
The type system helps exploration in two ways: * Libraries are clearer and easier to use because the types guide you towards the correct usage * You are braver about trying new things because you know the compiler will catch more mistakes Referential transparency helps because there is less implicit state flying around. As a result you very often get a working solution on the first try because your thoughts are clearer. For the specific case of IO that you mentioned, I always define a monad transformer stack with a polymorphic base monad. Then I can always add IO at any time.
&gt; Go handles errors by returning values that implement the error interface, not by throwing and catching exceptions Returning values in a _product type_, which is, 99% of the time, the wrong way to return error values (Haskell's Error monad gets it right, using a sum type). What does my usage of the language have to do with the arguments I'm making? I'm saying Go's type system is less expressive than Java's. You're saying.. I haven't used Go sufficiently? Do you expect me to go away and write lots of programs in a demonstrably inferior language for no reason other than to prove a point?
If you like Erlang, then you actually want the distributed-process library.
Google for "Monad Transformers - Step by Step". I'd link it but I am on my phone.
That may be the case. Haskell attracts many people from the sciences and with interest in programming languages who work in a different way than the startup web developers Paul Graham probably had in mind. I'm not sure what kind of hacker I am, both approaches are interesting. :) Thank you!
Terseness is not the issue. Java is absurdly verbose, but here in /r/haskell we know how to make languages terse without giving up power! Verbosity is just an issue with surface syntax--it does not really concern the relative values of the type systems. "Java with less crummy syntax" is actually easy from a language design point of view. Go's lack of parametric polymorphism is simply inexcusable (IMO). It makes programming much harder, and *does not come with benefits*. Almost *all* go programs use dynamic type casting that would not be necessary in the presence of parametric polymorphism. The argument for the lack of parametric polymorphism is invariably "simplicity," but having variables in types so long as all quantification is at the top level does not add significant complexity as ML and Haskell demonstrate. Go should have been designed with parametric polymorphism in mind. Variable types would behave just like interfaces when it came to calling conventions. Go's lack of mutability control makes it much less suitable for concurrent/parallel programming. Go's most compelling feature (Rob Pike's concurrency stuff) is crippled by this. Go's "type inference" is atrocious, and does not have to be. Type inference is decidable in systems [much more flexible than Go](http://arxiv.org/pdf/1104.3116.pdf). This is a solved problem. Go is significantly **more verbose** than it should be. Go's built in concurrency primitives could have been not built in and instead provided by a library with fairly small changes to the language. Perhaps it is worth it to have them, but I think this goes against the idea that it is somehow a "minimal" language. Go has no sum types. I think this is a mistake, but what ever. So yes, Go is much more terse than Java. Actually I think it is an attractive and usable language designed by some very smart people. But it is total shame that they didn't "get it right" when it comes to these things. 
The polymorphic base monad seems very interesting. So all your functions are in this monad? Do you find the extra polymorphism makes the code more complex?
&gt; If I don't actually know what my goal is, the types at least tell me when I'm really being stupid. Reminds me of a time when I forgot about `filter`, no idea how. I searched for the signature that I needed and my brainfart was over.
I have basically never wanted to add IO to a pure function except for debugging, for which `Debug.Trace` works fine. I can't even think of a situation where one would want to do this. For that matter, I can't really think of a situation where it technically makes any sense. To draw from a recent project, say I have a function that chooses a few players from some larger set of players, trying to maximize the amount of fun that their game will be. What would it even mean to add IO to such a function?
I wasn't thinking of printing exactly, but of other IO actions such as traveling directories and reading files, for example. Such as a datatype that previously could be constructed by a pure function but now needs information from the filesystem that can't be easily passed as a single parameter.
I use the `transformers` library, so all functions are polymorphic in the base monad by default. So if I write: {-# LANGUAGE NoMonomorphismRestriction #-} import Control.Monad.Trans.Class import Control.Monad.Trans.Reader import Control.Monad.Trans.State f = do x &lt;- lift ask put x ... and load that into `ghci`, it will automatically infer the correct polymorphic type: &gt;&gt;&gt; :t f f :: Monad m =&gt; StateT s (ReaderT s m) () If the base monad is polymorphic, then you can run it purely like so: &gt;&gt;&gt; runReader (runStateT f 2) 0 ((), 0) ... but if I later go back and add an `IO` command: f = do x &lt;- lift ask lift $ lift $ print 1 -- or: liftIO $ print 1 put x ... then everything still works out: &gt;&gt;&gt; :t f f :: StateT s (ReaderT s IO) () All I have to do is just change `runReader` to `runReaderT`: &gt;&gt;&gt; runReaderT (runStateT f 2) 0 1 ((),0) Polymorphism is one of the most powerful tools in a Haskell programmers arsenal. Any time you have something that you don't know what you will want to do with it later, just make it polymorphic. So if you don't know what you want to with your base monad, you just make it polymorphic! That same polymorphism trick applies to everything else. You have a data structure and you don't know if you want to expand it later or not? Just make one field polymorphic: data MyStruct a = Struct { importantData :: String, tally :: Int, other :: a } ... then if you need to expand the structure later on, you just stuff whatever you want inside the `other` field. Functions that never use this field will be polymorphic over the `a`: iDon'tUseTheOtherField :: MyStruct a -&gt; String iDon'tUseTheOtherField (MyStruct str n _) = str ++ show n ... so when you do decide what you want to do with that field later, all those functions will still work without modification.
I expect you to know how Go works before you claim it is more cumbersome than Java. Clicking through the tour is sufficient to gain an understanding of how Go isn't a "demonstrably inferior language." (By your extremely flawed logic, the success of a language depends entirely on the number of features it supports, not how the language is used. By that logic, every language ever designed is demonstrably inferior in every way to Lisp or C++.) For the record, there are [~8 different, standard ways](http://article.gmane.org/gmane.comp.lang.haskell.libraries/6382) to return errors in Haskell, and the choice is completely inconsistent across different packages. I love Haskell, but that was a spectacularly bad example for proving your point. This will be my last reply. You'll need to find another thread in which to mention you used to work at Google (without mentioning in what role) ;)
I'm not sure, but I think that can be done, too. Probably, you can do everything with `unsafePerformIO`. As long as you don't care about data interleaving, you're good to go. This seems more like a case for small functions and composition skills, though. 
I'm going to assume you originally had a function with roughly this type: mkProject :: Configuration -&gt; Project Instead of changing mkProject to use IO, would have done something like this: mkProject :: Configuration -&gt; WhateverOtherDataINeeded -&gt; Project loadExtraStuff :: FilePath -&gt; IO WhateverOtherDataINeeded To reiterate, if I have a perfectly good pure function, I just don't see the purpose is in adding IO to it. It's not a procedure. It's just a function.
That is very interesting, I will take a close look at the transformers library, which I had skipped so far. I guess I've become wary of adding polymorphism everywhere after a few bad experiences with C++ templates, where every extra type variable increased code verbosity and produced even worse compilation error messages when things went wrong. In Haskell I think these problems disappear though. Thank you very much for the code examples!
That was the original type. But the extra data wasn't very simple to obtain and depended on the configuration itself. Something like: the configuration lists these 3 sub-modules, so I'll need to access the filesystem and inspect their sub-directories to obtain extra information, guided by other information in the configuration file (such as whether each sub-module is a web application).
LYAH's explanation is far better. I've read through it before, but maybe I needed the second time for it to start to click. I'm going to have to play around with LYAH's example to really get the hang of it, though.
I'm not somewhere I can work on this now, but I'll give it a go when I can, thanks!
This I totally agree with. Don't get me wrong, Haskell has a vastly superior type system (the Go devs classify type systems as "just taxonomy"), and mutability makes concurrency trickier even using the CSP-style channels (Control.Concurrent.Chan), and no monads/purity means no STM when you really need it. Haskell wins, hands down, in terms of being a cutting-edge, powerful language. Go is a (small!) language that's fast, easy to learn, and easy to use (which can really save your butt when working in teams where you don't get to hire every haskeller you know..) To be clear, the major point that I disagree with is that Go is somehow more cumbersome than Java, which is by far one of the most cumbersome languages I've ever used. I believe Haskell and ML (but Haskell particularly) are great demonstrations of how strict and powerful type systems can be as terse as in a language that doesn't offer the same controls, and can be exploited with great success by people who understand them/have bothered to learn them/have worked for employers who had the foresight to ask them to learn it. And don't get me started on the separation of impure and pure functions... :) Edit: s/less cumbersome/more cumbersome/
I have not built anything but toy projects for myself in the web area so I cannot give any concrete examples from the web programming world. So I do not have a good idea of what "messy" implies in the web development world. Borrowing a quote that you quote elsewhere in the thread. &gt; For example, I was taught in college that one ought to figure out a program completely on paper before even going near a computer. I found that I did not program this way. I found that I liked to program sitting in front of a computer, not a piece of paper. Worse still, instead of patiently writing out a complete program and assuring myself it was correct, I tended to just spew out code that was hopelessly broken, and gradually beat it into shape. Debugging, I was taught, was a kind of final pass where you caught typos and oversights. The way I worked, it seemed like programming consisted of debugging. I think the above method of programming is made easier by Haskell's type system, not harder which seems to be the impression some have.
Ruby is the other language that interests me. :) Do you feel any significant differences in your ability to explore and add/remove features quickly using Haskell and Ruby?
I use `Control.Concurrent.Chan` for everything so far. The only STM thing that appeals to me in practise (it's very interesting in theory) is `TChan`, which is essentially the same from the outside.
In general with Haskell I try to avoid depth of computation and shoot for composition instead.
Incidentally, the canonical response to Hackers and Painters is [Dabblers and Blowhards](http://idlewords.com/2005/04/dabblers_and_blowhards.htm). It's an amusing essay. pg doesn't get nearly enough criticism for his Tom-Friedman-esque blithering.
I wouldn't say it's significantly different in either language. I always feel more confident removing things in Haskell (since in Ruby I have to grep the codebase for the thing I'm removing and hope I don't miss anything). And, as others have said, documentation in terms of types is much nicer than Ruby docs which just say "some value goes here" and you have to mess about a bit. Ruby's introspection is a bit better (everything can be printed, more or less), but enough Haskell things derive `Show` that this isn't a huge problem.
You're welcome. Polymorphism is MUCH cheaper and clearer in Haskell because it wasn't tacked onto the language as an afterthought.
Haskell-style actors (with distributed-process) really only make sense for, like the library is named, distributed processes. If you're interested in single machine, shared memory, concurrency, then there's no need for a fuller actor model. As people have mentioned, Chans (or TChans, which give a wider range of safe/atomic operations) give you actor-style message passing. MVars, which are concurrency primitives in Haskell are conceptually just one-element chans, and on their own can let you build a whole range of concurrency constructs. STM is really about a different set of issues, having to do with building more complex concurrency constructs easily and safely. Which is to say that you may prefer to use STM rather than classic concurrency constructs by default, even if you do very little explicitly stm-like. Even without doing anything fancy, STM primitives let you atomically acquire all-or-none of a set of locks or the like, or block on one of a number of events, with no special concerns about deadlock. There's a whole range of different concurrency constructs, suitable for different purposes. But it's hard to give a clear answer without some problem to begin with. What in particular are you trying to do?
Of course that critique is just lazy hand-waving. I suspect if you really thought about it, this gut feeling that dynamic languages make one more productive probably has a lot to do with the Expression Problem at its core (the dynamic language solves the problem by throwing up its hands in surrender). To get away from my own hand-waving above: I've been working on a large amount of dev-ops code (provisioning servers, build/deploy systems, etc.) in [fabric](http://fabfile.org), and can say that the majority of my time is spent on issues that would be solved (or could be solved) if fabric were written in haskell. I could elaborate on that, but I think it wouldn't be very interesting.
Well you can do this only for finite lists but if you don't mind the big spoiler you can read everything about your question and my remark here: http://www.haskell.org/haskellwiki/Foldl_as_foldr
&gt; rearchitect your entire program. If you were "architecting" your program correctly, you probably wouldn't need to do this. Tekmo offers good advice: &gt; I always define a monad transformer stack with a polymorphic base monad. Then I can always add IO at any time.
`lens` solves this problem without re-architecting your program. The only requirement is that you need to anticipate which fields of your data type may change and make them polymorphic. For example, let's say you have the following data type: data Game = Game { playerData :: [(String, Int)] , _score :: Int } pureGame :: Game pureGame = Game [("Mac", 1), ("Tom", 4)] 5 ... but you think that the `String` part might change (i.e. you might need to replace it with an impure `IO String` later). All you have to do is make your type polymorphic on that field: data Game a = Game { playerData :: [(a, Int)] , _score :: Int } deriving (Show) pureGame :: Game String pureGame = Game [("Mac", 1), ("Tom", 4)] 5 Now, let's say that times change, as they always do, and instead of storing `String`s, you want to instead store `IO String`s in those game fields (i.e. rpc calls). For simplicity, I will use `getLine` to fake an RPC call: impureGame :: Game (IO String) impureGame = Game [(getLine, 1), (getLine, 4)] 5 So far so good. We made our data type polymorphic on that field, so it gracefully handles storing the new type of value there. But what if we want to execute all the "rpc calls" and replace them with their pure result. In other words, we want the following function: fetch :: Game (IO String) -&gt; IO (Game String) Notice how similar that looks to the signature of `sequence`. Wouldn't it be nice if we could just automatically sequence that? With `lens`, we can sequence it automatically if we supply one thing: a lens to the polymorphic field. The library lets you derive these using Template Haskell, but I'll go ahead and hand-derive this one just to make everything explicit: name :: Traversal (Game a) (Game b) a b -- i.e. (Applicative f) =&gt; (a -&gt; f b) -&gt; Game a -&gt; f (Game b) name f (Game p s) = fmap (\abs -&gt; Game abs s) (sequenceA $ map (\(a, b) -&gt; fmap (\a -&gt; (a, b)) (f a)) p) With that lens in hand, our `fetch` function becomes trivial: fetch = sequenceAOf name Let's try it out: &gt;&gt;&gt; fetch impureGame "Mac"&lt;Enter&gt; "Tom"&lt;Enter&gt; Game {playerData = [("Mac",1),("Tom",4)], _score = 5} However, that's not the only thing we can do once we have our `name` lens in hand. Let's combine or `name` lens with some other lens functions and see all the new functions we just got for free: &gt;&gt;&gt; :t forMOf name forMOf name :: Monad m =&gt; Game a -&gt; (a -&gt; m b) -&gt; m (Game b) Or what if I want to just set all the names to something: &gt;&gt;&gt; :t set name set name :: b -&gt; Game a -&gt; Game b Or what if I want to just get the first player name in the list? &gt;&gt;&gt; :t firstOf name firstOf name :: Game a -&gt; Maybe a So Haskell not only permits this, but you also get a whole bunch of extra features you never even had to ask for.
Not really, the STM supports Alternates and IO does not. If your app doesn't obviously benefit from transactions, think about common use cases like Chans that need to deal with shutdown, you have a few options (or variants of..): * Chan (Maybe a) -- shut down when Nothing is received * Chan a -- use killThread to tear it down, sometimes with complicated MVar/IORef/Chan use to signal completed work With the STM you can do more interesting and (I think) safer control flow given this pair: * (TChan a, TVar Bool) -- read a value /or/ a 'global' boolean The less obvious advantage of splitting a (Chan (Maybe a)) into a (TChan a, TVar Bool) is the memory/GC benefit of deferring allocation of the unifying type until the value is read, instead of when it is written. So given a Chan version: writeChan c $! Just x -- notice that we've queued up a 'Just' that may survive to gen1 and eat into our GC time... -- particularly so because the Chan is unbounded switchThreads x' &lt;- readChan c case x' of Just x'' -&gt; process x'' Nothing -&gt; shutdown -- hope your writes into the Chan are synchronized for shutdown or -- exceptions will be thrown once there are no reader threads vs: -- would be saner to use a TBQueue but we'll use a TChan writeTChan c x -- explicitly prefer doing work until the Chan is empty, then shut down x' &lt;- atomically $ Just &lt;$&gt; readTChan tc &lt;|&gt; (readTVar tv &gt;&gt;= \ shutdown -&gt; case shutdown of True -&gt; return Nothing; False -&gt; retry) case x' of Just x'' -&gt; process x'' -- GC benefit: the Maybe value is allocated immediately before pattern matching -- and then discarded, very cheap to clean up Nothing -&gt; shutdown
&gt; Maybe this would be a good topic to an intermediate Haskell book. Or a [blog post](http://www.haskellforall.com/2012/07/purify-code-using-free-monads.html)
&gt; I think you've got the arrow of causality pointing in a wonky direction. That very well may be the case. &gt; I don't agree with your comment about unit tests being more useful than QuickCheck tests, though I don't quite disagree with it either. Well, in my experience with packages like snap and heist, the quickcheck properties that I'v written seemed like massive overkill that never really manifested any tangible benefit to me by catching bugs--whereas my unit tests have definitely saved me pain. Again, may causality arrow may be wonky here, but I'm very interested in learning how I might have gone about writing QuickCheck tests to handle all my test cases for these packages.
One thing that takes a while to get with the State monad is that what you get back is a function. So runState won't actually do anything, runState will just return you the function which is locked up in the State newtype wrapper. This function could be a chain of computations joined with bind (&gt;&gt;=) but if you want it to actually run you still have to pass it an initial state. This means that every line of do notation you see with the State monad is also a function waiting for a state to do something. Really though it's just a way to get a tuple out of a couple of inputs ! You have to do all the work to make that tuple represent your return value and your state. Another thing to understand about the State monad is that if you do 4 "put"s in a row, the entire resulting state will be the 4th put, not some combination of the 4 things you put.
What about http://hackage.haskell.org/package/DSTM-0.1.2 for distributed processes? Is it used anywhere?
So ```forMOf``` is like ```over``` but for impure functions? I've been searching through lens' doc for a function like that.
Maybe so, but that doesn't explain how to address this problem at all. What are you composing here? How do you determine that?
Actors are a more high-level concept than STM, in the same sense that functions are a higher-level concept than gotos and stacks, so you can't really compare them. Actors are *trivially* implementable in, or via, STM, the reverse is not true. When you compare STM-driven Haskell actors to Erlang, I think the main advantage of STM is that you can send channels over channels. For example, when sending a message to some actor, you don't need to subscribe to the actors messages in general, you can just send a back-channel or TVar alongside with your message. Once the actor computed the reply, it can create a spark and send the result back to you while serving the next message. The possibilities are literally endless.
That's the reason why most concurrent programs have `NFData` constraints scattered all over the place. Don't mess around with bangs, just force-evaluate the stuff to NF directly before you send it.
I appreciate what you're getting at and I can't wait to get more familiar with the lens package. But I rather feel your first two sentences are tautological and fail to address the question. "If you use the right architecture [i.e. polymorphic data], you don't need to change your architecture down the line." Perhaps it would be better to say (as you do elsewhere!) that Yes, you have to architect your program correctly from the start to avoid major headaches. But, doing so is surprisingly easy. Relatedly, does my intuition serve me correctly in thinking that using `transformers` and the functor and category patterns should mean that even going back and "fixing" previously-static types should be a straightforward process? Create a new, polymorphic type, make a type synonym for all the unchanged uses of the old type, inject the monadic behavior where required, then sprinkling `lift`s to taste. I assume you may have some experience with this.
Well that's excellent. For some crazy ass reason, the Vrije Universitiet Amsterdam teaches **Java** to its pure mathematics students as their *single* exposure to programming. Not even a language that they might hope to use someday for actual work, like Matlab or Mathematica. I'm not sure about the CS students, but presumably they at least also include some kind of languages which are not descendant from Algol... I hope.
Same here. Started out with C++/Qt. When we discovered Ruby it was like holy shit, this is so much more powerful, fun, and expressive. Nothing went from the new hotness to old and busted as quickly as Ruby did after I discovered Haskell.
I'm not sure I understand your name function. I'm not familiar with Traversal and I couldn't find it in hoogle. Could you break it down a bit for me? Or send me to some docs so I can do it myself :-)
That was a great read, thank you! Paul Graham really *does* like to draw conclusions from his metaphors. How come this isn't posted along with his essays more often?
Well, **I** would be very interested!
It's a misconception that "Monad" means "Impure": lists are a monad, Maybe is a monad, (-&gt;) r is a monad just as much as IO is a monad. forMOf is like over but for monads, which include the IO monad (which isn't technically "impure", but I don't want to get into that now. It's only impure if you use unsafePerfomIO or similar when you shouldn't.). forMOf is essentially a slightly less general version of forOf which works over any Applicative, some of which (for example, ZipList), are not monads, although all monads are applicatives (or can be made applicatives easy: for historical reasons, Applicative is not a super-class of Monad, although some (including me) think it should be).
If you find the recursive definition of foldr confusing I'd suggest seeing as a data transformation on lists. Wikipedia has [a pretty good description](http://en.wikipedia.org/wiki/Fold_(higher-order_function)). Another suggestion is to get a good textbook on FP. I can recommend Hutton's *Programming in Haskell* and Bird and Wadler's *Introduction to Functional Programming*. 
OO strives to do good things (implementation hiding, polymorphism, code reuse) but it uses the wrong tools to do so * Bundling all the types and code in a large type product for implementation hiding is a bad idea because you may want the code and types to be differently-variant on type parameters. See also: The list covariance problem. * Interface inheritance: Too weak to be useful. I see it as a very poor man's type-classes. By attaching the interface tables to the objects, you get a whole lot of trouble, and by detaching it into a separate argument as in type-classes, these problems are all solved. * Implementation inheritance: I see this mostly as an awful way to reuse code. It optimizes syntactic ease (namespace mangling means less imports, and automatic parameter passing) at the expense of much more complicated and hairy semantics. It does allow inter-dependent calls between the base and sub implementations, which might be useful (I've not been presented with use cases of this yet, but they may well exist). That said, in Haskell we sometimes tend to make a record of functions and pass it around. I guess we could say this is useful OO methodology as applied to Haskell.
I regularly sketch programs both on paper and in Haskell. When I sketch in Haskell, types are essential. There are two reasons for sketching: 1. Answering the question: "Can I express the functionality I need with the API types I desire?" For instance, I may know that I need a monad, but is an applicative functor enough? I can figure out on paper that the latter is likely, but to prove it, I have to write quick sketch that type checks because I'm not particularly careful about all the details on paper. 2. Discover an elegant implementation. The best way to this is to start writing some ugly implementation and then notice common patterns and abstraction and refactor accordingly. Oftentimes, I just throw away the sketch. (I have thrown away at least six implementations for reactive-banana, but each of them informs the next one.)
OOP is commonly taken to imply objects with internal mutable state, but it doesn't have to: see e.g. `java.lang.String`.
&gt; For example, when sending a message to some actor, you don't need to subscribe to the actors messages in general, you can just send a back-channel or TVar alongside with your message. You don't do this in erlang though, you don't subscribe at all. Messages are send explicitly from one to another.
Thanks! I'll give it a read. These are all very helpful comments, so hopefully by the end of the day I'll know enough about State to be able to actually write a program with it.
I'd like to better understand the large-type-product/differently-variant issue; could you maybe elaborate a bit more on that or link to additional references for further reading?
&gt; On the other hand, unit tests are extremely weak sauce compared to QuickCheck. (Of course QuickCheck is also quite a bit harder to learn to use well, but you get vastly more out of your investment.) Quickcheck is covered here, and I would consider it unit testing as long as the section of code it is testing is small. &gt; On the one hand, unit tests are familiar to many people, and they're better than nothing (unless you trust them too much, which in my experience all too many people do). It's also possible to trust quickcheck tests too much, not realising you may have made the same sort of mistake in the test as the code. I've seen this with tests using SMT solvers too. Some specific examples are very useful, particularly for making sure your edge cases are covered and also just to make you think about what you really want it to do.
&gt;Go's "type inference" is atrocious, and does not have to be. Type inference is decidable in systems [2] much more flexible than Go. This is a solved problem. Go is significantly more verbose than it should be. I really should take that down from arXiv. It was significantly wrong, and the conference review committee said so quite abundantly. An updated paper with similar results but a much-improved algorithm is under consideration at ECOOP, but still... I'm relatively confident that this *can* be done, but now that I'm older and know better, I'd prefer if the results weren't parroted until they're at least published, let alone verified independently. Now, if you or someone you know can help with the "independently verified" part, please, *please* PM me, and if you're not related to the ECOOP 2013 programming committee I can send you the "new and improved" paper. But it's not wise to trumpet, and I shouldn't have done it.
That final ")" should be part of your link ;-)
How do you feel about sketching for a third reason: discovering how your application will solve a particular problem in an attractive way? Have you found Haskell to be a good match for this kind of programming?
I wonder if we can move to "implementation abstraction" over "implementation hiding", because that implies some necessity to literally hide implementations from people, in documentation, code access or inspection, and that has always pissed me off.
Note: Haskell has something called "enumerators" too. I'm not sure what you're referring to, or if they're equivalent. Lazy evaluation allows for equational reasoning about your program. I don't think any ruby feature could allow for that.
This is somewhat of an apples to oranges comparison. 90% of the time you encounter laziness in Haskell it feels nothing at all like an enumerator because you're in pure code and enumerators are stateful. The next 5% of the time laziness crops up it feels a bit like enumerators. The last 5% of the time, you probably ought to change it to strict semantics (which is easy enough once you've found that 5%). Haskell also has a few libraries for generalized enumerators (enumeratee, pipes, conduit) which behave much more statefully like enumerators, but due to the way functional state works you have a hard time inspecting "intermediate steps of an enumeration" which discourages you from thinking about it statefully.
Could you elaborate some more on your second bullet point? I've been pondering the differences between type-classes and the "C++" approach to ad-hoc polymorphism, and I'd be interested in more examples of how they differ.
Sometimes? The entire class system is based on implicit dictionary passing—
Yes, that is a more appropriate assessment. I should have said "You just need to anticipate polymorphism in certain areas of your program to make your code future proof, but that is easy in Haskell". And yes, I do exactly what you described. This came up in a program I wrote where I had an `Atom` type that was originally entirely concrete: data Atom = Atom { ... } ... then later I decided I needed to attach a polymorphic "bag of holding" to it that I would use to store scratch work, so I just defined: data Atom' a = Atom { extract :: a, ... } ... then define: type Atom = forall a. Atom' a and then all my existing code is polymorphic over that field, so that when I add the code that uses that scratch space, it will now interoperate with the old code. There was still some refactoring to do, but not that much, and refactoring in Haskell is really easy.
Oh :-(. I had not worked through your paper, but I had been operating under the assumption that type inference with subtyping was a "solved problem" and one just had to define a system in terms of LUB and GLB functions to take advantage of it. I apologize for parroting it. I'm not affiliated with the ECOCP committee, and would love to read your current paper. Best of luck. Anyways, I'm not sure I needed it for my point. Interface polymorphism with dictionary passing need not be done with subtypes at all: typeclasses do just fine.
So there are a lot of things that don't even have to do with the type system per se, like python will happily get ten minutes into a server provisioning task (basically just a sequence of shell commands) only to fail at the very end when I use a function that I haven't imported, or has a typo, or has the parameters in the wrong order, or... so then I have to kill that instance and start over. A more interesting issue: fabric has support for running tasks in parallel (in the most basic way: on several machines at once), however this is unusable for me since some dependency of the ec2 "boto" library is not thread safe (or so I'm told). Essentially you can't do concurrency in python unless you control everything your code is sitting on, and isolate the concurrent bits. The best you can hope for is for someone to say "my code is thread safe" so that you can complain at them when it isn't. I should have been making a list of grievances over the last couple months, so I'd remember more...
Where can I read more about the "should haskell be lazy by default or not" argument?
&gt; Lazy evaluation allows for equational reasoning about your program Could you expand on that? I thought this was a result of purity.
It feels very different when you implement it though. Typeclasses are much more restrictive, since the dictionaries are declared at program-creation time and are bound one dictionary per type. Passing functions explicitely, on the other hand, is usually much more dynamic (can depend on runtime values) and overall feels more "object oriented", IMO.
Not IORefs, but rather STM values. You store your global values in STM variables so that you can safely modify them concurrently when simulating complex behaviors.
I think what kamatsu is referring to is the ability to use less restricted eta rules, like \x -&gt; f x = f which only holds in non-strict languages (and strongly normalizing ones). Unfortunately, it does not hold in Haskell because of `seq`. It turns out, that [you don't have to worry about this too much](http://www.cse.chalmers.se/~nad/publications/danielsson-et-al-popl2006.html), and can pretend Haskell does not have `seq, but it is an issue. 
"Ruby has more community support" -- can you expand on this a bit? I was under the impression that Haskell has rather excellent community support (via reddit, StackOverflow, the #haskell IRC channel on Freenode). If Ruby's community support is better I'd be quite interested to hear what that looks like.
I guess this is the kind of situation where you can't easily unit test typos and similar bugs away as dynamic languages usually advocate. Haskell does seem a like better match for the safety and threading you wanted.
I think he means you wnat a method's parameters and return types to be differently variant (in opposite direction) on type parameters. Canonical example is Student is subclass of Person but List[Student] is not subclass of List[Person] here's one way to phrase it because of covariance/contravariance problem, all OO languages are ultimately dynamically typed http://www.artima.com/forums/flat.jsp?forum=106&amp;thread=141312&amp;start=210&amp;msRange=15 --------------- contra-/covariance was the killer feature of C# 4.0, you can see how important it was to Hejlsberg (tho that's assigning sensible defaults to different collections, the safety scissors school of PL design. In scala you're treated as a responsible adult and you have to specify where you want co- and contravariance explicitly [47:00 in vid] http://channel9.msdn.com/Blogs/matthijs/C-40-and-beyond-by-Anders-Hejlsberg http://blogs.msdn.com/b/csharpfaq/archive/2010/02/16/covariance-and-contravariance-faq.aspx 
Fixed.
&gt; Object orientation is, to me, a strange idea that tries to encourage informal reasoning about programs by analogy. Object orientation is, to me, a concept that no two people ever give a similar definition for. :) Personally, I'm on the "expression problem" camp that says that opaque closures are OO while transparent ADTs are functional.
stackoverflow: Haskell clearly wins. One can easily verify that week old ruby questions on SO have almost no accepted answers (1/50 with yellow background) and many questions with no answers/downvotes/closed questions, while similarly aged haskell questions are handled well. http://stackoverflow.com/questions/tagged/ruby?page=9&amp;sort=newest&amp;pagesize=50 -------- edit: Oops i goofed, yellow background does *not* mean accepted answer, green background on number of answers does. But it's still clear that a higher percentage of haskell questions receive a satisfactory response
Scribbling in Haskell? Collect your combinators and compose them. The programs you come up with during a "hacking" session will usually be nothing more than just that. The static type system helps so much that a dynamic one would only hurt. If type inference fails, the composition doesn't make sense. This stops you from exploring caves you shouldn't begin to explore, and actually speeds up scribbling.
All good points. &gt; I see it as a very poor man's type-classes Aren't typeclasses the poor man's parametric module? :)
"Never gonna let you down"
That's a weird question; comparing actors and semaphores is like comparing apples and watermelons. Depending on which aspect you're interested in, yes and no: STM is much closer to actors in the level of abstraction, but closer to semaphores in that it is a shared-state model, which is more similar to the mutual-exclusion approach where you'd use semaphores than it is to the message-passing actor approach. That said, you could implement a naive kind of message passing on top of STM fairly easily, so perhaps the programming models aren't as different as they first appear.
There's not really an argument about Haskell; of course it should be lazy by default because we're long past the point where changing that aspect of the language would be too disruptive to consider. But you can search for writings about Haskell-like languages that have made different choices. For example, see Disciple, or Elm, or if you have masochistic tendencies, then try reading Robert Harper's blog comparing Haskell and ML.
Mine was the unforgettable: &gt; Dealing with console keyboard is not a hard task for a man, who wrote console tetris. Please write more!
Is it because WHNF for a lambda abstraction is the particular lambda abstraction? &gt; let f = undefined &gt; f `seq` True *** Exception: Prelude.undefined &gt; (\ x -&gt; f x) `seq` True True Another example I can think of involves partial application. Are there other obvious or simple ones?
Same with me transitioning from C, C++ and Java via Python to Haskell. It took me ten years, unfortunately.
Paul Graham has posted an in-depth rebuttal to *Dabblers and Blowhards* on Hacker News. Both sides make good points, but I'm not convinced by either myself. It's a worthwhile discussion though.
&gt; How do you feel about sketching for a third reason: discovering how your application will solve a particular problem in an attractive way? Ah, you mean writing an application to test how the application itself should behave and work, mostly when it comes to the user interface. (Everything else would be API design and is covered by the other reasons.) &gt; Have you found Haskell to be a good match for this kind of programming? Absolutely. In Haskell, I can express the functionality I desire (to test out) on a very high abstraction level. I really don't want to think on the level of stupid `for` loops when I am writing a quick throwaway application. The strong typing is not very important in this situation, but it still helps a lot with figuring out and reusing existing APIs very quickly.
[Class: Enumerator](http://www.ruby-doc.org/core-1.9.3/Enumerator.html) **tl;dr** Enumerators are for loops modeled as objects. They take some collection and a block (mostly like an anonymous function) and can produce the next result on demand by calling `next`, which means the code is only executed when the next element is needed (hence the OP's question in regard to laziness).
The Haskell community is very high quality. However, the Ruby community simply has a numerical advantage. Measure how you like, whether number of websites powered by Rails, or copies of Ruby books sold. Likewise, Hackage is high quality, though RubyGems has a numerical advantage.
This is exactly why I get upset when people try to make judgements about "static typing" based on only C++ or Java experience.
So what's wrong with [ExistentialQuantification][1] other than that you find it "unappealing" ? ExistentialQuantification has a couple quirks, but if you learn about [GADTs][2] (the big brother of ExistentialQuantification), you may gain a better appreciation for how it works. [Your transformation][3] appears to make the code harder to understand, with no apparent benefit. [1]: http://www.haskell.org/ghc/docs/latest/html/users_guide/data-type-extensions.html#existential-quantification [2]: http://en.wikibooks.org/wiki/Haskell/GADT [3]: https://github.com/singpolyma/permute-RankNTypes/commit/15690bb
Couldn't you use generics to keep that from happening? Something along the lines of interface Monoid&lt;T&gt; { public Monoid&lt;T&gt; mappend(Monoid&lt;T&gt; x, Monoid&lt;T&gt; y); } Doing that, I was able to get the compiler to error on the call to reverseAppend: Herp.java:37: error: method reverseAppend in class Whatever cannot be applied to given types; System.out.println(Whatever.reverseAppend(x, new Monoid&lt;String&gt;() { ^ required: Monoid&lt;T&gt;,Monoid&lt;T&gt; found: MyMonoid,&lt;anonymous Monoid&lt;String&gt;&gt; reason: no instance(s) of type variable(s) T exist so that argument type &lt;anonymous Monoid&lt;String&gt;&gt; conforms to formal parameter type Monoid&lt;T&gt; where T is a type-variable: T extends Object declared in method &lt;T&gt;reverseAppend(Monoid&lt;T&gt;,Monoid&lt;T&gt;) 
My transformation is primarily academic (I did it as an example of how such transformations are possible). However, RankNTypes are a very straightforward thing one sometimes wants, whereas this particular library is the only example of ExistentialQuantification I've ever seen that was not a hack to try and get a more up-cast-ish feel out of some data.
Thanks! &gt; forMOf_ (folded . _1) [('A', 1), ('B, 2)] $ \c -&gt; print c I also tried: forMOf_ (traversed . _1) [('A', 1), ('B', 2)] print It works too. However, forMOf (folded . _1) [('A', 1), ('B', 2)] print (the same, but I want to keep the final result) doesn't work, when the same with ```traversed``` works. In that example, ```traversed``` makes just more sense to me, as it serves the same purpose as ```Traversable```. But how would you monadically fold data (like, e.g. summing all the elems in ```(folded . _2)``` to make 3, while priting intermediate result)?
List&lt;T&gt; contains within it all of the methods that operate on lists, including those that take "T" as an argument, and those that return T as a result. This means that List is necessarily invariant on T when you have both kinds of appearances of T in methods. That means there's no subtype relationship between List&lt;T&gt; and List&lt;U&gt; even if there is one between T and U. If, instead, List&lt;T&gt; existed separately from the methods that apply to List&lt;T&gt;, and each could have its own co/contravariance, then you could have the subtype relationship. In Haskell, the subtype relationship is expressed as a simple function: `List sub -&gt; List base`.
In OO interface inheritance, especially with single-dispatch, you get type-directed choice of implementations only based on one special argument's type. This causes various problems: * You can't have a sensible `Eq` interface, since you have two interface dictionaries to choose from, with 2 different types for the compared arguments. Which do you choose? * You can't have type-directed implementation choice based on other parts of the type signature. Consider: return :: Monad m =&gt; a -&gt; m a How would you use an interface to capture the `Monad` type-class? There are also performance issues with attaching the dictionary to the objects. Consider a large `Array&lt;ISomeInterface&gt;`. In typical OO languages, each array element `ISomeInterface` pays for the dictionary pointer. With type-classes, and something like: `SomeInterface a =&gt; Array a`, you pay for the interface pointer only once. If you really need the per-object dictionary, you can recover it with the type-class approach via existentials: data ObjectWithDictionary = forall a. SomeDictionary a =&gt; ObjectWithDictionary a While Haskell exposes that this pattern is very rarely useful, most OO languages make expressing anything *else* very difficult.
Don't Java interfaces have an implicit "this" argument that's unaccounted for here? How do you implement "mempty" that's polymorphic on the return type?
Sure, but that's hidden. I don't feel I'm doing OO when I rely on type-classes. When I build explicit records of behaviors and pass those around, it does feel like OO, but I don't know if it actually qualifies.
I wonder where it saves the files: &gt; writeFile "letsSeeIfIcanCreateAFileThere" "oh this is no good" → &gt; readFile "letsSeeIfIcanCreateAFileThere" &gt;&gt;= print → "oh this is no good"
I personally haven't used first class modules much, but I think while they have a lot of overlap in use cases with type-classes, they also cover different grounds. Some things well covered by first class modules would be very tedious with type-classes. Similarly, every encoding I saw of the actual Monad generalization as a first-class module seemed terrible.
Emscripten emulates a file system: https://github.com/kripken/emscripten/wiki/Filesystem-Guide
I don't have a lot of experience with generics in Java (again, something I feel is done perfectly in the functional world, and somewhat clunky in the real world). But from the looks of it, this appears to be a technically correct solution. I would make the claim that this is a hair less elegant than it ought to be. Your implementations end up looking like this: class MyArrayType extends Monoid&lt;MyArrayType&gt; { .... } The parametrization serves the same purpose as the type variable `a` in `instance Monoid a`. But you end up repeating yourself just a slight bit. That is, you have declared `MyArrayType` has the`Monoid` interface.... you have already created the association you want between the type and what it's capable of.... but then you have to say it *again* in the part `Monoid&lt;MyArrayType&gt;` in order to prevent misuse. Again, I'd like to reiterate, I don't touch Java. There could be some syntax or accepted practice to avoid this (admittedly minor) redundancy. But it still falls short from the conceptual clarity of typeclasses: if reverseAppend :: Monoid m =&gt; m -&gt; m -&gt; m and instance Monoid Foo, then reverseAppend :: Foo -&gt; Foo -&gt; Foo Figuring out how it works is a check (is there an instance?) followed by substitution.
Even if a small percentage of people are helpful if there are 10 times more people you still get help. Counter example redit measures ~10k /r/haskell readers and 44 present. While /r/ruby has 14k readers and only 21 present. A benefit of the small community though is that there is more low hanging fruit in terms of tutorials and blog posts if you want to make a name for yourself.
Technically non-strict allows for equational reasoning right? Lazy is just one method implementing the non-strict semantics. That is why some people are working on strictness analyzers for GHC as an optimization, stricter then lazy code but still non-strict so it looks the same from the analysis point of view. I am a little hazy on it though because I can not derive it from fundamentals yet, I have to relay on what other have written on the matter.
I have never used first class modules, personally. I can't imagine Monads would be all that terrible in a module. The annoying thing, of course, and as I understand it, the main *disadvantage* of modules versus type classes is that type classes must have a UNIQUE instance per type. Modules, OTOH, can have any number of instances. The disadvantage, of course, is that there is you can't properly infer which instance is intended with modules. (There's also a performance hit, because you can't type-erase module parameters). But it is a trade-off. In Haskell, we have to talk about THE `List` monad, having `return x = [x]` and `(&gt;&gt;=) = flip concatMap`. But what if we take `(&gt;&gt;=) = flip concatMapRev` where `concatMapRev f xs = concatMap f (reverse xs)`. The result is still a monad. (At least, I'm pretty confident it is.... and only if we pretend we're working with finite lists, which we can easily enforce in Haskell if we had the time and energy to do so). It's analogous to how, in algebra, the integers can form a monoid in two ways. `(Z, +)` has `0` as the identity and `(Z, *)` has `1` as the identity. But yeah. I got distracted from what my point was. SPJ has some slides you've probably seen. Typeclasses are a very nice "lightweight" way to do many of the same things as modules.
Why not pick a few small projects, nothing significant, and use Haskell on them to get a sense of what it's like to work in Haskell. Hang around in the IRC channels, ask questions, talk to people. If community is a major issue for you, try living with the Haskell community for a bit, when it's not that important.
The forklift pattern is very interesting! I'll have to take some time to understand the operational approach to monads though. :)
I often code in Haskell. It's just that Haskell doesn't support nearly as many platforms as Ruby, and I'm wondering if I should try to take my Haskell mindset and apply it in Ruby.
Try to encode the monad generalization as first class modules, or look it up. It is nowhere near as nice as ordinary Monadic code in Haskell. Or similarly, if type-classes were fully covered by modules, why do the ML's have the nasty multitude of numeric functions rather than FC modules?
Can someone explain this to me please? &gt; :info IO → -- type constructor → newtype IO a → → -- constructors: → IO :: ((a -&gt; IOResult) -&gt; IOResult) -&gt; IO a → → -- instances: → instance Functor IO → instance Monad IO
&gt; Or similarly, if type-classes were fully covered by modules, why do the ML's have the nasty multitude of numeric functions rather than FC modules? Probably because modules are heavyweight tools. It's easier to take the naive approach and either separate `+` from `+.` or to cheat and give them special syntax status. Somewhat related, in Idris, we have some desire to do away with the awful numeric tower we have in Haskell. But even with the power of dependent types (which subsumes the concept of a module) AND typeclass facilities at our disposal, we haven't thought of a solution which is both terse (like typeclasses are) and "aesthetically pleasing" to us. (And by that I mean, what the hell is a `Num` anyway, and what useful theorems should we package with it?)
You won't find such a function in the `lens` library, but you can define your own! It took me a while to figure out exactly how to do it, but this is the function you want: import Control.Lens import Data.Functor.Compose ywen :: (Functor f) =&gt; ((f c -&gt; Compose f (Accessor c) a2) -&gt; a -&gt; Compose f (Accessor c) a1) -&gt; a -&gt; f c ywen l = fmap runAccessor . getCompose . l (Compose . fmap Accessor) &gt;&gt;&gt; ywen (traversed . _2) [('A', readLn), ('B', readLn)] :: IO [Int] [1]&lt;Enter&gt; [3,4]&lt;Enter&gt; [1,3,4]
I may be missing something here, but why *first class* modules. Shouldn't Monad be just a signature in ML?
and syntax, and it has some pep in its step.
Rather than treating IO as a "secret state monad", Hugs treats IO as a continuation monad. This lets it implement cooperative multithreading without any changes to the runtime system. See the prelude here: http://ogi.altocumulus.org/~hallgren/Programatica/tools/hi/libs/HugsLibraries/Hugs/Prelude.hs Some further explanation in a more recent (and fancier) version here: http://comonad.com/reader/2011/free-monads-for-less-3/ 
One aspect that struck me, and I would be interested in stats for, was how often there was only one answer, or a low number of answers, to a question with the Haskell tag. If true I am sure that boost the points per question. I took it as weak evidence of strong community consensus on how to approach some questions. There just was not a need to add more answers since the right one was already posted. I read this positively it would be interesting to see if this is backed up by the data though.
Haskell has Template Haskell, there is a recent stackoverflow question that deals with, and provides an example of, how to evaluate template Haskell at runtime even.
What platforms does Haskell not support?
Note, that yet another way of defining IO is to use GADTs, e.g.: data IO :: * -&gt; * where Return :: a -&gt; IO a -- This is basically the constructor above. GetChar :: IO Char -- One of the primitives ... -- other primitives One downside to this approach is that we need GADTs, another is that we have to pay the interpretation cost for interpreting the AST to produce actual I/O. Both of these can be dealt with by using the [Finally Tagless](http://www.cs.rutgers.edu/~ccshan/tagless/jfp.pdf) technique. But ultimately, this GADT representation is the real thing we have to work with. Everything else ---GHC's State monad, Hugs' continuation monad--- are all just tricks to optimize away the GADT.
We've got a project at work that's written in Java (soon to be redone in Scala), but most of my coding at work is in Ruby right now. I started playing around with that interface after posting my comment, and I'd say that it's more than a hair less elegant. You raised a good point about redundancy, and I played around with it for a bit and it just got clunkier and hard to work with. I'd be interested to see if anyone's written a Monoid interface that is half-decent; perhaps I was just doin' it wrong. It was interesting to play around with though. If anyone ever asks me how Haskell's type system is better than Java's, I'll just tell them to try to create (and use) an interface for one of the common typeclasses. Edit: functionaljava has a [Monoid](https://github.com/functionaljava/functionaljava/blob/master/core/src/main/java/fj/Monoid.java) class instead of interface. Probably a much better design than what I was doing, but it still seems inferior to Haskell to me.
Sure it does. He is talking about adding behavior to existing functions. Expanding them to include IO. This is usually the wrong approach as it adds depth to the computation. Instead, try to make a lot of smaller functions which have a single purpose, and then compose them together at the top level. Haskell excels at providing facilities for this. This way to make changes you simply modify the pipeline, instead of having to rewrite a whole bunch of code. Meaning - he is having these problems because he is thinking very imperatively. Just read the post where he says I'll need to do this which requires that (you can see the function just growing in your mind as you read his post). Try to decompose the solution better.
I think the real power of Haskell's type system has more to do with the fact it's an extension of [System F(ω)](http://en.wikipedia.org/wiki/System_F). Type classes are really useful, but there are other ways of doing similarly-powered things. But System F is the perfect model for generics.
The number one issue you get when you try this is that `ghc` does not allow cyclic imports, even though the Haskell specification says you should allow them.
Anything that has `void main()` as an example of C code is immediately suspect.
I appreciate that it's not easy to defend another language in this subreddit, so I applaud you for that, but you undermine your own arguments when you disrespect other people. I should know because I've made that same mistake countless times myself. I agree with kamatsu about sum types vs. product types for error codes. Go definitely got it wrong on that one. Sum types improve the safety of code because they ensure that you check every error and never extract the wrong value (i.e. an error when the function succeeded, or a result when the function failed). They literally make it impossible to do the wrong thing. Moreover, when you include sum types in a language, you get a lot of extra features for free. A lot of things that traditionally required language support become trivially implementable as libraries using sum types. It's really shameful that people still release new languages without sum types, because they represents a very fundamental advance in programming languages.
Well, there's nothing to discourage because it's not even possible. :) Some Haskell libraries are already really difficult to follow even without the restriction because they use several layers of indirection before they actually get to their internal code, so I suppose that cyclic imports would make that worse if they were legal.
Yeah, don't disagree with that. I don't disagree that Go has less at all, and I do appreciate sum types in particular. I vehemently disagree that Go is more cumbersome than Java, of course. (And presumably this was conceited since the comments have been edited to acknowledge that Go is terse--something I, at least, consider a major factor in whether something is cumbersome or not.) I responded what I perceived to be in kind. The Google comment may have been over the top, but it was the third time in this post (not just this comment thread) that the employment was mentioned without being related in the slightest. It rubs me the wrong way when somebody implies that because they've had the same mail suffix as Rob Pike (or whomever), they are somehow more expert in programming language design/qualified to judge the usefulness of a language. I've found this is extremely common among googlers, particularly ones that were not actually involved in research there. 
Couldn't you just use the `Free` monad to avoid the GADT? data IOSpec next = GetChar (Char -&gt; next) | PutChar Char next ... type IO = Free IOSpec
I agree with you, which is why I have not released this package, and have only put the code on github :)
How about something like, `new Monoid&lt;String&gt;().empty`?
Yeah, I agree. Most people in this reddit consider polymorphism, sum types, and type inference to be a no-brainer for new languages because they have been battle-tested and incur no overhead (or almost no overhead in the case of sum types). I'd really like to see a low level language that implemented those three concepts because I think it would be a serious contender to a C successor.
Sorry, what? HTML5/JavaScript? Perhaps you can clarify what you mean by this.
Finding the `=&gt;` was like solving a Where's Waldo puzzle.
That would be very interesting.
&gt; I know, but then I have to add a return 0 statement, which I thought was slightly distracting. That's pretty much obligatory anyway.
I'll tell you one thing it DOESN'T do.... Explicitly declare it's top-level binding's types :/
what does :. signify? the google is no help.
Unless you're talking about something else, [GHC supports mutually recursive imports](http://www.haskell.org/ghc/docs/7.6.1/html/users_guide/separate-compilation.html#mutual-recursion).
The vast majority of "crossplatform programming language frameworks" out there are based on running a WebKit instance, using JavaScript for the logic. Some of these compile your language code into JavaScript, and just about all of them are very non-FOSS (except PhoneGap). In other words, HTML5-based programming is sketchy, very undesirable currently. Not to mention JavaScript [sucks ass](https://www.destroyallsoftware.com/talks/wat).
When I saw `main = f getLine getInt` I immediately wanted to replace it with `main = f &lt;$&gt; getLine &lt;*&gt; getInt`, but that types as `IO (IO ())` instead of just `IO ()`! I did a quick bit of hoogling but didn't see anything that had the right type, so I wrote this: (Monad m, Applicative m) =&gt; m (a -&gt; m b) -&gt; m a -&gt; m b a &lt;&amp;&gt; b = join $ a &lt;*&gt; b infixr 0 &lt;&amp;&gt; which lets us rewrite the example as main = f &lt;$&gt; getLine &lt;&amp;&gt; getInt I'm not sure how often this pattern comes up, but it seems like `(&lt;&amp;&gt;)` could be useful. Maybe it's even out there somewhere and I just fail at searching :D
I found [this video](http://www.youtube.com/watch?v=XovXFGWPSRE) helpful in understanding how the IO monad works; basically, you're just building up a big "shell script", and eventually it'll be ran by main. Bonus for hearing Prof Wadler scream "DO IT!" (~33:10 in the video).
The hs-boot hack doesn't count as "supports"
What do you think are the right tools? Stateless OO + parametric polymorphism = first class modules.
Well, as long as we are playing what-if... Yes, this would work, if you automatically generated the .hs-boot file; otherwise, you'd be back in the bad old days of having to write each everything twice (.h vs. .c files). Of course, if each .hs file only declared one function (or one type), automatically generating a .hs-boot file would be trivial. You could even make {-# SOURCE #-} import the default in that case :-)
Well since Haskell is a compiled language, everything but JavaScript should be fine out of the box. For JS, there's http://www.haskell.org/haskellwiki/The_JavaScript_Problem
I made an effort to never use the word "monad" and to avoid infix operators: two things I hear newcomers criticize Haskell for. I also chose to present using `do` notation entirely because I like to build a concrete intuition before delving into the more abstract foundation. Also, I think it's more fun that way because then when they learn afterwards how `do` notation works under the hood they experience a really big "Aha!" moment. On the other hand, if you lead with the `Monad` class and its operators then you basically spoil the ending for them.
OO dispatch is *not* type directed. It is runtime dispatch. This is orthogonal to typeclasses which handle static resolution/overloading, but from another perspective, typeclasses are completely OO. An object is just a record is just a (first class) module. I don't see how they can be "bad." The existential pattern is not very useful in Haskell because of language deficiencies like lack of subtyping and path dependency--not do to any inherent limitation in this design. See OCaml, Scala, Coq, etc as languages that do a much better job when it comes to this. Even though Haskell is limited here, SPJ and Mark Shields wrote a paper way back in 2001 where they showed how to move towards making Haskell's worst of breed (compared to OCaml and SML) module system more usable by re-encoding modules as records with existential types. This is formally exactly what modules are. Abstract types *are* existential types. Look, Haskell is my favorite language, but OO is just dual to FP. Forcing OO on you always and in all situations is no good--but lots of things don't make much sense functionally either. I think the best piece of evidence for that last claim is how much we use the OO parts of Haskell. Typeclasses are constructive classes over types. The logic they encode is OO (it has subtyping, inheritance, implicit `this`, and handles products the right way). This is combined with a theorem prover to get appropriate `instances` (equivalently evidence for class membership or the object that proves this membership) when you need them. This particular relationship is covariant. So, using typeclasses you can encode sub-typing, but you have to write your type hierarchy upside down. The much celebrated `Control.Lens` takes this to an extreme to great effect. 
Typeclasses can be proved via local evidence encoded in GADTs. Functions can be defined to work over all monads. Although you can handle the second case by thinking of every polymorphic function as a module, this causes some issues. First class module resolve this. Typeclasses are first class modules that are resolved (almost) entirely implicitly via a theorem prover built into the language. This is a good design IMO, but really you should break them into separate features. Scala kind of does this but in a brittle way. Haskell's typeclasses lack existential quantification though, so it is a very restricted form of modules. 
Typeclass instances need not be unique in GHC. I have been meaning to blog about this.
i am conflicted as whether to call this ugly, or beautiful. 
Oh, but I don't lead with the `Monad` class. I start by pretending that `&gt;&gt;` and `&gt;&gt;=` are just `IO` operators. The point is to emphasize that *they're just functions*, and that what you're doing is using pure functions to assemble compound actions.
The purist way of doing such a thing requires [dependent types](http://www.haskell.org/haskellwiki/Dependent_type). The big idea is you define a data type `U` (a universe) and a function `Promote : U -&gt; Type` that turns objects of type `U` into real types. We think of the elements `x : U` as encodings for types. The cool thing about universes is we can talk about things syntactically. Then, when we are ready, we can bring our syntax to life using `Promote`. You might, for instance, write a function called `eval` for a small statically typed language. The problem, of course, is that the return type of `eval` depends on the input. If I give it the program `Plus One One`, then the resulting computation will should be `Nat`. If I do `Concat (String "hello ") (String "world!")`, the result has type `String` instead. (GADTs in Haskell allow you to do cool things similar to this. But this is only because Haskell's type system gets more dependently-typed features every year. One possibility that I'm not sure is possible yet in Haskell, though, is the ability to `eval` a program that DEFINES the type as part of the input program. Anyone know more about the limits of GADTs in Haskell here?) So, to make things a little more concrete, our `eval` function might get the clever type: eval : (t : U) -&gt; (p : Program) -&gt; (TypeChecks p t) -&gt; Promote t Read in English, this says "for all type-encodings t, and for all programs p, if p typechecks with type t, then return the type you get by decoding t". Using essentially this technique, it's possible in a dependently-typed language to write type-safe metaprogramming. The type `Program` is what takes place of our S-expressions in Lisp. Since `Program`s are just trees, we can mash them together, parse them from strings, read and write them to files, and have the users input them at runtime. Then, when we want to run them, we first must make sure they have the type we expect. We probably want to include a helper function like this: inferType : (p : Program) -&gt; Maybe (t : U, TypeChecks p t) Read: "for all programs p, return a type-encoding t along with a proof that p indeed has that type.... or `Nothing` if there's a type error" (This is effectively what `:t` does in GHCi or in Lambdabot). So then, your program looks like this: do p &lt;- getProgramFromUserDuringRuntime case inferType p of Nothing -&gt; putStrLn "You have a Type Error!" Just (t, prf) -&gt; putStrLn $ "Result is " ++ show (eval t p w) (Obviously, this code would require a typeclass instance for `show Promote t` for all `t : U`. Idris is a new language under development with both dependent types and typeclass support). Again, let me emphasize the cool part: You have full control over the type system in your macro language. You can't write a macro that expands to nonsense like `(5 (+ x y))` or anything like that. (Though, you could if you wanted to give such an S-expr semantics in your `eval` function). Programming with universes (type-encodings) is a very young and active field of research. To give you the true story, no one really knows what the limitations of the above technique are in practice. Hopefully some day they will enter the mainstream. But until them, most people will say you should stick with Template Haskell for most of your practical needs. Template Haskell does some cool things from what little I know about it, and it's used in many "industry grade" Haskell libraries (or soon-to-be industry grade ones, like Cloud Haskell). Anyway, here's some slides if you're curious :) http://www.seas.upenn.edu/~sweirich/ssgip/main.pdf
Doesn't surprise me. I don't know what the design limitations are, but on the surface, it sounds like a legitimately good idea.
oh. thanks
Yes that's true, but it's a technicality that probably wouldn't assist a newbie.
It's bioinformatics related. That is an implementation of [this algorithm](http://ultrastudio.org/en/Nussinov_algorithm) for [RNA secondary structure prediction](http://openwetware.org/wiki/Wikiomics:RNA_secondary_structure_prediction). Edit: [Nussinov R, Piecznik G, Grigg JR and Kleitman DJ (1978) Algorithms for loop matchings. SIAM Journal on Applied Mathematics.](http://rci.rutgers.edu/~piecze/GriggsNussinovKleitmanPieczenik.pdf) &lt;-That is the specific paper which contains the algorithm that is implemented here.
...if you use channels like that, you ought to wrap what you pass in a little ADT that tells the NF forcer whether it's already in NF or not.
Wow, not straightforward \^\^ Thank you! But strangely, it doesn't use ```folded```. Indeed, applicative composition can do that, but then what is the use of ```folded``` when compared to ```traversed```, then?
Why a screenshot? A [link to the package](http://hackage.haskell.org/packages/archive/Nussinov78/0.1.0.0/doc/html/BioInf-GAPlike.html) is much nicer. 
I'm not saying it's impossible to get good performance, but you end up having to jump through a lot of hoops. In a strict language, none of these things would have been a problem. I'm not sure why you have an attitude about this, like its super easy and I am too thick to get it. Using channels in Haskell needs all of the concepts of CML *plus* knowledge of laziness, strictness annotations, normal form vs weak head normal form, seq vs deepseq, how to make your deepseqs efficient, and probably more stuff I have not run into yet. This is strictly more complicated than message passing concurrency in a strict language such as CML.
In strict languages you usually aren't passing complicated things over channels, in the first place... and you shouldn't if you don't have to, in Haskell, either, you don't want to break locality. CML, to the best of my knowledge, doesn't have STM and lacks such a construct as "retry". Which is pure aspirin in itself, a bit of non-strictness complexity on top of that doesn't suddenly make Haskell concurrency as mind-bending as in more traditional languages. I did concurrency in Java and C and I don't ever want to do that, again. And as a Haskell programmer, you should already know all those concepts concerning laziness :)
It's quite simple, actually. The [Operational Monad Tutorial](http://apfelmus.nfshost.com/articles/operational-monad.html) explains everything.
I loved the bit about "Haskell's pineal gland".
I also created a ticket for this http://hackage.haskell.org/trac/ghc/ticket/7624
Others have mentioned Template Haskell, and there is also MetaML/MetaOCaml, which extends the concept to as many levels of code-writing-code-writing-code as you like, and is non-dependently statically typed (for pure functional subsets of the starting language) but doesn't allow code to generate types, only expressions. The basic operators are .&lt; &gt;., which create "code" values out of the expressions inside them, which can later be run with .!, or spliced into other bracket expressions with .~. (Like so: .&lt; 1 + .~(x) &gt;. where x is of type code&lt;int&gt;) http://web.cecs.pdx.edu/~sheard/papers/summerschool.ps is a decent introduction to MetaML. Walid Taha has some good papers on it as well.
&gt; One could even have multiple versions of the same function (declared in the same file) which might differ in algorithm then one perhaps can automate speeding testing and comparison almost as a language feature. The speed test results might then feed into the compile. This is an interesting idea. Naive, correct versions and optimised versions alongside each other, etc.
Uh, what's wrong with "f getLine getInt"?
Different learning styles for different folks. I know I can get caught up on this type of small in term usage inconstancies when I am learning a new language and I do not have a source I consider definitive.
I think from practical point of view this is unrealistic. I remember times of C na C++ with header files and implementation files and i know this is not 100% relevant managing mass amount of files in many directories is a pain. Especially if you don't use IDE's. Imho Examples of horrid directory / file structures are things like C# and Java.
+1 for writing the `do` keyword on the left side of your do blocks! ;-)
I think these points can be classified as [Red Herrings](http://en.wikipedia.org/wiki/Red_herring#Logical_fallacy). My point is specific to message-passing concurrency (i.e. `Control.Concurrent.Chan`) with laziness. I don't see why you would pass large values more or less commonly based on the evaluation strategy of a language or why that would invalidate the complexity argument. I do not see how STM and retry and Java and C are relevant. And knowing about laziness does not make it less complicated, just like knowing that C does not do garbage collection does not make manual garbage collection less complicated. In any case, I think we both like having the last word :P Also, I am the designer of [Elm](http://elm-lang.org/) which is a strict derivative of Haskell, so I am pretty decided on this issue :)
This is very impressive. I look forward to a write-up about how difficult it was to do. Web demos of research are a great way to get your message out.
Hmm, interesting, I didn't realize the signature wasn't in the source code. I would also have assumed that haddock wouldn't document such a function. Nice!
I'm on Chrome. I got it as an error message when I tried to run "putStrLn". I thought it was funny.
In opera there is "Never gonna give you up"
I was just saying that I found the idea quite clever.
I see, thank you for the explanation!
Note: you can get this same result in a more principled manner with the `reflection` package. The major benefit over this approach is that you retain confluence, while still being able to make dynamic instances. As or confluence already being a problem, it is only a problem for orphan instances. I have maybe a dozen of them in 60+ packages, and the compiler screams bloody murder at me about their unsoundness. Also, had my libraries request for turning on `PolyKinds` in `Control.Category` gone into 7.6.1, you could use the `categories` package directly. Sadly to my knowledge it wasn't put in. =/ The combination of that and a distinguishable `Any` inhabiting each kind currently has me implementing any interesting bits of category theory in GHC on hold until some time likely in the 7.8 era. =(
I'm so glad to hear it was there. I couldn't find it!
In luakit (and probably any other plain webkit browser), it's the only message you get.
and this is why my many forays into Haskell have failed. the type system just doesn't seem to click.
I'm only guessing because I'm on my phone and don't feel like pulling up code, but it is probably functor composition.
I do not think it is an antipattern in all cases. Sometimes you want some mutual definitions which can't see into each others' guts. Cyclic dependencies (aka mutually recursive modules) are a great way to do this.
This is doubly not a representative example. One, it's crazier than most people have ever seen (see post title, for example). Two, the library is using GHC type system extensions. The fully extended GHC type system is considerably more complex than the Haskell type system.
I completely agree that Haskell leads to a better architecture and better code in the long run! What I want to know is whether you have found being forced to deal with these difficulties right away, during the initial experimental part of a project, hurts exploration in a significant way. Does being forced to restructure code early on get in the way of testing new features and ideas for your new application?
Check your JS error console for errors. Perhaps it fails to load the JS at all, like Opera. If it is loading the JS, then this should run: hugs_console_log=function(s){console.log(s)}; hugs_run("1") If you get an exception from the `hugs_run` call, that's the cause of your Rick Astley quote.
I think I understand what you mean. For example, instead of having a mkWorkspace function calling mkProject for each project in the workspace, I should have called mkProject in the top level and passed the resulting Projects to mkWorkspace as an argument? Won't this cause a lot of logic to be on the top level though?
Bookmarked! Thank you :)
It's realistic only if you have a _very_ good IDE support behind. See LightTable, it's being built around the idea of not bothering about files: you declare functions, classes, modules, i.e. units of your language. Not files.
On the other hand it might get better because the need for such indirections would be reduced
What does functor composition in the type signature mean?
Of course; I just meant that OP's argument about version control w.r.t. single-function files is not convincing.
Of the reasons why Java sucks, the class named `InternalFrameInternalFrameTitlePaneInternalFrameTitlePaneMaximizeButtonWindowNotFocusedState`, which is itself actually only part of a complex of classes doing some things, is not any of them. That counts against the library, not the language. Any halfway decent language will allow arbitrarily large agglomerations of things; its what they do. It's better than limiting all variables to two characters or fewer....
I actually don't do that in my own code. It was necessary to make the nested do blocks clearer, though.
&gt; I have a feeling that in the future the OS will be more an expression of a particular language Maybe more _in the past_. You ever heard of Symbolic's Lisp Machines? It looked quite awesome. Indeed, I wish something like that would re-appear in the future, for multicore-oriented OSs.
[Typed Racket](http://docs.racket-lang.org/ts-guide/), which is a typed dialect of Racket, not only is a statically-typed language with macros, but it's _built_ entirely using Racket's macro system.
It really should not reveal anything about IO. The IO monad is abstract, and for a good reason; the internals of IO is magic no matter how it's implemented (I don't count the GADT version as a real implementation, because it doesn't really execute the code). That said, it can be very instructive to see how IO is done in different Haskell implementations. It's great that Hugs does it differently from GHC, since people often seem to think that IO is a state monad just because GHC does it that way.
In the article `f` expects a `String` and `Int`, not `IO String` and `IO Int`.
But that would also give you an `IO (IO ())` since `f` returns an `IO ()`.
You don't really update functions. As soon as you change the semantics of a function you are actually writing a new function that happens to have the same name and purpose. It is only by chance if this new function can be used as a replacement for the old function without breaking stuff.
You don't need dependent types for this. They make it nicer, but the same thing is possible in vanilla haskell with existentials and some form of typeable class.
I think that's algebra rather than category theory at work. But interesting read anyhow.
My argument isn't limited to C includes, either. Even Haskell's module system comes down to uniquely identifying definitions (and hiding some of them). But that is not all there is to a module system in languages like ML. I doubt I'll be able to explain it well here, if my above comment isn't sufficient. You should learn about ML or one of its derivatives (but not F#). I think you'll find that there isn't a simple correspondence to such modules and directory or file structure. The point is, those modules are genuine language entities, independent of how the code is stored on disk. Not just simple name spaces. They provide ways of structuring programs that Haskell can't (at least, not sufficiently nicely that you'd want to actually do it).
I think the fact this works in recent copies of GHC is not surprising. Implicit parameters in HEAD at least are merely type classes with a functional dependency, using type-level symbols: f :: (?x :: Int) =&gt; ... becomes: f :: (IP "x" Int) =&gt; where: class IP (x :: Symbol) a | x -&gt; a where ip :: a ~~so the translation should otherwise make complete sense I think.~~ I don't know how to explain the behavior in GHC before HEAD, although it's worth noting that `git blame` says that section of the users guide hasn't been modified since 2003. :) (`git show ce136f8bc`) ~~It's entirely possible this behavior made sense or people were willing to change their mind about since then, just nobody noticed because nobody really uses implicit parameters. The introduction of type literals and more powerful kinding/constraints likely made this possible before.~~ EDIT: see below, this is probably bad actually now that I went back and reviewed the implementation (5 months later.)
No category theory involved, but definitely cool math. As I mentioned in the other thread, they're actually working with galois connections. This is ultimately a special form of an adjunction, so you could actually do this in more categorical language. Not sure how much that buys you in this case though -- its really the special structure of symmetric polynomials and partial orderings thats key here.
&gt; It does allow inter-dependent calls between the base and sub implementations, which might be useful (I've not been presented with use cases of this yet, but they may well exist). This reminds me of the time when I forbid (in jest) one of my coworkers from using inheritance ever again in Java. He wrote a class B that inherited from A in order to add some functionality to A's methods. B's versions of the same methods were intended to do the same thing as A's, but to perform a small modification of the result. So he wrote something like this: public class B extends A { public Blah foo(Meh arg) { Blah raw = super(arg); return adjustBlah(raw); } public Blah bar(Gah arg) { Blah raw = super(arg); return adjustBlah(raw); } // ...and a bunch more methods like that } The problem turned out to be that the overridden superclass methods were interdependent, so that you saw situations like this: public class A { public Blah foo(Meh arg) { return bar(turnMehIntoGah(arg)); } public Blah bar(Gah arg) { return baz(turnGahIntoWhatever(arg)); } // ...and a bunch more methods like that } So now the problem is that when you invoke `B.foo()` that calls up into `A.foo()`, whose author thought it would call `A.bar()` but the override means that it calls into `B.bar()` instead, which then calls up into `A.bar()` which is supposed to call `A.baz()` but `B` overrides that. Now the problem was that `B.adjustBlah()` was costly and it only needed to be done once—but because of this setup it ended up being called dozens of times (the operations that `A.foo()` bottomed out to recursively invoked `foo()` on other `A` instances). So yeah, I tend to be skeptical of the thing you're interested in.
&gt; Couldn't you use generics to keep that from happening? &gt; interface Monoid&lt;T&gt; { &gt; public Monoid&lt;T&gt; mappend(Monoid&lt;T&gt; x, Monoid&lt;T&gt; y); &gt; } Yes, but not with `interface Monoid&lt;T&gt;` as you are proposing. What happens when I call your `mappend` method with a `SumMonoid&lt;Integer&gt;` and a `ProductMonoid&lt;Integer&gt;`? Both of these are subtypes of `Monoid&lt;Integer&gt;`. What you want is this: interface Monoid&lt;M extends Monoid&lt;M, T&gt;, T&gt; { public M mappend(M x, M y); } Now `Monoid&lt;M&gt;` is parametrized by a subtype of itself, and the `mappend` method signature is restricted to only accept that subtype. So now `SumMonoid&lt;M extends SumMonoid&lt;M, Integer&gt;, Integer&gt;` and `ProductMonoid&lt;M extends ProductMonoid&lt;M, Integer&gt;, Integer&gt;` only have `Object` as their common supertype. Obvious downside is this is painfully difficult to use. I had to correct my first glaringly wrong version of this...
Basic taxonomy, you have locking structures: semaphores, mutexes and associated plumbing, that if properly put on always block read-write conflicts and you have lock-free structures, e.g. STM, which you some reasonable assurance that no conflicts occurred on any given read or write (simplistically speaking). Goetz's java concurrency book is an excellent indepth view of locking plumbing, but there's not much about lock-free structures in there. Actors, when done properly have hierarchical structures where supervisor actors take a parcel of work and divvy up to worker actors they create. They then time out the workers or if they receive back good results, they roll them up. The point is that workers never communicate with other actors at the same level, hopefully making explicit the requirements to avoid deadlock, race conditions. The canonical example is erlang's OTP behaviors, like gen_server. I think you can google for relatively easy examples (erlang syntax should be understandable for haskellers. I think the best focussed writing on this is the first few chapters of Cesarini/Thompson's Erlang book (the same Thompson who wrote the haskell text)
One nice thing about type-classes, in a non-dependently typed language, is the guarantees you get that are not possible if you're free, as in Scala, to pass any instance you want. For example, a union of two `Map k v` values can assume the same `Ord` relationship holds. 
`empty` is polymorphic on the `a` in `Maybe a`. In your solution, do you mean to write something like: class MyList&lt;A&gt; implements Monoid&lt;List&lt;A&gt;&gt; ? How does the "mempty" thing polymorphically choose between MyList&lt;A&gt; and Option&lt;A&gt; ?
The Light Table model for file organization is basically Smalltalk's—try taking a look at a Smalltalk IDE. I think there's possible gain in such an IDE, but I've never experienced it. To me the (denotational) model of a sheet of paper with many things written along its length is so strong that I'm very comfortable using it when coding. Perhaps younger generations (though I'm hardly old) will be more accustomed to less physical metaphors for organizing information and can thus take better advantage of these more fragmented forms of organization. I will say I often open the same buffer in two panels in emacs at different locations to simulate something like what Lightroom would do automatically.
We currently have four all haskell courses at Chalmers, Sweden. Two of them are essentially the same but in undergraduate and (post?)graduate flavours. So two [introduction](http://www.cse.chalmers.se/edu/year/2012/course/TDA452/), one [advanced](http://www.cse.chalmers.se/edu/course/afp/) and [one focusing on parallelism](http://www.cse.chalmers.se/edu/year/2012/course/pfp/lectures.html).
This only kind of holds. See my blog post http://joyoftypes.blogspot.com/2013/01/using-compiler-bugs-for-fun-and-profit.html specifically the section "this should not be scary." Type classes are non-confluent for a host of reasons.
The monoid type class is popular in Haskell. But, we don't have instances like `Monoid Integer` because there are tons of different monoids on Z. Encoding the monoid dictionary as an object resolves this. You can view Haskell modules as being a restricted form of "true" modules, where all the types are existentially quantified (at the module level). AFAIK this is the only theory of abstract types that anyone has worked out in any depth--and it works perfectly. Subtyping turns out to be useful in module systems because it allows you to match multiple signatures. It also gives you a theory of manifest types. Consider encoding an ML module as a Haskell record data Set = forall a m. Set { empty :: m, insert :: m -&gt; a -&gt; m, contains :: m -&gt; a -&gt; Bool, remove :: m -&gt; a -&gt; m } the problem with this is that it is not really useful, because both types are existentially quantified. If we say want a `Set` of `Integer`, we are out of luck. We can fix this in various way: one would be to use universal quantification: data Set a m = Set { empty :: m, insert :: m -&gt; a -&gt; m, contains :: m -&gt; a -&gt; Bool, remove :: m -&gt; a -&gt; m } but then type sharing works by parameterization. Harper and Pierce showed that this could cause exponential explosion of type variables. Instead, ML allows sharing by specification or "fibration" using manifest types. Essentially you would use the first definition only have types like type IntSet = Set{a = Int} turns out this can be cleanly encoded with subtyping and equality types. Here we use psuedo haskell with sub typing data IntSet extends Set = forall a m. Set { empty :: m, insert :: m -&gt; a -&gt; m, contains :: m -&gt; a -&gt; Bool, remove :: m -&gt; a -&gt; m, AisInt :: a :=: Int } that this is a real subtype is trivial from standard subtyping rules. More generally, subtyping just fits really well with modules (and not as well with plain old data) because any module should be expected to match a plurality of signatures. My view is that "objects" are just first class modules with implicit `this` references. Haskell lacks a usable system for first class modules. Also, the lack of path dependency is a big pain. We have no way to "project out" the existential type for a module in Haskell. So, even the almost usable version of `Set` in current Haskell data Set a = forall m. Set empty :: m, insert :: m -&gt; a -&gt; m, contains :: m -&gt; a -&gt; Bool, remove :: m -&gt; a -&gt; m, } is a pain in practice because you cant write a function like singleton m a = insert m (empty m) a which you would want to have the type singleton :: (s :: Set a) -&gt; a -&gt; s.m instead, you have to write something like case m of Set{..} -&gt; insert empty a except even that wont work because an existential type escapes. You don't need full dependency--Scala gets pretty far with a notion of path dependency similar (although less flexible) to the one Shields and Peyton-Jones proposed for Haskell. 
I have been advocating the use of Applicative since the second summer school on Advanced Functional programming, and it has been the preferred interface of the uulib (1997) and the uu-parsinglib libraries ever since. Above there is some discussion about the sue of &lt;* and *&gt; and &lt;$. When I introduced these symbols the idea was that the "missing" &lt; or &gt; indicates that we are not interested in the result returned at that side of the infix operator. But we often do not even have to write these. In http://hackage.haskell.org/packages/archive/uu-parsinglib/2.7.4.1/doc/html/Text-ParserCombinators-UU-Idioms.html I show how often this indication can be made implicit. In most parsers the type of something describing the parser can be used to indicate whether we are interested in the result or not. In the example we write: &gt;&gt;&gt; run (iI (+) '(' pNatural "plus" pNatural ')' Ii) "(2 plus 3" Result: 5 Correcting steps: Inserted ')' at position LineColPos 0 4 4 expecting one of [')', Whitespace, '0'..'9'] in which "plus" is a string, which we interpret as a keyword of the language we are recognising, we are thus not interested in the result and the idiom classes we have defined above this example use a &lt;* to insert the parser in the combined parser for this. In the same way a single character is recognised but its result will not be included in the result. The (+) is a function, which is automagically lifted using pure. You can thus leave out most of the combinators and let the types do the work for you. Monads are to be avoided as the plague, since they prohibit all kind of nice analyses which can be done on the underlying parsing structure as is being done by both of the above libraries. So we can e.g. compute a parser as a combination of two parsers: one probably recognising the empty part and one for the non-empty part. This is something we need for e.g. implementing our merging parsers, as shown in: http://hackage.haskell.org/packages/archive/uu-parsinglib/2.7.4.1/doc/html/Text-ParserCombinators-UU-Demo-MergeAndPermute.html 
Given any decent language (i.e., one that can build strings and write to files) you can use that language to generate code for any other language, statically typed or no. Code generation is easy. But there are two major limitations to this approach. First of all, it introduces a hard separation between the metaprogram and the generated program; but sometimes we only want to metaprogram *part* of our programs rather than the whole thing. Second of all, there's no inherent type safety; just because our metaprogram type checks, that doesn't guarantee the generated program must be well-typed. (We can compile the generated code to detect bugs, but unfortunately the error messages will suck.) So all template, macro, and staged-programming systems are working on ways to fix those two issues. I agree with Jacques that the best-in-show these days is MetaML/MetaOCaml. However, MetaML does have limitations on what it can do; you can't metaprogram everything. Template Haskell was based on MetaML, but removing a lot of features (e.g., multiple staging levels, and helpful types) and adding a lot of features (e.g., generating data types, type class instances, splices in the type level, etc.) If you want your macro/meta system to itself be statically typed, then you're going to run into a lot of hairy issues. Just think about the hygiene issues in Lisp; now think about how you'd statically rule them out, while still allowing all the macros you want. Again, MetaML is a good place to get started; they do some interesting things to ensure that variables can't cross levels in a bad way. However, the big thing these days is trying to figure out how to define a well-typed macro system for a language with dependent types. Coq's tactic language is egregiously untyped; and that's a problem.
Sure, there are plenty of ways to avoid the GADT. The point is, underlyingly, the GADT is the bare-bones description of what we need. Everything else is aesthetics or optimization. (N.B., I'm a big fan of both those things.)
 hugs_console_log=function(s){console.log(s)}; hugs_run("1") ReferenceError: Can't find variable: hugs_run
Do you have any code that uses `reflection` to make dynamic instances? It would be interesting to look at.
&gt; Does being forced to restructure code early on get in the way of testing new features and ideas for your new application? Yes and no. It can, but working under the assumption that the end goal is either a less-experimental Haskell application or a decision to scrap the whole idea, I tend to consider such things as *an integral part* of the exploration. If the types are awkward and I keep shuffling things around to make stuff fit, that probably means I've gone down a blind alley and need to backtrack a few steps. Ideally, by the time I even get to the point of experimenting with any non-trivial amount of code by *running* it, I've already explored enough of the design space to have a crude skeleton of the eventual program in place. There are certainly trade-offs involved, but as a rule of thumb I think you get the most benefit and least trouble by fully embracing the Haskell-ish approach and adapting your exploration style to that.
Perhaps by changing //C void main() { to //C pseudocode (see footnote 1) void main() { or similar?
Yeah, that's a good idea. I will change it once I get home.
Did you end up writing a program with state monad?
I don't see the benefit of this approach over Haskell's encoding of Data.Set. The Set module might have functions that work with `Set a`, `Set b`, `Set c`. But you explicitly specify the `a`? The Monoid example is not really a problem, that's why we have Product, Sum, etc newtypes. If you have lens, working with the newtypes is as concise as having explicit Monoid dictionaries. Unlike the explicit Monoids, newtypes give you the guarantee that a `Set k` will always have the same Ord relationship for a `k`, so for another type variable which can be unified with the `k`, we can efficiently union two Sets because we can assume the same Ordinal works. 
Yeah, my initial attempt at it wasn't a very good one, and as you pointed it out, it gets really messy trying to do it right.
That's a good question. I don't think that's possible, but it may just be that I don't know enough about Java to know how to do it. Sorry.
I've played with it some, but I still don't feel like I understand it yet. I can walk through the examples, but I don't know how to apply it to improve code I've already written.
I see. I will keep that in mind. My contact with Haskell so far has been mostly by studying. Although I can write programs in it and do feel the many advantages it provides I don't think I approach problems using its mindset yet. Hopefully that will come in time as I write more and read other libraries and applications. Thank you so much for taking the time to explain, that was very interesting. :)
I just realized Conal Eliott's `WithCont` is actually the Free Functor.
I dont think it is used much for "improving code", and more for the basic architecture of your project. I never learned officially the monad state stuff or transformers, but I just started to intuitively do it one day, then realized I was doing it naturally without the libraries or modules to help.
Well, count yourself lucky, I've found myself wanting to pass around states before, but I don't understand how to use the State monad well enough so I keep ending up doing it in strange ways. I'm still relatively new at Haskell and trying to learn as much as I can, but some of it still eludes.
`FreeFunctor` is `CoYoneda`. It is known that `CoYoneda` is able to make a `Functor` out of things that aren't. One common usecase is `IORef`s. When `f` is a `Functor` then they are equivalent, because then `f` ~ `CoYoneda f`. It is called the 'Free monad generated by a Functor' after all. ;)
https://github.com/ekmett/reflection/tree/master/examples
I don't think even mods can edit titles, unfortunately :(
As I pointed out, that guarantee is broken by orphan instances. It can also be broken with the use of an unsafecoerce or GeneralizedNewtypeDeriving, and I suspect other mechanisms (places to look included: the new Typeable system and non confluent fundeps). I don't think it is a good idea to rely on this in current Haskell.
So given your Program type is a functor then Operational ~ Free, but Operational is "larger" due to the ability to "operationalize" non-Functor types?
Yes.
Actually, it is straightforward once you figure out the trick. The idea behind lenses is that the only non-trivial part is selecting the correct functor to specialize them to. So what I did for your example was that I first specialized `_2` to a binary pair to avoid the complicated type class stuff: import Control.Lens f2 :: Functor f =&gt; (b -&gt; f c) -&gt; (a, b) -&gt; f (a, c) f2 f (a, b) = fmap ((,) a) (f b) Similarly, I specialized `traversed` to lists: import Control.Applicative import Data.Traversable onList :: Applicative f =&gt; (a -&gt; f b) -&gt; [a] -&gt; f [b] onList f as = sequenceA $ map f as Then I asked `ghci` what their composed type was: &gt;&gt;&gt; :t onList . f2 onList . f2 :: Applicative f =&gt; (b -&gt; f c) -&gt; [(a, b)] -&gt; f [(a, c)] Now, here's the trick. All I have to do is ask "What functor do I set `f` to in order to make it do what you asked?" You asked me to do two things simultaneously: * Incorporate `IO` side effects * Fold the values The first one is already a functor (i.e. just `IO`). For the second one, I copied the way that `lens` folds values: it uses `Accessor r`, which is equivalent to `Const r`. If you combine those two functors (without `Compose`), you get: (b -&gt; IO (Const r c)) -&gt; [(a, b)] -&gt; IO (Const r [(a, c)]) ... which is equivalent to: (b -&gt; IO r) -&gt; [(a, b)] -&gt; IO r In other words, it does exactly what you wanted. All I had to do was just wrap things in `Compose` and `Accessor` to get all the types to check. In fact, one thing I omitted to point out is the type signature that it infers when you pass it your specific lens: ywen (onList . f2) :: (Applicative f, Monoid c) =&gt; [(a, f c)] -&gt; f c In other words, exactly the type you needed. Notice that I didn't have to figure out that it needed a `Monoid` part to fold it. The `lens` machinery derived that out for me as a result of the `Accessor` functor. I'm not quite sure of the advantage of `folded` over `traversed`, though, yet, as I haven't yet encountered a situation where `folded` worked and `traversed` didn't.
The thing that helped me understand monad transformers was to work through some [`MaybeT`](http://lambda.haskell.org/platform/doc/current/packages/transformers-0.3.0.0/doc/html/Control-Monad-Trans-Maybe.html) examples by hand, using equational reasoning (i.e., substituting equals for equals according to definitions). Consider the type of [`runMaybeT`](http://lambda.haskell.org/platform/doc/current/packages/transformers-0.3.0.0/doc/html/Control-Monad-Trans-Maybe.html#v:runMaybeT): runMaybeT :: Monad m =&gt; MaybeT m a -&gt; m (Maybe a) You can read this as follows: `runMaybeT` translates a "high-level program," written in the "`MaybeT m` language," into a "low level" program written in the `m` language that uses `Maybe` explicitly. Using this interpretation, you might try having a shot at writing `MaybeT` on your own; it helped me, I can say at least. Note that not all transformers can be understood like this, however; e.g., the "translator" interpretation fails for `ReaderT` and `StateT` because "translating" the program without "running" it. As for the state monad, I find it's most useful for computations where you are "threading" a value that you update all the time. One of my favorite simple examples is in [this Stack Overflow answer of mine](http://stackoverflow.com/questions/12658443/how-to-decorate-a-tree-in-haskell/12658639#12658639), where I used it to uniquely number the nodes of a tree, then to refactor the solution into reusable pieces.
&gt; If you're familiar with imperative programming, the State monad is just a way to have local "mutable" variables. I think the mention of "variables" here is confusing as an explanation, because it makes people think that you truly have multiple, named variables as in an imperative language, when you do not. Rather, what `State` gives you is *one* hidden, nameless, implicit, mutable box that's available everywhere in your `State` computation. Your computation can `get` the content of that box an assign it to a variable, or it can `put` a value into the box, replacing the previous content. You can simulate named mutable variables by using the "box" to keep a record type with fields for each variable you want to track, but as far as I have seen this isn't actually the most common use of `State`. Rather, what happens is that some algorithms are more easily expressed in terms of such a "hidden box." I like [my Stack Overflow example of decorating a tree's nodes with succesive values](http://stackoverflow.com/questions/12658443/how-to-decorate-a-tree-in-haskell/12658639#12658639): import Control.Monad.State data Tree a = Tree a [Tree a] deriving Show -- Postorder monadic traversal of a 'Tree' mapTreeM :: Monad m =&gt; (a -&gt; m b) -&gt; Tree a -&gt; m (Tree b) mapTreeM action (Tree a subtrees) = do subtrees' &lt;- mapM (mapTreeM action) subtrees a' &lt;- action a return $ Tree a' subtrees' postIncrement :: Enum s =&gt; State s s postIncrement = do val &lt;- get put (succ val) return val -- | Tag the nodes of a 'Tree' with successive values, starting from @init@. -- Postorder traversal. tag :: Enum t =&gt; t -&gt; Tree a -&gt; Tree (a, t) tag init tree = evalState (mapTreeM step tree) init where step a = do tag &lt;- postIncrement return (a, tag) What `State` gives you here is the ability to express the notion of "get me the next tag" (`postIncrement`) without having to carry tags around as arguments; the hidden box is used to hold the next tag. Or in other terms, `mapTreeM` and `State` allow you to write the `tag` function almost as if you were just mapping a function over the elements of the `Tree`.
Yeah, then it couldn't even load the JavaScript into the JS engine, like Opera. Not much we can do about that.
Wouldn't `touches` potentially leak memory? Why not? Also the post says the type of `taps` is `Signal { x :: Int, y :: Int }` whereas the docs say `Signal [{ x :: Int, y :: Int }]`, which could also leak memory if I'm not wrong about `touches`.
Are you envisioning a list that just keeps growing over time? The touches are only in the list as long as the are happening. So if you have three fingers on the screen, the list will have length 3. When you remove them, the list is empty again. Do you mean leaky in a different way? Also, the docs are definitely wrong. Thanks for pointing that out! Should be fixed now.
True, but I hadn't seen anyone else point out that `CoYoneda f` is the free functor generated by `f`.
Ah, that makes sense.
I'm not experienced enough with first class module systems to appreciate the advantages you listed. I of course don't find the newtype code convoluted. Note that the lens library also dissolves the need to copy large apis into type classes, because there are polymorphic lens type classes that cover sets, maps, and more with the same apis. In many ways lens is finally providing the polymorphic prelude we've all been waiting for. 
Orphan instance clashes can and should be detected at compile or link time. generalized newtype deriving and unsafe coerce are known to be broken. Custom Typeable instances are also known to be broken and there are plans to forbid them. Just like unsafe Perform IO doesn't mean we shouldn't trust pure types anymore, these compiler bugs/deficiencies shouldn't mean we don't trust the single instance guarantee. 
you can break the single instance guarantee with out a clash with orphan instances. Although the compiler warns you when you define them obviously. I have not worked with the new poly-kinded Typeable system, but I am highly suspicious that you will be able to break the single instance guarantee without custom instances. Anyway, this is all besides the point. It doesn't hold in current Haskell with code that does not generate any warning is and is inferred safe. Even if these "bugs" are fixed, we have a negative property of the system for which we don't have any reasons to believe holds except "we don't know of any counter examples." A bondage and discipline language written by constructivists shouldn't work that way. The single instance guarantee is not necessary--see Coq and Scala which both don't have it and manage fine. The correct way to build modular software is well known--use modules! What is more, the single instance guarantee is absurdly over-limiting. It is also fundamentally anti modular--magic implicit objects in global scope. We should treat it as something to be overcome.
&gt; we have a negative property of the system for which we don't have any reasons to believe holds except "we don't know of any counter examples." This is exactly what we know about Agda's soundness, as well. &gt; The single instance guarantee is not necessary--see Coq and Scala which both don't have it and manage fine For Coq, I understand, because dependent types resolve the issue. How does Scala do efficient Set/Map merging for same key type? 
Okay, so you walk your structure thanks to functor composition. I was aware of that technique thanks to [this paper](http://www.comlab.ox.ac.uk/jeremy.gibbons/publications/iterator.pdf), just didn't knew that ```lens``` made it necessary. **EDIT:** Maybe it doesn't: I found functions like foldlMOf or foldrMOf in Control.Lens. Testing it... &gt; I'm not quite sure of the advantage of folded over traversed, though, yet, as I haven't yet encountered a situation where folded worked and traversed didn't. Okay, so we're on the same page, here \^\^ The recent post: http://statusfailed.com/blog/2013/01/26/haskells-strength-generalising-with-lenses.html seems to be related to what I asked (it shows how to "sequenceA" some datatype with lens).
Why isn't `Command` a (trivial) functor? Simply declaring the instance instance Functor Command where fmap _ = id meets all of the functor laws...
That doesn't typecheck.
One of my favorite Haskell tools is `where`. It naturally programs the idea of a minifunction.
Actually, ```lens``` defines foldlMOf, that allows such a thing: &gt;&gt;&gt; foldlMOf (traversed . _2) (\acc x -&gt; print acc &gt;&gt; return (acc+x)) 0 [('A', 1), ('B', 2)] 0 -- printed 1 -- printed 3 -- The final result Or with your example: &gt;&gt;&gt; foldlMOf (traversed . _2) (\acc act -&gt; fmap (acc++) act) [] [('A', readLn), ('B', readLn)] :: IO [Int] [1]&lt;Enter&gt; [3,4]&lt;Enter&gt; [1,3,4] (In both you can replace ```traversed``` by ```folded``` and it doesn't change anything)
I don't think ImplicitParams is so much overlooked as frowned upon since Haskell is mostly about making stuff explicit (explicit error cases in return types, explicit distinction between two values represened in the same way but different in the domain,...).
Err... right.
Neat! It's slightly different from what I wrote, but still equally useful.
Yes, that's correct. Lens generalizes many prelude functions by qualifying them using the lens. The example I love to use is `sumOf`: &gt;&gt;&gt; sumOf both (1, 3) 4 The issue is that Edward is trying to basically generalize the entire `base` library, which is why `lens` is so gigantic. I think he should focus on just building a small lens prelude and then worry more about teaching other people how to add lens support to their libraries.
That's because `sequenceA` (from `Data.Traversable`) is kind of a "functor transposal": sequenceA :: (Traversable t, Applicative f) =&gt; t (f a) -&gt; f (t a) and in `strength :: Functor f =&gt; (a, f b) -&gt; f (a, b)`, this would be `t ~ (,) a`. Although there is the unfortunate stronger requirement `Applicative f`.
Yes, however, the libraries he's adding support to (vector, unordered-containers, text, etc.) are kind of de-facto standards in nowadays Haskell, and may soon belong in the Haskell-platform. So this may not be as much of a problem, it may even promote the use of those libraries.
s/bad/good/ Having the ability for separate interface and implementation is a _very_ good thing. (Requiring it, always, not so much, of course.)
My issue is that he should focus his energy on teaching people to define their own lens functions instead of doing it for them. It's more sustainable in the long run. Otherwise lens comes off as a tool that is only for elite Haskell programmers.
Perhaps I am losing something with the loose discussion of the types of 'f' and 'lift f', but for your second natural transformation for lift: f :: f a -&gt; g b lift f :: FreeFunctor f a -&gt; g b I think you need to change both "g b" to "g a" above. Consider: lift f (FreeFunctor x c) = fmap c (f x) The types of x and c after you open the "(FreeFunctor x c) :: FreeFunctor f a" is some unknown (f d) and c is of type (d -&gt; a). Consider (f x :: f ?) where ? had better still be 'd' or (fmap c (f x)) cannot work. So f must have been instantiated to 'f :: f d -&gt; f d'.
Operational is the free monad generated by a type constructor while Free is the free monad generated by a functor.
Not strictly Haskell-related, but it's an interesting example of what comonads can do. [Here](http://www.reddit.com/r/compsci/comments/179hv8/coeffects_typing_contextdependent_computations/)'s the thread from /r/compsci.
What are session types?
I call dibs on the first comonad tutorial
The idea was that you could write code for two actors that communicate over some medium (for example, a socket) and get the type system to guarantee that their communications were following compatible protocols. For example, you could write a client with a type that says "Sends an integer, and then expects either a list of pairs of a floating point number and a string, or a single integer and a single string." And then you could write a server that says the other side of that. And the code that set these things up and pointed them at each other would fail to compile if those protocols did not match.
&gt; composing these two functions will have unpredictable module boundary breaking behavior: even though all instances obeyed the typeclass laws. Well, introducing an "orphaned instance" is a violation of "typeclass laws," at least in my opinion.
I can see how that would get very complexly typed very quickly!
Correct me if I'm wrong, but isn't there a mistake on slide 7? I think it ought to be a union, not an intersection of coeffects. 
Is there a good intro to this library anywhere? This is the best I've seen anywhere so far but it's clear there's a lot going on that I haven't been able to decipher yet.
Ah, that makes some sense! I really like the way you can come at this solution from a whole bunch of angles.
At a basic level `machines` is rather simple. You build a `Plan` of what the `Machine` should do, using a `Monad`. In a plan you can 'await' something, or emit something, or have optional monadic side-effects. Effectively, a `Plan` just a free monad transformer, augmented with the ability to emit values. You can then `construct` `Machines` using that `Plan`. A `Machine` is a lot like a `Pipe` from the `pipes` package, except the input types can be more varied, e.g. you can have a machine that takes two inputs or which can block on more than one input non-deterministically if you want. Now, we call a machine that takes a single input a `Process`. You can extend the output any machine with a process. And with many other input types, such as a determistic `Tee`, you can `cap` one of the ends of the tee with a monadic (or pure) `Source`, this leaves only the other input of the `Tee` it into a `Process`. Generally the idea is you build a lot of simple machines, cap off some of the inputs with monadic or pure sources, then connect up the machines. When you're done you can drive it with a custom command language or just starve it of input and see what comes out. You can build machines in other ways too. e.g. you can use Mealy and Moore machines, which because of their more restricted 'call and response' nature admit more instances than you could have for a full machine. The overall shape of the API described so far, I'm pretty happy with. The parts I'd like to improve mostly center around how you compose the 'command language' for your machine. I would like a more compositional model than what we have here where we can upgrade a Tee to permit it to use confirmed consumption, or to allow pushback, etc. The design of machines permits you to split responsibilities between the driver and whatever monad you build the machine over. Effects delegated to the monad happen in a demand driven fashion, and have a harder time reasoning about resource management. Effects delegated to the driver (by baking them into the 'command language' of what you can `await`) have an easier time dealing with resource managment but you have a harder time composing machines made with them. We built the `machines` package mostly to sketch out the API design for the scala library Runar talks about at the end of his talk, but it was interesting enough in its own right that I pushed it out to hackage.
Well, intersection kind of makes sense. Coeffects track the contexts where the computation makes sense and as such more coeffects require more specialized environment. Using the example on that slide, we are given `writeFile` that works in environments `server`, `client` and `readInput` that works in `client`, `phone`, then `writeFile . readInput` should only work in environment supporting both operations, which is just `client`.
I don't understand all of it. Do I understand this correctly? * They are talking about a `Thing` that has kind Thing :: (Monoid m, Lattice m) =&gt; m -&gt; * -&gt; * (i.e. indexed, and indexed by a value not a type) and it is *not* a comonad in the sense of Control.Comonad, because their `cojoin` has type cojoin :: Thing x a -&gt; Thing x (Thing y) a (i.e. the type indices don't have to be the same; and I may have the `x` and `y` the wrong way round). (Not proven that it has to be a lattice; further research required.)
&gt; As the pipes library is not CPS'd this results in an asymptotic slowdown every time you do this. machines is structured so that you can't do this by mistake. This sounds worse than it actually is. All this does is add a small constant overhead to each step on the order of nanoseconds for the composed pipe. Also, you only pay it once since all subsequent operations after the composed pipe are automatically right-associated by do notation. In contrast, if you do the CPS approach, you also add a small constant overhead to each step (also on the order of nanoseconds) every time you compose two components. The reason is that composition requires a lot of operations at the head of the structure, which CPS is slow for. I benchmarked both approaches for pipes. The CPS approach is only faster if you rarely use composition. Once you start adding just a few stages the ordinary free monad wins out in speed, which is one of many reasons I chose the free monad (the others being code clarity and reliable rewrite rules). Also, in practice the point of streaming abstractions is primarily composition. Pipes does offer the ability to sequence actions after composition, but this feature is used significantly less than composition, so I optimize for composition. Finally, machines doesn't even offer the feature to sequence things after composition, so the comparison is moot. Edit: I also forgot to mention that because all the proxy operations are type classed, there is no reason you can't implement a CPS-style proxy and have it implement the `Proxy` type class. Then you can use it as a drop in replacement and it can reuse all the standard library proxies for free. I just never did so because of the reasons I mentioned above.
Thanks for sharing this :-) One thing that maybe needs to be more emphasized in the text is that there is a surprising difference between things that can be done with plain _comonads_ and with our _indexed comonads_. For example, Maybe is not a comonad ("counit" is not defined when the value is Nothing), but it is an indexed comonad ("counit" is only defined when the tag specifies that the value is Just a). There is quite a few comonad tutorials using non-empty list and passing additional context around (similar to Reader monad), but I think that a lot can be done with indexed comonads. On the ohter hand, with monads - you can add indexing (to track e.g. specific read/write operations) - but State or IO monads will work fine even without this.
The intersection on slide 7 is intentional - as @vituscze says, the idea is that you track environments where the function is allowed and - if you call two functions - the overall code can only run in the intersection of their allowed environments. It would be union if it tracked names of resources that are needed, because then you'd be unioning two sets of resources that must be available. (Anyway, the slides are less up-to-date than the paper, so I think reading the Section 2 in the paper is a better starting point)
I think that this article can be regarded as a 'status quo' summary of ghc as for dependent types. Very cool!
Every language can do this, if you write code generators and use them from your build system.
Your post claims that the singletons package doesn't use the GHC machinery, but according to [the website](http://www.cis.upenn.edu/~eir/packages/singletons/) &gt; November 1, 2012: singletons-0.8.3 released, now compatible with GHC.TypeLits. Does that mean what I think it means? Also, thank you for the tutorial.
The point I wanted to make is that there is a general reason why OOP is successful, which is much more basic then all the inheritance, re-usability and class concept discussion. This one is, it allows to keep and administer large amounts of state in a complex program in a reasonable manner. This itself seems to be quite easy to do also without OOP, but actually it is not. And this "feature" is combined with the mutuability of state in OOP. So I think the point was mutual state and OOP are very close to each other and OOP without it, might be quite useless. Of course I do not wanted to say, that all other OOP features are not important, I just wanted to highlight this single fact, since it is often not seen this way.
Oh, that's cool! I hadn't heard about that by the time I posted the tutorial, though it appears that at that time, it was already compatible! Thanks for letting me know. If I get a chance, I'll update the post with this information.
The point, I wanted to make (see also above answers) is that management/administration of large amounts of state in a program overall is possible with OOP (think for examples GUI's, Games, Simulations) and this is a core feature of OOP. So this is more on the big picture, whole program then on one data structure in it. This means: the internal modification of data in an object is an OOP language tool, which makes the complete feature: large scale state handling, possible. I think I need to detail out argumentation to make that clearer. 
Very interesting read! Found one small mistake: `Σ A B` is missing from the sigma-type definition, it should be: data Σ {a b} (A : Set a) (B : A → Set b) : Set (a ⊔ b) where _,_ : (proj₁ : A) → B proj₁ → Σ A B
Great turnout! I love how international the haskell community is.
Oh, thank you! I'll fix that right away.
The `Machine` I execute isn't CPSd and you don't have composition within plans, so I agree that the two approaches are largely incomparable. I prevent you from doing the thing that leads to a slowdown in your case. By CPS'ing the plan and using a flat unbindable machine, I'm able to avoid both forms of overhead, in exchange for giving up one style of composition in plans and another in the machine. That said I think your assessment about it being a one time cost is off, if you have any composition left associated you pay for it once for each left asociated bind that contain that composition.
The interesting thing about pipes is that the only reason it gets resource allocation and finalization correct is precisely because it is both a monad and category simultaneously. This lets me derive the error-handling version of composition in terms of the primitive non-error-handling version: -- Left composition is error-handling version -- Right composition is primitive version p1 &gt;-&gt; p2 = EitherT (runEitherT p1 &gt;-&gt; runEitherT p2) Same thing with all the other proxy transformers. They all work because the monad cohabits with the category.
There is nothing as clear as in the world of monads, but I think the paper The Essence of Data-Flow computations (http://www.cs.ioc.ee/~tarmo/papers/cefp05.pdf) should be a good introduction. Another paper by the same authors has a lot of math in it, but it describes the few standard comonads - see Comonadic Notions of Computation (http://www.cs.ioc.ee/~tarmo/papers/cmcs08.pdf). Dominic (a coauthor of the coeffects work) has been doing quite a few on comonads and I think his drafts also contain good introductions - see here: http://www.cl.cam.ac.uk/~dao29. One of his slide decks is - I think - very good and helped me understand what is going on with comonads: http://www.cl.cam.ac.uk/~dao29/talks/comonads-and-codo-talk-dorchard-2011.pdf Aside from papers, the blog posts that I've seen and found useful are: http://calculist.blogspot.co.uk/2006/08/sigfpe-explains-comonads.html with http://blog.sigfpe.com/2006/06/monads-kleisli-arrows-comonads-and.html and http://blog.sigfpe.com/2006/12/evaluating-cellular-automata-is.html (celular automata are similar to non-empty lists, but 2D). 
This topic was raised on the GHC mailing list at http://www.haskell.org/pipermail/glasgow-haskell-users/2013-January/023439.html - and at first glance, this looks like a recap of the same discussion from that thread. It may be more illuminating to read the thread. I'm confused by this page; there are some jarring typos (`forall b. exists b. G b ~ G a`), some false statements where it's unclear what the author meant (e.g., the claim that `t = F a` is not the only solution to the constraint `F a ~ t`), and just the choice of a font where "~" looks exactly like "-" until you zoom in 200%. It could just use some editorial attention, I suppose, but I'm finding it hard to follow.
Excellent, thank you! I have [1] and [2] in my to read list, but [3] and [4] are certainly new to me. With the blog posts I have seen them, but I think I really need to sit down and play with the code - get a real feel of it.
Apologies, for posting a badly formatted literate Haskell program that contains some typos and type errors :) More seriously: Thanks for reading and spotting some typo, should be (forall b. exists a. G b ~ G a). The issue raised in one sentence: *Non-injectivity may cause ambiguity and ambiguity may cause type inference failure* The issue has been raised several times by myself and others as early as 2008! I'm wondering how many people/emails have stumbled over this issue since 2008 and Conal's email? My dense blog post makes no attempt to provide a comprehensive introduction to this subtle issue. I've simply put together a minimal set of examples plus pointing out the connection to functional dependencies. 
This is not complete, but I am posting it here in the hope of getting some feedback on the design and the docs. Is there anything you think we should change or add? Source code is here https://github.com/ghcjs/jsc If you install package (as per the .travis.yml file) using cabal-meta and load TestJSC.hs in ghci you should be able to try out the "runjs" examples in the documentation.
Singletons are still a bit of a corner. But perhaps we can achieve more roundness by appropriate use of pi...
The links at the end are great, by the way.
You are right about actors are *trivially* implementable with STM, but you are wrong about the other way around. STM is *trivially* implementable with actors. The trick is to make a process and then factor all reads and writes through this process. Then you add a version tag and do MVCC (Multi Version Concurrency Control) on the data, and you just implemented STM. It will be slow as hell, but from a semantic perspective it is implemented. In my experience, most concurrency models can be implemented in each other if you want.
What makes Erlang's message passing model work is awfully subtle and it does take some knowledge about it to claim that the model is poor. I would at least need more than a subjective meaning about it being poor before I would accept it. In my opinion, both models have their strengths and weaknesses, which you must address. For a language like *Haskell*, actors are a bad fit. The way the mailbox works messes with typing, so you need another kind of model. STM looks like the current best solution. For a strict/eager language, you have other options like the Join-calculus or CML too. The approach to solving problems in the two models are quite different. In STM, you often wrap main data structures in STM's and operate on them in a safe manner by the STM operations, retrying upon collision. In Erlang, you make the data into a process and have the *process* do the work on behalf of the data. The approach is often completely dual and it works. Sometimes for the better and sometimes for the worse. Notable differences: * Erlang does distribution over multiple machines, STM's don't. * STMs provide a typed framework, Erlang doesn't. * Erlang cannot provide a typed framework because it messes with and makes distributed upgrades of the running system complex. * Erlang has a concept of error-handling through fault tolerance. It takes some effort to implement this on top of STMs. Especially if you need it distributed. * STMs can live-lock. * If you receive on several STM TChans in a selective receive manner with `orElse` then you need to handle *fairness* among the channels. You can't do it in the same order as some channels can then starve others. As for an example, look at `etorrent` and `Combinatorrent`... I am the author.
Advanced topics: - deep EDSL (typed) design - asm-level optimization - data structure design with strict/lazy semantics - proofs of Haskell programs - generating .hs from a prover - adding your own primops - adding your own GHC optimizations - rewrite rules design - type families design - new language FFIs (e.g Java) - embedding Haskell in another GCd lang - declarative UIs - research topics in FP...
A networked pong game with the binding to the SDL library.
Do you know QNX? I worked in a lab where this was used for real time experiments.
So far, I've been learning Haskell by building a CMS using Yesod. Now, this has already been done (by [clckwrks](https://github.com/clckwrks/clckwrks) and maybe a few others) and the one I'm building isn't going to be particularly powerful, and I have no intention to actually release it to the public. But it's been a great way to acquaint myself with Haskell in both a web-dev context as well as a back-end processing context, with a healthy mix of data types thrown in. It's been frustrating at times but I've learned a ton already.
My current toy project, which has been the most educational of all so far, is a spreadsheet. So far, I've had to: build a parser, build an interpreter, use various graph algorithms to determine evaluation order. Next up is figuring out ncurses bindings to build a UI.
I suggest vty over ncurses. ncurses is terrible.
What 'advanced' means scales with what you know. Basic Applicatives were advanced for me at some point. Now Lens, RankNTypes, GADTs, detailed optimizations are advanced. Let's see how that looks like in another year.
I just wrote a Scheme interpreter as a weekend project. It's hard to describe how strange it feels when you write down stuff you think works, then hit :r in GHCi followed by a fix at the corresponding line number, until it compiles - and it does what you thought it should do. Boy there were some bugs in there I could've spent a day on in C++. Why is this an especially good choice for a project? * When you're stuck there's an online tutorial (["Write Yourself a Scheme"](http://en.wikibooks.org/wiki/Write_Yourself_a_Scheme_in_48_Hours)) that may be helpful. * You can always extend the interpreter, it's not getting boring too quickly. Once you nail the [R5RS standard](http://www.schemers.org/Documents/Standards/R5RS/HTML/r5rs-Z-H-2.html), you can always go [R6RS](http://www.r6rs.org/final/html/r6rs/r6rs-Z-H-2.html) if you want. * You're learning Lisp on the way, which turned out to be a very neat language for me. * You're writing a code simplifier, which lets you get a glimpse of the kind of things GHC may do internally (on steroids though). * It's not nearly as mysterious as you may think initially.
Whatever Oleg is doing.
Nothing he's actually up to requires `IncoherentInstances`. It is only needed for the final excess `class Implicit x where implicitly :: x` and what depends on it -- i.e. `lookup'` and `tuckersName'` -- these had just been defined a bit more longwindedly as `lookup` and `tuckersName` without the extension. 
Take a look at the curses based UI I built for tabular data here: http://hackage.haskell.org/package/cursedcsv-0.1 In fact, if you wanted to fork and generalize it a bit so you could just use it as a backend, that would be great! (repo here: http://patch-tag.com/r/gershomb/cursedcsv/home)
Thank you for your great effort! I always admire ghcjs. BTW, I haven't taken care of webkit-javascriptcore these days since it's now beyond my ability. Would ghcjs take the maintainership of webkit-javascriptcore fully since it's adequate there anyway? 
p.s. Using github webpage as a haddock documentation server is brilliant. How did you do that?
Topic: Keeping a user updated on the progress of a long-running algorithm. Disclaimer: Haskell newb. I know, I know, this is likely considered impure and not playing to Haskell's strengths. Still, it's common, useful, and simple in many languages.
Sounds like a super fun idea :) I am gonna steal this as well.
You did a fantastic job with webkit-javascriptcore. It might make sense to make it a 'ghcjs maintained' package if you don't want sole responsibility for looking after it. If you want to move it into github.com/ghcjs I'll delete the fork that is there so you can transfer it. Then we can all manage it and you want need to worry about keeping the root repo up to date.
https://github.com/ghcjs/ghcjs.github.com I have this checked out in my ~/.cabal directory, so I can use it to upload .jsexe files and in this case the Haddock docs. I would like to set it up so that it is done automatically by Travis-CI, but to be honest it is fairly easy to do it manually.
I love this list. ESP since I'm in the progress of trying to get started on several of them within a single project :-) All of these examples crop up quite naturally when trying to make really great tools. 
Interleaving IO and work isn't an advanced problem. See e.g http://hackage.haskell.org/packages/archive/terminal-progress-bar/0.0.1.1/doc/html/System-ProgressBar.html Though you could get really fancy if you wanted to...
Okay, thanks! I now made my repo up-to-date with ghcjs/webkit-javascriptcore, so ready for migration. :-)
Abuses of CPS / Category theory / internals of lens.
This was my first major project. Learning Parsec is an early joy
Write Conways game of life. If you're feeling particularly adventurous put an opengl frontend on it. 
I have deleted the fork in ghcjs, so you should be able to "Transfer Ownership" of the repo to ghcjs now. Thanks.
Thanks, I didn't know about vty. vty-ui looks to be just what I was looking for.
Scheme is an interesting language to implement. The typical eval-apply loop is basically not what people tend to do in functional languages and is kind of odd. You can do it all with just eval. Dunno why they don't.
One of my first programs ever was to translate a printed pascal code of a Mandelbrot set generator I found in a text book to BASIC. I might try this for nostalgic reasons.
Please do. I think I know now: parametricity implies, among other things, observational equivalence `App ((.) &lt;$&gt; g &lt;*&gt; f) x == App g (f &lt;$&gt; x)`, which is one of the equation arising when trying to prove the applicative laws. Still, I'd be very interested in seeing a formal proof.
The way I eat them they end up more like comonads.
I'm writing a little article on implementing a frontend for the game of life using Reactive Banana and WX. I think it'd make a good introduction to FRP. I wonder if anybody is actually interested in that though...
You just need to wrap them up better. Of course, if you go too far, they end up looking like arrows.
I'm surprised no one has mentioned fixed-point types and catamorphisms, anamorphisms, etc over them.
This thread has taught me I have much yet to learn.
My suggestion would be to take a project you've wanted to work on and then decide that you want to write it in Haskell. You'll get a better sense of capabilities and drawbacks by working on a project that actually means something to you. For me, it was/is a game utility, and I've had exposure to a lot of the library ecosystem consequently.
Anything that's still and active area of research.
I don't see much of the point in "small" projects now. You will always learn best if you're doing something that your passionate about. The work is going to be really really hard, because you don't know the language, so you need that motivation. If you are buzzing about the idea you'll put up with all kinds of fuss just to get some results out. So I say, choose something that you really want to write, no matter how ambitious it is. Sure you're going to fail, but that's not really the point? You'll learn a *lot*, and most importantly - you'll learn the things that are important to you.
I'd read it 
You should really take a look at where `distributed-process` is at these days. Mailboxes and typing can go great together!
That's not sufficient unless you have a single global lock. MVCC doesn't even give you STM with Haskell semantics (you get write skew: http://en.wikipedia.org/wiki/Snapshot_isolation). And in fact writing decent MVCC logic isn't that trivial, even then. I don't dispute that of course you can write STM over another concurrency layer. It's just that the underlying algos are rather tricky. Meanwhile, there are a few subtleties to actor based communications, but it's actually much more straightforward to build, and the complexities come with building distributed communication and serialization at all, not so much the nuances of actor semantics.
You'd have to have an implementation of a declarative UI library competitive in scope with wx or gtk first...
You take a Burrito, mash it into pieces, and put that into another Burrito instead of Guacamole?
Parsec is usually one of the most enjoyable parts of writing a program in Haskell. Maybe I should store all data as String so I can use it more! ;-)
Or perhaps frp in Fay?
Ah, that `Serializable` instance is pretty neat. Do note however that the semantics implemented in `distributed-process` are not really "actors". Erlang doesn't in general match the original actor model, and neither does it seem that cloud haskell does. It is actor-like, but was not inspired by the actor model at all. One of the contention points is that you can choose to handle a message matching a pattern and you don't have to pick out the first one.
An interesting point is that the mnesia database implements a variant of STM, though without the `orElse` combinator, so it is not as if Erlang programmers have no access to the general pattern if they really need it. In fact, it is nice to have distributed database tables once in a while :)
I would read it, last time I tried FRP it went over my head and there didn't seem to be a ton of good resources besides academic papers.
Very nice. Having been [looking](http://stackoverflow.com/questions/13511511/functional-reactive-programming-is-fay-expressive-enough#comment19016621_13511511) for something like that. :D 
You're welcome. This is obviously for demonstration purposes only, though adding more Bacon.js bindings should be trivial. I hope qualified imports will be supported by Fay soon...
I noticed that this was released recently: [HsQML](http://www.gekkou.co.uk/software/hsqml/) If it implements enough of Qt Quick, it might be considered competitive in scope with wx or gtk (?).
I'm just learning Fay. I tried Elm very briefly and Fay seems like a better solution for many reasons. But the FRP ideas shown in Elm seemed very promising.
I think the Haskell symposium is a good place to look http://www.haskell.org/haskell-symposium/ , especially previous symposiums. What were people excited about in 2006? (http://icfp06.cs.uchicago.edu/) What were people excited about in 1996? (http://www.informatik.uni-trier.de/~ley/db/conf/icfp/icfp96.html)
brilliant!
Some goods ideas for the *Succ (Succ Zeroth) Obfuscated Haskell Code Contest*.
One thing I couldn't accomplish was something of the form True = Prelude.False False = Prelude.True ... otherwise = True ... so that deleting the definitions in the beginning can't be deleted without flipping the 'otherwise' definition, so it would be a self-worsening bugfix. Suggestions?
How about swapping the branches of if .. then .. else ..? {-# LANGUAGE RebindableSyntax #-} module Main where import Prelude ifThenElse True x y = y ifThenElse False x y = x test = if True then 0 else 1
The language extension kind of waves a red flag there.
Thanks for posting this clarification! I'm not sure many will notice it though if you post it here, because the thread is old and no longer on the front page of the subreddit. If you'd like more exposure, you might want to try a new post to r/Haskell highlighting your response. I only found it because you replied to me and I get notifications for that.
I found reactive-banana very easy to use, so I am surprised to see "declarative UI" listed as an "advanced topic". Then again, I haven't tried other libraries to accomplish the same.
You should of course make all functions pointfree. Even, or especially, if this is not trivial. 
&gt; QuickCheck is a statistical test that generates false sense of security, and should be avoided for that reason. This was probably meant as a sarcasm, but I kinda agree. [SmallCheck](https://github.com/feuerbach/smallcheck) is better in that it's not random and if it succeeds, you know which tests, exactly, it has run. *shameless plug*
QuickCheck supports means to test test coverage, and usually there aren't that many corner cases for a function anyway. Yank the tests up to 10000 per cycle and you won't miss a thing. And if you want the test cases printed, use verboseCheck. (Be warned: output may be long.)
Was the second sentence also a suggestion?
Still, it's strange that you need a package for it, or have to alter all your type signatures to accommodate this kind of logging. The difficulty also shows in haskell's "error" function, which basically admits "alright, fine, SOMETIMES it's inconvenient to have to denote IO in every function that can do it". For me, it is indeed a pain to add debugging messages where it would be trivial in other languages. Nonetheless, I'm doing my best to appreciate the Haskell way. I understand there's an advantage in separating out pure from impure, and agree it's not strictly speaking "advanced" but Haskell does make it hard.
For the submission? Oh, yes.
By and large it's become conventional to avoid error in Haskell. Really the convenience ends up hurting you in the long run since you end up losing confidence about the totality of your code. Packages like Safe and Errors really help to make that practice easier. You also don't need anything too special to get the progress bar going. It's just some IO actions. The majority of that library is there to make it cute. 
Don't forget to do as much type-level programming as you can. Not only does type-level programming mean it's more likely to be correct (because it's total), it also makes it harder for other people to change it, reducing their ability to make it *less* correct.
Ah, but you haven't *truly* begun to appreciate the sheer power of that naming convention until you've seen [what it does to the haddocks](http://hackage.haskell.org/packages/archive/numeric-prelude/0.3.0.2/doc/html/Number-Complex.html#t:T).
Actually, relying on namespacing for types or classes is a very bad idea because they won't appear qualified in Haddock documentation. The worst offender is `ByteString`, which can denote either the lazy or strict version, but the haddocks won't distinguish them.
Oh good I'm glad I'm not the only one, I got this error really inconsistently and was starting to think I had gone crazy.
Oh! There I am, right in the middle on the deck!
Even better, some functions/files use both of them! y'know, for conversions and such
Shouldn't all extensions be added on the command line? it's more flexible that way.
Well, this is convenient. What are you up to these days? I'm coming over to the Valley this Summer, so It would be an honor to meet you. *EDIT: grammar*
Are you aware of haddock's `-q` option?
Does that affect the haddocks on Hackage?
Type level programming in Haskell is _not_ total, though any stuck terms will get stuck at compile-time.
It isn't? Could you give an example? I honestly don't use much type-level programming in Haskell. I just use Agda instead :)
&gt; Naming conventions can help making code more readable. For example in (xs:x), xs stands for "x singular", and x contains the rest of the x. I laughed outright at this one.
If you put that option in the .cabal file, then it should, I guess.
 type family F :: * -&gt; * defines a nowhere-defined type function.
Fair enough! :) These days, that's what I find myself doing… data Nat = Z | S Nat type family Prec (n :: Nat) :: Nat type instance Prec (S n) = n So, we've defined a partial function over terms in the promoted `Nat` kind; of course, any type family you make over terms in `*` is also a partial function of some sort. Trying to instantiate `Prec Z` will fail at compiletime anyway, but if type-level programming were total, the type family itself would fail to typecheck. So, type families can mimic functions, but are not really functions, since on the one hand they are not total, and on the other hand, they allow matching on `*`. **Edit (additional notes):** If anyone cares, I did some work a long time ago that allows type-level functions to be reflected down to the value-level. That is, given a type family `'Nat -&gt; 'Nat`, we can generically generate a function `Nat -&gt; Nat`, given some sort of singletons kit. Whilst this does indeed sort of help to solve the problem of maintaining multiple versions of a function at different universe levels, I pursued it no further for the simple reason that type families are not total, and there's not any way (as far as I know) to ask the compiler to enforce totality for a particular type family. So, writing functions in this manner becomes very annoying and error-prone. I've long-since come to the belief that we rather need to just allow some notion of Π-types in Haskell, beyond the clunky singleton approach.
SmallCheck is great, but not always. When I was testing my bindings to the Snowball stemming library, whose UTF-8 version turned out to be broken, SmallCheck reported everything A-OK but QuickCheck quickly found the broken cases. For SmallCheck to find the same cases would take a very long time, because unicode is vast.
I usually check my functions once, and then I do pay attention to the console output. The code for the entire thing is around 10 lines. Anyway, stop talking about things that may be beneficial here!
&gt; Meh, most of these feel like straw men or not necessarily a bad thing. Some of these were born out of someone using them, and fighting for acceptance. When someone argues that "forefer" should be written with an f, you can say "it's not necessarily a bad thing", or you can take it with humor.
Riiight, how could I forget unicode! You're giving some pretty good examples already. I wonder whether there are more misleading ones - are there symbols that look a lot like normal ones? I.e. one could define a "map" function for Data.Map where the p is some different Unicode character so you don't have to import it qualified. By the way, your "Cyrillic" looks like a snowman for me.
Haha, that's a great idea. As I said up there, Cyrillic is great for this. You can have `map` and `mар` and `mаp` and `maр`. (Using the Cyrillic а and р.) So my `let a ☃ а = а - a` example uses two different variables called `a` and `а` respectively :). Some useful Cyrillic letters: аеорсух and capitals: АВЕКМНОРСТУХ.
Thanks for the feedback. Currently the Char generator just picks letters 'a' through 'z', which is hardly acceptable. I'll modify it so that it more or less fairly represents various parts of Unicode (any suggestions on what those should be?).
 let map mар maр = mар : map mар maр; mаp mар = map mар mар in mаp "map"
Isn't the point of SmallCheck that it tests exhaustively?
Yes, but there are many different orders in which you can enumerate values. (Or are you asking about the current generator? Yes, it can be considered a bug.)
...and a GHC 7.8 release is planned for "mid Feb" [according to SPJ](http://www.haskell.org/pipermail/glasgow-haskell-users/2013-January/023668.html) !
I believe the entire misunderstanding could be solved by using any of the numerous synonyms to "attractive" without the sexual connotation. If you'd have replaced "attractive" with "appealing" in your above text, none of the original article's criticism would hold.
I think we opened Pandora's Box here. Edit: I incorporated your ideas in the article.
I'd also be an honor if I could meet MtnViewMark.
Indeed, that was what I was also thinking. We, non-native speakers of English, might sometimes have a different (or incomplete) set of connotations with a word. When in doubt, ask a speaker or writer what he/she meant, before posting accusations on the net. They can do a huge amount of reputation damage, even when it is not deserved.
Glad that's put to bed, the witch hunt was despairingly enthusiastic and patronizing. As I [said at the time](http://www.reddit.com/r/haskell/comments/zxmzv/how_to_exclude_women_from_your_technical/c68oigs) (and as danieldk is saying [now](http://www.reddit.com/r/haskell/comments/17jy0e/my_reaction_to_how_to_exclude_women_from_your/c8679uc)), the detailed, dubiously-based character assassination of someone, even for a good cause, was out of order and that whole reddit post's discussion was kinda scary. Thanks for making a point to clarify this (which is what was argued that you should've done at the time, but understandable that you'd be confused by the sexism charges implicitly leveled at you at the time by all the silent people there who didn't say *anything*, but hopefully will in future!).
Uppercase greek Tau makes no visual difference from a latin uppercase T in many fonts .... Same goes for many cyrillic letters. There are various symbols like "middle dots", "bullets" etc. that all look more or less the same, and are still different.
I have heard about it, but fun to hear that was being used for experiments heh. That is kind of my go-to project when I want to get more experience with a programming language. Simple enough not to be scary/discouraging, but complicated enough that you really meet some challenges as to how you structure your data, how you make it flow between the different parts of the program (network, game logic, display).
Go for it. Try to keep the explanations simple though, I have read a few FRP resources that were not really understandable for people not already familiar with it.
Just noticed you studied in the same college where I am :-) Fun to see another south of France Haskeller!
But [reactive-banana](http://www.haskell.org/haskellwiki/Reactive-banana) can piggyback on any GUI framework, like wx or gtk, with only a minimal amount of glue code. As soon as you have an UI library with a wide scope, you now also have a declarative UI library with a wide scope.
Perhaps you could have a look at my persistence library (http://hackage.haskell.org/package/perdure-0.2.1). It is not that mature, and may place too much emphasis on correctness over performance for your requirements, but it should be relatively easy to use.
Well, to be fair, FRP still is a research topic. In particular, it's not entirely clear what a good set of combinators for GUI abstractions would look like, i.e. topics like [GUI elements with bidirectional data flow](http://apfelmus.nfshost.com/blog/2012/03/29-frp-three-principles-bidirectional-gui.html) and so on.
It depends what you need. For read only, you may be able to acheive some magic with lazy IO. If you need read/write then reads also need to be in IO.
Yes, I totally agree. I think the original accusations were totally over-the-top.
That's not ℕ, that's ℤ, is the right way of abbreviating the integers. If it wasn't a god awful idea to use Unicode in a computer program, that is.
This is an excellent example of why I don't like political correctness: person A says X, person B hears Y, and instead of B asking A if A really meant Y, or explaining to A why what they said is wrong, B attacks A in a public forum (usually in the most visible way possible), hurting their reputation. I'm not going to be surprised if Tim doesn't retract his original post.
I have to say that I'm having a hard time understanding what we're gaining here by making `Subset` and `Quotient` types which fail to encode even the constraint being discussed... Maybe I'm just missing something. 
And if he isn't trying to criticize other people for hyperbole? Hyperbole is a useful and common aspect of the english language. It is not a crime to use it.
Very cool! Can someone test for me if evaluating 2 ^ 999999 in GHCi still gives a segfault in this new release?
I found this study interesting (wish I could find the actually study instead of a news article): http://www.livescience.com/9772-geeks-drive-girls-computer-science.html 
I don't get a segfault doing that in 7.6.1.
The original thread was disturbing, and the toxic and childish nonsense spewed by some of the well known members of the haskell community completely changed my mind about wanting anything to do with the haskell community. And I know I'm not the only one who feels that way. I don't know how well acting like SRS is working towards attracting more women overall, but I know for certain it managed to scare one away.
The original post wasn't an indictment of Swierstra for being sexist, and we aren't a jury deciding his guilt. And really, that post seemed to be as much about the audience's reaction to what they thought he had said ("make the meetings more attractive", full stop, without any "for both"). That reaction was laughter (or silence from people who interpreted the remark as sexist and were troubled by it, but said nothing), and the audience's reputation—that is, the *community's* reputation—can't be redeemed simply by an explanation from the speaker.
A documentary isn't science.
This is my weekend tinkering project from last weekend. It's still very much a work-in-progress, but I thought I'd share the first version to solicit some feedback. Is this something that's worth developing further?
Here's the thing: human communication is based on perception, not intention. Regardless of what you *meant* to say, what you *actually* said was hurtful and dismissive to the women in the audience. When you say that you were "misinterpreted," you're placing the blame on everyone else for hearing what you actually said instead of what you thought you said. The correct thing to say is that *you* miscommunicated. If you really intended to say something benign and only said something hurtful by accident then it should be a pretty simple matter to explain yourself and apologize in earnest for your mistake, but instead you're blaming the people you hurt, essentially, for not being able to read your mind. If you're the speaker then it's your job to express yourself clearly and correctly, not everyone else's to divine what you're actually trying to say.
Except what actually happened is person *meant* to say X, *actually* said Y, which was hurtful and dismissive, and is now blaming everyone else for hearing what was actually said instead of what he meant to say.
No ones blaming anyone here. Doaitse just offered clarification. What's wrong with that?
&gt;what you actually said was hurtful and dismissive to the women in the audience No, it wasn't. You might feel it is hurtful or dismissive to you, but you aren't "women". Women are actually individual beings, each with their own brain. They do not have an automatic "be offended over nothing" trigger that is set off when a social justice warrior decides to start a fight over nothing. The fact that the whole thing was men arguing that women are so offended and oppressed over this demonstrates that they are not in fact concerned with women, but rather with their own self-obsessed need to feel superior and talk down to others.
I wasn't there, so I can't be sure about what happened, but according to Swierstra: &gt; Now the crucial remark, which was garbled on the video, but which definitely ended with "FOR BOTH" That is, in the room, you could hear the "FOR BOTH", but not on the video. Also, if you want to be very strict, what he mean to say was X, he actually said Y (could be interpreted as both X and Z), and some Tim interpreted it as Z. It may be because I'm not a native English speaker, but I would interpret "make the meetings more attractive" to mean "make the meetings nicer", which was what Swierstra meant to say. Also, on the video I definitely heard "FOR BOTH", but it wasn't very clear, just like the rest of the video.
Funny what omitting prepositions does to that title.
No, he meant to say X, and said X. People decided that using the word "attractive" is automatically offensive, and that using some other word like "appealing" would have been totally cool. That is simply not the case. Attractive does not carry any sort of objectification with it, and choosing to deliberately interpret innocent statements as objectionable speaks only of the person interpreting, not the one who made the statement.
&gt; I don't know how well acting like SRS is working towards attracting more women overall But turning this community into a gender issues police state will make it a "safe space"! Nothing says equal like the drama of being fiercely protected.
He's saying that other people misinterpreted what he said, not that he said the wrong thing.
He's saying other people misinterpreted _part_ of what he said, and when quoted with full context, his remarks are much harder to construe in that way.
hahaha
As a native English speaker, 99.9% of the time using the word "attractive" in reference to the other gender has connotations of sexual attractiveness. OP made a mistake, and if it was truly a mistake should accept blame and apologize for it.
You are not alone! In fact neither I am. Not that we are a tremendous number but I go to this meetup, which unlike its name suggest contains some haskellers: http://www.meetup.com/riviera-scala-clojure/ There are also some haskeller in Toulon. Generally they program in Python for food :).
At least the article gives some details as to the methodology and findings of the study. Most science reporting is incredibly light on details.
&gt;99.9% of the time using the word "attractive" in reference to the other gender has connotations of sexual attractiveness. And he didn't do that, so why are you insisting he did something wrong?
But I think it's a big mistake to interpret Tim's post as a "character assassination"; it's just pointing out some of the issues with women's participation in the community. (And let's face it, on this count Haskell does poorly even compared many of the other the overwhelmingly male-dominated open-source language communities.) And I don't see what was so scary about the ensuing discussion; again I think we handled it pretty well all-in-all, and I believe it was a positive thing for the community.
So? It's a thing to be fixed, and can be pretty easily as roche points out. I don't see what the limitations of our tools have to do with arguing that Modula-3 style naming conventions are an inherently bad thing.
Even assuming that attractive was the wrong word to use [for the record, I do not agree with that], that was not a sexist remark. Even in the video, the "FOR BOTH" part was almost as clear as the rest of the sentence. Edit: just realized that you wrote "in reference to the other gender". He didn't say it in reference to the other gender, therefore your comment lacks an argument.
Well, except for the part where there were people present who were very uncomfortable with it, too. This looks like an unfortunate combination of a sensitive subject, an international audience that doesn't share all the same nuances in the meaning of words, and an informal off-the-cuff discussion. Things like this are probably unavoidable in a diverse global community like ours. What's important is that we all make a point of being friendly and welcoming, so that when things like this happen, they are the exception and not the rule. We're doing a lot better than some; so let's just keep it that way.
&gt; person meant to say X, actually said Y, which was hurtful and dismissive Words don't have 'actual' meanings that are distinct from the subjective meanings in people's heads. The vast majority of the time the speaker and the listener agree on what a statement means, but not always. A speaker can have interpretation X of statement S -- a valid interpretation. And a listener can have interpretation Y of statement S -- also valid. Nobody has to be wrong. An apology for saying S, which caused pain, is always in order. But an explanation that X was intended, not Y, is also in order, and in my opinion can allay much of the pain.
I may be biased, but I feel a lot of the post targeted Swierstra.
The reaction was bad, I agree there, but I think it was also important for Doaitse to explain his intentions and to clear up any misunderstandings. I certainly wouldn't want to be thought of as sexist on account of such an unfortunate sequence of events.
&gt;But I think it's a big mistake to interpret Tim's post as a "character assassination" Really? The speaker said nothing wrong, or objectionable, or even questionable. And you feel this was an appropriate response: "exactly the kind of comment that tells women that a space is unsafe for them." "saying that women are valued for how they look, not for what they do" "And because many women see spaces where they are targets for the male gaze as spaces where they will be targets for more than just men's gazes, it's a comment that carries the underlying message that the computer science conference under discussion is not, in fact, a place where a self-protecting woman ought to be." "ones that limit women's choices, careers, and lives." "alienating to any non-heterosexual men" Ok, I'm only half way through and he's already accused the speaker of limiting women's careers and lives, alienating non-heterosexuals, saying women are only valued for how they look, telling women that ICFP is unsafe for them, and implied that women will feel likely to be assaulted because of the speakers statement. The statement I must remind you was "make the meetings more attractive for both". I do not see any way anyone could reasonably interpret that statement to have any of the absurd effects Tim made up, much less all of them. How would you characterize telling the world that someone limits women, alienates homosexuals, and makes spaces unsafe for women if not as character assassination? &gt;And I don't see what was so scary about the ensuing discussion The overwhelming desire to jump on it as though there were some merit to the absolute nonsense complaint? The condescending and patronizing dismissal of anyone who dared to suggest that maybe, just maybe, saying that making meetings attractive actually meant making meetings attractive. Do you really think "shut up, you are a man so you can't talk" is appropriate discussion? Nevermind that it is hypocritically being tossed out *by men* who are being offended on behalf of women who didn't find it offensive. "and oh boy do guys always want to have opinions about gender issues" "Why are men so sensitive?" "You may want to stop mansplaining." "If you think that even one woman needs to be condescended to in such a treacly way about the basic facts of her daily life, you need to have more conversations with actual women." "Yes, it's the school of "make it from one end of the month to the other without being assaulted, intimidated, or made to feel uncomfortable simply for who you are". Be glad you didn't have to go to that school." I mean, seriously? That's the kind of shit that the haskell community takes *pride* in?
Thanks for laying this to rest! I appreciate your candor, your open attitude about this, and your recognition that your remarks gave "ample opportunity for misinterpretation." I was very perturbed at the time that someone whose work I enjoy and respect might have had some views that your remarks, unintentionally, suggested. And now, I'm quite happy to see that this isn't the case! I now only wish others in the Haskell community (as demonstrated on this thread) could take some lessons from your approach, dial down their defensiveness, and demonstrate similar class.
absolutely!
I'm pretty quick off the mark to jump all over people who deliberately or ignorantly exclude others (especially women, in my case) from tech communities, but I really don't think this counts. He has been misinterpreted. Simple. You are totally correct that people can make innocent statements that exclude and hurt - I see it quite a lot, actually. They don't think what they've said is exclusionary, but it is. Something like joking that all programmers worth their salt sport a tell-tale beard. Invitations to industry parties indicating that "girlfriends" are welcome. That sort of thing. Rarely meant as a "girls get out" message, but it acts as one anyway. Those are the things we should call out. Intent *is* important. As someone else said, language isn't code and poetry is a great example of that. Words have different meanings and it *does* matter what he "meant." He may have worded it badly given his role as a speaker whose job is to be heard and understood, but this seems pretty extreme a reaction to his clarification. We have way more important diversity questions to answer than whether someone used the word "attractive" in a non-sexual / aesthetically-pleasing-to-straight-guys way.
regardless of the later discussion, pointing out that someone said something alienating is *not* the same thing as character assassination. The original comment *was* implicitly (hetero)sexist, at least given one interpretation (which was also presumably why people found it funny). Also Tim Chevalier is gender queer. I'm not sure its cool to describe anything as being "hypocritically being tossed out by men". I on the other hand am a heterosexual male who normally feels very comfortable with the Haskell community. But, I just don't think this time there is much of an issue. A good person said something at least potentially alienating. An activist made a stir. We all learned something. Avoid sexualized jokes in tech talks, move on. 
Your library looks fascinating but it says in the intro that it is not suitable for databases &gt; 100MB. Wouldn't that be a deal breaker for OP ? 
This sounds a lot like a project I've had on the back burner for a while. You can try to mmap in the data file as a bytestring, and lazily "parse" it into a Haskell data structure that matches the structure of your data. (Lazy IO really isn't appropriate here, much better to use the OS's "built-in lazy IO".) This is of course assuming your data file format is suitable for the kind of traversals you want to do on your data, and that you know the data file is correctly formatted. The problem you are likely to run into is having that Haskell data structure hang around in memory longer than you want it to. Depending on exactly what data you need to process, this will be more or less difficult to work around. For example if you only need to look up a single string in a trie, the Haskell space usage will be negligible. If you want to look up a whole bunch of strings, you can probably arrange to throw away the Haskell data structures you built after looking up each string. But if you want to process the entire trie in-order, say, you're likely to end up with the whole trie represented as a Haskell data structure simultaneously, which is very bad. What you need to fix this is some additional control over the process of replacing thunks by their values, such as a way to "uncompute" a thunk, or a way to declare that a particular thunk should remain as a thunk even after it is evaluated. There is a [package on hackage](http://hackage.haskell.org/package/ghc-dup-0.1) that is supposed to let you do this kind of thing, but it was not easy to get it working and even when it was working it felt a bit finicky. This general area is one Haskell doesn't have a great answer for, at the moment. (See also: GHC optimizations sharing large values that it would actually be cheaper to recompute than store, and the hoops people jump through to trick GHC into not sharing.)
Cross-posted from - http://www.reddit.com/r/haskell/comments/15h6tz/what_isnt_haskell_good_for/ I may, for good reason, get modded-down for this, but after playing with and studying Haskell for a couple years, I have concluded that Haskell is a language that every programmer should learn, but that very few programmers should seriously use. OCaml, F#, or even Scala would be much more practical in a production environment, IMO. Laziness by default is just the wrong thing for a general-purpose language such as Haskell. Perhaps in a DSL, it would be great. Also, being able to conveniently drop out to imperative style is too necessary, and it's something that Haskell does poorly, IMO. Having to jerk-off a monad (or worse, a stack of them) to achieve imperative styles is too painful. A fun puzzle and an enlightening experience perhaps, but that ought not be confused with language utility. Using row types to infer side-effects, such as with Koka - http://research.microsoft.com/en-us/projects/koka - seem to be a more promising way to contain effects. There are other complaints, but for me, these alone are the deal-breakers.
What? You mean someone might think liftCallCC = (ListT .) . (. ((runListT .) . (. ((ListT .) . (. return))))) is somehow bad practice? It's so *elegant*. You can tell by all the dots.
I've seen work in OCaml addressing this: Out-of-core Functional Programming with Type-based Primitives (2000) Tyng-Ruey Chuang , Shin-Cheng Mu http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.34.8689 It's an area that doesn't seem to have gotten much attention, I suppose because RAM has become so much cheaper in the last decade.
It's terrific, please continue! I like the concept and styling and apparently others do too ( docco/rocco/shocco/pycco/locco/nocco)... shouldn't you call it hocco ? 
I don't get a segfault in 7.4.1.
Are you on a Mac? I am... maybe that matters? Mountain Lion 10.8.2.
Super cool.
And could it just (or additionally) render haddock comments ( {-| -}, --| ) instead of requiring new ones ( {%- -} ) ? (Here's an [attempt at that](http://hledger.org/hocco/))
Sure. I added a new syntax instead of haddock comments because I felt that the kind of documentation I would write to run alongside the code is usually different than what I would write for Haddock comments. But I can see how this would be useful to have as an option. The [github issue tracker](https://github.com/shangaslammi/prose-doc/issues) can be used for any other feature requests that come to mind.
&gt; social justice warrior This lingo is pretty telling about some biases people are bringing to this thread that won't result in a real discussion. Edit: Typo
Which GHC are you using? 32-bit or 64-bit? (I don't even know if you can run the 32-bit on ML, but it's worth checking.) I run 64-bit GHC on 10.8.2.
&gt; How would you characterize telling the world that someone limits women, alienates homosexuals, and makes spaces unsafe for women if not as character assassination? I would characterize it as telling the world that someone limits women, alienates homosexuals, and makes spaces unsafe for women. I would not characterize it as character assassination.
that's very interesting. it won't help me now but it certainly looks enlightening.
I think the issue is that none of those comments are directed at a *person* being sexist, they are all about the way one *comment* could make someone feel. Perhaps the reaction was overblown. But, perhaps it was not. Different people react to comments differently. &gt;So? I might have misinterpreted what you said, but I don't think it is a good idea in this case to deride someone as a hypocritical male if there gender identity is more complicated. Perhaps that was not what you intended. Feeling like people are saying: "Haha, ICFP would be so much more fun if there were only some attractive women!" and getting upset about that is not totally inappropriate in these circumstances. That isn't to say that is how you *should* react, just that it isn't an absurd reaction. And, quite frankly, I want the functional programming community to be an inviting place for everyone regardless of how they react. Functional programming is awesome. Swierstra does remarkably cool work. CS research can be incredibly fulfilling. Most people who have experienced that want to share it. I don't want you *or* Tim to feel alienated by these events.
thank you for your answer. what do you mean with lazy-io backed tree? isn't that what i asked about? unfortunately the "directory"/"data" ratio is pretty big, about 1/3 _is_ the dictionary. that will still be too much to keep in (main) memory.
I can confirm that on 64-bit GHC 7.4.2 on OS X 10.8.2, evaluating 2 ^ 999999 segfaults. I can also confirm that it works fine on 64-bit GHC 7.6.1.
&gt;These are indeed the effect of the words No, they aren't. That is very much the point. If I were to say that your words caused all those things, would you not consider that character assassination? &gt;because it is simply a statement of fact It is no such thing. It is an opinion, one which many people obviously do not agree with. &gt;but I do not believe you can disagree with my logic. Believe.
What, can't it do lazy evaluation and wait until you request enough screens' worth of output? :-P
No more so than the lingo used to launch the attack in the first place. There is no term for "person who gets outraged over non-existent sexism for a living" that will not cause offence to those people. I think social justice warrior is pretty non-judgemental and factual.
Everybody better be putting their types where their mouths are.
64 but GHC on the mac has had consistent problems with unpredictable segfaults although the frequency seems to be reducing with each release. 
&gt; about 1/3 is the dictionary. that will still be too much to keep in (main) memory. May I ask why? Using 6-7GB of memory while constantly working with 20+GB seems reasonable considering that is something like 25-50% of available memory on a current desktop or server system. On the other hand if you aren't constantly working with the data latency of an individual request shouldn't matter so much so why does the data have to be in this very specific data structure?
&gt;I think the issue is that none of those comments are directed at a person being sexist Yes they are. If he were simply discussing the comment, he would have. But he repeatedly refers to the person who made it. He is deliberately associating that person with the imaginary bad things he is pretending his comment creates. Not only that, but he makes accusations about the character of attendees even: "I expected more from the people who attend the Haskell Symposium" &gt;I might have misinterpreted what you said, but I don't think it is a good idea in this case to deride someone as a hypocritical male if there gender identity is more complicated. His gender identity is male is it not? What is complicated about that? Are you suggesting that he is not a "real" man so his opinion doesn't count as a male opinion? And then by extension the other men making patronizing and dismissive comments to defend the non-existent offended women are also not hypocrites because they share the same opinion as someone who is magically immune from hypocrisy? &gt;Feeling like people are saying: "Haha, ICFP would be so much more fun if there were only some attractive women!" and getting upset about that is not totally inappropriate in these circumstances Yes, that is totally inappropriate if what was actually said was "lets get more women here so the conferences are more attractive to everyone". You can feel however you like, but if you are choosing to take offence to an innocent statement, and attack someone who is trying to do the very thing you want, you can't seriously expect nobody to criticize you for it. &gt;I don't want you or Tim to feel alienated by these events. Unfortunately, that doesn't seem possible. Tim will be offended no matter what anyone says, and having influential haskell community members act as they did in response sure as hell scared my wife away.
I don't think those are the only two reasons for metaprogramming. I use Template Haskell because it has access to compiler information that would be normally hard to retrieve, and because my toolchain understands it and invokes it automatically.
&gt;One could believe a character assassination occurred here if one believes the effect of the spoken words was mischaracterized Well thank you for telling me my opinion is valid, your condescension is certainly welcome. &gt;However it is my assessment that the words implied that women are welcome in order to increase the physical appeal of the community And you don't define the english language. The words he spoke do not carry the meaning you wish them to. If I claimed your words are supporting rape culture, would that make it a fact also? &gt;Regretfully, you advanced a well studied logical fallacy. No I did not. You called your opinion fact. I pointed out that it is an opinion. My additional information (that other's disagree with that opinion) does not make the point (that your opinion is not a fact) fallacious. &gt;I prefer not to believe things, but rather to prove them. No belief is required Then don't say "I do not believe you can disagree with my logic". Your logic is borderline absurd, to the point where I am not sure if you are seriously making the arguments you are saying, or if you are being sarcastic and trolling.
&gt;Pointing out that something is contributing to that climate is heroic Not when the something in question isn't contributing to the climate, and is in fact addressing that very climate and trying to help fix it.
I tried posting towards that end in that thread, but I got flamed for it. :-(
Gah I wish we could found a Haskell-powered company somewhere in PACA. How many haskellers are there, generally?
the paper discussing data.io.dup warns against multi threading and unsafePerformIO. that will need much testing :D.
Wow. I am somewhat surprised (but pleased) to see all of this support for Doaitse in this thread. I tried defending him in another thread because I figured that he was just being misunderstood and didn't deserve to be beaten on, but nearly everyone flamed me for my efforts as if I was a closet misogynist.