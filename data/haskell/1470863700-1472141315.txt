Found [slides](http://homepages.inf.ed.ac.uk/wadler/papers/categories/joy-of-code.pdf) on Wadler's page.
&gt; install packages "globally" I am a little hesitant about these aspects, for *my specific use case*. Even an installation for a local user might be quite undesired. Typically we just want to compile and run the binary locally, in its dir. If it were globally installed it would never be certain which version it is, what parameters, outputs etc are set. Such things typically change between projects or even computational runs.
A lot of people are suggesting using `Fix` and making your AST a functor; you can go one step further and use the cofree comonad, which is essentially Fix with an extra tagged value: `data Cofree f a = a :&gt; (f (Cofree f a))` You can use this in conjunction with a lot of functions in Kmett's recursion-schemes package. It's a very useful representation of annotated trees.
Isn't the Linux driver code irrelevant for cloud machine instances, since they all run on top of Xen (or some other hardware abstraction)? I understand that obscure driver code is buggy, but that's because few people use it. The kernel shouldn't load a driver unless the hardware is available, so I can't see how unused drivers can influence the security of the Linux instances I rent.
FYI: If you don't fullscreen the video, slides do change on infoQ page.
Thank you, this is a most interesting recommendation!
[Support coming to spacemacs](https://github.com/syl20bnr/spacemacs/issues/6233)
Awesome! Just bought my plane tickets. I'd love to give a 15-minute talk, as well, about my experiences teaching Haskell at the K-12 level.
That's great! We'll be putting out a call for talk abstracts closer to Hac φ itself, so when that comes around, send us something :-)
I would imagine it's because he's stating the same misinformation about Stack that he's done in the past, after having been corrected on it already. But this whole thread is all about people using downvoting to censor what they don't want others to learn: https://twitter.com/snoyberg/status/763294685091749889?s=09
For what it’s worth, here’s my boring, non-abstract solution. I typically split tokenisation and parsing. During tokenisation, I semi-automatically grab the Parsec source location before and after the token to get a source span: tokenize :: String -&gt; Either ParseError [Located Token] -- Tokens are just a bare enumeration. data Token = Foo | Bar | Baz data Location = Span !SourcePos !SourcePos data Located a = At !Location a -- Adds a source span to a parser. located :: Parser a -&gt; Parser (Located a) located parser = do begin &lt;- Parsec.getPosition result &lt;- parser end &lt;- Parsec.getPosition return $ At (Span begin end) result During parsing, I do basically the same thing, grabbing *token* positions rather than *character* positions. I then propagate locations more or less manually, for the sake of control over error reporting. parse :: [Located Token] -&gt; Either ParseError Term data Term = ThisTerm … !Location … | ThatTerm … !Location … | … It’s simple and not general at all, but it works well and it’s easy to understand. &lt;shrug&gt; 
On the distributive slide, are the products and sums transposed on the left side (and the compositions reversed on the top)? I've been going over it for a while and it doesn't make sense to me but I don't know if I'm screwing something up because I'm unfamiliar with the notation he uses.
You can somewhat 'fake it' by writing a type synonym which composes two injective type families (whose composition is the function you want) - this synonym will be expanded everywhere it appears but you can still use it in the same ways: type family SplitStep (arg :: Maybe ((a,b),([a],[b]))) = (res :: ([a], [b])) | res -&gt; arg where SplitStep 'Nothing = '( '[], '[] ) SplitStep ('Just '( '(a,b) , '( as, bs ))) = '( a ': as, b ': bs ) type family SplitRec (ps :: [(a,b)]) = (res :: Maybe ((a,b),([a],[b]))) | res -&gt; ps where SplitRec '[] = 'Nothing SplitRec ( p ': ps ) = 'Just '(p, Split ps) type Split ps = SplitStep (SplitRec ps) zipT :: forall as bs ps . (Split ps ~ '(as, bs)) =&gt; Proxy ps zipT = Proxy This consists of replacing recursive type family applications in the RHS with a constructor, then writing a second type function which 'decides' how to recurse based on that constructor.
Totally agree
Grabbing the compiler is an optional (opt-in) feature of Stack, but I AFAIK the majority of the Stack team strongly recommends it. We've received many bug reports because of malfunctioning system-installed GHCs (such as missing core packages in the global package database). Using Stack's installed GHCs is more reliable. Regardless: Stack and cabal-install both grab content - the package tarballs and package index - from outside of your distro's repository. So unless you use only Haskell packages supported by your distro, you'll need to trust something else. And if you _do_ try to go that route, you'll likely find it very difficult to pass your code on to anyone using a slightly different machine from you. &gt; It might be due to my very limited experience, but I've not seen such behaviour outside of Haskell. I think it's becoming more common. I first saw it with [RVM](http://rvm.io/), for instance. Everyone's fighting the same problem: distro packages cannot be relied upon to deliver all developers identical versions of tools. Which leads us to: &gt; Do newer versions of the compiler break old source code that badly? If it is the case: (a) Does it happen so often, that it has to be preempted, or is it rare enough that it could be just fixed in the cases where it occurs. (b) Oughtn't distro package maintainers include relevant legacy versions of the compiler (See, eg perl 2 vs 3)? Yes, absolutely. Here's a little taste: https://github.com/fpco/stackage/issues/1476 The Haskell story is worse than other languages, since we have a massively breaking release every 2 years or so. Even if you got your distro to support old versions (such as via Herbert's PPAs), it will still be a much larger chore for other developers to work on your code versus Stack. Compare: 1. Install Stack 2. Run `stack build --install-ghc` versus 1. Use a distribution with support for GHC PPAs 2. Add the PPA repository 3. Install the relevant PPAs (make sure you get the right GHC and cabal-install versions) 4. Modify your PATH to include the relevant directories 5. `cabal update &amp;&amp; cabal install` This doesn't even get into the issues around curated package sets vs dep solving vs cabal freeze files, which will add IME a much bigger wrench into the handoff.
Stack protects you against this. By "global," it means that there is a user-global directory per snapshot where packages are installed and shared among multiple projects. Multiple snapshots can share binary copies of a package, assuming they have identical flags, dependencies, GHC versions, and a few other things. If you modify any of those in your stack.yaml file, the package is instead put into a project-specific directory. A lot of work has gone into making Stack follow a reproducible build plan philosophy. I actually tout that as Stack's greatest strength as a build tool.
I shouldn't do this, but let's break this comment down. &gt; I honestly doubt This reads as "I've never actually bothered to look into Stack, but I'm going to say some things about it anyway." And based on your previous false assertions (like not support multiple GHC bindists for Linux), I think that's actually the case. &gt; I honestly doubt Stack handles fetching all other dependencies, such as C libraries and devel headers This is pedantic: I don't think anyone took the comment that way. But in any event... &gt; unless Stack manages something like a docker environment or invokes the system package manager Actually, Stack _does_ have optional support for both Docker and Nix. &gt; Nor can Stack support all real-world combinations of GHC versions and operating systems Not relevant to the point at hand here, which is being able to pass off some code to coworkers reliably. And no one claimed otherwise (strawman). &gt; Stack is not a magic bullet Strawman. &gt; I do agree that Stack may be good 80/20 solution for OP's situation Subtle jab at Stack not being a _real_ solution, without any real explanation of what downsides will be encountered (besides calling out issues that don't affect the OP). &gt; I do, however, disagree with the apparent trend to recommend to users to ignore/avoid system packaged GHCs which are configured, optimized, and actively maintained (including security fixes) for your actually used Linux distribution. That's a fair position, but this thread is full of reasons why using a distribution-provided GHC is going to cause a lot of pain in this use case, and you've done nothing to address those points. * * * Herbert: I get that you have a horse in this race, and that you want Stack to lose and cabal-install to recapture the hearts and minds of Haskellers. But finding every excuse to make an attack on Stack - or any library that doesn't follow the PVP - is _not_ healthy.
Thanks a lot for the very detailed answer. I very much appreciate it!
Could you show an example how to use it? Maybe even answer this question http://stackoverflow.com/questions/38462563/how-to-work-with-ast-with-cofree-annotation
[Support is in the develop branch already](https://github.com/syl20bnr/spacemacs/issues/6233)
Maybe something like: type Cofree f ann = (ann, f (Cofree f ann)) data Lang a = Number Int | Add a a data LocationInfo = Loc ... type AST = Cofree Lang LocationInfo now a value of type AST would look like: ast1 :: AST ast1 = (Loc(..), Add (Loc(..), Number 5) (Loc(..), Number 7) ) It's an idea of what could be done. I haven't used any parser libraries to get line numbers, so I'm not sure what form they would take. The good thing about the Cofree is exactly patrick_thomson's point that it is like an annotated Fix. Now we can define the base language only in terms of the expressions we want to support, without muddling it with annotations. The solutions above which suggest making annotations/metadata part of the base language bother me. Do you really want to have to support Metadata nodes everywhere you have your AST?
I just want to point out that I really like how Wadler made the point that we usually want to think of the domain of an arrow as a *context*, rather than a type, when talking about a CCC for lambda calculus. For whatever reason, this basic interpretation wasn't made clear when I was learning category theory. (It's not mentioned in Pierce's tutorial and I don't remember if it's made entirely explicit or clearly in Awodey). But it's a perspective that generalizes much better when interpreting parametrized families of types (aka, parametric polymorphism, and various dependent types). 
Is there something similar to recursion-schemes for Cofree? For example if I want to convert ast1 to string can I use something besides recursion?
The configuration is kind of a misnomer. If you set it as the completion backend, it turns on all the other features as well. https://github.com/syl20bnr/spacemacs/blob/develop/layers/%2Blang/haskell/funcs.el#L49 
So why not go all the way and make a dependent arrow a context transformer (morphism in a [contextual](https://ncatlab.org/nlab/show/context)/[syntactic category](https://ncatlab.org/nlab/show/syntactic+category))? IIRC this is how they model it in HoTT. 
Edit: He did flip the sums and products on the left side. ~~He is correct,~~ firstly lets break down the statement on screen. (A x C) + (B x C) ≅ (A + B) x C may be written as the two statements There exists an arrow (or function) from "(A + B) x C" to "(A x C) + (B x C)" There exists an arrow (or function) from "(A x C) + (B x C)" to "(A + B) x C" which we will tackle separately. The first of those two statements is: There exists an arrow (or function) from "(A + B) x C" to "(A x C) + (B x C)" he also gives a function that performs this operation &lt;fst;[cur(inl), cur(inr)], snd&gt;; app Edit: cur never seems to be properly defined (or maybe I missed it), so I don't know if thats actually is the correct function. However to understand this function, you need to watch to the end of the video on exponentials. However there is a more familiar and reasonable way to understand this using haskell code. Perhaps a more familiar syntax for haskellers would be using Either and tuples. Products are tuples, so "A x B" can be represented as "(a,b)" with type "(A,B)". Sums are Eithers, so "A+B" can be represented as "Left a" or "Right b" which have the type of "Either A B". distribute :: (Either A B, C) -&gt; Either (A, C) (B,C) distribute (Left a, c) = Left (a, c) distribute (Right b, c) = Right (b, c) We have the shown the existence of an arrow that takes an "(A + B) x C" to "(A x C) + (B x C)". I called this arrow distribute. Now for the second statement. There exists an arrow (or function) from "(A x C) + (B x C)" to "(A + B) x C" the function to do this he gave was: Edit: This is the corrected version of the function. &lt;[inl;fst, inr;fst], [inl;snd, inr;snd]&gt; Now written in haskell. unDistribute :: Either (A, C) (B, C) -&gt; (Either A B, C) unDistribute (Left (a, c)) = (Left a, c) unDistribute (Right (b, c)) = (Right b, c) Now we have shown the existence of arrow that takes a "(A x C) + (B x C)" to "(A + B) x C". I called this arrow unDistribute. Therefore, we now know (A x C) + (B x C) ≅ (A + B) x C This is useful, because now we know that "(A x C) + (B x C)" and "(A + B) x C" are structurally the same. Likewise in haskell "Either (A,C) (B,C)" and "(Either A B, C)" are structurally the same. 
More work involved. Stack supports things out of the box, like a notion of projects (containing n packages), a means to install GHC and a package (like Intero) automatically of the right version for your project, it gives me the targets available via the command-line; I don't have to link to it and pin my tooling to it (like the `Cabal` library). Associating that with the stack.yaml lets us transparently switch between projects. It even works with docker transparently. Look at the difficulty that ghc-mod has. Nobody can get the thing to work; that's not because the developers are doing a bad job, I tried with [ghc-server](https://github.com/chrisdone/ghc-server) to tackle the same issues and it's not fun. The complexity in tooling is all in the environment, and stack deals with that problem for you. I won't be supporting nix or cabal in the near future. As I wrote elsewhere: &gt; *One more way* of doing all this stuff is *exactly* the kind of thing that makes `haskell-mode` proper complicated and difficult to debug (and fragile to maintain; everything merged into `haskell-mode` lately seems to break something I'm using--there's a motivation behind not depending on anything in `haskell-mode` directly from `intero.el`). 😭 Trying to please everybody at the risk of breaking a smooth user experience on the intended way of doing things leads to configuration, and every time a user has to enter a configuration option to get basic functionality, we've failed as a user experience. ☝️ 
There's also bang patterns, if you've not got ghc 8: https://downloads.haskell.org/~ghc/6.8.3/docs/html/users_guide/bang-patterns.html . It's more work and you need to be careful (though that probably applies in ghc 8 anyway). Also `seq` forces evaluation, if you're using a Haskell that doesn't have bang patterns. Even trickier and more verbose though. 
Let's invert that Q... Why should it support anything other than Stack? What's keeping you from using Stack?
It doesn't seem to have keybindings for the automatic suggestion stuff, however.
It says it's based off ghci, does it keep bytecode in memory? i.e. if I try to load my project with ghci it uses too much memory and I have to use -fobject-code to avoid that. Will that be a problem with intero? 
I'm not even barely a mathematician, but this looks pretty good to my layman sensibilities. It'd be really nice if OP or any other mathematician would weigh in on this. I guess I could make myself feel better by trying to prove this is really a category, but Haskell is big enough that I'm not really sure where to start. 
Intero works and works very well with no gymnastics required for Stack projects. It really is a good companion to the no fuss goals of Stack itself. Great piece of work and I've seen rapid progress on features and stability since I started using it a couple of months ago.
Instead of hiding this in the function, it's usually better to make this decision at the call site, using the `$!` operator
Awesome! I didn't see any mention of type classes. Does this version of Backpack support them?
I love it, I'm fully on board. Meme away friend!
The problem is that you can trigger loading that kernel module as non-root. So it effectively is part of your attack surface.
The Emacs mode sets `-fobject-code` automatically for that reason. 
Well done. I think intero and stack each noticeably improved my forays into Haskell-land. Can you guys make GHC error messages better for your next trick?
Correct me if I'm wrong here, but isn't the fact that Haskell is lazy cause the "not-in-place" thing? Or am I missing something?
Great. Count me in if you have RSS. 
Round trip mostly works, assuming you don't care about field ordering. I noticed two issues: hpack-convert added extra newlines in a multiline description, and it tried to convert a flag description beginning with the text "If ..." into a conditional. These are affected by the yaml text syntax you use, the prefixes | ! &gt; seem to have special meaning. Great tool, thanks yamadapc! I had been procrastinating switching hledger packages fully to hpack for a long time. Now it's done. If you're wondering, the main benefit I get from using package.yaml is less redundancy and fewer errors, eg I can maintain one list of deps rather than updating it for the library but forgetting about the executables etc. Since I build with stack, I never have to run hpack myself. I do have to remember to run stack (or hpack) before committing package.yaml changes, so I can commit the corresponding cabal file changes as well; a small extra chore. 
Can you expand on immutability causing laziness? I've never heard that before, and I can't quite figure out why immutability makes laziness necessary.
Unfortunately it doesn't have an RSS feed. MailChimp has tools for going from RSS to email, but not the other way around. So to do this I'd need to host my own RSS feed and use MailChimp to distribute that as an email. That sounds like a better system anyway, so I'll look into doing that. Thanks for the suggestion! 
I agree that some drivers are part of the attack surface some of the time.
It doesn't. But the reverse is basically true: laziness does strongly push you toward immutability since lazy mutation is a dastardly hard thing to manage.
That's fair enough. I'm in favour of being opinionated when it leads to a better user experince. It's a shame it won't work with my cabal + nix setup though.
I can't find the RSS feed for Hacker Newsletter. Do you have a link? 
Ah, thanks. I somehow got your point backwards.
Isn't this your feed: http://us10.campaign-archive1.com/feed?u=49a6a2e17b12be2c5c4dcb232&amp;id=ffbbbbd930 ? (Found from looking at the source of your "View previous issues"-link.)
Here's a simplification using a small amount of singletons, plus I also fix the perceived bugs, namely that `[()]` should reduce to `Nat` and that we should also simplify inside lists. Note that we reuse `Nat` for enumerating instances instead of defining custom data for that (`GHC.TypeLits.Nat` works too, or probably the nicest one: `Fin n` finite enums). We also ditch `Proxy` in favor of `TypeApplications`. {-# language GADTs, TypeFamilies, RankNTypes, UndecidableInstances, TypeApplications, TypeOperators, DataKinds, ScopedTypeVariables, ConstraintKinds, MultiParamTypeClasses, FlexibleContexts, AllowAmbiguousTypes, FlexibleInstances #-} import Data.Singletons.Prelude import Data.Type.Equality import Data.Type.Bool data Nat = Z | S Nat deriving (Eq, Show) type family DropUnitCase t :: Nat where DropUnitCase (a, b) = Z DropUnitCase [a] = S Z DropUnitCase a = S (S Z) type DropUnit t = DropUnit_ (DropUnitCase t) t type family DropUnit_ (c :: Nat) t where DropUnit_ Z (a, b) = If (DropUnit a == ()) (DropUnit b) (If (DropUnit b == ()) (DropUnit a) (DropUnit a, DropUnit b)) DropUnit_ (S Z) [a] = If (DropUnit a == ()) Nat [DropUnit a] DropUnit_ (S (S Z)) a = a class Simplify' (c :: Nat) a where simplify' :: a -&gt; DropUnit_ c a type Simplify a = Simplify' (DropUnitCase a) a instance (SingI (DropUnit a == ()), SingI (DropUnit b == ()), Simplify a, Simplify b, t ~ (a, b)) =&gt; Simplify' Z t where simplify' (x, y) = case sing @_ @(DropUnit a == ()) of STrue -&gt; simplify y SFalse -&gt; case sing @_ @(DropUnit b == ()) of STrue -&gt; simplify x SFalse -&gt; (simplify x, simplify y) instance (SingI (DropUnit a == ()), Simplify a, t ~ [a]) =&gt; Simplify' (S Z) t where simplify' xs = case sing @_ @(DropUnit a == ()) of STrue -&gt; foldr (const S) Z xs SFalse -&gt; map simplify xs instance Simplify' (S (S Z)) t where simplify' = id simplify :: forall a. Simplify a =&gt; a -&gt; DropUnit a simplify = simplify' @(DropUnitCase a)
Do you plan on updating Lucid to use this technique? This is the first non-QQ HTML library proposal I've actually liked!
Cool ! I remember reading of the proposal a while ago but didn't realize it was already in GHC 8.
This is one of my favorite papers. They approach a very practical problem, and solve it in such an elegant, enlightening way.
I didn't know that. Thank you for the suggestion.
it took me a couple of goes to get through it, but I dont have a CS background and first attempt I think was my first exposure to monads, still managed to get a fair way through then.
Hey guys. There's still a *lot* of work that needs to happen here, but the Motivation, Tour and Cabal file format sections are relatively complete. Feel free to post comments, I'm listening.
Yes it does. I'll probably explain this in more detail in the GHC section.
Since the labels are already sensitive to whitespace (`&amp;#x` isn't the same as `&amp; #x`), I wish you could just do `#lisp-case`. In general, I wish `-` was allowed in identifiers instead of `'`. 
There's also compiler optimizations that evaluate strictness. If you use quicksort as a full consumer, then haskell will automatically compile it as strict.
It certainly feels like a foreign language. 
hvr_: any chance cabal might adopt the yaml format ?
The fix is dumb (but works) and you could not write a paper about it. The basic idea is that even if you typechecked properly against a signature, you might not compile against an implementation if it implements too many type classes. It seems to work ok in practice (for what small practice I have done)
Even with them automatically generated, try composing `#foo . #bar` and see how far you get. =( 
You just have to try and re-think whatever you're working on as processing data through a series of transformations, like a pipeline(possibly with multiple entrances, of course :3), and then you can start designing functions step by step. But mostly think pipeline.
I've been experimenting with Backpack'ifying unix, and I also have my eyes on base. The biggest problem is that GHC's build system would have to be adjusted to know how to handle Backpack, and I don't relish all of the Makefile hacking that would be necessary to make this work. Perhaps when Hadrian merges this will be easier. (Also unix has the additional problem that it needs to work for older versions of GHC which don't support Backpack.) As for when to use a signature as opposed to a typeclass, I see it this way: 1. If you can use a type class, and it seems to work well, use a type class 2. If you have an associated type, and the set of operations it needs to support is completely well known, use a type class. If there might be arbitrarily many operations and you don't know what they are a priori, or perhaps even if there are a lot of operations which you don't want to put in the type class, use a signature 3. If you have several, interrelated associated types, use a signature 4. If you need to make sure use of the typeclass always gets inlined into direct code, use a signature 5. If you want to morally add a type class constraint to an "entire" module or package, use a signature These are not rules set in stone and I hope as people start using the feature we will get a better picture when Backpack is appropriate.
There are some functions that you would almost never want to be lazy, though. For instance, foldl.
The pipeline concept was crucial for functional style to click with me. Now I get frustrated when a language makes constructing such structures cumbersome. It's interesting to note that thinking in this way will also help when working with libraries like Tensorflow!
That would make sense if ignoreMe was a function, but it isn't. It's just an IO () not an () -&gt; IO (). Unless IO a is really an alias for Foo -&gt; a, where Foo is some private unexported type that you can only get by being Main.main.
Any type can be considered a function from n units to that type. `IO () == () -&gt; IO () == () -&gt; () -&gt; () -&gt; IO ()` etc.
You're too optimistic. :)
It's pretty good for putting configuration or e.g. yesod url renderers in the reader monad. Passing that around as arguments in big templates gets old.
Thank you for this summary. I suppose I'll end up with the open variation after all. I remember having read that paper of Swierstra's some time ago and Benjamin Hodgson's idea with the functor product above has actually reminded me of it.
I tend to have "state" in my solutions in the form of let-statements that hold intermediate values, such that my code gets broken up into a step-by-step procedural style. I find I can mock up code in Haskell quite fast this way, and it's easy to move things around when revising it for readability and performance reasons. I hardly ever have any kind of mutation in my code though, I only turn to it for performance purposes in cases where such code will be pretty self-contained and not bleed into the overall logic of the program. Mutability is just something I don't reach for anymore when writing out simple program logic. You definitely have to get used to it first though. When dealing with hard problems in Haskell, where I really need to explore the space around the problem first before implementing an actual solution, I tend to opt for a declarative style where I start by describing simple relationships my input data has and then start building up from there towards more high-level non-obvious information until I realize that I now have the vocabulary to describe the relationship between my input data and my intended output data - at that point, things start falling into place and it then becomes an issue of cleanup, testing, and optimization from that point on. I also keep a notebook and pencil by me and sketch out stuff on paper with diagrams and pictures before committing to actual code. I have a much easier time realizing how to break problems down if I can understand my data spatially. 
You are discharging on the result type Html in `[x] -&gt; Html` with a little help from the [] around x. If I build an instance for a field accessor, then that will typically look like `Foo -&gt; x` where x is determined by Foo (unless it is a polymorphic field). If someone later comes along with #head meaning f a -&gt; b with a ~ b, for Foldable f, then you'd run into it. And of course OverloadedLists would break the dispatch on the `[]`. These may not be too bad for your usecase, though I suspect in a couple of years you may look back at the code as a fit of madness akin to when folks thought linear implicit parameters were the future. ;) 
I tried using the overloaded labels feature of haskell-gi in Leksah port from Gtk2Hs to haskell-gi. I am no longer a fan of overloaded labels. I found they: * Result in confusing error messages * Don't work well with type inference * [Make for very slow compile times](https://github.com/leksah/leksah/issues/303) In the end I used the haskell-gi `overloaded-*` flags to turn support for overloaded labels off. 
If you can consume Acedemic papers go with * [Even Higher-Order Functions](http://www.westpoint.edu/eecs/SiteAssets/SitePages/Faculty%20Publication%20Documents/Okasaki/jfp98sixth.pdf) * and [Parsec](http://research.microsoft.com/en-us/um/people/daan/download/parsec/parsec.pdf) Then, dig through the source code of [Attoparsec](https://hackage.haskell.org/package/attoparsec).
The key thing to learn (IMHO) is how to translate things you used to do in imperative languages to pure functions. So I would recommend figuring out (by yourself or by asking) how to translate (say) state changes to Haskell (State monad is one way for example). A key pattern I use a lot is to split an algorithm into a decision part and an action (IO) part. The IO part usually ends up being tiny.
Not sure if this is what you mean, but newtype is a zero cost abstraction. The constructors and accessors disappear at runtime, so you can distinguish values of otherwise equal types. Edit : derp, confused zero cost abstraction with zero cost futures, never mind. 
I was a developer for more than 3 years before I started with Haskell and it definitely took me more than a year before I stopped finding it really difficult. It took a long time to learn to program competently in the imperative style and functional is radically different. I don't know how comforting this is, but you're probably no dumber than I am! If it helps at all, I've found the mindset for functional programming is actually really close to the mindset for writing regexps. You think about how your data flows through the function and gets transformed rather than step-by-step shopping list style.
Nowhere, but I am able to make one. Pretty much every imperative language can define qsort on 7 lines. 
Using a garbage collector makes it very hard to really get zero cost abstractions. We get a few things like newtypes, but not nearly as much as Rust gets. Granted, the garbage collector is pretty good and gives us very *low* cost allocations. But memory in Haskell will likely never be as efficient as Rust. Thus, most abstractions are going to cost something in terms of memory and allocation performance. I've actually been thinking recently about what it would take to build a functional language similar to Haskell with an ownership system like Rust's. It might be even better in an pure language, since the memory semantics can be even more predictable. Furthermore, I wonder if purity could help infer ownership semantics, and insert them into a Haskell program during code generation where possible to bypass the garbage collector. Could enable a lot more "zero cost" abstractions by way of optimization.
Has Intero killed https://github.com/haskell/haskell-ide-engine ? Activity was already low, and Intero coming along seems to have given it the deathblow. Or is there still hope? 
Unlike Rust, Haskell stuck with green threads for its async IO solution, so we do have overhead there in the form of a heap allocated closure ([TSO](https://ghc.haskell.org/trac/ghc/wiki/Commentary/Rts/Storage/HeapObjects#ThreadStateObjects)) that holds the state of the green thread. So if you're planning on using GHC's off-the-shelf solution for async IO, then its not a zero-cost. You could still do it in Haskell, but you'd probably have to descend into FFI and Storable as to dodge the GC or find some clever way of convincing GHC to fuse out a bunch of intermediary allocations. It could be done, I just don't think the Haskell ecosystem has anything ready-made yet. The Zero-cost futures post for Rust reminded me of [this as well](http://blog.ezyang.com/2014/01/so-you-want-to-add-a-new-concurrency-primitive-to-ghc/).
Yeah... it appears that Stack is starting to fragment the tooling. Some tooling supports both, some supports only Stack, and some supports only Cabal. Now you have to actively choose which camp you're in *sigh*
In my experience, the problem is usually that I don't understand the problem well enough to formulate a concise model of it. Sometimes it helps to flesh out something ugly that just works, in order to get a better understanding of how to model it properly. In Haskell, and this is important I believe, you primarily need to be concerned about the types you use to encode your data. If you've got proper data structure for your data, your functions become much easier to implement. And in addition, the compiler can tell you if you've made a mistake.
You mean something like https://en.wikipedia.org/wiki/Clean_(programming_language)?
&gt; How does that play with lens? They don't play together. Lenses as defined by the `lens` package don't work out well with `OverloadedLabels` because a `Lens` is a type-class polymorphic "thing" which needs the `view` and `set` methods to determine something concrete, which makes it hard to figure out the `IsLabel` instance for `#foo . #bar`. I believe solving this is one of the things stalling the completion of the extensible records work in GHC.
At least in GHC-compiled code, if you use `forkIO` and just do IO they will become event-based automatically. There has been an discussion earlier here: https://www.reddit.com/r/haskell/comments/4vz3gs/polling_on_sockets/
Have you tried it? Let's try the journey to get this innocuous line to typecheck: baz = #foo . #bar Assuming we don't have Control.Category in scope to muddle things further, if we simply turn on `{-# LANGUAGE OverloadedLabels, AllowAmbiguousTypes, NoMonomorphismRestriction, FlexibleContexts #-}` that line will infer a type involving a `b` that appears nowhere on the right of the `=&gt;`. baz :: (IsLabel "foo" (b -&gt; c), IsLabel "bar" (a -&gt; b)) =&gt; a -&gt; c Now, my challenge to you is to write down that type so that you can check _that_ type, not just infer it, with that body unchanged. You can't write down the principal type of that term and get it to type check with that type. GADTs destroy principal types, too, but at least there the issue is that the most general type doesn't necessarily exist. Here it exists, but it can't be written it down and checked. You get an ambiguous type for the intermediate result type, even with `AllowAmbiguousTypes` at the most general type possible for that composition. If you pass it that signature it has no reason to believe your `b` and the one it generated are the same `b`. You can of course check more specific types: data Foo = Foo { foo :: Int } data Bar = Bar { bar :: Foo } instance (x ~ Int) =&gt; IsLabel "foo" (Foo -&gt; x) where fromLabel _ = foo instance (x ~ Foo) =&gt; IsLabel "bar" (Bar -&gt; x) where fromLabel _ = bar baz :: Bar -&gt; Int But isn't the point of having functions being able to compose them and reason about the composition? There that particular `#foo :: b -&gt; c` has `b` determining `c` and `#bar :: a -&gt; b` has `a` determining `b` at the particular types instantiated at. This was the issue I complained about in the initial thread on the topic where Simon proposed this class, before we went and added them anyways. The most general type can't be written. I'm not very comfortable to see labels in Haskell relegated to the same murky "you can't actually give the type in the language" situation that the temporary "functors" types you get for generic lambdas in C++ or the "we'll accept a particular type if you ask nicely enough" lambdas from C# find themselves in. But "wait", you say! I'm not being fair, because I didn't let you modify the line and you can't name the intermediate object! Maybe something like `#foo @something . #bar` would let me talk about the intermediate name? But you can't name it here. `#foo @whatever` won't work. `#foo` is an expression not a statement due to how it desugars, so you can't use type application in it. So even intrusively it seems that there is literally no way to check the most general type that comes out of that expression without abandoning the `#` syntax entirely.
I used to put imports into separate groups, then at some point I stopped bothering. What benefit does it really bring?
FWIW currently the stack/nix integration is not without hurdles https://github.com/commercialhaskell/stack/issues/2130
My understanding is that `intero` replaces `ghc-mod`; `haskell-mode` provides a set of more basic features (indentation, syntax highlighting, ...)
I have a similar background from Elixir, Elixir more functional than Ruby and emphasize immutability like Haskell. In database library, I think it solve in Elixir very well by use Ecto(DDL and DML), but in Haskell I think it couldn't solve it easily because Haskell hope to solve it type safely, the abstraction of DB related things should bring many metadata(like association, keys, default value, field type, field size), and Haskell hard to encode these in the DSL; in DML type-safe query synthesis is really hard!
I don't believe type-safety is causing the problem here. My hypothesis is that it's one of two things: 1. There aren't as many people write typical RDBMS-backed webapps in Haskell, as other tech-stacks, 2. And/or, the general assumption in the community that "ORMs are bad" 
You are right in the ecosystem problem, some attitude about ORM you can see in this [thread](https://redd.it/4vh4sg). And someone use other architecture (like: [Event Sourcing](http://abailly.github.io/posts/cm-arch-design.html)) to avoid RDBMS problem in Haskell.
I seriously want to learn the correct way of doing these things in Haskell. Can you help?
Really good article!
Lenses **can** be the solution. But working with them is harder than it ought to be unless the lens is baked right into Persistent. user &lt;- get uid user ^. key -- should work out of the box user ^. name -- should work of the box
Also, this is Haskell. You don’t have to constantly pattern-match any constructor; do it once and write an accessor/combinator.
Downvoting because OP has actually put in good-faith effort, brings valuable experience, and is generally quite open to change.
What would be the smallest code to get something like https://www.reddit.com/r/haskell/comments/4xceqz/why_building_webapps_in_haskell_is_harder_than_it/d6ee7ib to work?
While this doesn't begin to address some of your bigger concerns, note that the following *does* work: Entity _ user &lt;- get uid let y = user ^. userName ... There's a [setting](https://www.stackage.org/haddock/lts-6.11/persistent-template-2.1.8.1/Database-Persist-TH.html#v:mpsGenerateLenses) for generating lenses, to be used like share [mkPersist sqlSettings{ mpsGenerateLenses = True} ,mkMigrate "migrateStuff"] [persistLowerCase| User name String |]
ORMs *are* bad.
But that's exactly what I would do. I would implement an ORM-like-thing specific to my domain.
Really interesting post for me as I have a similar background. I have done few years of Rails and want to run away from dynamic typing. I am building a simple haskell CRUD application for fun and facing the same issues with the [mis]use of an Haskell ORM. I have never realized how great Active Record was before coding in Haskell. [edit] thanks for the link *recommended_way_of_dealing_with_db_associations*, that's interesting :-)
A few of the variables that I find are usually involved: * What is a primary key (string/int/guid/multiple columns?), are they consistent, does every table have one? * Is the DB normalized? * Are deletes actually deletes, or are they updates? Are updates actually updates, or are they inserts? Is it consistent? * How are transactions handled? And things like output parameters on update/insert? What do you mean with regard to eager loading?
&gt; 3. How to deal with nested records/tuples? The article did not include [Groundhog](http://hackage.haskell.org/package/groundhog) in it's list of DB libraries. Groundhog does have support for nested records. See https://github.com/lykahb/groundhog/blob/master/examples/embedded.hs. &gt; 5. How many different names can one come-up with for the same thing? This is never a problem for me. All names everywhere in the world should have uniqueness that is proportional to their scope. Record names in Haskell have global scope, and therefore they should be very unique. This is an example of something you can't map from the Ruby/OO world. You can't think of Haskell field names the same way you think of field names in OO languages because their scopes are very different. The "problem" mentioned by the article with the status field, Download, and DownloadFilter is trivially solvable by using the name downloadFilterStatus. A number of the OP's points revolve around the DB. This I think is a valid criticism. But it's also a pretty hard problem. I think Haskell is still missing some DB pieces that would really improve the situation. One that comes to mind immediately is roughly "esqueleto for groundhog". There are times when you want an ORM (groundhog does a pretty good job here), and there are times when you want a relational algebra library so you can express more complicated queries like joins, etc within the typed framework established by the ORM. There is more work to be done, but I don't think it's quite as bad as the OP makes it sound.
It gets stored as one or more fields within the record. It also gives you some degree of control over the naming of those fields.
&gt; "You can of course check more specific types." &gt; baz :: Lens.Lens' Business String That signature isn't the most general signature for `#person . #name`. I just want to typecheck `personName`, not the specific thing for Business and String, just whatever gets #person . #name. `id . id` works. I can give it the most general possible type `a -&gt; a` and typecheck the expression without losing anything. I don't have to turn it into `Int -&gt; Int` first. The composition gives a type you can't check in its full generality.
Would love to see your code. If not your code, your DB schema. If not your DB schema, then just your UI mockups.
Your example is a textbook case of Wadler and Blott. The implicit configurations paper on page 11 or so follows their reasoning that giving that function the `Eq a =&gt; a -&gt; a -&gt; Bool` type defeats the very point of having the local instances, as they can't be used to discharge an obligation that was originally incurred while they were in scope. Once you have a constraint that isn't discharged 'at the boundary of the let' you leak information about when it was discharged. Pragmatically: Move your example to `Ord`, say with the Ord Int in descending order. Now inside 'in' make a Set Int with `fromList` or a bunch of inserts. Outside of in `lookup` values in it and fail to find almost all of them. The API of `Data.Set` is safe today. It is unsafe in a world with your extension added. On the other hand, let's say you used reflection to build two such instances in local scope. newtype C a s = C { runC :: a { instance Reifies s (a -&gt; a -&gt; Ordering) =&gt; Ord (C a s) where compare x y = reflect x (runC x) (runC y) descending :: Int -&gt; Int -&gt; Bool descending = flip compare Now when you do the moral equivalent of that let: reify descending $ \(Proxy :: Proxy s) -&gt; reify ((==) :: Bool -&gt; Bool -&gt; Bool) :: $ \(Proxy :: Proxy t) -&gt; ... inside the ... you can refer to `C Int s`, and `C Bool t`, but the types s and t aren't available outside of the block for you to run into principal type problems with. You can't observe the infelicity pointed out by Wadler and Block. If the user captures the constraint by using an existential so that they can open it up later, that act of opening is the thing that lacks a principal type, as it is a GADT operation, but even with that you can't compromise the safety of any existing API.
But isn't that a coherence problem?
Wadler and Blott include the chosen instance in their typing judgments, so it is, ultimately, both things! In their notation: A1 |- e1 :: Eq Int \ eqInt A1 |- e1 :: Eq Bool \ eqBool are both types for e1 in that context. They took the Hindley-Milner type system, showed that if you added typeclass constraints to it, that you could plumb them through the typechecker in that form without impacting the existence of principle types at least so long as their over and inst rules had global scope. No properties of the type system were lost. Putting on signatures could only make fewer programs type check. At no point in time does a signature actually change the meaning of the term. That result is a bedrock stable thing to build a language on. Here you're saying, well, let's not do that, but rather say that type checking is some weaker bidirectional thing and we'll furthermore delegate the thing on the right of the \ to something nebulous we'll call coherence. But the chosen instance is part of the semantics and you have to talk about it somehow.
For what it's worth, I'm an IRC channel at the moment where the OP is asking some questions, and from what I'm seeing they're serious about learning and seem keen to listen to folks. Is it possible that's something that the problem is happening somewhere between what the OP is writing and what you are reading in reddit posts? Maybe it's the medium rather than the message.
If anyone is interested in sharing an AirBnB in the area, let me know! I'm from the Philly area, but will be heading in from Boston as well if you're flying in from there.
If by fragmenting you mean it pulls ahead, solving decades old issues and leaving behind barely supported, broken tools, then yes, i guess it does fragment the tooling. But i'd say, good riddance. We need better tools. 
That's indeed a legit point of view! The funny thing is how the perception changes, depending on which camp "pulls ahead". It really just depends on what side of the fence you're sitting. That's open source communities for you...
Or: "Why Yesod is Icky"
Thank you for your offering to help. I'm sure sensible setup/teardown code for testing a DB can be written in Haskell/hspec. But that's what makes development in Haskell harder. Basic stuff that you're used to in other tech-stacks does not exist out-of-the-box. I really want to use Haskell. I haven't yet completed my POC app. I'm struggling now with reflex-dom and GHCJS. I will take a call once I have something up &amp; running. Don't want to leave it midway.
I'm glad you told me what or why you click arrows!
&gt; Basic stuff that you're used to in other tech-stacks does not exist out-of-the-box. So please write it and put it in a box for us :)
I read this and I see 2 things: 1. You've ignored everything everyone said on the previous discussions you've started here on Reddit 2. You're unwilling to move away from patterns you've learned from other languages/frameworks # Regarding nested records Last time you asked about this, multiple people told you that you should be using tuples rather than nested records. This is an antipattern you're trying to bring in from an OO language and has never made sense when dealing with the database. The fact that you're unwilling to stop clinging to this idea makes me believe you're only interested in learning how to bring your Rubyisms to Haskell. In my opinion, nested records reduces reusability and composability. # Regarding "validations" You provide absolutely no information about any real problem you've experienced. # Regarding templates So don't use Shakesperean templates? The way you complain about it implies that this is the only option available to you when it's not. It's not even the only templating library available to you for Yesod. If you genuinely wanted, you could plug in whatever you'd like. Heist is an XHTML-based template language that any editor that supports HTML/XML can handle no problem *and* it offers sensible error messages. # Regarding ORMs Some of your points are either misguided or show your ignorance about databases in general. The only thing you point to as a positive for ORMs is that you don't have to write boilerplate, but you never explain what boilerplate you have a problem with (I'm just going to assume you mean "I want it to write queries for me"). &gt; Implementing limit/offset pagination sensibly without writing any boilerplate If your idea of "sensible pagination" uses `OFFSET`, this is wrong. In the databases I'm familiar with, this is a slow operation the more rows you're trying to skip over because the database has to go through everything that precedes the offset in order to determine where to start. This is not a big deal when you have only a few thousand rows, but you'll notice it when you start looking at millions of rows. &gt; Updating only those columns of a table that have changed (to reduce the chatter with the database) without writing any boilerplate code I have no idea why you're even mentioning this. There's no difference between updating 3 columns vs 8 columns in the database in cases where only 3 columns have changed (unless you're doing something dumb like updating one column per query). Even if you have foreign keys set to cascade on update, setting those columns to the same value does not cause it to cascade to other tables (at least not in Postgres). &gt; Maintaining createdAt/updatedAt timestamps for each object/row without writing any boilerplate code If your data needs to have specific constraints for some sort of business rule, the database needs to be doing the enforcing, not your code. It has **never** been safe to assume that your application is the only gateway to the database. CREATE OR REPLACE FUNCTION record_last_updated() RETURNS TRIGGER AS $$ BEGIN NEW.date_updated := now(); RETURN NEW; END; $$ LANGUAGE 'plpgsql'; --------------------------------------------------------------------- -- table foo CREATE TABLE foo ( foo TEXT NOT NULL PRIMARY KEY, date_added TIMESTAMPTZ NOT NULL DEFAULT NOW(), date_updated TIMESTAMPTZ NOT NULL DEFAULT NOW() ); CREATE TRIGGER record_last_updated BEFORE UPDATE ON foo FOR EACH ROW EXECUTE PROCEDURE record_last_updated(); -- table bar CREATE TABLE bar ( bar TEXT NOT NULL PRIMARY KEY, date_added TIMESTAMPTZ NOT NULL DEFAULT NOW(), date_updated TIMESTAMPTZ NOT NULL DEFAULT NOW() ); CREATE TRIGGER record_last_updated BEFORE UPDATE ON bar FOR EACH ROW EXECUTE PROCEDURE record_last_updated(); --------------------------------------------------------------------- -- insert/update foo INSERT INTO foo (foo) VALUES ('one'), ('two') ; UPDATE foo SET foo = foo ; -- insert/update bar INSERT INTO bar (bar) VALUES ('one'), ('two') ; UPDATE bar SET bar = bar ; Oh look, no boilerplate. On top of that, it is reusable for every single table you would ever write, as long as they have the same naming conventions for the columns.
I really, really recommend you give up on overloaded field names. You're never going to find anything as satisfying as just giving field names a unique prefix.
I think that is a major part of the friction that I'm facing. Why do you say that nested records are a bad idea in an immutable world? Is there some wiki/blog-post explaining this position? I tried going with the nested tuples approach, but found the approach "brittle". This was because I was depending upon the position and the nesting-depth of the element in the tuple. Wouldn't the code to access `File` in the first tuple be different from the second tuple? So, if I'm writing a bunch of HTTP handlers that deal with Downloads &amp; Files, and somewhere I realize that I also need to display Urls, and change my tuple to the second one, I have to change every single call-site where I'm dealing with a File? [(Entity Download, [Entity File])] [(Entity Download, [(Entity File, [Entity Url])])] "Conceptually" my data's relationship hasn't changed. But, just because of the way I'm representing it in Haskell, I have to go around changing all my call-sites. I don't believe this is a problem with nested records. What am I missing?
Ah, finally! I am not the only crazy one! Someone else also felt the need for the three states that I was talking about at https://groups.google.com/d/msg/yesodweb/sLZ53JzIjVQ/1Gb5aypiBgAJ
&gt; In every post he has made in all 3 of these threads. I'm finding it hard to see that ... &gt; But he explicitly mentions opaleye's approach as not good enough. Where?
Please read tom's comment at https://www.reddit.com/r/haskell/comments/4xceqz/why_building_webapps_in_haskell_is_harder_than_it/d6etq9f where he explains how Opaleye models this stuff in Haskell without losing type-safety.
&gt; but conveniently Exactly. Years of programming in Ruby have spoilt me. There is such a thing as "developer happiness" to optimize for :) Btw, I now realise that there can be two possible solutions: * https://www.reddit.com/r/haskell/comments/4xceqz/why_building_webapps_in_haskell_is_harder_than_it/d6etq9f * https://www.reddit.com/r/haskell/comments/4xceqz/why_building_webapps_in_haskell_is_harder_than_it/d6ehex7
If he didn't get it two weeks ago then perhaps we can learn to explain better.
&gt; How are nested-tuples a better solution than nested records? I think /u/_cimmanon actually means you should use *flat* tuples. Actually what we really need (and Ruby needs, and Python, and Java) is a *relation* datatype. That would probably give the right interface for dealing with all this stuff.
&gt; Regarding templates &gt; So don't use Shakesperean templates? The way you complain about it implies that this is the only option available to you when it's not. It's not even the only templating library available to you for Yesod. If you genuinely wanted, you could plug in whatever you'd like. &gt; Heist is an XHTML-based template language that any editor that supports HTML/XML can handle no problem and it offers sensible error messages. You're probably right. And I am in the process of ditching Shakesperean templates in favour of what I want to eventually build by UI in -- AngularJs, ReactJs, or Reflex DOM (still waiting for GHCJS to finish compiling...) But that doesn't mean that pointing out the problems in Shakesperean templates is a bad thing. It might save someone else the time from making the same mistake. However, isn't this a telling example of the "too much choice" problem, that I felt with the DB library decision?
They all are foreign keys. Direct means that the field is directly the record rather than its `Key`.
Indeed. I think the main problem here is that we're trying to squeeze an approach from a language without a good type system into a language with an excellent type system. Just describing the actual problem using only types usually makes everything a lot clearer to me. I'm tempted to write special data types just for the standard "key value" `[(a,b)]` type, because I think it makes the code really hard to read. I can't even bear the thought of nesting them.
Why is SQL seen as boilerplate? Why is mapping SQL data to types seen as boilerplate? In my experience an ORM divorces programmers from the actual representation. Then you get this strange friction where one representation is always being compromised and chasing the other. Either your records model your tables or your tables model your records. Neither is really ideal since each domain has specific constraints and strengths that should be fully exploited. I'd rather buy the "boilerplate" of writing SQL and mappings instead of buying the complexity that an ORMs impedance mismatch makes my application pay.
I'm genuinely trying to understand this "impedance mismatch" that people keep talking about. Say, you have the following relationship between your tables: User has-many Posts has-many Comments Post has-many Comment belongs-to User Comment belongs-to Post belongs-to User And you have to **efficiently** fetch a User with all his Posts and each Posts's recent-most 5 comments. With, or without an ORM, you would end up writing SQL on the following lines: select u.*, p.*, c2.* from users u left join posts p where u.id=p.user_id left join (select c1.* from comments c1 where c1.post_id=p.id order by c1.created_at desc limit 5) c2 Now, if you had to display a web-page with the user's profile information on top, all the posts along with the recent-most 5 comments, how would you represent the results of this SQL query in Haskell (or any other language)? 
/u/tomejaguar &amp; /u/runeks -- what would your types look-like in the case given at https://www.reddit.com/r/haskell/comments/4xceqz/why_building_webapps_in_haskell_is_harder_than_it/d6ewhz8 ?
Well I'm mostly joking. The article points out that the alternative requires knowing enough to piece together your own system. I'm quite fond of Servant/Raw Wai + Hasql/Raw LibPQ (to expose a Streaming interface) But for simple web services without custom backend logic it's overkill
&gt; I guess I'm just making a very loud feature request You'll probably find that in the Haskell world things are much more extensible than in the Ruby world. You don't need to wait for other people to add things for you. Take your `createdAt` and `updatedAt` as examples. You can write: data Timestamp' a b = Timestamp { createdAt :: a, updatedAt :: b } type TimestampColumns = Timestamp (Column PGTimestamptz) (Column PGTimestamptz) type Timestamp = Timestamp Time.Timestamp Time.Timestamp $(makeAdaptorAndInstance "pTimestamp" ''Timestamp) timestampColumns :: TableColumns a TimestampColumns timestampColumns = lmap (const ()) (pTimestamp (Timestamp optional optional)) and then to add timestamp fields to *any* table you just use `timestampColumns` appropriately fooTable :: Table (Column Bar, Column Baz, Column Quux) ((Column Bar, Column Baz, Column Quux), TimestampColumns) fooTable = Table "fooTable" ((,) &lt;$&gt; (pFoo Foo { bar = required "bar" , baz = required "baz" , quux = required "quux" }) &lt;*&gt; timestampColumns) That is to say, all you need to do to *any* Opaleye table to add timestamp columns into it is to combine it appropriately with `timestampColumns`. And that is the beauty of Haskell. Why doesn't this exist already? I guess no one wanted it before, or at least no one wanted it and articulated it like you have. They wouldn't need to ask me to add it. They could have written it themselves.
&gt; If you think it would be valuable to see the kinds of struggles someone like me has when considering Opaleye, I would be happy to take a few hours and try to give it a fair shake and then write up some notes and send them to you. That would be really helpful! Would you mind waiting for me to improve the documentation and the onboarding process? Then your feedback about further improvements would be really useful.
Using associated type families: class ToTuples a where type ToTuplesResult a :: * toTuples :: a -&gt; ToTuplesResult a instance ToTuples () where type ToTuplesResult () = () toTuples _ = () instance ToTuples xs =&gt; ToTuples (Columns x xs) where type ToTuplesResult (Columns x xs) = (Tagged x String, ToTuplesResult xs) toTuples (Column x xs) = (x, xs)
I read this post which seemed to be arguing that something like Hibernate was actually a good thing. For any non-trivial application my experience has been that ORMs in general create more problems than they solve. When you have relationships between tables and they are not bounded there will eventually be pathological cases and manual SQL will be written with paging and partial materialization. I consider a key piece of hibernate to be an antipattern where ever you find it which is lazy io. If I have a hibernate object User and a call User.getPosts() it now goes back to the database to get the data, allowing us to pretend that something is pure when it really isn't. Once you realize you have to control the materialization of DB data yourself then most of the perceived benefits of an ORM are discarded. In my experience you can never get away with not thinking about the physical representation and actual DB query plans and making a DB independent application is usually solving for the wrong thing.
There isn't some giant impedance mismatch here because the types are simple and there is an easy mapping between these record representations and the table itself. Mismatch happens when your types become more complex and the invariants you are attempting to enforce cannot be naively represented in the way an ORM would often wish to do. Algebraic data types are a common case where I have yet to see a database representation that I'd use in all cases. There are many representations that I might use given my type and circumstance and the invariants it needs to hold. Likewise, I'm not sure I'd write this query in this way. I might, this could be the most efficient way to handle it or multiple queries might be better. This is dependent on your data set and application. There really is no one size fits all query pattern which ORMs fall victim to. As for the Haskell representation, I agree with /u/tomjejaguar
I'd split it, but leave the separators (since I'm operating on them), then zip with `cycle [id,id,id,const "???"]`, and lastly join.
I think doing it more helps. And it can be in relatively small increments. So, jump on HackerRank or CodinGame or some site like that and solving some of the problem in Haskell with minimal use of IO/ST.
Are you sure that is the most efficient query? In my experience, outer joining to an outer joined table tends to have a less efficient query plan than the same query that uses inner joins. If this is the case, it might be better to query the user information separate from the post+comments. Two inner joins: mpq=&gt; explain select * from users join rosters on user_id = id join roster_latest_revisions using (account) limit 10; QUERY PLAN ------------------------------------------------------------------------------------------------ Limit (cost=1603.84..1611.86 rows=10 width=137) -&gt; Nested Loop (cost=1603.84..3478.59 rows=2337 width=137) -&gt; Nested Loop (cost=1603.56..2506.49 rows=2337 width=26) -&gt; HashAggregate (cost=1603.28..1626.65 rows=2337 width=13) Group Key: roster_revisions.account -&gt; Seq Scan on roster_revisions (cost=0.00..1231.52 rows=74352 width=13) -&gt; Index Scan using rosters_pkey on rosters (cost=0.28..0.36 rows=1 width=22) Index Cond: (account = roster_revisions.account) -&gt; Index Scan using users_pkey on users (cost=0.28..0.41 rows=1 width=111) Index Cond: (id = rosters.user_id) Two outer joins: mpq=&gt; explain select * from users left join rosters on user_id = id left join roster_latest_revisions using (account) limit 10; QUERY PLAN ------------------------------------------------------------------------------------------------------ Limit (cost=1809.41..1809.92 rows=10 width=137) -&gt; Hash Left Join (cost=1809.41..1999.77 rows=3759 width=137) Hash Cond: (users.id = rosters.user_id) -&gt; Seq Scan on users (cost=0.00..100.59 rows=3759 width=111) -&gt; Hash (cost=1773.55..1773.55 rows=2869 width=26) -&gt; Hash Right Join (cost=1688.83..1773.55 rows=2869 width=26) Hash Cond: (roster_revisions.account = rosters.account) -&gt; HashAggregate (cost=1603.28..1626.65 rows=2337 width=13) Group Key: roster_revisions.account -&gt; Seq Scan on roster_revisions (cost=0.00..1231.52 rows=74352 width=13) -&gt; Hash (cost=49.69..49.69 rows=2869 width=22) -&gt; Seq Scan on rosters (cost=0.00..49.69 rows=2869 width=22) The query plan for the outer joins does 2 additional sequential scans over the query plan for the inner joins. Make sure your query is actually as efficient as you believe it is before rigging up all of the code for it.
&gt; Unless IO a is really an alias for Foo -&gt; a [Pretty close](https://hackage.haskell.org/package/ghc-prim-0.4.0.0/candidate/docs/GHC-Types.html#t:IO)
If this is an X-Y problem and your end goal is really to define `columns`, then the product-profunctors library is designed to solve exactly this problem. https://hackage.haskell.org/package/product-profunctors-0.7.1.0 {-# LANGUAGE FlexibleInstances #-} {-# LANGUAGE MultiParamTypeClasses #-} {-# LANGUAGE FlexibleContexts #-} {-# LANGUAGE ScopedTypeVariables #-} {-# LANGUAGE TemplateHaskell #-} import Data.Profunctor.Product import Data.Profunctor.Product.Default import Data.Profunctor.Product.TH import Data.Profunctor import Data.Tagged import Control.Applicative import Data.Monoid -- A product profunctor which encodes what you want to do (extract a -- [String]) data GetColumns a b = GetColumns { runGetColumns :: a -&gt; [String] } -- How you want to do it getTagged :: GetColumns (Tagged s String) b getTagged = GetColumns (\(Tagged s) -&gt; [s]) -- Tell the Default typeclass how to do it instance Default GetColumns (Tagged s String) b where def = getTagged -- How to extract the [String] from anything runColumns :: forall a. Default GetColumns a a =&gt; a -&gt; [String] runColumns = runGetColumns (def :: GetColumns a a) -- { Boilerplate which is pretty much derivable instance Profunctor GetColumns where dimap f _ = GetColumns . lmap f . runGetColumns instance Functor (GetColumns a) where fmap _ = GetColumns . runGetColumns instance Applicative (GetColumns a) where pure _ = GetColumns mempty f &lt;*&gt; x = GetColumns (liftA2 (++) (runGetColumns f) (runGetColumns x)) instance ProductProfunctor GetColumns where empty = defaultEmpty (***!) = defaultProfunctorProduct -- } -- You can extract from any size tuple columns :: (Tagged Int String, Tagged Bool String, Tagged Double String) columns = (Tagged "Hello", Tagged "World", Tagged "!!!") example = runColumns columns -- &gt; example -- ["Hello","World","!!!"] -- You can do it for records too data Foo a b c = Foo a b c $(makeAdaptorAndInstance "pFoo" ''Foo) columns2 :: Foo (Tagged Int String) (Tagged Bool String) (Tagged Double String) columns2 = Foo (Tagged "This") (Tagged "is") (Tagged "great") example2 = runColumns columns2 -- &gt; example2 -- ["This","is","great"] By the way &gt; func users :: (Tagged String String, (Tagged String Int, ())) You mean `(Tagged String String, (Tagged Int String, ()))`. That threw me a bit! 
you can trivially make async a monad, i think: newtype AsyncM a = AsyncM { runAsyncM :: IO (Async a) } instance Functor AsyncM where fmap f (AsyncM x) = AsyncM (fmap (fmap f) x) instance Applicative AsyncM where pure = AsyncM . async . return (AsyncM f) &lt;*&gt; (AsyncM a) = AsyncM $ async $ do (f &gt;&gt;= wait) &lt;*&gt; (a &gt;&gt;= wait) instance Monad AsyncM where return = pure (AsyncM a) &gt;&gt;= f = AsyncM $ do res &lt;- a &gt;&gt;= wait runAsyncM $ f res asyncM :: IO a -&gt; AsyncM a asyncM = AsyncM . async waitAsyncM :: AsyncM a -&gt; IO a waitAsyncM = runAsyncM &gt;=&gt; wait
&gt; one data *itch*
How would you do it in imperative (so we can compare which one is the hardest) ?
Crikey...
No one is saying that nested *tuples* are better than nested *records*. What some people are saying is that it can be better to avoid nesting at all. 
&gt; I'm a Haskell hobbyist with decidedly unacademic leanings, Always a good perspective to have on a project.
Personally I'd just ditch SQL and DBs entirely and use something like [acid-state](https://hackage.haskell.org/package/acid-state) with the lovely fact that it lets me write code as if there wasn't *really* a persistence layer at all and my program was just never shut down. To be fair, I've never had to write anything that both could not be practically run on one computer *and* acted like a server, but even then, I think I'd rather stop and build a tool that was like `acid-state` but for multiple nodes than actually have to deal with the nonsense of DBs.
It may be scratching a slightly different itch, but have you seen [this post](https://byorgey.wordpress.com/2016/03/23/boltzmann-sampling-for-generic-arbitrary-instances/) and [the related library](https://np.reddit.com/r/haskell/comments/4dyrqq/a_library_of_boltzmann_sampling_for_generic/)?
Thanks for sharing that. I'll look into a bit more. I'm curious to know more about his solution. I actually have a piece in this package that creates a generic arbitrary (it is separate from `ToADTArbitrary`), but it is naive and a bit dangerous, have to be careful with arbitrary for recursive structures.
 :i IO IO (State# RealWorld -&gt; (# State# RealWorld, a #))
I would first get familiar how stack relates to the Haskell ecosystem if you're not familiar with it. Even though stack is just a view into hackage (so it's not distinct from hackage), it solves a lot of breaking changes problems that using cabal as a view into hackage had. So answers might be a little mixed/complex. Breaking changes are painful if I use cabal to access hackage, but not so much if I'm using stack to access hackage. I assume you're doing this for some research study? Basically I'm saying in Haskell's case you have to be more precise about "ecosystem" to specify the question precisely and not conflate the set of packages with the way they're accessed. The way one accesses the same (stack or cabal) greatly alters the degree to which breaking changes are problematic.
With a proper record system we could have that conveniently. You can get pretty far with my labels experiment: data User = UserUnsaved ("name" := Text, "phone" := Text, "email":= Text) | UserSaved ("id" := Int, "createdAt" := UTCTime, "updatedAt" := UTCTime, "name" := Text, "phone" := Text, "email":= Text) I think it's worth noting to newbies that the limitation is not a "static typing" limitation, but a "Haskell at present" limitation. PureScript for example can natively model the above type, or even Hugs's TRex.
[removed]
That's the same thing. Now you just have to pattern match before the application instead of after.
[removed]
Relational DBs are dinosaurs of the past and a cargo cult religion. A reminiscence in the remote past when math and business intermarried successfully for the last time in history. These times when monsters called mainframes and Unix gave birth to the gigantic web server to serve at most a thounsand users simultaneously. All of this with the promise of flexibility, to allow faster application development in the time of the Web. I was a time when storage and processing was scarce, when logging everything was unthinkable, when data had t be dis-aggregated since to have copies with different structures for different applcations where unthinkable for reasons of storage synchronization and processing load. Updates had to be destructive for limitations of storage. Transactions where done centrally since all that was available was a gigantic database machine whose storage and processing power would not match the power of your smarthphone. All of this to optimize storage and processing while trading performance for a little more flexibility than the more efficient but rigid hierarchical databases. This time has long gone folks. storage and processing are cheap. A relational algebra can be easily codified in the program if you like that nonsense. Transactions too with STM and storage...you have cheap cloud storage everywhere. Standard formats for storing information? The problem is that there are too many. use one. You need to reshape your data to be optimal for other applcation? do it. Do you need to recreate your database from the log of three years? do it in a few seconds. Do you want to log everything? you do it already. Scalability? you can develop a key-value daemon to serve a hundred app servers with local caching in 50 lines of haskell code. Synchronization? your cloud provider does it for you. Why people still use a separate SQL product to manage data? I don´t understand. It should be an irrational adherence to tradition in the worst sense. Because that is the way it has been done in the past. But technology is the only thing where tradition can be the enemy. Either the business-minded one or the matty one are conspiring to let you waste your time. Success and SQL are at odds. Look at the web: no successful company uses SQL databases. Why are you?
**THANK YOU** for the awesome explanation, in order to understand the mentioned example `(item '1' &gt;&gt;= \x -&gt; '2' ) "123"` (I have used `&gt;&gt;=` instead of `bind`) I will try to walk through it step by step bind :: Parser a -&gt; (a -&gt; Parser b) -&gt; Parser b p ‘bind‘ f = \inp -&gt; concat [f v inp’ | (v,inp’) &lt;- p inp] thus, the `(item '1' &gt;&gt;= \x -&gt; item '2' ) "123"` should behave as the following : `(item '1' "123")` should return a tuple of the consumed character and the remaining string `('1',"23")` which is combined with a function that takes a parameter x and return a Parser `(\x -&gt; item '2') '1' "23` so the char '1' should be needed to the parameter for the function that will become `(\'1' -&gt; item '2') "23` which consume the '2' and return the remaining string , a tuple as the following `('2',""3)` am I missing something? Note : in this example the character that we passed is useless sick we didn't use it .
This looks interesting. What would be in your opinion the pros/cons of TCache ? I suppose performances must be much better than querying a DB yet migrations might be harder to handle ?
`div :: Integer \ {0} -&gt; Integer` could do the trick.
Except that it is worthless since what the bind operator does is to destroy the asynchronicity, which is the effect that you want to preserve while doing another thing. by looking at the definitions of async and wait: a &gt;&gt;= b == do let v= newTVar; forkIO a &gt;&gt;= writeTVar v; readTVar v &gt;&gt;= .... then the next bind creates a new future and so on. But the problem is that this adds nothing to the IO monad: it chains IO computations by means of TVars. Only if you process them in parallel, using an hibrid of lists and futures you can perform a meaningful parallel processing and that is what the Scala futures are used for, where `bind` is called `flatMap` . Now Rust has it too. So the monad should consider lists of futures (streams) to be processed in parallel instead of single futures, in order to make any useful work that the IO monad can't do. And this newly says something about how haskell has fallen behind other languages, even using the abstractions that you claim as yours, like monads. You have been soooo fond of playing with worthless shits like the reader and writer monad and massaging lists that you have lost every race folks. 
All depends on the serialization-deserialization defined by you. It can be directly to/from a database, in whose case, TCache works like a program cache, with transactions in memory and periodic write synchornizations to database. this is faster and there are no migration problems, but there is no scalability. the main problem is scalability, but this is the main problem of any database system
Thank you this appears to be exactly what I would have needed with my approach!
[removed]
[This](https://github.com/servo/rust-quicksort/blob/master/lib.rs) is what I find when I look for rust quicksort in google. That's not _quite_ 7 lines. In general, I'd call bullshit on a 7 line implementation of qs unless you're not counting partitioning, swapping, imports and declarations.
There are some interesting approaches with open recursion and classy prisms, in the case where your annotations may not always be present / are wrapped in a `Maybe` I'm currently playing with an AST where I have classy prisms that work over both the `Mu`/`Fix` version and the `Cofree` version and allow me to build / pattern match the core AST. I have a separate typeclass that gives me a prism to the annotation (which does nothing for the `Fix` version of the AST). I'll be writing it all up once I get to the bottom of a few other rabbit holes that I'm digging into... sigh... :)
It looks pretty awesome at first glance. Is it mature tho? It doesn't seem like many programmers use it.
At a guess, I think you are right and the dict must contain all superclass dicts.
Well first of all, I mixed up `item` and `char`. I meant to use `char` which is a `Parser` that matches specific characters. ('item' matches the next character no matter what). I think you have the right idea, although the details are a little more complicated. The type of `Parser` they define in the paper handles ambiguous situations by carrying a list of possibilities. This is why `bind` uses a list comprehension: it is trying every possible parse of the input. For example, consider (many (char 'a') &gt;&gt;= \x -&gt; many (char 'b') &gt;&gt;= \y -&gt; return (x,y)) "ab" `many` tries a `Parser` zero or more times. The first `Parser` above doesn't know whether you want `("","ab")` or `("a","b")`, so you get both. Then the second parser is run twice: the first time with `x` equal to `""` and the input string `"ab"`, and the second time with `x` equal to `"a"` and the input string `"b"`. There are actually simpler ways to create `Parser`s. You could try writing `return` and `&gt;&gt;=` for a `Parser` with this type instead: type Parser a = String -&gt; Maybe (a, String)
tail is not defined like that.
I don't think any ORM does that for you. You need to understand your database and how to do things like this yourself.
@bitemyapp have you heard of Pandoc? 😜
I was wondering whether to mention lazy IO in ORMs as an anti-pattern. I agree with you, as does everyone I've ever talked to about it. The original poster is talking about Ruby's ActiveRecord, which isn't quite the same as Hibernate. I've used Hibernate and it's been fine but I've also seen it go horribly wrong, largely for the reasons you state.
Excellent explanation.
whoops O.o a little embarassing but at least now I know better...
Really good. I think point 8 sums it up and explains why depending on the situation, one is better than the other.
It also destroys the use of labels directly as lenses, doesn't it? As the types flow from the other direction, like with the Html example here. Wasn't that the original point of doing this more "general" thing in the first place to deal with the fact that information flowed in two different directions through the instances depending on what the labels were for? 
At least the crash will be explicit and by choice, and things stay within the realm of just one type checker.
On the plus side, you've massively reinforced your point that unnecessary partial functions are dangerous:)
&gt; Success and SQL are at odds SQL is not the same thing as a relational database, it just happens to be the language that most (all?) RDBMS use. &gt; no successful company uses SQL databases Source please. [StackExchange uses MS SQL Server](http://meta.stackexchange.com/questions/10369/which-tools-and-technologies-are-used-to-build-the-stack-exchange-network). While Google does not use a RDBMS for storing search data, some sources claim that Oracle and MySQL (or MariaDB) are used for other products. Just because some giant company uses a non-relational database does not mean it is appropriate for you. You keep going on an on about needing to scale, but how many companies actually need what you're talking about? Don't you think it's a little optimistic to tell people they should forget about RDBMS all together when the vast majority of them probably won't even need more than a single server (excluding backups, etc.).
Well, we get to pick two of: 1. labels as selector functions 2. labels as van Laarhoven lenses 3. type inference for composition Given that we want 3, it's just a question of whether to choose 1 or 2. My current inclination is to do 1, with a combinator to turn a label into a lens. But I'm open to other ideas.
[Image](http://imgs.xkcd.com/comics/standards.png) [Mobile](https://m.xkcd.com/927/) **Title:** Standards **Title-text:** Fortunately, the charging one has been solved now that we've all standardized on mini\-USB\. Or is it micro\-USB? Shit\. [Comic Explanation](https://www.explainxkcd.com/wiki/index.php/927#Explanation) **Stats:** This comic has been referenced 3347 times, representing 2.7461% of referenced xkcds. --- ^[xkcd.com](https://www.xkcd.com) ^| ^[xkcd sub](https://www.reddit.com/r/xkcd/) ^| ^[Problems/Bugs?](https://www.reddit.com/r/xkcd_transcriber/) ^| ^[Statistics](http://xkcdref.info/statistics/) ^| ^[Stop Replying](https://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=ignore%20me&amp;message=ignore%20me) ^| ^[Delete](https://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=delete&amp;message=delete%20t1_d6fzlba)
The one question I couldn't get from the page.. which string!?
There are actually 27 alternative prelude packages: https://hackage.haskell.org/packages/#cat:Prelude The foundation package isn't tagged as a prelude so it makes at least 28 of them and there might be more if some other aren't tagged. I generally use classyprelude for most of my applications. But I am also interested in protolude, and foundation appear to be also really interresting. Could someone would have the courage to help me (us) to choose the right prelude? For example by explaining some differences between each prelude. (Typically it is hard to see at first sight the difference between `foundation` and `protolude`) and what benefit we get by using one instead of another. Of course I can't imagine that someone tried all 28 alternative prelude, but at least an advice would be very appreciated. Also, I don't want to start a flame war, but isn't the number of alternative prelude an indicator that Prelude should take one of them and reverse the mindset. I mean, use an advanced prelude by default and state clearly to use **BeginnerFriendlyPrelude** in tutorials and Haskell learning material? That way it would be _explicit_ that some function are just there to help learning Haskell in the first few days? I remember my frustration when I understood that many prelude function shouldn't be used as it (typically `head` and `foldl`).
I found the [foundation-edge](https://hackage.haskell.org/package/foundation-edge) package, which aims to convert between "normal" types (`bytestring`, `text`, `vector`) and their Foundation equivalents. 
Foundation reminds me of [SubHask](https://hackage.haskell.org/package/subhask), which also isn't labeled as a prelude. They both seem to ask the question: "How *could* Haskell look if we didn't have any historical baggage?" Packages like `protolude` and `classy-prelude` answer the question: "How *should* Haskell look given the existing ecosystem?" 
What is partial? 
If you had `Lens Entity v` you can just compose your `Lens v whatever` with `(.)` to get `Lens Entity whatever` (unless I misunderstand).
Yes, it's for a research study: there's a description of previous work and goals for this survey at http://breakingapis.org. I'll take a closer look at the relationship between stack and hackage. Would it separate the situations more cleanly if we had survey-takers choose "Haskell (cabal)" vs "Haskell (stack)" as if they were separate ecosystems? Or would there still significant grey area?
I like this package, but I don't consider it an alternative Prelude. The pieces I use make it an alternative to all of text/bytestring/vector/utf8string. 
That doesn't sound dumb but I don't think I understand it. I guess I'll learn more from the docs as they progress.
I did this for a bit early in intero's life, but gave up. One big problem is that intero is still young and changing rapidly. As features are still being explored, ensuring they work when invoked in slightly different ways is too much of a burden for the available programmer-hours. For nix in particular, stack's nix support works really well for what it is. We don't get optimal sharing of packages, but that ought to be handled in the stack layer rather than intero, as it should "just" be a change in how stack finds packages.
First of all, sorry if my comment was a bit harsh, I shouldn’t have commented without taking a closer look. I’d be interested in differences/improvements over `text` and `vector` specifically. I took a brief look at the implemention of `text` and `Foundation.String`. It looks like both use `ByteArray#` internally and the main difference is that one uses UTF-16 while the other uses UTF-8. UTF-8 can save memory but whether it’s faster especially given the work put into `text` is at least open for debate. I’d love to see benchmarks. Or are the improvements intended more on an API level rather than a performance level? For vectors it looks like the main difference on boxed vectors is that `foundation` doesn’t keep track of the size and offset making some operations (e.g. drop) more expensive but reduces overhead. Unboxed vectors look quite different so I’d be very interested in the advantage of `foundation`’s implementation. Overall I am now exited about seeing how `foundation` evolves so thanks for your comment :) 
`Float` and `Double` are reexported but they don't have instances for the alternative numeric hierarchy in `Foundation.Number`.
P.S. I finished the spec, so all the pieces are done now!
Yes. Plus I want pinned and non-pinned based on size. I want efficient substring. And I want the string to be a newtype over an Array which is also usable so I can, in certain cases, switch reps. 
There is also a (currently rather short) list of [beginner friendly issues](https://github.com/jgm/pandoc/labels/beginner%20friendly).
We ditched Yesod templates and moved UI layer to purescript React based lib (thermite). It worked out very well for us. Purescript not only compiles much faster than ghcjs but also produces much smaller code and loads faster. 
Another option is to make use of dependent types, then have your div function demand the second argument be &gt;0. Better in Idris but it's limitedly supported in Haskell.
Mostly due to the fact that I rarely used them, but it will be there at some point. edit: While it's not supported all the way, they're available in the unboxed array, so at least you can pack/unpack them efficiently :)
I might have some extra time today, I'll take a look.
"Provide a better and more efficient prelude". It's a quote. :)
This is a work in progress but I plan on writing a new article every week or so.
Isn't this basically the worst of both worlds? You still need to manually unwrap the value while getting no safety out of it (I checked the source and `fromPartial (head [])` is ⊥). If you need a way to "switch off" partial functions, I think it is better to just dump all partial functions into a module (and mark all of them as deprecated) so people can import it if they really really want to. Once they are done with the prototyping, just remove the import and manually fix up the type errors.
I do not disagree with this, except maybe with the use of what I perceive as a non-standard definition of teaching. If it is not direct and practical I wouldn't call it teaching. If you sign up for a golf course which consists of getting handed a copy of [The physics of golf](https://www.amazon.com/Physics-Golf-Theodore-P-Jorgensen/dp/038798691X), that is hardly teaching. No doubt is that knowledge beneficial, put it is simply too far removed from the core skills that define the activity, to be awarded any teaching character, even if it can refine the activity. Learning the mathematical foundations of computation, the various approaches to semantics and how they relate is warmly recommended from my side and it will probably improve your programs in some ways but those improvements will be small compared to more fundamental knowledge and skills and might only be visible in certain domains (which are not the ones most professional programmers work in). The real benefit is personal understanding and appreciation of what you do, why you do it and what the alternatives would be. Category theory is also just a organizational framework (tremendously successful at that) in which for example computer science can be developed or cast into. That makes its contribution to any benefit you could reap by learning programming language theory even more indirect. It makes me uneasy that the OP somehow got the impression that there ought to be such a book. And I think we shouldn't contribute to the proliferation of the idea that Haskell (or programming in general) has something to do with category theory anymore than it has with set theory.
I think a good approach here might be to have operations that explicitly support stream fusion. For example, this part of conduit-combinators provides some functions that provide fusion directly via inlining, no rewriting required: https://hackage.haskell.org/package/conduit-1.2.7/docs/Data-Conduit-Internal-List-Stream.html http://www.yesodweb.com/blog/2016/02/first-class-stream-fusion This has the advantage of consistent performance - with explicit stream fusion, it's less likely that tweaks to the stage of RULES or changes in GHC inlining will affect the performance of your code. Also, if you do want to add fusion based on rewrite rules, having the stream fusion API will likely be quite helpful. 
Well, I have had broad definition of programming (which include programming language theory and CS) in mind when talked about "bettering design of my programs". So your argument (except CT for pure math) is off the point. I have skimmed over your recommendation and at first glance it doesn't seems all that different from other books I have tried: the full third of it remind me of dictionary consiting only of terms definitions and nothing else.
The git project [description](https://github.com/haskell-foundation/foundation): "a new hope"
I am confused by your comment. Half of Haskell more interesting typeclasses borrowed from CT and a lot of libraries are designing their API around CT concepts, yet you say that Haskell has nothing to do with CT.
I agree with the last paragraph but especially the second: I think that such mathematics is beautiful, and also category theory is incredibly hard to motivate without actual mathematics. 
The difference is the other two thirds put those to use to develop a systematic approach to reasoning about formal equivalences between programs and the "essence" of various algorithms.
&gt; it will probably improve your programs in some ways but those improvements will be small compared to more fundamental knowledge and skills and might only be visible in certain domains (which are not the ones most professional programmers work in) All I have to say is "this is not my experience". I also disagree that category theory is just an organizational framework. It is a legitimate subfield of mathematics! It has results and theorems and proofs and techniques and lemmas and all the stuff one would expect from any other field of math. If you only learn the first few definitions and examples of things that fit in them, sure. But that's like calling analysis just a set of notations for talking about real numbers.
Look at it this way: if you ask a category theorist for examples of CT concepts, you're likely to get a nice long list from algebra, topology, logic, etc. If Haskell is mentioned at all, it'll be a footnote: "Oh and some programming languages folks find it useful as well."
What if the use site for the subclass function doesn't know what instance it's using? Different instances might use or not use their superclass dictionaries, but they all need to have the same (core) type for polymorphism and for separate compilation. Or does *every* class method get passed all its superclass dictionaries, whether or not it needs them? That seems awkward compared to passing them once when the subclass dict gets constructed (the way evidence for instance contexts presumably works) and keeping them in the dict.
I want to learn CT to make better design decision i.e. figuring out neat and comprehensive API for my library. If I have to memorize full third of terms definitions in aforementioned book, so be it. It's just CT is the first field of math for me in which there is no familiar cycle "definition - proofs of useful facts about defined objects - new definition".
The link I gave, was not intended as recommendation. I mentioned it as something that comes to mind targeted at programmers, but thought I made it clear that it ultimately fails your two criteria. Now, if you equate computer science and programming, here you [go](http://www.math.mcgill.ca/triples/Barr-Wells-ctcs.pdf). But it is not helpful for mutual understanding to use such imprecise and broad definitions.
Thanks for link, it seems to be extactly for what I was searching for. About previous link and term usage, unfortunately I have overestimated my knowledge of English a bit, so these are totaly my faults.
Perhaps its string type should be put into its own package then! But, what's the difference between this and `Text`? Just the encoding?
Maybe! If you populated the package db locations stack uses with symlinks to the store, you could just pass read only commands to the real stack executable. Anything that might cause stack to install something would need to be wrapped, and I'm not sure how well-delineated that part of the API is. Alternately, having this be a mode of operation for the real stack seems not so far fetched to me as I think stack could probably stand to benefit from some tight abstraction at that level. For instance, I have this slowly decaying PR to hpack that would really benefit from stack itself having a content addressable store. If that is to ever happen, operations that affect the set of installed packages will have to be well encapsulated.
I happen to agree with you there. I just think that #label was made far more general than it needs to be, which is going to lead to a bunch of confusion when the thing it is used on has a general enough type that it isn't necessarily `(x -&gt; y)` and the user starts getting "no instance" errors.
If you have something like instance C a =&gt; D b where ... then the instance dictionary for `D b` is created by a function that gets the instance dictionary of `C a` as an argument. It can, by using fields of this passed instance dictionary, use the `C a` functions to implement the `D b` functions; but no dictionary is stored. And yes, this means if you have e.g. instance Eq a =&gt; Eq [a] where ... then you *can't get the `Eq a` dictionary from an `Eq [a]` dictionary*.
I'm in the camp that when I use a function from a library, I don't want to jump from my bed early Sunday morning with an alarm triggered by a critical server due to that function throwing SomeVeryCreativeExceptionalConditionYouWouldNeverExpectUnlessYouWereStudyingMyDocumentationLikeHolyScriptsException. Where do I fall in the taxonomy you described?
I agree with your argument that we shouldn't lump those together, especially since `div` is special for being a numeric function and that it maps directly to a CPU instruction. People occasionally need the highest possible performance (meaning they won't accept 10% overhead) when it comes to code manipulating numbers, but the same can't be said about a function that opens a connection to a database. So, for `div`, the arguments for partiality are performance and usability, but for `openDBConnection`, it's just usability, and I think it's more subjective.
Very nice, thank you!
`openDBConnection` is also going to be in `IO`, so having it throw an exception isn't as problematic; at least in principle, you can always turn it back into a `Either` type, though it may be painful to do so correctly. On the other hand, it's a heavy-weight enough operation in the semantic sense that having it return `Maybe` or `Either` doesn't seem that overbearing either. With `div`, throwing exceptions from pure code is extra problematic, but having to pattern match on every result is also extra obnoxious.
A user always has a list of posts, though it may be empty?
thanks a lot!
Cool stuff, it's a really elegant and beautiful way of expressing a solution to a problem.
&gt; I think a good approach here might be to have operations that explicitly support stream fusion. Yes, this is something that really needs to be explored.
yes. I blame, lack of better name and trying to provide something not too far from Num. StuffThatContainsDigits didn't have the same ring to it.
There is also this wiki entry on memoization in Haskell worth looking at : https://wiki.haskell.org/Memoization
&gt; I don't think there's anything too broken in what's already existing with Double. You mean except for `Double`s breaking almost all algebraic laws thereby requiring a separate family of typeclasses to represent their ops...
I have declared the following type type Parser a = String -&gt; [(a,String)] and some function to operate on the parser as the bellow succeed :: a -&gt; Parser a succeed v = \inp -&gt; [(v,inp)] when trying to run `stack ghci` in order to test the above function `succeed` I got an error that Parser is not an instance of `show` so I've tried to update the code and add the following instance Show Parser where show [(v,inp)] = show (v,inp) but I got an error that `Show` is expecting the argument to be `*` but it is `* -&gt; *` could you please help me so I can test the function in GHCI.
Still going ... ;)
So what was the motivation for writing this package then? Should it be seen as an experiment, to see what a clean-slate prelude would look like, and perhaps to gain some inspiration for slowly improving the official prelude? Or do you have plans to encourage ecosystem-wide adoption and replace `bytestring`/`text` in the long term?
One practical advantage is that when inspecting import list it's immediately known what modules you can jump to (e.g. with Emacs+hasktags) and to which you can't.
I think you should report the problems you ran into with Eclipse, Leksah and Atom on StackOverflow. If you know some Emacs, `haskell-mode` or `intero` are good and quite popular, but both require the use of more than one finger at a time. `intero` in particular does most of the compiler+project setup for you.
EclipseFP hasn't been developed since 2015, and its well known that it's no longer functional. I'll post about the other two on SO, what would be the right site for it? Stackexchange? Superuser? 
You're very welcome :)
I can not argue with your experience. I'm glad people are exploring the ways to apply category theory to programming so thoroughly and with great competence. Alas, a I can not help but notice that the results apply rather narrowly, if at all. There can be said a lot about this, the peculiar brand of category theory used in Haskell, the lack of useful concepts motivated by category theory (as opposed to just expressed in category theory after being formulated in Haskell or elsewhere), the difficulty of applying categorical constructions ubiquitous in mathematics outside of Logic/CS in useful ways, applications of obscure recursion schemes have yet to materialize and so forth. But we would just rehash old debates. To category theory as a subfield of mathematics. I'm willing to grant it that status, but I don't think it contradicts that it is just an organizational framework. There are very few categorical theorems that appear deep to me or even meaningful taken out of the context of it's applications, the Yoneda Lemma being the most notable exception maybe. This is in sharp contrast to analysis, where just a list of rather self-contained fundamental inequalities could go on for pages. Maybe this is just my lack imagination or maybe I just don't know enough category theory, but then again I don't think this is a particular fringe opinion. Writing this I feel the need to clarify, that I'm actually fond of category theory. There is nothing wrong with being a organizational framework. Finding definitions that are basically as inclusive as set theory, while admitting such a huge body of ready made analytical tools is not easy, and I'm not sure the favorable trade-off categories strike will ever be matched, though it is conceivable that some day some type theory might. It is just that the talk of category theory within the Haskell community greatly exceeds its applicability to programming in Haskell. If the Haskell community where so math oriented that this were just a natural consequence of using modern formulations I would not object, but it isn't. I rarely see discussions about core computer science topics, let alone even computational methods from more general mathematics. It's mostly just CT, and that is just plain backwards.
https://stackoverflow.com/
Sounds like you want the type checker to force you to read the documentation. It's a good idea! Let's examine whether each approach is a good fit for this purpose: * `div :: Int -&gt; Int -&gt; Int` lets you accidentally call the function with inputs which the documentation says are illegal. 1. If you do the right thing and ensure much earlier that the user's input is such that the denominator will eventually be non-zero, you need to manually track the propagation of your assumptions into the various intermediate structures, presumably using comments like `-- xs must be non-empty`, so you're creating more easy-to-misuse functions like `div` in your own code. 1. If the input conditions change, the type of `div` doesn't change so the compiler doesn't help you to find all the places at which you need to change your validation logic. * `div' :: Int -&gt; Int -&gt; Maybe Int` does force you to read the documentation in order to understand under which circumstances the output can be `Nothing`. 1. If you do the right thing, you still need to manually track the propagation of your assumptions, so you only call `fromJust` when you know for sure that the `Nothing` case is impossible. 1. If the input conditions change, the type of `div'` doesn't change, so you have the same problem as before. `Int -&gt; Int -&gt; Either DenominatorWasZero Int` would be better for this, but only if you were careful enough to use `case ... of Left DenomunatorWasZero -&gt; error "never happens"; ...` and not simply `fromRight`. * `div'' :: Int -&gt; NonZeroInt -&gt; Int` also forces you to read the documentation, in order to understand how to create a `NonZeroInt`. Presumably you create it from an `Int` using a partial function, and calling this partial function signals to the compiler that you have read the documentation and that you have checked that its input is indeed non-zero. I guess nothing stops you from calling this second partial function without reading its documentation though, so it should probably have a scary name like `unsafeMkNonZeroInt` or `partialMkNonZeroInt`. 1. If you do the right thing, you can use a variety of wrapper types like `NonEmptyList` to track how the early checks eventually translate to a non-zero denominator. 1. If the input condition changes to `PositiveInt`, the type of `div''` also changes, so you know exactly which assumptions you need to re-validate. Combined with a typed encoding of your assumptions as above, this can dramatically reduce the amount of code you need to re-examine.
That's such a great post. 
It might have been ok way back when, but nowadays I'd still say it's broken. The semantics of a String are a list of unicode characters. If that's not what a function expects/supports, it should make a newtype. This is Haskell, I need to be able to trust the types. I have the same issue with more "modern" libraries that use the Char8 ByteString functions/instances. If the bytes represent text, it's usually UTF8 nowadays, not ASCII, so that's a great source of bugs. That whole module should have used a newtyped ByteString. I might sound a bit cranky, that's because I've been dealing with too many of these bugs in Haskell, where we have the perfect solution (newtypes) to avoid all of them. It is especially bad with the `IsString` instance, since it leaks everywhere and allows you to accidentally use it.
You may want to stop reading these tutorials and blogs and hit text books and papers on the topic. Unfortunately, you may need to learn some pure category theory in order to apply it to functional programming. And to treat it like a tool, you'll need to gain some intuition in cat. theory and that's going to take some time and work. Here's a paper applying category theory to FP: http://www.cs.ox.ac.uk/ralf.hinze/SSGIP10/AdjointFolds.pdf
Hmm, is it really representing panics as exceptions, or the fact that some functions panic when given inputs which their type says they should accept? Consider the following definitions: -- panics on `div x 0` div :: Int -&gt; Int -&gt; Int -- Nothing on `div' x 0` div' :: Int -&gt; Int -&gt; Maybe Int div' _ 0 = Nothing div' x y = div x y half :: Int -&gt; Int half x = fromJust (div' x 2) half' :: Int -&gt; Maybe Int half' x = div' x 2 `div` panics when given an input which its type says it should be able to handle. I can understand why someone could prefer `div'`. `half` panics if there is a programmer error, for example if the documentation of `div'` meant that `div' x 0` is *one* example in which Nothing is returned and not the *only* example. In this case though there is clearly no programmer error, so I doubt anyone will say that they prefer `half'` (if someone does, please say so, I'd like to understand why!). And if we accept `half'`, how complicated does the code need to be before we need to use a `Maybe` to acknowledge the fact that we might have made a mistake somewhere in it?
It's a bit enormous, but I like the numeric hierarchy in [algebra](https://hackage.haskell.org/package/algebra).
Sadly this contradicts "functions are values". Functions are kind of lower class values. 
While I've used this technique to good effect, it makes me uneasy. Technically Haskell is non-strict, and while GHC generally uses call-by-need, it is allowed to use call-by-name wherever it wants, which would make this approach useless. Basically this approach relies on an implementation detail of GHC, which is worrying.
I was going through the paper, I have defined the `bind` and `result` operator and created a useful parser and combine it **without** making the `Parser` to be an instance of Monad type class. if we can create many combinator and achieve to build a useful parser without letting the Parser to be an instance of `monad` so why do we make it so? is all the benefit is the monad comprehension (close to list comprehension )!
no. 
Not really answering your question but to stop "fiddling" with the command line, learn to touch type ;-)
Heh. I actually use maps over arrays to get back a new array in PHP (it's PHP, so no one cares about memory use, anyway).
I do not want to drag on this conversation, but I'm not sure what you are getting at here. I'm very fond of categorical logic and Lambek's work specifically. I did not read that book, but it is probably good. Maybe the intended meaning wasn't captured properly by my sentence. I meant branches that are not categorical logic and computer science (read: more traditional/mainstream fields), such as algebraic topology, algebraic geometry, one of the numerous subfields that orbit around them or increasingly even number theory. Basic categorical constructions used in those fields like monoidal categories or sheaves do not find application in everyday programming and specifically in Haskell. This is not a bad thing. What is bad is that this is not clearly communicated. Yes there are typeclasses that represent well behaved interfaces analogous to categorical constructions, if you squint a lot. That analogy can be pushed further but the applications get increasingly thin. Some things can be described categorically in neat ways. But that's it. The same can be said about number theory, automata theory or linear algebra. Pretending anything else is false advertisement and alienates people who actually know category theory from a mathematical setting, most of them will say: Screw this, back to Mathematica. It hurts programmers (from non-mathematical backgrounds) who might not even pick up Haskell after reading Wikipedia on category theory and it also sends Haskellers who seek to improve into the wrong direction.
Neat! The package hasn't been updated in 3 years, though. I wonder if it's neglected, or simply... complete.
Great idea, thank you!!
Thanks for the awesome response, I got your point, I am getting into monad by trying to understand monadic parser.
I *guess* I'd consider myself in the "correctness" camp, but I'd never advocate `div :: Int -&gt; Int -&gt; Maybe Int`. Most people realize that you have to be practical with exceptional cases. Those of us that want type information for error handling mostly want it for *recoverable* exceptions, not for those exceptions which should hopefully never occur.
What's so unstable about leksah? I haven't used it in a long time but it's the only GUI I've had success with that handled everything without requiring the command line. My recommendation is to just learn how to write a cabal file. Just learn the cabal and/or stack command line tools. You don't need to understand all the options, just stick with the defaults and you'll get pretty far. What I usually do for a new project: * stack new my-proj * stack setup (if I haven't already installed the needed ghc version) * edit the generated my-proj.cabal file, adding any dependencies I need as I go along * edit the generated stack.yaml file (only if any packages are missing from the stackage snapshot) * stack repl (for experimenting with code I've written) * stack test (for compiling and running the test suite) Stack handles sandboxing, fetching, and installing dependencies for you pretty well behind the scenes. You just have to specify what those dependencies are.
We do not do server-side rendering. We simply do not need it. We are not facebook :) Our applications are not that huge and slow that we would need every microsecond. Initial rendering of our apps is practically instant without any serverside rendering. 
&gt; Basic categorical constructions used in those fields like monoidal categories or sheaves do not find application in everyday programming and specifically in Haskell. &gt; Monoidal functors yield applicatives in Haskell. Sheaf-theoretic semantics are one of the cornerstones of categorical logic. I agree that there are plenty of categorical tools used in algebraic geometry for example that don't find use in the logical side of things as much (in particular, lots of things that require abelian settings, or at least settings with some form of inverse structure). But that doesn't mean that category theory _as such_ (and especially via categorical logic and type theory) has little to offer. Mainly, I think you're unfamiliar with a fair amount of fundamental research in computer science that happens to use category theory quite seriously. So you're looking at category theory as you understand it from entirely different domains, and rightfully finding that much doesn't translate. But saying that its applications to algebraic geometry are "real" and its applications to semantics are "not mainstream" is just silly. Or consider for example the work of r.a.g. seely http://www.math.mcgill.ca/rags/ This stuff is all about programming languages and logic on the one hand, and all conducted via categorical methods on the other. It is fine to say that this doesn't interest you. But to pretend that this is not actually "category theory from a mathematical setting" is not useful. 
When a v2 comes out that redoes the api it can be called `second-foundation`!
Says experimental. As it that meant anything. It would be good to have a way to assess real package quality (like included in popular package). PackageRank
&gt; Category theory is also just a organizational framework This is certainly not an uncommon opinion but there are computer scientists who think otherwise, e.g.: Robert Harper. In fact type theory, logic, and category theory are the foundations of CS from an FP perspective: https://www.doc.ic.ac.uk/~gds/PLMW/harper-plmw13-talk.pdf
I made some progress trying to implement a minimal stack command (Python though for iteration speed): https://gist.github.com/teh/c603c450067bb2a46e260821fa7f985c Emacs calling this "stack" doesn't report an error for a file that ghci can parse but the flycheck hangs forever on error so I'd have to start digging into e-list now (see comment on that gist). I ran out of time but maybe I can make this work at some point. **Edit** got it to work: https://gist.github.com/teh/5c8de9eb7eb8b763730f37be75f88562 running intero in nix with this small shim **Edit 2** Removed a few hardcoded paths and fixed the ghci invocation. All working now and happily coding away. A testament to the intero developers really :) 
I honestly just forgot about `fromJust` when I was writing this, otherwise I would've used that.
What's wrong about using path as "opaque" identifier (.i.e as String) and let the system decides of its meaning ?
&gt; Those of us that want type information for error handling mostly want it for recoverable exceptions, not for those exceptions which should hopefully never occur. If that's true, I'm happy to report that we're all on the same side after all, because that's also what we want on the "practical" camp :)
I've seen a good amount of people on this sub saying using checked exceptions (`MonadError`) overtop IO is bad practice, because IO's unchecked exceptions serve the same purpose, only better. This is what I think is the debate. I'd personally advocate rarely using IO's unchecked exceptions, except for in cases where the exception should almost never be caught. But a lot of people still say you should use IO-style exceptions whenever possible.
It allows you to run arbitrary JS in Webkit window. The type signature looks like: executeJS: Ref Webview -&gt; ByteString -&gt; IO () You can also bind to any event via `bindEvent`. So, yes, any widget can be informed about events happening inside a Webkit window. 
Type theory and proof theory occupy a similar ontological position as category theory in my mind. So his trinitarianism is at least consistent. I don't think there is much merit to getting into a fight over how much mathematically original content any of those three subjects posses. I'm actually comfortable with the fact that people hold widely different opinions, on all kinds of things. I think 90 percent of those difference come down to miscommunication and slightly different definitions, which is a bit unfortunate. The remaining 10 percent account for genuinely different tastes, which is ok. That comes with being human. I would be disappointed if Mr. Harper wouldn't argue staunchly for proof theory as the most interesting mathematical subject, if provoked. After all he also thinks that Haskell has nothing over Algol 60, which may or may not be true. I did not check that claim.
Since this package isn't on Hackage I've added the Haddocks to the repo for convenient [online browsing](http://htmlpreview.github.io/?https://raw.githubusercontent.com/deech/webkitfltkhs/master/doc/index.html). [Here](https://htmlpreview.github.io/?https://raw.githubusercontent.com/deech/webkitfltkhs/master/doc/webkitfltkhs-0.0.0.1/Graphics-UI-FLTK-LowLevel-Webkit.html#g:9) for instance are the hierarchy and "member" functions supported by the `Webview` "object".
The difference between code and data blurs more in Lisp programming, where the syntax is so uniform (s-expressions) that it's often trivial to generate code. Von Neumann, for better or worse, designed the modern computer architecture such that assembler code and data are often interchangeable. A section of memory could contain instructions and/or data, and malware often takes advantage of this property to elevate privileges.
safe
There are lots of ways to talk about this, but in the specific context there he means the following. (pretending all functions are total and ignoring bottom, to keep things simple). Consider the functions of type `Bool -&gt; Int`. These are in bijection to data of the form `(Int, Int)`. Exercise: write the functions `f2pair` and `pair2f` that witness the bijection in each direction, and use equational reasoning to show that the composition in either direction is the identity. Now, generalize from functions `Bool -&gt; Int` to functions `Bool -&gt; a` for any `a`. Easy, right? Now, generalize from `Bool` to any other type, and consider what we require of that type to witness the bijection. What do we arrive at? The idea that functions can be thought of as purely the data of their "lookup tables" and also that "lookup tables" can be compressed into functions. Going a bit further, the definition of exponentials gives us a bijection between arrows of type `a -&gt; (b -&gt; c)` and `(a,b) -&gt; c`. Consider the special case where `a` and `b` are both `Bool` and so `(a,b)` has four inhabitants and think about what this identity yields, when viewed through the idea of "lookup tables".
&gt; I hope that there is a way to write a decision procedure like `decideSublist` but that is somehow lifted so that it fails at compile time instead of runtime. There is, but you need to write it at the type level using type families, whose syntax and idioms are a bit different from value-level functions: {-# LANGUAGE GADTs, TypeFamilies, TypeOperators, TypeInType, UndecidableInstances #-} type family DecideSubset (super :: [k]) (sub :: [k]) :: Maybe (Sublist super sub) where DecideSubset '[] '[] = 'Just ('SublistNil) DecideSubset (x ': xs) (x ': xs') = DecideSubset_Both x (DecideSubset xs xs') DecideSubset (x ': xs) xs' = DecideSubset_Super x (DecideSubset xs xs') DecideSubset xs xs' = 'Nothing -- there is no type-level case..of, using these helpers instead type family DecideSubset_Both (x :: k) (rec :: Maybe (Sublist super sub)) :: Maybe (Sublist (x ': super) (x ': sub)) where DecideSubset_Both x ('Just sublist) = 'Just (SublistBoth sublist) DecideSubset_Both x 'Nothing = 'Nothing type family DecideSubset_Super (x :: k) (rec :: Maybe (Sublist super sub)) :: Maybe (Sublist (x ': super) sub) where DecideSubset_Super x ('Just sublist) = 'Just (SublistSuper sublist) DecideSubset_Super x 'Nothing = 'Nothing -- do you need a value-level Sublist, or just a compile-time proof on known lists? mkSublist :: DecideSubset xs ys ~ Just prf =&gt; Sublist xs ys mkSublist = undefined test1 :: Sublist '[] '[] test1 = mkSublist test2 :: Sublist '[Int] '[Int] test2 = mkSublist test3 :: Sublist '[Int,Bool] '[Int] test3 = mkSublist -- Couldn't match type ‘'Nothing’ with ‘'Just prf0’ -- test4 :: Sublist '[Int] '[Int,Bool] -- test4 = mkSublist 
From my experience (of course not scientific proof), people with the ability to work on the command line, get things done, because they actually try to learn, even if they feel uncomfortable in that environment. The crazy thing is, that without an IDE you're usually faster and have more power. Learning Haskell requires a somewhat similar approach, with similar benefits.
that's interesting to know, thanks. Do you know much about the duality between functions and data?
The original proposal (modified to include # as grabbing a new namespace) would have been to just have this been basically a hook for the HasField type, where you can reason from the fundeps.
This lookup table view turns out to be exactly how Conal Elliot memoizes functions with memotries :)
Any example ?
fortunately, no one will find this package. it will be at hackage's end.
My motivation is to have a (modern) base replacement, with hopefully a community of people that can improve things together, with a bar for contribution as low as possible; Everything will be maintained and organized on github as starter, and the goals is to scale the number of maintainers (we're already 4 to have commit rights), not be stuck on 1 person that is overworked and too busy to approve anything. I certainly don't want to discourage a wider adoption, but I don't plan to chase after people to replace their stuff. Hopefully, this whole thing will speak for itself once we're cruising. Edit: Forgot to say, that I think the current Prelude (and for that matter `bytestring` and `text`) are lost cause related to improving them, so there's definitely no plan from my side to do this.
Hmm, looks like this glass is not pure.
It is certainly true that maintaining a carefully crafted set of focused packages is more work for the maintainer than maintaining a single monolithic package, which shifts costs ~~to~~away from the maintainer. On the other hand, big monolithic packages with a large API surface require more frequent version increments in general. This can cause costs for the end-user if you only need to rely on a small fraction of the exposed API, since you have to review for each major version bump whether the small part you're using was affected (case in point: `base`). Consequently, big monolithic packages are rather disadvantageous in the context of semantic versioning, which is first and foremost a mechanism designed for the benefit of (the many) API consumers, rather than (the few) API providers. EDIT: semantic typo
pinned/unpinned is related to garbage collection. In languages like Haskell (but also e.g. C#/.NET), garbage collection is performed by copying over the set of reachable data in the heap to a 'new' heap (think of 2 heaps between which data is copied forth and back). Since only reachable objects are copied, all unreachable objects are released when the old heap is released. This mechanism is particularly comfortable to use in a Haskell setting, see http://ezyang.com/jfp-ghc-rts-draft.pdf for more info or if I wasn't clear enough. Usually, objects are unpinned in this setting, meaning that the address in memory may vary due to that copying process. That's completely transparent as long as you stay in Haskell, but it becomes problematic if you want to interface with C, where every pointer is expected to stay valid even after a GC pass. That's why it might be necessary to pin an object, e.g. telling the GC not to move that object during GC passes. ~~Usually, pinned objects are not allocated on one of the GC heaps but on another heap, possibly using `malloc` and use another kind of GC mechanism for deallocation.~~ Not so in GHC, see the last link below. I wonder why? There is another motivation for pinned objects: GC performance might degrade for huge objects which have to be copied from one heap to another, so in some cases it might speed up things if you explicitly state not to copy that object. But that's mostly not a problem to worry about. [This link](https://goo.gl/dhEGwG) contains a lot of unrelated C\# stuff, but gets across the point rather well. [Also this is GHC specific](https://ghc.haskell.org/trac/ghc/wiki/Commentary/Rts/Storage/GC/Pinned) and very informative.
Does anyone know, can pinned arrays be used as a way of dealing with very large amounts of data without hurting the GC time?
The IDE story is definitely better than it was, but still not where it needs to be. Languages like Java and C# have IDEs that make so many common tasks completely trivial or automates them entirely. Haskell would have much more information available than those languages so I would expect a much more powerful IDE to be possible. @Neil Would you have an availability or interest in helping the haskell-ide project with what you learned with ghcid? At this point I think the easiest way to get to a proper IDE would be to make a well behaved server that can communicates with [Visual Studio Code](https://code.visualstudio.com).
Re IDE: I think Atom's also working pretty well as an IDE. Re Array/String libraries: Foundation looks _awesome_. Please forgive me linking this XKCD re Haskell string libraries: https://xkcd.com/927/
I use Atom with a couple of plugins, and - apart from the startup time that can be measured in years - it works perfectly.
You can't define instances of typeclasses such as `Functor`, `Applicative`, `Monad`, for that type synonym. A type synonym also exposes its "implementation details", and users of such a library might be tempted to use `Parser a` values as the function they are, rather than use provided combinators. Changing the definition of `Parser` (for example to be more efficient or to add features) would then break their code. By defining a `newtype` and hiding its constructor (which is not possible with `type`, you cannot hide the definition of a type synonym) you avoid that.
There are two main advantages that I can see here: - if you don't expose the implementation to the user, it means you have a chance at being able to change the implementation later without breaking the users code - if you wrap it in a newtype, you can write typeclass instances for `Parser`
And in my post I described a shortcut to having a nice IDE: use the Visual Studio Code platform. Then all we need is a server for it to talk to. We can gradually add features there purely by updating the server. All the GUI and crossplatform GUI complexity that e.g. Leksah has to deal with can be completely avoided.
&gt;The crazy thing is, that without an IDE you're usually faster and have more power. I humbly submit that you've never used a really good IDE to its full potential. Even in Haskell, if I have to save my file and run some command to find out type errors (I know you don't have to today, but just as an illustration) how can that possibly be faster than the IDE showing a red underline under the problem statement? How are you doing big refactorings? With grep and other unix command line tools? Followed by more commands to see if and where it went wrong? How can that possibly be faster than an IDE doing a Ctrl+R and knowing that the IDE, with its deep knowledge of the syntax of my language, updating exactly the right places across the entire code base?
Windows users seem to be a minority in the Haskell community. They seem to be far out numbered by Linux and Mac users. I do know that Visual Studio has recently been made available for Linux and Mac, but: a) I haven't seen the licensing details and I care about licenses b) Its not open source (and I care about open source). 
I like this an awful lot! Sir/Ma'am, consider it stolen!
https://github.com/Microsoft/vscode/blob/master/LICENSE.txt
I use it too and it certainly has come a long way but to me it's still a text editor with some useful features on top. What makes IntelliJ an IDE when I do Java is refactoring (renaming, moving, "type migration") and code navigation (find usages, jump to definition).
&gt; How to read Haskell for non-Haskell programmers That's a suggestion I take it! I spent five minutes googling for all I'm worth before I realised! 
What do you mean with SI?
Atom is a bit slow starting up, I agree.
I'm not terribly familiar with Iso's, so I couldn't really take that into consideration. Could Iso's be used to simplify the concept?
The product to which /r/alien_at_work refers is Visual Studio Code, not Visual Studio. It's similar to Atom in that it's open source and built on Electron.
https://en.wikipedia.org/wiki/Système_international_d'unités
&gt; While I am a big fan of laziness Sincere question: why? I'd love to read a blog post that describes the performance benefits of lazy evaluation, with real world examples versus a strict language.
Yes, my question is what are the benefits of your approach ? I m not saying you are wrong, am I genuinely interested as I desk with units sometimes.
https://en.wikipedia.org/wiki/Celsius
I guess that there could be a _slight_ loss of precision in scientific computing, when there's a hidden default unit of measurement, which you switch from and to every time you use the value. Ultimately, the implementation shouldn't matter that much in most cases. This was just the way I imagined it. If you wanted to have a data type a la data Temperature = Kelvin Double then that could work too. It was mostly the idea of using lenses (or as @Lossy noticed below, Iso's) to convert between units of measure.
indeed. 
fixed
What OP *wants* is a way to specify a value as "to be defined upon insertion". The usual way to do that in e.g. Ruby is to simply leave it undefined, so that's what OP originally asked for, but the original problem remains. So how should it be done? Partial values? Then we're right back at "implicit nulls". Related types? E.g. "User" and "User_uninserted", or "User" and "UserWithDBValues". That seems tiresome, but maybe ultimately the best we can do. `Maybe` is the wrong choice, incidentally, because the data isn't optional. There's simply two steps to creating a value: some fields are defined in Haskell, and others are defined in the DB. Unless somebody comes up with a great answer, and can show that it has already been common knowledge and accessible to people who have earnestly looked for it, I think OP's question is a useful one.
/u/phadej had an excellent point here: https://www.reddit.com/r/haskell/comments/4xss1d/using_lenses_to_convert_between_units_of_measure/d6i5mj0
Misinformation being upvoted. Visual Studio Code runs on Windows, Linux and Mac. And it's open source.
Yes, you could definitely build a browser. `webkitfltk` itself was created to back the [Fifth](http://fifth-browser.sourceforge.net/) browser. There's also a number of convenience functions exposed to Haskell users for web scraping, eg, `getLinkDetails` which will grab all the link tags from a page.
&gt; Re: Jump to definition: It only really works for global functions defined in the same file. It does? Then I have imagined it working across the project! I can't check right now. There are two jump to definitions: the built-in atom one and the ghc-mod one. I know both have pros and cons but neither work across the project?
Thanks! This is the way that I want to write the function, although I do need the value-level sublist. Fortunately, by adding this code to your example, I was able to get everything I needed: class DemoteSublist (x :: Sublist (xs :: [k]) ys) where demoteSublist :: Proxy x -&gt; Sublist xs ys instance DemoteSublist 'SublistNil where demoteSublist _ = SublistNil instance DemoteSublist s =&gt; DemoteSublist ('SublistBoth s) where demoteSublist _ = SublistBoth (demoteSublist (Proxy :: Proxy s)) instance DemoteSublist s =&gt; DemoteSublist ('SublistSuper s) where demoteSublist _ = SublistSuper (demoteSublist (Proxy :: Proxy s)) mkSublistBetter :: forall xs ys prf. (DecideSubset xs ys ~ Just prf, DemoteSublist prf) =&gt; Sublist xs ys mkSublistBetter = demoteSublist (Proxy :: Proxy prf) 
But Visual Studio Code is specifically designed to be a code editor (or an "almost-IDE") so standard things you expect to exist already do and require no extension. The apps you mention are extensible editors which means you have to maintain not only some kind of ide-server, you also have to maintain the extensions appropriate for the editor in question. This is more effort. I agree that the GHC interface being clumsy is a hindrance, but I think writing editor plugins is also extra overhead.
The loss of precision here isn't degenerative at all (unless you are converting infinitesimal quantities) so I wouldn't worry about it. The biggest precision improvement would be to convert with more precise formulae (more than 3 significant digits)
Here's a good classic post on that theme: http://augustss.blogspot.com/2011/05/more-points-for-lazy-evaluation-in.html
Makes me want to build a table that's like a hybrid of chevron style and the haskell logo. 
&gt;this is the camp that would advocate for a `Maybe` return value in `div` I would consider myself in the "correctness camp" and this is not what I want. I want the type to be as /u/gelisam described regarding LiquidHaskell. For the reasons he/she described.
I didn't even notice the atom one (Symbol View: Go To Declaration, right?), but that does nothing for me.
A [Seussian take on parsers](http://www.willamette.edu/~fruehr/haskell/seuss.html): A Parser for Things is a function from Strings to Lists of Pairs of Things and Strings! ---- It helps me to first think about methods defined without their `newtype` newtype State s a = State { runState :: s -&gt; (a,s) } pureState :: a -&gt; State s a pureState a = State (\s -&gt; (a, s)) becomes type State s a = s -&gt; (a, s) pureState :: a -&gt; State s a pureState = (,) without type synonym pureState :: a -&gt; b -&gt; (a, b) pureState = (,) ---- [`Compose`](https://hackage.haskell.org/package/base-4.9.0.0/docs/Data-Functor-Compose.html) newtype Compose f g a = Compose { getCompose :: f (g a) } fmapComp :: (Functor f, Functor g) =&gt; (a -&gt; a') -&gt; (Compose f g a -&gt; Compose f g a') fmapComp f (Compose x) = Compose (fmap (fmap f) x) becomes much clearer: type Compose f g a = f (g a) fmapComp :: (Functor f, Functor g) =&gt; (a -&gt; a') -&gt; (Compose f g a -&gt; Compose f g a') fmapComp = fmap . fmap Infix: type (f · g) a = f (g a) fmapComp :: (Functor f, Functor g) =&gt; (a -&gt; a') -&gt; ((f · g) a -&gt; (f · g) a') fmapComp = fmap . fmap Without type synonym: fmapComp :: (Functor f, Functor g) =&gt; (a -&gt; a') -&gt; (f (g a) -&gt; f (g a')) fmapComp = fmap . fmap Same with -- foldMap f (Compose t) = foldMap (foldMap f) t -- traverse f (Compose t) = Compose &lt;$&gt; traverse (traverse f) t -- pure x = Compose (pure (pure x)) foldMapComp = foldMap . foldMap traverseComp = traverse . traverse pureComp = pure . pure ---- [`Const`](https://hackage.haskell.org/package/base-4.9.0.0/docs/Data-Functor-Const.html) trips people up newtype Const e a = Const { getConst :: e } fmapConst :: (a -&gt; a') -&gt; (Const e a -&gt; Const e a') fmapConst _ (Const v) = Const v pureConst :: Monoid m =&gt; a -&gt; Const m a pureConst _ = Const mempty apConst :: Monoid m =&gt; Const m (a -&gt; b) -&gt; (Const m a -&gt; Const m b) apConst (Const m1) (Const m2) = Const (mappend m1 m2) with type synonym type Const e a = e fmapConst :: (a -&gt; a') -&gt; (Const e a -&gt; Const e a') fmapConst _ = id pureConst :: Monoid m =&gt; a -&gt; Const m a pureConst _ = mempty apConst :: Monoid m =&gt; Const m (a -&gt; b) -&gt; (Const m a -&gt; Const m b) apConst = mappend without the synonym the essence emerges fmapConst :: (a -&gt; a') -&gt; (e -&gt; e) fmapConst _ = id pureConst :: Monoid m =&gt; a -&gt; m pureConst _ = mempty apConst :: Monoid m =&gt; m -&gt; m -&gt; m apConst = mappend
- `path` is widely tested (used in `stack` for example) If you don't want to pretend you know whether the path is `Dir` or `File` you can leave the parameter undetermined. Also `foo/` is relative directory path. At least on my machine this is invalid filename.
OK. But **are** there any performance benefits that come from lazy evaluation for real world software?
All good points, but I'm not seeing the difference between ORM and Persistent here. I've certainly seen OO projects that took the approach you describe here.
Yes, sometimes. If you have a large list, and your process turns out to only need the first quarter of the elements of that list, laziness makes it possible to not even calculate the remaining elements. Stuff like this is improved by laziness, though there aren't many other things I can think of that laziness improves the performance of.
&gt; Whether a path points to a file or a directory can not be determined from the path itself Probably `path` isn't strict in requiring trailing slash for directory names in conversion, but it could. &gt; Usage != testing it isn't, but wide usage does highlight problems (i.e. windows or weird file-systems). Also writing extensive doctests/properties is good thing, but one can write tests for existing packages as well. I'd rather try very hard not to create new packages which are almost like already existing. IMHO `path` solves problem well enough. Maybe I'll just port your test-suite to `path` and `pathtype` and you'll miss that advantage :P It's unfortunate that now I have to choose between `type FilePath = String`, `safepath`, `path`, and `pathtype`. One xkcd-strip (referenced in the `foundation` thread) comes to mind...
Sometimes they are. Compiler may float the value onto top level, so it will get memoised. Actually and unfortunately there are no reliable way to prevent that. But if you want to be sure, you'll have to float the value yourself explicitly. If some intermediate value depends weakly on input parameters (i.e. some equivalence class, like whether the `Int` is even or odd), I guess no compiler is sufficiently smart enough to do such kind of optimisations.
A way of looking at the main point in augustss's post is this: *you don't write the same code in a strict language*. In a strict language, instead of resolving the problem into what it was in your mind - namely, say, the composition of 6 well-tested well-understood combinators, you write a complicated bug-ridden recursive mess, because in a strict language those six functions would all involve the complete evaluation of some intervening structure. The corresponding composition sure as heck does perform 'worse' in a strict language - it typically diverges, exhausts memory, etc. so it doesn't cross your mind to write it. Thus if your question is "Does the transcription of *standard strict code* into a lazy language perform better?" it misses the point. Of course all this freedom with composition and direct intuitive correctness and intelligibility occasionally gives you trouble. In fact where you *do* write a complicated specialized hand-written recursive mess in Haskell it is easy enough to enforce strictness, as desired, because all the guts are exposed.
&gt; http://utf8everywhere.org/ Yes, exactly, that site. To be more explicit: That site is wrong. UTF-8 makes sense for programmers, as described there, but it is wrong for a general text encoding. Most text content is human language content, not computer programs. And the reality is that most human language content is in non-European languages. And it is encoded using encodings that make sense for those languages and are the default for content-creation tools for those languages. Not UTF-8. From what we see here, by far the most common non-UTF-8 encoding is UTF-16. Yes, we all know that UTF-16 is bad. But it's better than UTF-8. And that's what people are using to create their content. In fact, many feel that Unicode itself is bad for some major languages, because of the way it orders and classifies the glyphs. But UTF-16 is what is most used nowadays. The language-specific encodings have become rare.
I think the first two are kind of catch-22s really. No one wants to build the tools for haskell because they'd rather solve their own domain-specific problem that they're actually interested in. If Haskell was designed by Microsoft or Google then we'd have a nice IDE with a package manager that "just works." But then it would be designed by Microsoft and it would lack all the things academia gives us. 
I haven't had luck getting it to work nicely with lenses, unless I wrap in a newtype, which isn't all that desirable. But I'd talk to /u/edwardkmett about this, not me. I'm very much an uninformed bystander in this issue.
I suspect it also has a lot to do with your first bullet point (you need a newline between them for the list to work): if you read responses in the recent threads that talk about IDE's you see people very skeptical of the idea and claiming "my command line tools are good enough". As long as most of the community feels this way, it's unlikely the IDE problem will be addressed no matter what the release cycle.
Since `network`'s fraught relationship with windows was mentioned: We are now utilizing windows CI for `network`. Hopefully this will improve the situation in the future.
Regarding the IDEs, Leksah is my to go IDE for Haskell dabbling. No idea why it isn't mentioned, as I imagine it is well known.
&gt; Are the corresponding real world users different? Well according to the definition above, they would be different. But most of the time, you'd want two users with the same id to be equal. What if this discussion was not in the context of nested records, but a simple flat-record mapped to a single row in the DB? What if the use changes his date-of-birth while retaining the same ID? Does it make it a different user? When talking about *equality* in reference to records mapped to DB rows, dont we usually mean *identity*? isn't that why we have an an `Eq` instance that we can override to perform the equality check based on the PK, which is what we generally mean?
Shameless self-promotion: [`text-show`](http://hackage.haskell.org/package/text-show)
Good to know., given that I'm currently considering switching to servant+OpalEye and wasn't so sure what to do with the front-end. Is it possible to share any code between purescript and Haskell? Data structures at least?
Everyone reading this has already seen that XKCD:) We're in an exploratory phase with string libraries, we know we need an alternative to `String` in base but we're not sure what it should be yet. During an exploratory phase proliferating options is great -- the more experiments the better. EDIT: nicer
Is this like a in-memory query-cache? To lookup objects by PK to avoid going back to the DB repeatedly? Or is it a replacement for an RDBMS?
I don't feel ditching an RDMS is sensible advice. It's well tested proven technology. I'd much rather go with a safer choice for my data, and do all cuttting edge stuff with programming languages - back and and front-end.
Wouldn't it be possible to still have an operational semantics as a specification of the results? After all, many operational semantics use substitution instead of environments. In particular, there are (underappreciated) papers on call-by-need lambda-calculus defined using a small-step semantics. The real problem would be giving a cost semantics to Haskell, in the sense that Harper argues for, because of GHC's optimizations. You'd specify either a very conservative semantics, or a sufficiently smart compiler that you can't implement.
How can you avoid nesting completely. A simple use case of sending back a post JSON with 20 comments and related votes would result in a nested object. And the source of the data would be a single SQL query, right?
One detail that you haven't gotten yet is that you might use a `data` or `newtype` even when a `type` would do, if that `data` or `newtype` allows you to control where a type variable is instantiated, usually with `-XRank2Types` or `-XRankNTypes` set. This can be as simple as thinking about a GUI framework: suppose you don't want to force people to only choose GUI components from a single monolithic `data` declaration that you provide (containing `Button` alongside `VerticalLayoutPanel` alongside `DataTable` alongside whatever else you can think of), but instead you want people who import your library to define their own components. Then you might define some very simple semantics, "all I want out of a component is three things, it must first off be able to handle some state of type `s`; second I must be able to draw it into a rectangle of a given width and height; third if it is rendered in a rectangle `(w, h)` I must be able to handle mouse events on that rectangle, producing a new state as well as maybe some larger actions into some `GUI` monad where you can put 0 or more new events that you want to happen, read from global data sources in the GUI, fire e.g. networking events and whatever -- these will ultimately determine some new state. This is well-handled by Haskell and functional dependencies: class ComponentState comp state | comp -&gt; state where draw :: comp state -&gt; (Width, Height) -&gt; Picture interact :: comp state -&gt; (Width, Height) -&gt; MouseEvent -&gt; GUIMonad state setState :: comp state -&gt; state -&gt; comp state What's the problem? Well above I said that one thing I might want to define as part of any GUI is a `VerticalLayoutPanel` which arranges objects on top of each other. To create arbitrary tables, though, I want to be able to embed these in `HorizontalLayoutPanel`s and vice versa, as well as put whatever components I want in here. At some point we have to have a list of heterogeneous components, right? Is the monolithic `data` the only way to go? Heck no: newtype Component = Component (forall c s. ComponentState c s =&gt; c s) drawComponent :: Component -&gt; (Width, Height) -&gt; Picture drawComponent (Component c) = draw c Hey look, it's a single type which hides all information about which component you chose: just wrap it in `Component` and it will be good. data VerticalLayoutPanel heights = VerticalLayoutPanel heights [Component] instance ComponentState VerticalLayoutPanel [Height] where draw (vlp heights cmps) (w, h) = cropTo (w, h) $ [ translate (0, y) $ drawComponent c (w, c_height) | (y, c_height, c) &lt;- zip3 (scanl heights 0) heights cmps] interact = undefined -- ... handle clicks between elements, interpret as resizing ... All we really need these other components for is, we will give them a rectangle, and ask them to draw themselves inside of there. 
My ORM experience is with Active Record and I find it helps speeds things up. And if you know what it's going under the hood you know what NOT to do. Pretty much like `fold` and `foldl'`
If JSON counts as human readable for your purposes, then using Aeson would provide performant type-driven serialization and deserialization.
Disregard my previous answer, I found a much better solution. module Temperature ( Temperature , mkTemperatureLens , mkTemperature , fahrenheit , kelvin , celcius ) where import Control.Lens newtype Temperature = Kelvin Double deriving (Show, Read, Eq, Ord) mkTemperatureLens :: (Double -&gt; Double) -&gt; (Double -&gt; Double) -&gt; Lens' Temperature Double mkTemperatureLens fromKelvin toKelvin = lens (\(Kelvin k) -&gt; fromKelvin k) (\_ t -&gt; toKelvin t) mkTemperature :: Lens' Temperature Double -&gt; Double -&gt; Temperature mkTemperature tempLens d = set tempLens d (Kelvin 0) fahrenheit :: Lens' Temperature Double fahrenheit = mkTemperatureLens (\k -&gt; (k - 273.15) * 1.8 + 32) (\f -&gt; (f - 32) / 1.8 + 273.15) celsius :: Lens' Temperature Double celsius = mkTemperatureLens (subtract 273.15) (+273.15) kelvin :: Lens' Temperature Double kelvin = mkTemperatureLens id id Now you can add temperature scales with `mkTemperatureLens` which take two conversion functions, and you get a free smart constructor, `mkTemperature`, for each lens. The module is now follows the open/closed principle, which seems like a nice addition to a much simpler module.
&gt; Everyone reading this has already seen that XKCD:) It's still good though :). I think the foundation string looks good too.
Property lookups can return null, which is at least as inconvenient as Maybe. 
Yeah, this approach would probably generate the most human readable output. If you needed to kick it up another readability level, you can use the `yaml` package, which uses `aeson`'s typeclasses but provides an `encode` and `decode` that are almost a drop-in replacement for `aeson`'s functions of the same name. (I say almost because the variants provided in `Data.Yaml` target strict bytestrings, not lazy ones).
I suppose you are right. I just found these estimates and used them as an example.
It's true maps are designed for efficient access via key, rather than by value. The reverse direction is easy but not efficient. It doesn't have to be a map per se. You could even make your own bidirectional association just using two maps. Json cannot handle cycles, so in your example I'd be forced to use a flat representation. Otherwise I could end up with the same user twice. This happens pretty often and it's fine. This isn't to say I'd never nest Json but only for data that's genuinely tree shaped.
The only reason I haven't tried it is I have never been able to install it. I haven't tried for years, though.
Awesome! Most appreciated :)
In hindsight, "values" was the wrong word for me to use. What I really meant (and I'm sure Conal really said) was that Haskell memoizes *data structures* rather than functions.
I've used this and it worked great.
Simply import `Control.Category`. Now `#foo . #bar` won't pick your instance correctly and we're back in the situation above.
Maybe it was "I care about licenses" and "I care about open source" that was being upvoted. I was wrong. I admit that. However, you have to admit that for roughly 20 years, the name "Visual Studio" was attached to a product that *only* ran on Windows and was *not* open source. 
I have it running on latest Ubuntu LTS, but had to compile it from source and it took the same time as if it was a C++ application, given how underpowered my netbook is.
I'd really like a rundown of "usecases that a string type should cover and why" that runs through them all and then stacks the current solutions up against them. Its hard for me to even get a handle on what all the varying requirements are that we might want out of the "one string to rule them all".
In no particular order: * [Functional Pearl: Getting a Quick Fix on Comonads](https://github.com/kwf/GQFC/raw/master/GQFC.pdf) by Kenneth Foner Wasn't directly relevant to any of my interests, just really well-written and an enjoyable, mind-expanding read. * [Propositions as Types](http://homepages.inf.ed.ac.uk/wadler/papers/propositions-as-types/propositions-as-types.pdf) by Philip Wadler. Probably the best-written piece on "Curry-Howard" that I have read. * [A Unified Theory of Garbage Collection](https://www.cs.virginia.edu/~cs415/reading/bacon-garbage.pdf) by Bacon, Cheng, Rajan Shows that reference counting and tracing garbage collection are just two extremes on a spectrum, with most practical methods having elements of both. This has probably been fundamental to my understanding of memory management techniques ever since. * [Physics, Topology, Logic and Computation: A Rosetta Stone](http://math.ucr.edu/home/baez/rosetta.pdf) by John Baez, Mike Stay The first thing I read that made me feel like I understand what this category theory thing is about, even a little bit. Shows how categoric constructs can be used to abstract out the common structures in some very disparate domains. (My experience with other introductions I read was that they all started off with categories, then functors, then natural transformations, which were grokkable enough; and then they all either ended there, or proceeded abruptly into incomprehensibility.) Another I haven't finished yet, but am liking a lot: * [Interpreting types as abstract values](http://okmij.org/ftp/Computation/FLOLAC/lecture.pdf) by Oleg Kiselyov, Chung-chieh Shan I'm about halfway through. Probably the nicest, gentlest introduction to typechecking, unification, and Hindley-Milner that I have encountered so far. 
I think your last bullet point is a bit contrived. Presumably `NonZeroInt` would already be understood. Yes, you eventually have to read documentation, but we like to push these problems down into base libraries and commonly used frameworks rather than every little package out there in the wild.
In my experience, the problems with `network` on Windows are usually caused by system configuration and not the library itself - things like wrong shell (Cygwin instead of MSYS) and/or wrong version of gcc (not the one in $GHC_DIR/mingw/bin) being picked up by the build system. This applies in general to libraries with build-type: Configure and external dependencies.
That's a fantastic post -- thanks for sharing!
Hmm, do you know if `readable` has any means for generating `Readable` instances for new types?
In that case, yes, you would want to convert to some sort of nested structure, for sure.
Years to make it production quality? Yes. Tough to give it high performance? No. Not at all. I made a prototype that could manage 800,000 updates - not queries, **updates** - per second, and I built it in *an afternoon*. It's certainly not something I'd suggest for production code, and I don't use it even for myself. But I think I'd rather get it to that point than actually use a relational DB.
It hasn't. If you need human-readable form, you can try parsing your input with attoparsec, but you'd have to implement all instances manually. You can also try one of the bidirectional parsing libraries to avoid having to write the same logic for reading and writing, but I won't be able to help you here.
I think my favourite paper is and always will be [Burritos for the Hungry Mathematician](https://www.cs.cmu.edu/~edmo/silliness/burrito_monads.pdf).
If you'll treat it like binary, and won't have to deal with corrupt data, I take everything in my comment away. The error safety is important only when you have the possibility of getting non-conforming data, and the performance of the plain "deriving(Read)" instances is very consistent.
Machine code is even older and better understood. It's all ones and zeros with a few operations. Let's program using Machine code then
https://atom.io/packages/ide-haskell Read the suggestions
Nope, I struggled just to answer the question to be honest. I was just commenting for discussions sake.
&gt; Hi, I was using a book titled "Haskell for beginners", which didn't hold a full program from beginning to end. Wow, what a lousy book! Has my "hello world" example helped? Are you able to write and execute small Haskell programs yet? I think a good exercise would be to write a small program which reads an Integer from a file, doubles it, and writes it back to a different file. Once you manage to do that, we can start to think about doing the same thing with ginormous numbers :) &gt; Is there a book that you would suggest for me that would be more useful? [Haskell Programming from first principles](http://haskellbook.com/) has received a lot of praise. &gt; I am not interested in mezzanine prime but other classes of prime numbers. I am very familiar with Haskell, but I am not at all familiar with large primes. &gt; Thank you for always responding back. You're welcome! By the way, reddit archives discussion threads after a while, which makes it impossible to write more comments, so it might be wiser to discuss via direct messages instead of here. Simply click on my username and then on "send a private message".
Sure, you can have an operational semantics specifying the result. But what would that give you over a denotational semantics?
Most likely for better handling the immutable data structures that stick around for a long time. But for large objects that require low-latency and soft-realtime access, I think it might be a better idea to just provides some good libraries for creating and using custom allocators that work on `Storable` data structures. If you're getting into a problem space where you've got a lot of data sticking around for a long time in memory, and you need low latency, then you're probably at the point where you'd want to be controlling the allocation and memory management patterns of those objects yourself rather than hand it off to the garbage collector, which isn't tuned specifically to *your* data and access patterns.
https://github.com/atom-haskell I'd say these are the best, and since they're made by the same group, they should in principle all work together. 
It's almost like those types needs equality parameterized by an epsilon for Eq to be decidable, let alone useful ;)
Not to be an ass, but that doesn't fit the type signature, [nor is that first pattern/definition allowed](http://stackoverflow.com/questions/1179008/pattern-matching-identical-values). Thanks for the effort, though.
It was edited. It used to rely on unification. -- type sig baseCase a b _ a = b -- ...
I didn't have a ghc nearby :/ so i wrote it from the top of my head and only verified it some minutes later, that's why I edited it
Well then, you might like recursion schemes. They let you write recursive functions *without recursion*. quickSort = hylo toList fromList where toList EmptyF = [] toList (NodeF x l r) = l ++ x : r fromList [] = EmptyF fromList (x:xs) = NodeF x l r where (l,r) = partition (&lt;x) xs `hylo` is an `ana` followed by a `cata`. An `ana` lets you make a new recursive structure by only specifying one step. That is, instead of a `[a] -&gt; Tree a`, you can write a`[a] -&gt; TreeF a [a]` function. Since it's a 'tree' of lists, you can (or rather `ana` can) apply your 'list to tree of lists' function to the branches, which are lists. It will keep doing this until your function no longer provides branches from the lists it is given. `cata` is the opposite. We turn all the leaves into lists, then since it's a 'tree' of lists again, we can just append the branches. ... so, with `basecase` you could instead write quickSort = hylo (basecase EmptyF [] $ \(Node x l r) -&gt; l ++ x : r) (basecase [] EmptyF $ \(x:xs) -&gt; NodeF x l r where (l,r) = partition (&lt;xs) xs Which reveals some more symmetry! Let's abstract more: baseEquiv a b f g = hylo (basecase a b f) (basecase b a g) So now, you can write: quickSort = baseEquiv EmptyF [] (\(NodeF x l r) -&gt; l ++ x : r) (\(x:xs) -&gt; NodeF x l r where (l,r) = partition (&lt;x) xs) Or without lambdas because it's getting ugly again... quickSort = baseEquiv EmptyF [] toList fromList where toList (Node x l r) = l ++ x : r fromList (x:xs) = NodeF x l r where (l,r) = partition (&lt;x) xs In short, this hints at more possibility of duality. And it's in fact there, but I'm not really good enough at explaining it to go much further.
Mathematician here: Haskell arrays suck. It's not that the libraries themselves are bad, but rather that there is no canonical library - I always have to decide whether I want Data.Array, Repa, or Vector. Whichever one I pick is incompatible with code/libraries written with the other two. Each has different performance/clarity tradeoffs. FML
I don't know if this well help any, but I'll give it a shot. A fold (referred to as cata above) can be used to summarise a structure - like taking the sum of list, or counting the nodes in a tree, etc... - in which case it's going from a structure to a value. An unfold (referred to as ana above) can be used to take a seed value and build up a structure. If you're doing an unfold followed by a fold, and if you set things up right, the intermediate structures won't even need to be created - this is known as deforestation. It gets a little hairier in this case because the seed value and summarised value also happen to be structures. At least the intermediate structure isn't _also_ a list, then you have to really pay attention :) In the above example, the list is logically being sorted by converting it to a tree and back - except if deforestation is in play, it'll do this without creating any trees at all. So you get code that makes logical sense and is faster than you might expect. These two links have a bit more: [here](http://fho.f12n.de/posts/2014-05-07-dont-fear-the-cat.html) and [here](http://cs.stackexchange.com/questions/10129/how-does-deforestation-remove-trees-from-a-program).
So, if you would like more details... (I'm presuming you can read point free notation a bit, if you can't at all, let me know) To work with this, you change how you define nested data structures. Specifically, you have *only one* datatype that represents nesting a data structure within itself. You can now write functions on *that* data type to automatically produce recursive functions. It's simpler than it sounds: newtype Fix f = Fix {unfix :: f (Fix f)} cata f = f . fmap (cata f) . unfix ana f = Fix . fmap (ana f) . f hylo c a = cata c . ana a ... but also weirder than it looks. So `Fix` is the notion of the structure of inherently recursive data structures (in Haskell, in more pedantic languages, there are a couple depending on what kind of recursion you're talking about). Give it some way of talking about *things*, and it gives you a nested bunch of those things. Consider `Fix Maybe`. Valid values are `Fix Nothing`, `Fix (Just (Fix Nothing))`, etc, replacing `Nothing` with `Fix (Just (Fix Nothing))` to nest deeper. Note that there's never a `Just` that contains anything other than a value of type`Fix Maybe`. Maybe has become equivalent to the Natural numbers (`zero = Fix Nothing; addOne x = Fix (Just x)`) So if we want a *Tree*, what can we do? data TreeF val branch = EmptyF | NodeF val branch branch deriving Functor type Tree a = Fix (TreeF a) We just use more type variables. Also that Functor bit is important for letting us be this lazy. What do we have here? A `Tree a` is a `Fix (TreeF a)`. That means the `branch` type variable in `NodeF` is getting filled by a `Fix (TreeF a)`, AKA a `Tree a`. We have a branching tree again! So a `Tree Char` looks something like: `Fix EmptyF` or `Fix (NodeF 'a' (Fix EmptyF) (Fix (NodeF 'a' (Fix EmptyF) (Fix EmptyF))))` Why is all this worth the trouble? Look at `ana`, in the context of our quicksort. `ana fromList` takes a list and... * Applies `fromList`, giving us a `NodeF x l r`, where `x` is the head of the list, `l` is a list of items less than `x`, and `r` a list of items more. * `fmap`s `ana fromList` over this `NodeF`. This is why we made it a functor. And functors (in general in Haskell, but not *necessarily*) are functors *over the last type variable*. That'd be `branch`. So this applies `ana fromList` to the `l` and `r` 'branches'. We're traversing the tree! * `Fix`es it, giving us a Fix (TreeF a), aka a `Tree a`. In short, `ana fromList` builds an ordered (unbalanced, which is actually why quicksort is O(n^2) in the worst case) tree out of a list. So now that we have an ordered tree, it's really easy to get an ordered list. `cata toList` is next. It's type is `cata :: (f b -&gt; b) -&gt; Fix f -&gt; b` It: * `unfix` es it, to give us either an `EmptyF` or `NodeF x l r`. * `fmap`s itself over the result. For `EmptyF` this is a no-op. For `NodeF x l r` we'll get a `NodeF x l' r'` where `l'` and `r'` are now lists, since that's what `cata toList` produces. * Applies `toList`, giving us either the empty list from `EmptyF`, or the list `l' ++ x : r'` Note, the `EmptyF` directly becoming `[]` is really important. It gives us a short circuit exit (since there's nothing to `fmap` over) and is where the thing of type `b` can come from, to start filling things in from the bottom of our whatever. Also, `GHC` can optimise this to remove the tree, so it's never even built, and this is in fact just an operation on lists.
Haven't even looked at it yet, but... Awesome! To add a bit too this, I had to use Go rather than Haskell for a freelance client. The reason being the bad concurrency and performance story of mysql-simple compared to Go at the time. I hope this library changes that.
I use Arch and VSCode is great! 
The problem with extensive IDEs is that they require significant resources to develop and maintain. Mostly the big companies, such as Apple and Microsoft have the required resources to fully commit to IDEs. Even Google only introduced Android Studio three years ago, with support for Eclipse IDE ended last year. The rest have to resort to using plugins to IDEs developed by others. I believe that right now one of the best options to develop Haskell is to use Atom and assorted packages. starting with [ide-haskell](https://atom.io/packages/ide-haskell). The page mentions what you need to get started. Please get yourself acquainted with [Stack](https://docs.haskellstack.org/) if you haven't yet. As for figuring out how to open ide-haskell in Atom, either create a .hs file and then open it in Atom. Or click "Plain Text" in bottom right corner and change it to Haskell. However, please keep in mind that both command line tools and IDEs are there to help you. Some things are easier to do in the IDE, some are easier to do on the command line. Learning best of both worlds will make you more productive.
If I were you I'd post another question (maybe here, maybe StackOverflow) asking about that precise use case. Other people must be doing this! I'd be surprised if they did it in an adhoc way each time!
**woww** this is very impressive, I feel the power of composition and how much it is useful in functional programming like we build a basic parser function and keep composing it till we build more useful one and the with the use of monad instance we benefit from many functions as you stated `seq function`, it 's really awesome
Thanks, this is a helpful case to think about. It occurs to me that `RebindableSyntax` could give us a way out here: we should be able to declare `fromLabel = getField` locally so that labels get treated as field selectors directly, or indeed give other interpretations besides `IsLabel`.
Is there a compelling example of something you couldn't do in a _pure_ strict language, like Idris or PureScript? The best example Simon Marlow gave to me last time this was brought up was writing `zip [1..]` but faced with lazy exceptions, IO, implementation difficulty and performance issues of laziness, that's hardly compelling.
Turns out I hadn't installed `ide-haskell-hasketags`, which enables the symbol view. Nice! Symbol View: Go To Declaration still doesn't work, but with `ctrl+shift+R` that's a only a minor gripe. 
For sure. Ballmer was a disastrous CEO and Gates was never known for his sense of fair play. But MS seems to be changing direction now, who knows. All I know is the free, multiplatform application they are now offering appears to be the be the quickest solution to achieve a high quality IDE for Haskell.
Well, `mrkkrp` is not a contributor to `path` (though is author of `path-io`), so it's not only his call to decide. Depending on `validity` is difficult thing (people have full range of opinions on the dependencies), but I like your points.
Thanks! :)
I nearly wrote out an answer to this, but then I thought that perhaps the question has no proper answer because it is relying on analogy. Monads are not "a computation that produce a value", though some monadic values can be thought of in that way. It's not a bad question, however. Can you expand on it? Do you other, related questions, perhaps about specific cases?
&gt; A Program to Solve Sudoku It's also freely available as a Functional Pearl : http://www.cs.tufts.edu/~nr/cs257/archive/richard-bird/sudoku.pdf
Did you not have the option of using a more proper database? Use of MySQL should be avoided if possible because there are always superior choices, whatever the requirement. EDIT: OP, please don't take this as disparaging of *your* work. Some of us *are* forced into MySQL development for political reasons (as may have been the case for codygman) so it's nice to have better tools when that is the case.
What do you mean by "computation", "value", and "produce"? Please be as specific as possible. Edit: Downvoters: It was a title-only submission when I made this comment.
You are thinking of [monads as computation](https://wiki.haskell.org/Monads_as_computation). That is not the only way to think of a monad, and for some monads that doesn't even make much sense. I'm not going to write yet another monad tutorial here, or even link to existing ones; I'll just go along with your "computation" approach. There is only one use of the "value" produced by a monad "computation" that all monads have in common: you can pass that value to another "computation" in the same monad, which in turn produces a value which might have a different type. The method which does that has this type signature: (&gt;&gt;=) :: Monad m =&gt; m a -&gt; (a -&gt; m b) -&gt; m b The symbol `&gt;&gt;=` is pronounced "bind". In English, this signature says: given a "computation" which produces an `a`, and a way to take a value of type `a` as input and get a "computation" which produces a `b`, we can combine those to get a "computation" which produces a `b`. For some monads, there is a way to "get the result value out of the monad" after the computation is done. Or if not to get the value itself, perhaps a way to use that value for something else, other than just feeding it to another monad "computation" in the same monad. But this is not required, and not all monads provide any way to do that. For those that don't - the final value is discarded if you don't feed it to yet another "computation" in the same monad. An example of a monad that does allow you to get the final result value is the State monad. After doing a computation using mutable state, the State monad gives you a way to examine the final result value (`evalState`), or the final value of the mutable state (`execState`), or both (`runState`). An example of a monad that does not allow you get the final result is the IO monad. If you get a value using IO, such as by reading an input from the user, the only way to use that value is to do another IO operation, such as showing something on the screen or writing something to disk. If you do not use the value to have an effect on the outside world in any way, the value is completely lost, other than perhaps dissipating a bit of heat from the CPU.
You might find this definition in Joy interesting: DEFINE qsort == [small] [] [uncons [&gt;] split] [enconcat] binrec. `binrec` is a combinator for binary divide-and-conquer. In Haskell it would be something like: binrec test base divide conquer = go where go input | test input = base input | otherwise = let ((ls, rs), x) = divide input in conquer (go ls) (go rs) x Then you can define things like QuickSort: quicksort = binrec small id (\ (x : xs) -&gt; (partition (&lt; x) xs, x)) enconcat enconcat as bs x = as ++ x : bs small [] = True small [_] = True small _ = False Or Fibonacci numbers: fibonacci = binrec (&lt;= 1) (const 1) (\ x -&gt; ((x - 1, x - 2), x)) (\ a b _ -&gt; a + b) Factoring out recursion combinators and making things pointfree is good in moderation, but it’s easy to make things unreadable by going overboard. 
question edited, hope its more clear now
You should edit it again, since it's full of typos. Edit: Downvoters: it was, in fact, full of typos.
Yep, see the details here: https://github.com/haskell/network/pull/198. Only 64-bit windows though. Is 32-bit still an important target in windows land?
Uber recently switched from postgres to mysql edit: Interesting replies, cheers! 
In a similar vein, I like [Clowns to the Left of me, Jokers to the Right](http://strictlypositive.org/CJ.pdf). I have seriously considered seeing if I can _somehow_ work a free monad into the structure so I can write a brief paper titled, 'Stuck in the middle with μ'.
They are distinguished by type: if `m` is a monad then `m a` can be viewed as the type of computations, producing a value of type `a` by performing side-effects specified through `m`. ## Example Let's look at a small example using the `Maybe` type constructor: data Maybe a = Just a | Nothing Let's consider a function, which takes a list and returns `Just` the first element of the list, or `Nothing` if the list is empty: maybeHead :: [a] -&gt; Maybe a maybeHead [] = Nothing maybeHead (x:xs) = Just x One way to look at `maybeHead` is to think about it as a computation, which takes a list of type `[a]` and returns an element of type `a`, possibly failing if the list is empty. This possibility of failing is sometimes called the side-effect of the `Maybe`-computation. The `Maybe` type forms a monad, which means that there is a special way of composing functions such as `maybeHead`, called *Kleisli Composition*. For the `Maybe` monad that looks like this: (&gt;=&gt;) :: (a -&gt; Maybe b) -&gt; (b -&gt; Maybe c) -&gt; (a -&gt; Maybe c) f &gt;=&gt; g = \x -&gt; case f x of Nothing -&gt; Nothing Just y -&gt; g y Consider we have a list of list of integers, e.g. xss :: [[Int]] xss = [ [1, 2, 3], [4] ] from which we want to return the first element (if it exists) of the first element (if it exists). In this case that would be `1`. We can achieve this with our monadic composition operator as followed: maybeHead2 :: [[a]] -&gt; Maybe a maybeHead2 = maybeHead &gt;=&gt; maybeHead x :: Maybe Int x = maybeHead2 xss So if `m` is a monad, then we can compose functions of type `a -&gt; m b` ("computations", "functions with side-effects of type `m`"), just as if they were regular functions of type `a -&gt; b`. ## Formalities Formally, in Haskell something is a monad iff we can describe a Kleisli composition which * is associative, meaning that `(f &gt;=&gt; g) &gt;=&gt; h` is equivalent to `f &gt;=&gt; (g &gt;=&gt; h)`, and * has a neutral element `n`, which has no effect on the computation, meaning that for all functions `f` it holds that `n &gt;=&gt; f` is equivalent to `f` is equivalent to `f &gt;=&gt; n`. In the special case of the `Maybe` monad, the neutral element is just the constructor `Just :: a -&gt; Maybe a`, which can be viewed as taking an `a` and returning the same `a`, always without failing. ## In Haskell In Haskell, monads are specified slightly differently. The `Monad` type class provides the following functions: return :: Monad m =&gt; a -&gt; m a (&gt;&gt;=) :: Monad m =&gt; m a -&gt; (a -&gt; m b) -&gt; m b `return` is the neutral element and `&gt;&gt;=` is almost the Kleisli composition. In fact the Kleisli composition is defined in `Control.Monad` as f &gt;=&gt; g = \x -&gt; f x &gt;&gt;= g - - ^oh ^my ^glob... ^almost ^yet ^another ^monad ^tutorial ^by ^accident ^;3
Ah, but a parser for a thing, may be a function from a string, to a list of pairs of things and monoids actions on strings Which I think might improve the rhythm slightly, and [has a few other minor benefits](https://www.schoolofhaskell.com/user/edwardk/heap-of-successes).
Again, I think it's great that you took on this project as people can always find themselves needing to use MySQL. But that comparison does not change my opinion that no one who *can* choose ever *should* choose MySQL. I have been simplifying my answers to save space, but the reason for my belief is that, in my experience, any use case where one could use MySQL there is a superior solution. If you want a relational database, there are better ones. If you just want a key-value store, there are better ones. If you want a key-value store with an SQL front end (for some reason...) MySQL might be a consideration then, but I would still expect to find something else to be better. If you want a key-value store for your Schemaless system (as Uber did), just use a proper Schemaless system instead of making a new one (unless that's your business model, in which case I would advise against building your system on top of MySQL).
**awesome** response *thanks*, so when I want to define any monad ( lets say parser monad) it should satisfy the 2 rules mentioned above in the `formalities` section?
I made an edit to my top post that nets this benefit, the 2*n scaling. It also makes the module externally extendable, by giving a helper function to make more lenses and it makes grants a "free" smart constructor per lens.
&gt;Then VS Code falls into the same bucket as Emacs, Vim, Atom etc. in terms of selection. Well, closer but each of those requires plugins (some of which the haskell community would have to maintain) to gain functionality. For VS Code, we literally need only write a server that speaks the VS Code protocol and nothing more. The plugins that would have to be written (and to be fair: they already exist, but our community has to maintain them) are part of the VS Code system and not our responsibility. The server must be written in any of the currently conceived solutions, but in every other solution a few other things have to be written/maintained as well.
Thanks for taking the time to give the comparison.
While not a research paper, *Type-Level Instant Insanity* from [issue 8 of The Monad.Reader](http://www.haskell.org/wikiupload/d/dd/TMR-Issue8.pdf) is something that you might be interested in: a delightfully straightforward introduction to type-level Prolog programming via multi-parameter type classes with functional dependencies. Also, I think the article's age can be a plus here, because it gives you a set of exercises after you read it, in porting it to use modern GCH features like type families and data kinds.
yes I can see :p and the format is more readable than before... 
Well, there you're going out to Double, which makes it a much more dangerous convention. My proposal avoids 'casting through Double'. e.g. newtype Kelvin = Kelvin Double newtype Celsius = Celsius Double newtype Fahrenheit = Fahrenheit Double makePrisms ''Kelvin makePrisms ''Celsius makePrisms ''Fahrenheit class Scale t where kelvin :: Iso' t Kelvin instance Scale Kelvin where kelvin = id convert :: (Scale s, Scale t) =&gt; Iso' s t convert = kelvin . from kelvin celsius :: Scale t =&gt; Iso' t Celsius celsius = convert fahrenheit :: Scale t =&gt; Iso' t Fahrenheit fahrenheit = convert instance Scale Celsius where kelvin = _Celsius . iso (subtract 273.15) (+273.15) . from _Kelvin -- i might have this flipped ... Now you just need to write temperature instances and you have type safe conversions, without the risk that the user casts out to double and back to a different temperature without normalizing correctly. Getting the double out is supported by the _Foo isos that are made by makePrisms for the newtypes, but isn't used as the primary conversion mechanism. Using `Iso` lets us flip around and use `from` whenever we want to go the other way. You don't have to have a 'dummy' temperature to set it to a given value.
Looks awesome. Have you considered using `Data.ByteString.Builder` instead of`Data.ByteString.Lazy` plus lists to build up your bytestrings? It's not exactly a drop-in replacement but you're building up bytestrings, so my guess is it'd do the job.
You might also want to consider integrating this with hasql if that's applicable. I remember reading a great walkthrough of performance and database bindings from that developer.
&gt; although the former is so simple that it can be easily motivated outside of category theory and the latter is somewhat obscure outside of logic and computer science. Agreed. Monads are, to an algebraist's first glance, just a group with less structure. And getting a good command of groups, then a good command of groups axiomatized by category theory simply won't connect to a good understanding of monads in haskell.
Responding to something said above by someone else, but you might be a better person to actually answer: &gt; main difference is that one uses UTF-16 while the other uses UTF-8 As described [here](https://jaspervdj.be/posts/2011-08-19-text-utf8-the-aftermath.html), converting Text to UTF-8 seemed to actually run slower and not even save space in lots of common cases. It would also require porting [text-icu](https://hackage.haskell.org/package/text-icu) which is built on top of the icu libaries, which themselves generally work better with UTF-16. Does Foundation.String use icu, does it duplicate the functionality or are those features just missing?
Even worse, when you get the user's input and try read "0" :: NonZeroInt you get back *** Exception: Prelude.read: no parse which is still an exception and way less descriptive. 
I admire the fact that the haskell community has rallied around postgresql, but it doesn't affect the fact that my company is running six year old installs of mysql and will never upgrade (ever, seriously). Due to that, mysql-simple is unfortunately the only way I can write tooling in haskell here. More power to the OP.
Many particular domain problems are best expressed in terms of naturality and commuting squares. As one particular instance, one of the most basic refactors in FP is moving from some type `a` to some type `b -&gt; a`, and categorical reasoning about presheaves is very helpful in this regard. another place where it comes into play is adjointness in syntax and semantics, where categorical reasoning is ubiquitous in writing parsers, interpreters, dsls, etc.
Here's a [translation table](http://comonad.com/reader/2009/recursion-schemes/) from the greek names to their function. [recusion-schemes](https://hackage.haskell.org/package/recursion-schemes) is the main hackage package you should investigate. There's a [couple](http://comonad.com/reader/2008/generalized-hylomorphisms/) [more](http://comonad.com/reader/2008/dynamorphisms-as-chronomorphisms/) articles that used greek terms that never got added to the table linked before. Dynamorphisms are one approach to do dynamic programming with only referentially transparent mutation. I recommend starting from [fold](https://hackage.haskell.org/package/recursion-schemes-4.1.2/docs/Data-Functor-Foldable.html#v:fold)/[cata](https://hackage.haskell.org/package/recursion-schemes-4.1.2/docs/Data-Functor-Foldable.html#v:cata) and [unfold](https://hackage.haskell.org/package/recursion-schemes-4.1.2/docs/Data-Functor-Foldable.html#v:unfold)/[ana](https://hackage.haskell.org/package/recursion-schemes-4.1.2/docs/Data-Functor-Foldable.html#v:ana), but eventually everything turns into [ghylo](https://hackage.haskell.org/package/recursion-schemes-4.1.2/docs/Data-Functor-Foldable.html#v:ghylo).
You forgot HMatrix too, as well as the blas stuff. I'm genuinely unsure what the right "one to rule them all" is here -- I tend to feel like Vector is unilaterally better than Array, and meanwhile there's still no best worked out approach for linear algebra, and it would be nice to have a single agreed on substrate that most libs could use (but unfortunately Array isn't it).
It's not unheard of, but it will make your code less readible to other Haskell programmers. Also, it exists already, so please don't define it yourself: https://hackage.haskell.org/package/base-4.9.0.0/docs/Data-Function.html#%26 See `&amp;`
The notation that is usually used in Haskell is derived from the one in mathematics, where some mathematicians (specially category theorists) consider that writing f(x) instead of (x)f is one of the biggest historical "errors" of mathematics. Personally, I prefer the style you proposed, and it happens that monadic appication `&gt;&gt;=` is in the order you proposed, but generally it is not as applied. In the end, its a matter of the community, I don't know if it is too late for this change to happen (as people believe is the case in mathematics).
I was at this talk, and it made me think of Lisp. One of the killer features that Lisp fans proclaim is the ability to live-reload and edit code, because it's all just a REPL. This isn't that interactive, but it's in a similar vein.
(.$.) is exported as (&amp;) from Data.Function. 
there's `=&lt;&lt;` too, tho
Esqueleto and opaleye both use tuples to do joins. This is not totally faithful to what the type of join is in an SPJR or an SPC relational algebra. Let's consider two relations in an SPJR algebra: people : {person_name : String, person_age : Int, person_id : Int} dogs : {dog_name : String, owner_id : Int} This syntax is made up and sort of resembles the syntax for the types of records in elm or purescript. What would the type of this expression be? crossJoin people dogs It should be: {person_name : String, person_age : Int, person_id : Int, dog_name : String, owner_id : Int} Because the type of a cross join is more generally: crossJoin : Disjoint as bs =&gt; as -&gt; bs -&gt; as ∪ bs Extensible records (and by extension, relations from relational algebra) are not handled super well in haskell. So, for cross join (and equijoin, since it's just cross join followed by restriction), almost all SQL libraries in haskell type it like this: crossJoin : as -&gt; bs -&gt; (as, bs) This ends up being ok for the most part. However, one notable deficiency is that natural join cannot correctly be represented by this approach. To go a little further, consider projection: project : as ⊆ bs =&gt; bs -&gt; Proxy as -&gt; as How do most SQL libraries in haskell handle this? Since we don't have records in the sense that purescript and elm have them, we end up using tuples. But this time, it's a little more annoying. What if there was only one field you wanted to remove from `bs` and it had ten fields? The type of your result will be a 9-tuple.
I like this style, but the community at large does not. See [this post](http://taylor.fausak.me/2015/04/16/on-the-reaction-to-flow/) for some discussion and links to relevant stuff. 
Nice reply. You explained it very well. Is Either good for the job? Although types is the obvious and maybe even correct choice since it can also help with validation etc'
Don't be wrong, I'm not arguing with you. I answered just to convince others, not to waste my time with someone who says "No" with no arguments. No arguments, no answer
I'd argue not needing an IDE is a plus. Any language or library that is so large and daunting it requires a special tool (besides a text editor and command line compiler and/or repl) is a turn off. I almost immediately don't consider it (the language) an option for any project if I need to go to such extremes to use it in the first place. This is one of the beauties of Go, C, Python, etc (in my opinion). It's so easy to just get started doing something, regardless. Caveat: I'm a completely new to Haskell, know hardly anything about the language, only started reading the recommended books, but I like to have fun it and potentially learn it as I can. From what I have done with it, I love it. Some day... Some day... sigh.
I'm currently interning at Myrtle - positions do exist :-)
I've got a pretty sweet job programming Haskell with a bachelors degree. Try applying on the off chance for some of these jobs
[removed]
It's an interesting UI concept that sounds like it would be pretty awful to actually use, but I'm all for trying wacky ideas and teaching others!
There are plugins for vim and emacs that can do most—if not all—of those things. The capability gap between traditional IDEs and traditional editors is growing smaller all the time.
Yes, very important I'm afraid :(. But the number of 32bit vs 64bit Windows breakages will be far lower than the number of Windows vs Linux ones. 
This is called pipe, and it's very common in F# (and elm) written as (|&gt;) and for the other direction (&lt;| which is the same as $). I personally write all my code (in F#) using the pipe approach however you put newlines between, so it becomes: [1 .. 10] |&gt; map ((+) 5) |&gt; foldBack (+) 0 I personally think it's a shame it itsn't more widely adopted in haskell also on a sidenote bind also gets easier as it stops being a special symbol and instead is just treated like any other function. [1 .. 10] |&gt; map ((+) 5) |&gt; bind (fun a -&gt; [a; a + 1]) In bash it's just "|" like when you write: cat log | grep failed 
https://www.youtube.com/watch?v=zpsYJrGb_yI
This style is common and reccomended in Elm. We use &lt;| for $, and |&gt; for your operator. 
I have an awesome Haskell job (working on supply chain optimization) and I didn't even complete my undergraduate degree :).
Oh, are you using `fgl` at any scale? One side-project I've been fiddling with on and off is an alternative to `IntMap` that's more efficient (at least at larger sizes) which would also presumably help `fgl` code. I'd love to hear that that would actually be useful for people!
`HLists` give you a good model for relational algebra. data HList f xs where Nil :: HList f '[] (:&amp;) :: f x -&gt; HList f xs -&gt; HList f (x ': xs) data Index xs x where Z :: Index (x ': xs) x S :: Index xs x -&gt; Index (y ': xs) x data Expression schema x where ATTRIBUTE :: Index schema x -&gt; Expression schema x AND :: Expression schema Bool -&gt; Expression schema Bool -&gt; Expression schema Bool .. data Relation schema where TABLE :: [HList Identity schema] -&gt; Relation schema FROM :: HList (Expression schema1) schema2 -&gt; Relation schema1 -&gt; Relation schema2 WHERE :: Relation schema -&gt; Expression schema Bool -&gt; Relation schema UNION :: Relation schema -&gt; Relation schema -&gt; Relation schema JOIN :: Relation schema1 -&gt; Relation schema2 -&gt; Relation (schema1 ++ schema2) 
This seems like a minor objection to me. As a user I'd be fine defining a separate record type for the two inputs of the join and a type for the output: data Input1 = Input1 { input1PersonName :: String , input1PersonAge :: Int , input1PersonId :: Int } data Input2 = Input2 { input2DogName :: String , input2OwnerId :: Int } data Output = Output { outputPersonName :: String , outputPersonAge :: Int , outputPersonId :: Int , outputDogName :: String , outputOwnerId :: Int } ... and then making the result of a database query an `Applicative`, so that I could do joins using `Applicative` operations: {-# LANGUAGE RecordWildCards #-} exampleJoin :: Applicative f =&gt; f Input1 -&gt; f Input2 -&gt; f Output exampleJoin = liftA2 adapt where adapt (Input1 {..}) (Input2 {..}) = Output {..} where outputPersonName = input1PersonName outputPersonAge = input1PersonAge outputPersonId = input1PersonId outputDogName = input2DogName outputOwnerId = input2OwnerId Same thing for projection. If the result of a database query is a `Functor` then I'd just `fmap` whatever projection function I want: exampleProjection :: Functor f =&gt; f Output -&gt; f Input1 exampleProjection = fmap adapt where adapt (Output {..}) = Input1 {..} where input1PersonName = outputPersonName input1PersonAge = outputPersonAge inputPersonId = outputPersonId I think all the current database libraries are trying too hard to be fancy and clever that those of us who are willing to do the explicit and verbose thing are really underserved.
`(&gt;&gt;=) ` is flipped because that argument order corresponds to its do bind sugar (`&lt;-`).
It's flipped to correspond with do binds.
Is VS Code a superior editor to emacs and vim? If not, I think people will be willing to put in the small bit of extra effort to write plugins. I'd love a superior editor to neovim, but I've not used one, and I'll continue working on Intero neovim in the meantime
This is exciting 😂
If you're getting into units of measure, check out https://hackage.haskell.org/package/units-defs-2.0.1.1 https://hackage.haskell.org/package/units-defs-2.0.0.1/docs/Data-Dimensions-SI.html#t:Temperature https://hackage.haskell.org/package/units-defs-2.0.0.1/docs/Data-Units-SI.html#t:Kelvin You can add your own units, just express them in terms of the base unit (which is Kelvin for temperature).
&gt; The idea that functions can be thought of as purely the data of their "lookup tables" and also that "lookup tables" can be compressed into functions When optimizing hot sections of code, both sides of this coin are used. Functions are accelerated by replacing computation with a lookup table. Large lookup tables are compressed by replacing some or all of the table lookup with computation. The end result is often a hybrid: perform a lookup in a smaller table, and then use that result in a shorter computation.
And not only because *benchmarksgame* ask for it: &gt; We ask that contributed programs not only give the correct result, but also use the same algorithm to calculate that result. *EDIT* well, maybe it could be written more idiomatically without sacrificing performance. But even then, you'll need to check `Core` and probably make "not-so-elegant" amendments, so that obvious performance eaters are gone.
hi again, I was trying to define the `many` Parser(using `bind` not monad comprehension) which is a parser that take a parser as parameter and return Parser [a] and has the below signature : many :: Parser a -&gt; Parser [a] I defined it as the below : many p= p &gt;&gt;= \x -&gt;many &gt;&gt;= \xs -&gt; succeed (x:xs) even I am not convinced with above definition since the `parser p` run and capture the consumed input `x` which is passed as parameter for the next parser `many` ( as I think it should not be passed to the second parser many since `many` should take as parameter only the new string to parse ) **NOTE 1**: when I have tried the this parser it always fail and return an empty list **NOTE 2**: I didn't ignore the first consumed input returned by the first parser `p` since I believe that I have to use it in the result to construct the final list `succeed(x:xs) could you please help me with the above!
Yes, that is why I stop wasted my energy and conceded a defeat :D
Do you sponsor visas ?
It is used with lenses. Instead of writing someLens .~ someVal $ someRec You often see it written as someRec &amp; someLens .~ someVal The reason for this is chaining. Without `&amp;` you'd have to write (lens3 .~ val3) . (lens2 .~ val2) . (lens1 .~ val1) $ someRec but with `&amp;` you can instead chain them as so: someRec &amp; lens1 .~ val1 &amp; lens2 .~ val2 &amp; lens3 .~ val3 I guess it's a matter of personal preference, but the examples in `Control.Lens` use it extensively. It makes it read a bit like setter methods in OO, that returns the object it self, making the setter methods chainable: obj.setVal1(val1).setVal2(val2).setVal3(val3);
Being right of wrong, there is a good mnemotechnic rule for conventional haskell code: pure equational and lazy code flows left while algorithmic code flows right
Very much depends on how you define "superior". For sure it's more approachable. For me personally, I used vi[m] for years but after 5+ years of working with it daily I was still commonly making mode errors. Then I read a paper about mode errors which was a strong critique against how vi* work so I switched to Emacs. At first I was enamored with the power and the fact that it was fully programmable, but at some point all the chording became a chore and I realized I didn't need 90% of the functionality I was getting. In my day job I had to switch to Visual Studio+Resharper and C# and after a bit of learning how to get the most out of the IDE it became really hard to consider anything else. I haven't really used vi or Emacs since. And also, what you're calling "a small bit of extra effort" is not. Writing the extra plumbing might be easy, but it becomes a maintenance burden from now on.
Don't take my comments as disparaging of your work. I just wanted to know how you ended up with different results than the previous group that tried this, with regards to strings. As far as what you say about benchmarks making UTF-16 look bad, I'm sure you can. The big question for me is: how are you getting the features provided by ICU bindings? Did you duplicate the library or are they just not present? It's true that you can simply convert to UTF-16 where needed (in fact I think the ICU API does this for you) but the library is more UTF-16 optimized than not. Depending on the use case, the conversions back and forth could end up dominating performance. I've heard people say UTF16 is awful, I don't know enough myself to have an opinion about it.
We're definitely open to junior roles if people are excited about what we're doing!
Thanks!
yep. its a little hard to follow - i take a screen shot so i can keep it on screen while installing. but thats the best recipe ive found. 
ive wondered why they cant be auto installed as dependencies of ide haskell. APM probably lacks that kind of functionality. 
Our compiler is solving quite a different problem to CLaSH; we're focusing on making image/signal processing code easy to get into a very efficient hardware design with many specific compiler driven optimisations. I can recommend using CLaSH for the more general aspects of FPGA development :)
But no. Problem was that my p consumed eof before eof had a chance to act
Oh dear, my brain (memory) has betrayed me. I think what I had in mind was [this](https://www.reddit.com/r/haskell/comments/2nxx7n/announcing_opaleye_sqlgenerating_embedded_domain/cmhyr17) possibly coupled with misinterpretation of some statements in the tutorial. But I've always wondered if there was another solution to deal with the aggregation you talk about in that post than arrows.
I have the intuition that pure code uses a commutative algebra, while "algorithmic code" means for me loosely like a non commutative algebra, where order is important. It is true that a IO action can be generated by a pure code, but when the action is executed, the code can not be pure, since order matters due to effects. It can simulate the aspect of a pure function in impure languages but it is not a function. But I may be wrong in some way.....
Anybody's welcome to submit a better solution if they have it.
Declaring some types (with each field prefixed with redundant information) every time you want to make a join is _busywork boilerplate_ of the kind people mock Java for, and probably contributory to "static types get in the way" attitudes of people who prefer dynamic typing. With my trivial [labels](https://github.com/chrisdone/labels) package I can get something way less verbose: {-# LANGUAGE OverloadedLabels, TypeOperators, DataKinds, FlexibleContexts #-} import Labels employees = [(#name := "Dave", #age := 32, #id := 0001) ,(#name := "Mary", #age := 24, #id := 0002) ,(#name := "Penny", #age := 42, #id := 0003)] dogs = [(#name := "Patch", #owner := 0003) ,(#name := "Mr Wiggles", #owner := 0001) ,(#name := "Tesser", #owner := 0002)] relationships = [(#dogName := get #name dog, #ownerName := get #name employee) |dog &lt;- dogs, employee &lt;- employees ,get #owner dog == get #id employee] Types all work out statically: &gt; :t relationships relationships :: [("dogName" := [Char], "ownerName" := [Char])] &gt; relationships [(#dogName := "Patch",#ownerName := "Penny") ,(#dogName := "Mr Wiggles",#ownerName := "Dave") ,(#dogName := "Tesser",#ownerName := "Mary")] Hugs's TRex supported this kind of thing, and is why the original HaskellDB was based upon it. In GHC-land we're not there yet, and it'll likely take a couple years. 
I agree that for a lot of cases, what you have suggested works fine. As far as I can tell, that's basically what opaleye does, except that it builds the AST for a query instead of computing the results directly. One disadvantage of your solution is that it becomes harder to reuse a restriction predicate. For example: restrict :: xs ⊆ ys =&gt; Relation ys -&gt; Predicate xs -&gt; Relation ys You have to do more of the lifting manually with your approach, which probably does not bother you. &gt; I think all the current database libraries are trying too hard to be fancy and clever that those of us who are willing to do the explicit and verbose thing are really underserved. I definitely agree. I would not say that I fall into the category of "those of us who are willing to do the explicit and verbose thing", but you're right that there is no library that takes the explicit but boilerplate-heavy (in the presence of lots of joins and aggregation) approach you've suggested.
Off topic: the people of the benchmark game have ruined his reputation with the latest interface. Previously was incredibly informative. But some genius of design has destroyed the site and now it is unusable. In the next version the pages will appear completely blank. It will be a clean minimalist design with no content.
In very specific situations, I like using natural join. Natural join is not possible without a correct representation of relations. Reusing restriction predicates is something I would like to be able to do. If I have some predicate: dog_name = "Fluff" &amp;&amp; person_name = "Derp" I want to be able to do restriction on any relation with column `xs` such that `{dog_name, person_name} ⊆ xs`. I know that opaleye provides a way to select the columns you want and then feed them into a predicate, but that a step I do not want to have to do.
In this example, join doesn't stop you from creating a relation with columns that share the same name. But other than that, yeah, it's a pretty decent representation of RA.
Is modelling RA enough though? [My understanding](https://www.quora.com/Is-SQL-relationally-complete) is that SQL is a superset of RA. So wouldn't that mean that a good RA representation wouldn't be able to represent everything that an underlying database system would be capable of?
So how did you go from writing a hangman game to optimizing loops, analyzing space leaks, etc? My impression is that Haskell knowledge isn't really useful unless you know how to reason about performance. And it's much easier to reason about these things in non-Haskell languages.
Yeah. We developed a lot of array stuff prior to having nice type/data families and a clean rethinking of APIs in light of the latter would probably make progress (repa already points some of the way here).
This is an extremely simplified form of SQL/RA. Lots of things like NULL are missing that add complexity. In this example, there _are_ no names, just numeric indices like ATTRIBUTE S(S(Z)).
It is a fairly recent addition.
Oh I see.
Please make specific observations about what you were trying to find on the website, what actions you took and what problems you encountered. Previously: - too difficult to use on touch screens - did not prioritize the few pages that the vast majority look-at - demonstrated that instant without-thought comparison was *Not A Good Thing^TM* - heavy pages == slower-downloads
Some. The job is about the tooling, new dev tools, scaling up the testing system, the CI system. Need a Haskell-focused tool builder who knows git.
I think it's a good solution to the demand for progress.
lol
But do we really want Turing-complete configuration file languages?
Ok, so "algorithmic code" has nothing to do with algorithms. Got it. Anyway, for working with monads without do notation I often prefer `(=&lt;&lt;)` to `(&gt;&gt;=)` exactly because the former has symmetry with `($)` and `(&lt;$&gt;)`. I don't think the "convention" you mention is anything more than a tendency to choose `(&gt;&gt;=)` because it is part of the class, involved in do notation desugaring, and most importantly is the thing that most people learn first. I don't think there's any left-to-right bias as a general principle.
For example, on the [Haskell comparison page](http://benchmarksgame.alioth.debian.org/u64q/haskell.html) click any of the underlined task names, such-as *n-body*.
Why would you use `units` over, say, [dimensional](http://hackage.haskell.org/package/dimensional-1.0.1.2)?
That did not work just one hour ago
A solution to model User and NewUser or any other combination of optional field is to use type family and phantom type data User n = User { name :: String, createdAt :: NewField n DateTime } data IsNew = Existing | New type family NewField (n :: IsNew) where NewField Existing a = a Newfield New a = () type NewUser = User New That way, functions which works on new and existing users just have to use `User n`, whereas function which need a particular one just use `User New` or `User Existing`. I have used this technique to deal with data with complex validation. I'm parsing a csv, some field needs to be Int but are String in practice. I have a `Valid` and `Invalid` version which allows to store string in what should be an Int and display an error message. Obviously, to work this technique would need to be supported by Opaleye or equivalent.
Thanks for the reply! I didn't know how to find that link (http://benchmarksgame.alioth.debian.org/u64q/measurements.php?lang=ghc) in the new UI -- but that's probably my fault. Where can I browse the cvs repository -- I can't seem to find the link for that either? (btw, indeed the first nbody looks more like what I remember -- interesting that the second now seems to have slightly better performance!) A bit of clicking around btw finally brought me to the full cross-language comparison on the benchmarks: http://benchmarksgame.alioth.debian.org/u64q/performance.php?test=nbody I wouldn't say (in response to the OP) that we're particularly slow here, though it would be nice to come to around 20. The rule of thumb is that well optimized Haskell should come within about 2x well-optimized C (given the general overhead introduced by GC and boxing). And indeed the best entries in C and C++ are at about 9 seconds to the 24 of Haskell. And with OCaml at 20, which is also a tough competitor, we're solidly in the middle of the pack, which I consider not too bad :-) I wouldn't imagine we could squeeze much more performance out, although it would be interesting to see if a new lib let us do more _idiomatic_ code with roughly the same profile.
Neat. Another thing python has that I've been jealous of and would like to try to bind to from haskell is numexpr: https://github.com/pydata/numexpr/wiki/Numexpr-Users-Guide 
You know that this problem is at least as hard as solving "the records problem", right?
I think that misled me was the `Opaleye.Join` module but now I see that the `Query` type is indeed an `Applicative` like I wanted As a side note, I think if `Query` could be made an `Alternative` then there's a [simpler way to do left/right joins](http://www.haskellforall.com/2014/12/a-very-general-api-for-relational-joins.html)
I can't disagree there. I've spent a lot of time looking into at least a half a dozen different ways to represent RA in haskell, and the need for constructive proofs in order to do many useful operations makes it quite a bit more demanding that the non-compositional typeclass-based solutions to "the records problem" I've seen.
Will this work for you? instance Num b =&gt; Num (a -&gt; b) where negate f x = negate (f x) -- ... etc for the other Num methods -- examples f x = x + 1 g x y = x + y h x y z = x + y + z test1 = (negate f) 1 test2 = (negate g) 2 3 test3 = (negate h) 4 5 6 You don't have to define all of the Num methods to run this. You'll get warnings about any unimplemented methods, but everything will still type check.
What about fmap for the reader, aka function composition?
Yea this would work just fine. Consider your third example: f :: Num n =&gt; (a -&gt; b -&gt; n) -&gt; (a -&gt; b -&gt; n) If you call `negate` on `f`, it will need to find a `Num` instance for `a -&gt; b -&gt; n`. Given /u/mn-haskell-guy's instance, it has to look at it like this: `Num x =&gt; a -&gt; x`, and it will see that `x` is `b -&gt; n`. Thus it will look for a `Num` instance of `b -&gt; n`, finding the same instance yet again, using `n` as the underlying `Num` type. So, a function of any arity which returns a `Num` type can be treated as a `Num` type.
[revision 1.92, Wed Oct 21 18:22:37 2015](https://alioth.debian.org/scm/viewvc.php/benchmarksgame/website/lib/compare.tpl.php?hideattic=1&amp;root=benchmarksgame&amp;r1=1.91&amp;r2=1.92) Before, just the name of the task was shown. After, the same `performance.php` URL as now. I think we're done.
I hadn't seen fmap before you mentioned it. I've looked it up, but it's not clear to me how it would be possible to write this with fmap. Could you show me?
It shouldn't matter as much as it does, but I find it very aesthetically unappealing.
SOLUTION: [Backpack](http://blog.ezyang.com/2014/08/whats-a-module-system-good-for-anyway/). The [spec](https://github.com/ezyang/ghc-proposals/blob/backpack/proposals/0000-backpack.rst) was just finished.
What about [Backpack](http://blog.ezyang.com/2014/08/whats-a-module-system-good-for-anyway/)? The [spec](https://github.com/ezyang/ghc-proposals/blob/backpack/proposals/0000-backpack.rst) was just finished.
any body ?
When your config needs functions, or even variables.
Though it would still benefit from a few (more) examples. I feel each CT topic would be best served with two or three examples, as in &gt; A cartesian product in set theory, a meet in order theory, a conjunction in logic — they are all specific examples of the abstract idea of a categorical product. (from his previous article https://bartoszmilewski.com/2015/09/01/the-yoneda-lemma/)
I'm not asking for a "catch-all" instance but separate instances for each `Applicative`. I don't really see why the `Eq` matters.
xmonad is configured in Haskell!
&gt; I don't really see why the Eq matters. Because the Haskell Report has `Num` as a subclass of `Eq`. GHC doesn't, but Haskell does.
Whoa I always thought GHC did. That's crazy.
I'd be particularly interested in seeing this adapted to a dalvik backend as well.
&gt; You can try to define a general postcomposition function, but you'll find that, even with type classes and lots of extensions enabled, it isn't easy to get the types right. [...] I hacked up something that works, albeit with a somewhat restricted signature I took that as a challenge to write the more general version :) {-# LANGUAGE FlexibleInstances, MultiParamTypeClasses, TypeFamilies #-} -- naming scheme: I use ab for (a2 -&gt; a3 -&gt; ... -&gt; aN -&gt; b) type family Output f where Output (a1 -&gt; ab) = Output ab Output b = b type family RewriteOutput ab c where RewriteOutput (a1 -&gt; ab) c = a1 -&gt; RewriteOutput ab c RewriteOutput b c = c -- | -- postCompose :: (b -&gt; c) -&gt; b -&gt; c -- postCompose :: (b -&gt; c) -&gt; (a1 -&gt; b) -&gt; (a1 -&gt; c) -- postCompose :: (b -&gt; c) -&gt; (a1 -&gt; a2 -&gt; b) -&gt; (a1 -&gt; a2 -&gt; c) -- postCompose :: (b -&gt; c) -&gt; (a1 -&gt; a2 -&gt; ... -&gt; aN -&gt; b) -&gt; (a1 -&gt; a2 -&gt; ... -&gt; aN -&gt; c) -- -- &gt;&gt;&gt; postCompose (round :: Double -&gt; Int) ((*) :: Double -&gt; Double -&gt; Double) 2 3 -- 6 -- &gt;&gt;&gt; postCompose (round :: Double -&gt; Int) (sin :: Double -&gt; Double) 1 -- 1 -- &gt;&gt;&gt; postCompose (round :: Double -&gt; Int) (1 :: Double) -- 1 class PostCompose ab c where postCompose :: (Output ab -&gt; c) -&gt; ab -&gt; RewriteOutput ab c instance {-# OVERLAPPABLE #-} (Output b ~ b, RewriteOutput b c ~ c) =&gt; PostCompose b c where postCompose = id instance PostCompose ab c =&gt; PostCompose (a1 -&gt; ab) c where postCompose f = fmap (postCompose f) 
Why not just say what your actual range is instead of being coy? You will get more interest.
Leaving aside the merits of moving `Control.Category` into the Prelude (highly dubious IMHO)... I still think there is some merit in having labels desugar to a common `IsLabel` class, in particular because it allows multiple libraries using labels to co-exist within a single module. What about the following? * WIth `OverloadedRecordFields` enabled (but neither `OverloadedLabels` nor `RebindableSyntax`), desugar to `GHC.Records.fromLabel` (the method formerly known as `getField`). * With `OverloadedLabels` enabled, desugar to `GHC.OverloadedLabels.fromLabel` (the existing method). Add an `IsLabel` instance for `(-&gt;)` so this is compatible with the previous case, provided it doesn't introduce ambiguities. * With `RebindableSyntax` enabled, desugar to whatever `fromLabel` is in scope. This retains pretty much the 8.0.1 `OverloadedLabels`, except that anyone wanting a different `IsLabel` instance for `(-&gt;)` will need to go through `RebindableSyntax` contortions. Moreover, it makes available the behaviour you want for `OverloadedRecordFields`.
Yeah, define competitive. If you want to hire good people, don't play games. It's not something that needs to be a secret.
Another monad whose value you can't get at is `Proxy`, which is a perfectly law-abiding monad, but doesn't have any value to get at, and never actually performs the functions that (&gt;&gt;=) is used with.
What if the configuration language were total?
It sounds like people have already answered your question. A related instance provided by the standard libraries is the [Monoid instance for (a -&gt; b)](https://hackage.haskell.org/package/base-4.9.0.0/docs/Data-Monoid.html#t:Monoid): instance Monoid b =&gt; Monoid (a -&gt; b) This is covered in greater detail in the [Equational Reasoning at Scale](http://www.haskellforall.com/2014/07/equational-reasoning-at-scale.html) blog post. Here's a contrived example of its use: &gt; let f = (\x y -&gt; Sum (x * y * 2)) &lt;&gt; (\x y -&gt; Sum (x + y)) &gt; f 4 5 49 
Imo it spawns many dialects of haskell, which makes it harder to read if everyone uses a different dialect. Idk though, just an opinion.
Here, since it came up on another reddit page, is the going "binary trees" entry in the shootout http://benchmarksgame.alioth.debian.org/u64q/program.php?test=binarytrees&amp;lang=ghc&amp;id=4 It diverges with `-XStrict` as far as I can tell. If the tree type is left to be spine lazy by removing the bang patterns, it takes about half as long as if it is kept spine strict. (The comments say it is strict, because this is required by the rules). This is very ordinary Haskell.
I'm sure it preforms "well enough" -- but if that lack of mutation induces another constant factor of say 2x or enough that's enough to completely disqualify it from being a contender in the benchmarks game :-)
I could live with that.
I do something this for logical operators. module Data.Predicate where import Prelude hiding ((&amp;&amp;), (||), not) import qualified Prelude class Predicate p where infixr 3 &amp;&amp; (&amp;&amp;) :: p -&gt; p -&gt; p infixr 3 || (||) :: p -&gt; p -&gt; p not :: p -&gt; p instance Predicate Bool where (&amp;&amp;) = (Prelude.&amp;&amp;) (||) = (Prelude.||) not = Prelude.not instance (Predicate p) =&gt; Predicate (a -&gt; p) where p &amp;&amp; q = \ a -&gt; p a &amp;&amp; q a p || q = \ a -&gt; p a || q a not p = not . p 
Why is this an "instead of" question? Lists and non null lists each have their uses. One doesn't supersede the other.
True, but since ClassyPrelude has rewritten `head` to work with non-null lists, isn't it promoting them as the default (better) alternative?
I think a better idea would be to compose negate with whatever functions you'd want. e.g. (negate . g) a b would do what you want. Is there any other function you'd like to generalize? Because for your example with negation I can't see why you'd want to do such a thing. 
That is sick.
FYI, your Monoid instance is provided as [`Option`](http://hackage.haskell.org/package/semigroups-0.18.1/docs/Data-Semigroup.html#t:Option) in the semigroups package.
I knew it was around here somewhere =P
(=&lt;&lt;) is a bit of a black sheep since it's not even part of the Monad type class. :/
Order is pretty important to folds (foldl vs foldr), and they're pure.
I can't imagine any compatibility issues with generalizing the `Monoid` instance for `Maybe`. There is no code using the instance that suddenly wouldn't be able to, assuming `Semigroup` becomes a superclass of `Monoid`.
_ex falso sequitur quodlibet_ https://en.wikipedia.org/wiki/Principle_of_explosion
Some tangentially: I don't think the existence of functions like `head` is a big problem. It would be if there was no alternative (i.e. pattern matching) and thus we had to use it all the time. Its mere existence is not particularly problematic for the correctness of our programs, except for beginners before they learn about pattern matching (which is what they should use 99.9% of the time). Having a function like `head` for those cases where you know a list is not empty but you cannot prove it statically is useful, just like having a partial division function is useful. The alternative would be to write a pattern match with an `error` branch, which is essentially the same thing as using `head`. Consider this analogy: why do we think it's bad to have null pointers in a language but don't lose any sleep over the presence of `unsafePerformIO` in Haskell? The reason is that in a language with null pointers we are forced to deal with them everywhere we pass around pointers, even though there's no need. This is not the case with `unsafePerformIO`, which we can use in those 1/10,000 cases that needs it but can otherwise ignore. `head` is like `unsafePerformIO`, not like null pointers.
I'd argue the difference is largely in community standards, not in anything mechanical. The problem with `null` in a language like, say, Java, is that the standard library uses `null` everywhere, and practically encourages its use. This forces it to be ubiquitous. However, had it been treated with the same stigma as `undefined` in Haskell, it likely would be a "1/10,000" solution. `unsafePerformIO` and `head` represent the same sort of problem as `null`, we just put a stigma around them that successfully makes people avoid them.
&gt; I'd argue the difference is largely in community standards, not in anything mechanical. The problem with `null` in a language like, say, Java, is that the standard library uses `null` everywhere, and practically encourages its use. This forces it to be ubiquitous. The problem is larger than its use in the standard library. There's no good alternative to `null` in Java to represent a missing value and there are no non-nullable references. All this conspires to nulls being prevalent and thus a problem in Java code. Null is already stigmatized everywhere, but if there's no alternative that doesn't really help. This is not the case for `head`.
&gt; There's no good alternative to null in Java to represent a missing value Java 8 has `Optional&lt;T&gt;`, which fits the bill quite well. &gt; and there are no non-nullable references. There are also no non-unsafePerformIO functions, and no non-head functions in Haskell. You can't call a function and be sure it doesn't call one of these.
We just need stack traces, like every other language on the planet. ;)
Well I think that's my point, `null` is only different because of the different conventions surrounding it. i.e. it's the same problem, except that you have to think about it more because there are conventions in place that make excessive use of it.
...that's GHC 8.6, right? Not `base-8.6`? :)
Null, before `Optional` in Java, has more than convention around it. There's no other good way to represent optional values so people use null. The problem is 1) having an error-prone feature and 2) having to use it. Put another way, I don't think Java overuses null, it just didn't offer a better option.
I don't understand where the confusion is coming from. If you need the type system to guarantee that your list isn't empty, use a `NonEmpty`. If it doesn't matter, use an ordinary list. What's the confusion?
Your question makes sense now. It just seems like a triviality to me. Converting between the types is trivial. And I would say that you'll never need to take working functions and change them to require non-empty lists just because the lists being passed happen to never be empty. You should only require non empty lists as parameters when you actually, definitely, for sure need the list to have at least one element. Otherwise what's the point?
Why is this question made complicated by depending on whether ClassyPrelude is a good idea at all? Just ask whether `Data.List.NonEmpty` is a good idea in its own right.
...or some kind of `PARTIAL` pragma to annotate `head` and friends to help people spot uses of `head`. Just like we have `SafeHaskell` to spot `unsafePerformIO` uses.
I mean, Nix *can* be used as a general-purpose configuration language. And it's a quirky but powerful lazy functional language, at that. That's the whole idea behind NixOS—most of the settings on my laptop are configured through Nix. 
Thanks for taking a look Syrak. I am very interested in your library. My GArbitrary was definitely a quick hack and not intended to be used without careful consideration of the data type. I'll take a look at your library over the weekend and give you some more feedback.
[removed]
&gt; Java 8 has Optional&lt;T&gt;, which fits the bill quite well. Yeah, but it's encouraged only as a return value. It's discouraged as parameter type because you can't force the caller to still put a null there and also discouraged as record field type because of efficiency. So the Java community standard still puts limits on the use of Optional. 
- `NonEmptyFoldable`: [`Foldable1`](https://hackage.haskell.org/package/semigroupoids-5.1/docs/Data-Semigroup-Foldable.html#t:Foldable1) - `NonEmptyAlternative`: [`Alt`](https://hackage.haskell.org/package/semigroupoids-5.1/docs/Data-Functor-Alt.html#t:Alt)
FWIW: I tried to hide `head` from prelude used by `Cabal` (relatively large and non-trivial codebase), there aren't that many use-sites. One I remember, is taking the first elements of lists returned by [`groupBy`](http://hackage.haskell.org/package/base-4.9.0.0/docs/Data-List.html#v:groupBy), though there is variant in `semigroups`which returns `[NonEmpty a]`. So, I think that we could use `head` much less we use it today, with help of `NonEmpty`.
Need any more interns?
I think what you're talking about is one of the things https://github.com/grammarly/rocker#from solves, by letting you have multiple `FROM` statements and `IMPORT`/`EXPORT` files from one step to the other. The multiple `FROM`s seems like a great idea, but I hadn't seen it until today, so I've to play around and see how to do a similar job without breaking too much code. We could either have interop with `Rocker` by letting people output whatever instructions they feel like or reimplement that functionality. My ideal would be to define a directive: addGlob pattern dest = do fs &lt;- liftIO doTheGlob forM_ fs $ \f -&gt; add f dest Then "register" it: dockerfileRenderWith [("ADDGLOB", addGlob)] =&lt;&lt; readFile "./DockerfileFP" And use it as normal: ADDGLOB ./**/*.md /docs/ 
I haven't been following Haskell for long (still learning it), but it appears [you might be onto something](http://benchmarksgame.alioth.debian.org/u64q/compare.php?lang=ghc&amp;lang2=go). Out of nine tests, Haskell wins in two either by performance or memory usage. Whereas Go wins six and one where they're about equal. 
You can't write better code, it's just an illustration that (&gt;&gt;=) is "favoured". When you write a monad instance you can't implement (=&lt;&lt;) even if that's the only one you use in the rest of the code. To me it's a bit like if the functor type class contained (&lt;&amp;&gt;) instead of fmap.
'Fast' is a relative concept - the 'fastness' you expect for simple benchmarking algorithms is not the same thing as with a complex program with many users and edge cases. It might mean 'responsiveness', 'load-distribution', 'resource consumption' - hell, it might not even need to be that fast; a best average case maybe is more desirable. Such is the case with many businesses in the world - yet most evidence proclaiming Haskell's fastness tends to fall into that first category of 'fastness': look at how fast it is to run some algos. These things already give functional languages like Haskell quite some advantage - and there is no need for GHC to focus making this part any better. I'm talking about the weak areas of Haskell - which unfortunately usually manifests in the second category.
Yes - because a fast but buggy program is not as useful as a slow but correct program. But for many use cases a slow program might be as useless as a buggy one just because it is required to be fast. So you end up picking the third option - write a fast and non-buggy program, with care and proper testing, but without haskell.
&gt; So you end up picking the third option - write a fast and non-buggy program, with care and proper testing, but without haskell. Not sure why you would do that. When I need my program to be faster, I optimize it. There also is the C FFI, but I never had to reach for it. Haskell programs are quite fast (in the ballpark of java/go, so much faster than Python/Ruby/Perl but slower than C/Fortran). Now there is the developer performance story to take into account. And that's where Haskell really shines.
Re GC pauses, I've seen people trying to deal with it in haskell but not much success - that's what I mean by picking another language. This is a deal breaker for services. (tbh I haven't personally encountered these problems, but just reading about them made me use other languages instead).
Oh, sure! I never had to deal with latency constraints, so I can't really tell. I would not expect it to be as easy as all the other optimizations I had to do :/
We should really distribute *built* compilers for various hard to build things. Don't give me long winded assembly instructions that might not work, give me the end product! 
"It is better to have 100 functions operate on one data structure than 10 functions on 10 data structures." - Alan Perlis I'm not sure Haskell is right to encourage lots of different types.
/u/jkarni The images in this blogpost are no longer working?
For long-lived data, the upcoming "compact regions" feature might help with GC times in some cases. (See section 5.5 of the [paper](http://ezyang.com/papers/ezyang15-cnf.pdf)).
See comments regarding compact regions.
Simple doesn't mean it can't be correct or high-level: in fact as you said it means compilers can make better optimizations. It also means that you always have the 'average best case' just by writing simple code, without any additional tweaks - one problem I see with haskell is that as soon as we start digging into performance we adovocate some coding practice *in addition to* the pure and beautiful language, sometimes even detracting from that elegance. Yet we still boast about that elegance as our strength. If features of elegance have to come with strings attached - maybe we should invest less time in making more of them in the future...
If Haskell's defaults would be untagged, unboxed values and strict evaluation, with the option to have tagged, boxed values and lazy data eval, then most issues would be void. It doesn't have to be like MLton, but I wager to say that some of the research ideas, while useful, do not make good defaults.
Awesome! Is there already a main target for polishing? Edit: just saw your reply on GC pause - sounds very promising :)
Yah for some reason the graphs are not showing in /u/jkarni's blog post. The latest benchmark is not released on TechEmpower yet so you're not seeing it there. But if you look at the blog post about servant and take a look at the raw data you will not be disappointed. Hopefully the graphs will be back up soon?
Honestly, I think a great first step would just be more learning material and documentation as to what RTS is doing when it's running certain programs. The GHC commentary is great as is the user manual, but it gets pretty sparse from that point on. I think right now there's more of a trust issue with Haskell's performance story more than anything else. There's a lot of cases where a Haskell enthusiast says that Haskell is "high performance", "fast", or "can be as fast as C", but then either doesn't back it up with a meaningful real-world example, or they link Don Stewarts stream fusion post - or the language shootout. Meanwhile, there's pretty interesting posts like [this](https://blog.pusher.com/latency-working-set-ghc-gc-pick-two/) and [this](http://prl.ccs.neu.edu/blog/2016/05/24/measuring-gc-latencies-in-haskell-ocaml-racket/) floating out there that contradict any steadfast endorsement of Haskell's tendency for high-performance. So which one is it, who to believe if you're just starting out with the language?
[removed]
Haskell is what it is, and has gotten to where it is, in terms of what it is, solely because of the motives that drove its development so far. That is exactly what sold it to all its users. You can call it elegance, correctness, coherence, consistency or all of it and whatever else you use it for in the first place. You cannot simply question the motives it is founded upon [ avoid success at all costs ], based on a desire to now have it work for you the way you want it to. I have tremendous respect for its makers, we all should have. So Haskell as it is, should not change its course, but it will only work toward its success if it could make industry requirements INCLUSIVE to its development. This is what I humbly feel. Please don't take it in the wrong way.
Sure - I totally feel your reasoning, and that's why I have tremendous respect for SPJ and the whole GHC team. (As one of the more enduring functional languages, haskell's origin and history fascinate me too) However, I always think that a project's direction should and would in most case reflect what its community cares about - that of course includes the original motivation of haskell, which is what brings most of us together in the first place. But I'm also seeing the trend where more and more people start to get interested in using haskell for industrial purposes - the fact that this group of people is getting bigger inside the community should normally lead to more changes catered for such ends - since if they truly care about what they are trying to do and if the project is truly open as it claims they should have the motivation to make changes/suggestions. Hence my question here whether there is going to be such change / or if there isn't, is there any reason.
It's favored because it corresponds to do notation binds, not for any other reason. Certainly not because "algorithmic code" (which, again, is a poor descriptor) should go left-to-right.
Good points, all. I agree.
Fixed! Thanks for the report. That said, /u/bartavelle just pointed out that `servant` is listed in the preliminary round-13 results, but there are still some issues. Hopefully I'll have time to fix them before the results are official.
Yepe, but, since they're pure, both produce the same result 
Certainly, please send an email!
You should create a bug report ticket on [Trac](https://ghc.haskell.org/trac/ghc) about that.
Awesome. Thank you for open sourcing this!
This currently happens with ghcjs.
This is a collection of type class tricks I have picked up, in no particular order. I'm looking for comments, more examples and some help organizing them in hopes of creating a nice resource for learning this peculiar part of Haskell.
Is someone working on this? Why can't we just generate this stuff each release for the most common ARM v*X* architectures?
Agreed, some **examples** would be awesome!
There's another way to do multiple constraints on one variable, or vice versa one constraint on multiple variables, using type families and type-level lists. You just prompted me to update my Hackage package now to showcase this: https://hackage.haskell.org/package/type-operators-0.1.0.2/docs/src/Control.Type.Operator.html#%3C%3D%3E It's really nice that this is possible when you're dealing with a lot of typeclasses or typeclass variable names that are trying to be descriptive.
Why not?
That's right, you can even replace the singleton case with the empty one: type family (&lt;=&gt;) (c :: k -&gt; Constraint) (as :: [k]) where c &lt;=&gt; '[] = (() :: Constraint) c &lt;=&gt; (h ': t) = (c h, c &lt;=&gt; t) The documentation for [`^&gt;`](https://hackage.haskell.org/package/type-operators-0.1.0.2/docs/src/Control.Type.Operator.html#%5E%3E) looks suspect. 
`Optional&lt;T&gt; x = null;` is a type error? Is "nullability" inferred? like `y = null; Optional&lt;T&gt; x = y;`. 
I don't know. If anything, it could be another useful set of regression tests. Particularly GHCJS. Since it targets JS there's no possible way releasing a binary blob could be problematic.
Ah, I see, `Output` is expecting the output to be a non-function (that's how it knows to stop looking further to the right), but in this case we do want the output to be a function, because we want to apply `op` to that output and it expects `f` to be a function. I don't know where you got stuck, but I think your idea of giving `RewriteOutput` a type to rewrite is a good one: -- we want RewriteOutput ab b c = ac type family RewriteOutput ab b c where RewriteOutput b b c = c RewriteOutput (a1 -&gt; ab) b c = a1 -&gt; RewriteOutput ab b c class PostCompose ab b c where postCompose :: (b -&gt; c) -&gt; ab -&gt; RewriteOutput ab b c instance PostCompose b b c where postCompose = id instance {-# INCOHERENT #-} (PostCompose ab b c, RewriteOutput (a1 -&gt; ab) b c ~ (a1 -&gt; RewriteOutput ab b c)) =&gt; PostCompose (a1 -&gt; ab) b c where postCompose f = fmap (postCompose f) -- | -- &gt;&gt;&gt; postCompose op (sin :: Double -&gt; Double) 0 -- 0.8414709848078965 -- &gt;&gt;&gt; postCompose op ((*) :: Double -&gt; Double -&gt; Double) 2 3 -- 8.0 op :: (Double -&gt; Double) -&gt; Double -&gt; Double op f x = f (x+1::Double)::Double I must admit that I don't grok the [overlapping rules](https://downloads.haskell.org/~ghc/8.0.1/docs/html/users_guide/glasgow_exts.html#overlapping-instances) yet, so I had to fiddle with the `{-# ... #-}` annotations until it worked on all the test cases.
It's possible to make a list type which encodes whether or not it is empty. (This is a special case of lists which indicate their length.) Using a few GHC extensions, for example, we can do: data Emptiness = Empty | NonEmpty data List :: Emptiness -&gt; * -&gt; * where Nil :: List Empty a Cons :: a -&gt; List e a -&gt; List NonEmpty a With this definition, it's trivial to define a total `head`: head :: List NonEmpty a -&gt; a head (Cons a _) = a Functions which do not change the structure of a list, like `fmap`, are also easy to define. Unfortunately, it becomes more complicated to write functions that *return* lists. For example, what is the type of `tail`? tail :: List NonEmpty a -&gt; List ? a You can't write it like this. You either have to introduce an existential wrapper or rewrite `tail` in continuation-passing style. (This particular problem would be resolved if the list was indexed by its length, but that won't help you write `filter`.)
Default strict would kill the language as we know it. You can't have opt-in laziness because one strict function in the chain kills it.
I'll try to reply with some (pseudo) code when I get to my laptop, but we need to be able to model three states at the type level: * Definitely empty list * Definitely non-empty list * Possibly empty list if we do that, `tail` can be defined as under: * tail of definitely empty list = definitely empty list * tail of definitely non-empty list = possibly empty list * tail of possibly empty list = possibly empty list have to work out how other list-oriented functions work with these three types. trying to head: * head of empty list = runtime error, or won't compile * head of non-empty list = element * head on possibly empty list = Maybe element
No problem! Feel free to comment on the code, open issue, ask question and anything else that can help you. I will be adding more comments to the code to explain what each function is doing.
Oops, `^&gt;` is not supposed to return a different type. Thanks for spotting that.
Possible to provide a slightly more detailed motivation for developing this library? does it basically ensure that you ToJSON and FromJSON instances are lossless? ie POSTing a JSON, results in the same JSON whet it is GETted back? 
Haskell sucks because it doesn't have Hash tables by default, memoization seems very hard without Hash tables.
Hi, does anyone know the keybinding on how do I kill the current process within the repl? If I hit C-c C-c it says: "while: Invalid search bound (wrong side of point)"
Thanks Plow!
I'd like to understand this better. If the default was strict and you had to annotate like OCaml's rec that something is to be lazy, and the libraries fit this execution model, what exactly would be the problem? I mean, I appreciate laziness in certain cases for obvious reasons, but when I look at all the code that works around it, it doesn't seem like the right default for code that's run on current machines architectures. Would you mind elaborating a little more?
This is a breaking change, technically: "Not allowing ? -&gt; Allowing ?" But here since you're strictly _not_ allowed to use ? in function names, no one could have possible used them - so I'm having a hard time understanding why libraries would be broken? It's just a matter of allowing ? to be in function names, just like '. Am I missing something here?
Sounds good - I'll take a look at the papers when I get time. There should be more publicity around these things though: for one thing, I'm definitely interested - and I think those people running into GC problems for big projects will be interested too - after all, we are not only users of the language but also most of the time first users of the program compiled. Just saying.
Agreed, I need a name for the *QuickCheck-trick* which is a common one (I follow [Chris Done](http://chrisdone.com/posts/haskell-constraint-trick)'s lead and call them tricks).
Are you familiar with [Go's GC](https://www.google.com/search?q=go+gc+1.5+1.6) (concurrent, very low latency)? Do you know if there's a technical reason that makes their design less suited for Haskell, or does it all come down to differing priorities (latency vs. throughput)?
Seconded! 
The leak wasn't really discovered by Phil in 1987. It had been well known for quite a while. The simple solution described by Phil was one I told him about. There are more elaborate solutions, that avoids GC involvement, and even more general ones. 
&gt; Semi-related: Why is the `ConstraintKinds` extension plural? There is only one `Constraint` kind. Edward has brought that up [once](https://youtu.be/hIZxTQP1ifo?t=2763) or [twice](https://www.reddit.com/r/haskell/comments/4w1jcr/using_typelevel_naturals_overloaded_strings_and/d63hgvk) :P
Very interesting, thanks for linking
I enjoyed the paper. I have nothing of importance to add so I will complain about the most insignificant thing in the history of mankind. ---- With no aligning, the definition of `result`/`results` would be result = sample g (die n) results = sample g $ sequence $ replicate k $ die n adding a space makes them line up nicely result = sample g (die n) results = sample g $ sequence $ replicate k $ die n but the authors added two spaces, presumably to test my faith result = sample g (die n) results = sample g $ sequence $ replicate k $ die n
I think it's a fair point to make
Everything else is very neatly aligned so it stuck out more :) while I'm at it they could use [`replicateM`](http://hackage.haskell.org/package/base-4.9.0.0/docs/Control-Monad.html#v:replicateM) replicateM :: Applicative m =&gt; Int -&gt; m a -&gt; m [a] replicateM n x = sequenceA (replicate n x) results = sample g $ replicateM k $ die n 
I find Go's argument for prioritising low latency quite convincing https://blog.golang.org/go15gc &gt; Today 16 gigabytes of RAM costs $100 and CPUs come with many cores, each with multiple hardware threads. In a decade this hardware will seem quaint but the software being built in Go today will need to scale to meet expanding needs and the next big thing. Given that hardware will provide the power to increase throughput, Go’s garbage collector is being designed to favor low latency and tuning via only a single knob. Go 1.5 is the first big step down this path and these first steps will forever influence Go and the applications it best supports. This blog post gives a high-level overview of what we have done for the Go 1.5 collector.
Reading on Go's GC: https://blog.golang.org/go15gc
Wow, great share! This is 100% relevant for me as I've been puzzling through building genetic algorithms in Haskell, which need a lot of constantly-updated probabilities. It's been a bit of a nightmare as a beginner Haskeller but this paper gives plent of extra insight. Cheers!
i am currently very busy, but i think these experiments can also be used as simple reference-projects.
I'm sorry I didn't understand your comment. Did you mean to give a list of reference projects?
For outsiders (like me) to get a feel for what it is like "in the trenches" to work on Haskell GC latency problems and solve them, I find [this Simon Marlow blog post](https://simonmar.github.io/posts/2015-07-28-optimising-garbage-collection-overhead-in-sigma.html) extremely enlightening.
I'd say one if the more telling quotes from that blog post is: "Furthermore, unencumbered by ongoing support for dozens of knobs, the runtime team can focus on improving the runtime based on feedback from real customer applications." The difference being that Go has a runtime team, GHC doesn't. It just doesn't have the kind of industrial development support that Go has. If someone wants to work on the GC then there would surely be support in doing that, but there is no team whose effort can be directed with the resource to rewrite the GC. I'd hazard a guess that if the lack of a concurrent tri-color GC was the biggest impediment to Facebook's Haskell use then Simon Marlow would write one. Failing that it's a matter of either another industrial user deciding to write or sponsor it, or a university or independent researcher deciding it would be an interesting challenge.
This is really well written and informative, very nice!
&gt; If the default was strict and you had to annotate like OCaml's rec that something is to be lazy, and the libraries fit this execution model, what exactly would be the problem? I've yet to see any language that purports to offer up that design give you libraries that are sufficiently lazy to get any benefit from laziness in terms of composability. You wind up with a bewildering array of special cases to build. Want a monoid for (&amp;&amp;) that short circuits? Now you need a `RightLazyMonoid` and lose code reuse w/ the normal `Monoid`, need to parameterize `Monoid` with the laziness of its arguments, so that you have to spend all your time thinking about something that usually doesn't matter to you, or you just don't get to have it. Compared to that, today in Haskell you can just use `Monoid` to support things lazy in either or both arguments to `mappend`, and they'll do the right thing if they can when applied to folding over infinite data structures. Heaven help you if the libraries you build atop didn't allow for such considerations -- and yet I can't find a single one in _any_ language that does! Then there are pragmatic concerns in the languages that purport to offer this today in that most strict languages that implement laziness do it in a fundamentally bad way. Languages like Scala offer you `lazy val`s but they live as fields inside of their parent object. Which means you can't even copy a lazy val to a lazy val lazily without holding on indefinitely to the surrounding object. Even if you do it right, to my knowledge nobody who offers such a 'Lazy' type that holds a 'thunk' object for the heap actually does the forwarding for it during GC that haskell does. So now my lazy values leak heap or cost twice as much. Not a great pragmatic trade-off to me. Ultimately, I think the burden is on those who want such a design to put forth a language with sufficient care in the implementation of the libraries that we who use laziness can actually get any benefit from it for composability beyond toy examples in practice. In the meantime I'll continue to treat this argument as fairly specious.
This &gt;, RewriteOutput (a1 -&gt; ab) b c ~ (a1 -&gt; RewriteOutput ab b c)) was the part I did not get.
Did you look through the [Yesod book](http://www.yesodweb.com/book) and the [Snap book](http://snapforbeginners.com/)? I'm a big fan of Servant, but it's not a full-blown web framework and if you want a bunch of ancillary library choices made for you, I think these frameworks have opinionated design that seems to be what you're looking for.
Interesting, hadn't known the garbage collector has special handling for this kind of thing! Just to make sure I'm not totally misunderstanding something though: in `fst (a, b)`, it's \*`b`\* that's the false reference and semantic garbage, and the whole thing should be forwarded to \*`a`\*... right?
Cool project try posting to r/coding and r/programming later
It's like the "polyvariadic function trick" used in `Text.Printf`.
Indeed. =)
`createProduct`: separate input, validation, and DB. Yesod has front end + validation solved IMHO, so maybe build on that. No need to reinvent layout, i18n etc yet again. The backend needs at least one data structure on the DB side and one on the domain side, with a DSL in between. You could define one implicitly, but then you would wrap them in a `newtype` anyway for separation of concerns. `getProductList`: opaleye-sot. Nuff said. Well, maybe not, but I really like the approach. Privileges still need some thought, but maybe the DB side is not the right place to start. Instead I'd start with a general framework to represent user groups and permissions, then add options to access the info, to load it from a DB etc. After that the DSLs can start to use it. Maybe it should actually be the CRUD-form-builder rather than the DB backend that makes the decisions? Otherwise it's only useful if the permissions are in the types to prevent implementer errors. `authenticate`: How about associated type families like Yesod uses? Definitely separate incoming and stored because incoming could be a `Text`, but stored is generally something like a tuple of a salt and a hash, right? *Right?*. I personally like `Either` because it's combinable as a monad. Sadly a thing Yesod did get wrong in some places, but well, I'll live. 
Agreed. Start here: http://learnyouahaskell.com The good news is that it looks like you'll only need to add code between lines 54 and 64, though you'll have to build part of the generating calculus problems yourself. Maybe [this](https://www.reddit.com/r/haskell/comments/3cyoyl/calculus_in_haskell/) thread can help. Good luck.
Here's a simple blog site I started working on after reading your post last week. https://github.com/LightAndLight/humblr It currently supports secure authentication over https, and token based authorization.
I think the OP's best approach would be to find a way to integrate opaleye into yesod. Persistent is not required for yesod (it is integrated with yesod-persistent so you can easily set your runDB function to run persistent), someone could make a yesod-opaleye that helps set runDB as a function using opaleye (it is possible someone has already done this, maybe OP could search around github if it is not already in a hackage package). OP will end up remaking a lot of features that Yesod already has if OP insists on doing it with Servant. The Ruby comparison would be Yesod is like Rails and Servant is like Sinatra. You can take Sinatra and remake Rails with custom options, or you can change Rails options directly (I'm not sure what the current state of Rails is so I have no idea how simple that is). Yesod is designed in such a way that a lot of the parts are exchangeable. 
So I have good news and bad news. The good news is, the best way to do this is to try and think of your program like a big equationally defined function, just like calculus. The bad news is, the best way to do this is to try and think of your program like a big equationally defined function, *just like calculus.* So my question is, do you just want this program, or do you want to know how to *make* this program?
I do not want to derail this discussion from "getting things done". I will respond to this in a separate blog post with more details about my stance.
Thanks for replying. Actually the original Snap code that you pointed me to was the wrong analogy. I dug deeper and found out the code analogous to what I was trying to write: [loginByUsername](https://hackage.haskell.org/package/snap-0.7/docs/src/Snap-Snaplet-Auth-Handlers.html#loginByUsername) and [lookupByLogin](https://hackage.haskell.org/package/snap-0.6.0.2/docs/Snap-Snaplet-Auth.html#v:lookupByLogin) and [checkPasswordAndLogin](https://hackage.haskell.org/package/snap-0.7/docs/src/Snap-Snaplet-Auth-Handlers.html#checkPasswordAndLogin) Now, relating this code to my **design decisions** for a higher-level domain API: * It seems like defining different types for plain-text password and encrypted password is what Snap is doing. So, it seems like a good idea and should be replicated at similar other places, yes? * It also seems like defining a GDT for different types of authentication failure is a good idea. And the `authenticateUser` should return some an `Either AuthenticationFailure User`, similar to Snap's `checkPasswordAndLogin` * Interestingly, `loginByUsername` uses a simple `ByteString` for the actual username and not a unique datatype. What're the pros &amp; cons of this decision?
&gt; The backend needs at least one data structure on the DB side and one on the domain side, with a DSL in between. Would that make it **three** types for essentially the same data? First one as a result of form validation, second one that the domain API uses, and third one that the DB uses. Is there any sane way to bring this down to one keeping [these scenarios in mind](https://github.com/vacationlabs/haskell-webapps/blob/master/ServantOpaleye/DomainApi.hs#L96-L129)?
In general if you have a function `foo :: Bar -&gt; Biz -&gt; Bhur -&gt; Bizz` to make it work "inside" an applicative functor (like Maybe or IO or what not) you just need to do a `pure foo` For example, Here is a function that reads name and age of people. data Person = Person { name :: String, age :: Int } age :: IO Int age = read &lt;$&gt; getLine readPerson :: IO Person readPerson = Person &lt;$&gt; getLine &lt;*&gt; age 
I'd say two. The validation might not need one because as long as the data is not validated you could store it "hovering" in functions or in the json you'll probably be using to shove it around. Also the records could actually be parametric in every field so you can easily replace them with some `Unvalidated a` wrapper. Needs more types again, but, well, type noise is the price for generality... I don't think you can do these tricks easily between domain and DB though because of the possibly huge differences. 
I agree, a simple function argument could suffice in the first case. Then this function should be made into the one core place for audits. In the other case... my initial gut feeling is to add phantom types that are controlled either by type families or by dependant types. Could be a bit hairy to get them from the field level to the record level and then to the structural level. Even more so when they must mix with other fancy types. So this sounds like a really elegant Haskell-ish way to go, but not like the practical solution. About incoming and outgoing: I think a good DSL should map all possible actions between domain and DB. That means all permissions would be handled at the same place as well, regardless of how they're implemented. What good is an API if I still have to write the same idea four times?
I admire your dedication to getting a satisfying webapp setup in Haskell. I'm very far from a webapp expert but please stay in contact and let me know how I can help you with any Opaleye-related matters. The Opaleye documentation is rather lacking and setting up your tables with Opaleye can be frustrating. That should be a one-off cost though, and after that everything should be very straightforward. 
Total languages would still have the problem a computation might take a very long time to complete, or produce an [enormous result](https://en.wikipedia.org/wiki/Billion_laughs). I wonder if there has been work about restricted total languages that garantee that the size of the "output" is always below some constant factor of the size of the program, and put similar bounds on running time.
&gt; PS: The links may highlight the wrong lines - I'm updating the file very frequently. After selecting some lines, press Y to get a link to the lines at that specific commit, so they'll always stay the same
Yes, I agree that rolling my own authentication is not the best idea. I haven't ever had to do it in Rails as well -- the Devise Gem is a de-facto standard in that world. Since I'm currently trying to build my minimum-viable-webapp in Servant, which has an experimental auth layer, it seemed like a good idea to add it to my scope. Also, the problems that I keep shutting between seem to fall in two buckets: * How to do X in library Y? Mostly missing glue code that I'm expecting to be already written. * How to **design** X on top of library Y, so that it enables type-safety while being easy to use. it's the latter ones that are extremely hard. things get complicated because of the former. 
thanks for replying Piyush. The relevant section in the Yesod book seems vague about this point. if you:ve worked with "input forms" before, can you tell me if they bail on the first validation error or keep continuing and collecting errors? also, in what format do they return the errors?
&gt; if you have the time, can you take a stab at designing a domain API, layered on top of Opaleye I'm afraid I don't have the time and it's a bit outside of my range of expertise, but feel free to ping me with any questions about Opaleye.
If you want to try something completely different, try transient: https://github.com/agocorona/transient/wiki/Transient-tutorial#transient-in-the-web-browser
Also did you notice that the `&lt;$&gt;` and `&lt;*&gt;` idiom can be used even when working with other applicative functor like maybe. For example. Here is what you do to build a person from `Maybe Text` and `Maybe Int` mkPerson :: Maybe Text -&gt; Maybe Int -&gt; Person mkPerson mName mAge = Person &lt;$&gt; mName &lt;*&gt; mAge -- A simpler version is of course mkPersion = pure Person. You can use this against IO as well. mkPersonIO :: IO Person mkPersonIO = Person &lt;$&gt; getLine &lt;*&gt; getAge where getAge :: IO Int -- this typing is not necessary just to illustrate. getAge = read &lt;$&gt; getLine 
If you want to have the congruence relation laws (i.e, Leibnitz equality), you typically only assume the laws to hold only outside the abstraction barrier, i.e., outside the defining module. Otherwise it's almost impossible to fulfill. 
Congratulations for your accepted contribution. Sorry, for "not-so-positive reactions", hopefully we (i) were constructive enough though.
I also think that Leibniz equality is more meaningful. Although I do think that having certain functions that violate it are fine as long as they have warnings on them. E.g inspecting the tree structure of a Set.
If you look for something widely used in the haskell community, you have no alternative, since the web practitioners comes from Ruby and Python. They have MVC-REST-OOP-SQL in the blood, like you. But that is available in any other language, only better. Why use haskell then? to show off in a blog that you have your own monad stack to manage a configuration file? I don´t understand. Type safety? it has a high cost in accidental complexity (gigantic error messages, for example). And type safety does not guarantee anything except types of variables. Your code can be wrong for many reasons different than not having memory faults. functional programming can help on keeping with the requirements, but for many haskellers, haskell is only about type safety and lens and monads and all of this is called "functional" right? 
Wow. Very positive reaction to some humbling (but hopefully constructive) criticism. Thank you for all your contributions.
This has been discussed before: https://www.reddit.com/r/haskell/comments/1njlqr/laws_for_the_eq_class/ In general, while knowing that a type has equivalence or congruence properties can be quite useful, not all *users* of that type need so much information. Even Coq allows you to define some setoids based on partial equivalence relations (PER setoids).
If you want the same data structure with different fields for different auth levels, a type family solution is possible. E.G.: data APIResult a = { sensitiveThingOne :: AuthCheck a ThingOne sensitiveThingTwo :: AuthCheck a ThingTwo ... } type family AuthCheck (a :: Symbol) b where AuthCheck "User" ThingOne = () AuthCheck "Admin" ThingOne = String ... data AuthLevel a where ALUser :: AuthLevel "User" ALAdmin :: AuthLevel "Admin" ... pattern AUser &lt;- ALUser pattern AAdmin &lt;- ALAdmin authoriseUser = ... authoriseAdmin = ... data AuthorisedResponse where ARUser :: (AuthLevel "User") -&gt; (APIResult "User") -&gt; AuthorisedResponse ARAdmin :: (AuthLevel "Admin") -&gt; (APIResult "Admin") -&gt; AuthorisedResponse It looks like a lot of work at the type level, but that's because we're *doing* a lot of work at the type level. Put `AuthLevel` and the `pattern`s and authorise smart constructors in a trusted kernel, and don't export the constructors. The compiler will now help security audit the rest of your code for you (in the absence of unsafeCoerce). You can do something similar with an `APICommand` type, and control user actions with the same authorisation types.
`&lt;C-y&gt;` in insert mode already has a meaning. It copies a character from the previous line. If you want to write good Vim plugins, don't presume Vim users don't use existing Vim keys. `&lt;C-i&gt;` is Tab, and there are users who prefer this to hitting Tab for speed and comfort.
There's a reason why there are no official laws for Eq. It's basically "some kind of meaningful notion of equality". If Eq should satisfy any laws, it should be the usual equivalence laws `x == x` and `x == y &amp;&amp; y == z =&gt; x == z` - and even those aren't strictly speaking necessary I'd say. Code that works with generic Eq constraints shouldn't assume more (nor should it need to assume more). For other datatypes, the one writing the type should make a call on what equality means based on how that datatype is meant to be used.
github link for the lazy https://github.com/snowdriftcoop/snowdrift
Which one do you find useful? I'm not aware of an nrepl-like robust solution for Haskell that editors can take advantage of (maybe haskell-ide-engine will be some day).
I have used ghc-mod in the past, although I've since mostly switched to spacemacs.
People use non-structural `Eq` and `Ord` fairly regularly to do things like turn set-like containers or heap constructions into map-like ones, deal with modular arithmetic, deal with syntax trees that quotient out information, etc. When you can it is beneficial to use the structural equality, because `x == y` =&gt; `f x == f y` is a nice law to have, but not every instance will satisfy this notion. Then there is the issue that the instances for IEEE floating point types violates even reflexivity, and is secretly only describing a partial order. 
Spacemacs master branch uses ghc-mod.
I meant I'm using spacemacs now, so I don't know the status of the vim ghc-mod plugin.
btw, Rust distinguishes between [Eq](https://doc.rust-lang.org/core/cmp/trait.Eq.html) and [PartialEq](https://doc.rust-lang.org/core/cmp/trait.PartialEq.html)
GHC gives awful syntax errors, especially compared to gcc, clang, or `rustc`.
I've been using it to split cases and show types recently, so (a) it works, and (b) it's comfortable with stack.
For your example, `-Wmissing-signatures` will also hint at something wrong with `mKPerson2'` since it's a top-level binding without a type signature.
&gt; How to design X on top of library Y My recommendation would be to just go do it. You'll learn a lot more by actually building something, even if it's a bad design, than by thinking and analyzing it to death. With time and practice you'll develop a better intuition for it.
I agree. I'm constantly looking for ways to use the unique features of Haskell to my advantage in web programming (which is what I mostly work on). That is the reason I'm leaning towards Servant for JSON API projects. The way it defines its routes it gives you a well formed request/response spec that the programmer can't violate. If the implementation changes, so does the spec. And vice versa. I have an open problem about how to use types (or any other technique) to make sure authorisation is being implemented correctly in the app. I'm also looking for guidelines/thumb-rules about when to use newtypes instead of common data types (like Int, String, Bool, etc)
Not a basic one since it uses `PatternSynonyms`, but you reminded me to submit [a ticket](https://ghc.haskell.org/trac/ghc/ticket/12515) about a terrible error message I encountered a few days ago. I guess the lesson here is that the parser should warn if it gets a `varid`/`varsym` when it expects a `conid`/`consym`.
Personally I think every single GHC error / warning is a good example of a bad error / warning.
So does `const (const True) :: a -&gt; a -&gt; Bool` witness a valid instantiation?
I took a quick look and played around with the example in 0.1.1.0. I also tested it with a doubly recursive data type (can build upon itself or another recursive data type) and it seems to work well. I would potentially like to get this in some of our tests for our production code. Is there any reason you set `base &gt;=4.9`? Our production code is still using ghc-7.10.3 because some of our dependencies from the public hackage do not work with ghc-8 yet so I wasn't able to try with the latest version. How confident are you of the Boltzmann implementation, I ask since you say it is experimental. Could you add an example to the README?
I use Spacemacs and have lots of trouble with ghc-mod (and stack).
The main reason is that I use the TypeApplications extension, but I'd be happy to revert to the traditional method of passing `Proxy` values and support GHC-7.10.3, if you want to. --- `Generic.Random.Boltzmann` has its core functions working, but could use a much cleaner API. I have a test example in my repo [`examples/tree.hs`](https://github.com/Lysxia/generic-random/blob/master/examples/tree.hs). I should make a proper post, along the following lines. Imagine you have data that isn't just an ADT, e.g., an abstract type with impure smart constructors: data Tree leaf :: IO Tree node :: Tree -&gt; Tree -&gt; IO Tree you can specify its recursive structure as a polymorphic function combinatorialTree :: (Alternative f, Embed f IO) =&gt; f () -&gt; f Tree -&gt; f Tree combinatorialTree x tree = embed leaf &lt;|&gt; (x *&gt; emap join (node &lt;$&gt; tree &lt;*&gt; tree)) "A `tree` is either a `leaf`, of size 0, or a `node` made of two sub`trees`, of size (1 + the sizes of the subtrees)". `pure` and `embed`-ded actions have size 0, while the `x` parameter marks size increments. Then there is some boilerplate that I plan to spare the user from when possible: combinatorialTree' :: (Alternative f, Embed f IO) =&gt; System f Tree () combinatorialTree' = System { dim = 1 , sys' = \x -&gt; (, ()) . Vector.fromList . (: []) . combinatorialTree x . Vector.head } Then `sizedGenerator` takes care of turning `&lt;|&gt;` into weighted choices with weights chosen to target a certain average size, and you get a generator: genTree :: IO Tree genTree = sizedGenerator combinatorialTree' 0 0 (Just mySize)
Hah, nice! Can't believe I missed it. Here's a star for you.
gcc, really? miss a single semicolon in a 50 lines program, you get 3 pages of complete nonsense as an answer... GHC error messages are pretty good for the simple cases at least. Yeah if you go down into the abstraction rabbit hole, it can get weird, but I don't think that's an easy problem (and gcc + template madness is probably still much worse)
what about: &gt; A constraint that relates monads and applicative functors is missing in the standard library, because Monad was created before Applicative. this post is not updated =)
Merged. Thanks!
isn't this true, but?
I'm not trash-talking Rails. I am discouraging copying it wholesale. Rather than making an entire framework, make a string of small libraries that work independently of one another and address real pain points in the web experience. For example, instead of porting ActiveRecord, make something that maps records to/from the database without the boilerplate of `postgresql-simple` or the Template Haskell of other libraries. You could leave the query DSL for later. I mention that specifically for three reasons: 1. Being a strong, statically typed functional language will make it nearly impossible to just port AR over 2. Haskell's community is not built around cranking out code as much. Generally my experience of writing code in Haskell is that it takes more work but I don't have to return much after writing it. 3. Some people would rather use DIY queries and just have mapping. It's important to understand the taste and culture of the community you are writing software for.
A pure Haskell MySQL binding was released very recently: https://m.reddit.com/r/haskell/comments/4xx8ey/ann_pure_haskell_mysql_driver_v01_now_on_hackage/ However as you see in there there are some questions of "why not just use postgres". Haskell bindings that are well maintained for databases lean towards technically superior solutions like postgres in my experience. The creator of this new pure Haskell MySQL binding seems to be more of the "use Haskell to deal with our software world as it currently is, only changing the language we use to be Haskell". I think this is a safer strategy than "use the best language (Haskell), the best database (postgres), and the best version control (idk, something not git?). Database access is a pain spot for me right now, as I was trying to connect to a mssql db with hs-odbc and ran into many of the build errors you talk about. Hs-odbc needs some more love or Haskell needs a way to connect to all the databases hs-odbc would support.
Mappable is instantly clear to anyone that has used `map` before in any language.
I would love a text based Rick and Morty game
Section 1.2 "Interactive Context" was a huge source of confusion to me when I started. I haven't looked in depth at the more recent Haskell instructional materials that have been written since I learned Haskell, but it would be really good if we could have a clear explanation of this in a prominent place.
&gt; I can understand why haskell is not widely adopted Too bad, the state of the Haskell ecosystem is also *because* it is not widely adopted. Nobody use MySQL with Haskell -&gt; Nobody write libs for it -&gt; Loop. Edit: Clarify: "Nobody" is only figurative
Yes and they will not realize what is means on a tuple, or IO a, etc.
I always found the name Functor confusing until I started to think about the name as an analog of Divisor. While the divisor is the last argument for the division the Functor is the last argument to a function.
`TypeApplication` is a very nice extension, but it would be great if you can use `Proxy` to be able to relax the restriction. And thank you for the example of how Boltzmann is used. 
&gt; It's even worse for foreigners. I only realized with Haskell that "to map" was not only about geography. I just checked this, the French word for this use of `map` is "application" :)
To expand, they will immediately understand that it's related to something they know well, and might not realize it's not the exact same thing. All monads are mappable. I use `map` on containers. Monads are containers!
"Monomorphism" is an overloaded term and doesn't have, in the MR context, anything to do with category theory. In this case, it's simply meant as the denial of polymorpism.
Welcome! Do you want an existing project or do you want to start from a blank page? If it's from a blank page, I've a few on my todo list, for example: * a [DXF](https://en.wikipedia.org/wiki/AutoCAD_DXF) parser (maybe who can convert to svg or to [hgeos](https://hackage.haskell.org/package/hgeos) datatypes); * a [fltkhs](https://hackage.haskell.org/package/fltkhs) demo using OpenGL, or even [GPipe](https://hackage.haskell.org/package/GPipe). I don't know if this one is *tiny*. 
Enabling Haskell to work in more scenarios is hardly ruining it. It's strictly better, isn't it?
It's really extreme to claim that mysql isn't an actual database. It is most certainly a database. I'd agree there are better ones - notably Postgress - but there's no need to put down people who are using it. Edit: mysql (not php)
&gt; Haskell will never be popular. You think this is good?
Persistent is not bound to any particular DBMS (in fact, Persistent also supports some NoSQL backends). There is a persistent-mysql package on hackage which seems to use mysql-simple under the hood.
I'd use a remove index. data Remove xs x ys where RemoveZ :: Remove (x ': xs) x xs RemoveS :: Remove xs x ys -&gt; Remove (y ': xs) x (y ': ys) `Remove` provides a constructive proof that removing `x` from `xs` gives `ys`. Then you can extract and remove that index from a record. remove :: Remove xs x ys -&gt; Rec f xs -&gt; (f x , Rec f ys) remove RemoveZ (x :&amp; xs) = (x , xs) remove (RemoveS ix) (x :&amp; xs) = let (y , ys) = remove ix xs in (y,x :&amp; ys) Then you can use a pair of remove indices which match on the type they're removing. innerJoinOn :: Eq (f x) =&gt; [Rec f xs1] -&gt; [Rec f xs2] -&gt; (Remove xs1 x ys1 , Remove xs2 x ys2) -&gt; [Rec f (x ': ys1 ++ ys2)] innerJoinOn tbl1 tbl2 (ix1,ix2) = [ x :&amp; ys1 &lt;+&gt; ys2 | (x , ys1) &lt;- map (remove ix1) tbl1 , (x2 , ys2) &lt;- map (remove ix2) tbl2 , x == x2 ] Hope that helps; I didn't check it with GHC.
nice
@takeover: Someone stepped up in April as a new maintainer, yet the author never even replied (see https://github.com/bos/hdbc-mysql/issues/17). so nothing happened :/ This is really frustrating, as sometimes, you don't have full control over your stack, because you have to integrate with legacy data. Yes, I'd prefer postgresql, but this doesn't work in this case.
Okay, I did not see that binding, thanks! Though it still looks very new, maybe it will mature over time. For me the main problem really is that I perceived Haskell as a cool language which, by now, has matured enough to be generally useful. To be generally useful, in my definition, includes that I can use it for simple applications I want to write at work. In this case it was "connect to mysql, run SHOW TABLES in a database and generate some SQL statements out of that and output them to the console". This is, in my opinion, a reasonable task to expect from any language which is "production ready", though, as outlined above, I really had problems with the mysql connection. And then we haven't looked at performance or thread-safety etc. of mysql-simple at all, which seem to be other problems. And not using postgres was just bad luck that my company uses MySQL, as do many others.
I recently posted about this: https://www.reddit.com/r/haskell/comments/4yhmye/looking_for_an_easy_real_world_haskell_project_to/ maybe you want to try your hand there?
These are examples where functional programming can guarantee correctness beyond what types can provide https://www.reddit.com/r/haskell/comments/4tagq3/examples_of_realworld_haskell_usage_where/d5g3cct?st=is6asuvk&amp;sh=dc02934f It is discouraging that the only functional programming concept developed and accepted widely in web programming in haskell after many years are the formlets, when there are opportunities in which functional programming in haskell can make a huge difference in componentization, correctness and faster development in relation with other languages. This is not a promise, this is real. [I've been there!](https://github.com/agocorona/transient/wiki/Transient-tutorial#transient-in-the-web-browser) ([two times](http://mflowdemo.herokuapp.com/)) ([no, three](http://tryplayg.herokuapp.com/))
Just for the benefit of the OP, I'll point out that /u/echatav outlines this in greater detail here: https://github.com/VinylRecords/Vinyl/pull/93#issuecomment-241170759 
The author of [mysql-haskell](https://github.com/winterland1989/mysql-haskell) here. I'd like to ask you to use it and help improve it : ) it's already used internally in my company for a while. As for the postgresql vs mysql thing, it depends on lots of factors, for example, if your team have many mysql DBAs, you may not want them to manage postgresql. All i can say is, MySQL is an industry proven solution, and it's as solid as any other DB techs. The mysql-haskell package intend to serve as a driver level between higher level ORMs and raw TCP layer, but that doesn't mean it's low-level to works with, actually it provide the same level functionality as mysql-simple: `connect/query/execute` and so on, so basically you can start using it like this: {-# LANGUAGE OverloadedStrings #-} module Main where import Database.MySQL.Base import qualified System.IO.Streams as Streams main :: IO () main = do conn &lt;- connect defaultConnectInfo {ciDatabase = "testMySQLHaskell"} (defs, is) &lt;- query_ conn "SELECT * FROM person" print =&lt;&lt; Streams.toList is Above code is found on a [user's blog](http://qiita.com/lotz/items/1d9c7b4333fd4d29a150), which also have some comment on this library. The only hassle is that we need new `binary` package(&gt;= v0.8.4) for fast `getFloat/getDouble`, which is not present in stackage snapshot yet, so if you use stack to manage your project, you may add these lines: extra-deps: - binary-0.8.4.1 - tcp-streams-0.4.0.0 - HsOpenSSL-x509-system-0.1.0.2 - wire-streams-0.0.2.0 - word24-1.0.7 As for other dependencies, i'll try to get them into stackage nightly soon. I really wish someone can join me and works on some glue code like `persistent-mysql` or `groundhog-mysql` though, because my time is just too limited to get them all running.
Actually i do want to do it too, because GHC's IO manager is very capable, i'm sure it's comparative to FFI version. But for now, all i wish is that i can get some time someday.
Actually we have a assessment on MySQL and others, and we choose it not because it's PHP friendly, our company have codebases written in PHP, Java, C++ and Go, and now Haskell, which C++ and Go dominate. In short, the choice of MySQL is because InnoDB delivery very stable write through-put under heavy traffic, and we have our own MySQL pool solution for PHP frontends which can't do pooling on their own. This is not the right place to start a discussion on MySQL vs PostgreSQL anyway, so please do not take my word as argument, you should always choose your DB solution based on your own requirement, and do benchmarks to verify!
Very clever. I'm looking forward to hearing about how this works out in practice.
Namely, 1.6 (`Applicative =&gt; Monad`) and 3.1 (`if then else` indentation within `do` &amp; co).
Can you elaborate on the C4 collector's pitfalls? Azul doesn't give any technical details aside from: Java GC that doesn't pause; give us your money.
Sounds great!
I would use the `mysql`, `mysql-simple`, and/or `persistent-mysql` packages to communicate with MySQL. Those packages all work fine in my experience, and a number of people are using them. MySQL isn't as well supported as postgres, but MySQL issues submitted to persistent-mysql do usually get solved.
Incidental bikeshedding: should you really derive `Num` for a `UserId` - adding or multiplying user ids shouldn't be a valid operation.
Ya correct. That's a mistake, will remove it in the next commit.
I haven't look at your code in details, but does the API defers from `mysql-simple`. If so, is there any benefit or could you stick to an API compatible with `mysql-simple`? That way, your library could be used as a drop-in replacement for `mysql-simple` (once backpack is operational).
Reading over it again- maybe not yet ;) It needs a few tweaks before it can serve as a minumum viable example. I'll fix it up after I've finished my assignment 
The simplest way to break down `Cofree` is to use `extract`. Another way is to use `extend` to reannotate the tree based on local information, and then `extract`. If there is a functor `g` which pairs with your functor `f`, then `Cofree f` will pair with `Free g`, which gives a nice monadic interface for extracting values deep in the `Cofree` structure: type Pairing f g = forall a b. f (a -&gt; b) -&gt; g a -&gt; b freePairing :: Pairing f g -&gt; Pairing (Free f) (Cofree g) For example, if you have a stream of values, given by `Cofree (, a)`, then you can use the free monad for `(a -&gt;)` to extract a value at some index. At each step, you can either `return` a function to extract a value from the current position in the comonad, or `liftF` a function which tells you how to handle the next value in the stream. Related: [The Cofree Comonad and The Expression Problem](http://comonad.com/reader/2008/the-cofree-comonad-and-the-expression-problem/)
You can try #haskell-beginners - get an invite to fpchat at: http://fpchat.com 
The main #haskell channel on freenode is *very* active, try there.
If you're familiar with Erlang syntax it's an extremely straightforward and friendly way to manipulate binaries. I miss it when not using BEAM languages :)
I think the type is `uncofree :: Functor f =&gt; (a -&gt; f b -&gt; b) -&gt; Cofree f a -&gt; b`. I have no idea what the name would be in this scheme. There are already two `unfold`s, for building up `Free` and `Cofree` values respectively: unfold :: Functor f =&gt; (b -&gt; Either a (f b)) -&gt; b -&gt; Free f a unfold :: Functor f =&gt; (b -&gt; (a, f b)) -&gt; b -&gt; Cofree f a so `fold` leaps to mind, but it's already being used by `Free` for something slightly different. To me this function seems most consistent with `iter` (via the `f b -&gt; b` function that summarises the contents of the Functor), but I'm not sure that's the right name either.
Since you mention Slack, checkout the #haskell channel at http://fpchat.com/
The operations are under the same name, but mysql-haskell comes with prepared statement support, and use io-stream for streaming result set, so it's not a drop in replacement .
In fact, "monomorphism" from category theory has a completely different meaning!
If you are just looking for things to dive into, take a look at Stephen Diehls list: http://dev.stephendiehl.com/hask/ Most of that stuff even with proper application :)
Maybe. As long as I have someone to critique my work habit/tool use and point out at least some things I don't know I'm lacking, everything else is just bonus.
I found solving the problems of [Advent of Code](http://adventofcode.com/) very entertaining and useful for learning some things. Lots of simple parsers to build (and getting experience with Parsec), some simple matrix manipulations (and playing with hmatrix and linear), very nice. But to counteract your feeling, I know almost nothing about webdev, and I have the impression that Haskell is well developed in this area. Servant in special seems really interesting.
Yeah, just as nice and familiar as Functors in ML or Functors in C++ right? Even though each one means a different thing :)
&gt; Do you know how high the costs would be? In practice I don't think the loaded value barrier is that expensive these days, compared to basically any other concurrent implementation technique I think it wins. &gt; I assume it costs less than reference counting *Everything* costs less than reference counting. ;) But in this case it costs less than any other fully concurrent collector technique that I know of as well. In the special case of Haskell we get to know a lot about the structure of our heap though. As an example, values in * that are evaluated can be copied in the mark-compact phase _without any locking_, which rather drastically changes the equation for how expensive moving around most of the heap is in practice given that much of the heap is made up of such values at any given moment in time. For me in C++ I tend to use a lot of lock-free and wait-free data structures. This drives me to using safe reclamation techniques such as EBR, QSBR, hazard pointers, etc. anyways. Those have the benefit of not paying for the live heap size at least compared to GC, but aren't nearly as broadly applicable.
It is? Forgive me (and 99% of the population) for not realizing this.
Oh my... this appears to be exactly what I want. I must confess I'm new to type level stuff and even GADTs. How do I construct a Remove type that satisfies (Remove xs1 x ys1, Remove xs2 x ys2)? I spent a good 45 minutes trying to figure it out and can tel that the remove function itself is what will give me the (Remove xs1 x ys1, Remove xs2 x ys2) type, but can't seem to get syntax right. I'm familiar with and have implemented Peano Numbers, but this is my first time seeing Peano Indices. Thanks for your help. I look forward to figuring this out more. EDIT: I did figure out simple things such as: λ&gt; :t remove (RemoveS 1) (head peopleRows) remove (RemoveS 1) (head peopleRows) :: Num (Remove '['Name, 'Age] x ys) =&gt; (Attr x, Rec Attr ('Id : ys)) λ&gt; remove RemoveZ (head peopleRows) (id: 1,{name: "Joy", age: 28}) λ&gt; :t remove (RemoveS 1) (head peopleRows) remove (RemoveS 1) (head peopleRows) :: Num (Remove '['Name, 'Age] x ys) =&gt; (Attr x, Rec Attr ('Id : ys)) λ&gt; -- though not sure how to get this value 'back' 
Thank you. That gives me a lot more to learn about :)
`Remove` is just a natural number denoting the position, so if you know the position of the `id` type in both records, say 3 &amp; 1 then you can construct the pair `(RemoveS (RemoveS (RemoveS RemoveZ)) , RemoveS RemoveZ)`. As /u/andrewthad mentioned, the `RElem` machinery just obscures the fact the fact that `Rec` is like a list which can be positionally indexed. Unfortunately, there's not enough information in `RElem` to construct a `Remove` index into a `Rec`, so either you can just construct it directly like I just wrote, or you can create some new class along the lines of my PR that finds the first such index into a list. innerJoinOn [Identity 1 :&amp; Identity "foo" :&amp; Nil, Identity 2 :&amp; Identity "bar" :&amp; Nil] [Identity 1 :&amp; Identity 'a' :&amp; Nil, Identity 2 :&amp; Identity 'c' :&amp; Nil] (RemoveZ , RemoveZ) === [Identity 1 :&amp; Identity "foo" :&amp; Identity 'a' :&amp; Nil, Identity 2 :&amp; Identity "bar" :&amp; Identity 'c' :&amp; Nil] ^ not verified. I didn't figure out `Remove` but was pointed out that it's in the [type-combinators](https://hackage.haskell.org/package/type-combinators-0.2.4.3/docs/Data-Type-Remove.html) library.
A talk about what Facebook uses Haskell for, as well as why: https://www.youtube.com/watch?v=mlTO510zO78
Thanks for the help. Here is the compilable version of your code above: λ&gt; innerJoinOn2 [Identity 1 :&amp; Identity "foo" :&amp; RNil, Identity 2 :&amp; Identity "bar" :&amp; RNil] [Identity 1 :&amp; Identity 'a' :&amp; RNil, Identity 2 :&amp; Identity 'c' :&amp; RNil] (RemoveZ , RemoveZ) == [Identity 1 :&amp; Identity "foo" :&amp; Identity 'a' :&amp; RNil, Identity 2 :&amp; Identity "bar" :&amp; Identity 'c' :&amp; RNil] True 
Commercial Haskell runs the gamut from web frontends to web backends to data analysis tools...many of the same things a company might use C# or Java for. Here's a talk about a commercial user of Haskell: https://www.youtube.com/watch?v=BveDrw9CwEg
You're right that hdbc-mysql seems sadly abandoned. I suppose somebody should initiate the package takeover process with it (https://wiki.haskell.org/Taking_over_a_package) That said, the fact that some packages on hackage are bitrotted doesn't mean they all are. To the contrary, there are a zillion other bindings that seem available: http://hackage.haskell.org/packages/search?terms=mysql And if you're set on staying with HDBC, then the hdbc-odbc package in conjunction with the mysql odbc connector may do the trick as well?
We adopted Haskell 4 years ago. My co-founder was playing with Haskell for around a year before that and he found the language to be very expressive, concise and its type system top notch. Since then we have used Haskell for * Web apps * REST services * EDSL * Compiler (A subset of Haskell to Java, Swift, C#) Apart from the reasons haskellers might give you on why they love it, as a CEO running a company I realized the following things, * With Web apps and REST services, I have heard people say things are tougher than it should be. Maybe it is. In our opinion it is not. But what is undeniable is that iterations/changes are far more easier and productive in Haskell. We have built what are pretty complex web apps/REST services for our clients and since we work with start-ups, features change all the time and moving fast is important and our experience with making iterations and changes has been great with Haskell. * With EDSLs and compilers we wrote, we know for sure that had we used any other language things might have been many times tougher and would have taken more time. * With better tools you can always solve bigger problems. Though Haskell may not be adopted by main stream developers in the near future, it can play a significant role in solving problems that exists in the development world beyond its borders. 
Meme posts are not allowed on /r/haskell. *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/haskell) if you have any questions or concerns.*
Interesting indeed.
Reading through the link. Getting confused between "term-level", "type-level" and "kind-level". &gt; but there is no way to directly manipulate a term-level representation for said PGBool in Haskell like we could manipulate an Int, a Bool or (). Much like the t in Tagged t a, PGBool and similar types exist only at the type-level, they have no term-level representation in Haskell at runtime. In fact, since these types have no term-level representation, they could have had a kind different from *, but for reasons beyond the scope of this article * works better when you have to assume an open world, so you will find that PGBool and similar have kind *.
To provide a different flavour of answer: basically anything where the primitive tools needed can be accessed with some simple C-bindings, and almost nothing where the task at hand requires integrating with an existing systematic "way of doing things", i.e. Android, iOS, Unity, etc.
One of the difference that i could see is, opaleye-tf provides monadic interface over the top of Opaleye Arrow interface. And, I could not see opaleye-tf tracking type changing operations (projection, aggregations..) on the db models.
Is this some kind of cata? uncofree :: Functor f =&gt; (a -&gt; f b -&gt; b) -&gt; Cofree f a -&gt; b cata :: Functor f =&gt; ( f b -&gt; b) -&gt; Fix f -&gt; b uncofree f x = f (extract x) $ map (uncofree f) $ unwrap x cata f x = f $ map (cata f) $ unfix x 
I don't know anything about what interests you, and Haskell gets used for a wide variety of purposes. I suggest that you search github for "language:haskell" and browse through the more popular results.
For me it's restful servers. Haskell is so much easier to maintain that it greatly justifies any excess up front time you may have to spend getting everything setup
We have been using Haskell for the backend since the beginning (~1.5 years ago) at [GetShop.TV](http://www.getshop.tv) which is basically an ad server &amp; platform. Here's a few advantages we have observed so far: * **Haskell is exceptionally good for junior developers**. Well-designed types and no implicit side-effects guide less proficient colleagues, helping them fit in faster and get a better understanding of the project. Also it's hard for them to mess something up without compiler noticing even before tests or code review. Turns out compiler can be a good co-mentor in a commercial project! * **Haskell helps with frequent code changes**. As a startup we often need to add new functionality or change old one. A couple of times we had to refactor most of the codebase and it was always quick and simple. * **Haskell helps write less code**. Apart from just being concise, Haskell allows many things (boilerplate) to be derived automatically. Among those are typeclass instances (JSON serialization, representation for DB, Swagger schemas, etc.), tests (property-based checking), even tests for the whole REST APIs (see [`servant-quickcheck`](https://hackage.haskell.org/package/servant-quickcheck-0.0.0.0/docs/Servant-QuickCheck.html) and [`Servant.Swagger.Test`](http://hackage.haskell.org/package/servant-swagger-1.1.1/docs/Servant-Swagger-Test.html)). * **Haskell makes code reviews easy**. Explicit side-effects, well-designed types and concise code make it fairly easy to check another person's work, leaving mostly only logic to be checked. Since we have to deal with small amounts of JavaScript too (unfortunately, for now), the difference is very noticeable.
The type-changing on projections with Voids and type-level lists and whatnot in dbrecord-opaleye seems really nice. I look forward to trying it out. It would be really nice to have an example of table creation... it looks this exists in the dbrecord package, but an example that includes e.g. setting primary keys and uniqueness constraints would be great. That's the biggest thing for me that's been missing from opaleye.
For those looking for the hackage link https://hackage.haskell.org/package/hScraper-0.1.0.0
And the monadic variants: cataM :: (Traversable f, Monad m) =&gt; ( f a -&gt; m a) -&gt; Fix f -&gt; m a cofreeCataM :: (Traversable f, Monad m) =&gt; (b -&gt; f a -&gt; m a) -&gt; Cofree f b -&gt; m a cataM f x = f =&lt;&lt; traverse (cataM f) (unfix x) cofreeCataM f x = f (extract x) =&lt;&lt; traverse (cofreeCataM f) (unwrap x) anaM :: (Traversable f, Monad m) =&gt; (a -&gt; m ( f a)) -&gt; a -&gt; m (Fix f) cofreeAnaM :: (Traversable f, Monad m) =&gt; (a -&gt; m (b, f a)) -&gt; a -&gt; m (Cofree f b) anaM f x = (\ y -&gt; Fix &lt;$&gt; traverse (anaM f) y) =&lt;&lt; f x cofreeAnaM f x = (\(z, y) -&gt; (z :&lt;) &lt;$&gt; traverse (cofreeAnaM f) y) =&lt;&lt; f x hyloM :: (Traversable f, Monad m) =&gt; ( f c -&gt; m c) -&gt; (a -&gt; m ( f a)) -&gt; a -&gt; m c cofreeHyloM :: (Traversable f, Monad m) =&gt; (b -&gt; f c -&gt; m c) -&gt; (a -&gt; m (b, f a)) -&gt; a -&gt; m c hyloM f g = cataM f &lt;=&lt; anaM g cofreeHyloM f g = cofreeCataM f &lt;=&lt; cofreeAnaM g 
the main branch have evolved substantially and doesn't adhere to the approach described in the article, where essentially a free monad is used to build a probabilistic computation. the article's approach is in the 'haskell2015' branch
I can't speak to vim/emacs, but what you're describing is typically handled by an IDE, where as the development mode of Haskell at the moment tends more towards "Editor plus". Having said that, if you [initiate a new project with stack](https://github.com/commercialhaskell/stack/blob/master/doc/GUIDE.md) it will set up this structure for you. What an IDE would do for you (and eclipsefp does/did) is that if you add a *new* file, it would update your cabal/stack file correctly. I don't know if the vim/emacs plugins do this for you. I use Atom+plugins myself and have to do this by hand.
Which company do you work for? Would it be possible to share links of webapps that you have built?
[removed]
Here is example module which has table creation code https://github.com/byteally/dbrecord/blob/master/src/Lib.hs Table class is where you can declare all the information related to Table like Defaults, Check constraint, PrimaryKey, ForeignKey, Unique. GHC 8 TypeError feature is used wherever possible to get nice domain specific type errors like for example • column "id1" does not exist in table Profile • In the instance declaration for ‘Table TestDB Profile’ another example • check constraint "notnull1" does not exist on table DBTable "user" 
Backend stuff in general. Validating posts on Facebook was one of the tasks. All posts are passed through a filter that scans links and decides if they're "safe" or to decide if it is spam.
That is interesting! I like the idea of making Bayesian probability type based, it seems like a natural progression (although I would never have thought of it before.)
That is Explict type application extension. In GHC 8, you don't have to use Proxy to fix the Type Argument for example, following code pick the Bool instance of read Prelude&gt; :set -XTypeApplications Prelude&gt; read @Bool "True" True 
From what I understand, this theory is also more general than classical probability in that it can include observations that influence the state of what's observed, as in quantum experiments. Mind boggling
See also https://www.youtube.com/watch?v=sl2zo7tzrO8 and https://www.youtube.com/watch?v=jG9PWdV1wso for more technical details.
There is a module for this in hmatrix: http://hackage.haskell.org/package/hmatrix-0.17.0.2/docs/Numeric-LinearAlgebra-Static.html
I would say something like extracting a square of integers with the appropriate size and filter by norm: circleQuarterInt x = let n = floor x in filter (\(a,b) -&gt; fromIntegral (a^2 + b^2) &lt;= x^2) ((,) &lt;$&gt; [0 .. n] &lt;*&gt; [0 .. n]) The square can also be build with a list comprehension: `[(a,b) | a &lt;- [0..n], b &lt;- [0..n]]`, but I used the `Applicative` instance for `[]`. The number of `Double`s is infinite in practice, filtering all of them would be impossible.
This can maybe help you: https://github.com/rhysd/ghci-color
I wrote a literate example of doing this using Yesod as the "host". https://github.com/yamadapc/yesod-servant-example#readme
Try making `Handler` a newtype instead of a type synonym. With a type synonym, there is no difference between the types `Handler Admin ()` and `Handler User ()`, so when you use `putMsg` it is ambiguous whether you want one or the other, despite the type annotation.
The abstract: https://arxiv.org/abs/1511.09230v1
They built a monad for handling requests to data sources that implicitly parallelizes and caches fetches, so that you can write trivial do-notation code and get those features for free.
Good suggestions both of you. Thanks!
It looks like this is just the state monad or something.
Just to make your code more readable ;D scrap your (scrap your boilerplate) boilerplate?
I'm seeing the same type of warnings; this puzzled me because what I see aren't even transitive dependencies of my project. I hope someone from the Stack team can comment on this?
I manage my projects with cabal. The extra bookkeeping is made worth it just for the automatic management of all the little `.hi` and `.ho` files. The cabal file goes in `$PROJECT/projectname.cabal` and my source goes into `$PROJECT/src` My $PROJECT/.vimrc : set makeprg=cabal\ build set path+=src/** The `path` here allows you to use vi's `:find` command to open and edit files, without typing the whole path to them. The `makeprg` setting lets you use the `:make` command within vi (do `:split` then `:copen` to open up the build error window). For `:make` to work right, `vi` needs to be invoked while you are sitting in the `$PROJECT` directory. So I have a couple of utilities that make it easy to reach down into the deeply nested module directories and edit files. In my `~/.bashrc`: function fvi { vi -c ":find $*" } function fless { less `find . -name "$*"` } _completefvi() { local curw=${COMP_WORDS[COMP_CWORD]} local wordlist=$(git ls-tree -r --name-only HEAD | /usr/bin/perl -e 'while(&lt;&gt;){print "$_";while (s#^[^/]+/##){print "$_";}}') COMPREPLY=($(compgen -W '${wordlist[@]}' -- "$curw")) return 0 } complete -F _completefvi fless complete -F _completefvi fvi This creates two commands, `fvi` and `fless`, with command line autocompletion, to view or edit any file tracked by the local .git repository. They won't quite work right if you have multiple modules with the same filename in different directories, but that's not usually a problem for me. My typical process is: 1. `fvi ModuleNam&lt;tab&gt;&lt;enter&gt;`, which completes the filename and invokes `vi`. 2. edit edit edit save `:make`, `:split`, `:copen`, edit, edit, save, `:make`, exit 3. `git add .`, `git commit -v`
Yep, [Haxl](https://github.com/facebook/Haxl). It's a pretty nice piece of work. 
At [LeapYear](http://leapyear.io/) we use Haskell nearly exclusively to develop tools to enable secure, privacy-preserving data analysis. The main advantages of Haskell we find is the low translation cost between mathematical research papers and code and the facility for writing embedded domain specific languages.
I'd be interested in automating a headless web browser in Haskell. Is their a publicly available package for that which you're using? 
Yeah, the filesystem management should be easy to automate with an editor function to rename a module.
Thanks for the config. If someone has the same for Emacs, would be grateful if you can share it.
which is pretty cool! I'm not sure if Haskell is the tool of choice when it comes to physics, but I'd imagine it would be pretty handy. Especially since Haskell code generally tends to last forever.
It puzzled us for quite a while too: https://github.com/commercialhaskell/stack/issues/2175 The core problem, IIUIC, was that `stack` cloned the [all-cabal-hashes repo](https://github.com/commercialhaskell/all-cabal-hashes) with `--depth 1`. Therefore the local git repos didn't contain older package revisions whose git SHAs had been specified in some snapshots, resulting in these warnings. The fix was to clone the full repo. stack-1.2.0 will be released very soon.
We're using Haskell for a web API and backend workers that need to be highly concurrent. Performs so good
We've only started using Haskell at Target and still have a relatively small team. We're developing a framework for supply chain optimization. The goal is to make it easier to build and work with models of parts of the supply chain, probably with a mix of DSLs, Hasklell libraries and tools. We want the actual models to be easier to write *and* more modular and reusable, and we want to write systems that help us solve larger, more complex problems. This is a very new project and it's still mostly in an exploratory state, so I don't really have anything more specific to say about it. I'm sure I'll have interesting, Haskell-specific stories a few months from now :).
You are right. It gives me a warning as well. I was probably tired when I checked. So that solves my problem why I only get a warning ... Anyway, do you know what is the justification behind a warning only ? I mean is it a bug or a design decision ? It totally throws away the if it compiles it works motto ;-)
Re transitive dependencies: Stack is just loading all cabal files for the entire resolver you're using—it's part of `stack update`.
It's clearly intentional, since the compiler goes through the trouble of generating a custom error message. I wish there was a flag to turn that (and also non-exhaustive pattern-matching warnings) into an error. Time to turn on `-Werror`, I guess...
Not to ambush you, but (proceeds to ambush you) What's the deal with cookies/client sessions? There have been closed PRs (e.g. https://github.com/haskell-servant/servant/pull/88) for that since last year. Lack of cookie/session support is one of the reasons I don't use Servant. We gave it a kick at my current company and had to stop when we saw what was missing and cookies/auth support was a part of that. I'm not keen to do the last 20% of the existing solutions (like Sean's) when something official will come along later. Would you please consider dusting off and integrating that PR if you don't have something about to land?
Oooh interesting. Thanks!
&gt; Do I need to worry this will cause problems later on? Those are warnings and they're usually harmless (say, 90-95% of the time), though this *can* in very rare cases change a working stack.yaml into a failing one. But older stack releases didn't even try to make builds so reproducible. The highlight is: that bug might lead to using, for the *same* package version, a slightly newer version of the cabal file than in the snapshot—that mostly means the package author adjusted the version bounds because of new releases of dependencies. Again, this does not change the *code*, only the cabal file. Which can can theoretically mean changing GHC options for the library. For yet more discussion on the effects on build reproducibility, see https://github.com/commercialhaskell/stack/issues/2217.
How complete is the CSS selector support?
I use Haskell for scripting some tasks. One script I did involves comparing two XMLs with same structure by tag attributes and values (it outputs a CSV of values with the same unique ID if they mismatch, or it outputs the ID and value if they are unique in one). I wrote it in Python, wrote it again in Python, and then I wrote it in Haskell. I look forward to automating more with Haskell since helps produce interesting tools for me.
only class and id as of now
This was not immediately clear to myself either ... so I worked around it with a simple WAI middleware that does the auth validation via reading a cookie and sets a header with the user id. You just have to configure your proxy to remove this header before it gets to your web app, otherwise your authentication is trivial to bypass. 
Hmm, it's possible I'm using it wrong, but it's not creating parenthesis around nested data types. That is, instead of: LogEntry 2016-08-23 19:22:28.5049337 UTC (AdjustProduct (Product (Location "Warehouse 1") ... ... I'm getting: LogEntry 2016-08-23 19:22:28.5049337 UTC AdjustProduct Product Location "Warehouse 1" Which isn't `Read`able.
&gt; What exactly do companies use Haskell for? Mainly as a filter, to check if people are smart. The logic being: if they are fluent in Haskell, then they must be pretty smart.
Probably better not to include a version number, just in case: https://arxiv.org/abs/1511.09230
&gt; Which doesn't implement any actual common authentication schemes. The tutorial example is basic auth which is almost never used in APIs or web apps these days. No, the basic auth thing is separate. The "generalized auth" thing is what I'm talking about. It should support most (if not all?) auth schemes. We're lacking out-of-the-box implementations for those but the generalized auth thing is recent so we might need to give users/contributors some time to implement those and send PRs our way. Or to implement it ourselves, when we need them. &gt; Do what you want, but much of my point isn't about the particulars but that this has remained unsolved for so long. Most use cases we heard about for cookies were related to auth, which got addressed with the thing I mentionned in my previous paragraph. Like I said, we want to provide out-of-the-box support for several auth schemes eventually, but it'll surely come as more and more people use this new machinery. Regarding raw cookie, yes, it shouldn't have stayed unsolved for so long but we wanted to address auth-related use cases in priority as this was the priority for many contributors and users. I guess now's a good time to get back to thinking about raw cookies =) &gt; From what I'm gathering from you on IRC, the known-implemented auth solutions are [...] The only one provided out of the box is basic auth. The rest is either implemented in people's private/public projects or not implemented at all yet. I'm expecting several schemes to be supported out of the box as people write servant support for those using the new auth machinery. I agree that it's often hard to get those things right, we just need some more time to get support for those things written and merged, as this is all open-source, not funded work. Also, having people voice their needs helps us prioritize, so feel free to abuse our issue tracker for this purpose. 
Haskeller from Chennai, very good to hear!
what is servant API if I may ask? I clicked around a ton but I wasn't able to get a really basic introduction to what exactly is the servant API.
We have a core service written in Haskell. It was an excellent choice because it has been extraordinarily stable and has provided exceptional performance and maintainability - not that it has needed a lot of maintenance. We are an Ed-Tech startup 
I see folks recommending IRC, if traditional irc clients are not to your liking take a look at services like cloudirc. They let you join the normal IRC channel but with a client that you *might* like better.
Not really a solution, but for me all log output was gone when I upgraded to the latest LTS (currently 6.13). Although I still experience long delays when building, e.g. before anything build related gets printed to the console.
Ah, there it is, I was using `genericShowb` and not `genericShowbPrec`. Thanks!
Looks like `cofreeAna` is named `unfold` in `Control.Comonad.Cofree`. Thanks for writing out the recursion schemes function with the analogous Cofree functions. That's a pretty helpful way to think about things.
I had to overcome the exact same issue for one of my projects, I've reached a satisfying, yet imperfect solution that relies on [`foldl`][2] library and prisms. First, let's define a sum datatype to represent each single piece of information needed to build `MyData`, and the corresponding prisms: data MyDataPiece = DataA1 A1 | .. | DataAn An -- Prisms will be useful later makePrisms ''MyDataPiece Now we can define a piece parser: parseDataPiece :: Parser MyDataPiece parseDataPiece = choice [ DataA1 &lt;$&gt; parseA1 , .. , DataAn &lt;$&gt; parseAn ] We are now able to parse a heterogeneous list of pieces (`[MyDataPiece]`), the next step is to applicatively (is that a word ?) feed them to the data constructor `mkMyData`, while dispatching each piece to the right position. That's exactly what [`handles`][1] does, and it requires 2 things: - how to transform each item (a `Handler`); using a prism will result in filtering pieces with respect to their type; - what to do if we have multiple items of the same type (a `Fold`); this is the ambiguity you mentioned, and I personally find it very elegant that the way we lift this ambiguity is completely encoded in a single `Fold` function; check out [`Control.Foldl`][1] for various primitives to build the function you desire. The implementation looks like this: import Control.Foldl (fold, handles, lastDef, sum) parseMyData :: Parser MyData parseMyData = do pieces &lt;- many parseDataPiece return $ flip fold parseDataPiece $ mkMyData -- assuming, for the sake of the example, that you want to keep the last A1 instance you receive, or a default value &lt;$&gt; handles _DataA1 (lastDef defaultA1) &lt;*&gt; .. -- assuming, for the sake of the example, that An is a Monoid and you want to sum all An instances &lt;*&gt; handles _DataAn sum The [`foldl`][2] library also provides monadic versions for these tools (`handlesM`, `HandlerM`, `FoldM`), useful when you want to include monadic effects (e.g. have the parser fail when some `Ai` is missing). I'm not showing it here, but you get the idea. So far, this pattern has worked fine for me, in terms of correctness and maintainability. I'm not sure the performance is ideal though... What do you think ? [1]: https://hackage.haskell.org/package/foldl/docs/Control-Foldl.html [2]: https://hackage.haskell.org/package/foldl
Careful you don't go too far down that route. I find that I'm very vulnerable to overengineering in Haskell. One time, rather than sticking with a simple AST, my coworker and I came up with our own variation on finally tagless style, which didn't work as well.
Building a web API is certainly not a poor use of Haskell. It'll help introduce you to higher level concepts like error handling, monad stacks, logging, managing a DB connection, etc. You'll definitely learn a lot about building a practical Haskell application.
May I also shamelessly plug my own library [hxt-css](https://hackage.haskell.org/package/hxt-css), which has support for more CSS selectors and works correctly for all of them (I think).
&gt; Haskell is exceptionally good for junior developers. You think Haskell code in production (i.e. with real side effects, various monads/monoids/applicatives in action) is easier to understand for the uninitiated than the equivalent code written in say Python or Java? I find this argument specious. Otherwise the other 3 items I think are fair. I'm wondering given the existence of purescript and elm why you're still using js.
99% of the population doesn't know about "map" in the programming context either. 
Thank you! I'll probably try it in some of our packages next week.
I first looked at your site several years ago and honestly, I still have no idea what it's really for or who uses it. Are you guys growing? Is there some niche somewhere in which it's the go to community site?
You may; and if it can promise a `+` that doesn't crash and a `&gt;` that doesn't prune, then I'm almost certainly switching to it next time I mess with my scraper. :)
http://blog.silk.co/post/148741934972/silk-joins-palantir :) You can still try out Silk if you want to see what it's about. We have some good tutorials and videos available.
Go is looking really impressive in all the tests. It is disheartening to see Haskell so far in the bottom. It'd be nice if Haskell could more actively promote itself as the language of choice for web and cloud, but benchmarks like that don't give potential developers a lot of confidence in Haskell.
&gt;But what is undeniable is that iterations/changes are far more easier and productive in Haskell.. In your opinion, what feature of Haskell helps mostly in this regard? I mean, Is it just due to static typing or is there something more?
I would say it is because of Haskell's type system. 
I probably wont even code in Haskell for months to come. My post talks about using Haskell from a CEO's point of view. That is clearly stated in my post. It's a pleasure sharing my experience. 
&gt; Nevertheless it is not good to see Haskell placed below ruby. The Haskell benchmarks are currently not optimal : * most of the servant ones did not work (I think /u/jkarni is on it) * I suppose the others are hindered by concurrency problems with the DB drivers It is expected that the servant results will be good. Supposedly, a huge part of the performance gain comes from using an efficient SQL library. There recently have been a release of a pure Haskell implementation of the MySQL protocol that was twice as slow for single queries than `mysql-simple`, but that would scale much better. Perhaps this could be a way for benchmark gamers. But even with all those excuses (and taking into account the fact that the "ruby" things are mostly a thin layer of ruby over a large paste of Java stuff), it's terrible to lose the benchmark games to Ruby!
Huh. GHC happily accepts it with a variable `a` but not with a concrete `Int`: test2 :: (Coercible a (f b)) =&gt; a -&gt; f b test2 = coerce And you can also fool 8.0.1 (but not 7.10.2 nor 7.10.3) into accepting something equivalent to `test` through the usual type family trickery: type family HideTheCoercible a b where HideTheCoercible a b = Coercible a b class (HideTheCoercible a b) =&gt; HiddenCoercible a b instance (HideTheCoercible a b) =&gt; HiddenCoercible a b -- causes spurious "pattern match is redundant" warning, but works fine if you call it test3 :: (HiddenCoercible Int (f b)) =&gt; Int -&gt; f b test3 = coerce **edit** as I belatedly realised, this much simpler thing also works: test3 :: (Coercible Int a) =&gt; a :~: f b -&gt; Int -&gt; f b test3 evidence = castWith evidence . coerce Common theme here seems to be "prevent GHC from trying to figure out the type roles of `f`'s parameters until it has a concrete `f`"
Clearly we need to [tell dons that there is a new shootout](https://ro-che.info/ccc/images/shootout.png) site! 
Some thoughs about these poor results? The only clear result: haskell-wai is about 39 % of the best result in the plaintext benckmark that is the only one that measures the performance of the web server. That is not bad, but it is not something to be proud about. https://tldrify.com/jfv The rest are performance measurements of databases and database interfaces with different platforms. Why that is called "web framework benchmark" is a mystery for me. And the conclusion is that, for the major databases, the library interfaces of Haskell have a very bad performance. Since it is not possible to take a look at the source code there is no way to learn more from that. Maybe they are in a secret option or by paying some fees. Maybe we have to wait for a filtration of wikileaks next year. 
&gt; Why that is called "web framework benchmark" is a mystery for me. IMHO, this doesn't make sense either, but for another reason. Most implementations have been carefully hand-tuned by expert programmers to squeeze out the last bit of performance. This doesn't reflect the typical use of a web framework, where performance usually doesn't matter that much and most requests are performed using provided defaults. This benchmark doesn't help me answer the question: "how fast will a typical X application perform vs a Y application ?". At all.
[servant on TechEmpower benchmarks ](https://www.reddit.com/r/haskell/comments/4s6g1q/servant_on_techempower_benchmarks/?ref=share&amp;ref_source=link) 
About the JavaScript part: we are currently looking into PureScript and Elm to replace the existing JavaScript code. We did know about those languages when we started, however, we could not immediately use them (or GHCJS) for production. One place we had to use pure JavaScript is in interactive content for SmartTVs. The problem with SmartTVs is they are incredibly slow sometimes and don't have much memory (which is already used up by an app we run our content in). We also need the content to load as fast as possible and SmartTVs aren't fast at interpreting JavaScript either. So far it seems that Elm won't work for this purpose and we are tending towards PureScript (since it has no runtime and it's output is much more susceptible to further optimizations). Another place is automated call scripting. Yes, they use JavaScript for server-side call scripts. So far we have our scripts working, but if we'll have to change them in the future, we'll probably try to replace JavaScript there with PureScript too.
&gt; The only clear result: haskell-wai is about 39 % of the best result in the plaintext benckmark that is the only one that measures the performance of the web server. There must be some sort of benchmark-only optimization in the other entries as In the "JSON serialization" test wai is at 93.7%. 
When using a function like `div` as an infix operator, you need to surround it in backquotes, e.g. 10 `div` 2 rather than 10 div 2 For `printarr`, you may want to try printarr = unwords . fmap show and use `putStrLn` to print the string it returns.
Why is Yesod getting hammered (it's [below](https://www.techempower.com/benchmarks/previews/round13/#section=data-r13&amp;hw=azu&amp;test=db&amp;l=8vmdz3) Sinatra (Ruby) for godsake) [Here's the Yesod code](https://github.com/TechEmpower/FrameworkBenchmarks/blob/round-13/frameworks/Haskell/yesod/bench/src/yesod.hs) and here is the spec of the test. Could the problem be MySQL? Or is the lack of HTTP keep-alive? **Edit** * And why is [`getDb`](https://github.com/TechEmpower/FrameworkBenchmarks/blob/round-13/frameworks/Haskell/yesod/bench/src/yesod.hs#L163-L174) reaching out to the WAI layer directly? Why isn't it using the standard Yesod way? Will it make it even slower? * And looking at the [Sinatra source code](https://github.com/TechEmpower/FrameworkBenchmarks/blob/round-13/frameworks/Ruby/sinatra/hello_world.rb) I can confirm that it's not doing anything atypical or trying o micro-optimize with the benchmark's point-of-view. &gt; For every request, a single row from a World table must be retrieved from a database table. &gt; The recommended URI is /db. &gt; The schema for World is id (int, primary key) and randomNumber (int), except for MongoDB, wherein the identity column is _id, with the leading underscore. &gt; The World table is known to contain 10,000 rows. &gt; The row retrieved must be selected by its id using a random number generator (ids range from 1 to 10,000). &gt; The row should be converted to an object using an object-relational mapping (ORM) tool. Tests that do not use an ORM will be classified as "raw" meaning they use the platform's raw database connectivity. &gt; The object (or database row, if an ORM is not used) must be serialized to JSON. &gt; The response content length should be approximately 32 bytes. &gt; The response content type must be set to application/json. Note that specifying the character encoding is unnecessary for this content type. &gt; The response headers must include either Content-Length or Transfer-Encoding. &gt; The response headers must include Server and Date. &gt; Use of an in-memory cache of World objects or rows by the application is not permitted. &gt; Use of prepared statements for SQL database tests (e.g., for MySQL) is encouraged but not required. gzip compression is not permitted. &gt; Server support for HTTP Keep-Alive is strongly encouraged but not required. &gt; If HTTP Keep-Alive is enabled, no maximum Keep-Alive timeout is specified by this test. &gt; The request handler will be exercised at concurrency levels ranging from 8 to 256. &gt; The request handler will be exercised using GET requests. 
[removed]
No, never *cough*
Understandable. I likewise tried elm and passed but not for performance. Their DOM processing is actually quite fast and my target isn't as restricted. I found that they impose too much on your apps architecture and the ports ffi stuff while highly functional is a bit rigid. It seemed like a real chore to interface with more complicated libs. 
Fair enough. Maybe we should raise a ticket ...
Thank you. This is pretty much what I thought it was, but I wasn't aware of the Monad instance for (,). It makes more sense now.
Cabal hell is just dependency hell at build time. Most common build systems don't actually make sure that transient dependencies match stated ones, so they silently let you through (and then you get a "class missing" or "method missing" at runtime in the JVM, for example). Cabal, however, doesn't even let you build if the dependencies don't match (and if they match but don't compile, due to bad versioning by the developer, then it also won't build). That's what Cabal hell is, dependency hell but not at runtime. And that's a good thing I say, Cabal Hell is good! So I don't know how the NPM model might help, but I'm not familiar with the internals of NPM. Does NPM just not suffer from dependency hell? Also, this is why Stack was created, so use Stack if you want to avoid "cabal hell". Edit: see reply below, turns out cabal hell's meaning has changed. 
This ain't a song about the Stack we know...
From the summary for the fortunes benchmark: &gt; In this test, *the framework's ORM* is used to fetch all rows from a database table (emphasis mine) Herein lies a significant disconnect. They say "the framework's ORM". As I [have discussed before](http://stackoverflow.com/questions/5645168/comparing-haskells-snap-and-yesod-web-frameworks/5650715#5650715), most Haskell frameworks don't have an ORM per se. You'll probably find that persistent is often used with Yesod, but my understanding is that's not required. Snap intentionally does not make any decisions about which DB to use. You can use it with persistent, groundhog, opaleye, postgresql-simple, etc. There is no "framework's ORM".
Kinda hijacking this post but is there a state of the art for GADTS generics?
The other comments here have already pointed out the bigger problems we'd have to address if we allowed multiple versions of the same package linked into the same executable. That being said, you may be interested in the following blogposts which give a good overview of the aspects of "cabal hell", and what direction Haskell is taking to address them: - http://www.well-typed.com/blog/2014/09/how-we-might-abolish-cabal-hell-part-1/ - http://www.well-typed.com/blog/2015/01/how-we-might-abolish-cabal-hell-part-2/ - https://www.reddit.com/r/haskell/comments/4hiwi0/announcing_cabal_newbuild_nixstyle_local_builds/
Conversely, benchmarks like that don't give Haskell developers much confidence in the benchmark. ;)
&gt; an open parenthesis missing its buddy This just made my day.
Thanks for that!
I thought npm's strategy of duplicating everything saved it from dependency hell, i.e. two packages can depend on different versions of the same dependency since the copies are isolated. Upgrading your dependencies and having everything break due to the lack of types isn't fun, though. Is this what you meant? Not really familiar with node. 
Looks like things have changed since 2014, including the fact that my website has more than one reader now. I will have to update the text before this semester's course.
&gt; However I'm wondering how a flat list solves this problem? Either way one of the two (or even both) packages will break. In the nested thing, it'll break because the root project depends on either one or other, and you can't "convert" between the two representations. It will break at compile time instead of runtime.
True. 
I think we should keep aside the philosophical/architectural preferences towards ORMs aside and debate the performance aspects alone. Even if they are an anti-pattern, why would Haskell implementations of ORMs be **slow** (slower than Ruby, in this case)? Btw, I'm not sure if the Yesod benchmarks are taking a hit primarily due to Persistent, or something else.
Irc just isn't made for people pasting in 20 lines of code. Slack, on the other hand, supports that stuff out of box. Having code `lookLike this` makes it so much easier to see what's happening.
&gt; NPM is a steaming pile of broken ...I feel like that sentence is incomplete? 
Hasql and snap have reasonable reasons to claim they will be fast, I'd like to see those too. As I say is be very interested in a deeper look at what makes the performance results so and how that translates to optimising other libraries or best coding practices. Which optimisations can be translated and which can't and how that relates to designing new libraries or making breaking changes to the current ones in order to access those optimisations
It's complicated even further in practice because libraries like `bytestring` often have some C code in them, and the C code's linker symbols *are not* namespaced by the package name. Meaning even if you had V1 bytestring and V2 bytestring namespaced by package name, when you try to link them together the C objects will collide and linking will fail (due to two copies of `bytestring_whatever()`) This one is much more difficult to solve without using macros or something in every C codebase you link into Haskell, so it's not a very general or scalable solution either.
Are you guys looking for interns next year if I may ask? There doesn't seem to be many intern positions for students in functional programming, at least here in London :/
Thanks! Why does everyone keep asking about Haskell? ;) The answer is something like "we'll see", I guess. I'll definitely keep using it in my spare time, regardless.
I am also interested in a Haskell internship. Do you know when your hiring season starts? Will you announce it on Reddit? If not where should I check?
Most requirements are wishes. You just need to stand above the other applicants.
&gt; Why does everyone keep asking about Haskell? Because, in some order, you all love Haskell, and we're worried about you, and you're one of the biggest users of my database library :)
I was playing around with GHC 8 yesterday, and this is what came out of it. In the end though, it only marginally used GHC 8 features - for slightly better syntax, and for better error messages. 
[removed]
&gt; What kind of values can it inhabit s/it inhabit/inhabit it/ Well, `proxy# :: Proxy# a`. I guess it's an unboxed proxy that doesn't actually exist at runtime.
See https://www.facebook.com/careers/jobs/a0I1200000IAGYKEA5/. Most internships are during the summer months, but you can apply well in advance.
Facebook posts are not allowed on /r/haskell. If you believe your post should be an exception, please send the mods a modmail with a link to this post for further review *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/haskell) if you have any questions or concerns.*
/r/softwaregore :) The irony of this being a post about haskell jobs at facebook for spam detection.
I don't know about London, but I knew some interns who wrote Haskell for the Protect and Care team at Facebook Seattle.
Huh, this actually seems to at least talk about continuous distributions. Thst merits a closer look. A lot of work marrying category theory and probability tends to presume / only consider discrete distributions, to my frequent dismay. This seems to not have that limit.
It's both, actually. GHC has always supported linking multiple versions of a package in the same binary, but prior to GHC 7.8 it wasn't really usable because there wasn't a way to link p-0.1 linked against q-0.1, and p-0.1 linked against q-0.2, in the same package (the symbol names for p-0.1 in both cases would be just p-0.1. Blegh!) In GHC 7.10 and forward, GHC puts a bit more information in the symbols so that such cases can be disambiguated. A happy side effect was that it became much easier to manage Nix-style local builds, since packages with the same version number don't stomp over each other in the package database, but it's really an orthogonal issue.
Any reason not to have a Monoid instance?
And new grad positions! It's hard to be exposed to functional programming in an upper-level CS course, fall in love with it, and then realize that it is very unlikely you will find a job working with it for many years.
Hmm, so the compiler could detect this implementation equality of the different Text versions, could have some kind of type class representing the conversion and could automatically implement an instance? Ok, without the library writer telling the compiler in a way which versions can be converted this can't work safely, because even if the fields of a data type are the same, the semantic meaning of the values might differ. 
In the limited case when only newtypes are used, this exists: it's called Coercible. The upshot is if each version of text implemented its type as a newtype to the same underlying representation, you can `coerce` between them as long as the newtype constructor is in scope. If it's a recursive data structure, it's possible `Generic` could be used to automate away the boilerplate. But clearly, this is a *very* special case, and for most people, you should just make sure the two versions are actually the same (which is what happens today!)
Well, you can get at it via https://hackage.haskell.org/package/text-1.2.2.1/docs/Data-Text-Internal.html
&gt; (if then else indentation within do &amp; co). What was the problem with this; what has changed?
You couldn't align the `if`, `then` and `else` at the same level of a `do` block: -- You could do this: do if someCondition then 123 else 456 -- but not this, despite it being a much more common style (outside of `do`): do if someCondition then 123 else 456 Now they corrected the grammar so that you can.
To add to what others are saying, NPM's model is also terrible from a packaging standpoint. I say this as a fairly prolific package contributor to the NixOS distribution of Linux. Packaging NPM applications has been a major pain and the source of countless hours dumped into tooling and clever hacks to attempt to kind-of-sort-of automate the process thereof. Contrast with Cabal/Hackage packages: we have a conceptually simple tool that crawls Hackage and spits out package definitions, and those definitions almost never require any tweaking. You can read an explanation of the challenges here: http://sandervanderburg.blogspot.com/2016/02/managing-npm-flat-module-installations.html
&gt; This seems to not have that limit. Ho ho ho.
 type Person = Book '[ "firstName" :=&gt; String , "lastName" :=&gt; String , "id" :=&gt; Id ] someFunc = do p &lt;- dbGetPerson addr &lt;- dbGetAddressFor $ p ?: #id let p' = p &amp; #address =: addr annoyWithMail p' Is this... type-safe JSON? :O Edit: OK, not JSON *per se*, but it feels like JS objects what with the arbitrary extensibility.
I would make the claim that "cabal hell" is a special circle (sorry, couldn't help myself) of build time dependency hell, in which the best way out is to delete a large amount of compiled data and do a long rebuild. This was particularly a problem before sandboxes (because sometimes you had to delete your `~/.cabal` directory) and a smaller problem with sandboxes (because sometimes you can't build without deleting the whole sandbox).
I didn't use unitialized fields directly. I used yesod scaffolding which uses wildcardrecord. However, It will make sense to use strict fields in the scaffolding (as it he application settings). I probably should use `-Werror`, but the scaffolding (again ;-)) didn't set it for me ...
If you can verb nouns, why wouldn't you be able to noun adjectives?
Ah! In the previous discussion I had not realized you were using the RecordWildcard syntax `let a = 1 in Foo {..}`, not just the record syntax `Foo {a = 1}` (in both cases there is also an uninitialized `b` field). In the GHC ticket you linked, it clearly says that the record syntax case was already producing a warning. Assuming there was a good reason for that, I can understand why SPJ wanted the RecordWildcard syntax case to be consistent with the record syntax case. The question is: why is the record syntax case only a warning? I understand that if it was an error, programmers would just write `Foo {a = 1, b = undefined}` instead, and as a result they would get a less helpful error message than what GHC can generate. But in my opinion this is so much more likely to be a mistake (and in the case of RecordWildcard, even more so), it's worth making it an error despite the cost of error message clarity in the less frequent cases when the programmer does want the field to be left undefined.
I assume like records, this doesn't incur any performance overhead?
I like being *able* to build broken things, but I should have to go out of my way a little. I usually get that by building `-Werror` by default. I'm open to other approaches. Maybe this should be an error by default but take a flag to downgrade to a warning? I think `-fdefer-type-errors` would be overboard.
That's what it feels like to me!
I haven't benchmarked it, but I wouldn't be surprised if it was roughly the same speed, since in general the dictionary for getting and setting is known at compile time. *EDIT*: There is currently a decent difference. I'll report back when I know more about it. I'm not sure what the overhead would be an overhead *of* in the case of deletion or adding a field, since you can't do that with normal records :)
I stand corrected!
I was literally just googling for papers of this sort yesterday. This is why I love Haskell, people here are totally into the same sort of research that I am!
If facebook wants to build me a better automoderator, I'm happy to test it. :) Honestly I just need to sit down and do the regex magic to make it less stupid wrt facebook posts, but finding time is hard.
The link was broken by Microsoft Research. It worked when I posted it. Google Cache still has the page listing the papers, see http://archive.is/PTuRz . The first link was also archived by Archive.is, and you can see the listing page. That link too, now redirects to Simon PJ's home page. Somebody screwed up. Sadly, Archive.is does not seem to archive PDF's.
This is what NPM `peerDependencies` are—anything that is part of a library’s public interface is a peer dep, but dependencies that are only implementation details are ordinary dependencies. In the JS world, this tends to hold together better than one might expect because most libraries do not define their own opaque data representations; that is, most libraries simply use JS objects as lightweight hashmaps. My guess would be that many NPM package authors mess this up a lot—there are likely a lot of things that *should* be peer dependencies that are just listed as normal dependencies—but due to the way JS works (and the way NPM dependency resolution works), it tends to be mostly harmless in practice. In Haskell, this would likely be more problematic, but Haskell has a static type system, so this distinction could probably be detected statically. I think something like that would be really cool, but things like the way typeclass instances are resolved would probably make it much more complicated than a naïve approach could solve. Personally, though, I think a language like Haskell has the potential to implement the NPM model *far* more safely and effectively than JavaScript can.
I don't think `getDb` is even run in this particular benchmark. All it does is hit `/fortunes`.
&gt; If you do something like forkIO and then recv from Network.Socket in a thread, it's not like actually forking a process or creating a thread with pthreads or the like. sorry, I don't understand. How can I create async and multhithreaded server or client then?
[removed]
Thanks for the heads up . Personally this actually suddenly made the offer attractive ! Unfortunately I'm going back to school for 1 year. And New York is far ah. Edit : don't have 4+ year experience anyway so now I feel better, haven't missed an opportunity 
Relevant thread: https://www.reddit.com/r/haskell/comments/4vz3gs/polling_on_sockets/ . Read the comments for explaination about how those IO functions (Not the type, the *other* meaning of IO) work. TL;DR: &gt; Should I use the normal sync module Network.Socket and then add concurrency? Yes. 
I was always wondering(in context of Haskell records), whether providing sufficiently expressive type system for users to build upon might be better design decision than providing baked-in solutions which are hard to design to cater for all use needs. So far I see people coming with quite a good stuff implemented in the language itself, and the 'record problem ' solutions are still on their way...
Do you have Haskell people in Paris ?
So I just did some tests benchmarking Go's sql library interface and the go-fast-sql driver vs the new mysql-haskell pure haskell bindings and the new bindings beat Go's drivers badly: ghc 7.10.3 (haskell-mysql) 1000 @ 0.256 seconds 10000 @ 7.770 seconds 50000 @ 8.656 seconds 100000 @ 12.104 seconds go (go-sql-driver) 10000 @ 55.021 seconds go (go-fast-sql) 1000 @ 1.916 seconds 10000 @ 8.453 seconds 50000 @ 33.385 seconds 
The Sinatra test uses a [connection pool of size 256](https://github.com/TechEmpower/FrameworkBenchmarks/blob/c56fab79ef4e5998126f7e88ddb97f6bb7570bd2/frameworks/Ruby/sinatra/hello_world.rb#L18). I don't know if it's static or dynamically sized. What's the default connection pool size for Yesod? I couldn't find it in explicitly set in [the code](https://github.com/TechEmpower/FrameworkBenchmarks/blob/round-13/frameworks/Haskell/yesod/bench/src/yesod.hs)
Btw, besides the C symbol clashes for `cbits` you can get (unless you workaround this by using the `CapiFFI`+`inline` trick), another major issue would be handling (orphan) instance collisions and/or incoherence issues, which can occur quite easily when multiple versions of the same package get linked into the same program.
Ha, "to verb a noun" is an example of verbing a noun.
Just to add to the other comments here, there's a book on the subject http://chimera.labs.oreilly.com/books/1230000000929
You see good stuff, I see fragmentation. TBH, I am a bit worried of the multiplication of records library for Haskell.
So GO wins because they could find a way to minimize memory allocations... I was thinking: maybe Haskell should get some plugin system for GC and memory allocators? As clearly there is no universal heuristic... then some pragmas, or Type to indicate it?
All of theses record library can be seen as a draft for a future ghc extension no? For example, bookkeeper is, apparently, a subset of the current record, once the `HasField` and `UpdateField` will land in ghc (hopefully in 8.2), most of the work of bookkeeper can be "merged" inside the standard record, with a nicer syntax and compile time performances.
This book answers exactly the question you ask. It implements a chat server to show how.
Interesting, thanks! This is a super cool lib.
thanks. 
If you want to learn to use asyncronicity parallelism, streaming, concurrency in a distributed app using sockets then the raw sockets library with forkIO or async is OK. I wish you luck. If you want to do it for some practical need in a few lines of code, use transient. https://github.com/agocorona/transient/wiki/Transient-tutorial This may sound irreal and alien, but no matter how it sounds to you, it works.
Could compiler options be fingerprinted too? (like Nix).
&gt; Most implementations have been carefully hand-tuned by expert programmers to squeeze out the last bit of performance. What do you mean? For the framework code? I'd sure hope so, and clearly Haskell is woefully behind in this regard! If you mean for the benchmark code, I have to contest that claim--looking over the sources of several of the framework examples that trounced Haskell, they look pretty normal to me. I don't see crazy misleading shenanigans. Many of them look like simple, obvious ways to use the framework. The key to doing well in (and the whole point of) these benchmarks is to have performant abstractions to work with, and frankly, these benchmarks demonstrate that Haskell does not have that. Note that this does *not* mean that the language itself is slow. Rust has traditionally performed abysmally in these benchmarks due to synchronous or heavy-threaded IO (though this looks like it will change soon!)
Planning on doing it after work.
there's only one way -- try to connect to a port, that's it.
I think even with RULES in general you're still going to have to pay for the indirection. Most of the time, the record value won't be inlined into the function.
Nice work! Also, monads for dependent samples, applicatives for independence ! https://medium.com/@jaredtobin/encoding-statistical-independence-statically-ec6a714cf24a#.82wdnjed4
I usually add the following flags to the cabal file: `-Werror -Wall -fno-warn-orphans -fno-warn-type-defaults -fno-warn-name-shadowing`. Maybe this will help. 
It's also not obvious how to write these RULES, since the compiler won't always know that the constraints are the same (as in, imply one another)!
Because the only "interesting" "exception" is bottom and it is a member of every type. Every other kind of error handling is expected to be handled at the type level. Have you looked at the Maybe type or the Either type?
The code allocates as-many-as-CPU-cores connection stripes, with a maximum of 1000 open connections per stripe. The actual pool size is dynamic. 
&gt; `-fno-warn-name-shadowing` *shudder*
I wouldn't have anything about something like: data Foo {-# UNDEFINED_FIELDS #-} = Foo { ... } 
Interesting. Putting it on the type isn't really relevant to my use case, though. It's usually someone else's type that I'm trying to build up. I think what would probably be best is an explicit flag to turn this particular error into a warning, off by default for ghc, and probably on by default in ghci. It's not obvious to me that it's worth the cost of development and maintenance, though.
It does! https://hackage.haskell.org/package/control-monad-exception To make things work really well you need difference types or subtyping for exceptions, on top of being able to deal with them parametrically. Without any of those, checked exceptions can be a bit painful. Java did checked exceptions, but allows arbitrary subversion of it via RuntimeException and I see a lot of that in practice because of the checked exception pain. It also makes things like Runnable / Callable / etc. less elegant interfaces. Being able to abstract over the exception type is allowed in Haskell -- via use of Either, or more sophisticated stuff. But, it's still got troublesome areas since we don't have difference types or proper subtyping.
&gt; . It's not obvious to me that it's worth the cost of development and maintenance, though. Do you mean the cost of changing the error to a warning or doing so as well as adding a flag ?
I wouldn't like to see a different default behaviour for `ghc` and `ghci`. You can always set your `.ghci` accordingly, so this shouldn't be needed. The nicest way might be to have `UNDEFINED_FIELDS` as some kind of ghc extension, that could be switched on, but I've no idea how much work that would be.