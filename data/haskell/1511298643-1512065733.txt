I don't the details here, but if "projects" are different customers, it might be better or even required for them to be in separate DBs.
Once you get the hang of it, custom persistent monads and custom `runDB` functions are not so hard. So if you prefer the first approach if it weren't for the "IO mess", try posting some specific problems you're having.
Did you modify your cabal config file to contain the following as described in step 2 of the install instructions? extra-prog-path: C:\Program Files\Haskell Platform\8.2.1\msys\usr\bin extra-lib-dirs: C:\Program Files\Haskell Platform\8.2.1\mingw\lib extra-include-dirs: C:\Program Files\Haskell Platform\8.2.1\mingw\include Omitting that step is usually what causes this issue.
We have a similar setup at my company. We have a master database and account specific databases. We use `runDB` for the master database, and then use a function `runDbWith :: ConnectionPool -&gt; SqlPersistM a -&gt; App a` along with `withAccountDatabase :: AccountId -&gt; (ConnectionPool -&gt; App a) -&gt; App a`. There's a tiny amount of boilerplate in passing the connection pool around, but it's not bad enough to warrant another `ReaderT` layer or similar.
I'm working on something vaguely similar at the moment. One nice thing for you is that Nix has aarch64 support out of the box now and is upstream in the Hydra cache, so ideally you can at least get Nix running on top of whatever BSP they ship you, at least, and it will just work. I don't know if the aarch64 binary cache includes GHC, right now... I'm not using Nix on my Jetson (though I should get around to trying it...) If it does then maybe you won't need to compile much at all... Another related issue is, my inevitable goal is to use Nix to cross compile a NixOS "buildroot" which I can image onto some ARMv7 SoCs right now, because I have some FPGAs with ARMs attached that I want to configure in a turnkey manner (embedded Linux is a nightmare) on the Linux side. Nix also has a pretty fleshed out cross compilation story here from what I've been dealing with at `$WORK`, but I don't think it can do x86-64 -&gt; ARM64 compilation, though it wouldn't be hard to add... In any case it's all moving along pretty well, so ideally you shouldn't have too many issues...
&gt; Do you know when the actual release is planned for? Now :) See https://www.reddit.com/r/haskell/comments/7elm1o/announce_ghc_822_released/ &gt; I can try installing it on windows running some tests if there isn't data for that, btw. In the future any testing that you can provide would be quite helpful.
I don't like to work with it. But that's mainly personal preference. Not objective one. However, I heard that yesod's routing TH becomes compile bottleneck once u have a lot of route. so that's one minus point for yesod.
I am not sure what your goal is, but you could try to use your own global package database: --ghc-pkg-options="--global-package-db=my_package_db" 
Use `parse` to turn your parser into a function, and then you can test it like any other function. There is a library hspec-megaparsec that could help. [`shouldFailWith`](https://hackage.haskell.org/package/hspec-megaparsec-1.0.0/docs/Test-Hspec-Megaparsec.html#v:shouldFailWith) looks quite relevant.
Release notes link seems to be broken in the email, a working one looks to be this one: https://downloads.haskell.org/~ghc/8.2.2/docs/html/users_guide/8.2.2-notes.html
In the long run I am hoping that `ByteString` will just become `UVector Word8` (with a type alias) and maybe something similar for `Text`. Although I admit `Text` is a lot harder due to the way `Unicode` works. Text is sort of `Rope Word16` but not really due to the way utf-16 works. Perhaps some sort of clever `UTF16` type that has variable size that depends on runtime values but can still be unboxed would work. Anyways the above combined with some nice typeclasses that cover all the common operations, perhaps with constraints if needed, would be really nice. 
Woot! the particularly exciting features for me are: Backpack, XDerivingStrategies, XUnboxedSums, complete pattern synonyms, windows bug fixes, and the ghc-compact package. https://downloads.haskell.org/~ghc/8.2.2/docs/html/users_guide/8.2.1-notes.html
Do note that this is just a minor release. The features that you list are also available in 8.2.1, which was released in July.
Do note that this is just a minor release; the features that you list are also available in 8.2.1 which was released in July 2017.
Don't you mean `Cons Stump Bail Wicket`? Anyway data UnsteadyWicket = UnsteadyWicket Bail Wicket is a `Monoid`.
huh, didn't know. I'd taken a break from Haskell then.
this seems to have already been from 8.2.1, but the new error message coloring and bracketing looks great https://imgur.com/a/epBH9
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/FXlIOSV.png** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) ^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20dq5wpsf) 
Well, `yesod` has predefined set of classes. But it doesn't have anything to work with multiple databases. I wanted to work in this (out of the box) setup and change as little as possible. But, as suggested in other comments, having my own custom `runDB` like functions is not a big deal. So yes, I guess I'll use projects table in main DB to generate ids, and create separate DB per project. I don't have any mechanics for backups. So far, I worked on features mostly, and used DB to test them. I guess I should clarify that this app is a scada system. Having separate database per project means I should decide where to keep users, in master DB or in project DB. Each user has it's own profile which defines set of actions user can perform. User may be a superhero in one project, and be nobody in another. But it's a different quest, I'll try to crack it by myselft first :)
I thought the same, but iirc, there has already been a failed attempt. I don't know if it's fundamental, or just took too much work to port the optimizations too. 
Could you possibly give an example of this in use, or does /u/catscatscat's example cover this?
I think for ByteString it is optimizations (same deal for fingertree vs seq). The latter is definitely just hard to get right / do at all. 
The programming wisdom is that data outlives applications. So you have to think about things like backups, replication, access from other applications, even written in other languages at some point. Or if you do not care about the problems other people will encounter once you are gone, then you can ignore all that. 
You absolutely can do these sorts of things, fortunately. For almost all of the fields, you can have a `sql=` bit that says what the actual SQL for that is. So: User sql=user_table_lol name Text sql=user_name age Int sql=get_rekt See [this blog post](https://www.schoolofhaskell.com/school/advanced-haskell/persistent-in-detail/existing-database) too. You can create a custom Primary key using the syntax `Primary fieldOne [fieldTwo ... fieldN]`. This will become the value stored inside the `Key` type. 
In essence, I'd like to build [this project](https://github.com/sgraf812/ghc-containers-conflict) with `cabal-install`. `stack test` works just fine, whereas `cabal test` doesn't. This must be due to `stack` trying to hide the dependencies of `ghc-8.0.2`, such as `containers-0.5.7.1`, to enable stackage lts builds or even an entry like `containers-0.5.10.2` in `extra-deps`. Not sure how they do it, but it just works for me, whereas `cabal-install` does not.
How much of it do you understand so far? What do you think it does?
it is a list comprehension 
Ok great. What's the input and what's the output. And what is the list comprehension doing?
Awesome! Thank you so much, this was exactly what I needed.
Why would I want to do that? Almost every action I've taken in the Haskell community has been seen as divisive by *someone*. I am so tired of explaining myself. If you insist, here's some links you could have easily found via google: - /r/haskell_lang - https://github.com/haskell-lang/haskell-lang/issues/2 - https://www.reddit.com/r/haskell/comments/4ruqbl/new_haskell_community_nexus_site_launched/ - https://news.ycombinator.com/item?id=12054690 - https://www.reddit.com/r/haskell/comments/4ghznv/improved_hpcaballess_wwwhaskellorg_in_the_works/ - https://www.reddit.com/r/haskell/comments/4zzmoa/haskellorg_and_the_evil_cabal/
I'm currently going through Hindley and Seldin's [Î»-Calculus and Combinators: An Introduction](https://www.amazon.com/Lambda-Calculus-Combinators-Introduction-Roger-Hindley/dp/0521898854), searching for stuff I missed or didn't know I don't know.
I'm reading [Programming and Reasoning with Algebraic Effects and Dependent Types](https://eb.host.cs.st-andrews.ac.uk/drafts/effects.pdf) - it's written for Idris but close enough. Also I'm reading Chapter 14 of [Type-Driven Development with Idris](https://www.manning.com/books/type-driven-development-with-idris). 
Have you tried making a separate type for valid email addresses, and making the validation function a smart constructor for that type?
Any thoughts about merging this functionality into stack? Or making it a stack plugin?
I'm a few chapters into Category Theory For Programmers. As a (mediocre) physics grad and newbie to Haskell and FP, it's been a really entertaining and adequately challenging read. 
Additionally it might be cool to create a list with all these papers containing 1) Paper title 2) Summary 3) Why it might be useful (e.g. code examples).
Additionally, it might be worth to throw all of these together in a list (GitHub or Haskell Wiki?) where we could add a short entry like, - Paper Title (and published date) - Summary (would the abstract be enough? Sometimes..) - Why it might be useful - If it's included in GHC, on the roadmap or not (if relevant) Then people could easily find interesting papers to read, and also find the papers that introduce some of the more complex stuff in the Haskell ecosystem. I would be up for setting something up if people thought that'd be a good idea. Some questions remain like what the order should be (alpabetically? date?) etc. 
Recently going through some chapters of "Practical Foundations for Programming Languages" by Robert Harper. http://www.cs.cmu.edu/~rwh/pfpl.html
Anyone have an idea on when stackage will have a stable resolver for 8.2?
Was thinking to buy this one. How do you compare it to TAPL? 
It depends on whether or not you can call into existence an arbitrary `Bail` to adjoin two `Stumps` If you can't, you cannot form a binary operation between two instances, because you could always have an instance of `Stump` on the left, and an instance of `Stump` on the right, no matter what sort of crazy newtypes or what have you you decided to wrap them in. However, if you do decide that an arbitrary `Bail` can be 'vivified' from nothingness, you could do like: data Adjunct a b = Adjoint a b (Adjunct a b) | Terminal a deriving (Eq, Show) data Stump = Stump deriving (Eq,Show) data Bail = Bail deriving (Eq, Show) newtype Wicket = Wicket {getWicket :: Adjunct Stump Bail} instance Semigroup Wicket where (Wicket (Terminal Stump)) &lt;&gt; (Wicket (Terminal Stump)) = Wicket (Adjoint Stump Bail (Terminal Stump) ) (Wicket (Terminal Stump)) &lt;&gt; (Wicket (Adjoint Stump Bail r )) = Wicket (Adjoint Stump Bail ( Adjoint Stump Bail r )) Wicket (Adjoint Stump Bail l) &lt;&gt; r = Wicket (Adjoint Stump Bail (getWicket (Wicket l &lt;&gt; r) ) ) And thus describe a process by which any series of Stumps may be joined by corresponding Bails.
Yeah efficiency is a big concern. One possible approach to that might be allowing things to be added to a class after it has already been defined. So for example: class Foldable t where foldMap :: Monoid m =&gt; (a -&gt; m) -&gt; t a -&gt; m
Can your definition of Entity actually be used with Persistent easily? And also think about using this in the larger context of your program (which people tend to forget). What happens to lenses? Suddenly you have to deal with nested records for most of your operations. What happens to auto-derived JSON instances? The JSON also is needlessly nested for every single entity on your data model. These are similar problems to using Maybe for ID, createdAt, updatedAt. The solution doesn't look so good once you think about all the call sites and use cases. 
Can your definition of Entity actually be used with Persistent easily? And also think about using this in the larger context of your program (which people tend to forget). What happens to lenses? Suddenly you have to deal with nested records for most of your operations. What happens to auto-derived JSON instances? The JSON also is needlessly nested for every single entity on your data model. Another solution is to use Maybe for ID, createdAt, updatedAt. Even there, the solution doesn't look so good once you think about all the call sites and use cases. 
**First the efficiency concern:** One possible approach to that might be allowing things to be added to a class after it has already been defined. So for example: class Foldable t where foldMap :: Monoid m =&gt; (a -&gt; m) -&gt; t a -&gt; m foldr :: (a -&gt; b -&gt; b) -&gt; b -&gt; t a -&gt; b instance Foldable ... -- Same as usual, define foldMap or foldr or both classext Foldable t where length :: t a -&gt; Int instanceext Foldable ... length = fastConstantTimeLength -- Extend types that are already foldable -- Not an orphan if all the functions defined were declared in same package -- Not an orphan if type defined in the same package -- Otherwise is an orphan instance Foldable ... -- If the `classext` is in scope you can declare core + ext at once This would mean that if any class can implement a function (and we are ok with the laws that go with that class / function) efficiency shouldn't ever be an issue. As we can always declare it as a classext. **Now as for the "typeclass abuse" concern:** I will agree that avoid signature complexity explosion is hard, as these types are essentially compiler verified documentation. So if the thing that is going on is non-trivial then the compiler verified documentation of it is also most likely non-trivial. I'm not sure of a good solution to this issue besides simply "proceed with caution". I will say that a constrained functor will infer just as well as our existing functor. But anything that simultaneously supports `Text` and `[]` almost certainly won't. Perhaps having both `fmap` and `omap` is fine as long as any polymorphic container admits the former and monomorphic ones admit the latter. **Now on to fine grained and wide hierarchies of typeclasses:** I think a few things that when combined could make it pretty clean to work with big hierarchies: Make it so that `instance FooClass BarType` automatically declares that `BarType` is an instance of all superclasses. Note that no implementation is implied, which avoids the issue of `instance Traversable Foo` and `instance Applicative Foo` making it ambiguous which path to take for implementing `fmap`. If the implementation members of the superclass are unambiguous, such as because there aren't any members (e.g if a class exists solely to specify that a superclass obeys more laws than the minimum needed to define the member), or perhaps because there is a default implementation specified on those members and the type has the necessary constraints to use that implementation. Then great, we are done! If the above is not the case, then one thing we can do to still alleviate boilerplate and improve backwards compatibility of inserting into the hierarchy, is make it so that you can declare the implementation of superclasses while declaring the subclass instance. Luckily the above solutions to not run into dangerous territory that many approaches do. Specifically they handle the open world assumption appropriately and will not break in the presence of extra subclasses being added or anything like that. Now for the benefits: Lets say we have a big hierarchy for ordering including `Eq`, `POrd`, `JoinSemiLattice`, `MeetSemiLattice`, `Lattice` and `Ord`. Then `Eq` uses `POrd` for defaulting, `POrd` uses semilattices + `Eq`, the semilattices use lattice, `Lattice` uses `Ord`, and `Ord` doesn't have any defined members itself (except perhaps some "exts", as `â¤` from POrd is sufficient). Now to define that something is a member of every single above class we just do: instance Ord Foo where (&lt;=) = ... Now lets cover a less than ideal situation where no defaulting has been established, perhaps due to things going over multiple modules or packages: instance Monad Foo where (&gt;&gt;=) = ... pure = ... (&lt;*&gt;) = ap fmap = liftA Still not too bad. Lets cover one more situation, adding a superclass, lets say we want `class Semigroup a =&gt; Monoid a`, but currently have `class Monoid a` (any similarities to a real life situation are purely coincidental). We can now do this with no breakage! module Data.Monoid (mempty, mappend) where class Monoid a where mempty :: a mappend :: a -&gt; a -&gt; a becomes: module Data.Monoid (mempty, mappend) where class Semigroup a where mappend :: a -&gt; a -&gt; a class Monoid a where mempty :: a Definitions of the form: instance Monoid Foo where mempty = ... mappend = ... Were the only valid option before, and are one of the valid options now. On a side note allowing for multiple default implementations that are tried in linear order might be worth adding to this proposal, as there are some situations in which my current approach will not lead to the absolute minimum number of declarations in all situations even with well engineered class hierarchies. For example if we added `Enum` to our ordering hierarchy as the subclass of all of them, then we would either have to change some of the existing defaulting (breaking our previous instances / making instances for non-enum types more verbose), or we would have to require `fromEnum` and `toEnum` be defined as well as `&gt;=` even though `&gt;=` can be defined in terms of them.
Great news. Congratulations to all the contributors! And thank you for all your hard work. 
I'm afraid I am not now qualified to give a full account between the two, but I believe PFPL suits my goals better. Obviously this depends on what you're after and is a bit subjective. FWIW here is one comparison which may help in selecting: http://blog.ezyang.com/2012/08/practical-foundations-for-programming-languages/
Did, but I didn't install into the default location.
I made an Alpine Linux docker image with `stack` and `ghc-8.2.2` pre-installed, in case anyone's interested: `docker pull terrorjack/meikyu:ghc-8.2.2`
I started using `cabal new-build` because of this. 
`throwM` exceptions.
There is an `hspec-megaparsec` package specifically meant for this. I've used it and I liked it for the most part. 
I started using `cabal new-build` for a different reason â support for Backpack. And I'm not looking back, I like it more than Stack.
Haskell related: - "Simpl eunification-based type inference for GADTs" studying GADT type inference and checking Not quite Haskell-related: - "Implementing Algebraic Effects in C" - "Type Directed Compilation of Row-typed Algebraic Effects" - "Structured Asynchrony with Algebraic Effects" - "Koka: Programming with Row-polymorphic Effect types" So lots of algebraic effects stuff. --- Meta: I think it'd also be great to have a similar regular thread on what's people been working on. [/r/rust](https://www.reddit.com/r/rust/comments/7e74ia/whats_everyone_working_on_this_week_472017/) and [/r/gamedev](https://www.reddit.com/r/gamedev/search?q=flair:SSS&amp;restrict_sr=on&amp;sort=new&amp;t=all) have similar threads and I often find interesting projects/libraries etc. from those threads.
I haven't considered either. Until now, I honestly didn't think there was a significant group of people who would care about this tool. Even now that I'm seeing a number of people interested, I'm not sure it's worth the effort of changing it to work as a plugin, and definitely don't intend to add this relatively niche functionality as a burden to the rest of the Stack maintenance team.
StudentMark is a type alias for (_,Int).
Is it hard to get Stackage resolvers working with new-build?
What do you mean? `new-build` implies using a different build tool (`cabal` rather than `stack).
It shouldn't be hard, Stackage already publishes `cabal.config` constraints (i.e. https://www.stackage.org/lts/cabal.config). With `new-build` this file can be saved to `cabal.project.freeze`.
&gt; You don't have to zip, chop, and rebuild the lists I'll admit that felt dirty, and I'm so happy there's a better way to do this! Your construction rephrasing it as a `Traversal` is much cleaner. For completeness, here are Applicative, aggregatePair, and runQuery: instance Applicative (T' x y a) where pure x = T' (\_ _ -&gt; pure x) (&lt;*&gt;) (T' f) (T' x) = T' $ \g a -&gt; f g a &lt;*&gt; x g a aggregatePair :: T' x y a b -&gt; T' x y c d -&gt; T' x y (a,c) (b,d) aggregatePair (T' f) (T' g) = T' $ \h (a,c) -&gt; (,) &lt;$&gt; f h a &lt;*&gt; g h c runQuery' :: T' [ ByteString ] [ Maybe ByteString ] a b -&gt; a -&gt; Redis b runQuery' (T' f) = f mget' Awesome work noticing that! I'll update my post later today to use this `Traversal` implementation.
Thanks. Is that the solution you would take again for a new project, or is that one of those fields where you ran out of research time / was constrained by earlier suboptimal choices?
&gt; Design the stuff around concurrently such that StateT s IO can enter it only if Semigroup s, then it'll also be obvious from the type signature what happens to the alternative states. This is also what I was thinking, and I was surprised there was no mention of it. It seems like the obvious thing to do - if you don't like throwing one away, then just demand a way to keep both! That being said, I do agree with the overall suspicion of some of these transformers. The libraries do seem a bit inconsistent in their choices, and due to this ambiguity, when I need these transformers I usually just end up writing them myself so I know I'm getting what I want. Or maybe that's just me hallucinating a justification for Not-Invented-Here.
My recent experience with `new-build` has been that it just works out of the box. `new-freeze` gets you idempotent builds too.
This is the culmination of a mini series on how the GHC build system works. Part 1: https://medium.com/@zw3rk/building-ghc-the-package-database-50c37cf6ce33 Part 2: https://medium.com/@zw3rk/building-ghc-the-tools-3d170a4db06c
It's not that obvious to me that 'db' libraries should offer such kind of validation beftore actual persisting...to counter your example: why would you want this validation to happen *right* before persisting? It seems like you want "shared" validation (since you've mentioned you want it to work for various endpoints), then I think you should be able to do this just fine with your usual abstraction methods (hence /u/bitemyapp wrote "I write functions" I believe) - just factor out common stuff, without requiring this from a library that is supposed to allow you to persist your data to database, and not cater for your domain logic. On top of that, it's not hard to imagine someone requiring admin endpoint to have different validation rules (why not, admin could be able to save such 'wrong' email address if someone insisted on doing so), so adding such features would be inviting potential complications and problems...
I can't say I'm too keen on this. String is just a more specialized type, and comparing them instantiates the `a` in `[a]` to `Char`. This is part of polymorphism and is free to come up anywhere in code where a single syntactic form can take on more than one type: ints :: [Int] ints = [] `ints == []` is true, `'m' : ints` is a type error. Maybe having the special syntax for String makes it more unexpected
&gt; but I don't think it can do x86-64 -&gt; ARM64 compilation [wasm-cross](https://github.com/WebGHC/wasm-cross) can do that, albeit only for static linking. The original intent for `wasm-cross` was just to make a cross compiling story for WebAssembly in Nix, but it turned out being fairly general purpose. Now it's basically a library that takes your cross compilation target as input, and produces the version of the toolchain for that target. It's all the same stuff for each target. Same compiler, same nix expressions; just a different target. It supports C out of the box on a lot of the targets that `musl` supports, and at least supports aarch64 for cross compiling with GHC.
I think I mentioned in my slides, it's not OR, it's AND. Some validations are better implemented closer to the DB, whereas some are better implemented at the endpoint/handler. If you're saying all validations should be implemented at the endpoint/handler layer, what is the process with which you reduce the occurrence of -- "oh, I forgot to call the correct set of validation functions in this particular handler?"
You mean something like that: http://packdeps.haskellers.com/reverse
I run NixOS on both a Jetson TK1 (`armv7l`) and a Jetson TX1 (`aarch64`). It works great on both of those boards. On the TX1, I use a Mailiya M.2 PCIe to PCIe 3.0 x4 adapter (https://www.amazon.com/gp/product/B01N78XZCH/ref=oh_aui_detailpage_o00_s00?ie=UTF8&amp;psc=1) along with a Samsung EVO 960 NVMe board to host the entire NixOS filesystem, and it really flies. Out of the box, Nixpkgs does not support GHC on any ARM architecture, because -- believe it or not -- the GHC bootstrap process on Nixpkgs starts by downloading the binary distributions for GHC 7.4.2, and there is no binary distribution of 7.4.2 for any ARM. (Even if there were, you would *not* want to wait for it to bootstrap through 3 or 4 versions of GHC on ARM! Building even a single version of GHC on the TK1 is brutally slow.) However, I've created a Nixpkgs overlay that downloads the Debian package for 8.0.1 and use that to bootstrap a Nix derivation for 8.0.2. I posted links to a rough version of that overlay in the comments here: https://github.com/NixOS/nixpkgs/issues/31666. I've been using this overlay to build Haskell packages for `armv7l` on my TK1 for months now with great success. I thought it worked on `aarch64` as well, but based on the feedback from a tester in that GitHub issue, it sounds like it doesn't work anymore for that platform. In any case, over the next week or so I'll try to post a working version of the overlay somewhere on GitHub. Re: the Jetson TX2, I can't get NixOS to boot on it, which is odd given how similar the platform is to the TX1. It can't find the root filesystem from the `initrd`. I even tried the official `linux-tegra` kernel, which is maintained by Nvidia devs and has bleeding-edge support for Nvidia's Tegra platforms, but to no avail. I haven't tried the recently-released 4.14 kernel yet, but I will soon.
Umm, this works!
After some thought: http://oleg.fi/gists/posts/2017-11-22-why-peano-and-not-typelits.html
Right you are---I accidentally built a single stump and an arbitrarily long series of levitating bails. Others' comments now make sense :)
Hah, you're right with what I've written. I made a mistake. The second version should read `data Wicket = Just Stump | Cons Stump Bail Wicket`, yielding a series of `n` stumps and `n-1` bails. Good point about removing the stump to conjoin things, though. That might reveal a new operation I hadn't considered.
Interesting insight. Might reveal something of interest in my real use case. Thanks!
https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/jfp-outsidein.pdf
Is it normal that when I start mingw64 or msys,I am not able to run GCC? Cuz all that's getting on my nerves are the compiler not working errors
&gt; I think that would be a useful additional metric to the current "DLs" (downloads?) on Hackage. Indeed, and we have already code for that in the works in hackage-server, but we cannot enable it yet due to technical issues that need to be addressed first. &gt; The statistics alone could help users decide which are well used (and thus which are likely to get patches/ist contributions /improvements), and which libraries are stale. I don't like the notion that such statistics *alone* would be meaningful. I'm not convinced this should be the primary metric to decide a package's worthiness. The statistics can be very deceiving (e.g. the release frequency of a package says little about its merit or quality; similarly reverse-dependency popularity should be taken with grain of salt; likewise DL count or vote-rating is also not a reliable indicator currently) and might, if taken too serious, make it harder for new alternative libraries to gain a foothold.
Of course I modified the paths to match the actual install location
Also, Cmake detected the mingw toolchain and also recognized the c compiler as nonfunctional.
I think, if you don't call the DB layer save/update functions directly in your handlers, but instead have it abstracted (in a way of your choice, be it just a function ('updateContact'), or something more fancy like MTL-like/Free-based approach (in which you can implement those things along with your concrete implementation/interpreter). If you don't expose raw 'runDb'/'update' functions, then it's not possible to 'forget' to call anything, right?
Currently working my way through Emily Riehl's [Category Theory in Context](http://www.math.jhu.edu/~eriehl/context/). 
And once you start formalising this pattern, you're very close to what ActiveRecord does. In our code we have implemented the following type-class for all models (simplified version): class DbModel a where type ValidationContext a strictValidations :: (HasDatabase m) =&gt; a -&gt; m [Error] create :: (HasDatabase m) =&gt; a -&gt; m (Either [Error] a) save :: (HasDatabase m) =&gt; a -&gt; m (Either [Error] a) rawUpdate :: (HasDatabase m) =&gt; (a -&gt; m a) -&gt; a -&gt; m a 
Both `Stump` and `Bail` are quite rich structures in my actual use-case, and `Bail` depends on its adjacent `Stump`s. I'm just trying to capture the fencepost-space-fencepost nature of their composition. I wasn't particularly clear on this point, though (to say nothing of the mistake in the original post).
Interesting, thanks for the info. This shouldn't work and I wonder how `stack` does it.
Nice insight. The defaulting of `Bail` isn't possible in my use-case, but yeah, if it can be defaulted then we basically recover a list/other monoid of `Stump`s. Thanks for reminding me about Semigroups---that removed the need for a unit which was another deviation from monoid.
I'm maybe viewing this from a pragmatic perspective, but I really think it should work! I don't mind a compiler error when working with `ghc` to tell me that I should use proper `-XPackageImports`. Also if this wouldn't work, we could never have bleeding edge (so to speak) `containers` versions in stackage lts releases at all.
And is it in any case worse than what you'd end up with if validations were handled by your db-persistence library? At least you're in control of it...
The only way to experience this "leak" is if you're holding on to an ever-increasing number of old bytestrings. It's not hard to argue here that the actual leak is the fact that you're holding onto an unbounded, increasing number of bytestrings as your program continues, but the problem here is that such leaks are made noticeably worse by the memory fragmentation of pinned bytestrings.
I mean `Map` IS a self-balancing binary tree.
Sounds like a good idea.
Use `unsafePartsOf` to turn the `Traversal` into a `Lens` returning lists, _then_ interpret that in the redis monad.
the Details: C compiler cannot create executables
Neat! It's a pity there is not a web service API for the reverse list. (the Atom feed can stand in as one for the restrictive upper bound dependency list) 
Sorry, that was not well put. Actually, I mangled two sentences/ideas: "These statistics could help users decide... " Ã "These statistics alone could aid much the assesment of libraries" I fully agree that the statistic "alone" shouldn't be used to inform the picking of a library to use. While it is true that this might make me libraries hard to take foothold, it would also help against ecosystem fragmentation. Currently you often get two pages of Hackage result when you search for a concept of field of application (in prose), and it's a huge chore to find out which library is most apt, mature, well maintained. In the search results list I'd like to see the following information at least: * name * reverse dependencies * publication date of most current version * lines of code * license And would like to be able to filter for those. 
I'd be interested if you could move your London office somewhere that remains in the EU.
 newtype T a b s t = T (Traversal s t a b) type (~&gt;) = T String (Maybe String) instance Profunctor (T a b) where dimap f g (T k) = T $ \o -&gt; fmap g . (k o) . f instance Traversing (T a b) where traverse' (T k) = T (traverse . k) instance Strong (T a b) where first' = firstTraversing instance Choice (T a b) where left' = leftTraversing runT (T t) = unsafePartsOf t mget :: [String] -&gt; IO [Maybe String] mget bss = putStr "mget: " &gt;&gt; print bss $&gt; fmap pure bss example :: a ~&gt; a example = T $ \o -&gt; (traverse o ["foo", "bar"] $&gt;) main = runT example mget () This prints `mget: ["foo","bar"]`. I'm using `unsafePartsOf` to aggregate the traversal into a list before interpreting it in a `Functor`. Also note that I'm using `T String (Maybe String)`, not `T [String] [Maybe String]`. A `Traversal` is like a `Lens` that can hit multiple targets, so when you have a `Traversal' s [a]` you really mean "`s` contains many lists of `a`".
Polymorphic literals are odd. So it's a problem with the `[]` literal, not with the `String` type. Similar problems happen with numeric literals: `sqrt 4 == 2` is true. `4 \`div\` 2 == 2` is true. `sqrt 4 \`div\` 2 == 2` is a type error.
Thanks for that :)
What haskell platform are you running and what windows version? Instead of running gcc does running realgcc work? (it should be in the same path as gcc) 
I see no reason why it should be hard; in cabal's model, they're nothing more than over-sized freeze files, i.e. merely additional constraints for the solver to satisfy. Noteworthy in this context is also [`jenga`](https://github.com/erikd/jenga): &gt; A trivial Haskell program that hopefully makes a Haskell Stack project with poorly defined package dependencies buildable with standard tools like cabal or mafia. 
We have a large Yesod application with hundreds of routes, and compiling the routes DSL is not a significant compiling bottleneck. Our tens of thousands LOC of shakespeare templates (HTML, CSS, and JS) do slow down compilation quite a bit, but it's still quite usable and practical at that scale.
Ah, I see. Yes, using `unsafePartsOf` for interpreting into the target works correctly, yielding a single `mget`! The `Functor` constraint makes me feel comfortable, too, since it guarantees nothing is being smashed together in the Redis monad. Very nice!
What are they doing again - hosting Hackage/haskell.org?
Strats seem to be taking over this bank.
If you'd like to build a SPA, give https://haskell-miso.org a try. It is like ReactJS (or Elm) but all in Haskell.
Note that in Scala, mutable data structures use equivalence-equality based on the shape of the collection at the time you compare it. collection.immutable.Seq(1,2,3) == collection.mutable.Seq(1,2,3) At least in a strict language, identity-equality is **_almost never_** what you want. The Scala behavior exemplified above is perfectly reasonable (inasmuch as it is "reasonable" to use mutable data structures!). Scala programmers rely on it _all the time_. We have a different `eq` operator for identity comparison, but it's only used rarely, in arcane implementations of performance-sensitive stuff. That javascript uses identity equality by default is just historical baggage likely inherited from Java.
There is no *need* for this.
Here is a link to [reflex](https://github.com/reflex-frp/reflex).
CDN for hackage (although, that's currently disabled since there were some issues with cache updates not propagating fast enough).
Again, what you say is correct: As I described in the GHC issue, a short (for example, 1-character) bytestring can keep an entire 4KB block alive, thus in the worst case blowing up the memory usage of each such bytestring by factor 4000. Yes, that is a "constant factor". But if your application requires to hold, say, `numByteStrings` = 10 million bytestrings as part of its normal operation (i.e. not "leaking"), then you can end up with 400 GB RAM wasted. A constant-factor leak is also a leak. Wasting factors-of-thousands memory is what I call a leak. You can continue to argue that it's "asymtotically not an issue"; in the real world, "just buy 100x the RAM and you'll be fine to run Haskell in production" isn't helpful advice.
We only have one DB, but we sometimes need access to the raw DB connection to use some custom features of PostgreSQL, beyond what can be accessed using `rawSQL` or a custom persistent field type. So in addition to the usual `runDB`, we have `runDBConn` which starts a transaction and then puts you into a monad with two operations: `runConn`, which puts you into a reader monad that provides the raw DB connection; and `runPersist`, which puts you into the usual persistent monad as if you had used `runDB`.
With respect to the ORM problem, I've wondered where the analogue to F#'s type providers are for some time. It shouldn't be hard to generate some TH at compile-time that maps to a predefined SQL schema. For example, the following provides nice guarantees about the resulting sql queries and you get help composing them with the type checker. http://fsprojects.github.io/FSharp.Linq.ComposableQuery/ I don't see why the same couldn't exist in Haskell. Sprinkle some FK traversal son top and you've got some convenient queries. 
I concur with what you've said here and have the same reasons. Validations happen at different reasons and different times with different constraints, it's not a purely DB concern and should just be DAL (data access layer) stuff you write yourself. A library cannot dictate DAL because it's almost always application specific. If you want structure for DAL, here you go: write a function You can write generic ones for the easy cases, specialize where needed outside of that. A common pattern for me is that I'll use UUIDv4 in my models so that are non-enumerable public ids that can be used to reference the data in a URI. I'll always put the UUIDv4 as the final field in the Persistent schema. Why? Because then I can have a generic function that takes a `(UUIDv4 -&gt; record)` value and generates and applies the final value, for insertion or otherwise.
It's not so much the allocation itself that makes the problem, but rather fragmentation: That any small bytestring can keep alive an entire 4K region, thus in the worst case (and this happens to my servers) multiply memory usage by factors of somewhere between 10 and 1000.
Tangential question: Does persistent really give a *runtime* error for this? Can it not pick it up at compile time?
It turned out I needed to use vec for a project, so this is great timing! There's a use case I'm not sure how best to handle, though. I want to insert a static Vec, i.e. of known length and content at compilation, into a module. It seems there are two options: use the `VNil` and `:::` constructors (with `fromPull` if necessary) or use list syntax and extract the value from the `Maybe`. The former is a little unwieldy, the latter means I'm introducing the possibility of a runtime error, if somebody changes the length of the list. How do you handle this, /u/phadej, on your projects? In similar situations I've created a QuasiQuoter that extracts the `Maybe` during compilation. Would you be open to a PR?
Just off the top of my head, maybe all thatâs necessary is adding citext to this function? https://github.com/yesodweb/persistent/blob/0b8c716e8910c49da434a6e84bd7cccd505bee24/persistent-postgresql/Database/Persist/Postgresql.hs#L445
This is a very interesting read indeed.
I chalk this up as yet another bad scala idea. This has a rather colossal downside: (==) is a non-pure function! When you perform it can change its answer. You have to care about _when_ you asked a question as well as what you asked. Moreover, assuming a sane structural equality, (==) in Haskell gives you what you need for `x == y` gives`f x == f y`. But that is no longer a guarantee you can provide when f can mutate x and y.
&gt; where a single syntactic form can take on more than one type I think that's the violation-of-intuition that makes it feel a bit odd. In simpler type systems, a term (in context) always has exactly one type. With the polymorphism in Haskell, we still have principal types, but not unique types, and so we get "weird" substitution behavior when the a term has different types in equality and the expression we are substituting into. --- Separate, but related, with type classes, we can also get the ambiguous type error when if a polymorphic term retains it's principal type instead of being refined by context. I think most will agree that it would be better if `[] == []` reduced to `true` instead of giving a type error.
Yeah, having typed guidance is so helpful. Part of why I wrote [`persistent-typed-db`](https://github.com/parsonsmatt/persistent-typed-db) -- one of our work apps has to access seven different database schemas, and moving that information to the type level has saved a lot of pain.
Given a list of StudentMarks, I will return a list of Int. For a list of StudentMarks, `stMarks`, match each element on the pattern `(st,mk)`. Create for me a list of the items which are bound to `mk`. Note, this implies that the input structure, `StudentMarks` is a type synonym for `(_,Int)`, where `_` could be anything at all.
&gt; I chalk this up as yet another bad scala idea with sloppy semantics. The semantics are not sloppy; they're perfectly well defined. Besides, how is it a bad idea to give users something that does what it looks like it should do, instead of something useless and surprising (comparing identity)? &gt; You have to care about when you asked a question as well as what you asked. That's what you consciously expose yourself to when you use mutable collections. It's a well-understood tradeoff that _every useful operation_ on a mutable collection has. Breaking the behavior of useful functionalities to make them artificially pure (like making equality compare identity) would be entirely pointless. Frankly, defending this stance looks like blind dogmatism to me. &gt; Haskell gives you what you need for x == y to imply f x == f y. But that is no longer a guarantee you can provide when f can mutate x and y. By definition, there _cannot be_ any valid notion of referential transparency as soon as you start using imperative features anyway, so it's not like you're losing anything! You're just talking about the benefits of purity here (which I already know). 
I'm interested in carefully tracking effects. Strictness or laziness doesn't enter into it. I'm not interested in using an ML with those semantics, either. On the swing side, the fact that languages like scheme gives so many immutable objects a checkable reference equality is actually a huge source of problematic semantics that prevent decent optimizations in that language. Reference equality for mutable objects has always worked out to be exactly what I was looking for when I actually use mutable objects. I use references and compare then for equality inside unification engines and type checkers. At this point I'm going to agree to disagree. The fact that the alternative isn't even a thought you can think once you start tracking effects is telling you something. "Blind dogmatism" is an argument that cuts both ways.
It should be possible to get it to check this at compile time. I've filed an issue on the repo, maybe I'll get around to it :)
&gt; I'm interested in carefully tracking effects. In that case, you don't use (untracked) mutable collections. So why do you have an opinion about the semantics of their equality? &gt; On the other hand, reference equality for mutable objects has almost always worked out to be exactly what I was looking for when I actually use mutable objects. I use references and compare then for equality inside unification engines and type checkers. I agree. This is why in Scala classes that are not data classes still default to Java's identity-based equality, which is fine. But when is the last time you compared mutable _collections_ for identity equality? In my experience, this is a need that virtually never occurs! &gt; The fact that the alternative isn't even a thought you can think once you start tracking effects is telling you something. Haskell's `Eq` cannot represent impure equality (which is the natural equality for impure collections), but that doesn't mean you should give mutable collections a **stupid** implementation of `Eq` instead! IMHO, `Eq` should not be implemented at all for mutable collections; instead you'd use another operator like `=!=` from some `MutEq` type class, which accounts for the effect. And if you really want reference equality, have yet another operator for it (like in Scala).
I don't think you win that much by using list syntax. I tend to format Vec (and e.g. `NP` from `generics-sop`) as myVec :: Vec Nat3 Int myVec = 1 + 1 ::: 2 + 3 ::: 3 + 4 + 6 ::: VNil You just have to keep in mind that `:::` is `infixr 5`, i.e. tighter than `&lt;$&gt;`, `&lt;*&gt;` or `==` and looser than e.g. `+`, `*`. (`:i &gt;&gt;=` in GHCi). I mean, you don't need to remember exact fixities, just be aware that comma in `[foo, bar]` binds very loosely, and `:::` doesn't
The `Map` is a self balancing tree of 'k's. The 'v' is just a tag-a-long. The exact same implementation with only 'k' would be your `Tree a`. This also exists in containers, it's `Data.Set` 
&gt; In that case, you don't use (untracked) mutable collections. So why do you have an opinion about the semantics of their equality? If I'm going to write things in "IO" in scalaz, I may as well reach out for immutable collections to make things perform better. There can be a log storage factor here and there lost to path copying otherwise. Again I'm going to disagree. I see no point in removing the only notion of equality that is possible, _a useful_ notion of equality simply because you don't want to use it. I use these very "stupid" semantics so that I can compare mutable reference equality with (==) just like any other equality, quite frequently. Consider syntax trees with an abstract type of free variables, flush in metavariables that happen to be references, then flush them out after zonking to compare for equality. The same (==) check I use to swap free variables (which are typically strings and the the like is exactly the same (==) I use to swap out the meta variables after I'm done with unification and subsumption checking. I see no need to write that code twice just to make you happy. It works out of the box with precisely the correct semantics. I have no objection to you making a class Monad m =&gt; LPTKEq m a | a -&gt; m where eq :: a -&gt; a -&gt; m Bool and tying your code to it, but vanishingly few uses of equality fit that mold. As for your last point, I already pointed out that beingable to even _check_ reference equality for most immutable things is just as problematic. When you can check for the equality of two immutable things you just created, commoning them up with common sub-expression becomes an inadmissable optimization. Check you can check random lambdas for some notion of equality you lose lambda lifting and lowering. Scala doesn't even try to optimize almost anything it produces for a reason. I want nothing to do with semantics that force me down that same path. As for comparing mutable collections for reference equality, I actually do this with arrays in Haskell on a fairly regular basis. See http://hackage.haskell.org/package/structs
That makes a lot of sense. I think due to the kind of events we need to store there, it's quite feasible to just use an SQLite table.
If you find a 'primary metric' by which to assess a package's worthiness, please share it. =P Cohesion in an ecosystem is generally better than fragmentation. I would err on the side of cohesion - Especially considering that, at current, there is absolutely nothing preventing newer packages from totally dominating the ecosystem, even if this has nothing to due with their relative merit and they end up abandoned at the advanced age of 6 months old.
&gt; Unfair. That can be said of any app in any language. No, if I do the same thing in a different language, this does not happen, and other non-Haskell server programs that I've run 10x longer and processed 100x more data have not suffered from this. I also don't think it's unfair. In the spirit of /u/saurabhnanda's slides, we should not find excuses for such bugs or talk them down. We need to fix them. Only then will we have better times with Haskell in production.
I wonder if Haskell banking jobs are more in building infrastructure or quant modelling of financial assets.
Lack of static linking is a real killer for me (e.g. linking to CUDA), but thanks for the tip. I might use a bit of this for some other work I'm doing...
&gt; Sorry for persisting with this. No problem, it's good that you do it. I don't know the answer for the web servers, but I'd hope that they by themselves should not hold to some long-lived bytestrings, unless they carry some kind of cache (e.g. for cookies or sessions or templates or something like that) around. With aeson I'm reasonably sure that unless your app holds to `Value`s across handlers, all ByteStrings that aeson allocates will be GC'd. If you need to hold to short bytestrings in your own code, you can convert them to [ShortByteString](https://hackage.haskell.org/package/bytestring-0.10.8.2/docs/Data-ByteString-Short.html) do avoid this issue. However, the difficult part about this bug is, as you correctly identified, not your own ByteStrings, which you can find with a simple grep, but those that may lay hidden in libraries that you use, which is why it would be great to have a solution that also works for normal ByteStrings.
What case are you using to house the 4x PCIe adapter? I'm just using a mini-ITX, though admittedly I'd like to utilize PCIe if possible...
Did you mean to say âlack of dynamic linking?â Only reason I donât have dynamic linking is that it seems like it would need a bunch of info about the targetâs file system. Iâm just not sure where to tell it to find the shared libraries, or how to get e.g. the musl shared lib to the right place.
Half of that was the lens library ð jk
Check this out http://haskelltools.com/ It does not have all features you've mentioned, but PRs are welcome
/u/tomejaguar yea it gives a runtime error. Maybe it's because it is template haskell? I don't - that's magic to me for now.
I'm currently reading [The Last Samurai](https://www.amazon.com/Last-Samurai-Helen-DeWitt/dp/081122550X) by Helen DeWitt, recommended to me by another Haskeller.
Thanks for that - I really enjoyed the talk! I found it to be quite relevant to my Haskell journey.
Thanks for that - I really enjoyed the talk! I found it to be quite relevant to my Haskell journey.
A generalization of [StÃ¥lmarckâs Method](http://research.cs.wisc.edu/wpis/papers/tr1699.pdf) - SAT meets abstract interpretation.
I can see that this really is indeed a serious issue and it needs to be dealt with. However, I will confess that I've never used or heard anyone use the word "leak" they way you are using it. With that definition, every single boxed data type leaks because it uses more memory than it theoretically *could*.
suggestions are very appreciated.
Yes, fixed!
 {-# LANGUAGE OverloadedLabels, DuplicateRecordFields -#} import Control.Lens ((^.)) import Data.Generics.Labels () data Record1 = Record1 { a :: Int, b :: Bool } deriving Generic data Record2 = Record2 { a :: String, c :: Int } deriving Generic test :: () test = let r1 = Record1 { a = 5, b = True } int1 = r ^. #a bool = r ^. #b r2 = Record2 { a = "Hi", c = 3 } string = r2 ^. #a int2 = r2 ^. #c in ()
The simplest solution should not be overlooked: takeDigits = P.takeWhile (isDigit) octet = do nums &lt;- takeDigits &lt;* string "." if (read (unpack nums) &gt; 255 || read (unpack nums) &lt; 0) then fail ("Invalid octet: "++show nums) else return nums 
thank you
Definitely Debian. You donât want to use Arch on a server. Debian is stable and wonât break things. Arch is all about being bleeding edge and breaking things. Also arch keeps breaking Haskell packages even more frequently than anything else. I use arch on my desktop computer with xmonad, every time I upgrade packages compilation of xmonad breaks, sometimes xmobar stops working and needs to be recompiled, etc... Arch is great if you donât mind some instability, the trade off is that my computer boots in less than a second, and had no crap running and slowing it down, that I donât need. Debian is still clean enough but is very reliable and you are unlikely to have downtime because the distro broke something.
or the conduit library, it feels like the thing loads and compiles everything that was ever written in haskell
Very nice. and the ipfs concept too
yeah ipfs is very interesting, I just wish there was default browser support so that users didn't have to rely on gateways.
Some ghcjs animations use webgl, if you don't see blue icosahedrons in the bottom then it's not working on your platform :/ (didn't happen in my tests yet though)
We're using Ubuntu at work and it's great.
I've been rereading Philip Wadler's "Monads for Functional Programming" ("the original monad tutorial") in preparation for talk on it in a month.
Looks good. How did you interface with IPFS? What's your haskell stack?
the blog was generated with hugo, the animations with ghcjs. IPFS doesn't need interfacing, you make all your links relative and you add it to ipfs. I can add ipfsjs and interface with haskell from there (that way the viewers would also be serving the website at the same time, no more traffic problems!) but sadly that wouldn't be very beneficial as the users mostly rely on gateways to see this stuff.
How would you go about building a haskell DB interface for an arbitrarily large application, wherein all SQL executed by said application was in the form of parameterized raw SQL? Would this approach benefit from type safety in any way? 
The idea is not having central servers
true but gateways are centralization points.
In the case of SCB, both. It started more as the latter, but has grown into the former.
When I run then from cmd, that all work. Currently I have removed the official platform and have a functional platform which I set up using stack. It's likely cabal's fault
What? What does cabal have to do with this? 
Ah I understand. I think that there is a javascript implementation, so a direct browser access would probably be available in the near future.
ipfsjs, someone should make ghcjs bindings for it! it's still not very usable for browsing since you still have to load the js file and it takes a while to connect. But it will enable awesome applications for sure (i'm waiting for an ipfsjs based decentralized comment system to appear so i replace the github thing) also check out crdts
why not nixos?
I always use ubuntu for work and arch at home on all my pcs. 
Assuming you want to host a website, use Debian because you're not living on the bleeding edge with it. To avoid problems with out-of-date GHC lIbraries, use `stack` or Docker to isolate your app from the libraries in the package manager.
So I've been working on monad transformers, and I've come up with a slightly different approach, using a number of classes: Firstly, [MonadTransConstraint](https://github.com/AshleyYakeley/Truth/blob/f675e24cb933114a5de44cf8aede83db0791991e/shapes/src/Control/Monad/Trans/Constraint.hs), which witnesses the lifting of constraints (e.g. `Monad` and `MonadIO`): class MonadTrans t =&gt; MonadTransConstraint (c :: (* -&gt; *) -&gt; Constraint) t where hasTransConstraint :: forall m. c m =&gt; Dict (c (t m)) This can be instantiated for `Monad` and `MonadIO` with all the transformers in the `transformers` package. Secondly, [MonadTransTunnel](https://github.com/AshleyYakeley/Truth/blob/f675e24cb933114a5de44cf8aede83db0791991e/shapes/src/Control/Monad/Trans/Tunnel.hs), which allows transfer of transformer from one monad to another: class (MonadTrans t, MonadTransConstraint Monad t) =&gt; MonadTransTunnel t where tunnel :: forall m2 r. (forall a. (forall m1. t m1 r -&gt; m1 a) -&gt; m2 a) -&gt; t m2 r remonad :: MonadTransTunnel t =&gt; (forall a. m1 a -&gt; m2 a) -&gt; t m1 r -&gt; t m2 r remonad mma sm1 = tunnel $ \tun -&gt; mma $ tun sm1 It's easy to write instances for all the transformers except `ContT`, so easy in fact you'll notice that the `m1` and `m2` types are completely unconstrained. (There *might* be an `instance MonadTransTunnel ContT`, but I couldn't see how.) Thirdly, [MonadTransUnlift](https://github.com/AshleyYakeley/Truth/blob/f675e24cb933114a5de44cf8aede83db0791991e/shapes/src/Control/Monad/Trans/Unlift.hs): class (MonadTransConstraint MonadIO t, MonadTransTunnel t) =&gt; MonadTransUnlift t where liftWithUnlift :: forall m r. MonadUnliftIO m =&gt; ((forall a. t m a -&gt; m a) -&gt; m r) -&gt; t m r class MonadIO m =&gt; MonadUnliftIO m where liftIOWithUnlift :: forall r. ((forall a. m a -&gt; IO a) -&gt; IO r) -&gt; m r This replaces [MonadUnliftIO from unliftio-core](https://www.stackage.org/haddock/lts-9.14/unliftio-core-0.1.0.0/Control-Monad-IO-Unlift.html). Now here's the clever thing: **I have instances of `MonadTransUnlift` for `StateT` and `WriterT` as well as `IdentityT` and `ReaderT` that I hope I can convince you do the "correct" thing with no discarding**. Consider this snippet foo = liftIOWithUnlift $ \unlift -&gt; do unlift foo1 forkIO $ do unlift foo2 unlift foo3 ... * It's obvious how this should behave for `IdentityT` and `ReaderT`. First `foo1` runs, then `foo2` and `foo3` run in parallel, all with the environment. * For `StateT`, I use an `MVar` to transfer state from `foo1` to `foo2` to `foo3` (or `foo1` to `foo3` to `foo2`), which are serialised rather than running in parallel, which then yields the final state of `foo`. The only caveat is that it's up to the caller to make sure that `foo2` has started before the main body of `foo` has ended, otherwise the thread will be permanently blocked. * For `WriterT`, I use an `MVar` to assemble outputs from `foo1`, `foo2`, `foo3`. Note that `foo2` and `foo3` can run in parallel. There's a similar caveat: `foo2` must finish before the main body of `foo` has ended, otherwise the thread will be permanently blocked.
Wasn't that supposed to be the entire idea behind `MonadCatch`? 
Hrmmmm.... The only law `Semigroup` needs to satisfy is associativity. Can a `Bail` be constructed using only information present in it's adjacent stumps? If so, you could still leverage this technique to create a valid semigroup instance.
Check out the functions ending in `M` or `M_` in `Control.Monad` - Of special interest whilst navigating the filesystem are `filterM` (useful in combination with `doesFileExist`) and probably also `zipWithM` Also, note that `System.Filepath` exports `&lt;/&gt;`, which can be used instead of `path ++ "/" ++ otherpath`, and avoids a bunch of dumb mistakes that I, personally, make all the time, like accidentally appending two path separators, etc. 
[traildb](http://traildb.io/) [traildb-haskell](https://github.com/traildb/traildb-haskell)
I don't get the comparison with the boxed data type. A box (`struct` with a tag + a single pointer in it in C++ terms) does something, it is part of your program; ghc does something productive with it. But if you have a 4K chunk of memory, and at its beginning sits, say, a 1-char ByteString, then the entire rest of that chunk is wasted free space. At some point in the past it contained data that your program used, but now that space is essentially empty, it does not contain any data that your program will ever access again, and the free space can never again be used to allocate more memory into it. This situation has all the characteristics of a standard memory leak (memory once-in-use, then never read re-used again) -- the same characteristics as if you had `malloc()`ed just under 4KB and forgotten to `free()` it.
The term "leak" usually implies an unbounded increase in memory usage over time. This is not the case with this bug.
Also note, the technically correct term for this is (as slyfox mentions in the linked issue) ["memory fragmentation"](https://stackoverflow.com/questions/3770457/what-is-memory-fragmentation); however, memory fragmentation as commonly referred to in C/C++ is usually measured in "a good percentage" of your memory, not large factors of 100, so I don't think calling it a "leak" is far-fetched.
Yeah I just think throwing around "leak" like that rustles people's feathers. I get that it's a serious issue. But I can't honestly call it a leak. &gt; forgotten to free() But that's exactly why I disagree with the term. No one has forgotten anything. It's just that they were insanely inefficient with space. The comparison to boxed data was not great. The point is, there are tons of cases where would use a more efficient memory layout. Those aren't leaks. They're just inefficient. This is a particularly *egregious* case of inefficiency.
Wow! Seems a lot like what I was looking for, where do I send the beer :)
 This kind of bugs are a big PITA, mainly because there is no easy way to fix it. If I remember correctly the reason why pinned memory is used by bytestring is to facilitate FFI calls to C. It would really be weird if the bytestring is relocated while a C function is munching around it.
&gt; The term "memory leak" usually implies an unbounded increase in memory usage Even a single `malloc()` without `free()` is usually called a memory leak. Valgrind calls it that way. [Wikipedia calls it that way](https://en.wikipedia.org/wiki/Memory_leak) too: &gt; a memory leak is a type of resource leak that occurs when a computer program incorrectly manages memory allocations in such a way that memory which is no longer needed is not released &gt; A space leak occurs when a computer program uses more memory than necessary None of these "leak" definitions require unbounded memory usage over time.
**Memory leak** In computer science, a memory leak is a type of resource leak that occurs when a computer program incorrectly manages memory allocations in such a way that memory which is no longer needed is not released. In object-oriented programming, a memory leak may happen when an object is stored in memory but cannot be accessed by the running code. A memory leak has symptoms similar to a number of other problems and generally can only be diagnosed by a programmer with access to the program's source code. A space leak occurs when a computer program uses more memory than necessary. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/haskell/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^| [^Donate](https://www.reddit.com/r/WikiTextBot/wiki/donate) ^] ^Downvote ^to ^remove ^| ^v0.28
&gt; But I can't honestly call it a leak. Well, OK. [Wikipedia calls it that way](https://www.reddit.com/r/haskell/comments/7e8bom/slides_joy_frustration_of_putting_34000_lines_of/dq80hiz/), so that's accurate enough for my purposes. I also think this is rather unimportant.
No doubt that if you want to run arbitrary C functions on it, that memory should be pinned. I think we should look into whether we can implement pinned memory in a way that doesn't have the amount of fragmentation blowup. The simplest idea that comes to mind: Could we just `malloc()` them? Of course that would be to somewhat slower than just bumping a pointer, but C programs still perform fine with that approach, and it would only be done for pinned memory, not all Haskell memory. If this is a correct alternative implementation, we should benchmark if it brings a noticeable performance penalty.
Thanks for the reference. I suppose the term is looser than I realized.
If you had transitioned from, say, Scala, to Haskell, I think your experience would have been much smoother. But it's worth noting that you went from the "dynamic, OOP" quadrant straight to the "static, FP" quadrant. Changing only one of those coordinates would have been a challenge, but you changed both! To be sure, many of your frustrations were not only at the language level, but the "culture" level. I'm sure you would agree with this. I'm only saying for the sake of others.
This is really cool! Few points: * Better names ford A B would be 0 I (uppercase o and i). * Do you know how many cpu instructions one rewrite takes? * Problem with lambda encoded data is they are very hard to understand. You said it took you few years. I guess you should write some guide with common lambda tricks to share your intuition. 
Thanks!
1. I'm aware. This toy syntax is so unreadable that I don't even bother improving that in particular. Wish we had a proper, serious language. 2. Very, very few; about 50 total, I'd guess? There are two rewrite rules. One frees 256 bits and rewrites 128 bits of memory. Another one allocates 512 bits, frees 256 bits and rewrites 128 bits of memory. It takes a few straightforward 32-bit comparisons to decide which to do. 3. I'd love to write a book (perhaps a long article, actually) with all I've learned so far! Don't have the time for that right now, though...
Of course you can hide the lambda encoding in library. But if you want to build language on top of lambda encoded data structures, you cannot write all the libraries yourself. It basically makes the contribution bar for your language very high, hurting the adoption pretty badly. 
Thank you !!
Not in a library, you could compile Elm to interaction nets eventually, for example, no need for new languages.
yes it would! Having an abstraction over the DB is fine, as long as it's transparent. Type safety should be a priority, IMO. An ORM just isn't the right solution, as it leads to all sorts of problems, such as reordering of side-effect.
Either one and then the nix package manager and/or stackage on top. FWIW I run arch on servers now without a second thought. You basically need to know what youâre doing, and bleeding edge is not always a bad thing. Stories of arch constantly breaking are over exaggerated.
Thank you, yes I want to host a website, based on some answer on a previous thread I decided to compile to statically linked binary with docker then `scp` to the VPS. I guess this is what you meant? however, that way do I need to install anything on the debian server?
what do you use for deployment if I may know?
There are a million places in mathematical notation where the ordering of symmetric things is taken advantage of for convenience purposes. Anyone who appeals to mathematics as having better notation than a programming language hasn't cracked their teeth on the syntax of enough mathematics texts.
thanks, do you deploy statically linked binary ?
Very nice writeup! I'm not even sure I would recommend writing it differently, but I wanted to point out something about the "unpredictable effects" caveat. The specific example of `withFile` is one of the examples where it's complete safe to run it in both `ExceptT` and `StateT`: * There's only one `m` input action * There's no branching possible on the result `a` value to allow not performing cleanup when a `Left` is returned I'm a strong believer in the general point you're making though, so I'm on the fence even making this comment :)
The underlying distro only matters in two ways: * how stable / maintainable it is (not Haskell-specific) * how frequent breaking changes are - this only matters if you're depending on C libraries provided by the distro The second issue goes away if you use Nix for C dependencies, which seems to be somewhere between bleeding edge and good practice these days. Personally, I'd recommend Debian, because server maintenance should be boring, and Arch has a habit of making breaking changes and assuming you've been checking their website to find out about them.
I use the Ubuntu approach as well. No docker, nix or similar things. Just statically linked executable and it works fine, built in Ubuntu 16.04 and deployed in 14.04 with no error. The only possible weak part of my program can be the postgresql library that requires it to be installed additionally (I guess that one is dynamically linked), it must be the same version, I didn't have any problem yet. I had a similar experience with mysql library (building in Ubuntu 16.04 and deploying in Debian 8.4).
Yes. (Also an Ubuntu user) we package up the static binary into a Deb package that installs the 3 deps every Haskell build has (libc, libgmp, libz). That's it. I'll ask my boss if I can share our Makefile
Docker. I have a CI setup with drone.io that is hooked to git and builds images for me on master push for prod and on dev branches for staging. 
I don't quite follow MonadTransTunnel or how exactly your MVar stuff works. For folks that are confused how the MVar approach might work for, say, concurrently + StateT, take a look at this example: https://github.com/mgsloan/misc/blob/master/statet-concurrently-2.hs . This allows the concurrent actions to run in parallel, until the 2nd action depends on the state, at which point it will block waiting for the first action to complete and yield a state. My first idea was a bit more clever and a bit more evil, was to use MonadFix and laziness to achieve this: https://github.com/mgsloan/misc/blob/master/statet-concurrently.hs It'd be interesting to see how this approach pans out in practice. I think explicit concurrency and synchronization is likely to be quite a lot more pragmatic. It seems likely to me that stateful monads just do not mix well with concurrency and exception handling.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [mgsloan/misc/.../**statet-concurrently-2.hs** (master â c09db8b)](https://github.com/mgsloan/misc/blob/c09db8bc7bf40c9afbad05ad9e90a84ace64eb81/statet-concurrently-2.hs) * [mgsloan/misc/.../**statet-concurrently.hs** (master â c09db8b)](https://github.com/mgsloan/misc/blob/c09db8bc7bf40c9afbad05ad9e90a84ace64eb81/statet-concurrently.hs) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dq899qh.)^.
Hmm I think I prefer the forced-serialisation approach for `StateT`, I think it's more predictable.
I think https://www.fpcomplete.com/blog/2017/07/announcing-new-unliftio-library deserves an honorable mention.
I am mac user, so I will not be able to build using my dev tools. however, I was thinking about using docker with stack or setup a VM similar to the server. 
If you use Docker this way, there is nothing more to install, because your static binary is self-contained.
Hmm, more predictable than explicit use of MVars? It seems quite counter-intuitive to me that usages of `fork` would essentially turn into sequential execution. If I understand correctly, this means that the author of code that uses such a fork would need to anticipate that some instances might block, and so cannot rely on progress being made on multiple threads at once. I agree that it is better than my toy examples, but these are just things I was playing with as a golf of "what if we could?!" rather than something we'd actually want to use.
no, I do not need to install any haskell stuff, I just copy the executable and the assets (config files, images).
What happens when you normalize `x.y.binAdd x y` or `x.y.binFold (binAdd x y)` on the two examples respectively? When you apply those reduced terms to the same numbers you tried, does it reduce the rewrite count?
Doesn't work for me QmfN5DojVnEsf1Une3DFwfUiFpfWnQf31f61qgybiXVeQE:192 Content Security Policy: The pageâs settings blocked the loading of a resource at self (âscript-src 'unsafe-eval' *â). Source: hljs.initHighlightingOnLoad(); I'm using a lot of extensions though. Also, the content is on IPFS but you still reach out to unpkg, google fonts and a few other domains
so highlighting is blocked. Yeah i'm using a template which included things like google fonts, unpkg is for the comments so the github comments thing works. but i moved most of the sources into ipfs. I will see what I left out. Maybe your extensions blocked the javascript and webgl stuff.
@bmabsout &gt; Does anyone know how to get gradientDescent to accept optimizing [V2 a] with the cost being of type [V2 a] -&gt; V2 a. I ended up converting V2s to lists and back For the input try [`Compose`](https://hackage.haskell.org/package/base-4.10.0.0/docs/Data-Functor-Compose.html#t:Compose). For the output try [`quadrance`](https://hackage.haskell.org/package/linear-1.20.7/docs/Linear-Metric.html#v:quadrance). Something like `gradientDescent (quadrance . f . Compose))`. (I probably don't understand what a cost of type `V2 a` means)
Thank a lot, btw how do I know whether my libraries depends on (libc, libz ...)?
well explained thanks, how do I make sure whether my libraries depends on c libraries or not?
Building a 8.2.2 nightly as we speak so it should be out any day now. 
awesomeThanks :)
Or maybe you meant LTS? Should also be soon, we were waiting for 8.2.2 for that as well! 
ya V2 doesn't make sense it should be [V2 a] -&gt; a but since this is polymorphic over any Floating a then it had to have the shape of [a] -&gt; a, I can actually assume that I have Vs always and make it make more sense. (So in this case V2 a is just pure of the quadrance). Thank you I will try Compose and add the Vs assumption!
That's a more interesting question - ideally they'd mention it in the documentation, but transitive dependencies cause issues there. I think the simplest approach would be to compile/run the program in a minimal environment, like an Alpine Docker container or Nix.* If it fails, then you know there's a dependency you're missing. * This is why Nix is so effective - it forces every program/library to be explicit about its dependencies.
let's say i have cost [V2 z w, V2 x y] = x*z*y*w and I want to gradientDescent (cost.Compose) [V2 1 2, V2 1 3] which gives my type errors. How can i do this? I don't understand AD well enough
One of my friends had pretty strong opinions and said MonadBaseControl is an anti-pattern. I'm on the fence. Sometimes it really is better to redesign your code so as not to need MonadBaseControl. Other times well it is a necessary evil.
Just dockerize 'em all, who cares about distros? :)
I have a project with about 10kLOC using Snap and while the documentation may not look abundant, I haven't found it lacking in any major way. You'll need to have a basic idea of how to use snaplets (not necessarily how they work) and how to use splices for your templates. From there you can easily use additional features as you need them. I have to say I'm quite pleased with Snap because it's easy to understand what is happening, as opposed to Yesod which is what I started with. If you're relatively new to Haskell I'd definitely recommend Snap over Yesod. That said, there some slightly more difficult things. I still haven't figured out exactly how compiled splices work and if it's worth switching to them. Interpreted splices are fast enough for me, which is why I haven't made a serious attempt yet.
In support of the above: We've been using the python based supervisord for many years to run our haskell and python long running code. Mostly in Debian, but also in Centos or Redhat and occasionally ubuntu. The variety of versions of supervisor (and python and other tools) in each distro/release have caused our devops team no end of pain. In response, we've recently we've switched to doing this using nix (the package manager) with a patched version of supervisor to set some default config appropriately for our software. (patches managed via nix too). This gives us a consistent environment across each distro/release. This has been a big improvement. Since then, we've started with a few green fields deployments on nixos machines (both physical and virtual). For our purposes, this has been nothing short of extraordinary. The ability to manage our code and config as systemd services has been just brilliant. The journald http gateway has been awesome for viewing and browsing logs from our services. All we lack is a web gui for our on call people to restart the odd errant process (though this hasn't yet been necessary either on a nixos machine... - coincidence I'm sure, one day it will be). When I go back to one of our traditional ("legacy?") machines, the need to manage the dependencies manually, the need to manage potential rollbacks, the lack of testing of final binaries and all dependencies and the need to actually change files on the server directly (and have the risk of local manual changes being overwritten) all mean our default position for any future changes is quickly becoming deploy a nixos machine (managed by nixops) and prepare to migrate services over from the old server. Anything else just feels primitive.
Nix is not really suitable for a production server since you won't get security support for any significant amount of time. The community is just too small.
Docker is a good way to keep thing security hole compatible for a long time.
Is this working for everyone else? I downloaded the installer on OSX, but `make install` fails for me with `mk/config.mk:533: *** missing separator. Stop.`
Do you have an example of this approach? I'm not seeing how you would do this without having generated SQL statements.
Are the three regular polytopes in higher dimensions the analogue of the tetrahedron, cube and octahedron?
I think your question inspired this blog post: https://www.reddit.com/r/haskell/comments/7ex5wv/monadbasecontrol_in_five_minutes/
Can you tell us what's around that line?
&gt; If you have a function `foo :: MonadCatch m =&gt; m a`, then you can specialize the type to any `MonadCatch` instance. For `(MonadCatch m, MonadIO m)`, you can specialize to `m ~ IO`. What's the `MonadIO` for here? You can specialize `foo` to `m ~ IO` as well, you say it yourself in the last paragraph.
Sure, starting at 525: #----------------------------------------------------------------------------- # C compiler # # NB. Don't override $(CC) using build.mk, re-configure using # the flag CC=&lt;blah&gt; instead. The reason is that the configure script # needs to know which gcc you're using in order to perform its tests. GccVersion = 9.0.0 7.5 ifeq "$(phase)" "0" CrossCompilePrefix = else CrossCompilePrefix = endif That `7.5` is line 533.
yes, there's n-simplex(n dimensional tetrahedron,) an n-cube, and an n-orthoplex.
Thanks for the advice. 
So it's a massive cosmic coincidence that the dodecahedron and icosohedron appear in 3 dimensions? 
If you can find everything you need in nixpkgs, then go for it! I've tried it only for a short time but it worked like a charm. The only reason I couldn't use it on my main machine was that I still need a proprietary application that is not in nixpkgs. Due to the way NixOS works, binaries must be patched and in my case was a PITA (I had to give up for the time being).
Do you use Stack?
Sorry, I was wrong and gave you wrong advice. module Main where import Numeric.AD import Linear.V2 import Data.Functor.Compose cost :: (Num a) =&gt; Compose [] V2 a -&gt; a cost (Compose [V2 z w, V2 x y]) = x * z * y *w results :: [Compose [] V2 Float] results = gradientDescent cost (Compose [V2 1 1, V2 2 3]) main :: IO () main = print (take 10 results) &gt; main [Compose [V2 0.39999998 0.39999998,V2 1.7 2.8],Comp[...] This is a complete example, hope this helps.
let's take hexagons for example, if you try to tile 3 you get a flat shape that you cannot bend. this is why the number stops at 5. this numberphile video explains it way better than i can: https://www.youtube.com/watch?v=2s4TqVAbfz4&amp;t=14s you should check it out
oh this is perfect thanks!
Ok, so the version detection of the C compiler seems to have found two versions "9.0.0" and "7.5" rather than one. I think it's worth filing an issue in the GHC issue tracker, and also letting us know the exact output of "`$CC -v`" where CC is the C compiler that was picked up by the `configure` script. For reference, here's how we currently extract the version number: fp_cv_gcc_version="`$CC -v 2&gt;&amp;1 | grep 'version ' | sed -e 's/.*version [[^0-9]]*\([[0-9.]]*\).*/\1/g'`"
It worked for me on OS X, I guess I don't have two GCC's! 
hah, that's funny. I don't even remember when I installed the real GCC. I thought I just had Apple's tools! Anyway, I'll post `$CC -v` tomorrow when I'm back at that computer.
No, I didn't mean to say that you should use raw statements, more something like [esqueleto]](https://hackage.haskell.org/package/esqueleto). 
No, we have a Docker image that has the necessary libraries installed. We build the binary on a compatible machine (either a dev box running 16.04 or a Docker container setup for the purpose), copy it into the deployment container, and ship it up to ECS. 
Ok, where did you install it?
There is a package called `flow` which provides (in my opinion) more appealing operators. You can find more information [here](http://taylor.fausak.me/2015/04/09/write-more-understandable-haskell-with-flow/). There was a big debate on whether using those operators is a "good idea", and if you are just getting started in Haskell, it would be discouraging to wade into that debate (or be exposed to it) this early. My suggest is don't use them **yet**. It will be easier for you to understand other's code (and for them to understand yours, when you ask for help in IRC / Reddit, etc), if you stick to the more common syntax. However, once that is no longer a concern, do what will get you writing more Haskell. ;-)
I'm a web developer by trade, so I wanted to learn how to build websites with Haskell. I made the mistake of going from one framework to another. I'd encourage you not to do this. The problem is that you'll get stuck, think "oh, that framework makes this easy", and then you'll get stuck in the new framework and jump to a third option. /u/bitemyapp often suggests yesod for beginners, though I hope he sees this and chimes (versus me putting words in his mouth).
Are there any options I can use to tell GHC to throw an error when there is a language pragma that is not necessary? 
&gt; Also arch keeps breaking Haskell packages even more frequently than anything else. I use arch on my desktop computer with xmonad, every time I upgrade packages compilation of xmonad breaks, sometimes xmobar stops working and needs to be recompiled, etc... I've resorted to just installing `stack-bin` from the AUR and compiling all of the haskell programs I use. Which has really driven home how slow GHC has become. I recently finished writing an ansible playbook to spin up a fresh dev machine, and it spends about 5 minutes installing packages and about 35 min compiling.
If anyone feels inspired by this talk and wants to write some exercises, here are two formats which make these easy for the author and/or for the learner. [Let's Lens](https://github.com/data61/lets-lens) consists of a large number of very small exercises teaching lenses from different viewpoints. It is implemented as a bunch of incomplete functions in which you need to fill in the `undefined` bits, in a way which makes the doctests pass. That certainly looks easy to author, but when trying to fill in the exercises, the format was quite inconvenient because `stack test` outputs the hundreds of failed tests for the functions I haven't yet implemented instead of focusing on the one I'm currently working on. (Also you have to comment out the `Lets.Lens` module because of [undefined is not a good dummy implementation for functions involving RankNTypes like lenses](https://www.reddit.com/r/haskelltil/comments/4q0oni/undefined_isnt_always_a_valid_dummy_implementation/).) At the other end of the spectrum, [Try Haskell](https://www.tryhaskell.org/#step2) makes it super easy for the learner: it runs in the browser so there's no setup, you focus on one exercise at a time, and it's easy to experiment with other Haskell expressions if you want to practice your new knowledge before tackling the next challenge. Unfortunately adding exercises looks quite difficult, as each individual exercise uses a [custom validation function](https://github.com/tryhaskell/tryhaskell/blob/83ae0d3699899a94a24d8269e08aa2764550c7bb/static/js/tryhaskell.pages.js#L67)! I wish there was a tool which could convert a sequence of easy-to-author doctest-based exercises into the corresponding tryhaskell-style web app, that would be the best of both worlds.
It depends on what production means. I probably wouldn't use it for a financial company doing huge bleeding edge stuff with critical components that could cause national security concerns... But there's a lot of stuff out there that doesn't need that level of assurance; even then, if you have people on your team who know NixOS decently well, I don't really see the security support being an issue.
Only the unlift invocations are serialised. Consider: foo = liftIOWithUnlift $ \unlift -&gt; do forkIO $ do longSlowComputationA unlift foo1 longSlowComputationB longSlowComputationC unlift foo2 longSlowComputationD The two threads run in parallel, except that `foo1` and `foo2` are serialised.
Do you understand what Haskell compiles to right now? I doubt many people do, but they use it just fine. If Haskell's ADTs were Church encoded I doubt anyone would notice as long as the performance as acceptable.
Those terms have no normal form, so we can't reduce them and copy-paste their normal forms as new terms, if that's what you're asking. Doing so would probably alter the count, because converting to/from Î»-calculus causes loss of sharing. If you're just asking about this, though: ``` f= x. y. (binFold (add x y)) (f X Y) ``` This increases the count by precisely 2 (because of the added applications).
I actually did not intend to post this just yet, I intended for it to be a draft and to gain some criticism and suggestions! Fortunately the comments are helpful and it seems to be going over okay :)
I recommend Yesod as a default reco to virtually everyone. Everything has some kind of sharp edge or project level full-stop-failure-mode that makes it unsuitable unless you really know what you're getting into. I also recommend using Persistent and Esqueleto. There are some videos of me doing some proper work with these things on my Youtube as I have been streaming myself working off and on: https://youtube.com/c/bitemyapp
Yes it was, but it's way too hard to use correctly.
Indeed I distinctly remember working on that patch to address this very issue but it looks like I never merged it. Quite unfortunate.
This is kinda out of nowhere, but a recent article on HN spurred a bit of thought in me. I just wanted to say thank you for all your hard work in the community. I originally started with zero Haskell experience, making a CMS (kinda advanced actually) in Yesod several years back, and have always liked your approach to explaining things and teaching things to others. Just a small thank you from a random Haskeller :)
I wonder if Linux namespaces might not end up reducing the need for patching? It seems absurd in this day and age that we can't just 'virtualize' away the whole environment surrounding an app.
ldd should probably be able to tell you.
How large is this Strats team, they seem to be hiring constantly.
/u/mightybyte Any updates on your book in physical form? :)
I'm getting a feeling we're going to be seeing Kombinators soon... Meanwhile, I'm reading the Three-Body Problem trilogy by Liu.
If somebody needs an example on how to run a Haskell server on NixOS and deploy it to AWS using nixops take a look at: https://github.com/basvandijk/nixtodo
I recently posted about wanting to remove direct references to primitives in an interpreter. I have no clue if it's good or not, but I used this to figure out a way to do it: https://gist.github.com/inversemot/c1ab2ba3a4a6ad65b15df129af277dbf
I'm amused that he writes Haskell data types like an ML programmer would. ::D
I've written several WordPress plugins that use Haskell as the "engine" for their functionality. It's hard to tell exactly what your WordPress app is doing. Most WordPress apps don't do much beyond the CMS / eCommerce side of things. TBH, if you were hoping to move away from WP as a CMS, I'd probably recommend against it. However, if you have a lot of custom logic and PHP APIs, then certainly you can write those in Haskell instead. But, start very, very small. Use the most basic tools possible. A very begin-friendly web server framework is https://github.com/ChrisPenner/Firefly. If you give your Haskell server access to the same MySQL database, then you could also interact that way.
Yeah, itâs got the basic blog/ecommerce functionality that WordPress seems to do well, but I also have a lot of user interactions, profile info and a bunch of other stuff that Iâve stuffed in via plugins, and Iâd like to move those out of WordPress. I thought about Lumen, and I know PHP better, but.. Haskell :-) Thanks! Iâll give it a shot, 
This has nothing to do with Haskell, per se, but have you absolutely, most certainly, reached the point where you can't do without microservices? Please avoid microservices if this is a serious app, and the only reason you are doing this is to be able to play with Haskell. If this is not a serious app, then knock yourself out. WP has a REST API now, which should make things easier. However, anything that generates CSS/HTML for the final website, is harder to do in anything apart from the WP PHP API. 
A lot of his resources are in Racket. So maybe the tendency to tuple data types comes from there?
Why do you dislike micro services? 
Hop on IRC or Slack FP channels. Asking lots of questions is the key to success.
For me the answer is: 1. It's a buzzword. ;) 2. 1 microservice + 1 microservice + 1 microservice = macroservice. I still have to maintain the same thing at the end of the day...just now I've got TCP, network, serialization, deserialization, individual versioning, and docker involved. 3. YAGNI. 
It adds a layer of complexity and many more moving parts, which makes life harder. Unless there is a clear gain which offsets this pain, avoid it. 
I was going to argue that in the future as the app gets more complex it would eventually pay off, but then I realized thatâs a classic case of over engineering, it seems... Iâm assuming the right strategy would be to avoid micro services until necessary, and when it becomes necessary, then start building new features as micro-services and slowly move pieces out of the monolithic portion and move them into micro services?
A network barrier is an insanely high price to pay for separating concerns, and if you don't get it exactly right, then you're hosed. Start a Haskell service if you want, and implement features in there. But that service can be called in a variety of ways, and a REST API is only one of them. You could shell out to a CLI tool or use [`call-haskell-from-anything`](https://github.com/nh2/call-haskell-from-anything) to expose a Haskell library to PHP. Once it needs to be on a different service, feel free to create the network boundary then.
I donât really feel like itâs a âbuzzwordâ but I havenât looked up the definition. At least it means something... I guess. Technically speaking, donât most kinda use a micro-service framework as it is? npm, rubygems, and phpâs composer all bring in a bunch of âmicro servicesâ or APIâs donât they? Finding this really interesting! Every other software subreddit has micro services in high regards... but the Haskell community seems to disagree with the rest of the world.
&gt; Anything else just feels primitive. This. Once I learned Nix, it felt like learning Haskell. Everything else suddenly felt cumbersome to use and less likely to work.
I wonder if there's any hope of Stack and Cabal new-build eventually having binary caching a la Nix. Binary caches are one of the best parts about Nix, and there's absolutely nothing forcing this to be unique to Nix.
Huh? Nixpkgs is one of the most comitted-to repos on GitHub. AFAIK, they're pretty on it about keeping stuff up to date usually. And whenever you need a security patch, it's dead simple to apply it yourself.
That's very kind, much appreciated.
Yes absolutely! I started building an "exercise-book" style tutorial based on the try-haskell backend, but the limitations have cut that implementation short: https://github.com/sordina/crashkell
Then how would it handle pragmas that alter program semantics when not present but still compiles? e.g. Strict
&gt; all bring in a bunch of âmicro servicesâ or APIâs donât they? Yes! I think you're right to see it that way. Haskell takes that idea *very* seriously--far more seriously than most languages. For example, mutable state is avoided and controlled by the compiler, effects are tracked, and composition is enforced to comply with a known protocol (type safety). This is all great stuff and it relates to microservices. However, if you can do that just in the same app...why would you bother to run it in two difference processes, or even two different servers? You add *tons* of complexity that way. You're right that the buzzword has real meaning. The trouble is that many people seem to think that it is the right solution to the basic "interfacing" problem. In fact, it makes *that* particular problem much, much harder. The point is, pick it for the right job. It's great at on-demand scaling, fault tolerance, etc. It's an extremely costly way of ensuring you don't share mutable state. GHC can do that for you with its eyes closed.
Is there a video of the talk?
Not to speak for /u/Bollu, but LLVM is written in C++ and provides its primary API as a C++ library.
Thanks for the write up! I always thought one of the bigger reasons to use micro services was a smaller code base, making things easier to test, and more re-usable. It seems to make sense what youâre saying. Looking forward to giving it a shot in Haskell. Just noodling around at this point. Maybe the design of Haskell makes âmicro servicesâ less necessary than if you were to use php or something else.
Ok, I did `cc -v` and here's what I get: Apple LLVM version 9.0.0 (clang-900.0.38) Target: x86_64-apple-darwin17.2.0 Thread model: posix InstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin Found CUDA installation: /usr/local/cuda, version 7.5 So it seems that `7.5` is actually coming from CUDA!
That's really neat. Is there a way to use this to interface with DBs with similar or the same code for both DB processing and CPU processing? Seems like this structure would allow automated code generation for both.
If you are dealing with LLVM that implies compilers. I can't imagine ever picking C++ over Haskell for compiler development, doubly so for prototyping.
I have done few Haskell widgets over my time at the uni. * [Computing Haskell using a human](https://functional-programming.it.jyu.fi/dc/tasks/doubleLambda/page). There are about a dozen different tasks, each with some basic Haskell construct. Of course you don't learn to program doing these, but hopefully one can learn the difference between Java and Haskell evaluation models. Authoring these is nice and makes use of dhall. (Most student errors are about parentheses) * (Actual online coding exercises](https://functional-programming.it.jyu.fi/new-resources/CardViewer/App.html?deckName=DestutterExercise) (log in as tester/tester) which are an ugly combination of hs-src-exts whitebox testing and smallcheck. These are seemingly fun for the students, but nasty as *bleep* to author. Managed to do some 20 or so before running out of steam. * [Parenthesis adding game](https://functional-programming.it.jyu.fi/master/pages/Currying.md#CheckPE9). Hardest thing in learning Haskell: where do the parentheses go? After 5 years of teaching, I'm almost willing to trade ML-style syntax for a Java-like one. I can certainly give these away to a better home if someone wants them.
Theoretically yes. Nested relational algebra is an old idea, proved to be equivalent (IIRC) to normal relational algebra which is what SQL is based upon. Practically, I haven't seen implementations, yet read about https://querycert.github.io/icfp17/resources/icfp17.pdf :)
&gt; Perhaps a more satisfiy choice, albeit less convenient in Haskell, would be to use an HList or extensible record for the keys. I used the [NP](http://hackage.haskell.org/package/generics-sop-0.3.1.0/docs/Generics-SOP.html#t:NP) type from "generics-sop" for [this small module](https://github.com/danidiaz/Grammatik/blob/master/Tabelle/src/Tabelle/Internal.hs). 
I would add the newtype to the transformers-base directly. I'm sure Bas won't object
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [danidiaz/Grammatik/.../**Internal.hs** (master â 9570d6b)](https://github.com/danidiaz/Grammatik/blob/9570d6bd581df3b295773e9e7092b8f11ec83e38/Tabelle/src/Tabelle/Internal.hs) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dq9nslp.)^.
All of the following do not work well across service boundaries: * profiling * stack traces * type-checking * IDE support They also slow down productivity for little benefit
I donât even know what my stance is on NN, but I was going to upvote this before I noticed two other almost identical submissions on the same subreddit by the same submitter, one per language. WTF?
I mean in a situation like that GHC isn't going to complain about the pragma since it does not meet the definition of "not necessary", since it affects semantics.
Because a type system can only help you with verifying the *internal consistency* of your logic. When most of your system's functionality is split between various programming languages, you're losing the benefit of having a type checker.
The last time I looked it seemed a lot like libhunt might be scraping programming language subreddits for content, presumably for SEO purposes. So multiple posts to different programming language subreddits would make sense.
I really like these kind of articles by matt might. I personally learned a lot about compilers from reading his posts. Two complementary article to the one OP posted are: - [Transforming to ANF](matt.might.net/articles/a-normalization/) - [Writing a CESK style interpreter](matt.might.net/articles/cesk-machines/)
Is it just a wrong impression that the standard for claiming a political opinion is so much lower than what we usually require in other areas of life? An example how an interesting discussion can start: http://marginalrevolution.com/marginalrevolution/2017/11/take-change-mind-net-neutrality.html It seems to me quite hard to come up with good answers either way. Shouldn't we be much more humble when forming such opinions?
Off topic
I do think politics (as it relates to programming) is important to discuss, but I want it to be fair discussion about long-term ideas and solutions, not being pressured into say "boo!" about whatever noise Twitter has decided is important for the day. For this reason, I support temporary moratoriums on whatever the hot-button political issue is until the crusaders have moved on to something else.
On a project of mine, `-j` just made it slightly slower. (Benchmark!) OTOH, ` +RTS -A128m -RTS` did make it a little faster.
At least, little benefit until you get to the ~100 engineers working on the same platform situation.
Is it really lower? What other areas of life are you referring to? Counterexample: &gt; "I think chocolate is the best ice cream flavor!" I don't imagine many to hold up such a statement to higher scrutiny than political opinions. --- But moving past that point, I like the approach the article's author takes: let's base the decision on data and evidence rather than opinion. Yes, I'd love to read more of those kinds of arguments than the ones I see most often around, e.g. saturating Reddit's front page. But further still, as some commenters point out, why not switch it over? Why not require that we hear evidence for the potential benefits of not-NN? On which side lies the burden of proof, anyway? And why? But moving further still, if only we could have some kind of an aggregation for political claims and decisions (or any kind, really, not just political), listing evidence supporting and undermining a statement at the same time; possibly sorted by relevance and weight, that would be phenomenal. Anyone knows about such a website perhaps? - Sidebar: Just as I typed that out, I hit on my forehead and took to Wikipedia. Lo and behold, [they do list arguments both for and against](https://en.wikipedia.org/wiki/Net_neutrality#Arguments_against). I've read through most of them. They are not quite so evidence based as I'd have hoped for, but at least aggregated. But continuing on; in the general case I wonder if we could compute over evidence, calculating what kinds of theories and statements are supported and what kinds aren't. And to do it in a transitive way: e.g. if a new discovery is made through experimentation (like the famous double-slit experiment) could we compute what then-commonly held theories the new finding undermines and how? I suspect some kind of semantical model would be required for this, right? Or, conversely, a rigid formal language? I seem to remember that someone has posted about a semantic graph application they were writing in this very subreddit, IIRC it was written using the `brick` package. Anyone remembers the name of that project? Could something like that be used for this? And if not, has anything like what I outline above been ever attempted? Or is this in the realm of science fiction? 
&gt; When most of your system's functionality is split between various programming languages, you're losing the benefit of having a type checker. I think this is an interesting point. If you wrote all of your services in Haskell though and built their API's using Servant (and imported the API type definitions) I think you could recover most of the benefit. &gt; It's great at on-demand scaling, fault tolerance, etc. I think in particular it's great for being able to independently scale different parts of your application.
I'm planning to try out an ANF CEK machine next. The nice thing about ANF is that there's no need for a bunch of "application" continuations. The CEK machine I posted has 3: two for function application and one for primitives. I can probably reduce that to two, by abusing the trick I used for the primitives (involving keeping a list of args.) That trick was necessary as otherwise you'd need continuations for each arity of primitive. Doing an ANF transform, means this is all done by the time you start interpreting, meaning you don't need continuations for primitives or functions. 
&gt; When building a product, I think you need to be very aware of your "novelty budget", and focus on solving business requirements and less on things like novel ways to express haskell records. &gt; Would you expect that a standard web-app that has been implemented already would be too much trouble in Haskell, that it would require "solutions to Haskell problems"? You'd think that for a language/ecosystem that's spoken of so highly, this would not be a novelty in this day and age. 
Another resource I liked was: http://www.cl.cam.ac.uk/~mom22/fp/lec3-opsem2.pdf It deals with a more complex language. It does have the problems I mentioned with primitives (add has two continuations, one for was subexpression.) I do like the unnecessary arguments added to the continuations though. By putting in explicit holes it makes it easier to understand what each continuation is for.
[Project:M36](https://github.com/agentm/project-m36) supports nested relations as "[relation-valued attributes](https://github.com/agentm/project-m36/blob/master/docs/tutd_tutorial.markdown#group)". Nested relations are actually fundamental to the relational algebra and not always isomorphic to an unnested representation. For example, without relation-valued attributes, it is impossible to make queries similar to OUTER JOINs in SQL.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [agentm/project-m36/.../**tutd_tutorial.markdown#group** (master â 4b4b595)](https://github.com/agentm/project-m36/blob/4b4b595facc407bcb8f760ae9ba0b1d8fb2db8a9/docs/tutd_tutorial.markdown#group) ---- ^(Shoot me a PM if you think I'm doing something wrong.)
I would argue still little benefit at even larger scales. The only compelling use case for services that I'm aware of is that they provide a fast way to call code written in another language
Comment out the import and look at the errors.
I've seen this approach before, in [Qubes OS](https://www.qubes-os.org/intro/) and maybe another one that I can't remember right now.
Maybe the size does not matter as much as the average time an employee stays in the team?
&gt; not always isomorphic to an unnested representation I find that surprising. Do you have an example?
Something like [this](https://en.m.wikipedia.org/wiki/Argumentation_framework)? Came up in a course I did on agent-based systems. Would be nice if more analysis pieces used tools like it. But, political movements aren't typically won with data and argument graphs, they're won with compelling narratives.
**Argumentation framework** In artificial intelligence and related fields, an argumentation framework, or argumentation system, is a way to deal with contentious information and draw conclusions from it. In an abstract argumentation framework, entry-level information is a set of abstract arguments that, for instance, represent data or a proposition. Conflicts between arguments are represented by a binary relation on the set of arguments. In concrete terms, you represent an argumentation framework with a directed graph such that the nodes are the arguments, and the arrows represent the attack relation. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/haskell/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^| [^Donate](https://www.reddit.com/r/WikiTextBot/wiki/donate) ^] ^Downvote ^to ^remove ^| ^v0.28
Non-Mobile link: https://en.wikipedia.org/wiki/Argumentation_framework *** ^HelperBot ^v1.1 ^/r/HelperBot_ ^I ^am ^a ^bot. ^Please ^message ^/u/swim1929 ^with ^any ^feedback ^and/or ^hate. ^Counter: ^115315
**Argumentation framework** In artificial intelligence and related fields, an argumentation framework, or argumentation system, is a way to deal with contentious information and draw conclusions from it. In an abstract argumentation framework, entry-level information is a set of abstract arguments that, for instance, represent data or a proposition. Conflicts between arguments are represented by a binary relation on the set of arguments. In concrete terms, you represent an argumentation framework with a directed graph such that the nodes are the arguments, and the arrows represent the attack relation. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/haskell/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^| [^Donate](https://www.reddit.com/r/WikiTextBot/wiki/donate) ^] ^Downvote ^to ^remove ^| ^v0.28
I am not sure whether [this](https://github.com/kwaleko/blog-post) help, recently I was was working on a simple blog site and its implemented with web server, REST api and DB functionality. please don't hesitate to ask me any question if needed
im sorry i did leave out watching it, im sure its good.. i love clojure for most parts.. but i heard from a friend that he doesnt like pattern matching because of it... so for sure regex is powerful and i dont want it in my code if possible... its just unmaintainable... BUT... pattern matching in elixir can be super awesome, but still readable and maintainable! like using pattern matching in function args, defining function multiple times, saying if arg is nil do this, if arg is this do this, ... (having same function defined multiple times with different guards is what i actually love so much about elixir to make it more favorite than clojure... also elixir pattern matching with `case function(blah) {:ok val}` is also something i really like...) anyways i love clojure (though i hate JVM), and i love elixir even a little bit more (though i had not yet the chance to do prod code with it :( )
/u/ephrion thank you!
Hello /u/MaxGabriel. I am 100% going to look at your repo, but I'm going to try and fix it myself first. ;-)
Just noticed the `universe` function; that's pretty helpful in one of these instances.
compile with `-ddump-minimal-imports`
Would love some discussion of `stack` vs `cabal new-build`, esp. seeing as I'm interested in messing with Backpack. If going the latter route, does one just install [GHC 8.2.2](https://www.haskell.org/ghc/download_ghc_8_2_2.html) and [`cabal-install`](https://www.haskell.org/cabal/download.html) directly? What are best practices when moving away from `stack`? Thanks!
It is not clear to me from the paper whether there are new ideas to make type checking better than with previous attempts, or whether it is mostly a new implementation with a specific choice of unification algorithm. Out of the three challenges described in Section 2 (unification, type abstraction and scope management), the last two look like reinventions of existing approaches in the type inference literature.
Laziness definitely has a lot of great uses and composability is definitely better with it. With a strict language this loss may be somewhat painful, but in my opinion the toll laziness takes on debugging ability alone is simply not worth it. Besides, laziness also has issues with space leaks, but perhaps even more importantly these are easy to introduce in places that did not have it before without it being obvious that it will be so. Many examples depend on brittle optimizations to remove this problem, but they can't be reliably depended upon and different compiler version may break code working code. Just see the page on full laziness and space leaks on the conduit website. There is some pain in strictness, but it's predictable and usually easy to locate (and large data sets need care even with laziness). I can't speak for everyone but I would trade it in even for just getting back traces and erroring out at definition rather than use.
I got lost in the threads about strictness and laziness and completely forgot that this is off topic, but still now that these threads are here I'll leave it hoping to be corrected in some way.
You can define infix operators for use in function types? What is this magic?
I thought something called a "vector" in haskell was technically a contiguous 'array' of somethings, whereas a "list" is a 'linked-list' . this 'vec' seems to confuse the naming by calling something that is an indexed-linked-list a 'vec'
Yes, video, please
Please stop making things up about us. I don't want to get into a fight, but we don't push anyone towards JWT. Some people use it, most don't, period. Thanks in advance.
As long as all the parts are running the same *version* of your API. ;)
Guys, let's keep making bad monad tutorials, so everyone will think we're very smart. :)
I can comment about going from `stack` to `cabal new-build`, when working on **private** multi-package (application) codebase. - Add `index-state: 2017-11-22T11:24:01Z` (or some other timestamp) to the `cabal.project` That will define the package universe in use. That's kind of similar to the `resolver` field in `stack.yaml`, but actually very different, as you have still access to multiple versions of each package. - You could (ab)use `constraints` in the `cabal.project`, if you are lazy defining proper bounds in every `pkg.cabal` file. (you probably are in multi-package projects). - Occasionally a dependency on Hackage has restrictive (upper) bounds. Then you can add selective `allow-newer: newtype-generics:base`, when `newtype-generics` has restrictive bound on `base`. For example, our `cabal.project` has `constraints: servant &gt;= 0.12`, so we don't need to repeat that bound across every service (= separate packages) in the repository. Also note, that there isn't upper bound, as `index-state` kind-of covers that. When we need newer version of some package, we need to bump `index-state`, but then we can verify that everything still compiles and (our) tests pass. When we found a working install-plan, we `cabal new-freeze` it. The next time we need to upgrade depedencies, we delete it, bump `index-state`, tweak `constraints` and eventually `cabal new-freeze` new plan. `git diff` tells nicely what is updated, as `cabal.project.freeze` is sorted. I want to **emphasize** that's that a different workflow from developing *libraries intended to be used also by others* (read: uploaded to Hackage). And IMHO it's good to acknowledge the goals and constraints are different.
http://snapframework.com/ they don't seem to have https enabled.
Please advise. Your relatively fresh eyes and pragmatic point of view provides a unique and valuable perspective. Just because youâve lost hope of Reflex becoming beginner friendly doesnât mean I have ;) But I need to know what is perceived as the problem for beginners.
# DON'T READ THE COMMENTS
Just write more programs and struggle through it. Eventually you'll stop failing.
I would guess that what feels like a big roadblock is probably a series of smaller ones. Maybe you could post specific issues and solving them might clear the way for you? To answer your Emacs question, according to a recent survey, Haskell users are pretty well split between Emacs and Vim. I personally use Vim. But there aren't any particular features that make me feel tied to my editor. If you have syntax highlighting and [ghcid](https://github.com/ndmitchell/ghcid) for interactive compilation feedback then you should be good to go.
Why is anti-intellectualism so rampant in software engineering? People are literally saying in the comments that if they have to think about something to understand it then that concept is a failure in and of itself.
This was a kind tutorial that was nice to follow and don't you EVER say otherwise again
Oh wow, you're right... lol. From my perspective (as a complete newbie to haskell, with no mathematics education beyond high school), this video and description is fantastic. It was a bit long winded, but some people like to describe that way -- and I learn best through catch-all, "windy" descriptions. I look forward to reading his book. :D
Hmm. Is it just me or is this explanation woefully misguided? The average person with a passing knowledge of computing will probably find just the Haskell data constructors and pattern matching, plus the âifâ and âcase ofâ syntax in conjunction with the Maybe type a bit too much to take in such a short amount of time. Let alone any notion of WHY that is a good, safe approach before then the dealing with the complexity that it itself causes and how Monads can then help clean it all up. 
I really don't understand the point of computerphile videos that take esoterica from within the field and try to explain it to a lay audience. What's the point? In the extremely unlikely scenario that a viewer might understand what a data constructor even *is*, not to mention everything that followed &amp;mdash; what do they do with this knowledge? 
I don't really know best practices for moving away from stack, but I can say for using backpack, I use 8.2.2 and cabal out of the box. (8.2.2 is important as anything older than the previous release candidate has problems with backpack when template-haskell is involved, 8.2.1 is right out.) My cabal.project files so far have been fairly minimal affairs, e.g. packages: coda.cabal lib/*/*.cabal package coda-lsp optimization: False I personally build against the latest of everything rather than use `index-state` shenanigans. Things I personally feel, "aren't quite there yet" on the new-build front: * The lack of `cabal new-install` is pretty annoying. * I'm currently hoping we will eventually get multiple-external-library packages, because once you start using backpack the status quo becomes really awkward really fast. I have ~20 packages in one git repo now, and I shudder to consider what it'll mean when I have to coordinate releasing them to hackage, dealing with needless deprecations as I add and remove from the set for purely internal implementation details, etc. The current approach just doesn't make sense. If on the other hand a third-party library could just ask for `coda:coda-dyck` and get the mixin package, and I could release the whole kit and caboodle to hackage in one go in one cabal file, where I can use cabal 2.2 common blocks to reduce the endless amounts of boilerplate? Where I can have _one_ version number rather than have to bump up 20 of them? Then that story starts to make sense. * I've still got a few corner cases I've found when working with backpack that I currently have to work around: e.g. [haskell/cabal#4908](https://github.com/haskell/cabal/issues/4908) On the other hand, I've been getting a lot of mileage out of doing things like using `backpack` to unpack previously un-unpackable things and starting to factor my code in ways that let me reuse large swathes of what would previously have been a lot of nearly cut-and-paste Haskell.
With both Elm's purity and Scala's complex type system under your belt, I would have expected picking up Haskell to be easy for you! I would be quite curious to hear in which way you feel like you're failing?
To be fair -- I wasn't an elm expert, I didn't use it long, I didn't love it. Scala is beautiful to me on the other hand. I go out of my way to write pure code as much as possible. All that fun functional stuff is already in my blood. With haskell I think the biggest problem at the moment is lack of awareness and experience. I remember when I wrote my first ever Java program, basically all I knew was `System.out.println` makes words show up and so it was basically the only method I used until I realized you could do more lol. I think I'm encountering this same problem now, but grown up and far spookier.
Without examples, it would seem you explicitly have gaps in your understanding of fp languages like Haskell. Compared to cats, Haskell's implementations of monadic effects and type classes are much more straight forward than how cats/ Scala work (scalas use of implicits). Use Haskell to solve problems, there is more to fp than referential transparency and pure functions. 
Walking off the beaten path in Servant is something that: 1. Happens more frequently in Servant because you're more narrowly focused 2. Is a lot more punishing and difficult to overcome in Servant, in part because of how much leverage you're squeezing out of the types. I'm speaking from experience here, I use Yesod and Servant in my day to day work. This is how it's been for the last couple years. &gt;Please stop making things up about us. I don't appreciate the implication that I'm lying when I'm speaking to direct experience with the framework in personal and work projects. You may not _like_ it, but past attempts at web frameworks had wider ranges of use-cases anticipated and covered and/or were less punishing when you did wander outside those anticipated use-cases. That's true of Yesod, Snap, Happstack, and the long-tail of "micro" frameworks like Scotty and Spock. I think this is mostly because you're attempting something more ambitious, which is great! But you need to realize that it's really only an experience absent hard blockers when people are building their app is more or less the same style as you do. Outside that, most people get quite badly stuck. Problems I've run into in the past like _optionally setting a header_ required a patch from Karni to get sorted out.
Nice! That was fast! And not too many changes: https://www.stackage.org/diff/nightly-2017-11-24/nightly-2017-11-25
It would have been two days faster if the old build server hadn't gone rogue and kept pushing out 8.0.2 nightlies :D 
I'm hopeful for a future where we have equal support in each major editor. At the moment I think emacs is ahead due to Intero. I switched from Vim to Spacemacs for it and it was a good decision.
It's hard to help without knowing what specific problems you're running into. Have you tried to follow the Yesod book and write a Twitter clone?
I haven't, but that sounds like an amazing idea! Especially if I can use Purescript or GHCJS alongside "normal" haskell.
&lt;facepalm&gt; If you don't want to learn, don't watch talks about programming!
That's unfortunate. I saw a few comments asking why this was better than just using `NaN` or defining `div x 0 = 0`, so it seems like the point is being missed. I bet a tutorial that uses `Maybe` to deal with null references would be more relatable for the average C#/C++ dev, and might help them see the benefit. Then I think showing an example of a different monad (like lists) would be helpful to show how the same machinery (do notation, etc) is convenient in a variety of contexts. I think it's hard for people to see through the terminology, and that makes it hard to relate. You have to tackle a familiar problem that they understand, and show them that monads are a nice solution. Then show them that monads really are the *right* solution, because they solve a lot of other problems too.
Honestly, I don't think it's isolated to software engineering (as you call it[1]). &gt; People are literally saying in the comments that if they have to think about something to understand it then that concept is a failure in and of itself. I think basically everyone (at this point) considers YT comments as poison. It doesn't matter what the subject is (unless perhaps if it's *purely* aesthetic), but you'll basically get the lowest of the low. (There are great videos/talks on YT, but the comments... yeeeesh.) [1] Personally, I don't believe we're anywhere close to "engineering". I also believe the difference is actually fundamental due to the absurd amount of non-linearity in TMs (etc).
It's possible to [program in a non-functional style](http://vaibhavsagar.com/blog/2017/05/29/imperative-haskell/) in Haskell, so if you're getting hung up on finding the 'pure, right way' and it's hindering your progress, I would suggest writing it the way you would in any other language and then cleaning it up later.
Function purity is natural to me, it's mostly just like... Actually learning Haskell that's hard if you understand
This. I think a series of small issues could be a problem to many users of Haskell. It happened to me various times, these are some examples of issues of different magnitude which happened to me: - I wanted to see the type of a function I needed to use, the tooltip was not working, then I had to search it on hoogle by hand. This is not so common eg. in Java or C++, where documentation inside editor works ok. In Haskell papercuts like this are common. - In other situation, some values were calculated in a wrong way in my code and I needed to inspect what was happening. I had to use Debug.Trace everywhere, which feels like using printf in C instead of breakpoints at runtime. - There are some platforms which have too little support or not at all, like eg. for programming multiplatform mobile apps, I have to use C++ because support is at least good enough for those tasks. - Some compiler errors are a complete mistery. Recently I had a problem with a library which does not compile using new stackage lts, apparently because of some low level C glue magic, which seems related to the no-PIE/PIE fixes in GHC 8.2.2. I will try with latest stackage nighly soon. - There is a lot of functionality not available on Haskell today, like eg. machine learning or good image processing libraries. I think this will change with time when community starts to grow. I know one can help implementing a library, but when a person have to get work done or needs more than one functionality which is not present, they try other languages instead. - Sometimes you need a plug-in to enable functionality, eg. one to add colours to code, one for indentation, one for autocompletion, one for documentation, etc. Sometimes that plug-in does not exist, is not compatible with the plug-ins you already use, or does not work with the GHC version, etc. - Some issues are common and have known solution, but it is not evident at first sight, eg. String vs Text and similar problems caused by old cruft which is there because of compatibility with old code. I think a lot of people like Haskell as a language, and they really want to help, but is not possible for a single developer to solve those issues alone, that's why they abandon. Sometimes community does even say that tooling is not needed, that is not ok. Haskell could be better in every aspect, even in tooling, documentation and friendliness.
I think one big reason is that there are so many self-educated people in software. Not that self-education is bad in principle, but in practice a significant proportion of those people are self-educated for reasons that cause them to have negative attitudes to formal education and academia in general. They tend to be resistant to the idea that there's important knowledge they don't have, or worse, that they secretly fear they might not be capable of learning. 
I actually thought this was one of the better Computerphile videos (and I think they are generally very good) and commented as such on the video in the YT comments. My FP experience is limited to OCaml (and, more recently ReasonML, which is still just OCaml at the end of the day), and have struggled a bit with monads. Monads of course occur in OCaml, but they are not as front and center, at least not as explicitly. This was the best video I've seen that really walked through it methodically. It also made me want to look at Haskell a bit more. 
I wasn't criticizing the video.
The funny part is that due to this monadophobia C# has three entirely distinct notations for talking about monads when one notation would be simpler and give users more power.
I at least wasn't implying you were.
latest lts is still using 8.0.2 :(
IIRC, some of the earlier versions of Haskell lambda encoded all data. At first this was almost as fast as the regular encoded versions, but eventually that gap widened quite a lot. (Disclaimer, this is all based on something I read and can not find the source of, so take it with a grain of salt)
There was about a 4 month lag from GHC 8.0.1 being released and Stackage having an LTS for it. For 8.0.2, there was only a month lag. We're at about 4 months now from the release of GHC 8.2.1, and I anticipate that stackage will probably jump straight to a GHC 8.2.2 resolver for LTS.
I have to say that I have never used the exception system in haskell really but then I have not really worked on genuine web app. As a mental exercise, can you see whether first class exceptions like in SML might be better. SML has other warts that I do not like (almost all standard functions are uncurried and so are constructors), no separation of IO vs pure stuff. But exceptions are first class. Which might simplify some of the issues that you are bothered about.
I hope thatâs not true in the large, meaning, those folks who are self-educated think that education itself is a wasted effort. That would be so backwardly myopic that it would break my mind. Software development is pretty much a wholly human generated field; one could argue that none of it is based on natural principles past the EE stuff, so Iâm not sure how someone could EVER claim to be a self-erected pedestal of software engineering. For context, Iâm a drop out, and yes, also a sample size of one, but I love working hard problems and I do not like learning the hard way, so I have to study and study hard. Just because I donât have a thesis to defend or a test to ace doesnât mean I donât slam the books and try to make the best system Iâm paid to make. Iâd like to believe there are a good number of folks like myself out there, who for some reason just couldnât handle academia but are good contributors and apply engineering principles to their work, instead of just shunning anything they personally didnât âdiscoverâ. 
I think you could call certain software projects âengineeringâ if they use engineering principles to build them. For example, Iâd assert that Avionics and Medical Decive software is engineering. It requires a shitload of work, and software is such a young field it is still like building digital trebuchets, but we get better and better at it as time goes on. I think, though, that the pace at which software development improves is so fast it would be hard to call oneself a âsoftware engineerâ for any real length of time.
Fast and cheap results are in such high demand that many people shake off the details in favour of immediate results -- and I don't blame them. They fear being out of a job and fancy theory makes one feel inadequate. Besides, making rough logical associations can take you a long way. Those folks who respond to new terminology with a cocksure "Oh, you mean &lt;loosely associated synonym&gt;" could very well go on to produce the hottest mustache app. Clone and adjust until the numbers work. However, re-engineering from the ground up using modern knowledge and understanding requires comprehensive definitions for even the most primitive terminology. Learning the true meaning of a strongly defined term enhances the underlying logic of language for both verbal language, and programming. We need to re-engineer the core framework of understanding to yield a new idea. (By the way, the only reason I personally don't shy away from diving head first into the deep end in programming and math is because I'm in no rush. I started at the bottom, working minimum wage jobs since I was 11, and I'm not afraid of falling behind.)
I know there's linq, but what else? I'm not a C# person.
`-XTypeOperators`
People watch it, therefore they make it. It doesn't need to be useful or even correct. It's like asking why we keep producing reality TV and cooking shows and reality TV about cooking etc etc. It puts eyeballs in front of ads and is more fun than working at Starbucks so people will keep doing it.
First pass at an ANF CEK implementation for the mini-language I've been playing with: https://gist.github.com/inversemot/79d26a78e6037b3c00de65990429c1ae I haven't yet figured out how to remove the Prim constructor.
Isn't a monad just a monoid in the category of endofunctors?
So my personal journey was from Scheme (SICP) straight to Haskell (I had some C++, Java, and Python at the time I suppose.) I think at the time I basically divided Haskell in half: a procedural language (IO) that has maybe slightly weird syntax (do) and a typed lisp with less parens. I think I worked in that mindset for about a year? Well, I still write code that way most of the time. I spend most of my time in ghci, so I really only use basic syntax highlighting in a simple editor (Kate.)
What's taking so long? At this rate the lag will become longer than the support lifetime of a major LTS series.
Extremely important point that most people seem to overlook. Orchestrating different microservices to have the same version deployed at the same time adds too much devops overhead. 
There is ?., which is basically the maybe monad, and there is async/await which is the future monad.
I do not think lisp is typed, which means list in lisp allows tree too. They are not the same type in Haskell. I imagine a category in using Haskell.
Most dev's don't write DLS grammars , so when a data type Expr is defined and he calls this "Expressions", most people watching probably have to idea this is a user-defined type. This all makes more sense once you've seen the toy language DSL before. a "Lit" and a "Var" are overloaded terms already used in programming languages, and re-using those terms for user-defined types makes it more confusing for those not familir with DSLs of this kind.
A new LTS is cut every 3-6 months, [per the FAQ](https://github.com/fpco/lts-haskell#readme). LTS 9.0 was released almost exactly 3 months ago, so we should expect to see a new LTS in the next three months.
Nixpkgs has a large base of contributors and is the one of the most active repositories on all of GitHub
I agree itâs unfortunate that Haskell has no turnkey IDE solution. Iâve never really been a person who makes much use of IDEs, preferring Emacs &amp; Vim, so I donât miss what I never grew to rely on, but I understand thatâs not acceptable for many people. For example, if I need to inspect the type of something, Iâll often write a hole (`:: _`) or deliberately wrong type (`:: fnord`) and recompile or reload in GHCi to get the type from the error message. This situation is a real shame, because Haskell is (or at *least* could have been) very well suited to unusually good tooling because of the large degree of static structure available in programs. To be fair, itâs complicated by the size &amp; complexity of the language. As for debugging, my general solution apart from `Debug.Trace` is âavoid the need to do itâ. I try to split things into many small, pure, obvious functions that I can test interactively or with QuickCheck or HSpecânot only avoiding state, but also avoiding data that takes many steps to set up (because if itâs hard then I wonât do it). I agree that GHCâs error messages could use improvement. However I have found that theyâre generally very useful as long as you stay in the realm of pure Haskell. Once you incorporate FFI, the situation becomes much more complex and things can fail for many more reasons. I havenât found a good solution for this. Iâve been considering setting up a repository of âHaskell folk knowledgeâ, the many small things that experienced users have learned through trial and error and asking on /r/haskell, /r/haskellquestions, or #haskell on Freenode. If anyone would be interested in collaborating on that, Iâd welcome comments &amp; messages about it. 
Because, I think, that the video is indeed a bad explanation of what exactly monads are. I barely explains why we need such a generalisation over effects, what benefits monads give over effects that people have in imperative languages. Show people how composition of state and continuation monads give you a coroutine, how non-deterministic monads help you to solve non-deterministic problems (i.e. finding all permutation). These are examples of great explanations (in my opinion): http://binaryanalysisplatform.github.io/bap/api/v1.3.0/Monads.Std.html https://discuss.ocaml.org/t/ann-monads-the-missing-monad-transformers-library/830/6?u=freyr666 https://discuss.ocaml.org/t/can-monads-help-me-my-refactor-code-for-an-enhanced-data-structure/1064/5?u=freyr666
You're correct Lisps are untyped and dynamic (at least to my knowledge? Not sure if any deviated, also may be misinformed) but Scala is close.
Itâs a systemic thing, see the open letter by Dijkstra arguing against Java replacing Haskell for teaching, and John Backus arguing for FP in his laureate speech for the Turing award. Or the paper where Haskell is evaluated for Prototyping, is the language which give the most concise Prototypes and is shrugged off as âtoo cute for its own goodâ (by some people, not the authors). 
First, thanks for your detailed answer. I will go part by part, because I think it is good to talk about those things to find solutions in the process. &gt; I agree itâs unfortunate that Haskell has no turnkey IDE solution. Iâve never really been a person who makes much use of IDEs, preferring Emacs &amp; Vim, so I donât miss what I never grew to rely on, but I understand thatâs not acceptable for many people. People does not want a full IDE, like eg. Eclipse (too heavy, too big, it has too many things that one does not even know about). Maybe they just want that main functionality is there and works well. I use emacs and vscode, and I would like at least core functionality working well without too much trouble, eg. type info tooltips, inline documentation, jump-to-definition, coloring, indentation, inline error reporting, build &amp; run. Do you think those are basic or advanced features? &gt; For example, if I need to inspect the type of something, Iâll often write a hole (:: _) or deliberately wrong type (:: fnord) and recompile or reload in GHCi to get the type from the error message. I use them as well, but this should be better done easily, just by approaching the mouse pointer to the piece of code. Because is something that we could be doing many times. &gt; As for debugging, my general solution apart from Debug.Trace is âavoid the need to do itâ. I try to split things into many small, pure, obvious functions that I can test interactively or with QuickCheck or HSpecânot only avoiding state, but also avoiding data that takes many steps to set up (because if itâs hard then I wonât do it). Sure, it is better to write good code, but sometimes you have analyze big projects you already forgot about or even projects written by other people. Sometimes the problem at hand is just too big by itself, and when there are too many functions, it is good to have tooling to find the culprit. In my specific situation, it was a port from an old C# project. I had to rewrite it keeping the algorithms and structures similar to the original one, which already worked to certain degree. The port did not work at the beginning only because a really small typo, which was a bit hard to find using Debug.Trace instead of step-by-step debugging. &gt; I agree that GHCâs error messages could use improvement. However I have found that theyâre generally very useful as long as you stay in the realm of pure Haskell. Once you incorporate FFI, the situation becomes much more complex and things can fail for many more reasons. I havenât found a good solution for this. I understand FFI is hard. In this case it was an error that affected a 3rd party library, which seems to be solved in GHC 8.2.2. I did not plan to deal with that code, from my pov I would have preferred dealing with that dep as a blackbox. In other project I have changed compilers, framework releases, complete libraries, etc without too much trouble other that fixing my own code. &gt; Iâve been considering setting up a repository of âHaskell folk knowledgeâ, the many small things that experienced users have learned through trial and error and asking on /r/haskell, /r/haskellquestions, or #haskell on Freenode. If anyone would be interested in collaborating on that, Iâd welcome comments &amp; messages about it. This would be really welcomed. Any effort for improving documentation is good.
He could have just use division and pair it with another operation instead of introducing `eval`. But then again, monad is an advanced concept and you can not expect someone to explain it and it just clicks. Itâs like trying to understand multiplication before addition. If you understand monads from any explanation you are likely to know ADTs and how to represent expressions already. Itâs a chicken and egg problem.
Sometimes is not about the code one wants to write. Sometimes is about really little problems related in an indirectly way. If it is not an IDE/editor plug-in which fails, it is some unmaintained library which does not compile anymore, some unimplemented feature / nonexistent library, or some compiler / tooling issue. When planets are aligned and things work ok, better to work fast and to not leave the pc alone, because something could fail after going to sleep or after going to drink a cup of tea/coffee for a few minutes xD Death by thousand paper cuts
No this is actually the incorrect order in a non-strict language because evaluation happens outside-in. In a strict language, forward piping makes sense because evaluation is done inside out, and thus forward piping lines up with evaluation order; however, it does not in a non-strict language.
Here's some statistics for fun... GHC Version | First LTS | GHC Lag | Last LTS | LTS Lifespan | Last Update ------------|-----------|----------------:|----------|-------------:|-------------: GHC 7.8.3 ^(*[2014-07-11]*) | LTS 0.0 ^(*[2014-12-12]*) | **154d** | LTS 0.7 ^(*[2014-12-28]*) | **16 days** | 1063d ago GHC 7.8.4 ^(*[2014-12-23]*) | LTS 1.0 ^(*[2015-01-11]*) | **19d** | LTS 1.15 ^(*[2015-03-29]*) | **77 days** | 972d ago GHC 7.8.4 ^(*[2014-12-23]*) | LTS 2.0 ^(*[2015-04-02]*) | **100d** | LTS 2.22 ^(*[2015-08-09]*) | **129 days** | 839d ago GHC 7.10.1 ^(*[2015-03-27]*) | | **â¥ 974d** | | | GHC 7.10.2 ^(*[2015-07-29]*) | LTS 3.0 ^(*[2015-08-12]*) | **14d** | LTS 3.22 ^(*[2016-01-10]*) | **151 days** | 685d ago GHC 7.10.3 ^(*[2015-12-08]*) | LTS 4.0 ^(*[2016-01-06]*) | **29d** | LTS 4.2 ^(*[2016-01-18]*) | **12 days** | 677d ago GHC 7.10.3 ^(*[2015-12-08]*) | LTS 5.0 ^(*[2016-01-26]*) | **49d** | LTS 5.18 ^(*[2016-05-23]*) | **118 days** | 551d ago GHC 7.10.3 ^(*[2015-12-08]*) | LTS 6.0 ^(*[2016-05-25]*) | **169d** | LTS 6.35 ^(*[2017-06-05]*) | **376 days** | 173d ago GHC 8.0.1 ^(*[2016-05-21]*) | LTS 7.0 ^(*[2016-09-14]*) | **116d** | LTS 7.24 ^(*[2017-05-28]*) | **256 days** | 181d ago GHC 8.0.2 ^(*[2017-01-11]*) | LTS 8.0 ^(*[2017-02-12]*) | **32d** | LTS 8.24 ^(*[2017-07-27]*) | **165 days** | 121d ago GHC 8.0.2 ^(*[2017-01-11]*) | LTS 9.0 ^(*[2017-07-26]*) | **196d** | LTS 9.14 ^(*[2017-11-18]*) | **115 days** | 7d ago GHC 8.2.1 ^(*[2017-07-22]*) | | **â¥ 126d** | | | GHC 8.2.2 ^(*[2017-11-20]*) | | **â¥ 5d** | | | 
"Haskell is cancer."
Timely security updates for every package in a Linux distribution of a couple of thousand packages for the usual 3-5 years from release of a compatible version (usually major or minor version, whatever allows you to keep using the same plugins and/or config files unchanged) you want on a production system is something none but the largest distributions achieve
Because most programmers are neither software *engineers* (legally protected term where I live), nor computer scientists. 
&gt;Most dev's don't write DLS grammars WELL THEY FUKKEN SHOULD
In one word, entertainment.
That's why I tried to qualify what I wrote, I definitely wasn't referring to all self-educated people. &gt; I hope thatâs not true in the large, meaning, those folks who are self-educated think that education itself is a wasted effort. Many people, no matter how they were educated, have a tendency to think that what they've already learned should be sufficient. So they instinctively look for ways to discount other knowledge. It's not that education is a wasted effort, but rather education without a purpose, where "purpose" is commonly interpreted as being useful in day to day work. There's also a cost to learning new things, so the calculation of how some knowledge might help in ordinary work is a valid one. Realistically, most imperative language programmers aren't wrong to conclude that learning about monads probably won't help them much in their usual software environment. The Youtube comments show people rationalizing this to themselves and looking for validation. &gt; one could argue that none of it is based on natural principles past the EE stuff I disagree with that - all current programming languages can be given a purely mathematical semantics, which means the job of a programmer is to develop a mathematical model that implements the behavior they want. It's just that for the most part, the languages they use to express those mathematical models are not traditionally mathematical. That's a bit tangential to the discussion, although it could have big implications for the longer-term future of software development. 
This is a LLVM based compiler project...
It's hard to give you any advice without understanding what specifically trips you. Where do feel stuck? What specific kind of things are you trying to do and can't?
Dank meme
&gt; I hope thatâs not true in the large, meaning, those folks who are self-educated think that education itself is a wasted effort. Invert the causality here and you will see why statistics would favor this: do you expect people who think that education is a wasted effort to invest on formal education? You just can't generalize a difference in ratios into a certainty over a population. The GP is also wrong on generalizing a large absolute number into a large ratio, although he has am hypothesis that is probably correct.
Proliferates uncontrollably as an organism matures?
No. You can watch the Computerphile videos on path finding or AI safety or edge detection or SQL injection and gain an appreciation for (a) algorithmic approaches to real world problems (b) the open questions in a deep field (c) the way in which design errors have widespread implications for end users. Not to mention all the ones on Turing machines, error detection and correction etc. Videos that I would gladly recommend to my non CS family. I can't even work out who the target market for this video is. It's someone who doesn't know Haskell but understands the utility of an expression tree, as well as types and data structures. That sounds like it should be a programming tutorial.
I suppose itâs a tragedy of the human condition to have that mindset; to lack, or at least fail to act upon, a lifelong passion for learning. I debated putting the part about software development not being natural in because I thought it could get philosophical, but it basically boils down to whether you believe math is discovered or invented. I think itâs invented so I stand by what I wrote but I can appreciate your viewpoint. I do believe this upswing in formal verification is the start of something bigger for the industry. It will be interesting to see how it shakes out, especially approaches like LiquidHaskell.
YouTube has a shit ton of infotainment content for some reason. I wonder if the idea is also to publicize computer science as a field to encourage high school kids to go into the field as well.
By that do you mean that you are writing a compiler that outputs LLVM? Because if so then my statement stands. If you are working on LLVM itself or something then I'm not sure anymore. 
This is just a really bad explanation, which is very surprising coming from Graham Hutton. It's very unfortunate that they chose to do this and to do it like this, since this will just be reinforcing people's ideas that you need a degree in CT to understand Haskell.
&gt; I anticipate that stackage will probably jump straight to a GHC 8.2.2 resolver for LTS. We will. Nightly builds are treated as incremental preparations for the next LTS series, so the fact that we have moved nightlies to ghc-8.2.2 means that the next LTS series will follow suit.
I described some of the reasons that LTS Haskell delays adopting the latest GHC in a blog post back when ghc 8.0.1 was released. https://unknownparallel.wordpress.com/2016/05/22/stackage-lts-and-ghc-8-0/ Two things we as a community could do to help speed things up: - periodic calendar releases of ghc. If these were more consistent and predictable, then we could line up LTS releases to match. - upgrade libraries prior to a new ghc release by testing them against RC releases.
I think some people are misinterpreting what I said. I'm not against Computerphile. I love the channel. I'm questioning the choice of subject when they try to do things like explain monads. It's a topic that even programmers with years of experience can fail to understand (see the YT comments...). The idea that it could be done in a 20 minute video aimed at a lay audience is hilariously misguided. Computerphile has some excellent videos, but this is not one of them. 
Horizontal scaling out is also great, we can scale out our read model for one service to 30 nodes and keep our write model at 10 nodes, and vary this for each service depending on demand.
I ignored your warning, and I deeply regret it. The amount of close-minded stupidity in that comment section is just incredible. All these "I am a 30 year tech vet and this is just useless shit because I can't understand it" ... I weep for the industry.
I found the following command to list all the dynamics libs that the executable depends on ( on Mac OS X) `otool -L exec-file`, for the commands related to linux and windows [here](http://gelisam.blogspot.co.uk/2014/12/how-to-package-up-binaries-for.html). a side question if I may, while building with stack I can see several file as the below : * greet * folderName-exe * intero * aeson-pretty * wrap as I understood that the project-exe is the executable file, which is to be moved to the server or do I need to move the other files as well!
Link to the plugin?
You'll find it here https://github.com/owickstrom/neovim-ghci :)
I am struggling to understand the motivation behind this Haskell Platform behavior: ``` Prelude&gt; import System.IO Prelude System.IO&gt; openFile "/tmp/junk" WriteMode {handle: /tmp/junk} Prelude System.IO&gt; openFile "/tmp/junk" WriteMode *** Exception: /tmp/junk: openFile: resource busy (file is locked) ``` However, this is OK: ``` Prelude&gt; import System.IO Prelude System.IO&gt; openFile "/tmp/junk" ReadMode {handle: /tmp/junk} Prelude System.IO&gt; openFile "/tmp/junk" ReadMode {handle: /tmp/junk} ``` This behavior is proscribed by the [Haskell 2010 Language Report](https://www.haskell.org/onlinereport/haskell2010/haskellch41.html#x49-32800041.3.4) and implemented in [ghc](https://github.com/ghc/ghc/blob/master/libraries/base/GHC/IO/FD.hs#L255) but I cannot find any rationale for this feature. This locking mechanism forced us to use platform-specific file APIs to allow our multi-threaded application to open, read, and write the same file with out-of-band locking. Is this design required due to lazy IO which could cause writes to interleave unpredictably? If so, why is there not a cross-platform variant of (perhaps strict) System.IO which could eliminate the locking? 
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [ghc/ghc/.../**FD.hs#L255** (master â a1950e6)](https://github.com/ghc/ghc/blob/a1950e6dc03b560105903ad44050b1570f3bc24f/libraries/base/GHC/IO/FD.hs#L255) ---- ^(Shoot me a PM if you think I'm doing something wrong.)
They do this for the last two major releases. See the continued commits for the second to last release, for example: https://github.com/NixOS/nixpkgs/commits/release-17.03
You need to be more specific
Can't really answer your question, but in order not to scare people off who might, could you please indent your code 4 spaces for proper formatting?
According to [this mail](https://nixos.org/nix-dev/2017-March/023016.html) on the nix-dev list they support one release, with releases apparently appearing every 6 months or so. This is very far from the security support you get on Debian, Ubuntu LTS, RHEL or Centos. As you can see e.g. [on the Debian Security page](https://www.debian.org/security/) you get updates to many more packages than the few core packages updated in the commits you mentioned over the last month too.
Also I think Windows is a major platform where this doesn't work at all ([As this .Net docs suggest](https://msdn.microsoft.com/de-de/library/y973b725(v=vs.110).aspx)). Does the Unix world allow opening the same file twice as writable? That's news to me.
&gt; anti-intellectualism I think people are reacting to the fairly poor explanation of what monads really are and what they're used for. In fact, many of the comments are just trying to explain or grasp the concept. ...and then you have the "Haskell is cancer" comment that has a stupid amount of upvotes. Thanks for making the downvote button less effective, youtube.
Specialized notation for specific monads isn't necessarily a bad thing, in my opinion.
Thats rather interesting. Thanks for providing the link. I've also found https://en.wikipedia.org/wiki/Argument_map following your lead and it seems to be just the topic I was interested in.
**Argument map** In informal logic and philosophy, an argument map or argument diagram is a visual representation of the structure of an argument. An argument map typically includes the key components of the argument, traditionally called the conclusion and the premises, also called contention and reasons. Argument maps can also show co-premises, objections, counterarguments, rebuttals, and lemmas. There are different styles of argument map but they are often functionally equivalent and represent an argument's individual claims and the relationships between them. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/haskell/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^| [^Donate](https://www.reddit.com/r/WikiTextBot/wiki/donate) ^] ^Downvote ^to ^remove ^| ^v0.28
Honestly I don't care about speeding this up. To me the whole point of LTS is stability and boringness. GHC has advanced to the point where I can wait several months for the new stuff, as the existing stuff does the job very well. If I want the latest, I can always use nightly. So if you want to speed this up, of course that's totally cool with me, but I'm just saying don't let the louder voices of people who say "latest, latest NOW!" outweigh the much quieter people who are just fine using old stuff in an LTS because that's kind of the point of LTS. It's just that people who don't care about new, shiny stuff aren't so vocal about the fact that they don't care about it. 
I'm very new to Haskell. Maybe 1 month in. I have experience with other languages. I've gone through about half of the Real World Haskell. I stopped Learn You a Haskell because I found Real World Haskell to be more my style. &amp;nbsp; How do you write more performant and clever Haskell? I've been doing some coding on a site that advertises wars (I will be posting a solution so I won't name the site). I wrote an implementation of a Reverse Polish calculator here: [Naive Reverse Polish](https://pastebin.com/hrkNq7yr). The leading solution is here: [Clever Reverse Polish](https://pastebin.com/f7sNu5cN). &amp;nbsp; So how do I get from my naive implementation to the clever implementation? I tried to break the clever implementation down by starting at line 14 and working from right to left but I get an error when trying to run map polish . words If I can't break these things up to see what they are doing, what should I be doing instead to getter a better handle on the language?
What are you arguing against here? That computerphile makes videos because people watch them or that people only watch them as part of a larger research effort to learn specific topics? Something else?
That's not me at all. I like programming because I love intellectualism, but hate school. I'd rather learn on my own at the time when I'm most interested in a subject. Stuff like Haskell is great for people like me because it gives me a reason and way to learn about stuff like monads without driving to a lecture hall where the professor doesn't say anything that isn't already in the textbook we were supposed to read before class and thus already know. I suspect the anti-intellectualism is actually from the educated: they don't see how this will make them money or get them an A on a test. They're not interested in learning, they're interested in a degree and a paycheck.
Haskell has it for [], for example.
Isn't a monad just [smashing boxes together?] (https://youtu.be/MvQxNm5gn8g?t=28m15s) ^I ^do ^apologize. â&amp;nbsp;Daniela Sfregola, A Pragmatic Introduction to Category Theory (which is lacking cats)
Monads is something very simple, but It is presented by bad teachers more interested in other carreristic things but teaching. Monads are the in order to chain function that return computations ( take a computation as an executable procedure). to chain one to the next, you use bind. To generate one of these executable procedures from a pure value, you use return. That's all. Develop this argument and a programmer will understand it. But usually the mathskellers try to get away programmers from programming, using examples of burritos os category theory. it is like explaining fluid dynamics to aeronautical engineers using the example of submarines. The reaction is not antiintelectualism, it is the reaction against a intelectual dishonesty in some way.
Awesome, was thinking of implementing this myself after writing a [ghcid plugin](https://github.com/ndmitchell/ghcid/tree/master/plugins/nvim) for neovim. I wonder how well they play together.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [ndmitchell/ghcid/.../**nvim** (master â 6380d7d)](https://github.com/ndmitchell/ghcid/tree/6380d7d7ff8c01fd2b30f828720214529687cf4f/plugins/nvim) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dqbnxoe.)^.
The question is more what are you arguing? I've been quite clear.
&gt; I think one big reason is that there are so many self-educated people in ~~software~~ **functional programming**. If programming is taught in an institutionalized way, it is taught in imperative languages. Making the leap from the internalized imperative to functional paradigm on your own might seem daunting, especially considering the apparently popular mathematical approach of teaching functional programming.
I would like to disagree. Math is a fundamental and universal language of science. Monad is a mathematical concept (so are almost all programming notions). And there is nothing wrong in presenting it in its original mathematical flavor. I would even go as far as to say that the categorical explanation of a monad is a far superior (both for understanding an practice) mode of instruction than the watered down 'programmatic notion' (whatever that is). To think that programmers lack the ability to understand the categorical concepts comes off as a bit patronizing to me. Don't get me wrong, I'm not against a 'simpler' programmatic explanation. That could go along with the mathematical explanation. These are not mutually exclusive after all. But that kind of simplicity hides the rich beauty of the categorical connections. And once you put in the additional effort to understand the deeper mathematical roots of the concept, you'd appreciate it all the more. Learning the underlying math behind monads opens a lot of new horizons and interconnected concepts to you. To put it in yourfluid dynamics analogy: the programmatic explanation is like learning about Bernoulli's equation, you could only ever use that equation for that particular problem and are not any more wiser because of it; whereas the categorical explanation of monads is like teaching about the Navier-Stokes equations, you could use it to solve any fluid dynamic problem and you'll have much deeper understanding of the flow (Once you know it, you know what simplifications and assumptions need to be made to get to the Bernoulli's equation i.e., the 'simpler' explanation of monads). PS: I am a fluid dynamicist (CFD engineer) who has a deep interest in how programming works (both as a language implementation and the deeper mathematical underpinnings). I had to learn (a bit of) category theory when I first discovered Haskell. I much prefer the rigorous math explanations to 'beginner-friendly programmatic explanations'. Just my 2 cents. 
I originally thought he was going to do the abstract algebra motivation, where `m x` is the type of expressions of a given kind with literals drawn from the type `x`, an algebra for the monad evaluates an expression to give a literal, and the monad laws basically say that order of evaluation is irrelevant.
I usually don't get to the point of a blog post for any simple trick I learn. When I do consider it, I often wonder whether to post to reddit, create blog post, or to add to github wiki for relevant project. I think posting to a projects GitHub wiki has the best chance of your tips being carried forward, as the project maintainer might fold it into an example, a test, or the projects documentation.
Just want to give my opinion (as someone who didn't have any category theory knowledge before I started to learn Haskell) on the various modes of teaching monads. I disagree with many who argue for a 'simpler programmer-friendly' explanation of monads compared to a more rigorous mathematical explanation. Math is a fundamental and universal language of science. Monad is a mathematical concept (so are almost all programming notions). And there is nothing wrong in presenting it in its original mathematical flavor. I would even go as far as to say that the categorical explanation of a monad is a far superior (both for understanding an practice) mode of instruction than the watered down 'programmatic notion' (whatever that is). To think that programmers lack the ability to understand the categorical concepts comes off as a bit patronizing to me. Don't get me wrong, I'm not against a 'simpler' programmatic explanation. That could go along with the mathematical explanation. These are not mutually exclusive after all. But that kind of simplicity hides the rich beauty of the categorical connections. And once you put in the additional effort to understand the deeper mathematical roots of the concept, you'd appreciate it all the more. Learning the underlying math behind monads opens a lot of new horizons and interconnected concepts to you. To put it in yourfluid dynamics analogy: the programmatic explanation is like learning about Bernoulli's equation, you could only ever use that equation for that particular problem and are not any more wiser because of it; whereas the categorical explanation of monads is like teaching about the Navier-Stokes equations, you could use it to solve any fluid dynamic problem and you'll have much deeper understanding of the flow (Once you know it, you know what simplifications and assumptions need to be made to get to the Bernoulli's equation i.e., the 'simpler' explanation of monads). PS: I am a fluid dynamicist (CFD engineer) who has a deep interest in how programming works (both as a language implementation and the deeper mathematical underpinnings). I had to learn (a bit of) category theory when I first discovered Haskell. I much prefer the rigorous math explanations to 'beginner-friendly programmatic explanations'. Just my 2 cents
&gt; How do you write more performant [...] Haskell? Haskell performance is a tricky topic, I wouldn't worry about it until much, much later, when you're comfortable with all the other weird parts of the language. &gt; I get an error when trying to run &gt; &gt; map polish . words The error I get when trying to run that in `ghci` is: &gt; map polish . words error: â¢ No instance for (Show (String -&gt; [Polish Double])) That is, `map polish . words` is a function, which cannot be printed in Haskell. In other languages you'd get something unhelpful at runtime such as `&lt;#function136&gt;`, in Haskell you get a compile-time error. In order to play with that function in ghci, you need to pass it arguments: &gt; (map polish . words) "1 2 +" error: â¢ No instance for (Show (Polish Double)) Unfortunately that doesn't work either because the author of that solution did not derive a `Show` instance for the type `Polish`. Again, in other languages you'd get something unhelpful at runtime such as `&lt;#Polish543&gt;`, in Haskell you get a compile-time error. One way to get around this is to use `ghci`'s `:force` command to only display the parts of a value which can be displayed: &gt; let r = (map polish . words) "1 2 +" &gt; :force r r = [Number 1.0,Number 2.0,Operator _] Ah! Now I understand why the author didn't provide a `Show` instance for `Polish`: the `Operator` constructor holds a function, which cannot be printed, so neither can `Polish`. You can pattern-match on `r` to get that function and confirm that it behaves like `(+)`: &gt; let [_, _, Operator f ] = r &gt; f 10 4 14.0 &gt; How do you write more [...] clever Haskell? By writing a lot of non-clever code first! Only then can you notice the patterns in the bits of code which are repetitive and abstract over them. Or maybe you notice the repetition but don't know how to abstract over them, and then one day you read a blog posts explaining some Haskell technique and you realize that it's a perfect solution for this particular problem and now you have a much better understanding of that blog post than if you hadn't tried to write the code the non-clever way first. In this particular case, the clever solution makes use of three important Haskell idioms: first-class functions, point-free composition, and using precise types to separate the code into parsing, transformation, and pretty-printing phases. This three-phase approach is described in the blog post [The Structure Of Programs in Haskell](https://medium.com/@jonathangfischoff/the-structure-of-programs-in-haskell-a54f5a0d703c). The basic idea is that if you first parse your possibly ill-formed string-based input into a much more robust representation, one which [makes illegal inputs unrepresentable](https://fsharpforfunandprofit.com/posts/designing-with-types-making-illegal-states-unrepresentable/), then the transformation step will be simpler because it won't have to worry about those illegal inputs. One key trick used by the author when choosing this precise representation is making use of the fact that Haskell functions are first-class. Googling for "haskell first-class functions" yields a large number of articles on the subject, but in this case the only part which matters is that functions can be put inside data structures. Here, `r` is a list of `Polish Double` values, whose `Operator` constructor holds a function. This allows the evaluation code to handle all four operators at once: eval (a:b:acc) (Operator op) = (op b a) : acc Instead of having to repeat the same logic four times like you did: eval oper nums | oper == "+" = (read (nums !! 0) :: Double) + (read (nums !! 1) :: Double) | oper == "-" = (read (nums !! 0) :: Double) - (read (nums !! 1) :: Double) | oper == "*" = (read (nums !! 0) :: Double) * (read (nums !! 1) :: Double) | oper == "/" = (read (nums !! 0) :: Double) / (read (nums !! 1) :: Double) Finally, the top-level function `calc` joins the three phases together: calc = head . foldl eval [0] . map polish . words `map polish . words` parses the input string into a `[Polish Double]`, `foldl eval [0]` evaluates it to a `[Double]`, and finally `head` extract the answer from the top of that stack. Again, googling for "haskell point-free" yields many hits, but the main idea is that code can be shorter and more readable when expressed as the composition of many smaller transformations instead of as a single larger transformation. Of course, it's only more readable if you're familiar enough with the smaller transformations and with the idiom itself!
Cool. There is some overlap it seems, but maybe not an issue.
I completely agree with /u/aurabhnanda here. To co-opt the famous Knuth quote...premature "microservice"izing is the root of all evil. There's absolutely no reason to split things up until you have a clear performance problem and doing so will help you scale. I've seen multiple situations where building things in microservices has a very significant cost and no real benefit.
the comment might be late a bit, does this static linking means that the option passed (step 1 ) statically link the C library as well so that way I do not need to install anything on the server and the executable will just work! 
&gt; If you wrote all of your services in Haskell though and built their API's using Servant (and imported the API type definitions) I think you could recover most of the benefit. Don't even use the Servant APIs. Just use function calls! No network overhead, need for serialization code, or chance for API incompatibility.
&gt; Happens more frequently in Servant because you're more narrowly focused. Well, I wouldn't say that we have a narrow focus. The lack of solutions for some things doesn't mean that we're not interested in them. But since we had to figure out a whole bunch of things to make our "API type" approach work, some very common or less common problems have indeed been left without a solution for a while. But the region of the design space that servant covers is very much disjoint from those covered by other web frameworks. The beaten path would be to just go with one of those options if the benefits of using servant are inferior to the drawbacks for a given use case/project. &gt; Is a lot more punishing to overcome in Servant, in part because of how much leverage you're squeezing out of the types. I absolutely agree here. But we're here to help and have spent a lot of time doing so. If someone doesn't have the time and/or interest needed to learn how to extend/tweak servant to bend it to their needs, there is absolutely nothing wrong with picking another web framework! One that just offers solutions out of the box for all those things that you care about for your project. &gt; I don't appreciate the implication that I'm lying when I'm speaking to direct experience with the framework in personal and work projects. I'm not interested in criticizing you in any way, calling you a liar or anything like that. But what you said about JWT is not true. That's all I wanted to say. There's nothing more to it. &gt; You may not like it, but past attempts at web frameworks had wider ranges of use-cases anticipated and covered and/or were less punishing when you did wander outside those anticipated use-cases. That's true of Yesod, Snap, Happstack, and the long-tail of "micro" frameworks like Scotty and Spock. Oh I don't have any problem with this. I even love it. Given that people already had many many options for writing web applications, we have been able to afford not to have out of the box solutions for various things, as it sometimes takes time for our brains to find a way to solve some common or less common problems in a way that fits with the whole "API as a type" story, which is a different enough setting to make that non trivial. But the very extensible nature of the library made it easy for people to add some feature locally in their project, or overcome an a limitation and what not. This has been very well received, and exploited by all the people who _had the time_ to learn their way around the library and solve previously unsolved problems, either locally or by contributing upstream/releasing a companion library. And it's okay for all the others to just use another library. I don't have any problem with that, absolutely none. &gt; Most people who would be capable of fixing something like that themselves don't need suggestions on what web stack to use in Haskell. You need to realize what a risk it is for people who are not quite adept at Haskell to use Servant for a commercial project right now. Oh I do realize, trust me, and we're honest with people when they ask us questions about various aspects of servant and whether it has a solution for X or Y and what not. And it's perfectly fine for people to decide not to use servant, they must act in their project's best interest. We're definitely not forcing servant on anyone. We even hardly do any "marketing". But it sounds to me like your evaluation of servant and how good a fit it was for your project given all the constraints might have been a little ambitious and you might have been better off using a more familiar, common alternative like Yesod, with which you are quite familiar if I'm not mistaken, and which didn't have the problems that servant had as far as your project was concerned. But should we be blamed for this? I get the impression that you think we should. I might be wrong. That however doesn't mean that I won't answer people's questions when they're curious about servant, or that I will myself discourage anyone from learning it, Haskell beginners or experts. In fact, many Haskell beginners managed to use servant and write actual applications with it. That's not exactly the typical servant user I agree, but it shows that servant is not entirely out of reach for them, at least for the most motivated ones. I certainly did not expect that when we initially released it, and it was a very, very pleasant surprise!
Thank you for the excellent answer! I think I see what's going on. So, `Polish` is an algebraic data type with type variable `t` containing two constructors and `map polish . words` creates a list of `Polish` of type variable `t` while the function `polish` always outputs a `Polish` of type `Double`. I also had a disconnect with how `eval` worked but I needed to look into how `foldl` worked first to see that the first argument to `eval` is the accumulated value and the second is the item `foldl` is on. I'm now able to play around with the various pieces. Again, I appreciate the time you spent to answer my questions.
I'm not sure what 'C library' you're referring to, but yes that's the idea of static linking. It's possible to statically link somethings and dynamically link others, but in general a "statically linked binary" refers to one where everything (as much as possible) has been statically linked and therefore there are no dependencies to bring along with the executable file.
&gt; `map polish . words` creates a list of Polish of type variable `t` while the function `polish` always outputs a `Polish` of type `Double`. Almost: since `polish` always outputs a `Polish Double`, `map polish` always outputs a `[Polish Double]` and so does `map polish . words`.
No. If youâre struggling with Haskell donât give yourself an extra challenge by using Purescript or GHCJS. Since youâre at home with the lisps just now cljs for the frontend. Take smaller steps and build on from there. 
&gt; Don't even use the Servant APIs. Just use function calls! No network overhead, need for serialization code, or chance for API incompatibility. In some situations the network + serialisation overhead may be negligible compared to the cost of executing the function. A slightly contrived example would be that you may have a _particular_ part of your application which can be accelerated using a GPU/FPGA. I agree that this is probably uncommon, but I was trying to suggest a solution to the type checker issue *given that* you had a genuine need for a micro service architecture. 
That's a flaw in my view, but it can be generalized to all monads if I'm remembering correctly, so it isn't as bad.
While I would not ever defend anti-intellectualism, and I heavily do feel programming as a trade could benefit from less tolerance towards it, I feel that a lot of what you read here are valid concerns over some of these notions. Don't get me wrong. Category theory is important. It is indispensable to some fields of mathematics, especially those involving geometry, topology, or logic. But category theory is far, far divorced from the concerns of a software developer. And the Haskell community (at least here on reddit) is not very honest with itself on that point. 
Hutton wielding Hutton's Razor is a pretty good meme.
Changes to endpoints not used by your application should have no effect. Changes to data types and endpoints used will mean your application will no longer compile - but shouldn't this be the case? Taking this further, as part of your CI you could check that all services which depend on your API still compile, which could let you catch breaking changes before deployment.
Many engineers that program use mathematics for their domain problems. They don't have to learn a computer science to do their stuff. Many of their maths are much more complicated than category theory and lambda calculus. Do they want you to learn general relativity when you use the GPS of your phone to navigate the city? Do they demand you to learn structural calculus before entering in your house? Do they demand you to learn economy before using their accounting applications? It should be good that you know such things but you donÂ´t have to. Neither they havo to know your stuff to program their programs. There are other things in life and life is short.
&gt; the Haskell community (at least here on reddit) is not very honest with itself on that point. The fact that Hask isn't a category and we don't have proper products because products are lifted and functors are just endofunctors in the not-category Hask is something at least I acknowledge. It is probably one of the biggest flaws with Haskell. However, [fast and loose reasoning being morally correct](http://www.cs.ox.ac.uk/jeremy.gibbons/publications/fast+loose.pdf) makes it useful to think in these terms. Having some category-theory-inspired abstractions supports lens. In platonic Hask, lens form a category. This structure is still useful to think about even if the language we translate to is only an approximation similar to how trying to work with pure functions and immutable data makes it easier to reason in an impure language. &gt; But category theory is far, far divorced from the concerns of a software developer. While it might be true that a lot of category theory isn't very useful for software development. It seems to me that some subset of it is useful. Why would you say you disagree?
I always start with a minimal program with the right sort of event loop (interactive, server, etc) and just build things out step by step. It doesn't feel particularly different than any other language in that way to me, since that's how I approach it anywhere else too.
Mostly, learning what functions exist and how they're used. Reading existing code is probably a good way to do this but that's not the easiest thing either, given I might not understand it.
Knowing relativity to use GPS and knowing structural mechanics to live in a house are not appropriate analogies to knowing monads. A more apt analogy would be a [black box](https://en.wikipedia.org/wiki/Black_box) tool like pandoc that is built out of such technologies (in this case, monads).To use pandoc, you don't need to know what monads are, similar to how you'd use GPS without knowing relativity/orbital mechanics/electronics or whatever. On the other hand, if you want to hack/tinker with your GPS machine, you do need to know how the internal components work. The same is true for Monads. 
**Black box** In science, computing, and engineering, a black box is a device, system or object which can be viewed in terms of its inputs and outputs (or transfer characteristics), without any knowledge of its internal workings. Its implementation is "opaque" (black). Almost anything might be referred to as a black box: a transistor, an algorithm, or the human brain. To analyse something, as an open system, with a typical "black box approach", only the behavior of the stimulus/response will be accounted for, to infer the (unknown) box. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/haskell/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^| [^Donate](https://www.reddit.com/r/WikiTextBot/wiki/donate) ^] ^Downvote ^to ^remove ^| ^v0.28
Existing code is all on hackages: http://hackage.haskell.org/packages/ You can click through to a package and browse the modules (what you're looking at are called "haddocks"). For instance, here we're browsing the `base` package (this is the "batteries included" set of libraries that ships with GHC): https://hackage.haskell.org/package/base And now we cal click through to the module named `Prelude` to look at the functions it exports, and their docs: https://hackage.haskell.org/package/base-4.10.0.0/docs/Prelude.html The prelude is the library that is loaded by default when you start `ghci` for instance, so its functions and types will be in scope already. From here you can click the "# Source" links near any of the function (sometimes you can't for reasons that are not really clear to me) and check out the source code, e.g. here we can look at the `maybe` function: https://hackage.haskell.org/package/base-4.10.0.0/docs/src/Data.Maybe.html#maybe Sometimes, as in this module there are hyperlinked identifiers in the html source so you can jump around. Finally you can use hoogle to look up functions by name or type (at least for things in `base`)
Perhaps, your understanding of FP concepts, category theory, and Haskell is still at a superficial level. And now that you have some of the basics down, the only way to push on is to write applications and release them into production or into the wild (open source). You can only really understand things by doing.
Are you using stack? It seems to have solved all the tooling issues, at least for me. I use it with spacemacs' haskell layer.
I have a couple of questions. I was recently asking about which relevant languages are the most concise, and someone recently said Haskell beats Clojure when it comes to conciseness. Do you guys agree with this. I was also wondering about how Ocaml/ReasonMLâs type system compares to Haskell. (Also expressive, with ADTâs?) Although, itâs hard to picture anyone here saying that the former is anywhere near as good. 
Do you think that to use monads you need to know category theory or not?
&gt; I think one big reason is that there are so many self-educated people in software ~~functional~~ programming. Fixed that back for you. In all honesty though, in *my* experience, there are WAY more people learning programming on their own through an imperative language - look at all the places you can learn Python, Ruby, JavaScript, PHP etc. The "self-learned" FP'ers usually don't learn it as their first language.
In the general sense, fast and loose reasoning is what software development is all about. I know the paper is more technical than that, but you can apply the same informal arguments about general software development. "Do you really need a 'correct' interpretation of a program if it 'works good enough'?" I wish I knew more about lenses than I currently do. But think for a second what they look like to an average software developer. "First class getters and setters" is the typical elevator pitch. But that doesn't seem very interesting. Getters and setters are largely symptoms of a disease the Java language transmitted to software development world. And I think it's unclear at first thought what advantage having them be first class would be. Lenses solve a problem that isn't a problem (or isn't seen as a problem) for the average software developer. &gt; It seems to me that some subset of it is useful. Why would you say you disagree? I'd rather software developers not have to burden themselves with category theory at all unless it interests them (like it does me). The real value of functional programming is in more mundane things: immutable data and transformations over mutations, explicit parameter passing over side-channel communication, and the notion of an evaluator to give denotation to data structures. Category theory is an excellent tool for studying programming languages. There's a reason that when writing an evaluator, we often see things like `eval (Add x y) = eval x + eval y`. And that is because of the relationship between functoriality and semantics. But I'm skeptical about the direct value of a programmer. I'm interested to see how Elm fairs, since it takes a similarly conservative approach. I'm not 100% in agreement with all of Evan's design choices, but I think it's much closer than Haskell to a functional language for the masses.
Sure, fundamentally, `Set a` cannot be isomorphic to `Set (Set a)` because the nested set can be empty. Let us consider a relational example: "show me all customers with their addresses" implying that each customer can have multiple addresses. If customer "Steve" does not have any address, the only way to "flatten" the relation is to exclude "Steve" from the flattened relation because his relation-valued-attribute of "address" has zero tuples. For the transformation to be isomorphic, we would need to be able to get back to the original nested representation which we obviously can't because "Steve" is missing from the flattened representation. You may think that this looks like an SQL OUTER JOIN and you would be correct. But even in SQL, the NULL is [completely ambiguous](https://github.com/agentm/project-m36/blob/master/docs/on_null.markdown#outer-join-ambiguity). The NULL could mean that there is indeed no data on the "outer" table to match *or* the NULL could actually be the NULL from the "outer" table's tuple- oops. 
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [agentm/project-m36/.../**on_null.markdown#outer-join-ambiguity** (master â 4b4b595)](https://github.com/agentm/project-m36/blob/4b4b595facc407bcb8f760ae9ba0b1d8fb2db8a9/docs/on_null.markdown#outer-join-ambiguity) ---- ^(Shoot me a PM if you think I'm doing something wrong.)
I feel like a lot of people don't know about [this video](https://youtu.be/ZhuHCtR3xq8) which is definitely the best I've seen on the concept.
&gt; I think itâs invented We can invent notation - symbols and syntax - and choose sets of axioms and rules, but we can't "invent" or choose the consequences of those choices. We can choose rules that have consequences we want, but there are limits to that when constructing a consistent system. That's why, for example, something like GÃ¶del's incompleteness theorems can tell us unequivocally that certain properties of formal systems are inevitable, and others are impossible. The closely related halting problem is an example of these same inevitable, discovered properties in a computing context. Further, even though we can invent formal systems as described above, it turns out they're not all unique. The [Curry-Howard-Lambek correspondence](https://wiki.haskell.org/Curry-Howard-Lambek_correspondence) is an example of this, in which three independently invented formal systems with different notation and concepts turn out to be fundamentally equivalent. This suggests that these formal systems are modeling something fundamental - a discovery, not an invention. Our *interface* to that discovery is invented, but the interface is merely the tool we use to explore the underlying, discovered truths. The inevitable consequences I mentioned also have consequences for the physical universe - things like the inverse square law, conservation laws, even the countability of objects, etc. are inevitable consequences of these mathematical truths in any universe which has the necessary properties - things like 3D space, differentiable symmetries, discrete objects, etc. Coming back to my original point, when we provide a mathematical semantics for a programming language, we're explicitly connecting the language to these fundamental truths. Properties like Turing completeness - another discovery that we don't have a choice about - allow us to prove that we can do this for any traditional programming language. So whether one recognizes it or not, writing any program involves developing a model which is an incarnation of, dependent on, and constrained by logical and mathematical truths that are demonstrably discovered, not invented. This suggests that having a grasp of those underlying mathematical truths is advantageous. The formal verification you mentioned is one example, although there are many much less rigorous examples. 
The other files are executables produced by various packages. e.g. [aeson-pretty](https://hackage.haskell.org/package/aeson-pretty) is a tool for pretty-printing JSON, and was likely produced because something depended on the library. You can ignore them.
Sure, I am using stack and I know it has improved situation a lot, but various issues out of the scope of stack continue to be there. Anyways, thanks for the suggestion.
I think youâre underestimating how *wildly* unapproachable that is. Just because itâs superior doesnât mean itâs teachable to non-mathematicians, or even relevant. To understand it from the perspective of category theory requires a reasonably large and dense set of prerequisite knowledge. That prerequisite knowledge, although loosely relevant to programming, is *far* from the most approachable and directly useful ways to solve any problems in programming. As much of a fan of category theory as I am, I think itâs kind of foolish the believe itâs the best way to describe or teach programming in general. Itâs given us some great tools in `Functor`, `Monad`, `Monoid`, etc., but even these are vastly more general than those versions which are most useful to programmers (the Haskell type classes). Telling a new programmer that concatenating lists is actually a categorical construct depending on the idea of categorical products and identities vastly over complicates a question of basic computation. Understanding how Haskell utilizes monads is similar. Thereâs the way that new people will actually be willing to learn, and thereâs the âcorrectâ way thatâs much less approachable and isnât appreciably more useful to people who arenât specifically studying the mathematical logic of it all.
Perhaps, if you are recompiling every single service and redeploying all of them simultaneously. But doing that starts to undermine the benefit of microservices. 
Perhaps it's the same point as watching videos about encryption and prime numbers, and then followed by a video on einstein and fast rockets moving near the speed of light. It's scientific trivia.
Does the file already exist? If not, it may be being created with the wrong permissions. Other than that, I have no explanation- except yes, in Unix you *can* open write handles to the same file multiple times. I wrote the following program and ran it on my Linux to prove this: ```#include &lt;stdio.h&gt; #include &lt;sys/types.h&gt; #include &lt;sys/stat.h&gt; #include &lt;fcntl.h&gt; #include &lt;unistd.h&gt; int main (void) { int fd1, fd2; fd1 = open("/tmp/junk", O_CREAT | O_WRONLY, 0600); if (fd1 == -1) { perror("/tmp/junk"); return -1; } printf ("fd1=%d\n", fd1); fd2 = open("/tmp/junk", O_CREAT | O_WRONLY, 0600); if (fd2 == -1) { perror("/tmp/junk"); return -1; } printf ("fd2=%d\n", fd2); close (fd2); close (fd1); return 0; } ```
Cool, Iâve only skimmed but this seems like a good way of dealing with impredicativity, which is one of those things I always felt *should* work, but is very tricky to get right. AsideâI loved this (apparent) typo: &gt; To ac**comonad**ate second-order and impredicative typesâ¦ 
The tool I use the most is a Google search with the word "hackage" in it, such as "hackage web server" Another tool that I've heard works well is Hoogle, but I haven't used it much myself
Hi bgamari, Thanks for the great work. Could you also change the following documents to 8.2.2? https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/
I think Haskell is only more concise than Clojure for small examples. Production Haskell code tends to have long import lists and qualified names due to lack of support for object-oriented method chaining and poor defaults in the standard library. I think the correct reason to adopt Haskell compared to Clojure is it's type system. Specifically: sum types + no nulls + type inference. If you can make invalid states unrepresentable it's a huge developer productivity boon.
You can improve your understanding of Haskell performance using two tools: First, learn how to generate Haskell core (an intermediate representation for the language), using the `-ddump-simpl -dsuppress-all` flags to the compiler. Then learn how to read the generated code and understand the performance implications of the intermediate representation. Second, learn how to profile your code and how to understand the generated profile. Conventional wisdom for how to optimize Haskell code goes stale quickly as the Haskell compiler keeps getting better at optimization with new release. I see a lot of people waste time trying to optimize their without profiling it first, only to discover that their changes had no effect.
Yes, the question is regarding specific Haskell Platform behavior which adds another level of in-process locking over OS-level file semantics.
Conciseness I would agree, at least when talking about actual code, imports and pragmas might even things up a bit. OCaml's type system is pretty cool, but the lack of anything that goes from type to value (typeclasses) is really annoying, and if we are talking pure power then yeah it's type system is much less powerful due to the lack of all the things Haskell's extensions give you. 
I mean if you just use a custom Prelude and take advantage of generic typeclasses then those issues pretty much go away. And you are back to Haskell being more concise. 
As someone who has contributed to a similar effort to port from Ruby to Haskell, my main advice is to do the quickest, dirtiest, first thing that comes to mind to get the Haskell parts into production, then you can interleave refactoring and adding more quick implementations. I hesitated too much early on with trying to both convert and improve in one step. When you get into Haskell and there are all these interesting possibilities and discussions around what abstractions to use, it can be easy to lose sight of just how much worse things are if you keep Python / Ruby / Java / etc in production even compared to a very simplistic implementation in Haskell. People might also be use to thinking that refactoring is hard or complicated, if they haven't used Haskell for a long time.
Can you briefly describe why the formalization "did not work"? Are you saying the operational semantics in that paper are flawed in some way? I was actually trying to implement the negative and fractional types described in this paper in a simple interpreter so it'd be pretty disappointing if there's some sort of show-stopper waiting for me.
Are you thinking of monad comprehensions? 
Obviously not or I wouldn't have asked.
Not a huge advantage to boxed types in a strict language. 
I honestly thought this was going to be about what to do if your colleague gets upset about a segfault.
Sorry I disappoint! :P 
No I think you just can't read. 
may I know whether it did work for you with alpine?
Have you considered [Ale](https://github.com/w0rp/ale) in addition to or instead of Neomake? I had more luck with [Ale](https://github.com/w0rp/ale).
The types are trying to tell you something. I think you are fighting with the types instead of believing what they say. The types say that `yesod-job-queue` does not do anything with exceptions. That means your jobs are fully responsible for any error handling. Any exception not explicitly caught inside the job will result in an asynchronous exception leaking out to the context where the job is run, and it looks like the library is not set up (yet) to do anything useful there. So it is your responsibility to catch all exception that you need to be caught, and then to do something with them. The usual warp+yesod top-level exception handling is not available - which makes sense, since you are running asynchronous jobs. So for example, you may want to get the logger object out of the `App` object, and use that for attempting to log any uncaught exceptions. But be careful not to do that for things like "out of memory" exceptions. In short, the types are telling you that you are on your own. Sorry.
Sounds like a good idea! Not entirely sure if it should be included in `-Wall` though.
Not yet, but I suppose it could be added with conflict. I've discussed with the maintainer of neovim-intero of merging the projects back together, so I'll hold off on anything major until we have settled that.
Microservice architecture is a heavy pill you take in order to cure an extremely rare disease (an extreme need to scale). Now, taking this pill is so crippling that it forces you to embrace a healthy lifestyle. But you don't need to take that pill to live that way. Ä°n fact, Haskell helps you do that out of the box.
The operational semantics are not flawed, in the sense that all operational semantics are fine if they 'work' (i.e. don't get stuck). Witness PHP, Javascript, ... So the programs written in that language, and many successor versions, run just fine. But they don't have the intended semantics. The finite types are supposed to contain all finite sets, and the reversible combinators are supposed to represent permutations. In the version of that paper, the link you refer to clearly shows that the only model will in fact be trivial, and all types collapse to `0`. You will succeed in implementing it. It's just that the end results are not interesting, even though they appear to be. There ought to be a 'nearby' system that behaves properly. We haven't found it yet.
This is indeed a great change that could have saved me a number of headaches back then. I would actually prefer something more succint, like this: DerivingStrategies.hs:18:12: Type error: | 18 | putStrLn False | ^^^^^ â¢ Expected type: String â¢ Actual type: Bool 
The fact you want to warn on `x = ... x ...` not `f x = ... f ...` should be emphasized. Latter is quite common, especially co-recursive case Can't the rule be implemented in e.g. `HLint` first? As it's purely syntactical.
Why do people think `-Wall` shouldn't mean ***ALL***? The problem is not with the warnings, it is with your code. If you're using `-Wall`, then it is perfectly reasonable to expect that you will need to fix your code between releases. If you don't want to do that, then don't use `-Wall`. There should be more subgroupings of warnings for the people who complain about `-Wall` to use instead. But when I use `-Wall`, I want **all** the warnings. Anything less is a lie (which it already is). I think GHC doesn't even have enough default warnings.
I agree. I'm all for as many warnings as possible by default. I'd like a pragma to disable individual warnings for individual functions as well.
+1 for making `-Wall` mean ALL. I would love it if you could make it happen :D
Since Clojure is a lisp, macros can be used to make the syntax much more concise than we ever could in Haskell. Well, kind of: [we have macros too](https://www.reddit.com/r/haskell/comments/61r64w/what_does_the_free_monad_offer_that_macros_dont/dfhhdeb/), but they don't tend to get used nearly as much as in lisps. Is defining application-specific macros common practice in Clojure?
I've remembered and found the haskell package mentioned in the above comment of mine: - Digraphs with text
Same question? Can this be done via hlint? I'd like to enable this via hlint, if possible, today!
Right. I was not sure and now you have convinced me.
hlint would have to implement effectively a renamer, or generally keep track of scoping. It's a lot more effort than putting it in e.g. Intero, or adding it to GHC directly.
I'm having trouble wrapping my head around whether this would allow me to safely and conveniently disable the warning about shadowing, which would make it very attractive.
Aha, thanks, yes, with that provisio I'm in favor.
Yeah, I'd much rather use -Wall and then opt out of specific warnings than use -Wall and still have to opt in to more warnings.
For what it's worth, `-Wall` doesn't currently mean _all_. You need `-Weverything` for that. 
why?
GHC has `-Weverything` doing what `-Wall` should do. Somehow it's not documented in the user guide.
&gt; `runJob` in yesod-job-queue does not have the `MonadCatch` constraint. So the simplest solution was to switch to `Control.Exception.Safe`. I don't understand. `Control.Exception.Safe` provides a [`MonadCatch`](http://hackage.haskell.org/package/safe-exceptions-0.1.6.0/docs/Control-Exception-Safe.html#t:MonadCatch) typeclass. Do you mean that you switched from `Control.Exception.Safe` to something else? Or perhaps that you switched from the [`MonadCatch`](http://hackage.haskell.org/package/exceptions-0.8.3/docs/Control-Monad-Catch.html#t:MonadCatch) in `Control.Monad.Catch` to the one in `Control.Exception.Safe`? &gt; This did eliminate the [type] error; but now when the exception occurred, the job remained stuck! Switching from one implementation of `catch` to another shouldn't change the behaviour of the program. Did you perhaps remove your calls to `catch` when you did the conversion? &gt; `MonadCatch` is a monad that catches errors that are thrown. It sounds like you're saying that all thrown exceptions in a `MonadCatch` computation are guaranteed to be caught somewhere? This is not the case: computations with a `MonadCatch` constraint may choose to use [`catch`](https://hackage.haskell.org/package/exceptions-0.8.3/docs/Control-Monad-Catch.html#v:catch) to catch some exceptions thrown in a portion of the computation. Or they may choose not to. &gt; So of course, that constraint cannot be eliminated. You *did* eliminate your constraint by specializing `m` to IO. This is a perfectly fine thing to do, since IO does have [an implementation of `catch`](http://hackage.haskell.org/package/base-4.10.0.0/docs/Control-Exception.html#v:catch), so IO computations may choose to use it to catch some exceptions thrown in a portion of the computation. &gt; So is there an effective way to handle an exception such that it does not obstruct the job ? That's a really weird way to phrase the problem. It sounds like you are saying that using `catch` to handle exceptions is causing your job to get "obstructed". But when you're saying that your job is "stuck" or "obstructed", clearly you mean that an exception occurred within its body, that the exception was not caught, that the thread which was running the job died as a result, and so the job never completes? If so, it is *not* handling the exception which "obstructs" the job, it isn't handling the exception in a "non-effective" way. &gt; we take the `catch` from `safe-exceptions` and monomorphize it to `IO` Why not monomorphize your entire job to IO instead? The type of `runJob`, runJob :: (MonadBaseControl IO m, MonadIO m) =&gt; master -&gt; JobType master -&gt; ReaderT master m () allows you to read from `master` and to run IO computations. That's it. So I know from the type that your job isn't doing anything special which can only be done in `m` but not in IO. I really don't know why yesod didn't give you the following much simpler type to begin with: runJob :: master -&gt; JobType master -&gt; IO () I guess yesod has only reached [brain level 3](https://cdn-images-1.medium.com/max/500/1*gh9POXppzNAgtncJj17K9w.jpeg) :)
What would a type saying "don't worry, the caller catches all exceptions" look like?
I think what we should focus on is: **GHC 7.10 already had this feature**. You could simply used a BangPattern to forbid recursive bindings. Example Recursive bang-pattern or unboxed-tuple bindings aren't allowed: !x = x And it said in the manual: &gt; a bang-pattern binding must be non-recursive It was removed for GHC 8.0 in these commits: https://github.com/ghc/ghc/commit/e7985ed23ddc68b6a2e4af753578dc1d9e8ab4c9#diff-47b9f40b0ede605e7b6bcb330b43ad53L1678 https://github.com/ghc/ghc/commit/46a03fbec#diff-02e7842997523ff8a5ad0312df2edfe2L10348 The question is: Why was this removed? Was it accidental, or did people find a legitimate use case for strict recursive variable let bindings? If not, we could simply re-introduce that, without a language extension.
We don't have that level of guarantee. But the usual context in yesod provides `MonadCatch`, and that is used by yesod to provide a 500 error page whenever there is an uncaught exception. So if that context is missing from the type, you know that this is running outside of the standard automatic exception handling mechanism. And since `MonadCatch` is the universal convention in the yesod ecosystem, you can be pretty sure that you aren't getting any other mechanism, either.
Uniform representation is still useful to support polymorphic recursion.
This isn't a limitation of yesod. It's a limitation of this particular third-party library. The `MonadBaseControl` and `MonadIO` context is there so that jobs can be triggered from within a yesod page handler. That context is needed for the Handler monad to make possible various kinds of generalizations that are available in handlers, such as automatic generation of type-safe routes, automatic generation of DB schema and matching Haskell types, subsite management, etc.
Still a newbie at haskell, and really appreciate your comment! this really helped, so many things I do not know.. lots to learn 
You said, "No." Then you started talking about the variety of different videos that are available and accessible to a non-cs audience. The former doesn't follow from the latter so I asked what point you were trying to make. Don't be an asshole.
Wasn't impredicativity deprecated in GHC because no such algorithm was known? But I agree with you about the need for more context. The author seems to have worked with known people in the past, so I'm looking forward to hearing more opinions. Anyway, you can't beat that github account name.
`-Weverything` was added for exactly this purpose so both camps could be generally satisfied. Alas, it takes you a few more characters to type. ;)
I stated my belief that you were wrong and provided counter-arguments. What more clarity is needed here?
I added my opinion re the !x restriction here on the ticket: https://ghc.haskell.org/trac/ghc/ticket/14527#comment:7 
TIL
TIL
What woud you do about f = \x -&gt; ... f ... ?
Wouldn't it be better to introduce a `letrec` keyword for recursive bindings?
Please no. To me `-Wall` means "all warnings that people pretty much universally agree should be warned about". I for one do not want to receive recursive binding warnings, and would be incredibly unhappy if this was added to `-Wall`. If we changed the meaning of `-Wall` to include every warning then I would have to either change every project I interact with to use `-Wmost` or whatever or I'd have to start arguing against proposals that I would otherwise have no issue with like this one. 
Yes! Getting a computer to deduce a fact like "experiment X invalidates theory Y" from unstructured text is hard. However, merely *representing* such a fact to the computer is easy. You just need a way to represent the experiment, the theory, and the "invalidates" relationship. In DWT you could just write, for instance, "the two-slits experiment #invalidates the light-as-particles theory". If someone else used different names for the same things, the equivalencies could be pointed out to the system (using the same notation), after which searching for one of them could yield the other. Online fora like Reddit, Facebook and news.ycombinator.com have taken to the idea of letting users provide metadata to guide each others' reading. On Reddit and y-combinator, posts with many upvotes are more salient. On Facebook something similar happens with likes. But the metadata that users could be providing is far greater. Suppose the last two paragraphs I just wrote were dumb, but this one was gold. A system that lets you create whatever relationships you want, rather than some predefined stencil like "up|down" or "happy|sad|angry|..", would let you point out explicitly to other readers exactly what you think of what you're reading. They could then use that information to navigate what they read -- perhaps promoting the things you called useful, perhaps attaching a gold star to them, etc; perhaps not seeing at all the things you called a waste of time; perhaps using some function of the metadata every commenter has added, or (more likely) some set of trusted or otherwise interesting commenters. The advantages are, in brief: * Every writer, not just the original author, can write about everything. * Readers can use other readers' information to help navigate information. * Readers can search for whatever they want -- "theories invalidated by experiments involving cats", "moneymaking ideas famous people don't have time for", etc. DWT already offers the flexible relationship syntax, and search. It does not yet offer the synonym-equating facility described above, or the user-defined display characteristics. Also great would be user-defined folds, like "count the number of times a passage by [author] about [topic] has been marked [quality]". DWT is not a lot of code (about 1300 lines), but it would be even easier if it were rewritten to use Datalog (a faster, more scalable, less expressive variety of Prolog). Since releasing that demo and nobody , I seem to have run out of steam for the project. I probably need teammates. 
I can definitely say Iâve used recursive bindings several more times than Iâve been hit by this issue. I donât think itâs wise to give warnings for things that are perfectly logically sound. Especially since most of these cases would be due to referencing shadowed names, which will already have a warning.
Clojure was done by someone who has no background in computing and the language is accordingly bad. If you want to check for yourself: https://www.reddit.com/r/Clojure/comments/4ps3xa/alan_kay_and_rich_hickey_about_data/ Alan kay has a background in computing. Bad languages like Clojure shows that doing a good programming language is hard. Meaning: "concise" doesn't matter if the language is bad. For some good examples google Alan Kay, STEPS, VPRI, etc.
What is the current state of custom preludes?
I'll leave the burden of proof to /u/chrisdoner. He says: I believe that in practice, I only actually use real recursive bindings once in a while. My gut feeling is the same, from top of my head, I only know these are used in `recursion-schemes`: cata f = c where c = f . fmap c . project I'd prefer to see a proof-of-concept run on e.g. Stackage set of packages, with stats and highlights about false-positives (hopefully there aren't true-positives!). Then it will be way easier to discuss the warning. Even that might result in the code written which is not-merged, but then the decision will be educated.
At the Core level, these are one and the same. So this should be easy to check after type-checking.
&gt; Monads are the in order to chain functions that return computations But List and Maybe are also monads. Heck, in F#, from your name, anything that implements `collect` and `singleton` is a monad (assuming all the laws hold), like `seq` and `option`.
That's something that you can explain AFTER the developer understand a case of monad in his own engineering context. if you start explaining monad with maybe to a java developer, you are teaching fluid dynamics to an aeronautical engineer using the case of submarines. the submarines should come AFTER they understand it with planes.
I mean, when I first read the "chain computations together" thing, it confused me because I thought there was something special about monads (rather than there being something special about, for example, IO) that let me do the "icky impure" stuff. To me, it's the other way around: talking about monads as "chaining computations" before just being a pattern for some things is like talking about integration before basic arithmetic. If you're talking about someone entirely new to functional programming and things like higher-order functions, then yeah, maybe I could see that. But given that lambdas are in every programming language and their mothers now, I don't see how that could be a problem for long.
That video is great too, though the Computerphile take manages to be merely a third in length. In exchange it's dryer, of course.
Well played.
And whererec and top level rec?
I think classy-prelude and base-prelude are both pretty popular. If your project is big enough you can also have a project specific one that you build up over time as needed.
That would break old stuff...I guess what you would need is to leave `let` alone and have `letdec` for non-recursive (`dec` as in [decursive](http://stevengharms.com/blog/2005/02/23/what-is-the-opposite-of-recursion/)?)
&gt; Especially since most of these cases would be due to referencing shadowed names, which will already have a warning. I think that everybody uses `-Wall` anyway, and we still hit these issues. So `-fwarn-name-shadowing` catches some mistakes but not others. &gt; I donât think itâs wise to give warnings for things that are perfectly logically sound. Name shadowing is also logically sound, but as GHC says, can lead to or be a mistake. &gt; I can definitely say Iâve used recursive bindings several more times than Iâve been hit by this issue. That being the case we can probably try applying this to Hackage to see how many cases we hit in practice. I will say though that in terms of trade-offs would you be against adding it if it helps others? How long would you spend reading and applying a tweak from a warning that suggests to use `~x`, multiplied by how many times you ever write a self-referential variable, versus how much time do you spend debugging an accidental self-referential variable, e.g. while doing a refactor or something? Maybe 10 seconds versus 2 hours?
In exchange it spawns a huge amount of controversy because many people don't get the concept : ) I really think the video I linked provides a way deeper understanding of the concept than the computerphile one. Some of the comments here (not as much on yt) raise very valid shortcomings: choice of Expr, choice of Maybe, not showing how the concept abstracts over more than one monad which is why the abstraction's so powerful.
That's certainly true. The Computerphile video is by no means perfect. It's merely not nearly as bad as especially the Youtube comment section suggests :)
[@angerman_io's latest tweet](https://i.imgur.com/Lw2WhWj.jpg) [@angerman_io on Twitter](https://twitter.com/angerman_io) - ^I ^am ^a ^bot ^| ^[feedback](https://www.reddit.com/message/compose/?to=twinkiac)
How fast is it actually though? Asciicinema removes any real notion of time passing.
&gt; Name shadowing is also logically sound I should clarify what I meant by "logically sound." I just meant that name shadowing almost always implies some bad logic involved, whereas self referential variables are logically useful in most cases. I've personally spent zero time debugging issues of this form, so I'm just surprised that others see it as such a problem. If hours really are being wasted on things this warning would prevent, then I suppose it's probably a worthwhile change. But it's also worth noting that the problem still exists in mutually recursive definitions, which actually seems like the more likely instance to me.
Clearly, -Wall should be called -Wcommonsense instead and -Wall should be allowed to adopt it's true meaning /s
I've never seen macros be common practice in clojure the same way they are in scheme. A lot of things tend to be done for you and the language becomes less about being a blank slate to define your universe in and more about being a pleasant language to use (at least, that's the assumption I've gotten from both common lisp and clojure although I do confess to not being super familiar with either.)
Although, honestly, using a custom prelude can be super lightweight. Just have a single module that immediately reexports the normal prelude and then tweak things from there. Hide some stuff, define common helper functions that exist project wide, etc.
Yeah absolutely. I should have clarified that "big enough" is a rather small threshold. 
Accidentally self-referential let or where is probably the most common actual "crash" bug I get. It's not a common bug in general, but the other bugs are unique, and they produce an unexpected result, not a crash. It's my first guess when something just stops with no output, but it can be tricky to track down because the loop could have been generated in one place and only evaluated someplace else. Added to that, if I take too long before killing the job, I can effectively bring the machine down by taking too much memory. I probably don't spend that much absolute time on them, maybe 2 or 3 times a month, and 5-15m each time, but they stand out because they form a clear category where other bugs don't, which also means they could be prevented in one place, while other bugs are not so amenable.
Itâs no more breaking than the `Strict` extension, since only new code would use it per module (or per Cabal/Stack project). I would accept an extension `RecursiveLet`, like `RecursiveDo`, that makes `let` non-recursive by default and permits `rec` blocks in `let` and `where` blocks to allow explicitly recursive bindings.
MLF and HML both handle impredicativity. GHC has tried to add support for it, but it's currently not working (see [here](https://ghc.haskell.org/trac/ghc/wiki/ImpredicativePolymorphism#Whatabout-XImpredicativeTypes) ). There are new plans afoot by the look of it, but I don't know how many people are working on it / where that work is in their various priority queues.
I've used OCaml which has this and found it to be *a real pain*. It's an extra annoying thing to keep in the back of my head as I write code and it forces me to group together mutually recursive bindings in ways that mess up the flow of the program. I highly appreciate not having to worry about that in Haskell! Moreover, I almost never run into bugs with recursive bindings. When I do they never make it past testingâthey result in infinite loops or weird behavior the first time I run my code in the REPL. The worst I've ever gotten thanks to Haskell automatically recursive bindings is an extra 30 minutes chasing down a problem in the code I'm writing, never any *real* bugs. I've lost massively more time to poorly localized type error messages! The risk posed by recursive bindings is nowhere *near* high enough to warrant the cost of explicit recursive bindings. It's simply a bad tradeoff.
That's not helpful. Can you provide us details on what are you trying to do? I'm guessing you are trying to build a web app?
I believe that the real problem is incorrect assumptions. It's not easy to convince someone that monads are easy. They attach too much attention to a basic concept that's nothing but a simple typeclass. Indeed, the [haddocks for `Monad`](http://hackage.haskell.org/package/base-4.10.0.0/docs/Control-Monad.html) says: &gt; The Monad class defines the basic operations over a monad, a concept from a branch of mathematics known as category theory. From the perspective of a Haskell programmer, however, it is best to think of a monad as an abstract datatype of actions. Haskell's do expressions provide a convenient syntax for writing monadic expressions. And, `(&gt;&gt;=)` &gt; Sequentially compose two actions, passing any value produced by the first as an argument to the second. But far too many newcomers still say 'I still do not get what the hell monads are.' I sometimes think that they are too used to complicated concepts such as classes or actors to appreciate even the existence of this simple structure. Therefore I argue against 'programmer perspective' explainations. Not because monads aren't programming, but the entire 'monad tutorial' business's *real* deal is to convince readers that they've known enough monads to program.
I've been thinking (not too deeply) that "recursive or not" should be a property of the use rather than the declaration. That said, `letrec` certainly has a lot of prior art.
How do the counter-arguments relate to the topic? I claim that videos are made because people will watch them. Then your "counter argument" was... that they make lots of different videos with broad appeal? Do you see why I'm confused?
I know I personally would not be against the introduction of this warning on the condition that it NOT be added to `Wall`. If it really does help lots of other people out.
Which actually begs a deeper question. Doesn't GHC have tests for such features and functionality to ensure things are not accidently removed or broken? 
How about `val`.
Or we could not break backward compatibility and change the spirit of the language (default recursive binders make more sense in a non-strict language) and instead introduce a new non-recursive monomorphic binder `val`.
Instead of braking backwards compatibility, we could introduct a new non-recursive monomorphic non-function binder `val`. It would look like this f x c = val y = x * 2 in val z = c + x in y + 2 + z input f = do line â getLine val result = f (read line) putStrLn (show result) Generally val Pattern [â· type] = value in Note, that since these bindings are not recursive only one binding is allowed per `val`. As a result, the weird double indentation that `let` requires is not necessary. Also, pattern matching on `val` should work like `case` on a single with a single clause. This should simplify working with single element GADTs. Do you think I should propose something like this?
We could do this without breaking backwards compatibility at all by introducing a new binder for non-recursive monomorphic variables `val`. See this comment of mine: https://www.reddit.com/r/haskell/comments/7fmjld/ghc_feature_proposal_warn_on_recursive_bindings/dqdyw8z/
GHC has a pretty extensive test suite, and I believe most tickets aren't closed until there's a test suite.
Don't pattern guards already provide a non-recursive binder?
&gt; The problem is not with the warnings, it is with your code Except when GHC starts warning about things that are *intentional*, sound, and **allowed by the Haskell Report**, like recursive bindings or the use of tabs as indentation.
"I Have No Issue With Functional Programming, and I Must Scream" 
I had been preparing to present Philip Wadler's "Monads for Functional Programming" at Papers We Love San Diego when I remarked to a friend, "You could have a whole meetup just on Haskell papers and other things." His response was, "let's start that." So that's how we decided to start a new Haskell meetup here (there hasn't been one for a couple of years). If you're around Southern California and interested, please come discuss Haskell with us. Thanks.
Finally someone understands me
Asciinema caps pauses to 2 seconds in my recording, which kicks in when I sit around and think what to type next. Then the playback speed is 3x in the link to keep you from falling sleep while I type. Here's one with 1x playback speed where you can get a sense of the plugin's speed: https://asciinema.org/a/q9I5eNblDLCoOiQlZjm1ce0ba?size=20&amp;speed=1&amp;theme=tango Worth noting is that this demo is on a blank file. I've used the plugin on codebases of 10-20k LoC, with multiple-package builds, and it has worked well. Depending on if you're using interpretation or object code compilation, the initial startup can take some time (this is GHCi startup straight off). When booted things are fast, though, in my experience. Others have reported it surprisingly fast for their large projects, also. I tried a bit with the PureScript compiler codbase to give you some numbers. It has ~150 Haskell source files at ~26k LoC. A *clean* startup with `-fobject-code`, loading all the modules, took 33 seconds on my laptop. Reloads are &lt;1 second, hard to measure but I'm guessing 200-500ms. Also, the plugin is a simple wrapper on top of GHCi, so whatever speed you have there, you'll get the same in Neovim.
Well I just answered about 25 comments. Whew that's quite a meditation on self-control.
&gt; Sometimes community does even say that tooling is not needed, that is not ok. That depends on your expectations. I do all my Haskell programming with syntax highlighting, GHCi, and the haddock documentation in a browser window. I would never do that in Java, but that is because Java is a very clumsy language.
Of course you can work without tooling. I have done thibgs using just nano, it is possible. what I am trying to say is that sometimes a group of people deny the benefits of using certain tooling. In my case I value the ones that save time, mostly inline error reporting and type info on hover, so I don't have to use the compiler or interpreter by hand losing time and energy.
I like the idea. Not crazy about overloading `~` for this, since it technically changes the language itself. However, I'd be fine with a pragma that disables it for a specific binding (e.g. `let {-# LETREC #-} x = x in x`
Just to put it on the table: I would assume this has *no* effect in `rec` or `mdo` blocks.
We actually already have `rec` as a keyword if you enable `-XRecursiveDo`.
Have you looked at the [browsix](https://browsix.org/) project? They have implemented a lot of Unix syscalls in TypeScript.
The section on that wiki page called "The way forward" says that impredicativity should be supported via explicit type application annotations, not by an algorithm that actually type-checks impredicative types. If so, the new efforts likely have something to do with the fact that we now have a nice syntax for explicit type application.
How have I not seen this yet? That sounds like exactly what we need. Will give it a deeper look soon.
[removed]
&gt; Since WebGHC/musl is a forked repo on GitHub, you can't open issues on it directly Can't you enable issues in repository settings?
Great progress! 
Iâm happy to add an approximate version to HLint, which in practice wouldnât be too hard and would be quite correct. I often wanted this rule, but never thought of a good way to ignore it. Your ~ trick is genius. 
I completely disagree. GHC had a coincidence that approximated this feature. No one added that feature deliberately. I think itâs a good idea now, but not just because history happened to meander nearby. 
That's news to me. Done. Thanks
Not Haskell, but I think it raises a number of interesting questions for PL designers. It reminded me of propellor (http://hackage.haskell.org/package/propellor), but translated to the cloud setting. 
Personal income tax rate in Singapore is progressive up to 22%. They mention 5% which is either a mistake or indicative or a low salary :-)
In Haskell you need to embrace and love your types like no other language you've used so far. Then you'll get to the point that you can use hoogle (think: "haskell google") to explore the language: you put in a signature you would like to see and hoogle will return functions with that signature.
I personally use VS code + Haskero + the syntax one (just search "haskell"). To use it you need to first install stack on whatever platform you're on. Then follow the instructions in Haskero (as I recall I think you just need to "stack install intero"). The plugin claims you need to start VS code in the project directory but you can also just make sure your PATH variable is set up properly. After that, you'll want to read up on stack at least enough to create a new project. It sets up a proper project directory structure. You can go into the generated *.cabal file and add what ever dependancies you want (you'll need to read up on cabal file format as well) and do a "stack build" to download them and create your application. "stack ghci" to play with it on a command line. The above should get you closer to the kind of environment you're probably more familiar with if you've been programming in Eclipse/Emacs on enterprise software. That is, all the boring stuff is handled by stack and your directory structure and you can focus on just writing code.
Couldn't you in theory, and in general, transform any async syscalls to sync by just sleeping in its thread until it returns/calls back? res1 &lt;- async syscall1 res2 &lt;- wait res1
Done!
According to the resident tax brackets[1], earning 90k SGD makes your effective tax rate exactly 5%. 90k SGD is ~67k USD. Not very competitive for an experienced Haskell engineer, especially in a city as expensive as Singapore. [1]: https://www.iras.gov.sg/irashome/Individuals/Locals/Working-Out-Your-Taxes/Income-Tax-Rates/
It could also be that the slightly more abstract concepts aren't immediately obvious how they help you write code. I personally like to understand how things actually work and go together, but I know that many people only cares about solving the task at hand.
Fair point; though the manual saying `a bang-pattern binding must be non-recursive` and a check being present, it felt like a feature.
Hardly seems to be different from using `~` to indicate "I accept this might be recursive"!
Perhaps. But itâs just unnecessary and adds some levels of indirection. Makes code more complicated and probably slower.
Definitely some runtime overhead. I suggested it in the spirit that this way maybe you can get something working more quickly, reuse more code of others. And later on you (or anyone who is hurting by even the small constant factor) can go in and refactor, optimize. Deferred work, in effect.
That is not intuitive to me. I'm self-educated, but am always trying to learn all the things I don't know.
&gt;you want a recursive binding, you can use the ~ tilde to say "I really mean it", That's already used for lazy pattern matches so I think we will need something else.
&gt; You could simply used a BangPattern to forbid recursive bindings ....with the additional effect of making that particular argument strict. Which can change semantics drastically *especially* in situations in which you might use a recursive `let` binding. 
&gt; The problem is not with the warnings, it is with your code. In this case, a lot of completely normal things like catamorphisms would also suddenly trigger `-Wall`, which sort of defeats the purpose. 
Maybe after the Haskell 2020 report is published. But I would be tentatively against it. It's rare that recursive bindings have caused me any real trouble in Haskell.
&gt; having trouble wrapping my head around whether this would allow me to safely and conveniently disable the warning about shadowing Name shadowing isn't bad because the compiler is confused, it's bad because it's confusing to the author/reader.
&gt; Not crazy about overloading ~ for this, since it technically changes the language itself Not to mention the fact that `~` is already used for another purpose...
&gt;This applies to let, where and top-level pattern bindings. What exactly does this imply? Would you even be able to write a `fibonacci` or `factorial` function? If you can't do recursion with top-level names I'm really confused how you can write Haskell at all with this warning turned on. 
Yeah; I guess that's what I meant by "overloading".
List of Singapore Haskell shops: * Standard Chartered. * Capital Match. Any others?
Is there anything explaining how this all fits together? All I've ever programmed with are high level languages and I've never gone in to the depths of Haskell but I do have a couple of years of using it professionally. I'm struggling to picture how all these github projects fit together to allow me to write Haskell that's run both on the server and in the browser client, sharing code where desired. Is this even what this project could achieve? (I hope it is!) Can a scrub like me get involved? Where do I even begin?
Has that really been your experience? Perhaps you interact a ton with externalities and need niche libraries to do things people typically don't need to do? My experience has definitely been the opposite, tooling and installing libraries has never been an issue.
Not sure if it helps, but I can relate to this. I'm fairly fluent in Clojure and from the beginning it felt natural to me, "at home" as you put it. I pick up Haskell every six months and learn a little extra since where I left. I'm half way into HaskellBook. But I don't feel at "home" in this language, at least not yet. It might take years or maybe it's just never going to happen.
Zalora?
Great questions. The README in wasm-cross tries to explain some of this, but it needs some fleshing out. The general structure for any Haskell toolchain is: 1. **The Haskell compiler:** GHC takes in Haskell code, type checks it, and compiles it to a representation that the C compiler can understand. 2. **C compiler:** Haskell's runtime is fairly dependent on C. The C compiler just needs to be able to convert C code and the intermediate representations that GHC emits (assembly or LLVM IR) to machine code. For Clang, this required a new LLVM backend to be written that targets WebAssembly. LLVM is the component that handles the intermediate representations like IR and assembly, and Clang is the component that handles C itself. 3. **The linker:** We use `lld` to link LLVM outputs into a final binary. Each Haskell module corresponds to one object file that the linker includes in the output. 4. **The C library:** `libc` implements all the standard C-level functions that Haskell expects to find at runtime. In this project, `musl` handles all of this and needs very little specialization for WebAssembly. 5. **The operating system:** `musl` emits syscalls to the operating system, asking it to do all the low level work like allocating memory and writing to file handles. For WebAssembly, we basically have to treat the browser like an OS, and write syscall handlers in JS to do the work that `musl` asks for. We should probably move this "operating system" into its own repo, but it's currently just a couple of JS files in the `musl` source tree. wasm-cross sits at a level above all of this. It's a Nix system for *producing* a working toolchain of that structure. In fact, it's generic enough that it works for anything that `musl`, GHC, and LLVM all target, not just WebAssembly. To use GHC for both the client and the server, you have to produce two different versions of this toolchain. One for WebAssembly, and one for the server's platform. The latter is most likely just your basic GHC installation. But the former requires a lot of setup, which is what `wasm-cross` attempts to automate. Once you have your two toolchains, you can use them separately on the same code base to share code and produce the binaries needed for each platform. Of these components, **the Haskell compiler** and **the operating system** are the two that need work. Clang / LLVM, `musl`, and `lld` all seem to work mostly well enough. GHC needs some tweaks for the build system to support our relatively limited platform. But the "operating system" needs the most work, and is the subject of this OP. It just needs to implement a lot of syscalls in `musl/arch/wasm32/js/wasm.js`. This is probably the easiest place to jump in, if you have any familiarity with C level languages. Each syscall is pretty well spec'd in Linux documentation; we just have to reimplement them in JS. It's nothing logically difficult, there's just a lot to do. Most likely, we'll want to import some virtual filesystem for the browser.
Hardly use bigger ides anymore, I almost exclusively use Emacs, thanks for the tips tho! Will keep in mind.
First, thanks for taking time to answer my comment. I will tell you part of my experiences. At work eg. I use C++ for mobile programming, which includes Qt for gui and OpenCV for image processing, and there is no alternative to this combo when using Haskell. Also I have used Python and sklearn for machine learning, server side, where Haskell is a bit weak today (I tried to implement the same solution and failed to do so without writing a machine learning library by myself). I used Haskell successfully at work for text processing, and in that case only tooling was weak (because of that it took more time than needed to find some wrong expressions and fix them). For personal projects I wanted to implement some multimedia apps, and I had to fix, update, or implement some functionality (eg. related to midi). I didn't finish the project, because I couldn't concentrate in my own code yet. I don't know if image processing, mobile programming and multimedia are niche. What do you think? Are they?
IIRC they don't use Haskell any more.
I mean sure those aren't niche, they do fit with my guess that they involve a lot of externalities, but I suppose the majority of apps probably do. I guess I just tend to work on very low-externality apps, or apps where the externalities are very popular (web dev / database interaction / http stuff). For mobile programming I'm personally just waiting for reflex mobile, which I think could be a killer feature for Haskell, with excellent performance but without having to change the way you develop too much compared to what you would do with web based reflex. Have you tried `friday` or `hip` for image processing? I haven't since I haven't had to do any image processing recently but maybe they would work for you? Not sure about the multimedia stuff, have never really looked into it. It would be nice if there were a bunch of full time developers just getting paid to make / fix / improve libraries, but unfortunately I do not currently have the funds to make such a thing happen.
If you're using stack, you can use Intero with VS Code (via "haskero") and it's fairly good. It has some warts that could use cleanup, but it's on par with every other experience. Really, I feel like stack makes haskell dev roughly the same kind of workflow as a node, golang, or java project. There is a project generator, a place to list dependencies, and a way to build.
Thanks for this nice article! It's a really good use for ad. I think this will motivate me to return to Haskell after a long time away.
Yes, that's the idea. 
The `~` operator would not be changed. Pattern bindings carry an implicit `~`, see the Haskell language report.
It's the same purpose. There is no difference.
Yes, because those are functions, not pattern bindings. This is a function: fib n = if n &lt; 3 then n-1 else fib (n-1) + fib (n-2) This is a pattern binding: fibs = 0 : 1 : zipWith (+) fibs (drop 1 fibs) which is equivalent to ~fibs = 0 : 1 : zipWith (+) fibs (drop 1 fibs) as per the Haskell report.
I think that's a much more breaking change. My proposal doesn't require modifications to the parser or language semantics. It literally is just a warning message.
I'm not so sure about that. Recursive do blocks still have bugs with self-referential things.
That'd require completely changing how you write code everywhere.
&gt; But it's also worth noting that the problem still exists in mutually recursive definitions, which actually seems like the more likely instance to me. It's already noted in my proposal.
So presumably, `~x &lt;- use x` would use the same concept. It's interesting because `rec` and `mdo` already explicitly call out recursion that is not possible otherwise, having this on top seems odd.
I see. A comment on the ticket led me to believe otherwise. Good to know!
Functions are a spanner in the works. Just giving something the right type doesn't help, e.g. f :: a -&gt; a f = f type checks but is still bogus. On the other hand, f = \case ... is a common pattern we'd like to allow unmodified.
An interesting idea would be to supplant `rec` and `mdo` with this new technique.
can you provide a simple example of porting a syscall, with explanation, as well as summarize the largest differences between the systems? for example, `printf`. people have mentioned asynchronous calls, but is there anything peculiar in, say, buffering or encoding, that someone who knows a little of JS and C (me) might not. otherwise, I can contribute some of the more mechanical bindings, but they might be a waste of time, if they need to be debugged too much by more knowledgeable people in the project. thanks. I'll definitely help out if I can, very interested in JavaScript backends for GHC. 
I'd be very interested in working on this over my upcoming winter break (January). I worked on [bindings](https://github.com/taktoa/ghcjs-electron) for the Electron API about 6 months ago because I'm heavily interested in writing cross-platform Haskell GUI applications that can compile for all platforms using Nix. I wonder how hard it would be to port those bindings to WebGHC.
about a virtual filesystem, I don't know anything about webassembly, but iirc, all browser storage is transient, and anything permanent must be explicitly permitted by the user (like from a pop-up). So otherwise, it's more like caching than storage. syscalls that talk about the filesystem might make assumption that differ from how browsers work? idk. Again, I'm happy to learn enough about this stuff to help out, but I still don't have a clear scene of what the problems in making bindings might be. also, for others, docs for syscalls can be found here: https://linux.die.net/man/3/printf 
[`nanosleep` was a really simple one](https://github.com/WebGHC/musl/commit/151379d21864ddfe8a2a324355573d58ee3ad5f8). Syscalls take between 0 and 6 Int32 arguments, which are often just pointers into the memory buffer. You read info out of the memory, and potentially write results in, and typically return some Int32 result (again, often a pointer). The process for implementing one of these is to basically find the Linux man page on that syscall then try to duplicate that behavior in JS.
Great use of line printer paper from 1983.
thanks! That helps a lot. Related to my virtual file system question, does webassembly provide primitives for the heap, or do we pre-employment a virtual heap too. like, is this: &gt; heap_uint32 = new Uint32Array(heap); just a js array? 
But it's less confusing without accidental recursion. 
WebAssembly binaries have their own instruction set for interacting with the heap directly. But the heap is just an ordinary JS `ArrayBuffer`, so JS can interact with it using all the normal `ArrayBuffer` APIs. `Uint32Array`, for example, is a thin wrapper around an underlying `ArrayBuffer` which provides an array-like interface for reading and writing unsigned 32bit integers to and from the `ArrayBuffer`. So WebAssembly gets instructions (which, after JIT and memory protection features, should perform about as well as direct memory access in a native C program), and JS gets an array-like API.
Yes, but it gives you exactly what you want. There is no reason to add warnings for things language features already cover. 
&gt; The average person with a passing knowledge of computing This person is not looking at a channel called "Computerphile" and certainly isn't watching a video on something called "Monads".
I actually thought it was a great explanation and have read and watched pretty much all of them. Different strokes...
tl;dw version Monads abstract sequencing computations.
makes sense, thanks again.
You note that the Euler method is very bad. What automatic integration technique does `hamilton` actually use then?
&gt;I use emacs and vscode, and I would like at least core functionality working well without too much trouble, eg. type info tooltips, inline documentation, jump-to-definition, coloring, indentation, inline error reporting, build &amp; run. &gt; &gt;Do you think those are basic or advanced features? Mostly advanced, definitely. As @evincarofautumn said, it would certainly be *nice* to have a good IDE, but with the exception of colouring and (arguably) indentation, those are all features of an *integrated development* environment rather than a text editor -- sure they're near-universal within IDEs, but it seems a bit disingenuous to say you aren't asking for a full IDE and then require nearly everything that distinguishes an IDE from a text editor. I get that you're trying to draw a line between "full" IDEs and minimal ones, but I have no idea *where* you're putting that breakpoint. Besides, and again with the exception of colouring and indentation -- or being on Windows, which *is* admittedly sub-par -- those aren't too difficult to replace by just keeping an extra pair of terminals open: one in the project root (with `cabal run` or better yet `cabal test` just an Up+Enter away) and the other running GHCi (with your modules loaded, and a [pair of commands](https://wiki.gentoo.org/wiki/Haskell#Integration_with_GHCi) for calling local hoogle in the init). Jump to definition is even as simple as Ctrl-F (or equivalent) `symbol ::`--- you do annotate all your top-level declarations, right? ---for most things, and functions should typically be short enough that local definitions are only a few lines away. I will however give you inline errors, even though I'm not sure how helpful they are over (failing while) building often and fixing the errors as they show up then. Likewise, while I agree in theory that debugging should be easy with small function scopes, there *will* always be times when the only input(s) you know fail are to functions several layers more abstract, and it *would* be nice to figure out how and where that input's been changed before it gets to the error point; that's where QuickCheck and SmallCheck come in, along with good documentation, but as you say, it can be nice to have step-by-step debugging to (re)familiarize yourself with larger projects. And even with FFI, I definitely share the stance that any library should provide a graceful interface for failure, or at the very least very clear documentation on everything that might cause the library to fail. From another of your comments, I don't think that it's an issue that people "deny the benefits of good tooling", I think it's just that we deny that the benefits of monolithic tooling (copy-paste/re-typing is really not *too* much harder, and I do actually want to avoid the mouse and "on-hover" as much as possible -- presumably I'm not the only one) are worth the effort to develop that monolithic tooling. Haskell's not C# or Java--- whose *build systems* all but require tooling ---or even C++, after all. And unlike the major imperative languages, Haskell doesn't have big companies with their own stake in the tooling, so whatever we do have is because individuals decided it was worth their time. As you've seen, many of us are more comfortable outside an IDE, so why would we spend all that effort developing something we most likely won't use? We'll leave that for [people like you](https://github.com/leksah/leksah/blob/master/Contributing.md), who do prefer that type of environment.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [leksah/leksah/.../**Contributing.md** (master â aaa832c)](https://github.com/leksah/leksah/blob/aaa832c17eee0866d2c017e220ce77a439fb567a/Contributing.md) ---- ^(Shoot me a PM if you think I'm doing something wrong.)
Well that spoils the surprise of the second link a bit. ;)
For anyone looking for a link to the `wasm.js` file: https://github.com/WebGHC/musl/blob/syscalls-js/arch/wasm32/js/wasm.js
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [WebGHC/musl/.../**wasm.js** (syscalls-js â d4dc48b)](https://github.com/WebGHC/musl/blob/d4dc48b9f1b55a486500f92b6e223a893922fed3/arch/wasm32/js/wasm.js) ---- ^(Shoot me a PM if you think I'm doing something wrong.)
If you want to know about better integration methods, then let me point you towards [RK methods](https://en.wikipedia.org/wiki/RungeâKutta_methods). Of course, there are more beyond this, but they are readily understandable. If you only wanted to know what `hamilton` uses, then I don't know.
**RungeâKutta methods** In numerical analysis, the RungeâKutta methods are a family of implicit and explicit iterative methods, which include the well-known routine called the Euler Method, used in temporal discretization for the approximate solutions of ordinary differential equations. These methods were developed around 1900 by the German mathematicians C. Runge and M. W. Kutta. See the article on numerical methods for ordinary differential equations for more background and other methods. See also List of RungeâKutta methods. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/haskell/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^| [^Donate](https://www.reddit.com/r/WikiTextBot/wiki/donate) ^] ^Downvote ^to ^remove ^| ^v0.28
[removed]
Hmm, I think you have a point there, and you are right in various ways about the editor vs IDE thing, but remember that for various people it doesn't matter if they work with and IDE or an editor, when it works. What I'm really trying to say is that even after giving lots of time to my emacs setup it feels broken, and after trying vscode plug-ins they feel broken too. I just want to work on my code without too much trouble, if it has to be from an IDE, I'm ok with that; if it has to be from an editor, it is ok too; but it has to work. For Python development I use emacs, and it works well. For C++ development I use QtCreator, and it just works. I would be happy using eg. vscode with haskelly or haskero, if they worked well, but they don't (and today they seem to be even less maintained than before). Maybe my breakpoint would be there, using a good editor with a good plug-in suite. The tool from your 2nd link feels even more broken, and I think it is a bit overkill and a bit alien, I prefer to stay in emacs and vscode land for now, because at least I can use generic tools provided by them as workaround (eg. grep or inline terminal with stack). I think in the end the real solution seems to be [haskell-ide-engine](https://github.com/haskell/haskell-ide-engine) paired eg. with [vscode haskell language server](https://marketplace.visualstudio.com/items?itemName=alanz.vscode-hie-server) or other editor plug-in. A common codebase for these functionalities will reduce brokeness (I hope).
Makes sense. I do think some of that is the user's expectations, though -- I've been working in vim without *any* plugins beyond colouring and match-the-previous-indentation, and while it certainly feels primative, it also feels smooth and well-fitting; I don't expect the "fancy" features to be integrated, so I don't miss them. Does that count as "everything works" when there's practically nothing there to begin with? Maybe if I used some plugins, things *would* (almost paradoxically) seem more broken as I run against the rougher edges of the community code. Maybe I should just test that to get a better feel for the ecosystem... But, yeah, Leksah is pretty broken in comparison to just about any mainline IDE (and plenty of minor ones for that matter). There's a reason I linked to the contribute page rather than the readme. It's not a bad showing for all the work everyone's put in already, but it still has a long way to go and I can't imagine extra hands would be turned away.
I'm afraid I still don't see the difference between the two. 
I can't speak for `hamiltonian`, but Hamiltonian dynamics-based algorithms for Monte Carlo typically use the [leapfrog integrator](https://en.wikipedia.org/wiki/Leapfrog_integration), for example.
Also note that "automatic integration" and "automatic differentiation" are different than "numerical integration" and "numerical differentiation" (and they are also both different from symbolic integration and symbolic differentiation). *hamilton* uses automatic differentiation, and numeric integration. 
&gt; explicit parameter passing over side-channel communication What do you mean by side-channel communication ? Something like use of mutable globals or mutation of the state of an object from which a method would be called ?
Yeah. Any time you are accessing data in ways other than the parameters, you are using a side-channel. This might be something obvious like a global variable. But it could also include reading or writing to the class's member variable, reading from a config file, writing or reading from *any* file, connecting to a database, sending a message or dispatching to a work queue, reading from the system clock, reading from the system to get timezone information, or generating a random number.
Alright, so it's not specific to programming (the expression itself), it just mean using indirect methods to communicate information ?
Exactly what I want is to write regular Haskell programs, not write awkward pattern guards to define everything,
I implied it meant the average person who would find Computerphile a good resource and had maybe heard the term Monad before. Maybe someone who has been taught programming in school or is self taught. Computerphileâs videos are very clearly pitched at educating non-experts. Most of the videos on that channel get it spot on. Look at the tone of the other videos on functional programming, other languages, algorithms, computing history or cryptography and there is clearly an attempt to explain things to the average person who has a keen interest but limited knowledge of a given subject. This one on Monads went down several different rabbit-holes in my opinion. I suspect the number of people coming away from that video having learned something about the subject is insignificant. The majority that watched it hoping to gain some insight about Monads probably had their time wasted.
Tsuru?
I made a wall of text again, I hope it does not bother u.u &gt; Makes sense. I do think some of that is the user's expectations, though -- I've been working in vim without any plugins beyond colouring and match-the-previous-indentation, and while it certainly feels primative, it also feels smooth and well-fitting; I don't expect the "fancy" features to be integrated, so I don't miss them. Does that count as "everything works" when there's practically nothing there to begin with? Maybe if I used some plugins, things would (almost paradoxically) seem more broken as I run against the rougher edges of the community code. Maybe I should just test that to get a better feel for the ecosystem... I think there is a point where a developer feels comfortably. It is possible to develop with just an editor and a compiler, but sometimes it is not so effective. I will give you an example... Recently I was working with C++ for a mobile project, and because certain issue with Xcode, I could not use the debugger for iOS. I had not problem debugging my code on Android, but there was an issue which happened only on iOS. Because tooling wasn't working, I had to do again and again the old "edit, compile, deploy, test" cycle, even with printf statements. That time I thought "if this was an issue on android, I could use the tools and finish in less time". I think that situation is similar to what I feel when Haskell tooling is not working. I know I can use the old "edit, compile, test" cycle or interact via ghci, but then I ask to myself "wouldn't be more efficient to have this info in the editor? it could have saved me various minutes and let me focus on the real problem". And I'm not asking about bigger things, like complex refactoring, cross compilation (eg. an equivalent to Qt Kits), packaging integration (eg. generate apk, ipa, snap, flatpak), live code hot-swapping, etc etc (even things I don't know about). It would be great to have those, but first we need at least coloring, error reporting and types on hover. Haskell is really good but today it is not only about the language, it is also about the libraries, about the tooling, about the documentation, the know-how, etc. &gt; But, yeah, Leksah is pretty broken in comparison to just about any mainline IDE (and plenty of minor ones for that matter). There's a reason I linked to the contribute page rather than the readme. It's not a bad showing for all the work everyone's put in already, but it still has a long way to go and I can't imagine extra hands would be turned away. Of course I really would like to contribute, and I'm preparing myself to do it, but I'm not good enough yet... Someday I will be a better programmer, able to do more for the community (I hope). Also thought about doing money donation to improve tooling, and more because here recently it became easier to exchange money with outside (dollar exchange, paypal, etc were limited, bitcoin was the only option).
Euler and RK integration methods don't preserve energy of the system over time. This is known as [Energy drift](https://en.wikipedia.org/wiki/Energy_drift). This problem shows up for about any system if you simulate long enough (although you may be lucky and have a constant average energy). The [Symplectic integrator](https://en.wikipedia.org/wiki/Symplectic_integrator) is one solution, if you do a hamiltonian transformation of your system beforehand. 
**Energy drift** In computer simulations of mechanical systems, energy drift is the gradual change in the total energy of a closed system over time. According to the laws of mechanics, the energy should be a constant of motion and should not change. However, in simulations the energy might fluctuate on a short time scale and increase or decrease on a very long time scale due to numerical integration artifacts that arise with the use of a finite time step Ît. This is somewhat similar to the flying ice cube problem, whereby numerical errors in handling equipartition of energy can change vibrational energy into translational energy. *** **Symplectic integrator** In mathematics, a symplectic integrator (SI) is a numerical integration scheme for Hamiltonian systems. Symplectic integrators form the subclass of geometric integrators which, by definition, are canonical transformations. They are widely used in nonlinear dynamics, molecular dynamics, discrete element methods, accelerator physics, plasma physics, quantum physics, and celestial mechanics. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/haskell/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^| [^Donate](https://www.reddit.com/r/WikiTextBot/wiki/donate) ^] ^Downvote ^to ^remove ^| ^v0.28
Cool, thanks for the response. Perhaps these are too early to ask, but are there any answers to following? Do you have any idea what kind of sizes the wasm binary shipped to the clients will be? (Are we talking a few kilobytes for `hello-world` or is it going to be in the order of megabytes?) Also, would we need to define a new .cabal executable or can we package code up dynamically*? \* What I mean: clientCode :: WASM clientCode = undefined -- | And then in some request handler providing the binary (pretending we're a yesod app) getClientCode :: Handler TypedContent getClientCode = sendWASMResponse clientCode 
As it stands, the toolchain generates a 32K binary for a basic `Hello, World!` program. But this is without having applied [wasm-gc](https://github.com/alexcrichton/wasm-gc), so it's likely including *all* of `musl`, which is disproportionately large compared to the minimal requirements for `Hello, World!`. GHC apps, however, are going to be much larger. I would guess that the WebAssembly output of GHC will be competitive with its native output in terms of binary size. As for your second question, you should think of it more like how GHCJS apps are deployed today. You have a frontend `.cabal` file that builds an executable that runs in the browser, and your backend serves that statically.
I was using it to mean in programming contexts, but I suppose it's very general.
Unconvinced. Itâs like demanding people write ~ on not UNPACK fields in records - a complete reversal of the logic.
Well... Self referential things are quite literally *the point* of recursive do blocks. It would seem kinda silly to me say you must use `rec` *and* `~`. This feature proposal is exactly why many people prefer `rec` to `mdo`; it's just more explicit.
I believe that data is from this [kaggle](https://www.kaggle.com/c/titanic/data)
It's not the point for all bindings in a recursive do block to be recursively defined. Typically, it's because one or two are.
This is an extremely common pattern: do rec x &lt;- ... x ... Where the rec is only for one binding. It would seem silly to also require a `~`. Furthermore, I would find it strange to explicitly opt in to recursive bindings with `rec` and still be warned about it.
Thanks :) 
Before even watching the talk: yes, of course it can. Matryoshka is Scala's version of the [recursion-schemes](https://hackage.haskell.org/package/recursion-schemes), so whatever you can do with Matryoshka, you can do with recursion-schemes.
Imagine I've set up a vm with nix in it. I've installed git and cloned wasm-cross. I can't figure out nix command to run to make it build / install / whatever wasm-cross so that I can execute commands.
Any dynamic linking opportunities to take advantage of when creating wasm binaries?
Aw man, a direct link to a PDF. Is it possible for mods to add [PDF] to the title?
&gt; Alan kay has a background in computing. &gt; [Alan Kay:] "What if "data" is a really bad idea?" Yeah... not sure what to make of your link there. Why don't you tell us in your own words what makes Clojure a 'bad language'?
I am starting a final year project related to session types and found the chapter "Session Types with Linearity in Haskell" from "Behavioural Types: from Theory to Tools" really interesting. http://www.riverpublishers.com/research_details.php?book_id=439 - Behavioural Types: from Theory to Tools http://www.riverpublishers.com/pdf/ebook/chapter/RP_9788793519817C10.pdf (Session Types with Linearity in Haskell) https://github.com/dorchard/betty-book-haskell-sessions (code from the chapter)
The answer to "can X be done in Y" for any value of X and Y is pretty much always yes if you're willing to put the work in. The relevant question is usually how hard would it be, should you, and is it easier/better to do it in another language.
A name binding is a lazy binding with or without the `~`. Saying it is lazy makes a lot of sense, since such recursive bindings depend on laziness to make any sense at all.
To be sensible, "all warnings that X" should really be named `-WX` and not `-Wall`.
Are they intentional? The whole argument is that usually they aren't.
Well, my tabs are. And, my hylomophism definitions are intentionally recursive.
There's just checking here, no inference. As far as I know, its only when you try to work in a setting where type inference is necessary that things get difficult.
I think that would require enabling flair, which would have larger repercussions. I will look into it... 
But if *most* of the time by *most* users they're unintentional?
It's extremely common practice and I'd say even the defacto standard to have Wall not imply warnings that many people are on the fence about. Just look at gcc or clang.
This is so cool - it actually helped me realize that what I don't like about distributed systems is the fact that the interactions between components are difficult to describe statically (in a compiler-checked fashion). This is just so slick. Longstanding question about Cloud Haskell though (and I've asked this on SO too, but still no answer): how does it compare to Akka? It would be nice to even get a feel for how Akka and Cloud Haskell measure up in speed/memory-use/scalability.
Then, *at the very least* I need a way to signal my intention without breaking Haskell 2010 compatibility. I don't like spurious warnings to be added to -Wall, because I want to be able to mandate a clean compile, while still allowing full use of Haskell 2010.
Dynamic linking should be an option eventually, which may help. But itâs not a priority right now
Yea Nix is a bit hard to learn. FWIW, you donât *need* to use wasm-cross for this. The README on WebGHC/musl actually shows how to everything necessary to help with syscalls by hand, and should be enough to test the C toolchain on whatever you like. But with wasm-cross, you can just change [this line](https://github.com/WebGHC/wasm-cross/blob/master/default.nix#L18) to `./musl/arch/wasm32/js` (with no quotes) assuming youâve cloned musl into the local directory. Now to test changes to the JS in that checkout, you just need to do `nix-build -A nixpkgsWasm.hello-example` to produce a `result` directory with `index.html` and whatnot. And of course you can change the C in `./hello-example/hello.c` to change the program youâre testing. I know this is a sort of ugly, ad-hoc, undocumented way to do things, but beginner-friendliness has not yet been a focus of this project :P
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [WebGHC/wasm-cross/.../**default.nix#L18** (master â 53c8e2a)](https://github.com/WebGHC/wasm-cross/blob/53c8e2a0293dff83d3c31c0172b726d6ce024945/default.nix#L18) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dqgpu4s.)^.
Oh! To directly answer your _real_ question: no, moderators cannot edit post titles, as a feature of Reddit. Posts can be deleted, but you can't edit the title even if it's your won post. The only way we could do something similar would be to add link flair (a visible tag) to the post 
is there an associated hackage library?
Yes there is, https://hackage.haskell.org/package/sessiontypes and the wrapper for distributed https://hackage.haskell.org/package/sessiontypes-distributed
I'm not sure I would have ever figured out to type that particular command, but it all seems to work, so when I have some extra time, I'll see if I can't make a few contributions.
Yea wasm-cross definitely relies on a working knowledge of Nix. But again, there are instructions for doing it manually in WebGHC/musl, if Nix gets to be too much.
Yea wasm-cross definitely relies on a working knowledge of Nix. If Nix gets to be too much, Iâd recommend the instructions in WebGHC/musl for doing it manually. Just getting a working C toolchain with this musl port has been greatly streamlined, between lld and clang.
Oh! To directly answer your _real_ question: no, moderators cannot edit post titles, as a feature of Reddit. Posts can be deleted, but you can't edit the title even if it's your won post. The only way we could do something similar would be to add link flair (a visible tag) to the post 
The first step is understanding the domain and algorithm. Once you've made that step, then the answer is yes. If it is computable, then it is computable. If you're interested in decision trees, no one is better qualified to research and write learning materials than you. If you don't understand the problem and solution well, even a fully documented library isn't going to be of help to you. There are plenty of resources out there about this particular subject, and once you understand it you'll be able to implement it in any general use language you're comfortable with. 
I had this idea for a long time: to learn some real world Haskell. I operate most efficiently when I'm challenged. So I talked to myself: I will create this list of small programs that I will have to implement, each a bit more complex then the previous one. Idea was quickly picked up be few friends and before I knew it, this project was created. It's still new, exercises are not yet complete...I guess far from being complete. But if you look for an excuse to finally learn some Haskell or you know somebody who does, maybe this is the right place. I will be adding exercises, solving them in the upcoming months, hoping this will be exciting journey. More people willing to come along the merrier :) You might ask: so you are a newb and you are creating exercises? Well, Haskell syntax is not new to me, I've been playing with the language in the past. I've been also fascinated with Functional Programming for a very, very long time and for the past year and a half I've been doing FP professionally in Scala (yeah, I know what you Haskellers think :) so the paradigm is not new for me as well.
It's gonna be interesting if [Constrained Type Families](https://www.youtube.com/playlist?list=PLnqUlCo055hW7kU-SBQEhC_87etA5Gqlq) will lead anything. I have been bitten or at least confused by all the Wat's mentioned in the talk and the proposed alternative seems much cleaner and newcomer friendly. Still, considering how long the Functor-Applicative-Monad Proposal took...
I have only skimmed the later half of the video. I am bit confused how much focus is put on implementing some specific solution without reasoning why this approach is appropriate. Wouldn't you use just factor analysis or something in that direction, determine/confirm your factors and then, as the final step, and really only if necessary, turn your results into some sort of (simple) decision tree to reduce computational overhead (at a minor cost of accuracy)? (and implementing this final tree should be mostly trivial and not really worth discussing.) Instead this presentation jumps in, does not mention _anything_ about the abstract/mathematical model, and reinvent some "iterative factor searching" using a tree data structure and then tries to fix this unnecessarily basic approach using whatever recursion scheme magic could possibly fit. I cannot tell if there are problems out there where this approach is the proper one, but I really want more motivation. (And if you want people to write tutorials, you might have to provide this motivation, /u/wizzup ).
Pretty sure no one will mind you being relatively new to the language as long as you're open contributions/improvements. What I really wanted to address was this: "(yeah, I know what you Haskellers think :)" Contrary to popular opinion, haskellers don't look down on other languages from their ivory tower. If you are tied to the JVM scala is realistically the closest you can get to statically typed functional programming. Besides, once all java devs have converted to scala, haskell's one step closer to world domination ; )
That depends how you evaluate it. For someone who's been learning haskell for a while and has understood Functor and Applicative, I think it's an ok explanation. For computerphile's target audience? I think you should go by the comment section's evaluation mostly. Linking the concept to function composition is much more understandable to computerphile's viewership imo. People already know it can be useful to chain (compose) "normal" functions. It's not hard to motivate you might want to chain functions that Maybe produce an Int or produce a Random Int. And how you can't always go back from a Maybe or a Random, replacing Nothing with a default value doesn't always make semantic sense and once you branch on a random value your outcome is necessarily random (in general).
There is a Haskell-like on the JVM already, https://github.com/Frege/frege
I think that using an standard virtual machine has all the advantages: interoperability, proven and efficient common infrastructure like the garbage collector. tools, a culture of real world programming around it, lower barriers for adoption... everything. For that reason I find Eta very promising.
If you're not too tied to Quebec I would recommend applying to Facebook. They have at least one team who uses Haskell a lot and there is some really interesting work being done there. PM me if you're interested in hearing specifics
Does the length of import length matter much though? I mean there are significantly fewer bugs per line then code. I mean the package dependency lists get pretty absurd, but that's only one file per package.
This is [CSV](https://en.wikipedia.org/wiki/Comma-separated_values) data, and there's a library that will do the parsing for you called [cassava](https://github.com/hvr/cassava). If you take the example [in the documentation](https://hackage.haskell.org/package/cassava-0.5.1.0/docs/Data-Csv.html) and adapt it for your type, it will look something like this. ``` {-# LANGUAGE DeriveGeneric #-} {-# LANGUAGE ScopedTypeVariables #-} module Main where import qualified Data.ByteString.Lazy as BL import qualified Data.Csv as Csv import Data.Vector (Vector) import qualified Data.Vector as V import GHC.Generics data Flower = Flower { sepalLength :: Float, petalLength :: Float, sepalWidth :: Float, petalWidth :: Float, specie :: String } deriving (Read, Show, Generic) instance Csv.FromRecord Flower main :: IO () main = do irisData &lt;- BL.readFile "iris.data.txt" case Csv.decode Csv.NoHeader irisData of Left err -&gt; do putStrLn ("Unable to parse iris data: " ++ err) Right (flowers :: Vector Flower) -&gt; do putStrLn ("Parsed iris data: data for " ++ show (V.length flowers) ++ " flowers") ``` There's a bit of boilerplate here â `Generic` is used to automatically figure out how to convert your data type to and from the CSV data, so you just have to declare the instance (`instance Csv.FromRecord Flower`). The main function is `Csv.decode`. It will give you a [`Vector`](https://hackage.haskell.org/package/vector-0.12.0.1/docs/Data-Vector.html) of your data type (or fail). Once you have that you can do whatever you want with the values in the vector.
**Comma-separated values** In computing, a comma-separated values (CSV) file stores tabular data (numbers and text) in plain text. Each line of the file is a data record. Each record consists of one or more fields, separated by commas. The use of the comma as a field separator is the source of the name for this file format. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/haskell/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^| [^Donate](https://www.reddit.com/r/WikiTextBot/wiki/donate) ^] ^Downvote ^to ^remove ^| ^v0.28
Did you know that [ghci uses byte-code](https://downloads.haskell.org/~ghc/7.4.1/docs/html/users_guide/ghci-commands.html#id3040816)?
Did you know that [ghci uses byte-code](https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/ghci.html#loading-compiled-code)?
[removed]
&gt; TH is not type-safe: Why? I still have compiler error when the types is not line-up TH *output* passes through the type checker, so in that sense it *is* type safe. TH itself is also type safe in the sense that you cannot pass, for example, a `Name` where a `Dec` is expected. When people say that TH is not type safe, what they mean is that it is possible to write well-typed TH that emits ill-typed Haskell. For example, you can write TH that generates expressions like `1 + "Hello"`, which doesn't type check. There is typed TH, but IMO the benefit is small, considering that, as you observed yourself, TH output still passes through the type checker. The real problem with TH's lack of full type safety is that the errors you get can be obscure: the type checker will complain about type errors in things that don't exist in the source code, only in TH output, and you don't normally get to see that unless you `-ddump-splices`, and even then it can be tricky to figure out where the offending code is coming from. &gt; TH can do arbitrary actions: So does build tools and linters. If I can trust Cabal and Stack I think I probably can trust a few packages that I used? Well, Haskell's type system is Turing complete even without TH, so if someone wants to DoS you by tricking you into compiling malicious code, they probably can. Still, having unlimited access to `IO` from within TH means that compilation is no longer deterministic; you can write programs that sample `/dev/random` at compile time for a random chance of succeeding or not, for example; and this breaks our informal assumption that compiling the same thing twice should produce equivalent output. &gt; TH increase compile time: Well, if it save me time from writing the code I really don't mind. This is more of an implementation detail than anything, and in practice, I have rarely found it to be an issue. There is certainly some overhead, but it's not usually prohibitive. Code that involves TH seems to trigger performance edge cases in the compiler more often though, for reason I don't fully understand yet. &gt; TH requires GHC: So does Generics, and various language extensions that one might enable already. Moreover I think GHC is used 90% of the time, isn't it? Well, one particular problem with this is that TH doesn't just use GHC, it uses a GHC build that can run on the compilation host and produce code that can run on the compilation host, while the GHC that compiles its output must be a build that runs on the compilation host but produces code that runs on the compilation target. This makes cross-compilation tricky; for example, if you want to compile on x86-64, but target ARM, then you need a compiler that can run on x86-64, compile TH to x86-64, but can also compile Haskell to ARM. &gt; TH is ugly: this is the final one. I agree that $(...) looks strange, but I write it like this and find it acceptable. My personal experience is that when the code goes into quasi-quote is arbitrary like Persistent it's like you have to learn a whole new language, but if it is an actual language like YAML (in case of of Groundhog) it is pretty sweet. What do you think if the use of TH is restricted to derive instances and about using YAML (or JSON, XML, ...) in quasi-quote? It's a two-edged sword. One part is that sometimes an SDSL is more elegant than an EDSL, and being to embed it in quasiquotes can greatly improve readability. OTOH, TH is obscure; you cannot inspect the code it generates without actually running it - this is a consequence of having the entire language at your disposal, including `IO`. Which means that the usual tooling will not actually look inside - if you write this, for example: ``` data Foo = Foo { _fooBar :: String, _fooBaz :: Int } makeLenses ''Foo ``` ...then most tooling will not understand that this creates two lenses, `fooBar` and `fooBaz`, and this breaks things like autocomplete, jump-to-definition, and similar IDE features. It also makes grepping more difficult, and this is actually a pretty big issue - when I stumble upon some identifier that isn't familiar, and I need to know what it means, the first thing to do is to find out where it's defined. But if that definition is generated with TH, then grepping will fail, and I have to resort to other, more manual, approaches, and most of these involve learning a bunch of conventions by heart. Persistent is pretty bad this way - it generates all sorts of types and functions that don't appear anywhere in the source tree, and you need to know the naming system in order to find the relevant definitions. YAML or JSON in quasi-quotes isn't something that I would like to use; I would either offload it to a file, and then pull that in at compile time using something like `file-embed`, or write a little bit of TH to pull in the file and also parse it. But even then, this is something I would probably only come up with for things like unit test fixtures; for actual application data, I'd probably go with loading at runtime, if only because this is bloody useful. To write literal data inside a source file, I'd prefer plain old Haskell record syntax, *maybe* `-XOverloadedStrings` and `-XOverloadedLists` to get rid of some of the clutter.
&gt; right now I'm very frustrated with Haskell's limitations in this area. I find it funny that you call those super-precise types a limitation! Unlike a more simple representation such as a string or `data UntypedExpr = Const UntypedExpr | Add UntypedExpr UntypedExpr`, you have defined a datatype which is so amazingly precise that it is not possible to write a value of that type which doesn't denote a well-typed program. This is useful for writing, for example, type-preserving transformations, or as the output of a type-checker. This would be a very bad type, however, for the input of a type-checker! For that you should use a dumber representation such as UntypedExpr. Similarly, if your transformation is not guaranteed to return a well-typed expression, then you'd be better off using something like UntypedExpr than the clever representation you have chosen.
/r/haskell_proposals/
I know this one, unfortunately it's not active and most of the posts are older than 5 years. 
Dependent Haskell will support real type level functions (but not just the syntax). IIRC, there was an estimate that Dependent Haskell will be ready between 2018 and 2020.
My (controversial) opinion is that we already have tracked exceptions (namely value-based exceptions) and that we should use exceptions for the kind of errors which we _don't_ want to track. In particular, we should use exceptions a lot less than we currently do. I made a whole [video](https://www.youtube.com/watch?v=8xkG660D6bI) on the topic.
"If you don't understand the problem and solution well, even a fully documented library isn't going to be of help to you." That's not really true. It won't give you that understanding immediately, but it might well be enough to solve your immediate problem, and regardless messing around with such will teach you things. (Also a library for doing X that doesn't provide any help understanding X isn't fully documented, imho.)
in the meantime, [the singletons library allows you to define type families using the regular function syntax](https://github.com/goldfirere/singletons#promoting-functions).
On greppability: I used to work in Ruby, and the company I worked at had two main rules around metaprogramming: 1. If you're going to metaprogram, make it greppable. This often meant not building identifiers up from strings, but passing in whole identifiers, and in rarer cases putting in comments like `def foo` so there's something to grep. 2. Metaprogramming patterns from well-known libraries (like ActiveRecord) are an exception to the above.
Check out [Liquid Haskell](https://ucsd-progsys.github.io/liquidhaskell-blog/)
Yeah, that was too broadly put. I didn't necessarily mean the implementation aspect so much as utility. If I don't understand an algorithm well enough to translate it from one language to another, I'm not qualified to say whether it's even the correct solution to my problem (I don't mean implementing the idea from scratch, that level of experience is usually unnecessary). Libraries can be helpful for cementing ideas, but you can't really leverage it until you've experimented enough to understand the underlying mechanics or reasoning in a general sense.
It's worth noting that this talk was prepared as an example of something which could be done using recursion schemes which wasn't the typical expression tree (see other scala talks on the subject). Also, there is some mathematical justification of the approach (talking about which optimization approach is used and shannon entropies) between 10-17.
You want every type you care about retrieving to either appear in the final type (`Expr a -&gt; Expr b -&gt; Expr (a,b)`) or be constrained to type class operations you're satisfied with (`(Typeable b, Monoid b) =&gt; Expr a -&gt; Expr b -&gt; Expr a`). In the latter case, you can now check if two of the values (that as part of a `Const` type you are later discarding, remember) are the same type and if so merge them via `Monoid` instance, otherwise arbitrarily pick one of the two to keep.
Great work! I love classical mechanics. &gt; I did some of the work for you for the case of time-independent coordinates where the potential energy depends only on positions (so, no friction, wind resistance, time, etc.). In such a case, the Hamiltonian of a system is precisely the systemâs total mechanical energy, or its kinetic energy plus the potential energy This reminds me of something fun! The Lagrangian for a particle moving in an electric field is: L = 1/2 m v^2 + qV(x,t) where V = V(x,t) is the electric potential. The action of the particle is the integral of this over time: S = int(Ldt) If there's a magnetic field we get the additional terms: L = 1/2 m v^2 + qV + q v_x A_x + q v_y A_y + q v_z A_z where v_x, v_y, v_z are the components of the velocity. This seems rather strange, but look what happens to such terms when we consider that we're integrating over t: q v_x A_x dt = q A_x (v_x dt) = q A_x dx because v_x = dx/dt, i.e. dx = v_x dt. Isn't that interesting! The terms of the magnetic field are the same as the terms of the electric field, except with dx,dy,dz instead of dt. No funny velocity business any more. But there's more. The kinetic energy term 1/2 m v^2 is actually only correct at low speeds. At high speeds it should become K = - m sqrt(1 - v^2). You can verify that this is equal to 1/2 m v^2 at low speeds. Let's consider K dt: K dt = - m sqrt(1 - v^2) dt = - m sqrt(dt^2 - (v dt)^2) = - m sqrt(dt^2 - dx^2 - dy^2 - dz^2) Isn't that interesting? The kinetic energy term becomes the spacetime length of the path of the particle, and everything is symmetric in time and space: S = int(- m sqrt(dt^2 - dx^2 - dy^2 - dz^2) + q (V dt + A_x dx + A_y dy + A_z dz)) One can actually visualise the V dt + A_x dx + A_y dy + A_z dz as a vector field, and the action is the spacetime length of the path plus the inner product of the path of the particle with the vector field. Another thing that's interesting is that the momentum is the Lagrange multiplier of the constraint v = dx/dt. Consider the action: S = int(L(x,dx/dt)dt) We minimise this action over all paths x (actually any stationary point will do). We can instead minimise it over x,v under the constraint v = dx/dt: min(S) = min_x,v {int(L(x,v)dt) such that v = dx/dt } We can handle such a constraint with a Lagrange multiplier p: min(S) = min_x,v,p { int(L(x,v) + p(v - dx/dt)dt) } Since no derivative of v is taken, we may bring that minimum inside the integral: min(S) = min_x,p { int( min_v {L(x,v) + pv} - p dx/dt)dt) } We give this inner function a name: H(x,p) = min_v {L(x,v) + pv} This is the Hamiltonian! Finally, min(S) = min_x,p { int(H(x,p) - pdx/dt dt) } This is the variational form of Hamiltonian mechanics. Writing down the Euler-Lagrange equations for x,p gives the Hamiltonian equations. It's fun to repeat this for relativistic mechanics, keeping x and t fully symmetric.
I'm in **exactly** the same situation as OP. Can you explain here about the work Facebook is doing or can I PM you?
I agree that making invalid states unrepresentable matters more than being concise
Feel free to PM
Or also Galois. https://galois.com/careers/software-engineer-intern/
Great! Thank you very much for the throughout answer. The point about functions and types that are not exists is pretty fair, I guess I don't see it much of an issue at first because I'm coming from the world where methods can be generated at runtime and naming convention is actually required - not preferred (you need to name things in the way the framework assumed)
I like what you're doing. Creating programs is the best way to learn.
While something like this is not total: \a -&gt; case a of { Just b -&gt; b; } Would this proof assistant be able to convert a Haskell program that at least explicitly marked non-total functions with `error` or `throw`? For example: \a -&gt; case a of { Just b -&gt; b; Nothing -&gt; error "null value"; } As I understand it, the function above is not total because it evaluates to "bottom" for some inputs. But it seems the proof assistant should be able to convert a program such as this? Or is it not possible? 
&gt; even If I could overwrite the constructor To be clear you can do that with `-XPatternSynonyms`: newtype Grade = G' Int mkGrade x | x &gt;= 1 &amp;&amp; x &lt;= 6 = G' x | otherwise = error "Grade must be between 0 and 6" pattern G x &lt;- G' x where G x = mkGrade x You can do what you want with ADTs somewhat obviously because if you have: data FooBarBaz = Foo | Bar | Baz you can always define: data FooBar = Foo | Bar It's a little harder to mess with builtin types like `Int` though. Also on a side note I would probably go with `mkGrade :: Int -&gt; Maybe Grade` and drop the `where` from the pattern synonym, since partially functions are generally not great to have lying around.
Which part are you having trouble with?
the answer will depend on what you did in the lecture and lab mentioned. the way is stated (simple brute force) it looks like as if it's ok if you just enlist all the combinations for the x_1,...,x_n, map E over them and filter out those inputs with the maximum value for E. That directly gives you hints for small functions to write: - a function that gives you a list with all combinations of the x_i - a function to map one combination of the x_i to E(x_1,..,x_n) - a function to collect the maximum values - plug them all together IMO it's probably all right if you use lists everywhere so most of those should be not to hard (ask again if you have trouble) Finding of the maximum values might be the hardest of those and maybe you'll want to divided this even further (but beware of the "implemented efficiently" constraint - not sure what this means but maybe you'll lose points if you "iterate" over the combinations more then once(?))
&gt;I made a wall of text again, I hope it does not bother u.u Not at all! Means it's a strong discussion that isn't likely to die through a lack of interest. That is a good point about debugging. If Haskell is missing a single tool, then, it is probably a `gdb` equivalent -- once we have that, we'd be able to integrate it into whatever *DEs we're working in. (EDIT: Apparently GHCi *does* have [a debugger](https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/ghci.html#the-ghci-debugger). That seems like something more people would want to mention more prominantly.) If it's missing a second, I'll give you it being a (stable) integrated editor. I've probably been falling into the same trap of "Alt-Tab is good enough for me" as an excuse for disparate tooling, but at the same time it *is* good enough to a degree that many other languages can't match without an IDE. And if it comes with learning a bit of patience as modules compile and tests run, well, it only helps our nostalgia for classic 80s programming. :P As for the libraries and documentation, on the one hand, I really like how everything's indexed in hoogle. I've had enough trouble tracking down docs in other languages and winding up with five sites open (in several tabs each) all with different layouts, that being able to go to one place and pull up anything I need feels really good. On the other, I agree it's almost all Haddock, and sparse Haddock at that; there's rarely the same care taken to describe the higher-level usage of the package, at least where it's easily accessible from the hoogle landing pages. Similarly, for Haskell itself, there's some really good pages out there on *how* to use things, but not *what* to use when: all the monad tutorials and discussion of error handling, but much less beyond "there's parts of the Prelude that are now considered harmful" that's not simply "use `Text` instead of `String`". I understand why there's no "Effective Modern Haskell" book out there to go with the C++ one, but it is definitely unfortunate that it's so hard finding shorter guides on *general* best practices. And, no, [WIWIKWLH](http://dev.stephendiehl.com/hask/) doesn't count with its sections labeled "This is an advanced section, and is not typically necessary to write Haskell." &gt;Of course I really would like to contribute, and I'm preparing myself to do it, but I'm not good enough yet... Someday I will be a better programmer, able to do more for the community (I hope). I wish I could find the post again, but I remember reading something once that argued lack of experience is no excuse to not get into contributing to open source; pick a small issue as an introduction to the project's architecture and style and learn the language using that. There were several other inspirational myth-breaking points in the same vein, and I have no idea what happened to it. That said, I've only ever worked on my own stuff as well, so I'm not in the best position to talk.
Hey, one of the authors here (Antal Spector-Zabusky). Thanks for the question! If you add the explicit `error` call yourself, then the translation will be golden, *if* you have an axiom for `error : forall {A : Type}, String -&gt; A`. Coq won't inspect that function call, and the pattern match is exhaustive. However, it turns out that the plain partial pattern match (your first example) actually works too, as long as `hs-to-coq` knows about the definition of the `Maybe` type â if `hs-to-coq` knows the constructors involved in a pattern match, it can determine if the match is exhaustive or not, and can insert an axiom for the missing cases on its own. As a concrete example, we can put your two functions into a `Test` module, where `fromJustUndefined` is your second example with an explicit call to `undefined` (instead of `error`), and `fromJustNonexhaustive` is your first example with a missing `Nothing` case. module Test where fromJustUndefined :: Maybe a -&gt; a fromJustUndefined (Just a) = a fromJustUndefined Nothing = undefined fromJustNonexhaustive :: Maybe a -&gt; a fromJustNonexhaustive (Just a) = a To translate this, we tell `hs-to-coq` that `undefined` is a custom axiom and that `Maybe` is Coq's `option` type (these are the *edits* from the paper) rename value GHC.Err.undefined = undefined rename type GHC.Base.Maybe = option rename value GHC.Base.Just = Some rename value GHC.Base.Nothing = None We also tell `hs-to-coq` to insert a custom `undefined` axiom into the preamble. Axiom undefined : forall {A : Type}, A. Once we've done this, `hs-to-coq` will translate this into the following Coq code (eliding some unnecessary preamble) Axiom undefined : forall {A : Type}, A. Axiom patternFailure : forall {a}, a. Definition fromJustNonexhaustive {a} : option a -&gt; a := fun arg_0__ =&gt; match arg_0__ with | Some a =&gt; a | _ =&gt; patternFailure end. Definition fromJustUndefined {a} : option a -&gt; a := fun arg_2__ =&gt; match arg_2__ with | Some a =&gt; a | None =&gt; undefined end. So in the end, these two functions come out looking the same â the important difference is that `undefined` is *user-provided*, whereas `patternFailure` is *autogenerated*. We have to provide our own `undefined`, since Coq doesn't have that axiom by default (thank goodness!), or we'd fail with an unknown name; and we have to switch to `option` because `hs-to-coq` has information about its constructors wired-in (we could also have defined `Maybe` locally), or we'd just get a partial pattern match (`fun arg_0__ =&gt; match arg_0__ with | GHC.Base.Just a =&gt; a end`) that Coq would reject.
About debugging, I have found this [phoityne](https://marketplace.visualstudio.com/items?itemName=phoityne.phoityne-vscode) plug-in recently. It seems to be using ghci as backend, but I think the complete sources are not fully released yet. Having things indexed in hoogle is a breeze. I really like that tool, it saved me multiple times when I didn't know which module to import for use a certain function of which I know the name or the type. Other languages don't have such an amazing resource. Certainly better usage documentation is needed, because sometimes types alone are not enough. Before I had to inspect the code of a library to know how to use it, because I couldn't understand with the docs provided (I think it happened to me with eg. amqp lib). About the harmful parts of the Prelude, it would be better to remove those when possible, maybe not now, but at least plan to do it in the future. Also give a warning when compiling the code, something like eg "Warning: String is a deprecated feature and will be removed in the future, prefer using Text instead". &gt; I wish I could find the post again, but I remember reading something once that argued lack of experience is no excuse to not get into contributing to open source; pick a small issue as an introduction to the project's architecture and style and learn the language using that. There were several other inspirational myth-breaking points in the same vein, and I have no idea what happened to it. That said, I've only ever worked on my own stuff as well, so I'm not in the best position to talk. Yes, you are right, there are many ways to contribute. I'm really trying to do something, eg. I have created the [haskell-ide-chart](https://github.com/rainbyte/haskell-ide-chart), updated the frag game a bit in [this repo](https://github.com/rainbyte/frag), and done some small fixes here and there. My contributions are poor now, but I'm working to improve :) Of course, when talking about the tooling, in order to contribute you need to know the language and API used by the target editor / IDE. In my case I would have to learn elisp for emacs or typescript for vscode (this one seems more promising lately).
let me elaborate me on why I am asking this question, the things is I have tried to statically compile a Haskell project but I did not succeed with that since it depends on some C libraries that are not available in static.on the other hand, on a virtual machine every library should be provided by the framework for example which make no chance to depends on a library that does not exist. however, I am not even trying to say latter is better, what I aim to is see what do haskeller think about the topic. 
Do you also have answer to "The relevant question"?
Out of interest what prevents haskell itself from being compiled straight to the JVM? Is it to do with complexities of implementing lazy evaluation / TCO / a GC that fits well with Haskell? Or is it more just a matter of it not being worth the time?
I wish we could have these syntax benefits without needing dependent types. Maybe I'll end up on the wrong side of history but the idea of a theorem prover in Haskell makes me nervous. What I personally want is the best syntax and user experience possible for Type -&gt; Type, Type -&gt; Value and Value -&gt; Value. So clean syntax for type level programming and lots of typeclass features such as perhaps closed type classes. With the above + adding a lot more type information to literals. So for example `fromInteger :: forall a b. (KnownInteger a, Num b) =&gt; Proxy a -&gt; b` for desugaring integers and similar for strings and lists and so on. I feel as though we could get a lot of the benefits of dependent types with no theorem prover. 
I have no idea, you'd have to ask the Fregge designers :3 I _suspect_ it has a lot to do with taking advantage of JVM-native type semantics over normal Haskell type semantics in some places (int, long, String), which is probably close enough to be practical and a lot faster than trying to match the actual Haskell spec.
That's what the tilde suggestion solves. Whether it is in `-Wall` is a separate question. I think it's fine that it's opt-in at least until consensus is reached.
IMO there is no such thing as "close enough to be practical". There is either "close enough to be Haskell" which would be awesome, or "not close enough to be Haskell" which may be a fantastic language in its own right, but then you lose the ability to write code that works with both GHC and the JVM. Now I'm not saying any language that isn't Haskell has no value, but I'm just saying it's not what I'm personally looking for as far as Haskell on the JVM.
âType families should be either total or behind a class constraintâ describes exactly how I thought type families should work when I first encountered the feature.
What a `Set` `Monad` needs is the ability to do this efficiently s &gt;&gt;= const (return x) &gt;&gt;= f when `x` is an element of a type with `Ord`, otherwise one may as well use `[]`. Could you explain how your implementation achieves this? (By parametricity I find it quite hard to believe it's possible *at all*!)
I enjoyed [this](https://youtu.be/Uw2ercJStdk) one. Anyone have experience with it?
If you can't do it the idiomatic way, i.e., use libraries (seems mean to me since getting to know the batteries is important when learning a brogramming languages), then you have two choices: * split by lines, then by commas. `Data.List`, which is part of `base`, is your friend here. I guess you know the way from there. In my opinion, this is much simpler and to the point than reading each line separately. If your data set is very big though, it's probably better to read line by line and use `Data.Seq` to accumulate the data since careless programming with lists will kill your performance. * If you are allowed to do so, use parser combinators. Maybe it is already enough to create an instance of the `Read` type class for the record data type. For both cases, when you do this often, you will recognize that you write structurally similar code when you convert your list of strings to your algebraic data types. Therefore, both ways are not considered idiomatic by "advanced" (a hugely ambiguous term, I know) Haskell programmers because Haskell gives us the tools to abstract repetition away. In the end a library should do the heavy lifting. The programmer is only required to interfere when customization is needed. But why implement such a library by ourselves, there is already `cassava` (and, similarly, `aeson` for JSON).
I guess Frege is an example of the latter. It is not 100% compatible to GHC because it has its own frontend. It implements some of the language extensions, but not all of them.
Can `fromJust` instead be translated to have as a prerequisite for its use that the pattern matches? As in, the user would have to submit a proof that only infinite loops can supply a `Nothing`, like when something of type `forall b. (a -&gt; b -&gt; b) -&gt; b` is given `(&lt;|&gt;)`.
Are these permanent positions or contracts?
Here is a talk about one project they are doing at facebook with haskell https://www.youtube.com/watch?v=mlTO510zO78
I really don't see how this is controversial. I mean, I know lots of people like to use exception, but this is Haskell, we're better than that.
I don't see how a VM would help with this particular problem. It sounds like what you want is a language which doesn't support writing bindings to existing C libraries, thereby ensuring that every single library is re-written from scratch in the language, and which also has a large standard library like Java's, so that you'd seldom encounter a situation in which you wish you could write, say, bindings to an existing GUI library. I'm curious now: how are Java libraries which use jni (Java's ffi mechanism) packaged and do they have the same problems?
A better implementation of [diet sets](https://web.engr.oregonstate.edu/~erwig/papers/Diet_JFP98.ps.gz). We already have the [diet package](http://hackage.haskell.org/package/diet), which works, but it basically just lets you insert/delete/lookup a *single element* at a time. I want to be able to do all of these operations on ranges as well. I want to be able to efficiently union two diet sets or subtract one from another. I want a Map-like data structure (with keys and values) based on this as well. All of these are these that Erwig mentions as possible additions in the conclusion of the paper I linked to, but they're just tricky to get right. Also, note that this is different from a segment tree.
I don't think there is a standard VM out there tuned for functional lazy languages. Reusing a existing one has many advantages and is probably better currently. But I doubt that something tuned for other (imperative/strict) languages will ever be a Perfect fit.
A simple reliable cross platform sound library. If you think GUIs and drawing to the screen have limited solutions in Haskell... well... try making your computer do anything more than just beep.
I say my opinion on Haskell exceptions is controversial because long arguments ensue every time I or someone else writes their opinion on that subject :) Other popular points of view on Haskell exceptions include: 1. never use exceptions, always use value-based errors such as Maybe and Either. only use `error` for unreachable code paths. 2. use Maybe for unreachable code paths too, e.g. `headMay . scanl f z`, not `head . scanl f z` even though [`scanl`](http://hackage.haskell.org/package/base-4.10.0.0/docs/Prelude.html#v:scanl) is guaranteed to return a non-empty list, because who knows if you'll change the code to `head . scanl1 f` in the future and makes that code path reachable after all. 3. use value-based errors for pure code and exceptions for IO code, because you have to deal with async exceptions anyway. I disagree with all of those and am prepared to defend my opinion in the debate which is certain to ensue below :) For comparison, my opinion expressed in the above bullet-point form is: 0. use value-based errors to entice callers to handle that case now, use exceptions to entice callers to handle that case generically much higher up. bugs, unreachable paths, and async exceptions fall in the latter category.
Sorry, I mean I recognize that it is controversial, I just can't believe it.
One day, my friend. :) We are slowly subverting their impure ways.. one programming language feature at a time.
There is [portaudio port](http://hackage.haskell.org/package/portaudio) but I guess it isn't complete?
Are the position available to work remotely?
That email domain is owned by 3rd party recruiting agency, so...
I'll check it out, but I'm leery of packages that haven't had working Haddock since version 0.0.1
(Another author here) The prerequisite that you have in mind would have to be part of the functionâs Coq type, right? The current approach is to preserve the types of Haskell functions as closely as possible, so it is not on the agenda so far.
I know :) It's just writing abstract FP code in Scala and then seeing the alternative in Haskell... just makes you wanna change the language (to at least Eta if you are bound to the JVM :) ).
The group Iâm in at megacorp in nyc is too. Thereâs some info in a r/Haskell post I did over a year ago about past prjects. 
[removed]
Are these figures annual or monthly?
I am afraid am not entitled to answer the question, it really makes sense how the points are structured, with the first point it would be peace of cake to generate static binary but am not sure if this is even feasible
Annual obviously
I intended to write a few suggestions about why and how this should be made a sticky but as my post became larger it occurred to me that I am not actually clear about what this thread should actually achieve. Should we try to maintain and expand on a common knowledge pool? Should we post "success stories" in the spirit of "I learned X after reading Y"? The former should try to avoid repetition and encourage making previous threads searchable or adding things to the [Haskell Wiki page on Research Papers](https://wiki.haskell.org/Research_papers), for instance. The latter would embrace repetition and to just make it a habit of making a post everytime you read and learned about something new. Then there was the idea of making a thread about "translating" papers for a wider functional programmer audience, which seems way more involved to me. Either way, we should agree on some common ground here, since this may be the reason why we don't have this kind of sticky yet in the first place. Last but not least my contribution. I am currently working my way through [Conor McBride's current Agda course](https://github.com/pigworker/CS410-17). I decided to pick up on Agda because I actually want to use dependent types for theorem proving and after also having looked at Coq and Idris (not starting a language war here). What really sold me on this course: While working my way through some of the written resources on the [Agda Wiki](http://wiki.portal.chalmers.se/agda/pmwiki.php?n=Main.Othertutorials), I either became lost in the thick of prerequisites or got bored about the same `Nat` and `Vec` examples over and over again. Most importantly, I didn't get a feeling for "how to think like an Agdaist", that is, what kind of questions should I ask myself when trying to prove something. This is hard to read up on and it really helps when you are shown - even though I normally dislike learning by watching videos. McBride does a really good job explaining things and is also (in my opinion) very entertaining. After seeing him jump up and down in Lecture 6 or so, you'll never forget what a monoid category is.
Well that's why I called it a Haskell-like and not a Haskell?
Location?
Better support for doing routine image processing, like resizing, cropping, etc., for web. Right now the only libraries are either no longer maintained or they focus on doing more interesting image processing. But routine stuff is all but missing.
What about a web server catching exceptions and returning 500? I'd imagine this is acceptable?
ð
ð
Have you considered making a proposal to Haskell Prime?
So ... ? Presumably they know.
&gt; in London 
I just read through about a hundred of the free-form responses to the "What is the biggest problem?" question, and I really enjoyed the responses. It isn't really possible to summarize them all. One amusing thing is that there are two very opposite mindsets out there. One of them says "we need to fix the messed up parts of the typeclass hierarchy" and the other one says "we need to focus on stability". Pretty tough to do both of those at the same time.
Sure. Which is why as I said I'm not trying to criticize the language. It just isn't Haskell on the JVM which is what this thread and me personally are asking about. 
[S]he is referring to this I believe https://www.reddit.com/r/haskell/comments/3re0kp/jpmorgan_haskell_team_is_hiring/ 
I had the same gut reaction, but even though I'm not 100% on board with his conclusions he breaks down his logic later in the comment chain. Namely, that data means nothing without an interpreter, and thus we should bundle the two together (iirc, that was his conclusion). The comment you mention in particular was too vague/baity, he should have continued it with the rest of his thoughts. 
Yes. More specifically, you need to run the two servers in separate haskell threads. `forkIO` from Control.Concurrent is the most basic way of doing this.
With London house prices growth trajectory, UK taxes and Brexit shitshow this may not be as ridiculous a question as it sounds ;)
https://hackage.haskell.org/package/async offers a nice abstraction for concurrent actions.
Thanks, I also found [parallel-io](https://hackage.haskell.org/package/parallel-io-0.3.3/docs/Control-Concurrent-ParallelIO-Global.html) which provide a a function `parallel_ :: [IO a] -&gt; IO ()`, maybe it is just an abstraction over `forkIO`
Whatâs obvious for one may be less obvious to others. Iâm not from London. Where Iâm from monthly figures is the common form. I heard that London is terribly expensive. From my familiarity with finance in NY these doesnât sound high.. so I asked.
Just break everything violently once with base 5.0 and promise to try real hard to never do it again? Seems like the best of both worlds; perhaps we could land that sort of thing in at a similar time to dependent types so all the huge potentially breaking changes happen at once?
Check the pound to usd conversion, it's 1.34 pounds to 1 usd, so the salaries are closer to $100k and $135k a year.
Which is lower compared to NYC salaries for similar positions (~$200-300k) but still not monthly :-D
Hmm, it does not appear that this implementation could do this more efficiently. You would need to use `fromOrdList [x]` instead of `return x`. As you said, this is probably not possible without having two different `return`'s (possibly with some sort of a typeclass), and making it choose the `return :: Ord a =&gt; a -&gt; m a` instead of `return :: a -&gt; m a` whenever possible. Maybe this could be done with overlapping instances?
I generally lean more on the side of "let's break stuff", but I can definitely respect that not everyone feels that way. Consider that changing a typeclass like `Num` (or `Enum` or `Fractional`) breaks a ton of existing code and breaks a bunch of older tutorial material (much of which would likely never be updated, leading to confusion). I think the committee's done a great job at rolling out the most important typeclass hierarchy improvements (Applicative Monal Proposal and Monoid Semigroup Proposal) in a (admittedly glacially slow) way that minimizes breakage. Also, I don't think that dependent types are expected to break any existing code.
I really feel like fixing language quirks that can be worked around with some schlep and are probably roughly equivalent to language quirks in other languages really needs to take a back seat to further work on the ecosystem / compiler. These kinds of issues didn't stop any currently successful language from becoming a powerhouse, demonstratively, they are not the fundamentally limiting issues that certain elements are making them out to be. I mean, hell, JS is blindingly popular (even in the server-side domain) and it doesn't have a single intuitive way to check failure cases in -any- of it's data types that might represent failures - (null, undefined, an empty Array, NaN, etc). I'm pretty sure that if a language like that can take over the world, we'll be ok hanging out with a poorly defined Semigroup / Monoid relationship for another couple of years.
I don't really understand the distaste for compile times. I've used compiled langs for a long time, and GHC is certainly slow. But GHC has GHCi, which is really fast, comparatively. I literally only compile my app when I deploy it.
Good news is that we won't be dealing with the poorly defined Semigroup/Monoid relationship for another couple of years. The superclass is [slated for GHC 8.4.1](https://ghc.haskell.org/trac/ghc/ticket/14191), so I think we're looking at a single year. That's a good point about JavaScript though. I agree with you there. Also, when you say "take a back seat to further work on the ecosystem / compiler", are the compiler improvements you're interested in things like Linear Types and Dependent Haskell, or are you talking more about things like concurrent GC and general improvements to the runtime. (or did you mean both equally?)
You typically package the binaries separately and add the library to an environment variable before running the code. Obviously the binaries are different per platform.
Semi-related: Is there some sort of 'Advanced math notation for programmers' out there? I have the relatively frequent occurrence of seeing greek notation(s) tossed out on this subreddit and would like to be able to follow along, but am not terribly interested in spending a few years re-learning concepts programming has already taught me - Experience thus far has been that any time I try to dig in on this stuff, I'll get several hours into a terrifying wiki-hole before I figure out this all translates to about 4-5 lines of painfully simple fauxcode with accompanying comments. 
Indeed, it basically maps `forkIO` over the list of IOs you supply it, with some bookkeeping for return values and making sure it does not overparallelize. Strictly speaking it uses `forM`, but that's just details :) You can find the source [here](https://hackage.haskell.org/package/parallel-io-0.3.3/docs/src/Control-Concurrent-ParallelIO-Local.html#parallel) if you are interested.
Honestly? No. You don't have to catch exceptions to return a 500. Combine it with timeout protection (which should use exceptions - because those exceptions should be *uncatchable*) listening for a sane response in an MVar (or some such creature) and if it's not available in a certain amount of time, kill the thread and 500. This also ensures you don't accidentally leak any exception data to your clients - your thread doesn't even know it when it returns the 500, all errors are thus indistinguishable externally, which is good. As far as I'm concerned, all you want to do with an exception is log it so you can make sure it doesn't happen again.
In this particular case, the fundamental problem appears to be that your mock class incurs additional constraints on what a Token is. You say " In most OO languages, this is trivial", but that's only true because in most OO languages *all* objects have the additional constraints that you require - equality and stringification. There is also the convenience of subtyping, but I feel you've glossed over the fact that you rely on extra constraints implicitly being provided by OO. So, with that observation, I feel there's not much you can do except slightly muddy the interface of `MonadToken`. Here's one suggestion. {-# language ConstraintKinds #-} {-# language ScopedTypeVariables #-} {-# language TypeApplications #-} {-# language GeneralizedNewtypeDeriving #-} {-# language KindSignatures #-} {-# language GADTs #-} {-# language FlexibleInstances #-} {-# language UndecidableInstances #-} {-# language FunctionalDependencies #-} import Data.Typeable import Data.Type.Equality import Control.Monad.Trans.Class import Control.Monad.Trans.Reader class FromJSON a class ToJSON a class (FromJSON t, ToJSON t) =&gt; Token t type UserId = String data BearerToken = BearerToken UserId data RefreshToken = RefreshToken UserId class MonadToken m c | m -&gt; c where encryptToken :: (Token t, c t) =&gt; t -&gt; m String data FakeTokenTToken :: * where FakeTokenTToken :: (Token t, FakeTokenConstraints t) =&gt; t -&gt; FakeTokenTToken instance Eq FakeTokenTToken where FakeTokenTToken (a :: a) == FakeTokenTToken (b :: b) = case eqT @a @b of Just Refl -&gt; a == b Nothing -&gt; False newtype FakeTokenT m a = FakeTokenT (ReaderT [(FakeTokenTToken, String)] m a) deriving (Functor, Applicative, Monad, MonadTrans) class (Eq a, Show a, Typeable a) =&gt; FakeTokenConstraints a instance (Eq a, Show a, Typeable a) =&gt; FakeTokenConstraints a instance Monad m =&gt; MonadToken (FakeTokenT m) FakeTokenConstraints where encryptToken t = do tokenMap &lt;- FakeTokenT ask case lookup (FakeTokenTToken t) tokenMap of Just str -&gt; return str Nothing -&gt; error ("encryptToken: unknown token " ++ show t) &gt; I imagine many people might suggest I reformulate my tokens as sum type instead of a class: Hmm, if I were to make a suggestion in this area, it would actually be that you represent a token as the methods in the class. That is, a token is precisely its `Token` dictionary. 
The way the salary has been quoted it must be permanent. 
Concurrent GC, general runtime improvement, but honestly, I feel like far more work can / should be spent on improvements that are non-specific to GHC. - Improving documentation and resources to aid mid-level programmers in producing idiomatic code. - Defining clear idioms for common tasks (note, catching exceptions in transformer stacks as an example of something that needs a set idiom that instead involves guesswork) - Improving editor tooling - Building a curated set of tools for runtime introspection / debugging / logging - Improving support for cross compiling to different architectures (this should not involve the user manually rebuilding GHC from source themselves if they're targeting a major OS) Etc. It's these sorts of issues that sink or swim software projects. Linear/Dependant types, or a better type hierarchy in Base do not result in 40+ hour problems to solve (or research projects to conduct) before a dev work can proceed. And that is the sort of thing that will sink or swim a program, not a fancy feature or lack thereof.
There's a Bayesian monad, so yes, *easily*.
A VM helps the author solve this problem, at the cost of solving it in the ecosystem. Running the code via VM / interpreter means that the interpreter of the VM is responsible for implementing a standard set of API calls to the language the author is using in order to provide said functionality. So, yes, you're right, a VM does not call into existence C libs that don't exist, and in fact complicates C interoperability to a non-trivial extent. But for the author of the code that runs on said VM, the experience is drastically simplified - The functionality you need either exists, or does not, there is no : "Oh no, I have just spent 10 hours trying to get this built on windows only to discover that in fact, it is impossible." Whether or not this is a net win for a language is, of course, up for debate, but that's what gets easier when a VM or interpreter is available.
[removed]
[removed]
In this particular case, could the JSON representations of tokens be used as keys in the map taken by "runFakeToken"?
I would also recommend using `async` which is a very thin layer over haskell's concurrency primitives (which were already pretty easy and pleasant to use) married with STM. Certainly click through and browse the source of `async` while you're using it.
`IORef` is not a monad. `IO` is a monad. An `IORef (a -&gt; a)` is not a function, it is a reference to a function. You need to pull the function out of the `IORef` using a function `readIORef :: IORef a -&gt; IO a`. The code would look something like this: func &lt;- readIORef funcRef let maybeValueB = fmap func maybeValueA
You can use the `MonadConc` typeclass from `concurrency` to write concurrent code that can also be systematically/deterministically tested using the `dejafu` library.
In this case, probably yes. In the more complex example I gave at the bottom with database records, no, so it doesnât solve the general problem.
Compile times bother me the most when benchmarking and optimizing a library, both because the feedback loop is so long but also because it seems the more effort you put into making your code fast the more ghc punishes you (and your users, in the case of libraries) for it in compile times. It's also certainly the case that compile times can (or at least have historically) varied wildly depending on what or how much of a particular feature you're using, or whether you are doing something "exotic" that happens to tickle some quadratic algorithm somewhere in the compiler pipeline
What would be the definition of `DatabaseRecord`? 
&gt; You say " In most OO languages, this is trivial", but that's only true because in most OO languages all objects have the additional constraints that you require - equality and stringification. And runtime type analysis a la `Typeable`, but yes, I think this is a fair point. Itâs just perhaps interesting how Haskellâs more precise type system works really well for ârealâ code but makes it a bit trickier to test in this particular case. We sometimes talk about how it would be nice to have exports only for a test suite, but maybe we also sometimes want more generally weakened guarantees for a test suite? Anyway, your proposal is definitely interesting, and itâs something I had started to consider but hadnât fully looked into. It might be a little cleaner with an associated type instead of a fundepâ¦ but upon further thought, that would probably just be a lateral move. I might give it a try, though it does seem like it could get confusing quickly.
I tend to default to fun deps as I've had much better luck with inference, but yea - an associated type could also work. If you do give this a try, please report back - I'd be curious how you get on.
&gt; Also, I don't think that dependent types are expected to break any existing code They won't, as far as I'm aware, but the possibility of having them means a ton of library code can get extensively rehauled to take advantage of the extra safety. Seems like a good time to break the hell out of the base prelude, to me.
probably! :) 
There are a few things that it would be nice to have added to `base`, like `Nat`, `Fin`, a length-indexed list, etc. which all seem to have pretty standard definitions across all DT language. I'd be happy to see those added eventually.
"etc"
Fortunately I like in "other locations" :P
Think [something like `PersistEntity` from persistent](https://hackage.haskell.org/package/persistent-2.7.2/docs/Database-Persist-Class.html#t:PersistEntity).
I'm going to guess this is Dfinity and the job is to work on their Haskell -&gt; WASM compiler. Pretty cool.
`[(BuzzWords, Enthusiasm)] -&gt; Platform -&gt; Profit` &gt;We have the first argument well in hand, we need someone who can computer up the second one for us.
What about doing the `Some` trick on the map instead of the token? data SomeTokenMap a = forall t. Token t =&gt; SomeTokenMap (Map t a)
Yes that does makes sense. Hopefully that's not the majority of your time working with Haskell. &gt; it's annoying to push and then have to keep coming back to check that it completed Heh well you can't blame this one on GHC. This just means you need a notification system. 
not to be a jerk, but I feel like the $350k number is bait and the real number is $180k
Haskell never stops amazing me. A multi-parameter type class with a *constraint* as a parameter.
&gt;&gt; If you could change anything about Haskell's community, what would it be? &gt; Edward Kmett and Chris Done form a partnership funded entirely by extremely generous and unspecific NSF grants. I'd watch that show!
Not sure, I think it would be a good idea to write up something like that. Lots of it is a straight 1 to 1 translation as well (eg sigmas are essentially for loops). The math in this post is a bit dense but only because there's an entire semester worth of implicit background the instructor is assuming people understand and can decode. If the professor was plain in asking them to implement a way to differentiate the function and solve it for it's zeroes (with pity points given for people who just brute Force it instead) it'd be pretty obvious what to do.
The math here isn't really related to anything that is specific to this subreddit, so maybe you're mistaking it with something else? This is solving a mathematical optimization problem, which is something that doesn't really have an inherent connection to functional programming (so this sort of math notation can be expected to be used to describe a problem like this regardless of the kind of language or programming environment being used). Or maybe I'm misunderstanding what you mean (I'm going based on the fact that you are specifically talking about it being frequent on this particular subreddit)? Also, the only Greek symbol I see here is the big sigma for [summation](https://en.wikipedia.org/wiki/Summation).
**Summation** In mathematics, summation (capital Greek sigma symbol: â) is the addition of a sequence of numbers; the result is their sum or total. If numbers are added sequentially from left to right, any intermediate result is a partial sum, prefix sum, or running total of the summation. The numbers to be summed (called addends, or sometimes summands) may be integers, rational numbers, real numbers, or complex numbers. Besides numbers, other types of values can be added as well: vectors, matrices, polynomials and, in general, elements of any additive group (or even monoid). *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/haskell/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^| [^Donate](https://www.reddit.com/r/WikiTextBot/wiki/donate) ^] ^Downvote ^to ^remove ^| ^v0.28
Playing the devilâs advocate here: thereâs just no need to write such granular unit tests. Design your types sensibly and write fewer unit tests. 
It would be impossible to use a map like this, as the type of keys are erased. You could construct one, but immediately lose the ability to use it. 
Well github was doing that for us with the little favicon which changes color. I guess there are a few little things I have to do to finish up a ticket which I need to wait on doing until the build is successful, so I can't just move on to the next thing.
&gt; I find it slightly off that you need a `MonadToken` in the first place. Why monad? Fair. But itâs a much simpler example than my real example, which involves interacting with a database. The full case I am dealing with would have simply cluttered the description of the problem. At some level, I need tests to ensure that CRUD endpoints insert the correct records into the database and appropriately handle failure modes (such as a bad username/password combo, for example). Thatâs very hard to prove with types in Haskell.
Shouldn't the backend get to decide what tokens/records it accepts and what database `Id`s look like?
I donât understand this question. In the database example, you might have two tables, one of which uses a UUID for a primary key and another of which uses an integer. It seems self-evident that, to enforce type safety, you need to ensure that you are required to use the right identifier type when you query each table. What would a different approach look like?
Please let's not break everything at once. Python made that choice and we know how that went. Break things in small consistent increments.
Yeah, that's what the constraints library does: https://hackage.haskell.org/package/constraints-0.9.1/docs/Data-Constraint.html#t:Class
They will talk you down from 180 too :)
Could you provide some more desired test cases of your ideal API? And/or ideal implementations for `runFakeToken` and `login`? Neither of these have to necessarily compile as-is. I'm interested in giving a try implementing something like what you wish for, and I imagine the above could be of aid.
It really pleases my heart to see this set of changes slowly working its way into Haskell. Other projects might disregard this forever due to purely practical concerns but Iâm looking forward to this change being adopted. As a tangent, this makes me think of how Englishâs verbs are becoming more regular. However, itâs only the less well-used verbs that are likely to be regularized. So the fundamentally irregular, and most used, to-be verbs will never be regularized, which is fine for practical reasons. But Iâm glad that the Haskell community has the will to regularize typeclasses that are fundamental and well-used.
We can't easily mess around with `Int` right now, it's an opaque hardware level type. We can make analogues, but we lose out on speed. But if you don't *need* the speed, and also don't mind throwing away all the math libraries that exist... data Binary a where Reify :: Binary a Zero :: Binary (2*a) -&gt; Binary a One :: Binary (2*a+1) -&gt; Binary a readBin :: Int -&gt; (Binary a -&gt; Binary 0) -&gt; Binary 0 -- This sig is necessary readBin' 0 f = f Reify readBin' i f | even i = readBin' (i `quot` 2) (f . Zero) | otherwise = readBin' (i `quot` 2) (f . One) readBin i = readBin' i id -- Yes, id does work here! The `a` is not quantified in the parenthesis :3 toInt :: Binary a -&gt; Int toInt (Reify) = 0 toInt (Zero a) = 2 * toInt a toInt (One a) = 2 * toInt a + 1 Binary is little endian, that is 5, aka 100 in binary, is written `(Zero (Zero (One Reify)))` The next part goes in a trusted kernel. If you export the `BinInt a` constructor, you can have fake witnesses! Boo! `BinInt a` is the actual value you use most of the time. newtype BinInt a = BI (Binary a) realized :: Binary a -&gt; Maybe (BinInt a) realized Reify = Just (BI Reify) realized _ = Nothing To use these toys, you need type families, and multi-type lists data HList a where Nil :: HList '[] Cons :: a -&gt; HList as -&gt; HList (a ': as) type family (&amp;&amp;) a b where 'True &amp;&amp; 'True = 'True a &amp;&amp; b = 'False -- for example... type family BoundedQ a b l where BoundedQ a b '[] = 'True BoundedQ a b (BinInt v ': vs) = a &lt;=? v &amp;&amp; v &lt;=? a &amp;&amp; BoundedQ a b vs type Bounded a b vs = BoundedQ a b vs ~ 'True Now you can have: data Grade a where G :: Bounded 1 6 a =&gt; HList a -&gt; Grade a Though if you want to do anything with the values in the list, you'll have to be prepared for all of your functions over lists of grades to be working over *differently typed* list of grades, non trivial!
First, let's argue that IORef is a Monad. Do these all seem possible? fmap :: (a -&gt; b) -&gt; IORef a -&gt; IORef b join :: IORef (IORef a) -&gt; IORef a pure :: a -&gt; IORef a That `pure` definition seems suspect - it seems we probably need `mkIORef :: a -&gt; IO (IORef a)`, but we'll ignore that for now. It seems to me that this example explains that arbitrary moands m and m' probably have not too much in common. What you probably want is something that let's you pattern match on `return` vice failure, so you could write transform :: (Monad m, Monad n) =&gt; m a -&gt; n a transform ma = case ma of Fail e -&gt; Fail e Return r -&gt; Return r But, of course, these semantics do not work. Functions are monads: fmap f g = g . f join ff = \x -&gt; ff x x pure = const Evidently (I claim), this monad has no translation to `Maybe`. 
&gt; I may want my test to truly enforce that it is encrypting the token, not just calling toJSON directly, and I may want to ensure that the resulting token is truly opaque data. I feel this contradicts the two reasons stated why you don't really need to use the actual crypto function in the tests. OOP mocks, it seems, facilitates testing impure functions that are full of external function calls, and thus the operational semantic solution is to give you more tools to verify the operations step by step. In contrast, unit testing pure function should only require mocking input data, otherwise you are attempting to test the libraries getting called by the function. In this case you seem to want to double check that the crypto library not only got called, but it's doing it's job properly... I would think that to get more robust code with a pure functional language that is using denotational semantics, you step in the direction of formal proofs using something like Coq, instead of using mocks, spies, stubs, etc which do help make OOP code more robust (since it gives you so much rope to hang yourself to begin with). 
I don't understand why the other commenters are being mean to the announcement. They obviously need a highly qualified individual and they seem to be willing to pay for it too. The announcement is also as concise as it could ever be.
Simon Marlow, the father of our run time system, reminds us that concurrency and parallelism are fundamentally different. Parallelism, in his view, deals only with making programs faster.
There are two use cases: you want the functions to run as long as one survives, or as long as either survives. The former is handled by `async_`, and the other is handled by: marathon_ :: IO a -&gt; IO b -&gt; IO () marathon_ ioa iob = do aa &lt;- async ioa ab &lt;- async iob e &lt;- waitEitherCatch aa ab either (const $ void $ wait b) (const $ void $ wait a) e
I'm pleasantly surprised to see that the proposal offers compelling practical reasons for doing this (as well as of course a strategy for going about it).
Nice, I wonder if we'll ever see `Apply` and `Bind` in the `Monad` hierarchy, as well as class Apply f =&gt; ApplyFail err f where fail :: f err instead of the current `MonadFail` (i.e why do you need `Monad` to be able to fail, and why does the failure have to be a `String`). With these in place, we could in theory have: addAndPrint :: ApplyFail (MatchFailure (Maybe Int)) f, Bind f) =&gt; f (Maybe Int) -&gt; (String -&gt; f b) -&gt; f b addAndPrint action print = do Just i1 &lt;- action Just i2 &lt;- action print $ i1 + i2 So that `addAndPrint` can be specialized for some `f` isomorphic to `Maybe . (,) a`. With some more magic in `ApplicativeDo` we could even get `addAndPrint` to require `Apply f` instead of `Bind f`, so that `f` can be isomorphic to `Either [MatchFailure (Maybe Int)] . (,) a` in a way that it accumulates all the match failures in `Left`.
ephiron and istandleet have already answered your concrete example well, but I'd like to add that whenever you feel the urge to mix and match things coming from two different `Monad`s, you're usually looking for `traverse` (or its flipped version `for`). `for`s signature (`... =&gt; t a -&gt; (a -&gt; f b) -&gt; f (t b)`) is almost the same as `&gt;&gt;=` (`... =&gt; m a -&gt; (a -&gt; m b) -&gt; m b`). It may also be that you need `t a -&gt; (a -&gt; f b) -&gt; t (f b)`, but that's just `fmap` :)
For a quarter million+ a year? I... think I'd try.
Are you hiring any interns for the next summer (2018) or just full time?
Nobody tosses a dwarf!
Maybe the mistake is in trying to enforce your constraints through tests instead of the good old type system and parametricity. Say you want to enforce that `login` only ever uses tokens by properly encrypting and decrypting them first; then you could start by changing `MonadToken`s definition to: class Monad m =&gt; MonadToken t e m where encryptToken :: t -&gt; m e decryptToken :: e -&gt; m (Maybe t) Then you could change whoever uses `login` to only accept a function of type: forall t e m. (SomeConstraint, MonadToken t e m) =&gt; ... -&gt; m String) While `SomeConstraint` exposes just enough to `login`, so that it can do its business (like interact with the database). This would force `login` to have a signature, where it knows nothing about `e` or `t` aside from the fact that it can use `encryptToken` and `decryptToken` to go from one to the other. This way, parametricity ensures that `login` does the right thing.
It is sadly true that most DBs live outside Haskell, far away, near StringyTypeLandia. It is fairly simple to write a Kafka style DB in Haskell, if you're interested. I threw together a working one in an afternoon that got ~400,000 updates per second.
I always saw parallelism as a subset of concurrency. Are there any cases of parallelism that are not concurrent?
I agree I think Haskell has handled this really well. They had a design bug, they admitted it and set about a reasonable path to fixing it balancing breaking code and the long term desire. Good judgement all around. 
We're all upset at our non-Haskell jobs that also pay less and don't involve sexy hacker topics.
If you're willing to live in California, that's not too unusual an offer for total compensation for an experienced software engineer. (Not in base salary; typically some of that will be in equity with a vesting schedule.) On the other hand, you have to live in California...
The range is suspiciously broad, but I'd *much* rather have that than no numbers at all. Salary transparency is a great norm to cultivate, and little steps like this is how norms get... normalized. I'd be more worried about how much (if any) of that is based on equity which is *really* hard to value for private companies.
In that pay range, you could afford to live in a nice car. ;-)
There should be a shared VM for functional languages, so we can unite against the real enemy, imperative programming. It would be pretty cool to be able to call SML libraries from Haskell and vice versa.
Just woke up, so probably missing something obvious, but how does the alias of (&gt;&gt;) and (*&gt;) work? (&gt;&gt;) expects m a -&gt; (a -&gt; m b) -&gt; m b, but (*&gt;) provides f a -&gt; f b -&gt; f b ?
You should have got more awake... You confused `(&gt;&gt;)` and `(&gt;&gt;=)`
https://i.imgur.com/rPNCqww.jpg
Yes, sort of - but it also introduces a global thread pool, which you absolutely do not need for this. In fact, you aren't really intererested in parallelism here, just concurrency. A suitable abstraction can be found in the `async` package: [concurrently_](http://hackage.haskell.org/package/async-2.1.1.1/docs/Control-Concurrent-Async.html#concurrently_), which takes two IO actions and runs them, well, concurrently, blocking until both finish (if you want to exit as soon as either finishes, use `race_` instead).
&gt; anotherturingmachine Are studying for the turing test?
You can do some fun stuff with that kind of thing. I feel like `ConstraintKinds` is a bit under-appreciated.
TLDR: Starting with GHC 8.0 you can enable the warning **`-Wnoncanonical-monad-instances`** and let GHC guide you to make your code forward &amp; backward compatible.
Are there laws for `Apply` without `pure`?
Always. I must be able to convince other humans that I am human, otherwise the robots might be able to deceive us.
In San Francisco, you could afford one seat in the car.
Yes: `(.) &lt;&lt;$&gt; f &lt;*&gt; g &lt;*&gt; h = f &lt;*&gt; (g &lt;*&gt; h)`
I think /u/dramforever is suggesting class MonadToken m token | m -&gt; token where encryptToken :: token -&gt; m () etc.
I was thinking about this a bit more last night, and though this doesn't entirely help your general problem, there's another possible solution to the `MonadToken` problem. You write &gt; Cryptographic functions are slow by design, so running hundreds or even thousands of encryption/decryption cycles in a test (especially feasible if youâre doing property-based testing!) is going to make a test suite that quickly takes a long time to run. But what if your *actual* production `MonadToken` implementation could be parameterised - like the strategy pattern in OO. In this sense, you would still run `MonadToken` with a proper implementation that uses cryptographic functions, but in your unit test you would override encryption with very simple yet insecure routines. These would be much faster, and subtle for testing but not for production. For this particular example, I'm not sure a mock is actually the right thing to be using.
I doubt it. Modifying the class hierarchy is a huge technical and political problem even for popular and universally accepted classes. Apply is not popular at all, so itâs not going to happen anytime soon.
I think the main advantage of storing mutable state in the "environment" variable of Reader are exception safety and consistency w.r.t. concurrent execution. This blog makes a number of examples in this sense: https://www.fpcomplete.com/blog/2017/06/readert-design-pattern
Could your fake implementation take a function of type Token t =&gt; t -&gt; Maybe String instead of a Map? Would that avoid the whole business of having to use an existentially qualified type? I'm still kind of a newbie, so sorry in advance if this is a dumb suggestion.
Go through the trouble of implementing `Reader` and `State`, and you will get a 'feel' of the two, which will in turn remind you to use them whenever you 'feel' `Reader` and `State`. These two are probably the opposite of 'complicated'. Do them yourself, and don't be hesitant.
Well that `local` runs a reader monad in a new environment - it's not altering the current environment.
sure, but when that new environment takes over and becomes the primary one, it's effectively the same as altering the environment, is it not?
I'll try that
No, if I have a son and give him a new name, that's not the same thing as altering my name!
I would advise against using `Read` for parsing input. It's certainly not idiomatic. `Show` is mainly for debugging purposes and should usually be derived. Also, `read (show x)` should always be the same as `x`, which is difficult to guarantee if you write your own instances.
`local` can't modify the environment for subsequent `Reader` actions combined in ways not under the first action's control. `modify` can do that for `State`.
Hmm... On the topic of `Nat` (and `Symbol` &amp; al.), I would have liked it if `-XDependentHaskell` made them obsolete/deprecated, by allowing *all* types to be transparently promotable. That is, if you want `Nat` then just use promoted `Narural`, and similarly use `String` (with all the benefits of pattern matching, unconsing promoted `Char`s, etc) instead of `Symbol`. By my understanding, however, Eisenberg did not seriously explore this avenue in his thesis, and the question of universal promotion of all types is still up in the air? 
sure, but if you never return to the old environment, it might as well have been discarded. If I use `local` to update a tally in my environment, I don't ever want the tally to decrease again, because that could lead to an incorrect result.
You seem to have some fundamental misunderstandings, or as I would call it, misanologies about how the 'environment' or 'state' is passed through your program. Please follow my other comment on implementing the stuff yourself, before proceeding any further.
Bind uh, finds a way
Maybe this will make my analogy clearer: example :: ReaderT String IO () example = do myName1 &lt;- ask liftIO $ putStrLn myName1 _ &lt;- local (\n -&gt; n &lt;&gt; "jr") $ do childName &lt;- ask liftIO $ putStrLn childName myName2 &lt;- ask liftIO $ putStrLn myName2 test :: IO () test = runReaderT example "dnkndnts" -- output: dnkndnts / dnkndntsJr / dnkndnts You can see here that I assigned my child a new name (using my original name), but that *did not* alter my original name, it merely provided the altered name to my child context.
It's already [in `transformers`](http://hackage.haskell.org/package/transformers-0.5.5.0/docs/Control-Monad-Trans-Accum.html#t:AccumT). Hope that helps too.
And in Moscow, it's a crapton of money.
`local` runs an action in a modified environment, but actions after the `local` revert to the original environment. This means that the environment inside `local` is different from that outside, but they're shielded from one another: the inner action can only read the inner environment, there is no way for the inner action to return a modified environment into the outer action. By contrast, `State`'s `modify` will cause all subsequent actions to run against the updated state, and in fact the original state is lost (unless you have explicitly kept it somewhere else). Example time. First, using `ReaderT`: flip runReaderT 1 $ do ask &gt;&gt;= liftIO . print local (+ 1) $ ask &gt;&gt;= liftIO . print ask &gt;&gt;= liftIO . print This should print 1, 2, 1. And now `StateT`: flip runStateT 1 $ do get &gt;&gt;= liftIO . print modify (+1) &gt;&gt; (get &gt;&gt;= liftIO . print) get &gt;&gt;= liftIO . print This prints 1, 2, 2. Unlike the `ReaderT` example, where the "inner" action runs in a modified read-only environment, which is discarded when `local` returns, `modify` causes all 'subsequent' (i.e., monadically bound) actions to use the modified state. If you go back to the thing that Reader and State abstract over, it may become a bit more obvious. `Reader` is essentially `s -&gt; a`; `local`, then, would be `(s -&gt; s) -&gt; (s -&gt; a) -&gt; (s -&gt; a)`, or even `(s -&gt; t) -&gt; (t -&gt; a) -&gt; (s -&gt; a)`, which should make it clearer what the implementation should look like: `local f a s = a s' where s' = f s`, or, if you prefer point-free, `local f a = a . f`. Either way, the thing to take away frrom this is that while `f` can modify the environment that gets passed to `a`, neither `a` nor `local` can pass any modifications back up the call chain, because the return types don't have room for it. And if we look at `&gt;&gt;=`, we'll draw similar conclusions: `(&gt;&gt;=) :: m a -&gt; (a -&gt; m b) -&gt; m b`, expressed in terms of our raw `Reader` function type, becomes: `(&gt;&gt;=) :: (s -&gt; a) -&gt; (a -&gt; (s -&gt; b)) -&gt; s -&gt; b`, or, simplified, `(s -&gt; a) -&gt; (a -&gt; s -&gt; b) -&gt; s -&gt; b`. Let's implement it: `(&gt;&gt;=) x f s = f (x s) s`. There really isn't much of another option here: in order to return a `b`, we *have* to call `f`, because that is the only thing we have that can produce a `b`; in order to call `f`, we need an `s`, which we have, and an `a`, and the only way to gain an `a` is to call the `x` function on the `s` that we have. But no matter what we do, the only way we can gain an `s` is by using the one that is being passed in; we cannot possibly modify `s` inside of our `&gt;&gt;=` implementation. Now let's look at `State`, which is essentially `s -&gt; (a, s)`. Things look different now. Let's first look at `(&gt;&gt;=)`. Recap its type: `(&gt;&gt;=) :: m a -&gt; (a -&gt; m b) -&gt; m b`. Substituting our `State` type gives us: `(&gt;&gt;=) :: (s -&gt; (a, s)) -&gt; (a -&gt; (s -&gt; (b, s))) -&gt; (s -&gt; (b, s))`. Let's implement it: (&gt;&gt;=) x f s = f c s' where (c, s') = x s The same reasoning can be applied as to why this is the only reasonable implementation: in order to produce a `b`, we have to call `f`. Calling `f` requires an `a` and an `s`; we have an `s`, but we need an `a`, which we can produce by calling `x`, which also happens to produce another `s`. Now the choices for our `a`s and `b`s are determined, but we still have to decide what to do with the `s`es, of which we have three: the one we get from the caller, the one returned from `x`, and the one returned from `f`. However, there is an obvious choice that becomes more clear when we generalize the type to `(&gt;&gt;=) :: (s -&gt; (a, t)) -&gt; (a -&gt; (t -&gt; (b, u)) -&gt; s -&gt; (b, u)`. Remember that `&gt;&gt;=` is morally related to function composition (read up on Kleisli composition for a more sound argument here), so if we remove the first element from each of these tuples, and ditch all the extra arguments it introduces, we get `(s -&gt; t) -&gt; (t -&gt; u) -&gt; (s -&gt; u)`, which is *exactly* the type of the function composition operator `(.)`. If we do this, then chaining the `s` values rather than discarding becomes the only option we have, and it happens to be the one we want. What this means, however, is that actions in a `State` monad are free to return a different state than the one they got passed. And that is exactly what `modify` does; we can trivially implement it ourselves against our naive non-newtyped type signature for `State`: modify :: (s -&gt; s) -&gt; s -&gt; ((), s) modify f s = ((), f s) OK, so, step back, what does that mean? In a nutshell, it means that `State` actions can pass modified environments through `&gt;&gt;=`, while `Reader` actions are forced to pass the original, unchanged environment. Both can do *other* things with modified environments though, such as calling `runReader`, that is, both can pass environments *down* the call chain (to things that they call), but only `State` can also pass environments *up* the call chain (back to its caller, which is typically `&gt;&gt;=` or `runState`). Now if you like, you can do the same thing with `Writer`, to argue why `Writer` can only pass state *up* the call chain, but never *down*. So, use cases: - `Reader` is basically just additional arguments passed along; `s -&gt; something` is exactly equivalent to `Reader s something`, and `s -&gt; m something` is exactly equivalent to `ReaderT s m something`. Which one you use is a matter of taste. A textbook example for `Reader` usage is when you have a `Configuration` data structure that you populate at startup, and then you want to have it available throughout your application. You could just pass it as an argument everywhere, but since much of your application won't actually need to look at the configuration itself, this will produce a lot of clutter, which `Reader` can avoid. - `State` is a more powerful abstraction, typically used when you need to chain monadic actions that drag along and gradually modify some data structure. Note that "State" is a bit of a misnomer, because it's not actually mutable state: you cannot, for example, share `State`'s "state" between threads, nor will state mutations typically survive exception handling. A typical use case would be tracking source position in a parser; the State monad allows you to ignore source positions entirely except for the parts of the codebase where they matter - eveything else will just use the usual monadic approach and remain blissfully unaware of the additional argument passing that happens behind the scenes. `Reader` won't do here: in order to tell subsequent actions that the source position has been updated, you would somehow have to wrap everything in a long call chain of `local` calls, which pretty much amounts to doing manually what `State` wraps up in its `&gt;&gt;=` implementation.
What if we are treated them as extensible records where restricting (`-`) removes methods type Apply f = (Applicative f - pure) type Pointed f = (Applicative f - (&lt;*&gt;)) -- Before Semigroup/Monoid proposal type Semigroup s = (Monoid s - mempty) type Semigroupoid cat = (Category cat - id) A similar problem is `MonadReader` / `MonadWriter` (discussed [here](https://www.youtube.com/watch?v=YTaNkWjd-ac&amp;feature=youtu.be&amp;t=3604), [here](https://github.com/purescript/purescript-transformers/issues/63) by Edward Kmett), separating algebraic operations (`tell` / `ask`) from non-algebraic (`listen` / `pass` / `local`) ones. This would usually be type MonadAsk e m = (MonadReader e m - local - reader) type MonadTell w m = (MonadWriter w m - pass - writer - listen) How would this work? Maybe something like this instance Semigroup a =&gt; Apply (Const a) where -- Can't define 'pure' (&lt;*&gt;) :: Const a (b -&gt; b') -&gt; Const a b -&gt; Const a b' Const a &lt;*&gt; Const a' = Const (a &lt;&gt; a') instance Monoid a =&gt; Pointed (Const a) where pure :: b -&gt; Const a b pure _ = Const mempty -- Can't define '(&lt;*&gt;)'
You're right, but there could be a chicken and egg problem here. In the beginning `do` notation only supported `Monad`s, and I think this is one of the major reasons why the discussions in the Haskell community are heavily biased towards them. Maybe if we had a finer grained type class hierarchy, people would find very useful `Apply` instances that can't reasonably be made into `Applicative`s, or maybe some useful patterns that can only expose `Apply` constraints. It's hard to say whether all this is worth the trouble, but I sure wish it were like this from the beginning.
OP isn't talking about implementing mutable state using `ReaderT (IORef s) IO` though, they specifically mention `local`, and how it "changes" the environment. Which it does, similar to `State`, but the key thing is that it's one-way, you can't bubble changes back up the call chain with `local`, but with `State`, you can.
Didn't know! Thanks!
Think more carefully about translating the mathematics into code. The problem involves matrix multiplication, and the multiplication you need is helpfully expanded as a double sum. Perhaps solving this problem in an imperative language is more natural: sums correspond to for loops in an intuitive way. In Haskell that's not an option so you need to express the sum another way. In my opinion it's best to start with the types, then write small functions and compose them. Think of how to represent a vector and a matrix in Haskell and write small functions for multiplying them together properly. Then compose them together in whatever way you need to solve the problem.
Nothing, itâs been done already :) Check out http://eta-lang.org/
All three can be implemented in Haskell easily. The reason to use them is saving the hassle of passing extra functions arguments (for Reader/State) or having extra elements in the return values (for Writer/State). So if you have a lot of functions which seem to take a common subset of arguments, move it to Reader environment. If you have a lot of functions that return tuples with a common subset of values, put them in Writer environment. If you use both, use State.
This is an assignment problem, and you should really try harder to solve it on your own. But indeed as /u/innatelogic remarked, `H` is a matrix, not a scalar. You want to maximise `x^T H x` for `x` belonging to `Z^N` (N-dimensional binary vectors). I have a hunch this is related to finding the largest eigenvalue of `H`, and using that to guide the search, but I'm not sure. 
Sure, but if I do this: example :: [a] -&gt; Reader Int Int example [] = ask example (_:xs) = do local (+1) $ example xs test :: IO () test = do let result = runReader (example "hello") 0 putStrLn $ show result -- output: 5 You can see that the resulting value increments because the old context is _effectively_ replaced by the new one. I do see the benefit of what you did, having a local sub-environment where something's different from the parent environment. I can see that being useful in a lot of contexts. But that doesn't change the fact that I can use `local` to change all future uses of the environment. This is the kind of situation that throws me off
I'm having a lot of trouble implementing `&lt;*&gt;` for State. the tuple throws me off a lot and I can't quite wrap my head around it
Might also be Digital Asset?
Sorry, my reference to `Nat` was a little ambiguous. I was just talking about the inductively defined `Nat`: data Nat = S Nat | Z I believe that your understanding is correct. Things like `Int`, `Char`, etc. will not be promotable. More precisely, any data type that contains an unlifted type anywhere inside of it will not be promotable. I think the problem is the dependent types work by allowing information to be learned by pattern matching, but most of GHC unlifted types don't support pattern matches. You cannot really use `Int#` as part of a proof (like, in a length-indexed vector) because there is no way to decompose `Int#`. Idris has some fancy magic that let's it swap out performant integers with inductively defined integers in some cases, but that's still a far cry from being able to promote everything. `Int#`, `Char#`, `Array# a`, `Addr#`, `ByteArray#`, etc.
Don't think so. It's very possible to get good money as a senior developer these days. But that moniker spans a wide range of expertise, so the range of compensation seems about equally wide.
London salaries tend to be a bit lower than NYC. (At least in tech.)
This is a very long and in-depth explanation, thank you. The thing about passing information up and down the call-stack is a perspective I hadn't considered.
That's a consequence of `example` being recursive. Try to get the same result with a non-recursive implementation.
Sure, but my entire point of confusion was due to a recursive example. My recursive parser used state, the recursive parser on stack overflow used reader. Which is more correct?
 I just posted a comment there on what I plan to talk about: &gt; My presentation is going to be about two things: first, the syntactic approach to implementing STLC (by proving that it is strongly normalizing with normal evaluation order) using the relevant chapter of Software Foundations as our guideline; and second, about my current research on generalizing this approach to arbitrary languages with simple enough type systems. This is an open-ended topic of course, and for the second half what I have are more ideas than worked solutions. I am hoping to make it into a bit of a workshop / open discussion, not just a talk. 
Videos from previous meetup: Moritz Angermann: [What's new in cross compilation](https://youtu.be/46A02obKt8g) Mietek Bak: [Informally about formal methods](https://youtu.be/8Q7Up-JToWM)
Thanks for the well-deserved praise of the community for the resolve, cooperative spirit, careful thought, and hard work that made it possible. But please note that this was definitely **not** a design bug. It was something that needed to be changed much later due to progress in other places. At the time, it was not only the correct design, not only an *excellent* design, but so good that it had a major influence on just about every other programming language. Keep in mind that the `Applicative` class didn't even exist then.
I'm trying to figure out what you're trying to do here. In particular, I'm sure you do know the obvious answer to your question "why do you need Monad to be able to fail": we don't, and "fail" doesn't mean that the monad failed, it means a pattern match failed in do notation. Are you trying to generalize to `ApplicativeDo`?
&gt; but it also introduces a global thread pool, which you absolutely do not need for this actually I have tried to use `parallel_` but the server get disconnected after few request, maybe this has something to do with the global thread. &gt;which takes two IO actions and runs them, well, concurrently, blocking until both finish actually I am trying to host REST API and some static html file so I am not sure that the action should finish(the server should stay running) or maybe it will finish when an exception occur! 
Reader and State are convenient, but don't have any special powers that normal function calls don't. Reader transparently turns a function of type `a -&gt; b` into a function of type `(a, s) -&gt; b` (practically) That is, every time you call a function, it sticks an `s` on under the covers. The Monad implementation for reader just passes the same `s` to every function. State turns a function from `a -&gt; b` into `(a, s) -&gt; (b, t) `, so when you call a function it passes an extra input in and collects an extra output out. The Monad implementation gets the `s` from one function call from the `t` of the function call before it, forming a chain where my transparent input is the previous transparent output, so I can "see" what changes it made. That's all those things do, and you can just write functions with those types explicitly, and thread the state yourself, and get all the same power. So `local`, for Reader, just passes a different implicit input, rather than the same one it would have passed to everything else. So that function call sees a different `s`, but since it still doesn't add anything to the return, that new `s` doesn't continue to be passed into the next function in the Monad. It is used for that one call, then goes away. The rest of the functions in that do block continue to see the same `s` as before. Now, that's what makes it different than State, but if you never use that State in a second call after the first (the `t` above), and if you'd implemented state using normal functions wouldn't have passed it into anything else, then you didn't really need State anyway, and they become functionally equivalent for your needs. To reiterate, these Monads are conveniences and that's all. They aim to reduce errors by taking the mechanical part of threading the return of one function into the input of another and making it something the computer does, rather than something the human programmer has to do, which reduces code complexity and possibilities of errors where I accidentally put the wrong variable in and "went back in time" to a previous state I didn't mean to. If you'd like, you should be able to write exactly the same algorithm you have without any Monads at all using the conversions I wrote at the top, which should make it much more clear what you are actually getting out of them. 
so in my case while I need both web server to run simultaneously, I do need concurrency right 
The women don't look too rough either.
It gives you nice guarantees. For example, traversing an AST - each "lambda" you see introduces a variable into scope as you recurse into the subtrees in the AST. A Reader monad for scope guarantees that variables don't "leak" into sibling scopes. A "State" monad needs special care to avoid it. Additionally, with reader, it's safe to run multiple computations concurrently (true for writer too, as `mappend` is associative). It also has more capabilities, for example, reader monads support "unlifting" (see Snoyman's blog posts). My reason for using Reader, Writer, and State and not just one of them -- is that the flow of data in my architecture is visible from these types. I don't need to read all the code to know that some of my data only flows "inwards" (potentially changing on the way in). Some data only flows "outwards" (writer monads, potentially changing on the way out), and some is threaded through ("state"). Just like knowing the data structures of some algorithm gives more information about the algorithm quickly than reading a similar amount of code -- knowing the information flow encoded in the monad transformers gives you a lot of information about how the algorithm works or even CAN work.
And taxation is a dream.
The original version of my code actually didn't have any monads, and had a function that looked something like this: simplify :: [Expr] -&gt; Expr -&gt; Expr where `[Expr]` was a list containing the parse history gained from previous recursive calls to `simplify`. I figured that instead of lugging `[Expr]` around everywhere, I'd wrap it in `State`. And it worked! I just changed the code to use `Reader` instead, and it still works, albeit with some minor changes to some function calls. I have a hard time figuring out _why_ it works. I get that `Reader` is in some ways just another name for a function, so the act of writing foo :: a -&gt; Reader b c is effectively the same as writing foo :: a -&gt; b -&gt; c meaning bar :: a -&gt; State b c would be effectively the same as writing bar :: a -&gt; b -&gt; (c, b) I guess since I never needed to actually return the state in my computation, but only needed to hold onto it so it could be passed around recursively, Reader was more than enough. ... or am I getting this entirely wrong?
My advice is to design a simpler programming language which doesn't _need_ a type checker, so that all your genetic mutations are guaranteed to produce a program which is still valid. Ah, if only there was a standard way to encode arbitrary functions as a vector of floats... :)
&gt; actually I have tried to use parallel_ but the server get disconnected after few requests, maybe this has something to do with the global thread. Sound unlikely, there's probably something else going on there. &gt; so I am not sure that the action should finish No, that's correct, it should not finish - but note that "blocking until both finish" in this case amounts to "blocking forever". And in fact, since *both* handlers are supposed to run forever, it doesn't even matter whether you use `race_` or `concurrently_`. &gt; actually I am trying to host REST API and some static html file Is there an important reason why you need to host them on separate ports? Because you can combine several Warp applications into one with just a bit of routing glue - just look at the request, if the path starts with `/api`, run the API application, otherwise run the static file application.
Might also be http://iohk.io/ Lots of these companies. Letâs see which ones are still hiring next summerâ¦ :-)
idk how subtraction works on records, but if we're assuming extensible records, why not just build them up? type MonadReader e m = { MonadAsk e m } + { local ... , reader ... }
Might even much more likely be http://string.technology/
Yeah, you're basically right. Your original function only took in an input you wanted to factor out, so you only needed Reader. State gave you an input and an output, but since you always ignored the output anyway, it didn't provide you with any advantage. Did your code get simpler or more straightforward when you ported it to Reader? The advantage of Monad is the way it allows you to chain multiple functions within a context, and leaves the handling of the context up to the implementation of Monad. If you were previously just running a function recursively, and you still are, and you have to modify the accumulator manually before you recurse using local, is using Reader getting you anything? 
I think we may disagree on the definition of a design bug. When Monads were invented essentially there were only what we would call today MonadPlus and Functor. It took time to come to a better understanding of the hierarchy of typeclasses and to understand the advantages of the intermediate ones. That certainly means that how the design bug came to be is quite understandable. It doesn't mean it wasn't one. Imperfect knowledge can lead to design bugs. 
I think other people have already done a pretty good job answering this, but I wanted to make sure this gets conveyed: the **monad instance** for `Reader` cannot change the environment. When you see this: m1, m2, m3 :: Reader Foo m3 = do a &lt;- m1 b &lt;- m2 return (combineFoos a b) You can be certain that the `&gt;&gt;=` operator that glues `m1` and `m2` together is not allowing the first computation to somehow taint the environment that the second one receives. In fact, the above definition is provably equivalent to: m3 = do b &lt;- m2 a &lt;- m1 return (combineFoos a b) With `State`, you do not have this guarantee. But, always keep in mind that you never really *need* `Reader` or `State` for anything. You can always just thread the environment (or the state) explicitly. The two aforementioned data types, along with all their typeclass instances, are useful as a convenience in some circumstances, but strictly speaking they're never truly necessary. 
I would generally shy away from jobs as these where the salary range is this large. It implies to me that they donât know what they really want. And writing code for people who donât know what they want tends to be a shit show. You can use job descriptions and salary ranges to determine whether thereâs some sense of competency on the other end. This one reals of none.
Wait why is it called eta if it's literally Haskell but on the JVM. Cool though!
Simpler? Not at first. My original version was written something like this: simplify :: [Expr] -&gt; Expr -&gt; Expr simplify olds new | new `elem` olds = new simplify olds new@(some pattern) = ... -- where I had this one nice case at the top that checked if the new context had already occurred somewhere in the simplification process or not. If it had, then the recursive function had entered a loop, if not, then it could continue trying to simplify. This did present a problem, using the monad, I no longer had that nice guard at the top. I got around that by doing this: (#) :: Eq a =&gt; a -&gt; State [a] a -&gt; State [a] a new # body = do old &lt;- get if new `elem` old then return new else do put $ new:old body which then let me do simplify :: Expr -&gt; State [Expr] Expr simplify new@(some pattern) = new # do ... Getting around the horrible boilerplate. After switching to Reader, it instead looks like this: (#) :: Eq a =&gt; a -&gt; Reader [a] a -&gt; Reader [a] a new # body = do old &lt;- ask if new `elem` old then return new else local (\old' -&gt; new:old') body 
Sadly, this is not the case. AccumT is implemented as newtype AccumT w m a = AccumT (w -&gt; m (a, w)) so it _looks_ like it is using the state monad under the hood in order to implement the Writer API. Unfortunately, the output `w` is _not_ the input `w` plus whatever was [`add`](https://hackage.haskell.org/package/transformers-0.5.5.0/docs/Control-Monad-Trans-Accum.html#v:add)ed during the computation. Instead, the output `w` _only_ contains what was `add`ed: -- | -- &gt;&gt;&gt; runAccumT accumDemo "[initial]" -- [initial] -- [initial][replaced] -- ((),"[replaced][replaced again]") accumDemo :: AccumT String IO () accumDemo = do w1 &lt;- accum (\w -&gt; (w, "[replaced]")) liftIO $ putStrLn w1 w2 &lt;- accum (\w -&gt; (w, "[replaced again]")) liftIO $ putStrLn w2 Note that the final output does _not_ include `[initial]`. As a result of this design, the implementation of still leaks: add :: (Monad m) =&gt; w -&gt; AccumT w m () add w = accum $ const ((), w) m &gt;&gt;= k = AccumT $ \ w -&gt; do ~(a, w') &lt;- runAccumT m w ~(b, w'') &lt;- runAccumT (k a) (w `mappend` w') return (b, w' `mappend` w'') That is, unlike `Control.Monad.Trans.State.Strict.modify (\s -&gt; s &lt;&gt; x)`, `add x` does not force the accumulated state, so in a sequence of `add`s, the `w` and `(w \`mappend\` w')` thunks are never unevaluated, and the `w' \`mappend\` w''` computations only get executed at the very end.
documentation is better done by the community than by ghchq. and while a structured format for error messages and code analysis from ghci will help a lot, the same is true for editor tooling. 
I don't think there would be a reliable way of doing this if you consider it to be a correctness requirement that this optimization takes place. You could try overlapping instances or rewrite rules, and they might be useful under some conditions, but if you rely on it in production code, you might be disappointed when one day an irrelevant innocent refactor somewhere in your codebase causes it to stop being efficient, and your product starts failing under real load. The bottom line is: the (`Monad`) law is the law! That said, I think it's perfectly OK to use `fromOrdList [x]` whenever you need to be sure that the optimization takes place. Isn't that the point anyway? If you rely on some behavior, it's much better to specify it explicitly.
[removed]
&gt; Why use Haskell to generate pictures on the printer? Just because Functors, Applicatives and Monads are obscurely cool! This is pretty much the exact reason I use Haskell for absolutely every project I do.
[removed]
First of all, sorry, I meant "why do you need the Monad constraint in order to be able to fail". I'm actually trying to do three things here; * Yes, generalize `MonadFail` so that it can work with `ApplicativeDo` (a possible future `ApplyDo` even) * Liberate `MonadFail` from pattern match failures (so it can be used with more error conditions in the future), by making `MatchFailure` explicit. * Make `MonadFail` useful outside debugging. As it currently is, you get a `String` that explains (arbitrarily) in English that a pattern match has failed. I really doubt that string will ever be useful in a production codebase. Instead of an implicitly generated English explanation, we had a data type that explicitly represented what happened, we could actually interpret it in such a way that the pattern match failure is also useful and is a part of the expected code path (I'm not saying this should **always** be the case, I'm just saying that currently the `String` in `MonadFail` can **never** be useful outside debugging).
static types aren't a fundamentally limiting issue, giving the popularity of JavaScript and Python, neither are functions, given the popularity of Java. that doesn't say much.
So, you lost the ability to pattern match your base case, but then replaced it with a base case operator you could put in each case which made it OK. After that, though, did the actual logic become cleaner due to the presence of Reader? Is it easier to read now, or clearer to reason about? Are you less likely to make a mistake now compared to before? 
Yeah you're looking for concurrency. See the async package and my other reply for this case. 
By "certain elements" do you mean some responses from the survey? I haven't heard anyone call it fundamentally limiting, SemigroupMonoid is just one of the many things you have to fix over time to keep things from getting worse. 
I just think it's instructive to consider the two cases separate. In particular, as Simon notes, it makes perfect sense to run a concurrent program on a single core. 
Yeah, sorry, I thought we were speaking to 'what does the ecosystem need' as a larger concept, as opposed to 'what do we need out of ghchq.' You are absolutely right, very little of what I called out above has much, if anything, to do with ghchq. This reflects my position that GHC and the language itself are in a strong position right now and in my opinion are not limiting factors in language use or adoption.
What do you mean you have had better luck with inference? Associated types infer the same way fundeps do.
 Thanks for comments! Should have clarified: - The project's implementation is currently entirely Haskell, as reference client (except crypto stuff in C/asm). We have quite a few Haskell fans on the team :) - The domains listed (compiler, crypto, distributed computing, vm/wasm), as we're interested in engineers with *any of these* deep domain expertise. The Blockchain project itself is complex and spans across all these layers, so we're quite aggressively expanding the engineering team. - Yes :) See http://dfinity.org/team, and http://github.com/dfinity - IMO the salary range listed are competitive (and they should be), but quite reasonable, at least for senior engineers by Bay Area standards. Token compensations are in addition. 
&gt; s there an important reason why you need to host them on separate ports There is no specific reason at all to host it on a different port. I did not figure out the way to combine several Wrap application into one so I though about making 2 web server. the thing is how to look at the request? I am building my REST with servant which give me the `Application` that I supply to the `run` function and I am not handling the request explicitly ( servant is doing this).
yeah, I agree. As excited as I am for dependent types over the next few years, GHC is powerful enough that I'm happy to use an older compiler version when some dependency requires it. Ironically, Haskell "IDE"s (I use ghcid) are worse than many dynamic languages, despite the static names and rich (newtypes and sum types in particular) static types, which enable precise editor features like refactoring and call graphs. I really want an IDE, but we just have to take the time to write it I guess :-)
Reach out as per the directions and we can chat 
&gt; why does the failure have to be a String Because `x &lt;- Nothing :: Monad Int` doesn't lend itself to a polymorphic failure type.
&gt; I have also read that if you are lifting you are doing it wrong I'm not sure what you mean. Can you point us to a particular example? Perhaps this will help: Did you know that `liftA` and `liftM` are identical to `fmap`?
&gt; What do you mean you have had better luck with inference? Associated types infer the same way fundeps do. That has not been my experience, but I don't have anything concrete I can show you right now.
I don't think anything's _wrong_ with `liftA2` and `liftM`, but using Functor and Applicative would certainly be more idiomatic: (:) &lt;$&gt; rollDie &lt;*&gt; rollNDice (n-1) fmap (x:) (rollNDiceM (n-1)) (x:) &lt;$&gt; rollNDiceM (n-1) those mean the exact same thing as the `liftA2` and `liftM` version, they are simply a more common syntax for them which also happen to work in many more circumstances.
[lifting is probably wrong](http://blog.ezyang.com/2013/09/if-youre-using-lift-youre-doing-it-wrong-probably/) I don't have a problem with lifts per se, I just wanted to better understand the reasons why some people recommend against it, and what the current opinion is. I often find that I am reading advice in Haskell that is out of date, because development of the language seems to move faster than I do. 
The rest of the code remained mostly the same, but I didn't gain much, other than fewer parameters to keep track of, which in and of itself is a benefit, I'd say
That article is about `lift`, not `liftA` or `liftM`. It's completely unrelated to what you are doing!
Either way may be sensible, my thinking is that we start with large constraints (`Monoid` = `mempty` + `mappend`) and many years worth of instances and I would like to disturb that a little as possible. I would want the type of `ask` / `(&lt;*&gt;)` to remain ask :: MonadReader r m =&gt; m r (&lt;*&gt;) :: Applicative f =&gt; f (a -&gt; a') -&gt; (f a -&gt; f a') with some special way of selecting from a dictionary, thoughts? askAlgebraic :: MonadAsk r m =&gt; m r askAlgebraic = MonadReader.ask -- :: (ask::m r) â MonadReader m r) =&gt; m r (&lt;.&gt;) :: Apply f =&gt; f (a -&gt; a') -&gt; (f a -&gt; f a') (&lt;.&gt;) = (Applicative.&lt;*&gt;) -- :: ((&lt;*&gt;)::f (a -&gt; a') -&gt; (f a -&gt; f a')) â Applicative f) =&gt; f (a -&gt; a') -&gt; (f a -&gt; f a')
``` rollNDice = replicateM ```
How is the deployed build compiled? Are you sure it uses the same optimization flags?
I've previously said: &gt; The best developers I know - those whose code I enjoy looking at and working with - extract monad operations and provide an API with all the primitives complete so I don't need to think about the structure of the monad while I'm thinking about the structure of the logic I'm writing. So to me, code with lots of `lift expr` is just code that is in a monad that isn't well thought-out. Separate the logic of the monad you need from the function you are trying to write. I said a few more words on the topic previously: https://stackoverflow.com/questions/46803597/monadrandom-state-and-monad-transformers/46804058#46804058
 class Foo a b | a -&gt; b And class Foo a where type FooT a Are equivalent when it comes to type inference. In fact I would go so far as to say that if you have found situations where the type family inference fails you should file a bug report with GHC. IIRC there was some discussion of directly desugaring: class Foo a b | a -&gt; b To: class (FooT a ~ b) =&gt; Foo a b where type FooT a Internally.