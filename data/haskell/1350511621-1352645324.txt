Hm, this got me thinking. Suppose we wrote the same code you wrote here, but using `do` notation and the `Cont` monad. qsort' :: Ord a =&gt; [a] -&gt; [a] qsort' xs = runCont (qsort xs) id qsort [] = return [] qsort (x:xs) = do ls &lt;- qsort $ filter (&lt; x) xs rs &lt;- qsort $ filter (&gt;= x) xs return (ls ++ [x] ++ rs) Interestingly, *we didn't need to use any Cont primitives*. We used exclusively the Monad interface for `qsort`, so its type can actually be `qsort :: (Ord a, Monad m) =&gt; [a] -&gt; m [a]`. Now consider this: qsort'' :: Ord a =&gt; [a] -&gt; [a] qsort'' xs = runIdentity (qsort xs) It comes up with the same result! The Monad interface abstracts over implementation details, such as whether or not "tail calls" are used.
Good point. The question originated from the point of view of bringing sanity to insane uses of regexes by using an actual parsing framework. Since most of what you want with a regex is either to recognize "does this match the format?" (syntax validation) or "break this apart for me" (echo chunks to output), it didn't strike me as much of a problem. If you want to actually do anything substantive as part of the parsing, the actions do become an issue, but you could stay pretty portable by making most of the actions function calls. You can see this as growing the "built-in" actions to be more complex than just ECHO. Reimplement your action library, fix up syntax differences, and you're basically good to go. Now, if something just existed like that…
This is what I came up with: import Control.Lens import Control.Lens.Plated import Control.Comonad.Store newtype Sum a = Sum {unSum :: a} deriving (Show) _sum = iso unSum Sum wiggle n = [n - 1, n, n + 1] wiggleSum = concatMap (experiment wiggle) . holesOf (_sum.traverse) -- &gt; wiggleSum (Sum [10,20,30]) -- [Sum {getSum = [9,20,30]},Sum {getSum = [10,20,30]},Sum {getSum = [11,20,30]},Sum {getSum = [10,19,30]},Sum {getSum = [10,20,30]},Sum {getSum = [10,21,30]},Sum {getSum = [10,20,29]},Sum {getSum = [10,20,30]},Sum {getSum = [10,20,31]}] How am I doing edwardk? (P.S. this module is not tested. I simply translated from a one-liner that I wrote in ghci. Results not guaranteed). Edit: Hmm, this isn't as lazy as I'd like.
I like it. =)
CPS is the functional-programming equicalent of "blowing up the stack" were you keep an explicit stack data structure reifying your control flow. Except that you don't have an explicit stack and use heap-allocated closures for that instead.
&gt;The instances realize the fact that, for example, the (,) and Either types are both independently covariant in both their first and second arguments. Can someone explain this to me. I only comprehend (co|contra)variance in respect to OO/existentials. I do not see how this use of the word has anything to do with treating a type as part of a super-set or sub-set. 
Edit: I'm wrong - see below Like most of the non warm/fuzzy terminology used in Haskell posts, this is the categorical meaning of "covariance", and doesn't have much to do with the OO meaning. Contravariance turns arrows around, and looks like this: http://hackage.haskell.org/packages/archive/contravariant/latest/doc/html/Data-Functor-Contravariant.html When you have a bifunctor or more, it does not need to be consistently covariant / contravariant in all its arguments, though I'm not sure what this would look like in Haskell..
He means covariant as in "covariant functor". I.e. if a type variable of T is covariant, then there exists as operation `f` of type `(a -&gt; b) -&gt; (T ...a...) -&gt; (T ...b...)` where the dots indicate some other type parameters on both other sides. A type variable might also be contravariant, meaning that we can write `(b -&gt; a) -&gt; (T ...a...) -&gt; (T ...b...)`. The left hand side of a function arrow (as in the `a` of `a -&gt; Int`) is a common example of this. Or, as the article notes, a type variable might be otherwise "indexed" and not functorial at all.
I disagree with your disagreement. mutation/destructive state can be very confusing to beginning students, and the need to introduce pointers early on and a strong distinction between ideas of "value" and "reference" can drown students in particulars when they should be learning precisely natural things like recursion :-)
Actually, the co and contravariance involved here actually *do* directly correspond to the contravariance of arguments and covariant subtyping of immutable container types. 
(-&gt;) is contravariant in its first argument and covariant in its second.
I guessed that, but being the first time I fiddled with such a large file, I opted for small, "dumb" passes instead of pushing my luck with only one regexp :)
Hmm yet another lens library?
How do you do code review without the word fuck?
Whoah, no way. Teaches me to prattle on about cats! My excuse is that I was misled by the "CovariantN" typeclass name in the context of functors. Should've actually read the post instead of skimming! For now I'll have to stick it on my to-read pile. Liking what the look of these recent blog posts!
Yeah, it's a pretty old article, but it is how I learned to use comonads. A similar exercise would be building Pascal's triangle comonadically. :)
Long story short: you can think of subtyping `A &lt;: B` as denoting a *specific* way to turn a `A` into a `B` by doing "nothing at all". If you generalize (in a reasonable way) this idea to richer kind of transformations from A to B, that is the function type `A -&gt; B`, you get the notion of variance used in Haskell. Haskellers come to this from a different point of view, category theory, where the notion of variance of a functor has a formal sense explained in a different way, but the two notion really do coincide: if you take a category where morphisms between objects are given only by the subtyping relation, rather than any kind of functions, you get the same notion of variance languages with subtyping have. The good thing about the categorical point of view is that it insists on the laws that `fmap` must satisfy (respect identities and composition), that are important for reasoning in general. On the contrary, nobody speaks about them in subtyping context because the fact that subtyping transformations are identities make these laws trivial.
You forgot about: instance Foldable Tortilla where ...
head
The `String` type is an alias for `[Char]`, a list of `Char`s. This means it's of the form `c:cs` where `:` is the cons operation for lists. So you can get the first element either by pattern matching or by using [the `head` function](http://www.haskell.org/ghc/docs/latest/html/libraries/base-4.6.0.0/Data-List.html#v:head). Note that `head` is a partial function, so you'll get a runtime error if you attempt to use it on an empty list.
Did not know this. You just solved me some pain in the future
It's called headErr. Here it is: http://hackage.haskell.org/packages/archive/errors/1.1.1/doc/html/Control-Error-Safe.html
This is a absolute beginner question, you should probably read some introduction first. Now, let's take the opportunity to raise a related question : what output would you expect if the string is empty ?
Honestly speaking, I haven't bothered checking the rationale behind it, as I have only manipulated the existing code. From a quick look, though, I guess is IO [()] because every string generation yields a (), and being replicateM defined as ```replicateM :: Monad m =&gt; Int -&gt; m a -&gt; m [a]``` this is the reason. Using replicateM_ would have yield IO (), I guess.
listToMaybe
Do you have any good citations on the categorical relationship? I looked at a bit of reynolds and cardelli and didn't see it spelled out.
Indent by four spaces to get a source code view. st :: String -&gt; Char st (h:t) = h Also, that's dangerous, and prelude already has `head`.
A data-lens, `l :: Lens a b` can be converted to a `Simple Lens a b` by the section `(l ^%%=) :: Simple Lens a b`. P.S. I didn't test this. P.P.S. `^%%=` is not a name that I invented.
Or `headMay`, which is re-exported from the `Safe` package. Oops: I meant to reply to Niriel.
Or [listToMaybe](http://hackage.haskell.org/packages/archive/base/latest/doc/html/Data-Maybe.html#v:listToMaybe) from Data.Maybe.
You'd think, but that's the whole point of the contravariance of the first argument of the function arrow. Think about substitutability a bit. If I have a context that can use a function taking the empty record, that context *can't* use a function taking a record that requires a field. But if I have a context that can use a function taking a record with one field, that context *can* use a function that takes a record with no fields. So a *subtype* of a function is a function that takes a *supertype* of the arguments and returns a *subtype* of the results. 
Thanks for the Scott reference in particular. I suspect there's something to do with limits/colimits of cartesian closed categories in general going on, but haven't fleshed out the intuition more.
Does inlining actually do the right thing there? it seems like leaving `charRangeStart` and `charRangeEnd` uninlined would avoid recomputing the `Word8`, but inlining them (as the author did) would actually recompute the values every time. Edit: I guess they don't recompute every time, but the inline pragmas don't help either.
I've attempted to generalize U and U2, but couldn't quite manage it. This resulted in two StackOverflow questions: * [Writing cojoin or cobind for n-dimensional grid](http://stackoverflow.com/questions/12963733/writing-cojoin-or-cobind-for-n-dimensional-grid-type) * [Are type family instance proofs possible?](http://stackoverflow.com/questions/12960374/are-type-family-instance-proofs-possible) Here's the dump of all the working code I have: http://hpaste.org/76460
I didnt see any mention on the site, and theres a post from april 2012. Are you sure the sites been abondoned?
The `take 1` is unnecessary.
`replicateM_` is also faster and won't leak space.
Point. I started with take 1 and then changed it without really thinking.
That's because it does a hell of a lot more, and does it very differently.
Shameless self-publicism alert. darcs get http://personal.cis.strath.ac.uk/conor.mcbride/CS410 and attempt Prac1.hs possibly after reading [this SO answer](http://stackoverflow.com/a/12361200/828361). Edit: late night dimwit meant darcs get, of course.
Possibly look at: * http://hackage.haskell.org/cgi-bin/hackage-scripts/package/vty * http://jtdaugherty.github.com/vty-ui/ * http://www.stefanwehr.de/software/#hscurses * http://hackage.haskell.org/cgi-bin/hackage-scripts/package/nanocurses
(n)curses or perhaps slang is the standard language-agnostic answer for "how do I do fancy things with the terminal". If you're looking at writing a somewhat in-depth text editor, I've heard good things about _[The Craft of Text Editing](http://www.finseth.com/craft/)_, although it's definitely written with a C/imperative bent. I haven't made it all the way through that book.
[ncurses](http://hackage.haskell.org/package/ncurses) definitely worked for me. However, regardless of what terminal option you use, prepare to spend a lot of time simply managing the terminal before you can get to implementing your actual problem.
Most of those posts probably came from boredom in his previous job.
I just met Dan today for the first time and he said he will probably blog more in the near future.
Ok, ok, I'll fix the article this morning :D Edit: Fixed.
Inline (in any language, including C++) means to substitute the definition in question into the use site. This is usually (in other languages) to avoid function call overheads, improve locality, and in GHC in particular, to make rewrite rules fire where they might not otherwise. In your case, you're substituting an expression whose value will never change (`c2w8 'a'`) into the place it's used. It just means that GHC will end up compiling the equivalent of `uniformR (c2w8 'a', c2w8 'z') g`. Without inlining, the top-level values would be unevaluated thunks, and the first call to each of them would force the computation, and afterwards you would just get the `Word8` directly every time you use it. With inlining, you're performing that `c2w8` work every time it gets called. Except that in this case, it only gets called once, so it should make no difference at all. Anyway, this is all moot, because `c2w8` costs essentially nothing :) all it's doing is truncating a number, so even if you recompute it, the cost is almost zero. Edit: I see the comment on -cafe about the inlining making a huge difference. I'd be curious to see if he peeked at core to try to explain the huge difference. Unless I'm misunderstanding something, I don't really see how that would happen.
Although it may not make any difference here, thanks for the explanation. It's always good to dig "under the hood" instead of merely say "it works". I'll core-dump the program to see what's going on :)
I think this could be a great idea. Why reivent the wheel? We have yi, so any enhancement to what tries to be the "de-facto" Haskell editor would be awesome, I guess :)
I prefer something like: parseSum = char "+" *&gt; skipSpace $&gt; Sum &lt;*&gt; parseAST &lt;*&gt; parseAST Parens are scary. =]
Downvoted with no comment, it is hard to see what is wrong with this reply. Am I the only one who read "get the first char of a string" as `String -&gt; Char` (which is necessarily partial, hence my other comment below) ?
I'm in the midst of writing a [high-level terminal ui library](http://github.com/startling/curslet); you might be interested. 
There's gold in them there dissertations! Thanks much for pointing to Fisher's PhD. In the paragraphs I've read so far, she explains the OO concepts — away from which I have steered my studies — and their interplay clearly and succinctly.
&gt;they have different preferred template and html generation systems That doesn't seem like much of an obstacle, neither is tied to a template engine, and both "preferred" systems work on the "opposing team's" framework. &gt;different preferred persistance solutions I didn't think either one had a preferred solution there? Don't both frameworks encourage people to use whatever fits their needs best? I realize acid-state comes from happs, but I got the impression that I was just as encouraged to use hdbc as acid-state, depending on what I needed. &gt;different approaches to modular extensibility As in snaplets vs something? What does happstack do for this area? 
Happstack favors stronger compile-time guarantees and a richer feature set, while Snap is focused on having a smaller, easier to learn API. This shows up in a lot of places. For example, The core Happstack monad is the ServerPartT monad transformer. Snap however, only offers a Snap monad.. no SnapT monad transformer variant. Happstack has focused a developing type-safe route combinators (going back before Yesod even existed). We've built a number of systems to try to make route matching composable and extensible. Snap uses a simple string based routing mechanism with no type-safety guarantees and numerous ways it can fail at runtime. Additionally, the string based method is not really extensible. But it is easy to understand, use, and will get the job done. In Happstack, we have focused most of our templating efforts on HSP which provides a fair amount of type-safe and compile time guarantees. Snap has focused on Heist, which makes it possible to edit some aspects of your templates with out having to recompile and restart the server, but at the expensive of certain compile time checks. If you look at the APIs provided for looking up request data (such as values in the query string and request body) you will find that Happstack has a very rich API with support for converting the string values to different types, accumulating errors, etc. Snap sticks with a very basic API. For the underlying HTTP engine, Snap is actually working to get away from left-fold enumerators and go back to the plain old IO monad. For Happstack we are still considering pipes (though we need pipes 2.5 to make a final evaluation). There is, of course, a lot of value in having a simple to learn API. That is why we created happstack-lite, which contains the essential subset of Happstack. The happstack-lite API focuses on having one simple way to do everything you need to do. When it doubt, we looked at the Snap API and removed anything that Snap didn't have :) Fortunately, if you need some of the more advanced Happstack features, you can just import them, since happstack-lite is merely a subset of happstack. So far the best method of collaboration has been building reusable independent libraries. Things like heist, hsp, acid-state, web-routes, etc can be used with both Snap or Happstack. Though, for Happstack we have gone the extra mile and created extra glue libraries where needed, and created documentation in the Crash Course to make it easier to use things like Heist, Hamlet, etc, with Happstack. (To be fair, snap does have an acid-state snaplet). Going forward with Happstack, we are looking at making it even less monolithic. For example, we use web-routes for type-safe url routing. And web-routes is already a stand alone package that can be use with Happstack, Snap, etc. In Happstack 8, the non-type-safe url system will be getting a major overhaul. Rather than embed that in the core happstack code, it will also be standalone, much like web-routes. Also, we are finally going to split the HTTP backend out as a standalone library. So, this should make it even easier to mix and match at a low level. Of course, that also makes it hard to build reusable components. That is why we have the happstack-foundation library and clckwrks, which aim to create platforms for reusable components by picking which components to use and providing some standardization. There was a time where I tried to get the Snap developers to refactor their packaging a bit so that Happstack could more easily use the Snap HTTP backend, but they were not really interested. Perhaps the new backend they are working on will be more reusable. Though, at this point, I am pretty committed to writing a new backend anyway. We did also do a bit of collaboration on the code for managing timeouts, though I ultimately ended up using a modified version of the timeout code from warp. The most interesting aspect of Snap, IMO, is the Heist stuff. Personally, I don't use Heist, but it does fill a specific niche and does it pretty well. So I am definitely committed to ongoing Heist support in Happstack (which requires very little effort anyway). The Snaplet stuff has some interesting ideas as well..but it doesn't meet the requirements that we need for something like clckwrks. So, in summary, we can't directly collaborate because Snap wants smaller, easier APIs where Happstack wants richer, fuller APIs. And for the key aspects like routing, templating, etc, we are headed in different directions. The good news is that we are not doing much duplicate work either. For example there is room for both Heist and HSP because they address different sets of requirements. It's not like Snap is trying to build a competitor to web-routes. The other place we can potentially collaborate is on the really low-level server performance issues. For example, the new GHC IO manager is great.. but doesn't really scale to multicore (though people are working on that). And there are lots of low-level performance tricks (like TCP_NODELAY, sendfile tricks, etc) that we can discover and share. That is why I created acme-http. It is intended to be a very simple, non-production HTTP backend were we can test and record the best practices for that low-level black magic. 
Video is for learning thing you didn't know you needed to know. Text is for learning things you know you need to know. I say this because of ctrl+f.
That's the best (as in most comprehensive) summary of snap vs happstack I've seen, you should publish that somewhere so it doesn't get lost.
This talk was given as the welcome keynote at http://reaktordevday.fi/2012/ From the program: &gt;Fast Code Nation: The Bright Side of High-Level Languages &gt; &gt; Bryan O'Sullivan / Facebook &gt; &gt; It's an article of conventional wisdom that languages with a lot of expressive power offer an unhappy tradeoff: you gain ease of development, but lose runtime performance. In this talk, we'll look at real-world performance concerns in two very different languages, Haskell and Python. We'll cover some useful and occasionally surprising tricks and tools that help us to write faster code, and what happens when we break out the "big guns" of rewriting code in C. 
Have more fun with your comments and exercise a wider imagination. “Fuck” is so passé, forgettable. Here are some examples one can start with instead: “Foul”, “abominable”, “inelegant”, “pig disgusting”, “Lovecraftian horror”, “poor imitation of a program”, “Beelzebub himself hath not seen such filth”, “fatally bad”, “eye poison”, “something an uphill ant with asthma would outperform”, “in need of various religious rites performed thereof before it can be deleted and erased from history”, “the monkey that thought he could”, “most unclean”, “Satanic verses”, “dark incantations”, “PHP-esque”, “unimaginably without precedent”, “platonic primordial soup”, “insidious”, “binders full of bad lines of code”, “darkness”, “interdimensional miscalculation”, “code of the beast”, “something wicked this way compiles”, “unimportable”, “it keeps asking me if I'm the keymaster”, “someone crossed the streams and this is what resulted”, “illegitimate spawn”, “algorithmically destitute”, “abandoned depravity”, “Brendlefly”, “ocean of slime”, “The Da Vinci Couldn't”, etc. That's if you think the code is OK. If you think the code is great: “Not bad”, “can't see any problems so far”, “suspiciously unbroken.”
&gt; CPython has so-so performance analysis support. &gt; Its cProfile module measures every function call. &gt; Measurements are expensive enough to distort performance Just as they are in Haskell, aren't they? However GHC records separately time spent in your code (MUT) and time spent in profiling functions? Is that CPython lacking what GHC provides or is it me misunderstanding something about GHC's profiling facilities?
Shouldn't it complain when I activate them, then?
Yes, but don't forget we're using mwc and it has apparently to run in IO or strict ST. Data.ByteString.unfoldrN takes a pure function. Meh, this is a problem... it's weird ByteString doesn't provide a replicateM function, like Vector does.
But you forgot to remove the '_ &lt;-' and the 'return ()'.
I deliberately left them, because I was in a hurry. Since I have to tweak it to remove the inlines, finally I have the weekend to ehance it a little with the help of the loyal unfoldrN :)
Whoops, forgot to use constraint kinds. Anyway, it doesn't work on the old ghc even when I do use it. I guess I'll have to install 7.6.1...
The Haskell community is extra concerned with making sure newbs^1 feel comfortable, so the kind of jokes that start looking like criticism receive a negative reaction. ^1 [EDIT]: or anyone for that matter
I believe the problem that vty-ui is intended to solve is exactly the one of "spend[ing] a lot of time simply managing the terminal before you can get to implementing your actual problem." I know the author, but I've never tried the library.
I agree that the community sometimes can be a bit too sensitive, but *that* really was a bad joke.
&gt; Edit: Traditionally (.) takes arguments in different order h b c -&gt; h a b -&gt; h a c Yes, but I don't like that order.
That awkward moment, when you wonder if you should downvote for inappropriateness wrt the topic, or upvote to salute the somewhat unexpected, funny and refreshing candidness.
Lazy IO is problematic, but that's not the only conceivable use of lazy Text values.
I found this an excelent companion to a [comment](http://www.reddit.com/r/haskell/comments/10q2ls/the_usual_type_signatures_for_foldl_and_foldr_are/c6gaup2) psygnisfive left a while ago.
PrancingPeach actually faces an interesting problem here, since (s)he is trying to actually implement a text editor. You've got your raw terminal access on one end, and code that essentially implements &lt;textarea&gt; on the other thus simply solving the target problem, while at the same time possibly functioning on a level of abstraction that may actually make it _harder_ to write your own text editor. I'm not sure if there's a great solution there.
I think lazy IO is the predominant use since once you have the entire ByteString in memory you don't gain much by separating it into chunks.
That assumes you read it via `IO`. It could be lazily generated from a computation.
Be sure to keep in mind the difference between continuations (in the general sense) vs continuation-passing style. CPS is a technique for rendering programs into a linear format (i.e., making evaluation explicitly ordered); and often yields performance benefits for that reason. On the other hand, continuations-in-general are a functional version of ye old GOTO statement; and consequently tend towards mind-bending programs. CPS is called that because it does use continuations, but it doesn't do so in any especially perverse or magical way. It's just keeping a todo list.
Yeah but it doesn't work very well
Love it
 &lt;+&lt;
This is a great question - I've been wondering about the same. Many libraries still seem to be using either Strings or ByteStrings which makes it a bit awkward to try to keep application code Text only. For example, hastache is almost all ByteString in its API even though the library is meant for text manipulation. Some other libraries like Snap also use ByteStrings quite extensively, perhaps for performance.. or maybe it's just hard to draw the line where to use ByteStrings and when to switch to Text?
I agree, being a fan of SOV orders in natural languages. However, no matter how you twist and turn it, the notation will always be weird. If you swap the arguments to natural application (`($) :: a -&gt; (a -&gt; b) -&gt; b`), then the order of arguments to a function is going to be counter-intuitive, especially from a perspective of types. If you have a function `f :: X -&gt; Y -&gt; Z`, and terms `x :: X` and `y :: Y`, then application becomes `y x f`, completely the opposite order of the signature of `f`.
'create' uses fixed seed and will produce same random numbers every time program is run. 'withSystemRandom' initialize PRNG from /dev/urandom
Yes this exactly. I'm not even sure how to convert efficiently between them.
Oh, right.
Yep,you'r right! I'm begginer 2 :D
You have to encode/decode to switch between them. ByteString is only for binary data, such as data that has already been encoded in the target encoding and does not need to be decoded again. Never use ByteString for anything that you still plan to manipulate as text.
&gt; “PHP-esque” Too vulgar. &gt; “it keeps asking me if I'm the keymaster” [Loud growl] _Zuuuuuul_... 
Thanks for the info :) 
We have an [official package for parsing Haskell](http://hackage.haskell.org/package/haskell-src-exts)?
Could you use an existing parser such as [haskell-src-exts](http://hackage.haskell.org/package/haskell-src-exts)?
I will look into it
indeed, this is an area where non-strictness shines. However, this pattern is not as common in my experience as the dreaded lazy-IO processing.
I really hope that Happstack is committed to remain a plain Haskell library, in the form of modules, types, functions, classes and instances. Gluing that together is the responsibility of the user, hopefully guided by great documentation and tutorials. Snap is great, and I have used it a lot, but I started to look elsewhere when the "snap init + various conventions" era began (around 0.4 or 0.5 IIRC). So please keep going, you are doing a great job, and I can only recommend to everyone to give Happstack a try.
You want dead simple? Print out an entire line of text on every keypress, and use \r carriage return and whitespace to blank the line each time. Do not print \n line feed. No libraries or infrastructure needed beyond a standard terminal to run inside.
You can do destructive state all you want in monadic IO with IOArray and IORef. The hard part is passing control between IO functions and pure functions.
But a comonad is a carbureto'
&gt; edwardkmett &gt; take a lens Yeah, should have seen that coming.
You will have a better time if you choose a problem whose monadic implementation is better than non-monadic implementations.
ccshan+dtburston's WordNumbers blog post is perhaps the canonical example of using a nuclear bomb of mathematics and kill a mosquito of a problem. http://conway.rutgers.edu/~ccshan/wiki/blog/posts/WordNumbers1/
In the perl world, they call themselves "monks".
Thanks for the answer. Interestingly, the following occurs when I try to isolate the anamorphism. *Main&gt; ana alg ghc: panic! (the 'impossible' happened) (GHC version 7.6.1 for x86_64-unknown-linux): nameModule &lt;&lt;details unavailable&gt;&gt; Already reported to GHC HQ.
Foldl is preferable to foldl' only when you ignore operations and only want denotations, when you want to think about correctness and ignore performance, as in theory work. When it comes time to execute on a machine, use foldl' always.
But if you want watch the program state expand and reduce until you see something unexpected, you have to step through laziness.
It puts on a gimp mask, knocks the other Haskell source parsing packages down, straps them to a bar and sticks a ball gag in their mouths. You can imagine the rest of the hot XXX Hackage package porn!
&gt; ByteString is only for binary data... &gt; Never use ByteString for anything that you still plan to manipulate as text. That's generally true, but never say never. Sometimes you have text which really is most naturally expressed as a stream of bytes in the ASCII encoding. In that case, you might be better off working with the functions in `Data.ByteString.Char8` than encoding and decoding back and forth to `Text`. One example is when working with text read from the Unix command line or from Unix environment variables. Those are bytes by definition. While it is true that nowadays there are some fairly standard ways of interpreting those as being in some particular encoding as determined by the current locale, it is often best to keep them as bytes. Another example is Postscript content. Text strings in Postscript are by definition bytes. Decoding the bytes as Unicode can sometimes be extremely non-trivial, involving among other things parsing and interpreting font data which may or may not be encrypted and may or may not even be present in the file. But I hope you can see from the nature of these examples that what singpolyma said is almost always correct. :)
Brilliant.
Sounds like a good opportunity for OP to contribute a new package, on improve existing ones. ;-) E.g. writing a set of bindings for Wolfram Alpha could be an interesting project.
I think it is a shame that most of the "Core overview"-articles don't deal with type applications (the at-sign stuff). I find that it's hard to understand what's going on when you ignore that. In general, I would just recommend that people read [this paper](http://www.cse.unsw.edu.au/~chak/papers/SCP06.html), which explains System Fc, which is pretty much just GHC Core in a language theory setting.
That is basically what I mean. But you don't get a navigable view of all subexpressions, you can only see each one after it is evaluated, before it is consumed by a super expression, so this induces an ordering that is not often conventient. Unless there is a way to force an expression in the debugger. And even that wouldn't help with space leaks related to insufficent strictness, where you do need to see the think buildup to eyeball the problem.
Why not just background-compute all sub-expressions being viewed in the debugger, catching any exceptions? Why do you care about evaluation order at all when debugging the value of an expression and its sub-expressions?
I was planning on covering this in a subsequent post. I try to keep each post from getting overly long so I split them into multiple topics if necessary.
Just to break down that ugly `cast` expression a bit further: main = main1 `cast` (Sym (NTCo:IO &lt;()&gt;) :: (State# RealWorld -&gt; (# State# RealWorld, () #)) ~# IO ()) Here it's applying the `cast` function to `main1`. What `cast` does is take an object of type `a` and a coercion between `a` and `b` and using it coerces the object from `a` to `b`. Its type signature could be written as: cast :: a -&gt; (a ~# b) -&gt; b Where `(~#)` is the type of unboxed coercions. Think of it as the compiler-internal version of `(~)`. Read [a paper by SPJ][1] if you really want to know the difference. As you can see, the type of the second argument of `cast`, the coercion, is declared in the core explicitly: :: (State# RealWorld -&gt; (# State# RealWorld, () #)) ~# IO () In other words: a = State# RealWorld -&gt; (# State# RealWorld, () #) b = IO () So it's casting from that first type to the second type, just as Tekmo wrote. (And `(# #)` is the type of unboxed tuples, just for the record.) The only part left is that it's not enough to give a type for the coercion: there also has to be a coercion *object*, which is generated by the compiler during type inference and type checking as evidence that the cast is legal. Here it looks like this: Sym (NTCo:IO &lt;()&gt;) `Sym` stands for Symmetry - a coercion is valid in either direction, and this just flips it around. `NTCo` presumably stands for Newtype Coercion. And it specifies which newtype coercion we're talking about: the one for `IO`, with the argument type `()`. Presumably the newtype coercion by default points in the other direction, so it has to be flipped around with `Sym`. I don't know this for sure, but it looks like `NTCo` and `Sym` are constructors for the `(~#)` type. Which would make a bunch of sense. And that's it. [1]: http://research.microsoft.com/en-us/um/people/simonpj/papers/ext-f/
Sorry if the title is confusing — there's a link to a HaskellWiki page. XMonad is already there.
Is there a library built on top of `criterion` that autogenerates test input?
Also, what are people using to get GHCi completion with Emacs? There's a minor mode called ghci-completion but it's very limited even by comparison with stock GHCi.
M-/ works for me with completion in interactive-mode. I don't think it's doing the same thing as TAB in a real ghci session though so that might still be preferable.
Not that I know of, as that's rarely desirable. You typically want fixed inputs so you can measure the effect of changes to your code. The code in the linked article that generates random inputs on every run would be a good antipattern for performance testing.
 I don't care about any particular order, but it is easier to follow long a Java like execution track than to follow the predictable order of the GHC runtime. If the debugger could save intermediate results (all, or some bookmarked functions) let me free navigate down from main and back up and sideways, looking for where my mental model diverges from the actual value of expressions, that would be great, better than a gdb-style stepping debugger But probably impractical in memory usage in all but the smallest cases.
&gt; example :: (forall a. p a) =&gt; ... -- not valid in GHC 7.6.1 And, I highly suspect, not valid in GHC &gt;= 7.6.1. The reason is type inference. GHC's constraint system currently only allows basic constraints like: conjunction, type class constraints, equality constraints, etc. to be specified in user signatures. When combining with GADTs, we can have these constraints appear on the left side of an implication constraint. To allow quantification (even worse, higher order quantification) to occur within constraint terms sounds like you'd end up with undecidable type inference, without any convenient decidable subset.
[This paper is the one to read](http://www.haskell.org/haskellwiki/Simonpj/Talk:OutsideIn).
Ah. I have that one, but was intimidated by its length. I'll get around to it. Thanks!
Oooooo. The rate of work on this library and active maintenance is very encouraging. &gt; I am currently writing a paper on how to map back the inferred types from the desugared code to the original code. If everything goes as expected, this bidirectionality will be added to HTE. This bodes well for Fay. And Haskell IDEs.
The first fusion reactor to run on monads?
By »Leibniz equality» it certainly will.
Good luck to anyone who applies!
These are some very simple slides I whipped up. I did them for a talk at a JS conf, but it gives a simple, quick overview of the state of Fay. [The IDE's](http://ide.fay-lang.org/) been updated slightly since I last posted it. You can add your own modules. Most there are tests people have done. Anyway, just thought I'd post it as it has a little bit of info about the internals. Not much to garner, as it tends to be like when people post slides, but you can sweep through all the slides in under a minute.
Since I do not have an wiki account right now, I shamelessly advertise here my own application "hoodle" (http://ianwookim.org/hxournal) (sorry that the web page is still using hxournal as its name but it will be changed soon. ) It's my every day note taking app on my tablet pc with ubuntu 12.04 . Works quite decently (of course in my point of view). Definitely cannot be developed as fast as I did if I chose a different language. Haskell rocks in GUI programming!
Random question. When I see this: var square = function($_a) { return new $(function() { var x = $_a return _(x) * _(x); }); }; I wonder, why is the temporary `$_a` used? Why not just this: var square = function(x) { return new $(function() { return _(x) * _(x); }); }; 
I think such thing is called a list.
Albeit a heterogenously typed one.
I think one issue with the approach is that as a nested type, it's more efficient at some things and less efficient at others. It's not flat in memory, which makes "cons-like" behavior efficient, but it makes getting the nth element take `O(n)` time. If an `n`-tuple were just syntactic sugar for the nested case, would you expect it to have flat performance characteristics? Because that's what the syntax would suggest, but if you could use the nested and non-nested forms interchangeably, you might assume that you can go from an `n`-tuple to an `n+1`-tuple in constant time, when it would actually take `O(n)` time. I'd keep both cases around and keep them separate. You can easily define an HList-like thing if that's what you need, and you will be forced to acknowledge the subsequent complexity trade-offs you're making.
The expression `square x = x * x` is really an instance of the general case of a function binding which includes pattern matching. So this code is handled by [the pattern matching code](https://github.com/faylang/fay/blob/master/src/Language/Fay.hs#L652). Consider: f 1 = 2 That gets compiled to: var f = function($_a) { return new $(function() { if (_($_a) === 1) { return 2; } throw ["unhandled case in `f'", $_a]; }); }; Another example: f (x:_) = x * x compiles to var f = function($_a) { return new $(function() { var $_$_a = _($_a); if ($_$_a instanceof Fay$$Cons) { var x = $_$_a.car; return _(x) * _(x) } throw ["unhandled case in `f'", $_a]; }); }; So the output you described is just part of a more general rule. It could make a special case for arguments that aren't pattern matched upon, but it's premature and probably pointless, given that compilers probably collapse these. Google Closure certainly does. It might be good to add the special case for readability, in which case it would be easy for someone to add it if they wanted it.
This is something that I don't understand. When a type has only one constructor what does Haskell gain by allowing pattern matching on that constructor to diverge, as opposed to saying it always succeeds?
I've wondered about this: Haskell doesn't do variadic types or functions: its type and data constructors and functions have fixed numbers of arguments. When it wants to do an n-ary thing (application, say), it takes a binary thing and folds it. Algebraic data *could* be represented with just binary products and sums (and is with Generic Haskell), but in the source language, it isn't. Which means that when it comes to truly variadic things -- like tuple types -- it doesn't know how to talk about them. By contrast, C++11 has variadic templates, which allow it to (for example) define variadic tuples in the language itself, and to abstract over them. But C++11 is missing many of the nice things that Haskell has, like curried functions. I don't have a conclusion. I'm mostly wondering if anyone has related points. (I guess the question is: why does it seem like C++11 has more expressive power when it comes to variadic things than Haskell does, how come, and is there anything we could do about it?)
I once discussed this with Augustusson. He explained there's some benefit on the compiler side. Consider: let p :: (Int, Int) p = p case p of (x, y) -&gt; ... x ... y ... With lifted products you've got some sort of result to pattern match there. Without it, you need to compile x and y to something like `fst p` and `snd p`. You can simulate this by always using irrefutable pattern matches on products. 
And implementing seq for unlifted products is hard. 
As an aside, `lensAppend :: Lens a b1 -&gt; Lens a b2 -&gt; Lens a (b1, b2)` isn't safe in general because "`b1` and `b2` might overlap".
In practice it's O(1) because when was the last time you used a 7-tuple?
Not manually, but I've written code which has expanded out to a 13-tuple. It was in C++ so it was flat in memory anyway, but these things can happen.
What *is* seq for unlifted products?
`[]`
It's sad that javascript has become assembly for the web. It would be so great if we could have something better to compile to. 
The difference is that higher-rank polymorphism introduces quantifiers over type constructors, not over constraints, which is a much more complicated problem! It feels like the difference between propositional and first-order logic. 
Haskell defines seq as returning bottom if the first argument is bottom, otherwise the second argument. The bottom for unlifted products is the pair of two bottoms, anything else is non-bottom. So for an unlifted product as the first argument, seq should return the second argument if either of the two elements of the pair is non-bottom. The only way I know to determine this is to evaluate both elements in parallel and stop when either of them reach WHNF. I regard this as another sign that seq is evil rather than as a sign unlifted products are evil.
Out of curiosity, how much time / effort did this take (so far)?
 Wouldn't code like 'let (a,b,c) = ...' then possibly result in a pattern mismatch during runtime? If yes, then it would contradict the normal usage of tuples. 
Been working on it since July. Here's [Github's contribution page](https://github.com/faylang/fay/graphs/contributors?from=2012-07-15&amp;to=2012-10-21&amp;type=c). [Ohlol page](https://www.ohloh.net/p/fay) is also available. I made it in a few spare weekends in March. Didn't touch it for a month and then tidied it up, put it in a repo and put it online in July. Couple weeks of reactionary hacking after that. Now I spend a couple hours per week tweaking some things, fixing a few bugs, merging pull requests. Adam Bergmark contributes a lot of feature code, Dag Odenhall contributes a fair bit of structural code and helped setup things like the continuous integration setup we have, and Ömer Sinan Ağacan also contributes some features. I'm concentrating on writing things with it now, making projects [like this](https://github.com/chrisdone/moogle/graphs/contributors) and [this](https://github.com/faylang/fay-server/graphs/contributors), and updating Fay as needed.
How does Fay compare to [Elm](http://elm-lang.org/), which to me seems to be almost the same thing?
Found some info on the website. Fay is more "GHC-compatible" than Elm, i.e. uses the existing type checker and parser code. Which seems like a good idea. Edit: Does this mean I can use new GHC extensions e.g. LambdaCase?
&gt;These are some very simple slides I whipped up. I hate you. :P
It's not a new idea, of course. There tends to be resistance from [people who prefer an “open web”.](http://en.wikipedia.org/wiki/Google_Native_Client#Reception) The same criticisms that Flash, Java, ActiveX, Silverlight receive. They don't like how NaCL is defined by its implementation and isn't portable. I can understand the resistance to these “black boxes”. People like view source a lot (even though minification has put a dent in that, ‘code culture’ still exists). There is resistance to virtual machines because there will be too much contention on establishing a standard one. JS just came first and avoided this process. Thankfully, the people driving JS understand that plenty of people don't like it and are compiling to it, and they are catering to generated code. Array buffers, typed arrays, uints, bytes, etc. Emscripten shows that compiling to JS is workable. It can compile Sauerbraten, an FPS, to run at 60fps. Check out the talk by [Brendan Eich](http://www.youtube.com/watch?v=Rj49rmc01Hs). People call JS the ‘assembly of the web’, but consider it like C. Many Haskell compilers compile to C, they can also compile to JS. While agreed a universally accepted VM would be cool, I don't think it's going to happen.
Surprise! This is a finger tree!
Accessing the seventh element a hundred million times would require seven hundred million dereferences, so the constant may indeed be important. Unlikely, but still, making people's code n times slower is usually not something people are happy with.
Why are people suddenly commenting on this 4 day old comment? Anyway, no. PHP is the worst. At least Larry Wall *cares*.
Although that is a mostly arbitrary choice of encoding.
That's a very cool idea. I'm not sure if this is going to be used a lot, but still, that will definitely be a 'plus' for the beginners.
The second case for strict is False because, as you said, strict palindromes are of even length. A list of one element thus cannot possibly be a strict palindrome. For the third case in strict, the pattern (x:xs) matches a list of an element (bound to x) and a (possibly empty) rest of the list, usually called tail (bound to xs). Now, we already handled the case in which there was only one element, so we know that xs is a nonempty list in the third case. If the first element is equal to the last element AND the "middle" is a strict palindrome, we have a strict palindrome. (init xs is the original list except the last and first elements, remember. x was the first element, so it is not included in xs.) Edit: Clarity
Did I just read a pun thread in haskell?
My confusion comes from the : operator being used. I thought that using : would append whatever was on the LHS to the RHS (presuming the RHS is a list). How is it that this is being passed as an argument? I'm sorry for my ignorance, I'm cursed by the thinking methodology of von Neumann models.
It would on the right hand side. On the left side, in the so-called pattern, you match the structure of the value, here a list, by using its constructor. If I made a datatype data Foo = Bar Int I would create a function that adds two to a Foo by matching the structure of the datatype with the constructor on the right hand side of the definition. addTwo (Bar x) = Bar (x+2) Lists have some magical syntax in the brackets, but : is just an infix constructor that you can use to pattern match.
Ok, so you need to understand pattern matching for this. strict [y] = False This matches when there is only one element in the list. If there's only one letter, it can't be a strict palindrome. The second one is slightly more complex: strict (x:xs) = x == last xs &amp;&amp; strict (init xs) Here `strict (x:xs)` matches a list, where `x` is the first element and `xs` is the remainder of the list. First it checks that the first letter (`x`) is the same as the last letter (`last xs`) (see edit). Then it checks that the inner part of the string is also a strict palindrome. Lets run through this with the string "anna". First, x is 'a' and xs is "nna". `last xs` is `a` so `x == last xs`, we're fine there. `init` gets all but the last part of the list. So `init "nna"` is `"nn"`. This is passed into `strict` again to check the inside of the string is a strict palindrome. EDIT - `last` gets the last item from a list. So `last "hello my name is dave" == 'e'` edit2 - " and ' clarifications
Best explanation I've read so far. Thanks a lot. 
Just take a look at [Data.ByteString.Internal](http://hackage.haskell.org/packages/archive/bytestring/0.10.2.0/doc/html/src/Data-ByteString-Internal.html). The functions you want are `create` and variants thereof.
You might understand it better if you define your own String type, as data MyString = Empty | Cons Char MyString In my opinion, you really must understand this definition before going further. "Pattern matching" is the action of matching a runtime value with a lexical pattern. The match can fail, or can match, bringing into scope values embedded in the structure of the value you matched against. Your "strict" definition could be written as : strict :: MyString -&gt; Bool strict Empty = True strict (Cons _ Empty) = False strict (Cons x xs) = x == myLast xs &amp;&amp; strict (myInit xs) Notice that at the third line, I did not even bother to "name" the unique char of the string, since I don't need to reference it in the right-hand side. The fourth line is different since the RHS needs the sub-components of the list to operate, so I named them in the pattern on the LHS. Once this is clear, the plain String variant should be clear as well, since String is an alias for [Char]. Also, there is a special notation in haskell for the plain List constructors : [] == Empty [x] == Cons x Empty x:xs == Cons x xs But in the end, these are just constructors, so you can use them for pattern matching as well. edit : removed unneeded brackets 
In the past, I've played with "pseudo-tuples" to the effect of infixr :*: data x :*: xs = x :*: !xs -- now (a :*: b :*: c :*: d :*: ()) is like (a,b,c,d) That avoids some of the issues that have been mentioned by others: The explicit empty tuple on the end eliminates the ambiguity in the lengths of pseudo-tuples that manifests when defining, say, snd -- in your scheme, only one of these can be the case: snd (a,b,c) = b snd (a,(b,c)) = (b,c) While, in my scheme, (a,b,c) is (a :*: b :*: c :*: ()) and (a,(b,c)) is (a :*: (b :*: c :*: ()) :*: ()) so I could define thus and get both behaviours: snd (x :*: y :*: _) = y The strictness annotation "!xs" ensures that pseudo-tuples have the same bottom semantics as tuples; it stops you having things like "half a pseudo-tuple" as it makes (x :*: _|_) = _|_ The strictness annotation may also allow a (sufficiently smart optimising) compiler to eliminate the performance penalties of nested constructors. With a bit of cleverness, one could probably implement either of our schemes as a template haskell [d| ... |] that you wrap your whole program in. Just some thoughts. [edit: formatting]
&gt; Not sure if I want to continue learning Haskell after this. So you do something silly like repeatedly call `cons` on a strict `Text` value and then say that "Haskell" is slow? Nevermind trying to figure things out what's wrong: let's complain and write an inflammatory blog post about it, instead. &gt; When you complain about Strings in Haskell being slow on some neckbeard forum, people will tell you to use Data.Text. Somehow I don't think this post is written in good faith.
This is from a recent Haskell performance talk http://bos.github.com/reaktor-dev-day-2012/reaktor-talk-slides.html#(34) I think it might try it with Streams tonight and see where I get with that.
While the blog post is very aggressive, playing with this I am wondering what the most efficient way to capitalize a word would be...
as godofpumpkins points out, consing (as well as taking heads and tails of) Text is algorithmically inefficient. Furthermore, you're comparing *different* algorithms in all three languages. The Python is mutating the word strings themselves, and the JavaScript is mutating the *array* of words. You can't write different algos using libraries with different properties and expect comparable results. Edit: Furthermore, I note that javascript's toUpperCase isn't unicode aware, and I'm not sure about python's method. *Both* the `Text` and `ByteString` approaches you use do it "right", which is quite expensive. Furthermore, `Text` does words and lines with more expensive unicode-aware splitting. Even the `ByteString` versions at least check for the range of unicode whitespace and indicators, as I recall, and not just ' '. So I haven't profiled but I bet a fair amount of time is lost because the Haskell libraries are more robust and general. If you care about text processing, you should probably care about *correct* text processing too. (nb. the above edit was edited again to clean up a claim involving newlines).
Your code isn't analogous at all to the code in that talk.
`readFile` is *not* strict.
So, using a combination of `Text` and `String` I can match (and sometimes beat) the performance of his posted python: http://pastie.org/5110350
&gt; The Python is mutating the word strings themselves No, it's not. http://docs.python.org/library/stdtypes.html#str.capitalize &gt; Edit: Furthermore, I note that javascript's toUpperCase isn't unicode aware, and I'm not sure about python's method. Both the Text and ByteString approaches you use do it "right", which is quite expensive. Furthermore, Text does unwords and unlines with more expensive unicode-aware splitting. Even the ByteString versions at least check for the range of unicode whitespace and newline indicators, as I recall, and not just ' ' and '\n'. So I haven't profiled but I bet a fair amount of time is lost because the Haskell libraries are more robust and general. Based on experimentation, Python's capitalize as used here is not Unicode-aware. But neither is the ByteString version, in any sense. There are slight differences between `.split(' ')` and `words` (involving repeated/leading/trailing delimiters and the ASCII whitespace characters besides ' ') but the Python and ByteString versions are essentially the same in terms of "correctness".
Note that your bytestring test begins with `module Byte` instead of `module Main`. That means that when you compile it with `ghc --make`, you don't get an executable. So I suspect that you just ended up running the lazy bytestring executable again, which explains why you got the same time. When I tried the bytestring version (with `module Main`), it was definitely faster than the python version on my input (the system dictionary). 
Thanks for saying this. Great summary of my feelings.
&gt; The Python is mutating the word strings No, Python strings are immutable. &gt; Even the ByteString versions at least check for the range of unicode whitespace and newline indicators, as I recall, and not just ' ' and '\n'. So I haven't profiled but I bet a fair amount of time is lost because the Haskell libraries are more robust and general. You're probably wrong: I converted the lazy bytestring version (last one) to `split` on newlines and spaces instead of `lines` and `words`, there's essentially no difference in performances.
i know it's been said a thousand times before, but reading TAPL, HSOE, PFDS, RWH and applying coding styles therein in other languages has made me a better programmer - much more likely to always return new values/fields than mutate state on an existing "object" and other such nastiness. one day i'll sit down and learn monad-transforming stacks and whatever else is needed to become an effective Haskell programmer instead of just writing "idiomatic haskell" in JS or OCaml. maybe?
&gt; So I suspect that you just ended up running the lazy bytestring executable again, which explains why you got the same time. I'm sure you mean the lazy text version. Using a dataset similar to OP's (an ~90MB repeating of a bunch of lorem ipsum paragraph) I get similar results to TFA's: http://www.reddit.com/r/haskell/comments/120h6i/why_is_this_simple_text_processing_program_so/c6r4mdw And I did use `module Main` in both bytestring versions. &gt; When I tried the bytestring version (with module Main), it was definitely faster than the python version on my input (the system dictionary). Doesn't the system dictionary have only one word per line? 
Right. rwbarton cleared that up for me. But the javascript is mutating the array of strings itself. The python, using a comprehension, isn't.
Tangentially, `toUpper` on the first character is not titlecase. Just like lowercasing both strings before a comparison is not a case-insensitive comparison. This is where it makes sense to have dedicated functions for such high-level functions: anglocentric users are likely to construct simpler, but wrong implementations. Also you can build in all the unintuitive optimizations you want.
`words` here is `Data.ByteString.Lazy.Char8.words` which checks for the seven Latin-1 whitespace characters. `toUpper` is unicode aware but it is being passed individual bytes from a UTF-8 file treated as Unicode characters, which is not correct in any sense! As some Unicode characters in the range 128-255 are in fact letters, this ends up turning some UTF-8 encoded characters into random other UTF-8 encoded characters.
This isn't even remotely close to the python performance on my system; it's over 4 times as slow.
I imagine the most efficient way would be to just thaw the underlying array and mutate it in the ST monad. Alternatively, you can just use a pure accumulation on the array. Both approaches require access to the underlying array and the problem is that neither library exposes a lens to the underlying array. Hopefully, Edward can add such a lens to `Control.Lens` since he already has limited support for `Text` and `ByteString`.
yep -- thanks for the precision here. i'd still maintain that these are doing more than the equivs in other languages, but obviously using bytestring for unicode will be frequently wrong. My point was less that it is doing everything better (although it is somewhat better I'd argue) than that it is doing *more*, and so slightly more expensive. And more generally my point was that if there are different algos on different underlying structures one should not assume that there's comparable performance.
Oh, weird. I just got a dataset from the lipsum site and it is indeed slower. My original dataset was the text of the blogpost (repeated enough times to be big enough to measure on). Perhaps line length matters quite a bit?
&gt; Tangentially, toUpper on the first character is not titlecase. True and relevant to building correct code, but not really relevant to the comparison itself.
&gt; Perhaps line length matters quite a bit? That's an interesting idea, and it would explain why Haskell handily beats Python when running the code on /usr/share/dict/words (which only has one word per line)
The bytestring is certainly not unicode-aware, as it treats the input as simply a string of bytes. It simply converts individual bytes to one-byte *Char*s. So it will not handle áéíóú for instance.
The Python code does not update any internal array.
Still not beating the python on this new dataset, but my friend came up with this: `upcaseWords text = snd $ T.mapAccumL (\space char -&gt; (isSpace char, if space then (toUpper char) else char)) True text`
http://hackage.haskell.org/packages/archive/text/0.11.2.3/doc/html/Data-Text-Internal.html go nuts!
Thanks! I did a cursory search and didn't find that, but I'll try my hand at it now.
Right. But the `toUpper` is unicode-aware and hence more expensive than the simple one you can do on ascii. Using unicode-aware functions on char8 streams doesn't make the *overall* code more correct, sure, but it does mean that we have a better but slightly more expensive set of core functions to begin with. That's really what I was trying to get at.
In this case it's better to use conduits, as iteratees give you only input and not output.
I don't think it's written in bad faith, maybe it's 'inflammatory' but to me it seems like someone gave Haskell a try in good faith and simply got frustrated by having to go through a few mental/conceptual hurdles to get a program to run at a huge performance cost. I'm not sure whether to community cares for wider adoption, but honestly I think his concerns are valid, even if it's not expressed super politely. Text processing is a very common use case and one that a lot of people may use to begin learning Haskell. Frankly, and maybe I too am an uber-noob, but Haskell doesn't provide many practical benefits in this area.
Issues of the author's rudeness aside, this is actually a valid question. (General hint: if you are asking for help to understand something, don't act the maggot, as we say in Ireland.) So I am going to set aside my usual policy of ignoring people who lack enough good sense to be polite, and dig into this a bit. The first thing to understand is that the Python script that is so fast is a very thin wrapper around a bunch of C code. There is absolutely no significant computation happening *in Python* in that script. This is also why the V8 code is only a small amount faster: there's simply not much to optimize outside of the raw byte-slinging. The Haskell program that uses lazy ByteStrings is about 30% slower than the Python script. That's not awesome, but it's not bad either. Meanwhile, the version that uses lazy Text is less than half the speed. At the highest level, this is because I fix performance problems reactively these days. My own benchmarks were running quite happily, and nobody had reported any speed issues recently. Now that I have a test case to work with, this is something I can improve. I've already made a small improvement to performance with this test case in hand, but I think there's plenty more to be gleaned.
You will for sure smash the python versions' performance numbers.
&gt; Both Text and ByteString are based on arrays, which permit fast update, but they don't offer an API, even an unsafe one, to update the internal array. That's because they're conceptually immutable values, and unless you want to get into `IO` or `ST`, there's not a good way to even write a function that modifies an immutable value in place.
Ok, here's a naive implementation. I can't really benchmark and compare because I don't have the same computer and input as the OP, but here's the code: import qualified Data.ByteString.Char8 as B import Data.Word8 (toUpper) import qualified Foreign as F import System.Environment capitalize :: B.ByteString -&gt; B.ByteString capitalize b = if (B.length b &lt; 1) then b else F.unsafeLocalState $ B.useAsCString b $ \p -&gt; do w8 &lt;- F.peek p F.poke p (fromIntegral . toUpper . fromIntegral $ w8) B.packCString p convert = B.unwords . map capitalize . B.words main = do [inFile, outFile] &lt;- getArgs str &lt;- B.readFile inFile B.writeFile outFile (convert str) Profiling shows that the capitalization is only 1/3 of the run time, the remainder being the `words` and `unwords` functions. COST CENTRE MODULE %time %alloc convert Main 68.2 81.4 capitalize Main 25.7 9.0 capitalize.\ Main 5.9 8.9 I could probably increase in speed by using `unsafe` functions. I didn't use `text` only because I'm not as familiar with doing case conversions on `Word16`s.
data point: grew to 2.1GB RAM on my machine (and a 90MB input file) and took 40s (TFA's lazy bytestring took 12)
So I profiled both the post's version (using lazy bytestrings) and my version (which you already know about), and neither of them has capitalization as the rate limiting step. Here's the profiling data for the lazy bytestring version of the original article: main Main 41.2 42.0 convertLine Main 37.8 38.0 convert Main 11.4 13.6 convertWord Main 9.6 6.4 As you can see, `convertWord` only accounts for 10% of the running time. The vast majority is `convertLine` (presumably the `words` or `unwords` functions is the true culprit). The `main` contribution is just because of the lazy bytestring's laziness, so the overhead of `words` and `unwords` is getting rolled into the cost of reading and writing the file. My own variation proves this, because when you switch to strict bytestrings then all the performance contribution goes straight to the `words` and `unwords` functions. Additionally, I know that raw Haskell file `IO` is much faster than that, comparable to C. So the real culprit are the `words` and `unwords` function and the Haskell implementation of `convertWord` in the post is actually probably as fast as the other languages and you are chasing the wrong performance block.
BTW, it may be worth observing that the above version of "strict" is inefficient -- it runs in O(n^2 ) time. The problem is that "last xs" has to traverse the entire length of xs to get that last character. It would be both faster and simpler to just say: strict s = (s == reverse s) &amp;&amp; even (length s) (Correct me if I'm wrong.)
To measure effect of changes one need same inputs every time benchmark is run. But there's no requirement ot use same value N time. For example PRNG with fixed seed could be used safely. It won't affect correctness. If algorithm's performance may vary for different inputs (e.g. it may need different number of iteration to converge) such approach could be even desirable. It average out fluctuations and do not know in advance whether one particular point is good/bad.
then again, no one will sit around poking and prodding simple script when the time is still in range of less then a minute ;)
Declarative programming is always more idiomatic and easier to reason about than manually recursive solutions like the one in the OP. Your answer should really be what the OP uses in the end.
Seq doesn't have to be polymorphic, it could have been overloaded, and then the problem would be "solved".
&gt; toUpper is a Unicode aware function. toUpper has type Char -&gt; Char, but there are characters whose uppercase equivalents are multiple characters.
Not if you take into account the fact that the documentation for text clearly indicates that cons is O(n).
I wonder how fast a perl -npe 's/(\w+)/ucfirst $1/ge' would be.
Why is this simple webpage blank?
&gt; I imagine the most efficient way would be to just thaw the underlying array and mutate it in the ST monad. Alternatively, you can just use a pure accumulation on the array. This is so not a solution: this is not idiomatic Haskell. Plus the Python algo is functional, it doesn't mutate anything and whereas functional programming is not Python's major paradigm it performs better than Haskell, a functional language. This is the sign that there is a problem IMO.
Chris, why did you decide to base fay on haskell-src-exts instead of the GHC API? The latter would have the advantage of having the entire GHC front-end available: parser, type-checker, desugarer and most importantly the GHC optimizer. It would also have the advantage of being able to support the whole Haskell language instead of just a subset. The final advantage is that you only need to compile the much smaller GHC core (or maybe even STG) to JavaScript. After having used the GHC API some years ago I found it a bit cumbersome to use. So I can imagine it has something to do with this.
So I wonder if the issue is the nested concats. Like would switching to a builder help? I suspect so.
It's interesting that you mention c++. Careless c++ version from http://pastie.org/private/49q42yiytabikriqqfnkw performs almost as 'bad' as haskell (13s vs 14s): $ clang++ -Wall -pedantic -O3 main.cpp $ time ./a.out &gt; /dev/null real 0m14.659s user 0m14.054s sys 0m0.605s $ clang++ --version Apple clang version 4.0 (tags/Apple/clang-421.0.60) (based on LLVM 3.1svn) Target: x86_64-apple-darwin12.2.0 Thread model: posix $ ghc -O2 -o main Main.hs [1 of 1] Compiling Main ( Main.hs, Main.o ) Linking main ... $ time ./main &gt; /dev/null real 0m13.544s user 0m12.965s sys 0m0.507s $ ghc --version The Glorious Glasgow Haskell Compilation System, version 7.4.2
I had not heard of acid-state before. I will have to investigate it :) Plugins is also a good one. Hot-swappable code is super.
I hate to say it, but that's generally been my experience trying to write performant Haskell. I spent a reasonable amount of time tackling SPOJ problems in Haskell, and ignoring the usual space leak problems that most Haskellers encounter, basic tuning issues like this were surprisingly hard to fix, and often required switching to unboxed types, using unsafePerformIO for mutation, or other ugly tricks that don't feel very idiomatic.
Use src-exts if you want to parse Haskell, then you can easily support more of Haskell when you find the need. See for instance how Fay does it: https://github.com/faylang/fay/blob/master/src/Language/Fay.hs Writing a new parser seems like unnecessary work. 
I agree. The correct response to criticism is to rise to the challenge and improve.
This is true, however its was more to show some of haskells stuff without using reverse again. 
Your declarative solution is great because it is easy to understand. If efficiency is a concern, however, one could optimize further: due to the definition of palindromes, the full string does not need to be checked for equality. loose :: String -&gt; Bool loose s = let n2 = length s `div` 2 in (take n2 s) == (take n2 . reverse $ s) strict :: String -&gt; Bool strict s = let n = length s n2 = n `div` 2 in even n &amp;&amp; (take n2 s) == (take n2 . reverse $ s) Also note that, for efficiency, the *even* check should be first.
&gt; Incidentally, if you really need speed this is at least 10x faster than any of the other programs And, as I've pointed out elsewhere, it's actually using the correct algorithm for this sort of thing: namely a two-state transducer which doesn't bother with all this line and word splitting nonsense.
&gt; Is there something that can be done to improve the performance of printing lazy ByteStrings with many chunks? You can use [unix-bytestring](http://hackage.haskell.org/package/unix-bytestring) which gives access to the `writev` syscall. This is essential for avoiding the overhead of making multiple system calls. There's even a [pretty wrapper](http://hackage.haskell.org/packages/archive/unix-bytestring/0.3.5.4/doc/html/System-Posix-IO-ByteString-Lazy.html#v:fdWritev) for lazy ByteStrings. unix-bytestring is, of course, restricted to POSIX/XPG compliant systems; but I'm sure Windows has some equivalent thing, since this is a fundamental issue for high bandwidth I/O in any language and on any (monolithic kernel) OS. If some Windows-knowledgeable person wanted to help, it wouldn't be hard to update the function used by the main bytestring lib.
Nice! Slight nitpick: it's slightly confusing that both ByteString.Lazy and ByteString.Lazy.Builder are imported as B; could be a little easier to read if they used different prefixes.
Of course, in the case of palindromes, `n` is rarely very large at all so worrying about algorithmic complexity may be more trouble than it's worth.
Pypy is probably just inlining the `process_line` method.
I was actually reminded to do the ugly version by Neil Mitchell's [supero](http://community.haskell.org/~ndm/supero/) slides ["Faster Haskell"](http://community.haskell.org/~ndm/downloads/slides-faster_haskell-10_aug_2007.pdf) which show an example where a high-level program optimizes down to this sort of efficient implementation.
Python's text processing functions are highly optimized C code. Even in C or C++, an unoptimized program that does mostly text processing is likely to be slower than a Python program. For most other uses, Python will be much slower.
&gt; Valuable outside of the Haskell community Hrm, does Agda count? It's not *for* Haskell, per se, but it seems like only Haskellers use it.
I was going to write earlier in the day that the most obvious and naive `String` solution is in fact something like: fixpr x y = if isSpace y then toUpper x else x conv xs = zipWith fixpr xs (' ': xs) main = readFile "monster.txt" &gt;&gt;= putStrLn . conv This is faster than the `Text` version for me, 8.869s vs 10.068s for 70mb (with the swanker `scanl` like `emilhedevang`s it is slightly faster still.) The original string solution involved a bizarre division into lines that had nothing to do with the problem but made the solution turn on the re-concatenation of the re-concatenation of a list of lists of Strings. None of these is equivalent to the python which doesnt, e.g. handle words preceded by tabs, I don't think.
Silicone chips? So if I upgrade my processor, my computer gets a silicone chip implant. Fun! :)
 Not in scope: `!=' Perhaps you meant one of these: `!!' (imported from Prelude), `/=' (imported from Prelude), `&lt;=' (imported from Prelude)
No, it's not (for recent versions of python).
It didn't come off as rude to me.
Is anybody taking account that System.IO.readFile and Text.readFile will actually do encoding conversion?
You and dcoutts should check with the exact same dataset. Maybe the difference comes from that, as it already happened on this thread.
Text also provides a Builder type, it would be nice to check how it behaves here, as in the spirit Text is closer to Python's strings than ByteStrings are.
Ah, inlining... But I take it neither Python nor JS inline anything, so it's not a very fair optimization.
If you further code up your own toUpper and isSpace without using the Data.Char ones, the speed will likely be even faster. In my case, it goes from 0.837s down to 0.121s for 18mb file, almost 7 times! Changing from Char8 to Word8 will bring a further 10% speed improvement.
I'm also interested in that so I tried this: Here's the (to my best knowledge) equivalent version using text builder... import qualified Data.Text.Lazy as T import qualified Data.Text.Lazy.IO as TIO import qualified Data.Text.Lazy.Builder as TB import Data.Monoid ((&lt;&gt;), mconcat) import Data.Char import Data.List convert = unlines' . map convertLine . T.lines convertLine = unwords' . map convertWord . T.words convertWord s = TB.fromString [(toUpper (T.head s))] &lt;&gt; TB.fromLazyText (T.tail s) unlines' = mconcat . intersperse (TB.fromString "\n") unwords' = mconcat . intersperse (TB.fromString " ") main = do name &lt;- TIO.readFile "/tmp/test.in" TIO.putStr $ TB.toLazyText $ convert name On my machine using GHC-7.4.1 python-2.6: 3.2s bytestring builder: 2.8s text-builder: 12.2s 
Fair enough. I just wanted to point out that despite being unicode-*aware*, it isn't actually unicode-*correct* (IIRC, Java has exactly the same issue). Does text-icu have the necessary function?
Further to my other comment, it occurs to me that what I described is probably related to HList: http://homepages.cwi.nl/~ralf/HList/
It's Python 2? Ok, but still we're processing here a text that may be unicode-encoded, not raw bytes.
It's about O(n log n) instead, with a hefty constant. Still not the linear result you can easily get by preallocation if data has a set length, but not completely awful. (Unless the runtime is being *really* tricky).
Nope, `open().read()` returns a `str`, which is bytes.
Wow! You really went for it, this is so cool! I played around with it. So much potential for this. :-) Once we implement [Cabal support](https://github.com/faylang/fay/issues/114), this gloss and its dependencies can be packaged up as regular Cabal packages. Regarding the unary negation, that was added recently on the git version. I've re-compiled the online IDE with that. I'll make a new release soon.
Nice, [on my machine and test file](http://www.reddit.com/r/haskell/comments/120h6i/why_is_this_simple_text_processing_program_so/c6r4mdw) this has the same performances as OP's Python on Pypy (time yields 6.499 total)
String munging programs? Possibly. My experience with non-stringy programs has been great. Haskell performs relatively well with little effort. I've used Python in the past, and it has been incredibly painful, performance-wise. I had to profile and rewrite large parts in C or Pyrex (Now Cython). 
Postscript doesn't specifically support Unicode, but it does support multi-byte character sets, so with an appropriate font you can support the Unicode character set. Probably not a conventional Unicode encoding such as UTF-8, but using an unusual multi-byte encoding doesn't mean it isn't Unicode. Source is [here](http://en.wikibooks.org/wiki/PostScript_FAQ#Does_PostScript_support_unicode_for_CJK_fonts.3F) &gt; It's also a fools errand to try to "parse" PostScript as you really need to interpret it as you read it In any interpreted language, the first two phases are scanning and parsing. You *have* to parse in order to interpret. The "as you read it" isn't really an issue - scanning, parsing and some aspects of later phases (building some intermediate representation at a minimum) are usually done effectively concurrently anyway, whether that's implemented by co-routines, state machine objects, lazy evaluation or whatever. Classing a phase as earlier or later is about data flow - early phases don't have to run to completion before later phases can start. 
Was the purpose to make you sound like a complete twat who decides that if after 5 minutes of trying something and it doesn't give the result you want, it must be shit and complain to all and sundry about it? This post says a lot more to me about you than any of the languages you've talked about. There are of course plenty of things you could do to speed this up, not least of which is run everything in parallel; something you'd likely find a lot harder in some of these other languages. Or maybe you could make it a fair comparison and use mutation like several of the other examples do. Of course there is often a cost for purity, but the benefits often far outweigh the sometimes slower speed. When you need to fine tune something, you always have the option dropping down to something lower level.
What about the python version?
When using *Char8*, one more optimization would be to use Kazu Yamamoto's [word8 package](http://hackage.haskell.org/package/word8), which improves the performance of *toUpper* in such cases. Kazu wrote about his motivation in the recent [Future work to improve the performance of Warp](http://www.yesodweb.com/blog/2012/10/future-work-warp) blog entry.
No, because it works better for larger chunks. 
[Summary](http://www.reddit.com/r/haskell/comments/120h6i/why_is_this_simple_text_processing_program_so/c6rjf84) of my feelings. Do you believe that you will see this performance disparity with Haskell for Python code in general?
&gt;I wasn't asking for help. That's nice. But just because you aren't interested in learning, doesn't mean nobody else is.
I don't think python's join performs allocation for every string added so that is not strictly equivalent. I'm fairly certain that it allocates only a single string which is long enough to hold the joined pieces.
Can anyone eli5 the second code? If it is possible... 
Which do you prefer? guard $ not $ null l Or guard . not . null $ l I prefer the second one.
Monadius! http://www.geocities.jp/takascience/haskell/monadius_en.html
I just updated the article to make it more accessible. Hope that helps!
Sometimes (but rarely) I prefer the latter to emphasize composition. But most of the time I just write the former, as you can notice. If I made this choice consciously, I'd probably write guard $ not . null $ l Besides, the first version looks more uniform.
Beware that `MonadComprehensions` slow down list comprehensions.
ghc 7.4.2, bytestring-0.10.0.1 (had to install a newer version), OSX ML, Python 2.7.3, dataset of about 75MB. You have take my 8 seconds compared to the rest of the results in the post. Python does 6s.
 Isn't it strange, that you needed to explain the second version in detail, but still it should be more clear than the first one? 
guard . not $ null l (minimizing the number of operators, and never being greedy with too many $'s)
Using 100 mb I can't find a way to make the python faster $ time python chars.py &gt;&gt; /dev/null real 0m11.537s $ time ./dcoutts &gt;&gt; /dev/null real 0m9.478s $ time ./charbuild &gt;&gt; /dev/null real 0m9.449s where charbuild is an idiot simple variant using Builder http://hpaste.org/76764
Sorry :) — I didn't want to spend much text on it. Here's a fuller discussion. http://en.wikipedia.org/wiki/Epsilon_calculus#Hilbert_notation Edit: I also edited the post — thanks for calling me out.
Reductio ad absurdum: https://gist.github.com/3954170 jamie@bandit:~/tmp/capitalize $ time ./a.out &lt; file &gt; /dev/null real 0m0.050s user 0m0.020s sys 0m0.017s jamie@bandit:~/tmp/capitalize $ time python capitalize.py &gt; /dev/null real 0m0.920s user 0m0.667s sys 0m0.230s Well, I guess I'm all done writing programs in python now too. Crap.
See [this answer](http://www.reddit.com/r/haskell/comments/11yrpi/fay_slides/c6rn364).
I think it's the same with any high-level concept - first you need to learn it, then it pays off.
While I wouldn't necessarily recommend this approach to beginners, probably the "easiest" (repeatable) way to get max performance with I/O is to use an iteratee library. I like Pipes/Frames more, but Conduit comes with all the batteries included. You can get the gist [here](https://gist.github.com/663d603cf77d38d642f3). Updated. See Gist for history. {-# LANGUAGE ScopedTypeVariables #-} module Cap2 where import Data.Conduit import Data.Conduit.Binary import Data.Char8 import Data.Foldable (fold, foldMap, toList) import Blaze.ByteString.Builder.Char8 as Builder import Blaze.ByteString.Builder as Builder import qualified Data.ByteString.Char8 as B capitalizeC :: (Monad m) =&gt; Conduit B.ByteString m B.ByteString capitalizeC = loopChunks ' ' where loopChunks contextChar = await &gt;&gt;= maybe (return ()) (loopBytes contextChar) loopBytes (lastChar :: Char) (moar :: B.ByteString) = do let scrubber = \last curr -&gt; if (isSpace last) then toUpper curr else curr yield $ B.scanl scrubber lastChar moar loopChunks $ B.last moar main :: IO () main = runResourceT $ sourceFile "input.txt" $= capitalizeC $$ sinkFile "output.txt" This runs about 75% faster than the original python over all but trivial inputs. Unlike the Python version, it runs in constant space and is not sensitive to things like line length, unlike other implementations which try to ignore history. The other point is this does not collapse whitespace, which honestly I consider to be a bug in the original script and something I'd probably do in a separate conduit. I am by no means an expert at Conduits, so if someone comes along and says, "This version is tedious to read! You could do it better by saying THIS" then I'd be happy to hear that. My experience with Data.Enumerator is that the only sane way to write iteratees without writing insane insider-baseball-style type signatures is to use ScopeTypeVariables on your inner loops.
The first and second observations are important. Here's a bytestring version that uses ASCII-only toUpper and isSpace functions and is 7-8 times faster than the python version on a 74MB lorem ipsum file. (Based on emilhedevang's, mifrai's, and ninegua's suggestions.) module Main (main) where import qualified Data.ByteString.Char8 as B import Data.Char (chr,ord) isSpace :: Char -&gt; Bool isSpace ' ' = True isSpace '\t' = True isSpace '\n' = True isSpace '\r' = True isSpace _ = False toUpper :: Char -&gt; Char toUpper x | (x &gt;= 'a' &amp;&amp; x &lt;= 'z') = chr $ ord x - 32 | otherwise = x fun :: Char -&gt; Char -&gt; Char fun a b | isSpace a = toUpper b | otherwise = b convert :: B.ByteString -&gt; B.ByteString convert = B.tail . B.scanl fun ' ' main = B.readFile "file" &gt;&gt;= B.putStr . convert **Update:** Indeed, this outperforms the C program on the OP's website. above program: 0.995s C program: 1.773s python program: 7.791s 
 In [10]: "You're sure about this?".title() Out[10]: "You'Re Sure About This?" I'm not.
I'm curious why the `toUpper` and `isSpace` functions from `Data.Char` are so expensive. Using them instead of the hand-coded versions above makes the program 7x slower (as I notice ninegua already observed). Is there something that could be optimized here, or are the complexities of proper unicode handling just that expensive?
It seems here that it is faster than all the others if it is compiled with a 64 bit ghc: ./charsghc64 0m1.504s -- the above haskell with 64 bit ghc python char2.py 0m1.757s -- the python one liner ./charsgcc64 0m2.053s -- gcc -m64 -O3 ./charsgcc32 0m2.541s -- gcc -m32 -O3 ./charsghc32 0m2.745s -- the same haskell with 32 bit ghc-7.4 python chars.py 0m12.373s -- original python
Change it to Word8 and it'll definitely beat the C version. -edit maybe not
 $ ghc --version (64 bit) The Glorious Glasgow Haskell Compilation System, version 7.4.1 $ khc --version (32 bit) The Glorious Glasgow Haskell Compilation System, version 7.6.1 $ python --version Python 2.7.3 $ gcc --version i686-apple-darwin10-gcc-4.2.1 (GCC) 4.2.1
I'm thinking that `words` is slow because `isSpace` is slow. Replacing `isSpace` and `toUpper` with hand-written ASCII-only versions gives me a 7X speedup. 
`many` repeats an action until it fails and collects all the successful results. However, there are multiple ways that something can "fail". In fact, there are as many ways as there are instances of `Alternative` (which defines failure as `empty`), so he has to select one and he chose `MaybeT`, which defines failure as returning `Nothing`.
Odd: your version is slower than my original version in my tests. Perhaps it's because of the difference in `isSpace`? 
Nope, I haven't tried it yet. But discussed it, I think a Worker monad would be in order. Web workers aren't capable of accessing the DOM and whatnot, it would make sense to encode that.
Python version also splits on newline.
&gt; | (x &gt;= 65 &amp;&amp; x &lt;= 90) = x - 32 Those aren't the ASCII values for 'a' and 'z', you're thinking of 'A' and 'Z'. It won't replace any letters.
Right, it'd replace with garbage. Fixed.
 fmap fromJust . runMaybeT . many . mfilter (not . null) $ lift getLine
 cheat :: a -&gt; b cheat = undefined
I've worked with GPipe. It wasn't too difficult to understand, except I had one function "paintColorRastDepth" using up 98% of processing time.
I assume you don’t need anything but a straightforward particle system for this purpose. If you’re familiar with the modern OpenGL API, then I don’t see the point of using anything else. If you don’t want to deal with vertex buffers and writing shaders, LambdaCube can hide them from you behind a uniform API. While it’s true that it’s not documented due to still being in constant flux, I think the [VSM example](https://github.com/csabahruska/lc-dsl/tree/master/samples/shadow-mapping) could be a good starting point, since it shows you how to define geometry as well as shading descriptions from scratch.
? class Seq a where seq :: a -&gt; b -&gt; a 
[More on that.](https://github.com/milessabin/shapeless/issues/7)
This is the approach I used: module Main where import Data.Char import qualified Data.ByteString.Lazy.Char8 as C cc :: Bool → Char → (Bool, Char) cc inWord c | isAsciiLower c = (True, if inWord then c else chr (ord c - 32)) | isAsciiUpper c = (True, c) | otherwise = (False, c) convert :: C.ByteString → C.ByteString convert = snd ∘ C.mapAccumL cc False main :: IO ∅ main = do name ← C.readFile "file" C.putStr $ convert name λ ~/src/haskell-strings &gt; time ./main &gt; /dev/null ./main &gt; /dev/null 1.32s user 0.04s system 99% cpu 1.360 total λ ~/src/haskell-strings &gt; cat case.py print open('file').read().title() λ ~/src/haskell-strings &gt; time python case.py &gt; /dev/null python case.py &gt; /dev/null 1.50s user 0.11s system 85% cpu 1.876 total λ ~/src/haskell-strings &gt; time ./script &gt; /dev/null ./script &gt; /dev/null 1.10s user 0.02s system 99% cpu 1.127 total λ ~/src/haskell-strings &gt; wc file 1247491 12024000 81402479 file
Is this the haskell golfing thread? ;)
I like the example. But the possibility to do a one-liner here depends on the existence of the function "title" which I suppose is provided by some library (perhaps a Python standard library, I don't know, it's been a decade since I last programmed in Python). The one-liner can be broken into two parts: IO and computation. print open('file').read() handles the IO, and .title() performs the desired computation. Similarly, main = convert `fmap` X.readFile "file" &gt;&gt;= X.putStr does the same thing in Haskell. The computation is performed by "convert", and the rest is IO. Sometimes you are lucky and there is a library function that already does exactly what you want. A good programming language should be backed up by lots of well equipped libraries. But since every possible function cannot be forseen and implemented, you sometimes have to implement it yourself. In the case of capitalizing initial letters, I find it desirable to be able to express the algorithm in a simple and straightforward way, for example as a scan over the string (or, equivalently, as a mapAccumL as suggested elsewhere). Ensuring that such a scan is computed efficiently without too many unnecessary intructions is a burden that rests on the shoulders of the wizards behind the Text and Bytestring libraries.
I wrote a library for doing simple scientific visualization. I have taken the liberty of hacking together a quick lorentz attractor example for you, I hope you don't mind. The result looks like [this](http://i.imgur.com/hpbiB.png). To build, follow these instructions. git clone git://github.com/ghorn/not-gloss-lorentz.git cd not-gloss-lorentz cabal install lorentz The code is [here](https://github.com/ghorn/not-gloss-lorentz/blob/master/src/Main.hs) My library is called [not-gloss](http://hackage.haskell.org/package/not-gloss) because it's inspired by the awesome 2d drawing package gloss. I use not-gloss for stuff like [this](http://www.youtube.com/watch?v=HBR9z2P3_as&amp;feature=g-upl) Btw I don't have incredible documentation but I do have [examples](http://hackage.haskell.org/package/not-gloss-examples)
As I said, it's debatable. (Although I didn't envision character count being an argument ;)
You have to admit that it took quite a bit of effort and caused a large number of people to scratch their heads. I also think it's unfair to judge the whole language based on performance in this small benchmark. The benefits of Haskell go far beyond performance.
It sounds like that function has the draw call. Nothing to be alarmed about.
That's exactly my point. What I wanted to show is that if you really want to learn and use Haskell, you can always come up with a VERY good solution in terms of performance. The way it was treated was so superficial that I felt I had to do something :) One last thing, though: it may seems overcomplicated, but we are only using the same algorithm already proposed, but switching over a more performant I/O system. What I like about Haskell is that you have a lot of choice in terms of how accomplish the same task, and that's the beauty of it!
Could I be correct in thinking the following? OpenGL is a very stateful system and the API is like a finite state machine. This contradicts "the functional way", so all configurations you declare before the draw call are set at that moment. 
what words accompanied the Nasreddin slide?
Well, my purpose was not raise a flame (I still hope not) but just to show that with some little tricks we can still obtain good performances. As someone said (I believe bos) Python's string mangling is a thin wrapper over C call, that's where the huge speed comes from. We should not forget that what we pay in Haskell is higher: we have lazy string, lazy IO and any sort of elegant data structure to work with, so what is considered "naive" in Haskell cannot be (usually) as fast as naive imperative, strict code. And from my point of view, I learned that's extremely superficial to compare orange with apples. My implementation is just like the original one, but the huge gain was from using a different I/O system. Not naive, but fair, I think :)
Would this work if the chunks were `["foo ", "bar"]` and not `["foo", " bar"]`? Also would be good to compare the conduitified version to the lazy IO equivalent, since you're not using words/unwords lines/unlines.
Elm is getting cooler by the day. Thanks for your work on it! I know this is a very shallow complaint and possibly not even in the direction you want to go; but, I wish the {;} weren't required. I know this isn't Haskell, but the syntax is just Haskelly enough that it puts it in the uncanny valley for me.
Thank you! It's my pleasure :) That {;} is actually valid Haskell syntax! You can do case, where, and let with that syntax in Haskell :) I would like to get rid of that though, and someone is working on more syntactic sugar at the moment.
Thank you for this variant. However, I don't understand why so many people bite this bone (178 comments so far is the thread you reference...). So yeah, somebody on the web found a 3 lines python script that runs faster than a 3 lines haskell script. But seriously, who cares ? As educated programmers we all know that this observation is absolutely meaningless. [someone is wrong on the internet :)](http://xkcd.com/386/) If somebody really just wants to capitalize the content of a file, should we advocate Haskell over anything else ? I certainly wouldn't, as any language at hand will equally do the trick. Even my text editor will do the trick in a couple of key strokes. Let's concentrate on real programs that are actually powerful and really better expressed in haskell than in other languages. And there are tons of them.
&gt; I believe it should be stated clearly and early in all Haskell tutorials that String, and in general [a], is a convenient and simple data structure, but at the same time very, very slow if large amounts of data are to be processed. Which is pretty ironic considering the list is considered *the* quintessential functional programming data type, and operations upon them, such as mapping and folds, held up as examples of quintessentially idiomatic Haskell. Personally, I think this issue is pretty simple: idiomatic Haskell is often slow. If you want performance, you have to start looking at unboxed types, monads that enable mutation, or advanced libraries like conduit. But out of the box Haskell, learned from a typical tutorial, is not going to demonstrate good performance. Period. And it's not trivial for a beginner to climb out of that box. Of course, not all code requires high performance, in which case idiomatic Haskell is just fine. But when you do, it's hard to find good references that help you write performant Haskell. As such, what I'd love to see is someone put together a set of tutorials on how to write performant Haskell using those libraries you listed, along with other more advanced techniques, so that beginners have an avenue for graduating to that intermediate level where they have a chance to hit performance goals when they pop up.
&gt; Would this work if the chunks were ["foo ", "bar"] and not ["foo", " bar"]? It looks like it works for those, but not for ["foo","bar"]. The output is "FooBar" instead of "Foobar". Depending on how conduit decides to break up the input, the output will be wrong. I tested this by replacing sourceFile "text.txt" with CL.sourceList ["foo","bar"]
Indeed, but we all learned something.
In my opinion, using things not in the standard library when the standard library has (possibly slower) versions of them never yields a naive implementation.
QQ. Does this one work with Unicode ?
&gt; That {;} is actually valid Haskell syntax But it's hardly used outside of generated code or getting one liners. I'm glad it's being worked on! Thanks
&gt; someone is wrong on the internet :) On of the best XKCD comics. And of the most relevant. But more seriously, here it's relevant so that people who don't know Haskell and who stumble upon Honza's post are not led into false beliefs regarding Haskell performances.
&gt; What I like about Haskell is that you have a lot of choice in terms of how accomplish the same task, and that's the beauty of it! I think that's wrong, so don't worry: you only have lazy IO (aka "the bad way") and the iteratee way (aka "the good way"). I know it sounds coarse, but I never saw any post in the Haskell community advocating the other way around, so it's not like you have a lot of (good) options to get one thing done. So we can hardly call that "functional perl". More like "a capacity to always improve our means". That's not Python's spirit. That's definitely not Java's spirit. But that might be Ruby's.
&gt; Your implementation is not a naive implementation. It is for someone who knows the new Haskell way to handle IO. The thing is that Haskell is kind of in a transition phase wrt this.
I am alarmed because it uses that much CPU drawing 100 cubes... The framerate was only about 20 as well.
Haskell has a well-defined desugaring from whitespace to {;} -- perhaps you could just reuse some part of ghc (or compiler of your choice, or maybe HSE) that performs this code transformation phase? I'm unfamiliar with the particular way that any given compiler handles this, but I imagine that *somebody* has written that desugaring as a standalone transformation.
&gt; or is the final acceptance that haskell is not a useful practical language just too painful to even respond to? If you are genuinely concerned about the efficiency and adoption of Haskell, I would recommend you find a way to rephrase statements like that, as they can easily be interpreted as trollish, as can another example in these comments: &gt; congratulations. it took an entire community three days to figure out how to quickly capitalize text in a chunk or lorem ipsum
It's rather disingenuous to complain that the "standard" ways of doing things have poor performance, and then turn around and also complain that people experiment with ways of getting better performance, which can perhaps be put under nice interfaces and some day become standard. What does it mean to be "less sympathetic" when people point out a proper library for working with arrays? It's ignorant to claim arrays and linked lists don't both have their appropriate uses - especially when [] is reasonably often used as control flow/iterators in ways that can be expected to fuse away. Is Rust's [list](http://dl.rust-lang.org/doc/0.4/std/list.html) module an acknowledgement that plain arrays are not useful?
I know very little of who are what kenny-rogers is/was. Singer? Skittish character from an unfunny sketch? It's just a name that happened not to be taken on reddit which was easily remembered.
&gt; If you’re familiar with the modern OpenGL API, then I don’t see the point of using anything else. Fair enough, but this doesn't end the discussion as it leaves you with three choices: - OpenGL (high-level binding) - OpenGLRaw - C functions making direct OpenGL calls and FFI to enable you Haskell code to call them. (can be convenient, as OpenGL is anyway low level, if you're already familiar with C OpenGL. Plus modern OpenGL forces you to go through some marshalling anyway.) (thouroughly debated before ;) )
Well, it's a good question - "Why is this so much slower?"- naively executing the Text version should be doing exactly the same amount of allocating buffers, copying, etc. as unicode-aware Python, so it's odd if it's slower (and bos seems interested in that). A more interesting part is the delta compared to C, not Python. Can a library be written so that kind of high level code turns into an efficient transducer on buffers competitive with C?
For my machine the haskell is faster as well: gcc cap.c -O3 -o cap; time ./cap &gt; /dev/null # gives user of roughly 4.3 ghc cap.hs -O3; time ./cap &gt; /dev/null # gives user of roughly 2.2 This is on a 485mb file of a repeating Lorem Ipsum paragraph in the tmpfs /tmp ghc version 7.4.1 gcc version 4.6.3
Whitespace sensitivity is really tough! I have not done it specifically because it is not easy. "reuse some part of ghc" is a really really hard task because every part is dependent on the underlying AST and design choices and because GHC is a really complicated project. Working on getting this feature though!
We’re taking the middle road with OpenGLRaw, so we can still use the world’s best imperative language. ;)
&gt; the haskell community used to be receptive to criticism. How is this conversation anything but reception to the criticism? Bos even popped up to say, "Despite the tone, I agree and we'll work on making this use case better." The blog post itself was obviously trollish. To argue that somehow the Haskell reddit community has not accepted this criticism with an unusually high degree of grace for a forum community is fairly outrageous. &gt; or is the final acceptance that haskell is not a useful practical language just too painful to even respond to? Haskell is every bit as much a useful and useful language as is Erlang, Java, Ruby, Python, etc. In that, like all tools, it has situational benefits and disadvantages. Erlang is still quite useful despite it being all but impossible to write a fast version of this program in Erlang. I think the reason you have downmodders on you is because it's pretty clear you have a chip on your shoulder and you've been rattling a saber about Rust. Like someone randomly coding a bad (not naive, unless naive also means really bad) algorithm for a non-problem is somehow evidence of anything besides one novice's inability to write to an environment. And also because your name is tr0lltherapy, you're flirting with user profiling anyways. 
Since when?
Everyone I know, myself included, spent maybe 2 hours on this exercise. Most of _my_ time was spent writing test cases in other languages. My first version outperformed the Python and only got better from there after a later realization I had missed an API call. So no. What you said is wrong. An entire community *discussed* this idea; with significantly more grace than most Reddit environments would have shown. 
Sure, why not?
&gt; we are discovering that this is untrue Says who, exactly? Erlang is _much_ worse at string operations, but has incredible applications in distributed systems. Java versions of this program need to be written very carefully not to accidentally leak memory everywhere because of the way strings are segmented. Shall we declare these two staples of the software industry useless when a novice doesn't know the right way to write something quickly? And just note, the Python version as presented is an incredibly bad program that can only be used in one-off wargaming like this. In the context of a real world deployment like, say, a Hadoop cluster or as a component in a web stack or as part of a GUI program? It will thrash the memory of a busy box if it hits a big input, messing up performance for everyone on that server. So yes. Haskell's interpretation of _this_ specific boneheaded algorithm is worse than Python's because Python relies on C to make this boneheaded operation "fast" in isolation. Writing it correctly for Python puts it within 3-5 factors of C, simple correct haskell implementations get within x0.75-x2.
I think that is very likely true. That's a very interesting way of putting it.
The reason they exist in libraries is exactly so they *don't* have to exist in client code!
Why bother with the Conduit at all? I think what made all the other versions slow, as well as the OP's Python version, is all the memory allocations. You can just print the result of convertWord directly. The OP's C version is still much faster though. % cat bs-capit.hs import Data.Word8 import Control.Monad import qualified Data.ByteString as B import qualified Data.ByteString.Char8 as C c2w8 :: Char -&gt; Word8 c2w8 = fromIntegral . fromEnum capitalize :: B.ByteString -&gt; B.ByteString capitalize = B.tail . B.scanl (\a b -&gt; if isSpace a then toUpper b else b) (c2w8 ' ') main = (B.putStr &lt;=&lt; (return . capitalize) &lt;=&lt; B.readFile) "file" % time ./bs-capit &gt; /dev/null ./bs-capit &gt; /dev/null 3.75s user 0.22s system 99% cpu 3.971 total % time ./stain &gt; /dev/null ./stain &gt; /dev/null 4.20s user 0.11s system 99% cpu 4.319 total % time python python.py &gt; /dev/null python python.py &gt; /dev/null 10.98s user 0.41s system 99% cpu 11.392 total % time ./c &gt; /dev/null ./c &gt; /dev/null 2.62s user 0.05s system 99% cpu 2.673 total 
&gt; A more advanced user can provide a little bit of magic to fix the situation (typically involving a hard reset to a hash from the reflog), but to a beginner that's all that is: magic. Are we comparing the use of Git's reflogs (which are sufficiently explained by their names: a log of a reference, the latter being quite a common concept since most Git beginners have a programming background) with the arcane skills needed to perform optimization? And I'm not even speaking about Haskell, it's not like tools like Valgrind are a total breeze to use by a beginner. &gt; cargo cult programming Nice one ;)
I have only learned Haskell relatively recently but I did not get the impression that lists were intrinsic to Haskell. Defiantly nothing like when I learned scheme or read lisp tutorials. edit: typo
Consider me alarmed as well, then.
Could `Data.ByteString.Lazy.Char8.unwords`/`unlines` be implemented using `Builder` internally, rather than producing lazy `ByteString`s with zillions of small chunks? I wonder how much performance one can squeeze out of the original code just by reimplementing the library functions it uses, rather than by making any more substantial changes to the structure of the code. For example I got a substantial speedup by reimplementing `B.putStr` using `writev` (though I imagine the same speedup would not apply to your `Builder` version).
One thing we learned in the discussion about the original post is that the `Data.Char` functions `isSpace` and `toUpper` were responsible for a lot of the slowness of the original ByteString versions. [It was shown](http://www.reddit.com/r/haskell/comments/120h6i/why_is_this_simple_text_processing_program_so/c6rns0b) that if you write your own ASCII-only `isSpace` and `toUpper` functions, you can get better performance even than the C program in the original blog post. (The slowness of `isSpace` also presumably explains why `words` was taking so much time in the original version.) I'm still quite curious about why `isSpace` and `toUpper` are so expensive. Granted, getting these right for unicode is more complex than getting them right for ASCII, but we were seeing a 7X speedup. Is it possible that there is something that could be further optimized in `Data.Char`? 
Here is a graph of what the project has raised: ooG|750K o | o | oo | oo | ooo |500K oooooooo | oooooooo | oooooooo | ooooo |250K ooooo | ooooooo | oooooo | oo | oo |0 -------------------------------------------------- 9/249/30 10/6 10/12 10/17 10/23 [Click to see full graph](http://canhekick.it/projects/adapteva/parallella-a-supercomputer-for-everyone) 
I love Haskell and am just getting into embedded software; why should I care about this? What sort of cool things could I do with Haskell and one of these boards?
Wait, so haskell doesn't have something like unwords . map ?isdigit . words into a single loop: for(i=0;x[i]!='\0';i++) { if(x[i]!='\n') x[i] = ?isdigit (x[i]) } I mean, does this never happens? In no case whatsoever? If this is the case, I'm amaze that we have so much performance and still have that amount of room for improvement.
Woohoo! It met its funding goal! Thanks everyone! I notice they list several languages currently being targeted at this thing but I don't see Haskell as one of them. That is far beyond my currently meager skillset but I'm hoping one of you can remedy that. I might just have to go for parallel programming this thing with Erlang at first if Haskell isn't yet up to it. 
In the second video, at 1:35 - High performance: up to 45 GHz performance - Easy to use: C / C++ / OpenCL / Python / etc Am I right if I assume that you either have 45 GHz, or you program it in C / C++? (Although I'm looking forward to it, sounds promising.)
Genius! :D
Kick ass
I did the same thing. I just used a jQuery example slideshow and replaced their demo images with my own. No need to write any code.
hask-owl
&gt; i'm saying easy tests are easy and every decent tool needs to ace them Thus apache and nginx aren't even decent tools? 
I believe the library does not implement fusion for words and unwords, which is why you don't get this tight inner loop.
The reference, in case anybody else didn't get it either - take heed the date - http://www.haskell.org/pipermail/haskell/2009-April/021173.html 
[Analogy as the Core of Cognition](http://www.youtube.com/watch?v=n8m7lFQ3njk)
Brent Yorgey has put it very well, I think: * http://byorgey.wordpress.com/2009/01/12/abstraction-intuition-and-the-monad-tutorial-fallacy/ * http://byorgey.wordpress.com/2012/07/19/monads-easy-or-hard/
I think the best approach is to begin from several concrete examples and then look for the overarching pattern. This is what the [best monad tutorial](http://blog.sigfpe.com/2006/08/you-could-have-invented-monads-and.html) does.
The class was called `Eval` back in the days when `seq` was first introduced. But the class was removed later.
http://learnyouahaskell.com/recursion The picture of the qsort implementation at the bottom of this page demonstrates the easiest way to explain the algorithm to just about any human being. I reproduced that very picture to explain it to some family members. Although, I'm also a little confused why these are used so often. I did not understand what fmap did until I tried this: fmap (+2) (Just 1) = Just 3 fmap (+2) [1,2,3] = [3,4,5] And I realized instantly what fmap was doing. Every time I encounter a new Haskell feature, I do a simple, stupid test like this, which usually produces a "well duh", meaning I understand it. This is how I figured out how monads, do notation, Applicatives, Alternatives, StateTs, ContTs, etc., and all work. Sometimes, I think the first mistake is assuming that these concepts are difficult. It's their application that can be... well, thought provoking. But one thing analogy and metaphore can occasionally to amazingly well is take something that *seems* complex and make it dumbfoundingly simple, like that qsort picture.
When someone has the task of explaining a difficult topic, the first thing they do is to try and find an analogy or a metaphor, regardless of whether it can be explained a different way. That's why.
&gt; pigworker destroyed this question. And my brain. I'll reread his answer several times over the next few days and see if it starts sinking in. I really like this sort of mathematical stuff, but I'm not familiar enough with that mode of thinking to grasp what he said without a lot of contemplation.
fmap (+2) (Just 1) = Just 3
Haskell Platform isn't distributed as a package on hackage. It is primarily distributed as a ] bundle for each OS's native installation system. The idea is that it is a standard way to bootstrap yourself into an environment where you can then cabal-install other things. The link of this post has links to native installers for Mac OS X and Windows, and the "source tarball" which can serve as a buildable bootstrap for unix-like platforms. Haskell Platform is also a reference of a consistent set of versions of a core set of packages. The .cabal file you found is spec. for that.
Lists are plenty useful yet. Idiomatic code works well 99% of the time. Performance is sometimes significantly more painful than other languages. Laziness is great, but also has large drawbacks. Despite all these tradeoffs, I find Haskell to be the most suitable language for applications programming by professionals today. The huge benefits of type safety, great syntax, and various other things that are mostly unique to Haskell dwarf the occasional performance difficulty you encounter now and then. 
Interesting! I do have an idea. Look at the code for `Data.Char.isSpace`: isSpace_DataChar c = c == ' ' || c == '\t' || c == '\n' || c == '\r' || c == '\f' || c == '\v' || c == '\xa0' || iswspace (fromIntegral (C.ord c)) /= 0 I assume the point of the disjuncts at the beginning is to avoid calling `iswspace` for the most common space characters. But of course, this short-circuiting only helps when the character being tested *is* a space; when it's not, `iswspace` always gets called. Nonspaces outnumber spaces in most normal texts. So one way to (try to) speed up `isSpace` would be to test for the most common *nonspace* characters before calling iswspace. Here's a version I tried: isSpace_Alt :: Char -&gt; Bool {-# INLINE isSpace_Alt #-} isSpace_Alt c | c &gt; '\x20' &amp;&amp; c &lt;= '\x7F' = False | '\t' &lt;= c &amp;&amp; c &lt;= '\r' = True | c == '\xa0' = True | otherwise = iswspace (fromIntegral (C.ord c)) /= 0 This was over twice as fast as the version copied literally from `Data.Char`: benchmarking isSpace@Char/isSpace_DataChar mean: 110.0236 us, lb 109.9142 us, ub 110.2757 us, ci 0.950 std dev: 807.3838 ns, lb 429.1935 ns, ub 1.656002 us, ci 0.950 benchmarking isSpace@Char/isSpace_Alt mean: 45.59692 us, lb 45.50833 us, ub 45.79376 us, ci 0.950 std dev: 647.2863 ns, lb 360.2003 ns, ub 1.304892 us, ci 0.950 found 7 outliers among 100 samples (7.0%) 6 (6.0%) high mild 1 (1.0%) high severe variance introduced by outliers: 7.504% variance is slightly inflated by outliers And I get over a 3X speedup when I use lorem ipsum text instead of 0..255 for the benchmark. 
Sorry, I left out the most important case, the space character itself. Adding this, and changing the upper bound for common nonspace characters so that 0xa0 isn't included, we get: isSpace_Alt :: Char -&gt; Bool {-# INLINE isSpace_Alt #-} isSpace_Alt c | c &gt; '\x20' &amp;&amp; c &lt; '\xa0' = False | c == ' ' = True | '\t' &lt;= c &amp;&amp; c &lt;= '\r' = True | c == '\xa0' = True | otherwise = iswspace (fromIntegral (C.ord c)) /= 0 we get a HUGE speed boost over Data.Char's version: 168us for `isSpace_DataChar`, vs. 30us for `isSpace_Alt`, using lorem ipsum benchmark data. (With your 0..255 benchmark data, the difference is less striking, 101us vs. 42us, but that is still significant.) Further note: if we remove the test for nonspace characters from `isSpace_Alt`, we still get big speed improvements (168us vs 92us with lorem ipsum, 101us vs 59us with 0..255). (Edit: fixed bug in code and revised benchmarks for new code.) 
(In case it wasn't clear, that last paragraph was mocking stereotypes, not being serious!)
Ahhh this is so cool. :-) Also serves as a good test subject for directions in optimization.
It teaches you to write type-checking code!
Using your idea to optimize isSpace, but for Data.Word8, I get the following timings for 200,000 copies of lorem ipsum (85 MB): * 17.514s Honza Pokorny's Normal.hs [link](http://honza.ca/2012/10/haskell-strings) * 1.546s fiddlosopher [link](http://www.reddit.com/r/haskell/comments/120h6i/why_is_this_simple_text_processing_program_so/c6rns0b) * 1.258s Andrew Stewart's C program [link](http://honza.ca/2012/10/haskell-strings) * 0.807s Response by CharlesStain [link](http://www.reddit.com/r/haskell/comments/124bg6/response_to_why_is_this_simple_text_processing/) * 0.768s tomejaguar [link](http://www.reddit.com/r/haskell/comments/124bg6/response_to_why_is_this_simple_text_processing/c6s9sci) * 0.343s My code (3.7x faster than C) [link](http://www.reddit.com/r/haskell/comments/120h6i/why_is_this_simple_text_processing_program_so/c6snv0d) 
ahhh, natural selection.
My one teacher described it fairly well: our ability to understand something can be helped by intuition, but the intuition can be quite wrong if untrained.
It's not a bug, it's a feature! :)
The reason is this: Your brain is like an airport. There is a TSA officer standing at the entrance, preventing any disruptive thoughts from entering. TSA officer: "Football; check. Java; check. Burritos; check." Tutorial: "And the burritos are monads." TSA officer: "MONADS ?!! OH NO!! Raise the alarm!" But it's too late. Now you understand monads.
Can someone post the tweet or an image? I cant access twitter from here.
Now I understand analogies.
Bryan O'Sullivan ‏@bos31337 says "So glad you asked! The best ghc bug ever involved a dev version of the compiler deleting your source file if it contained a type error."
About that. Can Fay handle .cabal packages in some way? I'm keen on getting my reactive-banana FRP library to work with one of the Haskell -&gt; JS compilers, but so far, the main problem is to compile all the dependencies.
This is great work. We should try to make sure it makes its way back to the core libs as a patch.
&gt; Tutorial: "And the burritos are monads." As from GHC 7.6.1, that meme raises a deprecation warning.
Agreed! How does one do that? 
Yeah, unfortunately as far as I can tell, GitHub won't recognize *.lhs files as READMEs, so it won't be shown on the front page. But I may end up renaming it anyway, for the reason you suggest.
Apparently there is already a package named "records" on Hackage, by Wolfgang Jeltsch.
This is the libraries process: http://www.haskell.org/haskellwiki/Library_submissions Since the change is not semantic, you do not need to follow the whole process. However, the feedback on the `libraries@` list can be quite good, so more eyes might squeeze out further performance. Either way (after a libraries@ discussion or not), you can submit a patch to the ghc trac, as described on the linked page.
&gt; So Elm does not have typeclasses right now IMO that's not a problem. You can easily get by with datatypes and explicit dictionaries (in the Haskell sense) passing. I think you should wait for people to extensively use and feed back on Elm to determine if undertaking the addition of the typeclasses monster to Elm really answers to a need of if it would just be a pointless copy of Haskell's features. Elm's not Haskell, after all, since it targets client-side web development it doesn't have to address as wide a range of issues. **tl;dr** Typeclasses are not _the_ way.
Perhaps your packages can be merged?
This is really cool. Is there a way to get around defining the fields twice? (Once for the field definitions, then again if you want to make a group of fields into a type)
I wish, [this course](http://www.scs.stanford.edu/11au-cs240h/) will become an online coursera course one day.
These timings for 200,000 copies of lorem ipsum (85 MB) are total = user + sys (ignoring real), choosing most effective optimization levels using ghc version 7.4.2 and gcc version 4.2.1 on Mac OS X 10.7.5. * 0.609 = 0.314 + 0.295, Syzygies, no check for _nbsp: https://gist.github.com/3969348 * 0.576 = 0.456 + 0.120, jamwt C code: https://gist.github.com/3954170 * 0.414 = 0.296 + 0.118, Syzygies C rewrite: https://gist.github.com/3969330 * 1.873 = 1.545 + 0.328, fiddlosopher (original) * 1.747 = 1.408 + 0.339, fiddlosopher2 (isSpace_Alt) * 2.932 = 2.585 + 0.347, fiddlosopher3 (isSpace_Alt, mapAccumL) The fiddlosopher code that I used is here: https://gist.github.com/3969553 So Data.ByteString.Word8 is much faster than Data.ByteString.Char8, and in Word8 (but not Char8) mapAccumL is faster than scanl. Assembly-style C remains fastest. 
Well, it's hard to learn Haskell without seeing the λ in the logo...
I've settled on the "pick silly distinctive name that is evocative of library function" approach. Free suggestions: vinyl, recardo, rec-roll, recstensible... All of those are terrible but please don't choose a generic subtly-different name.
Agreed! I'll try to take your advice to heart.
Using current language extensions, would it be possible to implement extensible records following the &gt; "fields should have to be declared and used explicitly, like type classes" camp? I ask because it seems like having implementations of extensible records seems to be a solid way to compare the pros and cons of each, package name collisions aside.
a bit different from Coursera, but still imho quite relevant [Dr. Erik Meijer - Functional Programming Fundamentals](http://channel9.msdn.com/Series/C9-Lectures-Erik-Meijer-Functional-Programming-Fundamentals/Lecture-Series-Erik-Meijer-Functional-Programming-Fundamentals-Chapter-1)
It's likely Fay will never be able to compile the dependencies for reactive-banana. Fay doesn't even have type-classes. GHCJS/Haste/UHC have better chances if you want “all Haskell plus bells and whistles”.
Also submitted to Hacker News: http://news.ycombinator.com/item?id=4709986
lovely ideas! :) 
This looks *really* neat.
&gt; So, this isn't your grandmother's "String Oriented Programming". But I like Javascript. Every day is a surprise! As for the operator my "complaint" was I didn't like reading the deliminator as 'and' in my head. All things considered you're probably better off spending your brain power on illissius's comment regarding scoping of field names (my vote goes to explicit and non-global).
I've just hacked up an alternative syntax for this: jon = Record :- name :=: "Jon" :- age :=: 20 :- sleeping :=: False Which I think is much neater. I'm open to suggestions for better operator names, but I still think this is an improvement. I can make a patch if anyone's interested. EDIT: Changes here: https://github.com/aninhumer/Data.Records/commit/ffb32d362327ed732c3fafe163e93c01c3ee2ccf Warning: this reverses the order of the list in the Rec type, and I haven't looked at all the uses of this, so it might lead to weird results elsewhere. 
I don't suppose it can be a Monoid? jon = name := "jon" &lt;&gt; age := 20 &lt;&gt; sleeping := False
Hey, that looks really nice! I'd love it if you would open a pull request with your changes and we can discuss them there. Thanks again!
Good catch! Of course the issue of whether the ASCII spaces or the ASCII nonspaces should come first could be different for perverse files. N.B., files with short lines and lots of blank lines would be perverse by this metric, though not necessarily bizarre to find in the wild...
I wonder what percentage of the time is spent doing the printf vs doing the for loop.... IO is expensive.
In fact with this style of records, I'm pretty sure you can mix and match both global and explicit styles of labels with no issue. However, I think global names are probably a better idea, since they minimize your dependency tree.
Different types in different places therefore no, no?
Smells kinda like duck typing. * edit - retracted - see below :) &gt; I only mention this so that people who might be thinking, "Wow! A working, extensible record system! Why doesn't everyone adopt it?" can instead get back to bickering over their differences. :) Indeed. A hard line to walk between never getting anything done out of fear of it being wrong and coming back in 2 years saying "We'd love to change it but we can't because everyone uses it". Hmm, I wonder if (apologies if I'm re-hashing old ground) module StandardCalculations where combinedSize :: (Has a "size" :: Int, Has b "size" :: Int) =&gt; a -&gt; b -&gt; Int combinedSize a b = size a + size b Would be a compromise. Limit your things by type if you want strict typing (so replace Int with BurgerSize), otherwise it still matches what other people want if you want it more generic. NINJA EDIT- This is already done in this library with :::! wakeUp :: IElem ("sleeping" ::: Bool) fields =&gt; Rec fields -&gt; Rec fields wakeUp = rPut sleeping False 
Ah, `Category (:=)` then? jon = name := "jon" . age := 20 . sleeping := False
Well, the dependencies for reactive-banana are actually just container types and the standard monad transformers, that's not the problem. What I am inquiring about is support for specifying dependencies in the first place. I mean, I would like to add conditional compilation to reactive-banana instead of creating a new package for each of the different Haskell -&gt; JS compilers. The current status is that it's possible to compile single programs, but there is little infrastructure for compiling larger programs.
The downside to the record representation is it is a list of (Field,value) and so there are many (,) and Field repeated everywhere. Wouldn't a newtype of the value be able to keep the Field type as a phantom parameter? What would break?
&gt; Nobody write about the parts of a game that result hard on Haskell. What do you mean here? &gt; The game loop, the global/world state, ...etc. If I understand what you're saying, I think it's a good idea too if the presentation abstracts out the details so that we can see the structure of the program at a higher level. The minutiae of state variables belong in the background. 
I think his point was that sometimes you aren't interested in the high-level part, and you are wondering how they dealt with issues like state in Haskell, because that's where people get stuck on what good design should be for that problem.
Okay, someone wanted his revenge...
&gt; data Firepower = Firepower { runFp :: Int } Use a newtype here. Data comes with an overhead which is not needed when you just want to go type-safe. Same for HP.
I haven't checked but I'm guessing that it's the input. Each alpha char has a 50% chance of being uppercase to start with. EDIT: Though I am amused of the `24 * 2` to choose a character!
There are almost no Haskell jobs out there, compared to, say, jobs that use Python or Ruby, and also compared to the number of programmers who know some Haskell. I don't think a mention of an online course will help you much. From what I've seen on Haskell ads, you probably need a project on Hackage to even be considered.
Indeed, I'm downvoting for this reason. It suffers from the same problem as the original post to which it replies; too much hot air, not enough meat or attention to detail.
Count me!
I don't know how efficient this is, but the functionality/code size ratio is very impressive.
Yes! Can always learn more!
I don't think it is dumb. It actually brings to light nuances that people are not familiar with , especially when experts chip in and start improving their language version. What is dumb is getting into flame-wars about it.
Because in english when chaining items you only say 'and' for the last item. I'm starting to think I shouldn't have raised the issue though because it's spawned a lot of conversation for something very minor. 
Thanks, will incorporate!
For what it's worth, honestly I'd prefer to just type `setf` than `:=:`, maybe I'm getting bored of silly operators these days, I don't know.
Sooner or later I will get to the point where the logic is satisfactory (for me), and will definetely add the architecture/plumbing. Maybe the next part is about logging, and afterwards we are ready to do the dirty work :) Undo as a feature is a good idea, didn't think about that yet. Why I chase to go "small gameplay" first? It's personal taste, I like seeing the essence working first. I know people who like the opposite, getting the menu screen, menu -&gt; game -&gt; menu transitions first, then doing the logic. The latter approach might have the advantage that the developer doesn't burn out at the mandatory but uninteresting part of creating menus, etc, since that is a prequisite to doing the actual fun part.
But the version using `ForeignPtr`s is far from idiomatic...
I like short introductory screencasts. 15-20 minutes on a specific topic that shows off some cool features. I use these more for motivation than for learning, though. It's a little easier to follow along in writing, but it's neat to see what can be done with different tools and, in my opinion, that's what's valuable about a screencast.
Here I would like to mention the functional reactive programming is definitely worth looking into. A very high level description of it is that it allows you to write your game logic as a function of time instead of a description of what should happen in every single time step, which is a lot more natural in a functional language like haskell. Also, it means that you don't really have to deal with state and game loops that much since it will be mostly abstracted away.
Sounds interesting; if you could show how you want it to work, I'd be interested in discussing it. If you really want to go all out, you could fork the project and send me a pull request... :)
You should check ELM
I'm lovin' this one! Haha!
Me too, but for god's sake, man, don't use inline unnamed decimal constants. At least use ('a'-'A') instead of 32, for instance. That was standard and recommended practice even back in the day. Nice tight C otherwise. Your point about system overhead and /dev/null is pretty important, but apparently overlooked by a number of the people doing these benchmarks. It's hard to get people to focus on these issues. BTW when I did a light cleanup of the other guy's C code, I made my own lorem file starting from standard lorem ipsum off the net, because I didn't notice the actual lorem file available for download with these other benchmarks -- did you do the same, or did you notice a download that I didn't?
The C code was written by Honza and published in his original blog post. I'm simply contrasting Haskell with what's already written. 
Just thought it's interesting/important for us to notice criticism coming from the outside, even if griping about Cabal has been done to death from the inside and folks are already doing what they can to improve the situation… (oh, sorry! didn't notice this was already posted to /r/programming yesterday)
For the record, I have not in a year and a half experienced "totally broken" from cabal, At worst, "slightly annoying". Then again, I install from apt whenever I can, because that's the right thing to do :)
I wonder where the repeated suggested to install the latest/greatest cabal-install came from. My guess would be that the author might not have put their ~/.cabal/bin (or ~/Library/Haskell/bin, etc) in their PATH? If so, maybe the “hey, there's a new cabal-install out!” could say something? Dunno if that diagnosis makes any sense though.
I know that. And Honza's C code sucks, and that needs saying. :) Just because someone else wrote it doesn't mean that it should be accepted at face value as a good basis for comparison, clearly. Somewhere in these threads, someone pointed out that the fine details don't matter as much as the fact that Haskell can surprise people with how fast it can be, so I am aware that even sloppy benchmarks can sometimes serve the positive goal of educating people -- but still.
Oh, hmm, he used `cabal-dev`. I avoid such weird things.
I never upgrade my cabal using cabal, that just smells bad to me. I keep the version my OS maintainers have vetted :)
Yes cabal, cabal-dev, version limits are broken. Yes, i can fairly easily install yesod, cabal-dev and start my web app. That's because i collected a lot of expert know-how via numerous tries and know the exact order of steps to not screw up. I can certainly see that for a novice this process would most likely be impenetrable. Having said that, we spend 2 days (!!!) preparing Delphi 7 dev. env for our new developer. :) And no, i have no insight how to fix this situation. 
That's coming AFAIK.
I don't think version limits are broken. What's broken is having a globally shared package database. That's being fixed in the next cabal release where we will have per-project sandboxes.
Is there an equivalent without Template Haskell? How much longer would it be?
Maybe the "Dependencies conflict" section at http://www.haskell.org/cabal/FAQ.html makes it clearer
I suspect the missing ~/.cabal/bin was the issue. Perhaps cabal-install should warn that the directory it installs binaries to is not in the current PATH? 
You cannot fix this problem save by changing the library in question. You know who also has this problem? Java. And it's really effing annoying. I've had to handle issues like this multiple times. I sort of wonder where the people who write articles like the top post come from, because my experiences with Maven, Ivy, Sbt, Leiningen, Rubygems, often contain bits of dependency hell like this. A lot of Java programs just ignore this problem, force compile, and pray it works. It's really fun to watch those expectations get violated in subtle non-exception-throwing ways.
A lot of swearing on some free software and its community.. While I felt annoyed by the same things he mentioned, I just did not feel compelled to bash the Haskell community, but instead found heaps of resources on how people are trying to fix it! His main point seems to be that the Haskell community is ignorant of it -- which is untrue. It just takes long to fix it, as it needs a lot of effort to really fix these issues properly.
Here's my Haskell code to generate the lorem file: https://gist.github.com/3977111 The single instance of lorem is a built in macro in TextMate, my OS X editor. I completely agree with your remarks on unnamed constants. I was modifying existing code. Maybe I'm a *really* old C programmer (mostly pre-ANSI) but I know these ASCII values by heart, so it didn't occur to me to clean them up.
 cabal install cabal-install May be redundant, but it is not a command you run that often, so that redundancy is not really an issue. If it had a special case, that would be much worse. Also, I am guessing your $PATH doesn't contain ~/.cabal/bin which is why "cabal" keeps referring to your system-wide cabal which is older.
It would be nice if we could depend/publish API signatures rather than version numbers. If you need semantic diffs, you could export stubs to signify the new semantics that others can depend on. That would be *much* better than version ranges. Additionally, if we could have the same-package-same-version installed multiple times, that would solve the second biggest problem. The per-project sandbox means a *lot* of redundant compilations, and will not really solve the problems -- only make them slightly smaller. 
Still, I did exactly the same thing as he did a few weeks ago only I didn't blog about it. I think he's helped more than I am by at least letting people know that another person ran into that wall that's been there a while.
Sweet. I am freakin pumped to watch this when I get home. Thank you so much.
Hmmm... while this is a bit risky, I wonder if cabal should say, "Hey, if I'm going to suggest that I need to update myself, I should look at where I would put that binary, and if there's one already there, `exec` that instead." It feels wrong but I'm not sure my feeling is correct.
Folks, I renamed the project to Vinyl to avoid conflicts. So, the URL is now https://github.com/jonsterling/Vinyl. Sorry for the lack of redirect... **Edit**: I've made a dummy repo with a link to the new one to avoid confusion.
Well, there's the `UnicodeSyntax` extension. :)
I don't agree. Having a globally shared package database is what makes the brokenness more likely to occur. But for everything that fails to work with a globally shared package database, there exists a project that has no good reason to fail, but will fail even with private sandboxes. What's a little more broken is having no distinction between the libraries I depend on, and the libraries I re-export. That makes it impossible to use certain libraries together, even though there's no logical reason for them to conflict. People are working on that. What's also a little more broken is how installing one thing in the globally shared package database can break other things even when dependencies are all explicitly given. This is also a known problem, and being worked on.
This is an excellent idea! I think sometimes we get too caught up solving the hard problem, and forget that some good social answers help in the interim.
[tryhaskell](http://tryhaskell.org/) for a quick play.
LYAH is excellent, but when you actually start to write "real" Haskell programs, you will want something a bit more meaty. You can use [Hoogle](http://www.haskell.org/hoogle/) or [Hayoo](http://holumbus.fh-wedel.de/hayoo/hayoo.html) to search for functions/packages/types, and read their respective documentation on Hackage.
I personally learned from the [Gentle Introduction](http://www.haskell.org/tutorial/) and then from [this tutorial to write a Scheme interpreter using Parsec](http://en.wikibooks.org/wiki/Write_Yourself_a_Scheme_in_48_Hours). It seems people have mixed feelings on the Gentle Introduction. I enjoyed it and found it to be a great reference, but others find it to be overly abstract. I suppose it appealed to my style of thinking since I'm actually a math major rather than a CS major. Nevertheless its example of writing a small domain-specific language using a monad stuck with me for a long time.
Nice stackoverflow thread http://stackoverflow.com/questions/1012573/getting-started-with-haskell
I actually came across this yesterday, very neat =) Although it's worth noting that this is for Repa 2. The newest version was introduced at ICFP this year. I modified the code to use Repa 3 and Gloss: https://github.com/tranma/falling-turnip (this is part of a falling sand/cellular automata game I'm working on for uni)
if you like videos, here are some favorites of mine: Nice intro at a google talk: http://www.youtube.com/watch?v=b9FagOVqxmI This blew my mind when I first read it in Graham Hutton's book. Beautiful code: http://channel9.msdn.com/Series/C9-Lectures-Erik-Meijer-Functional-Programming-Fundamentals/C9-Lectures-Dr-Graham-Hutton-Functional-Programming-Fundamentals-Chapter-11-of-13 Also: http://haskelllive.com/ 
Thank you for the bug report! I've fixed the post. (The `ForeignPtr` version still has a bug where it capitalises every 1024th letter ... oops. I'll fix that later)
I've written this blog post as a fast introduction to Haskell. It might be read before LYAH. http://yannesposito.com/Scratch/en/blog/Haskell-the-Hard-Way/
I wish reddit would let me give you more than one upvote! I particularly like the chapter headings and I particularly . particularly like the the Monads chapter heading :-)
I really like FRP and truly wish we could use a more functional language to write client side web applications. That's why I really appreciate your effort with Elm. But why did you need to implement a new programming language for this, instead of using Haskell (or even Agda)? I just don't see it. I think you would gain such a head start compared to where you are now when just combining some ideas from Fay with some existing (or new) FRP library. Or to take the DSL approach I took in my toy project[1]. Haskell has so much to offer that would take years to rebuild all of that. [1] https://github.com/sebastiaanvisser/frp-js
LYAH is probably the best place to start, if you're starting from zero. I was lucky enough to have a university course as my introduction to Haskell, which covered roughly the same territory as LYAH. After that I spent some time solving Project Euler problems. I'm not really sure when I "graduated" to truly understanding how Haskell puts things together and being able to write nontrivial programs. (In reality, this is never-ending, there are always new, fascinating things to learn. But you don't need to know everything that everyone has ever known before you can start writing useful things yourself.) I got (and still get) a lot out of [Planet Haskell][1], and the various participating blogs. Obviously you don't read every single thing there, but the ones which seem interesting and (nearly) comprehensible. Even when you don't understand every detail, it helps broaden your horizons. [1]: http://planet.haskell.org/
While the template language is quite nice, it doesn't seem like it adds much over a list based equivalent? listStencil [[0,1,0], [1,0,1], [0,1,0]]
Fine, we should document a bit more the rough edges. But before throwing rocks at a whole community like that saying that we are not aware of how stuffs are broken etc., it'd be a smart move to actually check that is indeed true. Which isn't. We do a lot of stuffs to solve this situation, or at least to make it less problematic. cabal sandboxes in the next release, scoutess, all the cabal-dev like tools, etc. We *try* to improve the situation.
I use M-/ in Emacs. 
He didn't say anything about programmers, he was talking about languages. Were you writing C++ or C# (or god forbid VB) during those 20 years? Because those languages certainly do smell. Nobody said anyone who uses them smells or is "a bad programmer" though, no need to try to turn a potentially thought provoking idea into an attack.
People need to spend some time with the original Smalltalk and Lisp IDEs. The existing ones are still trying to replicate what they offered in the early 80's.
Well I do .net in VS and Haskell programming - due to lack I use Sublime Text for the Haskell stuff - not a bad choice at all - don't miss and "texting" or navigation features at all. Want to know what I really miss? Do some F# programming in Vs - you will move your cursor over functions/values constantly to see the types and a short tooltip hel! THIS I miss for Haskell - true there is this nice plugin for Sublime that shows you (some) types if you have a cabal file ready - but this fails for lot's of scenarios. So if someone get's this packed with Hoogle support ... I'll pay for it :D
Thanks, fixed!
My point was that using TH to input a grid of numbers seems like overkill. If the Template Haskell also allows for more advanced stencils, then cool, but for this use case, lists seem adequate.
It is true that IDEs sometime alleviate language problems. For instance, I use Eclipse to do some sort of type inference in Java when I declare variables (Ctrl+1 is very useful). However this should not be a reason to condemn IDEs. Having an IDE for a language doesn't mean the language sucks. There are many IDE features that aren't related to the language itself. Having IDE around won't prevent peoples to use Vi, Emacs or anything they like. Why this crusade against IDEs?
Hope it goes well
You are not your favorite language. The programming community needs to learn this.
This seems to come up every time these kind of issues come up. You say "If you need semantic diffs", but there's no "if". You do need semantic versioning. You can use "stubName" instead of "1.2.3.4", but it fundamentally doesn't change the problem.
Analogies are like burritos! Or if you are feeling meta-physical, elephants! But anyway, burritos are just like abstractions. You don't know what's inside, but you know how to eat it. 
I think the Haskell community needs to accept some of rocks thrown its way with good grace. Saying, "Don't be mean though!" is sort of absurd. This post was obviously written in frustration, and quite frankly everyone agrees that cabal has serious issues as it stands that cabal-dev only addresses if your dep tree is quite small. And Yesod? Yesod is probably one of the _worst_ offenders here. I recently went through *exactly* the same dance with my Yesod installation only it was *worse* because I had installed the 64-bit platform on OSX and it is a known-but-not-easily-discoverable bug that the yesod executable just crashes on OSX with 64-bit haskell. So yes, the post is vituperation, but cabal earns this. It does no good to people trying to join the language now to say, "Oh yes cabal the proverbial princess with the pea in her mattress, but the future will be better."
I agree. Still, words said in anger are just words said in anger, and anger you earned is anger you earned. But I am sure you can envision how this played out. \#haskell on FreeNode varies from an incredibly friendly and helpful place to a maddeningly irritating and smug place, depending on your question and the time of day. Certainly I've felt like I regretted trying to install Yesod more than once.
This is a question about how Haskell does compilation, not commentary on anything, but... What does it mean for a module to be compiled *against* a dependency? Does the resultant object code have any ties to the dependency outside of typechecking?
Personally, for #2 I did the [99 problems](http://www.haskell.org/haskellwiki/H-99:_Ninety-Nine_Haskell_Problems) and [20 intermediate problems](http://blog.tmorris.net/20-intermediate-haskell-exercises/). [Typeclassopedia](http://www.haskell.org/haskellwiki/Typeclassopedia) also comes highly recommended as #4.
Just switch to Debian already. It has all the parts you like, but not as much of the hand-holding. &gt; Ubuntu is an ancient African word meaning, "I can't configure Debian." \^_^
It's good to see Fay catching on. I just want to point a few things Michael said that I strongly agree with. &gt; One of Fay's great features is its dead-simple FFI. Indeed, an excellent design decision. This is easy to under-appreciate, but when it comes to working with pre-existing JS libraries, I'm sure this is a godsend. &gt; as much as Elm is influenced by Haskell, it's still a separate language. This is the one thing that bugs me about Elm (and also Frege). If it were a subset of Haskell -- like Fay is -- then I'd be much less irked. Again, another great design decision by Chris. In contrast, I'm not bugged by Roy, because Roy mirrors JavaScript rather than being almost-Haskell. &gt; We really need to have some package management system for Fay. It's rather exhausting to have a package management system for every language and sub-language in existence. It would be nice to have a flexible package management system that abstracts over languages. Until we have that, though, then sure, a package management system for Fay would be a good idea -- if only to provide canonical bindings to commonly-used JS libraries.
This might be a stupid question (I've not used Fay) but why can't it use cabal to manage its packages?
`Prelude.catch` appears to have been removed. I presume you intend for `&lt;|&gt;` to catch all synchronous exceptions but [propagate all asynchronous exceptions](http://www.haskell.org/ghc/docs/latest/html/libraries/base/Control-Exception.html#g:4)?
Thanks. I'm getting started :)
I think I understand what you want better now. Thanks for clarifying. &gt; In practice, most potentially breaking changes will not break most user code -- meaning that this wastes a lot of everyone's time, and prevents lots of builds from succeeding due to over-conservative version ranges. Agreed. Several of us have stopped adding upper bounds and only add them after we know something breaks. Not ideal either though, as breakages get caught at compile time instead of at version resolution time. &gt; Another cool thing about API signatures -- is that we may have multiple packages supplying the same API, and they'll all be interchangeable as a dependency. This could be great for more easily swapping in an alternative implementation of a dependency to compare. Sounds like structural typing (aka duck typing), so it has the pros/cons of that. One con is that having the same API does not mean being compatible; I very much prefer an explicit (type class based) declarative approach to interfaces. &gt; Another possibility, which is not as nice, but perhaps more pragmatic, is to be able to specify dependency information retroactively on hackage -- rather than bundle it with a package forever. Duncan Coutts is working on this. Btw, how do you depend on instances being available under this scheme?
Even if you don't want to learn Haskell, LYAH is great for entertainment.
A major drawback of the no-upper-bound approach is that if you discover breakage and publish a newer, more conservative version, cabal may prefer the older more lenient package to go with a newer dependency. The type-class approach to package selection is likely to be clumsy (using indexed types/etc is a poor alternative to the module system). It also requires Haskell code to explicitly make the choice and potentially adds indirection overhead (dictionary passing). Dependence on instance exports and the exports themselves may be listed explicitly in the API import/export signatures that don't have to be Haskell code themselves.
Make a roman number to arabic number converter. Once you do that, make one that accept any number of character as the roman number set instead of 'MDCLXVI'
I have a flexible package management system: it's called apt Most OSs have one. OSX has several of varying popularity.
if you like math (i like math) project euler is your bag. Also, [this](http://www.haskell.org/haskellwiki/99_questions) is good exercise. Haskell's great fun! just make sure you don't do anything that's actually useful.
Michael's approach is attempting to get static assurances about the code while still getting upstream improvements from a large open source project. How's haskell meant to compete if it has to implement its own client side mvc framework that only other haskell programmers can use? Although I agree that fay + angularjs + yesod isn't going to be a water tight abstraction I applaud this approach for its potential. People complain about coffeescript's javascript and clojure and scala's jvm but good things still get made in these languages.
Would this be terribly hard to implement as an external tool? I honestly don't know. If you could make an external tool that, given a file with some import statements missing, gave you the needed imports (with different options in the case of name conflicts, possibly), adding support for it to Emacs would be pretty trivial. I'm imagining the GHC api could make this much easier, but I really don't know.
The point isn't that not having an IDE is good; rather, being essentially *forced* to use an IDE (which is my experience with Java) is unequivocally *bad*. That said, even for Java, I prefer using Emacs for anything outside of serious development. So I'm probably completely crazy.
This is THE purpose of the Cont monad. Also, if you want to associate a disposal routine, check out [Paolo's variation on it](http://paolocapriotti.com/blog/2012/06/04/continuation-based-relative-time-frp/).
&gt; Why is "using a specialized tool for its specific purpose" considered "cobbling together some add-ons"? Because I rarely see those add-ons act in an integrated manner: Can I go from the compile error message to the documentation? Does getting an in-line lint warning display well with in-line completion? Does in-line completion show you type signatures? This stuff can be made to work, but I observe mostly specialized add-ons to plain text editors doing one thing only. Excuse me for saying "all" - clearly you are not in that set. Of course, just because you don't like those features doesn't really prove anything about whether IDEs are a code smell for bad languages. Curious... are your apps over 100k lines of code? Do you collaborate with on them with more than 10 developers? If not, perhaps you don't need this stuff. But I've never known a project of those proportions that didn't. 
&gt; A major drawback of the no-upper-bound approach is that if you discover breakage and publish a newer, more conservative version, cabal may prefer the older more lenient package to go with a newer dependency. Cabal always prefers the latest patch level version so if you publish A.B.C.D+1 that adds an upper bound, that should be preferred. &gt; The type-class approach to package selection is likely to be clumsy (using indexed types/etc is a poor alternative to the module system). It also requires Haskell code to explicitly make the choice and potentially adds indirection overhead (dictionary passing). I will have to see if I find it clumsy or not. It's certainly is safer and doesn't introduce implicit dependencies (on module signatures), a problem with duck typing. It also adds a degree of polymorphism not available using import time "polymorphism". Like so: class Map m where type Ctx :: * -- the class constraint e.g. Ord insert :: Ctx =&gt; k -&gt; v -&gt; m -&gt; m -- etc. f :: MapLike m =&gt; m -&gt; ... `f` now works over different types of maps, in the same program. 
I appreciate your point. I never had the impression that one language was faster than another. It just really gets me that the two code examples didn't so much as use the same algorithm. It makes the comparison feel weighted and treats the languages like two incompatible problem solving tools.
Typescript is a very conservative extension of JavaScript to include some static typing. Dart is just horrible, and harmony hasn't gotten off the ground in any meaningful way.
You have some good points. However, when I say *forced* to use an IDE, I don't mean I was forced to it because everyone else was using one. I mean I was forced to it because of Java's extreme verbosity. I should never have to automatically generate things like getters and setters, much less the rest of that boilerplate! One of the reasons I really like Haskell is the support for generic programming: things like `deriving` and template Haskell. Where I would use an IDE to generate code for Java, I have the language take care of it in Haskell. This has two advantages: it's easier to write and it's easier to read. Haskell programs and modules are also generally far shorter than Java files. I feel bad if I have a module over 200 lines; in Java, even small classes tend to be longer than that. This makes getting around with Emacs far easier. Now, there are a bunch of IDE features that would help, but I can get by without most of them. I'm productive with Haskell--in some absolute sense--with just an (albeit exceptionally awesome and virtually omnipotent :P) editor. I could never do that in Java just thanks to the syntactic and conceptual drag the language induces. Also, if the IDE features would help, why not make them maximally helpful: write your IDE as a sever with the GUI as a client; that way, I can get the same functionality fairly easily in Emacs while retaining Emacs's superior usability and productivity. Coincidentally, this is exactly what projects like Scion, Ensime (for Scala) and TypeRex (for OCaml) do; I've found something like this to be the best option by far.
Organize imports is great, but I regularly use: * F3 - go to method definition * hovering over a name to see its documentation * Ctrl-Shift-T - go to a type declaration * the rename refactoring - change **all** occurrences of variable/method/type/etc. name from **one** location * the extract-to-method refactoring - highlight a chunk of code, packaging it into a method and replacing the chunk with a call to the new method * running JUnit tests &amp; navigating to the location of errors with one click * ... In fact, the rename refactoring is probably is probably my favourite. It allows you to improve the readability of the code; changes that otherwise wouldn't be made due to the sheer tedium of the work. A haskell IDE that could do these types of things would be great. (It's been a long time since I looked at EclipseFP -- perhaps it can do some of this...)
I'm not really sure why it's an issue whether or not something is technically a subset of Haskell or a separate language (i.e., Fray vs. Elm) when both look like Haskell and neither provides the full feature set of Haskell. In other words, from the perspective of the user of the language, what's the difference? Edit: Honest question. I just don't do much web development at all so I'm not sure if there's some extra concern here or not.
&gt;Incidentally, it's not good enough to say "oh, Java has that problem too". Java has many problems, of course (including the lack of a well-maintained global repository for libraries with easy installation... we solved that one). Maven central doesn't count? It is really that hard to add packages to a project? I find it rather trivial to add dependencies but I guess it's just me. Honestly though, if cabal could act more like ruby's gem where you can install multiple versions of the same library to satisfy dependencies for different libraries, that would be great. 
I too was looking for a mathy introduction to Haskell, but I gave up on the Gentle Introduction early on because I found several ambiguous or even wrong statements in the exposition (which were confirmed as such by people in #haskell).
splitting code into multiple files is for sure one thing that different people have different views about...e.g. in google's android source code you find *a lot* of very big files. While also quite unfamiliar to me in the first place, I can see it has some advantages when code is not scattered among a lot of small files... But anyhow, I don't think this is the main point of the article. For me the takeaway was that for some languages you really want the support of an IDE 'cause otherwise it's just too cumbersome to express your thoughts in code (-&gt; especially true for java and eclipse or intellij) The problem with that is that once you get familiar with an IDE, you want to use it for other languages as well...languages that might not be supported as well as java (which is pretty much every other language in case of eclipse). 
But if the language author has already implemented a typechecker and has already bothered to create a parser, I'm still not sure why this is a problem for the end-user (of the language). Unless the language author does an inferior job. Also, if a language must be a subset of Haskell, that seems to limit how it can grow. It can either emulate more of Haskell, or remain stagnant. Whereas if a language is technically completely independent of Haskell, if it turns out that something needs to be done slightly differently to accommodate the direction the language and its practitioners are heading in, well, that won't be an issue.
The real challenge of learning Haskell is not how to generate a list of primes or other small puzzles like that. It's how to do what you've already done in other languages. If you're in college, take a big program you had to write for a class in C or Java or Python or anything really which is not purely or mostly functional. Recreate it in Haskell. Ideally it will involve a complex user interface of some kind. Once you get the syntax of the language down, my personal recommendation is to follow along [Write Yourself a Scheme in 48 Hours](http://en.wikibooks.org/wiki/Write_Yourself_a_Scheme_in_48_Hours). It helped me *immensely* when I was trying to progress from knowing the syntax of Haskell to writing a worthwhile program. It helped me see "the big picture" of Haskell much, much faster than Real World Haskell did. RWH is a great book but not necessarily the perfect introduction that it has a reputation for being, IMHO. Though it's worth mentioning that writing your own code is a lot different from following a tutorial, and again a complex GUI program will still involve a *lot* of concepts not present in that tutorial which you'll need to re-learn, to some extent, to replicate in Haskell.
&gt; debugging angular is already..."interesting". throw in fay and you're in no-man's-land. Have you tried [batarang](https://github.com/angular/angularjs-batarang)? Especially with the great testing support, I found the debugging experience with AngularJS significantly better than with similar libraries or frameworks.
Slides with not a lot of content, but some interesting points, and a good discussion on [HackerNews](http://news.ycombinator.com/item?id=4721550).
IMO cabal is not such a major issue. It has deficiencies, but works ok if sandboxed. Many languages don't have even that, and yet are popular (well, maybe not in startups). Python became popular even with ugly easy_install, before that pip thing. And how many times I wished there was cabal for C, C++! You spend a whole day setting up new sandboxed project with boost, ilmbase, qt and kitchen sink. Also, what libraries hackage is missing?
Maybe Well-Typed could put up a Kickstarter project?
Right, that was my question: does anyone think that'd be viable?
For Cabal features I really wanted I would cough up a substantial amount of cash on a kickstarter. I really have no trouble parting with cash to make my life easier. I'm not sure how many others are the same, but there are about, what, 700 properly active Haskellers, for every one of them that has moaned about Cabal, they should put their money where their mouth is and either do the work themselves, pay for it, or stop moaning and wait. I would also scream [SHUT UP AND TAKE MY MONEY](http://marketing555.files.wordpress.com/2012/10/suatmm1.jpg) for the new Hackage to be up and running on `hackage.haskell.org`, *now*, with a Github project under [the Haskell username](https://github.com/haskell) called “hackage” that we can all make issues and pull requests to, with someone or some people there to review and deploy. As has already been explained by you and others, this isn't really a technical challenge, but a people and time thing. I'm not sure about the long term, though. Maybe there'd only be a month or two of reactive maintenance required, and then after that not so much time and effort? I.e. no need to pay someone full time or part time to maintain it.
Yes; please try that. I'll contribute personally, and I'm sure many people within the community will contribute too. You can definitely get a good number of small contributions, and, hopefully, you'll attract a number of large contributions too.
git based deployment makes little sense for Haskell. Should not need GHC to be present in production. Deploy binaries
Note that with Kickstarter, if a project does not reach the specified monetary goal, then all funds will be retracted and the project will not get funded. Aside from the Kickstarter, consider the way that [PyPy](http://pypy.org/) accepts monetary contributions from its community.
The core language semantics are the background. People program against the foreground of libraries and frameworks. Right now, for example, its proposed that the basic mechanisms for reading from and writing to a file or such like should change dramatically. That's a scary proposition for many organisations. Extensions certainly are very optional. But since different library writers like different sets of options that's another lever of complexity to manage while trying to build, manage and operate a consistent binary. 
I would definitely spend money on a project to improve Cabal.
&gt;This stuff can be made to work, but I observe mostly specialized add-ons to plain text editors doing one thing only. Yeah, that's the point. Specific tools to do specific tasks, that do precisely their job and nothing else. Unix didn't become ubiquitous by co-incidence. &gt;Of course, just because you don't like those features doesn't really prove anything about whether IDEs are a code smell for bad languages. Why do you think I was attempting to prove anything? I responded directly to comments you made, nothing more. &gt;Curious... are your apps over 100k lines of code? Do you collaborate with on them with more than 10 developers? If not, perhaps you don't need this stuff. But I've never known a project of those proportions that didn't. You seem to be arguing against yourself here. How many haskell projects are 100k+ lines and 10+ developers? A 100k line, 10 developer java project is a 10k line 2 developer haskell project. The original post already pointed out that java and C++ are IDE requiring, smelly languages.
I'm somewhat disliking this trend of slideshows being linked to without the content of a presentation to go along with them.
What do you think such a project will be about?
France, I'm guessing? You've given us Coq, merci beaucoup.
What a surprise. You should see some of the state funded research in my country. At least they are working on JVM in you country.
I would have preferred a visual cue to tell me to use the keyboard to navigate left and right.
IMO you should build something that YOU want to build. Intrinsic motivation for something helps get you through the rough patches when you don't feel like learning something. Having a burning desire to reach some larger goal always helps me a lot.
Haskell -- as it is perceived by the outside -- is what matters here. For a newbie, cabal can be about as fun as having an arterial gash in your neck. If your experience with cabal is crippling then your likelihood of reporting the wonders of Haskell to the world are greatly reduced. **Once bitten, twice shy.** This is about traction. Negative news on this front will persist into the future even if the problem is solved. The longer the problems with cabal continue, the longer and more profoundly the bad news will persist in the minds of ever-increasing numbers of candidates. Candidates who might otherwise have used Haskell in their environment, from individuals, corporations, governments, etc. Is Haskell at a publicity tipping point? I don't know, but if it is then perhaps it's time to stop the world for a moment and redress the situation.
... ? I'm not a pythonist, but I know in all my other primary environments (C, Ruby, shell, Java) there are *many ways* to do file IO. There are many ways because different applications have different needs.
&gt; Frameworks matter more in languages where you require a significant up-front in investment to learn each library that you use. I hope I never have to work in an environment where most libraries are that heavy.
You should make a kickstarter so I can pay you for making such a good comment, the upboats aren't enough. *Kickstarters for all!*
&gt;people don't want to spend effort deciding which libraries to mix together and want to get some sort of approved "one true way" of doing things so they can deliver results immediately instead of getting lost in the library ecosystem Exactly so. Many Java projects struggle with this—far, far too much effort gets sunk into finding combinations of libraries that will work and then begging Maven to build something deployable, and to build it the same way twice. In the .Net world that's managed a little better, but still you run into ~~.dll~~assembly hell from time to time. With Haskell, as a manager, I'd be worried that the developers would spend far, far too long working out which library was the provably optimal one. And then I'd worry every day that the team would decide that some new thing supported that extra level of abstraction that makes all the difference and whatever code they had written previously was now philosophically unsound and they must, as a moral and intellectual imperative, rip it all out and start again. &gt;outsiders fail to appreciate that Haskell differs significantly from most languages in that library adoption is very cheap I would be very interested to learn more about that.
It's not bad, but it has its annoyances (destructive install is the major one for me).
I don't agree. In Java, let's say, prior to the nio package arriving in 7, there's only one API for working with files and still only one concept of what working with data in a file means. That API has evolved over time but no-one has come along and said "hey, we've now realised that opening a stream onto a file and reading from it is philosophically incoherent and we need an entirely different concept of what ‘doing IO’ means." which is what I see Haskellers doing. 
What? When I work in Java it's a nest of FileStream, BufferedStream, DataStream, and other classes that one can mix-and-match depending on the application to produce the desired behaviour.
&gt; I'm still not sure why this is a problem for the end-user Another thing is reusing existing Haskell code. Obviously you can't reuse *any* code if you implement only a subset of Haskell, but there is still a large body of code that probably does fall within the Fay subset that can be reused. Yet another is reusing existing Haskell tools. We have plugins for Emacs, Vim, Sublime Text, Eclipse, etc. The main motivation to choosing a subset of Haskell is so that you don't have to reinvent the wheel for every new language. &gt; Unless the language author does an inferior job. In Elm, Haskell-like whitespace sensitivity syntax hasn't been implemented yet. It's a hard thing to implement, and being not-quite-Haskell means that existing solutions are difficult to reuse for it. &gt; if a language must be a subset of Haskell, that seems to limit how it can grow This is true. But in the realm of "languages that compile to readable JavaScript" I think that this limitation is acceptable. &gt; Whereas if a language is technically completely independent of Haskell, if it turns out that something needs to be done slightly differently to accommodate the direction the language and its practitioners are heading in, well, that won't be an issue. Fay's target audience is Haskellers, so I don't think this will be a problem for Fay. Elm does target a wider audience I think, so perhaps being disentangled from Haskell will prove beneficial in the end.
It is very difficult to justify spending start-up dollars on something that works well enough, at least once you know the warts. We drive a per-repo Cabal sandbox with Shake and work around issues as necessary. Others have invested the time to build alternative solutions, generating Makefiles seems to be a fairly common approach. While I don't believe complaining without offering solutions or dollars is very useful, it seems that the community has more to gain from an improved Cabal and Hackage than the commercial users will... and it will ease the way for the next generation of Haskell wielding start-ups and lower the bar for larger companies to start using Haskell.
How is writing analysers for the JVM a waste of money? The Java language and various JVMs run on **3 billion** devices and are being used right now by almost everybody. Haskell doesn't have any chance of catching up to where Java is any time soon.
https://github.com/haskell/cabal/pull/1091
Hmm, curly brackets _and_ macros. Explicit pointers, but made safe (rust would seem to owe something to Cyclone in that regard). Well, maybe Mozilla can make this fly. Interesting, thanks.
The public purse is like the internet, someone's spending it wrong. Always!
Yes, well, I'm of the opinion that a major non-technical blocker to the wider adoption of Haskell is the apparent belief amongst some of the more noisy Haskellers that not only are Haskell's solutions to various problems the only acceptable ones to use but also that the set of problems which Haskell addresses most keenly are the only acceptable problems to worry about. Come rust 1.0 and come a hypothetical re-write of Firefiox in it (so, some time around 2016 ;) rust might start to look like an interesting target.
I don't question that this is your (and others') experience, but I do wonder why that is. I've been doing Haskell full-time for a number of years now and I've never run into these major problems I keep hearing about. (Minor problems, sure; but I've only broken the world once.) Then again, I don't do any web development anymore, so I've never had to deal with yesod nor any of the competitors. Given all this, it seems to me that the real problem here is yesod and its ilk. I can't believe that I've just been stupidly lucky for my whole career with Haskell; a little lucky, sure, but not that lucky and for that long. I'm *all for* fixing cabal. But it sounds like someone needs to wrest with Snoyman to slay this particular beast.
In all fairness, once I deprecate frames and unidirectional pipes (although I will keep pipes in the library just as a reference implementation), there will be only one infix operator: `(&lt;-&lt;)` for proxies. One of the reasons I came up with proxy transformers was so that all extensions to the proxy type would not change the composition operator, to make your life easier.
Let's say I have libraries A and B, where library B depends on A. The compilation of B, therefore, depends intrinsically on the specific instance of A we have on hand. That's what we mean by B being compiled against A. Notably, it's not the *version* of A that matters, it's the specific *instance* of A--- the actual library object file on your harddisk. As for why there's that intrinsic dependency, that's more complex to answer. A big part of it is that GHC does a massive amount of inlining, and this inlining can happen across library boundaries. Thus, when compiling B, actual code from A can end up in the binary for B. In a lot of cases this isn't actually a problem: because the A-code that gets inlined is effectively source code from A, so as long as we stick with the same version of A, then we're fine. However, it can be problematic in certain cases. Consider, for instance that we have a new library C which gets compiled against our current instance of B. Because there's some A-code in the instance of B, we can end up getting some A-code in C--- or more to the point, some dependencies on the datatypes defined by A. This triggers the real problem: GHC does not specify a stable ABI layer. So when we inline some B-code in C, there's nothing to guarantee that this will be "the same" regardless of which A-instance our B-instance was compiled against. Thus, if we replace the current A-instance with a different A-instance (i.e., the same version of A, but compiled against different instances of its dependencies), there's nothing to guarantee that our B-instance will continue to link properly with the new A-instance. They may disagree about the representation of A's datatypes, or they may disagree about how many arguments one of A's functions takes (which can be affected by other optimizations). Devastatingly, if our B-instance breaks, that means the C-instance also breaks, and so on all the way up the dependency tree. It's the lack of a stable ABI (compounded by issues of cross-library inlining and other optimizations) which causes fragility, because it means we don't get the modularity which is necessary to keep changes from percolating through the whole web of dependencies. These problems are then further compounded by the fact that cabal only allows one instance of any given version of any given library--- you can have multiple libraries, and multiple versions of each of those, but you only get one instance of each version. Thus, you can't choose to leave the old A-instance in place (thereby ensuring the B-instance still works) when you install the new A-instance. This in turn leads to recompiling everything (at best) or the butterfly dependency problem (at worst).
I think cabal is a lot better than the alternatives for other languages (mainly in virtue of not being side-effecting nor context/platform-dependent). The problem is that we rely on it a hell of a lot more. Libraries in Haskell are ridiculously small things, many of them no more than a halfdozen modules. When you only have to link together 3~5 libraries you're less likely to run into problems than when you're linking together 20~50 libraries.
I code in strict Haskell98, but it's very sad how few others do. It's like the situation GNU created where a lot of people treat "what gcc does" instead of "the C standard".
&gt; As far as I am concerned, duck typing is only a bad thing if it implies dynamic runtime errors. If you get "duck typing" with static checks, then it is fine. I don't think so, as it introduces implicit dependencies, which will break in the presence of separate compilation. Nowhere in the code (except perhaps in comments) do you describe that a given module has a function *with a specific name and type* because that name and type is required so this module can participate in duck typing. With duck typing you don't separate interface from implementation. For example, in f aRecord = (field1 aRecord) + (field2 aRecord) what are the requirements on `aRecord`. Is `aRecord` required to also have a field `field3`, but in this implementation of `f` we don't happen to need it but in another implementation we might want to?
Merged!
Also: convolution rocks. They really should teach about it in more mainstream/early mathematics courses.
I think it's probably a bad sign of the code base I work with at work now that when you said "mocking things like AbstractSingletonProxyFactoryBean", I assumed you were talking about common unit testing practice... That aside, you do realize that things like the web site you linked to there are tongue-in-cheek, right? The Haskell community has a very healthy sense of humor, often aimed at itself.
git based deployment makes little sense full stop. You don't need version or change management in production. It's used because otherwise people would need to learn and configure two ways to send files over the network and developers are lazy.
There's not *a* C standard, there's ANSI/ISO, C99 and C11. Choosing to program in Haskell 2010 over 98 would be akin to pick C99 or C11 over ANSI/ISO. 
You do realise that I was borrowing the joke to mock the fanboys, right? That aside, I'm not sure that "healthy" is the adjective i'd apply to that sense of humour.
&gt; git based deployment makes little sense full stop. I tend to agree. Stock Git doesn't leave an audit trail, for instance, it's just about pushing content. &gt; You don't need version or change management in production. Now that's an odd claim. You cannot debug anything if you don't know what code is in production (not just your own code, but also all the dependencies). Without versioning, you can't even decide what code you're going to deploy next Tuesday. To some extent, all versioning (and change management) is just a convenience before code goes into production. But in production, both are an absolute must.
&gt;I personally don't really understand why C++ is any more confusing or awkward than C Because it adds an absolute ton of additional language features, while keeping around all the C ones, and having those features interact in very unintuitive and often dangerous ways. Complexity is all about the interactions that occur, and how much of that you have to try to hold in your head. You require tools to work with C++ to try to clear up some of the head space C++ insists on taking up.
&gt;I think the same is true of C, though. How? A typical C program uses like 80+% of the features C makes available. There's not much room to make it overly complex unless you actively try to. C++ on the other hand almost has to be treated as "we'll use this little 10% subset of the language and don't touch the rest". &gt;Also, I think it's worth pointing out that most C++ IDEs actually don't work all that well. Yeah, there's limits as to what they can reasonably do, because C++ has an undecidable grammar. You can't actually know what a token is in the parser, you need to see how it was defined to determine that. But there's no modules, just files. So you have no idea where it was defined to be able to look up what it is, you have to filter through all the includes to find it. See http://yosefk.com/c++fqa/defective.html#defect-2 and the next answer for a better explanation.
Right. Your answer was to someone talking about Haskell 2010 though, so it wasn't clear to me what you meant. 
&gt; How? A typical C program uses like 80+% of the features C makes available Because 80% of C is still not much as far as the programmer's toolbag goes. C is an extremely limited language when it comes to conceptual abstractions, and creating such abstractions can often create a remarkably verbose code-base. C also requires a considerable amount of extra boiler-plate code to handle edge cases that are often handled automatically by even the standard template library in C++. I think we're discussing similar but distinct issues. Complexity in C++ often arises from judicious or careless usage of inconsistent abstractions. My argument is that this is often avoidable by a skilled C++ programmer. Complexity in C arises in many cases almost inherently and unavoidably, though--not due to mixture of abstractions, but due to the overall lack of abstractions entirely. In some sense you could say the issues are dual to one another.
Step 2 indicates the dearth of intermediate level tutorials that explain the language design patterns and how to write idiomatic code.
It is great for reference, but both of them leave a lot of gaps like: how to translate existing impure code to pure code, how to define your own class instances, how to manage complicated data structures, how to build your own control structures, or how to translate idioms from other languages.
My experience is that Cabal is great until the number of libraries crosses a certain threshold, after which it becomes a veritable quagmire of dependency management headaches. Try, for instance, to use Cabal to install the GTK bindings, OpenGL, Yesod, and maybe 10 more libraries to a plain GHC installation. Somewhere along the line, a low-level package will break in such a way as to require you to clear out your Cabal libraries and start again. (Data.Text, I'm looking at you...) I hope I don't sound like I'm trolling here: I believe the situation is at this point more or less universally acknowledged to be a problem, so I'm really just stating the facts on the ground. The issue, as far as I've been able to make out, is that Hackage libraries are given such fine-grained control over their own dependencies that it becomes very easy for a project to pull in mutually incompatible versions of the same library. I've seen Cabal fail to recognize libraries that it has already installed -- libraries that are identical down to the identifying hash code -- whereupon it tries to drag down and compile the library again, somehow breaking in the process all the library's existing dependencies. In deference to Duncan's comments above, I really wish I had both the expertise and the time to help out with fixes for these, but I don't and I can't. Believe me, I would love to see more corporate adoption of Haskell. My résumé is primed and ready for this to happen. But it's a vicious chicken-and-egg cycle at this point: until developers with some degree of corporate backing are paid to fix things, these issues will persist; and until things are fixed, corporations will by and large shy away from Haskell. There are many bright rays of hope, however. First, you can avoid most Cabal dependency issues if you aggressively use a sandboxing tool like `hsenv` or `cabal-dev`. And discussions like this one help (provided, of course, that they inspire an eventual solution). But Cabal will eventually need to find a way to maintain a stable set of binaries, or Haskell as a language will continue to lose ground to Scala, which benefits greatly from the Maven/Ivy/Gradle family of JVM dependency management tools. (Say what you will about Maven -- I have never seen anything in a Maven repo break due to a minor version change.) EDIT: Small typo fix.
Your export signature changes and you break all your existing users (not just your own user code) when you rename an export. I think this is generally clear to library authors, so I'm not sure I understand you here.
I'm totally agree with this. I learned Haskell solving my own problems as well.
Workaround: source at [https://github.com/mwotton/hs_gbu/](https://github.com/mwotton/hs_gbu/). 
[H-99](http://www.haskell.org/haskellwiki/H-99:_Ninety-Nine_Haskell_Problems) is a good exercise for Haskell beginners.
Ah, I used some ambiguous terminology there. You need version*ing* in production, clearly. You don't need version control in the meaning of being able to switch branches in the folder your production code lives in or being able to merge that with something else. Any changes should mean another release and a different version number.
I disagree that an IDE is a language smell. A good IDE greatly increases how productive you are with that language. I use Java with an IDE when I have to work in it, and I am more productive using that combo than I am with most other languages that I use without an IDE. In fact, I think that IDEs and additional tooling that support the language are the core factors that decide how productive you are in that language. Presence of a good IDE for the language normally means that you will be a lot more productive than if you were going at it in a terminal with your favorite text editor. Consider Haskell. Right now, my usage of Haskell is limited to toy projects and I use vim to do it, looking up documentation in the browser as required. That alone causes a significant productivity hit. When I'm using multiple third party libraries, I normally end up with at least as many tabs open as the number of libraries (submodules under different libraries end up as additional tabs). Now, if I had an IDE that would show all this to me in-line, show me the documentation for the function I have highlighted, let me autocomplete, etc., I would save a lot of time. So, no, IDEs are not a language smell. They're olfactory enhancers, if anything.
&gt;I tried making it iterate only once, but that wasn't actually any faster. [try this](http://stelleg.dyndns.org/share/ZjMM) (takes filename) [or this](http://stelleg.dyndns.org/share/TCNJ) (takes filename + number of threads) compared for fairness with [this](http://stelleg.dyndns.org/share/yY2r) [output](http://stelleg.dyndns.org/share/EUuz) 
I would never, in a million years, bash cabal. I remember what it was like *before* Cabal and that was horrible. I would be interested in seeing well typed do a kickstarter project. IMO, well typed is more qualified than anyone else in the world to address the remaining problems effectively. And I have no doubts about well typed's commitment to the Haskell community. I certainly wouldn't view it as a money grab, or something negative like that. I think the key would be having very specific goals outlined. One question is whether to try to fund a lot of work in a big single round, or try to fund smaller individual projects in multiple rounds. There are arguments both ways. 
What's so great about DBI, if you don't mind me asking? Not trolling but genuinely curious. At a cursory glance, it looks like you just execute your query and manually extract your data, which is god awful, in my opinion but I could be missing something. What does it have over SLICK's StaticQuery, Anorm's parser, Dapper, or even BLToolkit?
You mean that I renamed an export in my example? I didn't, I simply changed the implementation of a function, unknowingly breaking the duck type interface, as I didn't even know I had promised to maintain one.
I personally think what "generic" database interface is bad idea anyway, since all sql dbs are so different. Only basic queries can be done so what postgres, mysql, mssql, oracle can understand it. And there is also that mongodb and other nosql nonsense. DB migration is like teenage sex - more talking about it than really doing. Its the first time I've heard about ISO8583 so I can't say how widely it is used, but since you had no trouble implementing it - I can't see a problem with lack of such library. And I'm sure you will share your on hackage now :) Java, C#, ruby became popular long before such specific libraries appeared for them. I'm more concerned with hackage having too much no longer maintained or experimental, not really usable packages, with one release a year ago.
Upvote for "olfactory enhancers". In fact, i would love to see an IDE similarly named!
I find it ironic that a community that pushes the boundaries of separation of concerns and modularity is suffering only in that it fails to separate the concerns of academia from the concerns of practicality. And what I find unusual is that all the problems of the community are blamed on the shortcomings of individual pieces of software rather than the smugness and sometimes outright snobbery resulting from the aforementioned intermingling. Because of this, Haskell's will remain a community with an above average number of Knights but of few soldiers. Haskell itself is fine (IMHO) but the community needs to become more user friendly! 
As I see it, sharing code is not the proposition of Elm. Also it's not that we're magically getting the ability to share code just because something tries to repeat the semantics (the keyword here is *tries*). I think it works out nicely for JavaScript (e.g. Meteor) because the runtimes are on the same level. It didn't work out with GWT all that magically, for example. In the past, I've run into numerous GWT inconsistencies between JVM execution and JavaScript execution. That said, I'm guessing the proposition of Elm is to abstract away not just JavaScript, but HTML and CSS altogether. It's not embracing anything. Why is that valuable is, possibly, better understood if you're doing a lot of front-end work and frustrated with the amount of technology snot surrounding it. This way, Elm's approach contrasts with just revisiting JavaScript. Because when you're doing that, you still have to pile up your language on top of other shaky parts of the web, like CSS. Evan evidently gets it, because he's [making fun of it](http://elm-lang.org/edit/examples/Elements/Position.elm). I'm also guessing that's the reason why Elm didn't get a simple FFI like Fay did, although it could (couldn't it?). It communicates with JavaScript [though events](http://www.testblogpleaseignore.com/2012/06/29/announcing-elm-0-3-5-javascript-integration-signal-filters-and-more/) which is another way of saying “keep it off my lawn”.
I said it is a waste of public money in the context of public research. It is supposed to have more ambition in long term goals than just fixing some legacy language design. The JVM runs on 3 billions of devices ? Fine, then there should be plenty of money in the JVM industry to improve their tool. Haskell-class of languages isn't so popular yet, in spite of their superior technical merits. The industry won't pull it, or at a very slow rate, because of the usual catch 22. This is were public money is usually useful in my opinion.
The problem isn't that you changed the implementation of a function, but that the *exported* interface of Impl1 changed (new name) so that other matching interfaces had to be fixed up. This could actually be mentioned by cabal, e.g: "Warning: Import signature matches fewer packages" when you add a new import requirement to test. Cabal and Hackage could emphasize which packages export the interfaces you need (the actual dependencies) and you would notice A or B becomes A, too. A type-class wouldn't necessarily help because you might want to have modular type-classes so that the user uses more than one type-class to provide the interfaces they need. 
Do you believe overloaded Num is a hack too?
That sounds great. Haskellers tend to over-use infix operators under some implicit assumption that their library is the center of the universe and so it makes sense its users will memorize all of their infix names. 
&gt; so we can get past the niceties and have real discussions I interpret this as you have the goal of having more efficient communication. Something close to crocker's rules: http://www.sl4.org/crocker.html I down vote your comments which are like: &gt; good points. i see rust as being the java to haskell's smalltalk or &gt;and the extensions are either well-understood (Hugs implemented existential types and rank-2-types) or very optional &gt;or often stupid hacks: OverloadedStrings They give me the conclusion you hold but none of the reasoning that lead you to your conclusion; the result is that it takes more effort to derive significant value from them. 
Out of curiosity, how exactly does FRP take care of the recursion issue? Does it merely do that via not updating dependent values again if a value is set to the same value it had before (which could fail if the two conversion functions don't match up exactly as might happen in floating point calculations) or is there some actual mechanism built into reactive-banana to stop infinite recursion in a case like this?
I don't know what you are upset about. We are not speaking a common language, which is why the conversation was going no where. I politely explained this. Pitching a fit seems like a rather odd response. Read the link I gave you above, and look up the word "complexity" in a dictionary. I'm using that definition, and it doesn't apply to C in any way. When you use a word to mean something totally different from what everyone else thinks it means, it becomes impossible to convey information to one another.
Of course you're entitled to your opinion but since when was making your apps portable a bad idea? From your logic, cross platform programming languages are a terrible idea because each processor architecture is so different and rewriting your code per database is a fantastic idea. The point of these libraries is to reduce the amount of boilerplate code you write and do trivial portable type safe queries for you with zero effort. And if you need more performant queries, these libraries allow you to map custom statements so you don't have to do ridiculous parsing! For ISO8583 messages, it's just time consuming and it's wasted time I could be spending on non trivial parts of what the client wants. Again, the point of libraries is so that you can build on top of it to save time and most likely the library has been tested and peer reviewed by many people so it's stable. Otherwise, why even write libraries? Do you rewrite collections in every language you come across? 
I'm using words to mean their dictionary definitions. If you can't take the time to read my posts or think about them in any but the most superficial manner, indeed, we are not speaking a common language. I "pitched a fit" because, even in this latest response of yours, your attitude is lazy, condescending (look up the word in a dictionary?), and you've made the same strawman twice now. You're putting absolutely zero effort into your responses, instead acting smug about it.
&gt; Ok, if you call simplicity "complexity" then obviously we're not on in the same library, much less on the same page. I suggest you look up the word "polite" in the dictionary. You don't know what it means. Not only that, but you didn't "inform" me of anything. You just stubbornly made a strawman argument twice and ignored what I actually wrote.
FWIW, it works fine for me (FF 15).
Wow. Tekmo sacrifices laws for performance.
You're not one of those "'healthful' is the preferred term" snobs, are you? :)
Why not use criterion for measuring these things more accurately than `time`?
Cool. Question about the code - where is the layout of the page actually specified? All I see is the description of the event flows. Ah, nm, I think I figured it out. The layout is [here](https://github.com/HeinrichApfelmus/reactive-banana/blob/master/reactive-banana-ji/wwwroot/CurrencyConverter.html). Although it seems you will need some way of talking about layout Haskell side for cases where you need to generate parts of the page dynamically.
Hm, not sure. Even if I drop the element using Firefox' debug tools, it still renders quite differently from Chrome, for example the content is not centered. There's a [bunch of errors](http://hpaste.org/77119) in the console. I suggest you grab a Firefox 16 install and try it?
Yeah, that was a very painful call. I spent a LOT of time trying to get the correct version to work before giving in. There is one reason I'm more tolerant of law breaking than I was for previous versions, which is that the category laws for frames demanded a correct monad transformer. However proxies and their extensions (even the resource management extension) do not require a monad transformer. I would have preferred to just define a separately named function (i.e. exec, or whatever) that made no guarantee of the laws, but I figured that would be asking too much of users to not use lift like they are accustomed to. So, long story short, proxies don't need to be a monad transformer any longer, but I compromise and keep the lift name for users.
Development focus is still mostly on the language. I suppose that once the endlessly growing pile of monad tutorials stops and we start to see more practical courses devs will start to care about deployment. Of course, that time may never come. Haskell might continue to be a research language for ever. With all these JS spin offs that wouldn't even be all that bad. 
That is what I wanted to do, but I still cannot resolve the polymorphic constraint issue that I need to type class utilities, so I decided that if I have to pick one implementation for now then I will go with the faster one because I would like users to not hesitate to drop in proxies even in performance-conscious code. When I resolve the issue I will provide the two implementations so you can choose. I forgot to mention that another reason I picked this approach was that I never saw anybody writing code in the wild that deconstructed the pipe/proxy type by hand. Most people just waited for me to release the feature they needed. In fact, part of the issue was that the correct monad transformer approach was more inscrutable to end users so they just stopped tinkering with the library at all. Paolo has actually argued that the monad coproduct is possibly a better theoretical approach instead of a monad transformer. While I am still not sure about that, I do know that I no longer need the monad transformer laws to be correct for extensions, which suggests that perhaps monad transformers were too specific of an abstraction in the first place.
You figured correctly. As far as I can tell, Ji can manipulate the DOM dynamically.
This example is a little trickier as it requires dynamic event switching, but reactive-banana can do it. I've written an example [BarTab.hs][1] for wxHaskell which does something similar. I haven't ported it to HTML / Ji yet. [1]: http://www.haskell.org/haskellwiki/Reactive-banana/Examples#bartab
As far as I can tell, this is the kind of "sacrifices laws" where the laws are effectively true unless you start poking around at data type internals. That doesn't bother me in the least. I'd prefer that the constructors be exposed in an "Internal" module, but otherwise there's nothing to complain about here.
Yeah, I may move the constructors to something like `Control.Proxy.Internal` if users are worried about safety.
Doesn't bother me either. It's just a bit funny, considering that Gabriel criticized conduits for exactly the same compromise not so long time ago (if I recall correctly). I'm totally in favour of practical common-sense compromises and I'm glad that Gabriel also is.
Is there a way to change the underlying monad of a Proxy? In the previous version of **pipes** I used **hoistFreeT** from the **free** package, but that will not work now.
I most certainly did! I hate admitting I'm wrong as much as the next person. :) Technically, I'd prefer to just not guarantee the monad transformer laws in the first place by not using `MonadTrans`, that way there is nothing to violate and a weaker contract with the users, but as I mentioned in another comment, most users expect the `lift` keyword, so I compromised there.
Yes. Use `Control.MFunctor`, which I include in the `pipes` library. It generalizes `hoistFreeT` and works with all transformed proxies as well. In fact, the old `Proxy` instance for `MFunctor` used `hoistFreeT` internally, although I now hand-write the instance after switching off of `free`.
Yes. That's the plan. The idea is that the new lazy bytestring will be a `Producer` of strict bytestrings with polymorphic base monad if it is generated purely or an `IO` base monad if it's generated from a file. I'm also going to be doing a lot of improvements on type synonyms when I bump the major version number to 3.0, so that you don't have all the dangling type variables and the input, so you still have very clean types like the original `pipes`. It will end up looking something like this: type LByteString p m r = Producer p ByteString m r -- or: Producer p (Maybe ByteString m r) to indicate EOF -- I'm still deciding on that The `p` part is basically a placeholder for `Proxy` or a transformer thereof and the reworked `Producer` synonym will be changed to: type Producer p b m r = () -&gt; p () () () b m r I'll also define a convenience type synonym of: type P = Proxy By taking the `p` type variable, it now plays nicely with proxy transformers, so if you add the `StateP` extension, you would just have: LByteString (StateP s P) m r ... and now you have upgraded your lazy `ByteString` with an internal state of its own. Without state, it would just be: LByteString P m r Maybe I will come up with better names, but that's the basic idea.
&gt; We have an in-house tool Why not write and in-house tool to generate HDBC whatetheritneeds then? Personally I'm feeling fine with a bunch of modules I've coded on top of postgresql-simple. Can adapt it to mysql-simple if needed. But good bunch of queries will have to be rewritten because, you know, no two sql database engines are interchangable. I use - 'with CTE' , triggers, views, recursive queries, stored procedures. it is not a multi-index key-value store.
[KDevelop][1] is excellent. It has a semantic understanding of the code (to some degree) and code completion takes into account scoping, types, and (iirc) overload resolution rules. It even understands many template metaprograms. It also doesn't force all the usual IDE bullshit on you if you don't want it. I mostly just open the file browser on the left, the terminal on the bottom, and use it as a glorified text editor with awesome semantic highlighting, tooltips, and code completion. Potential drawback: I'm not sure how well-supported it is on non-Linux platforms. [1]: http://kdevelop.kde.org/
There doesn't seem to be much recent effort towards keeping Haskell' going &gt; New revisions of the language are expected once per year. The first revision ("Haskell 2010") was released in late 2009. Certain revisions will be denoted "major versions", which are intended to be supported for longer periods. The latest major revision is still Haskell 98. So it's been 14 years since a major revision, and 3 years since even a minor revision. Either the "expected once per year" needs to change, or we the community need to step it up and breathe new life into the system.
This is unnecessarily complex. Parallel to /usr/local I have /usr/ghc where I have subdirectories cabal (for the cache), ghc-7.4.2i and so forth. I make this user writeable (so I can check I'm not installing elsewhere), and I adapt my path to select a version of ghc *for that shell*. I once had an elaborate discussion with a developer friend on the best way to do this. One could of course use symbolic links or whatever. We decided that a design requirement should be that one can select a new GHC and start using it without pulling the rug out from under any existing jobs using other GHCs. Paths are local. The one global variable that I couldn't avoid was of course ~/.cabal/config, but using a common cache location, this doesn't matter. If anything, this global variable softens our fanatical image.
I think this is a good idea, so I made a ticket. https://github.com/Gabriel439/Haskell-Pipes-Library/issues/19
The first advantage of a parsing proxy transformer is that you can give each stage its own parser that has its own local state. In fact, the parsing proxy transformer is really just a glorified wrapper around the `StateP` and `EitherP` proxy transformers (with some extra backtracking support), which gives proxy-local state and error handling. For example, the first stage in a pipeline might be deserializing or decompressing parser and the next stage might be a data format parser. You can't really stream something like that with `Parsec` (or any parsing library, for that matter). You could stream the first parsing pass, but once you do so you'd bring the intermediate representation entirely into memory before you could pass it onto the next parser. The second problem is that the only way you get streaming behavior with `Parsec` is using lazy bytestrings, and one of the big goals behind `pipes` is to replace the use of lazy `IO`. If I can improve upon `Parsec`'s performance, that would be a bonus, but the first two reasons are the most important ones for me.
It's my first time making a t-shirt on one of these websites, so I really have no idea what to do, especially regarding the proportions and spacing. I thought the "Efficiency through laziness" tagline was pretty good though, and I haven't a Haskell t-shirt at the moment. Edit: Realised I forgot to add actual link to the shirt design: http://www.spreadshirt.co.uk/design-your-own-t-shirt-C59/product/107256482/view/1/sb/l
Yeah, I meant to copy what Edward did for `free` by putting a note alongside the `MonadTrans` instance with the disclaimer, but I forgot to include it in this release.
I remember somebody once gave a really good summary in a reddit comment of how to use Debian's `update-alternatives` feature to switch seamlessly between `ghc`s. I wish I had bookmarked it.
w.r.t. to `~/.cabal/config`, its defaults are sub-optimal w.r.t. to compiler version separation; but it can be easily improved by having something like install-dirs user docdir: $datadir/doc/$compiler/$pkgid bindir: $prefix/bin/$compiler in there, to avoid having (possibly incompatible) artefacts from different compiler versions overwriting each other... 
I like the design and its colors but I'm afraid I hate the font - how about something less pixelated?
Oops, [Here should work](http://www.spreadshirt.co.uk/design-your-own-t-shirt-C59/product/107256482/view/1/sb/l) that might only work in the UK though. Though they definitely have a German site too so maybe there's a way of copying it between. There's also a whole Spreadshirt "store" thing they're using on the Haskell Wiki but I have no idea how to do that.
&gt; about 8 fonts Did you not notice the scrollbar in the font selector? I kind of liked the star trek-y font (Stop), but I also know I shouldn't be allowed near design decisions. [B (edit: A bold eurostyle or futura might work better in conjunction with the logo? Or the atomatic, haw haw.)
I don't understand what you think of as "elitist" about the community.
More or less exactly what I do (as I commented at the OP).
Wow. I knew Haskell had an unusually large and featureful package database, but I never realized it was comparable to popular languages.
In that `Behavior` does not support a notion of "update" in the first place. Semantically, it's just a function of time type Behavior a = Time -&gt; a and there is no way to detect updates or changes from that.
&gt; [...] I'd really love it if we could get to a single consensus on this so people would stop hitting the community over the head about having several going implementations of this idea. This is hilarious, since we're the only community that has these at all.
`xcalib -invert -alter` :)
I particularly like the Eurostyle.
Even so, people are getting hit with real problems, just trying to install two things which work separately (most recently in my experience Agda and lhs2TeX) together in the same environment. Per-project sandboxing would make a really big difference to the lived experience, and I'm glad to hear people are thinking about it. How do I get a consistent library of the very latest kit working together for my project? That's a hard problem, and it's impressive Cabal does as well as it does now. How do I make sure that a bunch of different projects, each of which is internally consistent, all work separately? That ought to be more tractable. Er, fingers crossed.
I think it is a big problem that many packages aren't updated when their dependencies are updated and after new GHC releases. Currently, you often have to use different GHC versions for different projects and having several versions of the same packages installed. A change of policy would probably be a solution, such that packages that are outdated would be removed (that is still available for download on the website, but not available for automatic installation from cabal-install). I think it would push package maintainers to update their packages more regularly. See for instance the message you get for R-packages that are outdated: http://cran.r-project.org/web/packages/tikzDevice/index.html
I think that if you can * replicate the Conduit interface by implementing it in terms of Proxy code, * provide sensible, maintainable source code, and * not lose performance then Michael probably wouldn't have any problem adopting Proxy as the New Conduit.
I think I could probably do that. There are still a couple of API issues I have to resolve to seamlessly mix unidirectional and bidirectional code. There is one "wart" for people who prefer unidirectional code, which is that bidirectional composition expects kleisli arrows. That means that users have to either: a) Define all their pipes to take an initial () argument, which leads to slightly uglier code when used within a monad: do a &lt;- somePipe () ... b) Define their pipes not to take an initial argument, which leads to nicer monad code, but uglier composition code: (\_ -&gt; somePipe) &lt;-&lt; ... I believe the former is "more correct" since it reflects the fact that everything is parametrized on an argument and a pipe is just something that has an empty initial argument so it can be run at any time. The other reason I believe the former is correct is because all four categories that the Proxy type overlaps have Kleisli arrows for morphism, so for elegance and theoretical reasons it is also probably better in the long run to ask users to standardize on Kleisli arrows as the fundamental "unit" of the proxy ecosystem. Kleisli arrows also play nicer with everything in the library.
Release 40,000 packages
It's not, the post is in error, though it looks to have been corrected.
Ok guys, I screwed up with my original package statistics in a pretty big way, so I had to go back and rewrite things. This means that a few of the comments here that hadn't caught my error are based on incorrect information. My apologies. A new version of the post is up now.
One question people sometimes ask is why does Cabal have so many issues when things like Debian doesn't. The answer is simple: Debian is carefully managed by a large team of people who check that everything works together before they release packages. Cabal is a free-for-all with hundreds of independent authors releasing packages whenever they feel like it with no coordination at all. So, it is no surprise that the algorithms needed to make Cabal work smoothly and automatically are more complex than a hand groomed system like Debian. Obviously, there are benefits to having a hand groomed selection, and that is one of the things that the Haskell Platform is exploring. But, given the number of people on the Haskell Platform team.. they can't be expected to manage as many packages as Debian.
This is a really interesting point and relates to the whole upper version bound debate. Upper bounds are necessary because otherwise packages will spontaneously break when there are API changes in dependencies. But upper bounds mean that it's a lot of work to update things when breaking changes come out to core libraries. Maybe some kind of automatic detection and removal like this would be a good compromise. We could even have it attempt to bump the version bounds and automatically release a new version if nothing breaks. But if it does break remove the package after a certain amount of time. On the other hand, I don't know that we want to prevent people from installing outdated packages. They can be useful.
Cabal needs to be fixed. I can do it, you can do it, the maintainers can do it. Someone needs to. But let's not make excuses. Let's just fix it ok?`
"Donate 25 and we'll post a cabal package with your name" (=
You should probably submit your link to http://www.haskellers.com/bling
I typically pick whichever thing has more restrictive deps, patch it until the deps match my newer versions, then send the patches upstream to the authors. But that means budgeting some real time for installing a new set of things I'll grant. On the upside, it also means that I know at least a little every single library that I've got installed in my repo, have browsed it's api, know which other packages rely on it and why, etc. I suppose I treat it like curating a library, with cabal just a tool to help me get there, not a substitute for the job I'm doing. I know this isn't for everyone though, and I know it doesn't scale well.
cabal-dev does not sandbox very well, though. I tried it with wxHaskell, but it didn't sandbox installed binaries, which undermined the whole point.
What do you mean? Binaries go in `cabal-dev/bin`.
Some of it is deferred to runtime errors, but perhaps the bigger point is that a Python module isn't bound to a specific set of dependencies since there's no ahead of time compilation and dependencies are loaded at runtime. That's probably one of the biggest issues in Cabal; it's what causes the "packages would break" error. Hopefully the GSoC for multi-instance installation will make this less of an issue.
My sandboxing [is like that](http://www.reddit.com/r/haskell/comments/12alw1/why_inbreeding_is_bad_for_your_community_cabal/c6trojp)
I check reverse dependencies for my packages regularly with http://packdeps.haskellers.com/ and it works like a charm.
You should make sure `$PATH` is set up properly. PATH=$PWD/cabal-dev/bin:$PATH cabal-dev install ...
I'd define an ordered n-tuple as a function from the set {1 .. n}. Of course ordered pairs are probably not defined this way since it's usual to define functions as sets of ordered pairs. But once you have functions, use them to define everything, because who doesn't love functions.
I've occasionally wondered and this seems as good an opportunity to ask as any: Would it be feasible for the compiler to implicitly insert `()` arguments where they were missing, instead of reporting an error? Would there be any negative consequences if it did? After all, there's only one `()` (yes, yes, bottoms), so it doesn't seem like there's being any information gained or lost by the programmer having to write it out explicitly. It doesn't feel like "the Haskell way" -- making ad-hoc special cases for convenience is more like something that C++ would do -- but I'm curious whether there would be any theoretical obstacles. Edit: I guess with type inference it would have a hard time deciding whether you intended to use a `() -&gt; Foo` and to infer the other types in the vicinity based on that, or whether you really wanted a `Foo` but didn't feel like writing the `()`? And the only way would be to type check the whole thing and then go back and try inserting `()`s if it failed, which wouldn't be very scalable?
With tests? Otherwise, you could have a convention where bumping the major number means "API break", whether it's at type-level or not (eg, the maintainer just changed the value of a magic string somewhere). Being just a convention, it wouldn't buy you an awful lot of safety, but it'd still be worth considering.
Yeah, I think type inference and ugliness are the two main arguments against it.
Why not have the LBysteString take Int requests so downstreams can ask for variable number of bytes? Then have a chunking proxy that always reads in a given chunk size and lets the downstream sent unit requests.
I think breakage from new versions that isn't type-breakage is pretty rare and perhaps a sign of a bad dependency maintainer. If semantics change, the names/modules/types can be changed accordingly.
What about this one? http://www.spreadshirt.co.uk/design-your-own-t-shirt-C59/product/107267863/view/1/sb/e
Suppose that package A depends on &lt;3.0 C, and package B depends on &gt;3.0 C, and every line of code in A, B, or C compiles by itself with my current compiler. Will "sandboxing" allow me to use A and B side-by-side in the same program? I'd like to read a mathematical proof that for the packaging scheme we move to, no deliberately troublesome scenario can fail to build, as long as every line of code involved builds individually. If one can construct scenarios that break our next scheme, we're wasting resources working on it. I fear that the line in the sand between the compiler and the packaging scheme is drawn in the wrong place, and without greater compiler support one can't have multiple side-by-side versions of packages available with per-file windows determining what each "import" really means. I fear that short of this support, it will always be a trivial exercise to break any packaging scheme. Rather than responding "we never meant to allow that" we should be nailing down the proof.
Both your examples are quite unconvincing. Most languages don't have an ISO8583 packet parser. Obscure things are obscure, surprise. As for SLICK, that is very new, most language don't have anything comparable, and it does have lots of bugs and limitations. You act as though it actually supports 8 different databases, but have you actually tried using it against DB2? It can connect to DB2, and query it mostly, because it rides on JDBC. But it generates all sorts of invalid queries. And limitations on outer joins still make it very limited in usefulness, just like scala-query. The reality is, haskell does have a comparable library, and has for quite a while. It is called haskelldb. It also has limitations and is of questionable use, just like scala-query. What is the difference other than one of them pretending it works on db2?
You lose all credibility when you suggest that any activerecord implementation is beneficial. Doubly so when it is the horrendous thing from RoR. Especially since persistent and groundhog do cover the same minimal subset of functionality, and yet you dismissed those. &gt;To change databases, i.e. from SQL Server to PostgreSQL or Oracle, simply change the yaml file and update bundler to include the required gem, which is a one liner. And then spend weeks finding all the bugs and fixing them, because that doesn't actually work unless your most complex query is "select a,b,c from foo". &gt;I estimate that 95% of queries that we do are easily done by these libraries with the other 5% hand tuned queries for speed or cross database queries. That means you are either doing something so trivial that it doesn't matter what you use, or are treating your DB as a key value store. Use the mongodb or riak bindings if you want a key value store. Less than half of our queries worked with scala-query, and slick regressed in several places. &gt;So our options are 3) use haskelldb. At some point you need to recognize that using sql server is going to put you at a disadvantage when dealing with open source DB libraries, regardless of the language. You are coming to a unix world, crying about the lack of support for windows-only software.
It might be possible to type class composition to do that, but that might cause even more confusion.
Yeah, I was planning on copying what other libraries do by providing two options: one with a default chunk size and one where you specify the chunk size.
When was the last time a library relied on bugs that contradict specified behavior? Breaking them seems like it would be a rare and minor problem compared to today's problems. It's how people resolve upper bounds in practice anyway. 
That's quite cool, it'd probably be better to make it in image editor first though if you want to use the logo like that (to guarantee printing is okay). I also thought of "Lazy, but Functional".
Er... Why are they not convincing? He asked for libraries hackage did not provide and I delivered! What kind of bullshit is this? The fact is that ISO8583 messages for other languages exist (that are popular no doubt) and in hackage it doesn't. Nor does accessing excel files and other things I didn't mention. Why even twist what I said? Also, what you said about SLICK being new and other languages don't have features is absolute bullshit. Entity framework supports many databases dependent on ADO.NET providers. So does DataMapper and AR. Even if the queries generated are not what you expect or the query is too complex, I explicitly said multiple times that these libraries help with parsing for custom fucking queries but that's a point that seems to be flying over people's heads or it seems to be something that is grossly underrated. 
That's what I mean. "Bumps version bounds when needed."
Sorry. I can't edit on my phone. But if you're saying that persistent or groundhog is the equivalent of AR, then you haven't tried using them. Groundhog is the closest but it still doesn't give me SQL Server support, which I'm sorry to say, is a very popular rdbms. 
&gt; Server error: public//learn/Escape-from-Callback-Hell.elm: openFile: resource exhausted (Too many open files) Whoa, popular post
"If I don't count all my stored procedures, generic AR style modules can do everything I need!". Wow, how utterly convincing.
I guess everyone got tired from cabal posts.
&gt;Why are they not convincing? I just explained why, one example is obscure and not even remotely limited to haskell, and the other is simply false. &gt;The fact is that ISO8583 messages for other languages exist The existence or lack there of, of a specific, obscure library is not an indicator of anything useful at all. Modules exist for some other languages, and don't for others. And that means precisely nothing, because it is an obscure thing 99.9999999999999999999999999% of users will never use. No language has a module for everything, and declaring them all to be useless as a consequence is meaningless and absurd. &gt;Also, what you said about SLICK being new and other languages don't have features is absolute bullshit You sure like to call bullshit a lot for someone who has done nothing but spew bullshit. AR is nothing even remotely like slick, and slick most certainly is new, it is less than a year old. Also notice how you ignored the fact that haskell has had a comparable library for several years: haskelldb.
You mean like persistent having mandatory primary ids? Or no support for composite keys? Oh wait. You used it so you must know what you're talking about. 
Er, can you give me an example of Scala, java, or ruby having this problem? I'll give you that there are gems that may not build due to not being pure ruby but I haven't encountered anything that's a show stopper. 
Right. I just think it makes most sense to get a default chunk size by composing the one where you specify with a generic "default chunker". That's what my existing proxy utilities do :)
I would not use Haskell if I was restricted to Haskell 98. One of the great things Haskell has going for it is its rapid evolution and release cycle. There doesn't seem to be enough manpower in the haskell community to really support more than one compiler that is continuously adding features.
It [blew up on /r/programmg](http://www.reddit.com/r/programming/comments/12iktf/escape_from_callback_hell_callbacks_are_the/) and [Hacker News](http://news.ycombinator.com/item?id=4732924). :-)
&gt;You have some sort of reading comprehension problem Irony overload. &gt;His original question asked what was missing in hackage and I answered. Yes, and the answer was unconvincing. I told you this, and you then pitched a fit and started acting like a toddler throwing a tantrum. A module to interface with the mars rover is missing from hackage too, but that isn't what most people consider an issue. &gt;Why even get defensive about this Again, too much irony.
I like how you addressed my question regarding to cross platform software. Them Linux or windows people must be doing it wrong along with lots of other people like Google and Mozilla. Because fuck things like HTML5 or Haskell right?
Relations have mandatory primary keys, that is not a bright thing to be complaining about. Yes, persistent is even more limited than groundhog, so? Both are essentially useless, just like AR.
&gt;But looking at several blog posts, it doesn't look too well It looks too weller than scala-query does. It just doesn't support commercial DBs that nobody cares about.
Yeah ! This is a great achievement, thank you very much Vincent. Having a native TLS implementation is something that will bring a lot of benefits.
Okay, let's break this down. You like logic right? You're smart too, right? I like to give people the benefit of the doubt sometimes. 1) the OP is about companies and Haskell complete with pros and coins about companies using it 2) the poster asked "what libraries are missing from hackage". No more, no less. 3) I mentioned things as a company, we couldn't find. QUESTION ANSWERED 4) you come along with "derp, I find that your answer is insufficient because its rare". Which has fuckall to do with the topic of the question. 
Not me at least. Cabal is a central piece of my programming activities. Discussing it, learning about it, encouraging people that do the hard work, see if we can help in a way. This is all very positive.
Are you serious? Pretend I just said groundhog instead of acting like a child because I dared to mention persistent, which is slightly more limited.
It is my way of responding to bizarre non-sequiturs that make absolutely no sense. Google and mozilla write software with very limited portability, and do so because the benefits of being more portable are negligible to non-existent. They are evidence that you are wrong, and you seem to think they are evidence that you are right. I have no idea what haskell and html5 have to do with anything, hence "are you high?".
&gt;Hell yes, I'm serious. Ok, good luck with that then.
So perhaps context is lost. Regardless, his post and question was directly responding to the OP, which explicitly talks about companies and Haskell. If you couldn't get the context from that, then I'm sorry. 
As a short-term solution, how about this: The main problem is people just don't update the upper bounds fast enough. Well, why don't we have a build-bot that tries to bump up the upper bounds of a package if a newer dependency is available, and see if it still compiles? If so, it could send an automated email to the package maintainer saying they need to up their version bounds. 
It's the same concept for using such libraries as SLICK. There are tons of databases out there and libraries I've mentioned only supports a very very small subset. But the fact is that if they have the support for multiple databases for subsets of queries then why not use them? Are you implying that the makers of HaskellDB, SLICK, Entity framework and numerous other ORMs are off their rockers?
[1.16 got parallel compilation](http://article.gmane.org/gmane.comp.lang.haskell.cabal.devel/9233)
Ok, now that you're on my side, go ahead and keep arguing with the other you and I'm going to go eat.
I'll write a blog post when we're closer to the release.
Something similar would be: dropRepeats (sampleOn (every 0.5) tags) I am working on improving this kind of scenario though.
Is there a comprehensible blog post about this monad coproduct with examples?
Not that I know of. Even I don't completely understand monad coproducts just yet.
I don't think FRP is the simplest answer to the problem in this post. Instead, a continuation monad would do a better job of avoiding manually writing CPS code.
Exactly. Shameless self-plug here: [monad-task](http://hackage.haskell.org/package/monad-task)，and tutorial [Invert the Inversion of Control](http://www.thev.net/PaulLiu/invert-inversion.html).
That was very clear, thank you. One thing to note: I got a deprecation warning when using *unintercalate* in ghci. The correct function is now *splitOn*. splitOn :: Eq a =&gt; [a] -&gt; [a] -&gt; [[a]] Prelude Data.List.Split&gt; splitOn "," "one,two,three" ["one","two","three"] 
Can someone explain to a layman what the committee actually does?
Steam Fusion, by Valve.
Check out hsenv and cabal-dev.
Another shameless plug: my [operational][1] package, in particular the [WebSessionState.lhs][2] example. [1]: http://www.haskell.org/haskellwiki/Operational [2]: https://github.com/HeinrichApfelmus/operational/blob/master/doc/examples/WebSessionState.lhs
Great, thanks. There's a problem with the feed though. Neither firefox nor my feed reader can parse it [as a valid rss feed.](http://validator.w3.org/feed/check.cgi?url=http://fplab.bitbucket.org/rss.xml)
Most projects tend to have some sort of developer mailing list or similar contraption. Hanging out there will give you an idea of what kind of people work on the project, and what kind of issues are being worked on. That being said, the easiest way is to just fix or implement something for a project you use yourself, and send the patch upstream. That's pretty much the traditional way. You see a problem with some project, you fix it, and share the fix with the world. Welcome to open source, you are now a contributor. In last instance, if we are talking about small and/or new projects, you can simply get in touch with the developer(s), and ask them if there is anything you could help with. This seems to be the attitude of the OP, since they are actively searching for coding hands. Also, bear in mind that anything you do, no matter how small or insignificant it may seem, is a contribution, and open source lives on contributions. Every little thing counts.
Welp, we're not far from New Years, maybe I could have a New Years resolution for once. I'm thinking I might start by installing archlinux. They seem to have a pretty awesome community when it comes to encouraging involvement, as shown by this [bug squashing party](https://www.archlinux.org/news/bug-squashing-day-saturday-17th-november/) I saw recently and the quality of their wiki. I suppose the other way I could start is by listing all the open source projects I commonly use and search their open issues for something small I think I could handle. Maybe even sort by size and pick the smallest one so my contribution is relatively larger.
Well, nothing happens in parallel in JS. Please explain your question.
What is the security status of this package? It's one thing to implement the protocol and, as most other implementations have learnt the hard way over many years, another to implement it securely. I'm prepared to believe that haskell's type system gives advantages over C in many places, but there are many subtleties, including in the exact timing of routines, in a secure TLS implementation, How much has the code been reviewed?
Each of those calls is asynchronous, so you can continue to process other things at the same time as the data loading in. Multiple sources of data can definitely be loaded in parallel. This means you could call getPhoto('tokyo', drawOnScreen); getPhoto('paris', drawOnScreenInSecondPlace); And the feeds 'paris' would be loaded at the same time as those for 'tokyo'. Is this the same with the haskell code?
I was thinking plentiful energy supplied by whistling, clanking contraptions of brass and polished wood, covered with quartz dials and large iron levers.
At the moment, the decision about which to use is easy: if you need a streaming I/O library today, use conduits. There's an order of magnitude more code that connects conduits with other data sources, parsers and transformers, and destinations. Conduits is also on much more solid ground today than it was 6 months ago in terms of the quality of its abstractions, and that's largely due to the presence of competing implementations. So having those competing implementations is a good thing. I see them continuing into the future; for example, Gregory Collins is apparently working on one that we'll see at some point. Not sure if Paolo is still hacking at his. And obviously Gabriel is still developing.
Actually, I think conduits may show an example of this - I don't know how to get around it, but I think using `liftM` to bring the changes in the state to the monad is defeating analysis that gets rid of the intermediate state. In fact, this is why I included the `purePipe` combinator. It didn't really give any performance boosts, though.
Thanks. I fixed it.
The key idea behind stream fusion is that the type is not recursive, which means that, with some care, operations producing streams don't need to be recursive, which means GHC can fuse them.
I think the performance boosts depend on how much the compiler optimizes the nested case statements that stream fusion generates. The original paper says that back then the compiler sometimes wouldn't completely fuse the nested case statements for excessively complicated cases, and I don't know what improvements have been made since then. However, at least you do see improvement. Also, I wouldn't worry about 'liftM' too much. 'PipeM' constructors can't be removed under any circumstances and you expect them to remain all the way to the final result. The performance gains will be almost entirely from the compiler destroying 'HaveOutput' and 'NeedInput' constructors in the optimization passes and in principle stream fusion should make that possible.
&gt; The haskell.org committee represents the Open Source Haskell community. Its responsibilities include: &gt; * setting the policy on what the haskell.org domain name, and its subdomains, may be used for * setting the policy on what the servers owned by haskell.org may be used for * determining how haskell.org funds are spent ~ http://www.haskell.org/haskellwiki/Haskell.org_committee
The classic quote from this blog post: &gt; It sure looks like 2013 is going to be the year of Haskell in the web browser!
It is easy because you are at the core of what's happening in Haskell. Ask newcomers to pick a package to solve x and you'll have n different answers. (Your post is very informative, and that's exactly the kind of thing that could pop up when searching for the best package to solve a task)
Yeah, this is a special case that is actually pretty useful. I am thinking of adding something along the lines of: delay :: Time -&gt; Signal a -&gt; Signal a which delays a signal for a certain amount of time and has been used in other FRP systems. This would let you say something like this: delayedTags = delay 0.5 tags isStable = lift2 (==) tags delayedTags tagRequest = keepWhen isStable "" delayedTags In fact, this would be super easy to add and is extremely useful, so I will probably add this in soon. I think you also want a way to emit events at certain times in a dynamic way, but I need to think about that more. If you want to read more about the historical and present limitations of FRP and how Elm deals with them, chapter two of [my thesis](http://www.testblogpleaseignore.com/wp-content/uploads/2012/04/thesis.pdf) goes through a lot of this stuff in an accessible way. Keep an eye out for dynamic dynamic things. That is the hard stuff! And it is largely addressed by [Automatons](http://elm-lang.org/blog/announce/version-0.5.0.elm) (called Arrowized FRP in academic literature).
Running a team research blog is quite difficult to do, because you have to persuade people that taking time away from their daily work for blogging is worth the effort. The [Nottingham FP blog archives](http://fplab.bitbucket.org/posts.html) display an impressive past achievement: if irregular (but that is to be expected), they managed to have a large diversity of writers, and a variety of technical topics. In my experience (shameless plug: we've recently begun a research blog [at Gagallium](http://gallium.inria.fr/~scherer/gagallium/)), the "team research blog" aspect is both a strength (it suggests a larger range of topic that you would write on your own) and sometimes a weakness, as clearly not all content you might want to write about is appropriate for this blog (and I don't think most people have the energy to have both a personal blog and a team blog). You do not want to disappoint either your colleagues or your readers. It also strengthens the difficulty of fighting perfectionism, and accepting to publish not-yet-satisfying things to increase post frequency. Do you know of other *team* research blogs? I know of the [Brown PLT blog](http://brownplt.github.com/), but I don't have much other examples -- arguably the [GHC blog](http://hackage.haskell.org/trac/ghc/blog) is another such example. (Of course the Haskell community has great personal researchish blogs, such as ezyang's.)
Interesting! It's slightly different behaviour, but simpler and cleaner than I expected. I suppose you can almost emulate a timer with something like this, but with tags replaced with a single thing that get's changed from False to True. delatedTimer delayTime = delay delayTime timerActive isActive = lift2 (&amp;&amp;) timerActive delayedTimer It's not quite there but I can start to see how these things might be possible. I'll have a read through those links, thanks.
StackOverflow is yelling at me that it's about to expire and all the bad things that will happen, so I suppose I'll give it some wider exposure. If you think my question / criteria are unreasonable or that one of the existing answers satisfies them as well as can be expected, that's valuable input, too. If a good answer arrives after the bounty expires, I'll make a new one. Edit: Hmm, just now I notice that one of the answers has been edited substantially since I last saw it... currently reading it.
That would be haskell wiki worthy
I have seen it most often as an `Applicative` to use with parsers - the hope was to get it as a `Monad` (that permuted around the `(&gt;&gt;=)`, rather than just the equivalent of `runPerm m &gt;&gt;= runPerm . k`), too (mostly just to see if it would work, but also to make it easier to use with mtl).
So (post-editing) gereeter's answer now has a lot of relevant detail. But now, as I wrote there, &gt; I feel like I'm getting lost in details instead of seeing a big picture... the topic is even more complicated than I thought. Maybe my question is impossible and there's just no way to gain this intuition except a ton of experience or working on GHC yourself? What do you guys think? Is that the case? If it's impossible to do any better then obviously it doesn't make sense to hold out for more, and I should accept the answer.
ghc-mod's a great bit of kit. I use it mostly in emacs, but it provides the 80% of IDE functionality I actually care about, and I'm sure the vim plugin is good too. Pity it needs autosave to work reliably though :/
What?
Will this package take advantage of CPU crypto extensions where they exist?
Try neocomplcache together with neco-ghc (ghc-mod based completion plugin for Haskell)
Thanks. I removed the apostrophe.
Remember that it's your question. If it hasn't been answered to your satisfaction, then it hasn't been answered.
I use exclusively vim for programming with yesod. You can stole my .vim. It contains : - Shakespearean highlighting. - auto completion (must install Haskell ctags I believe) - replace some operator by symbols (I've fixed most but not all align issues) - NERDTree for file browsing. And unfortunately a lot of other things not related to Haskell nor Yesod. I don't use a vim plugin manager yet. You should get it and discover manually what is for you or not. Sorry for the mess. nb: the solarized color scheme is awesome and everybody should use it. It took me some time to get used to low contrast. But now I couldn't use another one. https://github.com/yogsototh/Vim-configuration
it already take advantage of aes-ni if available, on some selected platform (linux, but just need some report it (doesn't|does) work on windows/macosx)
I found a Java implementation of a similar idea. It is called [Pipes](https://github.com/tinkerpop/pipes/wiki) as well.
Gah. I hate the arrow notation. I understand the arrow type class, and I know what arrows are categorically, but the arrow notation just looks like unreadable garbage to me. I wish scoutess didn't use it. Also, to me, this smells of overengineering. That list of desired features looks like you want scoutess to do everything including the kitchen sink.
Well, dataflow has been around for a long time. This library you linked to doesn't really look like pipes/conduits/iteratees/enumerators, to me. They can't be composed sequentially as far as I can tell; the only way to do things sequentially is to create a new kind of pipe yourself by subclassing `AbstractPipe` and implementing some interface. I also don't even see any emphasis on I/O, although I think it should be possible. Finally, pipe objects in this library actually encapsulate state, so they can't even be reused. If you want a reusable pipe, you have to, again, create a pipe class yourself and instantiate it everywhere you want it, or perhaps you could create some sort of factory instead, but the end result is the same either way.
Precisely. Note that this is the ‘raw’ API, and it’s meant to be built on top of. The attribute system has a lot of potential in eliminating all that redundancy.
You’re right, the way I summed it up there is misleading, even if it still presents the basic idea. There’s actually a sampling phase preceding the two update phases, and that’s when the point-wise transformations are applied.
Yes, the bounty should have been for a wiki entry.
We are discussing the use of arrow in our code base. We can get rid of this, if this proves to increase contribution, and overall understanding. So, we are open to modifications, it won't be hard to adapt the code to just a usual Scoutess monad.
The goal seems to be to build a modular framework that makes achieving the listed goals somewhat trivial by simply writing the "recipes" for each task. One thing that might seem a bit out of place is the "scan communities" goal but it might still make some sense because you'll already be running Scoutess bots on some servers and it'll already be connected to your IRC channels to make announcements. The whole point of scoutess is to watch out for and report on things relevant to a package maintainer, and this could include things like questions about your package on StackOverflow. As for the arrows, we could easily have simply a Scoutess monad instead. Jeremy has this to say: &gt; the difference is in something like, `output &lt;- foo arg &lt;- inputs`. There you know that 'arg' doesn't come from another black box..not really sure why that is useful to know though :) So it may or may not be useful. Time will tell, refactoring it later shouldn't be too hard anyway.
Thanks for pointing that topic out to me, it's really helpful advice. (Sorry I can't contribute though.)
I've downvoted your comment, because admitting error is a praiseworthy act that deserves a gracious response. And I think we as community should *want* foundational libraries, designed for widespread use, to face strict scrutiny. Some of the criticism will turn out to be wrong. So what? The Haskell ecosystem is stronger for the debate.
Ok, I have a question prompted by the Attr code, where the data type has strictness annotations on function members. Do those annotations do anything? I thought all functions are already in weak head normal form.
I installed more or less everything mentioned in this [guide](http://blog-mno2.csie.org/blog/2011/11/17/vim-plugins-for-haskell-programmers/) It is based on haskell-mode and neocomplcache (with neco-ghc plugin). There are some nice plugins after that but most of them are just *nice to have* and not strictly necessary in my eyes.
&gt; it could send an automated email to the package maintainer I'd also cc a mailing list dedicated to anyone who wants to also be notified.
This &gt; So it may or may not be useful. Time will tell, refactoring it later shouldn't be too hard anyway. Makes it sound like a experiment with some potential benefit and if there is not benefit is easy to reverse.
The project could certainly benefit from narrowing a bit its goals.
Neato! One more thing for your list of "See also:" links: http://www.cs.nott.ac.uk/~gmh/fold.pdf
You have a couple of: case ... of False -&gt; return Nothing True -&gt; do return $ Just ... * Why not "if/then/else"? * You could use "guard" in the MaybeT transformer, instead, and avoid all the indents.
Ah, yes. I can totally agree with that. 
Using case is just a personal preference, as the branches look more obvious with the indentation and the punctuation. As for transformers, I generally find them extremely annoying in IO-heavy code due to all the necessary lifting (which was for instance necessary with the LCM monad of the old LambdaCube). I rather prefer to introduce some local functions if the indentation levels start getting out of hand.
One of the things I dislike about the arrow syntax is that we have too many arrow symbols going on so you end up with a which-way-is-up mess. Part of that is Haskell's fault, using `-&gt;` for lambdas and case binding, vs using `&lt;-` for do-notation and list comprehensions. Even if you stick with H98, mixing and matching those features can lead to ugly/unreadable code. The arrows notation also adds `-&lt;` to the mix and only exacerbates the original problem.
&gt; foldr is actually sufficient to define ANY structurally recursive **function** on a list type. This is important. Catamorphisms can define all recursive (mathematical) functions, but they cannot define all recursive algorithms. Thus, while catamorphisms and paramorphisms can define the same functions (prove the same theorems), paramorphisms are strictly more powerful if we care about algorithms, performance, etc.
That is an excellent idea! It definitely fits in with the idea of creating a 'dashboard' that project maintainers can use to monitor the overall health of their project in a simple way.
I'm not sure what sense you're using the word "function" or "algorithm" here. I think more sources tend to treat functions and algorithms synonymously. Also, folds do not give rise to all (general) recursive functions. They only give you structure recursion (where you can recurse on sublists of a list). This kind of recursion covers most of what we care about, including nasty things like Ackermann, but we lose the ability to write things like eval. I would think compilation would be far enough removed from the concepts of category theory to actually produce some useful results :) But you're right in that most of the time, we're more interested in programming with recursors than folds.
If the Haskell community had achievements, this would be solved very rapidly. * Achievement [100 points]: OpenAndShutGL
&gt;But using map alone we cannot implement foldr. (This statement is morally true, but I’m not sure of the formal sense in which it is true). It is fairly trivial to prove by induction on the size of the input list that map will always result in a list that is the same size as its input, from which we can easily infer that there are some folds that aren't maps.
Could one then push corrections back to the OpenGL standard? (In theory and in practice could be different answers...) Any commercial work I've done was 5x harder than expected, and the 80% "dark matter" in the universe was cleaning up bad data.
He means that although functions like `tail` can be defined using a catamorphism like `foldr`, they are extremely inefficient (efficiency is a concern for algorithms, but not functions). A paramorphism, on the other hand, lets you do it in `O(1)`.
Sure, but there's a question of "what other operations are we allowed to use?". The reason I'm not sure how to formalise it is that I don't know what other operations should be permitted. 
&gt; Map ignores the structure of containers... To parallel this idea, `foldr` *replaces* the structure of a container.
Please don't call me or Laar masochists: [opengl-api](https://github.com/noteed/opengl-api) :-)
I only have one thing I can say to this: ROFL.
The "people familiar with type theory" circle should be much smaller.
Last time I checked, which was a few years ago, the SDL library redefines the `int main(void)` function as a macro. The solution was write and compile a C file that calls includes the SDL headers and calls the Haskell `main` function via the FFI. Unfortunately, this elaborate procedure means that you can't use SDL from ghci.
Haha, true. For that matter, so should the 'proponents of static types' circle. :)
Note that as far as I know on OSX, UI applications do not have the same path than the terminal, which could explain why it builds from the command line and not from an IDE. There are a few pages on the web that explains how to configure, notable [this one](https://developer.apple.com/library/mac/#documentation/MacOSX/Conceptual/BPRuntimeConfig/Articles/EnvironmentVars.html#//apple_ref/doc/uid/20002093-113982).
Is it me or is ghc-mod kinda crap? GhcModType only seems to work on code that compiles, which is less than helpful because most of the time I want to know what GHC is inferring my code as when it has type errors.
This page just shows me the raw source when I visit it. Will try again later.
The groups are already divided. Pointing out a division is not the same as creating one.
Nice observation! Note that your *compose* is just the *mconcat* of the [Endo monoid](http://hackage.haskell.org/packages/archive/base/latest/doc/html/Data-Monoid.html#v:Endo). So this also works: foldr' :: (a -&gt; b -&gt; b) -&gt; b -&gt; [a] -&gt; b foldr' g z xs = appEndo (mconcat $ map (Endo . g) xs) z
SRS
You tend to have one object that contains all the state, like data State = State { stateInventory :: IORef Inventory, … } data Inventory = Inventory { inventoryItems :: [Item], inventoryCounter :: Integer } either pass this to all functions concerned or wrap your app in a Reader monad and you're good to go. Whether you put the whole thing in an IORef (or MVar) or only parts depends on the granularity you want. I think anything else is something your UI framework should provide.
Conflating Haskell with all of static typing. That's a paddlin’.
Forgetting that the three most popular languages (Java, C, C++) are statically typed. That's a paddlin'.
All four are basically unsupported, but we do have the ability to compile code for ARM. You'll have to link to that code manually from the main app code though, so it's not the ideal solution.
Just start from the makefile version of this: https://github.com/simonmichael/hssdl-mac-example Works for me.
This is still true. After a quick googling I found [this](http://hackage.haskell.org/cgi-bin/hackage-scripts/package/TimePiece/) project on Hackage that did this work-around, although I can't check it right now to verify if it still works.
Pssst. I don't think that is what is meant when folks call Haskell a [Bondage and Discipline](http://dictionary.reference.com/browse/bondage-and-discipline+language) Language. =)
You made me laugh, nearly choking on my water. I hope you're happy.
Not Haskell, but Steve Losh's "Caves of Clojure" series has a post on solving this problem with managing game state in a functional, immutable style: http://stevelosh.com/blog/2012/07/caves-of-clojure-02/
The issue seems to be that it captures mouse moves on the browser, and then doesn't release the memory. The page http://elm-lang.org/learn/What-is-FRP.elm also captures (and displays) mouse movement, but it garbage collects the memory after a short while.
It's quite easy to read the comic as "people who prefer dynamically typed languages are just uneducated (about type theory)". Even if that's true, it's not a productive way to encourage people to educate themselves.
I would happily pledge for ghc/ghci on Android project on Kickstarter.
By any coincidence are you in Switzerland? Because, if so, I think I might be collaborating with your guys who do that bytecode analysis.
I think pointing out the division reminds people there is one and reinforces it if it exists. Also Tibbe is right. Maybe if there were four circles instead and the new circle was 'people who understand testing' or something and this circle hardly intersected with static proponents in the same way then the comic wouldn't be as biased in favor of static proponents. But it's not my comic.
I like this answer, but I'd just like to give a shout-out to the much simpler (and IMHO nicer) http://hackage.haskell.org/package/data-lens
I don't think this conversation is going to be constructive. It's probably best if it ends before it begins.
Brilliant idea. In the Khan curriculum does this depend on Algebra 1? :P I'm busy until later in the day so I can't watch it for a while but is there any homework? I think having something to work on after reading the material would help it sink in a lot more.
It certainly sounds like you're paying attention to all the right things (or at least attempting to). It's likely that to really gain trust you need your implementation to have been widely deployed for a fair while and for no issues to have come to light.
IT WORKS! See the latest https://github.com/sonyandy/perm/blob/master/Control/Monad/Perm/Internal.hs. I had to add a `Fix :: MonadFix m =&gt; (a -&gt; b) -&gt; (a -&gt; Perm m a) -&gt; Branch m b`. The tricky part is making sure that the `Fix` branch doesn't add another point to permute around - note that `zipP` calls `zipP` (rather than `zipM'`, like the other branches), `zipB` calls `zipB`, etc.
this looks awesome!! i'm debating on starting a project with cloud haskell + C vs pympi and C now
I've seen GADTs around but was unsure about the motivation. I am no longer, thanks to you. Great video.
Neat observation! I just wrote a response in the form of another blog post: [foldr is made of monoids](https://byorgey.wordpress.com/2012/11/05/foldr-is-made-of-monoids/).
`lens` is simultaneously massively more complex and makes use of many language extensions. `data-lens` has to this point solved what I needed solved with much less, much simpler, and more portable code.
Mr. Khil [approves](http://imgur.com/wKVut) this one.
I had the exact same sentiment as tibbe. Small communities don't have the luxury of being divisive. Calling people uneducated is not a good motivator. This may not be true for all people, such as yourself, but this isn't the way a community should be represented. You know what they say: you can catch more flies with honey than vinegar. You can show people they're have something to learn by making a good argument, but you can't call them dumb :).
 ala Endo foldMap
No context holes for now?
They are dumb (or stubborn?) not because they don't know something (type theory in particular), but because they get into arguments about it without understanding it first. I didn't intend to make fun of people who are just not educated about types.
No, they will be in the next GHC release IIUC.
yes, cloud-haskell is a good framework. although not to confuse anyone, they have very different goals. cloud-haskell is about message passing between haskell programs instance to do distributed computing whereas connection is about connecting to typical run-of-the-mill server on the network : imap(s), http(s), nntp(s), xmpp, etc
A warning for an issue that bit me, the segfault bug in GHC when taking the ceiling of a negative float still exists in the 32 bit version of the OS X platform. Not that it's a Platform issue as such, but meh! :-)
This has been in vim2hs for quite some time, as an UltiSnip snippet. Type `mod` and press tab. For executables the `main` snippet is similarly useful.
In fact, `fold f :: [a] -&gt; m` is *the unique* monoid homomorphism from `[a]` to `m` that maps `[x]` to `f x`. There is exactly one such homomorphism because `[a]` is the [free monoid "on `a`"](http://en.wikipedia.org/wiki/Free_monoid). (The type `[a]` is unique in the sense that all free monoids "on `a`" are isomorphic to `[a]`. But they need not be the same: an example for an isomorphic representation is data SnocList a = Nil | Snoc (SnocList a) a which represents lists backwards.)
Thanks to Heinrich Apfelmu actually, I'm just the messenger :)
I believe iOS is where most of the effort is these days. Stephen Blackheath and Karel Gardas built an LLVM backend for cross-compilation, and there's been ongoing work on it since. There are plans to incorporate it into GHC at some point. The current project is up here: https://github.com/ghc-ios/ghc/wiki
I'm happy to see that split got added; I always wondered why that wasn't a part of the base platform!
that would contradict &gt; small updates to the versions of GHC (now 7.4.2) it was actually 7.4.1
Indeed it would have been nice, but 7.6.1 was release just too close to the platform's version freeze date for us get all the packages in the platform updated, and be sure that, when release, most of the common hackage packages would be updated too. Last thing we want is for someone to download the platform, then cabal install some well known package only to have it fail to install because it hasn't had the version limits fixed!
Or to something else, like a finger tree. Why are lists special? I don't yet see it.
Glad to see Async and Vector added to the standard libraries! (Split's not bad either)
Ah yea; good luck to you and your husband with that!
Just out of curiosity, how was the video made? Is there a special program do do this, or is it just some maximized graphics program?
&gt;YAGNI That platitude is meaningless in rubyland, it is even more so here. There is nothing wrong with designing software, or putting thought into the process. Stumbling along blindly with no foresight isn't actually a virtuous thing and worthy of praise and emulation. When presented with "it may or may not be useful", your response is "well then don't find out, just delete it now". That's not a good way to end up with the best product possible.
Lists are special for the reason sebfisch gives: they are (up to unique isomorphism) the free monoid on a single generator. 
It's the composability. Haskell encourages the programmer to structure the code as small(ish) discrete units that are combined together with well understood composition operators (not just (.), but also &lt;*&gt; for applicative functors, etc). When refactoring, these self-contained units can be recombined in different ways without modifying the units themselves. ETA: This comment obviously applies ease of refactoring rather than the frequency of it. As geezusfreeek commented below, Haskellers refactor their programs all the time, sometimes mostly just to toy around with it. Type-safety and composability really fascilitate this.
And here I thought this was going to discuss write-once Haskell code for Windows, Linux and Mac OS X desktop applications!
Right, people need to get used to the idea that there will be a significant stabilisation period between shiny new major releases of GHC and them getting into the next platform release. Think of it like releases of gcc vs a Linux distro.
I can't speak for anyone else, but I refactor more often in my haskell projects than I did in other languages. Refactoring my haskell code takes me from "working code A" to "working code B". Refactoring in other languages often takes me from "working code A" to "seems like it works code B, but you'll find out after it has been in production for 2 weeks that your tests missed a critical bug you introduced in a seldom used code path". I developed an aversion to refactoring because it almost always resulted in me breaking shit. I think a lot of people have that aversion, but I found it pretty easy to get over once I started using haskell.
I was actually [just discussing this the other day on HN](http://news.ycombinator.com/item?id=4707743), in the context of the banishment of `reduce()` from the default scope to the `functools` library in Python 3 -- but I lacked the confidence/formal background to confidently continue the discussion. The counterparty to my discussion brought up the example of `foldl (/) 400 [4, 4, 5]` not involving a monoid. My research on wikipedia brought me far enough down the abstract algebra rabbit hole that I had no idea if [what I was reading](https://www.google.com/search?q=quotient+monoid) was genuinely applicable, or just [polysemous](http://en.wikipedia.org/wiki/Polysemy). *Are* there things, in practice, that can be expressed with `foldr`, but aren't, or aren't easily expressed as, monoids?
readfile and writefile both throw exceptions, but you can make them return eithers instead very easily. import Control.Exception eitherReadFile :: FilePath -&gt; IO (Either SomeException String) eitherReadFile = try . readFile eitherWriteFile :: FilePath -&gt; String -&gt; IO (Either SomeException ()) eitherWriteFile fp = try . writeFile fp There, now all four functions return Either which means you can pattern match on their results and deal with the case that any of them fails, such as log an error to let you know that this or that url failed to load or this or that file didn't exist for reading so that you can deal with them later.
Jesus Christ that code snippet makes me cry
I am not so good at coming up with examples on the fly :(
Haha it wasn't a criticism against you, just an argument for pattern-matching :P
Just in case some newbie sees this code: don't do this. The idiomatic way to do this is: f x = case x of Nothing -&gt; v Just x' -&gt; g x' or the more concise (but less newbie-friendly) f = maybe v g The advantage of both is that checking for `Nothing` and extracting the `Just` value is done in one step, so there is no room for errors (imagine switching the two branches of the `if_then_else` by mistake -- the type checker won't catch that)
Could someone write up a step-by-step for that?
What about hlint?
Thanks, I stand corrected. Though my latter point stands, I guess.
Not on a single generator, rather for a type a, [a] is the free monoid generated by the values of type a (so it is a single generator only if there is a single value of type a).
Haskell lets you put a lot of information in the types; moreover, it forbids you to leave out certain information (e.g., side effects). Together, these two points mean that we can leverage the type checker as a term-transformation checker. That is, when we monkey around with the value-level code, whatever we do is reflected into the type-level in an abstract enough way that the compiler can check for obvious bugs our refactoring could have introduced. IMO, the key point of this is that Haskell's types capture side effects--- which makes it radically different not only from Java and the like but also from ML and other typed functional languages. Side effects are a major hinderance to compositionality, and they're almost always implicated in the failures of refactoring in other languages.
Thanks for the errors package. I've recently come across uses of it in Snap and found it very useful. A suggestion: the [errors](http://hackage.haskell.org/package/errors) hackage page doesn't go into much length as to how this library is intended to be used. The sub-modules have some documentation but it's the top-page where most people first land - and I think this is where people decide whether or not they find your library useful.
I don't see any language blaming. 
An upvote doesn't do this enough justice! Control.Error is imported in practically every module I write.
On the opposite, this blog post is rather constructive. The author blames himself first, and relates his experience. And he is absolutely right that deferred evaluation is not very compatible with classic stacktrace-based debugging, this is a well know fact.
It doesn't just yet, it "just" need to be exposed with some simple settings in connection. The hardest part is probably just finding the best way to expose it to the user, all the plumbing in tls is already done indeed.
I'll try to add on why refactoring is better/easier/encouraged in Haskell: You have much more certainty in Haskell that your refactoring is correct. Since it's a pure language, and every function has it's 'own scope', you are sure that changing one function won't affect the others. When they do affect the others (like changing the type input of a function, it expected an int but now requires two doubles), the type checker will give you exactly where you need to change. Also, Haskell normally has less lines of code and less variables (so local values maybe, mostly none), which I guess would mean not having to search the entire file multiple times to see where that variable matters.
Anything isomorphic to lists is just as special.
I share a similar sentiment… the contrived examples of FRP are nice, but as a whole I'm still holding out on the research of this new area to produce something simple that is obviously better, not kinda-better-except-for-we-don't-know-how-to-do-this-yet-and-that-might-use-gigs-of-memory.
Congratulations! Great Job.
When I was having weird troubles in ghci with reactive-banana, it was immediately clear that the problem had to be in the use of `unsafe` functions. It's a very nice way to narrow down bugs.
The morale is probably to not test with optimization on. Or rather to test both: the functions without rules and the functions with rules.
The strong type system with good parametricity guarantees and isolation of side-effects in Haskell are to me the two largest reasons why it is amenable to huge sweeping refactorings. The strong type system catches a lot of obvious bugs. With parametricity I can constrain the possible space of functions that typecheck down to a much smaller subset. (See my reply to Kent Beck, [here](https://www.facebook.com/notes/kent-beck/functional-tdd-a-clash-of-cultures/472392329460303)). Finally, with IO safely contained there are many fewer things a function can do behind my back without being obviously bad. Finally, there is one more point not included in the above point, laziness. With laziness as the default, it encourages you to write algorithms once and for all in their most general form, because they can fuse together in ways strict algorithms won't. The encourages the use of parametricity, it also encourages that elusive Holy Grail of computing: code reuse. In an imperative language you are always tempted to fuse together two algorithms entangling your concerns. While in a lazy language, you gain the ability to write, say, `sort` once and for all and `take` once and for all and then fuse them into `take 10 . sort` in such a way that this doesn't bother to sort the tail end of the list, just inspect the elements to figure out which of them are the first 10, yielding an optional k-quickselect with better asymptotics. This compositionality isn't directly of concern while refactoring, but it does tend to enable you to refactor down into smaller components, which are easy to reason about in isolation, and let the compiler interleave them. From a team perspective, this is one reason why I love Haskell. You can have two different people work on two separate algorithms and compose them with (.) and if you have good system design things can fuse away into an algorithm that might be too complicated for either of them to hold in their head at the same time.
Or rather : don't expect the composition of a couple of functions to be a valid test for the behaviour of each individual function. In particular when they are unsafe or partials. 
What about don't use optimizations that actually change program semantics?
Anybody else just being sent a file named "download"? If so, try [just looking at the blog itself](http://ics.p.lodz.pl/~stolarek/blog/); it's the November 6 post.
In general, avoid `Eq.(==)` whenever you can use pattern matching instead. And no, using k-pattens (patterns with numbers) is not avoiding `Eq.(==)`. While patterns on ADTs don't imply the use of `Eq.(==)`, the following functions are equivalent due to the way numbers work in Haskell: fe 1 = 1 fe n = n * fe (n - 1) fa n | n == 1 = 1 | otherwise = n * fa (n - 1) The same holds for patterns involving characters and strings. Although implementations of `Data.Char` usually don't have a practically infinite number of constructors, they have to be treated as such.
So do I, now. But if I hit reload, then it works!
Thank you, this looks like exactly what I need.
I finally understood monads when I learned category theory. The whole "a monad is just a monoid in the category of endofunctors, what's the problem?" thing may sound like a joke, but if you 'understand' them any other way, you've not grasped the whole story.
I think trying to do it in C++ isn't likely to end well. As a professional C++ programmer, if I saw that code I'd think "why are you obfuscating what you are trying to do?" Because, in C++, ';' is bind and you're operating in the IO monad all the time anyways. And the language gets in your way--you don't sell people on a concept by showing them that they have to write five times as much code to do the same work. If I wanted to explain why Monads are useful to an audience of C++ programmers, I'd probably talk about nondeterminism (the list monad), since it's both understandable, and the second hardest "common" side effect to implement without an abstraction in C++ (the hardest being continuations/callCC). Perhaps implement a simple parser combinator library? `type Parser a = (char*) -&gt; [ (a, char*) ]` where you advance over the characters in an already-allocated string.
&gt; Thus I think the more important question is how easy a language makes it to refactor you application into the appropriate abstractions. Kind of feels that would be one way of looking at the question. So not so far from my original question I mean. edit: A "refactor-free language" would be a language that require minimal refactoring effort - or minimal effort in alterning the abstractions composing the application.
I will try to make it a bit more concrete; I'm thinking that ideally a program could be extended by simply adding new declarations, where 1+ function means 1+ feature. And you can go on in this fashion without adding more and more effort for each new feature. Each new feature makes use of earlier declarations but doesn't need them to change in order to go on. Or, in addition, a change in an earlier feature could also be made with the "same amount"* of effort. *= When I say "same amount" what I mean is constant on average. No matter how large your code base become, adding a new feature will not mean that consecutive features will be harder to add. edit: I realise that this may not be all clear.
here is a monadic query api (scala), decent advanced example: http://slick.typesafe.com/ muggles get list comprehensions, generalizing them seems a decent way to introduce monads without getting too much into category theory
So far I found Hastec backend very cool for compiling to JS. Then would use PhoneGap or alike to go multiplatform. See cool posts here: http://jshaskell.blogspot.com/
["You could have invented monads"](http://blog.sigfpe.com/2006/08/you-could-have-invented-monads-and.html) is the tutorial where I finally got it. A Haskell monad is an interface that overloads the `return` and `(&gt;&gt;=)` operations. That overloading is the entire point of monads. After all, we *could* use data-type specific operations (like `singleton`/`concatMap` for lists instead of `return` and `(&gt;&gt;=)`), but we use the more generic monad operations because we want to program generically to all data types that implement `Monad`. We prefer to program generically to the `Monad` interface for two reasons: a) Several functions are completely generic over the type of monad, such as `mapM` and `sequence`, so we can reuse these functions for any data type that implements `Monad`. This encourages code reuse, which makes life easier for library writers (they don't need to implement a custom `sequence` for their new data type) and for users (they don't need to learn a new API for each new data type they use). b) `do` notation is syntactic sugar generic over the type of monad, so we can reuse the same syntactic sugar for any type that implements monad. This beats creating a new type of syntactic sugar for every new data type that we add to the language.
Everyone has their own path to understanding monads. Its why we have so many monad tutorials!
I finally understood monads when I realised that a `Monad` is just a `Category` `k` with a bijection between `k a b` and `a -&gt; k () b`
There is an implementation of monads in the fc++ library. I cannot say much about it, since I have never used the library, or studied the source code in any detail (I would probably not understand the advanced parts of it at all.) http://people.cs.umass.edu/~yannis/fc++/ There are links to a paper and a short text about lambdas and monads in C++03 on the page.
&gt; As a professional C++ programmer, if I saw that code I'd think "why are you obfuscating what you are trying to do?" Because, in C++, ';' is bind and you're operating in the IO monad all the time anyways. But the stated point of the exercise is to explain monads, not to write idiomatic C++. That's like trying to explain the sonnet form and the next person turns round to say --- "why are you writing it in that funny format? You're adding words and changing the order to meet stupid criteria rather than just writing what you mean".
&gt; Can you have a template whose parameter ranges over class templates? C++ templates do support template template parameters however they have limitations and not particularly consistent with first-order kinds (normal template parameters). They can be quite painful to work with. For these kind of high-level abstractions an advance C++ programmer would rarely reach for template-templates as they are not particularly useful usually, instead C++ programmers can take advantage of the untyped nature of template parameter and would write code as someone would write code in a dynamically typed language. There are no kinds for types in C++, template type parameters are untyped. So when you screw up the types you're in for pain. C++ programmers define *Concepts* (kind of like type-classes) in documentation and/or comments but they are no language features currently for defining them. 
This is extremely helpful. Bookmarked! This paragraph in particular is really interesting: &gt; Thinking back to our previous example, we need to take two steps to fix this: provide a bind function to turn children into composable form, and write a unit function to turn the initial input – the heading – into an acceptable type. It implies that for any given type, it can be made composable by defining these operations. I've been thinking and explaining it backwards, that for any given definition, a type is composable.
&gt; We prefer to program generically to the Monad interface for two reasons: The second reason is impossible in C++ right now as far as I'm concerned (though I'd like to be prooved wrong). The first feels like a very difficult point to make because, while the most commonly used types can be used as monads, (sequences or lists and maybe values), they constitute only two instances. Although the monad implemented in that article perhaps articulate this well.
Thank you for telling me. (fixed)
There is no "one true" solution. Haskell is a language, not a framework. :) However, I can still give a lot of suggestions on several good solutions. In my opinion, the best solution for this problem is not to produce the intermediate list at all and to consume the list as you generate it. `mapM` (without underscore) leaks space for large lists so use an iteratee library instead and structure `getSKUs` as a generator like you would in Python. Without an iteratee library, consuming the list as it is generated is error-prone, non-compositional, and not very functional in style, but with an iteratee library you get all the benefits of a functional style while still getting efficient use of memory and resources. I'll shamelessly plug my `pipes` library for this purpose, although any other iteratee library would suffice. Also, if your list is not long and you use `mapM` anyway, I'm still introducing the iteratee solution first because it informs the canonical solution for `mapM`, too. The first solution handles the the error on the spot and only produces integers: import Control.Error import Control.Monad import Control.Monad.Trans.Class (lift) import Control.Proxy type Path = Int -- Just for demo getSKUPaths :: Path -&gt; Script [Integer] getSKUPaths x | odd x = return [1, 2, 3] | otherwise = throwT "Bad path" -- You can optionally add 'scriptIO's so that the base monad -- is still Script, but I'm being lazy getSKUs :: [Path] -&gt; () -&gt; Producer Integer IO () getSKUs pagePaths () = forM_ pagePaths $ \pagePath -&gt; do skuPaths &lt;- lift $ runEitherT $ getSKUPaths pagePath case skuPaths of Left err -&gt; lift $ errLn err Right ns -&gt; mapM_ yield ns Here's an example run: runProxy $ printD &lt;-&lt; getSKUs [1..4] 1 2 3 Bad path 1 2 3 Bad path The next solution produces `Either`s and leaves error handling to a downstream stage: import Control.Error import Control.Monad import Control.Monad.Trans.Class (lift) import Control.Proxy type Path = Int -- Just for demo getSKUPaths :: Path -&gt; Script [Integer] getSKUPaths x | odd x = return [1, 2, 3] | otherwise = throwT "Bad path" handle :: () -&gt; Pipe (Either String Integer) Integer IO r handle () = forever $ do e &lt;- await case e of Left str -&gt; lift $ errLn str Right n -&gt; yield n getSKUs :: [Path] -&gt; () -&gt; Producer (Either String Integer) IO () getSKUs pagePaths () = forM_ pagePaths $ \pagePath -&gt; do skuPaths &lt;- lift $ runEitherT $ getSKUPaths pagePath case skuPaths of Left err -&gt; yield (Left err) Right ns -&gt; mapM_ (yield . Right) ns Here's a demo: &gt;&gt;&gt; -- Print the raw unhandled values coming out &gt;&gt;&gt; runProxy $ printD &lt;-&lt; getSKUs [1..4] Right 1 Right 2 Right 3 Left "Bad path" Right 1 Right 2 Right 3 Left "Bad path" &gt;&gt;&gt; -- Handle them using an intermediate stage &gt;&gt;&gt; runProxy $ printD &lt;-&lt; handle &lt;-&lt; getSKUs [1..4] 1 2 3 Bad path 1 2 3 Bad path You'll notice that the iteratee style lets us decouple our error-handling logic from our list production logic, but without the space leak penalty that we would get from using `mapM`. Ok, so back to `mapM`. The first solution translates to: import Control.Error import Control.Monad import Control.Monad.Trans.Class (lift) type Path = Int -- Just for demo getSKUPaths :: Path -&gt; Script [Integer] getSKUPaths x | odd x = return [1, 2, 3] | otherwise = throwT "Bad path" getSKUs :: [Path] -&gt; Script [Integer] getSKUs pagePaths = fmap concat $ forM pagePaths $ \path -&gt; do e &lt;- lift $ runEitherT $ getSKUPaths path case e of Left str -&gt; do lift $ errLn str return [] Right ns -&gt; return ns Here's the demo: &gt;&gt;&gt; runScript $ getSKUs [1..4] Bad path Bad path [1,2,3,1,2,3] The second solution translates to: import Control.Error import Control.Monad import Control.Monad.Trans.Class (lift) type Path = Int -- Just for demo getSKUPaths :: Path -&gt; Script [Integer] getSKUPaths x | odd x = return [1, 2, 3] | otherwise = throwT "Bad path" getSKUs :: [Path] -&gt; Script [Either String Integer] getSKUs pagePaths = fmap concat $ forM pagePaths $ \path -&gt; do e &lt;- scriptIO $ runEitherT $ getSKUPaths path case e of Left str -&gt; do return [Left str] Right ns -&gt; return $ map Right ns handle :: [Either String Integer] -&gt; Script [Integer] handle es = fmap concat $ forM es $ \e -&gt; case e of Left str -&gt; do scriptIO $ errLn str return [] Right n -&gt; return [n] Here's the demo: &gt;&gt;&gt; -- raw output &gt;&gt;&gt; runScript $ getSKUs [1..4] [Right 1,Right 2,Right 3,Right 4,Right 5,Left "Bad path",Right 1,Right 2,Right 3,Right 4,Right 5,Left "Bad path"] &gt;&gt;&gt; -- handled output &gt;&gt;&gt; runScript $ handle =&lt;&lt; getSKUs [1..4] Bad path Bad path [1,2,3,1,2,3] In my own code I use the very first approach (the iteratee one that just prints any errors on the spot and then proceeds to the next value) because it's very simple, doesn't sacrifice compositionality, and doesn't leak space. Also, notice that with the iteratee solutions you handle the results as they are generated, but with `mapM`, you can't access the first result until the entire data set has been processed.
There are LOTS of monads in Haskell. I can name a several very useful ones off the top of my head: * free monad/cont/iteratees/generators * STM * Writer/State/IO * Maybe/Either * List * Parse * Any combination of the above (using monad transformers)
Start with the fact that Haskell was an attempt at studying lazy languages. Maybe demonstrate normal order evaluation of a math expression. Then maybe point to "short circuit" statements in C++ like &amp;&amp;, ||, and if-else if-then. Haskell takes laziness to its extreme. One consequence is that the order things get evaluated in a function is determined by the caller. The function itself doesn't get a say in the matter. Given the above, we have a problem. The order of certain things do actually matter. In particular, side effects. `cout &lt;&lt; "hello "; cout &lt;&lt; "world!";` is different from `cout &lt;&lt; "world!"; cout &lt;&lt; "hello ";`. The way we rectify this problem is through a tool called a [continuation](http://en.wikipedia.org/wiki/Continuation). A continuation is like GOTO for the functional world. To put them simply, they are functions that "get to choose" where they return their return values. So instead of `return x + 1` you have `return x + 1 to print` for some function print. The continuations act like the callee now. And so you have (indirectly) given functions back their ability to choose the order things happen. It just so happens that [(free) monads are the categorical equivalent of continuations](http://blog.sigfpe.com/2008/12/mother-of-all-monads.html). That's the bird's eye view of the problem in Haskell. The rest is all details and making a usable API. `do` notation is just a way of cleaning up [continuation passing style](http://stackoverflow.com/questions/4525919/continuation-passing-style-vs-monads). Specific monads, such as List or Parsec, are all just continuations with additional properties. But under the hood, it's all just about choosing where your return values end up.
Just a FYI, the link to the Arch Linux distribution under the [linux section](http://www.haskell.org/platform/linux.html) for the Haskell Platform hasn't worked for some time now (404 error since around March). It was moved to the [AUR instead](https://aur.archlinux.org/packages/haskell-platform/), with packages for GHC, Cabal, Alex, Happy, etc. being distributed individually via the official Arch Linux Community and Extra repositories.
As a C++ coder *and* Haskell coder, I can qualify that the code is a horrifying mess. If that weren't the case, I'd have started writing code with *that* instead of the more modern C++11 techniques. I also think their focus on mathematical data structures instead of the STL was mis-guided.
The problem I have in pointing to examples is that I have to write them first. In the standard C++ library, only Maybies and Lists (sequences) are there already, to my knowledge.
Yeah, that is unfortunate. However, this link does a good job of explaining the continuation monad for C++ programmers. http://fpcomplete.com/asynchronous-api-in-c-and-the-continuation-monad/
Um, foldl (/) x = foldl (*) x . map (1/) so expressing that one as using the monoid structure is easy. The point though is that we don't need to resort to add hoc transformations like this: foldr (and by extension foldl) can be defined just in terms of fold (on monoids) and map.
Thanks. &gt; The trick to using lens is to completely ignore the types, except at a superficial level (i.e. it expects some lens-ish thing here), and just assume that it works. The problem is that I hate doing that. It feels like going blind, groping around in the dark. It's hard to nail down exactly why, but hopefully you can relate. I find it hard to use something if I don't understand it. It's one of the reasons I don't like it when libraries play complicated type-level tricks just to make the syntax of the term level a little bit nicer (without implying that `lens` is guilty of this)... the types are much more important to understanding the whole thing. If there's one specific thing I would really like to seen explained in a simple and obvious way, it's why and how having four type parameters instead of two lets you do polymorphic update. What those type variables *stand for* conceptually, i.e. what human friendly names (instead of `a b c d`) you might give them if you thought being friendly to humans was important. And so forth.
Well it shouldn't have, that's the point ! He probably created a personal rule for GHC that fused a shift left and a shift right but when you create such a rule, the burden of the correction proof is yours and here it wasn't done... before the rule was implemented. He tested for correction but only **after** the rule was applied and so he missed the cases where his original functions were wrong.
I think there aren't so many refactoring opportunities that present themselves late in a project's life, as long as you've been pretty aggressive about it early (which is hard *not* to do). In that sense, the refactoring effort doesn't really get harder. I'm not sure how unique to haskell this is. I would also say that the difficulty of whatever refactoring you do perform grows sublinearly with the size of the project (since most projects have some semblance of loose coupling), I think just as with any other language. To be honest, I don't know what you are looking for. I know of no unique property haskell has which has unbelievably profound effects on refactoring. Refactoring in haskell is nice, and that's all I can really come up with.
[This post](http://www.reddit.com/r/haskell/comments/12lyr3/overveiw_of_current_scoutess_architecture_and/c6wguvs) explains why I dislike it.
In haskell, you can write [1,2,3] &gt;&gt;= (\x -&gt; [x,-x]) In C++, you write... std::vector&lt;int&gt;{1,2,3} ; []( int x ){ return {x,-x}; } ...? It seems to me that there's a difference between bind and ;.
I had actually neglected to implement sequence before! Here it is: https://github.com/splinterofchaos/Pure/blob/monad/Monad.h#L278 It could probably use some refinement, though. One can do a direct literal translation of the Haskell version, using *head* and *tail*, which are easy to implement, and *fold* like in [GHC's source](http://www.haskell.org/ghc/docs/latest/html/libraries/base/src/Control-Monad.html#sequence) but I have found GCC produces suboptimal assembly, especially with tail recursion, so a loop is better.
&gt; While learning a concept, usefulness is ultimately irrelevant. I don't agree at all...at least not in the realm of monads. If that were true, then it would only take 5 minutes to "learn" monads--a monad is anything that has operations return and bind that satisfy three simple laws. But unless you've got a LOT of experience with abstract math, category theory, etc this isn't going to tell you much. The important thing to be learned is how this pattern is useful in a bunch of different practical contexts. I learned monads over a long period of time by seeing the pattern come up over and over again in my code. I learned monads by seeing how much repetition they could help me eliminate. I learned monads by banging my head against ugly code for awhile until I found out about a new monad that allowed me to make it all pretty again. With monads usefulness is the whole point.
The fact that the loop works makes me a bit suspicious. Does this really do a Cartesian product when used in the list monad? sequence [[1,2,3],[4,5],[6,7,8]] -&gt; [[1,4,6],[1,4,7],[1,4,8],[1,5,6],[1,5,7],[1,5,8], [2,4,6],[2,4,7],[2,4,8],[2,5,6],[2,5,7],[2,5,8], [3,4,6],[3,4,7],[3,4,8],[3,5,6],[3,5,7],[3,5,8]] It looks like you're running each of the actions in the list and using side effects to push the results onto an external list. I'd expect behaviour something more like join from that.
Ah, I see that I overlooked how sequence actually worked. Fixed! https://github.com/splinterofchaos/Pure/blob/sequence/Monad.h#L278 I'd rather have a correct implementation than an optimal one, so I dropped the loop and translated it literally. And here's the test I did: https://github.com/splinterofchaos/Pure/blob/sequence/examples.cpp#L465 It seems to work perfectly. Outputs: &gt; sequence [[1,2,3],[4,5],[6,7]] = {{1 4 6} {2 4 6} {3 4 6} {1 5 6} {2 5 6} {3 5 6} {1 4 7} {2 4 7} {3 4 7} {1 5 7} {2 5 7} {3 5 7}} Thanks! One of the hardest parts of writing functions like this is not understanding what it really does. The thing that really gets me about people who think FP is impossible in C++ is it's actually really easy.
We support both the cabal-meta and cabal-dev use cases in the new cabal sandbox system with a general support for package environments (http://hackage.haskell.org/trac/hackage/wiki/PackageEnvironments). The author's example would be translated to: cabal sandbox init cabal add-source ../foo cabal add-source ../bar Note that cabal's add-source, unlike cabal-dev's, creates a link to the added package so it gets rebuilt as needed.
I am dealing with people who obviously want to learn. It's not like I'm pulling teeth.
The new changes sound great, Johan. I can't wait. Will it be possible to use the cabal-meta functionality without using sandboxes? I still like the --user package repository sometimes. For instance, maybe I want to install my local development versions of all of the above packages, but not put them in a sandbox so I can then use them in any other web app that I might create.
No, the rule wasn't his, it came with Repa.
&gt; Free monads do not have any particular relation to continuations I wasn't sure about this part. I know the relationship between monads and continuations is pretty deep, but I knew that calling monads "the equivalent" of continuations was wrong. So I qualified it without thinking too hard and hit submit. :) In any case, I think the basic idea was right. What we're interested in is a kind of continuation. 
&gt; Why does not the best implementation just 'win'? Perhaps you could be more clear about what you mean by 'best'?
I didn't write my own rule - as sevenfive_ said it is a rule that comes from Repa library. Also I test only with -Wall, not with optimizations.
These function were not meant to be partials and that test was supposed to verify that. I also don't think that writing properties without composition is a good idea - it would be hard to write any useful property. You wouldn't be even able to wrote simple properties like "(reverse . reverse $ xs) == xs".
&gt; I don't think that GHC/lazy evaluation is the one to blame regarding the segfault issue. As the author of the post: I don't blame the compiler and lazy evaluation. It was a first thought that this my compilers fault (I was of course wrong) and lazy simply lead me astray. What can I say - lack of previous experience in such problems, nothing else.
I second that. If there was one blessed way to do sandbox builds and references to other source trees, it'd be great as I could integrate it into EclipseFP. Users have already asked for sandbox builds, and Eclipse allowing of course project dependencies, we could use these dependencies to automatically manage the add-source commands, so that changes in one project would impact automatically the other projects.
&gt; I think Edward did invest a lot of effort in simplifying the types. Never doubted that. I'm in awe as usual. Still, simpler doesn't mean simple. &gt; I hope my blog post can convey the clarity he had at the talk. I hope so too. Good luck.
Didn't I see mention of a feature to add a whole directory tree as a sort of local repository where subdirectories containing cabal projects are taken into account for dependency resolution?
We aim for a release before the end of the year.
In C++ you write int x = 2; x = 3; printf("%d\n", x); In Haskell you write newIORef (2 :: Int) &gt;&gt;= \x -&gt; writeIORef x 3 &gt;&gt; printf "%d\n" x or, more idiomatically do x &lt;- newIORef (2 :: Int) writeIORef x 3 printf "%d\n" x My point is that people writing C++ are already using the IO monad for all their statements. They expect to be able to write sequences of statements with `;` in between and combine them into programs. But you can't control what happens at that `;` like you can with the more general use that Haskell allows, which means you can't do your example (list monad!) without additional syntax. All the monad laws say is that `;` behaves like you would expect in terms of combining programs together.
Well thanks to both of you then, messenger and creator :)
I think I realize now that I'm a bit off in what I'm looking for and a bigger issue for effort-per-feature growth is the composability (as mentioned in the thread). In less composable languages you find yourself having to rewrite/copy-paste almost the same thing over and over because you can't split up existing abtractions so that they can be assembled in new ways. *The interesting point though*: How does this cause added* effort-per-feature growth? I can intuitively see something like how creating a slight variation that which can't be well composed of existing parts will cause you to often not only having to rewrite the immediate module but possibly modules that it depend on. It seems important but I can't make that more concise at the moment. However, perhaps this is partially contradicted with your objection ("... refactoring you do perform grows sublinearly ..."). (*) - I have an intuitive feeling that Haskell or possibly more concisely - more composability leads to slower effort-per-feature growth. Might this be true? 
&gt; How is a monad any different than a closure? My answer to that would be to first explain the difference between an anonymous function, a first class function and a closure. A lot of dynamic language programmers through them (particularly the first and the last) together as if they were synonyms.
How is `;` bind? Isn't `&gt;&gt;` more like `;` (or rather `,`).
This is the first video listed by error792 for his [UReddit category theory class](http://ureddit.com/class/36451/introduction-to-category-theory).
`int x = foo(3); printf("%d\n", x);` vs `foo 3 &gt;&gt;= \x -&gt; printf "%d\n" x`
I dunno. That seems like a pretty liberal translation. I mean, I can see it roughly, but the semicolon isn't really passing anything in (I'll assume) C. It just seperates the two statements. They're related by both referencing the same state, so it seems closer to (foo 3 &gt;&gt;= write x) &gt;&gt; (read x &gt;&gt;= printf "%d\n") I think a more faithful association would send expressions with type t to `IO t`s, statements to `IO ()`s, f(x) to `x &gt;&gt;= f` (or `liftM2 ($) f x`), and semicolon and comma to `&gt;&gt;` (roughly, hopefully you get the picture). IOW, I'd prefer to call function application bind. Apologies if I am at risk of being needlessly pedantic.
Yes, I'm being a bit fast-and-loose, but I think it's the best way to communicate the idea. I know when I write/use an EDSL in Haskell I always just think of "now I'm operating in the CircuitDescription monad, now I'm operating in State NetList, etc." and IO is basically the 'writing C code' monad.
For your first problem, you might want to play around with [this](http://elm-lang.org/edit/examples/Intermediate/Stamps.elm) sample in Elm. See if you can modify it so you can pick which type of shape you want to stamp. (I'll work on the problem myself when I get a bigger chunk of time.) Elm isn't Haskell, but it has very, very similar syntax and has FRP built-in. Any solution in Elm could almost surely be done in reactive-banana (I think reactive-banana is a slightly more general implementation of FRP actually).
In functional programming tutorials (or programming tutorials in general), yes people tend to use the word "function" when they mean algorithm. The distinction I make is the same one made by mathematicians, algorithmists/complexity theorists, computer scientists, and scrupulous programmers. Namely, a function is a mathematical object *relating* inputs to outputs; whereas an algorithm is a *process for transforming* inputs to outputs. For example, the list sorting function merely relates input lists to an associated list in sorted order. While there is only one such function (or one per notion of ordering, if you're being pedantic), there are many algorithms which implement this function: bubble sort, merge sort, quicksort,... Thus, there are certain functions (e.g., `tail` as godofpumpkins mentions) for which the most efficient possible catamorphism-based algorithm is less efficient than paramorphism-based algorithms. If you're familiar with logics for arithmetic (e.g., Goedel's system T), this is the same difference as between iterators (cata) vs recursors (para). And in something like system T you can show that the same theorems are provable regardless of whether you use iterators or recursors; however, if you look at the normalization and efficiency aspects you see that they are different, because there are more (and better) recursor-based algorithms than there are iterator-based algorithms.
It is the was it is because that way the right hand column is all the current versions. I agree it feels weird to say '(same)' first, but the result is more functional(!).
Hah, that's exactly the opposite of what I was trying to say! &gt; Then say 'but sometimes ; isn't good enough'. Examples can include list comprehensions and LINQ statements. A monad is just being able to program that ';' to glue statements together in different ways depending on your needs! It's just that the intuitive concept of gluing together statements is something a C++ programmer does every day already, there's nothing magical about it, and it's a path to understanding that doesn't immediately jump into unfamiliar territory for your target audience. Monads are cool because they take that concept of gluing programs together into bigger programs and apply it more generally to different kinds of programs (parsers! non-deterministic lists! exceptions! call/cc!), without needing to find a new language every time the current one doesn't have the feature you want.
That makes a lot of sense actually since the left column is more or less there for convenient comparsion of version numbers between releases.
I should rewatch the catsters because I remember them being enthusiastically presented but *way* over my head. I think I might have better luck now.
I was actually looking at this stuff in preparation for learning to use Elm. In fact, it was learning about Elm and reading the blog of that Debian developer that convinced me to start learning Haskell (and FP). I tried to do what you recommended, but realised you can't have two lift statements simultaneously, and then got stuck on the monads. I think I'll try to finish learning the monads more (though I've read it in LYAH, I've not much used them aside from IO and formatting). Though at the moment I've been busy solving the Rosalind project problems in Haskell.
Where do you live?
F# is a .NET language, and so, there may be places using C#, who may take the risk on a similar language.
Sure, and a shop running PHP might look into haskell. The hangups people have about using less popular languages tend to be "I can't put in ad in the paper and get 50 resumes", not "it doesn't run in the same VM as another language we use.
It does seem to go against everything about how IO is supposed to work in Haskell. As a newbie I managed to get myself into some very confusing situations where a program would only work when I added printf style debugging (because I was unknowingly forcing the IO). 
&gt;Change of trends? "Unit-test all your things" replaced "let it fail and just debug"? I'm not sure these are diametrically opposing viewpoints. The Go people believe in both unit-testing, but letting things fail without the follow up debug. Unit-testing doesn't preclude letting things fail, it has more to do with how you choose to address exceptional conditions. &gt;I don't know, my opinion is just you need the three [logic/tests/debug] You need more than that, but it's a great start. &gt;but the ability to launch/crash/debug/change-just-the-faulty-code/pick-up-where-you-were is awesome. I think most Haskellers are fairly circumspect about the power that offers and aren't hellbent on believing that they don't need debugging just because it isn't readily available. There's a reason GHCi is so prominent in the community. Haskellers aren't, in my experience, afraid of power. They just want it marshalled and regulated in a particular manner. And yeah, I started out in Common Lisp. Great environment for getting things done. I mostly tinker in Clojure these days. Haskell has taught me a lot, but it's hard for me to overcome the level of productivity I get in Python or a Lisp. Further, the strict/lazy semantics in Clojure make 1000000% more sense to me. I continue to learn Haskell in spite of myself :)
I'm an expert on neither, but what I can say is that F# is really more similar to OCaml than it is to Haskell, so I think it might be more profitable to compare OCaml and Haskell on the one hand and F# and OCaml on the other.
Indeed, F# has everything but the kitchen sink thrown into it. And as far as I'm concerned, that is, on the whole, a bad thing. Haskell, on the other hand is a comparatively simple, elegant language. I find it far more natural and enjoyable to use than F#, despite the fact that I make my living coding in C#. As far as I'm concerned, F# has but two advantages over Haskell: * better tool support * easier integration with .NET applications/libraries I'm hoping Haskell will catch up with F# in these areas.
Going from Haskell to F# was an exercise in frustration for me. F# 2.0 gains a lot of complexity in having to, essentially, support interoperability with C#.NET and loses a lot of the elegance of Haskell (e.g., no type classes, mutual recursion becomes trickier, etc.). It's not all bad but I did find it to be significantly different in a non-better way. The only real advantage is that it is .NET. It gets really ugly if you try mixing C# and F# development (I've experimented with doing Unity 3D development in F#).
The catsters are excellent. I cannot second that enough. I had the luck to attend a talk by Eugenia Cheng last year, and she is a great speaker.
I'm a young college student coder who only uses Haskell for ProjectEuler so far. Can you tell me about your career as a Haskell software engineer? That sounds very interesting.
https://github.com/honza/snipmate-snippets also provides a snippet for auto-inserting the module name. All you need to do is type module&lt;tab&gt; if you have snipmate installed.
Where are you hiring? :)
&gt; (vs. Haskell lacks this ability) This isn't quite true, and If I remember correctly Haskell had this as an extension before F# existed and F# active patterns where inspired by them. &gt; F# have feature called computation expressions, similar to do-notation in Haskell, but instead of implementing mind-blowing wrapper for writing "imperative" code, it is used to create such things as asynchronous workflows and m-brace "Computation expressions" are just Monads with a nicer name, do-notation is just syntactic sugar for monadic expressions similar to how F# builder notation is syntax sugar for the same. You can write any of those kind of abstractions that exist for F# in Haskell and it would probably be easier to express in Haskell due to .NET limitations of the type-system. The problem with F# (due to .NET) is the lack of higher-kinded polymorphism, the lack of parametrization by type constructors (you may understand this as generic-generics) so you can not easily write the same high-level generic code you can write in Haskell, you can not easily mix different types of monads together in the same monadic expression. You can not easily write monad transformers to the same level of genericity as Haskell. You can not easily write very high-level abstractions based on category theory like you can in Haskell. F# also lacks higher-rank polymorphism, lacks the ability to use operators in interfaces (again due to .NET limitations) thus you cannot easily constrain parametric types to types that support a set of operators. Don't get me wrong F# is a decent language but for someone who knows and uses Haskell at an advance level they will feel frustrated with certain aspects of F# and there is very little reason for a Haskell programmer to switch over exclusively to F# unless that was the only option they can use instead of Haskell like commercially for example where the work place is a .NET house willing to use another .NET language but less willing to use Haskell. 
I see the opposite trend. People jump into debuggers where simple print statements would help them get all the values they need to examine on one screen in a convenient, custom format in a fraction of the time required to single step through the code to spot the wrong value.
&gt; Haskell, on the other hand is a comparatively simple, elegant language. Haskell has a *relatively* simple core, like most functional languages (but even then it's not obvious: how do you describe the semantics of type-classes in a really precise way? Can you use a "Core Haskell" without type classes?), but it's also a research language whose main implementation, GHC, has been extended to support a lot of new, experimental and sometimes conflicting concepts. If you allow yourself to use all of GHC Haskell, it is one of the most complicated type systems you can find (with Scala and Agda). Not saying this is a bad think, but "Foo has everything but the kitchen sink, Haskell is a comparatively simple, elegant language" is quite hard to make convincing for most values of Foo (except C++ obviously).
I'm surprised, there are [OCaml apps written for iOS](http://psellos.com/ocaml/) and I don't expect Haskell to be fundamentally less portable than OCaml (or Android a harder target than iOS), so I would suppose that it's possible to write Haskell code that runs on the mobile. Of course that will always be a minority choice, so you risk having less documentation/support etc (both among the Haskell and the mobile-apps community), but still that could be an option.
Dependency injection is OOP's version of monads, only worse. I was mystified about what sort of magic it might be, and after I finally managed to google up a straight answer, was like "That's it? That's all it is?" It's a big scary word whose utility in naming a thing is rivaled only by the hype and confusion it generates surrounding it. (For anyone else who's wondering, dependency injection is when instead of doing this: class MyClass { AbstractThingy* objectUsedByThisClass; MyClass() { objectUsedByThisClass = makeTheObjectOrGetItFromSomewhere(); you do this: class MyClass { AbstractThingy* objectUsedByThisClass; MyClass(AbstractThingy* objectWeGot) { objectUsedByThisClass = objectWeGot; which is just basic sense and never something I thought would be worth singling out for giving a big scary name to.) Anyway, now that I know what the hell dependency injection is, the analogy makes sense.
Ah, lazy exceptions ;) Some more ways to get bitten: import Data.Text as T import Control.Exception as E data PluginResult = PluginResult !Text !Int runPlugin :: IO PluginResult -&gt; IO () runPlugin p = do PluginResult t i &lt;- p `E.catch` (\(SomeException e) -&gt; return (PluginResult (pack . show $ e) 1)) putStrLn ("It returned "++show(t,i)) I tried to be so careful about strictness, yet this still contains (at least) *two* ways for exceptions to escape from the plugin: 1) While the first line of the `do` does force complete evaluation of the `PluginResult` value, the unpacking is not actually covered by the `catch`: runPlugin (return (error "err")) Fixed by using (evaluate =&lt;&lt; p) `E.catch` ... (Admittedly, this one was just sloppy thinking about the `do` desugaring, not a fundamental issue with lazy exceptions) 2) Exceptions in the exception: runPlugin (throw (ErrorCall (error "foo"))) I fixed it by a recursive handler that gives up trying to show the exceptions after a few levels... Oddly, this doesn't throw, even though `error` is supposedly defined as `throw . ErrorCall`: runPlugin (error (error "foo"))
In the sense that it's considered to be a big scary thing when it's really not, apparently (I am not /u/illissius)
better yet: "Donate 50 and we won't post a cabal package with your name"
I work as a research engineer writing mostly Haskell, Standard ML and Isabelle at NICTA, a ICT research institution in Australia. In the past I've used Haskell and Scala at startups and I've also received several offers from finance companies using Haskell and F# that I've turned down.
I'm a hacker at MusicBrainz and have proposed that the next project be done in Haskell (and it is - to great success!). I've been with MB for 4 years now, and we had the option to switch due to everyone disliking the current technology, and most of the plans for the next project being in my head - and I'm the Haskell fanboy :) The project itself is mostly just a high level interface to a database schema.
Yes. I haven't encountered any food product analogies (yet), but the phrase and the buzz around it definitely did more to confuse me than to help me. (Considering that I already knew about the "pattern", before yesterday had the impression that it must be some kind of advanced, complex technology, and only managed to find a straight explanation of it on the severalth attempt.) I can see the utility in giving it a name, but isn't there a more straightforward one? Anyway, I'm sure this has been discussed to death and I'm not adding much. &lt;/rant&gt; (FWIW, I'm not even sure that the same thing is true about Monads. But several people seem to hold the opinion.)
...and when can we expect this to hit upstream GHC? =)
It's all so good, but please, tell me, how can I write efficient matrix multiplication in Haskell? :)
Don't suppose the talk's video is online?
One of the reasons I'm doing stuff in Unity 3D is because it's built on .NET and works on Windows, Mac OS X, Linux, iOS, and Android. I'm pretty sure that there should be some Mono tools out there that will allow for developing in all these platforms. If you are developing from scratch, it should be a lot easier to do the majority of it in F#. The issue I had with developing in Unity 3D was two-fold: mutually recursive assembly references and that Unity 3D is really expecting an OO approach. While it is possible to work around both those issues, it was like taking the scenic route through the bad side of town: it's more time and trouble than it's worth. Including F# assemblies (e.g., a self-contained library), is trivial (just remember to include FSharp.Core.dll). No matter what path you take, be prepared to interface with whatever the local language flavor is.
Earlier work: http://research.microsoft.com/en-us/um/people/simonpj/papers/verify/ **Static contract checking for Haskell** *Static Contract checking for Haskell, Dana Xu, Simon Peyton Jones, and Koen Claessen. POPL'09.* Abstract. Program errors are hard to detect and are costly both to programmers who spend significant efforts in debugging, and for systems that are guarded by runtime checks. Static verification techniques have been applied to imperative and object-oriented languages, like Java and C#, but few have been applied to a higher-order lazy functional language, like Haskell. In this paper, we describe a sound and automatic static verification tool for Haskell, that is based on contracts and symbolic execution. Our approach is modular and gives precise blame assignments at compile-time in the presence of higher-order functions and laziness. **Extended static checking for Haskell** *Extended static checking for Haskell, Dana Xu, Haskell Workshop 2006.* Abstract. Program errors are hard to detect and are costly both to programmers who spend significant efforts in debugging, and to systems that are guarded by runtime checks. Extended static checking can reduce these costs by helping to detect bugs at compile-time, where possible. Extended static checking has been applied to object-oriented languages, like Java and C#, but it has not been applied to a lazy functional language, like Haskell. In this paper, we describe an extended static checking tool for Haskell, named ESC/Haskell, that is based on symbolic computation and assisted by a few novel strategies. One novelty is our use of Haskell as the specification language itself for pre/post conditions. Any Haskell function (including recursive and higher order functions) can be used in our specification which allows sophisticated properties to be expressed. To perform automatic verification, we rely on a novel technique based on symbolic computation that is augmented by counter-example guided unrolling. This technique can automate our verification process and be efficiently implemented. 
It's seems like it's still very experimental with active research. There's a github repo linked in the paper and although the actual url they provided no longer works the repo still exists: https://github.com/danr/contracts There's also currently a test results file here: https://github.com/danr/contracts/blob/master/testsuite/results/big-test.md And you can see an example contract file here: https://github.com/danr/contracts/blob/master/testsuite/Head.hs module Head where import Contracts import Prelude (Bool(..),error) head (x:xs) = x head [] = error "empty list" null [] = True null xs = False not True = False not False = True f . g = \x -&gt; f (g x) head_contract = head ::: CF :&amp;: Pred (not . null) --&gt; CF head_broken_1 = head ::: CF --&gt; CF head_broken_2 = head ::: Pred (not . null) --&gt; CF
Could you develop? I've been thinking about learning OCaml now that I spent 3 years learning and using Haskell, for cultural purposes and also for it's advocated as a nice and efficient language.
Well, that's inversion of control. The idea with dependency injection is that the component being injected is determined at runtime. You get inversion of control for free in Haskell, given that each function explicitly depends on its parameters. Of course, the problem with injecting components at runtime is that, in statically-typed languages, you lose safety benefits (things won't work if you forgot to configure a binding).
I still think it is such overkill to use the wording "cabal hell". Yes, things can be better than they are at the moment, but cabal is actually a pretty decent piece of software and without it Haskell would not have been what it is today. But, maybe I'm just lucky not to try installing packages like Yesod with a such an abundance of dependencies. $ cabal install --dry-run yesod | wc -l 102 
Hackage2 + scoutess can catch a lot of stuffs. Add to this the ability to edit the cabal file directly on hackage and you'll see a lot of improvements already.
Hmm. Will we ever get something that deals with the implicit sources of `BAD`, such as heap and stack allocation failures?
http://mschnlnine.vo.llnwd.net/d1/ch9/0/Beckman_On_Monads.wmv - a bit of source code digging goes a long way!
I don't understand what you're saying. The only sources of bad were things like failed pattern matches and calls to `error. By stack allocation failures do you mean like a stack overflow?
Correct.
&gt; 1. There is a "problem" with beginner users trying out experimental packages, like repa or Fay. ... this seems unsolvable for fundamental reasons. I would hope that attempting to solve this does not simply get relegated to the "unsolvable" pile, however. As the Haskell community moves to make the language industrial-strength, I should hope that it not lose its emphasis on research and experimentation. I'd hope the barrier to experimentation stay low.
First of all, I'm glad that this article has contributed a useful real-world example of a Cabal problem. This lets us to avoid much handwaving during discussions. I don't think I agree with the proposed solution. I guess it would solve problems with Yesod-like systems, but it doesn't address a broader issue of combining arbitrary packages. I see some technical as well as social measures we can take to remedy this to some extent. Technical: * Distinguish internal and external dependencies. E.g. if in my package I use parsec to do some parsing, it's not your business as long as I don't expose anything parsec-related from my package. It's perfectly fine for two packages have different versions of their internal dependencies and still be used together. This is not trivial to implement, and it won't solve the problem entirely, but I imagine it would make it much less severe. * I'm working on [HasFix](http://www.youtube.com/watch?v=Ae-6uIMQPmU). It should make updating packages much easier. I estimate it to be ready in 4-6 months. Social: * Identify 'maintained' packages. A responsible maintainer should always keep their packages compile with the latest versions of everything. It's fine for applications to depend on old packages, but it's not acceptable for a library that people can rely on. * Make co-maintainership a more widespread practice. This will ensure that packages remain maintained even if particular people 'drop out'.
you might be interested in the [Frege programming language](https://github.com/Frege/frege); it's sort of a Haskell on JVM and there's also *[TryFrege](https://tryfrege.appspot.com/)*
If I understood this correctly, this is broader concept of Haskell platform, having multiple 'platforms' being maintained (and used by a dev) concurrently. Which I guess makes sense.
I mean unsolvable for the beginner. Beginners just can't have stuff that is both experimental and works without a hitch.