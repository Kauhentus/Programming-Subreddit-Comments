That makes sense, the other person talking with me about this said parallelism can be faster than concurrency since the runtime has less to deal with. Has that been your experience too?
Haskell is not good for anything numeric and scaling in multiple threads. Haskell should be more open about those "known" issues. The reason is that Intel designed extremely bad CPUs, and the tiny caches don't work well with higher level languages. L1 is \~ 300x faster than main memory. Haskell -- except the very low primitives uses in the shootout benchmarks -- uses more indirection leading to worse code. On top, branch prediction seems not as good as well. Haskell doesn't scale well when using more than 3-4 threads (e.g., 16 threads, 120 threads, etc). I tried to start a google summer of code project, but did not yet get a reply to my last e-mail. I guess there is too much work and too little people that are interested or have time. Commercial haskell companies just bind to commercial C libs, etc., they are not interested in Haskell performance. 
Yes, one needs to rename each module separately, IIRC.
&gt; Haskell is not good for anything numeric and scaling in multiple threads. Haskell should be more open about those "known" issues. Agreed. Haskell is my favourite language, but it is bad at the things that computers are good for. &gt; The reason is that Intel designed extremely bad CPUs, and the tiny caches don't work well with higher level languages. L1 is ~ 300x faster than main memory. Haskell -- except the very low primitives uses in the shootout benchmarks -- uses more indirection leading to worse code. On top, branch prediction seems not as good as well. What would a more Haskell-friendly CPU look like? (A realistic design for which we can at least guesstimate the efficiency, don't just say "massively parallel graph reduction machine!")
Can this be used by GHC plugins, or do they still have to compile against the real GHC lib?
&gt;Do I understand correctly that this is essentially a strict subset of the official GHC source code? E.g., you took the GHC repository and then progressively deleted a bunch of stuff (with the appropriate tweaks to other things that you need to retain)? They have to compile against the real GHC, because they are called by the real GHC.
Thank you. Your solutions together along with type checking arr with `{-# LANGUAGE ScopedTypeVariables #-}` fixed my problem. &amp;#x200B;
Glad to hear it!
I don't have time to really give the code a thorough once-over, but there's some obvious "first steps" before doing more work: [https://github.com/astrojhgu/adaptrapezoid\_benchmark/blob/master/hs/src/Lib.hs](https://github.com/astrojhgu/adaptrapezoid_benchmark/blob/master/hs/src/Lib.hs) &amp;#x200B; First: use a definite type, don't be polymorphic over Fractional. Second, always compile for performance with \`-O2\`. Third: don't call \`fromInteger 2\` -- just use \`2.0\`. Finally, you almost certainly want to use a \`Vector\` here rather than a list. Calling \`last\` on a list is O(n) in the length of the list. But to do that properly you'll probably need to rework how the code works a bit. All the constructing and destructing of adjacent cons cells in the current implementation seems like it is creating a lot of memory churn, but it'll take some thought to produce an algo that doesn't do that. &amp;#x200B;
This commonly happens when your global namespace has been polluted by a something exposing a library with the same name. Unfortunately the error we get back from the system loader isn't very descriptive. But first things first, in cmd try running ghci with `-v3` which should give a bit more detail. In an msys2 console try `strace ghc --interactive` which should tell you which path it found for `libwinpthread-1.dll`. 
Thanks for the information. Just letting you know that you can try a proposal here: https://github.com/maiavictor/escoc. I do not see why what kind of complications one would need to ensure transports reduce properly (and I don't see why they wouldn't in such language), but I don't know enough to judge (yet), so I'll take your word on it. 
thanks you!
thank you!
Here's a start, with all the usual boring changes you'd make. https://github.com/astrojhgu/adaptrapezoid_benchmark/pull/5 It looks like these are probably the same as the changes in mistransky's PR, only I've tried to be more methodical. I may make more substantial changes or look more closely at this later.
Thanks for taking a look! when I type ghci -v3 i get the following output which throws the same error in the end as before. I dont have msys2 installed. Do I have to or is the info below enough? &amp;#x200B; &amp;#x200B;
Interesting. So you don't use Stack yourself. Care to provide actual concrete examples of how Stack supposedly "breaks" other tools?
Hmm no unfortunately, the information I'm after is hidden in the linker logs, which you'd need a debug build of GHC to access.. \&gt; I dont have msys2 installed. Do I have to or is the info below enough? You do have msys2 installed, it's bundled with platform, so should be there somewhere. \`strace\` would really show which one it loaded. As another simple test, in \`cmd\` type \`set PATH=C:\\windows\\system32\` and then try running ghci again. This will clear your PATH in that \`cmd\` session. If that works then we know it's something definitely on your path.
&gt;Yunti Thanks, great explanation, really helped me to see the power of variants/options more too. Are they powerful enough to handle a string which matches a regex and excludes the empty string (could probably handle this in the regex)? Is there a limit to what can be handled in this manner compared to what types can be constructed with dependent types? What is it that often makes dependent types hard to use in practice? 
[Time to find out!](https://github.com/haskell/haskell-ide-engine/issues/1078)
I cannot stress enough how much MGS helped me when I was still deciding whether to start working on a PhD or move to industry. The teachers are great, the atmosphere too!
Ben, you noticed "PARSER ERROR: ticket:16092:48:8: unexpected newline expecting '\]'" at the very beginning of Gitlab clone is absent on Trac, right? And then the actual text ("I want to write") starts in code formatting for some reason.
I have no idea how this package works from reading the README. Could you please add a paragraph explaining how this is possible?
&gt; Are [variants/options] powerful enough to handle a string which matches a regex If you "bake" the regex "into" the types, sure. But, to index a type on a regex you'd need dependent types. For example, to match the regex `.*\.[jm]pe?g` you can build up a type like: type LossyFile = ([AnyChar], Dot, Either Jay Em, Pea, Maybe E, Gee) type AnyChar = Char data Dot = Dot data Jay = J data Em = M data Pea = P data E = E data Gee = G pic :: LossyFile pic = ("hello", Dot, Left J, P, Nothing, G) vid = LossyFile vid = ("Birthday", Dot, Right M, P, Just E, G) instance ToString LossyFile where toString (dotStar, _, jm, _, opte, _) = dotStar ++ ('.' : either (const 'j') (const 'm') jm : 'p' : maybe "" (const "e") ++ "g") With dependent types you can have something like: MatchesRe : String -&gt; Type MatchedRe = {- Implementation left as exercise for the reader -} LossyFile : Type LossyFile = MatchesRe ".*\\.[jm]pe?g" -- pic ~= "hello.jpg" pic : LossyFile pic = {- Implementation left as an exercise for the reader. -}
Relevant? https://www.reddit.com/r/haskell/comments/2corq6/algebraic_terraforming_trees_from_magma/
Incidentally, [this talk](https://www.youtube.com/watch?v=aJvwORrBJ0o) by Harendra Kumar about high-performance Haskell is good.
Yeah, I read the readme a couple times and I still don't have much of a clue what's going on (no runtime...? wha?). Oh well, I'm happy that this will alleviate much pain for GHC tooling authors. I assume they understand perfectly why/how one would use this library.
Sounds like a pure function :)
I probably didn't explain my idea well. I'm thinking of doing something kinda similar: A program that would look through your code and auto-generate the imports only for the functions you are using, and remove the ones you are not. 
How does it compare to Persistent + Esqueleto?
There was a PhD in Twente working on designing a CPU for lazy functional languages, but I dunno whatever came of that. I should ask around...
Have you looked at very/insanely dependent types? It seems to be a similar thing. Ulf Norell has a small implementation at https://github.com/UlfNorell/insane
Thanks very much I'll look at it closely
That seems great
Thanks a lot!
I wish I lived in the UK, or that I could take these courses remotely.
Maybe I am doing something wrong or I am not aware of how to do it, but when i use strace it tells me that it is not a command. Similar when I search my computer msys2 nothing comes up? &amp;#x200B; &amp;#x200B; &amp;#x200B; &amp;#x200B;
A little disclaimer first: I am not a game developer, and I don't know typical solutions for many domain problems here, either in OOP or FP. For example, even when using a MessageBus, how can we guarantee that all the "subsystems" act at the same time and graphics and sound are in sync, etc. So I'll do my handwaving and fallback to my "enterprisy" experience with message buses etc. :) First, it looks like you could use channels (TChan, Chan) for communication between your subsystems. I don't think it'd be an idiomatic solution, but it would probably be close to a typical OOP implementation with asynchronous actors/observers. But it also looks like your "subsystems" represent effects of your application/game. For example, if my primitive game is described by this primitive ADT: data GameEvent = Jumped | FiredAt Point | TookAHit Damage | ... then my effects would be something like: soundEffect :: GameEvent -&gt; IO () graphicsEffect :: GameEvent -&gt; IO () .... and then I can compose them into a bigger effect: gameEffects :: GameEvent -&gt; IO () gameEffects event = forM_ [soundEffect, graphicsEffect] (\f -&gt; f event) or I could use `forConcurrently_`, whatever. Then I can call this "composed" effect wherever my business logic decides that something has happened and issues an event. The point here is not to write it like that (again, I don't know what is a good way to write a game), but to show that a lot can be done by simple functions composition. There are other approaches, too. Just to mention some: - `Free` monads + interpreters (where your interpreter would need to know which effects to schedule and in which order). - A number of effect systems that claim to have better performance than `Free` and each other. - A few FRP solutions (`Yampa`, etc.) For gamedev in Haskell specifically, there are resources that you may find interesting, such as: 1) A simple game in Haskell: http://jxv.io/blog/2018-02-28-A-Game-in-Haskell.html 2) Manuel's talk on implementing physics and graphics for a game: https://www.youtube.com/watch?v=kd8mlbN0Mws 3) Building Arcanoid in Haskell (very well documented resource): https://github.com/ivanperez-keera/haskanoid 4) FRP for gaming intro: https://www.youtube.com/watch?v=_CpcKcbnORY 
No runtime means no TH.
Looks fun. How long before it can replace Coq?
Full disclosure, I’ve never used Persistent + Esqueleto, but I did review them (some time ago) before deciding on beam for a project. Persistent is TemplateHaskell based, beam is Generics based (some people have preferences). When reviewing issues, it seemed like feature requests for esqueleto occasionally were answered with “that would require a big refactor so is not practical for now, use raw sql” where as this was essentially never the case for beam (every request I’ve made was implemented in a less than a couple of weeks by a maintainer, or by myself, a new user). Persistent is for modeling tables and simple CRUD, you really need to bring in a second library (esqueleto) for any real project, beam does both in one library. I (somewhat arbitrarily) get the feeling that although persistent and esqueleto are probably more widely used, they’re less likely to evolve over time. I’ve seen comments from their maintainers about the structure of the code being difficult to work on where as the structure of beam was quite simple for a new user (me) to be productive with. Anyway, these are some foggy memories I have about the two. I remember and have been very happy with my decision to go with Beam. I’m certain a better review of the two options (and others!) exists somewhere. Beam is almost certainly a smaller community from what I hear stalking Haskell communities online, but that doesn’t have to be the case forever!
Bit late of a response but thank you! I still don't fully understand all this (this gets to be pretty "advanced" type stuff to me) but at least now I know what to look into more for this.
Very happy to help. /u/bss03 already responded to the question about regexes, so I'll cover the other questions. &gt; Is there a limit to what can be handled in this manner compared to what types can be constructed with dependent types? Absolutely. Dependent types can, as mentioned, encode (almost) any type. Haskell's Algebraic Data Types can take you quite far, but at some point you'll get stuck and start relying on invariants which aren't reflected in the type system (like "this `Int` can never be negative"). Where exactly that limit is, in general, I don't know. &gt; What is it that often makes dependent types hard to use in practice? Oh boy -- I have quite the laundry list of complaints, but many of them relate to technicalities which are hard to explain without going into great detail. Maybe I'll write up a little rant the next time I'm frustrated with the termination checker or whatever. Anyway, the overall difficulty is probably just that proving things formally is hard. Even informal correctness proofs are hard because you have to take into account every possible behaviour of your program. Formal proofs are harder because you have to explain everything to the system in excruciating detail. A human won't even notice that you replaced `x + y` with `y + x`, but a computer will ask you for evidence that this is correct. There are ways to deal with this sort of trivial nonsense, but they come with their own tradeoffs.
Yes, this is the import script's behavior when it cannot parse a ticket's Trac markup. It produces the parser error message you noted and then pastes the Trac content verbatim.
Thanks /u/manpacket! I've opened tickets for all of these.
Coq is much, much more than just a language for proofs. It's an enormous body of work going on since 30 years ago. "Replace" might be a little too ambitious here, I guess, even if a lot of people will be interested.
There are a couple of KMeans implementations on hackage and I’ve got one (not on hackage) if it’s helpful. I rolled my own to add weighting and make a nice interface to the Frames library. https://github.com/adamConnerSax/Frames-utils/blob/master/src/Frames/KMeans.hs The actual KMeans implementation is at the bottom. The rest is for constructing the initial centers and interface to Frames.
Yeah, but this way there *will* be an extension node in the AST. And if it is strict, like it should, it will trap right away. The point of having Void is running guarantee it's non-existence.
;)
ghcup will likely never install system dependencies, because I consider it a disruptive anti pattern that doesn't align with unix philosophy. But YMMV. The idea is rather to have consistent documentation *upstream* about what is required for the bindist tarballs (similar to what we have for compiling GHC from source already on the trac wiki). See https://github.com/haskell/ghcup/issues/49 Since GHC seems to be in the process of switching to GitLab (?) it might make it easier once the switch is done.
We're currently using upstream bindists, though IIRC we've rolled our own in the past, at least for some distributions. If you're interested in collaborating on anything in this space, definitely feel free to reach out. I'm guessing a large amount of the logic and work here overlaps.
&gt;The reason is that Intel designed extremely bad CPUs, and the tiny caches don't work well with higher level languages. &amp;#x200B; I'd say that's unfair and untrue. If Intel has been designing "extremely bad CPU's", then so has everyone else. ARM, AMD, and many others have been competing with Intel for years to design better processors. If Intel was designing extremely bad CPUs (at least on desktop and laptop), then they would be out of business right now. Or all manufacturers are designing extremely bad CPUs (and that doesn't make a lot of sense, because you're essentially saying nobody can design a good CPU). &amp;#x200B; &gt;tiny caches Modern CPUs, including Intel CPUs have large caches. Large CPU caches are expensive, area-wise. A large fraction of chip area is already dedicated to caches. [Look at this picture](https://www.extremetech.com/extreme/188776-how-l1-and-l2-cpu-caches-work-and-why-theyre-an-essential-part-of-modern-chips). The regular-looking structure on the left is all memory. Memory takes up at least 60% of this die. If any more area were used for caches, then there would be no memory left for ALUs, data paths, control, I/O, etc. So you'd have a lot of memory and no way to use it. That would be a bad CPU. &amp;#x200B;
https://www.stackage.org/haddock/lts-13.6/ghc-8.6.3/HscTypes.html#v:lookupDataCon ?
But what I get from `lookupThing` is still an `Id`. `lookupDataCon` panics because the name is associated with an `Id` instead of a `DataCon`.
You can only use `strace` from within an `msys2` shell. Platform puts it somewhere but I don't know where. I don't personally use platform. ``` C:\\Users\\marti&gt;set PATH=C:\\windows\\system32 C:\\Users\\marti&gt;ghci 'ghci' is not recognized as an internal or external command, operable program or batch file ``` after you clear the path you must refer to `ghci` by it's full path. You can find it by using `where ghci` before you clear `PATH`. This is a bit of an annoying problem to debug as the way you would normally debug it is using `loader snaps` which require additional Microsoft tools to be installed.
 Don't forget that in whatever approach you take, you inevitably have to convert between your schema data-types and your application's data model. That conversion is essentially the same sort of operation and comparably the same amount of code, only in DSL, which is specifically fit for that purpose (e.g., things like abstracting over very precise parsing error-reporting using Alternative). I didn't cover the rendering to JSON part at first, because I wanted to review the problems of ToJSON in more detail. However I do also have an alternative solution to rendering: the ["json-bytes-builder"](http://hackage.haskell.org/package/json-bytes-builder) library. In the following code I'm gonna present you a solution to both parsing and rendering of the problem you've described. module Model where data Error = LifeIsMeaninglessError | LifeHasATerribleMeaningError Meaning | UserPasswordIsNotAwesome Password module Parsers where error :: Value Error error = object $ do type_ &lt;- field "type" (string text) case type_ of -- Notice how we parse a constructor from a completely different representation, -- which is underivable thru Generic: "life-is-meaningless" -&gt; return LifeIsMeaninglessError -- It's possible to support multiple representations: "LifeIsMeaningless" -&gt; return LifeIsMeaninglessError "life-has-a-terrible-meaning" -&gt; LifeHasATerribleMeaningError &lt;$&gt; field "meaning" meaning "user-password-is-not-awesome" -&gt; return (UserPasswordIsNotAwesome "") _ -&gt; fail ("Unexpected type: " &lt;&gt; show type_) meaning :: Value Meaning meaning = undefined module Rendering where import JSONBytesBuilder.Builder -- from "json-bytes-builder" error :: Error -&gt; Literal error = \ case LifeIsMeaninglessError -&gt; object $ row "type" (stringFromText "life-is-meaningless") LifeHasATerribleMeaningError meaningValue -&gt; object $ row "type" (stringFromText "life-has-a-terrible-meaning") &lt;&gt; row "meaning" (meaning meaningValue) UserPasswordIsNotAwesome _ -&gt; object $ row "type" (stringFromText "user-password-is-not-awesome") meaning :: Meaning -&gt; Literal meaning = undefined Please notice how these definitions are really not that much heavier than any conversion code, also notice the flexibility that they provide and support for different formats. Also keep in mind that parsing is gonna come with really detailed failure reports about schema mismatches - the problem that you have to manually handle in your conversion functions and haven't even touched in your example. To amplify the taste of flexibility of this solution, here's how you'd have to change the code above to support parsing of your `Error` type from both the String and Object values: module Parsers where error :: Value Error error = errorString &lt;|&gt; errorObject errorString :: Value Error errorString = string $ textMatcher $ \ case "life-is-meaningless" -&gt; Right LifeIsMeaninglessError "life-has-a-terrible-meaning" -&gt; Left ("The \"life-has-a-terrible-meaning\" value requires details on the meaning") "user-password-is-not-awesome" -&gt; Right (UserPasswordIsNotAwesome "") error -&gt; Left ("Unexpected error: " &lt;&gt; show error) {-| Previously just "error" -} errorObject :: Value Error errorObject = undefined Now this is something that you simply cannot achieve using FromJSON, and also what the real-world third-party JSON documents can easily be. 
I'm working on the `smuggler` GHC source plugin which aims to manage import sections automatically. There're plans to add feature you want by reusing existing GHC functionality: * https://github.com/kowainik/smuggler
I find that #ghc on freenode is a better place for questions like these!
Thanks! I didn't know that was a thing.
I didn't cover the rendering to JSON part at first, because it very rarely is the case that you need to both parse and render a value to JSON. Think about it: the request you send to a server is something that you only need to render and never parse, the response you get from it is vice versa, the file you read is something that you only need to parse and etc. It's true that having both rendering and parsing generated from the same type gives you certain guarantees about both conforming to the same format. But such requirement on its own is quite synthetic in most cases. OTOH, the price of flexibility that you have to pay for it is usually quite real. So yes, the choice does require a compromise. A problem is that people just don't know that there is one and blindly go for the autogenerated solution, simply because it has more marketing. Then they bump into the issues of the real world and end up stacking hacks to make the thing at least somewhat work. What they should do is first consider their requirements, and then make an informed decision about the technology they use. Don't forget that in whatever approach you take, you inevitably have to convert between your schema data-types and your application's data model. That conversion is essentially the same sort of operation and comparably the same amount of code, only in DSL, which is specifically fit for that purpose (e.g., things like abstracting over very precise parsing error-reporting using Alternative). I also didn't cover the rendering part because I wanted to review the problems of FromJSON in more detail. However I do also have an alternative solution to rendering: the ["json-bytes-builder"](http://hackage.haskell.org/package/json-bytes-builder) library. In the following code I'm gonna present you a solution to both parsing and rendering of the problem you've described. module Model where data Error = LifeIsMeaninglessError | LifeHasATerribleMeaningError Meaning | UserPasswordIsNotAwesome Password module Parsers where error :: Value Error error = object $ do type_ &lt;- field "type" (string text) case type_ of -- Notice how we parse a constructor from a completely different representation, -- which is underivable thru Generic: "life-is-meaningless" -&gt; return LifeIsMeaninglessError -- It's possible to support multiple representations: "LifeIsMeaningless" -&gt; return LifeIsMeaninglessError "life-has-a-terrible-meaning" -&gt; LifeHasATerribleMeaningError &lt;$&gt; field "meaning" meaning "user-password-is-not-awesome" -&gt; return (UserPasswordIsNotAwesome "") _ -&gt; fail ("Unexpected type: " &lt;&gt; show type_) meaning :: Value Meaning meaning = undefined module Rendering where import JSONBytesBuilder.Builder -- from "json-bytes-builder" error :: Error -&gt; Literal error = \ case LifeIsMeaninglessError -&gt; object $ row "type" (stringFromText "life-is-meaningless") LifeHasATerribleMeaningError meaningValue -&gt; object $ row "type" (stringFromText "life-has-a-terrible-meaning") &lt;&gt; row "meaning" (meaning meaningValue) UserPasswordIsNotAwesome _ -&gt; object $ row "type" (stringFromText "user-password-is-not-awesome") meaning :: Meaning -&gt; Literal meaning = undefined Please notice how these definitions are really not that much heavier than any conversion code, also notice the flexibility that they provide and support for different formats. Also keep in mind that parsing is gonna come with really detailed failure reports about schema mismatches - the problem that you have to manually handle in your conversion functions and haven't even touched in your example. To amplify the taste of flexibility of this solution, here's how you'd have to change the code above to support parsing of your `Error` type from both the String and Object values: module Parsers where error :: Value Error error = errorString &lt;|&gt; errorObject errorString :: Value Error errorString = string $ textMatcher $ \ case "life-is-meaningless" -&gt; Right LifeIsMeaninglessError "life-has-a-terrible-meaning" -&gt; Left ("The \"life-has-a-terrible-meaning\" value requires details on the meaning") "user-password-is-not-awesome" -&gt; Right (UserPasswordIsNotAwesome "") error -&gt; Left ("Unexpected error: " &lt;&gt; show error) {-| Previously just "error" -} errorObject :: Value Error errorObject = undefined Now this is something that you simply cannot achieve using Generic, and also what the real-world third-party JSON documents can easily be. 
Figured it out. See the notes in (compiler/basicTypes/DataCon.hs)[https://github.com/ghc/ghc/blob/master/compiler/basicTypes/DataCon.hs]. Thank you #ghc!! &lt;3&lt;3
If you could implement `step = hoist _` without using `error` (or any bottom), it is guaranteed that there won't be `ExtF` in the returned AST. step :: Fix (ExprF 1) -&gt; Fix (ExprF 2) step = hoist $ \case LitF i -&gt; LitF i OpF a b -&gt; OpF a b ExtF t -&gt; LitF (Text.length t) To gain the benefit from Trees That Grow, you can't use partial functions. The guarantee `Void` gives does not work under the existence of bottom. If you want an AST transformation which can fail, it is easier to do it explicitly with `Maybe` (or `Either` or `Validation` or ...) step :: Fix (ExprF 1) -&gt; Maybe (Fix (ExprF 2)) step = cata $ \case LitF i -&gt; pure $ Fix (LitF i) OpF a b -&gt; fmap Fix $ OpF &lt;$&gt; a &lt;*&gt; b ExtF _ -&gt; Nothing 
Are there any tools that the Haskell community would like to see built? Maybe something you guys see in other languages that would be nice to have.
There's one detail I missed in the example. I have `Inner Foo [f]` constructor. And it's always a root of whole expressions. Perhaps I can use Extension nodes as a lists too. One alternative, though, is composing extended expression functor with `(,)` and replacing that with `Const` or something like that.
I'm not sure what is the goal. Do you want all expressions containing `ExtF` be removed from `subexprs` in `Inner foo subexprs`?
Sorry, I feel like I totally missed the meaning of the original question. What should `step (Fix (ExtF "foo"))` be?
Great! So I still need to rewrite the GHC AST into one compatible with \`haskell-src-exts\`, but this seems like work worth doing either way. Unless you are planning on having \`hlint\` just use \`ghc-lib\`'s AST?
Yes, that's it.
Ok the response to strace was: $ strace ghc --interactive create\_child: ghc --interactive strace.exe: error creating process ghc, (error 2) And when after resetting the path and running ghci from its folder i get the same error as before: &amp;#x200B; C:\\Program Files\\Haskell Platform\\8.6.3\\bin&gt;ghci.exe GHCi, version 8.6.3: [http://www.haskell.org/ghc/](http://www.haskell.org/ghc/) :? for help &lt;command line&gt;: user specified .o/.so/.DLL could not be loaded (addDLL: pthread or dependencies not loaded. (Win32 error 5)) Whilst trying to load: (dynamic) pthread Additional directories searched: C://Program Files//Haskell Platform//8.6.3//mingw//bin/ C://Program Files//Haskell Platform//8.6.3//mingw//bin/../lib/ C://Program Files//Haskell Platform//8.6.3//mingw//bin/../lib/gcc/x86\_64-w64-mingw32/7.2.0/ C:/Program Files/Haskell Platform/8.6.3/mingw/bin/../lib/gcc/x86\_64-w64-mingw32/7.2.0/ C:/Program Files/Haskell Platform/8.6.3/mingw/bin/../lib/gcc/ C:/Program Files/Haskell Platform/8.6.3/mingw/bin/../lib/gcc/x86\_64-w64-mingw32/7.2.0/../../../../x86\_64-w64-mingw32/lib/../lib/ C:/Program Files/Haskell Platform/8.6.3/mingw/bin/../lib/gcc/x86\_64-w64-mingw32/7.2.0/../../../../lib/ C:/Program Files/Haskell Platform/8.6.3/mingw/bin/../lib/gcc/x86\_64-w64-mingw32/7.2.0/../../../../x86\_64-w64-mingw32/lib/ C:/Program Files/Haskell Platform/8.6.3/mingw/bin/../lib/gcc/x86\_64-w64-mingw32/7.2.0/../../../ C:\\windows\\system32
&gt;replace Coq? You mean a Succ(Coq)?
Check out the [Haskell performance checklist](https://github.com/haskell-perf/checklist).
Then expressing "`Inner Foo [f]` constructor is always the root" constraint of your AST in types would help. Like this: data Expr pass = MkExpr Foo [Fix (ExprF pass)] data ExprF pass f = ... step :: Expr 1 -&gt; Expr 2 step (MkExpr foo subexprs) = MkExpr foo (mapMaybe stepSub subexprs) stepSub :: Fix (ExprF 1) -&gt; Maybe (Fix (ExprF 2)) stepSub = cata f where f :: ExprF 1 (Maybe (Fix (ExprF 2))) -&gt; Maybe (Fix (ExprF 2)) f = ... Otherwise, you have to return `Maybe (Fix (ExprF 2))` or something as a result, not just `Fix (ExprF 2)`. Think of the question /u/Syrak gave you.
The parallelism in this context is GHC's spark parallelism. Here we have a spark thread per processor. If you use rpar on a closure we put a pointer to this closure on the WorkStealingDeque. If another processor isn't busy its spark thread will take closures and evaluate them. So to potentially run work in parallel we have to push a single pointer onto a deque. This is waaay cheaper than creating a new haskell thread for each element of a list. But that doesn't mean that it is the only form of cheap parallelism. Some libraries offers data-parallelism, for instance, which lets you write transformations over arrays and run them in parallel extremely efficiently. Data parallelism vs task parallelism is a topic by itself - the standard example for data parallelism is the gpu which can execute the same instruction thousands of time at once on different data. The accelerate library lets you run an embedded haskell dsl on the gpu, for instance.
The reddit markup for code blocks is four spaces at the beginning of each line. For `inline code` it should be single back-ticks.
Ok, looks like you can do \`stack upload --pvp-bounds both\`.
I went here last year - it was great. Highly recommended for people in the UK who wants to meet some great people and learn a lot at the same time!
That's just an example. You probably wouldn't want to write this in actual code. However, you can still have both: write a polymorphic function (perhaps with a more sensible name) and write concrete function in terms of it, then use the latter.
Urg... Hmm no strace failed to run. Also I just noticed you're trying 8.6.3, can you try an earlier version? 8.6.3 is broken on Windows. If you have the same problem with an older version I'll have to make a debug version of ghc for you to get the required output to find out what it's loading. This is unfortunate quite hard to debug since this case is one we've deferred to the system loader, and so debugging it requires more involved debugging tools. Do you happen to use irc? That would be faster than here.
These are really two different functions. It's not a trade off, just a different use case. You can use the type signature of the typeclass constrained function to tell what kinds of arguments to pass (anything that has an instance of Eq). If you have defined an instance of Eq for Coord, you could still use the first form of the function if you want to force the user to use Coord, or you could define a new function as follows moveCoordFromTo :: Coord -&gt; Coord -&gt; Coord -&gt; Coord moveCoordFromTo = moveFromTo Though it seems a little roundabout just to change the type signature. If you are reusing this function, the typeclass version is perfectly descriptive
The more generic one gives you *more* information in a sense though. The type of the first function says nothing about what it is doing internally. It could be spitting out a hard coded `Coord` for all I know. The second version however guarantees (modulo bottom) that the output is one of the inputs. Because the function can't know what `a` is, and therfore can't create values of that type on its own. Moreover the type also tells me that the only information that the function has about `a` is that it can be compared for equality. That says kind of a lot about what it could possibly be doing. This can be taken too far, of course. I have sometimes encountered functions that are so general that it's hard to make much sense of their types. Certainly felt that more than a few times the first time I did a tour of the `lens` package. But this is a fairly benign example imo. 
My personal position is that if making the signature more generic makes it harder to understand what your function does, than it either shouldn't be more genneric or you have the wrong abstraction. (But there are exceptions.) Does it really make sense to apply your move function to anything that implements Eq? I imagine it doesn't. If you are trying to support many different types of coordinates, you may want to create a new class.
PS: OP has asked for without use of special fonts and my method requires installing a special font. But I am just going to post here if anyone comes here in future and wants to use Fira Code in Emacs. &amp;#x200B; Here's how it looks for haskell: [https://www.dropbox.com/s/gbmrcmznmj6821w/haskell-ligatures.png?dl=0](https://www.dropbox.com/s/gbmrcmznmj6821w/haskell-ligatures.png?dl=0) My config is here: [https://github.com/tejasbubane/dotemacs/blob/87a2a2d04f75258cbdd1180c2eba2035af1e7fe0/utils/fira-code-mode.el](https://github.com/tejasbubane/dotemacs/blob/87a2a2d04f75258cbdd1180c2eba2035af1e7fe0/utils/fira-code-mode.el) Detailed instructions here: [https://github.com/tonsky/FiraCode/wiki/Emacs-instructions#using-prettify-symbols](https://github.com/tonsky/FiraCode/wiki/Emacs-instructions#using-prettify-symbols) &amp;#x200B; PS: It does require you to install separate [Fira Code Symbol Font](https://github.com/tonsky/FiraCode/issues/211#issuecomment-239058632).
You can do something like moveCordFromTo :: Coord -&gt; Coord -&gt; Coord -&gt; Coord moveCordFromTo = moveFromTo
Note that this is actually expected; we intentionally do not import "commit comments" as they are captured by GitLab's "commit mentions" notes.
Worth noting that if you want to learn more about this, Wadler's [Theorems for Free](https://people.mpi-sws.org/~dreyer/tor/papers/wadler.pdf) is a good start and "parametricity" would be a good keyword to google for.
ok I downgraded to 8.4.3 but I am getting the same error trying to run ghci :-( Thank you for spending time on this! &amp;#x200B; &amp;#x200B; C:\\Users\\marti&gt;ghci GHCi, version 8.4.3: [http://www.haskell.org/ghc/](http://www.haskell.org/ghc/) :? for help &lt;command line&gt;: user specified .o/.so/.DLL could not be loaded (addDLL: pthread or dependencies not loaded. (Win32 error 5)) Whilst trying to load: (dynamic) pthread Additional directories searched: C://Program Files//Haskell Platform//8.4.3//mingw//bin/ C://Program Files//Haskell Platform//8.4.3//mingw//bin/../lib/ C://Program Files//Haskell Platform//8.4.3//mingw//bin/../lib/gcc/x86\_64-w64-mingw32/7.2.0/ C:/Program Files/Haskell Platform/8.4.3/mingw/bin/../lib/gcc/x86\_64-w64-mingw32/7.2.0/ C:/Program Files/Haskell Platform/8.4.3/mingw/bin/../lib/gcc/ C:/Program Files/Haskell Platform/8.4.3/mingw/bin/../lib/gcc/x86\_64-w64-mingw32/7.2.0/../../../../x86\_64-w64-mingw32/lib/../lib/ C:/Program Files/Haskell Platform/8.4.3/mingw/bin/../lib/gcc/x86\_64-w64-mingw32/7.2.0/../../../../lib/ C:/Program Files/Haskell Platform/8.4.3/mingw/bin/../lib/gcc/x86\_64-w64-mingw32/7.2.0/../../../../x86\_64-w64-mingw32/lib/ C:/Program Files/Haskell Platform/8.4.3/mingw/bin/../lib/gcc/x86\_64-w64-mingw32/7.2.0/../../../ C:\\windows\\system32
7!
It means HLint can use ghc-lib, or ghc, interchangeably, perhaps - I still need to figure out what's the best way to fit these together. It might mean HLint can just use ghc and extensions for the latest version of GHC, and degrade to still work on others but not as a plugin? Unsure yet.
This looks great, but it still requires recursive references as a special case at the type level of recursivr definitions, right? The way you refer to Nat.succ in the definition of Nat seems to require this. I wonder if we could somehow extend this to have anonymous lambdas with fully inductive dependent types
Debian has a tool for extracting the C ABI of a library and ensuring that it only changes compatibly. Would be nice to have some automatic ABI / API extraction for the purpose of enforcing SemVer / PVP on services like hackage / stackage. Also, could possibly be tied into backpack so that you can depend on a particular (extracted) API (or ABI?) instead of a package name. While it's less of an issue for stack, cabal-install could use a better dependency resolver: https://medium.com/@nex3/pubgrub-2fb6470504f The current one is pretty good, even the error messages, but I'm fairly sure this one would do better. More work on haskell-ide-engine is, I imagine always appreciated. Even if the LSP current implementation of all the standard LSP messages is perfect, there's a number of LSP extensions floating around other there, and while I'm not well-versed in the LSP yet, I imagine there's some LSP extensions just waiting to be written around case splitting, typed holes, and partial type signatures, or doing a "greedy splice" of some macro (template haskell). Just some ideas off the top of my head. If there's a particular package or program (even GHC) that you use, might be worth dropping by their issue tracker to see if they can use some help.
Remember you can name your type variables anything you want. moveFromTo :: Eq coord =&gt; coord -&gt; coord -&gt; coord -&gt; coord 
We *just* need a transpiler from Coq to ESCoC. ;)
``` works on new reddit but not old reddit ``` works on both old reddit and new reddit Works on `new reddit` and on ``old reddit``.
But this would be confusing to read if the generic function has nothing to do with Coords…
I'm having trouble with this question, any help is appreciated 
I have several packages on Hackage all of which I manage locally using Stack. Here are some recent examples: * http://hackage.haskell.org/package/sexpr-parser (https://github.com/rcook/sexpr-parser) * http://hackage.haskell.org/package/oset (https://github.com/rcook/oset) If you check out the GitHub projects, you'll see that I don't do anything fancy with constraints or bounds or anything. I upload these packages by running `stack upload .` in the root of the project's Git repo and it just works. 
Perhaps it would have been better to link to [this](https://www.well-typed.com/blog/2015/05/parametricity/) post on Well-Typed about parametricity. What's tripping me up about `new` is that `new`'s type looks open because `val` doesn't occur in a function type's parameter. From their post: &gt;∀ab. a -&gt; b -&gt; a is an example of a closed type: all type variables are bound by a universal quantifier. An open type is a type with free type variables such as ∀b. a -&gt; b -&gt; a I couldn't find any types in their post that have a type variable in a function type's result that isn't also in a function type's parameter, e.g. a -&gt; b. In fact, they even say explicitly: &gt;Other than undefined (which we are ignoring), there can be no function f :: ∀ab. a -&gt; b I don't see how `new` with `val` is compatible with that. Perhaps the [second part](https://www.well-typed.com/blog/2015/08/parametricity-part2/) of the Well-Typed post that deals with type constructors explains it, since other people have pointed out how you can get `a -&gt; [b]` from `f x = []`, but it went over my head. It might help to have it put in more concrete terms. (I only really know Haskell 98, just in case that affects the scope of the discussion/answer.) Basically, my understanding is that Haskell types with a type variable have a forall type wrapped around the outside: `(key -&gt; key -&gt; Bool) -&gt; (key -&gt; Int32) -&gt; IO (HashTable key val)` is really `forall key val. (key -&gt; key -&gt; Bool) -&gt; (key -&gt; Int32) -&gt; IO (HashTable key val)`. But maybe I'm mistaken, and it's really `forall key. (key -&gt; key -&gt; Bool) -&gt; (key -&gt; Int32) -&gt; forall val. IO (HashTable key val)`, which case I can see how `val` is still implicitly bound. Is that how it works? And is that how `{ f x = []; f :: a -&gt; [b] }` also works?
In hindsight the pvp-bounds feature has been a mistake and its use is largely discouraged. Both maintainers and Stackage curators keep wasting cycles with the back and forth with each other over out-of-bounds issues for no good reason. Bounds are the main culprit for why new package versions are prevented from entering Stackage snapshots for many weeks. Bounds are why we can't have fast progressing Stackage snapshots. Do we really want to waste our time babysitting version bounds? With the advent of Stackage version bounds are now considered the original sin of Hackage. They really no longer serve any purpose thanks to Stack's resolvers which have proven to be a superior more effective solution. Finally Stackage has [stopped showing redundant version bounds](https://www.stackage.org/lts-13.6/package/stack-1.9.3/deps) so there's even less reason for setting bounds.
&gt; I don't do anything fancy with constraints or bounds or anything. Thank you! I just wish everybody did this so Stackage wouldn't be held up for weeks until the last maintainer has finally bumped their pessimistic bounds.
OK, so if you "fully" apply `new` (`new` and all of its nested functions), you get the type `forall val. IO (HashTable key val)`? Is the type of `new` then actually `forall key. (key -&gt; key -&gt; Bool) -&gt; (key -&gt; Int32) -&gt; forall val. IO (HashTable key val)`? If so, then similar to how the type argument for `[]` is determined at the place of use like so: ```haskell xs = [] ys = 1 : xs zs = 'a' : xs ``` then the type argument for `val` in `HashTable` is also determined where HashTable is used: ```haskell ht = new f g intIntHT = do insert ht 1 2 charCharHT = do insert ht 'a' 'b' ``` Is that right?
I typically keep the `base &gt;= 4.7 &amp;&amp; &lt; 5` bound which is included in the default Stack templates that I use for my projects but never put bounds anywhere else. This has never been an issue for me. Is this considered a reasonable practice? 
I tend to develop packages on LTS (with CI testing for nightlies). Those are my upper bounds + some space I judge appropriate according to my API stability observations. From there I try to push LTS resolver version down to reasonable compatibility strain (3 last GHC majors + sometimes more). Then collect lower version from there and sometimes relax minors a bit.
Are overly strict version bounds really the reason for the delays? That seems exaggerated to me. When bounds are discovered to be conservative, it takes two minutes to bump them on Hackage. And when it turns out there was a good reason for the bounds, they preemptively saved you from even trying to build the package and finding out after a build failure, or even worse, when the package compiles but the runtime behavior is buggy (that there is no way for stackage to predict). This kind of issue seems a much more plausible reason for delays in Stackage adopting some newer packages, with no easy workaround. Stackage provides a closed world in which everything Just Works™ (with the aforementioned caveat that it does not automatically detect breaking semantic changes), but you lose all guarantees as soon as you go outside (which happens often in practice). Upper bounds are the only scalable way to keep packages coherent in an open world. - You release packages without version bounds. A new release of GHC breaks all N versions of your packages. You must now fix all N versions to prevent solvers from picking them up. - You release packages with conservative version bounds. A new release of GHC is compatible with your packages. You can just bump the most recent version (or maybe a couple more) in two minutes: this works fine for most people, and older versions can be bumped on demand (which can be unlikely since most people who are upgrading GHC versions will also go through the work of upgrading their dependencies).
 &gt; You must now fix all N versions to prevent solvers from picking them up. No you don't. Older Stackage resolvers don't need to be fixed up. Remember that Stackage resolvers pin down specific package versions. No imprecise bounds involved. &gt; When bounds are discovered to be conservative, it takes two minutes to bump them on Hackage. 2 minutes here and there can quickly add up. But more importantly it's unnecessary. Stackage doesn't need version bounds so why even spend those 2 minutes to begin with? 
I find these sort of type-restricted aliases can be good for readability, and can also improve type errors during refactoring. (When two sufficiently parameteric functions meet, type errors sometimes travel across the boundary; when one side is a type-restricted alias, type errors tend to stop a the boundary.)
This is true of almost any type of polymorphism, ad-hoc, parametric, even subtype. Part of the engineering art is finding the right balance for the current project.
This logic only works for things that are not libraries on Hackage that others depend on. Such libraries need to be able to adjust properly to older / newer resolvers or build plans, not just the one the library author was using at the time of publishing.
A 2D game library like Gloss (http://hackage.haskell.org/package/gloss) that's cross platform. I like Gloss's Model / View / Update architecture, but it uses OpenGL which is deprecated on macOS. I think Haskell is a perfect language for turn based computer games, but the library support isn't there yet. If anyone starts on this let me know.
Ah, ok. But now, the new big question is: who the fuck uses the new reddit (intentionally)??? (and a smaller one: why the fuck cannot they backport this markup?) 
RES is growing support for new reddit, and then I'll switch over, I think. I prefer the old format (and it has more features), but they (reddit.com developers) aren't going to maintain it, and I'm sure the incompatibilities will slowly accumulate until it's unusable.
It’s not just for UK folks - everyone is welcome, and we have participants from all over the world. 
It’s not just for UK folks - everyone is welcome, and we have participants from all over the world. 
I'm something to be maintained, I agree. The first sounds convenient for throw-away scripts and playing around, though.
Only taking twice as long for doing 6x as much work seems pretty good to me. 
If you aren’t already: compile your program with with `-O2`. Programs using the vector library are really prone to having very different performance at different optimisation levels.
In other words, you're comparing an approach using destructive array updates to an approach using persistent array updates. It's a short array, but your program probably allocates gigabytes of new register vectors just to throw them away again immediately. The max resident space is no big deal, but it's not great to copy all those values over and over again. If you want a fair comparison to the js code, use a mutable vector. 
And your package is a disaster for non-stack users: https://matrix.hackage.haskell.org/#/package/sexpr-parser
This only works if you can stay within Stackage. There are large projects that can't upgrade to a newer snapshot all at once. But they may still want to be able to get updates incrementally (at least security-related updates), and version bounds are definitely helpful in those cases. &gt; 2 minutes here and there can quickly add up. They are dwarfed by the time spent developing the package, and the time spared for people who cannot rely on a snapshot-based packaging model for valid reasons such as the one above.
&gt; toInt is technically partial due to overflow. Overflow makes it total, just not injective (and possibly not what you want). 
Why will it be a disaster for Stackage? And how would bounds help Stackage? &gt;!The answer to both is "no, it won't"!&lt;
``` main = runCheeseT $ do ... ```
You can easily update individual packages from your current resolver with `extra-deps`. Packages with bounds typically only get in the way when using `extra-deps` incrementally.
ok, try the following, run ``` ghc --print-libdir ``` This will spit out a path, like mine gives `E:\ghc-dev\msys64\mingw64\lib`. Create a file `test.hs` with this content ``` import GHC import DynFlags import Linker import GhcMonad import GHCi.ObjLink main = defaultErrorHandler defaultFatalMessager defaultFlushOut $ do runGhc (Just "E:\\ghc-dev\\msys64\\mingw64\\lib") $ do _ &lt;- getSessionDynFlags session &lt;- getSession liftIO $ do initDynLinker session lookupSymbol "sem_wait" ``` Replace the path in `runGhc` with the one you got earlier, Notice that any `\` has to be escaped to `\\`. compile that by doing ``` ghc -debug -package ghc Test.hs ``` Run the produced file with ``` Test.exe +RTS -Dl 2&gt; dump.log ``` and put the content of `dump.log` somewhere like a pastebin or http://gist.github.com/. now in msys2 run ``` strace ./Test.exe | grep winpthread ``` which should give you something like ``` --- Process 31684 loaded E:\ghc-dev\msys64\mingw64\mingw\bin\libwinpthread-1.dll at 0000000064940000 --- Process 18360 loaded E:\ghc-dev\msys64\mingw64\mingw\bin\libwinpthread-1.dll at 0000000064940000 ``` paste that here. If strace still doesn't work, then use something like http://www.rohitab.com/download/api-monitor-v2r13-setup-x64.exe click the "monitor new process", open the `test.exe` we made and press ok. Afterwards go to File -&gt; Save As and save the trace. upload the file somewhere and PM me a link. 
Yes.
The example is a worker rush, so great!
If you want mutable arrays, you can use http://hackage.haskell.org/package/vector-0.12.0.2/docs/Data-Vector-Unboxed-Mutable.html These can work both in `IO` and `ST` (which lets you do in-place mutation that looks pure from the outside, so you don't taint your algorithm with IO unnecessarily). This isn't cheating, it's just choosing the right data structure. 
Honestly it was because it was the simplest example I could think of to get something "interesting" working :D
&gt; https://matrix.hackage.haskell.org/#/package/sexpr-parser Thanks, I did not know about this page. I will investigate this.
cool! thanks, I've finished my implementation and now have a bucket full of errors to fix, wish me luck!
If you can change the class, swap the arguments. Then GND will work. See https://stackoverflow.com/questions/33245788/using-xgeneralizednewtypederiving-with-xmultiparamtypeclasses Curiously, I run into this problem myself, but I can't change the argument order :( https://www.reddit.com/r/haskell/comments/al0g7n/proposal_accepted_to_add_setfield_to_hasfield/efb8dcg/ You can use `coerce` to manually write what GND would do
Well, I just fixed it for 8.4.4, 8.2.2 and 8.0.2. Now I'll look at 7.8.4. That's progress of sorts.
Thanks I'll modify the code tomorrow morning and report back 
You're right of course, guess I was expecting some kind of miracle. It's just a bit sobering that a data structure that is not found or only mentioned in passing in many beginner/beginner intermediate resources is required to surpass JS, where the most naive approach is also the pit of success. Maybe it's more of a teaching issue then because mutable data structures just aren't very present even though for stuff like Advent of code it can almost be mandatory in quite a few cases. I'll modify the code tomorrow thanks for the feedback :)
When bounds get in the way, you can always ignore them, so in the worst case there is no difference between having them or not. The bottom line without any version information is to try building things and wait for an error to happen. The "Stackage without bounds" model is simply offloading that work to a benevolent entity that has the resources to build a global snapshot, but that assumes users have the drive to keep up. But in the best case, version bounds may allow a constraint solver to find a solution that is likely to build (because the bounds are conservative) in a few seconds. `extra-dep` without version bounds is manual constraint solving, without any idea of what the constraints are. The few times I've investigated some cases of slow upgrades on Stackage myself, the reasons seemed legitimate, not spurious bounds. I did not look into it too deeply I admit, but I remain skeptical that these are a major source of problems. Honestly, maybe it's fine if you think it's too much work for you to maintain version bounds. But calling them a mistake in general is going too far. Version bounds are signal willingly provided by maintainers. If that signal is truly a mistake, Stackage is free to ignore it, but they don't.
Simply dropping in an Data.Vector.Unboxed.Mutable.IOVector instead of the pure variant takes runtime from 143s to 39s on my box. Pure vectors are great for write once read often. Or sometimes if you need to keep old value vectors around. But as a rule of thumb you should use a mutable vector if values are often overwritten. If you shift functions around a bit to get rid of some argument passing that also quickly goes down further (~25s on my box.). Example code here: https://gist.github.com/AndreasPK/8197a799ccbf41e12a05bfa5f461a1a4 but it's very much still a hacky solution. 
Fair enough, but still caused me to chuckle. :)
I own the class so I can change it. This works perfectly. Thanks very much!
I'd say Haskell is ridiculously honest. Mutating a referenced value isn't something you can just hide somewhere, and nobody would notice. It's very explicit, for good. It would be nice to have "FP" problems in AoC (maybe there were, I didn't participate this time); but I think they would make JS users much more miserable than inherently imperative problems make Haskellers struggle.
&gt;It's just a bit sobering that a data structure that is not found or only mentioned in passing in many beginner/beginner intermediate resources is required to surpass JS It's not as if JS is generally faster than Haskell without ST or IO, it's simply that when modeling an algorithm that is using a persistent mutable store, Haskell requires special effort, whereas JS does not. Similarly, when attempting to model many problems in JS recursively, you get spaceleaks or even runtime exceptions, where Haskell chugs along comfortably at a respectable pace. It's usually possible to re-frame the problem in such a way that you don't need to grab for ST, although I'll admit that the latter days of AoC do a good job of introducing puzzling challenges that can make that particularly hard to do.
The similarity of the name `sc2hs` to `c2hs` and `hsc2hs`, plus the fact that the former parses as `sc2 hs` rather than `c "2" hs`, makes me wish this was named something like `starcraft-2-bindings` instead... Otherwise, cool stuff :)
Google V8 is often faster than Haskell
Meep, that's a good point.
Perhaps this is already clear, but I figured I'd make sure - `nub` is pretty inefficient. See http://neilmitchell.blogspot.com/2015/02/nub-considered-harmful.html and https://github.com/nh2/haskell-ordnub . If you do indeed only want to depend on Eq then this would be a fair bit better. Still means that building a list with `consL` would be `O(n^2)`, but that's better than `O(n^3)` you'd get with the implementation in the post. ``` instance ConsL NubSet a where a `consL` (NubSet bs) = NubSet $ a : filter (/= a) bs ```
Fear not! I am not using `nub` in the real code (https://github.com/rcook/oset/blob/master/lib/Data/Set/Ordered.hs#L187). My `(|&lt;)` function is _O(log(N))_.
And thank _you_ for coming back and answering your question.
See also: https://reddit.com/r/haskell/comments/al5hrd/_/efw0aqa/?context=1
Are you comparing to Hugs?
Awesome episode, like always. 
I have created a package with cabal new-sdist, and now I would like to try it out. Is there some way of making it available locally, such that I can use it in build-depends of other local projects? Or is there an alternative approach that I have not considered? 
If you have cabal 2.4+, you simply specific the path to your resulting `.tar.gz` sdist file in the`packages: ...` field. E.g. if you have a package in your current folder, just create a `cabal.project` file with the contents packages: ./ /path/to/your/foo-1.2.3.tar.gz this will have two effects: with `cabal v2-build` it will force including `foo-1.2.3` in your build-plan (thereby shadowing any other `foo` versions that might be eligible from e.g. Hackage) and it will also cache `foo-1.2.3` in your Nix-style cabal store (this is a difference to pointing `packages:` to an unpacked source-tree, which would cause it to become an inplace/local package *not* cached in your Nix-style store)
I think Day 5 was a good example of something where FP came out pretty well. Take a look at /u/glguy 's solution, which is the top comment also: [https://www.reddit.com/r/adventofcode/comments/a3912m/2018\_day\_5\_solutions/eb4dchg/](https://www.reddit.com/r/adventofcode/comments/a3912m/2018_day_5_solutions/eb4dchg/) 
I'd say the teaching materials are correct in their approach. Using immutable data structures to quickly and easily build something that is correct is almost always the right way to start out. Most problems in the domain of Haskell and JS don't really need speed, and should absolutely value correctness far higher. If you really have a speed requirement, you can then turn to the intermediate/advanced topic of optimization, and learn about more error-prone techniques like unboxed, mutable vectors.
Thank you very much. This approach works. 
I think "often" might be a too broad statement. But a problem like this, where the Javascript is effectively statically typed, and mostly doing arithmetic is a very generous case to JIT-compiling. It should be spending almost all of its time on the hot-path, achieving near-native speeds except for startup overhead. So while it is not unexpected here, it is also not representative at all.
I tried to get the package into my package repo with cabal new-install --lib packagename and had several issues. These were mainly related to #include files in hsc file not being passed. By adding these to: extra-sources-files and Include-dirs it has moved a step further, and now I am dealing with some other issue related to directory and ghc versions. Adding -j1 and -v3 gives some debug details. Otherwise it just **fails silently**, which can be difficult to know for newcomers (as myself). The way to notice it is if it states "installing .." but does not follow-up with "installed" for similar. This means, that the "installing" step has failed.
Cripes. This word salad looks like its written by a markov bot written to troll the subreddit.
Besides the tracable JIT advantage, think also about caching differencces. Eg, tiny Intel caches. Maybe even branch predictions. 
No on both accounts. The first, just because that's not where the files get put. The second, because `hi` files don't have core in them. What's the end goal that you're working towards here?
hvr assisted me greatly on IRC #hackage, thank you! I had another issue related to cabal new-install --lib due to a default file in \~/.ghc/\*-8.4.4/environments/. Apparently there is an issue where multiple cabal new-install --lib end up self-conflicting. I deleted the default file, and it worked. 
Two points on dummy arguments. First, there is what I think is another con, which is that the implementer should use lazy pattern matching, if they are going to pattern match on it. To give you an example, consider a Storable that tightly packs to elements in memory: data Two a = Two a a instance Storable =&gt; Storable (Two a) What's the `sizeOf` a `Two a`? It's the size of its components: instance Storable (Two a) where sizeOf (Two a b) = sizeOf a + sizeOf b Sounds fair, right? Unfortunately, you're very likely to have just introduced a crash, because it's (too) common to just pass `undefined` to `sizeOf`. An innocent user might write let offset = sizeOf (undefined :: Two Int) Which will now crash, as we tried to pattern match on `undefined`. Instead, we should write instance Storable (Two a) where sizeOf ~(Two a b) = sizeOf a + sizeOf b Which shows that we don't actually care about are argument being given (because nothing should ever actually need it). This is an annoying complication that comes out of how people are currently tending to use `sizeOf` in practice. My second con is related to this, which is that it's really annoying for users to actually use these methods. You mentioned that sometimes you might be forced to use `undefined`. In practice, we almost always use `undefined` because even if you *could* construct a valid value, it's so tedious people just don't bother. Why write `sizeOf Person{personName="ignore me", personAge = 30}` when we could just write `sizeOf (undefined :: Person)`?
done :)
One way to prevent pattern-matching on `Proxy` is to hide its constructor altogether, and provide a smart constructor `proxy :: forall a. Proxy a`. In case the visibility of the actual constructor matters to `Coerce`, give it a more colorful name `data Proxy a = TheProxyThatMustNotBeNamed`.
Ah yes, didn't mean to mislead - just that if you're from the UK then there's no reason *not* to go!
Internet guy says Intel designed Extremely Bad CPUs, ok
Another downside of `AllowAmbiguousTypes` is that it must be enabled for the whole module, which may lead to accidentally ambiguous types elsewhere in the code and hide bugs. As u/ocharles said, I hope `DependentHaskell` will provide a proper solution through visible dependent quantification.
I like your observations about `Eq` in the function signature. Thank you for your thoughts!
Makes sense
That's a neat idea I feel, thanks!
I seem to like this approach, thank you!
Thank you so much for taking the time to not only understand what my undocumented code was doing but for also writing a version with mutable data structures. I took your code as an inspiration and modified the file (you can find it in the repo) and it's now running really fast at the cost of some safety :) I'll definitely have to read up on some things though like `Control.Monad.Primitive`
I like type applications personally, but sometimes it's a bit weird in Haskell since afaik you don't really have a way to apply only a specific type parameter. For example, in Agda if you have `f : ∀ {a b : Set} -&gt; ....` you can do `f {b = t} ...` and still let type inference do its work on `a`, whereas in Haskell I think you're forced to provide something for `a` in order to even have a chance to give something for `b`. This can sometimes be annoying in situations where `a` is complicated to write but should be inferrable but you still want to manually specify `b`.
I will check it out, thank you!
Yes true
Nice little overview, thanks. I recently removed all the `Proxy` arguments to my class methods which used to use them: type applications do the trick nicely, as you rightly point out. However Proxy is still useful e.g. in data types that have an existential component, so that functions that handle this datatype can have access to the type variable: ``` data AKnownNat where MkAKnownNat :: KnownNat n =&gt; Proxy n -&gt; AKnownNat aNatVal :: AKnownNat -&gt; Natural aNatVal (MkAKnownNat (_ :: Proxy n)) = natVal @n ``` After reading this blog post I think I'll switch to using `Proxy#` for these, thanks for the heads-up. 
This looks very similar to `traverse`: f :: Monad m =&gt; m (Maybe a) -&gt; (a -&gt; m b) -&gt; m (Maybe b) f m g = m &gt;&gt;= traverse g 
You can use `@_` to skip a parameter.
This is what monad transformers are for. You can turn `m (Maybe a)` into ` MaybeT m a` and then you get fmap, return, ap, and bind back for free. You just need to insert the appropriate calls to `MaybeT` and `runMaybeT`. 
Huh, TIL. Thanks!
Becaus Fractional is more general than Float. 75.86 isn't necessarily floating point number in Haskell. It could be Rational or any other instance of Fractional. Prelude&gt; let a = 75.86; b = 8.85; c = 8.13; Prelude&gt; a + b + c :: Rational 2321 % 25 Here all `a`, `b` and `c` are converted to rationals and you get exact rational arithmetic. There's no need to convert anything. You might add type annotation if you need concrete type (for example, `a+b+c :: Float`), but compiler usually can infer types itself. By the way, the outpus are different, because `Fractional`s are defaulted to `Double`s (not `Float`s) in ghci.
Fractional is a typeclass (an interface); Float and Double are concrete types, which have (among others) Fractional instances.
Also sometimes it is handy to write something like \`sizeOf :: proxy a -&gt; Int\`, so that you can pass any container type at hand (e.g. \`Maybe Double\`) as well as the \`Proxy Double\`.
What about something like this? :) instance Storable a where _sizeOf :: a -&gt; Int sizeOf :: forall a proxy . proxy a -&gt; Int sizeOf _ = _sizeOf @a undefined
&gt; From their post: &gt; &gt; &gt; ∀ab. a -&gt; b -&gt; a is an example of a closed type: all type variables are bound by a universal quantifier. An open type is a type with free type variables such as ∀b. a -&gt; b -&gt; a The sentence which immediately follows is: &gt; Note that this distinction is harder to see in Haskell where universal quantifiers are often implicit. We will not follow that convention in this article. Due to those implicit `forall`s, the type of `new` is `forall key val. (key -&gt; key -&gt; Bool) -&gt; (key -&gt; Int32) -&gt; IO (HashTable key val)`, and the type of `foo` is `forall a b. a -&gt; [b]`. Type variables are always bound by an implicit `forall`, they are never unbound. The reason the article even talks about unbound variables is because it is talking about mathematical formulas in which that distinction is important. Those mathematical formulas happen to tell us useful things about Haskell programs, but that doesn't mean we can interpret Haskell types as mathematical formulas. Again, you cannot write a Haskell type containing an unbound variable, there is always an implicit or an explicit `forall` somewhere. &gt; I couldn't find any types in their post that have a type variable in a function type's result that isn't also in a function type's parameter, e.g. a -&gt; b. That statement is true, but they only list 5 functions in that post, so there are obviously many, many functions they did not list. For example, they did not list any function of type `String -&gt; Int`, but that doesn't mean that `length` doesn't exist or that it violates parametricity. I think you may be a victim of [confirmation bias](https://explorable.com/confirmation-bias): you thought you had spotted a rule, and those 5 examples seemed to confirm your rule, but in fact there are many other rules which would also be compatible with those 5 examples. This rule just happens to be the first one you came up with. &gt; In fact, they even say explicitly: &gt; &gt; &gt; Other than undefined (which we are ignoring), there can be no function f :: ∀ab. a -&gt; b That statement is true, but is only about the type `a -&gt; b`. It does not imply anything about the type `a -&gt; [b]`. &gt; I don't see how `new` with `val` is compatible with that. `new` and `undefined` may both share the property that a type variable in their output type does not appear in their input type, but they do not have the same type. If you go through the motions and compute the free theorem for `new`'s type, you will get a different free theorem, not `b⃯ . f = f . a⃯`, and so you will not be able to derive a contradiction. &gt; my understanding is that Haskell 98 types with a type variable have an implicit, pseudo forall type wrapped around the outside: `(key -&gt; key -&gt; Bool) -&gt; (key -&gt; Int32) -&gt; IO (HashTable key val)` is really `forall key val. (key -&gt; key -&gt; Bool) -&gt; (key -&gt; Int32) -&gt; IO (HashTable key val)` This part is correct! &gt; except the type variables are constrained to being in function type parameters due to type inference. This part is incorrect. Type inference can flow from inputs to outputs, and from outputs to inputs. For example, in `id ([] :: [Int])`, we know the type of the input and can conclude that the type of the output is also `[Int]`. And in `(id []) :: [Int]`, we know the type of the output and can conclude that the type of `[]` is also `[Int]`. 
Few functions take monadic values as input. First use `(&gt;&gt;=)` or do-notation to get your hands on the `Maybe a`, and now you'll have the much easier task of finding a function of type `Maybe a -&gt; (a -&gt; m b) -&gt; m (Maybe b)`.
&gt; main issue seems to be that Cabal requires version constraints for all the dependencies I don't think so: Last time I tried, Hackage accepts your upload as long as you have bounds for `base` in your `.cabal` file / `package.yaml`. Another point of info: In addition to `stack upload`, you can also use `stack sdist` to generate a tarball you can upload via the Hackage website upload form.
I have to say I still haven't quite understood why Haskell does not have an equivalent of `variable.function` = `function variable`, something like Julia has for example. It seems like it'd be a rather convenient syntax for these situations, reducing awkwardness.
[removed]
How safe is heterogeneous equality? Do you know of any cases in which it can be broken, that is, be True when types do not contain equal values?
Dependent Haskell is the way to go, as you write sizeOf :: forall a -&gt; Storable a =&gt; Int but as you'll notice `forall a -&gt; ..` occurs *before* the `Storable a` constraint, this is mentioned at the end of the post, when we write `Storable` there is an invisible `forall.` quantification class forall a. Storable a where sizeOf :: Int peek :: Ptr a -&gt; IO a which affects *all* methods, sizeOf :: forall a. Storable a =&gt; Int peek :: forall a. Ptr a -&gt; IO a if GHC provided a way to quantify visibly `forall-&gt;` which I imagine it will in the future class forall a -&gt; Storable a where sizeOf :: Int peek :: Ptr a -&gt; IO a now `sizeOf` what we want but now `peek` uses `forall-&gt;` and we have no way of letting GHC know "well actually only `sizeOf` should be quantified visibly.." peek :: forall a -&gt; Storable a =&gt; Ptr a -&gt; IO a peek Bool boolPtr
Also works for &gt;&gt; :set -XTypeApplications &gt;&gt; :t fmap @((-&gt;) _) fmap @((-&gt;) _) :: (a -&gt; b) -&gt; (_ -&gt; a) -&gt; _ -&gt; b 
It will be so nice to be stop using `Proxy`s altogether, not being able to use `@ty` in patterns sometimes makes me have to rewrite a lot of code to add `Proxy`s
This is a known thing, Ryan says he talked to Stephanie Weirich and Richard Eisenberg about it, I don't know how much of this can be solved with syntax but one meh solution: class forall a. Storable a where peek :: Ptr a -&gt; IO a forall a -&gt; Storable a where sizeOf :: Int
There is a proposal for [Named type arguments](https://github.com/ghc-proposals/ghc-proposals/pull/196)
Then where are the files for the code? Since in the `lib` dir there are only `.hi` files, the real code (`.o` or `.hs`) must go somewhere right? I am making a compiler that compiles Haskell to a custom language. I already can compile core, and if I have the source I can do source-&gt;core by GHC API, and compile the resulting core. But I don't know where to find the depended packages such as `base`.
Couldn't you use [(&amp;) from Data.Function](http://hackage.haskell.org/package/base-4.12.0.0/docs/Data-Function.html#v:-38-) It gives you: argument &amp; function Although I've rarely seen it used in the wild.
&gt;Yunti Thanks for your reply , really helpful
For me this raises the question "if we allow visible quantification `forall-&gt;` in type classes" -- eq :: forall a -&gt; Eq a =&gt; a -&gt; a -&gt; Bool class forall a -&gt; Eq a where eq :: a -&gt; a -&gt; Bool can we use other visible quantifiers ([*Figure 4.1*](https://cs.brynmawr.edu/~rae/papers/2016/thesis/eisenberg-thesis.pdf) (page 59) of Richard's thesis) quantifying over the entire declaration? Haskell has used `(-&gt;)` for visible quantification in the past (visible, non-dependent quantifier), would it make sense to quantify that over a type class? Can it mix with other quantifiers? -- one :: Int -&gt; forall a. Foo a =&gt; a -- two :: Int -&gt; forall a. Foo a =&gt; a class Int -&gt; forall a. Foo a where one :: a two :: a and can we use `pi-&gt;` for dependent quantification? (I won't to use `foreach` unless forced)? Fun times class pi a -&gt; Foo ..
Thanks for the really clear explanation. That helped my understanding a lot. 
It's been proposed several times, but it causes some complex breakage. I believe Frege uses this syntax, requires spaces around the function composition operator. Currently, stuff.other in Haskell is used for `Hierarchical.Module.symbol`, `Module.symbol` (in both the cases symbol might be an operator or type class, not just a type [constructor] or value) as well as invoking whatever `(.)` operator is currently in scope, which is normally `Prelude.(.)`, but could be something else. ISTR there's some case where `Module..` means the `(.)` operator in `Module`, too.
Have you pushed your first attempt anywhere? I've been working on [adding window functions and aliases](https://github.com/jprider63/esqueleto/compare/434f81ed41795e3dd0754dbc5c75c4ed098631b3...jprider63:master) to Esqueleto. It currently works, but I want to improve it. Your debugging code/setup might be useful if you would share it. 
The \`.o\` files go somewhere. But the \`.o\` files don't have core either. Core is an intermediate stage, and the \`.o\` files have passed through a number of further stages to generate assembly, etc. You can only dump core with command line flags afaik, and there's no general workflow for keeping it around. &amp;#x200B; The best way to get the core of dependencies is going to be to pull their source from hackage and compile them to core with ghc as well. But you'll have to worry about the many primitives that are builtin to base, as well as the ffi.
I don't consider supporting GND to be important, so I'm a definite advocate for the `proxy a -&gt; res` style. It's the most flexible for users, and the hardest to get wrong when I'm implementing it (can't pattern-match). Well, until we get full DependentHaskell, then I'll just ask for the type as an argument. I also think roles are a hack that never should have gotten into GHC until they could deal with `join :: m (m a)` being a member of `Monad` without explicit annotations.
Still possible to mess up when you are providing an instance (`Proxy` vs. `_`), but it's less likely to be relevant and you do get GND back.
The intuition to build here is that GHC/i defaults to the most general set of constraints that could be applied to a term in order to typecheck. This is exactly what is happening when you type `2` and get `Num a =&gt; a` - 2 could be a float, it could be a fraction, it could be an integer, or it could even be a word. GHC/i doesn't have enough information to give it a concrete type at declaration, so it supplies the most general constraint it can. Later, when you call a function on that term, GHC/i may infer a more specific constraint or set of constraints, and default to a more specific typeclass that would be necessary for the term to typecheck there. When it "actually" gets called at runtime, it needs to assume a concrete implementation, where if one is not supplied or forced by the calling context, it will follow GHC/i's numeric type defaulting rules to get a type at runtime - That gets complicated and I'm not 100% clear on the exact set of rules that are used, but they differ between GHC and GHCi because of an implicit extension that is enabled in GHCi, `ExtendedDefaultRules`. What I can tell you is that the design philosophy generally favors correctness over performance, so usually you end up with say, `Integer` instead of `Int` (because `Int` can overflow), which can be a 'gotcha' in some contexts. Moral of the story - If you care about which concrete type you're using, force it at the call site, elsewise, inference generally lets you keep your options open at declaration.
I love this series. If there is any way to incorporate more type-level stuff in this series, that would be awesome. The data-centric functional side of things is pretty similar to what you would find in dynamically-typed FP languages like Elixir or Clojure, but I'd like to see more about how the types of Haskell will help us.
You could also replace `Proxy a` with a custom data type with no (exported) constructors.
`fmap (fmap (+1)) [Just 7, Nothing, Just 4]`
Resharper’s automatic imports is one of its best features. Takes a lot of the pain out of tracking which modules to reference.
We just need poly-kinded, parametric `Void`. ;)
Related question: when doing quick iteration (e.g. for debugging) it's a pain to edit import lists. Do people have suggestions to make that less painful? A thing that would work for me would be a feature that lets you put an import line anywhere in the program, but with a warning if it's not in the standard place. Or that lets you implicitly import a module where you use it, perhaps with some convention like `_I_.Debug.Trace.trace`. But those are both kind of silly. Something with hsimport might work.
This is not necessarily *about* Haskell, but I wrote it in response to a common problem I see when people write about Haskell. I'd suggest it's valuable reading to anyone who blogs about FP concepts.
Applying list of functions to value with scanr such that `scanr (\x acc -&gt; x acc) x [f, g, h] == [f(x), g(f(x)), h(g(f(x)))]` . I am having an issue where I have a function minEqRel :: Bool -&gt; Graph -&gt; [Graph] minEqRel v graph = let closure :: (Graph -&gt; Graph) -&gt; Graph -&gt; Graph closure f graph = unionUnsafe graph $ f graph getRelation | v = scanr (\x acc -&gt; x acc) graph $ closure &lt;$&gt; [symmeteric, transitive, reflexive] -- | v = [closure reflexive $ closure transitive $ closure symmeteric graph] | otherwise = [foldr (\x acc -&gt; unionUnsafe acc $ x acc) graph [symmeteric, transitive, reflexive]] in getRelation The commented out line works as expected (although only producing one value instead of the list I desire, just for testing). However the line above it does not work. To my eyes the two lines are the same in principle. Can anyone give pointers?
WFM: % ghci GHCi, version 8.0.1: http://www.haskell.org/ghc/ :? for help Prelude&gt; scanr ($) 0 [(^2), (*2), (+1)] [4,2,1,0] Prelude&gt; scanr ($) 5 [(^2), (*2), (+1)] [144,12,6,5] 
wow! I was so locked in to seeing $ as order of operation modifier I failed to see it as a function itself. Thanks!
&gt; I think visible quantification (Dependent Haskell) is the way to go Does that really help? /u/ocharles's point remains the same even if `data Two = Two Int Bool`
Functorality also helps with your first example doesn't it? instance Storable (Two a) where sizeOf twoab = sizeOf (fmap twoFst a) + sizeOf (fmap twoSnd b)
This actually gets used quite often in lens code, it’s reexported by `Control.Lens.Operators`
I usually just declare `Proxy` without any constructors, so it's impossible to match on it.
That doesn't look right. Don't you want sizeOf (twoFst twoab) (twoSnd twoab)? And that's my point about using lazy pattern matching. 
I mean in the hypothetical case where `sizeOf` takes a `Proxy`
A couple recommendations: * This post is absolutely unreadable. Post it on a site that understands the concept of code blocks and newlines. * If you can, build this on Linux first, where developer tools work well. After it works there, you can try make it work on Windows as well.
Oh right. Yea sure, it works wherever you need to essentially recurse through a structure that you can split (and I'm sure there are other uses)
I don't really see how `AllowAmbiguousTypes` can lead to bugs.
How safe is heterogeneous equality? Do you know of any cases in which it can be broken, that is, be True when types do not contain equal values? 
&gt; "How" without a "why" suggests a contrived solution to a made-up problem. In other words, it's easily read as "who cares?" IMO this extends to most everything in life, be it personal relationships, small talk w/ randos, submitting PRs, and really everything in between. A "how" without a "why" is just funneling information into the void; That person stopped listening a few mins ago.
Me neither. I'd love to see an example though!
&gt;Due to those implicit foralls, the type of new is forall key val. (key -&gt; key -&gt; Bool) -&gt; (key -&gt; Int32) -&gt; IO (HashTable key val) Shouldn't it be `forall key. (key -&gt; key -&gt; Bool) -&gt; (key -&gt; Int32) -&gt; forall val. IO (HashTable key val)`, since key is supplied by applying new, but val is not, e.g. `(new charEq) :: forall val. (Char -&gt; Int32) -&gt; IO (HashTable Char val)`? Another poster [agreed](https://www.reddit.com/r/haskell/comments/al5hrd/datahashtablenew_seems_to_have_an_unbound_type/efw0aqa) with this. If so, then I can see how this all works concretely, since val is still bound by a forall after new is applied. &gt;I think you may be a victim of confirmation bias: you thought you had spotted a rule, and those 5 examples seemed to confirm your rule, but in fact there are many other rules which would also be compatible with those 5 examples. This rule just happens to be the first one you came up with. I think perhaps what I wasn't seeing was the distinction between type variables appearing in function type results alone (`... -&gt; a`), as opposed to being applied to by higher-kinded types in function type results (`... -&gt; T a`). The former case is intuitive enough, as I understand it: It doesn't work because it's impossible to satisfy in the general case. The b in `a -&gt; b` must work for all types, but there's no real value (leaving undefined out of it) that you can put there that will do that. `\x -&gt; 42` has type `a -&gt; Int`, not `a -&gt; b`. That same constraint doesn't seem to be true for the latter case, though, which is what I think was tripping me up, especially when constructing the type doesn't require using a value of the type parameter, as in the case of `[]` for `[a]`, or, in this case, new for `HashTable key val`. Your remark below this about "it does not imply anything about the type a -&gt; [b]" seems to agree with this. (Please correct me if I'm wrong!) &gt;And in (id []) :: [Int], we know the type of the output and can conclude that the type of [] is also [Int]. Where is the forall in that expression? It might help me to make the type abstractions and applications more explicit. For `(id []) :: [Int]`, there seems to be an implicit type application there, just like there's an implicit (for Haskell 98, at least) type abstraction, where it's equivalent to something like `(forall a. (id :: (forall b. b -&gt; b) &lt;a&gt;) ([] :: a)) &lt;[Int]&gt;` (where `x &lt;y&gt;` is my pseudo syntax for the application of type abstraction x to type argument y). In this sense, the type for [] is still gotten by "inputs to outputs" via the type abstraction, it's just that that the input comes from further outside the expression via the type abstraction, looking like "outputs to inputs". Am I off the mark? What do you think?
Hey, it's a Richard I know! Cool stuff, glad cons isn't being implemented with nub :)
What about this one? class Storable :: Type -&gt; Constraint where peek :: forall a. Storable a =&gt; Ptr a -&gt; IO a sizeOf :: forall a -&gt; Storable a =&gt; Int Then an instance declaration has to specify all arguments, which ends up looking the same as what we're used to.
I don't think h is possible because we're not guaranteed to have a way to remove a value from a functor context... for example, this doesn't seem like a sensible specialization of h: h' :: Int -&gt; (String -&gt; Int) -&gt; IO String -&gt; Int The third parameter is "stuck" in the IO functor, so there's no way for us to get a plain String to pass to the second parameter, so `h' n _ _ = n` is the only possible definition. I think you'd need a separate typeclass with some function `unwrap :: a -&gt; f a -&gt; a` and, I guess, a law `unwrap a (fmap g (f a)) = g (unwrap a (f a))` or something like that so you can't cheat and define `unwrap a (IO _) = a`. I assume this typeclass already exists and I'm just blanking on the right name.
I'm not sure about Haskell, but the Idris and Agda heterogenous equality types only have a single constructor -- same as homogenous equality, so when you actually have one, you know know only have the types actually the same type, but the values are, too. Now, you can't necessarily use this information by pattern matching, unless you assume the "K axiom" which doesn't follow from the other axioms of MLTT. In many case doing so would aggressively unify type indexes, and be incompatible with some (rather conservative) extensions of MLTT (in particular HoTT). In Haskell you also have the complication of bottom inhabiting every type. (Idris avoids this with strictness; Agda through the totality [and other] checker[s].) But, I don't think this actually allows you to subvert the type system (you just might get a crash on a impure exception or a loop).
Really the best guiding principle is to want to explain, not to show off. The most common mistake I see is writers thinking they have to *impress* the reader so the reader believes them. If you're writing about something, it means you think it's complicated enough to explain. Even if the reader is a researcher or industry-expert, they don't want to be blown away with complex terminologies. They want to read something simple that makes its message obvious with obvious examples. Hammer in the message a few times. Use the same simple wording each time. *With that out of the way*, sometimes you do really need a high level of prerequisite knowledge to approach a given topic, such as certain terminologies and concepts. People looking for a shortcut will usually find themselves confused and in trouble.
You could also try adding strictness annotations (via [bang patterns](https://downloads.haskell.org/~ghc/7.8.4/docs/html/users_guide/bang-patterns.html)) and [UNPACK](https://wiki.haskell.org/Performance/Data_types#Unpacking_strict_fields) pragmas to your `Instruction` type. &amp;#x200B; {-# LANGUAGE BangPatterns #-} &amp;#x200B; data Instruction = Instruction { kind :: {-# UNPACK #-} !OpKind , a :: {-# UNPACK #-} !Int , b :: {-# UNPACK #-} !Int , out :: {-# UNPACK #-} !Register } &amp;#x200B; Let me know if that makes any improvement :)
There is also the approach of using `Tagged` instead of a proxy.
&gt; Nobody is going to read the whole thing from start to finish. Instead they're going to jump around, ignoring the pieces they already know, and looking for the bits they don't I read every sentence in linear order because I'm afraid I can miss something important otherwise.
Hmm you need to format it a bit better, that from what I can tell your make rules don't make much sense. In particular you're creating move.o from a ghc command without -c, so you're producing a binary instead of an object file. Also I can't tell what your final link command is. But you need to use ghc to link not g++, but because you're giving it object files you need to tell it which libraries to add, e.g. You're missing to tell it to link base and rts, hence your errors. It'll be easier if you rethink your approach a bit. I would suggest compiling your c++ file then give everything in one big compile and link command to ghc. Ps. Just ignore the people who tell you to use Linux. 
&gt; Post it on a site that understands the concept of code blocks and newlines. Like reddit, where you format code with 4 spaces prefixing every line: like this you silly
Welcome! I haven't used either but (iirc) I've seen the 2013 version recommended more often. Also, check out the sidebar for more resources.
This might address what causes a single reader to stay on the page rather than click away -- but what causes a reader to upvote or retweet to their followers? Want me to hit that retweet button? I retweet things that either 1. Make me laugh. 2. Resonate with my identity. 3. Leave me awestruck. Explaining something well in a way that I can easily skim it is great and I will love you for doing that, but even if I really appreciate how useful your article was and worthwhile it was to read, that's just not enough to make me want to help syndicate it. 
&gt; In the earlier example, I could look at the function and interpret the kind of arguments it accepts and returns but whereas in the alternate implementation using type classes, I cannot do the same. It is quite generic. This is actually an example of improving readability, it just doesn't feel that way to you yet. You have, by virtue of not suggesting something specific, pushed the reader towards thinking of the argument as some generic type that implements equality, which, given that this is a more accurate description of the reality of the function, is a more accurate read. Case in point is extremely strained. It is difficult to think of a scenario in which someone would use this function. But in general, when you realize a function could have broader utility, you'd also give it a name that reflected that broader utility, instead of just keeping the same domain specific name. ``` data Size = S | M | L | XL deriving (Eq,Ord,Show) -- first draft getBigger :: Size -&gt; Size -&gt; Size getBigger l r = case (compare l r) of LT -&gt; r GT -&gt; l _ -&gt; l -- second draft getBigger ::Ord a =&gt; a -&gt; a -&gt; a getBigger l r = case (compare l r) of LT -&gt; r GT -&gt; l _ -&gt; l -- final draft max ::Ord a =&gt; a -&gt; a -&gt; a max l r = case (compare l r) of LT -&gt; r GT -&gt; l _ -&gt; l ```
It's pretty important to point out that "why" without "how" is also a phenomenon seen in technical documentation, and I for one will dismiss that much faster than the other. "Why" I want a library or a framework, without "How" do I use this or "How" does it work is basically marketing with no substance, and is generally worse than useless, as it is guaranteed to waste my time and leave me frustrated, where as "How" without "Why" usually just leaves me somewhat confused, and may have taught me something I'll find useful later.
Thanks for the suggestion! I will keep that one in mind! I found a used copy of the Haskell School of Expression so hopefully I won't have too much trouble with the graphics librairies.
Ah yes, the old 'organically retire a featureset via active user hostility' approach to product management.
Thanks for the suggestion. I started with mathematics before moving to programming so that background definitely helps! But I still have a lot to learn!
&gt; if a generalization of `maybe` exists? &gt; &gt; h :: b -&gt; (a -&gt; b) -&gt; f a -&gt; b &gt; h = undefined The arguments of `maybe` have type `r` and `a -&gt; r` because the constructors of `Maybe a` have type `Maybe a` and `a -&gt; Maybe a`. The generalization of `maybe` to any `f` is simply `f a -&gt; a`.
It's actually already in `base` for quite long time: * https://hackage.haskell.org/package/base-4.12.0.0/docs/Data-Function.html#v:-38-
Additionally, the `h :: b -&gt; (a -&gt; b) -&gt; f a -&gt; b` OP is looking for is like `foldMap` over `First` or `Last` from `Data.Monoid` h b f = fromMaybe b . getFirst . foldMap (pure . f)
Almost everything is correct, yes! &gt; `x &lt;y&gt;` is my pseudo syntax for the application of type abstraction x to type argument y GHC already has a syntax for type application, check out the [TypeApplications](https://kseo.github.io/posts/2017-01-08-visible-type-application-ghc8.html) extension. &gt; Shouldn't it be `forall key. (key -&gt; key -&gt; Bool) -&gt; (key -&gt; Int32) -&gt; forall val. IO (HashTable key val)` In Haskell98, the types * `forall key val. (key -&gt; key -&gt; Bool) -&gt; (key -&gt; Int32) -&gt; IO (HashTable key val)` and * `forall key. (key -&gt; key -&gt; Bool) -&gt; (key -&gt; Int32) -&gt; forall val. IO (HashTable key val)` are equivalent, both work. With the TypeApplications extension, those types are not quite equivalent. &gt; If so, then I can see how this all works concretely, since val is still bound by a forall after new is applied. In Haskell98, it doesn't matter where the `forall` is. The `forall`s do not disappear when the function is applied to an argument, they disappear when the type is specialized by the context in which the function appears. For example, you can choose to specialize `val` but not `key`: new2 :: forall key. (key -&gt; key -&gt; Bool) -&gt; (key -&gt; Int32) -&gt; IO (HashTable key Int) new2 = new or equivalently: new2' :: forall key. (key -&gt; key -&gt; Bool) -&gt; (key -&gt; Int32) -&gt; IO (HashTable key Int) new2' f g = new f g With the TypeApplications extension, it does matter where the `forall` is, because that's the position at which we may use the `@a` syntax to specify the type at which we want to instantiate the `forall`. If the type of `new` is `forall key val. (key -&gt; key -&gt; Bool) -&gt; (key -&gt; Int32) -&gt; IO (HashTable key val)`, then I can use TypeApplications to implement `new2` like this: new2'' :: forall key. (key -&gt; key -&gt; Bool) -&gt; (key -&gt; Int32) -&gt; IO (HashTable key Int) new2'' = new @key @Int But if `new`'s type is `forall key. (key -&gt; key -&gt; Bool) -&gt; (key -&gt; Int32) -&gt; forall val. IO (HashTable key val)`, then I must write this instead: new2''' :: forall key. (key -&gt; key -&gt; Bool) -&gt; (key -&gt; Int32) -&gt; IO (HashTable key Int) new2''' f g = new @key f g @Int As you can see, each `forall` corresponds to an extra argument, which is filled with the `@a` syntax. If you find this style clearer, you might enjoy studying [System F](https://en.wikipedia.org/wiki/System_F), in which `forall`s are always eliminated this way. With TypeApplications, they _may_ be eliminated this way, or they can be inferred by the context as in Haskell98. 
**System F** System F, also known as the (Girard–Reynolds) polymorphic lambda calculus or the second-order lambda calculus, is a typed lambda calculus that differs from the simply typed lambda calculus by the introduction of a mechanism of universal quantification over types. System F thus formalizes the notion of parametric polymorphism in programming languages, and forms a theoretical basis for languages such as Haskell and ML. System F was discovered independently by logician Jean-Yves Girard (1972) and computer scientist John C. Reynolds (1974). Whereas simply typed lambda calculus has variables ranging over functions, and binders for them, System F additionally has variables ranging over types, and binders for them. As an example, the fact that the identity function can have any type of the form A→ A would be formalized in System F as the judgment ⊢ Λ α . *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/haskell/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
The subtitle isn't _so people will share it_ :) But I would love to read that!
I mean people won't read it if people can't find it which won't happen if nobody shares it. Unless you already have your 50k followers -- then optimize for clarity ;)
There is a 2015 version as well.
I would learn haskell like this: - Best book http://haskellbook.com - My lecture notes from ctfp (understand nomenclature) https://github.com/srghma/category-theory-bartosz-milewski-lecture-notes - then realize that haskell is very weak language (for now, they want to add dependent types) - https://plfa.github.io
&gt;then realize that haskell is very weak language This pretty much depends on your perspective, but generally I would advice against advertising Haskell as a very weak language, since it is very expressive compared to other "mainstream languages", which are often the type of languages Haskell newcomers know. 
Do you know of any exercises where I can compare my answers to idiomatic solutions?
I'm not convinced this is a huge issue in a community our size?
&gt; then realize that haskell is very weak language What does this mean?
How about this? newtype ByteCount a = ByteCount Int class Storable a where sizeOf :: ByteCount a alignment :: ByteCount a
It’s quite... misleading. He probably means that Haskell lacks certain features (such as dependent types) to let it express more precisely several concepts from (among other things) category theory. That is, the type system is “weak” when compared to, for instance, Agda’s. Still, it’s leap and bounds ahead of most mainstream languages, which sometimes have no support for basic things such as parametricity (e.g Go), or higher-kinded types (e.g. Java); some don’t even have proper sum types, let alone more advanced stuff.
This is what I was thinking of when I said &gt;I wonder if when you turn DependentHaskell on it should just require you to write explicit quantification But I wasn't considering the `Storable a =&gt;` bit. Thanks!
I use this all the time! When `argument` is a complex expression that's the "meat" of a problem, and all the stuff to the right of `&amp;` is annoying noise. I'll often write this over multiple lines: foo bar baz waffle + xyz ** 2 &amp; round &amp; fromIntegral &amp; show Obviously made up here, but that's the pattern.
Strictness annotations did shave off a couple more seconds but unpack doesn't work here (can't unpack Int)
The problem only arises in library code, when you export functions but do not use them anywhere. A silly example: class Show_ a where show_ :: String -- I forgot a parameter here, but it compiles Of course, this example is too simple to be problematic, but when writing more complex library code these errors can be more difficult to spot. The thing is, currently there is no way to distinguish between "`a` must be provided by the caller" and "`a` should be inferrable but it is not, because I made a mistake".
Would be more accurate to say that there are languages with stronger type systems. To throw languages into a binary strong or weak typed bin is too coarse when there is in fact a spectrum of increasingly stronger types.
The network package is strange to me. The API uses strings everywhere. In addition, it seems like it's too high-level/somehow not high level enough. At Layer 3 we are switching to the sockets package (see https://github.com/andrewthad/sockets), which provides a high-level interface to dealing with posix sockets, using the posix-api (https://github.com/andrewthad/posix-api). One thing i see as extremely unfortunate is that network (and similar libraries in other languages) seem to mime the unix way of all sockets sharing the same type, which is my MkSocket has always had such a strange type. Stream sockets and Datagram sockets should not be the same type, their behaviour is too different to allow users the ability to mess up by having them be the same type. sockets has one Socket type per (a simple newtype around Fd). The API is still incomplete/there may be bugs, but i feel the end product will be something more simple and flexible than network.
I know I'm late, but one thing caught my eye: &gt; It is also an almost direct translation from an equivalent C implementation, but guarantees that our mutable state cannot escape the scope of the nub function—something that no imperative language can do. I think, technically, this is something Rust can do. 
And don't forget about `lift`! I imagine they'll need that too. If I'm not mistaken, their actual example will need that, since the second function they want to bind to does not have `Maybe`. 
In addition to what the other commenters said, I would add 1. Try running :t on a + b + c 2. The defaulting rules used by GHCi are designed to make code like 1 + 1 “just work” and not require type annotations. In real Haskell code the defaulting rules aren’t turned on, and they wouldn’t usually be necessary (because you usually have enough information to know the concrete type like Float, because for example a comes from a function that returns floats, so b and c must be float as well).
r/unexpectedfactorial
&gt; Just ignore the people who tell you to use Linux. Well, thanks Tamar for the nice Internet communication -- that's totally how you do it. I think my point was a fair. You need to have a good amount more knowledge and patience to make stuff work on Windows with a mingw tool chain, and there are much less people who have experience to help you with that. So there is merit to first solve your problem in a simpler environment where you can expect to get lots of help, to see if you have the basics figured out or whether you're making general mistakes in building. _Then_ you can move it to Windows and solve the harder problem.
Thanks. I am on Lecture 3 now of CIS 194 and doing all the homework assignments. It's been fun. :) 
This in fact quantifies twice with no indication that they are meant as parameters to `Storable`, your notation doesn't distinguish class Storable a where peek :: Ptr a -&gt; IO a class Storable a where peek :: forall b. Ptr b -&gt; IO b It's clearer in dictionary form that we quantify over a single type that is in scope for both methods data DStorable :: Type -&gt; Type where DStorable :: forall a. { sizeOf :: Int, peek :: Ptr a -&gt; IO a } -&gt; DStorable a
Oh sorry, I should have mentioned that you need BOTH strictness annotations on the Int in the data declaration AND the UNPACK pragma.
If they are quantified irrelevantly with `forall.`, `forall-&gt;` then you can't pattern match on them. For example class Storable a where sizeOf :: Int instance Storable Two we can call it with `sizeOf @Two`, where is the problem
Not sure if this is what you mean, but there was an ICFP paper that verified a bunch of Haskell library code: https://dl.acm.org/citation.cfm?doid=3236784 
Taking it one step further, for multi-parameter type classes class Iso :: Type -&gt; Type -&gt; Constraint where iso :: a -&gt; b osi :: b -&gt; a I don't see a way of making this work
I was talking about this recently with colleagues. We need some tooling to aid in synchronizing project resolvers. Stack makes it trivial to build the codebase reliably by auto-installing the right GHC and package set. The downside is that it's easy to get dozens of different package sets built: * Work projects use different resolvers. There's no way around this usually without a large migration. * Open source projects have their different resolvers. * Your own projects get out of date over time. My .stack is 8GB. Better than yours but still big. I wrote some scripts to help me track this. https://gist.github.com/chrisdone/f19a98e66056be447c746afe78f81df7 I have three GHCs installed at the moment (`stack ghcs` script): chris@precision:~$ stack ghcs /home/chris/.stack/programs/x86_64-linux/ghc-8.4.3.installed /home/chris/.stack/programs/x86_64-linux/ghc-8.4.4.installed /home/chris/.stack/programs/x86_64-linux/ghc-8.6.3.installed That's after I've removed a bunch of GHC versions. I don't like that I have ghc 8.4.4 and 8.4.3 installed, probably just because one random project I work on is using that. I have various LTS versions installed (grouped into sizes): chris@precision:~$ stack snapshots 1.3G /home/chris/.stack/snapshots/x86_64-linux/lts-12.12 527M /home/chris/.stack/snapshots/x86_64-linux/lts-13.1 274M /home/chris/.stack/snapshots/x86_64-linux/lts-12.0 251M /home/chris/.stack/snapshots/x86_64-linux/lts-12.20 112K /home/chris/.stack/snapshots/x86_64-linux/lts-12.18 16K /home/chris/.stack/snapshots/x86_64-linux/ghc-8.4.3 16K /home/chris/.stack/snapshots/x86_64-linux/ghc-8.6.3 16K /home/chris/.stack/snapshots/x86_64-linux/lts-13.4 This is basically due to a combination of the things above. I imagine I have much fewer than the average person, because I recently did a cleanup. The 16K ones are just initialized ones that I didn't want to build. I am at least presently fairly successfully keeping all my projects on `lts-12.12`, but I can see some other ones creeping in. I see that for example lts-12.0 is due to purescript: chris@precision:~$ stack find-resolvers | grep lts-12.0 /home/chris/Work/purescript/purescript/stack.yaml:resolver: lts-12.0 chris@precision:~$ stack find-resolvers | grep lts-12.20 /home/chris/Work/fpco/planning/stack.yaml:resolver: lts-12.20 /home/chris/Work/vincenthz/hs-hourglass/stack.yaml:resolver: lts-12.20 I think to get better space re-use you'd need to use Nix. I imagine the two 12.0 and 12.20 package databases are direct copies of most of the files in there, which is not great re-use.
It sounds like a better plan to integrate it into the [`-XTopLevelSignatures` proposal](https://github.com/goldfirere/ghc-proposals/blob/top-level-sigs/proposals/0000-top-level-signatures.rst), see the `fromInteger` example class Num a where fromInteger :: Integer -&gt; a fromInteger :: Integer -&gt; forall a. Num a =&gt; a as such class Storable a where sizeOf :: Int peek :: Ptr a -&gt; IO a sizeOf :: forall a -&gt; Storable a =&gt; Int
The only thing DH gives us that we don't have already is quantifying it visibly `forall-&gt;`, but that's only a matter of using `@` to override the visibility
Nope doesn't work. ``` {-# LANGUAGE BangPatterns #-} ... data Instruction = Instruction { kind :: {-# UNPACK #-} !OpKind , a :: {-# UNPACK #-} !Int , b :: {-# UNPACK #-} !Int , out :: {-# UNPACK #-} !Register } deriving (Show, Eq) -&gt; 21.hs:26:20: warning: • Ignoring unusable UNPACK pragma on the first argument of ‘Instruction’ • In the definition of data constructor ‘Instruction’ In the data type declaration for ‘Instruction’ | 26 | data Instruction = Instruction | ^^^^^^^^^^^... ^C⏎ ```
Thanks I'm now down to 20s :)
I'm sorry you feel my totally reasonable and measured response was "not nice internet" communication. I think my point was fair as well. Someone comes with a Windows question and the response to that person is to go learn a new OS, an entirely new tool chain that they likely aren't familiar with and doesn't address the problem they asked. Not only does it do a disservice to the person whom asked the question, it also does a disservice to the person that asked the question, because it does not answer the question, and actively steering someone away from Windows insures that the platform never gets better and also is an insult to those who work on it. You seem to think that the linker works the same way on Linux as it does on windows. So your assertion that making things work on Linux will tell you what to do on Linux is just wrong. Full stop. 
Symbolic in what sense?
Congratulations and thanks on your new release!
Eh, your example doesn't require BangPatterns :) Using `!` for strictness annotations on datatypes is standard Haskell98/Haskell2010. BangPatterns is for using `!` to add strictness on pattern matches in functions.
The `sockets` package looks very promising. I'm curious, in practice are you mostly passing around `ByteString`s in your application, and then picking out the `Addr` to pass to the the socket layer? Or are you programming with `ByteArray`s directly?
That's the [`Tagged`](http://hackage.haskell.org/package/tagged-0.8.6/docs/Data-Tagged.html#t:Tagged) approach that Ed. K. mentioned in the thread, but didn't make it into the article.
Oh neat! TIL :)
It represents a better programming language.
I had in mind that any method would have to start off by quantifying free variables (one for each argument), then applying the type class to them, and then whatever else. But it would allow different names for the different methods (although I don't think you'd want to use that functionality). Admittedly it can be a bit confusing when the methods refer to the type class itself, just instantiated at different parameters which aren't the class definition parameters. I hadn't considered it! For your example you could insist on having to write it: class Foo :: Type -&gt; Constraint where foo :: forall a. Foo a =&gt; forall b. Foo b =&gt; ... With `a` standing for the class parameter. Given that `(Foo a, Foo b) =&gt;` desugars to `Foo a =&gt; Foo b =&gt; ...` as you've taught me, I suppose that if you decided to still allow the syntax class Foo :: Type -&gt; Constraint where foo :: forall a b. (Foo a, Foo b) =&gt; ... Then `( foo :: forall a b. Foo a =&gt; Foo b =&gt; ...)` means that `a` is standing for the class parameter and `b` is just another type.
I would prefer being able to write the intended type signature directly, without having to write the method twice (once inside the class definition, once outside). Although obviously being able to do even that is an improvement over the status quo. That's why I would like to directly allow class methods that are top-level signatures; in particular you can rename the variables if you wish to. If you insist on that, you then shouldn't enforce the type class parameters to be bound in the class definition head, but in the methods instead.
How do you define "better"? There's a few different paradigms I would consider symbolic or related. Perhaps a Prolog like logic language? A pure functional language like Haskell?
Lol that's a huge red flag, and frankly just plain disrespectful of potential applicants. You clearly have a salary range, or you wouldn't be hiring, and if you can't be bothered to share it, why should people go through the process only to find out that "competitive" turns out to be $50K because "haskell is so fun!"?
&gt; actively steering someone away from Windows insures that the platform never gets better That's not what I'm doing. Given that the question is about Windows, they likely need to get their Windows result eventually, and that is fine and we all support that. I claim it's an easier overall learning path to get there via a Linux where the task at hand isn't an uncommon thing (especially when they are already using cross-platform components like SDL) and they can make fast progress. Also, it's great that people work on improving Haskell on Windows; you and me both do that and benefit from each other's work. All I'm saying is that trying to do nontrivial linking things immediately on Windows is akin to trying to learn swimming in the coldest water you can find. There are more practical paths to the goal, and mentioning them isn't a disservice.
 Data.Traversable.for :: (Traversable t, Applicative f) =&gt; t a -&gt; (a -&gt; f b) -&gt; f (t b)
I'm still reading, do you have an idea for superclasses like `Eq a` as they mention the class parameter class Ord :: Type -&gt; Constraint where (&lt;=) :: forall a. Ord a =&gt; a -&gt; a -&gt; Bool I'm strongly for full signatures as they appear in `:type` and `:kind` but it's difficult to fit with type classes
The learning goes on. I have started with Haskell 17 years ago. If you need a mentor, write me a private message and we can do a Skype session.
 g :: (Monad m) =&gt; m (f a) -&gt; (a -&gt; m b) -&gt; m (f b) on its own isn't possible. You don't know anything about f, it could be ((:~:) a), which makes this clearly impossible, ignoring casual abuse of `fail` or bottoms.
I try as much as possible to use `ByteArray` in my own code. I'd like to support `ByteString` as well. It's not difficult, and I know that's important for a lot of people.
Something like the Wolfram language perhaps?
It was more just a joke.
I'm confused. I'm not talking about `forall`. I'm saying that the current definition of `Storable` doesn't do anything to protect you from writing data Two = Two Int Bool instance Storable where sizeOf (Two i b) = sizeOf i + sizeOf b and I don't see how Dependent Haskell would help.
Ah I actually thought you were OP at first so was trying to get clarification.
If we replaced `Storable` with {-# language DependentHaskell #-} class Storable a where sizeOf :: forall a -&gt; Int ... would I be protected from writing? data Two = Two Int Bool instance Storable where sizeOf (Two i b) = sizeOf i + sizeOf b 
Interesting challenge... how about *associated constraints*? class Ord :: Type -&gt; Constraint where constraint Ord a = Eq a (&lt;=) :: forall a. Ord a =&gt; a -&gt; a -&gt; Bool The *associated constraint* would be like a type family with return kind `Constraint` except that you would allow foralls on the right hand side? So similar to: class Constraint Ord a =&gt; Ord a where type Constraint Ord a :: Constraint type Constraint Ord a = Eq a ... "Going well beyond what we know"
Yes that wouldn't be possible, the argument is the type not the term. It would look more like this sizeOf Two = sizeOf Int + sizeOf Bool just like we could write visId :: forall a -&gt; a -&gt; a visId a (x :: a) = x but not pattern match on `a` without a `pi a -&gt; ..` (aka `Typeable`)
I'm not too sure exactly what you mean by this, i'm still very new to this kind of stuff and really want to try it out but i just don't seem to be getting anywhere. I think i did some of what you recommended and am getting a new error: undefined reference to \`ZCMain\_main\_closure' Also i put what i'm working on up on GitHub here if its easier to read: [https://github.com/Dylaann/Haskell-Project/tree/master/Demo%20Vscode](https://github.com/Dylaann/Haskell-Project/tree/master/Demo%20Vscode) I'm currently working on this as part of my 4th year thesis and it'l all be documented since i cant find anything similar anywhere. So any help will get a major thank you and reference at the end 👌 &amp;#x200B; &amp;#x200B;
There's a \[similar one for Liquid Haskell\]([http://goto.ucsd.edu/\~nvazou/real\_world\_liquid.pdf](http://goto.ucsd.edu/~nvazou/real_world_liquid.pdf)). Most interesting to me was the verification of parts of the text package. &amp;#x200B; There's also a \[verification of stream fusion\]([https://www.isa-afp.org/entries/Stream-Fusion.html](https://www.isa-afp.org/entries/Stream-Fusion.html)). The core concept is used in many libraries these days though the verification gap is considerable.
Alright, You were close! The reason for that error is because you put the `-no-hs-main` in the wrong place. You put it when generating the object file, (where it'll have no effect) instead of when you're creating the final executable. The following should work, see comments inline for why. ``` # Makefile #INCLUDE_PATHS specifies the additional include paths we'll need INCLUDE_PATHS = -I"C:\SDL2-2.0.3\x86_64-w64-mingw32\include\SDL2"\ -I/iconv\ -Wall\ #LIBRARY_PATHS specifies the additional library paths we'll need LIBRARY_PATHS = -L"C:\SDL2-2.0.3\x86_64-w64-mingw32\lib" # Since you're using a GUI application that uses WinMain you need to add # libmingw32 as a dependency and -optl-mwindows tells it to use the Windows # subsystem instead of the console one. You had a similar option before as # a compiler flag, but it's a linker flag! # # Lastly we add -optl-Wl,-allow-multiple-definition due to an issue where the # RTS exposes a symbol that is also exposed by libmingw32. In this case fpreset. # it doesn't matter which one is used so we'll tell the linker to pick the first one. # This is not ideal, but can't think of anything better right now. LINKER_FLAGS = -lmingw32 -lSDL2main -lSDL2 -lstdc++ -optl-mwindows -optl-Wl,-allow-multiple-definition COMPILER_FLAGS = -w # -no-hs-main needs to be here output: move.o circle.o game.o main.o ghc -no-hs-main main.o game.o circle.o Haskell/move.o $(INCLUDE_PATHS) $(LIBRARY_PATHS) $(LINKER_FLAGS) # no need for LINKER_FLAGS and LIBRARY_PATHS here as no linking will take place due to # the -c option being given. main.o: main.cpp g++ $(INCLUDE_PATHS) -c main.cpp game.o: Game.cpp Game.h g++ $(INCLUDE_PATHS) -c game.cpp circle.o: objects/Circle.cpp objects/Circle.h g++ $(INCLUDE_PATHS) -c objects/Circle.cpp # no -no-hs-main here since linking isn't being done, so it has no effect. move.o: Haskell/Move.hs; ghc $(INCLUDE_PATHS) -fforce-recomp Haskell/Move.hs -c .PHONY: clean clean: ; del *.o *.hi *_stub.h *.exe ```
Thank you!
I mean go to a CS101 class. Count them. Try to help them with their final project. Something like half just don't get it. They put in the effort, that want to learn it. And they don't. I mean really this is just noting that by young adulthood people have clear talents or lack there of. And yes that is unfortunate when those talents are needed, but uh, it is a fact of our current education system, and one we don't know how to fix. Not everyone can be a foot ball star, not everyone can be a coder. Now like i think the gender gap is basicly entirely explained by girls not taking high school cs sciences. So this is not a reasonable reason to discriminate on protected or irrelevant traits. 
In Haskell, it would be Arrow. In general, the rule is that you get the following functions: swap :: (a # b) &gt;~r~&gt; (b # a) (***) :: (a &gt;~r~&gt; b) ~&gt; (c &gt;~r~&gt; d) -&gt; ((a # c) &gt;~r~&gt; (b # d)) lambda :: (U # a) &gt;~r~&gt; a lambda_i :: a &gt;~r~&gt; (U # a) alpha :: (a # (b # c)) &gt;~r~&gt; ((a # b) # c) The laws are about what you'd expect.
In addition to Chris's detailed answer, I'll give the boring nontechnical answer: * A 1TB SSD costs 150$ these days. * I've been working daily in Haskell for many years and have a 42 GB `.stack` directory. * If it continued growing at the same rate, I could work &gt;80 years on this SSD. * The cost of this stack directory is 6.30$. From an engineering perspective of course I like it smaller, but it hasn't really impacted my life so far.
Ah I see. I find those "bugs" pretty benign though. It usually endup being "I didn't provide the capabilities I wanted to" coupled with an ugly error message. Not the more insidious "I thought my code was gonna do one thing but it did a bad thing."
I compiled the file attached to that ticket with `-O2`. On my machine, the `Data.Array` version took: real 0m0.090s user 0m0.070s sys 0m0.009s Whereas the `Data.Array.Diff` version took: real 0m0.639s user 0m0.597s sys 0m0.015s That's a pretty big difference!
since your notes are jpg scans, consider using this script https://gist.github.com/lelandbatey/8677901 to reduce size
Weakness in terms of expressivity is not necessarily a bad thing. People forget that expressivity comes with a price, always. It is not free.
I have learned to never buy tech books on the kindle for exactly that reason. 
transfer the kindle file out then convert it to pdf using Calibre
I like this for the same reason I much prefer 'where' to 'let'.
Definitely makes sens! :) Though maybe you are lucky to have your projects spread less across different snapshots. My `.stack` folder plus the folder with all haskell projects is about 85 GB and it seems like none of the snapshots have stayed longer than couple years on this PC. I don't really mind spending 100$ on an SSD. But at this moment I happened to have only 400GB drive from four years ago, and there are, you know, other files too. So now I have only 50GB of the ssd left, and I have to go buy new drive and reconfigure my system to move the home folder there, which is not super convenient. But my overall point is if we continue to follow the same philosophy and ignore the space usage, we might end up with a dev kit requiring 1TB in a few years, so your linear math would not apply :)
what do you want to do? like update dependencies via `stack solver`?
&gt; I think to get better space re-use you'd need to use Nix. I imagine the two 12.0 and 12.20 package databases are direct copies of most of the files in there, which is not great re-use. I believe better space re-use is one of the things cabal new-build excels at.
Aggree ... was ment more as a "are there type constraints on f which could make it work with the semantics described above?" - so basicly one or more of the type classes of which `Maybe` is an instance.
You can use `-fno-code` if you just want to type check you program. Have a look at ghcid
Yes. "Bug" was the wrong word. Maybe I should have said "incorrect (but not unsafe) code".
Are you talking about generating a `pkg.cabal` from the `package.yaml` file? I believe `stack` does this internally with the [`hpack`](https://github.com/sol/hpack) tool.
I see. That's neat.
Is there anyway I can 'pre-build' a set of packages? Like I've got sufficient storage space, and 'idle' time' to build packages. Like ideally I'd imagine there would be way to pre-build a percentage of the most popular packages? By 'pre-building' I mean the part in stack build where it goes: ``` x build x configure x copy/register. ```
/u/MeinLambda has been deleted. Let's see how long will they take to reappear with a new account...
have you tried that with other books, and it worked?
i ask because I've never bought kindle books for the reason OP mentions, but if that is a reliable workaround then it would make things much easier!
I'd jump into a repl with `cabal repl` (preferably `cabal new-repl`). It's much faster to rebuild code in the repl since it doesn't do linking or optimizing.
Packages from your resolver are cached in your `~/.stack`, so just build the packages you want to "pre-build" and next time you build a package which depends on them, stack will reuse the binary artifacts. But only if you use the same resolver.
stack build —dry-run
What do you mean by "heterogeneous equality"? That term usually refers to a type of proof object, not to a function which examines two values and returns True or False. Proof assistants like Agda try really hard to make sure you can't construct proof objects for false statements, so as long as there isn't any bug in your proof assistant, you shouldn't be able to construct such a proof object for unequal values. This applies whether you use homogenous or heterogeneous equality. Haskell isn't a proof assistant, as it allows you to construct a "proof" of any false statement using e.g. `undefined`.
If you're using the explicit import list style (e.g. `import Data.Monoid (mempty)`), GHC already suggests you in its warnings and error messages which imports are unused and which ones you should add. Haskell IDEs (e.g. intero) often have a key combination to apply GHC's suggestions automatically. This only works if the module is already imported though; if you already have `import Data.Monoid (mempty)` and you use `mappend`, GHC will suggest to add `mappend` to your `Data.Monoid` import list, but if you are not yet importing `Data.Monoid`, GHC won't try to guess which module you need to import.
Thank you all for helping. Here is how I solved it: &amp;#x200B; 1. I added swagger schema instances to appropriate API entities 2. I created a separate module **src/Swagger.hs** with following content `swaggerDoc :: Swagger` `swaggerDoc = toSwagger serverApi` `&amp; host ?~ "api.github.com"` `&amp; info.title .~ "GitHub Gists API"` `&amp; info.version .~ "v3"` `main :: IO ()` `main = BL8.writeFile "swagger.json" $ encodePretty swaggerDoc` 3. I run this command while compiling: `stack -- runghc src/Swagger.hs` 4. So I get swagger specs in the **swagger.json** file after the step #4 &amp;#x200B;
Which edition of CIS194 did you end up following?
Incidentally I chose the last variant, no arguments at all, as one of the first examples for type classes in my “Haskell for Readers” course. It might have been a bit bold, but on the other hand, nothing drives the point, that type classes are type-selected (and not value selected) home better: http://haskell-for-readers.nomeata.de/#type-driven-code-synthesis
Fall 2016. The one from the Haskell official website 
Please no. I want to keep that syntax available for when Haskell has row/record/variant support. `foo.x` should be a getter for the `#x` Symbol for a record / record-like thing on `foo`.
I keep Haskell space usage down on my disk by primarily using Nix and automatically running `nix-collect-garbage` every night. In my opinion, stack and cabal-install both need an equivalent concept.
I guess there is also https://github.com/rhaps0dy/android-haskell-sdl which looks very close to what I want, but has not been touched in four years, and need me to manually install an (old?) crosscompiler and Android studio.
6s seems high for 50 lines, can you share the code? I'd love to take a look.
To be fair, the `undefined` family of bogus proofs can be detected by forcing the proof object (e.g. `case prf of (Refl :: a :~: b) -&gt; ...`). However, you can also get your hands on a bogus proof with something like `unsafeCoerce Refl`, which is a bit more problematic since it isn't actually bottom.
I'm an Android developer by trade, and I would be interested to hear what success you may find. Unfortunately I'm not of much help for this. Is there a way to target the Android NDK with ghc? Again, uncharted territory for me, but there may be tools to help within the NDK.
We had a discussion about this a while back. Here are the answers we gathered: https://stackoverflow.com/questions/5151858/running-a-haskell-program-on-the-android-os Hope that helps! I would probably use Eta.
Perter Norvig has [a couple](https://norvig.com/lispy.html) [of tutorials](https://norvig.com/lispy2.html) on how to write a Lisp in python which are great to start with. Then there is [Built Your Own Lisp](http://www.buildyourownlisp.com/) by Daniel Holden, which goes into more depth on how to implement a Lisp interpreter in C (it's also an awesome way to learn C). Also there is the excellent [Compilers: Backend to Frontend](https://github.com/namin/inc/blob/master/docs/tutorial.pdf) by Abdulaziz Ghuloum which implements a Scheme (a Lisp dialect) also in C. &amp;#x200B; Going even further you probably want to learn more about [compiler design](https://www.goodreads.com/book/show/258558.Modern_Compiler_Implementation_in_ML) and try to implement a subset of the specifications of a language of your choosing!
I can't believe this actually works... It is not as dry as I thought.
So I'm building an app right now. We decided to use Nativescript to make it cross platform, and I've added Purescript to compile down to the JS used in Nativescript. I'm making bindings as I go (using Purescript's Javascript FFI), but so far it's working pretty well. Obviously it's not Haskell, but it's close enough for me, anyway. I've been considering releasing a full set of Purescript bindings for core Nativescript as a library after I've finished this app depending on interest.
As others have said: If compilation is part of your debug loop then consider changing your debug loop. &amp;#x200B; Some people use ghcid to load the code in an interpreter. Others use \`ghc -fno-code\`. For 400 lines, 8000 counting some immediate deps held locally, it takes about one second to load/type check / find a new failure: ``` % time (echo -e ':exit\n' | cabal new-repl 2&gt;/dev/null 1&gt;&amp;2) ( echo -e ':exit\n' | cabal new-repl 2&gt; /dev/null &gt;&amp;2; ) 0.60s user 0.28s system 89% cpu 0.991 total % cloc --hide-rate --quiet --include-lang=Haskell src | grep -v SUM | grep -v -- '---' | grep -v cloc Language files blank comment code Haskell 4 48 9 371 ``` For a more complex project, matterhorn, the time is about 5 seconds: ``` % time (echo -e ':exit\n' | cabal new-repl 2&gt;/dev/null 1&gt;&amp;2) ( echo -e ':exit\n' | cabal new-repl 2&gt; /dev/null &gt;&amp;2; ) 3.91s user 0.54s system 97% cpu 4.559 total % cloc --hide-rate --quiet --include-lang=Haskell src | grep -v SUM | grep -v -- '---' | grep -v cloc Language files blank comment code Haskell 84 1944 2021 11457 ```
try ETA.
&gt; there is no clear way to uninstall cabal packages Sure there is: delete the binary, same as stack. I have both directories on my path, yes. Why would I install a tool twice? That makes no sense. So, there's no conflict. &gt; I'd like to only use stack Maybe not the best idea: due to the nature of stackage snapshots, they often don't contain the newest version of a package, so you may be missing out on new features and (more importantly) bugfixes/security updates.
Ok I'll try this. Forgive me for my beginner ignorance, but following different tutorials on environment setup just ended up with me installing the same package for either build tool. For some reason having `.cabal/bin` in my path is crashing vim. Any ideas on that?
Thanks. I'm using hie in vim with ALE at the moment, which is much better (indeed, linking takes up most of the time). Still, async and all, it is still quite noticeable. I'll try doing it only in save and that should be good for now :)
If tomorrow I'll remember, sure. Although it's probably terrible code
If you want a quick way to try the GHCJS version running on android using jsaddle-clib (the jsaddle runner used by obelisk on android). 1) [Fix runInBrowser since it seems like it is not JSM on GHC.](https://github.com/nomeata/kaleidogen/pull/1) 2) Then do `ob init` to make an entirely new package kaleidogen-ob or something. 3) Add `kaleidogen` as a build dependency in the cabal file. 4) Add the following as the code for you main widget: postBuild &lt;- getPostBuild performEvent_ $ postBuild $&gt; runInBrowser renderDNA mainProgram 
I can be done better than what Cambridge Press did with the Kindle version, but this is partly why the ereader version of the [Haskell Book](http://haskellbook.com) is a PDF in a layout designed around the form factor of a Kindle. For an idea of what it looks like: https://twitter.com/bitemyapp/status/666838424821743616 I've made prototypes of an ePub variant (that I later yanked, they were inspired by some collaboration I was doing with someone that volunteers with the visually impaired) but it was just too janky and broken to be usable for sighted people.
What stops you from using non-snapshot packages with stack? I don't get the last argument.
If you just want to use stack, just use stack. Don't put ~/.cabal on your PATH, and add `export PATH=$HOME/.stack/programs/.../bin:$PATH` whenever some tool needs `ghc` to point to ghc. Works fine for me.
&gt; I've been considering releasing a full set of Purescript bindings for core Nativescript as a library after I've finished this app depending on interest. This sounds great, I'd be interested!
hdevtools doesn't work with stack. It seems to fail. 
I've been having issues trying to get hdevtools to work with nvim. Are you using stack or cabal to install hdevtools? I'm using cabal, and having `~/.cabal/bin/hdevtools` in my path crashes nvim when opening a `hs` file.
cabal, but I had it working with stack as well. Crashing in nvim suggests a problem with nvim. Try updating to more recent version or filing a bug report with them. How do you integrate those two together? I'm using ale.
I think the content here is probably good, but I found the examples quite difficult to follow, where are the exceptions coming from? I just see a bunch of threadDelays, maybe a quick editing pass assuming you know nothing about the problem would help with comprehension 😄
Thank you for the comment! Yes, I did not elaborate to show the program flows well. I'll try to fix it here. For instance, the code from the *Model 1* should read import Control.Exception import Control.Concurrent import Control.Concurrent.Async asyncTask1 = async $ do result &lt;- return 10 `catch` (const $ return 20 :: SomeException -&gt; IO Int) uninterruptibleMask_ (threadDelay 2000000) &gt;&gt; print result main = do a1 &lt;- asyncTask1 threadDelay 1000000 throwTo (asyncThreadId a1) ThreadKilled wait a1 Others are similar. I was talking about *asynchronous* exceptions, so they're coming from the main thread with the *throwTo ... ThreadKilled*.
Ahh, that makes a lot more sense, I was very confused as to why it would possibly hit the catch block, thanks!
So I did some digging and it looks like there is an open issue in hdevtools. The problem is that it doesn't work for the latest version of GHC (the nightly 8.6.0 release). So I could consider going to the LTS, which is probably what you're on? `$ stack ghc -- --version` `The Glorious Glasgow Haskell Compilation System, version 8.6.3` There isn't much of a benefit for myself as a beginner to be on bleeding edge, so maybe I'll try that. Right now, yes, I'm using `ale`. I've [pretty much followed this guide](https://mendo.zone/fun/neovim-setup-haskell/) and I'm liking it so far. However I'd **really** enjoy an auto-import plugin - which needs `hdevtools`. I'll most likely wait a bit and see how I feel about imports, and based on that rollback to the lts version of ghc. 
Actually it works with 8.6.3 - there are two pull requests. If all you care is 8.6.3 you can just check out code from https://github.com/hdevtools/hdevtools/pull/87, compile and install it. Maybe I should spend some more time to actually get this merged...
I haven't been able to really figure out the issue with `nvim`. If you're interested [here is a pastebin](https://pastebin.com/W0Na7tUE) of the log output for the crashes.
It's not nvim itself it's syntactic and it's crashing it's throwing error. Unfortunately I'm not familiar with syntactic's internals enough to help you here.
I'm trying to implement the code for my thesis that I wrote in C++ in Haskell. I took a class about Functional Programming and Haskell last semester and I think I understand a lot of the things that it can offer, however, I feel like it offers "too much" and I don't know which tool is best to use... &amp;#x200B; I was hoping you guys could guide me in the correct direction : &amp;#x200B; Here's what I'm doing in my C++ code: &amp;#x200B; Step 1: &amp;#x200B; I have a custom stack of unsigned int 16, with a maximum stack size of 4, together they are combined in a specific way to represent a unique unsigned int 64. The first step of the program is to iterate over all 2\^64 numbers using the mentioned representation which allows for efficient pruning in the context of the search, i.e, given predicate T, for a given stack S the next number might either be "Increase the top of stack" or "Push a new value to the stack" or "Pop the current value and increase the new top". &amp;#x200B; For the sake of efficiency, I also keep a record of certain properties of the previous state of the stack so that when I pop the top of the stack I don't need to recalculate certain values needed for the predicates I need to evaluate. &amp;#x200B; &amp;#x200B; Step 2: &amp;#x200B; Given the result of step one, I "reshape" the unsigned int 64 into a matrix of 4x16 and I need to look at each individual column. I have a function of type \` Column -&gt; \[Column\] \` , and now given the new representation \` \[Word16\] \` , I want to write a function \` \[Word16\] -&gt;\[\[Word16\]\] \` that gives me all the possible combination of a state. &amp;#x200B; So for example, if I have \` \[5,0,0,1\] \` and \` f 5 = \[1,2\], f 0 = \[0\], f 1 = \[3,4\] \` then \` g \[5,0,0,0\] = \[\[1,0,0,3\],\[1,0,0,4\],\[2,0,0,3\],\[2,0,0,4\]\] \` . &amp;#x200B; It'd be great if you guys could give me tips on what to avoid and what containers to use. &amp;#x200B; For example, it feels like for the first step I could write a record that keeps the stack and its properties and do all the calculations in the state monad (a,s) where a is the previous record and s is the current one. The second step screams to me nondeterminism and list monad. &amp;#x200B; But I'm not sure if these things are the best tools to use when dealing with "brute force of huge data sets", I'm sure there are better containers to use or better techniques to apply. &amp;#x200B; My C++ code is not restricted to the size of the unsigned it and will work for any size of unsigned int, my goal in implementing this in Haskell is to make it as general as possible, remove all the specific constraints of the current problem and make the code as polymorphic as possible. &amp;#x200B; &amp;#x200B; &amp;#x200B; &amp;#x200B;
Thanks, that helps! One adjustment to keep it in sync with other logging is that the logFatalError function should just call the same log function that is used to log non-error messages, to make sure it gets routed to the same place, but pass it a severity level to indicate that it's an error.
GitHub README: https://github.com/chessai/hedgehog-classes#readme
I'm interested too!
Tnx, but it don't think it will handle squared paper
I personally don't care about potential duplication. But I then, I don't generally install executables/packages globally using stack. Aside from stack itself (using the `upgrade` command). But `stack upgrade` is smart enough to realize that my `.cabal` dir comes first on my path and acts accordingly. That's probably a special case though. As for uninstalling, the method for both tools is basically `rm -r`. Or just `rm` for executables. At least, I'm not aware of cleverer mechanism than that. I can see how this could cause confusion if you install things using both. If you're forced to do this a lot, the least awful choice might be to replicate something similar to the `update-alternatives` symlink machinery. 
Nice! I'll try this one out when opportunity arises.
Is this a known issue? Readme seems to mention stack install method, so I think it is expected to work. Happy to help if you give more details.
What I have done so far with some degree of success is to treat the Haskell part as a library. Roll everything you need (including your dependencies) into a static library, and the build the actual application chrome, interaction, etc. in the tools provided for the platform (here Kotlin and Android Studio). - The most integrated option is probably Keera studios Andronoud. - The most reflexy thing obsidian.systems obelisk - The most native option Eta - The most direct way using one or more (arm, aarch64, x86_64, ...) ghc cross compiler with cabal and an android toolchain and treating the result like any c library. Then performing the rts initialization and target specific UI using the default/native language there. 
hdevtools or ghcid can give instant feedback for type errors at least on much larger codebases. ghci + :reload allow to test some logic without recompiling all the things.
I usually `ReaderT`, but I once made an experiment and have designed a couple of services using the approach that is called "Tagless Final" in this article. The experience of writing these apps was super exciting! Everywhere in my app I could add a constraint like `MonadBlah m =&gt;` which would give me `blah`, very convenient! I also found handy that I could run my tests in a different monad (even in `Identity`) so the code looked very test-friendly (at the cost of having separate `MonadBlah` instances, which is fine). So I was happy with the experience. Until some time passed and I had to maintain these solutions. Maintaining these apps is not as pleasant as I expected. The points that I found inconvenient are: 1) Unnecessary indirection. Every time I want to look at some `blah`, I am finding that it is a member of some `MonadBlah`, now I need to go and figure out which instance is used, and that's where I find the actual code. Not major, but annoying enough. 2) Separation of concerns is harder. Let's say I need to add some new functionality or just a function. Do I add it to an existing `MonadBlah` class or to some new one? How do I group functions into classes? Having something like `class MonadStore { read; write; }`? Now I find myself solving the same old "OOP" problem of how to group methods into an object, which I was happy to avoid doing FP. I am not used to these arbitrary lawless "grouped" concerns anymore... 3) Additionally, what if I sometimes want to provide "read" capability to a function, but make sure that it can never use the "write" one? Now my `MonadStore` is just too fat, and it is falling apart into `MonadReadStore` and `MonadWriteStore`. So I ended up with many classes with only one operation. And, realistically, each of them would have only one instance in the entire application. One very smart guy once made a joke when discussing a similar issue: "How do you call a type class with only one member? You call it 'a function'" :) 4) Lots of constraints like `MonadFoo m, MonadBlah m, MonadFizz m` etc. (in my case not all of them would start with `Monad` prefix, but it isn't important) didn't look pretty (not completely ugly too, I admit). But it became worse when I started aliasing these constraints, like `MonadStore = MonadReadStore + MonadWriteStore`. These aliases are bad because they just added yet another step in my quest of finding things without adding any clarity. Now I had to remember what these aliases mean, so I stopped doing it very quickly. Now I have returned to the `ReaderT
I usually use `ReaderT`, but I some time ago I made an experiment and have designed a couple of services using the approach that is called "Tagless Final" in this article. The experience of writing these apps was super exciting! Everywhere in my app I could add a constraint like `MonadBlah m =&gt;` which would give me `blah`, very convenient! I also found handy that I could run my tests in a different monad (even in `Identity`) so the code looked very test-friendly (at the cost of having separate `MonadBlah` instances, which is fine). So I was happy with the development experience. Until some time passed and I had to maintain these solutions. Maintaining these apps ended up being not as pleasant as I expected. The points that I found inconvenient were: 1) Unnecessary indirection. Every time I want to look at some `blah`, I am finding that it is a member of some `MonadBlah`, now I need to go and figure out which instance is used, and that's where I find the actual code. Not major, but annoying enough. 2) Separation of concerns is harder. Let's say I need to add some new functionality or just a function. Do I add it to an existing `MonadBlah` class or do I create a new one? How do I group functions into classes? Having something like `class MonadStore { read; write; }`? Now I find myself solving the same old "OOP" problem of how to group methods into an object, which I was happily avoiding doing FP. I am not used to these arbitrary lawless "grouped" concerns anymore... 3) Additionally, what if I sometimes want to provide the "store read" capability to a function, but make sure that it can never use the "store write" one? Now my `MonadStore` is just too fat, let's breal it into `MonadReadStore` and `MonadWriteStore`. So I ended up having many classes, each with only one operation. And, realistically, each of them would have only one instance in the entire application. One very smart guy once made a joke when discussing a similar issue: "How do you call a type class with only one member? You call it 'a function'" :) 4) Lots of constraints like `MonadFoo m, MonadBlah m, MonadFizz m` etc. (in my case not all of them would have `Monad` prefix, but it isn't important) didn't look pretty. Not completely ugly too, I admit. But it became worse when I started aliasing these constraints, like `MonadStore = MonadReadStore + MonadWriteStore`. These aliases were bad because they just added yet another step in my quest of finding things without adding any clarity. Now I had to remember what these aliases mean, so I stopped doing it very quickly, and continued with a pile of constraints. Now I have returned to the `ReaderT` approach. It feels more straightforward and simple, IMO. I use `generic-lens` to access my `Env`, which provides me with `HasType` and `HasField` instances "for free" and there is no need in manually declaring classes like `HasLog` and `HasBalance` and their instances. Which is a win :) Initially I thought that testing would be a strong point for having these monads around, but I found that it wasn't that much better (if any better) than just providing a different `Env` to my reader in tests. So now I am happily use `ReaderT` again :) I am not writing this text to show that `ReaderT` is "better" than `Tagless Final` (although this was the question asked in the original article). I just thougt that my personal experience would be somehow relevant to this topic.
It seems that the `concurrently_`s can't be expressed in the higher-level interface of the "business logic". They can only be called once a concrete monad is selected. `ReaderT` over `IO` lets you easily use `bracket`, launch threads, and the like. My question as a Haskell hobbyist is: how often is that required for "business logic" in real apps? My impression from the world of Java Spring applications is that explicitly launching threads or explicitly handling transactions is discouraged. Instead, some AOP mechanism is used, usually controlled through method annotations like [@Transactional](https://docs.oracle.com/javaee/7/api/javax/transaction/Transactional.html). Exceptions from the persistence layer [aren't usually caught](https://stackoverflow.com/a/10673199/1364288) and just bubble upwards through the business logic.
In my case, the UI is part of the Haskell code though, and not just the logic, so the last approach wouldn't work so well. Also, I am not very keen on having the learn about C programming for Android ;-)
&gt; Initially I thought that testing would be a strong point for having these monads around, but I found that it wasn't that much better (if any better) than just providing a different &gt; Env &gt; to my reader in tests. But that `Env` would be now tied to a specific implementation, wouldn't it? It would have a database connection for example. I wonder I Backpack could be useful here for abstracting components of the `Env` record.
The `Env` can contain "services" if I want it. Just like in that example with `envLog :: !(String -&gt; IO ())` I can have an `Env` that does nothing on logging... I don't often go that far though. In the real life my `Env` usually looks like: data AppEnv = AppEnv { options :: Options , awsEnv :: Env , statsClient :: StatsClient , logger :: Logger , metrics :: Metrics } deriving (Generic) and I can provide "empty" implementations for `StatsClient` and `Logger`. Note that there is `awsEnv` there, which is not easily replaceable. You could add `send` function to `AppEnv` (similarly to how `envLog` above is done), and it works, but for the cases where you call `send` explicitly, so inconvenient... It kind of forced me into a "good practice" (as I see it at least) of separating my effects (including going to AWS databases and stores) from my business logic. Like, `loadFeed :: (MonadIO m, MonadReader r m, HasType Env r) =&gt; S3Uri -&gt; m ByteString` would be one function, and operating on that `ByteString` would be another function, which I can test separately if I want. Or even if it is something like: processFeed :: (MonadIO m, MonadReader r m, HasType Env r, ...) =&gt; S3Uri -&gt; m FinalResult processFeed uri = do dataFromAws &lt;- someHardcoreAwsOperationHere processData dataFromAws then it usually does the job because I can test that `processData`separately. I am typically doing those two last approaches (first one more often), works well enough for me. 
So when you run `stack install hdevtools` you see this output: ``` $ stack install hdevtools hdevtools-0.1.7.0: configure hdevtools-0.1.7.0: build -- While building package hdevtools-0.1.7.0 using: /home/drew/.stack/setup-exe-cache/x86_64-linux/Cabal-simple_mPHDZzAJ_2.4.0.1_ghc-8.6.3 --builddir=.stack-work/dist/x86_64-linux/Cabal-2.4.0.1 build --ghc-options " -ddump-hi -ddump-to-file -fdiagnostics-color=always" Process exited with code: ExitFailure 1 Logs have been written to: /home/drew/.stack/global-project/.stack-work/logs/hdevtools-0.1.7.0.log Configuring hdevtools-0.1.7.0... Preprocessing executable 'hdevtools' for hdevtools-0.1.7.0.. Building executable 'hdevtools' for hdevtools-0.1.7.0.. [ 1 of 14] Compiling Daemonize ( src/Daemonize.hs, .stack-work/dist/x86_64-linux/Cabal-2.4.0.1/build/hdevtools/hdevtools-tmp/Daemonize.o ) [ 2 of 14] Compiling GhcTypes ( src/GhcTypes.hs, .stack-work/dist/x86_64-linux/Cabal-2.4.0.1/build/hdevtools/hdevtools-tmp/GhcTypes.o ) [ 3 of 14] Compiling FindSymbol ( src/FindSymbol.hs, .stack-work/dist/x86_64-linux/Cabal-2.4.0.1/build/hdevtools/hdevtools-tmp/FindSymbol.o ) [ 4 of 14] Compiling Info ( src/Info.hs, .stack-work/dist/x86_64-linux/Cabal-2.4.0.1/build/hdevtools/hdevtools-tmp/Info.o ) /tmp/stack4716/hdevtools-0.1.7.0/src/Info.hs:215:67: error: Not in scope: type constructor or class ‘GHC.PostTc’ Module ‘GHC’ does not export ‘PostTc’. | 215 | postTcType = const (stage&lt;TypeChecker) :: GHC.PostTc TypecheckI GHC.Type -&gt; Bool ``` Which is the [same bug reported on their issues tab](https://github.com/hdevtools/hdevtools/issues/85). I've installed with cabal instead, but I'm having issues with getting to work with nvim. There seems to be a [conflict with syntastic.](https://pastebin.com/W0Na7tUE)
I'm not sure how exactly hdevtools mechanics are done, but would it maybe work if hdevtools itself would be built via GHC 8.4? E.g. if you do `stack install --resolver=lts-12.26 hdevtools-0.1.7.0`, would it work ?
&gt; stack install --resolver=lts-12.26 hdevtools-0.1.7.0 This attempts to install another version of ghc.
Yeah, that's what I suggested :) stack works ok with multiple GHCs, for example my work project is on 8.4, while hobby projects are 8.6. unless you have some disk space constraints, should be ok
How is this version less safe?
Sure, I added a paragraph to the start of [https://github.com/digital-asset/ghc-lib#creating-ghc-lib](https://github.com/digital-asset/ghc-lib#creating-ghc-lib)
Some pain, rather than much pain. I have tried to spell it out more clearly in [https://github.com/digital-asset/ghc-lib#readme](https://github.com/digital-asset/ghc-lib#readme)
r/javascript is that way &lt;--
Thanks! That looks like a good start, although I am stuck at running `ob`: https://github.com/obsidiansystems/obelisk/issues/335#issuecomment-462173608 In the long run it seems a bit wasteful to go through `jsaddle-clib` and a full webbrowser engine when all I need is a window to use OpenGL on, but it might indeed be the quickest way to start.
thanks
https://github.com/Jyothsnasrinivas/eta-android-2048 looks promising too. I wonder if anyone has run SDL applications on eta on android yet.
Or maybe I can skip SDL and use eta’s FFI to create an [OpenGL surface](https://developer.android.com/guide/topics/graphics/opengl) and call the OpenGL function calls directly. I assume the Haskell OpenGL bindings won’t work out of the box with eta on Android, or will they?
&gt; iterate over all 2^64 numbers That doesn't seem tennable. If each number took but a single cycle, at 4GHz that would take approximately 2^32 seconds which is more than a century.
&gt; Maybe not the best idea: due to the nature of stackage snapshots, they often don't contain the newest version of a package, so you may be missing out on new features and (more importantly) bugfixes/security updates. Can always specify a newer version in your `extra-deps`!
The pruning is very efficient and the calculation is over within seconds for the 64 bit variant and takes barely a few minutes for the 128 bit variant. Each pruning step skip over at least 2^16 numbers. If you're interested we can discuss about the search more in private but I'm more interested in taking about proper implementation methods in Haskell. 
ETA has a slackand gimmer you can ask on. there is also an android template project you can use. dev group is fairly committed to android it seems.
OP: In general, whenever one of the arguments in the type signature you're hoogling is wrapped in `m`, remove that wrapping. `&gt;&gt;=` is usually supposed to connect monadic values.
I am reading [https://haskellbook.com/](https://haskellbook.com/) and made it to Chapter 12, but got puzzled by exercises, such as: &gt; Write a recursive function namedreplaceThewhich takes a text/string,breaks it into words and replaces each instance of “the” with “a”. &gt; replaceThe :: String -&gt; String it is "recursive function" and "breaks it into words" which confuse me. To break it into words I'd need to pass state between recursive calls, but type signature leaves no space for it. I could implement it using foldr, but then it wont be a recursive function. So far it is best I've got, enough to pass suggested test and with extra pattern matches can be taught to catch edge cases, such as "the" at the beginning and and the end of string, but it feels like cheating and completely not what authors wanted me to do, so I am missing something I can learn: ``` replaceThe :: String -&gt; String replaceThe [] = "" replaceThe (' ' : 't' : 'h' : 'e' : ' ' : xs) = ' ' : 'a' : ' ' : replaceThe xs replaceThe (x:xs) = x : replaceThe xs ``` as you can see it totally misses "break it into words" part of the task, but I just can't figure out how to do that, while still being a recursive function. Any suggestions how to make it proper?
Now that you explained, I do agree that I wish we could \`AllowAmbiguousTypes\` on a per-definition basis.
A note for the author; UndecidableInstances isn't so scary 😋. The code is deterministic and will either fail completely or behave as written, it's IncoherentInstances that is perhaps worth thinking twice about 😄
I am learning Haskell and I have some trouble with infinite lists. There are 2 exercises specifically that despite the fact that I know how to solve them, I feel like my solutions are not optimal. So I figured I would ask for help here, any suggestions are appreciated. The first exercise is to write a function that given numbers k and l generates the infinite list of all numbers in the form of (k ^x) * (l ^y) such that the list is sorted and contains no duplicates. The second exercise has almost the same conditions : this time given k and l the numbers should be in the form of (x ^k) * (y ^l).
At Obsidian, we just ban the use of the RecordWildCards extension altogether. It brings things into scope without giving anyone who has to read the code any idea of what was brought into scope where. That information is too important to be left implicit, is intensely confusing to anyone coming on to a project, and can even trip up people who are familiar with the code. As you try to refactor things in the presence of RWCs, it's not always obvious when you're going to cause new shadowing to occur. It also makes editing the definitions of the records themselves **highly dangerous**, because you might cause RWCs in another library downstream of the one you're working on to bind new variables that shadow existing ones. At least go with NamedFieldPuns, which have fewer such issues because they explicitly name the variables to be bound -- despite the intrinsic ugliness of shadowing the field selector.
The problem is that we have a lot of existing RWC code and the opinions are divided. We have been removing RWCs in places where clearly not necessary but just removing it all together isn't too viable. Just for simple `{..}` pattern, we have more than a 1000 occurrences. git grep {\.\.} src | wc -l 1375
What is with these unsolicited opinions. No one cares dude.
It might be worth trying to systematically replace those with complete NamedFieldPuns to start (just by doing search and replace).
It's too late to ban it now but we are trying to use less of them. I'm not sure how dangerous they are actually - I haven't seen any problems caused by shadowing as you describe it. At most passing stuff around that's not really used by anything else.
There are some datatypes with 100-200 fields...
Oh boy
They're occasionally useful, but I mostly avoid them as well.. And anyways, let's encourage this GHC bug bounty regardless of the specific issue. We all want this idea to succeed ;)
I like them a lot though I understand your objections. Haven't been bit by the record name changing issue. I rely on name shadowing warnings to catch other bugs anyway. I find they've made a lot of code a lot more readable (less noise, less proliferation of throwaway names). This might depend on the size of the codebase though.
Oh, don't get me wrong, I like the idea of the bug bounty in general, and given that features are in the compiler, I think they ought to work well.
There really is not much difference between `Environment -&gt; Result` and `Environment =&gt; Result`. However, as Haskell does not have dependent types the type class approach is better because you can do things like `Environment r =&gt; r` or `Environment r =&gt; EnvironmentResult r` with type families. Currently you cannot do `forall (e : Environment). EnvironmentResult e` in Haskell.
One possible way you could go about it... there are two functions called words and unwords that do the whole breaking a string into a list of strings: words "hello world" ["hello","world"] unwords ["hello","world"] "hello world" Given that you could implement replaceThe as follows: replaceThe :: String -&gt; String replaceThe "" = "" -- base case for empty string replaceThe "the" = "a" -- base case for the string we want to change replaceThe input = (unwords . go . words) input where go [w] = [w] go ws = fmap replaceThe ws -- recursive call if our words function didn't return a singleton list, i.e., just one word. To be honest I think it's a pretty convoluted way of implementing this functionality, but it meets the criteria you mentioned. Imho you wouldn't normally bother making this an explicitly recursive function, instead I would just do: replaceThe :: String -&gt; String replaceThe = unwords . fmap go . words where go "the" = "a" go w = w I hope that helps!
Things like `mapConcurrently` are quite useful from time to time. But then you'd probably want to use lifted version of these functions, like: http://hackage.haskell.org/package/unliftio-0.2.10/docs/UnliftIO-Async.html#v:concurrently_ or http://hackage.haskell.org/package/unliftio-0.2.10/docs/UnliftIO-Async.html#v:mapConcurrently
I think these might not be exclusionary. For example if we consider the current record implementation, then `foo.x` would yield field `x` of record `foo`. The
Just read through this, wanted to leave a note, great post! I've run into this in my own code and this kind of analysis through all the options was very useful to read.
I mean these knew extensible records/rows/variants would for sure not work like that, as you would no longer be dumping a getter function into the global namespace in such a bare way. So there would definitely be ambiguity as for whether `foo.x` should look for a global `x` function or an `x` field.
`mapM_` will perform an `IO ()` action for every element of a list and sequence them one after another. You just need to come up with a function `f :: Char -&gt; IO ()`, that prints that character followed by a newline character and the function you want can be easily defined as: g :: String -&gt; IO () g = mapM_ f
I agree with /u/cgibbard that NamedFieldPuns is a way better way to get these advantages. RecordWildCards is a mess
Even worse is when RWCs are used to construct a new value! I just ran into this last week. If the normal use of RWCs to concisely supply names is error prone, the reverse use to consume names is much less intuitive IMO.
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/haskell_jp] [Experiment: ghc bug bounty](https://www.reddit.com/r/haskell_jp/comments/apdtgf/experiment_ghc_bug_bounty/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
Interesting. Can you share any details? Preferably with original names you used?
Here's a fairly simple way to do it: take each character, add a newline to the end, and then concat the results. main :: IO () main = do str &lt;- getLine putStr $ concatMap (:"\n") str
Though the `mapM` comment is right, let's explain what's happening. First, `getLine` gives you an `IO String`, so you'll have to deal with IO. You could either use bind `(&gt;&gt;=)`, or if that is confusing to you, a `do` block (see below). Furthermore, a `String` is but a `[Char]`: to do a sequence of actions on it, you could use, again, `mapM`, but to make clear what happens, let's use `sequence`. map print :: [Char] -&gt; [IO ()] -- transforms a list of chars into a list of actions sequence :: [IO ()] -&gt; IO [()] -- transforms a list of actions into one sequence_ :: [IO ()] -&gt; IO () -- same but ignores each () from print printEachChar :: IO () printEachChar = do s &lt;- getLine sequence_ $ map print s Hope that clarifies it somewhat! Two more things: 1. `print` might not be what you need, perhaps `printf "%c"` might be better. 2. if `sequence` is still unclear... Try to write your own recursive function instead! printEachChar_ :: [Char] -&gt; IO () printEachChar_ [] = fixme -- we've reached the end printEachChar_ (c : cs) = do fixme -- print the char fixme -- recurse! Hope that helps. Sorry if the formatting is bad, writing code on a phone ain't great. ^^"
I prefer NamedFieldPuns, but there are times when I'd like to use RecordWildCards - when all fields *must* be consumed (e.g., serialization). The warning I actually want from RWC is the same as if you were to use NamedFieldPuns and then don't use a binding - e.g., normal unused variable binding warnings.
What are your solutions?
My codebase has a bunch of record types which are subsets of other record types, and we use this in a couple of places (literally two, that I can find). They look like largeToSmall (Large {..}) = Small {..} mkABC A {..} B {..} C{..} = ABC {..} (original names would probably have been fine, but I still prefer not to, sorry)
So basically the same names used in different datatypes? I guess that explains it. I tend to use unique names that's often prefixed with something derived from datatype name itself.
When you need to use `extra-deps` you end up [playing human constraint solver](https://old.reddit.com/r/haskell/comments/anq3of/adding_a_stack_based_project_to_hackage/efwofac/) though.
Great experiment. It worked well for other communities and also provides a way to push usability (since the "industry"-tickets often come from an applied, real-live haskell perspective).
&gt;A 1TB SSD costs 150$ these days. More like 300 dollars. Or, if you have a MBP, 4000 since you'll have to buy new laptop \&gt; The cost of this stack directory is 6.30$. Again &amp;#x200B;
To slightly elaborate on what others have already said, here is how I would address this problem: Step one: Notice that there's not really any point in including `a &gt;&gt;=` inside that function. I may as well do it outside. So instead write f :: Applicative f =&gt; (a -&gt; f b) -&gt; Maybe a -&gt; f (Maybe b) f b = maybe (pure Nothing) (fmap Just . b) (this is pretty much /u/Gurkenglas's observation) Step two: [Hoogle it](https://www.haskell.org/hoogle/?hoogle=+Applicative+f+%3D%3E+%28a+-%3E+f+b%29+-%3E+Maybe+a+-%3E+f+%28Maybe+b%29) to discover that it is `traverse`. (Strangely, [the search](https://www.stackage.org/lts-12.1/hoogle?q=%28a+-%3E+f+b%29+-%3E+Maybe+a+-%3E+f+%28Maybe+b%29) does not work on Stackage Hoogle. There is some difference between the two Hoogle setups but I can't remember what. It's always worth trying both.)
Well as to the first exercise : helper:: Int -&gt; Int -&gt; [Int] -&gt; Int -&gt; [Int] helper x y (z:zs) last = if last /= z then (z:(helper x y (sort ((x * z):((y * z):zs))) z)) else (helper x y (sort ((x * z):((y * z):zs))) z) generatePowers :: Int -&gt; Int -&gt; [Int] generatePowers x y = helper x y [1] 0 The solution to the second is similar uses the same idea but it is far more uglier and slower and I really do not want to post it here.
I don't think this is even worse - there are good warnings when you don't provide all field names. I frequently use RWC to construct values - it's a great compliment to `ApplicativeDo`, letting you applicatively construct a record: do x &lt;- foo y &lt;- foo z &lt;- foo return T{..} is much nicer than T &lt;$&gt; foo &lt;*&gt; foo &lt;*&gt; foo imo.
Don't you need a hard drive *regardless* of which language you use?
Someone saying "buy a bigger SSD" in response to someone complaining about 50GB of cached files is the type of user hostility that makes me use `cabal new-build`
&gt; Haskal hmm
&gt; The downside is that it's easy to get dozens of different package sets built: &gt; Work projects use different resolvers. There's no way around this usually without a large migration. &gt; Open source projects have their different resolvers. `cabal new-build` doesn't cause any of these problems
yes. I converted some of my math books to either ePub or PDF. 
This article is ace: [https://www.parsonsmatt.org/2018/05/19/ghcid\_for\_the\_win.html](https://www.parsonsmatt.org/2018/05/19/ghcid_for_the_win.html)
Two questions--one about typed parameters and declaring instances: 1) I'm working through the Allen/Moronuki Haskell book, and am having problems wrapping my mind around how Functor instances are implemented. I know I'm missing something critical, but can't quite grok it. As an example from the book, you have two data types: &gt;`data Three a b c = Three a b c deriving (Show)` &gt; &gt;`data OtherThree a b = OtherThree a b b deriving (Show)` &amp;#x200B; So two new data types, one with a three-argument type constructor, one with a two-argument type constructor. Both with three-argument data constructors. So far so good. Implement a pretty straightforward Eq (not required, but just for comparison) for both based on the type constructor "signature": &gt;`instance Eq c =&gt; Eq (Three a b c) where` &gt; &gt; `(==) (Three x y z) (Three x' y' z') = z == z'` &gt; &gt; &gt; &gt;`instance Eq b =&gt; Eq (OtherThree a b) where` &gt; &gt; `(==) (OtherThree x y z) (OtherThree x' y' z') = z == z'` &amp;#x200B; And then implement fmap, again, pretty intuitive: &gt;`instance Functor (Three a b) where` &gt; &gt; `fmap f (Three x y z) = Three x y (f z)` &amp;#x200B; So here's where I'm getting lost. In the only "legal" implementation of OtherThree, Functor takes an (OtherThree a), not (OtherThree a b). And the implementation of fmap requires that the f be applied to y and z, but not x, y, and z. Or only z.: &gt;`instance Functor (OtherThree a) where` &gt; &gt; `fmap f (OtherThree x y z) = OtherThree x (f y) (f z)` &amp;#x200B; Given the two-argument type constructor I would've expected it to be: &gt;`instance Functor (OtherThree a b) where` &gt; &gt; `fmap f (OtherThree x y z) = OtherThree x (f y) (f z)` &amp;#x200B; ...but it's not. Or for this also to be legal (implementation of fmap is applying f \*only\* to the third argument 'z': &gt;`instance Functor (OtherThree a) where` &gt; &gt; `fmap f (OtherThree x y z) = OtherThree x y (f z)` &amp;#x200B; Anyway, I spent most of last night just randomly making changes to the Functor argument and fmap implementation until I got it working, and the 1300 page Haskell book wasn't really offering a lot of insight. Would really appreciate if someone would point out what I'm missing. Thanks! &amp;#x200B; 2) Is there some sort of Haskell refactoring tool that people use? Just Googling "Haskell refactoring" I came across references to, for example, HaRe which seems to be defunct. Really I was just looking for a way to rename a given identifier with a quick C-x M-Q Esc Yadda &amp;tc... but didn't find anything.
Someone who is unfamiliar with the codebase will have a MUCH harder time understanding the former, ESPECIALLY if (as is the case here) the field names aren't related to the data structure in some way. You have no way of knowing which fields contribute to `T`. Is it x and y? y and z? It may seem obvious in this example that there would be a warning if any of them were unused, but with real world complexity, you often use these symbols elsewhere in the function, which would mean that you won't get those warnings. I prefer `T &lt;$&gt; foo &lt;*&gt; foo &lt;*&gt; foo` because it tells me a lot more without requiring me to hunt down the definition of `T`.
Problem is, that different size hard drives cost differently - bigger costs more.
See the example from /u/ocharles below. In the real world it's significantly less obvious than that simplified self-contained example.
thanks!
&gt;Someone who is unfamiliar with the codebase will have a MUCH harder time understanding the former, ESPECIALLY if (as is the case here) the field names aren't related to the data structure in some way. I only use this when the field names *exactly* correspond to the data structure. Maybe I should have said: data T = T { x, y, z :: Int } The problem with `&lt;*&gt;` is that the moment things start getting remotely complicated you are forced to start counting which argument is being updated, and then going back to the original data type and counting the fields. Worse, if you reorder the fields for some reason, you might have a type safe updated but you've most certainly changed behaviour - but nothing warned you! OTOH, using record wild cards my code is now safe against data type refactorings. If I re-order fields everything is exactly the same as before (including the order of execution in effects), and if I rename a field I will get two warnings: one for the now un-used binding, and one for the new binding. This will usually be enough information for me to understand that I need to change the name of the binding. &gt;You have no way of knowing which fields contribute to T. Is it x and y? y and z? Recall that we're using `ApplicativeDo` here. That means that none of the bindings can depend on any previous bindings. This means we know that those bindings must be used directly by `T`. It's a shame that you can't see `ApplicativeDo` is being used just from the `do` though.
&gt;In the real world it's significantly less obvious than that simplified self-contained example. I still dispute this, but it's a very subjective statement as to whether or not something is "obvious", and how obvious it is on the obviousness-scale. So here are some real world examples from work. optionsParser :: Parser Options optionsParser = do host &lt;- strArgument (metavar "HOST" &lt;&gt; help "Host to stream to" &lt;&gt; showDefault &lt;&gt; value "localhost") port &lt;- argument auto (metavar "PORT" &lt;&gt; help "Port to connect to" &lt;&gt; showDefault &lt;&gt; value 2019) return Options{..} commandLineParser :: OptParse.ParserInfo Args commandLineParser = OptParse.info ( do env &lt;- OptParse.argument OptParse.auto ( OptParse.metavar "ENV" ) factory &lt;- OptParse.argument OptParse.auto ( OptParse.metavar "FACTORY" ) _ &lt;- OptParse.helper return Args {..} ) mempty requestParser = withObject "putATimeTrack" $ \o -&gt; do timeTrackCreatedAt &lt;- lit &lt;$&gt; o .: "createdAt" timeTrackUpdatedAt &lt;- lit &lt;$&gt; o .: "updatedAt" timeTrackDeletedAt &lt;- lit &lt;$&gt; o .: "deletedAt" timeTrackOrderId &lt;- lit &lt;$&gt; o .: "orderId" timeTrackUserId &lt;- lit &lt;$&gt; o .: "userId" timeTrackTaskId &lt;- lit &lt;$&gt; o .: "taskId" timeTrackDuration &lt;- lit &lt;$&gt; o .: "duration" timeTrackNotes &lt;- lit &lt;$&gt; o .: "notes" timeTrackHasTiming &lt;- lit &lt;$&gt; o .: "hasTiming" timeTrackTimingOverriden &lt;- lit &lt;$&gt; o .: "timingOverriden" timeTrackId &lt;- lit &lt;$&gt; o .: "id" return M.TimeTrack {..} findFids :: [ ( Double, Linear.Point Linear.V2 a ) ] -&gt; Maybe ( Fiducials a ) findFids fids = do -- We expect exactly three fiducial points. Technically this is checked -- in the pattern match below, but being explicit here avoids finding -- all permutations of huge lists. guard ( length fids == 3 ) listToMaybe ( do [ ( d1, fid1 ), ( d2, fid2 ), ( d3, fid3 ) ] &lt;- permutations fids -- Find the correct fid configuration. guard ( isFidConfiguration d1 d2 d3 || isLegacyFidConfiguration d1 d2 d3 ) return Fiducials {..} ) &amp;#x200B;
Property testing for my supposedly lawful instances? Love it &lt;3 Thanks for the announcement!
I've been considering ways to fund Haskell open source work recently, too. Mostly people's willingness to work on something for free is dependent upon their need or interest. The boring work tends to be done poorly. Funding that work could be a good way forward. For small tasks I thought I could use my Monzo bank account, which lets people directly put money into my account (maximum £100) at https://monzo.me/christopherdone ([keybase signature][https://chrisdone.com/monzo.txt)). I thought that might be an easy way for me to say "I'll do that task if you send me £50" or whatever. I have some ideas for crowdfunding bigger things: * Merging Intero with HIE and achieving a high standard of Haskell working reliably well on Emacs, Vim, SublimeText, Visual Studio, and Atom on OS X, Windows and Linux. * Moving HIndent from HSE to GHC's parser. * Making a GHC Haskell tag-preserving interpreter that allows updating running code in-place (aka hotswapping) a la Lisp/Smalltalk/Emacs. But there are a lot of crowdfunding platforms and I haven't picked one to try it out with, and I'm not sure there are enough Haskellers to have a large enough pool of people with disposable income _and_ interest in these projects.
It doesn't sounds like you are iterating over all 2^64 numbers then.
`erd` ([README](https://github.com/BurntSushi/erd)) is a Haskell package that creates pretty diagrams from simple textual descriptions. It's made by [@BurntSushi](https://github.com/BurntSushi), author of `ripgrep` and several Rust tools and packages. He's looking for help maintaining this older project of his, which many people still find very useful. From my judgement, the code is fairly beginner friendly, so if you are new-intermediate in Haskell, understand the concepts involved in the package, and want to get into package maintainership, this might be a good start.
&gt; Given the two-argument type constructor I would've expected it to be: &gt; &gt; instance Functor (OtherThree a b) That's a kind error. `Functor` instances are all on types of kind `Type -&gt; Type`, but you've provided a type of kind `Type`. &gt;Or for this also to be legal (implementation of fmap is applying f *only* to the third argument 'z': &gt; &gt; instance Functor (OtherThree a) where &gt; &gt; fmap f (OtherThree x y z) = OtherThree x y (f z) That's a type error. The `OtherThree` constructor has type `a -&gt; b -&gt; b -&gt; OtherThree a b`. `fmap` in this instance has type `(b -&gt; c) -&gt; OtherThree a b -&gt; OtherThree a c`, so `x` has type `a`, `y` has type `b`, `f` has type `b -&gt; c`, and `f z` has type `c`. So, you've tried to call the `OtherThree` constructor on an `a`, a `b`, and a `c`, where it needs an `a`, and two `b`s.
If you won't post your code, it will be quite difficult for us to provide the assistance you need.
Take a look at `intersperse` function from `Data.List`. You can combine it with `getLine`/`putStrLn` or directly with `interact`.
&gt; To break it into words I'd need to pass state between recursive calls Merge consecutive recursive calls until you know you are back to the initial state. Maybe something like: replaceThe "the" = "a" replaceThe ('t':'h':'e':' ':x) = 'a' : ' ' : replaceThe x replaceThe x = word ++ replaceThe y where (word, y) = {- implementation left as exercise for the reader -} (`word` would contain trailing spaces)
Here is the solution to the second exercise then : helper :: Int -&gt; Int -&gt; [(Int,(Int,Int))]-&gt;Int-&gt; [Int] helper k l (x:xs) prev = if prev /= (fst (head sorted)) then (fst (head sorted)) : (helper k l sorted (fst (head sorted))) else helper k l sorted prev where f = fst (snd x) s = snd (snd x) xUp = calc (f+1) s k l yUp = calc f (s+1) k l ls = [(xUp,(f+1,s)),(yUp,(f,s+1))] sorted = sort (ls ++ xs) generateExponents :: Int -&gt; Int -&gt; [Int] generateExponents k l = helper k l [(1 , (1, 1))] 1
Hmm... I've recently written a tool to generate such pictures for Haskell types authomatically using generics. But his pictures are nicer than mine, I have to look what graphviz tricks he's using.
&gt; But there are a lot of crowdfunding platforms and I haven't picked one to try it out with, and I'm not sure there are enough Haskellers to have a large enough pool of people with disposable income and interest in these projects. Haskell's been growing, so maybe there would be. I don't think we can know until we try -- no one's been trying to raise money for haskell improvements with actually good marketing for a while, so we're in the dark. E.g. when I worked at a more established company with plenty of extra revenue, I thought about trying to get them to donate to haskell improvements. But the only place I could find to do so was: https://wiki.haskell.org/Donate_to_Haskell.org, which isn't exactly a great call to action.
 main :: IO () main = do input &lt;- getLine forM_ input $ \c -&gt; putStrLn [c] * `putStrLn` will output (to standard output) a `String` and a newline. * `[c]` is the `String` containing only the character `c`. * `forM_` runs an action (discarding it's return value) for each element (`c`) in a `Traversable` container (`input`). * `getLine` will input a `String` (from standard input) stopping at (and not including) a newline. In Haskell, `String` is an alias for `[Char]`, and `[]` is a `Traversable` container. `[]` are (singly) linked lists, so it is often much more efficient to use `Text` instead, but if this is all you are doing, `String` should be fine.
Instead of repeatedly sorting things, you could use a `Set` (from [containers](https://hackage.haskell.org/package/containers)), which maintains the sorted invariant more efficiently, and also gets rid of duplicates.
This isn't really a bad approach, I don't think. I'd probably try and complicate it by writing it as some sort of hylomorphism, but you've got the gist of what needs to happen -- we need to make sure only a finite amount of things are compared/sorted before producing each element. I think your implementation is "wasting time" re-sorting `xs`. If you merge/insert the `ls` values into it slightly more intelligently, that should save you some time. Also, I'm not 100% sure, but it seems like you might, in your helper function at least keep both the addend and the augend around so you don't have to calculate them "from scratch" when incrementing the indexes.
You can use [doctest](https://hackage.haskell.org/package/doctest) 
Thanks for the response. I think I'm starting to slowly come around. I think what threw me was that you declare the Functor instance by dropping args (dropping arity by one, or whatever). Then also not quite thinking through the constraint on the implementation of fmap where f has to be called on \*both\* y and z because they have to change together (having the same type) whereas you cannot call f on x while you're calling it on y and z because there's no guarantee x will be of the same type as y and z. &amp;#x200B;
&gt; think what threw me was that you declare the Functor instance by dropping args (dropping arity by one, or whatever). Yeah, Higher-Kinded Types are not common (in programming langauges), and Haskell supports all the exotic, even inconsistent ones.
&gt; doctest Ah, yes. I completely forgot about that! Thanks, I'll give it a try.
The `TimeTrack` example is the only one that I would be remotely ok with. The prefixed field names communicate enough about what is going on that it seems ok. If you don't have that, then the user is essentially required to look up the data type definition to figure out what is going on. I think that hurts readability substantially. Readable code is about reducing the number of things people need to hold in their head to figure out what is happening. RecordWildCards usually increases it--especially when field names aren't properly prefixed.
&gt; The prefixed field names communicate enough about what is going on that it seems ok You'll be happy to know we're phasing prefixed field names out ;) &gt; If you don't have that, then the user is essentially required to look up the data type definition to figure out what is going on Which is no different to (&lt;*&gt;) which conveys even less information!
Out of curiosity, which Obsidian are you talking about? The game studio?
You can use services like https://gonative.io/
[megaparsec's README links to a list of tutorials](https://markkarpov.com/learn-haskell.html#megaparsec-tutorials) Most of them have been updated last September. I took the first one and it Just Builds™: https://markkarpov.com/megaparsec/parsing-simple-imperative-language.html
Are you more interested in learning (one of) those particular libraries because you intend to use one e.g. in production, or are you more aiming to understand how parser combinator libraries are used in general? If the latter, HPFFP (https://haskellbook.com) has a good chapter using `trifecta` and `parsers`.
first one
Some good solutions here already, another that's very simple: `g :: String -&gt; IO ()` `g "" = do putStr ""` `g s = do putChar (head s)` `putStrLn ""` `g (tail s)`
I wrote [a blog post](https://vaibhavsagar.com/blog/2017/08/13/i-haskell-a-git/) that heavily involves `attoparsec`.
Also, [https://github.com/data61/fp-course/blob/master/src/Course/Parser.hs](https://github.com/data61/fp-course/blob/master/src/Course/Parser.hs) and other parser related files are not directly a tutorial of any parser library but it is super helpful to understand how it works. And that knowledge is directly transferrable to any parser library. After finishing it, I realized I am able to understand any parser library. It all began to make sense. At this point, it doesn't matter much if it's a megaparsec or attoparsec (though I use attoparsec) and I can just read the haddock and use it.
Quick note: I don't know if this is relevant but in order to test both, I'm modifying the call in main back and forth from "fibs x y" to "fibs' x y", re-building then re-executing.
You have to compile the program with `-threaded` and then run it with `+RTS -N`. Pretty sure the ghc-options line tells ghc to COMPILE with 4 threads. Also, are you sure you don't want fibs to spawn threads recursively? Without that the worker stealing queue architecture seems massive overkill over rpar. I haven't used par before but I'd expected a line like `spawn (fibs i)`?
So I noticed this after I posted and added -threaded to ghc-options, tested both parallel and sequential again, with no change in results. They both still take the same time. Also, in the command and results screenshots you can see I passed in -N2 on the parallel test
For this sort of split-join parallelism you usually have a parallel and a sequential implementation. The parallel one falls back on the sequential one once the problem size is under some treshhold, in this case maybe &lt;25-30? Glancing at the implementation on hackage, as far as i can tell spawn with the default scheduler only enqueues onto a mutable workqueue. This still costs a stack allocation and two atomic writes but it's better than allocating a thread.
Maybe I'm in a very privileged position, but &gt;"I'll do that task if you send me £50" or whatever. Is not really of any interest to me. Without trying to sound like I'm humble bragging, earning a quick buck on the side is pretty irrelevant to my life. I'm a senior engineer now, but even casting back to the days of being a student and wanting a little more cash, I would be grossly underskilled to do anything but the most mundane tasks, and those would then not be of interest to me because there wouldn't be much intellectual value. Even then, those that I would be interested in would have certainly needed supervision, and at this point things start to break down again. GSOC worked well for me as a student, because it gave me time, a mentor, a well scoped project, and the financial incentive was right. The really hard stuff with crowd funding could work though, but it really needs full time development, and a person with a very particular way of funding. Counterpoint - I helped with the Magit crowd fund which I was really hyped about, but I don't think it's really delivered (though I still love the software and am still happy to have given that money to the author). &amp;#x200B;
Why is this here?
Could you show me the code you ran it with? Also, you ran it in a different way than I did. Could you try it with stack build then stack exec 40 41 +RTS -N2 and see if it yields the same speed up?
[http://dev.stephendiehl.com/fun/002\_parsers.html](http://dev.stephendiehl.com/fun/002_parsers.html)
If you run `stack solver`, it will run `cabal-install` solver and automatically update your `stack.yaml` with updated `extra-deps`. No need to play human constraint solver, you can just use cabal's solver for those occasions where it's helpful.
Probably because OP mixed up TypeScript and PureScript. Not too surprising, I get confused too sometimes.
How is the ghci CLI different from a repl (I don't see it referred to as a repl).
You can replace ld with different linker as suggested at [https://www.reddit.com/r/haskell/comments/63shj7/link\_with\_the\_gold\_linker\_for\_faster\_build\_times/](https://www.reddit.com/r/haskell/comments/63shj7/link_with_the_gold_linker_for_faster_build_times/) &amp;#x200B;
The `.hi` files *do* have Core in them, if you're optimizing. It's just not what you usually want. When you enable optimizations, GHC produces an optimized Core version of each function, which it then compiles into object code. It also produces a differently (maybe not at all?) optimized version for any function that it thinks should be inlinable, and this is stored in the `.hi` file. When another `.hs` file compiles against that module, the original `.hs` is no longer necessary: inlining a function reads the Core directly out of the `.hi`, and then any remaining calls to the function are compiled into actual calls to the optimized version in the `.o`.
Please don't link pictures of text, link the text so people who want to reproduce this can easily copy and paste. Bonus points for a cloneable repository (did you know that you can clone gists from github?)
Obsidian Systems in NYC. We make mostly web and mobile applications, we're the ones behind reflex-dom and obelisk.
I've done this in the past but it's challenging indenting everything appropriately to format it for Reddit and I don't link to my GitHub from Reddit because I don't want my identity tied to my Reddit account. Do you know of an easy way to format code for posting to Reddit?
perhaps as pointing out a trend towards strong typing
You can use any paste bin out there and that will do the trick. Alternatively, most text editors allow for indenting a selection by 4 spaces quite easily.
Very nice! &amp;#x200B; As a nit, I'm a little leery about putting forth \`Laws\` for classes in base that are frequently violated by instances in base. We can maybe handwave \`Eq Double\` violating reflexivity with the justification that "floats are weird", but \`pred . succ = id\` doesn't even hold for \`Enum Bool\`. Do they apply only where the result is defined? skimming the source it doesn't seem so but I'm not very familiar with Hedgehog...
It is a repl (cabal even has a command `cabal repl` to launch it in a package's context).
REPL = (Read, Evaluate, Print) Loop GHCi is mostly a REPL, with some change and additional features. Firstly, if the expression evaluates to something of type `IO a`, then instead of just printing it, GHCi executes it, and prints the result. Secondly -- and most REPLs in non-homoiconic lanaguages do this -- it can read statements and update the current context/environment with the result of those statements. Like most REPLs it also has commands that can be read instead of statements/expressions, the commands range from common among most REPLs (some form of interactive help) to uniquely Haskell, GHC, or GHCi.
I haven't used a paste bin before, is there a fan favorite for Haskell?
Mind sharing?
- [pastebin, the allstar](https://pastebin.com/) ## IDEs - [rex tester](https://rextester.com/l/haskell_online_compiler) also supports compiler args - [Tutorialspoint](https://www.tutorialspoint.com/compile_haskell_online.php) supports compiler options - [repl.it](https://repl.it/languages/haskell) - [ideone](https://ideone.com)
You can use boundedEnumLaws for types like Bool. boundedEnumLaws makes sure that you do not use succ or pred in partial ways. enumLaws does not have this guarantee. boundedEnumLaws is the recommended function to use if the type has a Bounded instance.
A friend shared [this](http://nbloomf.blog/munch/munch.html) post with me, which is a literate Haskell program that builds a parser combinator library from scratch.
Not looking to write one from scratch but thanks
It was the jeeps that put me over the edge.
I found this lecture notes very accessible. http://cseweb.ucsd.edu/classes/wi15/cse230-a/lectures/lec-parsers.html
So I found the solution but I don't understand why it is the way it is: &amp;#x200B; when I issue the command: stack exec playground 40 41 -- +RTS -s -N2 it works and completes in 10 seconds. &amp;#x200B; but when I issue the command: stack exec plaground 40 41 +RTS -s -N2 it still prints out the stats, and says it's using N2, but does not in fact go faster.
Indeed. I've rephrase my question accordingly. 
`stack` is a GHC Haskell program. GHC Haskell programs accept a `+RTS rtsopts... [-RTS]` syntax. When you say stack exec addfibs 40 41 +RTS -s -N2 You are passing `stack`'s RTS the options `-s -N2`. `stack` runs in parallel, and it prints its own statistics. `addfibs`'s RTS is under its default options and does not do the same. Presumably, `stack` spends most of its time waiting for `addfibs` to finish, causing its statistics output to include the time required to run the program, instead of printing something obviously wrong. Like Unix tools in general, GHC Haskell programs understand the `--` syntax to mean "the remaining arguments aren't yours, pass these on to the 'underlying program', whatever that is". Oddly enough, this seems undocumented (but I gave only a cursory search), but the gist is that stack exec addfibs 40 41 -- +RTS -s -N2 Gives `stack` the arguments `["exec", "addfibs", "40", "41", "--", "+RTS", "-s", "-N2"]`, and `stack`'s RTS does not nab any of them. `stack` itself understands the Unix `--` convention, and it realizes that the options `["+RTS", "-s", "-N2"]` are not for it but for `addfibs`. It therefore executes `addfibs` with arguments `["40", "41", "+RTS", "-s", "-N2"]`. `addfibs` is also a GHC program, so the GHC RTS nabs the `["+RTS", "-s", "-N2"]` bit for itself, and executes the actual `addfibs` program with the options `["40", "41"]`.
&gt; Which is no different to (&lt;*&gt;) which conveys even less information! With (&lt;*&gt;) you at least know how many fields there are and which ones go where. It communicates a lot more about localized data flow. RWC makes that information invisible.
Really the "most correct" invocation would be stack exec addfibs -- 40 41 +RTS -s -N2 to make it clearer that 40 and 41 are arguments to `addFibs`, and not some kind of argument to `stack`.
Sorry if this is not on topic exactly...but what is involved in being a maintainer of a Haskell package? One thing that prevents me from taking such tasks is me wondering how to decide the future improvements in a package or things like that..Coming from an Embedded background and learning Haskell for a while, I do not have experience with library and API design..
This makes a lot of sense, thank you so much for taking the time to write it up! &amp;#x200B; My follow up question is this: is there a way to include the +RTS options in the .cabal file in a similar way to the "ghc-options" line? I tried adding "+RTS -N2" there but it did nothing.
after you read tutorials, I found it super useful to to learn properties of Applicative and Alternative typeclasses and think about how they would behave in terms of a parser. for example, I found ``` (&lt;*) :: f a -&gt; f b -&gt; f a infixl 4 Sequence actions, discarding the value of the second argument. ``` and ``` optional :: Alternative f =&gt; f a -&gt; f (Maybe a) One or none. ``` extremely useful
I decided to join my University's team for some friendly CS competitions and have been assigned to the Haskell event because "I worked with it once". &amp;#x200B; Can you guys/girls recommend me any ressources out there so that I can become competent enough to not humiliate myself? :) &amp;#x200B; Thank you!
Hello there! I'm a Haskell novice who's worked through a dozen or so chapters of various books, but hasn't really done a useful project yet. I'm also a powerlifter and I like to record my lifting in a notebook. However, I would also like to log things electronically and perhaps perform some simple analysis on it. I created [a gist](https://gitlab.com/snippets/1816854) of how I log my workouts in my book, which I would like to parse. My question is: Am I making things significantly harder by not adhering to some data format like csv or json? Or is something like the example I posted reasonable to parse for a beginner? I would rather write the logs in a human readable format as shown, but if that makes the task of parsing much harder, then maybe it would make more sense to create a tool to create the log file as csv/json, rather than hand type it myself. Looking for some input before I embark on this, as I'm in completely foreign territory here. Thank you!
Unfortunately, do test does not extract or build code blocks in module headers. Is there any way to do this?
 ghc-options: -threaded -rtsopts -with-rtsopts=-N2 
do -rtsopts and -with-rtsopts serve different purposes? also, if I want to pass multiple rts flags, do I have to use -with-rtsopts= prior to all of them or does it just signal that everything after that flag is an RTS flag?
Yeah, except `stack solver` has been broken for ages and doesn't seem to be a well supported feature in Stack. I just tried again with the latest Stack version and still no luck: ``` Using compiler: ghc-8.6.3 Asking cabal to calculate a build plan... Trying with packages from ghc-8.6.3 as hard constraints... The following lines from cabal-install output could not be parsed: hashable-1.2.7.0 (via: mono-traversable-1.0.11.0 hashtables-1.2.3.1 shake-0.17.5 shake-0.17.5 scientific-0.3.6.2 async-2.2.1 case-insensitive-1.2.0.11 unordered-containers-0.2.10.0) (new package) primitive-0.6.4.0 (via: hashtables-1.2.3.1 vector-algorithms-0.8.0.1 shake-0.17.5 shake-0.17.5 tf-random-0.5 scientific-0.3.6.2 vector-0.12.0.2) (new package) (new package) regex-base-0.93.2 (via: junk-1.0.0 regex-compat-tdfa-0.95.1.4 regex-posix-0.95.2 regex-tdfa-1.2.3.1) (new package) utf8-string-1.0.1.1 (via: ListLike-4.6 shake-0.17.5 shake-0.17.5) (new package) unordered-containers-0.2.10.0 (via: mono-traversable-1.0.11.0 shake-0.17.5 shake-0.17.5) (new package) vector-0.12.0.2 (via: junk-1.0.0 mono-traversable-1.0.11.0 ListLike-4.6 hashtables-1.2.3.1 vector-algorithms-0.8.0.1) (new package) CallStack (from HasCallStack): error, called at src/Stack/Solver.hs:174:16 in stack-1.9.3-F7FXKCpM3pk5wCtbL9Utvv:Stack.Solver ``` Maybe you shouldn't suggest the use of Stack features that aren't supported properly. And in fact it appears you're actually aware that [`stack solver` is broken and are considering removing it altogether from Stack](https://github.com/commercialhaskell/stack/issues/2888). 
`rtsopts` enables RTS options at all (it might not be necessary; I don't remember the default). `-with-rtsopts` sets the default RTS options. If you want to pass multiple RTS options, you need to quote those options. For example this is what I use: ghc-options: -threaded -rtsopts "-with-rtsopts=-N -qg" 
You're the man, I really appreciate the help
If someone knows about this area and find my approach to be the "best", then feel free to write so. I am very new to haskell ffi.
Stack exec requires the `--`s in order to pass the remaining arguments to the bianry itself.
The real advantage of FP is letting Monads do the heavy lift. However Haskell developers are notorious for wanting to wire things up manually and avoiding automation. Hence as AI automation takes over the software industry, a lot of FP will eventually get discredited the same way OO did.
"Learning Material" is linked from the sidebar.
Right, thank you! 
I tried it. It looks pretty cool. Nice job!
Some more information on Anduril: [Inside Palmer Luckey’s Bid to Build a Border Wall](https://www.wired.com/story/palmer-luckey-anduril-border-wall/).
It would be great to have some convenient way for filling in record fields with Applicatives, a sort of specialised Idiom bracket, if you will. product-profunctors is designed for that kind of thing but it forces some things about the design of your data type.
yeah... isn't this a bug?
Thanks for pointing to the (bv-little)[http://hackage.haskell.org/package/bv-little] library in your article. As the maintainer, I'd be interested in any feedback you have on the library.
&gt;As Luckey and his team see it, Lattice will become not just a system &gt;for securing the border but a general platform for geographic &gt;near-omniscience. 
No, it's common for programs like interpreters to do this with positional arguments they don't expect. "This 41 can't possibly be for me, since I wouldn't know what to do with it. I might as well past it on to the next guy and see if he has any ideas." This avoids the need for -- at all in simple invocations, and only ends up looking a bit weird when you use -- to explicitly defer some-but-not-all arguments.
 -- Precondition: input lists are sorted and deduped. mergeDedup :: Ord a =&gt; [a] -&gt; [a] -&gt; [a] mergeDedup [] ys = ys mergeDedup xs [] = xs mergeDedup l@(x:xs) r@(y:ys) = case x `compare` y of LT -&gt; x : mergeDedup xs r EQ -&gt; x : mergeDedup xs ys GT -&gt; y : mergeDedup l ys {- Preconditions: map head input is sorted, each element of input is sorted and deduped. -} mergeMany :: Ord a =&gt; [[a]] -&gt; [a] mergeMany [] = [] mergeMany [only] = only mergeMany ([]:xs) = mergeMany xs mergeMany ((x:xs):y:zs) = x : mergeMany (mergeDedup xs y : zs) -- Preconditions: 2 &lt;= k, 2 &lt;= l one k l = mergeMany [[k ^ x * l ^ y | y &lt;- enumFrom 1] | x &lt;- enumFrom 1] -- Preconditions: 1 &lt;= k, 1 &lt;= l two k l = mergeMany [[x ^ k * y ^ l | y &lt;- enumFrom 2] | x &lt;- enumFrom 2] The "secret sauce" is the the last clause of mergeMany which makes it more productive than it might otherwise be. It's only valid because of the precondition of mergeMany around the head of the lists. Prelude&gt; take 100 $ one 2 3 [6,12,18,24,36,48,54,72,96,108,144,162,192,216,288,324,384,432,486,576,648,768,864,972,1152,1296,1458,1536,1728,1944,2304,2592,2916,3072,3456,3888,4374,4608,5184,5832,6144,6912,7776,8748,9216,10368,11664,12288,13122,13824,15552,17496,18432,20736,23328,24576,26244,27648,31104,34992,36864,39366,41472,46656,49152,52488,55296,62208,69984,73728,78732,82944,93312,98304,104976,110592,118098,124416,139968,147456,157464,165888,186624,196608,209952,221184,236196,248832,279936,294912,314928,331776,354294,373248,393216,419904,442368,472392,497664,559872] it :: (Ord a, Num a) =&gt; [a] (0.01 secs, 1,477,416 bytes) Prelude&gt; take 100 $ two 1 2 [8,12,16,18,20,24,27,28,32,36,40,44,45,48,50,52,54,56,60,63,64,68,72,75,76,80,81,84,88,90,92,96,98,99,100,104,108,112,116,117,120,124,125,126,128,132,135,136,140,144,147,148,150,152,153,156,160,162,164,168,171,172,175,176,180,184,188,189,192,196,198,200,204,207,208,212,216,220,224,225,228,232,234,236,240,242,243,244,245,248,250,252,256,260,261,264,268,270,272,275] it :: (Ord a, Num a, Enum a) =&gt; [a] (0.01 secs, 1,056,008 bytes) If you need to handle seeds outside the range I give or start x or y differently, you may need to special case things a little bit, since the list comprehensions might not be sorted / deduped. (For example, for k = -2, one doesn't actually have a smallest element; for l = 1, the inner list comprehension of one is just an infinite list of the k^(x).) mergeMany can be written as a catamorphism (fold), and the list comprehensions can be written and anamorphisms (unfolds), so you could write one/two as hylomorphisms (magic). mergeMany isn't fully productive, because of the 3rd clause.
That’s what I thought about Facebook but here we are...
Would you agree that we should view the omnipresence of Facebook as a lesson in regards to Anduril's goals? 
I'm curious if you're interested in supporting rank/select operations: https://github.com/recursion-ninja/bv-little/issues/3. These would be invaluable for working with [succinct data structures](https://en.wikipedia.org/wiki/Succinct_data_structure).
**Succinct data structure** In computer science, a succinct data structure is a data structure which uses an amount of space that is "close" to the information-theoretic lower bound, but (unlike other compressed representations) still allows for efficient query operations. The concept was originally introduced by Jacobson to encode bit vectors, (unlabeled) trees, and planar graphs. Unlike general lossless data compression algorithms, succinct data structures retain the ability to use them in-place, without decompressing them first. A related notion is that of a compressed data structure, in which the size of the data structure depends upon the particular data being represented. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/haskell/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
Also, my colleague just showed me that [`UArray Bool` is internally implemented as a bitvector](http://hackage.haskell.org/package/array-0.5.3.0/docs/src/Data.Array.Base.html#line-508), maybe this is relevant to your interests?
&gt; Is there any reason I'd want to implement this in a way that doesn't use the state monad ? Might get into laziness / thunk leakage problems. I'd probably start with using state, and if performance is not what you'd like try ST or IORef. (You don't need TVar, and I think MVar is overkill for what you want.) &gt; I also keep a record of certain properties of the previous state of the stack so that when I pop the top of the stack I don't need to recalculate certain values needed for the predicates I need to evaluate. I could be wrong, but it seem slike many you are keeping a parallel stack of state.
Reminder for the conscientious: this is defense industry work for the US government.
IMO: Make releases, triage bugs, review contributions (PRs or otherwise). While having a vision for changes you want to see in the package and a drive to make those changes can be useful, it's not necessary. You do need to be able to make updates for when dependencies (including GHC) change, or attract developers to do so. Maintainership is more about making sure the code doesn't "bit rot" and follows process, and less about designing the library / API. You would need to know enough about API / ABI to decide how to bump versions on a new release (to make sure you are follow PVP, SemVer, or whatever the project decides) and this can also help inform your reviews of contributions -- if something "breaks" the API / ABI, maybe it goes on a different branch or just doesn't get merged until some sort of release window.
Is there a way to detect ambiguities when using parser combinators? I would like to get a forewarning `happy` gives while retaining sanity working with `megaparsec`.
I wrote this in the end: https://gist.github.com/rcook/8783521ddde2c0435aa65406101e30ac
I think this is in a better state than https://github.com/bitemyapp/hedgehog-checkers I'll try it out next time this comes up for me. Thank you!
Hm, yeah I guess that's how `perf` works (blanking on other examples to look at). But they document it: ``` perf stat [-e &lt;EVENT&gt; | --event=EVENT] [-a] &lt;command&gt; perf stat [-e &lt;EVENT&gt; | --event=EVENT] [-a] — &lt;command&gt; [&lt;options&gt;] ```
it's not a "mess" any more than imports are a mess; the objections seem to be the same, only RWC has a smaller scope (ought to be less problematic). I could probably live with `NamedFieldPuns` though
Possibly we ought to have some sort of syntax along the lines of Constr { fieldName1 &lt;- expr1, ..., fieldNameN &lt;- exprN } which would desugar into something the equivalent of: do fieldName1_r &lt;- expr1 ... fieldNameN_r &lt;- exprN return $ Constr { fieldName1 = fieldName1_r , ... , fieldNameN = fieldNameN_r } for both construction and record update syntax. It seems like it would make this kind of use of RWC unnecessary in most cases, if not all.
Looking into implementing this!
If you compile your project using ghcjs, then you can combine that with a react native scaffold application where you take some random react native module and replace the java-to-native code with your SDL code. Or likely there is some WebGL module already for react native. You want to run your ghcjs output through closure to get the size down.
&gt;Discussion last time around: [https://www.reddit.com/r/haskell/comments/9765vg/anduril\_industries\_is\_hiring/](https://www.reddit.com/r/haskell/comments/9765vg/anduril_industries_is_hiring/)
Anything reliable and not too obnoxious.. I like hastebin or gist.github.com
I like this pattern: stack exec -- CMD ... 
And then `rm package.yaml`, to avoid this problem in the future.
By the way, I just thought I should link [this other comment](https://www.reddit.com/r/haskell/comments/alywku/how_do_you_work_with_variations_of_sum_types/efs1rgu/) that I made a little while ago about how we use DMap and DSum to cope with situations where we're dealing with extensible records with many optionally-present fields.
I don't think there's anything like that at the moment but that doesn't seem entirely out of reach. In fact it sounds like a pretty nice project, to apply the analysis techniques in tools like happy to parser combinators.
&gt; Control whether the RTS behaviour can be tweaked via command-line flags and the GHCRTS environment variable. Using none means no RTS flags can be given; some means only a minimum of safe options can be given (the default); all (or no argument at all) means that all RTS flags are permitted; ignore means RTS flags can be given, but are treated as regular arguments and passed to the Haskell program as arguments; ignoreAll is the same as ignore, but GHCRTS is also ignored. -rtsopts does not affect -with-rtsopts behavior; flags passed via -with-rtsopts are used regardless of -rtsopts.
So does stack: ``` $ stack exec --help Usage: stack exec CMD [-- ARGS (e.g. stack exec -- ghc-pkg describe base)] ([--plain] | [--[no-]ghc-package-path] [--[no-]stack-exe] [--package ARG] [--rts-options RTSFLAG] [--cwd DIR]) [--help] ```
Note, defense industry does not always equal military. These guys are developing automated (or near automated) surveillance tech. Even if you're gungho about supporting our troops, you might want to think twice before handing that kind of tech over to the feds. If you've been paying any attention at all to how our civil liberties have been doing since 2001, you know exactly how long those cameras are going to stay pointing south of the border.
I disagree firmly - This can only be a feature. `Laws` as derived from base classes here express a series of assumed properties for an instance that, if violated, will lead to bad behavior or unexpected interactions. And, point of fact, the instances in base do lead to bad behavior and unexpected interactions, and this library can help you find them before you realize your goofy mistake made it into production.
Note that understanding how a toy version is implemented can be hugely helpful to understanding how to use real libraries. I didn't grok servant until I read through how the minimal version was implemented: http://www.well-typed.com/blog/2015/11/implementing-a-minimal-version-of-haskell-servant/
Not only that, but it's a big middle finger to anyone with a screen reader.
*Wooo* It's your **6th Cakeday** gelisam! ^(hug)
You can't properly analyze something like megaparsec (nor parsec, nor attoparsec, nor trifecta for that matter) because it relies on primitive Haskell-level recursion. The only two things you can do to a megaparsec parser is combine it into a bigger parser or run it; you can't take it apart. To be able to perform analysis, what you need is *observable* recursion. The starting point for such a project would have to be something like [FormalGrammars](http://hackage.haskell.org/package/FormalGrammars) or my own [grammatical-parsers](http://hackage.haskell.org/package/grammatical-parsers). 
If a language is too expressive to be analyzed, restricting the analysis to a subset of that language is as valid an approach as redesigning a less expressive language. Completeness for all of Haskell is too high a bar, of course, but most people don't use the full expressiveness of Haskell when writing parsers (if ever). A standalone verifier that can say "OK" with certainty in common cases is still certainly possible. There are trade-offs. One would have to figure out how to run that thing, and there may be no guarantees if it says "NO" or "IDK", but hopefully it doesn't require people to rewrite the code they already have. So I'm sure it could still help many people, so that would be a worthwhile contribution to the community. 
I think the state of Cabal and Stack are a hint at how mature or taken seriously Haskell really is.
The cause of a bug of these kinds is violation of one of these properties *plus* an assumption that the properties hold. IMO, given a context where we have prominent instances for which some properties do not hold, encouraging the assumption that they do is a bigger problem than maybe adding another instance for which they don't. Perhaps there's something we can do with naming and/or docs that'd be the best of all worlds (short of "fixing" base) - `aspirationalEnumLaws`?
The second half of this article builds a little "calculator" parser [http://dev.stephendiehl.com/fun/002\_parsers.html](http://dev.stephendiehl.com/fun/002_parsers.html) This article walks you through building a Scheme [https://en.wikibooks.org/wiki/Write\_Yourself\_a\_Scheme\_in\_48\_Hours](https://en.wikibooks.org/wiki/Write_Yourself_a_Scheme_in_48_Hours)
There are multiple approaches to avoiding these sorts of problems, but my recommendation is to use Stack, which can automatically install the appropriate GHC version. There are instructions available at: https://haskell-lang.org/get-started/linux I'd also recommend checking out the "next steps" section for how to get started using Stack. Also, there's a Gitter channel where you can ask follow up questions at: https://gitter.im/commercialhaskell/stack
By "undocumented" I meant the GHC documentation says the RTS recognizes `--RTS` as meaning "all further arguments are to go directly to the program," but it fails to say that `--` also means "this argument *and* all further arguments are to go to the program". I wasn't talking about `stack`.
Only one dependency per `--build-depends` should work: cabal new-repl --build-depends containers --build-depends "base == 4.*" I don't think you *need* to list `base`, though.
That's a solid argument. I don't think that stating these properties in this format should suggest that all prominent instances are law abiding, but as a question of subjective perception, I see how that could be problematic for some. I'd personally prefer to see violations in base encoded as part of the test suite, if possible, as some sort of doctest, so that it can function as a living document of failing instances, rather than making half-baked laws to 'formally' represent bad behavior.
The shorter form `-b` can also be used: cabal v2-repl -b "containers"`
&gt; perhaps you may want to go and open an issue on GitHub for a more flexible syntax You can look forward to https://github.com/haskell/cabal/pull/5845 landing in a future release :-)
I agree that it has issues and should definitely be used with discretion, but I'd object to banning it from the codebase outright, because there are some nice patterns it enables and does so safely when under controlled circumstances. For instance, I've made peace with the `DBUser{..} = let someOverrides = ... in APIUser{..}` pattern. And I only write such code in designated "translation" modules, where everything is imported `qualified`, so that nothing can slip into `APIUser{..}` accidentally.
I made this: https://hackage.haskell.org/package/shine https://github.com/fgaz/shine Cross platform but not native (it compiles to js)
&gt; The authors analyzed it was an impossibility to get it done in cabal (to which they had contributed before). I keep hearing this myth but the evidence at my disposal doesn't seem to back up that claim. The first time that Stack was publicly announced was in mid 2015 (NB: a couple months after the nix-style-cabal-builds GSOC had been accepted which was expected to address the major pain points that were supposedly "impossible to get done in cabal"...), after supposedly having been developed in secrecy behind closed door for about a year. And if you look at the [Git History](https://github.com/haskell/cabal/commits?author=snoyberg) those minor few contributions don't support the narrative of having been an active contributor to the Cabal project before deciding that Haskell inevitably needs a 2nd build tool. Anyway, those decision have been made for better or worse and are to be considered history that cannot be changed unless we invent time-travelling. I just wanted to set the record straight about the imo unsubstantiated myths I keep seeing spread.
I'm working on it, it currently it is part of my employers product.
Woah... I totally get this is could be seen as unsolicited advertisement for an alternative tool but -13 downvotes seems way too excessive to me. Is this really appropriate?
The problem is that he did this [in the last post of OP's](https://reddit.com/r/haskell/comments/an405v/ghc_up_installation_fails_with_error/efqmezh), and received a clear response that OP was not interested in Stack. Regardless of the tool in question, it's not about unsolicited advertisement. It just comes across as pointless pestering, and adds nothing to the discussion. Add on the fact that the suggestion was to throw the baby out with the bath water over a minor user error, and I don't see any problems with downvotes.
Functional programming noob here, trying to make the following type an instance of the Monad class: ``` data Expr a = Var a | Val Int | Add (Expr a) (Expr a) deriving (Eq,Show,Read) ``` I understand that I have to write a functor and applicative therefor in order to write a monad; I've written this for a functor: ``` instance Functor Expr where -- fmap :: (a -&gt; b) -&gt; Expr a -&gt; Expr b fmap f (Var a) = Var (f a) fmap f (Add x y) = Add (fmap f x) (fmap f y) ``` ...but I'm unsure how to write fmap f (Val Int) as Int's a specific type whereas fmap takes a function a -&gt; b. I'm gonna read more theory but any help would be appreciated!
`Val n` is both of type `Expr a` and `Expr b` so you can return it as is. fmap _ (Val n) = Val n
Ah, cheers for the help! ...noob question but I'm gonna have to ask, how come Val n is of type Expr a and b (as I'd intuitively think, as an Int, it'd only conform to one type, but my understanding is lacking)?
This is nice syntax! I vaguely remember seeing it proposed somewhere else.
GHC won't let you do this: \`fmap \_ val@(Val \_) = val\`, it'll complain that \`val\` is of type \`Expr a\`, not \`Expr b\`. To go from a \`Expr a\` to an \`Expr b\` you need to replace all \`a\`s with \`b\`s. \`Val n\` doesn't have any \`a\`s, so just re-packing it, \`fmap \_ (Val n) = Val n\`, will do the job.
You might be thinking of [Ed's talk?](https://www.youtube.com/watch?v=Txf7swrcLYs)
Right! That's why I couldn't find the post link, because it was [video](https://youtu.be/Txf7swrcLYs?t=504).
Is this similar to [haskell-emacs](https://github.com/knupfer/haskell-emacs)? If so, I tried for literally days to get haskell-emacs to work but could not. I will check out emacs-module :)
I can't see this as anything other than being antagonistic. 
How does this community feel about the readability of flip/uncurry? I usually write a local where/let function instead of these two functions because they just seem unwieldly to understand at a glance. Then again, I'm a guy who feels like he's flunking an IQ test every time he encounters "former" and "latter" in a paragraph, so maybe its just me. &amp;#x200B; &amp;#x200B; &amp;#x200B;
Thanks for the explanation about repacking, makes a bit more sense now!
As long as it's a named function, that's fine. Sometimes I find it more readable, sometimes less. Using a lambda instead of flip/uncurry just obscures things to me.
I agree that it's much easier to avoid the temptation of the rabbit hole by staying away from fancy types. My productivity generally plummets if I don't. But library code has very different trade offs. If the library implementation uses fancy types but using the library becomes safer/requires less mental overhead that might be a good idea. 
Hear hear! Very well said.
That's super cool! Some of the time ghcjs will be a dealbreaker though. In those cases it occurred to me that I could copy threepenny-gui style and have a locally running haskell server talk over a websocket to javascript. I don't think such a thing exists with a Gloss/Shine style API, but I'll be keeping my eyes out.
I agree the performance argument is way less important than the frequency at which it's thrown around makes it seem. The reason freer performance sucks is that you're repeatedly constructing and deconstructing trees at runtime. However, that is only a consequence of the implementation of freer as a GADT (initial encoding). I bet the final encoding can do wonders: newtype Freer f a = Freer (forall m. Monad m =&gt; (forall t. f t -&gt; m t) -&gt; m a) 
I only throw the performance argument around because I perceive no tangible benefit to free monads for their typical use case. The mtl style is more powerful, only trivially more of a burden to implement, and usually more than 10x faster.
Should I use [freer-simple](http://hackage.haskell.org/package/freer-simple) or [fused-effects](http://hackage.haskell.org/package/fused-effects)?
`freer-simple`
The tangible benefit is that it's really really hard to do this rewriting trick with `mtl`, which buys you a huge amount of implementation for amortized free.
This is really interesting; the idea is you let `f` be `Effect1`, and then instantiate `m` at `Freer Effect2` ad infinitum?
You let `m` be abstract, and once you've interpreted all effects in `f` away you're left with `Freer IO a` which becomes `IO a` by specializing `m` to `IO` and passing `id` as `forall t. IO t -&gt; IO t`.
&gt; Here's the skinny---I'd strongly recommend freer-simple. Failing that, if you really, really, really need the performance, take a look at fused-effects. Could you elaborate on this reocmmendation (i.e. why freer-simple over fused-effects)?
Looking at the [docs](https://hackage.haskell.org/package/freer-simple-1.2.1.0/docs/Control-Monad-Freer.html) in the `freer-simple` package, I'm not sure I understand their motivating example. In the example, they contrast the freer approach with composing two transformers and conclude freer is more general, but... that's not my understanding of the mtl approach at all. My understanding of the mtl is it should have been `(MonadReader String m , MonadState Bool m) =&gt; m ()`, which, like their freer example, is not specifically hardcoded to any two particular transformers like `ReaderT` or `StateT`.
&gt; At the most basic level, development is about efficiently turning money into software that does something while remaining modifiable and maintainable This is true from the perspective of the entity in control of the money (the employer). But from the employee perspective, 100% buying into this idea is inherently going to be against your best interest. As a developer, sometimes I'm going to do things that benefit me (growing skills, getting other experience, etc) even if they do not maximize my employer's yield from my salary. I encourage all engineers to do the same. You just have to know when and how to toe the line.
It's actually not usually 10x faster in practice though. And passing interpreters around instead of twiddling with the type class system is a nice benefit :)
When I was learning Haskell I read "Write yourself a Scheme in 48 hours", which introduces Parsec. There seems to be a newer version [here](https://wespiser.com/writings/wyas/00_overview.html).
&gt; This is hostile to a different refactoring, though, wherein the ordering of two fields with equal types is swapped. I consider two files with the same type in a single record to be a serious mistake. `newtype` wrappers can make this safe. I may seem like a pain in the neck to begin with, but as the project grows, you will be thankful. 
\&gt; In short, freer monads let you separate the high-level "what am I trying to do" from the low-level "how to actually do it." &amp;#x200B; This is true, but it's also true of typeclasses, and for that matter, interfaces. So the last two paragraphs about inversion-of-control aren't really unique to free monads. The previous notes about composition are stronger, but not sure how this composition is different from monad transformer composition (i expect something about linearization of effects comes in here). also, my understanding is that a free monad truly enforces the type to adhere to monadic laws, if you are worried about messing that part up. 
&gt; I consider two files with the same type in a single record to be a serious mistake. Agreed, but even so, field reordering is dangerous. Two other mechanisms exist that can scrutinize the type of a field and behave one way or another - type classes, and structural polymorphism (generics).
You mean the batching trick? I'll admit that's one thing where Free helps, but I don't think cases like that are nearly as common as the cases where mtl is better. The main feature of mtl being much better non-algebraic effects. For instance, the `catchError` with free monads isn't usable on errors thrown by the interpreter. data Foo a where Foo :: Foo String foo :: Member Foo eff =&gt; Eff eff String foo = send Foo errorInterp :: Member (Error String) eff =&gt; Eff (Foo ': eff) a -&gt; Eff eff a errorInterp = interpret (\Foo -&gt; throwError "Error!") bar :: (Member Foo eff, Member (Error String) eff) =&gt; Eff eff String bar = catchError @String foo (\_ -&gt; return "Caught!") -- Will return `Left "Error!"` baz :: Either String String baz = run $ runError $ errorInterp bar This is for the same fundamental reason that you can't have `MonadFix`, which I've reached for quite frequently. In general, you can't use non-algebraic effects in the "userspace" code (so to speak), and expect it to interact with the interpreter code at all (directly outlawing `MonadFix`). I have non-algebraic domains like this come up all the time, and they're utterly impossible with free monads. By contrast, the only time that free monads are better is a mere matter of convenience, in that they allow you to implicitly mangle a static sequence of effects more easily. This is just not that useful of a tool in my experience. Sidenote, in my experience messing with haxl and working with fraxl, I must say that I find implicit batching like that to be a bad idea. Its implicitness is a frequent source of bugs, when you accidentally introduce something that breaks the contiguous series of batchable requests. It's really only helpful in scenarios where it's basically helping by accident, which isn't very common; otherwise you might as well have done it yourself.
Kudos to the customer. Most companies would not dare replace the existing legacy code and thus couldn't break away from the existing vendor. 
&gt; It's actually not usually 10x faster in practice though I'm not aware of any benchmark where mtl-style doesn't outperform free monad styles by *at least* x10, except in extremely trivial scenarios like a single Reader effect where it gets down to about x5. To your point, in real world scenarios a lot of programs' time is spent waiting on IO, making this difference negligible. But in any other scenario, it's a dramatic cost. Performance wouldn't be such a deciding factor if there were many other serious deciding factors, but there's really not much other difference, besides non-algebraic effects (as I described in another comment) and... &gt; And passing interpreters around instead of twiddling with the type class system is a nice benefit :) Personally I find this a pretty trivial difference, unlike non-algebraic effects.
amen
JetBrains tools are amazing.,,give ‘em lots of love
Anyone know if there are plans to introduce instance (forall x. Monoid (f x)) =&gt; Alternative f where empty = mempty (&lt;|&gt;) = (&lt;&gt;) in the base alongside GHC 8.8?
OK, so this is first stable(ish) release since Oct 2017. The changelog is quite big, so everyone's welcome to give the update a try.
no, iirc, `haskell-emacs` uses IPC (or something similarly external) between the Haskell code and the emacs program. this provides haskell bindings to emacs modules. i.e. your Haskell code is dynamically loaded, then directly called (via FFI) by emacs process (like emacs builtins are, i think). see https://github.com/emacs-pe/emacs-modules
This is pretty level headed, and not the first level headed thing I've seen from tweag. So kudos to that!
1. Avoiding fields with the same type in a record 2. For things like binary and JSON serialisation/deserialisation, avoid derived implementations and use explicit implementations (with versions numbers if needed) instead. 3. Add round trip tests for all serialization. With the above rules, re-ordering fields, will either result in a compile error or a test/CI error, both of which are so much better than something dying in production.
&gt; I'd be surprised if you could really ignore the cabal file completely and just use a purely source based approach of importing all hackage and stackage packages A cabal file can be used fine for the project, while some files can be imported trough an URL. I just want to make this option available for certain cases during development. For example, for the top level files of a collaborative project which change a lot. The first step rather than an extensio is to create a preprocessor that does the same. 
That syntax is only valid is quantified constraints, right? I'm not opposed, or in the know, but it seems like quantified constraints is still too new to be a base dependency.
This argument seems to apply equally as well against regular field accessor functions and lenses, which I would consider pretty uncontroversial.
`$ cat Main.hs` `{-# LANGUAGE OverloadedStrings #-}` `import Network.Wai` `import Network.HTTP.Types` `import Network.Wai.Handler.Warp (run)` `main :: IO ()` `main = run 8080 (\_ resp -&gt; resp $ responseLBS status200 [] "Hello Web!")` &amp;#x200B;
When you say "too new to be a base dependency", do you mean that "it might be harder for people to understand the docs because this is a new extension" or something else?
&gt;cabal new-repl --build-depends "base &gt;= 2" --build-depends "wai" --build-depends "warp" --build-depends "http-types" --AACSJKAxxxx local.env Main.hs &gt; &gt;Resolving dependencies... &gt; &gt;Build profile: -w ghc-8.6.3 -O1 &gt; &gt;In order, the following will be built (use -v for more details): &gt; &gt; \- fake-package-0 (exe:script) (first run) &gt; &gt;Configuring executable 'script' for fake-package-0.. &gt; &gt;Preprocessing executable 'script' for fake-package-0.. &gt; &gt;Building executable 'script' for fake-package-0.. &gt; &gt;\[1 of 1\] Compiling Main ( Main.hs, /tmp/cabal-repl.-28085/dist-newstyle/build/x86\_64-linux/ghc-8.6.3/fake-package-0/x/script/build/script/script-tmp/Main.o ) &gt; &gt; &gt; &gt;Main.hs:1:1: error: &gt; &gt;Could not load module ‘Prelude’ &gt; &gt;It is a member of the hidden package ‘base-4.12.0.0’. &gt; &gt;Perhaps you need to add ‘base’ to the build-depends in your .cabal file. &gt; &gt;it is a hidden module in the package ‘hackage-security-0.5.3.0’ &gt; &gt;Use -v to see a list of the files searched for. &gt; &gt; | &gt; &gt;1 | {-# LANGUAGE OverloadedStrings #-} &gt; &gt; | \^ &gt; &gt; &gt; &gt;Main.hs:2:1: error: &gt; &gt;Could not load module ‘Network.Wai’ &gt; &gt;It is a member of the hidden package ‘wai-3.2.2’. &gt; &gt;Perhaps you need to add ‘wai’ to the build-depends in your .cabal file. &gt; &gt;Use -v to see a list of the files searched for. &gt; &gt; | &gt; &gt;2 | import Network.Wai &gt; &gt; | \^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^ &gt; &gt; &gt; &gt;Main.hs:3:1: error: &gt; &gt;Could not load module ‘Network.HTTP.Types’ &gt; &gt;It is a member of the hidden package ‘http-types-0.12.2’. &gt; &gt;Perhaps you need to add ‘http-types’ to the build-depends in your .cabal file. &gt; &gt;Use -v to see a list of the files searched for. &gt; &gt; | &gt; &gt;3 | import Network.HTTP.Types &gt; &gt; | \^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^ &gt; &gt; &gt; &gt;Main.hs:4:1: error: &gt; &gt;Could not load module ‘Network.Wai.Handler.Warp’ &gt; &gt;It is a member of the hidden package ‘warp-3.2.26’. &gt; &gt;Perhaps you need to add ‘warp’ to the build-depends in your .cabal file. &gt; &gt;Use -v to see a list of the files searched for. &gt; &gt; | &gt; &gt;4 | import Network.Wai.Handler.Warp (run) &amp;#x200B;
PS: I also would like to use 'cabal new-run' / script files with my local package-env.
Even if money / career are not factors: "Real Artists Ship!" No matter how beautiful / bespoke the internals, design, or interface: If no one besides you uses the software, development was a masturbatory exercise. If no one uses it, development was just a waste of time. (That said, I think some of the things the article decries are not necessarily always to be avoided.)
&gt; The anecdata of a single person might not be convincing to you, but if there were to be just one common theme across many of our projects at Tweag in the past year, it would be this: we have witnessed the high cost of entirely incidental complexity many times over. Our data suggests this issue is real and very common. This is a problem of your particular developers. I've been using type hackery in production since I got my first job and it has always made my code more reliable and sensible without incurring noticeable maintenance overhead and often even reducing it. [Here is one documented example](https://github.com/input-output-hk/plutus/blob/master/language-plutus-core/docs/Constant%20application.md) from my current job. I've seen things you people wouldn't believe. An entire class of bugs eliminated by a simple phantom type parameterized by terms promoted to the type level. I watched type-driven generation of programs in outer tests. I witnessed equality of type indices by means of dependent pattern matching over singletons in the middle of production code. `ReaderT` temporarily turned into `StateT` by the `Codensity` transformer. Generalized algebraic data types, type families, functional dependencies, higher-rank types, polymorphic kinds, constraint kinds, semigroupoids, monad morphisms, comonads, higher-kinded classes, Kleisli functors, indexed monads, algebraic effect handlers, `dependent-map`s, zippers, type roles, non-regular data types, polymorphic recursion, `RecursiveDo`, defunctionalization, final tagless encoding, prismatic error handling, void types, type-safe file paths and REST APIs, recursion schemes, type-driven strictness, Church-encoded free monads, left and right Kan extensions, levity polymorphism, polyvariadic functions, dependent data families, continuation-passing-style existential typing, parameterized higher-order abstract syntax. All those types will be lost in runtime, like tears in rain. Time to compile.
&gt; But surely I want to be forced to rethink every bit of code that's handled a Foo, given that it's now a different type with an entirely new field. A use of record selector functions, NamedFieldPuns, RecordWildCards, etc. is a clear indication that you don't want *all* the fields, just a few of them. If you *do* want all the fields, and want a compiler guarantee, then pattern matching on every field is safer.
&gt; If no one uses it, development was just a waste of time. Oh, you saying every bit of code everyone wrote while they were learning/figuring out stuff, were a waste of time? 
The author of this post seems to want to hire experienced people, people who have grown wise owing to the memory of their past mistakes burned into their brains. But they just don't want that mistakes to be made on their watch... Or may be they will share the magic by which one can become experienced without actually going through the experience....
That, and misc. quality concerns. It's rare, but sometimes an extension has to be significantly reworked, including syntax changes, after it's in the wild, due to implementation issues not found until the jungle started using it. It would be nice base (other than GHC.*) was readable to new Haskellers, but I know that sometimes core libraries need deep magic to bridge the gap between the language and the compiler.
If you don't use that learning to ship, yes.
I agree, and the best way to learn how to ship is by working for a company. But my advice in this realm is still "avoid shipping (at all costs)." Spending 5-10 years of your career 100% motivated by &amp; biased towards shipping value for your employer is doing yourself a disservice. Also, shipping isn't usually blocked by trying new things and otherwise going against the philosophy quoted above. Once you know how to ship things well enough, you can manage shipping software that was built partially with a process to extract personal value over company value just fine!
Is there a limit to how long of a string \`read\` can parse? I keep getting a "no parse" error when trying to parse this really long text file into \`\[\[Float\]\]\`. The data originally came from a matlab file; I have no idea how it could've been corrupted, so I thought I'd see if it was the parsing function and not the actual file.
For the record, Mark was referring to several projects we've been brought in to help out on after deadlines were slipping and estimates were wildly off. I think all of my colleagues are are big fans of the benefits of static checking. They also know that there's no silver bullet to robust engineering, and that extra static checks may or may not be a net benefit if at the cost of composability, code reuse, code size, learnability, or other important properties. We recognize that like most choices in engineering, there are tradeoffs involved. And yes, we celebrate that we've all been burnt before making the wrong choices here and there.
Overlapping instances are still controversial enough that it wouldn't be a good idea to make the standard library rely on them that much. It would make error messages quite confusing, mentioning `Monoid` instead of `Alternative`, or mentioning overlapping instances (in some corner cases), since this one overlaps with every instance of `Alternative`. On the PL design side, overlap makes instance resolution quite harder to specify and implement, since you now need to find the "most specific" instance. Without overlap, whatever matches is guaranteed to be unique. Surely that simplicity is worth something. IIRC there's also a long standing bug where overlapping instances can be inconsistent even without involving orphans (has to do with typechecking an instance method which uses an overlapping instance). Some people are in favor of instance (Applicative f, Monoid x) =&gt; Monoid (f x) where mempty = pure mempty (&lt;&gt;) = liftA2 (&lt;&gt;) so that's another source of conflict (or even more confusing errors if both would end up in there). 
&gt; I don't think cases like that are nearly as common as the cases where mtl is better. I'd gently suggest this is because you are not used to thinking about cases where this is particularly helpful. In my experience, pretty much *all of every application I've written* is details like these can be composed away. Your argument about userspace vs interpret code is well taken; I've never directly felt the loss of this. &gt; when you accidentally introduce something that breaks the contiguous series of batchable requests What do you mean by this? By emitting something that isn't allowed to be batched with the others? I'd suggest you've architected your thing oddly in that case.
&gt; This is true, but it's also true of typeclasses, and for that matter, interfaces. Fair, but at a significantly higher cost. Instances must be uniquely tied to types, which means you need to make a new type every time you want to be able to provide an instance. As soon as you do this you need to start monomorphizing your monad stacks (thus losing extensibility), or paying the O(n^2) MTL instance cost (thus wasting a bunch of time for no benefit).
&gt; pretty much all of every application I've written is details like these can be composed away Yea me too... That's not what I'm saying aren't common, and mtl handles that great. It's the idea that you can mangle the AST usefully without actually running any effects, as in the batching example. &gt; By emitting something that isn't allowed to be batched with the others? I'd suggest you've architected your thing oddly in that case. As per the definition of Monad, any operation that takes an argument is such an operation (unless you enumerate the possible values for the continuation and take some guesses or something, but we'll leave that idea alone). For instance: data Users a where GetFriends :: UserId -&gt; Users [UserId] secondDegreeFriends :: UserId -&gt; Eff '[Users] [UserId] secondDegreeFriends uid = do first &lt;- send (GetFriends uid) fmap concat $ traverse (send . GetFriends) first This cannot possibly be batched, per the monad laws. This is why fraxl and haxl are law breaking monads. And a standard free monad will not allow you to batch this. Thus, most operations aren't batchable without a lawbreaking monad. Which is a symptom of the larger point: The ability to statically analyze a prefix of an effectful program is not useful since that prefix is almost always limited to something extremely small.
&gt; Overlapping instances are still controversial enough that it wouldn't be a good idea to make the standard library rely on them that much. &gt; &gt; It would make error messages quite confusing, mentioning Monoid instead of Alternative, or mentioning overlapping instances (in some corner cases), since this one overlaps with every instance of Alternative. That's a good point, I didn't think about overlap; I just thought a bunch of downstream code would break. &gt; Some people are in favor of &gt; &gt; instance (Applicative f, Monoid x) =&gt; Monoid (f x) where &gt; mempty = pure mempty &gt; (&lt;&gt;) = liftA2 (&lt;&gt;) There's already the `Ap` newtype for it. Is there a suggestion/proposal to introduce this instance without the newtype?
I'm personally very much against basically all record related extensions. Generally because I consider them temporary bandaids that will be obviated by proper extensible rows/records/variants. Specifically for \`RecordWildCards\` and \`NamedFieldPuns\` though my issue is that they intentionally cause shadowing, which I like having \`-Wall\` tell me about, and said shadowing can make code quite confusing if you sometimes use them as values and sometimes as functions (due to you needing to interact with multiple objects of the same type for example). Although I am fine with the fact that adding a new field does not break existing code that uses \`Foo\`, I don't think there are a whole lot of situations besides serialization where it will make existing code \*less\* correct when adding a new field, and even with serialization the associated deserialization function shouldn't compile which will clue you in about the issue.
I agree with all of this. Very often a simple `newtype` is the only thing that one needs. I've seen terribly overenginereed systems and convinced people to remove fancy typing and use vanilla data types instead. But I disagree with this attitude: &gt; Haskell98 with some “benign” extensions is often the optimal solution in the trade-off between safety and simplicity. With conventional Haskell, developers effortlessly can stay in the pit of success. &gt; Using advanced features of the type system takes a fair bit of judgment to get right. Judgement comes from experience. But who really has this experience? Who are we but a relatively small group of engineers trying to build production software using tools that relatively few people have used for this before, and even fewer people have used successfully? Yes, I have this experience, what's so questionable about it? A non-Haskell98 GADT is not a big deal, getting, say, strictness and [asynchronous] exceptions right is an order of magnitude harder. It is common that "nice simple code" leaks terribly and loses data on Ctrl-c. But once the developer starts to carefully think about the denotational/operational/etc semantics of his program, fancy types become a friend and not some kind of scary attractive mermaid.
There's no official proposal but I've seen people ask about it on IRC and Slack before.
I think that's a fair assessment. Free monads are also overhyped.
This is a specific question leading to a general one. I am trying to parse a csv using CSV-Conduit, and I find the \`FromRecord\` and \`parseRecord\` to be incredibly cryptic, to the point that I have absolutely no idea what to do even with the example. So the specific question is, how do I use \`parseRecord\`? The generic question would be, how should I approach Haskell documentation to not be completely lost? I find Haskell documentation to be so alien.
So the problem is you're trying to mix --build-depends and --package-env commands into the same line?
I've often wondered if Haskell shouldn't have enforced what you're suggesting and replaced records with something type directed (i.e. "get the record of type `Foo`"). It seems like that would have a lot of benefits.
What do you suggest? I am amazed that the following don't result in errors: cabal --BOGUS local.env new-repl Main.hs cabal -BOGUS local.env new-repl Main.hs cabal new-repl --BOGUS local.env Main.hs So how do I know that it even uses the command line argument "--package-env"? What is your suggestion in using "--package-env" from the command line for the repl and cabal scripts?
&gt; FromRecord It's a type class which has some surface similarities to an Java interface or a C++ pure virtual class; basically it's just a name for a few functions. &gt; parseRecord The documentation assumes you know something about monadic parsing. The haddock also documents the default implementation available if your data implements "Generic"; you can mostly ignore this if you want, but otherwise you'll need to learn at least a little about Generic programming in Haskell. (Very different from Java Generics, so make sure you are learning the right stuff.) However, the input is just a vector (or "array" if you are more familiar) of ByteString (think `char []` if you come from C, `byte[]` if you come from Java, or just raw interpreted [so not necessarily Unicode] bytes) and you can treat the output type as if it were `return value` or `fail "Error Message"`. If you have a way to take raw data and either turn in into your data or fail, you've got an implementation. &gt; So the specific question is, how do I use parseRecord ? You probably wouldn't call it directly. You can, but it's not the preferred entry point. It's called by the library to convert the CSV data into a custom data type. (Well, sort of; FromNamedRecord appears to be where that starts.) You would call `readCSFile`, that would use a `FromNamedRecord` instance (either provided by you or the library) to convert each CSV row (except the header row) into the type that has the instance. (caveat: you'll also need a `ToNamedRecord` instance, even if all you do is read.) &gt; The generic question would be, how should I approach Haskell documentation to not be completely lost? I find Haskell documentation to be so alien. Follow the types, it's much more relevant in Haskell than in some languages. Follow the instances, too; generic code it like that. That said, sometimes the documentation is just bad. Ask questions of people, possibly even the package maintainers, look for other users' experiences and experiment until you figure it out. After that, a patch to improve the documentation based on your experience will likely be appreciated. (This is the first time I've read the docs, and I also can't find a use for `FromRecord` / `ToRecord` or any of their fields, everything seems to be built around the named versions.)
Does it have any infinities in it? `read "+Inf."` is a "no parse" error. I do not think there's any sort of hard-coded limit to read, though it might take quite a bit of memory if the file is large. Heck, `last . last . read . show . replicate 10000 . replicate 10000 $ (0 :: Double) :: Double` in GHCi nearly killed my laptop as I was writing this. You might also make sure you aren't defaulting to a non-Float element type. `Int` and `()` are parsed very differently, and GHCi seemed to default to one of those when I did: Prelude&gt; last . last . read . show . replicate 10000 . replicate 10000 $ (0 :: Double) *** Exception: Prelude.read: no parse 
What else have you seen!? C-Beams glittering off the shoulder of Orion? The Tannhauser gate?
No infinities. Also I'm pretty sure that `read "Infinity"` parses correctly. I wouldn't be surprised if it took a very long time to parse, but it isn't parsing at all. Probably going to have to rewrite the file or make matlab write the data to a binary file.
 Prelude&gt; read "Infinity" *** Exception: Prelude.read: no parse Prelude&gt; read "Infinity" :: Double Infinity it :: Double (0.00 secs, 81,392 bytes) Thanks.
Do you have a complete &amp; working source code for the example?
&gt; Maybe you shouldn't suggest the use of Stack features that aren't supported properly. And in fact it appears you're actually aware ... That isn't me, that's Kirill. I no longer do much work on stack. Too bad stack solver is broken, it's very rare that I need to use it, so I didn't realize it was broken.
&gt; once the developer starts to carefully think about the denotational/operational/etc semantics of his program This is very hard to do when the requirements (driving the denotational/operational/etc semantics) are not well defined or are constantly changing
Here is the *official* doc for writing Dynamic Modules: [Writing Dynamic Modules @ *doc/lispref/internals.texi*](https://github.com/emacs-mirror/emacs/blob/33dbe23bb8114d6a71ad267b0e172909dab7b387/doc/lispref/internals.texi#L1039) The above documentation is a *refined* version of [https://phst.eu/emacs-modules](Emacs modules | Philipp’s documents)
Thanks! I am fine with the syntax, I didn't know what the correct syntax was. 
Unless your Wrong Abstraction also happens to be serialized to disk, digging out of it in Haskell is often not *that* bad. The hardest part is not getting overwhelmed and refusing to look it in the eye in my experience. 
`ghci Main.hs -package-env local.env` is the command you want, I think. Why are you trying to go via `new-repl`? The latter only works within project settings, and the argument there is not a file, but a target. Also, are you on the latest cabal? testing with 2.4.1.0, all the above do indeed give errors. I recall there were some bugs with argument handling in past releases...
I disagree with the triviality of the burden. I tend to dread writing new MTL classes even when I know I’d benefit from them. So I think this argument is a bit more subjective. If that trade off was perceptible all of a sudden you have a real decision to make
Thank you very much! I am on cabal version 2.4.1.0 as well. Strange. I found out how to also do runghc: runhaskell --ghc-arg="-package-env local.env" Main.hs The last point on my quest is to get 'new-run' and cabal scripts also to accept my local package env. I tried this: #!/usr/bin/env cabal {- cabal: -package-env local.env -} ... but this failed.
I don't think this is possible, tbh? Cabal new- commands generate package.env files to let ghc make use of them, but I don't think cabal can make use of them itself. Rather, you use project files and enumerated dependencies for new-run of projects and standalone scripts, respectively... However, I'll ask if anyone else has any advice.
I get a little frustrated as I'm writing them, but it usually doesn't take all that long now that I'm used to it, and it's pretty close to a one time cost per project.
Also interested to here about this recommendation, and if possible, an opinion about freer-simple over extensible-effects. 
 import Data.Semigroup data B = B !Int !Int deriving (Eq, Show) instance Monoid B where mempty = B 0 0 instance Semigroup B where (&lt;&gt;) (B a b) (B c d) | b &lt;= c = B (a + c - b) d | otherwise = B a (d + b - c) parse '(' = B 0 1 parse ')' = B 1 0 parse _ = B 0 0 balanced xs = foldMap parse xs == B 0 0 
They are. I'd like to know more about how JetBrains does it.
&gt;Here is one documented example from my current job. &gt;We need to apply a built-in function to several arguments, how do we do that? built_in_function arg1, built_in_function arg2, built_in_function arg3.... You have seen all those things, but don't know how to call function on a bunch of arguments?
Why?
&gt; This cannot possibly be batched, per the monad laws. This is why fraxl and haxl are law breaking monads. Yup, and the thing is all you really need to [do it correctly](https://identicalsnowflake.github.io/QueryAggregation.html) is that `Traversing` profunctor thing. Works out quite nicely algebraically.
They go for/after language tooling. With all that knowledge about the code, rest is doable ;)
&gt;Plugin will automatically build Intero and Haskell Tools (HLint, Hoogle, Hindent and Stylish Haskell) to prevent incompatibility issues Built in project, or globally? (Without a mistake of a century which is '--install' option of stack - for development dependencies of a project) &amp;#x200B;
Intero is built in project, tools are built in separate Stack root
&gt;The Haskell tools are built in a IntelliJ sandbox with LTS-13. So they have no dependency with Stackage resolvers in your projects. After Stackage LTS-13 minor updates one can use: Tools \&gt; Update Haskell tools; Ok, tools are built once for the plugin itself.
Ok, got a suggestion on how to use package-envs, but again the idea is you do it instead of new-run. Just use the shebang to point to ghc, and pass it the env, like so: `#! /opt/ghc/bin/runghc -package-env=-` 
Thanks!
&gt;If no one uses it, development was just a waste of time. Man, I hate this argument that everything should be useful or productive to have value. Things can have value without being useful or productive. Just look at mauke's [poly.poly](https://github.com/mauke/poly.poly), its utterly useless, unproductive, no one will ever use it for anything, but you know what? It's a fucking piece of art and honestly a million times more valuable than some shitty library, no matter how many users it has.
But which extensions are "benign" and which ones are "malignant"? Everyone have such list and it's different for everyone.
/u/rainbyte I love [your chart](https://www.reddit.com/r/haskell/comments/5wkx3l/haskell_editoride_support_chart/). Any chances for update?
To provide you some context for the downvotes: https://twitter.com/GabrielG439/status/1043374646198857728
I'm going to type this on my mobile phone so formatting will suck, sorry about that. I've invested a couple hundred hours into Haskell. I went through one book, used it for a toy program, most of Advent of code and I specifically looked into things I didn't understand, such as recursive state monad with replicateM. I feel comfortable with MTL and transformers, can do basic profiling and performance optimizations and so on. But reading this post makes me want to quit and just get shit done in a simpler language. What's up with the apostrophe and colon in the type signature? Why am I supposed to use handleRelayS a function which appears to be an internal function of the imported module? How does it work? Its type signature is certainly scary. What's up with those combinators? Is this like lenses and parsers where you can only be productive once you've learned a certain number of combinators? Because I didn't need any for MTL nor for transformers. And if I sit down and sift through all of that complexity I get Haskell that could very well end up at the speed of python? Can I copy paste and these snippets? If this is the simple version of freer monads than I don't want to see the non simple version. The above is overly grumpy I know. The post seems neat and I have nothing but respect for the author and everyone writing posts like that. But for me, it's always an uphill battle against frustration where I need to remind myself every couple of minutes why I invest time in this language. :( Maybe I'm just not the target audience for such posts.
&gt; ContT is Not an Algebraic Effect I don't understand what that entails. Clearly we can implement `shift/reset` in terms of `Eff`: data Shift0 r es a where Shift0 :: ((a -&gt; Eff es r) -&gt; Eff es r) -&gt; Shift0 r es a shift0 :: ((a -&gt; Eff es r) -&gt; Eff es r) -&gt; Eff (Shift0 r es ': es) a shift0 body = send (Shift0 body) reset0 :: Eff (Shift0 r es ': es) r -&gt; Eff es r reset0 = interpretWith (\case (Shift0 body) -&gt; \k -&gt; body k) program :: Eff (Shift0 [r] es ': es) (Bool, Bool) program = do b1 &lt;- shift0 (\k -&gt; liftA2 (++) (k False) (k True)) b2 &lt;- shift0 (\k -&gt; liftA2 (++) (k False) (k True)) return (b1, b2) results :: [(Bool, Bool)] results = run (reset0 (program &gt;&gt;= \a -&gt; return [a])) &gt; results [(False,False),(False,True),(True,False),(True,True)] So what does it mean? That `reset` is not an effect (it is a handler)?
&gt; You have seen all those things, but don't know how to call function on a bunch of arguments? A built-in function. We develop a language there. A built-in function is just a name for a Haskell function that one needs to apply to a bunch of target-language constants (only constants were supported when the doc was written). The problem is how to check types in the presence of arbitrary built-in functions (think of them as FFI). The solution is to encode types using GADTs. You can try reading more than once sentence, maybe it'll help.
Yeah, the post was an explicit allusion.
&gt; This is very hard to do when the requirements (driving the denotational/operational/etc semantics) are not well defined or are constantly changing. If you're dug in on a particular encoding of fancy types, you may discover later you have chosen The wrong abstraction, and then have to dig yourself out. Sure it is. But this applies to any kind of development, be it simple types (I've seen complete mess written using simple types), fancy types, OOP patterns or even no types at all. Most programming languages have multiple way to define abstractions: modules, data types, functions, type classes, interfaces, classes, etc and all of those can expire. But guess what, when your fancy typed encoding has expired, you at least now that, because it becomes hard to type check your code. And you have to remove obsolete stuff, which is not too problematic in Haskell compared to other languages, especially imperative ones. In some dynamically typed language programmers can easily write more mess on top of existing mess without even thinking to change the abstraction, because the latter is expensive and the former is what the language allows to do cheaply.
BTW, are you aware of `stack build intero --copy-compiler-tool`? This is what intero for Emacs does, so that Intero is built once per version of GHC; subsequent executions of `stack exec intero` or `stack ghci --with-ghc intero` will use the appropriate version of intero for the current GHC in use on the project.
You mean if you compare all the lists there is not a subset of 50–80% that most people share?
&gt;I never really understood this one as stated---I've never actually used ContT in a real monad stack. Have you? Yes, [here](https://github.com/circuithub/fast-downward/blob/master/FastDownward.hs#L295,L320) it's used in my `fast-downward` project. Whenever you try and `read` from a variable, I capture the following continuation. Then, whenever something else writes to that variable, I re-invoke all captured continuations with the newly written value. This lets me exhaustively enumerate all possible transactions without having to build up any data structures (other than managing the continuations). This is significantly faster than the previous approach (which stored reads/writes in a map and selectively re-ran transactions whenever new information came in).
There are plenty of ways to use type classes without the O(n\^2) cost. [https://hackage.haskell.org/package/simple-effects](https://hackage.haskell.org/package/simple-effects) is my current favourite.
How does it perform with larger Haskell projects?
&gt;I agree the performance argument is way less important than the frequency at which it's thrown around makes it seem. I think there is a legitimate worry that once performance *is* a problem, we don't have a story on what you have to do, other than rip the whole thing apart and start again. Whether or not it *becomes* a problem is much harder to quantify. I agree that for most "boring" applications dominated by IO, it's unlikely to be a problem - but that is very vague and makes it hard to truly evaluate the technology to see if it will be appropriate. &amp;#x200B;
I mean there're number of extensions that one *have* to use. OverloadedStrings/FlexibleInstance/FlexibleContexts/TypeFamilies come to mind. On other hand IncoherentInstances is universally considered evil. Outside of that there are no agreement. Some people say RecordWildcards hurt readability I think it improves readability. Are GADTs OK? RankNTypes? DataKins?
No, was not aware of that. Thanks for your feedback!
Oooh, this sounds like a treat. But I'm getting a lo of cryptic error messages when I try to use it on a project of mine. Is there an IRC channel that I can bother about this or something?
There's Gitter — [https://gitter.im/intellij-haskell/Lobby](https://gitter.im/intellij-haskell/Lobby) 
I definitely agree with the design philosophy that you're encouraging in this post, but I don't think it means you have to use free(r) monads. The same idea can be done with type classes and transformers, but things become a little less direct. Furthermore, the worry that this requires O(N\^2) instances is actually a choice [https://hackage.haskell.org/package/simple-effects](https://hackage.haskell.org/package/simple-effects) shows how this can almost all be done using a single transformer - essentially `ReaderT` carrying a record of implementations of methods. What I've started to feel recently is that we ultimately end up writing something like this: runM . runRedis . runFTP . runHTTP . runEncryption . redisOuput @Stat mkRedisKey . ... which really means that we want an implementation of `Input` (`Reader`) such that `nextInput` goes off and hits Redis. But why are we paying for this cost by repeatedly reinterpreting the *entire* program? What we really want to do is provide a really tight optimised implementation for `nextInput`, rather than repeatedly transforming the whole program. This is what I was trying to explore in [rio-effect](https://github.com/ocharles/rio-effect) \- in this library the idea was you would write programs using some high level effects, and then provide effect morphisms when you run. In this case you'd start with a program in `MonadInput`, and you would "run" that with a `MonadInput -&gt; MonadIO` morphism, which in this case you would construct out of `csvFile :: String -&gt; MonadInput -&gt; MonadCSV`, then `MonadFile -&gt; MonadEncryptedFile`, etc. Basically the idea was because you have full implementations GHC can do a load of optimization magic to provide the final `MonadInput` dictionary. I never got this entirely figured out though, so this is all just rambling for now...
I'm working on a 40K LOC Haskell project (without dependencies) and ij-haskell is doing great. It runs an intero REPL per build target, and is starting them sequentially (for now), so for my project it takes a while. There's also a rare performance issue I run into sometimes that we are yet to track down. 
That will do, thanks! :)
I gave it a spin and I really like it but I do have one feature request. When scrolling in autocomplete to show documentation alongside selected item. At the moment it only works when pressing CTRL+Q on already written text. Anyways,keep up good work!
I think that belongs to IDEA settings — https://www.jetbrains.com/help/idea/2018.3/settings-code-completion.html#d04a4ca8 Go ahead and report a bug if it doesn't work for you though!
Eh, I'm not sure I buy this argument. However, I do think fooToString :: Foo -&gt; String fooToString Foo {..} = "id = " ++ fooId should warn if `Foo` gains a new field though. That declaration clearly says "I want you to bring *all* of `Foo` into scope". `Foo{fooId}` says "I just need `fooId`", so I don't think that should warn if a new field gets added. If you have functions that *might* be sensitive to new fields, then I would argue they should use `Foo{..}`, but also use `const` or something to discard the fields the function doesn't care about: fooToString :: Foo -&gt; String fooToString Foo{..} = "id = " ++ fooId &amp; ignore fooAlive &amp;#x200B;
I agree with you, the docs are making the wrong comparison.
I am aware, ij-haskell doesn't use it though. Yet.
Also `--copy-compiler-tool` overwrites built executables unconditionally, but older ones could be built with newer dependencies. 
Is some of that code open source?
I love the completeness of this plugin. The *only* downside is that debugging doesn't work in the IDE. That's the only feature that I miss. :) Other than that, it's a lot better experience than using Visual Studio Code with a ton of plugins. Just plug and play! 
I'm absolutely all for finding a better performing way of doing this! But I'm also explicitly advocating for the programmer's ergonomics; if something is *possible, but lots of work to do,* it's going to get done a lot less often than it could be. I think too many libraries in this space forget that! Along those lines, I've been working through @Syrak's final encoding of this, and the results are looking really encouraging. Some of the microbenchmarks are *faster* than MTL :o
I have no idea! This stuff makes my brain spin, but yeah, I think the argument is that this is a handler, and thus it doesn't interact nicely with user code?
&gt; But reading this post makes me want to quit and just get shit done in a simpler language. Please don't! Remember that all this stuff is strictly opt-in. In my experience, it neatly solves a very real problem, but it's only one point in the solution space. &gt; What's up with the apostrophe and colon in the type signature? The `':` operator is the list constructor for *type-level lists*. These effects are tracked in a type-level list to give extensible effects with usable semantics. If you're interested, the relevant keywords are "data kinds"---but again, it's not really something you need to understand in order to get work done. &gt; Why am I supposed to use handleRelayS a function which appears to be an internal function of the imported module? I originally learned all of this stuff against `freer-effects`, where that was the only handler available! Things in `freer-simple` are indeed simpler, most of the time you don't need to pull out this big gun. That being said, I felt like the post gave a reasonably good explanation of what this thing does. If it didn't connect with you, that means I've failed at my job of explaining it!
It's got less conceptual overhead for getting things done, at the cost of runtime performance. It's easier to write with `freer-simple`, but you keep the path open for refactoring it for perf later if you need to.
That said, I have no idea how well-received a newtype carrying the instance you described would be. I would like to see a discussion of it on the libraries mailing list.
Thank you for your level headed response to my pretty rough post. Apologies, I should have had coffee first. And thanks for the info about further topics and reading material. Now I'm a bit embarrassed about my knee jerk reaction but it is what it is and I'll hopefully be a bit more mature next time.
&gt;I've been a fan lately of applying normalization (eg from relational algebra/SQL databases) to Haskell datatypes. Factoring out common structures into their own records and replacing nesting with references helps tremendously to *avoid* the problem lenses solve by not deeply nesting things in the first place. And you get all the benefits of a normalized SQL database, too. Can you elaborate a bit? Maybe some examples? It'd be of tremendous help.
I'm not affiliated in any way. Apologies if it was posted already.
He he...your motivation explanation is literally one sentence, and it didn't motivate me to read more..Sorry. If you have written about this somewhere for people without a whole lot of surrounding context I ll be happy to read it...
It is
Art is produced and consumed, and it's creation is productive.
This site does a great job of promoting F# and functional programming in general. We don't have anything like it in the Haskell community (or didn't, last time I made an effort to look).
I hope you write another blog post covering this!
OK, could you reproduce it in beta44 (it's in stable channel BTW) and report a bug?
Will do! I'm still working out some of the ergonomics, but hope to have something to share by the end of the week.
&gt; haskell job in my country :) &gt; At least 3 years industry experience in Haskell or other typed functional programming language :(
I'll have a side order of COBOL, injected straight into my optic nerve
I'd rather have a side order of COBOL, injected straight into my optic nerve 
I think I'll have that, with a side order of COBOL injected straight to my optic nerve 24/7
Can someone explain why this is desirable? It's not clear to my inexperienced self what problem this solves. 
I have seen code where GHC failed to specialize to a concrete instance of a type class and that caused a 10x performance hit.
You make it sound like you are directly refuting this : &gt; we have witnessed the high cost of entirely incidental complexity many times over. Our data suggests this issue is real and very common.
That's right, the entire argument remains the same for just plain Haskell records. There is no super clean solution. I use the following when I want code to notify me when fields are added: ``` fooToString :: Foo -&gt; String fooToString x@(Foo _ _) = case x of Foo { fooId } -&gt; "id = " ++ show fooId ```
&gt; or other typed functional programming language &gt; ~~or other typed functional programming language~~ &gt; C++ templates
I'd like different syntax (\`{..}\` vs \`{..!}\` maybe?) for "introduce all these and it's okay if some aren't used" vs "introduce all of these and warn if any are unused". "Each of these must be used or shadowed" is another option that I think I'd ever have found useful, but which might well not carry its weight.
Right, this too-fast-too-free repo you have is definitely along the lines of thinking of. The reason this is so fast is because you get to use `&gt;&gt;=` from whatever you finally instantiate `m` to - monadic binds become super cheap. And that's just how it should be! The majority of our code is just boring binds, but when we work in just `forall m. Monad m`, GHC can't inline multiple binds together into efficient code - so it really matters how fast a non-inlined `&gt;&gt;=` is. Here though, your dictionary will just be the `Monad IO` dictionary which has very fast binds because it just shuffles an unboxed state token around. Looks promising!
Because yes
That's what https://www.fpcomplete.com does, though with quite a more corporate feel. fsharpforfunandprofit is also from [a member of] a for-profit consultancy, but feels more like a one-person/community site.
As someone still learning the language I especially enjoyed the part breaking down language extensions.
I see, no worries. I started doing it once using Diagrams, but turns out Diagrams is a much more of a huge beast than I anticipated and I ended up losing motivation and abandoning the problem.
I am interested in the haskell ABI concerns, do you have a link(s) I can read more about it?
I find it interesting that Haskell wasn't disqualified more due to it's relatively poor supporting ecosystem. &amp;#x200B; Personally, I find it pretty easy to discourage the use of 'fancier type system features' in long lived development projects than to build an integration layer for some legacy tech from scratch. In my experience, most enterprise developers do not need very much encouragement to stay away from unfamiliar or esoteric featuresets.
I do not. 😕
To have the power of dependent types along with the ecosystem of Haskell libraries and tooling.
Because of speed. Agda and Idris are highly unefficient (I still love them as languages). Also, Haskell ecosystem.
You're applying a pure function to a monadic computation. This calls for `fmap` or it's operator version `&lt;$&gt;`. So you're almost there: contentsLines &lt;- lines &lt;$&gt; hGetContents handle
Thank you
What is the perceived disadvantage to dependent types?
I think the issue is already raised here https://github.com/rikvdkleij/intellij-haskell/issues/187 but it's still open.
The current haskell ecosystem should be rewritten in order to take advantage of the new typesystem. Otherwise, haskell ecosystem could be transcopiled into Idris/Adga. Moreover, Haskell compilation time would increase due to the more complex typesystem. Another problem may rise with type inference, more complex due to the enlarged typesystem
More complex type inference and longer compilation time. Apart from the fact that developing it would require time, and there are languages similar to haskell that already supports it. 
This is so 2018.
Not quite. I'm refuting this: 1. "But who really has this experience?" -- plenty of people have experience in maintaining code with fancy types. At the moment `servant` has 26575 downloads in total and types there are really fancy (which does have its tradeoffs, for example, error messages suck). Mark himself has some very nice libraries that use `DataKinds` and `TypeFamilies` (I've used his `req` and I think this library is very well-written). `lens` has 260436 downloads a total, is there somebody who thinks types are not fancy there? 2. I've never seen a situation where fancy types are a problem, but otherwise everything is fine with the code. Junk at the type level -- junk at the term level. Low test coverage, missing documentation, silly exceptions handling, messed up business logic, broken abstractions and so on -- in my experience those always accompany overengineered types. If somebody does not think too much about trade-offs, he gets mess everywhere, not just with types. My guess is that tweag.io started to help people with their code and realized that it's hard for them to do so, because there is a huge mess at the type level. This is indeed common. But that's not the root of all evil, it's just what someone new to the codebase immediately stumbles upon. The root of all evil are irresponsible developers, incompetent managers and short deadlines (and perhaps a bunch of other not easily fixable things), which all entail bad software development practices at each level of the development process. So that advice "stick to Haskell98 and stay in the pit of success" is just not going to work, because, you know, people write bad code without fancy types all over the place.
I'm curious about how dependent types will interact with Haskell-style typeclasses. Also, IIRC the fact that `Type :: Type` will make some things easier thanks to not having to bother about hierarchies of universes, but at the cost of [dropping some guarantees](https://www.reddit.com/r/dependent_types/comments/4gm4lb/is_typetype_dangerous_in_practice/).
&gt; You almost certainly do not want to use stack install I was very confused by this until I realised that author was addressing a crowd that doesn't have a background of using `./configure &amp;&amp; make &amp;&amp; make install` a lot.
I find that this is a strawman argument. I've yet to see any enterprise project derailed by the use of fancy type system or high level abstractions. For some reason author assumes that if you let people use haskell, then everyone will be creating cryptic operators and peppering their code with them. &amp;#x200B;
*Wooo* It's your **5th Cakeday** eacameron! ^(hug)
has anyone used Haskell with MSSQL in anger?
You mention \[\`Any\`\]([http://hackage.haskell.org/package/base/docs/Data-Semigroup.html#t:Any](http://hackage.haskell.org/package/base-4.12.0.0/docs/Data-Semigroup.html#t:Any)) a lot in you're segment regarding \`Strings\`. I don't think this is the type you meant. I think you meant \`(a -&gt; a)\` or \`(a -&gt; Text)\`.
Yes... the only library available to use was an ODBC wrapper by one of FPComplete's devs. Was mostly fine with the exception of the lack of transaction support. I added a pretty quick version but I've yet to clean it up enough where I feel like I could submit a PR. The FFI was surprisingly nice to work with though!
No, they don't. They don't mean the `Any` type you linked to either though. They literally just mean "any". As in a type that can hold any value, not an expression that could have any type.
Previous discussion: https://www.reddit.com/r/haskell/comments/7wmhyi/an_opinionated_guide_to_haskell_in_2018 I learned a bunch of neat stuff from it. default-extensions in particular helps reduce clutter at the top.
No, it's a bit more typed than that: It's a sequence of bytes! Sure, you can give it other meaning, but it can't contain pointers, so it can't contain any other object. It's just `UArray Int Word8`, really.
I mean [this](https://en.wikipedia.org/wiki/Binary_large_object) kind of blob
**Binary large object** A Binary Large OBject (BLOB) is a collection of binary data stored as a single entity in a database management system. Blobs are typically images, audio or other multimedia objects, though sometimes binary executable code is stored as a blob. Database support for blobs is not universal. Blobs were originally just big amorphous chunks of data invented by Jim Starkey at DEC, who describes them as "the thing that ate Cincinnati, Cleveland, or whatever" from "the 1958 Steve McQueen movie", referring to The Blob. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/haskell/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
Disappointing article from an otherwise great site. The contents and the way the arguments are presented smell like Blub programmer syndrome, specially the Haskell/Scala/Idris ones where there are *actual* reasons such as legacy complexity and tooling deficiencies to criticize instead of "it's just like Blub but with all this other complex stuff I don't need in day to day, and it's of course too hard for new developers!". And it's a specially sad scenario in Idris' case given that dependent-typing absolutely shines at the business domain modeling tasks fsharpforfunandprofit.com otherwise presents so well. I of course do not know how much time Scott has actually spent writing Idris, but I really wish he'd spend more time researching how well it binds with the ideas he literally sells and how much it retains some of F#'s nicer features and improves on them, such as type providers and being able to compile to many different platforms.
Haskell's language extensions are one of the most alluring things about it in my opinion. They're akin to power-ups in games, but it's a power-up of your understanding and ability. Each one signifies a capability you've learned on top of the base language. Also, with the extension lines at the top of the file, it's the first thing a reader sees, so it gives an indication of the sophistication level of the author and sets some expectations for the reader. That said, I only know how to use a few so far! But it's exciting to learn a new one because it really is akin to leveling up in a game, in order to bring more power to bear on the problem you're solving.
As far as I understand it would rather be a GHC extension, which means if you don't want it you don't use it... It's actually one of he most underrated part of GHC, its modularity.
I dunno. Given how many of the popular libraries do this it's not an unfair concern. And even if they don't, all it takes is for someone to use something like Lens...
The issue isn’t about stringly typing. It’s that a lot of Haskell apps use ByteString as a sort of “optimised” UTF8 String, _after_ the boundary point (eg Cassava). The documentation promises it’s ASCII or UTF8 but the type doesn’t guarantee that. It’s a bizarre omission in a language that otherwise uses separate types for separate semantic meanings. ByteString is essentially a raw untyped pointer, Haskell’s equivalent to C’s `void*`. It should almost never come up, yet there are quite a few libraries that use it as an optimisation. Really, `String` should be deleted (in an age of UTF grapheme clusters it has negative pedagogical value), `Data.Text` made the default, and `ByteString` usage as a maybe-UTF8 String challenged relentlessly. 
Ah cool, at least MS is paying attention to drivers on linux now https://docs.microsoft.com/en-us/sql/connect/odbc/linux-mac/installing-the-microsoft-odbc-driver-for-sql-server
I hope those allegations turn out to be some form of misunderstanding. And when they do it'll still be disturbing that so many SJWs were attacking Chris instead of applying the innocent until proven guilty principle.
"This post is part of an effort to encourage contributions to Smos." But how am I supposed to contribute? Looking at the smos repository everything seems to be licensed "All Rights Reserved". I'm probably in legally questionable waters to even write a patch and send that to you. So, of course, I won't and there's not even a point for me to look at this further since I can't do anything with it.
&gt; ByteString is essentially a raw untyped pointer, Haskell’s equivalent to C’s `void*`. More like `(unsigned char)*`, no?
Yep, got pretty acquainted with that article in the process haha.
*Advocatus Diaboli* `ByteString` seems fair enough for representing ISO-8859-1 (latin1) text (say, when parsing legacy formats/protocols). A newtype wrapper might be better, but it's not such a big deal IMHO, given how isomorphic `ByteString` is to a hypothetical `Latin1String` (the iso commutes with indexing and basically everything) - in contrast to a `ByteString` vs UTF-8 text.
&gt;The current haskell ecosystem should be rewritten in order to take advantage of the new typesystem &amp;#x200B; That's not a good plan, or a rational belief. &amp;#x200B; Even if dependent types are implemented in an incredibly wonderful and elegant way, they will not be the best fit for every library out there. &amp;#x200B; Similarly, very few people are going to want to switch to writing large sections of their codebase in a different language and "transpiling" it to Haskell. GHC compile times would likely go up for code that is using dependant types. I don't see any particular reason it would go up for code that isn't. A similar situation applies to type inference. &amp;#x200B; GHC currently has some extremely powerful and really great type system features today, and they have the same trade-offs you're discussing here. These problems have not leaked into everyone's Haskell experience universally. I see utterly no reason to believe that dependent types will be any different.
The primary reason that I've heard was so that we didn't have to abadon all the libraries that had already been written for Haskell. Idris and Agda are great, I agree, but their library ecosystem is much smaller than hackage, and coming from Python (or even Rust) hackage already feels a bit small.
Well, no one is thinking of rolling dependent types into the next report or even allowing them in GHC without an explicit extension. The implementation does actually change the GHC internals, but one of the promises / requirements is that any expression where the type of correctly inferred before the change will still have it's type correctly inferred after the change. I don't have any information that might assuage your fears around compilation time, though. --- Let me know when you have that Haskell -&gt; Idris/Agda transpiler ready though. ;)
Well, no one is thinking of rolling dependent types into the next report or even allowing them in GHC without an explicit extension. The implementation does actually change the GHC internals, but one of the promises / requirements is that any expression where the type of correctly inferred before the change will still have it's type correctly inferred after the change. I don't have any information that might assuage your fears around compilation time, though. --- Let me know when you have that Haskell -&gt; Idris/Agda transpiler ready though. ;)
Thanks. Taking my time to digest what you've said.
&gt; or even Rust I find that hard to believe. Haskell and Rust are very similar in popularity according to various metrics. 
*Using* a library with fancy types is often a lot easier and less "dangerous" than trying to invent one's own fancy-types-based abstractions. I find that it's the latter which causes most of the problems -- it really takes a lot of practice to get that sort of thing right.
Saying that Purescript is frontend-only is equivalent to saying that Javascript is frontend-only, which is of course nonsense. Purescript has no runtime, so there's no reason you can't run Purescript on Node. I'm building an app right now (which is very much "enterprise software") doing just that, in fact. Since Javascript is probably the most ubiqutuous language on the planet, it seems like Purescript checks all the boxes in this article, all while having better functional programming facilities than F#. HKTs, Typeclasses, and other advanced type system features are immensely useful for domain modeling and enterprise software. They enable us to have consistent, simple, and expressive APIs like `Functor`s, and things like [type-safe, ergonomic checked exceptions](https://github.com/natefaubion/purescript-checked-exceptions) (especially useful considering how critical error handling is to enterprise software). The more guarantees we can get out of our compiler, the less we have to deal with inscrutable runtime errors, and thus more maintainable code.
That's when you use a spcialise pragma right? I fail to see why you would need template haskell.
Thanks for the feedback! I have seen there are new tools available and test them soon. Please feel free to create an issue or even a PR adding more info.
Given that we have type families in Haskell which allow us to express type-level computations, how is a core language of System FC sufficient? Put another way, why do we not need something like (making up stuff here) System FωC? Is this related to the point that type families need to be fully applied?
Any byte sequence is also a valid big integer or an RGBA buffer and a host of other things. There is nothing about `ByteString` that suggests that there are Latin1 characters in it, and in fact I’ve never had this situation come up despite commonly using it. The point isn’t that `ByteString` is a bad format for data to be in, the point is that it is bad as a type because it doesn’t tell you what’s in it. You’ll have a pretty bad time once you try to display your Latin1 as a RGBA texture.
&gt; use it as an optimisation But it’s not! Wrap a newtype around it, problem solved. Not sure if fusion works through new types, but even if it doesn’t you could just provide bulk operations that internally unwrap.
`void *` and `char *` are more or less equivalent in C, in that they can be freely converted to each other and both basically mean “pointer to anything”. Only minor differences like technically `void *` doesn’t support pointer arithmetic per the standard because `void` has no size, but it’s in practice supported as an extension nearly everywhere.
Personally I am *strongly* against default-extensions. Sure you "waste" a few lines at the top of files, but it means all I need to know is in a single file, rather than having to remember whatever happened to be in the cabal file. 
&gt; it doesn’t enjoy the same amount of caching as `cabal new-build` or Nix, it caches most packages, and it also makes things like Git-hosted sources incredibly easy, which (as far as I can tell) can’t be done with `cabal-install` alone. For some reason it's hard to find this information in the cabal docs, but it's there: https://www.haskell.org/cabal/users-guide/developing-packages.html#source-repositories e.g. put this in your `cabal.project` file: source-repository-package type: git location: https://github.com/blah/repo subdir: subdir (if necessary) tag: commit_sha
More extensions do not imply higher sophistication or a better design, in my opinion. In fact, often the opposite. Several extensions truly are all upside (e.g. most of the deriving family). But I think the more advanced extensions to the type system, and even many of the syntactic extensions are best used judiciously.
I don't see how that is different from having the compiler turn on extensions (e.g. PatternGuards is on by default) but then I don't feel very strongly about it. I do my thing if it is my own code, and follow other people's conventions when working on their code. 😄
&gt;I don't see how that is different from having the compiler turn on extensions (e.g. PatternGuards is on by default) but then I don't feel very strongly about it. I do my thing if it is my own code, and follow other people's conventions when working on their code. 😄 It doesn't turn on extensions by default, PatternGuards (and also EmptyDataDecls and I think one other) extension were included in the Haskell2010 and are therefore standard Haskell, but everyone always forgets Haskell2010 did, in fact, change things. 
That said, if you're writing code for yourself and using extensions is fun (since you're leveling up, as you put it), then go for it! Just re-read my comment and realized it's a bit of a fun squasher. Didn't intend that.
f# is not suited for professional software development; because its type system is much too weak; it does not even have type function polymorphism \["higher kinded types"\] only 2 industry-strong languages have it : haskell, scala; i think haskell is the better one
Fair point, I overlooked that. I don't think either of us is going to convince the other so let's call it a day.
I always wonder why we don't have an `mtl`-like TH library that you can use to construct your monad stacks. But unlike `mtl`, the stacking would produce a single type with all the bells and whistles. I know the approach in `mtl` is more powerful, but I see the following pattern very often, and these cases can easily take advantage of TH: ``` type AppM = FooT (BarT IO) ``` So we could instead have: ``` $(flatten "AppM" $ fooT (barT io)) ``` Which results in a guaranteed-to-be-efficient flattened `data AppM = ...` along with all the `MonadXXX` instances.
Honestly, that last comment was mostly to enlighten the reddit peanut gallery and stop telling people that PatternGuards is an extension ;) 
Agda already transcompiles to Haskell (https://en.wikipedia.org/wiki/Agda_(programming_language)#Backends) the other way round is of course not possible because of the less expressive type system. For this reason is it possible, theoretically, to exploit the Haskell ecosystem in Agda but not the other way round, at least without losing the enriched Agda types. 
There are a few reasons. Firsty, we have the "obvious" benefit (but is it really obvious?) that dependent types could let us write more correct software. Haskell is a production language with an attitude that we don't like to take chances. The more we can specify in the types, the better the potential for us to write correct software. Next, there is the interesting research aspect. How do you turn a non-dependently typed language into a dependently typed one? How do you do it while retaining all the other features of Haskell? Third, Haskell's dependent types are not just cargo culting dependent types from other languages. Haskell is \*not\* a formal proof system, and it never will be. Dependent types don't change that. So now we have dependent types but without a totality checker, and a lot of the other machinery that they are often associated with. What does this look like? We don't know! Fourth, I think it's clear that people have a desire to have this feature. We have a ton of language extensions that are all approximating various directions of dependent types. Dependent Haskell could do a good job of unifying those features under a common framework, and perhaps providing much better (less!) syntax at the same time.
RemindMe! 10 days "Reading about constrained type classes, come back later to understand better"
I will be messaging you on [**2019-02-25 09:18:12 UTC**](http://www.wolframalpha.com/input/?i=2019-02-25 09:18:12 UTC To Local Time) to remind you of [**this link.**](https://www.reddit.com/r/haskell/comments/anou14/does_using_type_classes_mean_balancing_tradeoff/) [**CLICK THIS LINK**](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[https://www.reddit.com/r/haskell/comments/anou14/does_using_type_classes_mean_balancing_tradeoff/]%0A%0ARemindMe! 10 days ) to send a PM to also be reminded and to reduce spam. ^(Parent commenter can ) [^(delete this message to hide from others.)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Delete Comment&amp;message=Delete! ____id____) _____ |[^(FAQs)](http://np.reddit.com/r/RemindMeBot/comments/24duzp/remindmebot_info/)|[^(Custom)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[LINK INSIDE SQUARE BRACKETS else default to FAQs]%0A%0ANOTE: Don't forget to add the time options after the command.%0A%0ARemindMe!)|[^(Your Reminders)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=List Of Reminders&amp;message=MyReminders!)|[^(Feedback)](http://np.reddit.com/message/compose/?to=RemindMeBotWrangler&amp;subject=Feedback)|[^(Code)](https://github.com/SIlver--/remindmebot-reddit)|[^(Browser Extensions)](https://np.reddit.com/r/RemindMeBot/comments/4kldad/remindmebot_extensions/) |-|-|-|-|-|-|
I’m which cases do you need to know which extensions are enabled? In other words, which extensions cause you to read Haskell differently? I find that I’m able to deduce, from reading the code, which extensions are enabled.
I see your point, and it helped me better understand the underlying idea over dependent types, but I don't think this "dependent extension" is so easy to implement as you may seem to believe. The GHC typesystem is unique, and it doesn't work with the extension mechanism you said. It gets enriched year after year in order to support more and more features. A major change happened in 2006 with the inclusion of type coercions in order to support GADTs. (https://www.microsoft.com/en-us/research/wp-content/uploads/2007/01/tldi22-sulzmann-with-appendix.pdf?from=https%3A%2F%2Fresearch.microsoft.com%2Fen-us%2Fum%2Fpeople%2Fsimonpj%2Fpapers%2Fext-f%2Ftldi22-sulzmann-with-appendix.pdf) Actually, the only reason for requiring the GADTs flag during compilation is because is not yet a standardized Haskell syntax, but in truth no module/extension is enabled when the given flag is set. Now, even if you don't use GADTs at all, the compiler will typecheck your program using FC, and not the previous, smaller, typesystem version. But luckily, the impact in compilation time is very small. I'm not so confident the inclusion of dependent types would behave the same (see TAPL https://www.cis.upenn.edu/~bcpierce/tapl/ page 464). And a future GHC version in which different typesystems co-exists and are enabled/disabled in order to support or not dependent types seems to me too much work (a typesystem change requires years and a lot of beautiful mind's work). For those reasons I believe a DT support would bring, apart from a lot of work, also some disadvantages, increased compilation time at least. So maybe is not the right path for GHC, now that has reached worldwide popularity. I'm quite new to GHC actual implementation, I'm studying it for my thesis. I thought probably the reason GHC does not support dependent types yet is exactly this one, I don't know...
please look at my reply to IronGremlin https://www.reddit.com/r/haskell/comments/aqmmu6/dependent_haskell_why_should_be_implemented_in/egiqfdv/
@TravisMWhitaker &amp;#x200B; Do you guys use any open and standardized communication protocols for your meshed network devices? &amp;#x200B; It would be very interesting to tinker with applications to manage collective intelligence but there's this vast amount of open protocols and it's unclear to me whether there's any consensus on a standard in the industry. &amp;#x200B;
It is going to be interesting for sure! I love dependent type systems, but, as you said, turning a not-dependent language into a dependent one is far from a trivial work (if possible at all). I believe that a dependent types Haskell versions would for sure bring undoubtedly advantages, but also some disadvantages. And an "on/off" approach is not suitable for the complexity of the job (please look at my reply to IronGremlin https://www.reddit.com/r/haskell/comments/aqmmu6/dependent_haskell_why_should_be_implemented_in/egiqfdv/). I think we should be at least cautious with this feature, introducing only the good parts of dependent types. Those systems have the bad reputation to not scale due to the complexity of the typesystem (at least the fully dependent ones).
A person, who went (`C#` -&gt; `F#` -&gt; `Scala` -&gt; `Haskell`) here ;) This article is good and echoes a lot with my experience. However. &gt; More importantly, there is no gradual migration path to Haskell – you are thrown in the deep end. That might be great for learning FP but is not suitable for enterprise, IMO. I don't think it is true. What is the "gradual way" to learn, say, JavaScript for a C++ developer? Or the other way around? Or WinForms -&gt; Xaml (so it is not specific to languages)? There is probably none. And that's OK. Because it is usually not that the whole team/company is just "thrown in the deep end". Even with F#, I doubt that the migration happens with PM's decision "tomorrow we all write F#". Most likely it happens from within the team when there already is some interest, some (even little) experience, some understanding. Even if no one at all is familiar with the technology, then it'll be research, learning, until _some_ expertise is acquired. But the same works for `Haskell` (or anything else). We start small, we learn, we expand. How writing a non-critical small service in Haskell is not "gradual migration path"? Or running a little internal weekly workshop? There can be many steps in this path. I guess it is not like being "thrown in the deep end" to separate those who drown and those who are great Haskell developers :) &gt; I’ll let you judge whether this is a deal-breaker for enterprise development. That was a deal-breaker for me (and I always stayed with enterprise). Not typeclasses, but HKT. It was the reason I left F# for Scala. And I don't think that HKT is not implemented in F# because it is not needed for enterprise development. There are deep technical reasons for it to not happening...
The usual way to handle that is to use precedence levels as in `showPrec`. Have a look at the documentation of [Text.Show](http://hackage.haskell.org/package/base-4.12.0.0/docs/Text-Show.html) which shows how to write a typical instance of `showPrec`.
And if we had UTF8 and ASCII and Latin1 newtype wrappers around these, each with appropriate (and necessarily different) implementations of things like toUpperCase, both I and the original author would be happy. But instead we have a bag of bytes, which the docs say _should_ be UTF8, and so we hope rather than know that the custom UTF8 toUpperCase we imported causes no runtime errors, since there’s no information for the compiler to provide any guarantees. And if I’m happy with runtime errors, then why am I using Haskell when I could just be using Ruby? 
Looking at the type signatures: lines :: String -&gt; [String] hGetContents handle :: IO String (=&lt;&lt;) :: (a -&gt; m b) -&gt; m a -&gt; m b so for `??? =&lt;&lt; hGetContents handle` to typecheck we need a function of type `String -&gt; IO [String]`, like return . lines =&lt;&lt; hGetContents handle And return . f =&lt;&lt; m == fmap f m == f &lt;$&gt; m This is one of the laws of the monad typeclass. Written another way: (&lt;=&lt;) f g a = f =&lt;&lt; g a return &lt;=&lt; f == f f &lt;=&lt; return == f f &lt;=&lt; (g &lt;=&lt; h) == (f &lt;=&lt; g) &lt;=&lt; h Which are the monoid laws.
use a newtype + generalizedNewtypeDeriving ``` newtype AppM = AppM (FooT (BarT IO)) deriving (Functor, Monad, Foo, ...) ``` last time I tried this, it would (with optimizations on) remove the layers and be just as efficient as a handwritten instance. 
I think compile times as you mention is an issue, but if our tooling can be fixed to make better use of all available cores, also during compilation of a single package, and even during compilation of a single module, then that would offset the slowdown. I believe that 99% of developers have idle cores while compiling Haskell these days.
Wonderful. The next step is to use type classes to 're-sugar' things, yes, using "tagless final" and make the approach even more modular.
Type inference would obviously be a problem, yes. I don't think compilation time is really that bad though
Just apply.
Cuz it didn't work? :^)
* crates.io: 22k packages (https://crates.io/) * hackage: ~14k packages (I can't find a good summary, but https://wiki.haskell.org/Hackage_statistics helped) Not quite double, but hackage still seems small.
Agsa -&gt; Haskell is rather off topic for the current conversaion, so I'm not sure why you brought it if. If Haskell -&gt; Agda is impossible (which I do not believe, type synthesis and tagging are always possible approaches to having the compiler "enrich" the type system), that just reinforces the need to bring dependent types to Haskell so we can benefit from both them and the large body of libraries on hackage.
I absolutely agree with the general idea. We shouldn't use the same types for distinct domain concepts just because they have (or can be made to use) the same representation. I guess my reasoning was that most (all?) the *operations* on `ByteString` are meaningful on `Latin1` too, e.g. if we have `decodeLatin1 :: ByteString -&gt; Latin1`, then `decodeLatin1 (x &lt;&gt; y)` = `decodeLatin1 x &lt;&gt; decodeLatin1 y` `decodeLatin1 (take n x)` = `take n (decodeLatin1 x)` ... and so on. I agree the newtype is still better, but the payoff is less than with big integers or RGBA buffers, which have very different domain operations. Maybe a clean but less boilerplate-heavy way would be `newtype Char8 = Char8 Word8` with `type Latin1 = Data.Vector.Unboxed.Vector Char8`.
It just works?
There is also https://code.world (simplified subset of haskell) and https://code.world/haskell (GHC Haskell, but no template haskell iirc). maybe u/cdsmith could comment on the differences
oh, that is really cool, I'm really surprised how these things are not more widely known. Is there some reason people don't write tutorials in an environment like that and share them?
The thesis of this project is that the best Haskell sorting library on Hackage / github is 30% or slower than the best sorting library available in Rust or Julia or R.
I would say the advanced language features are more "confusing" than "dangerous" most of the time. And libraries can use them internally and I personally don't care. The only problem is: 1) if I have to use them to use the library (i.e., I must enable language extensions to even use the library) 2) the library exposes a billion operators (i.e., "wtf is &lt;#\^#&gt; ? Oh, it has a named function called \`append\` that's a synonym for it")
Thanks for the mention! I want to point out that you can get IHaskell in your browser with minimal effort by visiting https://mybinder.org/v2/gh/gibiansky/IHaskell/master
What kind of people we need: People who can program in C, Rust, or Julia; people who know Intel caches and Intel SIMD instructions well; people who know sorting algorithms such as Pattern-defeating quicksort or even better ones; people how to know details for optimizing ghc code (e.g., prim operations); people experts on benchmarking.
But... It works for me!!! (Joking - didn't apply)
uuuuh, excellent! Another TIL!
&gt; And I don't think that HKT is not implemented in F# because it is not needed for enterprise development. There are deep technical reasons for it to not happening... Would you mind elaborating? I’m curious about your thoughts on this.
Also, I would greatly appreciate any points to relevant papers. The one I found just now: [https://arxiv.org/pdf/1811.00833.pdf](https://arxiv.org/pdf/1811.00833.pdf)
For me it's more about documenting how the code works. Harder to know which extensions were required to build the file if they're not listed in the file.
This was not available at the time the article was written IIRC
I turn on extensions as I need them as well, I don't currently have any that are on by default. A really simple one is TupleSections, I find myself using it a lot but I wait until I need it before enabling it instead of having it on by default. I totally agree that simple is better!
If you open a new notebook inside the `ihaskell` directory you can use any of the packages listed in [`ihaskell.cabal`](https://github.com/gibiansky/IHaskell/blob/master/ihaskell.cabal), otherwise you can also [install your own packages](https://github.com/gibiansky/IHaskell/issues/1000#issuecomment-452174417).
Iirc, hdbc-odbc ([http://hackage.haskell.org/package/HDBC-odbc](http://hackage.haskell.org/package/HDBC-odbc)) still works fine too.
Here is the output of `:!stack exec -- ghc-pkg list` in a notebook cell: ``` /opt/ghc/8.6.3/lib/ghc-8.6.3/package.conf.d Cabal-2.4.0.1 array-0.5.3.0 base-4.12.0.0 binary-0.8.6.0 bytestring-0.10.8.2 containers-0.6.0.1 deepseq-1.4.4.0 directory-1.3.3.0 filepath-1.4.2.1 (ghc-8.6.3) ghc-boot-8.6.3 ghc-boot-th-8.6.3 ghc-compact-0.1.0.0 ghc-heap-8.6.3 ghc-prim-0.5.3 ghci-8.6.3 haskeline-0.7.4.3 hpc-0.6.0.3 integer-gmp-1.0.2.0 libiserv-8.6.3 mtl-2.2.2 parsec-3.1.13.0 pretty-1.1.3.6 process-1.6.3.0 rts-1.0 stm-2.5.0.0 template-haskell-2.14.0.0 terminfo-0.4.1.2 text-1.2.3.1 time-1.8.0.2 transformers-0.5.5.0 unix-2.7.2.2 xhtml-3000.2.2.1 /home/jovyan/.stack/snapshots/x86_64-linux/lts-13.7/8.6.3/pkgdb (no packages) /home/jovyan/.stack/global-project/.stack-work/install/x86_64-linux/lts-13.7/8.6.3/pkgdb (no packages) ```
I believe repl.it and ihaskell are both running sandboxed code in the server. CodeWorld is compiling to JavaScript with GHCJS and running in the browser. That means CodeWorld is a little further away from a standard Haskell environment; but on the other hand, it makes graphics, animations, and games possible in a way that really isn't when running on the server.
&gt;The thesis of this project is that the best Haskell sorting library on Hackage / github is 30% or slower than the best sorting library available in Rust or Julia or R Source for the 30% figure/estimate?
I think it comes down to "marketing is hard". I have done my best to let people know about CodeWorld, but after a while I'm just spamming the people and subcommittees I know about.
One thing that might be helpful is to have a list of a few of the coolest CodeWorld examples online. I'd find that useful for giving short lessons on Haskell because I could go to one of those projects and show how easy it is to add features to a haskell program in a low-overhead way.
Unambiguity in lambda expressions come from associativity of declaration and application. For example: \x.\y.xy is the "shorter" version of \x.(\y.xy) since there is no need for the parantheses because declaration is right-associative. \x.(\y.xy)z however cannot be shortened this way. Similarly, ((\xyz.xyz)(\a.a))(\b.b) can be expressed as (\xyz.xyz)(\a.a)(\b.b) since application is left-associative. Again, this will not work on something like z((\xyz.xyz)(\a.a))(\b.b). That means you can remove parantheses of declarations to the right of declarations as long as these span "to the end" (in the corresponding necessary parantheses) and you can also remove parantheses of applications to the left of applications as long as those span "to the beginning".
Re: 1: I agree somewhat. This can feel like treading on dangerous ground, but it obviously depends on the extension! :) Re: 2: Meh. Some libraries might be a little operator-happy, but *usually* there are plain-text aliases if you *really* want them[1], but usually there is some sort of coherent system to them. (I note that this is also straying a bit from the original complaint ITT.) [1] Of course enforcing usage of 'named' operators might be a bit tricky, but... this something that can be 90%-solved by code review.
Hi! This was a really helpful comment. I immediately added the MIT license to all of the pieces: [https://github.com/NorfairKing/smos/commit/424c9571a81ff1002ae76404f7240c8f5f4bc99f](https://github.com/NorfairKing/smos/commit/424c9571a81ff1002ae76404f7240c8f5f4bc99f) I hope that helps.
\&gt; as a question of subjective perception &amp;#x200B; For sure. That ambiguity is why I'm only "a little leery" rather than significantly opposed :-P &amp;#x200B; \&gt; I'd personally prefer to see violations in base encoded as part of the test suite, if possible, as some sort of doctest, so that it can function as a living document of failing instances &amp;#x200B; Something like that could be great! &amp;#x200B; \&gt; making half-baked laws to 'formally' represent bad behavior. &amp;#x200B; I'm not sure that's how I'd characterize it? I see it as avoiding stating that things are laws when they aren't actually obeyed everywhere (relevant).
During my PhD, I did study the topic for 3 weeks about a year ago. I also did benchmarks that probably were not good/wrong. However, from the tests and study, 30% seems to be a doable goal. I would not be too surprised if even 50% or twice as fast would be possible for the user of a sorting library. There are guesstimates, were Haskell looses time; another 'guesstimate': pointer indirection, non optimal caching, non optimal branch predication, etc.
Then I think what I have is a naming complaint. `enumLaws` to me implies "laws all enum instances should respect". If it was `unboundedEnumLaws` and `boundedEnumLaws` I'd be entirely on board (WRT `Enum`, handwaving `Double`).
https://leanpub.com/haskell-cookbook/
A great follow-up https://www.manning.com/books/haskell-in-depth
It should work, then you didn't put it the right way. If you use it on an imported function, you need to declare the imported function INLINEABLE.
This is looking *really* good. Should out to @chrisdoner for Intero! \o/
Fully qualify it. eg, even if `join` isn't in scope, you can still write `Control.Monad.join`.
https://simonmar.github.io/pages/pcph.html
[TDD w/ Idris](https://www.manning.com/books/type-driven-development-with-idris) Similar syntax and TDD can be used in Haskell, albeit to a lessor extent. See: https://www.reddit.com/r/haskell/comments/99toaz/tdd_with_idris_book_vs_haskell/
I mean, that's the whole point of testing software though. You have desirable behaviors, and you have implementation, and testing is measuring the distance between those two things. `Laws` in this context aren't supposed to be a document of how things do work, they're supposed to be a model for how things should work. Because they are software tests. If there is some desirable quality to the difference in behavior, that's one thing, but for the most part it's just that the implementation of the instances didn't quite get there for one reason or another, not that they deliberately defined a new set of desired behaviors. Blessing them as a standard by which you can measure success just means someone else can write a bad instance and get a green checkmark that tells them they did a good job.
All true statements. But for me at least, not having a huge list of mines that you can avoid stepping on if you catch it in code review is on of the primary features of Haskell, so adding on to the list isn't exactly a win. I'm not sure it's a mechanically solvable problem unfortunately. On one hand, the ++--_$ operator is super gross. On the other hand, I want an operator for both the cross product and the dot product...
Is that actually a real operator?
My understanding was that there's some problems with representing HKTs in CLR bytecode, similar in feel but different in detail to the way Java Generics are erased before reaching the bytecode (back in 1.5, I don't know if the implementation has changed enough that they are retained in some way). I'm fairly sure as soon as HKTs have some reasonable representation in the CLR, that F# will implement/expose them ASAP. (But, I haven't watched the CLR evolution much, last one I really understood was shipped w/ .Net 3.0)
You should just need to add `{-# language FlexibleContexts #-}` to the top of your file, or type `:set -XFlexibleContexts` into `ghci`.
And the module itself with double quotes IIRC
I appreciate you taking the time to answer, but the original post shows the second error that comes up when I do add flexible contexts. Could you help with that second error?
thanks
it works
Oh sorry, I struggled to see that through the formatting. I'm not sure on that one in afraid
isn't it confusing to say "TDD" when most people think it means "test driven development"?
You're mistaken.
[Thinking with Types](https://leanpub.com/thinking-with-types/) by Sandy Maguire.
Sounds more like a third one for me... As great as it may be !
Please enlighten me
Just keep saying it until most people think the T stands for type!
I think the second error is saying you need an instance of the form Instance NFData a =&gt; NFData (IList a) The documentation for that class has some examples of how to get such an instance using deriving + generics. I'd copy it to this comment, but formatting code on a phone is a pain. Don't know if that's what you need exactly, because I'm not familiar with that book and not quite sure what you are doing. But that's what I make of the error message, at least. 
In fact I think OP does not want FlexibleContexts; GHC was just throwing up a bad error message to begin with.
[http://haskellbook.com/](http://haskellbook.com/)
I agree that instance is needed. As to why it's needed (just in case): note that the type of `put` requires its argument to be an instance of `NFData`: http://hackage.haskell.org/package/monad-par-0.3.4.8/docs/Control-Monad-Par-Scheds-TraceInternal.html#v:put GHC notices your uses of `put` at the type `NFData (IList a)`, and so needs to know how to satisfy that specific constraint. Since you've told it that the function `streamToList` can only be used on types `a` that satisfy `NFData`, the instance /u/yakrar suggested would let GHC bridge the gap between the to constraints. HTH. From your question, it seems like you're unfamiliar with some "basics". You might consider staying with a less advanced book, then come back to the great stuff in that one. Enjoy!
There's been a bit of discussion around higher-kinded types on GitHub at [dotnet/roslyn#2212](https://github.com/dotnet/roslyn/issues/2212) and [dotner/csharplang#339](https://github.com/dotnet/csharplang/issues/339) (the former was moved to the latter)
I think it's valid. Its a bit of a contrived example, but 3+ character operators are quite common in my experience. Lens defines &lt;&lt;**~ and while I can just not use this operator, lens is a popular library so I'm likely to encounter this library in real code.
I went through the entire Haskellbook, and have been plugging through this one, I'd like to pick up some of the basics that slipped through the cracks as I go skiing
I'm not usually forced into unusual code, so I can't realistically gauge the relative importance of your experience. I can only that that this is not a thing I (personally) have experienced being a problem in practice.
The simple solution to that is not using bytestring for text. It's not what it's for. 
Sometimes. I have used the phrase "the other TDD" before. I wish we did either at my day job. :P
:/ I think this is a better first book, but it might be worth skimming the free version to if it covers something "Get Programming with Haskell" didn't.
I believe that may not reasons for it would be: - Types reification in CLR. HKTs are possible in Scala/Java because of Erasure. It'd be possible to implement "properly" if they wanted to add some extra stuff into CLR, but they (MS) won't do it just because one (and not the most popular by far) language wants it. Making changes to CLR is committing to support it across all the .NET languages, which is a much bigger concern. - Interoperability with C#. I think it is one of the "hard" requirements and selling points for F#. Even if we say that F# compiler could erase types in order to support HKTs (or do something similar about it), it would break probably make this "seamless interop" not possible. - Not everyone in the community wants it. There are loud voices advocating "simplicity" and actively saying "cool, but no, thank you" when topics like HKTs, modules, lenses, etc. are discussed. There are ways in F# to partially emulate HKTs, but it comes with the price, and the complexity of these solutions is a high IMO.
Counterpoint: Isn't it confusing to say `[Int]` (for a singly-linked list of pointers to closures returning signed machine words) when most people think it means "continuous storage of many signed machine words". We have our own jargon on this side of the Curry-Howard correspondence. ;)
I'm under NDA.
[This comment](https://www.reddit.com/r/haskell/comments/8gfan5/a_new_haskell_library_for_talking_to_odbc/dybhzzw/?context=99) from the creator of the more recent `odbc` library suggested otherwise.
Thank you for your hard work! I visit this subreddit quite often (probably too often), but I somehow missed it.
Out of interest, does anyone know why it‘s in Berlin? I doesn’t seem like any organizer is based in Berlin.
I just sent out an email for this. Thanks for the suggestion!
Good article, I agree with most, but I totally disagree with the section about Bytestring. Bytestring is neither about Strings, nor about Any. Bytestring is for low level binary data, such as what you read from or write to a file, or send over the network. For example when you need to parse a binary fileformat, or read data from a low level protocol. It should be always used for converting something from the outside world to haskell and back. It should not be used for processing data inside the program. If you want to have a structure for efficient low level processing, such as images or sound, you are better of with unboxed Vectors. For example, I use bytestrings in my opentype library for reading and parsing binary opentype files, and it's perfect for that.
You need something like this: {-# LANGUAGE DeriveGeneric #-} import GHC.Generics (Generic) data IList a = Nil | Cons a (IVar (IList a)) deriving (Generic) instance (NFData a) =&gt; NFData (IList a) 
[Thinking Functionally with Haskell](https://www.amazon.com/Thinking-Functionally-Haskell-Richard-Bird/dp/1107452643) by Richard Bird.
Highly recommend this as well. Still in draft, but it's worth it, as there are only minor grammatical mistakes here and there, nothing more. I personally cannot wait for the next chapter.
Real World Haskell It’s free!
It’s co-located with ICFP (which changes location each year)
What’s yr day job?
&gt; Anecdotes seem to suggest that enabling `TemplateHaskell` everywhere leads to worse compile times, but after trying this on a few projects and measuring, I wasn’t able to detect any meaningful difference. I will write this up publicly soon, but the key problem with TH is that it destroys incremental compilation (you see `[TH]` as the recompilation reason in GHC's output). When you change a module, then all modules that import it and use TH must be recompiled. If you use TH in every module (e.g. if you use it for logging, or generating lenses in about every file), then modifying any of n files will result in O(n) modules being recompiles, instead of the O(1) that incremental recompilation is supposed to give you. If you have 300 modules, this makes the difference between 3-second recompile time and 3-minute recompile time. I've seen it in many large projects. This problem can be fixed in GHC assuming somebody sponsoring that work.
I used hdbc-odbc very successfully, though not recently. So I wouldn't be so sure that this wasn't a specific (local) issue rather than a general problem...
Honestly -- when I learned there wasn't even a _first_ book to start with. My advice is to start trying to read papers (maybe functional pearls to start) and go straight for the red meat.
&gt; Also, from the enterprise manager’s point of view, it’s critical that the language and its ecosystem have deep support for enterprise databases (Oracle, Sql Server), enterprise web servers, enterprise authentication (AD, LDAP), enterprise data formats (XML) etc. According to this article, I labor daily in "enterprise development" and yet I can't really stand so many of these enterprise things. I must be doing something wrong.
&gt; =&amp;amp;gt; Wow, that used to be `=&gt;`, then it got html-encoded to `=&amp;gt;`, then it got html-encoded again to `=&amp;amp;gt;`? You must have a [really convoluted](https://xkcd.com/763/) way of copying code around :) &gt; loop :: forall a. &gt; NFData (IList a) =&amp;gt; &gt; [a] -&amp;gt; IVar (IList a) -&amp;gt; Par () I see you already know that you can indent your code four spaces so that reddit preserves your formatting, could you please do the same for the error message, so we can read it more easily?
Software Engineer IV at the [TGCS](https://www.toshibacommerce.com/) office in Bentonville, AR. We have tests, we just don't really do tests-first development. So, there's a lot of coverage gaps. And, my lack of self-discipline in the area doesn't help.
My solution is to create a new file containing only the record then to qualified imports. it's awful but easy for beginners to understand and works.
Lol I literally copied it straight from Visual Studio Code into the text box on Reddit, it's obnoxious how much stuff gets lost in translation. I indented the code before I posted it and it just removed the indentation so I had to go add all the indentation back by hand
Thank you, that did the trick!
I didn't even finish LYAH and got this book and skimmed the chapters on MVars and Chans. It was so easy to write concurrent, interactive programs with what I got from that. I didn't even get monads yet! So yeah, read this book and try to apply it! That's really the next step as a Haskeller - building something cool with Haddocks in hand.
I think that's roughly what the article says. It describes Bytestring as useful as an efficient, low level string of bytes but not a useful representation of text because it doesn't appropriately represent characters. Alexi is not complaining about the way you describe using it. She's complaining about programmers who try to use it to represent text (perhaps latching a little too strongly to the "string" part of "Bytestring" when the important part is really "byte")
It's always collocated with ICFP. So you ought to ask why ICFP is in Berlin :-)
Wow, is that true? That's great if it is, but I thought GHC never optimized memory representation of things, like flatten out sums of sums to get rid of layers of indirection etc. Would be glad to learn otherwise.
Thank you for the in-depth explanation
Thanks a lot for the long and thoughtful response, sorry I couldn't reply earlier. I don't think serializing and deserializing the same value is uncommon at all. Maybe you're writing it somewhere so you can read it back, maybe it's a vocabulary type between the client and the server, so, it appears in various places in both directions, maybe you just want to test your server through the API or maybe both the server and some of the clients is written in Haskell. I agree that an explicit parser/printer pair is the most flexible, but it's also the least safe and the least convenient. The example you've kindly put together defines the parser and the printer separately, so, writing more code is just a little annoyance, but you also have to make sure that (1) they do the same thing (2) nobody forgets to update any of them when a new field/constructor appears in your type. And the worst part is, this is just for To/From JSON, as I've mentioned I might be interested in many more operations like this. For instance, I almost always need a [ToSchema](http://hackage.haskell.org/package/swagger2-2.3.0.1/docs/Data-Swagger-Internal-Schema.html#t:ToSchema) instance as well, and I occasionally need CSV instances too. I don't want to discredit your package, sometimes you do need the flexibility of an explicit parser, but sometimes you are the one defining the API and it's OK to just derive the instances from your types as long as the serialization is sensible, and in that case, using generics makes life so much easier on many fronts.
I mean, I've seen that in languages that aren't Haskell. It is a real thing that can happen to software projects. I can't say if it happens with Haskell more frequently, but there is no reason I can see for it to be harder to avoid in Haskell than any other language.
Just a note, you renamed yourself from "Tom Sydney" to "TomSydney" - not sure if that was intended as the LICENSE files read differently. 
Somehow I would trust proficient haskell devs more than I would trust proficient java or dotnet devs. Screwing up is not language specific. &amp;#x200B;
There is nothing problematic with using `lens`. We use `lens` and `generic-lens` in every single project. And we have beginners and mid-level haskellers in our teams. And it has been an enormous productivity boost so far. Even simple examples like msg ^.. field @"timers" . each . from microseconds . to posixSecondsToUTCTime or attack ^.. field @"cidrs" . each . field @"naic" . each &amp; nub are much easier to understand when reading the code than what it would be without using `lens`. Even for beginners. I suspect that anyone who just looked at these examples above would guess what this code is doing, even with zero exposure to `lens` :) The rest is code review and education. `lens` has a pretty good naming convention for all its operators. When you see an operator first time, and you need to understand it, you can have a pretty good guess (often 100%) about what it means, if you know the convention. So one way would be to educate the team about the convention. Also, when something too "cryptic" is spotted, it is always an option to request an explanation or rewrite or to propose an alternative solution during the code review. Sometimes an easier alternative can be found, sometimes not, sometimes I have my "a-ha" moment. But I don't think that "We don't use that lib because it happens to have unfamiliar operators" is a productive approach.
Great blog post, thanks! Some thoughts... &gt;**Boilerplate** What would it mean to be boilerplate-free? The only thing I can think of is choosing fully instantiated types. Even the RIO approach of using a type class into the environment is boilerplate. But if you fully instantiate your types you end up writing all your programs in a very capable monad (I don't want that, because I want my types to constrain the programs I can write). In `freer-simple` and `simple-effects` you just write out the "signature" of your effect once (the type of each method, e.g., `greetUser :: User -&gt; m ()`) and then derive the rest (the syntax sugar to send those effects to an interpreter) - in `freer-simple` there is Template Haskell, and in `simple-effects` there is support for GHC generics. In `mtl` programs the boilerplate is writing a type class (OK, we have to specify the type of our operations somewhere), and an instance declaration (OK, we have to specify the implemantion somewhere) - the only boilerplate might be if we choose to also provide a `newtype` \- but that's optional. I suppose what I'm saying is boilerplate is part of the trade of to writing individual effects that you want reflected in the signature of your programs. It's not inherent to free monad approach, but is a property of *any* general effect system. &amp;#x200B; &gt;**Bracketing** `simple-effects` and `mtl` do support bracketing (see [`bracket`](http://hackage.haskell.org/package/exceptions-0.10.0/docs/Control-Monad-Catch.html#v:bracket) from `exceptions` and [`bracket`](https://hackage.haskell.org/package/simple-effects-0.13.0.0/docs/Control-Effects-Resource.html#v:bracket) from `simple-effects`). It's more first-order free monads that struggle with this. I think `fused-effects` pulls this off as it's working higher-order (see `HFunctor`). Nicolas Wu has papers on this. &amp;#x200B; &gt;**Concurrency** I do not agree that `Applicative` is the answer here. That's going into the realm of automatically providing concurrency, but we know that in reality that almost never works. You generally need some control as to how concurrent evaluation happens - maybe it's through bounded worker queues. I'm fine with adding some extra concurrency primitives and pushing that out into a library. Again, `mtl` (see [`concurrency`](http://hackage.haskell.org/package/concurrency-1.6.2.0/docs/Control-Monad-Conc-Class.html#t:MonadConc)'s `MonadConc`), `simple-effects` and `fused-effects` should all be capable of writing these effects. &amp;#x200B; &gt;**The wiring** I'm afraid I don't really understand this section. If there were some concrete things the author didn't like, I might be able to better respond. &amp;#x200B; My conclusion is that effect systems are still worth it. I agree with the author that if you don't have a good story for higher-order effects like bracket then the system is going to hold you back. In `simple-effects`, the idea is really just to reify any `mtl` class into a record (explicit dictionary passing), and then having a single `MonadEffect` type class to look up this dictionary, using type class elaboration to lift it appropriately. Furthermore, the magic `CanLift` constraint means you can say how an individual effect interacts with other effects. As we know, mixing state and concurrency is dubious at best, but you could say that the concurrency effect can only lift through "stateless" transformers. This gives you something akin to `MonadUnliftIO`, but without bringing `IO` into the picture.
We can pretty much "decrypt" operators in `lens` by following the convention: `^` means accessing the field `~` means “set” `%` means “modify” `..` means “traverse over many of them” `@` means indexed (at the position) `=` means “stateful” `&lt;&lt;` means “also give me the old value” Knowing that `**` is `power` in Haskell, it is possible to have a good guess at what this operator is doing ;) In my experience, _writing_ expressions with `lens` can be hard (also fun and rewarding), but reading them is easy, and I like this tradeoff. To be honest, most of the time when reading code that uses `lens` I don't even care about what these operators are doing. They all read just as "composed with" or "andThen" :) I see that it is a `lens`, and the types match, and it returns the correct result... I see which "fields" are accessed, I don't care that much about the mechanics of accessing them, whether it is `(^.)`, or `(^..)`, or whatever. That's usually enough for me: there is no much room to introduce a bug, even in future refactorings, so that's fine :) YMMV of course :)
I care
&gt; Because I think that both in Haskell and Scala you can build modular, extensible, understandable, easy-to-refactor applications using simple constructs, the resurgence of the so called “final tagless style” is not really much more than that: interfaces and implementations, with the twist that they are parameterized by a type parameter F[_] for added abstraction. By "final tagless" the post means something like record-of-functions, doesn't it? One aspect of freer I find appealing is the ability to decompose a capability into lower-level capabilities that are still uninterpreted. I wonder if it would be possible do do something like that with record-of-functions. My hunch is that it would require some kind of extensible record system. 
Final tagless usually means moving data type constructors to be type class methods. So data Expr where Add :: Expr -&gt; Expr -&gt; Expr IntLit :: Int -&gt; Expr becomes class Expr e where add :: e -&gt; e -&gt; e intLit :: Int -&gt; e In the context of effect systems, it's really just `mtl` imo. Rather than writing data Reader r a where Ask :: Reader r r We have class MonadReader r m where ask :: m r &gt;One aspect of freer I find appealing is being able to decompose a capability into lower-level capabilities that are still uninterpreted. You can do this using an `mtl` approach - it doesn't require anything too fancy: class MonadReddit m where getLatestHaskellPosts :: m [RedditPost] class MonadHTTP m where httpGET :: Request -&gt; m Response newtype RedditHttp m a = RedditHttp (m a) instance MonadHTTP m =&gt; MonadReddit (RedditHttp m) where getLatestHaskellPosts = parseResponse &lt;$&gt; httpGET redditReq Here's a Reddit effect and a HTTP effect. `RedditHttp` provides an implementation of `MonadReddit` assuming you have an instance of `MonadHttp` available - but it doesn't commit you to any particular one. Is that what you mean?
Yeah, basically that. The newtype approach is viable but it doesn't seem that you can modify a free-floating computation, say convert `MonadReddit m =&gt; a -&gt; m b` into `MonadHttp m =&gt; a -&gt; m b`. 
I'm not sure I follow. foo :: MonadReddit m =&gt; a -&gt; m b foo = ... bar :: MonadHTTP m =&gt; a -&gt; m b bar a = case foo a of RedditHttp m -&gt; m Does what you want, no? More generally, redditToHttp :: MonadHTTP m =&gt; ( forall n. MonadReddit n =&gt; a -&gt; n b ) -&gt; a -&gt; m b redditToHttp m a = case m a of RedditHttp m -&gt; m
Thanks!, You'd think searching the link would return it.
Thanks for voicing up on this topic. Even though I get what one is trying to achieve using those techniques, I never really understood the fuss around tagless finals and free in Scala. As you stated, they are basically equivalent to interfaces and implementations. To me it always comes down to if it is really worth it, and from what I witnessed, it really adds a lot of complexity for little gain (at least in Scala). Maybe I am just on the « reasonable » side of the Scala community. I was very happy to learn and practice it in my toy project, but again, I would not feel comfortable to put this in production. Technology is cool, use it with care ! Once again, yes free and tagless is cool, interfaces are boring, but they do wonder when it comes to decouple the « dsl » and the interpreter. It is what they are meant for.
Any recommendations for introductory papers?
&gt; I still find mtl-like code to be the best bang-for-buck. When paired with simple-effects you get rid of the explosion of instances and all the pain of orphan instances. Do you happen to have any non-trivial example programs you could share? I'm just curious how a complete program with this particular combination looks.
Idk the reddit link search has been broken for me for a while now.🤷‍♂️
I don't, but I don't know what type of non-trivial program I could write that would also be good enough to study. Elm has the "single page app" example, maybe we need something equivalent for Haskell.
&gt;I suppose what I'm saying is boilerplate is part of the trade off to writing individual effects that you want reflected in the signature of your programs. It's not inherent to free monad approach, but is a property of *any* general effect system. Partly agree, partly disagree. If you look at the approach taken by Frank/Unison, it shows that cutting down boilerplate by a significant amount is very much possible. Whether this is possible to do in Haskell with a purely library based solution (or even with a compiler plugin), that part isn't so clear (at least to me).
Woops, fixed!
You're right, but it's not easy for GHC to infer that this is safe. Many `vector` operations (for example the `replicate` you are using) are implemented in terms of ST, which is again implemented in terms of `unsafePerformIO`. The types make it safe, but in principle `vector` could use some optimisation whereby it caches replicated vectors and re-uses them multiple times. If would be hard for GHC to understand the IO code to know whether this takes place, and hence whether a destructive update would be safe. These kinds of optimisations tend to require something like *uniqueness types* (or linear types, or affine types) to be safe - and at that level it's no longer an optimisation, but a guaranteed property.
Yeah, I have. Was using a fork of HDBC-odbc that worked for me: [https://github.com/agrafix/HDBC-odbc](https://github.com/agrafix/HDBC-odbc)
I don't have that problem when I paste from my editor, so maybe Visual Studio Code is implementing copy-paste in an unusual manner??
&gt;The wiring &gt; &gt;I'm afraid I don't really understand this section. If there were some concrete things the author didn't like, I might be able to better respond. I'm still doing a very bad job at explaining what I see as a problem :-). But look at this \`main\` function main = runM . runRedis . runFTP . runHTTP . runEncryption . redisOuput [@Stat](http://twitter.com/Stat) mkRedisKey . postOutput [@Record](http://twitter.com/Record) mkApiCall . batch [@Record](http://twitter.com/Record) 500 . ftpFileProvider . decryptFileProvider . csvInput “file.csv” $ ingest How would you run the \`main\` function with a different \`Redis\` effect? You have to rewrite that whole function, Or you want to momentarily change the logging configuration but just for the \`FTP\` commands (if there was a \`Logging\` effect)? You will probably have to rewrite that full \`main\` function to use different interpreters. And maybe make the \`runFTP\` take an extra argument for how logging should be done. For me it is an issue that we can not easily say "this \`main\` function but slightly different". We have a similar issue with MTL/transformers where changing one effect implementation requires the definition of a new \*full\* transformer stack (even if there are some \[tricks\]([https://chrispenner.ca/posts/mock-effects-with-data-kinds](https://chrispenner.ca/posts/mock-effects-with-data-kinds)) to help with that). &amp;#x200B; This is why I like the idea of being able to "recompose" function calls to inject mock values or different constructors/interpreters.
I think a solid standard app would be something that touches on pain points in every business app at some level. * Logging * Input from a user * Database queries * Serializing/deserializing json and t least one ad hoc hacky csv-like-ish format * Business logic that isn't elegant or nice in any way * How easy is it to: add more logging, add a query, change the business logic, etc? Seems like a business calculator form would check all the boxes. "Check how much you could save by using our stuff" type of deal. Hit some DB to get product info, read some ad-hoc format and/or json for the business logic variables, etc. The refactoring would be designing the form for one business product and then adding a second after it's done. Bonus points for cli and RESTful inputs using the same code :) Of course, the real trick is adding enough complexity to touch on all of these without making it so complex that it would take more than a casual weekend for someone experienced to build it...
Haskell is an excellent language for writing integrations! At my work we build integrations with other web-services including OAuth2 applications (like Fitbit) and this is only a pleasure. Static types really help code maintainability in long run but Haskell has very powerful type system on top of just static types. Having \`IO\` to communicate with other world is not really a problem. Dealing with \`IO\` type doesn't introduce hassle as others usually say. Think of \`IO\` as a way to explicitly annotate each function with some documentation like \`this function works with the outside world\`. But instead of being just documentation for human, it is actually checked by the compiler. So while implementing integrations you still can have pure functions that do some data processing work and those functions are used in \`IO\` actions.
Unfortunatelly, GraphQL situation in Haskell is not the best at the moment. There's Hasura's \`graphql-engine\` but it's an application **written** in haskell, not a haskell library from what I understand.
Yeah, that's the kind of thing I was thinking of. Database queries could be "stubbed" by just using an in-memory list for storage (or similar), but still written using a separate effect. I think some form of bracketed access to resources would be a good idea because of how often it's necessary in Real Life™ code. (The serialization and business logic stuff might be a bit of overkill perhaps. It seems pretty orthogonal to how a full application is structured as long as there's *some* reasonable way to handle errors/validation.)
There are a lot of "basics" to gather, yeah. PCPH has some really good content; it's great that you're working through it. I think this section of LYAH touches on the concepts relevant to the second error message in your post. The first error message was a bit of a red herring, as /u/ElvishJerricco called out. http://learnyouahaskell.com/making-our-own-types-and-typeclasses#typeclasses-102 --- the paragraph starting with "But how are the Maybe or list types made as instances of typeclasses?" The instance there is needed for the same high-level reason that your `NFData a =&gt; NFData (IList a)` instance is needed: `IList` and `Maybe` are both functors and their class methods need to be able to do their thing on the stuff "inside" the functors.
Thanks this is a nice suggestion. I'll keep it in the back of my head. I do think it would be the most useful comparison for effect frameworks. Right now everyone is just trying to out micro-benchmark each other, but that is just one of many dimensions to consider... We really want an "effects-zoo" like the frp-zoo.
The serializing and business logic are included really for more personal reasons. I often see people criticising Haskell for being all about the elegance and unable to deal with practical matters. "what does a bibbity-bobbity-morphism have to do with business logic?" Business logic also has the wonderful feature that there's about a million edge cases and no real nice way to fully abstract the differences out. It's a real test of refactoring capabilities in my experience. And at work a lot of our more canonnical sources of information come from random json files in codebases or the like. Bracketed access is definitely a good one to touch on.
I hate to be that guy, but unless your team is pretty small/nimble moving your entire team from Ruby to Haskell is going to be a mammoth task -- and if you're doing it primarily to write plumbing code and use expose GraphQL I don't think you're going to find the ecosystem that Haskell offers to be as good as what you'll be able to get from day one in ruby land. Hiring for Haskell is also still quite a bit harder than hiring for Ruby developers. I'd like to think that the quality of Haskell developers is higher overall (even if it's just due to the much smaller group), but that would also count negatively on the business-decision side of the scale. Why not use some gradual typing on the ruby side and see how you like that? I would have suggested Sorbet (Stripe is working on a new type checker for ruby) but it looks like that hasn't been released yet. Maybe [steep](https://github.com/soutaro/steep) would help? All that said, Haskell is an excellent language for the problem, but you're probably going to feel a *lot* of friction just starting with it in a high pressure environment -- I assume your integration with third party systems is going to be under impossible/quick deadlines because the tasks may *seem* simple (from my own experience that's how it goes). Haskell isn't really a language for bolting things together quickly, unless you've built up the necessary abstractions (which takes time), then it's *excellent* for bolting things together safely *and* simply.
&gt; These kinds of optimisations tend to require something like &gt; uniqueness types &gt; (or linear types, or affine types) to be safe - and at that level it's no longer an optimisation, but a guaranteed property. I don't quite understand. In order for GHC to type-check a function like e.g. f :: Vector Int ⊸ Vector Int what information does it use? As far as I can see, GHC must be able to infer *exactly* how many times the first argument of `f` is used, in order to assert whether or not `f` is linear in its first argument. I'm arguing that GHC should use exactly this information to do the optimizations. In other words, if GHC is able to type-check whether a function argument is used linearly, it must have exact usage information available in the first place. &gt; The types make it safe, but in principle `vector` could use some optimisation whereby it caches replicated vectors and re-uses them multiple times. I see your point. However, to get around this, GHC *could* just copy the replicated vector once, and do destructive updates on that. Granted, that'd make the algorithm slower, but it'd only be O(2n) (one for the creation of the vector and once for the copying that makes it safe), which is still a huge difference compared to O(n^2).
The error complains about `main :: return ()`, which is incorrect, it should be `main :: IO ()`. But your code does use `main :: IO ()`, not `main :: return ()`, so that code definitely doesn't produce the error you gave.
&gt; what information does it use? As far as I can see, GHC must be able to infer exactly how many times the first argument of f is used, in order to assert whether or not f is linear in its first argument. I'm arguing that GHC should use exactly this information to do the optimizations. I don't know exactly how GHC type checks linear functions (has it even been merged yet?), or whether it would provide sufficient guarantees in this case. But even if it does, how would GHC know what an "in-place update" even *means*? `Vector` is a library type, after all, so GHC would have to untangle its data representation (which seems to be mainly a `ByteArray#` at the bottom, plus some auxiliary size information), and understand how the various functions interpret that byte array. In particular, GHC would have to figure out which of the internal `vector` definitions that create new arrays that could re-use the storage of the original array. Remember that `V.//` is not a compiler primitive! `Vector` is a particularly challenging example, because it also uses multiple representations to facilitate rewrite rules, so GHC would have to "look ahead" to even understand that the vector is manifested at some point. I don't think this is realistic as an automatic compiler optimisation. However, when/if GHC gets linear types, the `vector` package itself could present linear O(1) update functions (like it already does for mutable vectors in the ST and IO monads).
One factor to consider is the kind of integrations. If the integrations have company sponsored libraries in e.g. Ruby it will probably be easier to use them rather than write your own Haskell ones (if only because often APIs just aren't documented very well). A huge win of Haskell for integrations is that the type system + libraries like aeson (for JSON parsing) validate all the data when it comes in, so you can deal with any parse errors there and not get random exceptions down the line (or worse, garbage in your database). I've done many integrations with ad network APIs in Ruby, which were all HTTP JSON APIs. The lack of data validation meant we got all sorts of stuff in our database (for example, country codes that were not part of the set we were using). I think Haskell's a great language and we use it exclusively (for backend work) at my own company. If you hire for Haskell engineers you will probably get very quality engineers, especially if you allow remote. That said, Haskell has a very high learning curve and I'd highly recommend someone experienced to guide the team. GraphQL support is kind of a question mark/concern for me though. I think Haskell's graphQL support is relatively immature.
In your code snippet, you have the line `execute conn "DROP TABLE tabela"` however `execute` has a type signature of `execute :: ToRow q =&gt; Connection -&gt; Query -&gt; q -&gt; IO ()` so it seems you need to add another argument onto your call to execute. 
Please post code in textual format, not images. You can use a site like pastebin or GitHub Gist to upload text.
https://pastebin.com/ZxiQTeyy ERRO Text
Tanks, There was only one "()" after the sql command
Some college courses use CodeWorld to teach Haskell. Example: [CIS194 UPenn Intro to Haskell Course](https://www.seas.upenn.edu/~cis194/fall16/) Personally, I am loving CodeWorld mainly because it lets me focus on learning the nuances of the language and building amazing games rather than setting up the environment and installing dependencies. This is especially crucial if you are a beginner, like myself. When I was learning Clojure, I had a hard time setting up my dev environment. Some of the best resources on Clojure use Emacs to teach, which takes time to get used to. At one point I felt I was learning Emacs and not Clojure. Such obstacles make it difficult for beginners to make leaps in their learning journey. If anything, I would love to spend time on contributing to these projects because such tools should never go away. Talking about examples, the course I mentioned has many live examples of haskell working inside CodeWorld. Check out this [tree animation](https://code.world/haskell#PdE2EGgt7PRQNBPaNuB9jWQ) Do give the course a try, you will be able to appreciate how beginners can benefit out of a tool like CodeWorld
To reduce Ruby boilerplate, try out dry-validation and dry-schema ( [https://dry-rb.org/gems/dry-validation/](https://dry-rb.org/gems/dry-validation/) / [https://dry-rb.org/gems/dry-schema/](https://dry-rb.org/gems/dry-schema/) ). GraphQL is a heavy system, but I think Ruby's GraphQL gem ( [https://github.com/rmosolgo/graphql-ruby](https://github.com/rmosolgo/graphql-ruby) ) is more pleasant to work with than Haskell's solutions. Haskell is a wonderful language, but I don't think your situation calls for the overhaul to Haskell. Ruby will work just fine. Ensure your database has well defined constraints to enforce data integrity. It's better that your system fails and throws errors than to allow malformed data traverse through your code and onward to other parts of the system. &amp;#x200B; "I've never used Haskell more than on a hobby basis" -- this should be your first sign that it is not a good idea to move your company to Haskell. You're being hired to write quality software. If you're leading the charge to Haskell and you don't know what quality means in Haskell, then you're doing a professional disservice. 
I'm so looking forward to a day when there's an FP language, maybe even Haskell, with first class in language support for effects, lenses and dependent typing, with all that it entails: support in editors, refactoring tools, compiler optimizations, documentation and common patterns of doing things...
I don't understand why this is a problem. Why do you _need_ a different redis effect? And especially when would you need one and expect to get away without recompiling?
Thanks for writing this up! This is exactly the type of discussion I was hoping that would arise from my original post. If I can solve the performance and bracketing problems, would you fall more favorably on the side of freer monads?
I have a cli program with some command line options currently handled with \`optparse-applicative\`, and it works great. Now I would like my program to accept the same options from also from configuration file. I've seen that \`configuration-tools\` offer this, however it seems huge (aeson, yaml, ...) and boilerplate-y (or Template Haskell). I'm very new to Haskell, so it's a bit much. On the other and I saw \`console-program\` which seems much more focused, simpler and leaner, but the config file is fixed (and I need to be able to specify it on the command line) Do you have any suggestion?
Depends on the topic! The wiki has a good (if somewhat outdated) list of classics: https://wiki.haskell.org/Research_papers Also take a look at the functional pearls, as I mentioned: https://wiki.haskell.org/Research_papers/Functional_pearls
Some relevant (well, linear) reading: Wadler, P. (1990, April). Linear types can change the world. In IFIP TC (Vol. 2, pp. 347-359). Wadler, P. (1993, August). A taste of linear logic. In International Symposium on Mathematical Foundations of Computer Science (pp. 185-210). Springer, Berlin, Heidelberg.
Try running hie with `-d` and `-l some_log_file.log` to get debug log and see what's happening there. Also, what OS are you using?
I'm using Ubuntu 18.04 and I'm able to use HIE in VSCode. It seems to only be neovim that has this behavior. 
I'm also curious about this. I haven't been able to find any resources on anything more than builtin LaTeX in haddocks. Perhaps u/hvr might know.
You could make your own `haddock.sty` (by copying the existing one and then tweaking it) then specifying it via the `--latex-style=FILE` option.
I had a hard time distinguishing which parts of this post were about language-specific issues (in scala in particular) and which ones were more general.
I've been curious about this for a while: Do most Haskellers prefer 4-space indentation over 2-space? I strongly prefer 2-space because the increase in whitespace reduces readability, IMO. I actually find it mildly annoying to read Haskell code indented more than 2 spaces.
Where did you find this information?
How many spaces should "where" have? 1?
There appears to be some plagiarism, or at least a lack of citing sources. Example: Constructor fields should be strict, unless there is an explicit reason to make them lazy. This helps to avoid space leaks and gives you an error instead of a warning in case you forget to initialize some fields. -- + Good data Point = Point { pointX :: !Double -- ^ X coordinate , pointY :: !Double -- ^ Y coordinate } -- - Bad data Point = Point { pointX :: Double -- ^ X coordinate , pointY :: Double -- ^ Y coordinate } Additionally, unpacking simple fields often improves performance and reduces memory usage: data Point = Point { pointX :: {-# UNPACK #-} !Double -- ^ X coordinate , pointY :: {-# UNPACK #-} !Double -- ^ Y coordinate } [haskell-style-guide](https://github.com/tibbe/haskell-style-guide/blob/master/haskell-style.md) Data types Constructor fields should be strict, unless there's an explicit reason to make them lazy. This avoids many common pitfalls caused by too much laziness and reduces the number of brain cycles the programmer has to spend thinking about evaluation order. -- Good data Point = Point { pointX :: !Double -- ^ X coordinate , pointY :: !Double -- ^ Y coordinate } -- Bad data Point = Point { pointX :: Double -- ^ X coordinate , pointY :: Double -- ^ Y coordinate } Additionally, unpacking simple fields often improves performance and reduces memory usage: data Point = Point { pointX :: {-# UNPACK #-} !Double -- ^ X coordinate , pointY :: {-# UNPACK #-} !Double -- ^ Y coordinate }
There appears to be some plagiarism, or at least a lack of citing sources. # Example from OP article Constructor fields should be strict, unless there is an explicit reason to make them lazy. This helps to avoid space leaks and gives you an error instead of a warning in case you forget to initialize some fields. ``` -- + Good data Point = Point { pointX :: !Double -- ^ X coordinate , pointY :: !Double -- ^ Y coordinate } -- - Bad data Point = Point { pointX :: Double -- ^ X coordinate , pointY :: Double -- ^ Y coordinate } ``` Additionally, unpacking simple fields often improves performance and reduces memory usage: ``` data Point = Point { pointX :: {-# UNPACK #-} !Double -- ^ X coordinate , pointY :: {-# UNPACK #-} !Double -- ^ Y coordinate } ``` # From [haskell-style-guide](https://github.com/tibbe/haskell-style-guide/blob/master/haskell-style.md) Data types Constructor fields should be strict, unless there's an explicit reason to make them lazy. This avoids many common pitfalls caused by too much laziness and reduces the number of brain cycles the programmer has to spend thinking about evaluation order. ``` -- Good data Point = Point { pointX :: !Double -- ^ X coordinate , pointY :: !Double -- ^ Y coordinate } -- Bad data Point = Point { pointX :: Double -- ^ X coordinate , pointY :: Double -- ^ Y coordinate } ``` Additionally, unpacking simple fields often improves performance and reduces memory usage: ``` data Point = Point { pointX :: {-# UNPACK #-} !Double -- ^ X coordinate , pointY :: {-# UNPACK #-} !Double -- ^ Y coordinate } ```
I usually put 2.
`haddock-ghc-8.6.3 --help | grep latex`, then looked up the option in Haddock's manual. Since I work on Haddock regularly, I went straight to the source: https://github.com/haskell/haddock/blob/ghc-8.6/doc/invoking.rst.
Awesome, thanks!
"This document is a collection of best-practices inspired by commercial and free open source Haskell libraries and applications." What's the problem? 
I don't appreciate the attempted gaslighting while feigning ignorance. You ignored what I wrote "a lack of citing sources," which is a problem. What you quoted doesn't mean anything and doesn't respect the work of primary source authors. Licenses? Wake up. Cite sources or you're just ripping off content and not giving credit where credit is due.
You are coming off a bit aggressive about something that is not really a scientific paper, but a mere collection of best practices. The post can surely be improved, but calling "plagiarism" is disproportionate. 
http://book.realworldhaskell.org/
I mean, by definition copying someone else's words without giving credit is plagiarism. I'm sure it's not malicious in this case though.
Because it seems more and more like integrity is a new idea to you, or you might be a troll. Either way, Let me repeat it for you again since you completely mischaracterized what I wrote: "or at least a failure to cite sources". It doesn't matter if it's a scientific paper or not, it's a work of public writing. Ripping off content into compilations is unethical, sloppy and just not something anyone possessing integrity would do. Maybe you should be barking up the right tree of unethical/sloppy/credit-thieving public writing practices rather than shooting the messenger, eh?
I'd like to see default setups for stylish/brittany and common editors (emacs/vim) according these rules.
I, too, prefer two spaces, but I use four for Haskell because relative indentation is significant, and two spaces is not flexible enough.
Define "n-space". I do what I consider "4 space", but half indents are not uncommon: foo :: Foo foo = baz * qux where baz = qux foo :: Foo foo bar = case bar ^ 2 of Qux -&gt; Bar Bar -&gt; Qux data FooBarBaz = Foo | Bar | Baz deriving (Eq, Show) data FooBarBaz = FooBarBaz { foo :: Foo , bar :: Bar , baz :: Baz } deriving (Eq, Show) And so on
I don't think anyone uses F# for any of those reasons. I think they use it because of .NET libraries. &gt; That’s great, but I believe that in the specific context of enterprise development, too much abstraction can cause problems. Once again, a lack of features is not actually a feature.
Does anyone know the historical justification for putting the comma at the beginning of the line? It doesn't solve the issue of being able to reorder lines because now the first element starts with a non-comma character, and it violates the English orthographic convention (and probably the orthographic conventions of other languages using a Latin script.) It's perverse. ;)
I don't think half indenting is common. I'd be tempted to say that you have 2 space indenting but sometimes double indent...
I don't use LSP tools much but was just debugging `python-language-server` earlier today. Be sure to check [`LanguageClient-neovim`'s debugging section](https://github.com/autozimu/LanguageClient-neovim/blob/next/INSTALL.md#6-troubleshooting) (really just "here's a minimal config and our test suite"), also [double-check their docs](https://github.com/autozimu/LanguageClient-neovim/blob/next/doc/LanguageClient.txt). Hopefully this helps?
It's weird. A high percentage of the "style guides" I see for Haskell recommend 4 space indentation. But the overwhelming majority of the actual Haskell code I've seen is 2 space (commonly with half-indentation for the `where` keyword).
In case it becomes relevant, arrays involve a more 'machine level' definition. Access to an array is based an offset from the array reference (hence why arrays start at zero). So 'under the hood' myArray = 1000, where 1000 is literally address 1000 in memory. For myArray[3], the code gets compiled to access memory address 1003 (or 1000 + 3). It goes without saying that accessing memory by 'index' is done in constant time.
[configurator](http://hackage.haskell.org/package/configurator) is fairly simple to use for simple cases.
Does anyone use a 'punctuation-at-the-end style'? Like so: ``` -- + Best createFoo = Foo &lt;$&gt; veryLongBar &lt;*&gt; veryLongBaz ``` or ``` run = runApp . runMtlStuff . compute $ someData ``` I find it minimises refactoring effort and places the important stuff at the start, rather than burying the lead behind a boring connector operator. 
Do you have to cite sources when it's all open-source, and we all crib from so much prior art that it's impossible to draw lines and demarkate ideas? A readme in an inactive open-source project? Gosh, we all start with the Tibbe 'bible' when we write our style guides. Their attribution was perfectly in line with existing standards. But your tone, aggression, gaslighting and subsequent hysteria is less than perfect and out of step with community standards. Please adopt a lighter touch.
I'm wondering if a function, called `mapRelative` here, which I use to get the relationship of elements to the whole, already exists somewhere. I can't find anything similar with Hoogle, but it seems generic enough that it would exist, maybe under a more cryptic signature in recursion-schemes or something: λ&gt; mapRelative f g = (&gt;&gt;= fmap) (flip g . f) λ&gt; :t mapRelative mapRelative :: Functor f =&gt; (f a -&gt; b1) -&gt; (a -&gt; b1 -&gt; b2) -&gt; f a -&gt; f b2 λ&gt; mapRelative maximum (/) [2,4,6,8] [0.25,0.5,0.75,1.0] (&gt;&gt;=) here is bind for the Reader monad, so maybe easier to read as: mapRelative f g x = fmap (flip g . f $ x) x
&gt; The maximum allowed line length is 90 characters. say what now
This is as far as it gets from my preferred style :D https://int-index.com/posts/haskell-style
I like putting argument documentation right under them, instead of to the right. So instead of: printQuestion :: Show a =&gt; Text -- ^ Question text -&gt; [a] -- ^ List of available answers -&gt; IO () do: printQuestion :: Show a =&gt; Text -- ^ Question text -&gt; [a] -- ^ List of available answers -&gt; IO () Syntax highlighting would make it really easy to spot the difference between arguments and documentation (I don't think it's supported by Reddit?). I believe it solves multiple problems. It's easier to see what documentation belongs to what arguments. This is especially noticeable in combination with long names / many fields / many arguments. printQuestion :: Show a =&gt; FooBarEnterpriseReadyFactory -- ^ Q -&gt; [a] -- ^ A -&gt; [a] -- ^ V -&gt; [a] -- ^ Z -&gt; IO () It's much less awkward to write longer/multiline docs: printQuestion :: Show a =&gt; FooBarEnterpriseReadyFactory -- ^ A darn, 90 -- chars already? -&gt; [a] -- ^ A -&gt; [a] -- ^ V -&gt; [a] -- ^ Z -&gt; IO () I also like that the `^` now directs your eyes to the argument, instead of to the void. Last but not least, [you could use non-monospace fonts on your blog without readers being dicks about it](https://i.imgur.com/6wahxWJ.png) :\^).
It's visually striking, like a lot of Haskell idioms (like applicative style).
That said, if your team (management included) is interested in FP in Haskell and you're willing to own everything that entails: * The build * Teaching developers FP * Writing missing API bindings * etc! Go for it! It may not be the most optimal business decision, but it'll be a good experience. Nothing wrong with slowing down to learn and speed up once you're all intermediate Haskellers.