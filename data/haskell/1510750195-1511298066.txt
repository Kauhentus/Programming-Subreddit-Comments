We primarily use Haskell for our products, so we thought there would be some interest from Haskell community. Post updated to include that. 
&gt; Most of this information is in the Haddocks. Almost all of it should be in the Haddocks if it isn't. If you can provide evidence that it's easy to update the Haddocks in base with high-quality contributions then I shall feel very reassured! 
Great! It would be very useful and often used feature!
Thanks for the feedback!
What’s Serokell’s relation to IOHK?
There are already implemented Haskell libraries for manual bit manipulation. But the point is if it's good to be implemented in the GHC itself, so this happens automatically/implicit as optimization. So all bitwise operations happens automatically. If you need to get boxed version - it'll be represented as usual Bool, but strict version could treat is as bits.
I feel as if every page on MDN is like this. My motivation is based on my personal experience. I don't think Haskell should be a difficult language to learn, however, I had difficulty learning it. When I learned C, I had the GNU C reference Manual, OpenGroup, and manpages which made it easy. When I learned JavaScript, I had MDN, and w3schools which made it easy. But when I leaned Haskell, I had "Learn you a Haskell", and "Real World Haskell", which, while good resources, I had trouble learning from due to tutorials being linear by nature. I don't learn particularly well from tutorials, so I thought others may benefit from the kind of documentation I would like if I was learning Haskell again: detailed description of datatypes, classes, and functions.
One of the best things about MDN is the navigational experience, the constant organized set of links to the left. A strong, cohesive navigation system is key to documentation UX and this would be very difficult to pull off in the wiki. Wikis in general are really kind of terrible at directed navigation. Readthedocs might work in a pinch. 
Haddocks don't support enough navigational concepts for this usecase. They're a bad fit for general education. 
I like the idea of GitHub Pages. Basically I need a solution which fits the following criteria: * I don't have to set up anything on my own machine, I just don't have a good enough connection to host anything. * The documentation would get decent exposure (GitHub, HaskellWiki, Haskell.org are good candidates) * It's easy for others to edit and improve (I can't do all of this work myself) - this criteria automatically excludes the haddocks which require you to edit source code of individual projects. I'm feeling quite motivated to kickstart a project such as this, but I can't do it all by myself, I have a full time job!
Exclusive business partner.
I think that the idea is awesome the MDN is some of the best documentation available. The primary points for at least are UX, exposure and ease of modification in a coordinated manner. You will need exposure to create the momentum needed to get a large enough contributor base. All good documentation is living and based on a large pool of knowledge so you would need people. Also, examples, examples and more examples. The UX part is primarily ease of access to further APIs/concepts. This requires a lot if insight from the people managing the links. Answers to questions you didn't even know you had yet. Most importantly it has to be easy to search, to some extent even in non-Haskeller lingo. Finaly there has to be some sort of gate so that not all edits go live without any sort of vetting. This is required to keep the content quality high enough. Just my two cents :)
Perhaps this should be somewhere else, as this is 100% useless information to beginners and people who don't know anything about CT. 
Who said anything about beginners?
I agree that it should remain open to as many editors as possible - I just added some list-without-sugar detail to the `foldr` page, which is how i remember `foldr` vs `foldl` distinction. Two related aspects: Firstly one I noticed while editing that I have no idea how the wiki inserts newlines and spacing between paragraphs. Half the time there is space around the `pre` blocks, half the time there is not. And more general, I hate having so many markup formats around. If I publish some haskell library, there is haddock for in-source, some random markdown flavor for the readme and now some wiki format if I was to add a page there. That's just bonkers. Secondly, and I think mentioning msdn hints at this already: As nice as it is to have such extensive examples, I don't currently like the idea of putting such articles in the wiki or in the haddocks, for a different reason: The lack of structure. Consider the logical location of the `foldr` article: `base library/Data.Foldable/class Foldable/foldr`. That's already four (or perhaps five) deep. The haddock currently shows whole modules, and those would become rather large (yes there is the synopsis, and perhaps we could improve folding like the instances, but i am not convinced of those superficial navigation aids.) For a wiki, we could of course add per-module list-of-links pages and generally throw cross-references at this problem, but that feels like a rather manual approach. In contrast, msdn is inherently tree-structured (some trivial example: [the glClear function in the msdn](https://msdn.microsoft.com/en-us/library/dd318372\(v=vs.85\).aspx) note the parents displayed at the top, and the siblings left.) I think this kind of documentation _needs_ to be a tree, transparently, from the start. My personal motivation to write more docs is fairly limited until these two points are addressed, i fear.
&gt;What about complexity of tying the knot? As linked lists in functional programming are often used as *control structures* rather than data structures, try to formulate it into an equivalent program using loops. This might help you grapple with these things a bit better :) &gt;How to formally reason about the correctness of tying the knot algorithm? (including absence of bottom) If your list doesn't have any bottoms, then it won't have any bottoms when you tie the knot. Assuming the functions you apply to intermediate parts of the list aren't partial, of course.
This is an effort to improve documentation. Are you suggesting we shouldn't try to make it beginner-friendly?
I'm talking about the category theory comment, not the topic of documentation in general
The problem with this style of reexporting is it makes the Haddocks useless and even misleading. protolude has the same issue. 
One pattern I've started trying is to create a new type and monad very much like App and Handler, but without web related bits. I call the type Backend and the monad is just a synonym for MonadLogger and MonadReader Backend. You can see this pattern here: https://github.com/restyled-io/restyled.io/tree/master/src/Backend. The makeFoundation function may appear complicated, but for the backend it's really just putting together your AppSettings and a ConnectionPool, so IMO it's better to not try and over-DRY through reusing that function. In a backend, the logging is simplified and really you're just trying to run IO with the DB connection threaded via Reader. I'm traveling now, but can say more next week if you want to try this pattern and have additional questions.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [restyled-io/restyled.io/.../**Backend** (master → 9c3695e)](https://github.com/restyled-io/restyled.io/tree/9c3695ec258dbc2189e81f27c592e391abb54d82/src/Backend) ---- ^(Shoot me a PM if you think I'm doing something wrong.)
Well, I'm just suggesting that if we do make a community effort to improve documentation, we think long and hard about what we put in and where we put it. A beginner opening up the docs for Functor to immediately be greeted by CT jargon that could be considered irrelevant in many useful contexts isn't the ideal scenario, IMO.
Personally, I don't think that a wiki page called "Functors" should favour a practical over a mathematical one. If you have a page called "Functors in practise" then sure, but I don't think it's appropriate to prioritise one approach over the other on a page which doesn't discriminate. Regardless, how else are you going to have an opening paragraph for "Functors"? I don't think there is any beginner intuitive way to describe a functor in words without examples
One of Haskell's biggest problems is that people don't think it's a practical language. I definitely think we need to focus on the practical aspects. Of course, the mathematical explanation has a place on such a site. But I definitely do think that there is a way to explain something like a functor without diving into mathematical jargon. There has to be, because explaining it to someone without the requisite background, using mathematical jargon, is basically like spouting random nonsense at them.
Unlike Python, PHP, Ruby, you can actually just fork a thread and have it run in the background. I've never used Yesod specifically, but you should be able to use the `async`, `stm`, and `stm-chans` packages to easily build a system for running background tasks based on a queue. For full-blown job queues driven by a DB of some sort, you could look into https://hackage.haskell.org/package/yesod-job-queue
Pretend that plus is really expensive and thinks are what you want here :) 
&gt;One of Haskell's biggest problems is that people don't think it's a practical language. I definitely think we need to focus on the practical aspects. People don't think that, they know it's a practical language they just don't know how to use the practical features. &gt;But I definitely do think that there is a way to explain something like a functor without diving into mathematical jargon Please, have a go. Personally I think there is no way of explaining a functor without just listing the typeclass. "A functor supports a function fmap that takes a function (a -&gt; b) and a value f a and produces a new value f b" is pretty much the best you can do IMO.
&gt; Please, have a go. Personally I think there is no way of explaining a functor without just listing the typeclass. "A functor supports a function fmap that takes a function (a -&gt; b) and a value f a and produces a new value f b" is pretty much the best you can do IMO. I guess it's good that you don't teach programming to beginners, then. There's plenty of ways to explain this more intuitively, even if it ends up becoming kind of handwavy. 
Well, I haven't seen a good example of explaining functors in general without just giving the type of fmap. Other than that you have to give specific examples like lists of Maybe. And, I do teach programming, but in Python
Looking at Python documentation, it is part of the [python repository](https://github.com/python/cpython/blob/3.6/Doc/about.rst), but in a separate folder than the source code. It is hosted on [its own website](https://docs.python.org/3/about.html). Couldn't / shouldn't we do the same on the [GHC repository](https://github.com/ghc/ghc/tree/master/docs) ?
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [ghc/ghc/.../**docs** (master → 47ad657)](https://github.com/ghc/ghc/tree/47ad6578ea460999b53eb4293c3a3b3017a56d65/docs) * [python/cpython/.../**about.rst** (3.6 → eb38367)](https://github.com/python/cpython/blob/eb38367f20b05f2ad04a4833bceb369b5e78b1a3/Doc/about.rst) ---- ^(Shoot me a PM if you think I'm doing something wrong.)
I wrote [an article about exactly this kind of abstraction](https://github.com/rampion/conkin/blob/master/README.md#readme). [Here's the reddit discussion](https://www.reddit.com/r/haskell/comments/78xxql/structures_of_arrays_functors_and_continuations/).
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [rampion/conkin/.../**README.md#readme** (master → b396c5b)](https://github.com/rampion/conkin/blob/b396c5beb3f3a41486a53a20f982ca919699fdf5/README.md#readme) ---- ^(Shoot me a PM if you think I'm doing something wrong.)
The reasons are non-obvious to me, but I use alternate Preludes where possible, so perhaps I'm blind :) WOuld you be willing to share?
The custom type error facilities that have been introduced recently really need to get more widespread use. They're *great* for a UX perspsective.
MDN is a wiki.
The functor constraint is present on hoogle, so I think it should also be present here. I would edit, but I don't have an account yet.
Has anything come of this?
I get super nervous whenever people start gushing about tutorial style documentation. It usually exists at the expense if having good reference documentation. Even the MDN makes me really have to hunt to get to the useful part of the docs. At least it has them though, unlike ruby's "the tutorial that covers 1% of the functionality should be all the documentation you need" approach. I think it's massively important that any change doesn't harm the quality and ease of use of the reference docs. They're the ones that explain how to actually use the library. 
Great stuff, thanks! Can you drop a hint about part 2?
protolude is a pretty "soft" prelude. just fixing things, not redefining the world.
God rest its soul
Thanks for compiling this information. One thing though... the labels on those graphs are incredibly small. To the point of being unreadable for me (unless I zoom in).
Would like to have motivating use case / descriptions + illustrative examples for all methods. On a side note, I would quibble with the definition of `foldr` as operating "from right to left" – it manifestly doesn't or else it couldn't work on infinite lists. Instead, `foldr` is a right-_associative_ fold; it "processes" from left to right, same as `foldl`, but where the accumulation comes from the right instead of the left. foldr :: Foldable t =&gt; (a -&gt; b -&gt; b) -&gt; b -&gt; t a -&gt; b foldr (+) 4 [1, 2, 3] == 1 + (2 + (3 + 4)) Another way to look at it: 1 + acc where acc = 2 + acc' where acc' = 3 + acc'' where acc'' = 4 (The above isn't syntactically correct but gets the point across I hope). The point is that this is evaluated starting from the left, not the right. Because of laziness, as it evaluates, if any part of the expression doesn't use the next accumulation then it can terminate early and not use the remainder of the initial foldable (useful if it is infinite). Once it hits what is essentially a base case (either your end value or an expression which doesn't use the accumulation), then yes – concrete values begin bubbling back up the expression tree from right to left. But the whole thing doesn't _start_ there. I am sure you all are very familiar with this already, but I know it required me to make a leap from the more familiar strict `reduceRight` from JS which _does_ go right-to-left. If the docs are supposed to be beginner-friendly, I think this is an important aspect of `foldr` to understand.
Hijacking this comment - another thing I noticed is that question 9 seems to have a superfluous "How". Otherwise, I agree, nice compilation!
&gt;I’m not sure why fewer than 200 people said they use Haskell at work in the previous question but more than 600 said they use Haskell at work at least some of the time in this question. I'm only one example, but I answered this way because I like to prototype things in Haskell first then rewrite them in languages sanctioned by the company. I have to do this because I have a really hard time convincing people that Haskell is even worth learning.
Thanks for the feedback! I had a heck of a time wrangling the [`Chart`](https://hackage.haskell.org/package/Chart) library into producing usable graphs. I was previewing them as big SVGs so I didn't notice that the labels were tiny. I'll see about re-rendering them with bigger labels.
The original survey question was "How do you use Haskell at work?" but I simplified it (and the answer choices) in the graph. 
Why mention type families as an example of types depending on types and not type constructors?
That makes sense (and was a common thing that I saw). However I still can't understand why only 177 people said they use Haskell at work but 306 said they use Haskell at work all the time. It doesn't add up. 
I am a bit confused if I would use Haskell at university, is it school or work? I never encountered somebody calling a university a school and at least in German, they are different.
In the United States calling university school is very common. You could say university is like a sub category of school. In fact, I think most people even call it college instead of university, even if you are attending a university.
As an American, I would consider undergrad (BS/BA) to be school but grad/postgrad (PhD) to be work. Basically it's work if you're getting paid to do it. 
&gt; Which language extensions would you like to be enabled by default This is a *great* question but I've just realised that there's an equally important question which was not asked: "Which languages extensions would you like *not* to be enabled by default?". I think it's the difference in these two values that's an important predictor of which extensions should be enabled. 
What should it mean? `uncurry (||) (True, undefined)` gives `True`.
You can check out these slides, from page 26 and up until the part on row kinds: http://s3.amazonaws.com/erlang-conferences-production/media/files/000/000/756/original/Oskar_Wickstrom_-_Finite-state_machines__Your_compiler_wants_in!.pdf?1510133482
Bitwise operations seem like the kind of domain-specific optimizations that are already well handled by rewrite rules or compiler plugins.
I was under the impression that this is what Literate Haskell was made for. One could easily write a *.lhs tutorial module and make a PR into `base` so this stuff can live on hackage in the official docs (and be type checked).
I see much potential to improve your Decode module. You can use `eitherDecode` instead of `decode` to get an error message instead of Nothing if it fails. The `Except` monad could simplify the decoding functions (easy aborting with an error message if a problem is encountered). I think [justErr](http://hackage.haskell.org/package/errors-2.2.2/docs/Control-Error-Safe.html#v:justErr) can be used to turn your `Nothing`s into descriptive error messages. Your types in the Type module are confusing me. All they do is giving common things (String, Integer, ByteString) different names that make it more difficult to understand your code. In my opinion they serve no purpose and should be removed. The only exception is the data definition and `Listing` (which should be changed into a data definition). I am a beginner too... so others will probably give you better/more precise advice.
I assume GP just means people who use alternate preludes are likely to be more interested in the poll and thus more likely to respond. A common type of hard to eliminate bias in many surveys.
Great roundup. I misunderstood it when I answered though. I answered that I've never used Haskell and was routed out of the survey. I have been "using" it for 1.5 years while learning it though, just not written any real projects with it, and thus answered "never used it". Should probably clearify this question for next year as I doubt I'm the only one interpreting it this way. 
Yes but strict pairs aren't products in the first place, so there is no curry or uncurry function in the category of strict functions unless there is some weaker construction I'm unaware of. 
Indeed. This is just to save some keystrokes for developers who already know those modules very well. 
&gt; what happened with type classes--they didn't add anything you couldn't do before No they do. They give you coherence and a thin category to reason in. Without them, those things are hard to encode. Even with the full power of dependent types you cannot recover coherence. Watch this video: https://www.youtube.com/watch?v=hIZxTQP1ifo&amp;t=11s
Someone suggested something like this when I first published the survey: https://www.reddit.com/r/haskell/comments/7a3fad/first_annual_haskell_users_survey/dp6yjt7/ I'm not sold on the additional data being worth the additional complexity. 
Sorry about that! I definitely would've liked to hear your responses. For what it's worth, the instructions to skip around the form were suggestions to avoid wasting anyone's time. I'll make that clearer next year. 
&gt; It would not; the "magic" of type classes is that it automatically assembles the dictionary for you, and the restrictions on type class instances ensures that this construction can be done unambiguously The most important thing about type classes is that the give you coherence and a thin category to reason in. Without type classes, those things are hard to encode to the point where even the full power of dependent types cannot recover coherence. Unfortunately, GHC has some extensions that mess with that.
&gt; 11: What is the total size of all the Haskell projects you work on? &gt; Small to medium size Haskell projects are the most popular. That being said, there are a fair number of large to huge Haskell projects out there. How can you conclude anything about the size of individual projects from the total size of multiple projects?
Good point! I can't. 
He wrote a thesis then left for Intel. Unfortunately it was a rather large change to the code-base, and without someone actively pushing, that sort of thing goes to the wayside.
Well, then I double misunderstood :)
I'm surprised how many people use GADTs. What are they being used for? I never seem to come across a situation, but maybe you have to start using them to start noticing them... which is a bit circular.
I would like to know too! I don't think I've ever personally written a GADT, although I'm sure I've used them before. Maybe people want the GADT syntax? 
&gt; Most people used Haskell for a significant amount of time before stopping. Haskell has a reputation for being hard to learn. I think this data supports that reputation. Even if you have been using Haskell for a year, you might still give up on it because it’s either too hard or simply not worth it. As per selection bias --more serious Haskellers being more likely to fill out the survey-- the data should be totally sk(r)ewing here.
We've been using a custom prelude that's a bit more experimental/opinionated. Quirks include: - Ultrafine typeclass heirarchy - Ultrafine toplevel module heirarchy with liberal reexports - Lens everything - Copious plumbing operators
if you don't promptly and successfully do (1) and (2), then not exposing the internals is strictly harmful. 
I guess I could use type constructors as well to demonstrate that point in a different way. I need type families for the rest of the article and with them I am explicitly specifying the type of type constructor parameter.
My apologies, I didn't mean to imply that this was a difficult feat in any wiki, but rereading my comment, I very obviously said exactly that. You're of course right, it's not the wiki format in general that is the problem, as there are good counterpoints out there. What I meant to say, was that many wikis, such as the haskell wiki, are structured around exploratory navigation, and that this is not a site structure conducive to good documentation.
&gt; What are they being used for? For me, it is often with stuff like `operational`. I also used it recently to describe an intermediate language, which is the poster-child for GADTs usage.
I've only ever used it in combination with DataKinds. In my neural network library the type of neural network structures statically determined to have no stochastic elements is `NNStructure False`. These can be used without providing a random number generator. I haven't finished any function that either uses or produces `NNStructure True`, but running these will require a random number generator.
Certainly, but it's worth noting that Haskell had types depending on types before type families were introduced. And type constructors also specify the types (kinds) of their parameters. That's the difference between, say, `* -&gt; *` and `(* -&gt; *) -&gt; *`.
I replied this way as well. I use Haskell at work for scripting and spiking things here and there when I need a little more than bash/shell scripts. These are run on my local machine only, none will ever be checked in or shared or anything like that because a different language is used at work.
Maybe there is the possibility of a less intelligent but easier to implement strategies, a variant of chunky evaluation that may work well in many cases, or a space leak detector that may eat it by evaluating it in an independent process, like a kind of garbage collector. 
On the other hand, their use cases are pretty limited and it seems like nobody is fixing the bugs that arise around them.
This doesn't seem like a constructive comment to me. Harmful to whom or what, and how? And who defines "promptly" or "successfully"? Is there more you want to say here?
This is what I'm thinking we need something like a garbage collection for evaluation order. Non-strictness is to date flow what garbage collection is to memory, so it makes that something similar would be necessary.
When I filled out the survey I was confused about the *ghc-pkg* build tool choice. And I'm even more confused now to see that 92 respondents actually state they use *ghc-pkg*. Can somebody ELI5 to me when you'd use *ghc-pkg* rather than say *Stack*?
If you have any ideas for how to avoid selection bias, I'm all ears. 
I don't think this is accurate. Prelude default vs. non-default is a fairly polarizing issue in general, so although I think you're right in that the poll will be populated by people with strong opinions on the subject, you may be underestimating the prevalence of people with strong opinions about sticking to the default, such as myself.
I would like to know that too! 
It's my opinion that this sort of comment is exactly why Haskell has a reputation as impractical and only used by ivory-tower academics. Is the given definition wrong? Sure. Does the technically-correct definition help anyone? Not really. Does it scare off every poor programmer who's just trying to figure out what a functor is? *Absolutely*. There's a time and a place for this kind of pedantic hyper-correctness. MDN-style documentation, designed to help people get things done is not it. 
Typed initial-style DSLs
Yes, I too would appreciate larger labels. And maybe you could also consider writing up your experience using the Chart library. I would be interested in someones journey in rendering charts with Haskell.
I'm intrigued! is it open source?
It doesn't even mean that, it's an enriched functor from the self-enriched category "Hask" to itself (which isn't even true because Hask is not a category). But just like you might object to that being an overly technical definition, so is yours to people that just want to learn how to program in Haskell. You have to know your audience!
I like the idea, but some criticism of the Functor page: you use Functor both to describe type constructors `f` they are instances of functor but also values of type `f b`. That seems to me like it would be quite confusing to a beginner, especially one that is just getting used to the idea of a first class type constructor.
Selection bias https://en.wikipedia.org/wiki/Selection_bias
when your build system is more complex and you can't incorporate stack into it.
Thanks! It was originally just an implementation of Gloss API on an HTML Canvas, running on a web page. But then as I used it for education, I ended up customizing more and more, both in Gloss, and in the Prelude, and it's turned into something different. But it's no surprise that it converted so easily, since both of these libraries started with Gloss. One interesting addition to CodeWorld, though, is multi-player games. Here's a multi-player variant: https://code.world/haskell#P6V8Y54-pedHVac7Js7Xjvw The only code changes were: * Switched `interactionOf` (the CodeWorld equivalent to `play`) to `collaborationOf` * Addition of the StaticPointers language extension, which is required to ensure both sides are running the same event handlers and such. * Added an extra parameter to the event handling and screen display functions. In event handling, only accept a move from the correct player number. In the display function, add the text "Please wait..." when it's not your turn.
The harm is that they were forced to fork, and do the work of failing to convince you beforehand. and then transitive users, like myself, might not user that depends on a fork. Promptly means the user feels like they continue on their project with your dependency without being blocked, whether that's hours or weeks depends. Successfully means your problem was solvable with the existing interface, or you are able to extend the interface to solve it. Sorry you feel that it isn't constructive. Don't mean to be curt, but I'm busy today and just wanted to mark my disagreement on this discussion before forgetting. There are similar argument about executable exposing their libraries or not, and making interfaces larger or smaller. There's not much else I can say. Exposing versus encapsulating is an old debate and if arguments from people like ekmett (googlable, if you haven't read them) aren't enough, mine won't be. I just don't feel comfortable contributing to or even depending on packages that refuse to expose their internals. I do only reluctantly when necessary, like bos's ubiquitous and extremely useful packages, which don't have alternatives. 
Do you believe Reddit and Twitter are generally more likely to use alt preludes than the general population of Haskell developers?
to clarify, I still feel less comfortable with a package that depends on the internals of another package without sharing an author. But still more than a whole fork. 
&gt; I guess people aren't likely to talk about using defaults :) Yeah that's the point. People who use customized stuff are more likely to answer the poll, and even to share it with more people who use customized stuff…
I think the problem here is a PR to base. You're essentially relying on active developers to make changes to the docs. Wikipedia has accuracy on par with encyclopaedia Britannica, but with far greater coverage. MDN is a wiki too. Nothing gets vetted before it goes live. Their policy is to just have documentation there, and then afterwards make it accurate, and they have some of the best docs around. I'm sure the tooling is great, but something like this requires a lot of people to do it, and frankly, the lower the bar for entry, the better. 
Agreed. I don't think I could ever write documentation that everyone is happy with, and my description of foldr could definitely do with some more work. Ideally, I would have a page that anyone could edit so that when someone does have a problem with the docs, or feels as if it could be written better, the changes are made to the docs for the next person to come around.
I agree with everything here except for the gating process. MDN is a wiki and some of the best docs around because of it, so is Wikipedia. If I can provide documentation of a high enough quality for people to understand, and I'm a beginner, than so can others. The only think you really need are spam filters, so you don't end up selling viagra on the docs site.
&gt; Moritz Angerman has been hard at work on a number of areas of the compiler, with a general focus on portability and cross-compilation. Not only has he single-handedly rewritten much of GHC’s ARM and AArch64 linker, but he is also adding cross-compilation support to Template Haskell, improving cross-compilation support in the build system, and rewriting the LLVM backend. Thanks Moritz! Damn dude do you even sleep?
&gt; The harm is that they were forced to fork, and do the work of failing to convince you beforehand. and then transitive users, like myself, might not user that depends on a fork. I'd agree that a fork is undesirable. &gt; Promptly means the user feels like they continue on their project with your dependency without being blocked, whether that's hours or weeks depends. Successfully means your problem was solvable with the existing interface, or you are able to extend the interface to solve it. That sounds reasonable to me. Unfortunately "able to extend the interface" depends on a lot of things, not least of which is an in-depth discussion with the user(s) about what is needed. That can fizzle out for a variety of reasons, many of which won't be apparent to the casual observer demanding a no-questions-asked change of interface. :) &gt; Sorry you feel that it isn't constructive. Don't mean to be curt, but I'm busy today and just wanted to mark my disagreement on this discussion before forgetting. There are similar argument about executable exposing their libraries or not, and making interfaces larger or smaller. I don't consider this to be an issue of exposing internals. *But exposing internals was the proposed solution to the OP's problem, and I contend that it is neither appropriate nor necessary. I am currently engaged with the OP offline to resolve this.*
What does the pd stand for?
Wow I didn't even know about the :cmd command and I can see this tool being helpful. Thanks!
In case nobody else has pointed this out, there's a typo in the email in your third paragraph: info@haskelweekly.news
Yeah, that sounds great. (btw, I've been interested in native functional reactive programming backends, and even tried making my own minimal one for the terminal too. so I got a little excited for brick-reflex, and then disappointed about the fork) my (unsolicited) lobbying for exposing internals just feels like a pessimistic and/or realistic compromise with having finite time. It always feels dangerous to me when they're not. I've had to abandon dependencies and thus fork or re implement them, even when they are well documented / compile / function correctly / etc, because the author wasn't responsive in helping me work around it or even just exposing something trivial; and conversely, I expose everything, with a warning including the severity of some invariant getting violated (like whether it might give the wrong input, loop, maybe even segfault, etc) and how stable I'm expecting the implementation to be, which hedges against myself not being able to help out, while providing some informed consent about accessing the internals. &gt; extending the interface depends on many factors [paraphrasing, I'm on mobile and can't select text] I agree, and to further clarify, i defined 'successful' w.r.t. the user "getting done what they wanted to do with the tool you've made", not as some obligation on the library author or as a optimal software maintenance thing. they (the contributor) likely won't know as good a way as you (the author) to get any given task done, and thus the author's involvement is always ideal. I'm definitely anticipating more stuff being built on brick, and I personally didn't need anything internal when I used it. 
Not OP, but I’ve had a pretty good time with `Chart` with the notably relevant exception of label sizing and placement.
plotlyhs looks useful https://github.com/diffusionkinetics/open/tree/master/plotlyhs
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [diffusionkinetics/open/.../**plotlyhs** (master → 05b3979)](https://github.com/diffusionkinetics/open/tree/05b397973b54fa7de66a4c216eaf7d64c2465135/plotlyhs) ---- ^(Shoot me a PM if you think I'm doing something wrong.)
Is there a good tutorial or write up on what all this means / how it works? I've ordered the new Catagory theory book but haven't been able to read it yet.
Thank you! I maintain a vim repl and have been working on sending code snippets to the repls. This will help a lot in making ghci work.
Oops! Thanks for pointing out the typo. It should be fixed now. I didn't have any other numbers to compare the GHC contributors to. But you're right; one out of ten is pretty good! 
Yup, that's exactly the area I spent the most time fiddling with it. 
plotlyhs does look nice, but I prefer static charts (SVG or PNG) to dynamic charts (JS) for this type of thing. 
CoC is a particular flavour of lambda calculus which supports dependent types. The gist is a simple implementation of an evaluator and a type checker for a very simple model of a core CoC.
It would be helpful to have a Haddock to Wiki or Haddock to Github (markdown) tool to make it easy jump start the documentation from the Haddocks. Is there any tool like this available?
If you want to know how Calculus of Constructions works, see https://en.wikipedia.org/wiki/Calculus_of_constructions. Basically, instead of just function types f:X-&gt;Y, you have pi types f:(a:X -&gt; Y) (where Y might have a in it), meaning that the type of f(a) depends on a. For example, say that Array[n] is the type of arrays of integers with n elements. You can make a function that takes n and outputs the array of elements from 1 to n of type Array[n]. CoC can also be used a proof system, and is about as strong as ZFC.
**Calculus of constructions** In mathematical logic and computer science, the calculus of constructions (CoC) is a type theory created by Thierry Coquand. It can serve as both a typed programming language and as constructive foundation for mathematics. For this second reason, the CoC and its variants have been the basis for Coq and other proof assistants. Some of its variants include the calculus of inductive constructions (which adds inductive types), the calculus of (co)inductive constructions (which adds coinduction), and the predicative calculus of inductive constructions (which removes some impredicativity). *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/haskell/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^| [^Donate](https://www.reddit.com/r/WikiTextBot/wiki/donate) ^] ^Downvote ^to ^remove ^| ^v0.28
Oh wow. That's neat. Thanks. I'll spend some time reading my Catagory theory book and see if I can understand all this.
... and despite that Base Prelude is still the predominant answer
AFAICT if your strategy gets much simpler, then it doesn't work out. Eager Haskell tried a very simple chunky evaluation that cut off after n steps and it was comparatively awful. A large part of the publishability of Ennals' thesis seemed to be the "hey, it actually paid off this time!" factor. As for space leak detection in a separate process, the nesting depth used by this is really shallow, so I expect by the time a separate process can smack it upside the head and get it to stop running on, it'll already be far too late.
I'm currently playing around with using a GC to do some of my evaluation. Notably if your garbage collector can do size-reducing graph reductions on an SKI machine, you can get it to do things like resolve the Wadler class of garbage collection leaks as a side-effect of doing arbitrary graph reduction, even when the pairs are church/scott encoded. If you can recover uniqueness information for pointers with it, you can even do some S reductions. (This should all generalize to a more GHC-like supercombinator story, but I'm interested in machines I can evaluate that have a very small number of base combinators for SIMD and GPU use.)
Uh-oh! My boss can see me spending my time on open source software!
Cool! Which one?
I was just thinking about this earlier today. It seems like a variant of [the robustness principle](https://en.m.wikipedia.org/wiki/Robustness_principle). “Be conservative in what you send, be liberal in what you accept.”
In my case at least, it's definitely not the syntax! I use the regular syntax for ordinary sums. GADTs can be used to make illegal states unrepresentable. For example, you can make ill-typed terms unrepresentable: data Term a where TTrue :: Term Bool TFalse :: Term Bool TZero :: Term Nat TSucc :: Term Nat -&gt; Term Nat TIf :: Term Bool -&gt; Term a -&gt; Term a -&gt; Term a data WellTypedTerm where WellTypedTerm :: Term a -&gt; WellTypedTerm example :: WellTypedTerm example = WellTypedTerm $ TIf TTrue (TSucc TZero) TZero Ill-typed terms like `TIf TTrue TTrue TZero` or `TSucc TFalse` won't type-check. This isn't just useful for static terms like `example`; this is also useful for guaranteeing that program transformations preserve types, and by using a type like `infer :: UntypedTerm-&gt; Either TypeError WellTypedTerm`, you can make sure that your type checking algorithm never accepts ill-typed programs. GADTs can be used to implement "type witnesses", that is, a proof that the type of an otherwise polymorphic value has a particular form: -- (), Maybe (), Maybe (Maybe ()), ... data IsNestedMaybeUnit a where IsUnit :: IsNestedMaybeUnit () IsMaybe :: IsNestedMaybeUnit a -&gt; IsNestedMaybeUnit (Maybe a) A similar effect can be achieved with a typeclass thas has an instance for `C ()` and one for `C a =&gt; C (Maybe a)`, the two approaches have different tradeoffs. GADTs can be used to implement HLists and extensible records. GADTs can be used to implement [singletons and length-indexed vectors](https://gist.github.com/gelisam/3d185f196a4f6c9edaec59cc10efb3a8). GADTs can be used to express the types of operations manipulated by libraries like [haxl](https://gist.github.com/gelisam/0549eb2a292f86ca2574) and [freer](https://gist.github.com/gelisam/7ab28af444c37b456dec633b6a61b69c). GADTs can be used to implement indexed monads, e.g. [to make sure files are only read from after being opened and before being closed](https://gist.github.com/gelisam/9845116). I find GADTs very useful.
Nvim-rappel. Not a big one but I'm trying to make sure that it's the simplest to configure.
https://www.andres-loeh.de/LambdaPi/ https://www.microsoft.com/en-us/research/publication/henk-a-typed-intermediate-language/
How is termination of normalization guaranteed? Only well-typed terms are guaranteed to normalize. 
Not sure how helpful it is to you, but you might want to look up the [Habit Language Report](http://hasp.cs.pdx.edu/habit-report-Nov2010.pdf). The goal was to be a strict dialect of Haskell for system's programming, so lots of support for explicit manual allocation, etc. I'm not sure what happened to the project though, as I haven't heard anything about it since the report came out.
Take a look at [Linear Haskell](https://pay.reddit.com/r/haskell/comments/791qy7/linear_haskell_practical_linearity_in_a/). More generally, linear types are a principled way of bringing resource awareness to high-level abstractions.
Using a different evaluation mode during GC is not a new idea. https://books.google.com/books/about/Stingy_Evaluation.html?id=G7iktwAACAAJ I’m afraid I don’t know how to get an electronic copy. 
so sad :-( 
&gt; What is the total size of all the Haskell projects you work on? This is a very strange question I think. Particularly, it doesn't tell you the actual size of any given project like you seem to be implying in your write up.
One possibility is to x-post the survey to /r/programming. You may get some garbage responses but could also get people who have tried it in the past and gave up on it; still active developers but probably not following any of the Haskell communication channels anymore. To control for quality you could open up the survey in two batches: the first time communicated through the usual Haskell channels, and then a second time more broadly. This could at least allow you to see the responses from people active in the community separate from possible garbage input. If you opened it more widely you would have to do a lot more data cleaning beforehand (for example: "I have &lt;1y experience with Haskell but I'm an expert", maybe not a set of responses you want to take into consideration). Surveys are hard.
I'll check it out. First impression: ah, it's just like you Haskell folks to write a paper on a programming language instead of writing it. ;) But seriously, I will take a look at it.
There is a function to check if a term is well typed. I guess I should have had `normalization` check this, and throw an error if it isn't well typed, but it didn't seem like a big deal. For what its worth, the code never tries to normalize something without making sure its well typed first.
The Hume project seems dormant since 2008, but it would've suited. Most of the links are broken; here's an overview paper, and a link: [http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.10.7501&amp;rep=rep1&amp;type=pdf](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.10.7501&amp;rep=rep1&amp;type=pdf) [http://www.macs.hw.ac.uk/~greg/hume/](http://www.macs.hw.ac.uk/~greg/hume/) 
Nice way to think about it.
Added to my backlog of things to read about Haskell + types. Thank you
If you want to use your own custom templates, you can point `stack new` at a local template file or a URL. E.g. `stack new my-project https://example.com/foo.hsfiles`. For an executable-only package, you can use the `simple` template.
Impossible within the budget I'm afraid. What is possible is to have line up some of the questions between surveys of different prog langs. Then data becomes more meaningful as you can compare it to what the other communities have self reported on that topic.
I don't know what aspect you're going for, but pure functional programming doesn't play so nicely without a garbage collector. The fact that, one produced, a value must not be modified dictates that you must not free memory. As others have said, there are linear types and other ways to encode these kinds of things. But they are not well-explored in any practical setting.
Viewing on mobile is sufficiently difficult that I stopped zooming into each one and skim read the rest of the results, only zooming when I spotted something particularly interesting.
Haskell-like syntax doesn't make sense if your language is completely strict.
Yeah that should be the case when only speed is considered. But if execution space is considered, maybe a fair heuristic could be to trigger the evaluator of thunks when some threshold of memory has been reached, even if this slow down the performance in some cases. Maybe the benchmarks are focused in short snippets that are unrealistic examples of programs. Unlike an snippet of code, an application may stay in memory for a long time, so space is very important specially if occasional leaks degrade the performance and there are many users connected. If produces space leaks, a computation that is very fast when executed one time could be slow if executed many times in a loop since the single execution does not take into account garbage collection overload.
Another example of strong selection bias (very relevant): https://twitter.com/sigfpe/status/930918488440414209
Please demonstrate why. Thanks.
On second thought, I don't like the idea of having a new documentation site, separate from [the official haddocks one](https://downloads.haskell.org/~ghc/latest/docs/html/libraries/index.html). It would lead to inconsistencies and duplication of effort. Haddocks supports collapsible sections : this offer a nice way to expand the documentation of a function. Here is an [example](https://user-images.githubusercontent.com/2515201/32661567-66fb0044-c5f5-11e7-9ed5-0344a989b9bd.png)). Hopefully, GHC will move to GitHub some day. When it comes, Haddock will be the way to go.
Do you not consider the ML family's syntax Haskell-like?
The unordered nature of the where clause. The default mutual recursion of the where clause. The where clause relies on the fact that it is lazy so that one can use variables created in branches that might not execute without worrying about whether or not those expressions will be evaluated if that branch is not reached
I'm referring to Haskell's syntax in particular as many of the choices were made with non-strict evaluation in mind. Other ML family languages are designed to be strict and thus have notation that makes sense to have in a strict language.
How is that different from having local functions like in Ruby. As you see here the `where_clause_*` are not executed: ``` def complex(a) def where_clause_1; 1; end def where_clause_2; 2; end def where_clause_3; 3; end if a &lt; 0 where_clause_1 elsif a == 0 where_clause_2 else where_clause_3 end end ``` If you are bothered with them needing to be defined before (lexically) being used, then one could wrap 'm in a module: ``` module Group def complex(a) if a &lt; 0 where_clause_1 elsif a == 0 where_clause_2 else where_clause_3 end end def where_clause_1; 1; end def where_clause_2; 2; end def where_clause_3; 3; end end ``` This should allow mutual recursion of the simulated where clauses. To be honest I think laziness has benefits, but not i allowing where clauses to work.
In fact, ["How to contribute a patch to GHC"](https://ghc.haskell.org/trac/ghc/wiki/WorkingConventions/FixingBugs) says it is already possible to submit a pull request via GitHub, for "simple changes that are likely accepted without much review". This should apply to documentation change. I wish that procedure was more clearly explained. To be further investigated.
Where clauses aren't local function they are memorized. The point of them is that since you don't have to think about evaluation order (most of the time) in a non-strict language it is convenient to write in an mathematical style where you have a body expression with variables in that expression being assigned meanings. This mathematical style doesn't make sense when everything is implicitly sequenced. Variable declarations in a strict cannot just be thought of as creating definitional equalities because order now matters. It isn't about laziness making where clauses work so much as the semantics to justify them are not present in a completely strict programming language (there is however a way to make them compatible with some modifications to semantics.
&gt; I'm not sold on the additional data being worth the additional complexity. I mentioned it as an idea of something that might be "nice to have" but given that it's been voted to the top I suspect it would actually be worthwhile seriously considering implementing this next time.
Does `getLine &gt;&gt;= \x -&gt; eval x + 1` even pass the type checker?
&gt; Where clauses aren't local function they are memorized. This is possible because of purity, not laziness/strictness. Right? &gt; Variable declarations in a strict language cannot just be thought of as creating definitional equalities because order now matters. Yes. But it also has to do with immutability. Both memorization and immutability can very well be (and are) used in strict languages. Just like, as you say, where clause could be implemented in them (as I showed that they can be simulated Ruby with ease). 
It’s not obvious to me how that works, since type checking involves normalization. 
Nah, it's not really a categorical thing.
This sounds like a bread and butter technique in a JIT compiler for Haskell.
&gt; This is possible because of purity, not laziness/strictness. Right? Being memorized makes them lazy. I wasn't speaking of possibility. &gt; Both memorization and immutability can very well be (and are) used in strict languages. Just like, as you say, where clause could be implemented in them (as I showed that they can be simulated Ruby with ease). If where clauses are implicitly evaluated lazily, they make sense. Otherwise, a language should have them. A language can be strict and have lazy where clauses, which is why I qualified my points with completely strict. However, if they are implicitly strict like most variable declarations in strict languages than they do not make sense.
From page 10: &gt; Abortion alone is enough to guarantee correctness, i.e. that the program will deliver the same results as its lazy counterpart. Indeed, previous work (discussed in Section 13.2) has evaluated a non-strict language using only eager evaluation and abortion. However abortion alone is not sufficient to guarantee reasonable performance—for that we need online profiling, which we describe in Chapter 3.
Thanks.
After some discussion on freeNode, I have now created a page "[how to change GHC's documentation](https://ghc.haskell.org/trac/ghc/wiki/WorkingConventions/DocumentationChange)". If it is as i'm told, it could not be simpler !
Respectfully I disagree. Inconsistencies, yes, duplication of effort, unlikely. It's been decades and there is no thorough documentation of a standard Prelude, which means it's probably not going to happen in the near future either. I personally would not feel confident in writing documentation accurate enough to be placed by the source code of the program itself, but what I would feel confident to do would be to write pages that can be openly reviewed and modified by others, which haddock specifically prevents.
Isn't TemplateHaskell a macro system?
That's great to know. I was asking for such evidence here: https://www.reddit.com/r/haskell/comments/7d0pn0/expression_of_interest_mdn_style_documentation/dpurbxo/
See https://www.reddit.com/r/haskell/comments/7dbmqx/good_to_know_updating_ghcs_documentation_is_easy/
PS: I am better in dependently typed programming, I can do Haskell but I'm not quite as fluent.
https://www.youtube.com/watch?v=HAT4iewOHDs&amp;feature=youtu.be&amp;t=25m43s
Good to know. We didn't cover any type theory at Uni, so picking it up along the way is interesting. Thanks
If documentation in the source code is the issue, you could suggest to create a directory [in the docs folder of GHC](https://github.com/ghc/ghc/tree/master/docs). It would have an official seal of approval that will help you attract contributors. And it would be on GitHub as you rightly want. (You'd need to request committer rights, but that should not be a problem).
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [ghc/ghc/.../**docs** (master → 07ac921)](https://github.com/ghc/ghc/tree/07ac921f48baea84b40835b0b7c476806f7f63f6/docs) ---- ^(Shoot me a PM if you think I'm doing something wrong.)
You can also take a look at [`hs-init`](https://github.com/vrom911/hs-init) tool which uses `stack` templates underhood but allows you to configure your project interactively with flexibility to some degree.
Hi! I already have async up, the issue is having it able to use yesod's db and logging functions :) I'll look into that package, thanks!
Thanks! It sounds a little scary, but I'll look into it
I've seen this approach suggested elsewhere too, but I fear there are too many things I don't know yet :/ Thanks!
I've found this to be an excellent book to start you off: https://www.amazon.com/Type-Theory-Formal-Proof-Introduction/dp/110703650X - it's less focused on implementing type systems like Pierce's 'Types and Programming Languages' book, so it allows you to ramp up quickly to get to the juicy CoC stuff :)
Type theory is a different, though related field to category theory. I've found this to be an excellent book to start off, they might have it in your library: https://www.amazon.com/Type-Theory-Formal-Proof-Introduction/dp/110703650X - it's less focused on implementing type systems like Pierce's 'Types and Programming Languages' book, so it allows you to ramp up quickly to get to the juicy CoC stuff :)
Woah it will be so awesome once this becomes the normal way to build GHC. But yes, the documentation part is really important. The main reason we are all so excited about the switch to Hadrian is because the current build mechanism is so hard to understand. This is a golden opportunity to fix that. But it would be a shame if it ends up being a step backwards rather than a step forwards in approachability.
Not OP, but I just put together some charts in `Chart` a couple days ago. As both OP and another commenter have said, fiddling with the labels is a real pain. I ended up having to split up some of my charts more than I would have liked, because (as far as I could tell, anyway) you can't rotate the labels, and if the library decides you have too many to fit, it will just omit half of them. Fine for a numerical axis, but not when each label is for a distinct category. Also, a minor complaint, but giving each bar a different color isn't directly supported; you have to co-opt the tools for having different data series at one index. Also, if you haven't seen it already, [Oliver Charles has a short write-up on `Chart`](https://ocharles.org.uk/blog/posts/2013-12-24-24-days-of-hackage-charts.html).
The plot thickens. Cheers
Thanks for sharing that write-up!
I've been playing around with that, it's fantastic. Thanks for the link.
Thanks, Conkin seems pretty awesome, definitely need that deriving mechanism though. I'm loving this whole mind twisting experience, reminds me of when I first tried to grok contravariant functors.
To those who might be stumbling over this now: The next Haskell Meetup in Munich will take place in our offices: https://www.meetup.com/de-DE/munich-haskell/events/244861031
I did post it to r/programming. It was removed, apparently because they have a "no surveys" rule. Someone also cross-posted it to r/rust. I posted it to Hacker News and Lobsters. I announced it on Twitter. Short of buying ads for it, I'm not sure what else I could do.
I'm not sure if runtime could benefit from JIT compilation in any way, but I suspect other aspects might: quicker 'compile' times, quicker loads into GHCi, and shorter feedback loop to see changes.
I see you are familiar with TemplateHaskell's `[|...|]` quoting syntax. Are you familiar with TemplateHaskell's `$(...)` splicing syntax? It's basically `eval`, except it a syntactic construct, not a function, which seems to be what you want anyway.
There’s nothing stopping you from running a Haskell typechecker and interpreter/JIT at runtime. `eval` can throw an exception (or return `Either`) if the given AST can’t be turned into a value of the expected type. You can do some things like this with the GHC API or a wrapper like `hint`. I dunno how you’d go about implementing `quot` though—you’d need something that lets you reify the structure of any value, like `Typeable` does for types.
Off topic: The fact that almost every link on the mailing list gets broken because of a period is /r/mildlyinfuriating. Is this the case for everyone or is my browser playing tricks on me?
You're welcome!
Several things that are currently hard or impossible to optimize could be optimized. Such as if you use rank n types to take a polymorphic function as an argument, it’s not possible to specialize calls to that argument unless you get inlined, which isn’t always desirable.
I can confirm that it is easy. Many thanks to the maintainers who handle these documentation PRs.
[Simplexhc](https://github.com/bollu/simplexhc-cpp) has a JIT that I use for testing. I plan to steal ideas from kmet in the JIT once I get compile time performance wins. The idea to be stolen is one of representation, in some sense. GRIN is an alternate intermediate representation for haskell that requires whole program knowledge. So, it's not scalable, even though it could generate faster code (according to the graphs in the paper, IIRC). However, a tracing JIT is as good as a "whole program oracle", since you know at runtime what parts of your program are affected by what other parts. So, the hypothesis is that you can use GRIN-style optimisations in a JIT, with appropriate deoptimisation code that uses the slow path if we mis-speculate. That's the gist of the idea, I haven't gotten around to implementing yet. Patches always welcome ;) 
Yes, I fully agree. I will not consider the project finished until it is fully documented.
Oh no! It's the same for me. You can find this announcement with working links in this blog post: https://blogs.ncl.ac.uk/andreymokhov/hadrian-is-on-the-way. 
https://hackage.haskell.org/package/template-haskell-2.12.0.0/docs/Language-Haskell-TH-Syntax.html#t:Lift
 quot :: a -&gt; AST This looks fine from a type system point of view but I'm not sure if it can be a useful *function*. Maybe because of laziness it can. eval :: AST -&gt; a This looks like it will break parametricity. It should be something like eval :: Typeable a =&gt; AST -&gt; a or eval :: Typeable a =&gt; AST -&gt; Maybe a if you want to avoid exceptions.
That's cool. Which language are you referring to?
[Github repository](https://github.com/ashinkarov/heh)
&gt; It was removed, apparently because they have a "no surveys" rule. Hm, https://www.reddit.com/r/programming/comments/4imzad/launching_the_2016_state_of_rust_survey_xpost/ wasn't removed last year. Sorry the mods did that to you!
I am doing most of my work in [Agda](https://github.com/agda/agda). It's trivial to include other Haskell library via FFI and has also support of OO style programming (via ooAgda).
I would have found it useful. I am actively against extensions which break type inference being enabled by default, so I would have voted against `OverloadedStrings`. As it is, you can't tell whether there is overwhelming agreement to enable it or if there's just as many people against enabling it; i.e. you can't tell if it's "popular" or "controversial" to use Reddit terms. In terms of UI, you could have a single list with a slider with positions "prefer disabled", "no preference", and "prefer enabled".
Are you writing fun applications or proofs? I've read that Agda is more suited for mathematical proofs than apps.
Good to know. I also found some stuff in an old paper on SKIM II by Stoye that suggested something similar as well. The main thing I'm doing is re-exploring a sort of alternate history where Hughes supercombinator paper and your subsequent paper on how to evaluate supercombinators efficiently never came out. If Hughes was gunning for a 40% speedup and you maybe doubled it, my question is can I exploit the fact that jumping to arbitrary code costs a complete pipeline stall to ask whether using a smaller combinator base in conjunction with a wide SIMD or GPU unit can allow me to recover that performance drop and then some. I don't yet know if the answer is yes, but it seems like I can't find much in the way of serious exploration of the Turner-esque approach since Hughes.
They're useful as you approach dependently typed programming. They let you vary the type parameter of a type based on the data constructor. They can let you avoid `error "should never happen"` cases and make the type system enforce your invariants. Consider data Foo a where CInt :: Int -&gt; Foo Int CBool :: Bool -&gt; Foo Bool COtherInt :: Int -&gt; Foo Int getInt :: Foo Int -&gt; Int getInt = \case CInt i -&gt; i COtherInt i -&gt; i The pattern match is exhaustive, so GHC won't warn. You are less enticed to use wildcard pattern matches, and so GHC will warn you if your data types change. Imagine `COtherInt` being added later: data Foo = CInt Int | CBool Bool | COtherInt Int getInt :: Foo -&gt; Int getInt = \case CInt i -&gt; i _ -&gt; undefined Not only is `getInt` a partial function in this scenario, but GHC won't warn you when you add `COtherInt`.
LOL
I hear FSMs and Turing machines come up disproportionately more than their cousins push-down automata, or combinatory logics. ([Automata Theory](https://en.wikipedia.org/wiki/Automata_theory)). Could you explain the tradeoffs between using one over the others?
**Automata theory** Automata theory is the study of abstract machines and automata, as well as the computational problems that can be solved using them. It is a theory in theoretical computer science and discrete mathematics (a subject of study in both mathematics and computer science). The word automata (the plural of automaton) comes from the Greek word αὐτόματα, which means "self-acting". The figure at right illustrates a finite-state machine, which belongs to a well-known type of automaton. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/haskell/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^| [^Donate](https://www.reddit.com/r/WikiTextBot/wiki/donate) ^] ^Downvote ^to ^remove ^| ^v0.28
Yes, we should. 
I think this post greatly misunderstands what a macro system is. Macros do not break type safety because they run at *compile time*, and the resulting code is still typechecked, just like how the result of Template Haskell’s splices are typechecked. There are a variety of statically typed languages with macro systems, and they work just fine. Now, admittedly, there is some desire to have “typed” macro systems that can prevent macros from even *producing* ill-typed code, but that’s a separate problem. What you are describing sounds closer to [fexprs](https://en.wikipedia.org/wiki/Fexpr) than macros. Fexprs are functions that are provided their AST when called, and they use dynamic evaluation to evaluate parts of the AST as necessary. Fexprs are largely abandoned, even in dynamically typed languages, because they are very hard to reason about, and they’re extremely difficult to optimize.
**Fexpr** In Lisp programming languages, a fexpr is a function whose operands are passed to it without being evaluated. When a fexpr is called, only the body of the fexpr is evaluated; no other evaluations take place except when explicitly initiated by the fexpr. In contrast, when an ordinary Lisp function is called, the operands are evaluated automatically, and only the results of these evaluations are provided to the function; and when a (traditional) Lisp macro is called, the operands are passed in unevaluated, but whatever result the macro function returns is automatically evaluated. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/haskell/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^| [^Donate](https://www.reddit.com/r/WikiTextBot/wiki/donate) ^] ^Downvote ^to ^remove ^| ^v0.28
Oh you're right! I actually based my code on morte (basically trying to create a small version of it), and it turns out [it has the same issue](https://github.com/Gabriel439/Haskell-Morte-Library/issues/68).
I'm writing fun apps (Graphics, etc.) but as part of research. What you said is more wrong than true. Agda is primary a programming language and secondary a theorem proofer. Wikipedia (first sentence) "Agda is a dependently typed functional programming language". If you read Ulf's thesis, this states the same thing as well. Basically you get a much more expressive type system with dependent types. And cool features such as Holes. But if it justifies currently using Agda is debatable. 
Am American Can Confirm. I went to *college* at a *university*, continuing my *school*ing before I joined the workforce.
That sounds like an incredibly convoluted and specific way of getting people into programming
You have a point. The model I had in mind was something similar to Github's pull requests. Where you can comment and interact with the incommig content. In my mind that leads to a higher quality content. Granted I don't have experience with that model for content of this kind. But handling source code that way is the primary cause of improved code quality on projects I have worked on.
Are current nominations posted anywhere?
I never said my language is strict. My main goal is avoiding dynamic memory. I even have ideas on how to make the language lazy using a similar method to my closure implementation, though that would be easier within functions than in return values because a lazy int wouldn't by a concrete type so much as some closure with arbitrary captured values, takes a unit, and returns an int. Within a given function this is less of an issue since the closure doesn't escape into the caller. That said, it should still be possible to have lazy return values and parameters, but that would include a *lot* of bloat due to monomorphisation. For an example of how lazy evaluation can be implemented on the stack, see [this rust code](https://play.rust-lang.org/?gist=7173e9d3610b84854bf9a2474a50f72f&amp;version=stable). Yes it uses mutation, but lazy evaluation implicitly requires mutation anyways. Other than that, would you have preferred if I said "Elm-like" or "F#-like"? In addition, I consider `where` clauses to be little more than sugar over `let` bindings. And lazy or not, the computer is fundamentally sequential, I could say in the first release that evaluation order is undefined.
Rust has explored affine types and it's not a big step to linear types. That said, there really needs to be more experimentation with substructural type systems like linear and affine types. I'm personally leaning towards uniqueness types, but that type of static checking I'm going to hold off until I can get C-like code generation. (Aka: all safety handled by programmer. I'd rather have an unsafe language that does something than a safe language that's can't compile anything.)
&gt; Where clauses aren't local function they are memorized. Would you mind elaborating on the difference between these? From my perspective, a memorized value is just an sum type of a local function of no (or unit) arguments and its output when evaluated. When it's evaluated, it overwrites the function pointer with the returned value and sets a flag.
I agree with both of you (@yitz and @pilotInPyjamas): the Haskell docs would be greatly improved by adding more examples, but improving the Haddocks seems like the obvious way to go here (very low hanging fruit). The Haddocks already support collapsible examples, they're just not commonly used. For example, in the entire `Prelude`, only [`Maybe`](http://hackage.haskell.org/package/base-4.10.0.0/docs/Data-Maybe.html) and [`Either`](http://hackage.haskell.org/package/base-4.10.0.0/docs/Data-Either.html) have examples, but each of them have lots! If it was standard to have both local examples on specific functions and comprehensive examples at the module header level, and perhaps entire tutorial modules, I think we'd be most of the way there. But right now it's just not common to do this, and I have no idea why. Perhaps a practical problem is that Haddocks use an ad-hoc custom markup syntax, instead of something like Markdown, and building Haddocks is relatively involved. If Haddocks were in Markdown and you could check that a documentation edit rendered correctly right from the preview in GitHub, I think people would be more likely to submit small documentation PRs ([example of documentation PR from yesterday where I got the Haddocks wrong](https://github.com/mrkkrp/megaparsec/pull/256)). (And on your specific examples @pilotInPyjamas, your `Functor` example docs just seem like an impoverished version of the Haddocks with no examples??? I assume you have something more like the `Data.Maybe` and `Data.Either` docs I linked to in mind, i.e. full of examples.)
Haddock already supports collapsible examples, they're just not commonly used. In the entire `Prelude`, only [`Maybe`](http://hackage.haskell.org/package/base-4.10.0.0/docs/Data-Maybe.html) and [`Either`](http://hackage.haskell.org/package/base-4.10.0.0/docs/Data-Either.html) have examples, but each of them have lots! 
https://developer.mozilla.org/en-US/docs/Learn/Getting_started_with_the_web/JavaScript_basics I am not sure that adapting Haddock to provide that kind of site UX is possible without compromising the core design goals of Haddock. Perhaps a site structure that supported embedded Haddocs?
Possibly tangentially related, if you're in London https://www.meetup.com/London-Haskell/events/245141178/
&gt; So, it's not scalable, even though it could generate faster code (according to the graphs in the paper, IIRC). Whole program compilation can absolutely scale, and has been able to for a long time, specifics of the GRIN representation aside. (See MLton which has been doing [this for years using a very straightforward approach](http://mlton.org/References.attachments/060916-mlton.pdf), bootstrapping a 200kLOC code base easily -- or any number of the new link-time optimization tools like GCC LTO or LLVM ThinLTO, which are approximately in the same space, and scale to multi-million line programs.) The secret is they don't tend to make tradeoffs that developers like; e.g. they are very bad at incremental compilation flows or interactive (REPL) based approaches, and they take more time to compile. That's a huge problem, but different than whether or not your compilation approach can handle big programs at all (and techniques like ThinLTO close the gap a lot, anyway)...
[removed]
I think they're talking closures 
I started using Dhall a while back and it is really neat. Although some limitations feel a bit artificial, dhall has worked very nicely in practise for me. Eg. the exercise [here](https://functional-programming.it.jyu.fi/dc/tasks/FunctionComposition/page) is produced mostly from a dhall expression. Thumbs up.
&gt; easy to learn hmmmm
I haven't tried implementing systems like the one in my example with pushdown automata, but I think it might be interesting to try modeling hierarchical state machines that way. Other than that, I cannot give you a good answer in terms of tradeoffs. Based on my understanding of pushdown automata, it does not seem to give much when the workflow is quite linear (as in the blog post example). But again, I haven't explored it enough to give you a good answer. Would love to see a good comparison!
Interesting, I was unaware that MLTon does incremental compliation! I Could be wrong about ThinLTO, but from what I know, they threw a *lot* of effort at it to bring down memory usage. I'm not sure about compile time impact.
Addon developers are, by definition, already "into programming." I'm not sure what you mean.
does it increase and or decrease the cognitive load when reasoning, in your mind, about configurations or seperation of concerns?
It makes complete sense for PureScript which is strict and has Haskell syntax.
No it doesn't. That's why they are considering adding evaluation order polymorphism: https://github.com/purescript/purescript/issues/1445
Hard to say anything about cognitive load with my current experience. What I find is that I do need additional thinking when writing the configurations (resp. yaml or ini files), but that is more than balanced out by the types and the added security they provide. However, I end up doing more complex things with dhall than what I would try with yaml and that might affect my experience somewhat. But it really is awesome that I don't have to reload my service to know that my config-file will work with it.
That's a "would be nice" research discussion. PureScript is established as a strict language and it works great, with Haskell syntax. 
The Eta people are reporting some good synergies between the GHC generated code and the JIT compiler of the JVM.
The point is they are acknowledging the point I'm making.
That's what I was missing! On both parts. Thanks
You need to add a check [here](https://gist.github.com/ChristopherKing42/d8c9fde0869ec5c8feae71714e069214#file-calculusofconstructions-hs-L34) that a lambda domain type is actually a type.
MLton does not do incremental compilation -- that's one of its trade-offs, but it means its design is simple and straightforward. Many SML projects support both MLton and SML/NJ for deployment vs development to have both features. ThinLTO is precisely about reducing compile time impact of global approaches (but it's good at memory usage, too). It's closer to what GHC does, in a sense. ThinLTO makes incremental development possible by carefully allowing the compiler to compile units of code in parallel, while efficiently being able to 'globally import' functions from other modules, thanks to a clever index with low overhead. That allows global optimization opportunities at a lower cost than the traditional method(s) would, and allows parallelism. So it's an attractive approach. But modern compilers have put a fair amount of work on memory usage anyway. Even Firefox can do an LTO build with good memory usage with a modern GCC (~8GB or less), I believe, which is pretty good, and that's an incredibly large amount of code. It's mostly a development cycle irritation at that point... Regardless, I don't think whole program optimization is impractical at all and many compilers do it in one form or another these days. It's very simple and easy to implement and has good results. It just doesn't have some of the features we're used to, so some of these hybrid approaches are more attractive while adding some complexity (and so approaches like GRIN that mandate whole programs become less attractive, in turn)
I hate when people say this about Haskell. It makes me feel very stupid as it has taken me a lot of effort even to reach a low novice level...
That is done automatically when we check if `tf` has a type, I'm pretty sure.
Well, it's true that combinators have not really been explored since the 80s, but there was plenty back then. Look for things like "director strings" and "graphinators". There's also more modern approaches with FPGAs, like Reduceron (loosely base on my BWM). When it comes to evaluation during GC, as far as I know the idea was independently hatches by both David Turner and me. Later Phil Wadler wrote about it. The continuation of that was the stingy evaluation work by my student Christina. The basic idea there is to generate two code sequences for each function. One for regular evaluation, and one for stingy evaluation. The latter is called by the GC, and it does what it can with bounded resouces, and then copies itself to the new heap.
It's too late. You put an unverified `ta` into the context in `typeIn ((v, ta):ctx) b`. That can make the checker loop or return junk.
I, personally, took to Haskell like a fish to water. After only a few months^\* of going through the Haskell programming from first principles book, I was decently productive doing normal things^\*\* I think it's really hard for people to estimate reliably how much implicit background they've had when they say "oh, Haskell was easy to learn". Haskell is terribly difficult to learn, so is python. The fact that I "learned" python in a few hours says more about the similarity of it to what I already knew and the difficulty of what I was using it for than my ability to "learn languages". So if Haskell comes more difficult to you, don't worry! It comes difficult to everyone who learns it, some people just have more prerequisites invisibly learned than others when they start. ----- \* ...and years of math, years of computer science education, familiarity with self learning, experience, using computers in a very technical way, and reading Haskell articles on and off for over 3 years. Then, being able to spend a ridiculous amount of free time reading literally everything about Haskell, functional programming, type theory, etc, I could get my hands on in a very short amount of time when I started actually learning Haskell. \*\* I have yet to write anything more complicated than a web scraper that checks for 404 links.
You can scale whole program Haskell compilation to several hundred klocs. The Standard Chartered Mu compiler does whole program compilation, and compiles 500kloc (using about 4GB).
Great paper! I love it when things generalize to include multiple concepts inside one unified framework. In addition to extending this to allow for finite streams, do you think this sort of thing can reasonably be generalized to arbitrary indexable data types? (Perhaps any data type that implements something like Haskell's Traversable typeclass, for example). I've got no idea what sort of optimizations might be possible for transformational code that works on Tries, but it's fun to think about.
I have to agree with your skepticism. I really love Haskell and don't plan on giving it up anytime soon, but after 4 years of learning, I think I can only call myself an intermediate Haskell-er, and the lower end of Intermediate, at that.. It wasn't too difficult to go from complete newbie to feeling like I could write something basic, like an IRC bot. But when it comes to some of the more advanced libraries used in our ecosystem, or complex projects, I feel very intimidated and like I wouldn't be able to properly contribute. This is something I would really like to overcome, with time, and I wonder how many others feel similar.
It won't make it return junk since if `ta` is badly typed the whole thing will return `Nothing`. Also, putting `ta` into context won't make `typeIn` loop for `b`, since it doesn't assume that the context is valid.
I'm in a similar boat. Been learning Haskell on-and-off for a while. I feel like the Haskell community is really lacking in intermediate level learning materials. There's tons of stuff on the basics. And there's tons of stuff on the arcane theoretical side that frankly sounds like Greek to me. But there's not so much in the "okay, you know the syntax and you kind of understand how the language works - now let's build a real project!" realm. It has gotten better, 5 years ago there was virtually nothing in that category, now there's stuff like i.e. the Yesod book that dives into applications.
That novice level you have reached is probably way more useful in real life than you currently think.
What's an example of something that you're thinking of? I've been meaning to write more tutorials/guides/documentation etc. and having an idea of what's missing is helpful.
Ah, got it. Thanks for the clarification. 
In fairness, this is the first time I've seen the phrase "easy to learn" unqualified. 
woah, i just read the (awesomely detailed) tutorial https://hackage.haskell.org/package/dhall-1.8.0/docs/Dhall-Tutorial.html I'll definitely try Dhall for doing configuration in my current project.
I think the problem is that people forget how hard it was to learn non-Haskell programming in the first place. It’s *hard*, and purely functional programming is so different that it’s basically as hard as learning programming all over again. Once you know one imperative language, it feels like all imperative languages are easy to learn. And once you learn Haskell, it feels like all the Haskell-like languages are easy too. But getting to step one is not easy.
Your normalizer is also not quite right. For example: test = App (Lam 0 (Pi 0 Star Star) $ Lam 1 Star $ Lam 2 Star $ App (Var 0) (Var 1)) (Lam 0 Star (Var 0)) -- "normalize test" returns: Lam 1 Star (Lam 2 Star (App (Lam 0 Star (Var 0)) (Var 1))) You can fix it by having: normalize (App f a) = case normalize f of Lam v _ b -&gt; normalize (subst v (normalize a) b) Now, `typeOf loopy` loops: omega = App (Lam 0 Star (App (Var 0) (Var 0))) (Lam 0 Star (App (Var 0) (Var 0))) loopy = Lam 0 (Pi 2 omega Star) $ Lam 1 omega $ App (Var 0) (Var 1) So you should indeed move the `ta` check earlier as I suggested.
Not op, but I find that almost all languages are lacking in how-to-structure-projects type guides (with the exception of web servers and the like). I have no clue how to logically go about architecture with Haskell for projects that are more complex or involve more than just running a single pipeline of commands - e.g. long running processes.
In the list of features we have: Total - Programs always terminate and will never hang Safe - Programs never crash or throw exceptions But then there's this: Distributed - Expressions can reference other expressions by URL or path So what if an expression refers to a URL which is blocked or your network is otherwise down such that you can't get to the URL. Surely we would see either a hang or a crash then ?
It can cause the *compiler* to throw an exception. Once compiled, its crash proof. (At least, I think that's how it works.)
I wish Dhall had richer operations on its types. `&lt;` etc for Ints/Doubles. `==` for most types. I get why Dhall excludes such things though. I guess what I really want is a guaranteed-to-terminate language for describing total functions, not a config language. More like a total, pure scripting language.
You can resolve and normalize configuration files ahead of time if you are concerned about them failing in this way. Also, to directly answer your question: the URL would timeout after 30 seconds.
You can use Dhall's Haskell API to build your language that is a variation on Dhall. It's designed to be customizable for your sort of use case The key functions you want are: * [`typeWith`](https://hackage.haskell.org/package/dhall-1.8.0/docs/Dhall-TypeCheck.html#v:typeWith) * [`normalizeWith`](https://hackage.haskell.org/package/dhall-1.8.0/docs/Dhall-Core.html#v:normalizeWith) The reason Dhall has these limitations in the default language is to encourage people to not use weakly typed representations of data (such as strings or numbers) and to prefer more strongly typed representations like records and unions
Did you install Cabal? Did you set it correctly on the PATH?
You mean like morte?
I run IHaskell + Jupiter locally on my MacBook with no problems. Vaibhavsagar’s instructions in another comment should work. No need for Docker.
Is there any blog post talking about the motivation for Dhall? I've come across it every now and then, but fail to appreciate it, because I don't understand the problem it's trying to solve. 
Nothing has taught me more about this language than actually writing a non-trivial program in it - I have a project now up to 3.7kloc, which is still small, but big enough that I've gotten my hands dirty with a bunch of new concepts. I've learned lenses by actually using them, I'm very comfortable with monad transformers, including ContT, writing my own transformers, and MonadBase. I used STM to cope with concurrency. I inherited some type operators from a library that, in the end, I've mostly rewritten because it used unsafe code unnecessarily, and I had to more or less replace the libzip bindings with my own FFI code because LibZip tries to link against a deprecated, now removed, function call and won't even build on modern systems. 
I do wish I could get my hands on a copy of her dissertation, but it seems to just be buried in a library somewhere there at Chalmers. =/ Here's a sort of braindump of my thoughts in this space: The approach I've been using is in the same vein as the "stingy" style as far as I can tell. The two refinements of your description I've been using is that I only allow the GC to do work that provably is "work-safe" for the heap. That is, even in the presence of additional sharing, the work done by the GC should never increase the amount of stuff on the heap, otherwise the asymptotic cost of moving that larger amount of garbage can change the performance profile of the whole program. And second, if I have local uniqueness information then when the gc is moving a locally unique reference, it doesn't need to coordinate with other threads, but it also can use a slightly less stingy form of evaluation using the amount of stuff there on the heap as fuel to run a much larger class of reduction without fear of sharing, so long as it actually shrinks. e.g. (((S x) y) z) -&gt; ((x z) (y z)) can rebalance in place at the cost of marking the z reference duplicated, but I can also evaluate a number of primitives, etc. Stoye talks about doing the S reduction like that in place using a one-bit-rc, but its kind of nice that I can exploit it to do things like reduce inter-thread communication in a gc, as if i hold the only reference to a thing, then nobody can care if i move it. The other thing that seems to bother me about the the old SKI{BCY} style compilation is that there is a quadratic expansion in theory and (n^1.5 in practice), but given the best bounds I know of about using explicit substitutions it should be able to be n log n'ish. It seems to me the fairly naive environment propagation strategy by using S to push everything around fairly eagerly leads to an asymptotically bad translation. (One could build more tree-like environments and push them though together. Using an okasaki-esque catenable random access list I can build explicit substitution models that give log time indexing and merging, and they should transcode to combinators without an asymptotic hit. Mind you when this gets compiled down to a more modern supercombinator approach that sort of extra factor seems to get buried in the fact that it at least all happens in contiguous memory, and that indexing into that memory is cheaper than fiddling with Ks and Is, and turning an extra linear amount of contiguous memory wastage into an extra logarithmic amount of scattered memory usage likely won't help in practice, but it has been useful for me when it comes to understanding the difference in asymptotics between Turner's translation and Curry's older one.
This was the approach the Eager-Haskell folks used. They ran for some largish number of steps, then would freeze all the in-progress computations reifying them out to the heap. It had the correct asymptotics as you note. It also, like the optimistic evaluation strategy described here in Ennals thesis shared the benefiit that you could actually understand the stack traces for the most part. The constant costs, however, relegated to the dustbin of history.
To expand on this very accurate point - Also, this is a language where there are not very well documented sets of paradigms and best practices to guide beginners through building their first applications. And, to top it all off, the ecosystem and tooling is not nearly as fully fleshed out as many other languages people choose to learn, like Ruby, Python, JS, etc. So building your first few programs is probably a lot less : "Glue these 6 open source packages together for instant magic!" And much more: "Time to go research more about basic computer science while also trying to learn a totally new paradigm!" It's not so much that the syntax is hard, or the concepts are super 'advanced,' or that you need to dive deep in order to get productive, so much as it is that there is basically no way to do anything intermediate without jumping headfirst down a very deep rabbit hole.
By the way, since paths aren't dynamic and fully known at parse time, the contents of identical uri's are identical/cached within a program, for purity, right? Like let not1 = www.not.com ... let not2 = www.not.com not1 and not2 will always be the same. 
Yes, a second reference to the same URL triggers a cache hit
The case study at the beginning of this `README` shows an example use case: https://github.com/dhall-lang/dhall-lang/blob/master/README.md
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [dhall-lang/dhall-lang/.../**README.md** (master → f018f1b)](https://github.com/dhall-lang/dhall-lang/blob/f018f1b89d71f4582370d05e150040a85c137c6d/README.md) ---- ^(Shoot me a PM if you think I'm doing something wrong.)
What specific design differences exist between PS and GHCJS? Why would someone choose one or the other?
Ok so then dhall just catches the timeout and ends the program cleanly? I guess that way it's still not a crash, it's just a slightly different end to what was expected. I guess at the type level the whole program is a Maybe :)
Plz no.
Would the book presented in [this thread](https://www.reddit.com/r/haskell/comments/6ck72h/functional_design_and_architecture/) help ? I didn't read it yet, but it seems it would touch on what you want.
https://news.ycombinator.com/item?id=1924061
Keep in mind that you should expect this for any configuration language since there is a possibility that the configuration file itself was missing. In Dhall, the import of the root configuration file is not semantically distinguished from transitive imports it might depend on.
My response remains "plz no".
Thanks. 
I think Haskell is hard not because of the core language, but the broken library ecosystem. 
Any plans to migrate to something like Shake? I'd imagine it would likely be preferable for most cases?
You're welcome!
Very nifty use of plated! Suddenly parsing expressions with infix operators doesn't seem like such a chore.
In what way is it broken exactly? Particularly in what ways is it worse than other languages, as basically all library ecosystems have their issues.
I just realised that there is an error in my code. Unparenthesised operators of the same precedence but different associativity should be regarded as an error, because the expression is ambiguous. Example: ``` let (-:) = (-); infixr 5 -: (+:) = (+); infixl 5 +: in 5 -: 4 +: 3 ``` can validly be parenthesised as `(5 -: 4) +: 3` or `5 -: (4 +: 3)`. I'll update the post to fix this. Stay tuned.
Most of my upcoming talk at FnConf2017 is about this. Will share the video/slides after the talk.
That `Optional` syntax, though.
 data PairL a b = PairL a b deriving Show partition :: (a -&gt; Bool) -&gt; [a] -&gt; PairL [a] [a] partition _ [] = PairL [] [] partition f (x : xs) = if f x then PairL as (x : bs) else PairL (x : as) bs where PairL as bs = partition f xs data PairS a b = PairS !a !b deriving Show partition' :: (a -&gt; Bool) -&gt; [a] -&gt; PairS [a] [a] partition' _ [] = PairS [] [] partition' f (x : xs) = if f x then PairS as (x : bs) else PairS (x : as) bs where PairS as bs = partition' f xs The only difference between the two functions is the choice in data type, and the latter strict pair version is much slower and does not work on infinite lists.
&gt; That said, it should still be possible to have lazy return values and parameters, but that would include a lot of bloat due to monomorphisation. In an unboxed setting, the language must almost necessarily be strict because of this. &gt; In addition, I consider where clauses to be little more than sugar over let bindings. The fact that it isn't syntactically clear when they are evaluated makes them an artifact of non-strict language design. A strict language should not have them if it doesn't plan to give them non-strict semantics. &gt; I even have ideas on how to make the language lazy using a similar method to my closure implementation, though that would be easier within functions than in return values because a lazy int wouldn't by a concrete type so much as some closure with arbitrary captured values, takes a unit, and returns an int. Within a given function this is less of an issue since the closure doesn't escape into the caller. If lazy evaluation is explicit in the types, your programming language is strict. &gt; Yes it uses mutation, but lazy evaluation implicitly requires mutation anyways. Yes the purpose of lazy evaluation is to give pure languages convenient access to a safe form of mutation. &gt; And lazy or not, the computer is fundamentally sequential No it is made sequential by the program counter to preserve von Neumann semantics. One can imagine a computer designed for non-strict semantics that was less so. 
For what it's worth, `isLTE : Natural -&gt; Natural -&gt; Bool` is definable in dhall out of the box. It's not included as a primitive, but you can get there using folds and `Natutal/isZero`. If you need equality/order for other types, you're going to have to extend dhall yourself. 
I have to totally agree. A language like Go may look simple on the surface, but with experience and project size it's inevitable to reach limits of the provided expressiveness and find dark corners where the language designers had to add hacky solutions for problems unsupported in their original, restricted design. What is also often forgotten is the mental baggage one has to carry in languages that are not Haskell or OCaml due to semantics. I like the GHC extension model and cannot wait for Haskell 2020, whatever will be folded into it.
I have Haskell and C++ experience, and they're similar in that there's a lot of stuff one can but doesn't have to master to know it all. So I can understand why someone learning Haskell, which is more coherent than many languages, thinks it's easy. I find some of C semantics hard to remember and kinda "hate" all the undefined behavior.
The big thing I still run into is discovery. In many other languages you can search for a library and at least find some blog posts that compare alternatives. In haskell there often are several nice libraries with different trade offs together with a dozen experimental or outdated ones. Figuring out which is which and what the alternatives are in the first place is often nontrivial.
Talk title is trolling of the year :)
I've not used the IntelliJ-Haskell plugin, but have you tried the HaskForce plugin? Although not the most reliable thing in the world, I have gotten it to work, so you could give it a try. The problem I was having with it wasn't due to the plugin itself, but ghc-mod not supporting cabal 2. If you can avoid cabal 2 for now, it might work for you.
What about security concerns? Allowing the configuration to include arbitrary URLs or filesystem locations seems like something that might lead to information disclosure issues (both contents of files and knowledge about program start times via URL calls) at the very least.
I would actually say the library ecosystem and tooling in Haskell are a lot more fleshed out than the half-baked things JS has to offer.
That's a gem. What do you use for the Web application?
Yeah, discovery and associated resources about a library are the problems. 
&gt; Any plans to migrate to something like Shake? Actually yes!
Something like http://blog.mbrt.it/2016-12-01-ripgrep-code-review/ would be very nice
It seems like there's an alternate interpretation of this which is something like "The inputs to your test suite should be deterministic". Specifically, you shouldn't start running a new version of `hlint` without very specifically enabling it in your CI setup.
It would appear that the algorithm is used only when the extension is enabled. In particular, with `-XDerivingStrategies`, if you leave out the strategy, then GHC will use the algorithm as a heuristic for choosing a strategy for you (instead of the nonsensical current behaviour shown in the beginning example). So yes. It appears that the extension not only allows you to say what you want, but it also strives to do a better job of figuring out what you want when you don't say it. In particular, not that the Wiki page encourages you to always specify a strategy, since the algorithm is so complicated, and you could end up with instances you didn't expect. 
This algorithm is what GHC uses whenever you don't provide an explicit strategy keyword (i.e., when you write `deriving C` instead of `deriving stock C`, `deriving newtype C`, or `deriving anyclass C`). Whether `DerivingStrategies` is enabled or not has no effect when you derive something without a strategy keyword.
Have you looked at http://hackage.haskell.org/package/t-regex ? 
I really appreciate author’s investigation of previous solutions like what he did before(megaparse, modern-uri), it helps Haskell beginner understand and select library, and output an awesome solution! Thanks! Mark Karpov!
What program did you write?
Having a great syntax highlighter, with links and stuff, would be *so good*.
Damned, I did not read the last line: &gt; The last one probably could be a project on its own :-D 
&gt; In an unboxed setting, the language must almost necessarily be strict because of this. We'll see about that. &gt;:D &gt; The fact that it isn't syntactically clear when they are evaluated makes them an artifact of non-strict language design. A strict language should not have them if it doesn't plan to give them non-strict semantics. Many strict languages have unintuitive evaluation order. Some languages evaluate parameters right-to-left, others left-to-right. If the compiler can determine if a function is pure, it may drop it off entirely. And then there's the processor itself, which can rearrange instructions in bizarre ways while still maintaining the semantics. &gt;If lazy evaluation is explicit in the types, your programming language is strict. Does it count if said types are hidden from the programmer? Closures with captured variables are also "explicit" in the types, does that make it not a functional language. &gt;No it is made sequential by the program counter to preserve von Neumann semantics. One can imagine a computer designed for non-strict semantics that was less so. Come back to me when you can get a hold of a machine that doesn't hide non-sequentialism behind a sequential interface and make a Reddit reply from it. I'll be waiting.
Can't you just configure hlint per project basis? Or am I missing something?
I would use the [`simpleCors`](https://www.stackage.org/haddock/lts-9.13/wai-cors-0.2.5/Network-Wai-Middleware-Cors.html#v:simpleCors) middleware from the `wai-cors` package. 
&gt; We'll see about that. &gt;:D I would be glad to be proven wrong. In my view, the only downside of non-strict evaluation is the inability to work in an unboxed setting. &gt; Many strict languages have unintuitive evaluation order. Some languages evaluate parameters right-to-left, others left-to-right. That's a bad thing. &gt; If the compiler can determine if a function is pure, it may drop it off entirely. I think you need totality for the compiler to be able to do that. &gt; If the compiler can determine if a function is pure, it may drop it off entirely. &gt; Does it count if said types are hidden from the programmer? Depends on what you mean. What I said was actually incorrect though as there is a way to present strictness information in the types while still having a legitimate claim to being a non-strict language: evaluation order polymorphism. &gt; Closures with captured variables are also "explicit" in the types, does that make it not a functional language. If you mean something like C++ lambda capture clauses than no that doesn't affect it because the conceptual type of functions is the same (you can use `std::function` to contain it regardless of what is captured -- I know that technically it generates separate types for each lambda for performance reasons, but I don't think that is relevant from a denotational perspective). &gt; Come back to me when you can get a hold of a machine that doesn't hide non-sequentialism behind a sequential interface and make a Reddit reply from it. I'll be waiting. The reason I mention it is because in some ways current day processors lend themselves to non-strict evaluation but the fact that today's languages are strict and impure forces them to be conservative and come up with ways that preserve these semantics to take advantage of those features such as branch prediction. &gt; And then there's the processor itself, which can rearrange instructions in bizarre ways while still maintaining the semantics. If only the processor didn't have to worry about preserving the strict and impure semantics then it could do more. 
Because if there's a `Traversable` instance for something, there's always a `Foldable` instance for it as well. More succinctly we would say that `Traversable` implies `Foldable`, just like `Applicative` implies `Functor` and like `Monoid` implies `Semigroup` (that last one's getting fixed in the typeclass hierarchy in the next year).
I recommend using the haddock documentation. It's excellent. https://hackage.haskell.org/package/Spock-0.12.0.0/docs/doc-index-J.html
Define 'work' - I got it to function, ish, on windows, but it was dodgy, fragile and incomplete. This project has an awful lot of promise: https://github.com/haskell/haskell-ide-engine I managed to get that working (to my satisfaction, at least) in VSCode and SublimeText3 (although sublime text doesn't seem to support all of the features surfaced by that project). So far the most painless IDE-like experience I've had has definitely been spacemacs haskell layer - No muss, no fuss, just install and it works. The disadvantage is that then you have to learn how to navigate spacemacs, which is, um, exciting.
Hello /u/c_wraith. Is it generally the case that the documentation will tell me where to import a function from? Another example [selectList](http://hoogle.haskell.org/?hoogle=selectList). If I look a the [persistent docs / index](https://hackage.haskell.org/package/persistent-2.7.1/docs/doc-index-S.html), it list the module in which the function is defined, and two other modules which export the function as well. I'm guessing that at this point it's up to how good the documentation is, but going to the index of the package is a vast improvement over what I was doing. Thank you!
GHCJS is full Haskell compiled to JavaScript, including something like the GHC runtime running in JavaScript. PureScript is a separate language from Haskell (in particular, it is strict), and is very minimal by comparison. It has no runtime, and no required standard libraries. You can use it a bit like TypeScript, as a completely standalone thing, generating very small, self-contained libraries.
To instantiate /u/andrewthad’s point: You can define `foldMap` using `Const` and `traverse` newtype Const a b = Const { getConst :: a } instance Functor (Const a) where fmap f (Const a) = Const a instance Monoid a =&gt; Applicative (Const a) where pure _ = Const mempty Const x &lt;*&gt; Const y = Const (x `mappend` y) foldMap :: (Traversable t, Monoid m) =&gt; (a -&gt; m) -&gt; t a -&gt; m foldMap f = getConst . traverse (Const . f) `foldMap` is one of the minimal complete definitions for the `Foldable` class, so this is sufficient to prove that all `Traversable`s are `Foldable`. Take a look at the source for `Foldable` in base to see how `foldr` can be implemented in terms of `foldMap`
Why would someone choose one over the other? What niches do each fill?
I'm think the few cases of people trying to and succeeding at teaching Haskell to kids show reasonable evidence that it's actually easier to learn from scratch than C-style or OO style programming.
You can use hoogle to search for a function and find where it comes from. I believe Stack has a command line interface for using hoogle against your project’s dependencies, but there’s also https://hoogle.haskell.org for a limited set of libraries. Cabal and Nix also have ways of using hoogle. Finally, you can use `hoogle` serve to start up a great web interface for hoogle.
so if i had a Traversable, i would already have a Foldable?
&gt; Is it generally the case that the documentation will tell me where to import a function from? The index will tell you all modules that export the identifier and you can import it from whichever of those seems most canonical to you. 
Thanks! I tried compiling with ghcjs but I wasn't able to install it. So I just put the executable on my web server (runs Flask), the Decide button sends a request to the server which just shells out and runs the executable, after some input validation. https://git.modalduality.org/modal/duality/tree/app.py#n178
You still have to write the instance. The constraint is just there to enforce that there must be an instance, since it’s proven that one can exist. To lower the boilerplate, you can use `LANGUAGE DeriveTraversable` to let you simply derive both classes. Or if you need a custom `Traversable` instance, you can use `foldMapDefault` from `Data.Traversable` to stub in the correct `Foldable` instance for your custom `Traversable` instance.
I think it only does URL calls and such at compile time, but I'm not sure.
That's a complicated question with many trade-offs. The generic answer is that you need to consider your use case carefully and decide for yourself. I wrote a bit about the PureScript side of things here: http://blog.functorial.com/posts/2017-08-09-Why-You-Should-Use-PureScript.html
It looks interesting, but inline operators would make it much more approachable. Also, I got a parse error when I tried to use the constant 1. 
Are there types that are Foldable but not Traversable?
/u/tomejaguar thank you. Thi solves my problem precisely.
In this passage: f :: Π (x :: Bool) (if x then Int else String) -&gt; String f x = case x of True -&gt; 42 False -&gt; "abc" is the "-&gt; String" supposed to be there?
`Set` is an example
Hey, thanks for the feedback. I might add those if I can figure out a good fixity order, maybe just the same of whatever Haskell does. For literals (and I know this is tedious, will make this easier), you have to declare them first. So `Ex.La=1.(&lt; x a)` says there's a natural less than 1.
A good diagram to help: [typeclassopedia diagram](https://wiki.haskell.org/wikiupload/thumb/3/39/FunctorHierarchy.svg/558px-FunctorHierarchy.svg.png)
Shit I took ~5 years of off and on effort to learn it. I even gave a talk about this.
&gt; The reason I mention it is because in some ways current day processors lend themselves to non-strict evaluation but the fact that today's languages are strict and impure forces them to be conservative and come up with ways that preserve these semantics to take advantage of those features such as branch prediction. &gt; &gt;&gt; And then there's the processor itself, which can rearrange instructions in bizarre ways while still maintaining the semantics. &gt; &gt; If only the processor didn't have to worry about preserving the strict and impure semantics then it could do more. This I think is actually an interesting problem. Under the covers, modern CPUs do act in a non-strict manner, but hide everything behind a strict assembly language. High level programming languages are forced to compile to the strict assembly because everything else is hidden. It would be interesting if it were possible to signify to the processor certain invariants to allow it to be more flexible with optimizations, but instead all we have access to is the compiler. &gt; In my view, the only downside of non-strict evaluation is the inability to work in an unboxed setting. I will have to respectfully disagree here, though many of my issues couldn't be grouped under that heading. If you ask me, non-strict evaluation at runtime (ie: excluding compiler optimizations like dropping entire calls that aren't used). For one, there's the overhead of having to signify whether or not a computation has passed. (It may be a single byte, but it's still non-zero) In addition, if the value itself is smaller than 1 word, then it has to be padded to a full word to hold the instruction pointer (and even further to contain captured values if not behind another layer of indirection, which itself can cause slow downs due to cache misses). For simple operations like array indexing and basic arithmetic, the overhead of even calling any function can exceed the cost to simply perform the primitive operation. (Function calls are not instant! That's why inlining exists, but indirect calls tend to confuse inlining optimizations.) There are probably others, but the main point is that non-strict evaluation tends to require a lot more bookkeeping than strict evaluation. That said, I do plan on experimenting with non-strict evaluation, but that will have to wait until I can figure out why my typechecker doesn't understand that a given free variable is actually a type of closure.
One place where it might soon be nice to have some JITing is in the type-level computations that GHC has to perform during typechecking: Certainly the algorithms that folks implement using type families could stand to be jitted, but also the logic program for instance queries could perhaps be compiled to native code. As type-level computations grow in complexity, it might be nice to run them faster.
I'm struggling with the similar issues like you right now, OP. However I am trying to use GHC 8.2.1 (which cannot be installed through Alpine's package manager). In case you'd like to use GHC 8.2.1, here's my working image: https://hub.docker.com/r/carbolymer/haskell/ Were you trying to run the Haskell application using Docker, maybe? I am experiencing segfaults when running statically linked binary inside Docker (even inside the same image which was used to build the app). But without the Docker, everything works fine... 
Wow this is a stretch for me. In data SMethod m where SGET :: m ~ 'GET =&gt; SMethod m SPOST :: m ~ 'POST =&gt; SMethod m deriving instance Show (SMethod m) what does the S stand for? What language extensions are being used? What does the leading apostrophe signify in 'GET and 'POST? Are SGET and SPOST types rather than constructors?
The main example, as /u/ephrion points out, is `Set`. There's another that goes even further: data Repeat a = Repeat Int a deriving (Functor) instance Foldable Repeat where toList (Repeat n a) = replicate n a The type `Repeat` has been given a `Functor` instance and a `Foldable` instance, but there is no `Traversable` instances that follows the laws about how it is supposed to be related to the `Functor` and `Foldable` instances. For what it's worth, you could alternatively give `Repeat` the normal `Foldable` instance that you would have gotten from `deriving Foldable` and then there would be a `Traversable` instance for it. But, basically, `Set` is the only thing I ever actually see in practice.
http://www.imdb.com/title/tt0119081/quotes/qt0402441
You can make this a non issue by making it `AST a` Then you have data AST a where ... quot :: a -&gt; AST a eval :: AST a -&gt; a toMacro :: (AST a -&gt; AST B) -&gt; a -&gt; b toMacro function = eval . function . quot In other words... why not extend type safety into your macro language too? Note that in Haskell, the real problem you're going to run into trying this is referential transparency (on principle) and type erasure (as a concrete 'you shall not pass go' issue). You simply can't do anything interesting to an arbitrary `a`. You need to know something about it, for example that it satisfies some typeclass constraint.
Parsing everything uniformly than fixing up associativity / precedence is also a critical part to the Agda-style mixfix operators.
I have tried it as well but it did not work runServer conn = simpleCors (serve appAPI (appServer conn)) and then call it like this runServer'' :: IO () runServer'' = do conn &lt;- connect run 8001 (runServer conn) but it did not solve the problem. in addition to that, I have tried to add the below to the response header of the post api and still same issue 
Oh, you're using Servant. Uhh... hmm. I figured the WAI-level wrapper would work in any case, but maybe read through [this issue](https://github.com/haskell-servant/servant/issues/278)? 
Slightly confusingly there are both [http://hoogle.haskell.org/](http://hoogle.haskell.org/) which indexes more packages and [https://www.haskell.org/hoogle/](https://www.haskell.org/hoogle/) which defaults to only the prelude but has much better fuzzy searching.
IIRC this issue covers fine-grained, "integrated" OPTIONS (for CORS) support in servant. But _wai-cors_ is the way to go really, people have used it with servant, yesod or anything else wai-based. In fact, the web framework used is pretty much irrelevant; all this library needs is a wai `Application` that it can then transform, to add CORS support.
I mean the specific thing you are asking isn't possible. Since the output type depends on the input value. (6 and 7) :: Expr (3 or 4) :: Expr recurse (6 and 7) :: Automata (Int, Int) glyph recurse (3 or 4) :: Automata (Either Int Int) glyph
Many people (myself included) have successfully used wai-cors with servant (and other web libraries -- in fact, it doesn't matter to wai-cors, see my other comment). Maybe you need to tweak the default settings that the middleware uses? You might want to bring that up on the issue tracker in case you don't get an answer here.
Have you seen [order-pp](https://github.com/rofl0r/order-pp)? It's a programming language (with lambdas, higher order functions, etc) written in the C preprocessor. Its primary goal is to be a metaprogramming library for C, but along the way it's a full functional programming language in its own right.
Assuming you mean: https://github.com/rikvdkleij/intellij-haskell Then yes, I've used this as my Haskell editor exclusively (for about a year) on MacOS. It is a bit finicky to get running at first, so make sure you follow the readme exactly (it might be worth uninstalling, then reinstalling before trying again). FWIW the author was very responsive to the one issue I've had. If you continue having issues it might be worth bringing the problems you're having to his attention. If you can post specifics about what you tried and what errors you see, I can see if I can help (no affiliation to the project).
[pipermail being a piece of old garbage, as always](https://mail.python.org/pipermail/mailman-users/2008-February/060564.html)
Why on earth would you use intellij tooling for haskell? 😃 
Are you sure this can be considered Turing complete (an expression representing the largest prime might be a good demo)? I've read your intro here, and it sounds like you're describing a strongly normalizing system: https://github.com/twhitehead/c99-meta/blob/master/Recurse.h
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [twhitehead/c99-meta/.../**Recurse.h** (master → adab059)](https://github.com/twhitehead/c99-meta/blob/adab059bbb3145defd5a43b032986f475d010b24/Recurse.h) ---- ^(Shoot me a PM if you think I'm doing something wrong.)
No.
I thought hoogle.haskell.org had a mostly working type search? It seems to work for me just fine...
When you reorganize your code, make all of your imports qualified. One of my pet peeves is when I come across a module with dozens of unqualified imports - it makes understanding the core a lot more difficult in my opinion. Here’s a great article on the wiki: https://wiki.haskell.org/Import_modules_properly
Would be cool if you could support a JSON syntax for Dhal and have some interoperability with JSON-Schema. Then I could use it in some of my day-to-day work. Should be doable right? Maybe a bit perverse though haha!
http://hoogle.haskell.org indexes all of stackage so the set of libraries is quite large.
Hm okay. It looks like imports are implemented by extending Dhall, so maybe that would be a good place to look to understand exactly how to do it?
That was pretty much my main motivation for writing [hsimport](https://github.com/dan-t/hsimport)/[vim-hsimport](https://github.com/dan-t/vim-hsimport). The basis is the functionality of `hdevtools findsymbol` for finding definitions in the currently used libraries.
I agree, this is very irritating when it happens. I usually resort to a combination of Hoogle, function search in github (`language:Haskell &lt;function&gt;`), reverse lookup on packdeps (https://packdeps.haskellers.com/reverse). We should all have better discipline at using qualified imports, for one thing. Also the `PackageImports` extension is handy to document stuff: https://www.schoolofhaskell.com/school/to-infinity-and-beyond/pick-of-the-week/guide-to-ghc-extensions/basic-syntax-extensions#packageimports Spock is certainly well-documented, but the possibility of not knowing where to find stuff is very real in this language in general.
Is it based on Haskell Source extensions or on the GHC API?
Single colon becomes double colon again :-)
hdevtools uses the GHC API.
Awesome, thanks.
From the [pro-FTP wiki-page](https://ghc.haskell.org/trac/ghc/wiki/Prelude710/FTP#superclass): &gt; **It should also be noted that `Traversable` can be added to `Prelude` without adding `Foldable`. Today `Foldable` is a superclass of `Traversable`, but there is no real need for that. (E.g., building the lens package and all its dependencies only requires 9 trivial changes when `Foldable` is no longer a superclass.)** &gt; &gt; Removing `Foldable` as a superclass of `Traversable` destroys a relationship between the `Traversable` class and 33+ operations with well known and well understood semantics. &gt; &gt; The same argument has historically been applied to argue against adding `Applicative` as a superclass of `Monad`. &gt; &gt; It can also be applied equally well to say that `Ord` doesn't need `Eq` as a superclass, despite the fact that `Ord` provides us obvious laws giving us guidance for how the corresponding `Eq` should work. &gt; &gt; It would seem remarkably backwards to finally unify all the work on `Applicative` and `Monad` and simultaneously cleave apart all the work on `Foldable` and `Traversable` creating a variant on the `ap` vs. `(&lt;*&gt;)`, `fmap` vs. `liftM` problem but now spanning dozens of combinators. 
Very interesting, the only problem I have is that all is in one file on a big pile. I have recently finished my first haskell project too ([expressions](https://github.com/jakubdaniel/expressions) and its bindings to the z3 solver [expressions-z3](https://github.com/jakubdaniel/expressions-z3)). I would like to read more on your approach (involving automata apparently).
I think parsec-style parsers wouldn't be possible without laziness. I remember having a problems defining one in elm the way I wanted to. Parser combinators are often defined recursively, but I wouldn't say they are an infinite datastructure. (I'm sorry, this answer doesn't have much substance to it, I don't remember what problem exactly I had anymore)
Set is an example. data Rep a = Rep Natural a instance Foldable Rep where foldMap f (Rep n0 a) = go n0 (f a) where go k m = case quotRem k 2 of (q,0) -&gt; go q (m &lt;&gt; m) (q,1) -&gt; go q (m &lt;&gt; m) &lt;&gt; m is Foldable and Functor but has no corresponding Traversable. It is useful when working with run-length encoding. Notably, its `Foldable` provides many operations in log time rather than the obvious linear time.
Exactly. And the `Foldable` should be defined in such a way that it is compatible with the `foldMapDefault` definition in `Data.Traversable` otherwise things like `traverse`, which needs `Traversable` and `traverse_`, which only requires `Foldable` start diverging in behavior in ways that would be terribly surprising for users.
Actually, imports are a separate phase that precedes type-checking and normalization so while you can customize Dhall's import behavior it won't illuminate how to customize type-checking and normalization
What is the difference between euclid :: Int -&gt; Int -&gt; Int and euclid :: Integral a =&gt; a -&gt; a -&gt; a In this function. To me the int -&gt; int one makes more sense, but calling :t in ghci gives the bottom one euclid a 0 = a euclid a b = euclid b (a `mod` b)
You can generate JSON from Dhall: https://github.com/dhall-lang/dhall-lang/wiki/Getting-started%3A-Generate-JSON-or-YAML
The latter is more general than the former. The latter will work on any integral type(`:info Integral` in ghci for some examples). Perhaps most importantly, the latter will work with `Integer` which is Haskell's "bignum". When defining a function without an explicit type annotation, GHC will always attempt to come up with the most general type.
The difference is that the bottom one accepts more values. The first signature limits the function to only working on `Int`s, but the second one can work on `Integer`s, `Word`s and things like that. Check out that instances of the `Integral` class https://hackage.haskell.org/package/base-4.10.0.0/docs/Prelude.html#t:Integral
Lollerskates -- I was about to mention Boost.PP, but I see that order-pp was actually written by a contributor to PP.
When splitting into modules, I usually just copy/paste the original module and start by changing the "module" declaration (obviously) and removing the functions that I don't want in the new module. Then turn on "-Wunused" or whatever it it's called and GHC will tell you which imports you don't need. (Other suggestions ITT are of course good if you're looking for a particular function/import. For some reason I just do this copy+remove thing usually.)
Well, I mean aren't all programs that actually run at the mercy of this problem? I mean there's never a guarantee that the OS (or even CPU) will execute the next instruction, so... Granted, network fail several orders of magnitude more often than CPUs/OSs, but when proving "progress" for any type of algorithm you kind of have to assume that the underlying machinery can also make progress.
This is really interesting -- I'm pretty used to doing config via e.g. XML/YAML/TOML, etc., but it really irks me that most bindings to those languages have oodles of runtime failure scenarios[1] where Dhall seems to be able to resolve most of those *immediately* on load. [1] It's not *that* big a deal (how often do you change configs?), but it *is* annoying when internal references in configs fail at runtime.
I kind of agree *and* disagree: I think it's quite possible to learn enough Haskell to be reasonably productive if you join an existing team with a good code base where you can draw examples from and adapt them to the situation at hand. (Code review can help the people mentoring you to figure out how to avoid the danger of copy/paste here.) *However* if you haven't programmed in an imperative/OOP language without an effect-constraining type system or compiler, you won't *fully* appreciate what Haskell (and e.g. Idris and other similar languages) gives you over those abominations[1]. Disclosure: I migrated having seen the massive utility of Algebraic Data Types in O'Caml and working in O'Caml for a few years. However, *then* I discovered type classes... [1] Yes, I mean this.
I'm not sure what "fleshed out" means, but the general quality of the stuff on Hackage is *massively* better than the random shite you find on NPM. I *believe*, but cannot prove, that it's because Haskell forces you to actually think about what you're doing -- at least to a certain extent -- before/while writing code. The JS world is a massive clusterfuck of 10x programmes and 0.01x programmers and from the outset it can be very hard to tell the difference because maybe the 0.01x programmers are *very* good at writing visually appealing documentation (or have a massive organization behind them, doing that).
By "fleshed out" I mean that the JS ecosystem just seems to have lots of implementations of tools that do the bare essentials (e.g. downloading packages and resolving dependencies but nothing like Haskell's LTS snapshots to make sure the packages actually work together or actually having unit testing integrated with the package manager so people can actually test if stuff works,...).
Great stuff :) .
Oh, yes, that's probably true at least as an initial impression/experience. It's just that (IME) it later turns out that the things are massively broken... but you only find out waaay to late to do anything about it.
To be fair, I just stole the example from an [old post of yours](https://www.reddit.com/r/haskell/comments/5nfek8/why_traversable_is_the_real_deal/dcc2qh6/).
the later one is more generic.
the problem was that simpleCors function does solve the issue but it does not support sending POST request with JSON format. so I have `cross` function while making the endpoint to accept to another format app conn = cors ( const $ Just (simpleCorsResourcePolicy { corsRequestHeaders = ["Content-Type"] }) ) (serve appAPI (appServer conn))
The previous replies to your question are sufficient - `Int -&gt; Int -&gt; Int` is a function that can be applied **only** to integers. The other function is a function that can be applied to any type that has an instance of the `Integral` typeclass. The following goes a bit beyond your question, so feel free to ignore: There is an interesting relationship between how general a function is, and what the function can "do", in a general sense. For example, in the `Int -&gt; Int -&gt; Int` version `euclid`, the `euclid` function, we know that arguments are going to be values of type `Int`. If you type in `:info Int` into `ghci`, you will see that the `Int` type has instances of `Bounded`, `Enum`, `Eq`, etc. That means that the version `Int -&gt; Int -&gt; Int` version of `euclid` can use any method from any of those typeclasses. It can do a lot! The `Integral a =&gt; a -&gt; a -&gt; a` version of `euclid`, on the other hand, knows less about its argument than the `Int -&gt; Int -&gt; Int` version. It can **only** use methods are required by the `Integral` typeclass. There are only 7 of them (if I counted correctly). So, using the more general function has a few benefits. * Because the more general function can do less, it is easier to write and get right; you have less choices. * The more general function communicates more; a reader of your code can look at that and understand that that function does "integral things", not "Integer things". * It allows you to swap out what value you actually apply `euclid` to, without having to change your code; you can use a `Word` at first, and then your code changes and it ends up apply `euclid` to a `Int`, and you don't have to change `euclid`. Notes: * In order for a type to have an instance of `Integral`, it must also have an instance of type `Real` and type `Enum`, so there is actually a lot that `Integral` can do, but the point that it is less than `Int` still stands. * The version of `euclid` that is `Int -&gt; Int -&gt; Int` is called the *monomorphic* version; it can be applied to only one (mono) type. * The version of `euclid :: Integral a -&gt; a -&gt; a -&gt; a` is the *polymorphic* version; can be applied to many (poly) types. 
What's stopping us from including `.hi` files in packages? Context: whenever I need to build a new project, I inevitably need to download and build a significant chunk of hackage; often, I'm building it because I want to make changes, and usually, my first change is a type error (or I want to use ghci to explore the types). If packages include `.hi` files, I could (in principle) typecheck without compiling. Relatedly, what stops `GHCi` from byte-compiling modules as run code actually requires it rather than all at the beginning? I understand that one thing stopping us is "a lot of engineering work." Is that the only thing?
Thank you for clearing up the confusion.
`stack image container` + anything that can run Docker
I would also recommend Docker. For me, it's the easiest way to deploy anything, not just Haskell. That being said, if your development and production environments are more or less the same, you could deploy a binary. That might be easier. 
This is exactly what I do too. 1. Copy-paste into a new module. 2. Change the module name. 3. Add an explicit export list that only includes the stuff I want to export. 4. Keep removing stuff until it builds `-Wall` clean. 
I user Mac, so I was thinking about using virtual box for installing a linux version which will be same as prod env and deploy on this VM. am not sure whether this way is easier than docker
If it's a simple Haskell server application with trivial system dependencies running on a single machine (I'm not sure what `ocean db` is, but I'm assuming you mean DigitalOcean), you can just copy it over, write a little systemd unit so it starts at boot and be done with it.
shouldn't I build the binary then deploy to DigitalOcean?
Yup, that would work. If you're not already familiar with Docker it might be a little much to dive right into. Building in a VM (or even building on "production") will probably be easier. 
a) Docker b) Heroku c) Keter + VPS (probably?)
The resulting .hi file for a given .hs file can vary greatly depending on the version of GHC used, compiler flags, and targeted platform. For instance, it will be very different depending on whether optimization’s are turned on. Or there may be a difference in the format between GHC versions. Not to mention `LANGUAGE CPP` can make absolutely everything change. You can have GHCi save binaries by passing it `-fobject-code` or using `:set -fobject-code`. This will tell GHCi to use its fancy dynamic linked with actual binary objects instead of its interpreter. The only downside is that it’s slower, I believe.
I had a good experience with Heroku. just use some buildpack and it's correctly deployed to heroku
I use Nix. So I can just use `nix-copy-closure` to upload a build off my machine. You can even build and push entire system configurations, including stuff like systemd services for running your code or whatever else. Then deploying is as simple as copying that up and doing a Nix profile switch. It’s a lot like Docker except much more declarative and composable, plus Nix is just way more powerful than Docker. Another benefit of this is that if you need a whole complicated system environment in order to run your code, you can use the same Nix config to define nixos-containers for that environment so you can test locally using exactly the same stuff (again, much like Docker, but more composable). So I just separate the system requirements from the services that actually run my code and have the latter import the former. This way I can use local incremental tools for developing the app locally and still share the system requirements config.
Excuse me, are you trying to add side effects to Haskell code?
If you want it to be absolutely trivial to your users to install, a common pattern is to generate a binary, pick out the shared libs you installed , put those in a relative dir, have a wrapper launcher script that sets the ld library path, and bam you have a relatively portable (provided you’re on the same Unix / architecture) executable . Any sufficiently sopshticated prject should be packaged nicely for the target environment. But this is an ok rough cut
That only works if the two systems use the same OS, right? I can't copy the binary from my macOS dev machine to a Linux server and expect it to run, the binary has to be re-compiled for the correct host.
Right. What I described only works if you have the build products for the system you’re targeting. However, even from macOS, you can set up Linux build slaves. But that’s obviously getting much more complicated.
[removed]
yeah, I still need to figure out how the import system works... expressions looks cool, especially the prenex/flattening functions. Can z3 decide bounded Peano arithmetic propositions? My approach is basically outlined in Sipser's book. It turns out you can "add numbers" with DFAs if you're given the input in a special form, by representing the numbers as binary where each character of the input is a tuple of the nth bits of both the terms and the result. So if we're checking 0100 + 0011 = 0111, the first character is [0, 1, 1], corresponding to the LSBs of the three numbers. The DFA just keeps track of the carry bit in the state. For existentializing a variable, instead of looking at the input for the bit, you non deterministically go to the states for both 0 or 1 for that bit, and universals are just `not exists not` as you know. https://imgur.com/a/9btlg if you'd like the full details.
Yes this is wrong, I need to correct it. 
I have a script that can set that up in one command. :D source &lt;(curl -fsSL https://raw.githubusercontent.com/LnL7/nix-docker/master/start-docker-nix-build-slave)
The *easiest* way that works for any system is probably this: 1. Use statically linked binary. 2. Use stack's Docker support to build a Linux binary (unless you're already on Linux, of course). 3. `scp` to your server/VPS/etc.
Note that this is basically what NixOps does, plus much more.
Here's an idea: {-# LANGUAGE LambdaCase #-} data Core prim = Lit Integer | Prim prim | Var Int | Lam String (Core prim) | App (Core prim) (Core prim) deriving Show data Primitive = Add | AddN Integer | Negate deriving Show prim :: Primitive -&gt; Core Primitive -&gt; Maybe (Core Primitive) prim p (Lit i) = case p of Add -&gt; Just (Prim (AddN i)) AddN i' -&gt; Just (Lit (i + i')) Negate -&gt; Just (Lit (-i)) prim _ _ = Nothing step :: (prim -&gt; Core prim -&gt; Maybe (Core prim)) -&gt; Core prim -&gt; Maybe (Core prim) step prim = \case (App (Prim p) x) -&gt; prim p x (App (Lam _ c) v) | isValue v -&gt; Just (shift (-1) (substitute (shift 1 v) 0 c)) (App v c) | isValue v -&gt; App v &lt;$&gt; (step c) (App c1 c2) -&gt; flip App c2 &lt;$&gt; step c1 _ -&gt; Nothing step' :: Core Primitive -&gt; Maybe (Core Primitive) step' = step prim Now the primitive-specific code is contained in `prim` and you can define languages with different sets of primitives rather easily. To be more helpful, I would have to know why exactly you are unsatisfied with the hard-coded primitives. There are a lot of different ways to generalise this design, and to choose between them, it is necessary to know what you want to do that requires additional flexibility. (If you only suspect that you might need additional flexibility later, then wait until that point. Premature generalisation is almost as bad as premature optimisation. ;))
I don't really think Nix fulfill's OP's criteria - namely being the "easiest" way to deploy Haskell code. While Nix _is_ very powerful, and can be used to solve problems that Docker simply can't (e.g. the complex build system needed to get GHC to compile for iOS, Android, and x86 platforms), it's also largely unapproachable, and carries a much steeper learning curve. Generally speaking, when I ask for the "easiest" way to do something with Haskell, I'm really looking for a means to use the language in a way that fits with the current, larger software ecosystem. Nix, imo, really doesn't fit this bill, as outside of the Haskell community it's used very sparingly. So, while I do think Nix is very interesting and it's on my list of Things to Learn, it's something I'd give a bit more pause before recommending to someone who's just looking to do a simple backend deployment to DigitalOcean.
S stands for nothing special, leading apostrophe comes from a GADTs extension. You are probably confused about the GADTS syntax used here, you can take a look at ghc manual or some other resource about this http://ghc.readthedocs.io/en/latest/glasgow_exts.html#generalised-algebraic-data-types-gadts
Yes, that's what I meant with "it" (what you have built).
Consider multiple backends. While an interpreter can rely on Haskell's operations (so the list of primitives is just the operations from Haskell that I want) once I start writing the compiler I need to change primitives depending on target language. It would be nice to have an external representation and just provide different environments depending on target. I think the strategy you presented would work for that though.
Please, a multi-line table extension.
I haven’t looked into it too closely, but my intuition says it’s probably a Turing-complete *language* whose *implementation* will always have some arbitrary (enormous) limit on the number of reductions, similar to how we use TC languages on real computers, which are bounded-storage/finite-state machines with some (enormous) number of possible states.
I think you're right. The examples demonstrating `iterate`, `cycle`, etc make that clear here: https://github.com/twhitehead/c99-meta/blob/master/Tests/List.c
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [twhitehead/c99-meta/.../**List.c** (master → adab059)](https://github.com/twhitehead/c99-meta/blob/adab059bbb3145defd5a43b032986f475d010b24/Tests/List.c) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dq0nhep.)^.
Fair. Nix is not easy *to learn*. But once you know it, it’s probably one of the easiest methods to deplo, at least for me.
This is excellent, but I wasn't very clear. I'm talking about the correspondence between the type of a Dhall configuration and a schema. Is it possible to have an external type defined for a Dhall file?
Yes. For example $ cat schema.dhall { foo : Integer, bar : Text } $ dhall-to-json &lt;&lt;&lt; '{ foo = 1, bar = "ABC"} : ./schema.dhall' {"foo":1,"bar":"ABC"} Key things to note: * A schema in Dhall is a type annotation * A type annotation is an expression * Any expression in Dhall can be imported from another file (or URL) So you can import a schema from an external file
I blame hackage for not being extensible.
I'll second keter. I had a project with a one command deploy through keter on digital Ocean and it was great 
Ah cool! Just tried it: ~/temp/dhall-examples $ cat ./type.dhall ∀(x : Text) → List Text ~/temp/dhall-examples $ dhall (λ(x : Text) → [x ++ "!", x ++ "!", x ++ "!"] : List Text) : ./type.dhall ∀(x : Text) → List Text λ(x : Text) → [ x ++ "!", x ++ "!", x ++ "!" ] So if there was a way to build a Type &lt;~&gt; Schema partial bijection then I could do what I'm looking for! I'll have a play around.
Heroku with this backpack: https://github.com/mfine/heroku-buildpack-stack Every push gets deployed.
I see. In that case, I would imagine that it would not be necessary to have different sets of primitives, but that one would rather emit different instructions for each of the primitives on different architectures. My compiler-fu is very weak though, so I may well be wrong.
Nice! Tell us how it goes if you can :)
Afaik that is what ghc does.
Another fun one is that you can LZ78 compress an input string into a token format that you can decompress directly into a monoid. This gets a ton of sharing on the intermediate parts of the sequence as a Foldable if there are any repeated subsequences. http://hackage.haskell.org/package/compressed-3.10/docs/Data-Compressed-LZ78.html#t:LZ78 has a Foldable container that uses this fact. The Monad/Applicative for it do not yield an "optimal" encoding, just a legal use of an LZ78 encoding scheme though. =/
Good to know! But where is this feature documented? I can't find it in the stack manual under Docker integration: https://docs.haskellstack.org/en/stable/docker_integration/
It's in the first paragraph on the very page that you've linked: https://i.imgur.com/upgFTc3.png
I am honestly not sure about z3, I only ever use presburger arithmetic. Thanks for the pointers regarding the technique you adopt in your library.
&gt; I would actually say the library ecosystem and tooling in Haskell are a lot more fleshed out than the half-baked things JS has to offer. Ah yes all these state of the art IDE's with great refactoring support and the excellent debugging tools. If only they where real. Haskell tooling is decent for people enjoying the vimacs + grep workflow but isn't good past that. (Although it improved a lot recently). Saying it's **a lot more** fleshed out compared to what you can get for Java/C/JS/Python just seems like wishful thinking to me. And while libs might be better they have to exist first. Which is still less likely in Haskell if one picks the problem before the language. I do think Haskell is a better language. And there are great initiatives happening to improve tooling. But it isn't quite there yet.
I use vim. I installed [hoogle](https://hackage.haskell.org/package/hoogle) and [vim-hoogle](https://github.com/Twinside/vim-hoogle). I also use [hasktags](https://hackage.haskell.org/package/hasktags) for searching the current project. Basically, if some function isn't in scope, I look it up in hoogle and add that module to my project. 
I wouldn't consider the tooling to be more fleshed out than Java's. I wouldn't consider the libraries to be better documented than Python's either. I would say C, despite its age, does suffer from relatively poor tooling in some areas, e.g. there is no package manager, API search tool or similar tooling for it. There are, however, quite a few tools on specific Linux distros to work around its shortcomings (e.g. debhelper makes things significantly easier). To be fair though, it had the disadvantage of an early birth and other languages could benefit from its mistakes. Javascript overall has incredibly poor tooling considering its popularity. This seems to be largely because the community prefers reinventing the wheel instead of focussing all that effort on one or two implementations that actually work well and are more feature complete. As for "state of the art IDEs", at least on Java those tend to be more to work around deficiencies in the language (e.g. Java's extreme verbosity and lack of expressiveness in case of the refactoring tools). Haskell does have debugging tools that do work quite well, though I suppose not using step debuggers all the time does take some getting used to. In summary, the Haskell tooling and library ecosystem are not perfect, far from it. Some languages do a little better in some areas, some do a lot better in others. However I would say that Javascript in particular (the only language I mentioned in my post, quite deliberately) is worse in just about every way when it comes to tooling or libraries.
Can someone provide an explanation of how this recursion works? The double call to subset in the last line is confusing me. A visual representation would be great too! :) subset::[a] -&gt;[[a]] subset [] = [[]] subset (x:xs) = subset xs ++ map (x:) (subset xs) 
You might want to look at [`fast-tags`](http://hackage.haskell.org/package/fast-tags). It is a hell of a lot faster than `hasktags`. (To the point where I don't have a problem letting it run automatically in my vim settings, as I just don't notice it even when it has to run on some huge codebases.)
Thanks! This looks great. 
I feel like once I get to the llvm backend I'll choose different primitives, but maybe I'm wrong. Maybe I'll find I have less then.
On build machine(s) stack image container docker push &lt;container&gt; On server: */5 * * * * docker deploy mystack.yml
docker is "native" on Mac now, so no need to use virtualbox.
Well, let's consider each subset `ys` of `x:xs`. - Either `x` is not in `ys`, so `ys` must be a subset of `xs`. - Or `x` is in `ys`. If we disregard order, then there must be some subset `ys'` of `xs` so that `x:ys = ys'` Do you agree with that? So we can express this in code by saying: subset (x:xs) = -- consider each subset ys of x:xs. -- Either x is not in ys, so ys must be a subset of xs [ ys | ys &lt;- subset xs ] -- Or ++ -- x is in ys. If we disregard order, then there must be some subset ys' of xs so that x:ys = ys' [ ys | ys' &lt;- subset xs, let ys = x:ys' ] Follow so far? Then we can simplify each list comprehension to be the code above. 
They are talking about using a VM instead of docker so that it would be the same environment as production. It's possible, but I would still go for docker. In a VM setting, a lot of things can go wrong/get out of sync.
Is the `a` type parameter/`Embed` constructor what I should use the extend dhall? It's kind of unclear to me from the types. Also, is there a better venue for me to ask these questions? A dhall mailing list/chat room?
To be fair, that's true of other methods mentioned in this thread, too. Docker, for example, relies on both systems being Linux (and only works on OS X by running in a virtual machine)
I remember having issues with static linking of the GMP libraries, both technically and with regards to licencing. Docker is more robust, especially when you don't own the hardware.
Oh yeah, I forgot about this. The answer is that you can use the `Embed` constructor to extend Dhall, but only if you forgo the import system You can extend the parser to parse new things in the `Embed` constructor using: https://hackage.haskell.org/package/dhall-1.8.0/docs/Dhall-Parser.html#v:exprA ... so if you have a parser for your additional built-ins: example :: Parser MyBuiltIns ... then you can parse an expression with builtins but without imports using: exprA example :: Parser (Expr Src MyBuiltins) You can always ask questions about how to use the Haskell API on the issue tracker here: https://github.com/dhall-lang/dhall-haskell/issues
Honestly, reading ghc is the best way to demotivate me on my language journey. Everything is so complicated in there. Just to figure out how variables are represented requires looking through half a dozen files.
I just followed the Archwiki and installed the static package. Following the instructions for statically linked binaries got me up and running without the headaches I experienced with the dynamically installed versions.
I've used this method myself without issues, but I'm sure there are some variables involved. IIRC, static linking GMP is only a problem if you are distributing the binary itself. In the case of a server this is not an issue. However, last I knew, stack actually supports using `integer-simple` builds right out-of-the-box, so that's an option if needed.
Yeah, keter feels to me like little, useful gem forgotten in all that docker rage...
It is as evincarofautumn and jberryman have noted. The language is not technically Turing complete. That is, it will not be able to loop forever in the mathematical sense. It can, however, easily by made to loop for longer than the life of the computer, our lives, and the life of our sun, which it forever in the practical sense. Hence why I called it effectively Turing complete. It more detail, the total number of iterations it can do is set by how many nested macro definitions you are willing to create. The lines of definitions grow at a rate of c*M*N while the number of iterations grows at a rate of M^N, where c=6, M=4, and N is the number of nesting levels. The code right now is nested to a level of N=8. This gives 65536 iterations and takes [192 lines of macro definitions](https://github.com/twhitehead/c99-meta/blob/master/Recurse.h). Change this to N=32 would take 768 lines of macro definitions and give 18446744073709551616 iterations. I haven't needed more than 65536 so far though, so I haven't upped this yet.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [twhitehead/c99-meta/.../**Recurse.h** (master → 8e68a54)](https://github.com/twhitehead/c99-meta/blob/8e68a545d515ca3afc5bbbc906fc64162244b4a0/Recurse.h) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dq1cv46.)^.
By they way, I have also now [added another example](https://github.com/twhitehead/c99-meta/blob/master/Tests/Goldbach.c) that generate the sequence of Goldbach's numbers (infinite list of paired primes where each pair sums to the next even number) as you suggested. It has the original Haskell code that I wrote to do it in comments for comparison, as well as sample input and output for testing and understanding each function. The biggest pain I found in doing this was ensuring I didn't get bit by the laziness duplicating computation or building needless thunks. Interestingly enough, both gcc and clang suck back order GBs of memory if you get it to compute very many of the even-number-prime-sums. This is despite the fact that dumping the string that their pre-processor is manipulating at each step (effectively the programs stack) shows it to be of order KBs. The Intel compiler, by comparison maintains a very modest and stable memory usage on the order of 100MB throughout the computation. I guess gcc and clang are not particularly well optimized at the moment for this sort of pre-processor abuse.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [twhitehead/c99-meta/.../**Goldbach.c** (master → 8e68a54)](https://github.com/twhitehead/c99-meta/blob/8e68a545d515ca3afc5bbbc906fc64162244b4a0/Tests/Goldbach.c) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dq1cyh9.)^.
Thank you, any suggestion?
I hadn't. Thanks for the link. The next step I was going to do, back when I was working on this, was to turn it into a mini-language in it's own right (i.e., add a tokenizer and compiler to give more syntactic sugar and upfront error checking). Very cool to see something similar has already been done.
&gt; Use statically linked library This is easier said than done, I haven’t been able to build a static binary for an app of ours which uses Postgres’ libpq because most distros don’t provide static libraries. 
&gt; Honestly, reading ghc is the best way to demotivate me on my language journey. Everything is so complicated in there. Just to figure out how variables are represented requires looking through half a dozen files. It wasn't meant as "look at how GHC does it" but rather a confirmation that the approach mentioned by /u/limperg is sound. While at times confusing I found the GHC codebase pretty readable for what it does. But what it does adds enough complexity to make it hard to process if you only look for inspiration for implementing something simple. Something like [Implementing functional languages: a tutorial](https://www.microsoft.com/en-us/research/publication/implementing-functional-languages-a-tutorial/) might be better suited for that.
Ooops ^_^ Thanks
They are very useful for complicated data structures. Particularly when you really want the type system to verify correctness for you. For example a data structure I have recently needed is a list of two alternating types where both the type of the first element and the type of the last element are known at compile time. Now you can do it unsafely with `[Either TypeA TypeB]`. But to verify it at compile time you can do: data AltList a b c where (:+:) :: (b ~ c) =&gt; a -&gt; b -&gt; AltList a b c (:+) :: a -&gt; AltList b a c -&gt; AltList a b c And you can't really do that without GADTs. I'd say it's good to be familiar with them and see a few examples of them so that when you do need them you realize it quickly and are able to implement it quickly. However most of the time you probably don't need them, as either ADTs are sufficient for correctness or correctness can be handled at the value level such as `Data.Set`.
&gt; In summary, the Haskell tooling and library ecosystem are not perfect, far from it. Some languages do a little better in some areas, some do a lot better in others. However I would say that Javascript in particular (the only language I mentioned in my post, quite deliberately) is worse in just about every way when it comes to tooling or libraries. Full disclosure I haven't worked much in JS. But even so the first IDE I picked up when I had to use it supported go to definition, find usages and refactoring. Debugging was also a complete non-issue. There are also packages to do anything and then some. There is still a lot of madness that is just accepted in JS. But it's far from "worse in just about every way" when not talking about the language itself. Reliable code completion, refactoring, go to definition and renaming are not required. But I think the advantages of having them are downplayed too much. Similar for debugging. Sure GHCi is workable however so is printf debugging. And compared to the IntelliJ or VS debuggers it's pretty bad. But [recent](https://hackage.haskell.org/package/phoityne-vscode) [initiatives](https://github.com/haskell/haskell-ide-engine) [hopefully](https://github.com/commercialhaskell/intero) have already improved the situation and hopefully close the gap some more in the future. 
Thinking out loud: Can dhall be the next .cabal ?
Oh, I didn't mean to imply that's what you meant. I was already doing it to try to figure out what ghc was doing. And it is the biggest problem I have with ghc, partially because the code doesn't have a proper heirarchy. The languages in TAPL are all strict and the language I want to make is strict (or at least close enough that lazy by default doesn't make much sense.)
Another difference is that mmark has a BSD3-style license, whereas pandoc is GPL.
I'm a Haskell beginner myself, so I explain this to myself using equational reasoning. Using that kind of reasoning makes the recursion much easier to understand, at least for myself. I figured this was visual enough. The last time I tried a graph for this, a man disappeared in a black hole in Devon. -- case of 0 elements subset [] = [[]] -- case of 1 element subset [a] = subset (a:[]) = subset [] ++ map (a:) (subset []) = [[]] ++ map (a:) [[]] = [[]] ++ [[a]] = [[], [a]] -- case of 2 elements and further number of elements subset [b, a] = subset (b:a:[]) = subset [a] ++ map (b:) (subset a) = [[], [a]] ++ map (b:) [[], [a]] -- substituting `subset [a]` with result from case of 1 element = [[], [a]] ++ [[b], [b, a]] = [[], [a], [b], [b, a]]
logict is really wonderful!
Is there an extension which allows type classes to have parameters? I'd like to create a family of type classes parameterized by an integer - for example, the family of vector type classes parameterized by the dimension of the vector.
&gt; We could venture into the land of linear types, a feature recently proposed to be added to GHC. With linear types, we could ensure state values are used exactly once, making our current approach safer. I have a question about this hypothetical use of linear types. In this example, they would be used to enforce a protocol, and not to ensure the release of low-level resources like file handles. Would it be correct to say that this use would not require changes in the underlying monad? Apart from moving to continuation-passing style, perhaps.
As far as I've understood the proposal, there would not have to be any change to the monad, no. It would be all in the type class methods' type signatures, i.e. the events in the encoding used. But I'm not that well read up on linear types in general, and not on the specific proposal, so I should try to give you a definitive answer. Maybe someone can chime in. :)
This is really nice. How does it integrate with the standard gRPC workflow? To paraphrase the [gRPC site](https://grpc.io/docs/guides/), one of the fundamental ideas of gRPC is that the basic workflow is the same in all supported programming languages: You define the service in the [proto3 language](https://grpc.io/docs/guides/), a DSL for describing protobuf protocols. Then you run a vanilla proto3 compiler on that to generate a code stub automatically in your favorite programming language either for a server or for a client. Could your approach scale up to a full proto3 implementation for Haskell, with gRPC as a special case, like in other languages?
Hello fellow Haskellers! I have a situation where I share data types between two projects, and in one of them I would like to write some instances for a type class for the data. Is it bad practice to write instances for imported data? Should I wrap the data in a newtype? Cheers! 
Too bad it's one of those libraries in the ecosystem that comes with 0 examples and 0 explanations. 
it’s defined in a lovely paper with good examples and explanation of how it’s derived, though! http://okmij.org/ftp/papers/LogicT.pdf helped me quite a bit
but yeah i hear you! maybe some docs are in order. i think it’s likely bc not many have used it. 
Just curious, how do you make sure it builds like that? As far as I can see, you don't pin the Haskell packages (i.e, you just use the ones from nixpkgs), and in my experience IHaskell is a bit picky when it comes to its dependencies. I find that the only way to build IHaskell is to use a stack snapshot. BTW, thanks a lot for your maintenance efforts!
Whlie this is a great intro to Repa, it's worth noting that the matrix-multiplication approach taken -- multiply every row by every column -- exhibits terrible memory locality, and so performs _really_ slowly for anything but a toy example. In numerics memory bandwidth is often _at least_ as important as compute bandwidth, maybe more so. In the case of matrix multiplications, the [tiling algorithm](https://en.wikipedia.org/wiki/Matrix_multiplication_algorithm#Cache_behavior) is the recommended approach to minimising cache misses.
If it is part of internal modules, and you don't export some modules without the instance and some with the instance, I think that's fine. Otherwise newtypes
See DataKinds and GHC.TypeLits 
A minor nitpick (since I know this was just an example) but assuming the layers will always go linearly up is an invalid model for parsing network packets (though will usually work).
Hey, anyone knows how to tell cabal to hide the transitive dependencies of the `ghc` package while building a project? `--ghc-options="-hide-package=containers"` doesn't seem to work. Also [this](https://stackoverflow.com/questions/47353751/cabal-install-refuses-to-solve-conflicting-ghc-and-containers-versions).
That state machine transition can be done with a monad with backtracking and continuations: getProducts &gt;&gt; cartSelect &gt;&gt; checkout The two verbs necessary are a pair of primitives `backpoint` and `back` back will go back to the previous `onBack` point. This monad would implement the state transitions. here I present a very similar example using such monad: (uses `onUndo` and `undo`) https://www.schoolofhaskell.com/user/agocorona/the-hardworking-programmer-ii-practical-backtracking-to-undo-actions#backtracking The advantage of using a monad is is that such the combination of these three elements is an element that is first, class . it is an expression that composes: it can be inserted in bigger expressions.
No, this isn't possible. The reason is that List Comprehensions are just syntactic sugar is computations in the List Monad. The List monad sequences operations together using concatMap, which has no awareness of the "list so far". It just maps a list producing operation across a list, and flattens the results into a single list.
There's a [middleware to do exactly that](https://hackage.haskell.org/package/wai-middleware-throttle).
Here's a workflow I like to use when developing servers. I use four shells: one for editing the code, one for building the executable and displaying build errors (using `stack install --file-watch`), one for displaying the server's output, and one for reloading and running tests. Interestingly, the server isn't running in the shell which displays its output, instead I use `mkfifo` to create a fifo file and I `tail -f` its contents to stdout. The server runs in the reloading shell, in which I use [`fswatcher`](https://hackage.haskell.org/package/fswatcher) to run a reloading and testing script. So whenever I edit the code, `stack install --file-watch` produces a new executable, which triggers `fswatcer`, which runs the script. The script kills the server, spins a new one whose output is redirected to the fifo, and runs the tests against it, displaying the result in this fourth shell.
To expand on this a little, the version with `x &lt;= y` will be desugared something like this: concatMap (\x -&gt; concatMap (\y -&gt; if x &lt;= y then [(x,y)] else []) [1..6]) [1..6]
You can use a program called [`ghcid`](https://github.com/ndmitchell/ghcid) to watch your program's source code and reload GHCi on save. `ghcid` by default will only compile your code and check for warnings/errors, but you can also have it run a custom function. This is mostly done to run test suites, but there's no reason you couldn't have it restart a web app as well. I haven't verified it, but [this appears to be an example](https://github.com/khanage/servant-workshop/blob/master/src/Bloggo.hs) from [this project](https://github.com/khanage/servant-workshop) on getting code reloading on save. You can use something like [`rapid`](https://hackage.haskell.org/package/rapid) to save state in your application between GHCi reloads if you need to.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [khanage/servant-workshop/.../**Bloggo.hs** (master → 762b6a9)](https://github.com/khanage/servant-workshop/blob/762b6a9ea19c089012fa3099927e06d9767a28fb/src/Bloggo.hs) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dq1y647.)^.
I use ghcid [0] for this, and if I have a multi package project I use it with stack like so: ghcid -c stack ghci package1 package2 -W -T main Then when any files in package1 or 2 change, ghci is reloaded and main runs when there are no errors. [0] https://github.com/ndmitchell/ghcid
That's an interesting point, I need to think about that. :) By the way, are you the creator of MFlow? I've been wanting to try writing an instance for a state machine protocol like this using MFlow, for a web application. Do you think that is feasible?
This doesn't look like the untyped lambda calculus at all. It looks more like the simply typed lambda calculus. Because your step function doesn't always work - it fails if the expression is not well-typed. Perhaps you would prefer to write `Core` as a GADT with a type parameter.
You have re-invented the wheel called [yesod-bin](https://hackage.haskell.org/package/yesod-bin).
Does https://snapframework.com tell you everything you would need to know about using the snap framework? The site might be down right now (unless it doesn't like VPN's), but if I remember correctly there weren't many pages of documentation, and Hackage doesn't seem like a very user friendly place to learn from. I've thought about reading the entire Yesod book simply because it's an entire book of creating something specific with Haskell, unlike the more general purpose books on this language that are out there. But Snap seems like it *could* be better, it's still a little early to tell. 
Awesome, I'll definitely use this technique from now on!
That middleware is good for providing a good user experience when legitimate traffic is more than you can handle. But it won't help much for a DoS attack. For that you need to throttle upstream at the router or the load balancer, or even at the DNS level.
For the sake of efficiency, what you really want to do is [(x, y) | x &lt;- [1..6], y &lt;- [1..x]] -- y &lt;= x or [(x, y) | x &lt;- [1..6], y &lt;- [x..6]] -- x &lt;= y and both of these are perfectly simple and legal.
I don't known but I guess that monadic bind require (at least) one in memory thread context and probably some connection context (eg. websockets). The problem is you should maintain that resources for a long amount of time (minutes) t1. The stateless (old fashion way) version only require that resources for specific transactions (milliseconds or seconds) t2. The later can handle t1/t2 more users than the former. I'm wrong or exists some workaround for that? Anyway, the monadic way is still great for a bunch of use cases.
I am aware of them, however my understanding is that they would allow me to create a vector data type for each natural number. While this is close, I would like to create a type class for each natural number. Am I wrong in understanding what they allow me to do? Cheers!
Yes. MFlow was created for that purpose in web applications. If the user press the back button and execute a post request, the navigation expression backtrack until a statement match with that post.
Not necessarily. If we talk about web applications, the thread can wait for a time for request but after a timeout it can store his identifier and status and die (or die immediately, depending on the configuration). When the application receives the next request, it can recover the state. I did it in MFlow. The same happens in a state machine. 
I usually use ghcid when testing programs which terminate, but if I tell ghcid that my test function is my webserver's main function, this test will never terminate and ghcid won't kill it when the code changes. That's why I use a [more complicated setup](https://www.reddit.com/r/haskell/comments/7e24nx/code_reloading/dq1xwwt/) when testing non-terminating applications such as webservers. However, I just discovered a simple fix: the test function can simply fork a thread, and ghcid will leave it running in the background! Now, there are two complications: ghcid won't show you the output of that thread, so you still need something like my fifo trick, and also there's no easy way for the test function to kill the thread forked by the previous invocation, so you need that thread to co-operate and die on its own somehow. In the example you linked, it seems like the forked thread watches for a special file and dies once that file is created, that would work.
The differences between two docker environments, one on Mac and one on Linux are tiny, if any. They are both linux environments.
It depends. If you move the frontend to 10Gbps spot instances on AWS you get 5Gpbs external bandwith. Alternatively OVH provides packet filtering. Then there's cloudflare. You'd probably still want to configure rate-limiting in Warp.
It *could* be desugared that way, but that's not how GHC desugars it. GHC actually has two different systems for desugaring list comprehensions: one for when optimizations are enabled and one for when they're not. Neither of these systems uses `concatMap` or `Monad` methods.
Great! Thats was my train of thought as well. Thanks! 
Who has tried froid and what's been your experience?
You're my hero
Multiparamtypeclasses will work for that. You can have the first parameter be the number and the second be the actual type you want to be a member of said typeclass.
Why not write the instances in the package that defines the data types?
Sorry but that's not true: foo = [succ . sum $ take x foo | x &lt;- [0 ..]]
Heh, that's interesting. So your server get restarted every 5 seconds? 
There is a piece of the puzzle missing. Looking at the type of `selectCard :: State m NoCard -&gt; Card -&gt; m (State m CardSelected)`, it always returns something with a state of `CardSelected`. This fails to account for the possibility that `selectCard` failed, eg if the user typed an invalid card number. The fix is to use a different variety of monad to account for your dynamically-determined knowledge about the state. Details in [_Kleisli Arrows of Outrageous Fortune_](https://personal.cis.strath.ac.uk/conor.mcbride/Kleisli.pdf).
*NIX. Mac OS is much more BSD.
A Discord bot which performs network RPC on demand and on schedules, and has a small web service bundled. The multiple moving parts gave me experience with managing monad transformer stacks and concurrency. The code is not published, as it contains behaviours I don't want to share.
See: https://github.com/dhall-lang/dhall-haskell/issues/78
/r/haskell_caveman I'm back here to say that I'm week 3 of CIS194 and I just finished building a small sokoban implementation with Haskell. It's wonderful how much I learned and enjoyed studying with this. Thanks again! :)
It is actually the untyped lambda calculus. Malformed terms don't error, they just can't be reduced. This is more significant due to the extension of integers, but is no less true in the true untyped lambda calculus.
That's awesome, thanks for the note! Highly recommend the book mentioned in the post (by the author): http://haskellbook.com/
Please keep in mind that this middleware needs a bit of work before it can safely be used on high traffic production sites: The representation of the retry state is quite inefficient and due to the read and write in the STM var being split in two transaction you will loose writes. Also the internal state is never cleaned up.
Great point. Looks like it might benefit from `stm-containers`.
Hm, in theory yes, but I think when you reach that point that you actually need rate limiting you are probably already running more than one frontend instance thus the rate limiting state must be distributable and you probably want to store the information in something like redis. 
I'll definitely do it!
Ahh, that explains it :) Thanks!
IIRC correctly there was a a package called `rapid` to solve this exact problem. I tried but couldn't get it to work. YMMV. 
&gt; globally import functions from other modules Does that refer to the inline optimization (/ pragma) providing a function definition with its signature in the interface files? 
Very cool. How long did a recent full build take? 
Very cool. How long did a recent full build, say on a laptop, take. 
Does it work for multiple Imports and for the language pragma? I hate ` :set -X...`ing everything. (I'll try it when I get home)
I think you can implement filter by yourself. 
&gt; behaviours I don't want to share. I have no idea what you mean by that, but ok. Sounds neat.
I mean the code speaks a proprietary protocol that isn't mine to detail, so it would not be appropriate to release the code, that's all.
The readme for yesod-bin mentions that stack obsoletes a lot of yesod-bin's functionality, not to mention works with any Haskell server stack, not just yesod.
Chicken or the egg?
Hm... I think I see part of my issue. First, the function `nonEmpty` has a bad implementation. The success branch should be `Success input`, rather than `Success "Valid integer"`, because whatever the tagged value is will be passed as the input to the next validation function. (So essentially, I am passing `"Valid Integer"` to the `integer` function when at the moment, when I shouldn't be). However, even after correcting that error, the behavior of `integer` function has not changed. It is still reducing to the `Error` branch. 
Sorry for the delayed response! Ahh so I actually don't depend on Alpine's package manager but instead basically depend on stack to do the heavy lifting -- in almost all cases I do the `stack setup` and it downloads a local 8.0.2 for me to use. I'm actually successfully running the container in production and was able to get past the issue -- making sure the binary was running in the same OS as it was built in as indeed the fix. Have you tried to find out what's causing the segfault? As hacky as it is, you could `docker exec`/`docker attach` in, and use `gdb` (or any other better haskell-specific debugging tools) to figure out what's wrong?
It requires no language pragmas. What do you mean by "does it work for multiple imports"?
afaik, you can't paste pragmas or multiple import lines into a ghci prompt, even with :{
I do pin nixpkgs when I'm using IHaskell [as part of a project](https://github.com/vaibhavsagar/experiments/blob/0e2e103682c229e97a5be8ac2f39c6a7cda9f8dc/git-from-scratch/default.nix), but I also run [a Hydra instance](http://128.199.234.106:3000/jobset/ihaskell/master) that confirms that IHaskell builds with 17.09 nixpkgs as it gets updated.
Didn't know until you asked. It works: jeff@jbb-lenovo:~/code/readHsAsGhciCmd$ cat test.hs x = [1 ,2,3,4] :set -XViewPatterns jeff@jbb-lenovo:~/code/readHsAsGhciCmd$ stack ghci readHsAsGhciCmd-0.1.0.0: initial-build-steps (lib) Configuring GHCi with the following packages: readHsAsGhciCmd GHCi, version 8.2.1: http://www.haskell.org/ghc/ :? for help [1 of 1] Compiling ReadHsAsGhciCmd ( /home/jeff/code/readHsAsGhciCmd/src/ReadHsAsGhciCmd.hs, interpreted ) Ok, 1 module loaded. Loaded GHCi configuration from /tmp/ghci16269/ghci-script *ReadHsAsGhciCmd ReadHsAsGhciCmd&gt; let f (even -&gt; True) = 3 &lt;interactive&gt;:1:8: error: Illegal view pattern: even -&gt; True Use ViewPatterns to enable view patterns *ReadHsAsGhciCmd ReadHsAsGhciCmd&gt; :cmd ReadHsAsGhciCmd.main "test.hs" *ReadHsAsGhciCmd ReadHsAsGhciCmd&gt; let f (even -&gt; True) = 3 *ReadHsAsGhciCmd ReadHsAsGhciCmd&gt; f 2 3 *ReadHsAsGhciCmd ReadHsAsGhciCmd&gt; :q Leaving GHCi. 
I started [a little documentation](https://github.com/JeffreyBenjaminBrown/learn-logict) for logict.
The compilation speed is about 1,000 lines/s, just type checking about 10,000 lines/s. 
I see, thanks for the answer. My concern was that if I had an old ihaskell commit I wanted to compile, I wouldn't know which nixpkgs version to use, but then I noticed that ihaskell is in nixpkgs itself, so it's versioned there. Wouldn't it be better advice then to tell people to get their ihaskell running purely from nixpkgs?
Yes, you are right. The puzzle is not fully laid out, though. I decided to cut error handling out from this post to keep down the amount of new concepts and moving parts. The approach I've used is very simple, returning something like `Either CardError (State m CardSelected)`. I forgot to add a note about that in the post, which I'll do today. Not sure if I should write a post on error handling before or after moving on to indexed monads. Perhaps best to tackle error handling first. Anyway, thanks for the link to the McBride paper.
thanks for the info! 
Hmm... How about combining this with `-XApplicativeDo`? 
Well I want to be super clear where computational dependency is happening (i.e., queries cannot be aggregated) and where there is no computational dependency (we can benefit from aggregation), so I want to be explicit in distinguishing Monad and Applicative.
I wouldn't say it's for enforcing that it exists. The primary purpose, and main usefulness of the constraint - is that a `Traversable` constraint carries with it a `Foldable` constraint. So you don't need to require both constraints: (Foldable f, Traversable f) =&gt; ... Can become: Traversable f =&gt; ... Same is true for `Functor` which can be derived from `Traversable` as well.
It’s been in my articles repo for ages, made when it was fashionable to discuss Javascript equality. Turns out equality (of standard types) is exactly as boring and easy as it ought to be.
How does this compare to haxl, or purescript-fetch ?
One thing I don't like about this setup is that you need to have a full env with stack installed. This is not exactly a problem for me (and most of Haskell devs), but when one works with other people (eg. in my case mostly nodejs and python devs), they don't want to deal with Haskell tools on their pc's. A workaround I have tried (based on an article I read here) is creating a setup with 2 docker containers, one of them would contain the Haskell dev environment (stack, ghc, etc) and the other just the things needed at runtime. It is not perfect, but useful until a better solution appears. Using a setup like that avoids other people having to configure a Haskell dev env, all of that would be transparent to them, maybe they will not even realize they have Haskell tools on their pc's :D
Cool visualisation! It would be nice to see a side-by-side comparison to other languages :D
I've never used either, so I may be wrong about this, but I *think* Haxl works by running `&lt;*&gt;` in a parallel IO monad. I don't know anything about Purescript-fetch. The goals are probably similar, but the difference is `mget` is not a bunch of queries executing in parallel, it's actually a distinct builtin command. The goal of the post is to model the construction of such an interesting structural primitive in a type-safe way.
- [Javascript](https://dorey.github.io/JavaScript-Equality-Table/) (There is a `===` button at the top that’s not quite as idiotic as the default `==` view) - [PHP](https://ruempler.eu/2015/01/03/the-php-equality-table/) 
So is this another argument to use Text instead of String ;)
Well that’s basically what I mean by “enforcing it exists” =P This enforcement implies two properties: That knowing the class implies the superclass, and that the relationship between the class and it’s superclass is consistent with the proof that the one is in fact a superclass.
I talked a bit about the relationship between profunctor strategies and Haxl-like strategies [here](https://elvishjerricco.github.io/2017/03/10/profunctors-arrows-and-static-analysis.html). Seems kind of similar.
Yes, your post is wrestling with lots of these same ideas! My target was also the `Traversing` class, although I couldn't find a way to get `FreeTraversing` to give me quite as much freedom as I wanted. It allows you to *traverse* independent queries (obviously), but I couldn't figure out a way to get it to *combine* two arbitrary queries - `aggregatePair :: a ⟿ b -&gt; c ⟿ d -&gt; (a , c) ⟿ (b , d)`. Part of my (hopefully not misplaced) excitement about the structure in my post is that it satisfies `Traversing` as well as allows that `aggregatePair`.
From what I remember haxl would allow you to batch any way you like. I think this is exactly what haxl was designed for (with ApplicativeDo having been designed for haxl, in order to expose implicit data dependencies as Applicative structure). Not that there's anything wrong with exploring other ways of doing this.
Thanks, that's a good example. Not only is it not a syntax tree, but it also uses type equality, which is different from the usual example, which does a phantom type thing by replacing a variable with a concrete type. I don't think I've needed that particular data structure before, but hopefully if I stumble across that occasion I'll remember this! Any other examples of fancy data structures like that? Maybe doing more work in Idris would good training for how to see such opportunities.
The good old PHP pickaxe.
In addition, [reflection](https://hackage.haskell.org/package/reflection) might help with passing values to instances, if that's what you want.
It would make the dependencies graph pretty nasty and probability cyclic in this case 
Haxl (at least the initial versions of it, haven't followed up since) allowed to bundle `get` requests into `mget` as well, not only perform parallel IO. I did exactly this (as a PoC) in https://gist.github.com/NicolasT/705e4a6e518daf2d5ee5 It's definitely less principled than your approach though, I guess.
I wasn't expecting them to be symmetrical tbh.
Me neither, I'm sad to admit they have at least that for them
&gt;I'm sad [Here's a picture/gif of a cat,](http://random.cat/i/dWy2U.jpg) hopefully it'll cheer you up :). ___ I am a bot. use !unsubscribetosadcat for me to ignore you.
[Video of the talk](https://www.youtube.com/watch?v=j0b_8FOPalY)
Should be a 1-dimensional affine space: https://hackage.haskell.org/package/vector-space-0.12/docs/Data-AffineSpace.html
Huh, I guess they're analyzing the query at runtime and allocating the batches then?
This might not be the right place to say it, but I'm not sure why it makes sense for `[] == []` to result in "ambiguous types" but `[] == ""` to not result in ambiguous types. Surely if the first one is unresolved the second one should be unresolved? Likewise it should be able to unify two unbound types? 
&gt; Ahh so I actually don't depend on Alpine's package manager but instead basically depend on stack to do the heavy lifting -- in almost all cases I do the stack setup and it downloads a local 8.0.2 for me to use. This line in your Dockerfile in your post, tells the stack to use system GHC, which you have installed few lines earlier: RUN stack config set system-ghc --global true &gt; Have you tried to find out what's causing the segfault? As hacky as it is, you could docker exec/docker attach in, and use gdb (or any other better haskell-specific debugging tools) to figure out what's wrong? I've used GDB in the meantime and I found out that that I was affected by this [old glibc issue](https://hub.docker.com/r/frolvlad/alpine-glibc/). The solution is to use dynamically linked binary + [alpine-glibc](https://hub.docker.com/r/frolvlad/alpine-glibc/) image + install [`gmp-dev`](https://pkgs.alpinelinux.org/packages?name=gmp-dev&amp;branch=&amp;repo=&amp;arch=&amp;maintainer=) additionally. 
Because `""` is equivalent to `([] :: [Char])`. But if you turn on `OverloadedStrings` you get λ&gt; [] == "" &lt;interactive&gt;:3:4: No instance for (Eq t0) arising from a use of ‘==’ The type variable ‘t0’ is ambiguous Note: there are several potential instances: instance Eq a =&gt; Eq (Maybe a) -- Defined in ‘GHC.Base’ instance Eq Ordering -- Defined in ‘GHC.Classes’ ...plus 24 others In the expression: [] == "" In an equation for ‘it’: it = [] == "" &lt;interactive&gt;:3:7: No instance for (Data.String.IsString [t0]) arising from the literal ‘""’ The type variable ‘t0’ is ambiguous Note: there is a potential instance available: instance Data.String.IsString [Char] -- Defined in ‘Data.String’ In the second argument of ‘(==)’, namely ‘""’ In the expression: [] == "" In an equation for ‘it’: it = [] == "" 
`[]` has type `[ a ]`, where `a` must be solved through unification. `""` has type `[ Char ]`. In the second equality, the concrete type of the RHS unifies the `a` in the LHS to `Char`.
It also appears that `[] == []` does work λ&gt; [] == "" &lt;interactive&gt;:2:4: Warning: Defaulting the following constraint(s) to type ‘()’ (Eq t0) arising from a use of ‘==’ In the expression: [] == [] In an equation for ‘it’: it = [] == [] &lt;interactive&gt;:2:4: Warning: Defaulting the following constraint(s) to type ‘()’ (Eq t0) arising from a use of ‘==’ In the expression: [] == [] In an equation for ‘it’: it = [] == [] True 
`""` is the empty string, not the generic empty list. So `[] == ""` has to unify `[a]` with `[Char]`, which is unambiguous.
…in GHCi, with its extended default rules.
this post https://ro-che.info/articles/2013-01-08-torsors ?
That function is fine *Main Data.String&gt; integer (fromString "55") Success 55 *Main Data.String&gt; integer (fromString "xyz") Error "Unable to parse int" The problem must be somewhere else.
You back-end can, yes. But indeed it's a runtime thing.
An additional strategy could be to end the thread and store the continuation and the backpoints, so that when a new request arrives, the continuation that matches the request continue execution. These pointers to continuations can be freed after a timeout. then the former mechanism (serialized state) can be used. 
The warnings also tell you why it works in that particular instance. You can make GHC compile code like that, but it doesn't do it by default.
Statically linked executables are great, except when you have a dependency on `ghc` or `libdl`.
I'm wondering what Haskell would be like if floats were not instances of `Eq` and `Ord`. They arguably break the (implied?) laws of these type classes. Maybe the laws just don't matter much in practice.
Well, floats are certainly ordered, and ordering implies equality. The problem with floats is not that equality is broken, but that people expect floats to represent real numbers, and not a very technical approximation to them.
NaN is exactly the problem here - it means the usual laws of ordering only hold for a subset of the values of the type. 
Also, in Haskell we won't have such StackOverflow questions: https://stackoverflow.com/questions/359494/which-equals-operator-vs-should-be-used-in-javascript-comparisons Questions on Haskell's StackOverflow are more like this one... https://stackoverflow.com/questions/45829110/monad-laws-expressed-in-terms-of-join-instead-of-bind/45829556 (well, actually not, but still a good joke)
Right - but my point is, why does it fail to unify [] with []? Surely if both are _actually_ ambiguous types, then unifying the two types won't cause any issues further down the line?
Sorry for tl;dr, but Joy or Frustration?
As with everything in life, it depends :) Lots of initial frustration, turning into joy as our code-base grows.
`[]` unifies with `[]`. But to compare lists, you have to be able to compare their contents – in this case, the lists both contain `a`, and GHC cannot figure out which `Eq a` to choose from. Sure, the `a` will never be used in the computation, but that’s not known at the type level.
Right, morally `[] == []` has type `Eq a =&gt; Bool` and there's no way of knowing what `a` to choose or that it doesn't matter!
How about in the package which defines the typeclass?
Ah, thank you! This got to the root of the issue for me.
As far as I know you can't do this. I remember a proposal for adding something like this, but I can't find a link to it now. The best you can do is hide the dependency behind a flag. 
By "not quite as idiotic" do you mean quite reasonable? All I see is a straight line in the middle.
It wasn't this post, but yes, it was about torsors! At least now I have a word for what I'm looking for!
That is how it works right now, but with newtype wrappers. Better safe than sorry 
&gt; This line in your Dockerfile in your post, tells the stack to use system GHC, which you have installed few lines earlier: Ahh apologies, I was actually thinking of a completely different post I recently wrote on how I do Continuous Delivery with this setup, what you noted is absolutely correct, I'm relying on the GHC installed by `apk install`. &gt; I've used GDB in the meantime and I found out that that I was affected by this old glibc issue. The solution is to use dynamically linked binary + alpine-glibc image + install gmp-dev additionally. Interesting, thanks for sharing -- hopefully someone who needs this finds it. I'm a little curious though, alpine uses musl libc from what I understand, was there some reason you have to use glibc?
Do you mean [this](https://www.reddit.com/r/haskell/comments/4pf86b/cabal_build_variants/)?
I am using stack, could you please mention any article about the subject if there is any?
When I recently read the ExceptT docs (after EitherT was deprecated) my thoughts were exactly "this usage of the term 'exception' could be rather confusing". (And just like Either () ~ Maybe, MaybeT ~ EitherT (). Both allow the same control-flow-operation one might call "abort". That they now officially are MaybeT and ExceptT is just unnecessarily confusing in itself..)
I just looked this up and, apparently, there is a totalOrdering predicate standardized in IEEE 754 that could be used which orders everything (it seems to be separate from the normal comparison operations, though)
per day!
You are looking at the `===` one. People are talking about the `==` one. It may seem unfair to kick at the `==` operator, because you can just use the other one. But many of the underlining assumptions are everywhere on he language, and you can't escape them in other contexts.
You may be misunderstanding that I was explicitly referring to `===`, by replying to: &gt; There is a === button at the top that’s **not quite as idiotic** as the default == view (emphasis mine) And I was pointing out that to me the chart for `===` doesn't seem idiotic whatsoever.
&gt; I'm wondering what Haskell would be like if floats were not instances of Eq and Ord. Unusable. The rules around nan have good reasons to be there. They are very relevant, they just disagree with the mainstream laws.
Done - https://github.com/haskell/mtl/pull/49
It claims that `[] === []` is false.
I think the laws still hold morally if you consider NaN to a kind of hardware-level bottom. `undefined` is neither larger nor smaller than `undefined`, because even asking that question means you've done something wrong.
Most of the slides resonated with me. I however feel you are unfair in the "lack of best practices" slide. One of the things that rocks in Haskell is the constant reuse of shared abstractions, such as `Monad`, `Monoid` or even `Num`. I feel that I can use existing libraries very quickly thanks to that (once I found one that is good, which is indeed a problem).
Yes, thanks! Specifically u/acow's `hpack` PR linked in that thread: https://github.com/sol/hpack/pull/112
You literally can't put the bar lower than comparing with JavaScript.
Gabriel explained this to me pretty well [on this issue](https://github.com/dhall-lang/dhall-haskell/issues/62). Scroll down near the bottom.
But you aren't any safer...? If the instances are defined with the class definition that's 100% safe. 
Why oh why was the type not called `EitherT`?
In Haskell best practices are often libraries :)
Because it compares non-primitive values by reference. E.g to avoid infinite loops on recursive structures. `var a = []; a === a` is true. Also even in Haskell, unfortunately, `[] == []` is a type error without `-XExtendedDefaultRules` as others have also pointed out in this thread.
So, "lack of best practices" was not targeted at `Monad` , `Monoid`, etc. but larger architectural patterns, eg. * mtl vs free * how to write domain APIs that can result in "validation errors"? If you start using `Either` how do you compose the Left values? I went down a research-y rabbit-hole in the early days. Finally I gave up and started using `throwM` like nobody's business. * which exceptions package to use? I remember being royally confused between stuff in base, ExceptT and Control.Monad.Catch * how to fork from deep within an mtl stack? (/u/snoyman has spoken about this recently at LambdaConf) Before this I didn't completely understand why most libraries only take `IO a` arguments and not `MonadIO m =&gt; m a` We wasted a lot of time struggling with this, eg. https://github.com/lpsmith/postgresql-simple/issues/9 * How to deal with nested records? After another research-y merry-go-round I gave up, and [tried dealing with nested tuples and got stuck](https://github.com/prowdsponsor/esqueleto/issues/145#issuecomment-237649440). Esqueleto was dead at that time, and /u/tomejaguar helped me with it, which is why I started looking at Opaleye. * How to generally structure a web-app in Haskell? I tried asking around to see some code for a mid-sized Haskell webapp and as pointed to the source code of haskellers.com and was very unimpressed. * How does one deal with auth / permission checks in Haskell? Can the type-system help in enforcing that every dev has implemented a permission check in each API endpoint. I'll stop with the rant now.
Ah one more. A well-accepted way for IO functions to emit structured log data, that all libraries implement. 
Yep, bad misunderstanding from my part.
/u/saurabhnanda It would be awesome if you could write a post explaining the control patterns you settled for.
That's not true at all. You can compare to [PHP](https://i.stack.imgur.com/xI8W7.png)! where strings of numerals are converted to floating point before comparison, and 0 == "true", "true" == TRUE, and 0 != TRUE.
Thanks for your sharing! As a web developer (Django/Rails/Phoenix&amp;Ecto) and Haskell fanboy, I have many concern which use Haskell as web development language, and this slides mentioned many of these. 
&gt; Here's an excerpt from the config I use to do CD where I don't depend on that and start from alpine: Interesting. Somehow I wasn't able to install GHC 8.2.1 in the similar manner. Thanks for posting Dockerfile. I'll try it. &gt; I'm a little curious though, alpine uses musl libc from what I understand, was there some reason you have to use glibc? When using musl, still some files are missing (maybe because I was compiling using glibc), so I did not bother with it. Using glibc was easier here. 
Ping me if you need help with anything. 
&gt; Surely if the first one is unresolved the second one should be unresolved? It has to do with the decision to make a `String` a `[Char]`. I doubt anyone would make that decision again, honestly. 
This is a great idea.
It was. But then people realized Either must be symmetric, and Except is for asymmetry.
What's unsymmetric about `ExceptT`?
That makes me wonder, could Haskell get away with treating NaN as an actual bottom?
I don't have time to invest in this effort at the moment but I would be very glad if performance of rasterific could be improved (I'm using to render UI with Haskus system http://haskus.fr/system/). Porting the benchmark would be a good first step.
I'm kind of amazed that in response to asking the _easiest_ way to deploy, we are suggesting nix, various home-grown deployment systems (scping a binary, starting / restarting with cron, etc), docker with an unspecified server-side setup, when, at this point, Heroku actually just works: 0. Ensure haskell app uses PORT env var to set PORT it's listening to. 1. Create heroku app, set buildpack to https://github.com/mfine/heroku-buildpack-stack 2. Set git remote for heroku 3. Push to that remote. It'll build, deploy, etc. And it caches dependencies between deploys (with a somewhat non-obvious cache eviction policy, but it's not super frequent). No maintaining servers, monitoring, restarting, figuring out nix (and getting it on a server), getting docker running on your VPS, etc. I've done many of those things (and, there are reasons to prefer it for some reasons), but if /u/kwaleko wants to know the _easiest_ way to deploy haskell to production (and they've said they are using stack), it's kind of hard to argue with Heroku.
Is there a way we could say 'for all a. Eq a =&gt; Bool' that would allow the truth value of '[] == []' to exist? I suppose that's what you might mean by saying there's no way of saying a doesn't matter?
Consider `length []`. Morally this has type `forall a. Int` but by parametricity we know it's the same for each value of `a`. I believe GHC plugs in the type `Any` to extract the answer (which is `0`). No such parametricity result holds for typeclass polymorphic values and so we can't apply the same trick.
I think that would introduce a lot of the same problems that we already have from the whole `String`-is-actually-`[Char]` mess. For instance, if you want to read or serialize some standard floating-point format, `NaN` might be a valid value; it would be frustrating to handle an exception from reading well-formed input.
Unification is performed in order to compare [] with "" as equal, so it should be allowed to compare [] with [] and [[]] with [[]].
But what if I absolutely *need* an operator that returns True for `op(0, '')` and `op(0, '0')` but False for `op('', '0')`? That clearly is the correct use case to cater to.
NB: Out of the box this works with no warnings. You seem to have a pile of extensions turned on in a .ghci file somewhere.
Using a current GHC should clear out warnings as the instance for `IsString [a]` has a `a ~Char` constraint as a context. http://hackage.haskell.org/package/base-4.10.0.0/docs/Data-String.html#t:IsString
Right, but IEEE compliant functions would still exist. They just wouldn't lie about following those particular mainstream laws. 
In other words, floating point numbers are still broken.
I’m actually curious about this. There *is* an [`EitherT` in the `either` package](https://hackage.haskell.org/package/either-4.5/docs/Control-Monad-Trans-Either.html#t:EitherT), but nobody really uses it. As far as I can tell, it is *completely* identical to `ExceptT`, modulo the different name. In fact, I believe this means there are now three *identical* transformers in the ecosystem: - `EitherT`, as mentioned above. - `ExceptT`, which is currently the idiomatic choice. - `ErrorT`, which used to require an `Error` constraint, but that constraint has since been removed, making it equivalent to the above two transformers as well. This is a bit of a mess. Some of it is unavoidable in Haskell for historical reasons, but in new languages, we should be able to fix it. PureScript uses `ExceptT`. Currently, [Hackett uses `ErrorT`](http://docs.racket-lang.org/hackett/reference-monad-transformers.html#%28def._%28%28lib._hackett%2Fmonad%2Ferror..rkt%29._~23~25hackett-type~3a.Error.T%29%29). Perhaps it ought to use `EitherT`, instead? The only reason I am conflicted is because we don’t call `ReaderT` `FunctionT`, nor do we call `WriterT` `TupleT`. Generally, we call transformers what they *do*, not what they *are*, with the exception of `MaybeT`… but maybe that’s only because the other names are unclear or ambiguous. Whatever the case, I *do* think that having a class called `MonadEither` would be very strange; *that* probably ought to still be called `MonadError`. But maybe the concrete transformer should be renamed `EitherT` while I can still fix it.
And wouldn't use the `(&gt;)`, `(&lt;)`, and `(==)` operators, and wouldn't fit structures like maps, and wouldn't work with standard functions for sorting. Maybe the Haskell's laws need some revising before throwing floating point arithmetic out of the standard. There must be some common ground we can place on less restrictive classes, and have Ord and Eq only for a subset of their current operations. 
The only one of these you should use (if you can help it) is `ExceptT`. `ErrorT` is deprecated, as is `EitherT`. Edward has provided some good [historical commentary](https://www.reddit.com/r/haskell/comments/3ded39/why_cant_we_have_an_eithert_in_transformers/ct4mnk1/) on how the situation ended up this way. At least it's finally resolved.
I wrote about [something similar](https://www.iguanasuicide.net/node/23) about 5 years ago, when PHP equality tables were being passed around.
Your library is strange in that it doesn't seem to export any module.
I'm not sure `[0] === [0]` and `[1] === [1]` shouldn't be `True`. Those work in Haskell, even if GHC rejects them. (IIRC, Haskell has Eq has a superclass of Num, but GHC does not.)
As I said in another comment, this is because JS has a different semantics around equality checking to begin with: reference comparison for non-primitive types. That's why `var a = [0]; a === a` and `var a = [1]; var b = a; a === b` are both true. You can also do `deepEqual([0], [0])` to check for value equality instead.
ORM is an antipattern. What would you do with an ORM that cannot be done with queries?
What's the story about the "Our take on this divisive issue..." and the "Customers who bought this item also bought" slide? Was your talk recorded or is there a slide deck with speaker notes to get more context?
Well they already don't work with maps, you'd just get a type error instead of the wrong result.
Why?
Another one of my stupid jokes. There was a HUGE fight in the community about what should be the recommended method for installing Haskell on the Haskell homepage. The joke was that it doesnt matter how users install it, if you don't tell them the shitloads of RAM that they're gonna need. That and the fact that Haskell is probably the only language that has two competing home pages. The conf organisers are post-processing the video. Will share once it's up. 
Okay, that one really needs a blog post :) Short answer - you can do much more with raw SQL queries, but you can do common stuff much easily, much faster with ORMs.
Thank you for your answer, tbh, I get confused with the first and second step. in the first is to use statically linked binary, does this mean compiling the code with Haskell to binary? 
&gt; This instance, of course, overlaps with things that the [Char] flexible instance doesn't... Such as?
What you're saying is correct, but I don't see how it contradicts my logic. The last bullet point mentions that it's a problem with an ambiguous dictionary (as opposed to a unification problem). Am I missing something?
Could you please define what you mean by control patterns? So that I can address appropriately. 
The last two rows/columns are killing me.
do you mean a or b or c? 
so no need to VPS right? any article to or guide please?
In no particular order. Pick whichever you have prior knowledge of. 
The picture is willing to compare [] with "". To do so it has to pick `a = Char` by unification. Syntactically, this is exactly what happens when you write `[] == []`. There it picks "a", which you then say would be ambiguous, but we _do_ have defaulting, so it'll pick `a = ()` as evidenced by the fact that this works: Prelude&gt; [] == [] True If we turn off defaulting we get what you seem to imply. Prelude&gt; default () Prelude&gt; [] == [] &lt;interactive&gt;:3:1: error: • Ambiguous type variable ‘a0’ arising from a use of ‘==’ prevents the constraint ‘(Eq a0)’ from being solved. Probable fix: use a type annotation to specify what ‘a0’ should be. These potential instances exist: instance Eq Ordering -- Defined in ‘GHC.Classes’ ... • In the expression: [] == [] In an equation for ‘it’: it = [] == [] 
https://hackage.haskell.org/package/acme-php probably has you covered somehow.
[This is how I imagine person using that term.](http://i0.kym-cdn.com/entries/icons/facebook/000/018/666/fellowkids.jpg)
&gt; this is because JS has a different semantics *All* of the differences, include the other ones that make even *less* sense are due to this reason. It's not a excuse for doing things incorrectly. :P
Thanks, looking forward to the video!
I'm not sure why anyone would think that `Either` should be symmetric. The order of type parameters matters very much in Haskell. The `Functor`, `Applicative`, and `Monad` instances for `Either` aren't symmetric, so I'm not either why an `EitherT` would need to be symmetric, either.
`("" :: Text) == []` is a type error. `("" :: String) == []` is true, which is a bit odd.
Disclaimer: I wrote `yesod-colonnade`. Honestly, the `colonnade` ecosystem is not a great fit for this kind of thing. It's better when your columns are statically known. If you're dealing with a matrix of arbitrary dimensionality, your columns are not statically known. Also, a tabular visualization of a matrix has no need for a table header. I've written [a gist](https://gist.github.com/andrewthad/44f6512b371151f2e2e186564c70deff) showing how to build table from a matrix. I'll provide the code here as well: {-# LANGUAGE RankNTypes #-} {-# LANGUAGE ScopedTypeVariables #-} import qualified Data.Matrix as M import qualified Text.Blaze.Html5 as H import Text.Blaze.Html (Html,toHtml) import Text.Blaze.Renderer.Pretty (renderMarkup) main :: IO () main = putStrLn (renderMarkup (matrixToHtml myMatrix)) matrixToHtml :: M.Matrix Int -&gt; Html matrixToHtml m = H.table $ H.tbody $ generateM (M.nrows m) $ \rowIx -&gt; do H.tr $ generateM (M.ncols m) $ \colIx -&gt; do let x = M.getElem rowIx colIx m H.td (toHtml (show x)) generateM :: forall m a. Monad m =&gt; Int -&gt; (Int -&gt; m a) -&gt; m () generateM n f = go 1 where go :: Int -&gt; m () go ix = if ix &lt;= n then f ix &gt;&gt; go (ix + 1) else return () myMatrix :: M.Matrix Int myMatrix = M.fromLists [ [1,2,3] , [4,5,6] , [7,8,9] , [10,11,12] ] Also, it's unfortunate that the `matrix` library uses 1-based indexing instead of 0-based indexing, but this code sample accounts for that.
I added details to my original list! Hope they help. Static linking means that your final executable *includes copies* of all (most) libraries that you use. By default your project uses dynamic linking, which means it does not copy those libraries into the binary file itself.
Why no lens? Also, I think it should have been fairly obvious hat postgresql-simple was going to have those problems. What was wrong with persistent+esquelto? Also, it seems like the only thing you like about Haskell is the ability to refactor stuff and have the type system enforce the refactor. But this advantage is present in pretty much all statically types languages. What parts of Haskell are better than, say, Java in your eyes? Finally, I think you really ought to come to the table with some proposed solutions here. It seems like most of these problems either are really hard problems, or are simply due to the lack of a huge company putting tons of money into Haskell (or in the case of editor tooling, both). I think Haskell is doing extremely well for a volunteer-based language. I think for the most part, these problems are already pretty well recognized. The problem at this point is a lack of plans, not a lack of recognition. I encourage you to think about these problems in terms of being constructive instead of out of frustration. We don’t get anywhere by yelling at clouds; we have to make *actionable* plans to move forward.
String = [char]
 data ExtChar = ExtChar Char | Something Else instance IsString [ExtChar] where fromString = fmap ExtChar was admissable before, but now overlaps the replacement instance.
Your frankness is absolutely fantastic. Thank you for pushing everything forward. There are absolutely things you could have avoided, *but* that's the point: you didn't. Making it clear how to avoid them is part of what we need.
Did you forget to add stuff to `reexported-modules`? (Adding a `mixin` clause does not re-export) You probably need to show more of the cabal files involved here for folks to figure out what is going wrong. Also you probably want both the executable and library to have separate `hs-source-dirs`, otherwise you're likely picking up cruft from one accidentally for use in the other by filename, rather than the package dependency. It'll help you figure out a bit more about accidental file-based inclusions to split them up more carefully.
Re: records. I have to agree. They are horrible. However, I recently discovered that `generic-lens` combined with [`generic-lens-labels`](https://github.com/duog/generic-lens-labels), `OverloadedLabels`, and `DuplicateRecordFields` is a wonderful boost to Haskell records. It lets you: 1. Stop prefixing your record fields with the record name. This is possible because of `DuplicateRecordFields` *and* because using `generic-lens` means your field accessors are row-polymorphic and will work for any record with fields of that name. You can easily avoid any possible ambiguities arising from this by simply using the lenses instead of the record fields. The only place you *must* use the record fields is to construct a new record. But this is not ambiguous and it works fine. 2. You don't have to export/import any lenses generated by TH. `generic-lens` just derives each lens on the spot based on the record's fields. 3. You get the benefits of `generic-lens`, such as row-polymorphism and subtype relations! 4. The syntax is actually bearable: `record ^. #_field1`. 5. You can have record field names and record field lenses that look *almost* identical: `_field` vs `#_field`. 6. Stop using TH for this. Note: `haskell-src-exts` *had* a bug that would not recognize `#_field` as valid Haskell. It has been fixed *but not released*. This means any tooling (like `stylish-haskell` or `hlint`) depending on that library needs to be rebuilt with a newer version of it, or it will complain that your code doesn't parse.
I know, which is a good reason to use Text instead of String, outside of some special teaching exercises.
Really? Is it because of ambiguous types?
Yes, you can use [flags](https://www.haskell.org/cabal/users-guide/developing-packages.html#configurations); Cabal will automatically infer which flags to enable from their optional dependencies (unless you make them manual). 
You could probably find a library (dynamic maybe?) or write a typeclass for it, but may I ask why you want such a thing? Not being snarky, I'm genuinely asking.
As of version 0.4 and onwards it has no documentation. This is the state of Haskell ecosystem. Shame.
Nice! If I'm reading this documentation right though, the flag will be enabled if a version of package X which is compatible with the rest of the constraints can be found and installed. That sounds exactly what the OP asked for, "if package X is _available_", but that's not quite what I was hoping for. Suppose there is package P which define a new typeclass and also provides instances for data types from hundreds of other packages, including package X. Next, I write an application which uses both P and X. I would have hoped for a solution which would only build X and the fraction of P which provides the typeclass and the instance for X. Instead, those 99 other packages _can_ be built, and so they will be built because they are "available", even though they are not needed.
Make the schema aware type checker scream at you when compiling instead of three weeks after deployment.
I stripped down my cabal file for the purposes of posting to reddit. You can find a complete example [here](https://github.com/centromere/backpack-reddit-question).
Your slides really resonated with me. I LOVE using Haskell, and come from a Scala background. I feel stupid for the amount of hoops we have to jump through to accomplish simple things (record naming etc). The embarrassment I feel when pairing with someone who's Never used Haskell is frustrating. It's not the language, it's the ecosystem.
That was pure sarcasm. :P
Hah totally went over my head. Carry on. :)
[mail-provider-google.cabal](https://github.com/centromere/backpack-reddit-question/blob/0e9c4aee456702376aaf12a598e91bc3e7baaab8/mail-provider-google/mail-provider-google.cabal) isn't saying quite what you think. The build-depends: mail-provider-sig line screws you up there in a manner that isn't obvious until later. You wind up with a package that has an unresolved (and unused) `MailProvider` dependency and which _also_ exports a completely unrelated `MailProvider.Google`. That line should be dropped at least. What you seem to be trying to do there is to express something like mixin: mail-provider-sig requires (MailProvider as MailProvider.Google) to check that `MailProvider.Google` matches the `MailProvider` signature, but this can't work either, as it would be mixing in a backpack package against a module being defined in the same package which triggers an error. If you really want to check that MailProvider.Google matches the signature before you go to mix it in somewhere, then you can add an extra test-suite that checks the signature and does nothing else. An example of that pattern can be found [here](https://github.com/ekmett/coda/blob/71d8863c06e93101660cc9a5ae675ba59da31005/ref/ski/ski.cabal). Anyhow the fact that that package now has an unresolved dependency is what breaks you when you get around to writing the `executable` stanza in the `mail-repl` package. There in the library you instantiate mail-provider-sig again, but now with concrete arguments, but you're still left with the dangling reference from the first usage mentioned above, so when you incur the dep in your executable its left with an undischarged signature. Finally, when you do get around to fixing that unused import in `mail-provider-google.cabal` you'll run into the fact that your `mail-repl` library doesn't actually export anything for use by the executable there without a `reexport-modules` command
It’s truly horrifying. I kind of understand converting a numeric string to a number when comparing to a number, but why would you ever convert two similarly typed values to another type for comparison? Especially to something like floating points where equality is not well behaved.
Regarding the problems with applicative parsers and digestive-functors: Using `ApplicativeDo` and `RecordWildcards` you can do something like: data Foo = Foo { bar :: Bar , baz :: Baz } fooParserOrForm = do bar &lt;- barParserOrForm baz &lt;- bazParserOrForm pure Foo{..}
Yeah. Can't figure out what `Eq` instance to use.
Ah yes, missed that. (I read "available" as "installed".) 
&gt; Because it compares non-primitive values by reference. The great mistake of imperative languages. Royally screwing up equality.
 class PHPEq a b where (=?=) :: a -&gt; b -&gt; Bool {-# OVERLAPPABLE #-} instance (Default a, Eq a, Read a) =&gt; PHPEq a String where l =?= r = a == (fromMaybe def $ readMay r) {-# OVERLAPPING #-} instance PHPeq String String where (=?=) = (==) (or something like that)
You don't *need* `ApplicativeDo` for this, if the parser can be monadic.
I was rushing headlong into that until I discovered how much work it took someone to write a version of [MiniKanren using LogicT](https://github.com/jvranish/MiniKanrenT/tree/master/src/Control/Monad) that [will not compile for me](https://github.com/jvranish/MiniKanrenT/issues/2). My latest plan is to run one of the images of Datomic on DockerHub and query it using Servant.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [jvranish/MiniKanrenT/.../**Monad** (master → 6077263)](https://github.com/jvranish/MiniKanrenT/tree/60772630ea31a64a1d7d15cf24524fd1b5031010/src/Control/Monad) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dq3ygzw.)^.
Please don't do that.
I think you missed the thrust of my question. I knew that already, but I work on a Haskell-like language called Hackett, and it currently has [a transformer named `ErrorT`](http://docs.racket-lang.org/hackett/reference-monad-transformers.html#%28def._%28%28lib._hackett%2Fmonad%2Ferror..rkt%29._%7E23%7E25hackett-type%7E3a.Error.T%29%29). I’m only wondering that, *given I have the chance to do it right the first time*, should I call it `EitherT` instead?
So equality in PHP isn't transitive? That's astounding.
&gt; but we do have defaulting …where »we« is GHCi, which I would consider less Haskell-y since it’s tuned towards quick prototyping, wouldn’t you agree? Maybe I should mention this in the remark to avoid confusion, since GHCi isn’t very loud about its extended defaults.
 if (isNumberString(a)) { return compareAsNumberWith(a, b); } …or something along those lines
Another example is if you want heterogenous yet still statically typed dictionaries: data Dict :: [(Symbol, Type)] -&gt; Type where (:-:) :: Item k v -&gt; Dict as -&gt; Dict ('(k, v) : as) Nil :: Dict '[] infixr 5 :-: instance (Show (Item k a), Show (Dict as)) =&gt; Show (Dict ('(k, a) : as)) where show (a :-: as) = show a &lt;&gt; " &lt;: " &lt;&gt; show as instance Show (Dict '[]) where show Nil = "Nil" data Item :: Symbol -&gt; Type -&gt; Type where I :: forall s a. a -&gt; Item s a instance (KnownSymbol k, Show a) =&gt; Show (Item k a) where show (I a) = "I @" &lt;&gt; show (symbolVal $ Proxy @k) &lt;&gt; " " &lt;&gt; show a class Insertable (k :: Symbol) (v :: Type) (as :: [(Symbol, Type)]) where type Insert k v as :: [(Symbol, Type)] (&lt;:) :: Item k v -&gt; Dict as -&gt; Dict (Insert k v as) infixr 5 &lt;: instance Insertable' nk nv '(k, v) as (CmpSymbol nk k) =&gt; Insertable nk nv ('(k, v) : as) where type Insert nk nv ('(k, v) : as) = Insert' nk nv '(k, v) as (CmpSymbol nk k) na &lt;: (a :-: as) = insert' na a as $ Proxy @(CmpSymbol nk k) instance Insertable nk nv '[] where type Insert nk nv '[] = '[ '(nk, nv) ] I nv &lt;: Nil = I nv :-: Nil class Insertable' (nk :: Symbol) (nv :: Type) (a :: (Symbol, Type)) (as :: [(Symbol, Type)]) (c :: Ordering) where type Insert' nk nv a as c :: [(Symbol, Type)] insert' :: (a ~ '(k, v)) =&gt; Item nk nv -&gt; Item k v -&gt; Dict as -&gt; Proxy c -&gt; Dict (Insert' nk nv a as c) instance Insertable' nk nv '(k, v) as LT where type Insert' nk nv '(k, v) as LT = '(nk, nv) : '(k, v) : as insert' na a as _ = na :-: a :-: as instance Insertable' nk nv '(k, v) as EQ where type Insert' nk nv '(k, v) as EQ = '(nk, nv) : as insert' na a as _ = na :-: as instance Insertable nk nv as =&gt; Insertable' nk nv '(k, v) as GT where type Insert' nk nv '(k, v) as GT = '(k, v) : Insert nk nv as insert' na a as _ = a :-: na &lt;: as Which needs GADTs (among a LOT of other extensions lol). Likewise heterogenous lists and statically sized vectors also need GADTs. As others of mentioned any situation where you want a type error in your EDSL to be a type error in Haskell will tend to require GADTs. The general idea is that if you ever run into a situation where there is an invariant you want Haskell to verify at compile time, but can't figure out how to do it, you should consider GADTs. One thing to lookout for is situations where you want only a subset of constructors in a sum type to be possible depending on the situation (a.k.a type). For example with heterogenous lists if the type is `HList '[]` you want the only possible constructor to be `Nil` and NOT `Cons` (and vice versa for `HList (x ': xs)`). A situation like that basically guarantees the need for GADTs.
Oh I get how, it’s the Why that puzzles me.
I’m pretty sure I’ve actually never encountered a real world scenario where `type String = [Char]` was useful.
Yep: http://eckyputrady.com/2017/02/18/Haskell-Heroku-Mailgun-Redis/ Anyway, I suggest to use this buildpack https://github.com/mfine/heroku-buildpack-stack instead of the one in the article. The one in the article is a fork of the buildpack above. I use it bcs at the time of writing, the original one has issue. Now the issue has been fixed.
I am surprised you attempted to simultaneously invent/solve novel solutions for old haskell problems, while trying to ship an app. When building a product, I think you need to be very aware of your "novelty budget", and focus on solving business requirements and less on things like novel ways to express haskell records. IMHO, You are trying to be too novel at too many things simultaneously. there are many good problems identified here; it's just a bit concerning that some of them (but not all) are a bit incidental to the express goal of implementing the customer's requirements.
&gt; mtl vs free both are bad. The short reason: they do not compose, are not compatible they don't compose effects. Need an standard that composes. A graded monad. &gt; how to write domain APIs that can result in "validation errors"? If you start using Either how do you compose the Left values? I went down a research-y rabbit-hole in the early days. Finally I gave up and started using throwM like nobody's business. such monad should incorporate the formlet model. The left values should have monoid instance and concatenate. &gt; which exceptions package to use? I remember being royally confused between stuff in base, ExceptT and Control.Monad.Catch They are bad. like native exceptions, they don´t compose. They are not mathematically sound . Backtracking with alternatives is a better option. &gt; how to fork from deep within an mtl stack? (/u/snoyman has spoken about this recently at LambdaConf) Before this I didn't completely understand why most libraries only take IO a arguments and not MonadIO m =&gt; m a We wasted a lot of time struggling with this, eg. https://github.com/lpsmith/postgresql-simple/issues/9 mtl is is something made by the enemy. It does not compose. Don´t use that. &gt; How to deal with nested records? After another research-y merry-go-round I gave up, and tried dealing with nested tuples and got stuck. Esqueleto was dead at that time, and /u/tomejaguar helped me with it, which is why I started looking at Opaleye. Don't use records, don't use lenses. use maps or some fancy library that uses maps. Rich Hickey is right. How to generally structure a web-app in Haskell? I tried asking around to see some code for a mid-sized Haskell webapp and as pointed to the source code of haskellers.com and was very unimpressed. If I put here my opinion of mainstream Haskell web frameworks This post would disappear buried in negative votes (It would happen anyway) &gt; How does one deal with auth / permission checks in Haskell? Can the type-system help in enforcing that every dev has implemented a permission check in each API endpoint. No idea. 
That's to underscore that documentation is not actually useful when you have to read the source anyway to be absolutely sure... I should submit an issue: "Please add incorrect documentation"
Oh hey, Curry! I'm currently taking Functional Logical Programming from Sergio at PSU right now; I'm really starting to love the language and I found your paper about compiling Curry to Haskell quite fascinating :) I'm super excited for type-classes getting added and polished up. Are there any other fun features you're looking into implementing? (I'd definitely apply for this sort of thing, but unfortunately I already have a job and I doubt they'd let me leave for 3-ish months to travel to a different country...)
This visualisation for JS would be interesting. I guess all the grey would become mostly red with some weird green spots where you don't expect it.
To be fair, as arrays are mutable objects this is the correct semantics. &gt; :m + Control.Monad.ST Data.primitive.Array &gt; runST $ (==) &lt;$&gt; newArray 0 undefined &lt;*&gt; newArray 0 undefined False
We could go on and on about this I'm sure, but many people find the use of `Either`/`EitherT` for *errors* to be too confusing. Either is just your canonical sum type. The fact that you might use it to encode errors is something else entirely. I think calling it `EitherT` is good IFF `Either` is not more-or-less synonymous with `CanFailWith err a`. The fact that `Either` is right-biased only serves to strengthen its "artificial role" as a short-circuiting error-handler.
You've never used list functions on strings and been happy that they just work? Even simple things like `toUpper &lt;$&gt; str` or `filter (/= '\n') str` and so on. I get that these things can be reimplemented for `Text`, but being able to use one nice interface for lists and strings as well as use various typeclasses like `Functor` on strings is kinda nice. Overall I do agree we should push people towards `Text`, but I do think you are exaggerating.
Thanks for the reply! I was wondering if the different shapes might make things trickier; I had forgotten that the shape representation of an ADT might have the same size as the structure itself, which definitely makes things trickier. Perhaps it might end up being that, while that's the case, the vast majority of the time we can assume one thing or another and that would make the problem more tractable... I'll bang my head on a whiteboard about that :)
Old problems or not, if they don't have a (well-documented, maintained) solution what are you supposed to do when you run into them? And who are you to judge (or even have any idea about) whether they managed their "novelty budget" appropriately?
This sounds pretty great! I haven't played with all of these together yet but it looks like I'll have to try it out soon. It would be really nice to have a small recipe-book for common things one might do with this; do you know of any out there?
Good talk. I feel like the phrase "[StateT or ExceptT] shoudn't be used in the large, for your entire application" is a poor summary. The argument that I heard was "Don't use StateT and ExceptT over IO, because they're redundant", which is a sensible claim. If your application's core logic doesn't rely on IO (no concurrency, resource handling etc.), and you can write it as a non-effectful computation with IO at the borders, then I would argue you should still use StateT and ExceptT. I think you made this point during earlier in the talk, but I think it might be overshadowed by that bold ending statement.
This sounds great! Could we have it on Memrise too, perhaps?
It isn't in JS either. 0 == "0" 0 == "" "0" != ""
What if we just ripped out every single list function and put it all in Functor and then made every single Text/ByteString/etc an instance of Functor? There's absolutely no way that could backfire on us...
I think I would have to be really convinced about the benefit of the dictionary before going to all that work for it! For instance, the corresponding record of hardcoded fields and Maybes would have to be pretty awful. On the other hand, the interactive datastore example from the Idris book is a pretty nifty use of one of those... but of course the whole point of playing with Idris is to indulge in situations like that. I actually do have an EDSL or two where I'd like some more type checking, but invariants always seem to be up there in dependently typed land. For instance, assert this number is divisible by that one... Nats in Haskell seem too scary in terms of clunky syntax or efficiency or error messages, while in Idris they're relatively reasonable. And of course it's not so easy as just n/m, but it's more like "the sum of the durations of the contents of this list is divisible by m." I seem to recall that a couple of ghc releases ago, Nats (or type level integers in general) were actively undergoing improvement, but I don't know if they got good enough, or if progress stalled. Thanks for the advice though, I will keep my eyes open.
That did the trick, thanks! Another question: Under what conditions is it actually better to use type classes as opposed to Backpack? What design considerations would lead me to use one over the other?
That's true. I think the point is that applicative style parsers are nice because each field is independent, so you can get all the errors instead of only the first one. The problem is that the applicative style can't be used with named record fields. -- this works Foo &lt;$&gt; getBar &lt;*&gt; getBaz -- but there's no way to say something like this Foo &lt;$&gt; bar = getBar &lt;*&gt; baz = getBaz
Well integer is working. What is getting passed to it? 
 class Torsor a where type Discriminate a diff :: a -&gt; a -&gt; Discriminate a integrate :: Discriminate a -&gt; a -&gt; a -- idk if torsors require this instance Torsor UTCTime where type Discriminate UTCTime = DiffTime diff = diffUTCTime integrate = addUTCTime That you want DiffTime to be a vector space (addition, scalar multiplication, linear) would simply be a class on DiffTime.
Doesn't the initial example (with `ApplicativeDo`) handle named records with applicative semantics?
I am considering publishing this as a package on Hackage, but I wasn’t sure if something like this already existed somewhere. Essentially, I sometimes end up using operations that require `MonadBaseControl IO` with others that require `MonadIO`, and I find having them both obnoxious and ugly (I’ve mostly switched to using `MonadBase IO` instead of `MonadIO` in my own code to avoid the proliferation of constraints). This provides a way to convert one into the other, but it seems so simple that I find myself wondering if it’s already been done before.
You can get close, but not with normal `elem`, as it traverses the entire set. You'd want the ordered-list elem: elem' x [] = False elem' x (y:ys) = x == y | (x &lt; y &amp;&amp; elem' x ys) This would allow you to write your definition, relying on the fact that your list produces an ordered list. Testing this: let elem' _ [] = False; elem' x (y:ys) = x == y | (x &lt; y &amp;&amp; elem' x ys) in fix $ \ls -&gt; [(x,y) | x &lt;- [1..6], y &lt;- [1..6], not $ (y,x) `elem'` ls] This, however, loops; in particular, it's not clear if (1,1) is an element of the list. If (1,1) is in the list, then it ought to fail elem` I guess the answer, in the end is your new definition seems to have a paradox
False. We never forget to bash javascript.
Wait, that sounds good to me. What am I missing?
I would guess that this function is passed (somewhere) to `withText`. The original value is a number, so it dodged your code entirely. You probably want something like pInt :: Value -&gt; Parser Int pInt v = parseJSON v &lt;|&gt; withText "Int" integer v
You can't make Text/ByteString/etc Functors, due to the fact that Functor works on things of kind `* -&gt; *` but the types in question have kind `*`. Also you can't implement most list functions in terms of a functor, e.g `filter`. Thus existing functors will cease to be able to be instances and the name "functor" will become rather inaccurate and misleading. Now I would like to see more interesting typeclasses to generalize as many list functions as possible. Since for example `MonadPlus` / `Filterable` are sufficient for filtering, `Traversable` for sorting, hell technically `Witherable` gives you uncons and `Foldable` gives you indexing. Now I would like to do this in a lawful way that we can reason about (some of my examples may not be very well constrained by laws, haven't thought enough about it to be sure), and I would also like good performance. Since for example `Foldable` for indexing is `O(n)` even on arrays unless you special case indexing like so many other foldable functions are (`length`, `maximum` etc.). I would also love to see "smarter" classes that can allow for more things to instantiate them, such as typeclass constraints on functor so we get `instance Functor Set`. This could also potentially indirectly get you `instance Functor Text`, although not with the existing kind `*` `Text`, you would need a data family based `Text` that takes in an element type. And then I guess `type Unicode = Text Char` or something like that. Perhaps a better approach than that would be (if possible / practical / performant) `instance Functor Rope` and `type Text = Rope Char`.
https://www.reddit.com/r/haskell/comments/7e7ggt/haskell_equality_table_because_sometimes_we/dq47re1/
Thanks. That is a really good point. Should have seen it.
Indeed I did miss it. I think that `EitherT` is the most natural (and expected) choice for a name.
Yes, I was trying to show why `ApplicativeDo` is necessary. 
No. This is how you advance an ecosystem you know you want to build on.
&gt; Haskell DOES NOT leak memory in production. Then you haven't put enough data through it yet. All Haskell processes that use ByteString will eventually run out of memory. https://ghc.haskell.org/trac/ghc/ticket/13630
I only mentioned that because `ApplicativeDo` is still a scary new shiny extension that may or may not be something you want to try. Especially if *all you want* is to name your fields.
I don't know of anything like that, but it's an excellent idea! Haskell in particular lends itself to that kind of help because it is so good at abstraction. The downside to lots of abstraction is that you can't always see how to piece things back together in a way that helps you *right now.* If something doesn't exist, we should start it.
&gt; broken Only if you think "good enough for all practical applications but logically inconsistent" is a form of brokenness. I mean, maybe it is. Dijkstra had a point. But that issue is *far* bigger than floating point numbers, and if you feel it sincerely you probably don't want to spend your time in a language with `unsafePerformIO`.
I personally think `EitherT` makes sense. I'd even argue that there is nothing wrong with `FunctionT` and `TupleT`, since `ReaderT` and `WriterT` both have very concrete (and publicly exposed) implementations, namely using functions and tuples respectively. There are other possible monad transformers that convert something that isn't necessarily a writer to something that is. Such as a strict tuple. The current tuple implementation is just arguable the most intuitive one. Now I'm not saying we should put any effort into changes those names. But I DO think the case for `EitherT` is quite strong. I would say that `MonadError` / `MonadReader` / `MonadWriter` are good names and should stay for sure, since these classes explain what something does and not what it is.
I agree that going too deep in the direction of an ORM is questionable, and will often lead to situations where you need to put in much more effort to express what you want when compared to good old set theory based queries. However with that said EDSL's or "ORM"'s that are more of a lightweight (and generally typed, which is really really nice) layer over the queries themselves is really useful. I would pick something like that over using raw query strings or anything close to that any day.
Except floating point. Those instances just shouldn't be there. Which instances do I mean? Well, I think `Float` and `Double` should certainly be instances of `Show`, `Read`, `Binary`, `Typeable`, `Data`, and a reworked version of `RealFloat` that doesn't have superclasses, but basically nothing else interesting. Putting `RealFloat` in its own module would let it reuse the usual operator names: `+`, `-`, `-`, `*`, `/`, `==`, `&lt;`, etc.
Young and foolish, mate. Young and foolish :) 
Backpack is pretty heavy-weight to use. You need 2-3 modules, and typically 2-3 _packages_ to even start using it. Imagine making such a thing in the middle of a package. I've had to split one package up into 20 or so to backpack the resulting mess due to the strict tiering required. It can be rather awkward to avoid orphan instances when using backpack. I've always been able to do so, so far, but at the cost of even more packages and modules. Backpack completely recompiles the code involved each time you instantiate it. This is good in that it enables you to UNPACK the data types from your signatures even when they are abstract, but its bad in that it adds a non-trivial amount of overhead. The main things I'd reach for backpack are when you have a whole module (or package) worth of data types and definitions that really don't want to pick up a ton of otherwise useless proxy and type arguments to pick an instance that you'll only instantiate 1 or 2 ways throughout your program. It can be good for reducing code duplication when you're duplicating it mostly to get unpacking right. It can reduce the amount of type parameters in your code. It can reduce the amount of explicitly passed dictionaries to methods. I'm still trying to find the most productive ways to exploit it myself, but that is the general direction I've been going. Note: you need the latest release candidate if you start using it in earnest, though. I was able to get linkage failures before that in some complicated scenarios.
I did not write ad-hominem attacks and hope you do not do so in the future again either
Rust uses Result = Ok | Err, which makes the intent much clearer IMO. 
&gt; Finally, I think you really ought to come to the table with some proposed solutions here. Why? Just because someone can identify a problem doesn't mean they are capable of solving it.
&gt; The short reason: they do not compose Could you give me the long reason? mtl and free are composable. Do you advocate for something like `Eff` in this https://hackage.haskell.org/package/freer? &gt; such monad should incorporate the formlet model. The left values should have monoid instance and concatenate. It's impossible to write a coherent monad that does this. However, there is an Applicative: https://hackage.haskell.org/package/validation
If you have mutable datastructures you have both identity and equivalence to worry about and letting `==` (or well, `===`) mean identity makes a lot of sense imo. Sure, equivalence looks nice in the example of `[1,2,3] == [1,2,3]` but if you have a DOM node or something, you definitely want identity This is more a problem of mutation in general rather than a specific design flaw in JS
Hey Saurabh, I think mtl vs. free is a false dichotomy. Unless you mean "freer", as in [the extensible effects library](https://hackage.haskell.org/package/freer). The free monad and mtl are not competing to solve the same problem. Regarding validation errors- I have struggled with the same problem, and wrote this as an attempt to fix it: https://gitlab.com/LightAndLight/haskell-arrow-validation/blob/master/src/Data/Check.hs (there is an example in the docstrings). I haven't published it because I'm not sure if it's going to be useful to anyone else, but is that the sort of technology you're looking for?
If you are dealing with mutable data, both reference equality and value equality make sense. However, value equality can be defined by the user using reference equality and `typeof`, while reference equality can't be defined by the user. Because of that, making `==`/`===` use the more primitive operation and relegating the more complex operation to a function is plausible. 
I don't understand what you mean by 'old ihaskell commit', do you mean an older version of IHaskell? The IHaskell in nixpkgs is not up to date or as customisable as what is available in the repo so I would recommend using the repo if possible.
I think /u/saurabhnanda meant to direct that at himself/herself i.e. they tried to solve those problems due to naivety of youth.
Thank you for your openness @saurabhnanda! What a slide!
I believe that the "aa bail mujhe mar" joke will be missed without an Indian/subcontinent background. Particularly when the photo is that of a Spanish bull fight. 
Why the use of `IdentityT`?
Only to get the instances for free via GND. It’s much easier than writing all the typeclass instances by hand.
Ahhh, that's a neat trick to have -- thanks for sharing! :D
Why not have `EitherT` in Hackett as a general structure, and a `newtype ErrorT = EitherT`, and the convention / style guide / idion to use `EitherT` for general sum types and `ErrorT` for errors/exceptions? 
I don’t really understand why anyone would want to use a right-biased monad transformer for anything but errors. What would a use of `EitherT` look like for “general sum types”? I don’t get it.
No, I think this is exactly the right place to talk about the deficiencies in Haskell. Bring it up to the community, if other accept it as a real problem, someone can start digging into solutions. 
If you're doing that, could you name the field without a leading underscore? At work we have our own version of this and it is pretty great. We do use TH instead of `generic-lens`—I didn't know about that package when I wrote our record utility—but it works remarkably well. As a bonus, we were able to integrate an extensible record system with no real hassle because it was directly compatible with how we accessed fields on normal records!
I think the important thing is consistency. Do you already have `Maybe` and `MaybeT`? Do you expect people to use `Either` for error handling or early termination? If both of those are true, I think `EitherT` would be the best name.
I'm sure within Rails everyone just grabs the Rails logger out of thin air. I guess this approach isn't drastically different from if someone made a Yesod library that operates in some Yesod transformer that includes access to a logger. But ya, logging in Haskell is a mess. This might be one case where unsafe IO is probably useful - I wouldn't advocate that in a Haskel app, but languages where you can log anywhere probably do gain from not really having to think as hard about logging. 
I mean, if I pin an IHaskell release and 2 years pass, it might stop compiling with the latest nixpkgs, and I wouldn't know which nixpkgs to use. But then again, what's stopping me from pinning nixpkgs as well :)
So 34,000 lines of Haskell replaced how many lines of other code?
Some good stuff here about reducing build times: &gt; To have module-level parallelism, it is needed to use `stack --ghc-options=-j`. &gt; (...) &gt; In practice we also have several more options passed to GHC, mostly RTS options to control garbage collection for GHC itself which has been [shown](https://rybczak.net/2016/03/26/how-to-reduce-compilation-times-of-haskell-projects/) to improve compile times by up to 50%. Merging all packages back into a single one has demonstrated up to 20% improvement in build times.
Thus spake `monotraversable` 
We're not replacing anything written in Rails or Angular v1 -- all of that code is still being maintained and getting small feature enhancements. All new modules and big features are being developed in Haskell.
unfortunately monotraversable is not sufficient to give us the ability to do the following: map toUpper myTextValue map (join (,)) [1, 2, 3] Don't get me wrong it is still cool, but functors with constraints (plus perhaps data families) can give you more, hopefully it is worth it to use such a thing.
Didn't know about that. Does this happen with String and Text as well? Also, in a typical web-app environment, where the slate is wiped clean at the end of every request/response cycle, are we still at the risk of such memory leaks?
Heh. It's actually hard to find good CC-licensed stock images. I couldn't find anything appropriate to go along with this joke, and had to settle with what finally went on the slide!
Is this a validation library for the handler/endpoint layer or the DB layer?
It wasn't directed at you.
&gt;&gt; how to write domain APIs that can result in "validation errors"? If you start using Either how do you compose the Left values? I went down a research-y rabbit-hole in the early days. Finally I gave up and started using throwM like nobody's business. &gt; such monad should incorporate the formlet model. The left values should have monoid instance and concatenate. I think this needs a slightly longer explanation. Consider the following "low-level" functions: data ShippingAddressError = UnknownPostalCode | PostalCodeNotServiced saveShippingAddressToDb :: (HasDatabase m) =&gt; ShippingAddress -&gt; m (Either ShippingAddressError ShippingAddress) data ContactError = InvalidEmail | InvalidPhone saveContactToDb :: (HasDatabase m) =&gt; Contact -&gt; m (Either ContactError Contact) Now, how would you write the function `saveOrderToDb` where: type Order = (OrderDetails, ShippingAddress, Contact) data OrderError = LowStock | UnknownSku saveOrder :: (HasDatabase m) =&gt; Order -&gt; m (Either ???? Order) 
One thing that your mention in the talk is that you use the derive instance of JSON using Aeson library. I am not sure if Aeson gurantees that the derived instances for a data type Foo will remain unchanged across versions of aeson releases. This can be a problem if you are doing some json specific things. Even if your data type Foo does not change, a change in the Aeson version can break due to change in the derived instance. The place I can see this breakage happening is when for example you have a json based chit-chat between the client side and server side. It can also be problematic if you use the PostGre's json fields. 
It looks like the `Text` value being passed was `"11.0"` instead of `"11"`. Not sure why, but I can definitely take care of that. :-)
&gt; They are bad. like native exceptions, they don´t compose. They are not mathematically sound . Backtracking with alternatives and state that may store the error condition is a better option. That composes. Better a stronger form of backtracking with continuations, like the one of transient, that allows closing of resources on errors. Till now, exceptions are the only pragmatic way I have found to deal with validation errors in a typical webapp. In most webapps when a validation error occurs, there's not much you can do. You have to abort the current operation and send the validation error back to the UI. As a best practice, try not to bail out on the first validation error. Try to collect as many validation errors as possible and send them in a batch, for better end-user UX.
&gt; Don't use records, don't use lenses. use maps or some fancy library that uses maps. Rich Hickey is right. I'd be happy to share our experience of why structured records are better than free-form maps, but I'm not sure if you're trolling or if you're serious.
&gt; When building a product, I think you need to be very aware of your "novelty budget", and focus on solving business requirements and less on things like novel ways to express haskell records. I absolutely agree to this point, which is why we didn't go with GHCJS or Purescript in the frontend even after spending quite some time evaluating them. However, if you're writing Haskell, there are a few problems that you are going to hit no matter what you do. Records is probably the first thing you'll hit. If you are committed to using Haskell then you have to solve them. There is no way around it.
Let me also add a concrete question here which I am asking because I remember you were planing to store json data in the PostGres fields. How do you handle migrations in schema in such cases? This is relevant to your use of derivation of ToJson and FromJson classes.
Yes, this is correct. But most examples and tutorials don't really promote this. Here are some examples: * [Yesod book's chapter on form processing](https://www.yesodweb.com/book/forms#forms_create_literal_aform_literal_s) -- most of the chapter is about applicative forms written in this positional style. Even the section about monadic forms actually uses an applicative style in the constructors. * [digestive-functors tutorial](https://github.com/jaspervdj/digestive-functors/blob/master/examples/tutorial.lhs) -- all examples use positional parameters * [Digest functors tutorial on snapbeginners.com](http://snapforbeginners.com/digestive-functors.html) -- applicatives using positional parameters * [Happstack's crash-course about the `reform` library](http://www.happstack.com/docs/crashcourse/index.html#type-safe-form-processing-using-reform) -- applicatives using positional parameters My experience tells me that trying to push all your validations to the endpoint/handler layer is not entirely correct. There are many validations that are much better implemented at the DB layer.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [jaspervdj/digestive-functors/.../**tutorial.lhs** (master → 30ad08b)](https://github.com/jaspervdj/digestive-functors/blob/30ad08bd0129b38b37aad65676d439829c01f090/examples/tutorial.lhs) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dq4o6p3.)^.
`monotraversable` absolutely can do `omap toUpper :: (Element mono ~ Char) =&gt; mono -&gt; mono`! `map (join (,))` does require that you have a polymorphic container, though.
I discovered `generic-lens` fairly late. Is it stable enough for use in large code-bases? Any perf issues (compile or runtime) at the record level (core data-structure for a typical webapps) are going to be a huge issue.
Thank you for your kind words :) Just doing our bit. We have another series of blog posts lined-up about what our working solutions to each of these problems were. 
It's just data validation in general. I was using it in servant handlers, lifting DB calls into the validation logic. Not sure if that answers your question properly.
Uhh... I mean that's my point, it can't do both of those, neither can `fmap`.
&gt; Why no lens? In what way? &gt; Also, I think it should have been fairly obvious hat postgresql-simple was going to have those problems. How? We have never had to bother with low-level SQL libs in the Rails world at all. pg-simple seemed like the most used and obvious choice and we just **assumed** that it would have everything sorted. But, very soon we [hit a bug in the hstore implementation and even raised a PR for it](https://github.com/lpsmith/postgresql-simple/pull/215) &gt; What was wrong with persistent+esquelto? When we looked at it (~ 12 months, more more, ago), esqueleto was an abandoned library. (/u/bitemyapp has taken the charge of maintaining the package since then). Issues with Persistent: * Without esqueleto it is a half-baked library. That is the only way to do left-joins, etc in persistent. I sincerely feel that esqueleto should be merged into core persistent. * Persistent's separation of `enttityId` and `entityVal` is a dirty hack around the core records problem (not trying to dis Persistent in any way -- they built this solution quite some time ago when a lot of the new type-system extensions weren't available). This solution works for only one of the fields that need to be filled-in by the DB -- the primary key. But almost all tables in our schema have TWO others -- `createdAt` and `updatedAt`. * No concept of validations at the DB layer. * Models (and routes file, in yesod) file is a compile bottleneck. You change one model and almost your entire project ends-up getting recompiled. In our workflows, we have a DB-first thought-process. The first thing that one changes it the DB schema. Everything else follows from there. And the DB schema is changed a number of times during the dev process as we realise that we forgot to take into account some data, or modeled it incorrectly. With 130+ tables, we cannot keep going through a full-project recompile every time this happens. * Nitpick - not a big fan of defining your DB schema via Haskell records (or even Python classes IIRC). In our world, the DB is king. It outlives any code surrounding it. It has a very rich language (SQL) to define it's schema which allows us to do a lot of other **extremely necessary** things as well -- eg. check constraints, trigger constraints, fts indices, trigram indices, etc. There is no way we are giving that up. DB is the single source of truth. Haskell records should be derived from there. Not the other way around.
Float is probably the reason Eq and Ord have no laws, so I think I agree - but at least that was a design decision, and not an oversight. 
I believe the fragmented ecosystem, hard to find libraries are a vicious cycle of positive reinforcement: You don't find what you need immediately (even if it's there), so you roll your own foot your use case. And then of course release it for the benefit of the community, ultimately, to the detriment of the ecosystem. 
I never really bothered about this in the Ruby world because the problem was already solved. After facing it in the Haskell world, I looked at how Rails delivers the following numbers for every request: * Total time spend in a request/response cycle * Split by time spent in rendering views &amp; time spent in accessing the DB. It turns out each layer simply emits structured log data to a channel in a pub/sub kind of architecture. A logging subscriber listens to the channel, consumes the structured log data and either emits it to a log file or transmits it to a remote server for aggregation and reporting. Further, a lot of drop-in instrumentation libraries in other languages, simply "wrap" known existing library function calls without changing their semantics. It is possible in those languages because such changes do not reflect in the type-system at all and code just continues to work as-is. Ruby/Rails is on the other extreme of this spectrum where "monkey patching" was, till some time back, an acceptable practice to extend the behaviour of core libraries. See what ActiveSupport does.
You can split your DB schema w/ Persistent models QQ. I know a lot of people that do and save themselves churn in the process. &gt;No concept of validations at the DB layer. Not at all clear to me why you'd want or need that in a library. I write functions. 
&gt; I am not sure if Aeson gurantees that the derived instances for a data type Foo will remain unchanged across versions of aeson releases. This can be a problem if you are doing some json specific things. Even if your data type Foo does not change, a change in the Aeson version can break due to change in the derived instance. Does it change the internal implementation of the auto-derived JSON instances, or the JSON it emits? We don't particular care about the former, but if the latter happens, I'm going to cry bloody murder! 
I am not saying that it _will change_ but saying that it _might change_. I do not know about this so you better think about it in advance. The point is that there is some code that takes your data types definition (TypeRef) and generates the necessary `FromJSON/ToJSON` instance either using TH or the generics. This code can change in subtle ways, for example the internal data structure used by the code changes from list to map and as a result the fields are sorted or some such thing. You might want to confirm this.
&gt; Not at all clear to me why you'd want or need that in a library. I write functions. Let me try provide some motivation for this problem. Say, you have a Contact model/table that can be manipulated by multiple endpoints (or code-paths, if you will). The endpoints/APIs exposed to the end-user, the endpoints/APIs exposed to the admin, the endpoints/APIs exposed to the customer-support. Irrespective of the code-path, you want to ensure that whenever a user's email is changed it is ensure that no known anonymous mailboxes, like mailtrap, mailinator, etc are used. Where would you put this code? We tried putting code like this at the endpoints/handler layer (which seemed like the recommended approach, given all the literature around digestive-functors), but soon, we introduced a new bug-vector. Devs forgot to call the appropriate validation functions at all endpoints. We tried putting code like this in a function called `saveContact` which wrapped the underlying `Opaleye.runUpdate` query and it worked like a charm. And thus the age-old pattern of having validations in the DB layer re-emerged, and we wrote a type-class to formalize it.
&gt; You can split your DB schema w/ Persistent models QQ. I know a lot of people that do and save themselves churn in the process. Thank you. We are going to re-investigate the DB layer libraries and this helps. Getting the DB library for us is extremely critical. &gt; I don't think this is inconsistent with how Persistent works. I need a representation of the data in my code with proper types and the nicest, most explicit way to do that I've found is to define the schema and generate the types from that schema unambiguously. You don't have to do all your migrations etc. through Persistent if you don't want to. We've written a tool that introspects the PG schema and generates all the DB boilerplate code in well organized modules. Write now it works for Opaleye, but it probably can be extended to any DB library out there.
Use Anki my fellow dude
Great question. As PG kept evolving, its support of JSON kept getting better. Our usage of the `jsonb` data-type also kept increasing in lock-step. Somewhere along the way we started using the [postgres-json-schema](https://github.com/gavinwahl/postgres-json-schema) extension to validate all JSON values stored in the DB. Whenever the shape of any JSON data changes, first the schema is changed, but it only validates newly inserted or updated values. Our internal dev process dictates that a migration be written which changes the shape of all old JSON data as well (usually a single UPDATE statement). This schema validation is mostly required for the Rails side of things. Possible next-step to evolve this process: On the Haskell side, we anyways have a strongly-typed record which provides the same type-safety as a json-schema check in the DB. However, to ensure that all old values also conform to the new JSON shape, we can probably setup a test in CirlceCI, which randomly loads ~5% JSON values back into the Haskell land to ensure none of them have parse failures.
It's marginally easier, at the expense of transparency. I've worked on code where the ORM took care of the database. It took 45 minutes to generate a simple report, because the ORM took hundreds of queries. The team spend months refactoring the code. With a simple SQL query the problem wouldn't even have existed. Having some lightweight syntax on top if SQL is nice, but giving away database access to a framework is a recipe for disaster. I had to debug problems where the semantics of the program changed because a lazy load got triggered at a different time. There is nothing an ORM gives that cannot be done with SQL. You trade program maintenance and reasoning about performance for a small convenience.
Do you also think the need for this indicates that we have a serious problem?
We're having this problem in our code, but I'm not sure if this version: foo :: (MonadBase IO m, ...) -&gt; m () foo = adaptMonadIO $ do ... is any better than the original: foo :: (MonadBase IO m, MonadIO m, ...) =&gt; m () foo = do ... Essentially it replaces a constraint with a term.. there's still redundancy.
Reports is a place where the ORM abstraction breaks down into splinters. We don't use ActiveRecord even for reporting in our Rails app. However, please don't throw the baby with the bathwater. ORMs are exceptionally good for the `findByPk`, `findWhere`, `findWithAssociations` kind of boilerplate, which aren't complex and don't return too many rows.
There is another angle to it. Library footprints that are too small, resulting in a number of sister libraries which one has to glue together for any common use-case. Bloated libraries are also bad, micro libraries are also bad. There is a sweet-spot which we need to achieve.
The idea is that you may run into library functions that use `MonadIO`, but it’s common to only call such functions in specific places, either because you’re wrapping them with your own abstractions or because they’re simply a self-contained subsystem of the program. The trouble is that while the term may only appear in one spot, the constraint “infects” the entire call stack. So if the top level entrypoint of your program transitively calls a single function with a `MonadIO` constraint on it, then the *entire* transitive call graph needs the pointless `MonadIO` constraint, too. This allows you to discharge the redundant constraint at the call site, then never worry about it again.
&gt; like native exceptions, they don´t compose What do you mean by exceptions not composing? If you mean composing exception-throwing functions, that clearly IO exceptions are composable in that sense (if `f1` throws `Err1` in IO monad and `f2` throws `Err2` in IO monad, `f1 &gt;&gt; f2` throws both in IO monad), so I'm not sure what you're talking about. Do you mind elaborating more?
I heard just too much moaning like "herp derp I can't remember all those lenses". Personally, I don't think this is most important obstacle.
No, docker under Os X is linux, not OS X. My point is that you no longer need VirtualBox. Docker has been ported to hyperkit which is extremely lean.
I've found that logging from IO only *results in better logs* because it ensures anything you log has the right context (ie what input caused the log to be created). All the log statements we do via unsafePerformIO are useless for anything but local debugging because they lose context. (For debugging, it's equivalent to `Debug.Trace`.) That said, I'm working on something that is &gt;90% out of IO and very much *not* a web app, so my experience might not generalize to web programming.
It would be nice if these cards had Haskell and Lens tags imo too, would make them nicer in card browser.
The command should deploy *or update* the stack. So when there's a change, yes it should update after a short while.
Incidentally, this answer is a proof for the "Broken library ecosystem" slide... Now, I understand the cause - it seems like in this case people are still experimenting, trying to figure out what works well, but the end result is obvious :)
e.g. what exception handling libraries + mtl combinations you adopted, what approaches are most extensible w.r.t adding effects (say, logging). 
And a pruning mechanism. And good tutorials for each library: you won't find the generalized, super abstracted library, even if it does your use case, if it's so general, that the documentation of our is written entirely in category theoretical terms without reference to common concrete applications. 
That doesn't look like something that makes a process run out of memory. What will happen is that every allocation might be rounded up to 4k. That's still bounded.
&gt; Now, we're reaping the gains. But does this mean it has already paid off to invest in Haskell, or does it mean it just started paying off and you *predict* it's gonna be the case for foreseeable future?
For fixing discoverability, I think the most important thing needed is that hackage becomes extensible. There has basically not been any visual change to hackage in the last 5(?) years. Imagine what it would have looked like if it was extensible using micro-services that "decorated" the packages with arbitrary information? Hackage needs to start either embedding iframes doing structured linking to other services or *something* that makes it possible to improve the situation without hacking on hackage itself. 
It might change, and the different json versions supported by GHCJS and main stackage is painful. What we're doing is explicitly configuring aeson and using separate instance derivations so we can configure explicitly. Anything else is too dangerous IMO.
Yes you can. The only reason I don't is because I still export all the record fields and I don't want to pollute the namespace. I didn't make that clear in my post, so thanks for clarifying.
Right on both accounts I'm afraid...
If there's no way to find this from [this page](https://hackage.haskell.org/package/lens), then it's value is a fraction of what it could have been.
These are questions that still need to be answered with rigor. But the approach is promising enough that I am trying it. `generic-lens` makes performance a big priority, so I think it's a great starting point for high volume (as opposed to something like `vinyl`). (That and the fact that pretty much anything is faster than compiling TH in GHCJS.)
Not entirely the point of your post, but in the case of those rank n arguments, either constructor specialization can kick in, which specializes *recursive* callees for some argument, or otherwise the non-recursive callee can be inlined at the call site. In both cases, the definition of the rank n function (e.g. a `MonadReader m =&gt; Int -&gt; m String`) is available at its call sites within the callee's code, so important further optimizations can happen. This sometimes fails because GHC considers the function too big to create a new unfolding for it. This may happen on functions you can't mark as `{-# INLINE #-}`, like [here](https://github.com/sgraf812/datafix/blob/cb1a547908e3adfb217d53b5a37d43554fe3e902/examples/Analyses/StrAnal/Analysis.hs#L3), where I want the *specialization* (`TransferAlgebra` is some rank n type) of `transferFunctionAlg` to inline. Using `-funfolding-creation-threshold=999999` helped.
&gt; This is a bit of a mess. Some of it is unavoidable in Haskell for historical reasons, but in new languages, we should be able to fix it. If it was possible to add annotations to packages in hackage, this *wouldn't be a mess*, it would immediately be obvious to anyone looking at obsolete abstractions that they were obsolete. This is purely a discovery issue IMO. Hackage needs plugins.
&gt; Then you haven't put enough data through it yet. Unfair. That can be said of *any* app in *any* language. Every system has inefficiencies *somewhere* that are exposed at extreme scale. Certainly GHC and Haskell's ecosystem is no exception!
I half agree. My statement assumed that "your entire application" would live on top of `IO`. If you have an application that somehow lives the majority of its time in a pure mode, then by all means use `StateT` and `ExceptT`. However, my point is definitely more than `StateT` and `ExceptT` being redundant. My point is that they are _confusing_ and perhaps even dangerous. But saying "Don't use `StateT` and `ExceptT` over `IO`" is stronger than the claim I made. I explicitly state that it's OK to use these "in the small," and I do so regularly. My advice is to avoid using them for application-wide mutable state and exception management.
It *is* possible for the JSON emitted for auto-derived to change when you upgrade the library, etc. However, these *should* be major version bumps and if you're relying on auto-derivation in a way that is vulnerable to change, then you should have tests to ensure your historical data conforms to any code changes. If you detect a change then you can write that instance manually to conform to the old schema.
I actually learned about this from /u/nh2_, so he's a better authority than I am. But in some: no, it doesn't happen with `String` and `Text`, since the garbage collector is free to move those values around (the memory is not pinned). And yes, I believe there is still a chance for such memory leaks. It's possible to limit the impact by using pools for buffers, at least in theory. I don't know if any real investigation has taken place into this.
&gt; All Haskell processes that use ByteString will eventually run out of memory. This is false. Those blocks will still be GC'd when all their inhabitants are GC'd. You will only leak those blocks if you're leaking bytestrings already. Sure, it exacerbates the leak, but it won't *cause* a leak.
I've only been doing haskell for about 3 years now, but I've never seen Aeson change its JSON format. I doubt this ever happens except in dire circumstances.
There is a function called `join :: Monad m =&gt; m (m a) -&gt; m a` that can turn `bearFiles` into a function that returns `IO ()`. But a better way of doing this is to replace `return (moveFiles old final)` on line 37 with `moveFiles old final`, since the function `moveFiles` returns `IO ()` anyway. Making this change will give `bearFiles` the type signature `:: IO ()`. Since `main :: IO ()`, we can replace your `main` method: ` main = do final &lt;- bearFiles final ` with ` main = bearFiles `
Would either you or /u/nh2_ be able to comment on what happens at the end of a wai/warp request/response cycle? Even if something is using pinned memory for bytestrings, wouldn't it be GCed at the end of the response, once the forkIO thread exits?
I make no claims that my new discovery is discoverable. Most of this is relatively new stuff. But if it pans out I will suggest this very thing!
Also your function `createNewDir` could also return `IO ()` instead of `IO [()]` by using the function `mapM_` instead of `mapM`. In this context, the underscore at the end of the function means '...and discard the output'. So `mapM_` is a function that applies mapM to the variables and discards the output. A couple of times you have functions that look like `(\(x, y) -&gt; someFunction x y)`. This is a pretty common problem in haskell, so we have a function called `uncurry :: (a -&gt; b -&gt; c) -&gt; ((a, b) -&gt; c)` which does this for us! Using `uncurry`, we could replace: getNewPath f b = map (\(x, y) -&gt; replaceDirectory x y) $ zip f b with getNewPath f b = map (uncurry replaceDirectory) $ zip f b A better solution might be to use `zipWith :: (a -&gt; b -&gt; c) -&gt; [a] -&gt; [b] -&gt; [c]` which performs the zip and the map and the same time. You could replace getNewPath :: [FilePath] -&gt; [String] -&gt; [FilePath] getNewPath f b = map (\(x, y) -&gt; replaceDirectory x y) $ zip f b with getNewPath :: [FilePath] -&gt; [String] -&gt; [FilePath] getNewPath f b = zipWith replaceDirectory f b and even eta reduce the function to get getNewPath :: [FilePath] -&gt; [String] -&gt; [FilePath] getNewPath = zipWith replaceDirectory but that last step might be a little beyond you for the moment. All of these are just suggestions of course, what I consider idiomatic Haskell might be considered ugly by someone else. 
&gt; I'm not sure why anyone would think that Either should be symmetric. `Either` is symmetric in the sense, that `Left` and `Right` don't have any associated meaning. Sure, in a lot of cases they have meaning by convention, that `Left` is the error case and `Right` the valid case, but `Either` is also used in cases to express this or that data, where both data represent valid data. So the `Either` data type is by nature symmetric, but its monad instance and `EitherT` aren't, because for them `Left` is always the error case. If this really is a problem is another discussion, but some people liking Haskell also have a appreciation for clarity and correctness, which is IMHO the whole reason to have something like `ExceptT`. But somehow I like the Haskell community because they care about such things. :) 
Swapping left and right in Either is acceptable. Swapping the same variables — error and result — in Except breaks its semantics.
Haskell syntax is so poor so it doesn't allow you to define `Either` in a symmetric way. You have to choose one of the arguments as "primary" and other as "secondary". This choice is arbitrary. 
I like a lot Dhall. Could it be as successful as JSON was? JSON can be seen as a non Turing complete subset of javascript, so Dhall could fill a similar spot for Haskell. In the game world, Lua is often used as a configuration language. It's Turing complete, but it's much higher level then C/C++ that is used for the low level programming. Could Dhall fill Lua spot? 
I don't understand, so using `StateT` is bad, because `concurrently statefulAction1 statefulAction2` is not well-defined. Aside from the fact that the main problem here being the absence of the semantics of `IO`, there are obvious solutions to this: 1) Design the stuff around `concurrently` such that `StateT s IO` can enter it only if `Semigroup s`, then it'll also be obvious from the type signature what happens to the alternative states. (Note how this also makes the pathological example illegal, since `Int` has no canonical `Semigroup` instance) 2) If you like mutable variables so much, fine. You can still have the action be explicit about what state it works on by letting it have `StateT s` in its type, then you can peel off its `StateT s` layer by simply wrapping it in a function that accepts `TVar s`, fetches the `s` feeds it to the action, then writes the resulting `s` back into the `TVar`. You have exactly the same semantics, but now at least you're more explicit about the state thing. Isn't this one of the fundamental tenets of functional programming? to push side effects as far out towards `main` as possible? 3) Maybe you shouldn't be working on `StateT s` anyway, maybe you need something like a combination of `ReaderT s` and `WriterT [sDelta]`, along with a fold `s -&gt; sDelta -&gt; s`, so that your concurrent actions do not manipulate `s` directly, but they emit deltas to it via `WriterT`. Then it'll be super obvious what running them concurrently means. 4) If you really really need to have multiple actions running in parallel while simultaneously modifying a common state, then you should probably model what you're doing better, since shared mutable state is no joking matter. I'd think long and hard in order to model purely exactly what's happening, write my concurrent actions according to that model and interpret the pure model in IO in production. If none of that is an option really, then fine, go ahead and use mutable state, but I wouldn't advise everyone to skip purity without thinking. Very similar reasoning holds for `ExceptT` as well BTW.
As I understand it, as long as any of the memory inside the overall block is still in use, then the block as a whole will remain unrecycled.
Excellent article! These high-level architectural blogs were sorely needed, and I'm glad we start seeing them more often. Also, hadn't seen injective type families used before : ``` class BusinessModel a where type Event a = event | event -&gt; a ... ``` I just learned that thye have been available since GHC 8.0 : https://ghc.haskell.org/trac/ghc/wiki/InjectiveTypeFamilies 
That non composability can be seen when resources have to be freed as a result of an interruption. Unlike haskell snippets, true programs usually have to stay in memory and recover from the error condition and free the resources used by the failing computation. Handling this with interruptions need a cascade of catches and re-throws or nested `bracket` or `finally` that is typical in any long running application. This code changes the structure of the program and break in in pieces of exception hell. Exception is a goto to a label. A functional program should have no goto's. The correct syntax could be: res &lt;- useResource `onException`(\x:: TypeOfException) -&gt; freeRespurce rest of the code that may produce the exception res2 &lt;- useResource2 `onException` (\x :: Type .... more code A monad with continuations could implement this semantics.
Disclaimer: I work at Clever Cloud. We have a pretty good haskell support on Clever Cloud (https://www.clever-cloud.com/doc/haskell/haskell/) I've made a small video with a scotty application: https://www.youtube.com/watch?v=TnmNoPJdQvk All you have to do is git push, we'll build and deploy your application (if you use stack). You have 20€ worth of credits for testing when you create an account, but if you need more, ping me so I can give you a discount.
All you need is a monad with polymorphic state (with a map for example) and continuations. Whit that you can program primitives for any effect: validation, exceptions , early termination,sending a man to the moon. whatever. No need for stacking monad transformers. All your program can run in a single monad.
Oh, and we support EKG, so you can have access to all the runtime metrics you collect (GC, request count, response latency, custom metrics…)
Absolutely, if this isn't your wakeup call I don't know what would be.
to the second: yes, you can implement it if you add the early termination effect to the mix. to the first: long explanation: they don't compose. No two different mtl stacks or free monads can be combined with `&gt;&gt;=` `&lt;*&gt;` `&lt;|&gt;` without more or less contortions that may reach ridiculously complicated. That makes experimented developers in other languages to run away from Haskell for their lives 
&gt; and even eta reduce the function to get &gt; getNewPath :: [FilePath] -&gt; [String] -&gt; [FilePath] &gt; getNewPath = zipWith replaceDirectory Seeing this simple code I would start questioning the utility of having a standalone `getNewPath` function instead of just using `zipWith replaceDirectory` everywhere. It depends on taste though.
[removed]
There must be a communication error somewhere. mtl classes are easily composable. {-# language FlexibleContexts #-} import Control.Monad.State import Control.Monad.Except put_length :: MonadState Int m =&gt; [a] -&gt; m () put_length = put . length data MyError = MyError Int add_1_if_lt_5 :: MonadError MyError m =&gt; Int -&gt; m Int add_1_if_lt_5 n | n &lt; 5 = pure $ n + 1 | otherwise = throwError $ MyError n composed_computation ls = put_length ls *&gt; (get &gt;&gt;= add_1_if_lt_5) Maybe you're referring to concrete monad transformer stacks? (StateT, ExceptT etc.) Free monads require a bunch of boilerplate to compose, so I'm not going to make any argument there.
Why is that odd? A `String` is a list of `Char`. That's what makes it so much easier to use than `Text`.
Thank you for your work and patience in answering questions. &gt; Persistent's separation of enttityId and entityVal is a dirty hack around the core records problem [...] But almost all tables in our schema have TWO others -- createdAt and updatedAt. Why did the following not work for you? data Entity a = Entity { entityId :: ID a, entityCreatedAt :: Time, entityUpdatedAt :: Time, entityValue :: a } 
Fun stuff! I guess you probably know this but your `T x y a b` is isomorphic to `Lens a b [x] [y]`, as evidenced by your `runT`. And you might get some mileage out of `Traversal a b x y`.
You are using the same monadic stack in both operands.
OK, if that's what you want. In the untyped lambda calculus there are no malformed terms. It makes perfect sense to apply a variable to an expression, it just doesn't reduce. Whereas here I'm not sure what it means to apply an integer to an expression. If that's allowed and means something, then fine. If it's malformed, you'd probably be better off designing your type so that only well-formed expressions can be represented.
I don't know of an implementation in Haskell. If your project can stand dealing with the Java Virtual Machine, Spark has interfaces to read and write parquet files, and sparkle offers Haskell bindings to these interfaces.
To my knowledge, there's nothing more recent. We plan on using http://hackage.haskell.org/package/sparkle to handle Parquet files (as Spark natively supports them), in case that may fit your bill too (you shouldn't go through the hassle of reading/writing Parquet unless you have a distributed application in mind, else I advise to just quickly implement a tool in Java to convert them).
&gt; I guess you probably know this Nope, I did not! Tbh I've never done a deep dive into lenses before. I've used them, but only in simple, high-level ways that didn't require any comprehension of how they're actually constructed in the first place.
Stack only obsoletes *other* legacy functionality of yesod-bin. The readme does not say that yesod-bin obsoletes `yesod devel`. And it's not just for Yesod. The readme includes instructions for how to use yesod-bin for non-Yesod web development.
Why does `ExtChar` satisfy the equality constraint `a ~ Char`? It's definitely a very different type. That would be quite surprising.
I think you have my position completely reversed. My point is that it doesn't. Before we had the flexible instance instance IsString [Char] so you could put instance IsString [ExtChar] next to it without overlap, but if you only knew it was a [a] without knowing a, the compiler would choke. Now we have instance a ~ Char =&gt; IsString [a] so the moment the compiler learns you have [a], it discovers a ~ Char. reverse "foo" is an example of something that before would just get stuck without more type information.
Or maybe use `inline-java` for this, if it's just a short function.
I am actually thinking of using this one unless there are better options. Alternatively, ORC has C++ implementation, so maybe it is possible to add a tiny C layer on top and then FFI to it... 
Whoa, generic-lens seems awesome! I'm especially impressed by how effortless Structural subtyping seems from its readme!
I don't see how the Semigroup stuff would help with concurrently.
yay, more packages using `inspection-testing`! :-)
I've always wanted this, but could never find it ^.^ The only thing that makes me a little wary is the UndecidableInstances, but very nice, otherwise.
You combine the two states using the semigroup instance, instead of doing something arbitrary like "discard the first one"
I don't see why swapping left and right in `Either` is acceptable. It breaks what you expect the `Monad` instance to do.
For now I switched to `Control.Exception` and I'm doing everything in IO. I'd still like to hear what a proper solution would be! :)
It's allowed and means basically the same thing as a variable applied to an expression.
What is this second homepage? I only know about [haskell.org](https://www.haskell.org/) and figured it was canonical.
Yeah, I don't know that it's a particularly deep connection. `ThreadId ⟿ RoseTree CommentId` (ie `Lens ThreadId (RoseTree CommentId) [ByteString] [Maybe ByteString]`) will never be a lawful `Lens`, because you can't put in what you got out - `set l (view l x) x` doesn't type check. Interesting to think about though, and I wonder if your design would look different if you'd known that before you started.
http://haskell-lang.org/
Does `liftIO` not work in this case? Assuming `myMonadCatchMethod` has the same type modulo the constraints only, runJob x y = mapReaderT liftIO (myMonadCatchMethod x y)
Small gains have already started. Larger gains are predicted. I hope they materialize, else expect a big blog-post bitching about Haskell :)
Sum of types is commutative. `Either` in its mathematical sense is not biased, but `Monad Either` is written right-biased, that's why it breaks.
Thanks I'll look into it. 
This is exactly what I was looking for. Thank you so much. 
It's just fpcomplete.com's Stack-opinionated take on how Haskell's official page should be like. It's not the official Haskell page. 
Right, it seems we collided on the same (up to isomorphism) structure, but the way we're using it is quite different. Although even that may not be completely true - if I'm not mistaken, the Traversable approach also looks basically the same. &gt; I wonder if your design would look different if you'd known that before you started. Maybe, although I'm not really "done" with the design per se. If I encounter something better, I'm happy to change it! That being said, at least from the limited perspective of the motivating Redis example, I think it does everything I need. I can't really think of any other useful ways to combine queries that aren't supported.
Yup.
It's a bit sad that these have taken three-letter package names :(
That isn't likely to be useful in most cases IMHO, although the idea of monad-specific instances of control operations (when they make sense) might be a good one.
Yes, that is also my understanding. Pinned memory (and thus ByteString memory) can leak during the entire lifetime of the Haskell _process_, so practically until you restart the entire Haskell server.
Is this it? https://vid.me/vJvm/stop-treading-water-learning-to-learn-by-edward
I've probably come across this before and not even realized because of the similarity in design. Why is fpcomplete doing this? Or rather, why do they feel like it's necessary and not immoral? I don't see how it differs from phishing. It's not as if haskell.org is biased against stack. (I'd like to see nix included as a recommended way of installing haskell though, but that's beside the point.)
&gt; only ... if you're leaking bytestrings already That depends on your definition of "leaking". They might be bytestrings you need to keep around for the remaining duration of your program as part of your normal program logic. It is almost impossible to control this. In practice, programs that process a lot of data and run for weeks do run out of memory that way (so I should probably have said "all real-world Haskell processes").
If you have two actions that share a common state and you want to run them concurrently, what happens to that state is non-trivial. Saying, let's not use `StateT` bacause it doesn't just work, and let's use mutable references instead is just pushing the problem elsewhere (not to mention somewhere very imperative), you just have to take the non-trivial decisions about how to interleave the state manipulations somewhere else, and that won't be visible in the types unfortunately. I guess `StateT` could indeed be a source of the problems, but I still wouldn't jump on mutable references. If instead of `statefulAction :: StateT s IO a` we had `statefulAction :: MonadState s m =&gt; m a`, and type M = SomeMonadThatCoordinatesDuringStateModificationT concurrentlyS :: (MonadState s m, MonadIO m) =&gt; M m a -&gt; M m b -&gt; m (a,b) `concurrentlyS` could run the two computations in such a way that it interleaves the state modifications as it sees fit, and the `statefulAction`s would essentially be implemented the same way. And the nice thing is that you'd be able to test these `statefulAction`s purely! I think the problem usually arises from specializing the types too early. Like using the `StateT` transformer instead of the `MonadState` constraint. IMHO the infamous quadratic typeclass instance issue of mtl-style is also a by-product of early specialization.
&gt; This choice is arbitrary. It's not arbitrary. The final argument to both type constructors and functions is the "least fixed". The evaluation rules, form of an instance head, even the syntax of partial application make this clear. This is actually really deeply tied to Core / System F, since neither one of them can do simultaneous binding, so anywhere it seems Haskell is binding two variables (or type variables) at once, there's actually been a choice made for which order to bind them and that choice induces asymmetry.
It's pretty crazy how available the package-name real estate still is on hackage. About a year, I realized that `ip` wasn't taken, and I was writing a [package for dealing with IP addresses](http://hackage.haskell.org/package/ip), so I used that name.
i was just examining the sized vector landscape and was finding it hard to choose. this seems like a solid option :)
It's a bit odd because they are clearly different. For example, `1 : ("" :: String)` doesn't typecheck, but `1 : []` does. If two values are equal, you should be able to substitute one for the other in any larger expression.
&gt; So the Either data type is by nature symmetric Nope, the syntax and semantics of Haskell [doesn't allow that](https://www.reddit.com/r/haskell/comments/7e8bom/slides_joy_frustration_of_putting_34000_lines_of/dq563js/).
Most of lens use is: ^. ^? ^.. %= .~ These plus understanding traverse get you very far imo. 
Wow, thanks a lot of the article! I've been lobbying to introduce event sourcing into our codebase for some parts of our persistent state, but I've been worried about how to actually store these events! I wish Capital Match open sourced their event store, just something simple like a streaming interface for reading a sequence of `ByteString`s from a file and appending individual `ByteStrings` to it, in a way that's safe to use concurrently from multiple processes, and with an API that lets you know when you can be sure that your event has actually made it to the non-volatile storage medium (and not some volatile HW buffer). It should also be impossible to corrupt that file due to power failures (short of HW damage). Maybe there's already such a library?
is bearFiles meant to be bareFiles? 
Brilliant stuff. Wish I had access to the [original article](https://abailly.github.io/posts/cm-arch-design.html) a year ago. A follow-up topic could be a detailed analysis on RDBMS vs Event-sourcing for writing new data pushed to a system.
The worst case scenario is that each bytestring held in memory requires a useless 4k memory to be kept. Therefore the upper bound is 4k * numByteStrings. Thus as long as you aren’t leaking bytestrings, this number is bounded; i.e. you can’t hold on to a constantly increasing number of bytestrings for the duration of your program without expecting the *minimum* bound (0 + numByteStrings) to also be a leak.
I should also mention that I consider Kafka to be too complicated for simple things (isn't simplicity the point of event sourcing anyway?). No need to bite the bullet for exabyte level scalability from day 1.
As a beginner, please don't touch reflex. Have you ever built an SPA in any other tech-stack before? If not, stick with HTML pages generated on the server-side. I'd recommend using yesod with it's shakespeare-templates to get the the fastest results. If you could share visual mockups (use moqups.com) for what you want to build, I could give you more targeted advice. 
Anything that you can share or give examples of? 
&gt; Merging all packages back into a single one has demonstrated up to 20% improvement in build times. This makes me sad.
Yes lol
https://hackage.haskell.org/package/vector-sized invents fewer wheels
are you building an SPA or not? If you build non-SPA, I would recommend scotty (for web routing) + blaze-html (for building HTML)
Is there a reason to prefer Peano numbers to type-level arithmetic built in GHC? It's quite capable since 7.8.
&gt; TQueues Use `TBQueue`. Don't use `TQueue`s unless you can magically guarantee that the producer will not outperform the consumer (which probably means reimplementing `TBQueue`).
Yes, the its weakness may not appear if you don't have big demands on the database. But your application may grow, and it can become a problem later. You give up scalability for a little convenience. And in a functional setting you don't have objects, so it matters even less. I agree that having a lightweight EDSL layer can be welcome, but with a full ORM it's just to easy to shoot yourself in the foot.
`toUpper` for unicode can't operate on the character level. See [this](https://german.stackexchange.com/questions/2544/what-is-the-appropriate-capitalization-of-%c3%9f/2545#2545) SO answer.
idk about to Haskell platform, but I work on Windows, and for me, `stack` (it's a build tool) successfully installs the compiler and packages. After installing stack, you can touch a stack.yaml file like this and then run `stack install` from the same directory to test it: resolver: lts-9.13 compiler: ghc-8.0.2 https://docs.haskellstack.org/en/stable/README/
Nice article! I remember reading the original a couple years ago. It inspired an early version of [my event sourcing library](https://github.com/jdreaver/eventful). I love hearing stories of folks using event sourcing with Haskell in the wild; it is such an appropriate language for the task. The more I work on an event sourcing library, the more I realize that a library can only offer a toolbox of tools to help users build an event sourced application. A framework feels impossible to build because there are so many application-specific variations in how things are done in an event sourced world. (A lot of the CQRS/ES "gurus" tend to agree that a framework is an anti-pattern in this domain.) I think the solution to get more Haskell programmers comfortable with event sourcing is to provide a solid substrate of very basic functionality, like storing/getting events and creating generic projections over events, and then filling in all the holes of "well how do I do X?" with docs, tutorials, FAQs, and code snippets. I look forward to more posts like this!
(btw, the resolver and compiler keyvals should be on separate lines, and whether it's not displaying correctly on mobile or I just didn't format it correctly, they look like they're on the same line to me lol)
That's a good question. Looks like that already GHC-7.10.3 doesn't report an non-exhaustive pattern match in {-# LANGUAGE GADTs, DataKinds, KindSignatures, TypeOperators #-} import GHC.TypeLits data Vec (n :: Nat) a where VNil :: Vec 0 a (:::) :: a -&gt; Vec n a -&gt; Vec (1 + n) a infixr 5 ::: phantom :: Vec 0 a -&gt; Vec 0 b phantom VNil = VNil -- GHC 7.8.4: -- Pattern match(es) are non-exhaustive -- In an equation for ‘phantom’: Patterns not matched: _ ::: _ as GHC-7.10.3 and later know: Could not deduce (1 ~ 0) from the context (0 ~ (1 + n)) However, you cannot write induction :: forall n. KnownNat n =&gt; f 0 -&gt; (forall m. f m -&gt; f (1 + m)) -&gt; f n induction = _ Or next :: KnownNat n =&gt; (KnownNat (1 + n) =&gt; r) -&gt; r next x = _ without relying on the implementation details. See e.g. the source of http://hackage.haskell.org/package/constraints-0.9.1/docs/Data-Constraint-Nat.html And that bothers me. I use `GHC.TypeNats` for some things, but for others (like `Vec` size) I prefer peano naturals. As I said, it's good question: when to choose either one. I don't have better answer right now.
Really appreciate this. I'll check out moqups.com since I've never built anything on the web.
Why does undecidable instances make you wary? 
never built an SPA so I'll build non-SPA for simplicity. Thoughts on yesod?
If your event volume is not very high, just using a table in a SQL database (I use postgres) is perfectly fine. There are some caveats to trying to use the primary key for global ordering though (you either need to ensure you only have a single writer or use use a full table lock on every insert), but in general it is a nice experience and it gets your feet wet with event sourcing without having to deal with EventStore or Kafka.
Do you know the status of upgrading actual records to a language feature? I just spent the weekend getting `vinyl` records to derive a `FromJSON` parser, to explore some data for which I had a loose schema. I love the CoRec's for n-ary sums and the Functor that lets you build pseudo-Applicatives, and hope that whatever records eventually make it into the language share that. But I also just want real row types, with better syntax, better inference and/or simpler error messages, that exclude duplicate labels, and with better capabilities for doing record operations (like subtracting them, mapping across them, maybe even renaming them, etc). 
It was mentioned in mail thread. It seems like nice package, and it uses `GHC.TypeLits` if you prefer them. See other comment here. I wonder, why index :: forall n a. KnownNat n =&gt; Vector n a -&gt; Finite n -&gt; a but izipWith :: (Int -&gt; a -&gt; b -&gt; c) -&gt; Vector n a -&gt; Vector n b -&gt; Vector n c Looks like 0.5.0.0 started to use `finite-typelits`, but only for indexing. :/ 
Don't feel about that. These classes are utterly incomprehensible to most.
One way to see that is: If you use a library using type-level naturals, than `GHC.TypeNats` -based would be nicer to use (more convenient, maybe faster). OTOH, when writing a library, less "magic" is better. 
It makes me sad too! Building packages is really slow compared to building modules. https://github.com/haskell/cabal/issues/4175
The *real-world* names are free, `nat` and `peano`, and `Vec` OTOH are taken, to mention a few. They offer various levels of *usefulness to others*.
I'm not sure if it meets your needs, but check out u/jdreaver's [Eventful](https://eventful.readthedocs.io/en/latest/) library. There's also [acid-state](https://ocharles.org.uk/blog/posts/2013-12-14-24-days-of-hackage-acid-state.html), which was mentioned in the post. 
For instance, if I want to put a graph in my web page that calls an API to update prices, how would I go about that? Not asking for a step by step, literally a "you would use javascript for that, check this link out"
I believe Haskell has bindings for eventstore
Come on, I think that I clearly stated what people mean when they say that `Either` is symmetric.
See if you can build the mockups. It'll hardly take me 10mins to give you a step by step guide once I understand what you're trying to build. 
What would it take to make Reflex more beginner friendly to you? Making Reflex more accessible is a personal goal of mine, but there is a lack of constructive input for me to go on.
I'll follow-up over again. Do wai/Warp, yesod, servant, or Aeson use pinned bytestrings? If threads are not sharing any values between them, will the termination of a thread result in GC of bytsetryings that it pinned? 
/u/snoyjerk want to jump in here?
I don't think reflex can be made beginner friendly. It can be made intermediate friendly, perhaps. The pain of reflex is offset only when you need its gains for a large, complicated project. Most beginners don't start off with large complicated projects. 
It's displaying fine on my desktop display. But why did you include the `compiler` value? The resolver is the only thing needed, unless you're overriding the compiler included with the resolver (which you aren't doing here).
Thanks for the suggestion. Could you elaborate on why you don’t think it can be made beginner friendly, and why you don’t think “the pain of Reflex” can be made less than it is today?
just to be explicit, since i change either a lot, but yeah, you're right
Does stack handle the C compiler too? OP’s error message implies the issue is with the C compiler, not GHC
Fair 'nuff :)
On Windows, the GHC bindist includes a mingw distribution that has a C compiler in it. FWIW, this isn't a Stack-specific feature, but part of the official GHC release. Stack _also_ ships a full msys environment to allow things like configure scripts to run, but that's orthogonal.
That's probably relevant, but I've also gotten very irrelevant error messages sometimes when installing Haskell packages on Windows. For example, commands hanging or failing with permission errors, that then succeed when rerun (and not, afaict, the standard staging issues from depending on Haskell executables). so I just ignored it and was suggesting to try something different. 
yeah, working with strings as literal list of characters is extremely convenient. the abstract Text should be the standard string type etc, but [Char] definitely has benefits. 
No, because literally every lens operator has an alpha numeric equivalent. 
It's very similar to the [overloaded-records](https://hackage.haskell.org/package/overloaded-records) package. Both are based on the GHC proposal for overloaded record fields that hasn't been finalized yet. Works really well in practice.
Do you know when the actual release is planned for? / How is the candidate coming along? I can try installing it on windows running some tests if there isn't data for that, btw. 
Yeah that was a joke that I apparently didn't make obvious enough :) It sounds like it should work, but it really doesn't.
Thanks for the elaboration! I was making a joke, but wasn't quite thick enough on the sarcasm I suppose (or I'm just bad at jokes). I really like your typeclass and smarter typeclass ideas though, that would be really nice. It's difficult to solve the problem of allowing for most efficient implementations of certain things, and I still don't know how we're going to solve "typeclass abuse" resulting in the explosion of function signatures, loss of inference, and so on. Plus, it doesn't seem to me that it's easy yet to work with very fine grained and wide hierarchies of typeclasses, so that will also be an interesting problem to solve, especially in the face of deriving classes.
One thing is that inference is very bad. If you don't throw signatures with `MonadWidget` everywhere, it is difficult to compile.
I'm running GHC on a Jetson TX2. It's basically what you said: download the existing aarch64 binary distribution of 8.2.1, and just use it to bootstrap a copy of `master`. It should work fine. There are still some bugs in GHC 8.2.1 w.r.t. AArch64 (mostly relating to GHCi/Linker support, which GHC does not use to build itself), so compilation should be stable -- I basically just immediately built `master` and started using that. I don't know OTTOMH if 8.2.2 will have the necessary patches... Note that `master` (soon 8.4) has now diverged quite a bit from 8.2 due to changes like `Semigroup` becoming a superclass of `Monoid` in `base` -- so compiling some code "in the wild" might require some changes to fix things. Compiling AArch64 GHC might actually be the least of your worries, really...
It's reflexive and not entirely rational. I've just had bad experiences with them.
Yeah I was only asking because I've had good experiences with them, unlike say overlapping instances or incoherent instances
Wouldn't it be easier to simply add project id to the tables, than create separate database per project? Have you thought about backups?
Shouldn't it be in `base`?
Is there a way to turn a data constructor into a type so that it can be used in type declarations (without making the data constructor a type and it's type a typeclass)? For example: data Bool = True | False -- magic alwaysFalse :: Bool -&gt; False alwaysFalse _ = False
Floating point is probably also why `Num` doesn't impose the ring laws, and why `Fractional` doesn't impose the field (or even division ring) laws.
Honestly I made my way through LYAH and didn't learn much because my prior experience was just python scripts (error checking in excel spreadsheets etc). I am halfway though Haskell Book which is by far the best resource so far for somebody like me. Reflex seems awesome but I feel like I need more time with Haskell itself. For instance I got servant up and running and I could do simple things with reflex like display text but I didn't have a clue as to how I would display the result of a servant request in my browser using reflex. I think the issue is that I'm just too much of a beginner to mess with Reflex. A tutorial building something in reflex that involves calls to APIs and maybe displaying that data as a graph would be insightful to me.
I did some experimentation: it comes out super clean with `Traversal`. data T a b s t = T (Traversal s t a b) instance Profunctor (T a b) where dimap f g (T k) = T $ \o -&gt; fmap g . (k o) . f instance Traversing (T a b) where traverse' (T k) = T (traverse . k) If you want to write client code which manipulates lists you can use [`unsafePartsOf`](https://hackage.haskell.org/package/lens-4.15.4/docs/Control-Lens-Traversal.html#v:unsafePartsOf) - which basically performs the same function as your `runT` - but you have to promise not to change the list's length. Generally speaking `lens` will "do the right thing" even when confronted with non-lawful lenses like the ones in your article, so you can still use `lens` to be productive. Another way of thinking about `Traversal` is as a composition of the `(a -&gt; f b) -&gt; _` functor with the `Star` profunctor. When you phrase it like that, you can see exactly how and why `T` is a `Profunctor`: data T a b s t = T (forall f. Applicative f =&gt; (a -&gt; f b) -&gt; Star f s t) instance Profunctor (T a b) where dimap f g (T k) = T $ dimap f g &lt;$&gt; k instance Traversing (T a b) where traverse' (T k) = T $ traverse' &lt;$&gt; k
See DataKinds and Singleton. But what are you using it for? If you only want to make sure the result is one of your constructor, turn it into a datatype of it's own. data Moe = Cat Meow | Dog Very Cute could be turned into data Doggo = ADoggo Very Cute data Moe = Cat Meow | Dog Doggo now if you want to make sure a function always return a Dog, just make it return Doggo. BTW ADoggo can be renamed Doggo. I use different name just to avoid confusion.
I want to use it to create functions that operate only on valid state and to create functions that can operate on both valid and invalid state data State = ValidState | InvalidState invalidateState :: ValidState -&gt; InvalidState iDontCareAboutStateValidity:: State -&gt; something
Yes, create a datatype for valid state, one for invalid state, one for either of them. Shape function accordingly.
Which error system did you go for in the end? 
Is it considered bad practice to create your own operators. For instance when a function composes multiple functions it's easier for me to reason about it piping forward with an F# style pipe, ie: (|&gt;) x f = f x func x = x |&gt; a |&gt; b |&gt; c |&gt; d |&gt; e etc 
If stumps were monoids you could write a Monoid instance for Wicket.
Nah. It seemed odd but kinda cool. Was a good read. Thanks
You can also use it for early returns. For example, using `foldM_` to find the product of all integers in a data structure, you can use `Either Int` to return 0 as soon as you encounter a 0 without having to traverse the rest of the structure.
More like standard practice, as long as they make sense and dont conflict with popular one. For |&gt;, we usually go with where, but I see no problem introducing this. func x = e0 where a0 = a x b0 = b a0 c0 = c b0 d0 = d c0 e0 = e d0
I wish internships in general were available not only for students but for all, including mid-career professionals. I believe a lot of such persons would want to experience working at various different companies, projects and countries for a limited time without abandoning their current life/career. Kind of mini sabbaticals. Why aren't companies interested in such endeavors? 
Dangit, there goes my hopes for my cool intellectual property library.
Fwiw, that exists as `(&amp;)` in `Data.Function`. Generally, I avoid custom operators unless I’m writing some kind of DSL, or the operator lives in a relatively generic type class like `Monad`. Operators are great when you know the rules that a function must follow, but not the purpose. The `Alternative` class for instance, gives use rules about how `(&lt;|&gt;)`should behave, but what it does depends entirely on the instance.
Good advice in general but yes in this context we can guarantee the consumer is so fast that the producer won’t outperform it. 
Looking at the second version, you can split that up into a stump and a List of Bails, with the List being a Monoid. So instead of adding a Bail you could also remove the stump of the second wicket
Hm, it doesn't seem to me that either of those model a wicket in a meaningful or useful way (but I'm going entirely by your description and the picture I just looked up on wikipedia). Depending on what you need to _do_ with it, here's a structure that might work: newtype Wicket = Wicket [Maybe Bail] deriving Monoid type Bail = () This allows us to represent bails that are either up or fallen (sorry for my lack of cricket terminology); there is, implicitly, a stump "between" each of the elements in the `Wicket`'s list.
Hi, Thanks for the reply! The rock64 is arriving later today, but the TX1 looks quite interesting. Thanks for the tip, I wasn't aware of it as another low cost option. Hopefully the io using emmc isn't too terrible. My initial goal is to create a trivial web service using servant. Thanks for the warning about packages that may need updating on the master/8.4 branch. In addition to getting ghc to build I'm also going to try &amp; get nix to work as well (which I'm using for x86-64 packages). If I can get that far, then if there are some small changes to packages I depend on then I can use nix to pull dependencies from a github fork and provide pull requests if that hasn't been done already. I'll give it a try and see how far the rabbit hole goes :)