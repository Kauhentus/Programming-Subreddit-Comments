Yeah, this approach has it's ups and it's downs. The main problem, if you ask me, is that functions don't belong to Eq, meaning that you can never tell if two rectangles are equivalent, with this approach. All the data stored in the function has already been closed off.
This approach is almost all "downs", in my opinion. The _only_ thing these "shapes" can do is determine if a point is within them. You better be _damned_ sure that's absolutely, positively the only question you ever want to ask of them. So, what's the natural solution with this approach? I guess we can construct a bundle of functions that answer various questions, right? So one function for "in", one function for "give me bounds", one function for "give me a series of vertexes/beziers/whatever", one function to feed to an efficient rasterizer, etc. Still makes sense, right? Well... congratulation, you've just reconstructed type classes. Type classes are _already_ a series of functions. You're already rigidly not allowed to peek "inside" the actual value outside of the typeclass implementation and can only use the defined functions to access it. Together with the fact that you can combine typeclasses and data arbitrarily, and after thinking about this article for a bit, I'm afraid I have to call it actively wrong.
In fact, 2d shapes (or 'regions'), images and even 3D surfaces are often represented by functions like Point -&gt; Bool, Point -&gt; Color or Point3D -&gt; SignedDistance. (By often, I mean there is quite a lot of papers that use that representation; sorry no title in mind right now :( ) Edit: not a paper, but nice (and quite unknown): [Pancito](http://www.acooke.org/pancito-code/)
kernel probably not, but decoder maybe.
This approach is not so great if you actually implement it this way. It's *very* nice if you use it as a semantic model for your API though. Consider generalizing the model to `type Image a = Point -&gt; a`. You have obvious meanings for Functor, Applicative, Monad, Monoid, and all kinds of other type classes and functions. These can guide your interface and, thus, your implementation.
And [Pan](http://conal.net/Pan). See [the image gallery](http://conal.net/Pan/Gallery/), and then [papers](http://conal.net/Pan/papers.htm).
Yeah, I went though a phase when I was unwittingly reinventing classes at every turn in Ocaml. I had a clever system where you could send a polymorphic variant to a single function and it would dispatch the value to various other functions which would return the current value or a modified value. The dispatch function would then return itself with the value curried in. Something like this: let rec dispatch v msg = let newv = match msg with | `Foo arg -&gt; foo arg | `Bar (arg1, arg2) -&gt; bar arg1 arg2 in dispatch newv This dispatch function is basically an object. You instantiate it with `dispatch (createCircle point radius)`. Any objects that respond to the same messages are the same type and can be put in a list. It could also support an _ case that ignores any unsupported messages. I realized what I was doing after a while. It's silly. Ocaml supports classes. Just use them if you really must have dynamic dispatch. However, it's quite possible that you don't. The functional approach is to use a simple variant type and some pattern matching, if you ask me, a lowly Ocamler. Both approaches are pretty much equivalent but have different strengths. Classes allow you to add new types more easily. Variants allow you to add new functions more easily.
&gt; I'm afraid I have to call it actively wrong. Eh, I wouldn't go that far. There are times when you don't need most of those type classes. Geometric shapes is a bad example. I've used this approach for cellular automata. Because each unit could affect it's adjacent units, I simply modeled this transformation as a function along with the cell's visible state. Totally straight forward. So the end result was an array filled with functions (that operated on that array). The alternative here would be to encode each behavior into data, and store that along with the state of each cell into the array. I liked the latitude that I could use any old function and assign it to any cell I liked.
Excellent. Agda is very fun to play with, primTrustMe.
maybe I am missing something but why not just create type called `Shape` like so: data Shape = Rectangle Point Point | Circle Point Float and then `inShape` can simple do some pattern matching... inShape (Rectangle x y) = ... inShape (Circle x y) = ... 
The lack of readability of the code above has nothing to do with cryptic variable names and everything to do with cryptic function names, particularly in this case `#` and `%`. Furthermore, it's a mathematical function, which means understanding it requires understanding the math. The instructions themselves are not particularly convoluted. I don't think there's anything wrong with reusing `x` and `y`, and find `(x:xs)` as an idiom to be no more confusing than the pervasive use of `i` and `j` as counter variables in most imperative languages.
he addresses this in a footnote
Noice!
Since it's already here, I have a couple of questions for you Haskell people: * Is there a search engine for functions given a signature? Say, you write (a-&gt;b)-&gt;(a,a)-&gt;(b,b) and it gives you the name of the function (I know, it's `fmap` on the pair functor) * Is there an equivalent of my `split_all`? Remco's code is usually very tight, so it was somewhat puzzling to see `init $ zip (inits xs) (tails xs)` written in full, as it seems to me to be a more-than-semi-useful function Thanks in advance.
I think [Hoogle](http://www.haskell.org/hoogle/) is what you're looking for. I do not know of any ready-made haskell equivalent of your neat function.
link to [itunes](http://iTunes.com/apps/IcyWiki) There's another one for wowwiki, which does the same thing ..
check this: http://hackage.haskell.org/package/split
Readability is mostly relative, anyway. I find Japanese to be completely unreadable. I'm sure I'd have more luck if I was actually familiar with the language. EDIT: To be fair, I'm not saying that people who find Haskell to be unreadable are simply inadequately experienced. I just think that readability concerns tend to boil down to what a person is used to and how they write their code.
&gt;while we're speculating on languages of the future Javascript will be the new C. You serious?
To answer your first question indirectly: join (***) Neither Hoogle nor Hayoo would come up with that as far as I know, but those are the two tools to try for the functionality you want. Djinn, however, would come up with a function definition for it, then you could pipe that through Pointfree... [Edit: Djinn and Pointfree give horrible results: `flip ap snd . (. fst) . (const .) . (ap =&lt;&lt; ((,) .))`] And no, it's not `fmap` on the pair functor. `fmap` on the pair functor has type `(a -&gt; b) -&gt; (c, a) -&gt; (c, b)`.
It's not pointfree, but `(\f -&gt; f &amp;&amp;&amp; f)` seems a bit more natural than using `(***)`.
Hoogle is excellent, but it won't get everything since it doesn't index non-portable packages (last I heard; there was talk of changing that, but I don't recall how it was resolved). [Hayoo](http://holumbus.fh-wedel.de/hayoo/) is the other main API search service.
I think you mean `(\f -&gt; f *** f)`.
sure, I'll blog more about it ... starting point is http://projects.haskell.org/ghc-iphone/ try to compile against the latest iPhone SDK, undefine everything alone the way that doesn't compile..
I always have this definition handy: both = join (***) to match those of first, second, etc. init $ zip (inits xs) (tails xs) could be written as: init . liftA2 zip inits tails 
May I ask what your code was before the edit?
I am intrigued by dependently typed languages. I was always wondering, though, about the way naturals were defined (Zero, Succ) -- and the possibility to compile those into code that makes efficient use of hardware's numeric support. Are the definitions of naturals and various functions duplicated by ones optimized for numeric hardware support? If so, how are they proven to match the exact behavior of the inductive definitions? 
For some reason I read the request as `(a-&gt;b)-&gt;a-&gt;(b,b)`. For `(a-&gt;b)-&gt;(a,a)-&gt;(b,b)` the `join(***)` version is fine and natural.
&gt; And no, it's not fmap on the pair functor. fmap on the pair functor has type (a -&gt; b) -&gt; (c, a) -&gt; (c, b) That's what happens when you post without the typechecker at hand `;-)` I realized that it should have been applicative `first` (or `second`for that matter) while falling asleep.
In Agda, (many) actual computations on naturals are turned into operations on whatever the underlying Haskell Integer representation is (and since Agda probably only works with GHC, that means GMP) so long as you use the right pragmas. The underlying representation is more or less assumed to be correct, which isn't that terrible in the case of natural numbers. You could, of course, define representations that are closer to an efficient representation, and prove that they're equivalent to Peano numerals, but even those would be far slower than something like GMP, or hardware ints, so currently, you probably have to just hope the efficient implementations don't have bugs.
Yes, Hoogle does it! It seems that the documentation path changed somehow, and links to it are broken. Hoogle links to a path of the form: http://haskell.org/ghc/docs/latest/html/libraries/base/ but the documentation lives in http://haskell.org/ghc/docs/latest/html/libraries/base-4.2.0.0/ A link would fix that, I think. Where can I submit a report for this?
Also, I can't discern which specific instances are used in resolving the type of `join (***)`. I suppose it should be `Arrow (-&gt;)` but I don't see the `Monad` instance, unless it's `Monad (-&gt; r)`. Is there a way for GHCi to show the relevant classes?
Sure, I'll be humble: sequence . repeat 100 Which should have been: sequence . replicate 100 Which is better expressed as: replicateM 100
I'm not sure about getting GHCI to show it to you, but your guess as to which Monad instance it uses was correct. `join` in the `(-&gt;) r` monad is the same as `f x -&gt; f x x`.
That's awesome. Cross "Objective C" off the list of reasons not to write an iPhone app!
I don't understand the headline. Edit: turns out "pure data" is an application that can be extended with "externals". 
Hmm? I wasn't reacting to the picture (which is funny), just to your point that articles about Haskell make Haskell seem difficult. Yes, it seems difficult to someone who is unfamiliar with it. But none of us popped out of the womb knowing what a monoidal functor is.
Download URL: * http://ecn.channel9.msdn.com/o9/ch9/6/5/6/3/1/5/C9LecturesMeijerFPC13_2MB_ch9.wmv
Ugh this guy again. He is a terrible presenter.
It's really no biggy to click on the main link. It's not like its mass farm of adds as many other sites are. Also many people prefer the other file formats.
[some notes on compilation](http://jinjing.funkymic.com/2009/12/25/A-few-notes-on-compiling-GHC-iPhone-against-latest-SDK)
Haha, will try. :-)
No.
aye aye
actually, where do you get them from?
Where are you going?
Many of them can be found on [Planet Haskell](http://planet.haskell.org), though I'm not sure Don gets them from there.
Yeah, I know, not exactly recursive... ;)
But we thought your idea of vacations was coding some Haskell ... ;-)
OK, I'm off to write a monad tutorial!
Not parallel stuff per se... this isn't at all related to that other language really.
Someone should write an article about what dons' is doing on his vacation.
Don't forget to come up with a terrible metaphor first.
ESHP - Extra Sensory Haskell Perception
Exactly. He's off coding and is taking a vacation from posting articles.
Very cool. Does it work offline, i.e. does it come with a copy of the Hoogle database? Or does it need a network connection to talk to http://www.haskell.org/hoogle/?
The library is well named to do all the "heavy lifting" for implementing neural networks... titter... hee hee... hahahahaha... I'm so funny.
Monads are like gifts.
Monads are like endofunctors that generalize closure operators on posets to arbitrary categories. Oh wait...
Have a nice vacation!
I couldn't get this to install on OS X 10.6, GHC 6.12.1 using cabal install. There's a known dependency issue in cabal install that forces one to separately install alex, language-c, c2hs, as these are tools, not libraries. However, c2hs failed to compile for me: src/C2HS/C/Info.hs:117:53: Not in scope: type constructor or class `CLDouble' I didn't try very hard after this error, just posting my experience as a heads up.
Cross-posted from proggit. The example given for an operation that's hard in a purely functional setting but easy in C doesn't immediately sound like something I've ever wanted to do in my imperative career (but that could be a failure of imagination). I'm guessing that "a function" was not an appropriate metaphor for the phenomena being described (do I get the captain obvious award for this one?). I haven't gone trough the linked back-story, so maybe I don't have the full picture
The CLDouble type went away from 6.10 -&gt; 6.12. Here's a link to the discussion on the c2hs mailing list: http://haskell.org/pipermail/c2hs/2009-December/000983.html 
I'm sorry, but no. That example doesn't break the entire paradigm and it sounded like poor programming practice anyway.
Monads are like... sushi?
Interesting, but releasing this as GPL-3 is a rather severe restriction. I could be wrong, but I doubt this could be directly monetized through e.g. dual licensing, and a full GPL could be a significant impediment to adoption. It would seem to me that LGPL would be a much better choice for this particular bit of software.
Ugh, what nonsense.
How does this differ from MissingPy? Can anyone explain?
Not that I understand how you get any numbers out of that :(
I don't get it.
http://www.haskell.org/haskellwiki/Image:Haskell-logo-revolution.png
Oh, haha, thanks. I feel stupid now.
Everything which depends on c2hs is broken in 6.12, until a new version of c2hs can be released. I'm sticking with 6.10 on most systems until broken libraries are updated. One note for building under OS X: the Python framework doesn't place its pkgconfig files in the standard location, so you'll need to add something like this to your `.bashrc`: export PKG_CONFIG_PATH="/Library/Frameworks/Python.framework/Versions/3.1/lib/pkgconfig/"
This question pops up again! I'm surprised; didn't know anybody used / cared about MissingPy until now. [I answered this on the mailing list](http://www.haskell.org/pipermail/haskell-cafe/2009-December/071346.html), but for completeness sake: MissingPy left a lot of the C API unbound; while it makes a limited set of primitives available (such as some of the Object protocol), using those is slow and verbose. For example, compare: -- emptySet :: IO Set emptySet = callArgs (getItem (getAttribute (importModule "types") "__builtins__") "set") [] &gt;&gt;= fmap fromJust cast emptySet = toList [] &gt;&gt;= iterableToSet I'm also trying to bind the module creation functions, which will allow extension modules to be written in Haskell.
I'm not trying to monetize this, and I don't care how many people adopt it. Using the GPL allows other Free software to use it, while offering an incentive for proprietary developers to liberate their source code.
If you care about people's freedom you should let us do whatever we like with your code. But that's your decision. 
I care about developer freedom, but I care about user freedom *more*. You may have some of my rights, so long as you pass them along.
Enemy of the ST
MissingPy is usually used just for scripting Python apps in Haskell, or for providing Python bindings to existing functionality in Python as the FFI does for C. For that, all you need it to get your hands on the particular function you need in Python and call it. And marshal data back and forth between Python and Haskell. MissingPy is fine for that - although it is still at Python 2.5, and forces you also to pull in all kinds of "missing" stuff that isn't really missing anymore. The author of this new package is aiming at actually programming using the full CPython API - with the eventual goal of writing Python modules in Haskell instead of C. And it targets the current version of Python. So this package could have served as a nice replacement for the popular uses of MissingPy as well. Unfortunately, the onerous licensing restriction renders this package pretty much useless in the Haskell world, as others have pointed out. Let's hope MissingPy gets a much needed update soon.
That example isn't exactly fair - MissingPy dates back from before the set built-in even existed in Python. An updated MissingPy would certainly provide a nice interface for marshaling between Python sets and Data.Set. In fact, the interface would likely be even simpler than yours - just use Data.Set as an argument to a Python function, and *vice versa*, and the right thing will happen automatically. But obviously, doing anything more than just calling a Python function and marshaling the basic built-ins would be much easier and nicer using your package.
Also, this code would be more painful to work with. Compare adding "Triangle" to both versions.
I'm a huge GPL fan, but releasing a library under the GPL is, I believe, a fairly big mistake. I see the GPL as much more useful for applications than for libraries, especially low-level libraries. I'd say I'm pretty productive as a Haskell and general open source author, and I deliberately released all of my Haskell libraries under the BSD license to ensure that they see use by as many people as possible. I am about 99.99% certain that nobody will ever create proprietary forks of those libraries, and even if they were to do so, I'm not sure I'd care. It seems to me that jmillikin is guaranteeing that his library will be used by almost nobody, which is a shame and a waste of effort.
The [earliest release of MissingPy on Hackage](http://hackage.haskell.org/package/MissingPy-0.10.0) was released 2008-05-09. [Python 2.4](http://www.python.org/download/releases/2.4/), which included the `set` and `frozenset` types, was released 2004-11-30. That example isn't specific to sets, anyway -- any case where the type is implemented in C will be easier when the C API is directly exposed. For example, `complex` is easier to create using my library than `decimal.Decimal`. This isn't really intended to be a competitor to MissingPy -- I just wanted to use the Python C API in Haskell.
Interesting site. In addition to potentially clarifying existing solutions, further work can be done on the [Tasks not implemented in Haskell](http://rosettacode.org/wiki/Reports:Tasks_not_implemented_in_Haskell)
It is especially bothersome, to my mind at least, that your library has a more restrictive license than the python interpreter itself. We _can_ bundle python into a product and sell it, we _can_ bundle the GHC RTS and sell it but we _cannot_ bridge the two with your library and sell it. I cannot fathom how this does anything other than arbitrarily restrict the ecosystem, to say nothing of protecting user freedom. Can you explain how the GPL protects user freedom more than a more liberal license, especially given that your code has no applicability outside of development? It seems to me that the user you're protecting is the developer you're restricting to protect the user from the, possibly malevolent, developer. Nifty library though. Shame I can't use it.
&gt; The earliest release of MissingPy on Hackage was released 2008-05-09. Python 2.4, which included the set and frozenset types, was released 2004-11-30. MissingPy existed *long* before Hackage came into being. The development of the Python set type - to which I contributed - may have been happening around the same time or a little before MissingPy first came out, I don't exactly remember. But I was referring to when set became a first-class mainstream built-in, which was in Python 2.5 iirc. Anyway, that was just a side point. Your package is much more than the MissingPy bindings. And yes, it could ultimately become their replacement. Nice work.
Maybe not with dons specifically, but I'd imagine that eventually, he'd end up kind of like the newspaper: people come to him with stories. I know that if I did something interesting with Haskell, I'd probably want to talk to dons about it. 
&gt; Can you explain how the GPL protects user freedom more than a more liberal license, especially given that your code has no applicability outside of development? If a user receives an application under the GPL, then they either possess the source code to their copy or have a legal means to obtain it. There is no such protection under other options, such as the BSD or X11 licenses. Additionally, using the GPL for reusable code gives Free software an advantage over proprietary software. See [Why you shouldn't use the Lesser GPL for your next library](http://www.gnu.org/licenses/why-not-lgpl.html). &gt; It seems to me that the user you're protecting is the developer you're restricting to protect the user from the, possibly malevolent, developer. Everybody who obtains a compiled version of this library is a user of the library. The GPL protects them not just from malevolent developers, but also repackagers, maintainers, and other middlemen. &gt; Nifty library though. Shame I can't use it. Sure you can, if you're willing to give your users the same rights I give you.
FWIW, 12% of hackage is LGPL, ~4% is GPL, and 70%+ is BSD3.
&gt; If a user receives an application under the GPL, then they either possess the source code to their copy or have a legal means to obtain it. There is no such protection under other options, such as the BSD or X11 licenses. True enough, though I don't see the advantage here. Clearly you, the author, have a special interest in maintaining the availability of the library's code. &gt; Additionally, using the GPL for reusable code gives Free software an advantage over proprietary software. Dubious. The essay you've linked claims this advantage: "If we amass a collection of powerful GPL-covered libraries that have no parallel available to proprietary software (...) [those wishing to use these libraries will] begin to consider making software free, even some commercial projects (...) [and] free software as a whole will stack up better against the competition." Such coercive communitariamism is meant to counter "the advantage of money". Unfortunately, the essay is underpinned by a false dichotomy: a software developer is either entirely proprietary or FSF free. BSD developers, desiring maximum utility of their work--deplored as an issue of mere ego, as the essay puts it--are left entirely out in the cold, or lumped, unceremoniously and incorrectly, into the proprietary camp. A BSD developer does not have "the advantage of money" but is still assumed outside of the community, an heretic individual that cannot be tolerated. Harsh, unjust and, I assert, more eroding of my freedom as a user and producer of software. A strong claim could be made, as well, that arbitrary and incompatible distinctions in licensing harm the overall software ecosystem, maximizing intangible freedom while degrading tangible quality and variability. But, then, you "don't care how many people adopt it." &gt; Everybody who obtains a compiled version of this library is a user of the library. The GPL protects them not just from malevolent developers, but also repackagers, maintainers, and other middlemen. I still fail to see how the user is protected. Also, it seems overly cynical to consider the user laudable and all else as despicable churls. &gt; Sure you can, if you're willing to give your users the same rights I give you. No, I can't. Without a link exception, I can't include your library in a binary containing the GHC RTS, even if my code were GPL. Moreover, I fail to see how publicly distributed BSD licensed code would not grant _more_ rights to an end-user.
&gt; A BSD developer does not have "the advantage of money" but is still assumed outside of the community, an heretic individual that cannot be tolerated. BSD-using developers have made their choice, much as GPL-using developers have. I use the GPL, which means I get to use libraries such as Readline and Qt but cannot use OpenSSL. We are inherently outside each-others communities because the communities are separate. Perhaps eventually the two camps can reconcile, but until then, the most I can do is continue to release software under the license I believe to be most appropriate. &gt; I still fail to see how the user is protected. Also, it seems overly cynical to consider the user laudable and all else as despicable churls. Yes, yes, in a perfect world, everybody would publish source code anonymously into a global public-domain repository, and every binary would contain a "view source" button, and adorable baby animals would rain from the sky, and so forth. I think everybody would agree with you that the GPL is quite cynical, and that we'd all be much better off if it wasn't necessary. However, the best way so far discovered to discourage despicable churls is to have in place defenses (such as the GPL) against them. &gt; Without a link exception, I can't include your library in a binary containing the GHC RTS, even if my code were GPL. Of course you can -- the GPL explicitly affirms the right to link with system libraries. Otherwise, distributing Darcs binaries would be illegal. &gt; Moreover, I fail to see how publicly distributed BSD licensed code would not grant more rights to an end-user. The end user is not guaranteed access to the source code for the software they possess -- all they can do is go online, find my source, and hope their version has not been modified.
We obviously have different goals when releasing libraries. When I release a library I want as many people to be able to use it as possible. The purpose of my release is to get it used, after all. You have some other purpose and that's totally your prerogative, but I would most likely never be able to use your library. 
Ahh, a religious war emerges. The elephant in the room for me is why isn't such a binding a "computation" rather than a custom-tailored package with its own license. The progress in mathematics has been that over time, theorems become computations; as we learn to compute more, the line in the sand moves for what is original enough to claim credit, rather that an exercise. Seems parallel to writing bindings libraries. I've contemplated writing or admired various bindings libraries. The thought is always the same: Has someone already written X -&gt; Y bindings, where the grammar for Y is so accessible that one could lift it to Haskell in a single programming session? Oh yeah, and a friendly licensing environment for the work one is lifting. At some point in the future, it will be embarrassing for languages like Python to not present an interface for their libraries that is a cake-walk to mechanically translate into bindings X -&gt; Z for all other languages Z. But alas, we're not there yet. Nevertheless, I dream of a Haskell package that outputs bindings libraries. License-free results of computations, so no brave soul has to sink years into, say, the OpenGL binding I depend upon. Or start a religious war with the license terms of their hand work.
&gt; No, I can't. Without a link exception, I can't include your library in a binary containing the GHC RTS, even if my code were GPL. Not true, GHC is released under a BSD licence, and there is nothing preventing you from distributing GPL code linked with GHC's run time system.
That is more or less my thoughts as well. Personally, I'd like to see the AGPL added to the "officially blessed" licenses available on Hackage, instead of having it listed as "Other" or "AllRightsReserved" and having to dig to find out it's AGPL. The AGPL, like the GPL, makes sense in some contexts. 
I never thought there would be so many people complaining about the GPL -- it's not like I released it on Codeplex. If you'd like a straightforward mechanical binding of a well-designed library, a small preprocessor will get you 90% of the way there. For example, some of the GNOME libraries come with API description files in S-expression format which can be parsed and printed directly to .hs files. But libraries which aren't designed with binding in mind, such as Python's C API, need a bit of fiddling and extra shim code to get reasonable Haskell. Even now it's rather awkward, mostly because everything is in IO. &gt; Has someone already written X -&gt; Y bindings, where the grammar for Y is so accessible that one could lift it to Haskell in a single programming session? I think "Y" is called "C".
&gt; We are inherently outside each-others communities because the communities are separate. Why though? And why, if the essay you linked provides such a valid justification, does it rely so fundamentally on an equivocation? The advantage of GPL libraries, which I admittedly think is dubious at best and fascistic at worst, would seem to restrict the actions of users in order to preserve their "freedom", provided that the user limits themselves to a very specific range of actions. Less choice is freedom? &gt; Yes, yes, in a perfect world, everybody would publish source code anonymously into a global public-domain repository, and every binary would contain a "view source" button, and adorable baby animals would rain from the sky, and so forth. I think everybody would agree with you that the GPL is quite cynical, and that we'd all be much better off if it wasn't necessary. This is a very flippant response to a point of contention put forward quite seriously: How is the user protected, in what way and from whom? &gt; However, the best way so far discovered to discourage despicable churls is to have in place defenses (such as the GPL) against them. How are you quantifying this? &gt; Of course you can -- the GPL explicitly affirms the right to link with system libraries. Otherwise, distributing Darcs binaries would be illegal. Quite so, I was incorrect here. It is difficult to keep a diet kosher without a Rabbi. &gt; The end user is not guaranteed access to the source code for the software they possess -- all they can do is go online, find my source, and hope their version has not been modified. I quite understand this, but it is irrelevant. I asked this question, more or less: How is this ability worth the sacrifice of other end-user actions? 
&gt; Ahh, a religious war emerges. Presumably I am one of the warriors, or the only. I would like to state that I'm not attempting to fight any battles but am, instead, merely attempting to understand a set of assumptions leading to a conclusion that are foreign to me. I don't comprehend building software for public release without the intention of wide applicability (beyond the basic joy of creation, of course). I would like to. Direct questioning is, in my experience, the best method of bridging such gaps in my knowledge.
The application has a very large size (&gt;10 MB) for iPhone standards. This is an important concern, e.g. it cannot be installed over the 3g connection. Is this due to the size of the binaries generated by ghc?
partially yes, the local index files contributed half of the size in the bundle. Without this cache, an app can be as small as 6 mb. Thanks for pointing out!
Also on [hackage](http://hackage.haskell.org/package/GPX). It would be nice if this were integrated with my [GPS](http://hackage.haskell.org/packages/archive/gps/0.4.0/doc/html/Data-GPS.html) library (which could use some cleaning, but I like the general concept). Perhaps I'll ask Tony if he'd accept a patch.
this seems like a fun problem with which to try the "tying the knot" technique
alternatively: let dropAt l n = take (length l - 1) . snd $ splitAt n (cycle l)
And this is how I lose the first hour of my day at work: let dropAt l n = take (length l - 1) . drop n $ cycle l
Fun links to older blog posts covering similar topics: http://www.randomhacks.net/articles/2007/02/22/bayes-rule-and-drug-tests http://blog.sigfpe.com/2007/02/monads-for-vector-spaces-probability.html http://blog.sigfpe.com/2007/03/monads-vector-spaces-and-quantum.html Also related: http://web.engr.oregonstate.edu/~erwig/pfp/
Also some other references, helpfully provided by Stephen Tetley: * http://web.engr.oregonstate.edu/~erwig/papers/abstracts.html * http://web.engr.oregonstate.edu/~erwig/papers/PFP_JFP06.pdf * http://hackage.haskell.org/package/probability * http://hackage.haskell.org/packages/archive/probability/0.2.2/doc/html/Numeric-Probability-Distribution.html I did not suppose that the probability monad was new. But the Erwig paper (2006) refers to a pure-mathematics formulation from 1981!
You work on New Year's Eve? :) I thought I was the only workaholic here :D
not the whole day ... in fact, I'm probably going to leave right now
This is amazing. I wish it were easy to come up with these sorts of algorithms.
 minMaxEle :: [FilePath] -&gt; IO (Double, Double) minMaxEle = fmap ((minimum &amp;&amp;&amp; maximum) . (maybeToList . ele =&lt;&lt;) . (wpts =&lt;&lt;)) . readGpxFiles really? at first I was like "fmap ((minimum &amp;&amp;&amp; maximum)" is the kind of code that looks good and if types match everything is fine. but then I saw ". (maybeToList . ele =&lt;&lt;) ."
Why not write a high-level library in Haskell?
In the article, he mentions the following program, which he attributes to Albert Y.C. Lai: play = execState pro [] pro :: State [Bool] () pro = do pro s &lt;- get put (True : s) About this program, he states the following: &gt;Instead of tail recursion, this employs head recursion. The question is, what does `pro` return when you run it, and why? I found it easy to guess the correct answer, but my reasoning was completely wrong. Of course, if viewed through a wholly imperative mindset, this leads to non-termination, but the lazy state monad extracts usable information from this definition. Intrigued by this, I decided to follow the function's evaluation. It turned out to be pretty interesting, so I thought I'd share the exercise here. First, let's rewrite `pro` in desugared form (i.e., without `do` notation). `do` notation looks imperative, which invites the casual reader to "evaluate" it as if it were an imperative program. In this example, that causes us to run into problems. So let's avoid that pitfall entirely by getting rid of it. pro = pro &gt;&gt; get &gt;&gt;= put . (True:) Now, `(&gt;&gt;)` is just sugar for `(&gt;&gt;=)`, so let's substitute that: pro = pro &gt;&gt;= \_ -&gt; get &gt;&gt;= put . (True:) Let's also break this apart slightly, with the following helper definitions: pro = pro &gt;&gt;= part1 part1 a = get &gt;&gt;= part2 part2 a = put (True : a) I could have written `part1` and `part2` point-free, but in this case the pointed definitions will make the derivation clearer. Ok, so when we type `play` in ghci, it gets executed -- but what does that mean, really? Remember, in a lazy language like Haskell, only things that are *needed* are executed. So, when I type `play` at the ghci prompt, it notices that the return type of `play` is `[Bool]` and attempts to print each item of the returned list in sequence, via `print` and ultimately via `show`. Because Haskell is lazy, it will request the head of `play` and leave the tail unevaluated, print the head, and then repeat the process for the tail of the list. Because of this lazy behavior, as long as it can get one element from `play`, it can print something. If the language were strict, it would try to evaluate `play` in its entirety *before* printing it, which would result in non-termination. This is all pretty basic stuff, conceptually. So let's move on to the evaluation. Before we begin, we'll need a few definitions from `Control.Monad.State.Lazy`. First, the definition of the lazy state monad itself: instance Monad (State s) where return a = State $ \s -&gt; (a, s) m &gt;&gt;= k = State $ \s -&gt; let (a, s') = runState m s in runState (k a) s' Along with it, we'll need `get` and `put`, and of course `execState`: instance MonadState s (State s) where get = State $ \s -&gt; (s, s) put s = State $ \_ -&gt; ((), s) execState m s = snd (runState m s) Now we're in a position to "evaluate" the definition of `play` to see why it works the way it does. For those of you that are newbies, this kind of equational reasoning (which is generally not easily done with imperative languages) is a very good way to get an idea of how Haskell evaluates things. Furthermore, keep in mind that the rules for evaluation are well defined, much as the rules of algebra are. Can you imagine a compiler carrying out the following simplification algorithmically? It's precisely this sort of thing that allows GHC to carry out some frightening optimizations. Remember, all we're trying to do is reduce `play` into a form that looks like `head : tail` -- once it looks like that, ghci will be able to print `head`. So: play = execState pro [] from the definition of `execState`, = snd (runState pro []) and the definition of `pro`, = snd (runState (pro &gt;&gt;= part1) []) Now what? We substitute the definition of `(&gt;&gt;=)` from the lazy state monad, like so: = snd (runState (State $ \s -&gt; let (a, s') = runState pro s in runState (part1 a) s') []) `runState` is just the deconstructor for `State`, that is to say, `runState . State` is the identity. So, = snd ((\s -&gt; let (a, s') = runState pro s in runState (part1 a) s') []) Now we have a function `(\s -&gt; ...)` being applied to an argument, `[]`. We apply it (this is called *beta reduction*), getting rid of the lambda term and substituting `[]` for all `s`: = snd (let (a, s') = runState pro [] in runState (part1 a) s') Now we substitute in the definition of `part1 a`: = snd (let (a, s') = runState pro [] in runState (get &gt;&gt;= part2) s') Because `part1` didn't use its argument, `a` is no longer used anywhere. Therefore, we can rewrite the pattern-match `(a, s') = runState pro []` as `(_, s') = runState pro []`. In fact, that's the same as `s' = snd (runState pro [])`. Where have we seen that before? Why, that's the same as `s' = execState pro []`, which is in turn the same as `s' = play`. So we can start to see the recursion emerging here, but we're not quite there yet. Substituting `play` for `s'` and dropping the `let`, we get: = snd (runState (get &gt;&gt;= part2) play) Again, we expand this out using the definition of `(&gt;&gt;=)` for the lazy state monad, = snd (runState (State $ \s -&gt; let (a, s') = runState get s in runState (part2 a) s') play) Again, collapsing `runState . State` and beta reducing the lambda term we get = snd (let (a, s') = runState get play in runState (part2 a) s') Substituting in the definition of `part2 a`, = snd (let (a, s') = runState get play in runState (put (True : a)) s') Now, we substitute in the definition of `put`, = snd (let (a, s') = runState get play in runState (State $ \_ -&gt; ((), True : a)) s') Again, collapse `runState . State` and beta reduce to give us = snd (let a = fst (runState get play) in ((), True : a)) Now we can evaluate `snd`, because we've simplified its argument down to a pair. Let's substitute out `a` while we're at it. = True : fst (runState get play) At this point, ghci can print out `[True,` (and it does). Let's simplify this further though, using the definition of `get`, = True : fst (runState (State $ \s -&gt; (s, s)) play) = True : fst (play, play) = True : play At this point we've reduced the definition of `play` down entirely to just play = True : play ... which doesn't involve the state monad at all, and is rather easier to understand. As an exercise, carry out the same evaluation using the definition from `Control.Monad.State.Strict` instead, being careful to allow `seq` and friends to force you to evaluate things you don't actually need. It's not difficult to see that this results in non-termination before printing anything. 
eh?
Feel free mate.
Has anyone watched the earlier episodes? Are they worth a view, or should I find keep reading my RWH? 
We've got [LambdaCube](http://www.haskell.org/haskellwiki/LambdaCubeEngine), and [GPipe](http://www.haskell.org/haskellwiki/GPipe) is a very, very cool way to push pixels. Rumours go that Conal is hacking away on a successor to fieldtrip using CUDA, so things will stay interesting.
This looks like it's correct, a few comments: First, your argument is a denotational equivalence; the definitions may or may not be operationally equivalent (for various levels of "operational equivalence"). For example, compiled `ghc -O`, the original `play` creates a cyclic structure, whereas interpretation or compilation without optimization does not. By contrast, `play = True : play` is always cyclic in GHC. Second, there is no `seq` in the strict state monad... in fact you don't need it to force evaluation. I find this function useful from time to time: demandList :: [a] -&gt; () demandList [] = () demandList (x:xs) = demandList xs Once `()` is demanded, this will force the evaluation of the spine of the list. This may or may not cause the evaluation of the elements of the list: they won't be evaluated unless their evaluation is needed to evaluate the spine. Now, you cannot *ensure* the evaluation of the elements without either using `seq`, or knowing something about the type of the elements, which makes `demandList` less polymorphic. It's never absolutely necessary to use `seq`, although it can be clumsy to avoid it. In general, Haskell's pattern matching forces evaluation of something, unless the pattern is irrefutable. It should be noted that pattern matching in `let` and `where` clauses is irrefutable by default. So, you can switch from a lazy state monad to a strict state monad simply by changing the `let` that appears inside of `&gt;&gt;=` into a `case` statement. Finally, the order of evaluation is a non-local property in Haskell, as it is driven by the consumer. So in many cases it's impossible to know how evaluation will proceed without also knowing how the result is used. Jones and Gibbons' labeling algorithm is a good example of a particularly non-trivial case of this phenomenon. :)
Yes! May it be full of Monads, Monoids, Applicative Functors, and the occasional Zygohistomorphic prepromorphism!
The idea is that the recursion magically subdivides itself until it is sure that the approximate value for the integral is within some bounds given by the user.
Miller Puckette's [Pure Data](http://puredata.info/) (Pd) is one of the most popular languages for audio/visual programming, using a graphical environment to describe data flow between boxes that perform operations. The boxes can be built-in functions (like arithmetic operators), "abstractions" which are written in Pure Data and contain "inlets" and "outlets" (basically re-useable blocks of Pd code), or "externals", which are exactly like the built-in operations but live in dynamically loaded .so/.dll files.
This post contains almost no information, which is quite strange since the author wrote the ogre bindings. Let me outdo it with minimal information content: Haskell+OpenGL plays very nice, you can do [things like this](http://www.youtube.com/watch?v=cQNcmYlVFcQ&amp;fmt=22) with it; Haskell+Direct3D is [basically](http://mokehehe.blogspot.com/2009/04/using-directx-from-haskell.html) nonexisting; as already mentioned, there is the LambdaCube engine, which is a pure Haskell "higher-level" 3D engine, using Ogre's formats; and also GPipe. And there are bindings (eg. ogre, cal3d), and experimental projects (eg. fieldtrip). And 2D stuff (eg. chalkboard, drawing-combinators), which are, of course, 2D, but still relevant. The place to get more information is probably the Haskell-OpenGL mailing list, simply because the people interested in both 3D and Haskell are there.
short answer: yes (i learned that from the prolog guys :) long answer: because the design space is *very* big, and there are no easy choices, especially if you keep performance in mind. So people are still experimenting with various designs, none of which is near optimal. Then throw in immutability (3D data structures are typically "doubly linked", as in tying the know; and large, which means mutability can be a serious gain), and the fact that it is very hard to get good number-crunching performance from the existing compilers. (disclaimer: it's 1st of january, i'm a bit dizzy and arrogant...)
Now _that_ is random downmodding - I saw a score of -1. There _has_ to be downmod bots aimed at Aussie-like statements EDIT: I think I see. You got into a spat with Pythonists or some such and they went and downmodded everything on your first comments page? Sheesh.
No, monads are burritos!
very insightful.
he addressed that complaint specifically [on twitter](http://twitter.com/sigfpe/status/7248009065).
I would prefer to see initial introductions to Monads in this form as opposed to a side effect control mechanism.
How exactly does the lub operator know that the approximate value is within the bounds?
Thanks to lazy evaluation, it doesn't; it will lazily generate finer and finer approximations. It may be easier to understand with a concrete implementation: {-# LANGUAGE FlexibleInstances #-} import Control.Monad.List import Control.Arrow ((***)) -- Provisional implementation of the Partial types newtype Interval a = Interval { unInterval :: (a,a) } deriving (Eq) newtype Partial a = Partial { unPartial :: [Interval a] } deriving (Eq,Show) -- list of approximating intervals instance Show a =&gt; Show (Interval a) where show (Interval x) = show x instance Functor Interval where fmap f = Interval . (f *** f) . unInterval -- this is only valid if f is monotonically increasing -- otherwise the maximum and minimum of f -- on the interval (a,b) can be very different -- from f(a) and f(b) respectively instance Functor Partial where fmap f = Partial . map (fmap f) . unPartial -- Numeric operations type R = Double -- ignore warnings about not having defined (*) etc. instance Num (Interval R) where (Interval (a1,b1)) + (Interval (a2,b2)) = Interval (a1+a2,b1+b2) instance Num (Partial R) where x + y = Partial $ zipWith (+) (unPartial x) (unPartial y) -- scalar multiplication (.*) :: Functor f =&gt; R -&gt; f R -&gt; f R a .* x = fmap (a*) x -- Creating and improving approximations between :: Ord a =&gt; a -&gt; a -&gt; Interval a between a b = Interval (a,b) improve :: Interval R -&gt; Partial R -&gt; Partial R improve x (Partial xs) = Partial (x:xs) -- Integration integral :: (Interval R -&gt; Interval R) -&gt; R -&gt; R -&gt; Partial R integral f low high = ((high - low) .* f (between low high)) `improve` (integral f low mid + integral f mid high) where mid = (low + high) / 2 example = integral cube 0 1 where cube = fmap (\x -&gt; x^3) example' = take 10 . unPartial $ example Here, `Partial a` is implemented as a list of smaller and smaller intervals, much like a [sequence of nested intervals](http://en.wikipedia.org/wiki/Nested_intervals). Try the `example'` in GHCi: *Main&gt; example' [(0.0,1.0),(6.25e-2,0.5625),(0.140625,0.390625),(0.19140625,0.31640625),(0.2197265625,0.2822265625),(0.234619140625,0.265869140625),(0.24224853515625,0.25787353515625),(0.2461090087890625,0.2539215087890625),(0.24805068969726563,0.2519569396972656),(0.2490243911743164,0.2509775161743164)] 
A monad is a monoid in the category of endofunctors, what's the problem?
"All" I want is the ability to describe a 3D model, a la drawing-combinators, and then have it rendered in a window where I can rotate the object or move the camera. I don't need any fancy raytracing; I just want to visualize some objects that are generated by a Haskell program. What's the easiest way to do this today?
While sushi is clearly parametrically polymorphic in the contents of the seaweed wrap the bind operation might be messy.
I read your comment before the paper and I was skeptical, but I agree. In a crowded field, this is by far the best explanation of Monads _for programmers_ I have seen. Add a bit about how you can't "escape" from the tree unless the tree provides such a function to evaluate it and connect that back to IO, and it's done. Also, any experienced programmer who sees the line "There are as many types of 'computation' as there are interpreters for tree structures." will immediately (or quickly, at least) grasp how powerful it is for building and interpreting tree structures to be an integral part of the language instead of a painful, manually-implemented add-on. A beginner may be less impressed, but, well, what can you do about that anyhow. (Object orientation _thought_ it had a good story for trees, but this beats it senseless.) I think this is a great way of showing how building monads into the language is a genuine innovation, and with that line I highlight goes a long way towards explaining very clearly both why there are so very many different kinds of monads (just as there are so many kinds of interpreters) and also why they are so useful for creating fairly-true DSLs. What is a DSL, or indeed what is an L, but an interpreter for a tree? Which also explains the "programmable semi-colon" idea, since the Monad is a tree interpreter, but I think that's less useful than looking at it this way.
I understand this, thank you very much.
Thanks for filling in the details. Nice to see the idea run and producing a correct answer. :) I know you know, but for the sake of reader, I want to emphasize that the interval Functor instance is incorrect in general (because of non-monotonicity).
How is the model defined? Triangles? Voxels? NURBS? Quadrics? Union of all of the above? Is the surface textured? What about lighting? Transparency? Animation? How many degrees of freedom on the camera? There's a lot of choices to be made, and chances are that a "minimalist" 3D rendering environment that does only what you want without all the baggage is useless to just about everybody else, and thus hasn't been written. That said, check out Conal Elliot's stuff, like FieldTrip. Although, I found this quote on the wiki: "Since FieldTrip is functional, it is about being rather than doing. One describes what models are, not how to render them." so you have to agree with him about what a 3D model "is" (a parametric surface) in order for the library to be useful. On the other hand, If you're willing to put in some effort to learn a little OpenGL (likely to pay dividends if you do this often) then GLUT is and always has been the easiest and most portable way to get triangles on the screen. If you don't want to get sucked into the wonderful quagmire of 3D user interface design, then the other option is to write it to a file and use a separate full-featured 3D program to view it. I'm partial to the archaic but well-supported [AliasWavefront OBJ](http://en.wikipedia.org/wiki/Obj) file format. Blender and Wings3D are two good free modeling programs that support it. 
(m &gt;&gt;= f) &gt;&gt;= g == m &gt;&gt;= (\x → f x &gt;&gt;= g) -Monad law 3 English-to-english translation: Avoid success at all costs! -Simon Peyton-Jones m is a monad iff Kleisi composition (&gt;=&gt;) is associative with return as 2-sided identity. If monads had been defined this way, they would have been blisteringly obvious to anyone who had managed a C+ in "Modern Algebra I" for math majors, and Haskell would now be too big to be any fun anymore. So perhaps the powers-that-be went with (&gt;&gt;=) rather than (&gt;=&gt;) to keep the algebra like breathing the atmosphere on Venus. Or perhaps after ten years of practicing category theory it all looks the same? (Beware of following mathematicians who can walk through a brick wall without feeling a thing, unless you're walking exactly behind them.) (Hint: Look at the source code for (&gt;=&gt;), then look back at monad law 3.) 
The more category theoretic presentation with `join` isn't terrible, either. There you have two natural transformations: return : I =&gt; T join : TT =&gt; T `T` being the monad in question, and juxtaposition representing composition. Then the associative law is essentially that T(TT) ~ (TT)T in that using `join` with either grouping gives you the same result. Identities state that IT ~ T ~ TI by going through the relevant paths IT =return=&gt; TT =join=&gt; T Which is somewhat more obscure, unless you're used to a categorical definition of monoids.
&gt; I want to emphasize that the interval Functor instance is incorrect in general (because of non-monotonicity). Yep, and I better put a corresponding comment in the example code. :-)
for a comparison this is the image she based it off of. http://www.haskell.org/haskellwiki/Image:Monica_monad_falconnl.png
I totally agree. The monad laws should presented in terms of Kleisi composition and return. And when I first saw monads it was. I'm not sure when that was lost. 
As the creator of the original image I have to say that is one great-looking monad :)
ahh, the neverending chase of absolute perfection that renders you unable to do anything. I have this resolution every monday, every 1st, every birthday and every new year. But this time I mean it!
I was slightly misled by the title, it should've been something like "Explaining Haskell IO without Monads". I thought it would be [something like sigfpe's article](http://blog.sigfpe.com/2009/11/programming-with-impossible-functions.html) here. Nice tutorial though.
You are totally correct - I've fixed the title now. Glad you liked it.
This was great--I think more tutorials aimed at this post's audience are needed!
This article is in very helpful counterpoint with the current trend (me too) to explain Haskell Monads without IO. Both have good points: IO is a bad pedagogical example of a monad; you don't need to grok monads if you want to do some IO. Monad obsession is an unhealthy aspect of the Haskell vibe. Thanks for fighting it.
Anyway, what's your problem? you don't Like the monad class? create another one, it takes Like 10 lines of code. But then you'll Like loose do-notation! So the other day I was Like reading some Oleg stuff, and then I saw this: http://okmij.org/ftp/Haskell/types.html#restricted-datatypes , and I was Like "no way!", but it totally works! yeah, you should fix the name of that Like guy.
I wonder what would be a good interface for capturing the properties needed to implement the functor instance in greater generality. At a minimum it would need to support pointwise combination as well as functional composition. Let's suppose we restrict ourselves to continuous functions. What's a good interface? We could directly implement the epsilon-delta definition with interval analysis. data Continuous a b = Continuous { apply :: a -&gt; b, bound :: Interval b -&gt; Interval a } compose :: Continuous a b -&gt; Continuous b c -&gt; Continuous a c compose f g = Continuous { apply = apply g . apply f, bound = bound f . bound g } You can then implement, say, pointwise addition of continuous functions by composing two values of type `Continuous a b` (where `Num b`) with the continuous lift of `(,)` followed by the continuous lift of `uncurry (+)`. Actually, that does not quite work if we use intervals (more generally, higher-dimensional squares) as bounds. The inverse image of a single point in the range of addition is an infinite line's worth of points in the domain, and the inverse image of an interval thus becomes a fat line, neither of which is boundable by a finite square. You don't have that problem if you stay with curried addition. But then you have to ponder the meaning of intervals in function spaces. That seems scary on the surface, but when you think about the usual calculus proof of how addition preserves limits, the answer seems pretty simple: an interval in `a -&gt; b` should be a function that takes an `a` and returns a corresponding interval in `b`; that is, an `a -&gt; b` interval is a `b` interval parameterized over `a`. In retrospect, this is simply Kleisli lifting for the interval monad. Anyway, this is all off the top of my head. Have you thought about this at all?
You seem to have skipped over what I think is one of the most powerful aspects of Maybe here, especially when compared to Java's null pointers, which is that the type system requires you to deal with it. In Java, you could easily skip the null checks and get something like public Month getPathMonth(Map&lt;String, String&gt; m) { p = m.get("path"); d = parseDate(p); return d.getMonth(); } and the compiler won't care. However, you've now set yourself up to crash horribly if either "path" doesn't exist, or the string is malformed. However, the equivalent Haskell code to this simply cannot be written. You are forced to deal with the Maybe monad. And the other bit is in Java, *any* object could theoretically be null, and you have to consult the documentation to find out whether a function could return null or whether it can accept null. However, in Haskell that's all implicitly documented by the type system.
&gt; Why not exceptions? &gt; &gt; Well, I see a few reasons why didn't use exceptions here: &gt; &gt; * Exceptions are much slower for various reasons. &gt; * They have to be caught again, which leads to very verbose code. exceptions are slower? using the new failure framework you can use Maybe monad for exceptions. they have to be caught? just like Nothings in Maybe monad, you catch them with every bind you take, every pattern-match you make (god, I hate that song).
you can use exceptions in Java. but I agree, making NullPointerException unchecked exception (or providing user with a possibility of creating unchecked exceptions at all) was a big mistake. on the other hand, in haskell we have bottoms. do s/null/undefined/ and s/Java/Haskell/ and read your post again. it's weird that it still makes sense, isn't it?
It's totally true that Haskell's monad class sucks for things like sets, or vector spaces, probability, etc. I think most everyone agrees with this. There does seem to be a nice upwards compatible solution to this problem. You need context synonyms and associated contexts. There is an experimental implementation and I hope it gets added to GHC, because it solves a real problem.
Sorry for the misunderstanding, but the "Why not exceptions?" part was exclusively about Java, and the decision the java engineers took when designing the `HashMap` class.
You say that as if Java *doesn't* have bottoms.
The failure framework is merly based on a type similar to the Either or Maybe types. Which are quite different from an exception mechanism buit in the language using stack frames.
I'm not trying to defend java, I just don't understand all the java null bashing by haskell people, when undefined (or error), either explicit or implicit (missing case) is used exactly like java's null, and has the same results. I assure you that the final user of application doesn't care if he gets "Prelude.undefined" or "java.lang.NullPointerException" so it is about undefined/error, not about bottoms. without undefined the only value of type a, would be "let x = x in x", and in that situation, I'm sure no-one would use non-termination and we would very quickly get explicit apis (and get rid of head&amp;tail from Prelude).
No! failure framework isn't based on anything, just like Read isn't based on Parsec. failure framework is designed in such a way, so that library doesn't return Nothings, or Lefts (or that forbidden function from Monad class), but just calls failure and lets the caller decide how to handle it. the caller can choose simple Maybe, IO (and not handle anything), things like Either SomeException (Attempt I think) or even my favourite - a checked exception monad.
&gt; used exactly like java's null OP is a counterexample to that. You can test for null after the fact, so it's used extensively for "no value here". A function that returns bottom in a legitimate part of the domain is useless.
ok, that's true. but all the other problems remain.
undefined is not used nearly like null. In fact, it's very rare precisely because it can't be used like null. 
i think he was referring to java exceptions which are slower because they, IIRC, require an interrupt and two context switches 
Take Java's switch for example; you have to return something on every branch. For branches that don't make sense, people return null. Haskell lets you omit cases in er, cases, and in function definitions, automatically returning bottom. If Haskell forced you to return something, people would probably pick some sort of bottom. OTOH, one might argue that for values that are in the domain only because the type system isn't up to excluding them, the function *should* return bottom.
This is something that C# is starting to get right with Nullable types, which is basically Microsoft's attempt at the Maybe Monad. It allows you to do compile-time null-checking, and while that's great and all, where it falls down is that you can't use it on several existing types (like "string" &gt;_&lt;), making the whole thing next to useless until they forcibly deprecate some of their old types for newer variants. I forget the restriction itself they've used to justify that choice, but I do recall getting incredibly frustrated after spending a while refactoring some code to use Nullables and finding that a whole bunch of types I used were largely incompatible. Still, we can live in hope that the equivalent of Maybe/Nullable will eventually filter out to other languages.
Time for a FRP reddit? ;)
I prefer `(&lt;=&lt;)` for composition, but yeah. I've often railed against the formulation of the triple laws on the Haskell wiki. Comparing the `return`/`(&lt;=&lt;)` version vs the category laws with `id`/`(.)` is forehead slapping fun.
this should be added to wiki(?) for the binary package
I'd love to hear your comments, since I'll be revising this article furiously in preparation for the January Monad Reader publication.
Wow -- I hadn't thought of using the definition of continuity to reveal a representation of continuous functions. I also hadn't thought about Interval as a monad, nor function-valued intervals. All wonderful stuff! I wonder whether this `Continuous` notion, with its inverted interval computation, can be used to solve the same problems as standard interval arithmetic.
Glad you like it. I don't have much time right now to develop these ideas, so if you take it much further then I'd love to hear about it.
It looks pretty good. :) One thing that sticks out at me is this: &gt; Backtracking lends itself well to constraint satisfaction problems, since given any partial solution we can try all further possibilities and see if they violate any of the constraints we were given, and it tends to be faster than brute-force enumeration of the solution space. This seems misworded: given any partial solution, you can eliminate all further possibilities if the partial solution already violates a given constraint. I'll keep on reading... **edit:** or better yet: Backtracking tends to be faster than a brute force enumeration of the search space, because if a partial solution violates a constraint, then all further possibilities can be eliminated.
&gt; you can't use it on several existing types (like "string" &gt;_&lt;), making the whole thing next to useless until they forcibly deprecate some of their old types for newer variants You can't use it on reference types, which already have a null value. You can write a more universal Maybe type without problems, however. You just need to wrap the standard library functions. I use it constantly (or, more often, `Result&lt;T&gt;`, which either stores a value or an exception).
Actually, "returning bottom" is a lot closer to throwing an exception than to returning null.
I installed logict-0.3 and i had to manually create the Show instance for the Logic type to get the first exemple to work. **edit**: In listing 4, you don't use the trans alias. More coming :)
&gt; the C++ macro processor blows goat dick. And he's surprised he got indignant responses?
There is a lot of auxiliary code (especially Show instances) that I omitted in order to keep the samples small; the source (which I suppose I should post) is a literate Haskell file which is directly loadable. As for the trans alias, yeah, I don't use it; it gets used in listing 5. I'm a little ambivalent about whether or not it should stay; on the one hand it keeps the two listings closer to each other, on the other it's not necessary. *Edit:* I misread your statement; while I wrote a bunch of custom show instances for a lot of data structures, I did not define an orphan Show instance for Logic. Usually I'd just run observe* to get it into a form that I could show.
Thanks for the rewording. I've put that in with a slight tweak.
How about: Backtracking tends to be faster than a brute force enumeration of the search space, because if a partial solution violates a constraint, then *further extensions of that partial solution* can be eliminated. Search trees are pruned to cut any further losses.
It sure is.
If they already have a null value though, not allowing the use of Nullable&lt;T&gt; seems a bit silly. I can understand the reasoning behind only allowing Nullable&lt;T&gt; on types that can't be null (like Datetime, which took me a long time to realise - doh!), but if Nullable&lt;T&gt; extended to types that *can* be null, you can eliminate a major cause of errors in common software. It seems an arbitrary restriction :/. Result&lt;T&gt; sounds pretty cool though, I'll have a look at that next time I have to do some C# work (typically I'm not a C# programmer, but used it during a year-long Uni project).
&gt; if Nullable&lt;T&gt; extended to types that can be null That would simply be Maybe&lt;T&gt;. I also would prefer it. Really, the distinction between reference and value types (`class` and `struct`) is very important to understand in .NET. If you start working with C# again, I advise you to learn it. For Result&lt;T&gt; and Option&lt;T&gt;, see [functional-dotnet](http://code.google.com/p/functional-dotnet/) or many other libraries.
Minor thing: Two True :: Bool -&gt; GADT Bool not Two True :: True -&gt; GADT True
IANCE, by any means, but the imperative paradigm isn't just useful for IO. Ever try to make a mutable\* functional doubly linked list? If you haven't yet, just go ahead and try. Now maybe it's just me, and maybe it's just my "unconscious limiting assumptions" but making a DLL is way easier with pointers than with the massive amount of thunk magic it takes to get a mutable DLL with FP. EDIT: \*added
Thanks, this is good advice. I don't plan to do this terribly often, so rather than learn how to use a complicated modeling program, I found a simple OBJ file viewer at http://www.avl.iu.edu/?projects/GLM_AVL which is fairly intuitive.
One other, often-overlooked benefit of using *Maybe a* to represent values of type *a* that might be missing, is that *a* itself can be a *Maybe* type. So you can distinguish, for example, between a missing key and a missing value: module Missing where import Data.Map as M type MapWithPotentiallyMissingValues k v = M.Map k (Maybe v) myMap :: MapWithPotentiallyMissingValues Int String myMap = M.fromList [(1, Just "Foo"), (2, Nothing)] And, to give it a test, run: *Missing&gt; :load "/tmp/Missing.hs" [1 of 1] Compiling Missing ( /tmp/Missing.hs, interpreted ) Ok, modules loaded: Missing. *Missing&gt; M.lookup 1 myMap -- key and value present Just (Just "Foo") *Missing&gt; M.lookup 2 myMap -- key present; no value Just Nothing *Missing&gt; M.lookup 3 myMap -- key not present Nothing 
The address of a location in memory (e.g., a pointer) can be expressed as an integer. Memory and its addressing scheme can be though of a Map data structure. type Key = Maybe Int type DLL a = Map Key (Wrapper a) data Wrapper a = Wrapper a Key Key -- Functions for insertion, deletion, etc. This is something that is functionally equivalent to a DLL, though it will have different performance (the actual performance will be dependent upon the implementation of Map).
I've seen a video of Simon Peyton Jones talking about this sort of thing... how in the 80s, people had all these ideas about alternative computer architectures designed for FP. Eventually, most people got the right idea: the Von Neumann design is well known and current designs achieve incredible performance. The gap between what the programmer expresses and what the machine executes is covered by increasingly sophisticated compilers. The alternative (special hardware) is expensive and inferior by comparison to nice, general-purpose von neumann machines. There is no need for so-called "liberation" from the VN paradigm, because it's not really that restrictive to begin with. Go back to CS 101 — whenever anybody comes up with a new type of computer, the first thing they do is prove it's equally powerful as any other computer. The way to do this is to write an translator for programs for some other computer to their computer. For example, you devise the register machine, and write a translator from the Turing machine to the register machine. The good news is that the compiler technology is there right now to do this, and it works very well. GHC is mature and produces good code. A couple decades ago it didn't look as rosy. Part (much?) of the failure of the Itanium was due to the difficulty in creating the compiler. (The Itanium was designed for performance at the expense of code generation complexity, an extreme form of the philosophy behind RISC.)
&gt;Ever try to make a functional doubly linked list? If you haven't yet, just go ahead and try. What exactly would you need this for in a purely functional environment? If you want a cursor that can walk both "up" and "down" a list, a zipper is a good solution. Most of the use cases of doubly linked lists involve destructive updates, which you oughtn't to be designing around if you're writing Haskell. My guess is that for your particular use case, there's a better data structure than a doubly-linked list for what you want.
This is the Haskell reddit. The thunk magic is turned on by default, you have to turn it off by adding strictness annotations to your data structures. Here is a doubly-linked list in Haskell: data List a = Node a (List a) (List a) | Nil dll :: [a] -&gt; List a dll = dll' Nil where dll' prev (x:xs) = let this = Node x prev next next = dll' this xs in this dll' _ _ = Nil Typical Haskell programmers will not use such structures directly in their code because you can't add items (anywhere!) without copying the entire list structure. They typically use single-linked lists, Data.Sequence, or mutable structures.
I would like to see some specifics. I am curious about Conal's use of the word "continuous" in respect to IO. In my general understanding of the term, I probably am more comfortable with discrete things than continuous things. I don't think discreteness has to imply imperativeness, but I haven't really thought about it too hard.
It's not restrictive, but it's also not very composable.
&gt; This is the Haskell reddit. The thunk magic is turned on by default, I don't think you fully appreciate the degree of thunk magic needed to solve this problem. If it were trivial, you would have posted the solution to a mutable DLL instead of a monolithic one. Haskell is very, very good for immutable data. No one is debating that. That's not the problem. &gt; Typical Haskell programmers will not use such structures directly in their code because you can't add items (anywhere!) without copying the entire list structure. This is my entire point. Perhaps I wasn't clear enough. Making monolithic DLL are easy, making them mutable is a major pain in the ass and an area of open debate. [Here's](http://www.haskell.org/haskellwiki/Tying_the_Knot) a start. The three solutions in the article are (in order): * Make a normal list look like a DLL and toss out any benefit of using a DLL * Make a memoized data structure look like a DLL and ditch laziness (ie reinvent pointers) * Make a recursive tree instead of a DLL
&gt; My guess is that for your particular use case, there's a better data structure than a doubly-linked list for what you want. Yeah the answer is a recursive tree. Please see my response to klodolph below. But that's my point. The haskell solution to using doubly linked lists is to not use doubly linked lists. For DLLs, there is a better alternative (recursive trees), but not so for other cyclic data structures, like arbitrary graphs.
Please see my link in my response to klodolph for a thorough analysis of this technique to replicate DLLs. 
No, you misunderstand me: I'm not saying don't use them (you can certainly build doubly linked lists in Haskell, exactly the same way you would in an imperative language -- in the ST monad) I'm saying that I can't off the top of my head imagine any situation in which they would even make sense. What is your use case?
Doubly linked lists are not an end in themselves, so whether or not they are useful in Haskell is a silly thing to care about in isolation. The important question is whether one can solve the same problems in Haskell with comparable elegance and performance as other languages.
&gt; you can certainly build doubly linked lists in Haskell, exactly the same way you would in an imperative language -- in the ST monad This is what Conal sees as a problem. It's easy to define DLLs in IO or ST, but then it's not *really* FP. &gt; What is your use case? My work involves a lot of graphs (ie the data structure). They have all the same limitations as DLLs, except for special cases, like a normal list.
I just tried to make one and link it here, the immediate result was that the reddit was banned. Perhaps the name looked too much like a random series of letters to reddit.
The answer, if you ask me is that you can't. FP is inherently crappy for cyclic data structures. That is, unless you use imperative programming, which is Conal's problem.
Fixed, thanks!
It won't really be 2010 until I need to evaluate the year and write it down as part of a date.
I agree with most of what you say, but it has little to do with IO, really. 
I'm not sure I buy the argument that FP is so bad for cyclic data structures. Can you, by chance, come up with some specific scenario that you think would be problematic?
&gt;This is what Conal sees as a problem. It's easy to define DLLs in IO or ST, but then it's not really FP. I'm not really sure that was his point, from my reading -- certainly not that IO or ST are not *really* FP (they are both purely functional). &gt;&gt;What is your use case? &gt;My work involves a lot of graphs (ie the data structure). They have all the same limitations as DLLs, except for special cases, like a normal list. This doesn't really answer my question. I suspect you're avoiding giving specifics because you know that I or someone else will most likely respond by telling you that you shouldn't be using a doubly linked list in the first place, which is not what you consider a productive answer. To your mind, even if in your particular scenario a doubly linked list is not the right Haskell solution, there simply *must* be a place where doubly linked lists are the most elegant way to solve a problem. Am I wrong in my assessment? What I'm trying to tell you is that I don't expect there *is* such a situation. If you'd rather not talk about your own specific work, why not provide us with an example of code that requires a doubly linked list? I honestly can't think of one. In particular, what's wrong with a zipper?
&gt; To your mind, even if in your particular scenario a doubly linked list is not the right Haskell solution, there simply must be a place where doubly linked lists are the most elegant way to solve a problem. Am I wrong in my assessment? Way to be an asshole. Come on. This is the haskell reddit. Have some fucking class. The whole point of this thread is that I don't use functional representations for this type of data, because it's not well suited. I use mutable arrays to express graphs. &gt; In particular, what's wrong with a zipper? I think it's ironic that you assume that I have no idea what I'm talking about, and then suggest a solution that is manifestly impossible to implement on an arbitrary graph. 
How would you define a mutable graph without using ST or IO?
&gt; ... forced us all ... to see that all languages are broken and inadequate, each in their own way. I realized this just a while ago. No language is better than another, just better at X or Y and worse at Z. I've had a few arguments with some friends who try to corner me into saying that I'd use Haskell for anything. The best I can say is that it's my default language, but I'll quick switch if it isn't the right one for the task. I like to think that I know an uber-language, which is sort of a the superset of all the languages I know, only I have to stick with one language per file. Then I can glue them all together with linking and pipes and use whatever I feel is best for each part.
On the one hand, far be it from me to discourage somebody from trying to push the boundaries and go where few or none have gone before. On the other hand, I observe that the real world is not functional. In the real world, things have state, right down to the atomic level. In the real world, that state is mutable. There are non-reversible transforms. The immutable, stateless world is fundamentally unable to simply cozy up to the mutable, stateful world without an interface, almost by definition. While it may perhaps be true that locking it behind an IO monad is not the right answer, and it may be true that there is some way to push the interface even closer to the real world and contain even more of our code in the happy immutable stateless world of FP, it seems fundamentally impossible to eliminate all vestiges of the imperative world without rewriting the very nature of reality. And I mean that straight, not as a flowery post conclusion; when your base primitives are imperative, you can implement an FP abstraction on top of that but there is no escape from the final underlying primitives you have.
I began learning Haskell around 5 (perhaps 6) years ago, and early on in that period, I read about functional reactive programming. And it looked like a great way to do GUI programming. I've been waiting for it to arrive for probably 4 years, and it hasn't yet. In the past one or two years, folks have been talking about how it could replace the `IO` type, which I hadn't thought of before, but sounds nice. So I'm ready for FRP. I'm ready for it to arrive and displace `IO`. What I don't have any use for is articles telling me that I don't care enough about purity, or am mentally shackled by `IO`. When Backus wrote his famous article, he had a language that wasn't shackled by the von Neumann style, and wrote about it. I don't know if he had an implementation for people to use, but he had a language that could demonstrably be implemented. He didn't just write about how the von Neumann architecture stunk, and _something_ better should be done. I don't need articles about how C is just as pure as Haskell with some hand waving. That's BS. Show me some programs written in just the C preprocessor, with no actual C. I routinely write programs in Haskell where 95+% of the code doesn't touch `IO`. Hell, I write stuff in Agda, where there isn't an `IO` at all unless you bring it in via the FFI to Haskell, which I don't. So Haskell without any I/O capability period can still encode lots of interesting programs. Show me your CPP-without-C programs for comparison. I have [a paper by Amr Sabry](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.27.7800) on my hard drive defining "purity", and showing how Haskell-with-monads satisfies it (and good luck proving that CPP is a conservative extension of the lambda calculus; but anyhow). If you don't like the Sabry definition of purity, that's fine. Can I have a better one? I hear "`IO` isn't pure, but FRP will be." How will I be able to tell? What does "pure" mean? I have a definition, but it isn't good enough, because it thinks using the `IO` type is pure, so obviously it's the wrong one. What is the metric I can use to tell whether some solution is pure or not? How does `IO` fail, and FRP pass it? I'm on board. I'm ready for the `IO` type to be replaced by something better. When you show it to me, I'll start using it. But it isn't my personal pet peeve, so I'm not going to drop everything to help you make a nebulous 'something better' happen. I don't need articles telling me that the fact that killing `IO` isn't my #1 interest means I just don't care about functional programming or real purity. It's an insult, and you're not winning any points by insulting me instead of actually figuring out FRP, so we can all happily use it instead of committing war crimes with `IO`.
He does point out in the article that he thinks Python is strictly better than Java, it seems, so perhaps the quoted point should be taken in context: &gt; In the old days I could write something in Java and Python and Python would clearly smoke Java.
State :-)
My impression was that functional data structures were reasonable for most graph algorithms? In particular, that representing graphs directly as actual graphs with pointers rather than via lists of edges, etc. was in fact a generally bad idea? So you could expect some typical persistant/mutable performance trade-offs but it wasn't a bigger issue? Do people in the imperative world really usually represent graphs directly?
What sort of `Show` instance were you looking for? I'd usually consider a `Logic` computation to be just that, and not a data structure that one would want to output. But I could add a `Show` instance if people generally find it useful (it'd probably produce something like "`pick [x, y, z, ...]`" where `pick` would be a name for `msum . map return`).
It's not true at all that the real world is made up of state and mutability. That's merely the model through which you're interpreting the world. It's not even necessarily the best model. Physicists, for example, describe the world in great detail and almost always do in terms of mathematics precisely because it provides greater rigor and expressiveness than trying to use a state and mutability model. Exactly the same reasons functional programmers avoid state and mutability. It's instinctive to reach for state, mutability and agents as the first model to understanding something largely because we've evolved to be social creatures and that model is a good fit for describing social interactions. But it's just that: a model. It's not an objective reality and for many things a functional model fits much better. 
A mutable graph is a means. To what end is that means? I'll answer your question as directly as possible, but it's important to remember that one chooses a data structure for a purpose, and no matter what I say, one could find something wrong with it for some particular class of use cases. There is more to programming than "make a doubly linked list" or "make a mutable graph." Your question as posed is geared to be impossible (you can't have *real* mutation without `IO` or `ST`), but nonetheless, the efficiency of a mutable graph, even assuming you are talking about cyclic graphs, can still be captured in pure code, even keeping the benefits of purity such as free undo. Here is a directed graph with nodes of type `a` and labelled edges of type `l`. data Graph l a = Graph { nodes :: IntMap a, edges :: IntSet (Int, Int, l) } This gives us O(1) insertion and removal for both nodes and edges. There is bit of constant overhead compared to the same structure in, say, C, but this also gives us the advantages of purity. It also fits your criteria of not being in `ST` or `IO`. [Edit: Changed `Graph` definition to a record for clarity.] [Edit 2: Okay, this particular structure isn't the most efficient for a few things, like finding what edges connect to a particular node. I would look at something like the FGL (which I can't pull up on Hackage at the moment, otherwise I would save you the trouble and paste it right here) for a better version.]
Well all computers are equivalent, but that doesn't say anything about time complexity. You'll recall deterministic and non-deterministic Turing machines are equivalent in power as well. The real reason to implement an FP friendly hardware would be to speed up common operations in functional programs, just as Von Neumann machines are optimized for common operations in imperative languages. Turing equivalence isn't the relevant issue here, it's whether or not FP on such a specialized machine would be faster than FP on VN
&gt; I think it's ironic that you assume that I have no idea what I'm talking about, and then suggest a solution [(zippers)] that is manifestly impossible to implement on an arbitrary graph. A zipper for an arbitrary graph: data GraphZipper l a = GraphZipper { graph :: Graph l a, current :: Int } Where `Graph` is as I showed in another comment in this thread.
\#1 is not worth it. \#2 is really easy. You don't have to "reinvent pointers" in Haskell, Haskell already has a half dozen different kinds of things for creating different kinds of mutable objects. Here I'm using ST, which has the advantage of being completely outside the IO monad, and therefore usable in a pure section of code. You could also rewrite this using IORefs for a "classic" implementation or TVars so you can use it in transactional code (try doing THAT in C, Java, Scheme, or almost any other language). import Data.STRef import Control.Monad import Control.Monad.ST data List s a = List (STRef s (Node s a)) (STRef s (Node s a)) data Node s a = Node a (STRef s (Node s a)) (STRef s (Node s a)) | Nil newList :: ST s (List s a) newList = liftM2 List (newSTRef Nil) (newSTRef Nil) toList :: [a] -&gt; ST s (List s a) toList xs = do l &lt;- newList mapM_ (append l) xs return l append :: List s a -&gt; a -&gt; ST s () append (List first last) x = do nlast &lt;- readSTRef last node &lt;- liftM2 (Node x) (newSTRef nlast) (newSTRef Nil) let ref = case nlast of Node _ _ next -&gt; next Nil -&gt; first writeSTRef ref node writeSTRef last node rgetList :: List s a -&gt; ST s [a] rgetList (List _ last) = readSTRef last &gt;&gt;= rgl where rgl (Node x prev _) = fmap (x:) $ readSTRef prev &gt;&gt;= rgl rgl Nil = return [] Here's an example of using it: Main&gt; runST $ toList [1,2,3] &gt;&gt;= rgetList [3,2,1] Haskell programmers tend to use option #3, where you use a recursive tree instead. There's a good reason for that: it makes it easier to reason about the program's behavior if you know that your data structures are immutable. Typically, if you want an atomic queue, you just stick the recursive tree inside an MVar or TVar and you're done. You don't need to worry about affecting other threads that are using the value. I really don't see how this was very hard. Maybe it's hard in ML or something? I don't know. But it's easy in Haskell. edit: formatting
You realize you used ST in that explanation, and that is exactly what Conal objected to. **The entire point of my original post is that using imperative programming makes this shit easy!!**
I want to vote up because Oleg's stuff is good, but I detest URL shorteners.
Which one? A link would be nice.
I want one! Are they stackable?
No, it's made up of state. The stateless model is the pre-quantum-mechanics deterministic view of the world, one initial beginning and an iteration function for time, where it's debatable whether "time" even "exists" in any meaningful sense. However, in the quantum world, there are numerous state values that can only be determined by observation. Yes, the _probabilities_ of those states as they vary through time can be described with an equation, but the real world involves observation of the actual state of the resulting outcome. If the many-worlds hypothesis is true, the whole set of possible quantum realities could be viewed as stateless again, but that only matters from the outside. We are not outside, we are inside. In here, there is state. In here, there is no backtracking. In here, there is no constructing a new object out of bits of an old object while still retaining the old object too. In here, any moron can interact with any object any old time in any old way and you need to deal with it because you don't have your own immutable copy anywhere. In here, there's a "before transform" and "after transform" and ne'er the twain shall meet in the general case. Reality does not resemble pure functional programming in any meaningful way. There's not going to be any way to hook up to the real foundational world without dealing with that fact. That has absolutely nothing to do with whether functional programming is a good paradigm for programming or not, by the way. The reasons in its favor we cite are still good reasons. I just am not sure that there is even in theory a such thing as a "purely functional" approach, even if it is still ultimately the best idea possible to wrap the real world ASAP and stay in this hypothetical functional land as much as possible. Computing is great that way.
Woop dee do. Imperative programming makes mutation easy. You are assuming some need for mutation, otherwise your point is completely useless. [Edit: It's hard for me to respond when you keep deleting your comments. :\ ]
xyphus, I'm not sure if you're honestly confused about mutability in Haskell or if you're just being disingenuous. ST is pure FP. The runST function is just a function. It's not some special impure construct like unsafePerformIO. In traditional FP, you'd just munge some pointers. Haskell says if you want to mess with pointers and mutability, it has to show up in the type signatures. In my experience, that's a big win. You can do some cool stuff with STM and ST that you can't do in other languages. You can also get some parallel code in Haskell almost for free. I can't think of any other general purpose language that allows you to reason about mutability — e.g., C++'s const is not what I'm thinking of because a mutable reference can alias a const reference in C++. It sounds like you don't like reasoning about mutability in the type system which is fine, that is a matter of style and I respect people's decision when they choose to write imperative programs. I also don't go trolling the Ruby and Java boards just because I can't stand their strict OO and terrible type systems. 
"[IO and ST] are both purely functional." I know IO is not pure. I think ST is pure.
All programming is abstract. This rambling is irrelevant.
here is what i added: instance Show a =&gt; Show (Logic a) where show = show . observeAll this makes sense to me after reading the *List monad equivalence* section.
The ST monad is purely functional in its semantics - it just happens to be implemented in an imperative way, but it doesn't have to be. In principle you could reimplement ST in a purely functional way.
&gt; I've been waiting for it to arrive I guess you could do web UI programming with Flapjax.
Heh, I saw ur monad.
Clearly not. You need monad transformers: imperative programs in disguise.
&gt; What I don't have any use for is articles telling me that I don't care enough about purity, or am mentally shackled by IO Perhaps you shouldn't take it so personally. I think Conal is addressing people who are less aware of the quest for purity that was active before IO was used. If you are aware, but decide not to divest your energy in finding the better solution, that's fine, I don't think Conal is attacking or insulting you.
&gt; I don't need articles about how C is just as pure as Haskell with some hand waving. That's BS. Show me some programs written in just the C preprocessor, with no actual C. I routinely write programs in Haskell where 95+% of the code doesn't touch IO. The thing is, your ability to write programs "in just X" is not the criterion for whether X is purely functional or not. The point is not that Haskell-minus-IO == CPP, just that in one particular aspect (Being purely functional) they **are** identical, even if in many other aspects they are extremely different. Being "purely functional" is an absolute term, not a relative one (at least by some definitions) and has little to do with the power/usefulness of a language. In this qualitative sense, CPP and C are like Haskell and IO. &gt; I hear "IO isn't pure, but FRP will be." How will I be able to tell? What does "pure" mean? I think the definition of "purity" that Conal is pushing forward that FRP matches and IO doesn't -- is simple, tractable semantics. The semantics of the Behavior type are specified (Time -&gt; a) and all of the type-class instances behave exactly as if you really had a (Time -&gt; a) and not just a (Behavior a). The semantics of the Event type can be specified as [(Time, a)] (Though this definition is knowingly problematic, and cannot easily have type-class instances that behave like a [(Time, a)]). There is really no tractable way to specify simple semantics for IO and how they compose -- in the same way Behavior/Event were defined above. So if purity="simple, tractable semantics", then FRP is pure, and IO isn't.
In a technical sense ST is purely functional, but come one, in spirit it's good ole imperative programming. I think it's amazing that you can use it inside Haskell and encapsulate it with runST, but that doesn't make ST any less imperative.
He's using "continuous" with respect to time-varying values. Mathematically, they can be represented as (Time -&gt; a), whereas discretely-changing values can be represented as [(Time, a)] or as continuations. (Time -&gt; a) is much simpler, mathematically (even if more complicated, implementation-wise), which makes it simpler and more composable similarly to the way a natural number represented as such, rather than as a [Digit] or [Char] is mathematically simpler and thus more composable. At least much simpler, when the time varying value really is continuously changing.
&gt; because it's not really that restrictive to begin with. Its restrictive not in the set of computations it can express, but in the ability to mathematically reason about programs, compose them, etc.
The imperative paradigm is definitely useful. Conal is not disputing that. He's merely saying that it is useful to **implement** things (at least on imperative machines), not to **model** them. A doubly-linked-list is not an interesting thing to model, or an interesting semantic model -- its an implementation concept, not a model - so does not belong in the functional side.
You are confusing modeling and implementation. One can view the universe as stateful, or one can view it as a pure function of time/space.
What do you mean by 'In here, there's a "before transform" and "after transform" and ne'er the twain shall meet in the general case.'? You can talk about both of them in the same sentence, so they have met. You can talk about the two of them in the same formula, so they have met. Or are you saying that two things that have a time-like separation do not "exist" at the same time? Well, duh, that's the what it means to be separated in time. I'm not even sure how to talk about things as if they have some identity (which you need to be able to talk about "before" and "after" for the "same" thing). Just think about the Unruh effect where different observers cannot even agree on the number of particles they observe. So I'd be very careful about saying that the physical world is stateful or stateless, I don't know what it is. And the more I learn the less certain I get.
No, you don't understand. Somebody is [wrong on the internet!](http://xkcd.com/386/), dammit!
I'm well aware of the definition of what a "purely functional language" is in the Sabry sense, which is the only formal definition I've seen. Part of that definition is that the language in question is a conservative extension of the lambda calculus. He mentions in the paper when talking about effect-passing-style (monads), that one couldn't simply make the entire language monadic, because that would no longer be a conservative extension. So, that the language contains a lambda calculus fragment (as Haskell does) _is_ part of the definition of a pure functional language. One could perhaps relax this criterion, to say that it contains some roughly equivalent language (but perhaps not with higher order functions and such) as a fragment, and I'm sure some devious hacker has shown that CPP is Turing complete, but it's questionable whether CPP on its own is "pure" (especially when you're doing tricks to get recursion), and I don't think it's in any way correct to say that CPP is a fragment of C. CPP is, more or less, a language that munges strings (which needn't even represent C programs), and CPP macros are not really values able to be referred to in a C program and such. It's rather like saying that Haskell is a fragment of C, because I can write a Haskell program that will output a C source file. &gt; So if purity="simple, tractable semantics", then FRP is pure, and IO isn't. Well, I still think this remains to be seen. I know how one is supposed to think about behaviors and events, but all the FRP toolkits seem to have their problems, so far. And that's just in the GUI department, not even speaking of taking over all of IO's duties. It isn't difficult to give pure (as in datatype and lambda term) implementations of a lot of IO stuff. ST can be done with any pure functional map implementation, storing dynamics (which loses mostly efficiency). Concurrency can be implemented with a sort of continuation passing (although you lack preemption). File I/O is rather like ST, but with filenames instead of references you create on the fly. None of this stuff is that complicated on its own, but it gets there when you stick it all together, and it's not that obvious that dataflow programming (which seems to be a lot of what FRP is about) will suddenly make everything simple again. But perhaps it will. I've been waiting for it to happen for a while.
I always thought that backtracking is about enumerating solutions in a search tree in depth-first order. Whether or not solutions can be generated incrementally and rejected based on a partial instantiation seems orthogonal to backtracking. Isn't this equally possible using a different enumeration strategy, e.g., breadth-first?
On page 9 you mention the [stream-monad](http://hackage.haskell.org/package/stream-monad) package. Unlike written there, it does not implement *earlier* work by Oleg Kiselyov but *subsequent* work. It is conceptually simpler and provides a more fair interleaving. (See first two entries on [Oleg's monad page](http://okmij.org/ftp/Computation/monads.html). The are other related packages on Hackage. Luke Palmer has implemented [control-monad-omega](http://hackage.haskell.org/package/control-monad-omega) and [level-monad](http://hackage.haskell.org/package/level-monad) provides breadth-first search and iterative deepening depth-first search. 
if time itself was treated as continuous, it seems that would necessitate some sort of analog computer Some processes are certainly simpler to model mathematically with continuous time, but I don't think all are. For those that are not, I would call them naturally iterative (cellular automata comes to mind). I think of time as a somewhat artificial concept in modeling the world, a side-effect of iterative processes.
Thanks for highlighting the monotonicity part. I was looking at the algorithm trying to figure out why it made sense otherwise.
Do what [FGL](http://web.engr.oregonstate.edu/~erwig/fgl/haskell/) does: Don't make the representation a research problem, just use a bleeding [Patricia Tree](http://www.haskell.org/ghc/docs/6.10.4/html/libraries/containers/Data-IntMap.html) A bag of Node -&gt; [Node] mappings is, btw, what imperative graph libraries use, too, though they most likely use stuff like arrays or a Hashtable. Messing around with pointers between nodes gets just waaaaay too messy. You see, if you want to have shorter hair, you can either shave your head, or ask some whacky scientists to make you a clone that has shorter hair. In the end, it doesn't matter where the boundary between mutability/immutability lies.
&gt; I'm not even sure how to talk about things as if they have some identity (which you need to be able to talk about "before" and "after" for the "same" thing). This is not a problem in practice. I find myself wondering what planet all you people live on who are downmodding me. All those other comments of mine that you did not quote; do you somehow not live in that world? Are you in fact capable of making a new object out of pieces of an older one while still having the older one around? Do you in fact have immutable copies of things lying around (because I'd like to get in on that action)? Can you name any way at all in which the real world actually resembles the functional programming world? Or have I tapped some sort of weird strain of programmer religious fundamentalism in which one's preferred programming paradigm must not just be the best way to program, but simply _has_ to be the way the entire universe is structured? I mean, good lord, denying the existence of [quantum state](http://en.wikipedia.org/wiki/Quantum_state)? It's right there in the name, for pete's sake! And they do not demonstrate the characteristics of functional programming either, so you can't pretend it's "not really state"; they are not immutable, there are ways of getting to a new state where you can not get back to the old one, you don't get your own immutable copy of anything. (Recall the notion of "mutation" fundamentally incorporates the notion of time, so you can't get out by saying that the given state at a given point in time is immutable; in that case, all variables and values and in fact everything is immutable so the entire term becomes useless. Note I said _useless_, not false; it may be true that the entire universe is immutable, but that doesn't help us write programs. Arguments that involve devaluing an entire term to "win" aren't winning.) And I'm just going down to the quantum world to show the problems at the bottom. _Clearly_ the classical world is even more clearly full of state. It has _every characteristic of state_ in the programming world that makes functional programmers decry state so. Seriously, make a list of the arguments against using state in your program, and every single one of them will apply to the real world. Every one. Unless you think you can reach under that layer to some hypothetical stateless layer below it, you're never going to live in a stateless world.
I did not argue anything about state (I could have, though) in programming, nor did I down-mod you. I am arguing with with your claim that quantum mechanics somehow implies that the real world has state that is being modified. That may be your interpretation of QM, but it's not mine. QM is far weirder IMO. Assuming we can assign identity to, say, the chair I'm sitting in, then I do live in a world where the chair right now is not the same chair as it was one second ago. They are separated by one second (in my reference frame) and will never meet, but they are not the same IMO. They are related, of course. (Mostly unrelated, but if one believes set theory then the Banach-Tarski paradox says you can take an object, disassemble it, reassemble and have both the old one and the new one. Well, with a very liberal interpretation of B-T. :) ) BTW, if you read another one of Conal's blog posts he also argues that being able to access the past from the present is something unnatural in FRP.
Fair point. "Partial" here means multiple things. I think the original expression merely uses "partial" to mean pruning search trees, e.g. alpha-beta pruning. Whereas I think you use it to refer to the incrementality in "partial evaluation," which I agree is an orthogonal issue.
if lists themselves were treated as infinite, it seems that would necessitate some sort of infinite computer. Some lists are certainly simpler to model mathematically as infinite ones, but I don't think all are. Indeed, infinite lists can only be modeled and not directly represented, just like continuous time can only be modeled and not directly represented. In the implementation side of things, there are only finite data structures and discrete time, but using these models is simpler for some purposes - which makes them more composable and general.
Is the introduction of concurrency consistent with Sabry's paper? A quick skim leads me to suspect not. So maybe we can look at FRP as one in a family of approaches to providing deterministic semantics to concurrency, or providing a deterministic semantics which holds in the presence of concurrency.
When discussing the purity of IO, one has to consider what purity means. In Haskell, we model `IO` as a strict state monad which passes around a `RealWorld` variable representing the state of the world, and I/O operations are essentially updates to this variable. This is a purely functional representation of IO. It's not the only way. The `ST` monad is modeled the same way, except that the state variable is specific to the particular computation and we use the type system to prevent you from joining to computations in ST that arose independently. Now a person might say: "That's just a model, but that's not how it *really* works, the way it *really* works under the hood is imperatively, there's no *actual* `RealWorld` variable," and they'd be right -- but does that mean that IO is not purely functional? You can make the same argument about arithmetic, or purely functional arrays. The former is modeled in the lambda calculus using church numerals and the latter by function composition, but in reality we don't use either of these representations, because they're slow. A purely functional array is still a chunk of memory that just happens to be copied on update, and arithmetic still comes down the CPU's representation of numbers. Are they not purely functional? Generally, when people say something is purely functional, they mean that it is well behaved: that it is referentially transparent, for example. Certainly, IO actions satisfy this requirement, as do ST actions. They behave exactly as you would expect them to. If you have a polymorphic function that takes an arbitrarily typed argument, you can pass it a value of type `IO a` or `ST a` and everything will work out just fine. These values don't require any special consideration. They are purely functional. The fact that purely functional languages are built as abstractions over a Von Neumann architecture does not mean that they are *not* purely functional, right? At the very bottom, it's all stores and loads, no matter what.
Thanks for that review. It seems that CHP's advantages are well behaved conveniences and additional functionality (barriers) as opposed to CML's closer to the metal feel.
&gt; Being "purely functional" is an absolute term, not a relative one (at least by some definitions) and has little to do with the power/usefulness of a language. In this qualitative sense, CPP and C are like Haskell and IO. Thanks Peaker. You got what I was saying: "pure" is absolute, not relative. 
Exactly! Lazy functional programming already specializes in the infinite. Thanks.
And if you do view it as stateful, please consider whether it's (sequential) *von Neumann* stateful. BTW, I don't mean to drag von Neumann's name through the mud by identifying him with the sequential von Neumann architecture. That architecture was quite helpful for its time, and he did other wonderful work.
I think bringing "the real world" into the discussion is philosophical, and you're getting downmodded for being philosophically short-sighted. From within the system (i.e., the universe), we have no way of determining if things are changing or are being recreated with slight changes every instant of time, or something completely different that we can't imagine. And you can't really apply Ockham's razor because no explanations at that level are fundamentally simpler. As for my opinion on the matter, I don't have one. My only interest in debating unknowable facts is to convince others who think they know them that they don't.
I have to take issue with this at a basically philosophical level: there's something similar to the fallacy of composition going on in a lot of this kind of thinking, namely that for some desired state there are many things that suck about System X, that therefore it is system X that is at fault, and that we must be liberated from system X into a higher level system Y, which by virtue of being more abstract, is free of limitations. You can see this pattern in mystics, religion, and in the wistful meanderings of programmers. The last category at least doesn't pretend at revealed universal truth (save for golden-hammer cranks) but still avoids admitting that it's all too possible to get "trapped" in high-level paradigms. I used to be a Zen Buddhist, and myself used to search endlessly for a "bigger picture". Then I got hungry, had a burrito, and was enlightened. 
I'm not really sure. It might depend on how you think concurrency is actually implemented. For instance, you can implement concurrency by having monadic combinators build up a term representation of threads, and then a scheduler will decide how to interleave the execution of the threads. Such a scheduler would seem to depend at most upon a source of (pseudo) random values, for when it has to make arbitrary choices between multiple threads, and random numbers are pretty easy to explain as a state monad holding a seed. Is this how things actually work on the machine? No. But, FRP won't really be `Time -&gt; a`, either. It's probably always going to be something of a judgment call whether your formal description is adequate, and how much of the real behavior is just an optimization of what the formalism describes.
Considering the controversy around [Can functional programming be liberated from the von Neumann paradigm?](http://www.reddit.com/r/haskell/comments/alx2v/can_functional_programming_be_liberated_from_the/), we in #haskell figured this would be appropriate.
 &gt; When discussing the purity of IO, one has to consider what purity means. In Haskell, we model IO as a strict state monad which passes around a RealWorld variable representing the state of the world, and I/O operations are essentially updates to this variable. This is a purely functional representation of IO. It's not the only way. It's not even *one* correct way. It's a popular but mistaken myth that IO is *modeled* functionally via the (functional) state monad. That model cannot express nondeterministic concurrency (`forkIO`). I don't usually make claims about whether something is "functional" or "pure", because there so much confusion and ambiguity surrounding these terms. I prefer more specific &amp; well-defined statements about simple &amp; precise denotational models. By design `IO` doesn't have one. After all, its purpose is to hold everything we do not understand "denotationally" (the "sin bin").
Or #4, do it in the ST monad with STRefs and go have a cheeseburger
Code brevity is very important, but it isn't the only measure of the resulting quality of the code. I think Haskell code is much easier to refactor and has much higher reliability than Python code, by virtue of just being in Haskell...
Also, an HTTP redirect to conal.net would be slightly easier to maintain.
[Haskell: America's Design-Build Leader](http://www.thehaskellco.com/en/Services/Construction/) 
Right -- but that's not deterministic, and if you make it so, then that's not what, e.g. Concurrent Haskell actually does. So its useful, but inadequate, I think.
but there are different kinds of infinity. An infinite list is denumerable, whereas I don't think continuous time is Now, I am not in favor of the von Neumann model either, although I am not sure at what level of abstraction you are attacking it. Can everything that we currently represent iteratively be alternatively represented continuously? For some reason I get the feeling that somehow that question is tied to the question of P = NP.
The view of IO as a state monad over RealWorld is sufficient for sequencing and state. If you to integrate forkIO then you can consider a set of possible orderings of the different actions. If I well remember this is part of the WhyNot project
Sorry I had the Attempt type in mind which is part of failure framework but not the only one. 
I have a fair bit of multithreaded code. I think this blogpost (along with the one on poison) just convinced me that if I had to do it again, I'd build it on top of CHP.
Why are monad transformers "imperative programs in disguise" any more than (some) monads themselves are?
Go back and read my first message. It doesn't matter what the "true nature" of the universe is. What matters for programming is what we can get to. I'm _explicitly_ not being philosophical, the entire point is to be practical. And what we can get to bottoms out in stuff with state. What's at the very bottom is totally unknowable because we can't penetrate past Turing completeness, and it doesn't matter because we can't program at that level. I'll worry about that level of the universe when we can write programs in it. Let me know when you do that. Either that or let me know when you disprove quantum mechanics. The amount of people swaggering about being sophisticated here while aggressively misunderstanding my point has been quite distressing; I expected better. Nobody actually disproved my point; everybody had to retreat into the unknowable. Meanwhile, my point about the actually-knowable stands essentially unaddressed. Probably because it's basically undeniable; denying the actual stuff we can reach and program in is stateful requires extraordinary mental gymnastics, and that's exactly what we see.
Wow, thanks for that clarification! I'll update the article accordingly.
Yay!
I'm not sure it poses much more of a problem than randomness, really. A seed in a state thread with a pseudorandom generator using that seed is deterministic. But in practice, IO starts each program with a genuinely different/random seed, determined by nothing in your program. Likewise, interleaving state threads using some (pseudo) random source to introduce (pseudo) non-determinism into the scheduler can be seen as actually deterministic, in the context of running the scheduler with some known seed. But that seed is inaccessible to the program, and different every time (potentially), which looks deterministic. I was more concerned about the fact that GHC can actually preempt based on memory allocation, so you can get race conditions that don't exist in the coarser grained term representation above; like writing a multi-word value to an IORef.
OK - it's in! Thought you might want to know.
To be honest, I'm not actually sure what you're saying. It seems to be lost among a bunch of verbiage which, to me also, looks philosophical and irrelevant. Can you please restate your point in one simple, clear sentence? 
Thanks, Peaker. I'm surprised anyone would take my article personally or read insult into it. I expect some folks to be curious, some inspired and the rest neutrally disinterested. Unless, I guess, if my words resonate with a reader's repressed internal criticism.
The Wikipedia article seems to disagree on this point: http://en.wikipedia.org/wiki/Backtracking
I don't think that anyone has said that FRP is free of limitations. Your argument just seems to be a generalisation of an essentially conservative political argument.
A very deserving candidate.
Yep. Peaker, you got it again. I think you actually *read* my posts instead of reacting to what I might have said in them but didn't. Forget "purity". I'm talking about precise, simple denotation. I'm talking about the strong mathematical properties, and hence good composition. As in John Backus's Turing Award lecture.
They're not. It was a play on "Transformers: Robots in Disguise", one of the series in the Transformers franchise.
&gt; Your argument just seems to be a generalisation of an essentially conservative political argument. Your argument seems to be a generalization of the ["it's obvious" argument](http://conal.net/blog/posts/fostering-creativity-by-relinquishing-the-obvious/). 
Q: Can functional programming be liberated from the von Neumann paradigm? A: Maybe. 
I've fixed now.
Monads are purely functional. IO is not. IO is necessary if you want to do something other than just calculate the results of functions on a computer. Monads provide a clean way to encapsulate the non-functional IO so that the rest of the language can remain purely functional, but IO can still get done. Haskell is not a purely functional language in the strictest sense of the term. It is a language that enables purely functional programming without remaining completely academic. It allows huge swaths of a program to remain purely functional and strongly encourages a purely functional style. That's good enough for me to call it a functional language. More so than Lisp, which often gets that title. I think "Purely" is tacked on to differentiate it from Lisp or F# or O'Caml which are must less strict about their functional properties, and although it's technically a misnomer, I don't think anyone needs to get their panties in a twist about it. 
I've been using CHP for a few months now, and I'm a satisfied customer. It is well behaved and easy to use.
Section 1 of the Wikipedia article (Description of the method) states that the algorithm traverses the search tree in depth-first order. So, also according to Wikipedia, the order in which "partial candidates are abandoned" is a distinguishing property of backtracking.
What's the difference between a continuous model and an infinitely precise number? And what's the difference between an infinitely precise number and a list?
I hadn't thought about eschewing the bridge and calling Cocoa just like C. Then again, I agree that marrying Haskell and Obj-C has a lot of drawbacks and not much utility. In any case, he's doing more work than necessary, with some of those build settings. It should also be possible to create a compiler spec for Xcode so it can invoke GHC directly (though that's relatively undocumented, you have to look at the existing specs to see how that's done). I'm also left wondering how effective it would be to try and call objc_msgSend() directly through the FFI in order to interact with Obj-C (though there's some edge cases there, like when the various variants are used and which are entirely architecture-dependent).
The end of `transCatch` can also be written `catch (fmap return io) (fmap return m)` for maximum symmetry. =)
An even simpler Haskell version: import Data.List.HT main = print $ groupBy ((==) . succ) [1,2,3,5,6,8,9]
next version of GHC should have dynamic libraries working for Mac (they're already working for Linux), so you can at least avoid the pain of hunting down the .a files.
I can look at this and immediately tell you that it's efficient. Whereas the rather convoluted definition in the article looks like it's inefficient, although I could be wrong, as I didn't take the time to study it in depth.
&gt; Monads are purely functional. IO is not. And yet `IO` is a monad. &gt; IO is necessary if you want to do something other than just calculate the results of functions on a computer. In the implementation, but necessarily in the programming model/semantics? 
In this post and elsewhere, I recommend shifting the discussion from "functional" to "denotational", i.e., having precise &amp; simple denotation and therefore strong mathematical properties. I don't know if this message is getting across. Did you miss it? Any pointers on which passages of mine are hard to understand? Edit: I ask because I keep having the experience that I write about the denotational perspective, and then people respond as if I'm talking about "pure" and "functional".
&gt; &gt; Monads are purely functional. IO is not. &gt; &gt; And yet `IO` is a monad. I think that I understand the point that the article is making, and I believe it's a valid one, but this looks like intentional typographic confusion. There is nothing wrong with saying that "Monads are purely functional. IO is not.", even though "`IO` is a monad"—because there's a difference between the monad `IO` and the programming concept of IO (which I would write I/O). I can't help being facetious and pointing out that, similarly, the fact [Io](http://www.iolanguage.com) is object oriented sheds no light on the present discussion. :-)
While cute, those tricks always bug me when I see them in real code. It's often really hard to think about the same function name being used by multiple typeclass instances at the same time. In this case, liftM and (.) speak exactly to what you are trying to do and so feel more clean to me.
doliorules and i had a chat and cleared up a number of misunderstandings.
shouldn't error monad be the top layer? then you can provide MonadIO instance for it, that runs IO computation, catches all errors inside it, and either returns result, or wraps that exception inside your error monad.
I agree, that's why I used liftM and (.) in the real code and in the example. =)
I'm not totally sure I understood what you were saying. But if I did, then I don't think it addresses what I want to do. I explicitly don't want errors (exceptions) to stop the computation; I want to recover and continue. The error monad I know (Either) simply stops the entire computation as soon as an error (Left) occurs, bubbling it up to the top level, which is not the same. That said, I did discover `catchError`, which does more or less what `transCatch` above does, but with a different class constraint. I'm using that now. =)
If only the typical computer science curriculum was more mathematically- than professionally-oriented, there would be fewer blank looks when you reference "denotation design." Mark Kac: The main purpose of professional education is development of *skills*; the main purpose of education in subjects like mathematics, physics, or philosophy is development of *attitudes*. 
I did something like this a few years ago and my main problem was to garbage collect stable pointers that were allocated on the Haskell side. Taking the currency converter as example, there is a Haskell function convert :: String -&gt; String that does the calculation, i.e. reads the amount from a `String`, converts it to another amount and then back to a `String`. Then, the result is put into a stable pointer and passed back to Obj-C, but the problem is: how to garbage collect that stable pointer from Obj-C? My solution was horrid, I basically made Haskell-land responsible for deallocating storable pointers and implemented some ugly form of manual deallocation. At least, that's what it should do to be useable. I think this is where HOC helps big time, taking care of garbage collecting Haskell stuff from Obj-C and the other way round.
What Paczeslowa is saying is that `Wyvern` should be an error monad already. In other words, there is a function catch :: Wyvern a -&gt; (Error -&gt; Wyvern a) -&gt; Wyvern a which also catches any exceptions from `IO`, like this catch (liftIO $ readFile "foo") (\e -&gt; liftIO $ print e) The idea is that this is entirely equivalent to first catching and then lifting liftIO $ catch (readFile "foo") (\e -&gt; print e)
Right, and even if you don't know the appropriate library function (Haskell's prelude is much bigger than F#'s/OCaml), generalizing the recursion is always a preferable alternative to direct translation. *Edit:* And in any case, a [control break with advanced reading](http://alaska-kamtchatka.blogspot.com/2008/10/extreme-recursion.html) is the way to solve these grouping problems. Old techniques never die.
I kind of did my own work on the Haskell Currency Converter last November, but still have not published it yet. I then created a ObjC proxy object "HSConverter.h" this is basically how it looked like: extern HsStablePtr c_newConverter(double amount, double rate); extern void c_freeConverter(HsStablePtr converter); extern double c_convert(HsStablePtr converter); @implementation HSConverter -(id)initWithAmount:(double)amount rate:(double)rate; { [super init]; converter = c_newConverter(amount, rate); return self; } - (void) finalize; // this will be called by the GC { c_freeConverter(converter); [super finalize]; } -(double)convert; { return c_convert(converter); } @end The creation of HsStablePtr is triggered by ObjC and so it is not a problem to gc it from ObjC again from finalize. But I am just starting with Haskell and so I might be missing something. 
I'm in Singapore this week and don't have anything planned for tomorrow night, so there's a good possibility I'll show up for this.
I have to admit that part of me has a strong dislike for mechanized proofs; for example, I consider the proof of the 4-color theorem an atrocity. The purpose of a proof is to explain *why* a theorem holds, not that it merely happens to be true. A machine telling me that a completely undecipherable proof is true would not satisfy me one bit, to the contrary. Much to my delight, the author shares this sentiment: Introduction, [Engineering with a Proof Assistant](http://adam.chlipala.net/cpdt/html/Intro.html#lab9) &gt; In comparisons with its competitors, Coq is often derided for promoting unreadable proofs. It is very easy to write proof scripts that manipulate proof goals imperatively, with no structure to aid readers. Such developments are nightmares to maintain, and they certainly do not manage to convey "why the theorem is true" to anyone but the original author. One additional (and not insignificant) purpose of this book is to show why it is unfair and unproductive to dismiss Coq based on the existence of such developments. 
I do hope you'll write up your findings. I've read up on Coq several times now but, sadly, can't seem to find a post to hitch my horse to, as it were. I _want_ to like the language, but every free introduction seems too dense, too trivial[1] or both. Coq'Art by Bertot and Castéran is a great improvement over free materials, but the resulting code/proofs are so impenetrable as to suggest that learning and becoming proficient in Coq would hardly be worth the trouble. [1] A treatise with motivating examples of minor complexity is said to be _trivial_. EDIT: Formatting. I rather wish Reddit had a preview feature for posts.
Wow, that's really nice and succint. I should have mentioned in my post that I'm still inexperienced in FP in general, and a total newbie in Haskell. However, it was not the point of the article to come up with the most efficient solution for that concrete problem, but rather to show some common pitfalls when translating Haskell to F# and doing recursion over lists in F#. It's really a post about F#, not Haskell, I wouldn't have posted it on haskell-reddit.
Then, of course, there's the flip side. With Agda (say, since it's what I know best), proving is _difficult_. And it isn't merely difficult in the sense that figuring out a strategy for proving the theorem is difficult, but in figuring out all the individual rules that need to be applied precisely to justify "this holds by induction on n". It's similar to translating a high-level algorithm into assembly (though not nearly as bad, naturally). Coq, by contrast, lets you write down something closer to a proof sketch, with the machine filling in the details. But I agree, I share a similar distaste for the actual result. So it seems we haven't yet found the ideal (or even, particularly good) abstractions for proving.
The [ghc-goals](http://github.com/sebastiaanvisser/ghc-goals/) project tries something similar to this. Agda has the `?` which gets turned into holes.
NSFW
happy to have you
I said something else, but I meant what you said:) bad english+being dumb makes things hard to explain.
This seems like a poor way to do these sorts of things, as the first comment says.
what about coq-ide?
I disagree. understanding proofs is only needed if you want to modify the code (or you just like proofs). how often do you modify code from some library? usually, you just use that library. which version of qsort would you use: entry-level one, or some highly specialised version running in ST monad, that's based on some research not yet publicly available, that looks like shit but promises to be faster in 2% cases? but, if it provides proof of correctness, do you really care how it works? also proofs are great for your own work, because you don't have to understand them! you write a theorem, proove it in so many lines that sometime in the middle of the proof you forget what you're trying to prove, so you split into more cases, type stupid things and finally coq swallows Qed. you still have no idea why does it work in the big picture, but hey, it typechecks. writing that proof as a function in haskell, you'd immediately have doubts. with coq, I look at the [proven] theorem and I have no doubts, I don't have to think "what if", I don't have to think. I like it that way.
If you could write the let bindings on more than 1 line you could end up with something just as powerful as a makefile, and fairly similar in complexity. One thing you can't have a makefile do is run a test in GHCi - which is how I tend to run my tests anyway.
Good points! In that light, I refine my opinion: * I have a strong dislike for proof-finding * I don't like proof-checking either, in the sense that I value proofs that are mostly handwaving, hence completely unchecked, but explain what is going on. (For me, a prime example is the generalized version of [Stoke's theorem](http://en.wikipedia.org/wiki/Stokes%27_theorem).) These kind of proofs tend to lose all of their value when crammed into an automated proof-checker. But then again, textbook proofs aren't necessarily written in an enlightening fashion either and the other part of me sees automated proof checking as an *opportunity* to write down more enlightening proofs. Namely, if my proof is inundated in tedious details, teaching it to the computer will make it even more tedious, to the point that I'm forced to think about clarifying the structures that make my proof work.
I've tried reading it long time ago, but it was too hard as a tutorial, and it was missing too much text (it's a draft after all), don't know if it's still the case. I think this could be a better tutorial: http://www.cis.upenn.edu/~bcpierce/sf/
I take *lambda biloba* daily in the hope of becoming smarter. ;-)
This post reads a lot like personal notes unfit for external consumption. I stopped reading after this: &gt; I like very much the typeclass feature of Haskell, I already used it for many things. But hey, HNN is a project for the Haskell community, I just can’t throw some typeclasses in it without actually thinking about why it would be good, and why it wouldn’t. Wouldn’t it be overkill and thus make haskell code using HNN less nice ? All that waffle doesn't add up to anything of substance. Rereading the [haskellwiki](http://www.haskell.org/haskellwiki/OOP_vs_type_classes) was a better use of my effort. 
Looks fine to me. Ah, and my troubles were in a time when Obj-C did not have garbage collection yet. I think my real problem was that the part of the Cocoa framework that I was interacting with (`NSTable`) put the burden of ownership on my Haskell code. But Haskell's garbage collections relies on the users of a memory object to signalize when it's no longer needed.
The web site for the event is: http://haskell.org/haskellwiki/ZuriHac
I think I used all of them plus a few more. I don't know if that is good or bad.
I think the only one in the tower that I have never used is ImpredicativeTypes.
Haskell got 4chan'd?
One of the things we want to be "free of" is the "one-word-at-a-time" notion, which is at the heart of the Von Neumann machine...
It's a blog. And the post title starts with "Some thoughts on...". You knew what you were in for when you clicked. 
In your docs you quote 'man stdin' so: &gt;"...Under normal circumstances every Unix program has three streams opened for it when it starts up, one for input, one for output, and one for printing diagnostic or error messages..." &gt;"...The stdin, stdout, and stderr macros conform to C89 and this standard also stipulates that these three streams shall be open at program startup...." &gt;So now I'm confused... are these standard file handles always open on program startup or are there abnormal situations when they are closed? --- I think the answer to this is that they can be closed in situations where someone has written code that does its own fork() and exec() - where the parent and child processes have some "understanding". I don't think you need to worry about the possibility of this happening in a Haskell program.
 $ ghc --supported-languages | ghc -e ':m + Data.List Data.Ord' -e 'interact (unlines . sortBy (comparing length) . lines)' CPP PArr GADTs Arrows Generics MagicHash Rank2Types RankNTypes RecordPuns RecursiveDo QuasiQuotes ViewPatterns TypeFamilies BangPatterns MonoPatBinds PatternGuards UnicodeSyntax TypeOperators UnboxedTuples KindSignatures EmptyDataDecls NamedFieldPuns RelaxedPolyRec ImplicitParams PackageImports TemplateHaskell ImplicitPrelude RecordWildCards PostfixOperators ParallelListComp UnliftedFFITypes FlexibleContexts TransformListComp OverloadedStrings PatternSignatures FlexibleInstances ImpredicativeTypes StandaloneDeriving DeriveDataTypeable LiberalTypeSynonyms ScopedTypeVariables IncoherentInstances ExtendedDefaultRules TypeSynonymInstances OverlappingInstances UndecidableInstances PolymorphicComponents MultiParamTypeClasses NewQualifiedOperators FunctionalDependencies MonomorphismRestriction ConstrainedClassMethods ForeignFunctionInterface DisambiguateRecordFields ExistentialQuantification GeneralizedNewtypeDeriving 
&gt; The biggest problem I'm having with Coq is the user interface. On the command line it's hopeless. If I undo a few things I really can't tell where I am and it's pretty bad if you're fumbling around unsure what's going to work. I've started playing with Proof General but it's making me read a lot of documentation about Isabelle, much of which doesn't apply, in order to learn how to use it with Coq. (And I need to relearn emacs which I gave up 20 years ago...) Ditto. Up to this point, I used proof general a lot, but I haven't got the latest release working with emacs 23.1.1 yet (emacs is my typical environment.) Personally I think you should probably give CoqIDE a shot. The nice thing about it is that it's pretty easy to tell where you are in the given file you're editing. You can just step through every line as if you were in a debugger, and if you think you've done part of your proof wrong you can just step backwards and rewrite it. Also has some minor syntax highlighting and error reporting, etc. I've started using it now and I think it's pretty nice. I've tried coq several times too (and looked into a lot of adam's other work: ur/web, ynot, etc), and it has been hard to really get over the 'hump' he describes here. I looked at CPDT a while back but much of it still seemed unwritten and since then it seems to have been updated a lot. Hopefully I can have proven programs soon. :)
I was going to criticize the lack of approximate centering, but then I wondered what the best way to do that was. Maybe something like this? X XX XXX XXXX XXXXX
Why is FlexibleContexts on there twice?
Well, it depends. Agda can be easier for proving things by supplying terms which inhabit dependent types, I think.
Unicode has more space characters, but your fixed width font will probably still make them the same size. Here is a test: With HAIR-SPACE X XX XXX XXXX XXXXX With THIN-SPACE X XX XXX XXXX XXXXX 
Why do *you* Haskell?
&gt; but when we bring in small bits of the imperative world to help us in cases such as the State monad, we acknowledge the imperative paradigm is useful. I wouldn't exactly call the State monad imperative. 
A priori, "imperative" isn't a dirty word, lots of problems naturally lend themselves to solutions that boil down to "do this, then do that, and modify this state while you're at it." And if you couldn't "fire the missiles" then your language would be useless. In most other languages, however, you're *forced* to accept the fact that any random piece of code you execute can have arbitrary side effects, potentially touching all kinds of hidden shared mutable state. Post-Haskell, I've come to regard this as being rather rude!
Because it's so much nicer than the alternatives.
* It's challenging. * It's powerful; less code does a lot more. * It makes me think; I spend a lot more time designing and verifying on paper than I do writing code. * It's clear that it's much better to plan first than it is to start writing code. * It makes me look at imperative systems in an entirely new way. My favorite accomplishment is when I implemented Prolog-style back-tracking. I spent 3 days working on notepads trying to figure out how to do it. Eventually, I figured it out did some proofs (6 years since I graduated with a BSc CS and this is the *first* time I've done a proof to solve a real problem). The code ended up being under 100 lines and all of the unit tests passed on the first run. Eventually, I plan to use this code in rules-based AIs for video games.
Because the type system lets me focus on the hard, creative parts of my project.
Heh, that just reads like a complete success story :)
Because I can :)
Axman6 raised a similar point in the blog comments; I've reposted my response here: I probably used inappropriate/misleading terminology in that sentence. And I also do not mean to belittle the State monad, which is far easier to reason about then spaghetti state in imperative programs. But when I build a finite state machine because I need, say, an HTML5 parser, I am tying myself to not only an implementation but also a way of thinking about the problem domain. And while it is not difficult to do theme and variations on this base implementation, it is substantially more difficult to find a paradigm that is better equipped to solve the problem more elegantly and more quickly. Maybe such a solution doesn’t exist, or maybe it’s difficult to understand in other ways. Maybe the State monad really is good enough. But I can’t help and wonder.
I'm probably alone in this, but I use haskell, because I don't like to think: * it's functional, and recursion+pattern matching is much easier than regular, imperative loops that were always giving me trouble (I always had one-off errors). this is the reason I became interested in haskell. (come for the fun) * it's pure - I don't have to think "what if" all the time, "what if it fails?", "what if there's an error?", also I don't have to trust other (or even mine) code and read docs to know that it will be ok. (stay for the purity) * types. I can write something that looks good, then I can play with it (without thinking) until it typechecks. (liftM here, ($) there). bang it works. * types2. I love the idea of advanced types, that forbid me from doing things I'm not supposed to do. there shouldn't be a way for the user to hurt himself with library code. * Oleg. because! 
Hackage can't build it because it lacks the C library, but if you have the C library, it works fine. Docs aren't available through hackage though, so I've added a homepage and generated docs there.
Hackage can't build it because hackage can't build its Sprig dependency, but if you have the C Sprig library so you can build the Sprig libraries, it builds and runs fine. Docs aren't available through hackage though, so I've added a homepage and generated docs there.
Do :{ and :} work for multi-line input in a .ghci file?
Just let Data.Text decide; piping onward with | ghc -e ':m + Data.Char Data.Text' -e 'interact (Prelude.unlines . Prelude.map (unpack.center 60 (chr 32). pack) . Prelude.lines)' CPP PArr GADTs Arrows Generics MagicHash Rank2Types RankNTypes RecordPuns RecursiveDo QuasiQuotes ViewPatterns TypeFamilies BangPatterns MonoPatBinds PatternGuards UnicodeSyntax TypeOperators UnboxedTuples KindSignatures EmptyDataDecls NamedFieldPuns RelaxedPolyRec ImplicitParams PackageImports TemplateHaskell ImplicitPrelude RecordWildCards PostfixOperators ParallelListComp UnliftedFFITypes FlexibleContexts TransformListComp OverloadedStrings PatternSignatures FlexibleInstances ImpredicativeTypes StandaloneDeriving DeriveDataTypeable LiberalTypeSynonyms ScopedTypeVariables IncoherentInstances ExtendedDefaultRules TypeSynonymInstances OverlappingInstances UndecidableInstances PolymorphicComponents MultiParamTypeClasses NewQualifiedOperators FunctionalDependencies MonomorphismRestriction ConstrainedClassMethods ForeignFunctionInterface DisambiguateRecordFields ExistentialQuantification GeneralizedNewtypeDeriving 
...assassin's guild?
&gt; Which version of qsort would you use [...] but, if it provides proof of correctness, do you really care how it works? Ah, I had another context in mind, namely proofs in mathematics (say [Fermat's Little Theorem](http://en.wikipedia.org/wiki/Fermat's_little_theorem)) as opposed to proofs about implementations being correct (say the `sort` from the library works correctly). I agree that in the latter case, I don't really care what the proof is and I wouldn't even bother to check it by hand. But I think that this is because the theorem it proves is uninteresting, I would just assume that `sort` is correct anyway, even if the library did not include a proof. &gt; Also proofs are great for your own work, because you don't have to understand them! You write a theorem, prove it in so many lines that sometime in the middle of the proof you forget what you're trying to prove, so you split into more cases, type stupid things and finally coq swallows Qed. Well, I don't think this is every going to work for interesting theorems. Sure, you can outsource fiddly logical manipulations to Coq, but it's not going to magically come up with insights or key lemmas. For instance, proving that the standard `qsort` implementation works does require a few key insights, namely that `xs ++ ys` preserves sortedness if `xs` and `ys` have the additional property that all elements from `xs` are smaller than those in `ys`. 
&gt; For example, a proof of the strength of some encryption algorithm can be put to good use long before the proof itself is understood (assuming, again, that we can trust the proof assistant). I don't think that it's possible for someone to devise a new encryption algorithm and prove it strong without having an intuition of why it should be strong. Proof assistants can take care of the fiddly details, without this key intuition, they won't have a chance at finding a proof. &gt; If you accept only beautiful proofs, then you limit yourself to proofs that are within the intellectual grasp of humans. Yes, pretty much. However, "intellectual grasp" is much broader than it may seem like. For instance, Tarski proved that the theory of [real closed fields](http://en.wikipedia.org/wiki/Real_closed_field) is *decidable*, i.e. there exists a computer program which can prove or disprove any proposition about the real numbers if it involves only addition, multiplication, inverses, comparisons and quantifiers. I'm happy to accept a computer performing the tedious but thanks to decidability "straightforward" steps for me. But I do want an explanation of why such a decision procedure exists in the first place. 
Yes! Thanks a lot, that makes it quite pleasant.
I was rather mystified that one of his complaints was that you couldn't take 2 pure functions, A and B, and decide to make them dependent upon state modified by the other. That sounds like a terrible thing to do!
nice, thanks. How can you explain the little balls speed reduction during a chain reaction ? Is it possible to fix it ?
The flexibility, the static verifyability, the compiler-assisted design and the auto-documentation. Static verifyability, compiler-assisted design and auto-documentation is provided by the type system. It saves you from writing useless unittests that don't test any program logic, just that you didn't make any stupid mistakes such as calling an integer, or taking the union of file handles. A type system as powerful as Haskell's also helps in the design of your program by bein able to tell you where you are and where you've got to go; sometimes it's enough to mindlessly fill in the blanks with something of the right type to make it work. It also provides pretty decent surrogat documentation when you (or someone else) have been too lazy to write it. The flexibility is in the term language; what actually exists at runtime. Almost everything is first-class, which means almost everything can be abstracted away. This leads to few "design patterns" (in the Java et.al. sense), since you can just wrap it in a function and be done with it. This leads to less clutter in the code (and indeed, less code overall). You don't have to worry about the same niggling details over and over again, leaving your brain free to forget a problem after only solving it once. Purity is also very nice, and I'm a big fan. The separation of evaluation and execution in the type system encourages separation of I/O and algorithms, making both easier to reason about, not to mention how referential transparancy and equational reasoning are very powerful tools that are only available when you have purity. However, even in the parts of your code where you have to forsake purity, the flexibility and static verification still help you write cleaner, more correct code. This is why I Haskell.
http://www.mit.edu/~assassin/
I'll have to remember that point when countering the widespread claim among non-Haskell programmers that the state monad isn't "functional" and therefore Haskell is a cheat; not only is it actually functional, but it also rigidly contains the "state" in a single value as you say. And I don't know about you, but I'm yet to write a state value that has three or four hundred fields in it. (And the other point I make is that it really is nothing more and nothing less than threading the state through a series of functions, an ancient truly-functional technique. If you take the time to look at the definition of the monad, which I just happened to be doing last night, it's right there staring you in the face.)
Ones I have never used: PArr Arrows RecordPuns RecursiveDo UnicodeSyntax NamedFieldPuns PackageImports ParallelListComp UnliftedFFITypes TransformListComp LiberalTypeSynonyms NewQualifiedOperators ConstrainedClassMethods DisambiguateRecordFields Ones I have used and regretted later: IncoherentInstance Ones that I wasn't aware of and now might use: LiberalTypeSynonyms ConstrainedClassMethods 
It was fun to meet and hang out with you guys.
I really wish more people distinguished between "concurrency" and "parallelism."
Fear not! Simon Marlow is one of those people who makes a clear distinction.
UnicodeSyntax is really nice. I use it for all my projects now. You can write code like: (∘) ∷ ∀ α β γ. (β → γ) → (α → β) → (α → γ) (∘) = (.) PackageImports is useful when you want to run a module from a cabal project in GHCI and there are mutually exclusive packages, like mtl and transformers. But I think I can now replace PackageImports with [.ghci files](http://neilmitchell.blogspot.com/2010/01/using-ghci-files-to-run-projects.html).
I don’t see any slowdown. The animation should be smooth unless something is wrong with 3D acceleration on your end.
dear mister MechaBlue, the list you presented in your comment is perfect. I agree with you on every point of it. the third point has actually driven me to buy a whiteboard today (fresh BSc CS here, needed the money for books and booze. well, actually I needed it only for books). &gt; it's challenging definitely, and that's the part I like most of the language. &gt; it's powerful nothing says 'DO ME CONCURRENT' more then a pure functional language. and the strong typesystem, damn it's really powerful (although it's difficult to hunt down bugs) &gt; it makes me think yes. actually, for every piece of code I've written using Haskell I used at least one sheet of paper. &gt; It's clear that it's much better to plan first than it is to start writing code. yep &gt; It makes me look at imperative systems in an entirely new way. I knew that by learning me a Haskell for a greater good I would look at imperative languages in an entirely new way. it went so far that I once missed *map* and implented it. I definitely am looking forward to dive deeper in the fascinating world of Haskell 
Alternatives, where?!!!!
If there's anything missing that has really bit me, it's been the answer to the question "why?" Why do I want $GHC_FEATURE? When is it a good idea to use? When is it a bad idea to use? Sometimes a single example is given, but those aren't always very helpful because then you can't tear apart what's just plumbing for the example vs. actually relevant information, and it still doesn't help to know the why. This isn't _completely_ uncovered, but there's way, way more of it stuck in oral tradition than either I or Google am particularly comfortable with. Being on an IRC channel really shouldn't be a prerequisite for learning a language.
I have been trying to pick up Haskell over the last 3 or 4 weeks. I have basicly spent the last 28 year coding in various imperative languages. Documentation is always a problem for most languages. I think the biggest problem right now for Haskell documentation is that it tends to stress to much on the theory of functional programming and not enough on the practical here is the language. At some point one will be forced to delve into more of the theory and academia of the language particularly if one wishes to professionally code Haskell but from a practical standpoint one needs to code incrementally harder and harder programs to learn the language and have a body of work to reference and understand so as to have a fundamental understanding of the language to apply the theory to. It held true in learning imperative languages and I suspect it will hold true for functional languages as well "you can't learn a programming language by reading about it, you learn a language by coding." Right now I feel that I have been reading for weeks but haven't pickup enough of the basic language to code much of anything Now before everyone in the Haskell community goes all ballistic on me I might suggest that you take the time to read through the various Haskell threads and see how many people have struggled to pickup Haskell. Now on first blush this is attributed to the the paradigm shift from imperative to functional programming but might I suggest it might be more do to the academic style of writing. Where are the Cliff Notes!
As a Haskell enthusiast, I wouldn't come down on you for any of the sentiments in your comment just above. A lot of things were hard for when I was first learning. Today I feel solid about my skills, but even so I feel like I'm still getting better at a noticeable pace.
A while back I remember people saying the future is functional because it is "inherently parallel". Not only is the words: * par * seq * pseq very confusing, it makes the code very explicit, verbose and messy.
I've been thinking a lot about haskell's type system as it compares to testing (I'm a ruby dev for a living), and clearly its better for the case where you need to "get it right" the first time. But in application domains where some bugs are acceptable (web dev) it _can_ be a handicap because you can't dive in and debug your logic easily. EDIT: and you should have tests either way, you just need less with Haskell. 
There is simply no general purpose auto-parallelized language. You either program in a restricted subset, or you give parallel annotations.
A book on optimizing Haskell code would be great indeed.
The monad transformer provided in the split of loop-while package is identical to [MaybeT](http://hackage.haskell.org/package/MaybeT) which could be reused to implement 'loop' and 'while': loop :: Monad m =&gt; MaybeT m a -&gt; m () loop l = body where body = do x &lt;- runMaybeT l when (isJust x) body while :: Monad m =&gt; Bool -&gt; MaybeT m () while b = MaybeT . return $ if b then Just () else Nothing The MaybeT package lacks an Applicative instance, the loop-while package lacks a number of instances provided by MaybeT. The Functor instances are implemented differently but should coincide for reasonable monad instances. The monad instance of MaybeT provides an implementation of 'fail' in terms of 'Nothing', the loop-while package doesn't.
The problem is the following, consider the following two language design options: - Explicit parallellism. You write parallell code where you want it. Downside: verbosity. - Implicit parallellism. Every function is executed in parallel. Downside: overheads. I think Haskell, with annotations, has chosen an elegant path between those two options.
And if you go for implicit parallelism then the problem will be that the compiler will do amazing auto-parallelisation in some cases you didn't really care about and in the case you really did want to parallelise it'll not be able to work it out and you'll go back to explicit/annotated parallelism. To some degree we already have this "magic optimising compiler" problem with ghc in that to get the best results you do need to know a bit about what the mighty simplifier does and does not do. Of course it's not magic, it's a rewrite system but if you don't know what rules it applies then it can seem like magic. The point is, the more dramatic the speedup due to some optimisation (including paralellisation) the more important it is that it be predictable.
girlfriends?
After some other thought, this would also help counter people who claim "Haskell has no good way to do X" if you could just point to the web page that explains how $GHC_FEATURE does X nicely and here's some examples....
Certainly someone will complain about being general purpose and a whole host of other things, but don't forget about languages like [Verilog](http://en.wikipedia.org/wiki/Verilog) and [SystemC](http://en.wikipedia.org/wiki/SystemC) where everything is (very-fine-grain) parallel by default, and you have to work to make things sequential. (Not that anyone is going to start to use these languages for anything other than HDL tasks.)
I don't have a CS degree, and I lack a strong maths background, so there have been a lot of times in my learning Haskell when I've had to stifle the feeling of being the five year-old in the room when all the grown-ups start talking politics. That said, I've actually found documentation to be *less* of an issue in Haskell than it is with some other languages and systems I've seen. I'm always surprised when I look at the source code in the Prelude how *clean* the logic is. I've never had that feeling looking at systems-level C code. I also feel like Haskell the language is well-served at the moment by its various books and tutorials. Between RWH, Hutton, Hudak, Learn You a Haskell, and YAHT, it feels to me like there are answers out there for just about anything I need to learn. OTOH, I feel like Haskell the platform is a bit of an orphan by comparison. There's a lot less good information on the *tools* that surround the platform than there is on the language itself. For instance, I've spent the last few weeks trying to figure out why my Windows OpenGL library won't link in GHCi, even though it links and runs just fine when compiled using "ghc --make". Google searches have yielded little, and the responses to past mailing list inquiries have been anything but encouraging. I know I'll find the answer, eventually, but if if it's at all possible, I'd like to find the answer *faster*. The current documentation for GHC is decent, but there's plenty of room for improvement, especially on the subject of optimizing, debugging, interfacing to external libraries, etc. There are a lot of little tricks and traps in the interface between GHC and gcc that I, for one, would love to learn from.
It seems the Haskell community fell in love with epoll et al. There were already bindings to libev and to epoll, hack-handler-evhttp and hack-handler-epoll and finally now this. Quite a few.
Missing: An annual Haskell day parade. Unless this counts: http://www.youtube.com/watch?v=7X9VEwuliqw
Why not write a parser that accumulates the results as you go with lazy IO? Your parse trees are likely not very deep, so it would be reasonable to use an LL(1) parser via Parsec, or you could use (the more complicated) Happy to generate a (likely) more efficient parser. You could attach semantic actions to the nodes to build a count or collect a subset of posts for display.
Nice work! You should post in the comments of the blog you linked to asking for an update of the benchmarks. Knowing jdh, the comment may be deleted, but it's worth a shot.
Try the [haskell cheatsheet](http://cheatsheet.codeslower.com). Yes, I am the author :)
We're awfully short on "rock-stars" and "superheroes". Er.. unless you count bos and dons.
wow. does anyone else see a spiral going from the bottom to the top of that?
I'm in the exact same boat that you are. I have spent the last few weeks attempting to learn Haskell and still feel as though I can't write anything worthwhile in the language. Mostly because anything I want to do involves IO of some sort. In Real World Haskell, IO is not covered until Chatper 7 (page 165). In the meantime you have to slog through an almost exhaustive list of list operations and information on data types. This makes the book work great as a reference guide to people who already know the language but tough for people who are looking to be "up and running" any time soon. Haskell's list operations are very powerful but it's hard to appreciate that power when every list that I process is trivial and typed into ghci by hand. In a book on imperative programming I would not hesitate to skip to Chapter 7 to grab the information I need because I'm already familiar with most of the concepts in the early chapters; this is not true in Haskell. Much like Isumnler I am anxious to learn enough to actually start programming and begin really learning Haskell. Isumnler is also correct in stating that a language barrier exists between the academics in the community and people who are attempting to learn the language. The place I first noticed this was in the error messages from ghci. If you do not know category theory then some error messages are completely opaque. What is an infinite type? I have no idea, I just read it as "wrong type." Someday I would like to learn category theory and the lambda calculus but for now I am just trying to learn Haskell. Don't assume that I dislike Haskell because nothing could be further from the truth. So far, Haskell is the most interesting programming language I have learned. It's just disappointing that it takes so long to be able to do anything "useful" in the language. 
Prize question: why doesn't it perform beyond six cores?
First am I down voted for the thing I said be somewhat bad/wrong or the usual down voted because of the great agreement with the replies. Implicit has never by definition meant overheads have to exist. Infact it has nothing to do with overheads. Overheads are a result of poor code, poor compiler, or just code that does a lot of checking at runtime. Implicit parallelism does exist somewhat already through out many areas of the programming world and I think this should be taken further. Is something necessarily impossible just because the research has not been carried out to the fullest? I think there is still room for improvement but we could just "assume the world is flat" but once certain achievements have been, you wouldn't want to go back.
It'd be interesting to see how this compares with a good DPH implementation of a raytracer. Very nice though, to get such good performance with such simple code :)
Firstly, that's a significantly more complicated approach. I think it's a nice selling point of Haskell to be able to easily binding to C libraries. It's even cooler that I'm able to include the entire library in the Cabal package for ease of installation. But even if I wrote a Parsec parser, wouldn't it still need to read through the whole file first to make sure it was valid YAML?
&gt; But even if I wrote a Parsec parser, wouldn't it still need to read through the whole file first to make sure it was valid YAML? A lazy parser can start giving you results for prefixes of the input, on the assumption that the remainder will be a valid parse. This would usually be used to construct a lazy representation of the input. If it hits a parse error then the lazy data structure just has to be annotated with the parse error. The functions that consume the lazy structure have to be prepared to cope with the error case. By error I mean an explicit representation as data of the parse error, not a call to the error function. As far as I know parsec cannot do lazy parsers, but other parsing libs can, for example polyparse.
I can't take credit for the simplicity, it's really about 95% Lennart Augustsson's work, he really did well on it! I just tweaked a few things lower in the algorithm, and added the parallelism (which was just a one-liner), and fixed the output slightly (I think it was outputting UTF-8 resulting in a garbled picture, so I set it to latin1).
Ah, good question! I would guess (and it's really only a guess) that it is either because of: * the GHC runtime has a greater GC/other burden with more cores (not sure) * the i7 I ran it on has 4 cores, but you can get 8 threads out of because it does Hyper-Threading, I'm not sure if you can treat Hyper-Threads in the same way as another core. I think the second guess is more likely, but I am unsure. I could try it another machine without hyper-threading to see.
You could also try chunking it up into "tiles". So each task would do NxN pixels. That's more coherent than splitting it up into scanlines or whatever.
Another interesting concurrent language is Occam ( http://en.wikipedia.org/wiki/Occam_(programming_language) ), which is based on CSP.
Here's a typical example of parsing I need: I have a YAML file that contains all the entries for a website, and I want to generate a navbar. Using the left-fold approach, it works great: the left fold collects the names of the entries and returns them. My fold function doesn't need to deal with parse errors, since the library handles it automatically. I *think* this would be more complicated by using an online parser, and while it would be possible to write a strict parser that would have the same efficiency, I would probably need to write a *new* parser for each type of parse, which is clearly untenable. I'd be happy to be proven wrong on any points, since more options are always a plus. However, even if this issue can be addressed, I still see two reasons to stick where I am: * The C library is available and maintained. In theory I will be receiving all the updates in the YAML standard for free from them. * This avoids any need for lazy I/O, which seems to me to be a bit of a hack. For simply reading in bytes/characters from a file, it's a simple enough operation that we can pretend it's safe, but I envision interleaving this YAML parsing with output generation and other such scenarios.
Yeah, I know that feeling. Having Coq respond with "Proof Completed." on typing `auto` would make me feel better about skipping uninteresting details, too. :-) Sometimes, it's tricky to see where the details end and where an actual idea is required, though, especially in calculus proofs.
I guessed that the "ghc bug" jdh reported regarding this was due to the encoding switch -- good to see it confirmed!
On the note of predictability, I think that toStream . fromStream = id is the most brilliant optimization rule ever. I wish for more of that calibre.
It boils down to vastly different backgrounds, I guess. It is quite understandable that someone coming from a background in, say, C needs more time and sweat to wrap his head around the monoid of endofunctors than someone who has taken a course in abstract algebra. Not to mention that to learn the latter, it actually helps to *unlearn* the former. Now, while learning Haskell does take effort, good teaching can reduce this effort. The latter is scarce, of course, and depends on dedicated individuals that write books like Real World Haskell. What I wonder is whether the effort required for teaching can be lowered as well, by means of technology. The [wikibook](http://en.wikibooks.org/wiki/Haskell) would be an example of what I have in mind, and while it doesn't suck, the technology does not really make writing easier, nor does it leverage collaboration. (The wiki model is great for minor edits, though.) I think that a lightweight mix of * RWH's comment model (to get the invaluable reader feedback) * wiki editing (to crowdsource minor edits) * a focus on small, independent parts in the style of [Connexions](http://cnx.org/) (to reap benefits from collaborative writing, this is hard) * a means to group and rank existing material (to crowdsource quality, I'm thinking of an editorial as opposed to a reddit vote style) * a good markup language for Haskell (&lt;code&gt; tags are quite tedious) and some minor other spice could make a great teaching portal. (If someone wants to write a web-backend for such a thing, contact me, I'd be very happy to participate in the result.)
Isn't entering the unicode characters uncomfortable?
Smells like Linux, which doesn't make effective use of the last core in GHC parallelism. Macs perform much better here.
Yes, that is a problem. With emacs I solved it by writing a small elisp macro that replaces the symbol before the cursor with the appropriate unicode symbol. The only reason I wrote a custom function was that the build-in abbrev mode only understands words and not operators like -&gt;. With other editors you need something similar I guess.
I've heard of stdin and stdout being closed so that the next files opened take their IDs, and input and output can be done on files as if on stdin and stdout.
You don't need category theory to understand what an inifinite type is. [Int] is a list of ints. [[Int]] is a list of list of ints. [[[....]]] is a list of list of list of.... and if you keep going, that's infinite. So the expression `let x = [x]` has the type [[[...]]] but while GHC is smart enough to figure out that it has an infinite type, the Haskell language disallows actually working with or using things of such a type (without "cheating" which we'll ignore).
The problem is that when you just have 'fully implicit parallel evaluation' for a general purpose language like Haskell, you almost will certainly get bad results - the compiler, while smart, cannot automagically deduce that "this computation is worth putting into another thread right now, but this one is not." Creating a thread to evaluate `2+2` is a complete waste of time: the amount of time necessary to compute it is far smaller than the cost of creating a thread, resulting in a net loss in performance. But even operations which look complex to the compiler can still take less time than the creation of the thread, again resulting in a net loss in performance. (Another problem is that automatically parallelizing compilers are only even remotely practical for a handful of languages, namely, ones that are pure like Haskell. Otherwise it is not safe for the compiler to do such optimizations. It just wouldn't work in a standard C compiler.) Research on this front has already been done, in the context of GHC actually: look for 'Feedback-directed implicit parallelism.' It describes a prototype built into GHC that automatically parallelizes programs, and uses runtime information from the program upon recompiling to further guide smarter uses of threads - it is essentially profile-guided optimization for threading. I can't exactly remember all the details of the paper, but from what I can gather the implementation was very much a prototype and it never got merged mainline because it was fragile; on top of that it's a question of whether all this compiler-complexity is *worth it* given that you can use very simple parallel annotations to achieve almost the exact same results, with minimal code modification. (Another nice benefit is because Haskell is a pure language, you can litter every line with these annotations and they will *never* change the actual semantics of your function. It may just make it evaluate in parallel.) You think `par`, `pseq` and `seq` make code 'very confusing, very explicit, verbose and messy'? This makes me think you haven't spent much time writing highly parallel code (haskell or otherwise) - come back when you've tried to turn a sufficiently complicated sequential C program into a parallel one using pthreads, and then we can talk ugly.
&gt; Knowing jdh, the comment may be deleted Or even more likely, he'll change to benchmark until it again shows the point that he wants to make. 
Wow, I wrote about something [_very_ similar](http://www.reddit.com/r/haskell/comments/anczm/catching_exceptions_inside_a_transformer_stack/) just a few days ago! The moral of the story for me was to lift twice, once in each monad. For example, for `withCString`, you can write withCString' :: MonadIO m =&gt; String -&gt; (CString -&gt; m a) -&gt; m a withCString' s f = join . liftIO $ withCString s (return . f) To generalize to your `with` function, we can use `Rank2Types` as follows: with :: MonadIO m =&gt; (forall r. (a -&gt; IO r) -&gt; IO r) -&gt; ((a -&gt; m b) -&gt; m b) with f g = join . liftIO $ f (return . g) We can now recover `withCString'` as withCString' s = with (withCString s) edit: Actually, I'm not sure why I added `Rank2Types`. I just tried it in ghci; while the type that's inferred is a bit mind-boggling, it works just fine: -- with :: (MonadIO io, Monad m) =&gt; ((a -&gt; m b) -&gt; IO (io c)) -&gt; ((a -&gt; b) -&gt; io c) -- withCString' :: MonadIO m =&gt; String -&gt; (CString -&gt; m a) -&gt; m a with f g = join . liftIO $ f (return . g) withCString' = with . withCString Taking `m = IO` and `b = io c` in the type signature of `with`, this specializes to the more readable type -- with :: MonadIO io =&gt; ((a -&gt; IO (io c)) -&gt; IO (io c)) -&gt; ((a -&gt; io c) -&gt; io c)
True. I agree that that is a trivial example. Unfortunately I didn't write down the most confusing error message that ghc has returned me. It used a term that I had never heard before but later discovered was related to category theory (if I get it again, I'll post it here). Again, I don't think this stuff makes Haskell a bad language, it just makes it a hard language to learn (maybe more so for people who can already program). Haskell's pure mathematical underpinning is both its greatest strength and its greatest weakness.
&gt; The biggest problem I'm having with Coq is the user interface. You might like [proofweb](http://prover.cs.ru.nl/).
The high-level Haskell LLVM bindings use this trick extensively...
please don't use "high-level" and "VM" in the same sentence. you might summon jdh30 accidentally.
&gt; There’s no built-in system for writing functions that take variable numbers of arguments, and it seems like it would be difficult to write one. The standard approach is to take a list instead I'm sorry but you don't give haskell enough credit. Using list (or array in C) approach is exactly what every other language on this planet does. And because of that, in every language but haskell you're limited to a single "class" of objects, unless you want to use unsafe casts. In haskell you can use any types of arguments, as long as you can describe them in some kind of process and it doesn't have to be as easy as those printf ones, you can have a function that accepts only prime number of arguments, that are all strings, but occurences of integers at the indices that are fibonacci numbers. http://okmij.org/ftp/Haskell/vararg-fn.lhs
That's infinitely better than what I was looking at doing. You just cleaned up my code significantly; hats off to you sir! I'm assuming that this would extend quite easily for functions like finally; I look forward to trying it.
I count you among both those *categories*
http://kevinbeason.com/smallpt/ and http://www.hxa.name/minilight/
What, there aren't enough ray tracers written in Haskell yet? I admit to not knowing much about this stuff -- what makes smallpt or minilight special?
Doesn't this end up actually running the wrapped computation outside the scope of the wrapping function?
Wouldn't build for me under GHC 6.12.1. Saw strange dependencies of the form "pandoc-1.4 depends on bytestring-0.9.1.4 which failed to install" when in fact I already have both pandoc-1.4 and bytestring-0.9.1.5 already installed. This is using "cabal install". Either there's an issue with the hakyll cabal package, or an least a "messaging" bug in cabal-install.
Heh, good catch. That would explain my sudden segfaults. I guess I'm stuck with the With typeclass for now :(.
Are you sure you did a `cabal update` first? Because the latest version of Hakyll doesn't even depend on `bytestring`, so it's a rather strange issue.
I'm up for it, might give it a go tomorrow. If so, I'll post again here and my blog :)
You need to do something specific for each monad to make this work safely. Consider how something harder like applying StateT works, which actually forces you back to a rank-2 type, but with a With typeclass like before.
...oops. Maybe I should have read [the documentation](http://www.haskell.org/ghc/docs/6.10.2/html/libraries/base/Foreign-C-String.html#v%3AwithCString): &gt; the memory is freed when the subcomputation terminates (either normally or via an exception), so the pointer to the temporary storage must not be used after this
&gt; And because of that, in every language but haskell you're limited to a single "class" of objects, unless you want to use unsafe casts. Except for those pesky dynamic languages (Lisp, Ruby, Python...) where you can use safe (but potentially slow) reflection or other ways to handle different types at runtime.
Well, that's safer than just casting in the sense that it doesn't depend on memory representations of things, but essentially everything is unsafe in a dynamically typed language, because any condition you're not prepared to deal with gives rise to a runtime failure rather than being caught at compile time when you can do something about it. Debugging runtime failures is in general a heck of a lot more headache-inducing than being told by the compiler what the problem is -- especially if they happen when the programmer isn't the one who is running the program.
True.
Both are pretty minimalistic code that produce pretty pictures. smallpt also uses OpenMP for some parallelism; this is a place where Haskell could shine. Minilight is translated in various languages to make it somewhat possible to compare the language style/verbosity and speed of execution of some implementation.
[OpenCL version of smallpt](http://davibu.interfree.it/opencl/smallptgpu/smallptGPU.html)
So then there are two questions I've got: * Is there any way to generalize this approach so we don't need a separate "with" function for withCString and finally (the latter takes two monadic arguments). * Is this useful enough that it warrants its own package?
I try it today but i'm stuck on the loop
A bit more information would be useful. In particular you didn't post your pair implementation. On my machine the first two run in ~1.2 seconds while the last one (library) in ~2.9 seconds. I presume, the latter is because Complex is polymorphic, which inhibits many optimisations. By making all arguments of `mandelImpl` strict I get a bit under 1s for the first two. Anything else would require profiling and perhaps a look at the Core output. Also, make sure to define Complex with strict Floats. You forgot to do that in the second implementation.
According to [GHC documentation:](http://www.haskell.org/ghc/docs/latest/html/users_guide/faster.html) &gt; If you're using Complex, definitely use Complex Double rather than Complex Float (the former is specialised heavily, but the latter isn't).
32bit floating point (Float) is (much) faster than 64bit floating point (i.e. Double). I wanted to compare to my C and my SSE versions of the code (http://users.softlab.ntua.gr/~ttsiod/mandelSSE.html) but from what you said, it appears that I can't... :-(
I've seen hakyll tutorials on the programmer's blog and it was love at first sight. I'll soon move my blogs to it :) Thanks to the developer.
When I said "pair", I meant the first two implementations, where I create my own complex type, using just a pair of Floats. The third implementation, which as you noted is significantly slower, uses the "Complex Float", which is the Haskell library-equivalent of what I did - if anything, I expected things to get faster, not slower! You also claim that by being polymorphic, Complex inhibits many optimizations. If I understand correctly, however, this is not run-time polymorphism, it is compile-time (close if not identical to C++ templates - something like Pair&lt;T&gt;, where T is Float in our case) - which would not explain why this is slower than my two first versions that use a simple pair of Floats...
http://www.lifeisajoke.com/simpsons/magnets/Doh.bmp Thanks. Now I get further, but hit another error: src/Text/Hakyll/Renderable.hs:5:0: Warning: The import of `System.FilePath' is redundant except perhaps to import instances from `System.FilePath' To import instances alone, use: import System.FilePath() [ 7 of 13] Compiling Text.Hakyll.Page ( src/Text/Hakyll/Page.hs, dist/build/Text/Hakyll/Page.o ) src/Text/Hakyll/Page.hs:126:17: Couldn't match expected type `Control.Parallel.Strategies.Eval a' against inferred type `()' In the second argument of `($|)', namely `rnf' In the first argument of `seq', namely `(($|) id rnf rendered)' In the first argument of `($)', namely `seq (($|) id rnf rendered)' cabal: Error: some packages failed to install: hakyll-1.0 failed during the building phase. The exception was: ExitFailure 1 
The right answer would be fairly invasive. Arguably, the standard library would be better served if almost all, if not actually all, methods that worked with IO instead used a MonadIO constraint instead. Unfortunately, this pretty much guarantees a performance hit, unless you also go through and follow up with a ton of SPECIALIZE pragmas to cover the old IO case. =/ Also, I lied slightly above, you can write With for state without requiring a rank 2 type, but that implementation relies on an IORef.
Okay, I released a small patch (1.0.1), please try again using cabal update cabal install hakyll-1.0.1 Good luck :-)
Yup, all good.
Okay, I'm glad we fixed it. It was basically a version incompatability in `parallel` I didn't know about.
http://rosettacode.org/wiki/Towers_of_hanoi The Haskell example is a reasonable definition. I wrote the Clojure one. :D
Now if there was just a full implementation of the Haml spec in Haskell I'd pleased as punch.
Well, the link seems to be broken, could you give the right one please ?
Compile-time polymorphism, I think, translates to runtime polymorphism as a space-vs-speed trade-off, to avoid code bloat. 
Here you go: [HStringTemplate](http://www.haskell.org/haskellwiki/HStringTemplate) It's a Haskell port of StringTemplate. I successfully used it on a few pages and it's included in Happstack. The only problem I see is that its code is very, very difficult to read. There's this too, with Django templates syntax : [Press](http://hackage.haskell.org/package/press)
The problem is the definition of `magnitude` in `Data.Complex`: http://haskell.org/ghc/docs/latest/html/libraries/base-4.2.0.0/src/Data-Complex.html#magnitude I guess it's written this way for numerical stability (?) but it's clearly a lot slower than the version you wrote. If you hide `magnitude` in the import and implement it your way instead, the performance of the second and third versions become indistinguishable. (Indeed, the core generated appears to be identical up to renaming variables.) By the way, has anyone ever written a tool to compare core up to renaming variables? It's basically just unification, right? Doing this manually was how I discovered what was going on, but it would have been easier with a tool.
You could add `{-# SPECIALIZE ... Complex Float ... #-}` pragmas to `Data.Complex`, this might ameliorate some of the differences between C and Haskell. Also, `-funbox-strict-fields` is something that should be looked into for your code. (Equivalently, `{-# UNPACK #-}` pragmas give you finer-grained control over the same technique) There isn't anything particularly special about `Data.Complex`, feel free to take a copy of the source, rename the module, and experiment away!
Complex is a polymorphic pair, which can't be fully optimized (no CPR for example). 
I think list zippers are relatively simple to explain, certainly much more so than tree zippers.
You need magnitude to look like that otherwise it will overflow at half the maximum exponent. The problem is that scaleFloat&amp;co are really slow, even if they really can be implemented with just a little bit fiddling.
Author here. I didn't mean to insult Haskell, it's a beautiful language and I love using it. It's just that if you're absolutely hell-bent on writing a variadic function, it's not intuitively obvious how to write one, as opposed to Perl with its @_, or Python with its *args and **kwargs.
You could just make those functions you can't make use of "undefined" but I agree this isn't ideal. 
True, but GHC won't try too hard (since it may lead to code size explosion.) You can convince GHC to try harder using SPECIALIZE pragmas as mentioned in the other comments, but apparently that doesn't work out of the box (with the current libraries).
&gt; And because of that, in every language but haskell you're limited to a single "class" of objects, unless you want to use unsafe casts. C++0x offers variadic function templates. Edit: &gt; you can have a function that accepts only prime number of arguments, that are all strings, but occurences of integers at the indixes that are fibonacci numbers. What would such a function look like? 
The zipper structure has been invented by Gérard Huet, not Carl Huet.
happstack supports server side templating. See the [latest release notes](http://www.haskell.org/pipermail/haskell-cafe/2009-December/071083.html). * HTML/XML/etc templating via HStringTemplate, HSP, or Text.XHtml
[Global illumination](http://en.wikipedia.org/wiki/Global_illumination), meaning more realistic images
The type system is telling you that matrices aren't Num(bers). In fact your post proves it when you say "What is the absolute value of a matrix? There are _some possibilities_", my emphasis. Part of the Num interface is that there is one and only one relevant concept of "absolute value". It is also an intrinsic part of a Num that there is some sort of sign. And finally, more weakly but still relevant is that you want operations that don't exist in the type class at all. (This is relevant not because you couldn't define your own further type class with those additional operations, but because reading between the lines when you say you want to use existing Num code I assume you want to be able to pass a scalar and a matrix into a Num multiply and get a scaled matrix, which Num can not handle.) What _could_ be a Num is a matrix, plus a particular definition of absolute value, plus a particular definition of sign. So, instead of Matrix, you have MatrixAbsIsDeterminantSignIsPositiveIfNotSingular. Not necessarily with that name. You get a family of Matrix types instead of just the one matrix type, based on your choice of definition. The type system will then of course tell you that they can not be casually mixed together, you can't add a matrix that is working under one definition of absolute value to one that is working under another definition. It is correct, because they aren't the same thing. There are other violations of the Num typeclass, too. The type signature of + is (Num a) =&gt; a -&gt; a -&gt; a. It is at the very least impolite to add a 2x2 matrix with a 3x3 matrix and get a crash in the middle of a (+) call that really shouldn't crash. None or little of the "preexisting Num code" that you want to use will have the first clue what to do about (+) crashing. Matrix add is more of a Matrix -&gt; Matrix -&gt; Maybe Matrix sort of thing, or some other system for handling errors. In some cases, this would be an OK thing to do, which is where newtype comes in. However, I don't think it works in this case. I think you're going to have to live with a non-Num solution. It isn't enough that a small subset of Num makes sense to matrices, the whole interface has to make sense, or it simply isn't a Num. I share this with a bit of stridency as I come off of about a six hour fail where I tried too hard to jam something that wasn't a Monad into the Monad typeclass, for much the same reason ("man I like being able to use the Monad stuff automatically, and isn't the syntax pretty?"). (And it really isn't... what I want is a completely different type signature and I knew at the beginning I should have trusted the type signature rather than try to jam it in. I intellectually know that but it hasn't made it down to the coding instincts yet.... which reminds me I meant to Hoogle my desired type signature soon...)
aaaand I just now realize that, hey, new community, this *might* have its own formatting syntax!
As far as I know, which isn't very far, it is run-time polymorphism. In Haskell, typeclasses are looked up at runtime, unless you stick a SPECIALIZE pragma in the code somewhere. On a somewhat related note, INLINE pragams can dramatically speed up your code too.
http://rosettacode.org/wiki/Towers_of_Hanoi
This is why I would rather see type classes for algebraic structures than for this vague concept of "number."
&gt; "man I like being able to use the Monad stuff automatically, and isn't the syntax pretty?" Can `-fno-implicit-prelude` help? (It allows you to use the `do`-notation for your own `&gt;&gt;=` and `&gt;&gt;`: see http://okmij.org/ftp/Haskell/DoRestrictedM.hs)
Foo a =&gt; a for some class Foo. I'm kind of fed up with type-level programming at the moment, but I'll give it a shot as soon as I want some more types.
If you want a fast magnitude, define a newtype wrapper for Float that has everything the same, but which uses a no-op and inlined scaleFloat.
The usual trick is to factor out of the radical the greater of *x*, *y*: import Data.Complex hiding (magnitude) magnitude (a:+b) = mag where mag | x == 0 = 0 | otherwise = x * sqrt (1 + sqr (y / x)) x = a' `max` b' y = a' `min` b' a' = abs a b' = abs b sqr x = x * x *Edit:* Complete, straight-line code.
That would be a much better definition for Haskell since it doesn't require clever bit fiddling to be fast. Also, it works better on vector processors. 
&gt; where Haskell could shine groan.
That's actually why I found this piece interesting. I had never thought of applying zippers to anything other than lists, but the tree example gives a relatively simple example of how you can use a zipper on more complex data structures.
&gt; TFTFY They're called template functions colloquially, the standard refers to them as function templates. You'll see both of them used. &gt; Even in C++0x there is no elegant way to analyse a string literal (therefore string formats) at compile-time, the nearest you can get in C++0x is with constexpr user-defined literal using variadic template of non-type parameter (of char) and that is a f**ing disgusting mouth full. Unfortunately, the literal operator template form is only available for user-defined integer and floating point literals, not string literals. I'm not sure why you bring any of this up though, I was only refuting the statement that only Haskell offers type-safe variadic functions. I didn't claim that it is possible to analyze a format string at compile-time. Is it possible to analyse a string literal in Haskell? I'd also be interested in seeing the definition of a variadic function like the one Paczesiowa described.
I thought zipper wasn't a structure so much as a way to access a structure, or in this case, sounds like it actually greatly complicates the structure of anything but a list. :-)
It might; I haven't completely examined it. I am concerned that it could be very deceptive, though, since it won't really be a monad, so, for instance, mapM won't work at all. Sure, the typechecker will tell you right away, but it's still off. Not sure if the balances work out in favor of doing that, but it might.
It always baffles me that people complain about the nicely formatted, informative error messages that GHC produces, but nobody seems to have any trouble with the mix of vomit and diarrhoea that C++ compilers throw at you when you make even the slightest mistake.
I have spent a lot of time with n3000. I interpret sections 3,4 &amp; 5 of 2.14.8 to mean that the literal operator template form is not available for user-defined string literals. In particular, section 5 (which describes user-defined string literals) does not list a literal operator template. Sections 3 and 4, which deal with integer and floating point literals, include the literal operator template form. If you have a different interpretation, that'd be great. It would mean that C++ is one of the few languages where it is possible to implement a printf with compile-time type safety as a library.
I have read the wikipedia page as well. I see the draft standard as a more authoritative source and I still interpret 2.14.8/3,4,5 to mean that the form isn't available for user-defined string literals. &gt; I'm going to assume you would have to write a hack that uses user-defined literals and proxy objects to emulate type-safe, type-checked printf. I wrote a proof-of-concept for such a thing a few weeks ago (messy code available upon request): Printf("blah: %d foo:%s"fmt, 1, "bar"); Since GCC doesn't implement user-defined literals yet I had to resort to emulating the call with fmt&lt;'b','l','a',... and the whole thing worked beautifully. I was a bit miffed when I realized that the literal operator template form isn't available for string literals. But if you claim that it does work then I'm back in business! ;-)
Then you're going to _love_ [this](http://en.wikibooks.org/wiki/Haskell/Zippers). Don't miss the [Conor McBride PDF, _The Derivative of a Regular Type is its Type of One-Hole Contexts_](http://www.cs.nott.ac.uk/~ctm/diff.pdf) in the sixth reference for yet more juicy details.
This article was great, as I hadn't learned about zippers for trees yet. However after reading it, and the wikipedia article, I found that the haskell wiki here http://en.wikibooks.org/wiki/Haskell/Zippers actually explained it the best. I've since implemented a working zipper and I think I will go tackle some of the Project Euler problems that imply the use of a tree that I've been skipping. Yay.
What I've done in the past (before Numeric Prelude) is to define all the functions in Num that don't apply to my type class to call error with a message explaining how badly Num is designed. Switching to Numeric Prelude is more productive than complaining, though, so don't use my old technique.
I'm glad to see the author focusing on living beside C++ instead of replacing it. I feel that too often people feel that one language should be all you need, but I firmly believe that every language design requires trade-offs. Any sufficiently complex program will contain parts that are best written in different languages. Interoperability and linking should be where we focus our energies.
Interesting post. You might want to use the [gnuplot bindings](http://hackage.haskell.org/package/gnuplot) directly instead of piping to it the next time.
STL and Boost allow you to write C++ in a functional style, albeit with a lot more syntax. Doing a bit of searching led me to this which may be a good starting point: http://www.keithschwarz.com/cs106l/spring2009/handouts/280_STL_Functional_Library.pdf
Well, it is hard to coexist properly with C++ because C++ has this retarded FFI that is not standardized in many ways.
That actually looks quite helpful, thanks. I wasn't aware of the possibility of overloading ()... EDIT: I really like these handouts so far, both the style and content; very clear, concise, and seems to contain the answers for many of my questions. I can recommend them to anyone in similar shoes! Link to the course homepage: [http://www.keithschwarz.com/cs106l/spring2009/](http://www.keithschwarz.com/cs106l/spring2009/)
Thanks for the tip.
Effective C++ is pretty good if you have a basic understanding of the language already. It's essentially a big list of language pitfalls and how to avoid them.
Couched in weasel words and faint praise, but I believe this is a first. :-)
So, why are you feeding the troll again?
Much like Haskell.
I've found the biggest change in my C/C++ habits as a result of learning Haskell is that I've become preternaturally aware of all the possible ways code could go wrong. RAII is a great C++ technique for managing any resource robustly, even in the face of exceptions. I find myself writing tons of RAII classes. I don't think it's particularly elegant to write functional-style programs at the lowest level in C++. Let your functional insight drive the overall design of your program, but for the actual implementation of a function I find C or C+ is much more elegant when you use a little mutable state and a loop or two. (Alternatively, you'll wander for fourty years in the function template jungle. This might change when C++0x lambdas become more common.)
But is it correct? On my machine: myMagnitude ((-12.936926911905847) :+ (-5.429932106037269)) ~&gt; 14.030261601273445 whereas Data.Complex.magnitude ((-12.936926911905847) :+ (-5.429932106037269)) ~&gt; 14.030261601273443 (The last digit differs.) I found that using QuickCheck: import Control.Monad (liftM2) import Data.Complex (magnitude, Complex((:+))) import Test.QuickCheck (quickCheckResult, Arbitrary(arbitrary)) myMagnitude :: (RealFloat t) =&gt; Complex t -&gt; t myMagnitude (a:+b) = mag where mag | x == 0 = 0 | otherwise = x * sqrt (1 + sqr (y / x)) x = a' `max` b' y = a' `min` b' a' = abs a b' = abs b sqr n = n * n prop :: (RealFloat t) =&gt; Complex t -&gt; Bool prop x = myMagnitude x == magnitude x instance (RealFloat a, Arbitrary a) =&gt; Arbitrary (Complex a) where arbitrary = liftM2 (:+) arbitrary arbitrary -- test :: IO Result -- test = quickCheckResult prop
Modern C++ has some cool stuff in it. Honestly though, every time I try to write C++ the way I write Haskell, I'm struck by how much uglier the C++ is.
I go back and forth between the two a lot. If you try to write Haskell-style code in C++ you will encounter pain (much like you will in the reverse, though that is probably more obvious to you). So don't think about it terms of adapting functional designs to C++ (you wouldn't approach adapting a C++ program to Haskell by using a bunch of IORefs, right?), but rather that the languages are very different and that to work effectively in C++ you must use C++-like designs (typically state-carrying objects).
Actually, by the word "adapting", I meant including changes because of the obvious difference of the underlying languages. I don't mind state-carrying objects. However, I don't think we have to discard everything we learned with Haskell; quite the opposite. To have a concrete example, let's suppose I have to write a parser in C++. I think I would rather encounter some pain writing something Parsec-style than use lex/yacc...
If you want to do combinator libraries in C++ the best way to do it is via expression templates like boost spirit (which is a combinator parser framework). Nowadays boost.proto is framework for defining EDSLs in C++. The fact that you wasn't aware of overloading the function call operator says to me you might have a painful climb.
I recommend trying to use the absolute latest features in this area (due to the C++0x standard) since they simplify some things desribed in the above paper. I'm talking about being able to use "bind()" instead of bind2nd(), etc. Search for "TR1". 
Thanks, but I hope being uninformed at the moment does not imply anything about the future. After all, nobody is born with overloaded call operators *templated* in their brains already...
How well are the C++0x features supported in the current compilers? I guess it is reasonable to restrict ourselves to GCC and MSVC. Google says GCC started to support new language features from 4.3 (and adding more in 4.4 and 4.5) and MSVC from 10.0 (some of them). However, I'm not sure about the availability of these (MSVC 10.0 seems to be unreleased; I'm also not sure about the new GCC-s on OSX); also, they say nothing about the library extensions.
Well, Haskell goes one step further I believe and doesn't even have an internally standardized ABI between versions, or does it?
Damn.. I had that idea on my long-term TODO list for a while now.. Probably better that someone else did it, though, since I'm still a Haskell newb.
The latest GCC release has many of the TR1 standard library extensions.
Come hack on it :) Seriously, it is great fun :)
This is neat. I did a little bit of Curry in college and seem to remember it requiring a non-free Prolog implementation. Now, if only I remembered any of it.
Now I don't do web programming at all so I may be wrong but... I think using a single process could be the problem here. Since the answer is extremely simple ("Pong!"), using multiple threads can be more of a burden than a help, while if the server did something more significant like access a file, the single process solution would completely break down. How about a version that just reads a little page, compresses it or manipulates it somehow, then serves it? That would be very simple in all these languages, and still give a slightly more realistic microbenchmark, IMHO.
I done this one (all but the refraction) http://haskell.pastebin.com/m2d539341 but if I use more samples i get a darker image. I think i losing the color in operations but i don't know where.
Thanks, this seems very useful.
I think part of it is the terms are a bit odd. When I was starting, I didn't find the errors helpful. 'Expected [[a]], inferred IO [a]'? I didn't get that at all, until I started mentally rewriting it to 'I need [[b]] but you gave me a IO [a]!'
It's a structure that's a partial derivative of another structure. The link to McBride's paper from jerf is a good one.
Feel free to come again next time you're in singapore.
Honestly I don't think it's fair to compare the epoll version against the others in Erlang and Python. The epoll stuff is highly non-idiomatic Haskell and it's also a completely non-standard package. Of course this will change when bos and tibbe finish getting the ghc rts to use epoll. Then the `forkIO` + `ByteString` version should be the one to hold up for comparison with other languages. That version is short, simple and does concurrent IO in the standard idiomatic way. Also, presumably there is no actual concurrency in the epoll version as it is currently written. So that's not quite a fair comparison. Though presumably that could be fixed by adding a forkIO in the right place.
&gt; highly non-idiomatic Haskell I thought it was a pretty good implementation of user land epoll, actually. Nothing too sneaky. And its better than the Python epoll version, imo. Of course, once threadWaitFd is in terms of epoll, we'll have something to shout about. An interesting approach would be to have one thread per capability, and one epoll handler per capability. tibbe, any ideas on that?
It may be a fine implementation of user-land epoll, but that's not something you generally want to use. It's not something we can recommend to your average Haskell hacker when they ask about network programming. It means using a totally separate IO system, no Handles etc. That's what I mean about it not being idiomatic. I accept that it's equivalent to the Python version that also uses epoll directly with explicit non-blocking IO.
Yep, certainly. It's not newbie ready yet, by any means. Should be fine for those writing high performance servers though, I think.
On another note, I wonder if using epoll actually make any difference here. In the original benchmark, how many client connections are being handled concurrently? I've not looked in detail but I'm guessing that although we have many 1000's of requests per second, that there are not that many connections active simultaneously. The main advantage of epoll comes when there are 1000's of simultaneous connections. It looks like the Python and Haskell epoll versions are getting their performance mainly from bypassing all the normal IO and threading stuff. I suspect that equivalent versions using `select()` would be as fast.
I would hope that it's not necessary even there. Giving up all the standard IO and concurrency libs is a major cost. With the rts using epoll the only overhead should be the forkIO + scheduler overhead. I would hope that this is low enough that we can still write high performance servers. Most servers do more than echo "pong".
does this mean 6.12.x or 6.14.x?
Yes, although you can still do IO via the regular IO management layer, that's additional performance issues to think about.
Just ran the benchmark using both the `poll` and `epoll` backends and the request rates are comparable (~19k reqs/s for both on my machine.) We need a better benchmark.
Using one event loop per capability is a promising approach and should let us scale up with the number of cores. I've seen this approach recommended in the documentation for libev. We will look into this as soon as we begin integrating the code into GHC.
any idea why the time for k-nucleotide was so bad? 
Downvotes? I'd like to know why...
Keep in mind those are 6.10.3 vs 6U13(? - or maybe I'm wrong, Why can't java version like normal things?) numbers. It'd be interesting to see how 6.12 vs 6U18 looks.
You could post a cure for cancer and it would get down votes.
Hmm. In all the cases where the Java version is slower, compare the CPU loading. The haskell version is unthreaded, while the Java version is threaded. I wonder how much time the Java version is spending synchronizing. I think I will take a crack at writing a straightforward non-threaded version of the benchmark in Java. 
Downvotes on a comment asking about downvotes? lol
Oleg's [iteratees](http://okmij.org/ftp/Streams.html) have a long history of confusing the hell out of people[1][2][3][4], including me. Maybe they're just a bit too abstract to understand easily, but I think I big part is that the existing documentation is difficult to read. I figured a page documenting my understanding might be helpful. Hopefully, this will encourage people to use iteratees in place of lazy I/O more often. [1] http://www.haskell.org/pipermail/haskell-cafe/2008-December/052181.html [2] http://www.haskell.org/pipermail/haskell-cafe/2009-February/056816.html [3] http://stackoverflow.com/questions/1319705/introduction-or-simple-examples-for-iteratee [4] http://therning.org/magnus/archives/735
I did not downvote, but I have to admit that clicking on the link and seeing one of the worst imaginable charts ever made an itch...
Unfortunately, they still confuse me. What problem exactly are they trying to solve and how? From the implementation details, I only gather that it's pretty much a state monad with some abstractions *removed*, i.e. where I have to take care of some invariants myself. For example, I'd have to make sure that my code is independent of the lengths of the different `Chunk`s.
Why is it a state monad? Iteratee is basically a "Consumer", and Enumerator (sort of Hungry Iteratee -&gt; Full Iteratee) is basically a "Producer". The producer's code drives the loop (inversion of control) so that resource management/etc can be done gracefully and correctly. The Iteratee abstraction lets you write new producers and consumers separately, and link them up together almost as simply as you link lazy I/O together, and without the drawbacks of lazy I/O.
Iteratees are useful for when you want to run a computation until you run out of input or it stops needing it and then have your hands on a continuation that expects more input. A traditional parsec parser has to be given all of its input up front. An iteratee parser can run over, say, the 64k buffer you have in hand, and gives you back a continuation that you can use to resume parsing after that chunk. Moreover, iteratees as defined by default don't backtrack, so you don't need to worry about them holding onto input. This is useful if you want to run the parser continually over some kind of socket and produce results as you go. I have found this to be very useful for some forms of incremental/parallel parsing. Viewing the simplified types: data Buffer = Chunk ByteString | EOF data Iteratee a = Done a Buffer | More (Buffer -&gt; Iteratee a) An Iteratee is either done, in which case it doesn't need any more buffer to compute its result, or it wound up needing more input than it had, and so it takes either an additional bytestring or an EOF marker to continue processing. You have a couple of invariants in addition to what is above. (i.e. when you have an 'More f', 'f EOF' must yield a 'Done' value.), but the gist can be obtained from the types. I have some slides talking about a variation on the theme[1]: [1] http://comonad.com/reader/2009/iteratees-parsec-and-monoid/ [Edit: bumped back to an older URL which actually had the slides!]
&gt; I have some slides talking about a variation on the theme[1]: &gt; &gt; [1] http://comonad.com/reader/2009/iteratees-take-2/ Bad link? It doesn't seem to have any slides. I'd love to learn more about iteratee-based parsing, especially with Parsec.
Here's an easy way to understand the problems being solved. Given this code: main = do buf &lt;- readContents "myfile.txt" print buf Can you answer the following questions? 1. When is the file handle closed? 2. How much space (ie RAM) will be used? 3. What happens if an error occurs when reading the file? For an even better example, consider this (broken) code, which attempts to fix 1: main = do buf &lt;- withFile "myfile.txt" ReadMode hGetContents print buf If you understand what's wrong with it, you'll know why iteratees are useful.
Fixed.
binary-trees: haskell is paralell, java is not parallel = same elapsed time.
A hashtable implementation was written inline for it, as the standard hashtable wasn't too performant due to a compiler bug. I'm guessing this will change with 6.12 since the mutable array bug has been fixed: http://hackage.haskell.org/trac/ghc/ticket/650
Iteratees are one of those useful concepts that, in true sigfpe style, you'll eventually invent yourself if you do enough Haskell programming. Maybe not as elegantly or as abstractly, but they're such a good fit to such a common problem that it's pretty much inevitable.
Out of curiosity, what ever happened to [unify](http://www.seas.upenn.edu/~lipeng/homepage/unify.html) ? Why didn't it succeed as a Haskell concurrency model ? I know it's networking-related, but seemed like a promising approach.
You mean running one IO manager thread per capability? Or do you mean something using the event package as a standalone thing? In the first case, yes that may be well worth investigating. I expect the tricky thing will be with managing thread migrations. If you use separate mappings from fds to waiting threads per-capability then you need to do something special when a thread is migrated from one capability to another (unless you can guarantee threads blocked on MVars are not subject to migration).
&gt; Fortress goes beyond Haskell in that the type `Maybe[T]` extends type `Generator[T]` so that it can be treated much like a set that will generate either one item or no items at all. Actually, this is the point of Haskell's `Foldable` type class. Lists in a non-strict language like Haskell serve much the same purpose as generators do in Fortress. You can call `Data.Foldable.toList` on any `Foldable` value. Types with `Foldable` instances include `[]`, `Maybe`, `Array`, etc. Regarding the `Condition` trait, it seems to me any implementation must necessarily be isomorphic to `Maybe`: `Condition` is simply a representation-independent specification of `Maybe`'s semantics. In Haskell there is a tradition of using concrete standard data types like `[]` and `Maybe` as shared building blocks, so you would achieve the same effect with existing `Data.Maybe` functions. For example, if v &lt;- member m k then println("We found a value: " || v) end would be try $ do v &lt;- member m k return $ putStrLn ("We found a value: " ++ show v) where `try = fromMaybe (return ())`. Admittedly this won't win any beauty contests. The visible intermeshing of the two monads--the outer arbitrary monad and the inner `Maybe` monad--is the main issue. If the outer monad has error handling facilities then it may have `Maybe` faithfully embedded. That gets rid of the `return` around `putStrLn` but now requires the `member m k` term to be explicitly lifted.
This would be nice for my Nokia tablets and my BUG from buglabs!
Awesome! I haven't seen [buglabs](http://www.buglabs.net/) before, thanks! EDIT: We still have a long way to go - even if an ARM NCG existed today most people would still want to then make GHC a cross compiler.
Just to remind people that I recently got GHC head working with iPhone and OMAP3 architectures (N900,Droid). And for the native codegen let's just wait for the LLVM codegen. 
I still think there would be a benefit to having an ARM NCG separate from LLVM. Aside from dropping a dependency the currently planned [LLVM backend](http://hackage.haskell.org/trac/ghc/wiki/LlvmBackend) will be unregistered just like the C/gcc backend is. Edit: I should also acknowledge that I generally consider this a fun thing to hack on and a way to poke at GHC internals. And if anyone was wondering - yes I have looked at the [LLVM paper](http://www.google.com/url?sa=t&amp;source=web&amp;ct=res&amp;cd=1&amp;ved=0CAkQFjAA&amp;url=http%3A%2F%2Fwww.cse.unsw.edu.au%2F~pls%2Fthesis%2Fdavidt-thesis.pdf&amp;ei=H1dVS7-eEY-yswOEz_XXBw&amp;usg=AFQjCNFPKiFVuf4yRkBKP3oupm_u9fIWfg&amp;sig2=BxSyOWke0r7dYj-RGJGYJQ) showing performance competitive with the registered/NCG backed for x86.
I meant the former. Thread migrations are tricky and is something I've spent some time thinking about.
See also the discussion in the programming reddit: http://www.reddit.com/r/programming/comments/aqyss/the_maybe_type_and_its_consequences/
I would like to see more like this in the community, a list of essential reads for learning how to become at a more advance level of Haskell. Like Typeclassopedia and the paper on Applicative Functors. There is so much research and ideas involved around Haskell (indeed many interesting Haskell extensions in GHC) but it's so hard to find/filter out the most relevant/useful/pragmatic stuff to read and worth spending time and effort on. I would be quite interested to see some other suggestions here (or anywhere online).
Can someone explain why the winning entry is so useful in short so I can skip reading the paper? I don't like the blogger's implementation of simple and currently all the others including the runners up seem more important to me.
This might very well be my favorite Pearl. I re-read it every year for the sheer pleasure of it.
That helps a lot with my understanding. One thing isn't clear though -- you say that iteratees consume input, and enumeratees are needed to convert one stream to a second stream. But it looks to me like iteratees do produce a list of results -- a succession of Yields, with the left-over chunks fed back into the iteratee with subsequent input, should give a list of yielded values, so why do you need enumeratees? Or did I misunderstand something?
Ah, thanks for the comments, Edward, jmillikin and Peaker. I think I can articulate myself better now. So, as I understand it, iteratees aim to solve the resource problems that lazy IO has. The basic model is the same as ordinary handle operations like main = do f &lt;- openFile "myfile.txt" ReadMode buf &lt;- hGetLine f and what iteratees add to this is: 1. Parametrization over the handle type. In other words, a value of type `Iteratee a` can be used for say both file handles and network sockets, whereas `hGetLine` is hardcoded to `Handle`. 2. Iteratees can be suspended, which facilitates resource management. In contrast, `hGetLine` has no way of stopping the code that reads from the handle. Sounds good to me. My confusion was that I was expecting something grander, namely an abstraction that also solves the problem that programming with `hGetLine` (or fixed sized buffers) is awkward compared to lazy IO. So, iteratees are a generalization of handles, but they don't aim to replace the buffer-based programming model.
&gt; Why is it a state monad? While it's not *the* state monad, it pretty much behaves like Iteratee e a m b = StateT (Chunk a) (ErrorT e m) b because the main operations are getInput :: Iteratee e a m (Maybe (Chunk a)) getInput = liftI (\c -&gt; Yield c (Chunk [])) and `throwError`. There's also put c = Yield () c but I'm not sure that inserting arbitrary chunks back into the stream is a good idea. Hm, but only caring about the result `b` misses the point of functions like `dropWhile`. Looks like there's still confusion left about how exactly the abstraction works. 
The State monad has more power that Iteratee intentionally tries to prevent
Yeah they aren't that deep, they just let you turn the consumer inside out so that you can resume it as more input becomes available. 
neat, I was considering getting an arm netbook someday, and perhaps doing some hacking on GHC on ARM myself...
I don't know how well developed this idea is, but I think Haskell's missing a library project in the style of C++'s Boost. Take for instance the current state of Haskell libraries (and you shouldn't read this as attempting to disparage anybody's efforts): * There's relatively few of them. There's millions of C libraries for performing every task. Though there's now hundreds of Haskell libraries and bindings to C libraries, the only way we can compete is to up the ante and make library writing / binding to C libraries more of a priority. * What libraries and bindings currently exist vary greatly in documentation quality. A badly documented library may as well not exist, IMO. Very often libraries and tools come with very few examples of how to get common tasks done and with no documentation other than an autogenerated Haddock file. * There's no guarantee that a maintainer will stick around. Suppose a new version of a C library comes out. Is it guaranteed that the Haskell binding will be updated? At the present, no. Whilst that may suffice for amateur coders, I presume the situation is different for corporations who look at Haskell and Java (to pick one example), and opt for Java in favour of the stronger guarantee that the extensive libraries are going to carry on being maintained. What I've been thinking about is if Haskell had a "library project". Basically, a group of people all under one banner writing C bindings and Haskell libraries. The organization provides some guarantee that a library is not going to be orphaned by a maintainer going missing (the library is no longer maintained by an individual, rather the "Haskell Library Project", or whatever). Further, the organization provides documentation, examples and tutorials for the libraries that it releases, etc. It could be a real community effort: those who don't feel their skills are up to writing libraries for release can still help with the documentation, for instance. I don't know how well this idea would go down? Perhaps there's something I'm not considering?
Iteratees consume input until they produce *a single* `Yield`, possibly with some leftover data. Enumeratees feed data to the same iteratee repeatedly, creating a sequence of yields.
Typeclassopedia is really essential reading for any Haskeller beyond the beginner stage. 
Surprisingly, I just discovered this paper. I feel so deprived for not finding it earlier! (Then again, that's how I felt when I also "discovered" the Stern-Brocot tree via Project Euler... :)
Or forM_ (lookup m k) $ \v -&gt; putStrLn ("We found a value: " ++ show v) where `forM_` is the one from `Data.Foldable`. 
&gt; debian "Alioth.debian.org is ... much like sourceforge or savannah but it's a service for Debian Developers." The benchmarks game website and source code repository is just one of the 871 projects **hosted** on alioth.debian.org Debian have nothing else to do with the benchmarks game.
&gt; In all the cases where the Java version is slower, compare the CPU loading spectral-norm &gt; a straightforward non-threaded version of the benchmark in Java Maybe someone already has - here's [the *forced onto one-core* fastest comparison](http://shootout.alioth.debian.org/u64/haskell.php)
&gt; Normally I’m very down on overlapping instances: it seems really fragile to rely on details of the type-inference engine to select what code should run. But this is a particularly benign case: here the overlapping instances are over phantom type variables, so the same code will run in either case. Maybe Haskell should allow that in general; overlapping instances are fine if the same run-time code is produced however the instances are resolved. Phantom types don't guarantee nothing (wrt overlapping instances). It's not true that phantom type variables don't have any link to the "code" that'll be run: data IsNothing = IsNothing data IsJust = IsJust data TMaybe l a where TNothing :: TMaybe IsNothing a TJust :: a -&gt; TMaybe IsJust a In fact, phantom types are problematic solution - you have to trust, that the author (Jose Iborra in this case) correctly designed them, didn't introduce any bugs, remembered to close/make abstract some things that would allow to create arbitrarly typed phantom values and so on. What about overlapping instances? This use (I mean control-monad-exception) is ok, because it's used to implement type-level functions, that overlap on argument's type (they do type-level equivalent of pattern-matching or if-statement) and in both cases they return values of different types, so even if something goes with type inference not as user planned, it will result in another type, probably even type error. There's nothing generating random surprises. So, &gt; overlapping instances are fine if the same run-time code is produced however the instances are resolved. , but sometimes there is no run-time. Sure, it can easily stop being that nice wtf-free if someone adds a new overlapping instance. but it's not unsound and if you do what you're not supposed to, don't be surprised that there are problems.
The odds of producing a be-all-and-end-all web app library interface that would satisfy everyone are pretty slim imo -- even for something simple like hack. Personally IMO rack isn't really that interesting, especially if you take O(1) space streaming as an absolute requirement (which I do) -- iteratees are pretty clearly the best answer there, although there are going to be lots of people who wouldn't want to deal with them. I appreciate the impulse but really a "rack"-like standardized interface isn't really even that important, HTTP is simple and standard enough that you can write a trivial wrapper to make nearly any two http server libraries work with each other. What Haskell really needs in this space is a well-tested, well-documented, integrated component-based framework for building web apps out of interoperable parts, coupled with a screaming fast web server. The details of which function you call to add a cookie to the response are trivial compared to that.
Most of the time when you build an interpreter or compiler in Haskell, you build up some kind of abstract syntax tree: data Exp = Var Var | B Bool | If Exp Exp Exp | Lam Var Exp | App Exp Exp and then you build an interpreter over that: eval :: Exp -&gt; Value The problem with this is you wind up with some cases where you need to lie a bit about the types, If above should probably only take a boolean valued expression as its first argument. To get around that the accepted wisdom is to turn to GADTs and make a more complicated expression type. data Exp t where If :: Exp Bool -&gt; Exp a -&gt; Exp a -&gt; Exp a B :: Bool -&gt; Exp Bool ... And you wind up with a routine like: eval :: Exp t -&gt; Value t or eval :: Exp t -&gt; t You then rely on the fact that the very act of pattern matching forces additional type constraints that ensure that the program is correct. What the paper demonstrates is that this encoding isn't necessary at all. Another encoding is to have the values that you construct be the computation you want to perform on it. But you may have several different things you want to do with the syntax tree. So you make up a type class that looks like: class Symantics repr where if_ :: repr Bool -&gt; repr a -&gt; repr a -&gt; repr a b :: Bool -&gt; repr Bool ... Then they go on to define several implementations of Symantics, from a naive interpreter, to a partial evalating compiler, to a CPS transformation, and how to use them and compose them. This encoding avoids the need for GADTs, opening up the option of using this encoding in languages that aren't as flexible as Haskell, but it also avoids all of the pattern matching overhead. Hence the representation is 'tagless' and can be basically overhead-free; an obvious implementation for the Symantics class above uses the Identity monad, in which: eval = runIdentity This leads to a fundamentally different approach to writing interpreters and compilers -- one in which you don't have an ADT at all! You can materialize one if you want to, with a suitable definition for repr, but you aren't forced to.
The Haskell wiki has a nice list of [functional pearls and research papers](http://www.haskell.org/haskellwiki/Research_papers/Functional_pearls).
As you say, with phantom types you have to trust the library author. What's wrong with that? You always have to trust someone. With the ghc runtime system you gave to trust Simon Marlow. Does the fact that is written in C give you some extra level of trust?
Good point.
Good luck with that. I am also starting a topic with writing a compiler for Verilog. But I'll post it when I'll have some time (it is a project and his due date is coming)
I think the Python and Ruby worlds will disagree with you here. Something like mod_python is only possible because of the WSGI. Also, did you look at my proposal? It gets "O(1) space streaming" without iteratees.
Quite a hack, especially the Cabal abuse bit.
No, but I'm not happy with the situation that I have to trust someone (even as smart as Simon). I can't wait for the days of usable certified compilers. I also see the difference between exception library by Jose Iborra (and Simon Marlow in fact), that I fully understand, and I can write alternative implementation of that (I just did), and ghc runtime that I don't know anything about, so I'm not entitled to an opinion about it.
slides of the control-monad-exception talk http://www.scribd.com/doc/25467100
It might be nice but that is a lot to read and study. Which ones are less experimental research or superseded by another and more pragmatic and/or idiomatic Haskell? that is the problem.
Well, even then, any list of essential reading probably has to be tailored to the reader. I.e. "What are your prerequisites and where would you like to go?" I'm thinking about designing a community website to that end, but I currently don't have the resources to build something like that myself.
Another Haskeller trolled by Harrop? No surprise.
Any idea why Iteratees are based on chunks rather than single elements? One could always parametrize the element type to be a list/chunk...
You've maybe gone and proved my point here. You solve the O(1) streaming problem by providing an imperative I/O interface (a legitimate thing to do) but I'll prefer iteratees here. &gt; There are a number of issues with left-fold enumerators IMO. It is basically promoting an inversion of control. This may be often times valuable. However, to make this the *only* interface precludes other use cases. The most basic one I ran into was wanting to interleave with read processes. The "inversion of control" is the best feature -- it means your HTTP parser can be run against any byte stream source (including a bytestring), which makes it more testable. Also, like a "withFile" combinator it relieves you of some of your obligations re: resource management. Iteratees are basically continuations, so they're easily composable. The "interleaving read" problem you reference is a no-brainer, if you pass an iteratee that terminates early into your enumerator, it returns another iteratee that sends the unread stream into the next action -- you just have to save the continuation. Obviously you can convert an imperative I/O scheme to one using iteratees (and vice-versa) but for me iteratees win out over handles. For you obviously the analysis is different. Even with examples you cite like WSGI there is not an absolute consensus in the Python community -- see e.g. [this blog post](http://www.b-list.org/weblog/2009/aug/10/wsgi/). With Haskell the situation is even more fractured, there are a zillion partially-completed web toolkits, each subtly defective in its own way (especially when it comes to documentation). Again, I appreciate the desire to have a standardized web-app interface, but attempting to standardize when there is no clear standard-bearer is premature IMO.
Chunks, rather than single elements, are used for efficiency. Many enumerators can be written so that they generate multiple elements at a time, which results in less calls through the generic iteratee functions. For example, consider an enumerator which reads bytes from a handle (`Enumerator e Word8 m b`). Bytes are usually read using the current page size (4096, on my machine). Having to either 1) read one byte at a time or 2) run the iteratee for each byte would be horribly inefficient.
Well, can't the "chunking" be in an upper layer? Enumerator e [Word8] ... Better yet: Enumerator e ByteString ...
The first option prevents writing generic iteratees, unless they're written as `Iteratee e [a] m b` (which is equivalent to the current types). It also means that your chunk type must be `Chunk a = EOF | Maybe a` (since there needs to be a way to represent "empty but still valid"), so iteratee implementers still need to deal with unpacking.
no slides, videos or links to papers?
I've sent you an email long time ago and haven't received any response, so I'll ask my questions here: 1. somewhere in cme sources, there's a note that MonadCatch is not so useful any longer, why is that? am I correct to say that even writing some generic code (e.g. with failure) we can handle exceptions with specific catch function, and still be generic in the result? 2. in those slides, there is a different MonadFailure definition, than in failure library (much better one). will failure library switch for this version? current MonadFailure is unusable, such a class synonym is inlined in all inferred type signatures and pollutes them.
Apologies, I didn't see your email. Anyway, I can answer your questions here. 1. The MonadCatch class is problematic in practice and does not fulfill the goal of producing generic code. Think it twice: would you find those MonadCatch constraints, referring to possibly unexported exceptions, meaningful at all in the API of a library ? 2. That definition was in control-monad-failure 0.5.0, we changed it for 0.6.0 to allow for error handling datatypes which are not monads (e.g. applicative functors). Why do you think it pollutes signatures ? It should inline into Monad and Failure which is what you want anyway, isn't it ? Do you have a concrete example?
Talk to the ACM...
Firstly, I think you miss the point of this proposal. It's meant to be a low-level interface shared by web servers (Happstack server, Hyena, CGI) and web applications/frameworks (Happstack, turbinado, my Yesod framework). &gt; Again, I appreciate the desire to have a standardized web-app interface, but attempting to standardize when there is no clear standard-bearer is premature IMO. Please explain to me how you would propose achieving a "clear standard-bearer" in this realm without someone trying to standardize things. Now, as far as your specific iteratee versus RequestBody/ResponseBody (what you call imperative I/O). Why not bring these issues up on the cafe, where others will see them? I said explicitly that this was a straw-man proposal, meaning it *should* be attacked and brought down wherever possible. index2 :: ResponseBody -&gt; IO () index2 rb = withBinaryFile "index.html" ReadMode helper where helper h = do eof &lt;- hIsEOF h unless eof $ do b &lt;- B.hGet h 1024 sendByteString rb b helper h I know I didn't spell things out in my e-mail regarding the interleaving read issue, but this is what I was alluding to. Maybe I just don't get iteratees, but how would you deal with using a function like withFile in an iteratee context? Also, my approach *is* a "withFile" approach as far as I can tell. If you'd really like to discuss the merits of this proposal, I'd prefer to do so on the cafe. **EDIT** Sorry, forgot about your comment about being unable to test. I think this addresses it sufficiently: newtype ResponseBodyTest = ResponseBodyTest (MVar BL.ByteString) instance ResponseBodyClass ResponseBodyTest where sendByteString (ResponseBodyTest mlbs) bs = modifyMVar_ mlbs $ \lbs -&gt; do return $ lbs `BL.append` BL.fromChunks [bs]
... or, for the moment, talk to me. I have a few copies of the printed proceedings left, and will give them away for free, via snail mail. First come, first served. Also, no videos were taken during the workshop, but I am pretty sure that you can find many of the slides and papers on the authors' webpages. Of course, the definite paper versions will soon be visible in the ACM Digital Library. Unfortunately, that will be behind a pay wall. (But many university libraries are subscribed, so often you will have free access from a university account.) 
mod_python existed well before wsgi
although I do have access to acm, I'll pass for similar reasons to why I don't use closed-source software. The only paper that I was really interested in (about checked exceptions) is available for free at author's website.
Dammit, darcs. You keep getting more and more awesome, and I don't want to want to switch from git. Also you have no github :(
Actually, they are already visible in the ACM DL: http://portal.acm.org/toc.cfm?id=1706356
I'm definitely not going to enter into a discussion about ideology here. But I am sure that many of the reasons that speak against closed-source software do not at all speak against the ACM DL (any more than they speak against going into a bookshop and buying a book). Anyway, of course I do not at all know what *your* reasons for not using closed-source software are. (And yes, I also like being able to download papers from the authors' webpages.)
That's (all) for PEPM above. PADL proceedings will not be in the ACM DL. 
I stand corrected. Let me restate my point: without something like WSGI, each framework/application must specifically add support for mod_python, as opposed to having it automatically baked in. If you want an example from the Haskell community, look at Happstack and Turbinado. Both of them (if I'm not mistaken) began by only supporting their own standalone servers. There was demand to have these frameworks work with FastCGI as well, and so support was added. I believe it is still not possible to have them work with CGI. Compare this to frameworks and applications built on Hack. By simply changing which handler is selected, the same app can run as CGI, FastCGI, standalone server, or any other handler that comes along. I'm actually surprised that anyone *doesn't* see the obvious benefits of this approach.
Wrong. Try [patch-tag](http://patch-tag.com/).
Awesome. Lemme try it again: "Also, there's no way to interoperate between different vcs systems and communities."
ad. 1 that's what I thought, but I couldn't put it clear enough:) ad. 2 how many applicative functors that aren't monads are there? how often are they used for 'normal' stuff? Monads are lingua franca in haskell, so I think nicer sigs for default case are important. paczesiowa@desktop ~ $ ghci GHCi, version 6.12.1: http://www.haskell.org/ghc/ :? for help Loading package ghc-prim ... linking ... done. Loading package integer-gmp ... linking ... done. Loading package base ... linking ... done. Loading package ffi-1.0 ... linking ... done. Prelude&gt; :set -XNoMonomorphismRestriction Prelude&gt; import Control.Monad.Failure Prelude Control.Monad.Failure&gt; let faily = failure :: MonadFailure e m =&gt; e -&gt; m a Loading package failure-0.0.0.1 ... linking ... done. Prelude Control.Monad.Failure&gt; :t if True then faily () else faily "" if True then faily () else faily "" :: (Monad m, Control.Applicative.Applicative m, Failure () m, Failure [Char] m) =&gt; m a Prelude Control.Monad.Failure&gt; :t faily faily :: (Monad m, Control.Applicative.Applicative m, Failure e m) =&gt; e -&gt; m a I think that type simplifier could be fooled by using overlapping instances and defining another, more specific instance in module with that synonym, that should stop it from simplifying such types. But I still think that custom MonadFailure (or MonadFail) would be better solution.
patch-tag is nice, but it ain't no github
What about [FieldTrip](http://www.haskell.org/haskellwiki/FieldTrip)?
Patch splitting and hunk editing! I've been wanting these forever. Awesome!
&gt; Firstly, I think you miss the point of this proposal. It's meant to be a low-level interface shared by web servers (Happstack server, Hyena, CGI) and web applications/frameworks (Happstack, turbinado, my Yesod framework). I get it, I think -- I'm just saying that personally I am not very interested in writing to any of the standardized interfaces that have been proposed (hack, CGI, happstack DSL, yours among them), and converting from any one to any of the others should be trivial. &gt; Please explain to me how you would propose achieving a "clear standard-bearer" in this realm without someone trying to standardize things. Market dominance precedes standardization. Happstack is probably currently the most-used but it has enough fundamental problems that I've stopped using it for the most part. &gt; Why not bring these issues up on the cafe, where others will see them? Because I'm of the opinion that there's no consensus to be found in this space yet and I don't want to decrease the signal-to-noise ratio over there. We can take this private if you're willing to discuss further, say the word here and I'll shoot you an email. Re: your index2 code: in an iteratee context you'd set up an enumerator: enumFile fp i = withBinaryFile fp ReadMode (\h -&gt; enumHandle h i) and the HTTP response body would contain this enumerator. The HTTP lib would hook it up to an iteratee that consumes the input and shoves it out the Socket. You can compose these without writing a typeclass -- pass it "stream2stream" and you get a bytestring out of it instead. Also iteratees have the really nice property of being easy to generate straight from parsers, you can turn an attoparsec incremental parser into an iteratee using a conversion function, which is very convenient for parsing HTTP headers/etc. Things like chunked transfer encoding are quite easy to do with enumerators also.
The implementation in the paper explicitly uses epoll, which is why the I/O manager in GHC is being rewritten. The threading models of both are about the same (many lightweight processes running on a few genuinely parallel heavyweight processes; GHC already does this). GHC doesn't have the dual, event programming model, but that's a question of API, and not of performance.
&gt; I think the Python and Ruby worlds will disagree with you here. That's my experience, at least in Rubyland. Rack may not be the ultimate solution, but it opened up a very productive path for Web development. Maybe this is a case of "worse is better", but it's a big win. My serious inexperience with Haskell may be keeping me from seeing programming approaches that just wouldn't fly in languages like Ruby or Python or the Rack approach, but having a base layer to abstract away issues of HTTP and offer a common API seems, if nothing else, a valuable step in the right direction. Is there something about this that would *hinder* a better/different approach?
I'll be there!
Wait what? Will epoll be used in the new io manager? I'm confused.
Not exactly. There are efforts to port tibbe's event library to work under the hood of I/O manager. So on different platform the suitable backend will be used — epoll, kqueue and etc.
Ohh, that makes much more sense!
Please, send me an e-mail on this, I *would* be interested to hear what you have to say about the iteratee approach. Frankly, it's new to me, and I (obviously) don't understand the intricacies yet. Also, I don't quite understand how you intend to deploy your web apps. Do you have your own standalone server that you use? I mean, at some point, you have to interface with the backend, and you've eliminated most of the interfaces available.
Well, if I made a "good enough" package, released it, and it got acceptance, it might hinder a better approach. Arguably, that's what happened with Hack, though I'm very happy with what Jinjing did in that realm. I had already written a much inferior version of Hack for my internal use (standalone server for testing, FastCGI for deployment) and latched onto Hack as soon as I saw it. Getting back to the point: I'd really like to get community input on this package before I even **think** of releasing it, because I don't want to pollute the Haskell world with an inferior interface.
&gt; Getting back to the point: I'd really like to get community input on this package before I even think of releasing it, because I don't want to pollute the Haskell world with an inferior interface I appreciate wanting to think this through and not wanting to have a ton of cruft for people to dance around when picking Web tools, but I bet Hack came about because no one was going to come up with a better approach any time soon. Also, while it's nice to get feedback, running code beats abstract arguments. Better to have things people can build on and bitch about than wait until all the what-if's and but-what-about's are considered, because in the meantime people will be building things to work with Hack and any other tool that actually exists today. Your "good enough" package might very well be the better approach, but without using it no one can really tell. Meanwhile, the better approach is code that runs right now.
I'm presently writing a compiler in Haskell for a subset of Pascal. If anybody's interested in having a look, mine is hosted on [github](http://github.com/effigies/compiler). It's in the midst of some pretty major refactoring, but [here](http://github.com/effigies/compiler/tree/128ec2baa706c690ef655d951400de04a1ed5d53)'s the last point at which everything both worked and (I think) was used. (I'm using this as one of my primary vehicles for learning Haskell, so it has required a few refactorings, as I learn how better to think in Haskell.)
Well, it's not as if the code isn't there; it's all available in [my github repo](http://github.com/snoyberg/wai). Also, it's not like I've been sitting on this for the past year; I first e-mailed the cafe about the WAI proposal last week, and this sunday submitted my proposal. However, I agree with your sentiment in general. We'd probably be in a better place right now if the original author of the WAI wiki page had released a package to Hackage. Nonetheless, I think waiting two weeks for community feedback won't kill us ;).
A couple other differences, the approach in the paper lets you implement your own scheduler which may be useful for tuning servers doing lots of IO (eg delaying things, doing stuff in batches to increase throughput at the cost of latency). By comparison the ghc scheduler is relatively simple. On the other hand the standard GHC IO threads are preemptively scheduled (and across multiple CPUs). The approach in the paper cannot use preemption.
You can't submit a link with a description on Reddit. You will have to either try again or add the link to your description.
What's in the description seems like a perfectly good non-tutorial to me!
It's interesting to think about, "intensional understanding" vs "extensional understanding." Definitions in terms of intensional qualities work well if you are well-versed enough in the context, that is if the words in the definition are rich to you — but work only very shallowly if you are a beginner. I don't think it's totally useless to be "interested in monads" because you want to be Paul Hudak's friend or whatever — maybe without that spark you'll be writing boring monomorphic Haskell programs forever!
&gt;The IO a data constructor can be seen as a function that takes a value of type a and returns a C program. I love it.
If only it were true. IO isn't even a data constructor, it's a type constructor. It takes a type, not a value.
I think what José meant by "IO a data constructor" (and please correct me if I'm wrong): The "invisible" function that is used to implement `return`. And this invisible constructor is of course of type `a -&gt; IO a`, or conceptually: `a -&gt; CProgram`
Exactly: "*IO a* data constructor" = "The data constructor of the type *IO a*".
&gt; Well, it's not as if the code isn't there; it's all available in my github repo. Good point. The people interested will poke around. Hopefully, they'll be enough of that to help guide discussions.
I suggest to have a look to safe-lazy-io http://www.haskell.org/pipermail/haskell/2009-March/021133.html
In other words, you don't get to see the data IO constructs, but since programs are data (thanks lisp) it's still true. Saying it's a C program is a little too literal, but it is a program for sure, in as much as any subroutine is a program. Just know that it's code that will run at some point (possibly not when you expect due to non-strictness, if you're having difficulty with that) and that it will return a value. I guess in this way you can understand all IO in terms of foreign functions, and pretend Haskell has nothing to do with IO, ever. Am I going too far? This analogy seems to want to go that way.
In the last weeks I've been playing a lot with Monads, and sous-vide cooking (using a PID controller from freshmealssolutions.com). In common, they're two fantastically useful tools, crippled by being ridiculously overhyped. The sous-vide controller is great for heating bangers straight from the freezer, when I'm too busy programming to cook, without the vacuum bag puffing up like the first Soviet space walk. Monads are simply a way of laying some of one's plumbing under the street. There are patterns for doing this, and people learn patterns fastest using practical examples unencumbered by hype. More helpfully, if one recognizes that "laying plumbing under the street" is the entire, utilitarian blue-collar (no Ph.D. needed) goal of Monads, then one spots opportunities for their use faster. A twin set of irritations lead to an epiphany for me. Learning parsers, I didn't actually think that the example code was as impressively clean and concise as touted. And I found myself wincing as I used (++) to combine my output, rather than composing functions as done with ShowS. So why do I bury my input plumbing under the street, but hand-carry my output in buckets? Homer Simpson "Doh!" time. Using StateT with a value of (a -&gt; a), and using liftM2 to lift (.), one can handle all basic parser description as function composition, outside a do construct. One only needs a do block for unusual constructs like matching an arbitrary &lt;&lt;EOF string at the beginning of a heredoc with the corresponding EOF at the end. Monads are simply a way of laying some of one's plumbing under the street. So bury as much as you can. 
Any docs / source for the live coding exercise?
Thanks for posting this, I've been considering learning one of the new JVM languages for fun. I'd also been looking at Cal, so it's nice to find out earlier rather than later that it's probably been abandoned. I know it's been brought up before, but any chance of GHC getting a JVM backend?
CAL works well, although you're right that it hasn't seen much activity. It has just moved to github (http://github.com/levans/Open-Quark) and may see more activity now.
Ok, an idea. So what about using CAL as the basis for a GHC Java runtime, and just porting full Haskell to the JVM?
That's an interesting read. I'd heard of Scala being complicated, and the slides seem to give a hint of just how complicated it really is.
I think any form of lazy I/O is unsafe, based on this example: http://www.haskell.org/pipermail/haskell-prime/2010-January/003076.html The only safe thing to read lazily are files which do not change while they are being read.
A lot of the library stuff is already there (std::share_ptr with std::make_shared, std::unique_ptr, std::tuple), and the required language supported as well. TR1 should be pretty much covered, but it is just an enhanced standard library. The main thing missing are lambdas, I think. http://gcc.gnu.org/projects/cxx0x.html doesn't even list them.
haven't you done this already in your thesis?
So does this mean it's _not_ dead?
There wouldn't happen to be a video to go with the slides, would there?
Nice presentation, too bad I couldn't make it down there this time.
No video, but a transcript is available in the "speaker notes" feature. Go to the bottom, under Actions, and select Speaker Notes.
A transcript is available in the "speaker notes" feature. Go to the bottom, under Actions, and select Speaker Notes
Seems kind of like Bamboo, which I'm already using.
*I don't think it's totally useless to be "interested in monads" because you want to be Paul Hudak's friend or whatever — maybe without that spark you'll be writing boring monomorphic Haskell programs forever!* I know that's what keeps *me* going. Um, but not with respect to Paul Hudak in particular (no offense, Paul), just that so many functional programming people are so cool that even if I didn't care about functional programming anymore, I'd still want to keep up with it just so I could talk to them.
That depends on what you mean by dead. There haven't been any new versions of the language for a couple of years, but as it all works well you can use it for doing interesting things anyway. For instance, I've integrated it with an STM implementation and deployed a CAL application to AppEngine. In the standard open source cop-out, I'll point out that you are free to contribute :-)
I don't really see anything in the slides that a Haskeller would find complicated. Much of the subtlety are in areas like self-types, path-dependent types and other type system features that have no counterpart in Haskell or indeed almost any other language. If you're interested in some of these features, you can read Odersky's papers which present a translation of them into a custom dependently-typed core calculus called vObj.
I'm fearful scala is going to be overenginerred like c++, it just feels like it wants to support everything. Personally I find the type annotation bothersome to read in complex code. For example a fold doesn't have the beauty of ocaml or Haskell. That's just me though. 
Hey, if you want Haskell or OCaml, you know where to find them. I'm happy that Scala isn't trying to be yet another functional language with a Hindley-Milner-style type system. The locality of Scala's type inference was a carefully weighed design decision based on the intended audience.
Perhaps because people already know about the shootout?
&gt; The __*IO a*__ data constructor That doesn't seem wrong to me ?
IO a is a type, not a data constructor. IO on its own is a type constructor. The IO a type has no data constructors, except if you want to talk about GHC's implementation mechanism, which is mostly a low-level hack anyway. Even if you *do* want to talk about that low-level implementation, it's nothing like what's described here. (IO takes as its parameter a typically-impure function of type `State# RealWorld -&gt; (# State# RealWorld, a #)` where `State# RealWorld` is a type with no values that is only used to help the compiler keep the order of execution straight using its existing mechanism to keep data dependencies of functions straight. For what it's worth, one can imagine a somewhat-practical implementation in which the IO type has data constructors that effectively just represent (higher order) abstract syntax of imperative programs, and where the runtime system handles the impure business of carrying out the effects, effectively separate from the process of evaluation. But as things are stated here, I think it's probably more confusing than anything.
Why not say return then?
Oh ! I confused data contructor and type ! Thanks for explaining. I feel stupid now, I thought I was over those mistakes now :(
I've discovered these "inlined" existentials construct myself. and as usual - Oleg already did it: http://okmij.org/ftp/Computation/Existentials.html Lennart Augustsson mentioned this: on #haskell &gt; &lt;augustss&gt; Paczesiowa: yes, that kinds works. But what if I want to apply 'foo :: Show a =&gt; a -&gt; Int' to the thing in the box? &gt; &lt;augustss&gt; Paczesiowa: A function I didn't foresee when making the box.
I have also discovered the same thing (with help from quicksilver's first rule of type-classes "You don't need one") when writing my own [Widget Set](http://github.com/Peaker/lui) and it almost convinced me that existentials were virtually useless. But then, quicksilver again showed what utility they might have with the example type: exists a. (IO a, a -&gt; IO a) I couldn't find a way to encode this type *properly* (e.g: Not replacing "a" by Int and using a memo) without existentials.
Well, precisely. It's not any one thing, it's the combination of the whole lot that gives Scala what looks like a large surface area. I do find it kind of funny that after years of people recoiling from Haskell in fear of its complexity, two fashionable languages should be Clojure and Scala, each of which is of the same order of magnitude in complexity. (Note that this is not a complaint; more power to anyone who wants to learn any of the three.)
There are no observations on a, so that type is isomorphic to (IO (), IO ()). Did you perhaps mean: exists a. (IO a, a -&gt; IO a) I *think* this is equivalent to Mu IO: newtype Mu f = In (f (Mu f)) via: fromE :: (exists a. (IO a, a -&gt; IO a)) -&gt; Mu IO fromE (m,f) = In (go &lt;$&gt; m) where go a = In (go &lt;$&gt; f a) toE :: Mu IO -&gt; (exists a. (IO a, a -&gt; IO a)) toE (In m) = (m, \(In m') -&gt; m')) 
Thanks, I missed something above, will have to think about the example some more :-)
If I understand correctly, the concern is that you have functions that work on arbitrary instances of the type class, rather than their "inlining". So if you have: data AnyShow = forall a. Show a =&gt; AnyShow a And you want to apply foo to the a inside, it suffices to have a Show instance for AnyShow and apply foo to the AnyShow directly.
AnyShow can have non-trivial implementation of Show - like adding a string prefix or something, then it's different than applying said function to the insides of the box.
Well in that case you would make a proper one which does have a trivial implementation, give your a to it, and pass that to foo.
and that's the inconvenience of the encoding:)
Nice work! Unfortunately patch-tag seems to be down at the moment. Is the code available elsewhere ?
unfortunately no. I wanted to upload to hackage today, but I've lost with haddock and cabal (the code and documentation is already written, the remaining thing is to force haddock to display what should be displayed and figure out what should be inside cabal package). I could send you snapshot by email.
A bit of coincidence, just thinking about existentials + typeclasses last night when I was looking at the source code for [super nario bros](http://svn.coderepos.org/share/lang/haskell/nario/). Existential &amp; typesclasses being used for game objects (check Actor.hs and Actor sub-directory). So is this really an anti-pattern? looks like a nice simple solution in this context to me.
my name is Bartek Ćwikłowski, Paczesiowa is my nickname. Yes, I'm aware that you can't pronounce any of that:)
I'm not necessarily saying I want Ocaml or Haskell (although I do use them). I am saying, though, that I find the type annotation in Scala bothersome. This, for example, I find to be convoluted. But again, that's just me. def partition[T](items: List[T], p: (T, T) =&gt; Boolean): List[List[T]] = { items.foldRight[List[List[T]]](Nil)((item: T, items: List[List[T]]) =&gt; items match { .... 
Sorry, The blog didn't have any other indication, so my guess had to be sufficient :(. I'll issue a correction next week, and hopefully all the unicode in there will make it out the other side... :) Also, really awesome article, btw. 
You've got a bug in your blog markup. Specifically, the `instance Foo x y =&gt; Foo z (x -&gt; y) where ...` block is typeset incorrectly. *Edit*: To be more specific, reading further it appears you deliberately kept it from being lhs, but it's still hard to read the way it is. Can you not typeset it appropriately and simply omit the prefixed &gt;'s?
I'll rewrite it all as .lhs file with html tags. right now it was .lhs file with plain comments, exported with hscolour to html and pasted into that crappy blogspot editor where I "clicked" the markup. never again.
This pattern makes a bit more sense when there are other classes that extend the class. When there are lots of these classes then the upcasting via `toWidget` becomes rather tiresome. For example Gtk2Hs uses something like this pattern and it gives us (safe) upcasting for free. It is only in rather rare circumstances that one has to explicitly upcast to loose the info about which instance so as to be able to do things like have lists of different widgets.
Ah that makes sense, hijacking classes to get decent record subtyping support. I think most of us agree that it'd be nice if Haskell had better record support.
I'm not sure what you mean by that example as it isn't well-formed code. But def partition(item: T, items: List[List[T]]) = ... isn't much more convoluted than partition :: a -&gt; [[a] -&gt; ... partition item items = ... though I agree the Haskell version looks cleaner due to separation of the type level and term level. As for explicitness, I tend to prefer explicit type annotations for module-level quantities even in Haskell. Types are the predominant part of a module's interface. I don't want that interface to be implicit or have it change by accident as I make code changes.
&gt; I do find it kind of funny that after years of people recoiling from Haskell in fear of its complexity, two fashionable languages should be Clojure and Scala, each of which is of the same order of magnitude in complexity. It's a very different kind of complexity that people recoil from in those three cases. Here's how someone unfamiliar with either Haskell or Scala might see it. Haskell takes away ad hoc side effects and enforces a very different way of thinking and writing code. Scala merely encourages the programmer to limit the use of mutable state in much the same way as Scheme: partly by convention and culture, partly by making the functional way the easiest and prettiest in most cases. Its type system supports Java-style OO and much more for the OO-minded programmer, seamlessly integrated with type system features that grew up in the functional world. It should be obvious why this proposition has broad appeal. Large surface area is part and parcel of that. I haven't personally used Scala very much, but I've studied it enough to see why it's catching on.
&gt; Yes, I'm aware that you can't pronounce any of that:) That'd be "Chvikwofski" wouldn't it?
I'm sorry, my example did not paste properly for some reason. Here is the full code I was referencing (which is from the 4square blog). I put it on gist: http://gist.github.com/285611 And to be fair, I do like toplevel type annotation, what bothers me in the case of scala is the type annotation on the fold + the ()() notation for explicit partial evaluation. I think these things are part of Scala's overengineering and, in my opinion, might make it more powerful but at the cost of readability, like C++. EDIT: The type annotation is not an example of overengineering but simply a weakness in the type system.
The last time I ran across this I punted: [my memcached lib](http://hackage.haskell.org/packages/archive/starling/0.1.1/doc/html/Network-Starling.html) All of the functions return in IO (Either ErrorInfo a) which isn't great. The most common way I've seen people handle errors is in ErrorT, but my error type doesn't fit into the MTL error class. So I punted.
This is a non-issue! There are only two ways to handle errors in Haskell, either purely (using Maybe or Either) or using exceptions (suitable for I/O code). 'error' could be considered a third, but it's really a runtime assertion, not a method of error reporting. ErrorT is a helper mechanism, equivalent and interchangeable with Maybe / Either and is not generally used in APIs. The horror of "8 ways to handle errors in Haskell" is IMHO a meme that spreads from one person who doesn't understand Haskell to another. "Haskell is nice to play with, but such inconsistencies still make it less appealing for serious development". ?! You've gotta be kidding me!
I can't say I find the "it can be encoded using other means" argument particularly convincing. Existential types are the (an?) essence of a certain sort of abstraction (implementation hiding), and being able to work with them directly can be nice, even if they aren't absolutely necessary for solving some problem. Conversely, if you have existential types, you can use them in lieu of coinduction, but you still might want the latter even if you already have the former. For instance, if you want an abstract type of stacks, you could have: Stack a = exists t. { empty :: t ; push :: a -&gt; t -&gt; t ; pop :: t -&gt; (a, t) ... } And that can trivially be encoded by an infinite tree (of sorts) of all possible observations for any given implementation, replacing whatever implementation the existential is hiding with however the language implements codata (essentially). But that just obscures the type you're trying to write down, which is "abstract stack implementation." Maybe I'm just over-infatuated with fancy types. I want them all. Existentials, quotients, substructural types.... Saying, "I can encode existentials as some isomorphic, non-existential type" sounds like "I can use loops instead of proper tail calls in python" to me. And I don't believe there should be only one way to do things. :)
Programming or math. Nothing there is specific to Haskell.
Well, Lee works at Galois and wrote a paper for the Haskell Workshop on using Haskell in space, so he gets some Haskell bonus points. Plus, it is an interesting article.
Or the new reddit I created a few hours ago, [softwarebugs](http://reddit.com/r/softwarebugs), which is where I've just submitted it. (Although it relates to hardware faults too, obviously.)
After looking at the examples and the code I have to agree: the interface is nicer than control-monad-exception. The encoding of exceptions using polymorphic variants appears to turn out very well, even better than in Ocaml. And fortunately all the type hackery does not surface in the API. I really like the way handle works in your library, this is what the MonadCatch class in c-m-e was meant to be. The idea of locally handling an exception and then returning a computation in a different monad is very, very cool. I am eager to put your library to use. Oh, and don't let MonadFailure stop you from using failure. Ignore it and use Failure directly, or define your own synonym, it doesn't matter. 
that's pretty close, but 'ch' and 'ć' sound different (if you're not polish, I wouldn't be surprised if you haven't ever heard such sound in your life).
there are plenty of submissions that aren't haskell related (all the agda stuff) but they are accepted here, we have broader horizons than proggit.
What does "punting" mean, in this context? Sorry, I'm British. :-)
Yes, that's the idea, with the exception of bind, who should execute the foreign function. But, of course, this is just a way of think about IO monads. The real implementation doesn't need to do things in this way.
I didn't properly specify the scope of my post -- I didn't mean to imply that existential types in general are the antipattern, because you can always encode it using other means. Rather, keep them in their place: writing a typeclass and then passing around existentials over that typeclass is a red flag; maybe you should consider losing both the typeclass and the existential and just using a record. It's interesting -- I design most of my modules as abstract data type plus operations. You can encode this using an existential. You can unroll existentials. I wonder if it is worthwhile to unroll whole modules? (Maybe not in Haskell -- invent language features at your leisure) Are abstract data types a stepping stone?
What if the Monad typeclass didn't contain fail :: String -&gt; m a but instead something like fail :: Error b -&gt; m a ? (Perhaps this needs a `forall` somewhere, but I'm no Haskell expert.) 
Huh, trying this with a subset of graphics-drawingcombinators. Trying to eliminate: exists image. { render :: image () -&gt; IO (), sample :: R2 -&gt; image a -&gt; IO a, point :: R2 -&gt; image Bool, line :: R2 -&gt; R2 -&gt; image Bool, convexPoly :: [R2] -&gt; image Bool, transform :: Affine -&gt; image a -&gt; image a, tint :: Color -&gt; image a -&gt; image a } I have: Image a = { render :: ??? sample :: R2 -&gt; IO a, (%%) :: Affine -&gt; Image a, tint :: Color -&gt; Image a } point :: R2 -&gt; Image Bool line :: R2 -&gt; Image Bool convexPoly :: [R2] -&gt; Image Bool But I don't know how to fit render into the picture. It seems like you could use a GADT (so much for simplicity!) with two constructors: data Image a where MkImage :: (R2 -&gt; IO a) -&gt; (Affine -&gt; Image a) -&gt; (Color -&gt; Image a) -&gt; Image a MkImageWithRender :: IO () -&gt; (R2 -&gt; IO ()) -&gt; (Affine -&gt; Image ()) -&gt; (Color -&gt; Image ()) -&gt; Image () But that is disgusting -- oh and also not accurate because you can construct an Image () that can't render itself. Is there a better way? Or is my hypothesis just false...
Maybe you guys want to collaborate and just patch c-m-e instead? Just a thought, it sounds like they are targetting the same domain. Also, [we discussed the failure package at length](http://www.reddit.com/r/programming/comments/atlw9/pure_extensible_exceptions_and_selfreturning/c0jbw5a) over in proggit.
...But some will have immunity from this meme, if they're in the habit of screening out subject lines implying lists, which are usually shallow bait for a high delicious.com ranking. I nearly ignored this, but then I heard Paul Simon singing "50 ways to leave your Monad" and I had to look.
Yeah, I hate the ErrorT's type-class restriction. A general EitherT would be nicer.
The goals are similar but the implementation is pretty different. c-m-e reuses the extensible exception datatype already included in the base library while pure-exception uses a completely new mechanism based in polymorphic variants. It is not just a patch on top of c-m-e, pure-exception is a completely different approach. That said, I am more than willing to accept patches for c-m-e. Regarding the MonadFailure issue, I am on Michael's side. While I agree that type class synonyms are not nice, you can always use Failure directly together with a Monad constraint.
Upgrade to [monadLib](http://hackage.haskell.org/packages/archive/monadLib/3.6.1/doc/html/MonadLib.html#t%3AExceptionT) :)
Nice story, but misleading title. It had more to do with optimizing Uniplate then optimizing HLint.
some ideas could help cme (like api based around MonadFailure), some code could be re-used (like modelling hierarchies, but it's rare in practice) but these solutions are based on different ideas - mine is pure and full of type-hacks and it will be at-best as fast as ErrorT and I wouldn't be surprised if ghc wouldn't know hot to optimize those polymorphic variants at all. cme has one advantage - it doesn't have any runtime cost, I cannot compete with that. I just wanted to show that it is possible to do it in pure haskell. for practical things I'll use cme.
here's my message to snoyberg: I don't know git, I wanted to send you a patch made by diff, but I realised that the patch is trivial, and there are many ways to implement it - so I'll leave it to you to choose an implementation. the problem is not with MonadFailure class, the problem is with its only instance - it's a type synonym. if I define a function with an explicit MonadFailure context: raise :: MonadFailure e m =&gt; e -&gt; m a raise = failure when using it, ghc will realise that this class is just a synonym and inline it immediately, which is wrong, it pollutes the type sigs. to stop ghc from simplifying, there must be another instance, e.g: instance MonadFailure e Maybe then ghc cannot simplify, because it's not always type synonym. there is a problem with previous instance, it overlaps for Maybe, that's not a problem, OverlappingInstance would allow this usage, but they're not needed. another idea is to provide the same instance but for another type than Maybe, type that's never used, so no one will detect that it overlaps. it shouldn't be exported, so people don't get confused. Here's one implementation: newtype DummyMonad a = DummyMonad { unDummyMonad :: Maybe a } deriving (Failure e, Monad, Applicative, Functor) instance MonadFailure e DummyMonad it needs GeneralizedNewtypeDeriving, if you're ok with that, than fine. if not, those instances have to be written by hand (they can be "error "dummyinstance" or something), but to implement Applicative, it must be imported (not only pure).
unfortunately, seeing some code by glguy: http://www.reddit.com/r/programming/comments/atlw9/pure_extensible_exceptions_and_selfreturning/c0jc43c made me realise that if I have many errors, I will carry nestet monad transformers and unwrap them with handle, there's no need for polymorphic variants. I'll have to think how to fix it - handle will probably get uglier type to maintain the same types inferred for user code. if it cannot be done, I'll have to go back to the previous version that was almost the same as your EMT. but to make it total (runEMT) I had to define GADT variant of either that new from its phantom type if it could fail, it worked, but the type for handle had three lines, explicit foralls and was very ugly. damn:/ making things safe is easy. making inferred types nice is hard. making type errors readable is the hardest thing.
Got it. As I said earlier, I didn't exactly understand the blog post ;).
All the Agda stuff ties in much more closely, what with the type-hackery. It is extremely useful as an implicit contrast to Haskell, showing the power (and drawbacks) of dependent types, which can at least help the community decide if dragging Haskell that way is a good idea or not.
I had noticed the complexity problem when nesting handlers. Maybe it is possible to apply the trick of "Asymptotic Improvement of Computations over Free Monads" by Janis Voigtlander. After all, Either is a free monad. But on the other hand, the problem illustrated in that paper does not look the same as the problem here, so don't hold your breath. Interestingly, glguy's Throws class replicates the Throws class in cme. His code avoids the compositionality problem by nesting handlers, but at the cost of multiple traversals. Theoretically, polymorphic variants fix the compositionality problem without incurring in the complexity issue, so there must be something wrong with your approach. I.e. do not ditch p. variants so fast. 
no, no I'm not going to ditch them (not after spending so much time to understand them). the problem is only with handle function, multi-handler (I named it handles for the lack of better name) applied to list of many handlers correctly forces only one FailyT level, with polymorphic variant of errors. I think glguy's approach cannot model multi-handler.
I have a cousin who once met a guy that had heard of Haskell. Does that mean that if he takes a picture I can post it here?
The link here to this three year old blog post revives bad practices of ten years ago, just when we thought they were finally fading into history. One of the worst parts is its strong recommendation to use *fail* for reporting errors. No, no, never do that! *fail* is not really part of a monad, and it's not meant for error reporting - it's just a hack that supports pattern matching syntax for variable bindings in *do* blocks. Using *fail* for error reporting makes programs fragile. It circumvents the safety of Haskell's type system: the type signature claims that the function works for any Monad, but really it is only safe for instances of MonadPlus, or for the IO monad but only when wrapped in *catch*. It's like de-referencing a pointer in C that might be null. Being ancient, comments have been closed on this post for ages. Unsuspecting Haskell newbies might click on the reddit link, read the hand-full of supportive comments at the bottom, and start coding that way.
"..the real solution is not to do stop-the-world GC at all..." What is planned to replace it? I expect you don't mean an outright incremental GC, so.. garbage-first? Or just per-core heaps of some kind?
Incremental and on-the-fly are the only two options I'm aware of.
Well the result of a bind would require the execution of the foreign function, but it is not necessarily true that bind would have to be evaluated in a strict way. Control.Monad.State seems to do a non-strict bind by default, unless you include Control.Monad.State.Strict anyway (I've been bitten by that)
But dude... It's haskell... in space. Haskell. In. Space. I'm okay with an occasional bit of offtopicery if he's using Haskell. In. Space.
30s-10s was fixing a performance regression in Uniplate, which might not count as the actual optimisation. Of the remaining 8.5 seconds saved, 2.5 came from Uniplate, and 6 from HLint. Certainly the intention when I started was to optimise HLint, it was just profiling that drew me towards Uniplate.
Why don't you try out the failure package? Instead, you could have: MonadFailure ErrorInfo m =&gt; IO (m a) or if you're willing to be tied down to a monad transformer library (eg, mtl or transformers): (MonadIO m, MonadFailure ErrorInfo m) =&gt; m a
Feel free to provide a link to better practices.
Nevermind, I remember putting off reading this a few days ago: http://paczesiowa.blogspot.com/2010/01/pure-extensible-exceptions-and-self.html
Not Polish, but I had a Polish--Czech friend growing up (and have a linguistics background). I couldn't remember whether Polish devoices the "w" before unvoiced sounds, like Russian does, or not.
Yes, I don't see a good way to encode that, without doing something like: render :: TEq a () -&gt; IO () which would be fairly undesirable. Anyhow, I wasn't necessarily responding to the article, but to Peaker's sentiment of "it almost convinced me that existentials were virtually useless." Even if every use of existentials could be easily eliminated by encoding them in other ways, they might still be useful because they directly express some intent that isn't readily apparent in the encoding. Of course, functional programming naturally devalues existentials outside of module systems, but I don't think that quite relegates them to useless. The fact that GHC's existential quantification is confined to data declarations probably limits their usefulness further, of course. That keeps them from being used for on-the-fly, first-class abstraction.
Do note that there might be some short cuts that can be made, in his stating "at least for minor collections"
I have that in the darcs branch. But I didn't understand how to consume the type - since I can't unify it with ErrorT ErrorInfo IO a, what do I do with it? And if I want to be able to unify with ErrorT, I then need to provide an instance for the Error class. Which means I need to pick between the mtl or transformers. Maybe I'm missing something about how the failure package is intended to be used.
My lib returns errors in such an akward way I'm forcing any user of it to wrap it in their error-handling API of choice. So I'm taking the ball and kicking it to the other end of the field for someone else to deal with.
There are Failure instances for Either, Maybe, IO built in. If you use the IO (m a) approach, then without any work you can treat it as IO (Either ErrorInfo a). If you use the MonadIO approach, then you've already made the decision between mtl and transformers. If you want some more conveniences built on top, there are two other packages to consider: control-monad-exception and control-monad-attempt. I wrote the latter, so have more experience with it, but the former is more designed for dealing with one exception at a time. That said, you can make the latter deal with each type of exception you have. [E-mail me](http://www.snoyman.com/blog/) if you need some advice.
It is always nice to see people re-implement built-in features as a library, hence, I am curious about Andrew Coppin's article "Implementing STM in pure Haskell". How does it compare to [A High-Level Implementation of Composable Memory Transactions in Concurrent Haskell](http://www.springerlink.com/content/y8555212g6w6w85u/)?
I do *not* like InfoQ's interface. Direct video link [here](http://flv.thruhere.net/interviews/09-nov-matzyukihiro.flv).
Matz is such a breath of fresh air compared to other "benevolent dictators". I admire how open to new ideas he is.
Heinrich Apfelmus's article is a remarkably clear explanation of old wine in new skins. The use of GADTs is particularly beneficial. I have a small remark on laws that seems to come up every now and then: In the section on breadth-first parsing, Heinrich states &gt; the idea is that *expand* fulfills &gt; &gt; foldr mplus mzero . expand = id whereas it doesn't. The given *mplus* is not associative (but represented by a constructor). The monad plus laws, used in the explanation of *expand* hold *only up to observation* by the interpreter functions: although there are different terms that should be equal via laws, such terms cannot be distinguished after running the interpreters. The aspect which laws come for free and which need to be ensured by the interpreters is slightly neglected in the article. 
great intro!
Nice read, though it does seem every issue has to contain some sort of Monad-from-scratch/tutorial/explanation or what not :-) I wonder about MonadPrompt -- why is it a better approach than a custom monad that has those specific operations, plus a type-class for that custom monad, if you want the indirection for testing purposes? I think writing the GADT and then matching on it is very similar to writing the type-class and then instantiating it. And the type-class/custom-monad approach can probably be Haskell98, too.
I like articles in the style of Yang's, similar to Yorgery's "Typeclassopedia". Are there any more articles knocking around in a similar style?
If you want to achieve the same effect, you'd have to make your code polymorphic in the monad and use two different instances. example :: MonadRandom m =&gt; m Bool example1 :: State StdGen Bool example1 = example example2 :: [(Bool, Probability)] example2 = example Furthermore, the ability to pattern match on the prompt is very useful, see also "The Operational Monad Tutorial". ;-) 
"Implementing STM in pure Haskell" -- Dang, somebody beat me to it! Oh well, my version doesn't even work the same as this or the GHC implementation (was intending to benchmark a few locking tricks against GHC's implementation), so I'm still somewhat in the clear.
Agreed - and I wish they'd let you just download the transcript for offline reading (faster to skim that watching the vid!), rather than having that crappy Q&amp;A panel on the page. I know I can copy/paste, but that loses the info on who's speaking. Ah well. Thanks for the link!
Glad I could help. =)
Programmers could learn a lot about good behavior from the language creators they idolize. The mere mention of ruby will send some pythonistas into angry tirades, for example. But the guys running Ruby, Python, Haskell, etc. are actually cool dudes who wish only the best for the other languages.
I worry that this kind of statement appears elitist, alienating, and is ultimately counterproductive. 
&gt; elitist, alienating, and is ultimately counterproductive I think you meant to say [unscientific and ultimately destructive](http://dis.4chan.org/read/prog/1202913500/1).
In short: the recent additions of further yields in the scheduler seem to perform much better than issuing futex syscalls on Linux, while the later solution was anticipated to perform much better.
Ouch. What a spiteful discussion. Sometimes I forget how vicious the world can be out there. Quick, back to the warm, sheltered atmosphere of /r/haskell.
Not only that, it's also nonsense. I have yet to see a half-decent programmer that seriously tries to understand FP and doesn't. Some are not interested in trying to understand it, but that is an entirely different matter.
All the more reason to tackle it head-on. Rarely if ever an important truth is couched in comfortable sounds.
Great quote, but I fail to see what relevance it has to Don's post.
I would upvote this 10 times if I could. There is likely a correlation between being interested in FP and being able to understand it, but I really think there is no crucial personality factor required for the latter. If you can understand algebra (which most people can), then you can understand FP. I think the real problem is getting students to "see the point" of FP, and what benefits it brings over imperative programming.
The actual content: On Mon, Jan 25, 2010 at 7:37 PM, Luke Palmer &lt;lrpalmer at gmail.com&gt; wrote: &gt; I have just read "Asymptotic Improvement of Computations over Free Monads" by Janis Voigtlander, since I have been working with free monads a lot recently and don't want to get hit by their quadratic performance when I start to care about that. &gt; But after reading the paper, I still don't really understand how "improve" actually improves anything. Can anyone provide a good explanation for where the work is saved? With a free monad, the structure keeps growing via substitution. This requires you on each bind to re-traverse the common root of the free monad, which isn't changing in each successive version. Lets say the size of this accumulated detritus increases by one each time. Then you will be traversing over 1, 2, 3, 4, ... items to get to the values that you are substituting as you keep binding your free monadic computation. The area near the root of your structure keeps getting walked over and over, but there isn't any work do to there, the only thing bind can do is substitution, and all the substitution is down by the leaves. On the other hand, when you have CPS transformed it, at each layer you only have to climb over the 1 item you just added to get to where you apply the next computation. This only gets better when you are dealing with free monads that provide multiple points for extension: i.e. that grow in a treelike fashion like: data Bin a = Bin a a data Free f a = Return a | Free (f (Free f a)) data Codensity f a = Codensity (forall r. (a -&gt; f r) -&gt; f r) If you build the free monad Free Bin, then you will wind up having to walk all the way down to the leaves on each bind, well, modulo demand, that is. But since it is a free monad, the 'body' of the tree will never change. Just the data you are substituting at the leaves. Codensity (Free Bin) lets you only generate the body of the tree once, while your substitutions just get pushed down to the leaves in one pass. An interesting exercise is to work out why Codensity can change the asymptotics of certain operations over Free Bin, but Density doesn't change the asymptotics of anything non-trivial over Cofree Bin for the better. -Edward Kmett 
Just a funny example of great minds thinking alike; Don's comment and The Sussman's email show a similar way of thinking (pragmatic and anti-elitist) and by coincidence a similar formulation.
Interesting package. It is slightly annoying that Program is a synonym rather than a newtype. When declaring instances (for MonadState or MonadPlus etc.) one has to spell out the internal type and sometimes even needs to import Control.Monad.Identity only to mention it in the instance head.
Don't try to be modest dons, we all know you are better than us. I am letting you off the hook, please don't worry about sounding conceited.
&gt; The monad plus laws, used in the explanation of expand hold only up to observation by the interpreter functions True that. :-) For simplicity, I lumped everything into the "is not a constructor" bucket, claiming that a few selected forms like `merge p q &gt;&gt;= is` can be pattern matched on. Ooh, and here a fitting proverb transformation: * If a tree falls in a forest and no one is around to hear it, does it make a sound? * If two constructors are built and no one interprets them, are they different?
Yes you have to be very special.
"an EDSL is where you craftily warp the host language to look like a completely different language with all sorts of frighteningly clever hacks" lol
Demo from Simon Marlow: http://www.youtube.com/watch?v=qZXq8fxebKU We'd appreciate any feedback about ThreadScope and the event logging framework in GHC. Thanks!
Demo from Simon Marlow: http://www.youtube.com/watch?v=qZXq8fxebKU We'd appreciate any feedback about ThreadScope and/or the event logging framework in GHC. Thanks! 
Do you guys really think that everyone can pick FP up, provided they're interested enough? Did those young fellas back in the first grade have problems with reading text and subtracting numbers due to their lack of interest, too?
I was actually hoping to see Simon and there's not even any voice recorded on this video:(
cabal: cannot configure threadscope-0.1. It requires cairo -any, glade -any and gtk -any There is no available version of cairo that satisfies -any There is no available version of glade that satisfies -any There is no available version of gtk that satisfies -any 
We are talking about CS/SE students, no? I said: &gt; If you can understand algebra ... then you can understand FP. "Understand algebra" does not mean everyone. But most, and certainly those who go into CS or SE.
You haven't installed gtk2hs. gtk2hs is *not* on hackage.
I'm ok with that implication, but I don't agree that most people fit in the premise of it. Also, being a half-decent programmer likely implies the general ability to understand FP, but how many half-decent programmers are there? At our faculty, they admit students who rank above the 40th percentile in a mixed logic/knowledge test (common to most, if not all, faculties), giving almost everyone a chance to catch up and grow into a computer scientist/programmer. Haskell course is mandatory, and recommended in the first year. Last year the success rate was around 30 %. That's 1/3rd of the top 60 % who applied to a university. But yes, 40th percentile is a low barrier and I ignored interest, though I still think that your “most” is naive. I know actual people who earn a fair amount of money in IT and study to get a bachelor in CS, who I believe will get the degree eventually (ie. passed the course, somehow), yet neither me nor they would say they have a chance to actually understand and effectively use FP. Hence I conclude that not just CS students, but even CS graduates may not be able to pick up FP without serious long term suffer.
&gt; I ignored interest I have to say that I still think this is by far the largest factor. I have yet to see a student who is interested and willing, but still unable to understand FP. &gt; et neither me nor they would say they have a chance to actually understand and effectively use FP &gt; ...even CS graduates may not be able to pick up FP without serious long term suffer It all depends on their motives. A problem with FP is that many newcomers simply don't see the benefit, which in turns means their interest is limited. If there is something about FP that is inherently hard to understand, we should be able to point it out, shouldn't we? At least I can't see it (which doesn't mean I am right).
&gt; I have to say that I still think this is by far the largest factor. I have yet to see a student who is interested and willing, but still unable to understand FP. Fair enough. Somehow I felt that the motivation incurred by preparing for a last re-sit should be sufficient (and indeed such people perform much better than random freshmen), but your wording of "interested and willing" convinced me. :-) I do feel that there is something inherently hard to understand, but I'm unable to point it out.
Yesterday's [exercise](http://programmingpraxis.com/2010/01/26/primality-checking-revisited/) updates the Rabin-Miller algorithm used in the referenced article to the Baillie-Wagstaff algorithm that uses Lucas pseudoprimes combined with strong pseudoprime tests to bases 2 and 3, which is the same algorithm that *Mathematica* uses.
I started trying to cabalize bits of gtk2hs, but man is their build complicated. I now have a working separate cabalized cairo and was (last I was fiddling with it) trying to make it so gtk2hs would then depend on that cairo from hackage...
It would be nice if essential tools did not depend on stuff (gtk, cairo, etc) which are almost impossible to install on certain systems... My experience with these large Linux-originated opensource projects that they mostly work on Linux (probably because the distro maintainers spend a lot of time and energy to make them work, and then prepare binary distributions), and they either don't work at all or are extremely painful on anything else. Now, the dependency hell that the Linux ecosystem is makes basically any part of it unusable without the rest, and the above means you have no chance to have the whole thing. That is true for OSX, even though it have *two* separate *dedicated* port system...
The lesser problem is that gtk2hs is not on hackage. The bigger problem is gtk itself... Edit: I changed my opinion. Both are equally big problems.
Well, there are some inherent complexities in algebra... Mathematically speaking, classical logic is braindead simple (compared to, oh, modal logics, type theory, or anything interesting). But that doesn't mean plenty of people don't quite get it, or that even folks who get it don't occasionally mess up. It's not that "math is hard"; one of the standing difficulties in epistemology is explaining how we can know all the basic facts and know the rules for manipulating them, but yet we don't immediately grasp all the consequences derivable from them. Just like with programming (of any sort) there's an inherent complexity here and a difficulty with making the leap between implementation details and the big picture. I don't think functional programming is particularly difficult (compared to other programming), but its simplicity means it's so close to the algebraic and logical underpinnings that it only makes the inherent complexities of computation and mathematics stand out more clearly. Perhaps this is where your feeling comes from?
I've never had a problem getting GTK on Linux (not surprising) or Windows (perhaps more surprising). Are you on Mac? Or perhaps a not-XP Windows system? It would be nice if the big budget OSes used some of their budget to maintain a repository and package system for their users.
This is hardly an essential tool and I can't think of a way to make it without a largish dependency on a library such as GTK or WX... and I have a much easier time with GTK than with WX in Haskell.
To install on ghc-6.12.1 I had to modify the .cabal to accept containers package version 0.3 (I already had the gtk2hs installed). It works fine at first glance, will use it more. Edit: Posted on the ML.
You can always have a bare-bone version using OpenGL+GLUT, which at least works everywhere.
I'm on OSX. Here is the typical situation: You try to install something (eg. gtk) from the volunteer-maintained port system. It downloads 100000000 dependencies in source form, tries to compile them, and after 12 hours of compiling, you get a random error message at the 7248172th dependency. Now, in the case a GTK, there is a totally independent (and thus, different) binary distribution prepared by some random guy, but still. The main problem imho is that in the Linux ecosystem, everything depends on absolutely everything else (and of course, 90% percent of it simply shit...). Then add braindead build systems. The windows dll situation looks positively happy compared to that. Actually, now that I check it, it seems that somehow finally I managed to install gtk2hs at some point in the past, using this binary GTK distribution; but it must have been very traumatic if I don't remember that at all, and still not have anything depending on it installed... So, next step, glade. Googling for "glade haskell install" gives nothing usable. After a while, I realize that maybe it is bundled with gtk2hs, and indeed, it is. But it seems you cannot install it separately, so let's start the whole process again... Nah, sorry, but I think the whole situation is simply horribly wrong. On Windows, you either have a binary distribution which works; or you don't, and then it's more healthy to accept it as a fact of life.
It would be great if you could reply to the announce email thread from this post with a diff of the changes you made. 
Got ThreadScope to build on OS X. You need to have the following lines in e.g. .bash_profile: # Get MacPorts includes export CPATH=/opt/local/include # Get MacPorts libraries export LIBRARY_PATH=/opt/local/lib export LD_LIBRARY_PATH=/opt/local/lib Then type sudo port install gtk2 cairo librsvg libglade2 cd gtk2hs-0.10.1 ./configure --prefix=$HOME --with-user-pkgconf make make install Then just use cabal install to install ThreadScope.
Isn't Threadscope developed primarily on Windows?
Very cool. I'm hoping to try this quite soon. :) One suggestion, please update the Hackage entry with documentation for the Modules. Haddock can generate the docs for you if you annotate your source code. Thanks!
How well would these adopt Haskell's lists' laziness?
Gosling
I think the reason that the API documentation hasn't been generated is that hackage fails to build this library because 0MQ is not installed on the build servers. The actual source code seems to contain haddock comments.
gtk2hs has some [difficulty](http://hackage.haskell.org/trac/gtk2hs/ticket/1165) installing on Windows if you installed GHC via the Haskell Platform. Or if you just installed it anywhere with spaces in the path. And the error message doesn't help at all. (Not that it really could...) It's one of those things you could develop with and never notice the problems, but when others go to use it they can't find any way to duplicate your setup.
Reading on the [zeromq site about the Haskell bindings](http://www.zeromq.org/bindings:haskell), it's quite pleasing that the installation instructions look so straightforward. It's nice to be reminded occasionally how far we've come with packaging. 
Have been waiting for this. Thanks :)
Beautiful. One question: In System.ZMQ.Base there is defined: data ZMQMsg = ZMQMsg { content :: !(Ptr ()) } Were that changed to: newtype ZMQMsg = ZMQMsg { content :: Ptr () } then wrapping and unwrapping overhead would vanish entirely at runtime.
Agreed. I love seeing posts along the lines of: * click &lt;haskell platform&gt; * cabal install $foo And I think back to 2001.
Thé cost of newtypes is not guaranteed to disapear at run time. For sure you allocate a constructor, but you may have to deeply copy a structure (even if rewriting rules tries to cure that, f g = g ZMQMsg
Oh man. I feel your pain. I tried building GHC on Windows and gave up. Essentially I need Cygwin, the GCC toolchain, darcs, perl, twice the number of environment variables that I have at the moment (including a LONG path), and enough dependencies to build from source for a few days of fun, profit and headaches. Not to forget that the documentation tells you to run certain scripts from the command line in the multi-platform section, and then when you look at the file it's a bash or perl script... I'll contribute when I get a *nix working, which is when I reformat my drive to be able to dual boot, which is when I get an extra HD and plenty of free time...
The indentation looks very strange on github (and it actually depends on the browser). Do you use tabs? I would recommend to eliminate any tab characters from the source, and maybe use a smaller indentation. Apart from that it looks very nice to me!
Yeah formatting came out a bit weird and yeah I was using tabs but I didn't realize until now just how much white-space eclipse was using, looks like 8 spaces long.
HLint likes it (http://community.haskell.org/~ndm/hlint/). One thing: do putStrLn "failed" ; return () could be rewritten as: putStrLn "failed"
Oh yeah I must have just over looked that. By the way that link appears to be dead but I googled it, sounds like an impressive tool!
I may have overstated my case. I agree and never meant to imply that the compiler is smart enough to magically realize that the type constructor for a newtype is an identity function and that it should never map it over the structure, etc. But the issue with using the single constructor strict unpacked data type is that you ensure that every time you wrap or unwrap the type you pay to allocate a fresh constructor and copy the contents over into it, whereas if you unwrap and rewrap a newtype without doing anything to it, you share in many common cases like: inFoo f = Foo . f . runFoo 'inFoo id' will result in sharing with a newtype and a completely new Foo with the same contents with a strict data type. So switching to a newtype for a single argument data type that is strict in its argument is still a pure win.
I find too much space around " = " and " &lt;- " difficult to read. It's hard to follow lines.
 data MessageDir = MessageDir { upMessage :: Surface, downMessage :: Surface, leftMessage :: Surface, rightmessage :: Surface } indentation is weird (but I guess it's a matter of taste), but you might try to align "::", it ,akes it easier to read long series of definitions. M-x align-regexp RET :: RET
See: http://hackage.haskell.org/trac/haskell-prime/wiki/Tabs: according to the Haskell Report tab stops should be 8 characters wide.
I didn't want to indent like that but I'm using some of my work code conventions which has become a habit for me, where curly braces line up but Haskell data type definitions I can't do this without indenting on the new line. I will change it too: data MessageDir = MessageDir { upMessage :: Surface, downMessage :: Surface, leftMessage :: Surface, rightmessage :: Surface } Also I used tabs when I should have set the editor to convert them to white-space characters, even I know this. I will sort out these issues.
&gt; By the way that link appears to be dead I think community.haskell.org is having issues today.
Definitely take a shot at it. Maybe create a public repo somewhere so other people can contribute and you can shard it out?
I would normally complain about how imperative this looks, but SDL and OpenGL are imperative APIs, so I don't have much to complain about that hasn't already been said.
Well I didn't as such want other people to write it for me. I'm hoping to learn from this myself. I was hoping for some examples to see what I'd use in Haskell to replace the ML lexer/parser generators. Maybe I should have stated that. I know the Haskell platform seems to have something built in but how nicely it maps to ML-Lex and ML-Yacc I don't know.
I stopped reading at &gt; The essence of monads is to use abstract types to enclose a mutable state
Of course I agree on using newtype here. I was just clarifying one point
take a look at [happy](http://www.haskell.org/happy/) and [alex](http://www.haskell.org/alex/).
Not to flamebait, but what really is the value in writing Haskell equivalents to lex and yacc? I speak as one familiar with parser combinators but ignorant of L and Y.
@ simonmar: Presumably MSR pays for EC2. Is it not cheaper money-wise in the long run and hassle-wise in the short to just requisition a build box at work? How often do you do parallel builds? 
The benefit is that the examples from the book will translate readily. In the end while I want to write the code in Haskell I want to stay as close to the original ML as possible. There are a few cases where I've used the IO monad where practically I could have rearranged stuff to get more of my code into functions. However this would increase the gap between what the book is talking about and what the code looks like on paper. Also if something comparable to the mentioned libraries are in the Haskell platform then it decreases the amount of dependencies I need to drag in. As it is it seems that happy and alex are the packages I want. I will have to see how close they are to ml-lex/ml-yacc. I would actually be interested in a book that writes compilers in Haskell using say parser combinators but I've not seen such a thing. It just isn't useful IMO to take a book that talks about lex and yacc and use distinct systems to implement the end result.
Will do. Thanks.
"How much does it cost to requisition the box? How much power does it consume when you're not using it? How many platforms can you get on a single build box?"
My first reaction to this was that it is kind of sad that GHC is so slow to build we are having to resort to building it in the cloud. EDIT: It is no fair having you guys take my comment seriously.
I should note that parsing is also only a tiny part of compilers. The real work comes later, and at that point it doesn't matter how the parsing was done. Along those lines, parsec used to come with an example Tiger parser. You can still get it from the [source](http://legacy.cs.uu.nl/daan/parsec.html).
My preferred formatting for this: data MessageDir = MessageDir { upMessage :: Surface , downMessage :: Surface , leftMessage :: Surface , rightmessage :: Surface } 
Validating ghc has got a lot slower recently, since we started building everything twice (static and dynamic), and building and installing a distribution as part of validate, also generating all the docs takes a while. An ordinary development build can omit a lot of this stuff and be much quicker. 
sure, we have good development machines at work which are obviously more convenient than using EC2. This is more for when I'm working from home (remote access to machines inside MS is kind of awkward), but also I'm hoping that having easy access to VMs with plenty of different OSs will be useful for testing. Setting up an EC2 image with a new OS is easier than installing it in a VM locally, and it ends up being more accessible. 
If you check my old version of lesson 9 (improving it at the moment), you can see where I've tried to localize stateful/side-effecting code while keeping the Button class/'module' purely functional (mostly).
It's nice to read code by other type-abusers. &gt; One solution (valid Haskell98) is to stick these definitions in separate modules, and import them qualified (e.g. import qualified Host.Host as H). that's not valid H98:) it's valid H2010 though. these generic selectors (like name) could use a convention to always be defined in a module like Library.User.Selectors, then 'C' user would only import one "Selectors" module. gets hairy when one module (and corresponding selectors module) has more than one record. anyway, could you update your BlogLiterately library for 6.12? it only needs base dep bump.
oops-- `import qualified Host as H` then! But my *description* describes a solution that would be H98, even if the example wasn't! :). And, yes I will.
What do you see as special about Haskell lists? Laziness is pervasive in Haskell; it is not a per-data-structure thing.
Very cool. I first heard this whacky idea from Jón Fairbairn ca 1989. But I've never seen it implemented. 
Yeah, that really shows his ignorance. 
I'm curious as to whether it is capable of infinite length recursive tuples. You seem to imply such to be the case, but there's Data.ByteString and Data.Bytestring.Lazy. But perchance I should read further, since the difference there might be in the strictness of the indices
http://hackage.haskell.org/package/QIO-1.0
[url fixed](http://www.galois.com/blog/2010/01/29/tech-talk-an-introduction-to-the-maude-formal-tool-environment/)
Using parallel gnu make, I've built ghc from scratch in &lt; 5 mins. I think this post is more about running the validation suite, which is a bit of a different beast.
You'd get both Darcs-style cherry picking (as in works with all commands, on both hunk-level patches and collections on them), and interoperability with state based revision control, even to the point of happily working with other people who don't use the same porcelain; and the only price to pay is a sort of "event horizon" where cherry picking no longer works. Sounds great! Most cherry picking I do is on recent stuff anyway. The only difficulty may be that it's a bit hard (for me) to get an idea on how recent is recent enough, but I think that's a small problem in practice. Perhaps this is a possible future for Darcs, or at the very least, a source of inspiration. I'd like to sell the idea of the Darcs Project as a long-term umbrella for principled approaches to distributed revision control, a vehicle to support work like David's Iolaus and Ian Lynagh's [Camp](http://projects.haskell.org/camp/); or the Jean-Phillippe's [FoCAL](http://lists.osuosl.org/pipermail/darcs-users/2009-January/016988.html). In the short and medium term, I think it would be prudent to keep hacking away at Darcs, making it faster, making it slimmer, giving it a sensible library. But this would largely be about build a bridge to a hopefully not too distant future. In the long term, I just want to be using something as easy as Darcs... but what will it be? It doesn't necessarily have to look like Darcs in its present shape. There are three delicious looking futures ahead of us so far... exciting stuff! 
uhhh, what?
I tried to follow your instructions. It did not quite work for me, but now I have a running ThreadScope on OS X 10.5. This is what happened: The MacPorts stuff worked fine for me, although I was a bit wary about installing xorg and all. Should have tried with [GTK for OS X](http://www.haskell.org/haskellwiki/Gtk2Hs#Using_the_GTK.2B_OS_X_Framework) first but didn't. Then I installed the [binary package](http://www.haskell.org/ghc/download_ghc_6_12_1.html#macosxintel) of GHC 6.12 in order to be able to create eventlogs with ThreadScope. Then I wanted to install gtk2hs from source which did not work with GHC 6.12. I also could not use my old GHC 6.10 from the Haskell Platform which was gone after I installed 6.12. I still had 6.8 but that did not work either. Hence, I installed gtk2hs from MacPorts, which installed GHC 6.10 from MacPorts so now I have 6.10 again and a working gtk2hs. I then installed ThreadScope from Hackage and can start it from X11. For compiling things with 6.12 I needed to remove the MacPorts library paths from my .bash_profile because of [linking problems](http://www.mail-archive.com/haskell-cafe@haskell.org/msg69139.html) but after I did, I can now create eventlogs using 6.12 and view them in ThreadScope. Quite a hassle but finally it works. Although I wonder why the GHC 6.12 binary deleted GHC 6.10. *[edit: removed typos]*
Nothing prevents you from having your local git repository and only pushing to the public repository. Other people can then work on cleaning it up, etc. That doesn't mean you have to pull from other people's code. But it'd be nice to have a really repo with really clean and well-documented solutions, which is more work than simply solving the problems.
finished improving lesson09.
A couple of sentences of introduction as to what the author's goal was would have helped immensely with understanding this article. I think it was about making a simulation of a quantum computer in haskell, but I could be completely wrong.
This project sounds like a great and sensible idea! A concern I have is about interoperability with other porcelains or even plain git. Will this multi heads style be compatible/manageable outside of iolaus. Moreover what about branching?
You can use ST arrays of some sort instead of sieving with lists. That's likely to be faster.
This LtU thread and the paper is well worth your time: http://lambda-the-ultimate.org/node/3127
I also tried this one from haskellwiki (runs half as fast as BayerPrimes): [http://www.haskell.org/haskellwiki/Prime_numbers#Using_Immutable_Arrays](http://www.haskell.org/haskellwiki/Prime_numbers#Using_Immutable_Arrays). Is that what you mean? 
Somebody (hint, hint) should benchmark these and make a mark by the fastest one: http://haskell.org/haskellwiki/Prime_numbers
There was a long discussion on primes on the cafe recently: http://www.mail-archive.com/haskell-cafe@haskell.org/msg69376.html I didn't really follow it, though.
Interesting read, thanks. However, ONeillPrimes (also found in the [package I linked](http://www.reddit.com/r/haskell/comments/7gbz4/a_collection_of_fast_prime_number_generators_in/)) runs even slower than BayerPrimes for me. It might be a genuine sieve, but somehow it is too slow nonetheless. Ideas?
Not saying this is awesome or anything, but I think we need way more Emacs integration with automated compilation (I'm looking into FlyMake) and testing, and Haskell in general. Here's the start of what will be my contribution because it's what I need right now. 3,000 subscribers to this reddit so I figure it's a good place to throw it.
No, I mean something from the 'Using Mutable Arrays' section.
Note, needs a patch to cabal-install, http://www.haskell.org/pipermail/cabal-devel/2010-January/006030.html
And they say lazy evaluation is hard to reason about!!! [Badam Tish!](http://instantrimshot.com/).
Thanks. The first one is very slow. The second one I don't understand. What is nthPrime supposed to return? It does not return the nth prime for sure, but also not the primes below n.
I'm not sure. [Here's the prime sieve](http://www.haskell.org/haskellwiki/Shootout/Nsieve_Bits) I usually use when I need such a thing, and it needs to be fast. It was used on the shootout, although I can't find that benchmark on the shootout website anymore. It fills an array of booleans up to n. The array will have True at an index if the index is prime, and False if it is composite. I think it's got some irrelevant stuff in there to count how many primes there are up to n, as well, which the shootout required. Building a list recording all the primes you find instead shouldn't be too difficult (if that's what you want).
I am fairly new to Haskell and haven't yet looked into Monads and do-blocks. The few times I tried messing with them I had much trouble compiling anything. Thanks anyway.
Interesting how he talks about exponential growth of the complexity of the language when you add features. Is that something Haskellers should be concerned with? We see that adding type system extensions is already very hard because one has to define the interplay with all other type system extensions. And it gets harder with every one you add.
The source tree looks amazingly much like the one of plain old darcs. Only the src/Darcs/Repository directory is gone because it now uses git repositories. Technically, this is interesting. I'm going to try it out to see how this compares to plain old darcs in terms of speed and usability. Making darcs run on top of git could outsource the work on much of the functionality to a much more tested codebase. And having a git repo makes it easier to interface with the rest of the world. Socially, I keep wondering why David broke up with the darcs development team that had formed around him, and keeps surprising us with his darcs-related ideas when they're already implemented.
Did anyone try this with ghc-6.12? It depends on franchise, which I failed to build.
I thought House was dead. The link to hOp is dead, there are releases from 2006 on the site, apparently no source control at all. The latest note says: &gt; Kenny Graunke has made available a a newer version of House, ported to GHC 6.8.2 (October 2008) I'm interested in this kind of project but it looks almost hostile to contributors compared to eg. Renraku OS. Perhaps because of its foundations in academia. It seems common in Haskell that someone does something really cool and interesting as graduate work, then abandons the project to die silently, or sticks it somewhere with no documentation or reference website.
What java code are you using to generate them so quickly? It takes me a bit over 7 seconds on my laptop to generate the millionth prime in java with a simple Sieve of Erathehowever you spell it. It takes me about 1.8 seconds on the same machine to perform the same task in Haskell (using the ONeillPrimes.hs compiled with optimizations). I'ld love to add a faster way of doing it in java to my algorithms directory though.
Same thing took 3.2 seconds on the same machine with BayerPrimes.
2.9 seconds in C++ with a naive sieve. (same machine). Was 11 seconds before I recompiled with optimizations. Oh, and a bit over 16 seconds in Python. But I am using a pretty vanilla, out of the first chapter of CSCE 350 textbook sieve in Python and the C++ is really straight-forward also. (I grabbed the java one from the web, and the haskell ones are from the same package you are looking at).
Y'know, I forgot about this and was reading entirely something different when I stumbled upon a way to do the same thing in 1.3 seconds with 20 lines of scheme. That beats my 1.8 s in Haskell, but is still a far cry from 260 ms in Java (on your machine; whatever that might be).
I'm surprised the article doesn't even mention Djinn (http://lambda-the-ultimate.org/node/1178). Lennart implemented exactly the same system from Dyckhoff's 1957 paper for his Haskell term finder.
I am likewise surprised, and flattered at the suggestion that my paper dates from when I was barely at school :-). In fact it appeared in 1992, in vol 57 of the JSL.
The millionth prime is a prime below 15.5 million. I don't have the Java program at hand, but maybe I can suit you with a C++ sieve I made. You can see it [here](http://pastebin.com/f46eb69cf). If you want a bitset, it finishes in 245ms (for primes below 20 million). Then you can: * ask whether integer i is prime * run through it skipping all non-primes to iterate primes. Depending on what you do this might be more efficient than the alternative, see next. If you define GENERATE_VECTOR, the function also populates a vector with primes (it is hacky to do it but only for demo purposes.). If this is enabled, it finishes in 318ms (also for primes below 20 million). This takes longer because of the extra loop to populate the vector. Let me know how long it takes on your machine. I used g++ with O3 btw.
&gt; Setting up an EC2 image with a new OS is easier than installing it in a VM locally, and it ends up being more accessible. Good point, and that's probably due to the copious range of AMIs. Thanks for furnishing the context. 
You did your best work early, I guess. :)
Aren't you agreeing with the title then?
Thank you for providing that. Gives me something fun to compare with the other examples. 220ms (with GENERATE_VECTOR). A decent stretch ahead of my 1.8s Haskell.
How do you compile with optimizations, can you give me the whole command? I wonder why ONeilPrimes is that much slower for me than for you.
Y'know, I took for granted that in the wiki when it said ONeill's was fastest that it actually would be. Try "2.5.1 Using ST Array" on the haskellwiki Prime_numbers page someone linked earlier. I can get it to spit out 15485863 in 280ms on my main machine. Your cpp sieve runs in about 95 ms. Getting closer. I don't remember precisely what I used for the ONeill test. For the ST test just now, I used: "ghc --make -O2 -o sieveST sieveST.hs". There are a number of other weird ones though that I have seen around, but I can't say that I know what they do (and I am not using them in this test).
Oh, and the Bitwise Prime Sieve on the same page looks fast. I'm too much of a beginner to understand it. I can compile it and spit out 50847534 in about 3 ms! (which is not a really big prime). It is actually "The correct number of primes less than 10^9." (according to the Prime Curios site - http://primes.utm.edu/curios/page.php/50847534.html). So apparently I am asking for the number of primes less than 1000000000 when using that function... It definitely spits it out fast but is not doing the same thing. Could it be modified? Beats me. I don't understand STUArray's and unsafeRead's and what not. My Haskell expertise at the moment lies somewhere between finishing the first chapter of "Haskell Road to Logic, Maths and Programming" and finishing most of LearnYouAHaskell.
Oh God, thank you! -O2 really did the trick. For the last prime below 20 mio it took 17s before, now 0.4s. This optimization should be turned on by default! By the way, you can edit your comments.
Sweet! Glad that helped.
If a lazy version of 2.5.1 existed it would be perfect. So you could do something like take 50 primes 
The emailed version had unicode issues, this one did not. Ah the fickle nature of text encoding... 
I implemented Gentzen system for SL, but then got bored. If you want to have fun you should implement resolution and unification for FOL. The problem is that it is already done.
It makes me sad that algorithms written like that can probably only scale if re-written less elegantly to operate on mmap'd or explicitly read/written files, due to the dichotomy between large (file system) memory and small (virtual) memory. We have a big, slow, non-volatile memory, and a small, fast, volatile memory. The OS really knows better than the applications which parts of memory are used, it should be moving things around to optimize memory access. I am hopeful that a library like Iteratee or other could allow composition of algorithms that are completely agnostic to the storage medium of their inputs and outputs...
Sorry, Roy, I should have caught this as not making sense while typing. :-) BTW, I mentioned other work built on this stuff of yours last March. It now appears at FLOPS: http://www.iai.uni-bonn.de/~jv/paper.pdf. 
I'm sorry:) it wasn't worth making such a fuss, no one cares anyway.
anyone else REALLY like this topics picture? Look at that icon; that's a great icon!
&gt; I'm interested in this kind of project but it looks almost hostile to contributors compared to eg. Renraku OS House is used as a research tool at Portland State - you seem to be expecting a full open source development community, but there is only one person paying active attention to House (Kenny). IOW, I'd consider this an issue with expectations - you expected or assumed this was something it never intended to become.
mmaping a file really isn't any different that allocating a big-ass array in virtual memory, and there is a whole category of so-called "cache oblivious" algorithms that are both elegant and have good scaling behavior once you move beyond physical memory. (Although I don't know if sparse matrix multiplication falls into that category. Probably scales better using message passing in a cluster, rather than an out-of-core streaming-from-disk solution.) For example, here's one for computing an approximate k-nearest-neighbor graph that uses a morton/z-order layout to achieve good locality. http://sites.google.com/a/compgeom.com/stann/ In the related paper http://compgeom.com/%7Epiyush/papers/tvcg_stann.pdf they claim good performance in just such a scenario where you simply allocate an array that is bigger than your physical memory and let the OS deal with mmapping.
There is no xmonad-extras package in arch at the moment. I am sure it would not be too difficult for me to figure out how to create one, but just curious if there is a reason it does not exist (maybe it is included in some other package I am not aware of?)
You looking for xmonad-contrib ?
http://hackage.haskell.org/package/xmonad-extras It is not included in contrib I don't think. I just find Actions.Volume useful. 
Oh, I need to add that to AUR.
&gt; mmaping a file really isn't any different that allocating a big-ass array in virtual memory Technically, virtual memory is usually bound by the size of physical memory + swap space size, whereas mmap'd memory is usually bound by the minimum of the address space size and file system size. In either case, can we have our Haskell heap operate within that space?
Thanks dons!
Yes well obviously you have to increase the size of your swapfile to accommodate your data, which is a million times easier and less error-prone than writing your own mmapped-based custom memory allocator. The only downside is that this is not a friendly thing to do in a multitasking environment, but who calculates page ranks in the background anyway. 
&gt; you expected or assumed this was something it never intended to become. True, but it only reinforces what I said: many things end up getting shelved or forgotten once the (usually lone) author finishes his research project. I do understand that starting a real open source project doesn't fit in the schedule when you're writing a thesis, so I'm not blaming the author :) I just hope this grows eventually, because a Haskell OS would certainly be nicer than a pile of duct-tape like Linux. (Oh, certainly a decent pile with a lot of work into it... but still a fairly fragile one, with a lot of sharp corners).
I am slightly enthusiastic about &gt;&gt; JSON-based webservice interface for Hayoo as it would put this DB somewhere in the near of mnesia / couchdb and with the recent addition of efficient polling into GHC I envision some very fascinating matches / benchmarks!
I'd like to stress that's it's very important to come up with good projects. Projects that improve the Haskell tool chain.
Hehe, it's fun to correct myself... plus -- as mentioned -- my penance was to reread the post, which was awesome. Seriously, any opportunity to use unicode, I'll take. English can be a painfully simple language to type...
&gt; It is correct to say that we can reverse a list of bananas, but it is incorrect to say that the reverse function is in some way related to bananas. Reversing a list of bananas is merely a single instance of an infinite possibility. For precisely this reason, it is also incorrect to say that monads are in some way related to I/O. Nicely put! Composability wins through *independence* of features. The Haskell ecosystem is more powerful thanks to monad *not* being about I/O. &gt; In Java 1.5, List is not a type, because it requires a type variable [2]. Replace "type variable" with "type parameter" (or "type argument"). Similarly for: &gt; HashMap is a type constructor which is kinded (*,*) -&gt; * since it takes two type variables to reveal a type. I guess (unsure) you're mixing up formal &amp; actual parameters. &gt; We can also generalise applicative functors, comonads, arrows, covariant and contravariant functors, binary covariant functors, structure traversal and many other concepts that are well documented in the literature. &gt; Monads get far more attention than they deserve! Amen, and thank you! 
Thanks Conal. With regard to type variable versus type argument, I am just being sloppy with terminology. I will amend accordingly, thanks.
If we assume that hackage forms a directed acyclic graph then presumably there is a simpler algorithm that does not require iterating to a fixpoint.
It can be hard for people outside of Academia to know who has done what before.
Great stuff. This instantly makes monad transformers a lot more attractive (to me at least).
Even with cyclic graphs, PageRank can be seen as an eigenvector of the adjacency matrix of the graph. There are fast algorithms on sparse matrices so it should be fairly straightforward to compute quickly without explicit iteration to a fixed point.
Another smashing post by sigfpe. :-)
automatically resolve import statements in emacs (like ctrl-m in eclipse) put your cursor over a function call, press ctrl-m (or whatever) and the proper import is added to the top of the file. if there are conflicts, the user is present with a list of possible packages to satisfy the request, or a message in the minibuffer saying none were found
Excellent idea. I'm looking forward to trying this out.
Neat hack! However, why not label them by creating named lifters? Instead of: data A = A data B = B data C = C data D = D TStateT A Int (TStateT B Int (TStateT C Int (TWriterT D String Identity))) Int why not use: newtype MyT m a = MyT { runMyT :: StateT Int (StateT Int (StateT Int (WriterT String m))) a } deriving (Monad, MonadTrans, MonadIO) liftA = MyT liftB = liftA . lift liftC = liftB . lift liftD = liftC . lift ? EDIT: Oops, I don't think MonadTrans is what you want to derive there, but rather you want a separate instance: instance MonadTrans MyT where lift = liftD . lift
Because if you change the order of the monad transformers you now have to change in two places: the type definition and the lift definitions. 
It's hard for people in academia too. :)
Well, that's really "one place", since they are intentionally kept close to each other. Also, type checking will help me here. Similarly, if I want to add a new layer to the former definition, I need to add in two places ("data E = E"). 
From what what I understand (and what I've written) you only need explicit use of lifts if you have stack of monad transformers which are the same (like a stack of StateT in the example). I'm having similar issues of excessive lifts with liftIO because of binding libraries that have actions in IO monad. So the best thing I could come up with is either I compose all my IO actions into one and lift the single composed one or if I want to try and preserve a code translation from another imperative language I use a lambda with liftIO: foo :: SomeMonadTransformerStackWithIO () foo = do ... arg1 &lt;- get/ask arg1 &lt;- get/ask ... liftIO $ do foo &lt;- ioAction1 arg1 arg2 bar &lt;- ioAction2 arg3 ioAction3 ... ... Does anyone else have a better idea? maybe I can use this tagging trick?
Perhaps someone who understands PageRank better than me can explain how it is different as a ranking than simply summing counts of transitive dependencies. That's what I've been planning to do for the hackage rank (in some combination with download stats). Would PageRank be better? How? Why?
How about throwing some resources at EclipseFP? Things like code completion, resolving imports (like dmead's suggestions for emacs), or being able to browse through functions and datatypes defined in a script from the sidepane would be extremely useful.
PageRank doesn't make any sense for an acyclic graph. It's a bad metaphor. If package A depends on package B, the probability that I go from A to B is always 1, and if A depends on both B and C, then the probability of A-&gt;B is 1 and A-&gt;C is also 1 and so the so transition matrix isn't even a stochastic matrix. The "random surfer" can't be in two packages at once. (Quantum computing notwithstanding.)
you want to use monads. monad transformers are tools that make creating custom monads easier. you shouldn't use lifts in your code (or lift.lift.lifts), you should newtype your monad stack, derive instances that are derivable (so no MonadState if you have two StateT composed), and provide new, primitive operations for "overlapping" methods, that can be easily distuinguished (so no "get", "lift get", "lift $ lift get"), like getCounter, getSomeOtherThing.
Interesting but what about liftIO? what if you're dealing with binding libraries you can't change and is not feasible to have lifted/generic versions for every function.
&gt; what about liftIO? `liftIO` can be avoided in many cases, but even when not, there's always MonadPrompt. :) &gt; what if you're dealing with binding libraries you can't change and is not feasible to have lifted/generic versions for every function This isn't about lifting things. This is about making good abstractions. If you are merely lifting things from one monad to another, you probably have an abstraction leak.
liftIO is for MonadIO just like get for MonadState. but liftIO has to be parametrized over things, because IO monad (in contrast to every other monad) has infinite number of "methods".
&gt; This isn't about lifting things. This is about making good abstractions. If you are merely lifting things from one monad to another, you probably have an abstraction leak. I'm talking about liftIO specifically. Have a read what I wrote in seperate comment here. Should be more clear what I mean.
Thanks, I'll stick to the simpler method for calculating a hackage rank then.
However the point of safe-lazy-io is to solve this particular issue as well. At the cost of restricting the inputs interleaving by fixed schemes that does not depends on the strictness of functions used while retaining some lazyness.
oh, it's just for playing. it's true that experimenting has more relaxed rules - even bottoms are ok. those tagging methods are only needed if you have couple of the same transformers in the stack, why not use types to label layers? unless you have two StateT Int, in that case, using pair of ints and some arrow goodies will be better. if you have three StateT Ints, then you're doing it wrong, even if it's just prototyping.
Can you post this to the list? 8-) You should be safe if you're processing only one stream lazily at a time, but I had the impression that safe-lazy-io wasn't that restrictive.
Ok, if refinement types are suddenly practical, how can I use them in Haskell?
I disagree. it's about overloading - you think that + should be named differently for different types - plusInt plusDouble. or using that trick/hack - (plus Int) 2 2, (plus Double) 2 2. I do like names - I'm not saying that all the code with this polymorphic get should be pointless - you still say "counter &lt;- get". anyway, I do like this trick - it may be used for some other things. oh, and I'm stupid so you're probably right:) edit: &gt; You could argue that the arguments to + ought to be given distinct types and that you don't need names because you could use the type to disambiguate. what? I still have to apply + to two things 
unfortunately, just because something is practical, it doesn't get magically added to the next version of ghc. I've seen a paper about refinement types, maybe it was based on haskell (or some ML, I can't remember). The problem was, that type sigs were getting exponentially huge.
No mention of dependent types?
Extend the Haskell FFI with native support for binding to Objective-C code, along the lines of http://hackage.haskell.org/trac/ghc/wiki/ObjectiveC
Benjamin Pierce is a dead ringer for Rowan Atkinson!
What are refinement types?
Thanks for the post. I nearly had a heart attack when I saw the sudden spike in hits to the blog.
Try to follow up on the work on a LLVM backend. Get the needed changes into LLVM.
One of the things I've been noticing as the Haskell community continues to solidify its knowledge about how to teach monads is an increasing emphasis on the structural properties of monadic code, which I think is really neat, and having originally been a schemer, is quite a validation of "code is data".
[more comments](http://www.reddit.com/r/haskell/comments/au7h6/the_monadreader_issue_15_pdf/) in a previous discussion
Well, some of us better avoid success at all costs...
As I think I said on another post, this will also go a long ways towards demystifying monads to those outside the Haskell fold. (Pun half intended; I didn't do it on purpose but I'm leaving it there.) This sort of Monad tutorial makes the programmer in me leap up and say "Hey, this is awesome, why don't all languages do this? I gotta learn this!" whereas burrito metaphors and category theory is intellectually interesting to me but doesn't have that sort of compellingness to it.
i wrote something similar a few years ago http://taz.cs.wcupa.edu/~dmead/code/prover/Prop/
Interesting series and nice explanations. I enjoyed the last bits :) &gt; It is very probable that 4 kids playing the Game of the Goose will have a fight
I thought by the title this was a post about [KiCS](http://hackage.haskell.org/package/KiCS-0.9.2).
I thought this by the first part of the title, too, but the second really clears this up.
By the way, concerning the algebraic structures, I think there's something funny going on with the state monad. I wanted to comment on that when you posted [Where do monads come from?](http://blog.sigfpe.com/2009/12/where-do-monads-come-from.html). So, using the terminology from my tutorial, we have two primitive operations `Put` and `Get` and the state monad is `Program (StateI s)`. In your post, you argued that thanks to the obvious laws about `get` and `put`, this type can be *optimized* to `s -&gt; (a,s)`. In other words, we have two homomorphisms canonical :: Program (StateI s) a -&gt; (s -&gt; (a,s)) canonical (Return a) = return s canonical (Put s `Then` is) = put s &gt;&gt;= canonical is canonical (Get `Then` is) = get &gt;&gt;= canonical is inject :: (s -&gt; (a,s)) -&gt; Program (StateI s) a inject f = Get `Then` \s -&gt; let (a,s') = f s in Put s' `Then` \_ -&gt; Return a that are inverse to each other, i.e. inject . canonical ~ id canonical . inject ~ id at least with respect to the appropriate interpreter on `Program (StateI s)`. So far so good. But you also claimed, or at least that's how I read your post, that the optimized type can be *derived* in some principled fashion just from the two laws put s &gt;&gt;= get = return s get &gt;&gt;= put = return () -- EDIT: corrected law This would be very interesting! But unfortunately, I don't see how. :-D 
Okay, since there's [edit: going to be] support for Haskell already I can almost guarantee I'll give this a shot.
That's just an article about it on geek.com. The actual contest page is [here](http://csclub.uwaterloo.ca/contest/index.php).
Curious, what does this have to do with Google?
I'll give it a shot when they provide the Haskell kit :) It's nice and it fits very well with my plans for this month.
Ah, by *derived* I meant to do it on paper, it's not feasible to do this automatically. Still, I somehow got the impression that there should be a way to start with the laws and calculate the `s -&gt; (a,s)` from that on paper. The current situation is that you have to make the ansatz get &gt;&gt;= \\s -&gt; let (a,s') = f s in put s' &gt;&gt; return a first and then prove that it works by giving an isomorphism. But what I have in mind that you start with an arbitrary term and systematically rewrite it with the laws, which *a posteriori* leads you to the above normal form. Something like a Groebner basis for monads.
It has a great [README](http://github.com/droundy/iolaus/blob/master/README.md).
They currently don't have compile support for Haskell on their server, but they said they would add it soon. Also, there is no official starter package yet. I've quickly put one together to start playing with it if anyone is interested: http://github.com/ozataman/tronhs
This is the standard composite operation and it is effectively the same monoid operation available for [Data.Colour.AlphaColour](http://hackage.haskell.org/packages/archive/colour/2.3.1/doc/html/Data-Colour.html#t%3AAlphaColour), except that I store colour data premultiplied thus avoiding the division.
What Luke seems to have done here is derived the monoid operation from a monoid action. Luke already knows how an AlphaColour acts on a Colour (say (@) :: AlphaColour -&gt; Colour -&gt; Colour) and wants to find the monoid operation (say (&amp;) :: AlphaColour -&gt; AlphaColour -&gt; AlphaColour) so that this known action will be a monoid action. In otherwords he is taking the monoid action law (x &amp; y) @ z = x @ (y @ z) and solving for (&amp;). This is neat, because I've never solved an algebraic equation for an operation before.
omg my *eyes* aaugh!!!
The download page now has a Haskell starter package. http://csclub.uwaterloo.ca/contest/starter_packages.php
It is not that restrictive you can manage as many input stream as you want. Deeply change each of them independently. However you have fuse the results with yhe combinators provided which ensure either a strict extraction or a fixed combination.
Premultiplied alpha is pretty much the way to go.
there's a bug in the first axiom.
&gt; I'm curious as to whether it is capable of infinite length recursive tuples. What type signature would such tuples have?
if you mess up the axioms, everything "compiles":)
it's apply (f a) is the same as f $ a
Larger example: a b $ c d e $ f g = a b (c d e $ f g) = a b (c d e (f g))
That's a good example. I read RWH a few months ago so I've since learned how to use $. I really just wanted to mention this, in case they choose to address this in future editions. Note also that "apply" is not in the RWH index, but "applicative functors" is, and that is where the $ type is finally given on page 398. 
In the online version of the book, the operator is first introduced on this page: http://book.realworldhaskell.org/read/using-typeclasses.html Several readers/reviewers already posted comments raising the same issue, but suggested clarifications didn't make it into the print version. Nevertheless, the authors are already likely aware of this criticism, so if there is a second edition, it may be addressed.
&gt; a b $ c d e $ f g = a b (c d e $ f g) = a b (c d e (f g)) = a b . c d e $ f g = a b . c d e . f $ g 
Woops, and dating back to 2008 too. As long as they're aware of it. Thanks.
You're probably better off reading "learn you a Haskell for great good!" or one of the other basic Haskell tutorials/books before taking on Real World Haskell. RWH tries to introduce Haskell and how to apply it in a real world context, but tends to favor the latter in most contexts. Its organization can create a lot of confusion for a total noob. It's really a second or third book in the language for an intermediate programmer. 
Agreed. I read RWH first up to about chapter 14, and then put it down and read School of Expression and I finally *got* it. I would definitely recommend starting with SoE or LYAH. Reading the last half of RWH is so much easier now.
http://csclub.uwaterloo.ca/contest/language_profile.php?lang=Haskell Hey, we could use some help here representing our favorite programming language ;-)
&gt; if you mess up the axioms Such as by [letting 5 = 6](http://www.reddit.com/r/programming/comments/9q6a8/let_5_6_is_valid_haskell_but_its_not_as_bad_as_it)? :-)
Looks legal to me! Prelude&gt; let 5 = 6 in 42 42 It's just very hard to use the result of that binding... =)
We should really try to integrate Hayoo! with the new hackage server. Perhaps a project for the next hackathon. The other alternative of course is Hoogle.
You need to use some suitable implementation of FORTRAN so you can change constants.
Their Haskell starting package is very inefficient. I had to rewrite it almost from scratch to get some performance - I'd share the code if anyone here is interested?
Yes, please do. Are you reading the maze to immutable 2D arrays ? EDIT: I can see you switched from Java to Haskell on the weekend. Nice move ;-)
[Here you go](http://jaspervdj.be/posts/2010-02-08-tron-ai-arrays.html)
Thanks a lot ! Good bye (tronMap !! y) !! x
Are they homogenous? The only traits noted in the explanation are that lists are variable lengthed, while tuples are memory efficient. There's more differences than that between tuples and lists
Lists are homogenous: `[a]`. Tuples are heterogenous and fixed: `(a, b, e, a)`. The fixedness of tuples means you can't have 'infinite length recursive tuples' in any sense I know.
"Learn you a Haskell is a great resource" too bad the more advanced chapters have not been released yet. It's been a while.
The latest released version of gtk2hs doesn't compile using GHC 6.12. Compiling ThreadScope with 6.10 and the program you want to generate and event log for with GHC 6.12 works.
Oleg's index-based implementation of nested `break` and `continue` has dynamic scope. It is also easy enough to implement nested `break` and `continue` with named lexical scope: - Augment `ExT` with unique supply semantics, so you can generate unique identifiers on demand. - Add a variant of `for` which in addition to the current list element also passes to the iterator function a `Label` value which is generated from the unique supply once per loop. - Add variants of `break` and `continue` that take a `Label` argument. - Modify the cascading `catch` handlers to check for equality between the loop's and the exception's `Label` values. If equal, they are handled as in Oleg's code. Otherwise the exception is re-raised. The use of the unique supply is a little inelegant. It helps to implement lexical scope on a dynamically scoped substrate (i.e. exceptions). Haskell's lexical scope is used for matching up producers and consumers of `Labels` in a way reminiscent of higher-order abstract syntax.