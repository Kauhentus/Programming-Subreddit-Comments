We're actively exploring alternative designs for `ad 4` that should permit monomorphic AD modes.
My current HMC implementation in Haskell is effectively using automatic differentiation inside an EDSL that describes programs to run on a GPU. This is in essence using AD to implement symbolic differentiation, by taking an automatic derivative of a symbolic representation. If you do that in a HOAS setting with observable application a la Nikola you can recover the loop structure, and do all the usual compiler tricks.
Nice diagram. Thank you for posting that.
Ah, I misunderstood you. I meant a naively generated function that makes no effort to share terms. When I generated such a function, it was quite a lot faster than using *ad*. 
If I might ask, why not just parse with the irc library to begin with? I'm the maintainer, so let me know if you need an API change, it's largely been stagnant for the last few years :)
This sounds very cool. Maybe I'll give that a go again. Is there any available code for doing this kind of gradient differentiation? 
Sadly, the work I did in this space has been licensed, and I'm not free to open source it at this time. =/
I will take a look at the repo later today.
You're the maintainer of `simpleirc` or of `irc`? Either way, I like the bot-shell with the event interface that `simpleirc` offers, and wouldn't want to write it myself. It has enough features to get basic things done quickly, and it's relatively simple to put additional features on top of. If it would use `irc` for its parser and message format, I'd be a happy camper. My main complaint about `ircserver` arose when I wanted to use some WHOIS information, in which case `parse` and the `IrcMessage` datatype don't make sense. For example, a 311 (RPL_WHOISUSER) reply: &gt; parse ":server.irc.com 311 mynick theirnick theiruser theirhost * :realname mcrealname" IrcMessage { mNick = Just "mynick" , mUser = Nothing , mHost = Nothing , mServer = Just "server.irc.com" , mCode = "311" , mMsg = "theiruser theirhost * :realname mc realname" , mChan = Just "theirnick" , mOrigin = Just "mynick" , mOther = Just ["theiruser","theirhost","*",":realname mc realname"] , mRaw = ":server.irc.com 311 mynick theirnick user host * :realname mcrealname" } Of course, all the info is present, but I couldn't figure out where to find it without first printing the result IrcMessage.
I'm a biologist who's into Haskell too. I only had a rough idea of what you were up to in grad school - seeing these posts now, this project is really awesome. I'd imagine that people reading this thread think of this a a very cool thing, but not quite as useful as (for example) Pipes itself. But I'd venture a guess that being able to search on motifs in protein structures is HUGE for all the structural bio people searching for receptors' binding partners, or trying to back out heuristics in the folding problem, etc. I think Suns might end up with more direct users than Pipes? (at least until worldwide Haskell adoption) There are some serious barriers for neuroscience folks who want to try to make progress with anything other than matlab. Suns is such a great example of a fully-fledged tool in haskell, and one that wouldn't have been engineered as well if it were carried through in c. You are a great role-model for folks like me: scientists who desperately want to not be crappy programmers, and who also want to move science forward! Thanks for this work. :)
I'm the maintainer of irc. I see now the difference, thanks for the reply :)
&gt; This means that Haxl violates the Applicative/Monad laws, and rewriting potentially changes semantics, so no rule should be applied. Exactly, so your proposal does not solve the problem. =]
Isn't this work 3 years old and largely superceded by [CHSC](http://www.cl.cam.ac.uk/techreports/UCAM-CL-TR-835.html)?
Sure. The other way is use trick, which ArrayList from Java uses - doubles it's reserved space when list is near limit. BTW, done some benchmarking on topic: https://gist.github.com/danbst/6983872 Interstingly, pipes beat naive recursion! And pipes with lazy bytestring builder beat recursion with builder (but just a little). Do you have any opinion on this?
For whatever reason, reading the local filesystem isn't made asynchronous. This is the result of that.
Warning, ad 4 in its current state is pretty broken. We went down one path that didn't turn out to let us write many of the combinators we could write before.
No worries :)
I get the error **hClose: resource vanished (Broken pipe)** many times one relatively big packages. What the problem may be?
Awesome, that'll be great to read through tonight once I've moved house :P. I'm more inclined to compare the apparent scalability of each method rather than raw speed for a large test case. I.e. test problem sizes of `[10 ^ n | n &lt;- [3 .. 7]]` or something. Which I will do at some point.
Also, note that I could simply use the 'experimental interface with standard handles'... from [Hackage](http://hackage.haskell.org/package/serialport-0.4.5/docs/System-Hardware-Serialport.html): h &lt;- hOpenSerial port defaultSerialSettings line &lt;- hGetLine h I might just do that. But trying to figure this out has been an excellent learning experience.
&gt; Lens defines 121 operators. 100+ of them follow a consistent schema. You don't need to use them. The problem is that even if I don't use it, other people will, and I will encounter their code. Not just that, these other people will come up with idioms made up of operators that I don't use. At that point if you're reading the code your best hope is that the code does exactly what the types say...
Yeah I see your point, but I do think you were warned this would happen by the documentation and do not need to understand the internals to see why this behaviour is expectable. Namely in [Control.Concurrent.threadDelay](http://hackage.haskell.org/package/base-4.6.0.1/docs/Control-Concurrent.html#v:threadDelay) &gt; There is no guarantee that the thread will be rescheduled promptly when the delay has expired, but the thread will never continue to run earlier than specified. When a "threadDelay" resumes the only guarantee is it is after the desired amount of time. If you wanted a status every X intervals, "threadDelay" is probably the wrong tool for the job. Edit: Also I'm not sure it would be desirable to change the ByteString.readFile to be chunked, as it will be slower when running without concurrency.
Do you have benchmarks that prove the new implementation has a measurable performance gain? 
I like the proposal! It provides a nice interface for incrementally assembling JSON ouput upon which one could build additional abstractions. One thing that I will say: aeson's Value type seems to be general enough that other libraries are using it (yaml comes to mind). With that in mind, it would be nice to decouple aeson's intermediate representation of objects from its encoding of them for the sake of having a way of encoding data into different formats! If I only needed to write a single type class instance that could encode into JSON, XML, msgpack, or YAML, that would be an incredibly nice time-saver.
Love the new API, except that you are supposed to write 'mixed' for every element in a mixed array. I would be very happy if I only had to type that once. Instead there could be a different combinator for an append that mixes types that would also be used for Object &lt;,&gt; EDIT: Actually I don't ever really have mixed type arrays. It is possible to see arrays being used as 2 or 3 item tuples though
You have to use 'mixed' only when the type of the elements in an array differ, which is probably not that often.
You're making a faulty argument. (A) If the people modifying your code embrace proprietary licenses, you also don't get to use their changes. So this is nothing special about the GPL. And by choosing BSD/MIT-like licenses, you do nothing to deny them this option. (B) Whereas, if you want to ensure that you get access to others' modifications of your code, then *you* should embrace the GPL. If you were to release your own code under the GPL, then others would be forced to release their changes back to you. If what you want is to gain access to the changes other people make to your code, then the viral nature of the GPL is exactly what you want! You can't have it both ways.
As I said many times, my problem is that the GPL community (which is a fucking open-source community after all), is directly hostile (because of how GPL works) to the say, BSD community (again an *open-source community*, with the same fucking goals). I also have some other problems with GPL, but I don't think there is any misunderstanding here. My opinion just seems to be different from most people here (a bit surprising, honestly, I really think what I'm saying makes perfect sense, sociologically. Of course legally it is true what all of you is saying about GPL. And I'm arguing that this is bad for humanity).
&gt; The problem with the encoding method currently used by aeson is that it occurs via a translation to the Value type. While this is simple and uniform, it involves a large amount of intermediate work that is essentially wasted. When encoding a complex value, the Value that we build up is expensive, and it will become garbage immediately. Might adding rewrite rules to perform fusion help?
I didn't think my example was particularly pathological. I assumed (possibly incorrectly) that the symbolic differential of a neural net would exhibit similar complexity. Moreover it's not clear how one would symbolically differentiate e.g. a finite difference solution to a PDE (on my to do list). I.e. I think there are cases where it would be very difficult to use symbolic differentiation. Can you point me at a specific example of a data analysis problem where symbolic beats automatic? HMC is also on my to do list :-)
Simon Meier has been working on a new serialization scheme for the `binary` package at ZuriHac at Euridify. I don't exactly remember, but I think he tried to solve a similar problem. He used some simple intermediate data type that greatly sped up the bytestring building process. Maybe he could help.
The representation that AD packages use is essentially a sparse vector of derivatives. When you have an expression where those vectors are dense there can be a lot of bookkeeping overhead; so much that it can be 10x slower or more. Another case where there can be a lot of overhead is if you have an expression that would benefit from backwards AD but you're using forwards AD or vice versa. For example if you have `f(g(h(a+b+c+d+e+f)))` then the AD package will essentially construct a vector of 6 elements for the derivatives with respect to a,b,c,d,e,f and then multiply each of those 6 elements repeatedly by the derivative terms for f,g,h, whereas it would be more efficient to do it backwards: first multiply together the derivative terms for f,g,h, and then multiply each of the 6 elements by that quantity *once*. Instead of doing `3*n` multiplications you have to do only `n+2` multiplications.
I've used Aeson a bunch, and mostly not in the ways that the library authors prioritize (they are generally focused on using it from Aeson to Aeson, I mostly use it to interface with non-Haskell systems). My initial impression is that this API definitely feels more confusing to program with. If it's the bullet you have to bite to get better performance then it might be worth it, but if it weren't for that I would definitely be against the change. The existing implementation is a straightforward replication of JSON's type system, while this one privileges Haskell's notions of types, which is not preferable for me.
Not only should the Object type probably have a phantom type, but a single keyvalue pair should have a different type from the final object, so you can tell in the types if the braces have been added.
I suppose his point is, is it worth discussing this at all if using `Builder`s directly instead of the intermediate `Value` isn't a real performance gain? I'd expect it is, and that Bryan knows it to be the case, but I share the preference to always present design decisions about performance with actual data. OTOH, maybe the performance depends on the particulars of the API. Chicken and egg.
Here's a thought: I don't think the type of the _count field of Builder needs the full power of Int, it really only needs a Zero | One | Many... defining this could avoid potential Int-overflow situations in some (admittedly ridiculous) scenarios to do with the monoid instance. data CountNum = Zero | One | Many deriving (Eq, Ord) instance Monoid CountNum where mempty = Zero mappend Zero n = n mappend n Zero = n mappend _ _ = Many or something 
Actually I think it just needs Bool, because the Monoid instance for Build a as given is wrong: It should insert commas when *both* of the *parts* are not empty, not when the combination is longer than 1. The given one breaks for appending mempty's to things with already more than 1 element.
&gt; One thing that I will say: aeson's Value type seems to be general enough that other libraries are using it I'll add a +1 to this comment. I have found the Value type quite useful even in situations where I'm not working with JSON.
In other words, even though `view` distributes like this: view (foo . bar) == view bar . view foo which looks backwards, `over` distributes like this: over (foo . bar) == over foo . over bar which looks normal, especially since `over foo` has the same sort of feel as `fmap`, as you mentioned. I suspect most people new to lenses have a `view`-centric understanding of lenses, because `view` is usually the first example presented in many lens tutorials. Even when lenses are described off-hand by people, they are usually explained in relation to field accessors, because the setter aspect to lenses does not have as familiar of a correspondence in Haskell-sans-lenses Your `map`s example should be a standard answer to people who ask about the "backwards" composition. The example addresses (one of) the actual inferential gap(s) between people with a `view`-centric understanding of lenses (e.g. me a few days ago) and people with a somewhat broader understanding of lenses. On the other hand, the "it looks like a mainstream OO language" explanation is a coincidental benefit (?) of the composition order, not a justification for it. 
An immutable history alone doesn't give you the information when a certain part of that history was current. Hackage actually does include all the old package versions (unlike e.g. many Linux package manager repositories), it even includes the upload dates, what is lacking is mostly the ability to filter the hackage cabal sees by upload date.
But it _isn't_ the function that blocks all cores! It's the garbage collection that does it! So its not just ByteString, anything that A) triggers a large alloc and then B) takes a large chunk of time will have this behavior. Nor does this have anything to do with the FFI -- that's a red herring. Its just what happens with a stop-the-world garbage collector, sorry :-(. It is however a good example of a "gotcha" with such a collector, and could perhaps go in the _GHC_ documentation. Chunked readfile by the way is similar the standard readfile you get with `Data.ByteString.Lazy`, not to mention the lazyIO readfile in base. (Although of course if you want "strict" chunked behavior you first do the lazy read then force the whole spine in one fell swoop).
Mixed seems like a possibly reasonable place for existential type erasure—which then makes me wonder if there's a way to encode that more directly.
I have a couple of remarks. 1. The efficient implementation for encoding can be derived mechanically from the existing `Data.Aeson.Encode.fromValue` function. The algebraic data type is replaced by the type that we want to map into, and constructors like `Array`, `Bool`, etc. are replaced by functions `array`, `bool`, etc. Here an example: type Value = Builder bool :: Bool -&gt; Value bool b = if b then "true" else "false" array :: [Value] -&gt; Value array [] = "[]" array xs = "[" &lt;&gt; mconcat (intersperse "," xs) &lt;&gt; "]" In other words, this is a standard exercise in "fusion". The only remaining trouble is to decide which "constructors" to offer for arrays and objects. In the example above, I have used the standard list type, but if you want to avoid intermediate list constructors, you can again use a different representation for lists: type List = Builder nil :: List nil = "" cons :: Value -&gt; List -&gt; List cons x xs = x &lt;&gt; "," &lt;&gt; xs array :: List -&gt; Value array x = "[" &lt;&gt; x &lt;&gt; "]" Note that the `Mixed` type is wholly unnecessary, everything is a mirror of the already existing collection of types. (For simplicity, I have written the code using type synonyms. These should be implemented as `newtype`, of course.) 2. A more interesting question is: how would this interact with the existing algebraic data type `Value`? Is it possible to use both approaches in the same library? I think the answer is "yes" if we use polymorphism. The idea is to use a phantom index for the `Value` that keeps track of the representation. data Value rep where ADT :: Data.Aeson.Value -&gt; Value ADT Builder :: Builder -&gt; Value Builder class R rep where bool :: Bool -&gt; Value rep ... instance R ADT where bool = ADT $ Data.Aeson.Bool instance R Builder where bool b = Builder $ if b then "true" else "false" Depending on the requested type `rep`, the function `bool` will return either the representation as an algebraic data type or the "bare metal" version. In essence, we are using a type argument to choose between representations. The nice thing is that type arguments can be inferred. Since the types are evaluated at compile time, there is good hope that we can convince GHC to inline the application of type arguments. 
Hmm, in that case could we scrap _count altogether and just have the builder in a Maybe?
I would like this, but a couple of points: 1. You haven't addressed how this deals with records that are subtypes of each other. A simple solution is not to have any subtypes at all, I suppose, but that might get mighty inconvenient. 2. The "." syntax is overloaded quite a bit already, making it potentially a bad choice for record fields. 3. This doesn't interfere with most existing haskell features, but neither does [this](http://ghc.haskell.org/trac/ghc/wiki/Records/OverloadedRecordFields/Plan) proposal, which, unlike yours, is actually being implemented right now. 
Data.ByteString.readFile cannot be chunked because it returns a single strict ByteString. So sure it could read in chunks but then it'd still have to concat (i.e. copy) into a single one, so you're still at some point going to have to allocate one huge buffer. It's better to alloc that once and then read directly into it.
http://www.jonmsterling.com/posts/2013-04-06-vinyl-modern-records-for-haskell.html
The overloading is such that a field accessor that knows how to get a field of type 'a' out of a record of type 's' looks like `p s a`. When `p` = `(-&gt;)` you get a function. When `p` is a `Lens` from `data-lens` you can use the field accessor directly as such a lens. Similarly for `fclabels`, `accessors`, etc. However, a `Lens` in the `lens` package sense doesn't look like that. It is just a type-alias. Moreover, it also composes in the opposite direction. The solution is making a combinator that takes a `lens`-style Lens out of a newtype wrapper that fits the right format. We haven't picked a name but for sake of discussion we repurposed `lens` then you'd have view (lens foo . lens bar) = bar.foo This deals with the reverse composition and the newtype unwrapping. You also wind up needing to use such a wrapper if you go to use one as an fclabels polymorphically updatable label modulo the backwards composition.
Well, I would: 1. Make a datatype for the genes. It should be a normal ADT, and probably you would make bigger genes from small ones. 2. Choose a genetic programming library: this would provide the basic machinery of "making a population of individuals, mutate, cross and select according to a fitness function". I played with `genprog`. (You could also handroll your own, but I would recommend a library). 3. You choose how you want to display that evolution: in that example it seems like they just display each generation. 4. You choose a graphics package, and use it to represent the evolution graphically: you'll need to know how to display individual genes first. `gloss` is nice for beginners. So in your code you would have things like: data Gene = Gene -- put an actual definition here displayGene :: Gene -&gt; PositionOnScreen -&gt; Picture -- as far as I remember, Picture is an actual gloss datatype kickstart :: SomeRandomGenerator -&gt; (SomeRandomGenerator, Population) -- you could also use State advance :: Population -&gt; SomeRandomGenerator -&gt; (SomeRandomGenerator, Population) -- ditto The kickstart/advance dynamic is interesting because it is kinda what you would do with gloss. It's just a suggestion really. I would recommend first ensuring that evolution itself works, without displaying, and then looking at the gloss functions and see how to use them for what you want. 
file a report on github.com/haskell/hackage-server/issues :) 
Done https://github.com/haskell/hackage-server/issues/137
&gt; Sadly, this is mostly wish fulfillment fantasy. We've had 15 years of: "Here is some syntax, lets go!" but nobody has taken it through to completion. I don't get it. The standard explanation that I've heard as to why no solution has been implemented to this day is that there are too many nice proposals to choose from. To me this implies that there will be resources found to target at this problem as soon as an optimal proposal is presented. I believe it's possible the more if we trigger the community to at last unite and draw a decent attention to this problem. The voice of major figures like yourself wouldn't hurt. There was a Hugs' [TRex](http://cvs.haskell.org/Hugs/pages/hugsman/exts.html#sect7.2) project which solved it similarly to subject proposal. There have been concrete proposals from Simon Peyton Jones [in 1999](http://research.microsoft.com/en-us/um/people/simonpj/Papers/records.htm) and [in 2003](http://research.microsoft.com/en-us/um/people/simonpj/Haskell/records.html) inspired by TRex. What happend to them? Why settle with something that's half-harted? &gt; 2.) It doesn't steal syntax that is currently in use. It interoperates with the various lens libraries to make field accessors usable as lenses. This effectively gives you the (.) for field access, lens style, and you know, doesn't break all the other uses out there of (.). The style guides recommend to always leave spaces around the compositional operator, i.e.: `a . a` instead of `a.b`, - to disambiguate from qualified imports. From my experience the "lens" library is the only library promoting otherwise. Hence it would be the only conflict, which would be easy to solve by updating docs on "lens". &gt; 3.) It has a story for polymorphic field update, and we have a story for how to finish up multi-field polymorphic update by borrowing more technology from lens, and using a more complicated desugaring eventually. This proposal can be extended. What's provided under the original link is just a sketch to induce discussion. I've intentionally put it on gist for forks to appear. With this discussion I hope to come up with a collectively approved proposal. So both suggestions and criticism are welcome. 
I ripped the builder parts out of aeson and did it by
&gt; It should be a normal ADT, and probably you would make bigger genes from small ones. ADT?
Was she Jonathan Tang?
The idea was to basically use "instructions". E.g., "write this integer", "write these 4 words". One goal was to avoid unnecessary bounds checks; every instruction knows the amount of bytes it tries to write. Not sure this is applicable here. I'll ask Simon if he has comments.
Well, my aim with json-builder was to be able to write a serializer for aeson's Value type that was at least as fast. And I did achieve that, at least for the benchmarks I wrote on an older version of aeson.
A little Wikipedia research revealed her name was Autrijus. http://pugs.blogs.com/audrey/2005/12/runtime_typecas.html
&gt; There was a Hugs' TRex project which solved it similarly to subject proposal. There have been concrete proposals from Simon Peyton Jones in 1999 and in 2003 inspired by TRex. What happend to them? TRex records are actually quite inefficient. They are more or less forced to be implemented HList style as something like a linked list. One of the reasons TRex was never adopted is GHC is a much more complicated beast than Hugs. &gt; Why settle with something that's half-harted? I personally view Adam's proposal as bringing more to the table than this proposal. His fixes the existing records, rather than making up something completely new and it doesn't require horrible non-backwards-compatible parser hacks. &gt; The style guides recommend to always leave spaces around the compositional operator, i.e.: a . a instead of a.b, - to disambiguate from qualified imports. I'm not concerned about it from a style perspective, but from an understandability perspective. Dealing with the two meanings we have is bad enough.
`audreyt` was a huge influence on me joining the Haskell community. I hopped on the `#perl` and `#haskell` IRC channels and she was always a blur coordinating all the different people who were hacking on `pugs`. I confess I stole a bit of her management style a year or so back when I started just handing out commit bits to anyone who looked at my code sideways. It is exhausting to say the least, but it is a very productive way to work. She wrote a summary of what spun out of `pugs`: http://pugs.blogs.com/pugs/2010/04/how-to-implement-perl-6-in-10.html
Anyone who wants to see some really great native extensible records ought to look at what Elm has done recently.
I'm talking about structural subtyping. E.g {x : Int, y : Bool} is a supertype of {x : Int, y : Bool, z : Char }
Audrey Tang is one of the first great Haskellers.
What does it matter? She **is** Audrey Tang, and to focus on anything else is to participate in the transmisogyny the article is combating.
Indeed interesting: http://elm-lang.org/blog/announce/version-0.7.elm
I see. A reasonable subject for discussion. Although to me it seems an overcomplication, and again, as from the "tuplish" position this shouldn't be.
A big problem is _lots_ of people want at least some weak notion of structural subtyping, or at least "contains 'at least' fields x y and z". Absent that, you haven't even started to address most of the significant problems people have with the record system.
It matters because if she did work as Jonathan Tang it would be nice to know about that as well.
Down-voted for making an innocent question? I'm in pro transsexual rights. And against rewriting history. Should we pretend that Muhammed Ali never answered to the name Cassius?
For visualisation, I'd use gloss. For genetic algorithms, try genprog. Then you just need to define the model.
Math has exactly one type: Number.
There's nothing unhaskellish about controlling evaluation order, depending on how you do it! We control evaluation order all the time -- not by specifying all of it, but by recognizing the innate dependency graph of our code. Every case statement, forcing a pattern match on a constructor, necessarily forces an evaluation! And what's core basically composed of? Just constructors, case statements, and application! So constructors delay evaluation, case statements force it, and application glues it all together. That's it! If we're reasoning about the space/time complexity of our programs we _need_ an innate feel for the dependency graph reduction structure of Haskell evaluation, and this means that introducing explicit dependencies should feel entirely natural.
i tried to figure out how to resuscitate the BLAS lib last summer, and a more recent variant that Pat Perry wrote (late last summer i was digging into it). There were a number of test suite problems i couldn't figure out, and a number of other related design issues.... SO I wound up spending much of the past year working on a better substrate. :) Aiming to get the basic Blas analogue out soonish, though life is intervening and keeping me pretty busy
An ADT is just the technical name ir what you define with data statements.
theres another approach everyone's overlooking! use AD to write the objective function, and then (either at compile time via TH or a ghc plugin, or at runtime using say LLVM) generate specialized code! Theres a few different experiments I know of underway in this direction, and I myself hope to explore this area a bit myself if i have time early next year (unless one of those preexisting efforts gets released)
well, theres that too :) I'm mostly interested in making manual vectorization easy, I've generally not been impressed with the performance of autovectorization vs manual vectorization. I've actually a few ideas I hope to get ready for 7.10 in that regard too :) 
I just used the standard Raspbian distribution and installed the Haskell Platform on it. GHCi doesn´t work yet, but compiling with GHC does (slowly). The readme on the github page has some more info on using the library. Good luck, and do not hesitate to ask if you have any questions!
I'd like to think if I changed my name all my past accomplishments weren't erased or assigned to someone else.
In this case I tend to blame cabal for preferring to install the old versions of packages while the new versions are available and installable. Here's a quote from my email to Adam: &gt; You are right that in this particular case having an upper bound on &gt; optparse-applicative would avoid the issue. But it's more of a &gt; coincidence. &gt; &gt; Think of it this way: haskell-packages-0.2.1 is an old version of the &gt; package. You don't want to use it anyway — even if it did compile with &gt; optparse-applicative-0.6! It could be very inefficient, or simply buggy. &gt; It could support a different command-line protocol, so even if it &gt; compiled, it just wouldn't work correctly with Cabal. &gt; &gt; So, the fact that cabal picked the old version is a problem by itself, &gt; regardless of whether that version compiles. And even if I had upper &gt; bounds on optparse-applicative, there's a chance that cabal would simply &gt; pick an older version of optparse-applicative, which doesn't improve the &gt; situation at all. 
Usually we only need to reason about evaluation order for performance reasons. Lazy IO means we need to do it for correctness reasons when interoperating with bracket like APIs.
That's a really weird argument. Most of the time, a new version just adds to or changes the API. There's a lot of value to being able to build with old versions of packages. Just think of unmaintained software that you want to check out, or take over.
Cabal could definitely be more clear here, "Using old versions of the following packages ...". But if it picked the newer version it might mean (as it did in the mtl case with that HP version) that other packages would break because of it. Cabal would need to try to reinstall all of these as well, which might lead to even more confusing errors. And if you have a lot of other dependencies, say you cabal install snaplet-fay which adds all of snap's dependencies as well, there are even more potential mismatches. If you have a large set of packages, like we do at Silk, we can't upgrade them instantly. We only want to upgrade to another major version if we know it fixes things we care about, otherwise we might as well wait. Issues like this show up frequently where we have to do major bumps for no apparent reason. 
&gt; If you have a large set of packages, like we do at Silk, we can't upgrade them instantly. When I worked for Barclays, we had a system that ensured that we had our package versions locked down. We certainly didn't want to rely on package maintainers complying with our internal policy. See also Greg Weber's [work](http://blog.docmunch.com/blog/2013/haskell-version-freezing) in this direction.
&gt; But if it picked the newer version it might mean (as it did in the mtl case with that HP version) that other packages would break because of it. That's why we have sandboxes :)
Is labelWithColon_ supposed to call itself rather than labelWithColon or is that a typo?
While I agree with both of you, it's worth noting that many trans people have a traumatic association with their birth identity and particularly their birth names, so it's polite to avoid discussing it. Of course no one is obligated to be polite.
In Elm they have it in the following form: titles = map .title [ book, demain, gertrud ] Actually, there's plenty of ideas to harvest concerning this proposal from there: http://elm-lang.org/blog/announce/version-0.7.elm
&gt; If you're using an old version, there's a high chance of it having known bugs. Maybe, maybe not. But it should be my choice to upgrade, and when to upgrade, not yours. &gt; So someone abandons their software, and now I should do more work so that they can avoid doing their work and have their packages working? I'm not committing to this. I have enough work to do in my spare time. Not they, but people who want to build their software. Abandoned software can still be perfectly usable. At the speed the Haskell community is moving, it could break within a month. It's your choice if you don't want to use upper bounds; it's your time, after all. Just know that your trading your time for somebody else's. It reminds me a bit of the tragedy of the commons. 
&gt; When I worked for Barclays, we had a system that ensured that we had our package versions locked down. We certainly didn't want to rely on package maintainers complying with our internal policy. See also Greg Weber's work[1] in this direction. Such a system would definitely be nice, but you'd still get all the grief when you want to relax some version constraints later (i.e. upgrade some packages). It would give you a choice over *when* you get the grief, though.
Thanks, the infinite recursion is the cause of the problem. Somehow neither me nor my coworker saw it when looking over the code. Both versions exist because the originals are part of a utility library where I want to provide a variant that returns the result of p and one that does not, the easiest way to implement both is to make one use the other.
If you imply that a function using fields `x` and `y` should accept any record containing those fields, yeah, this totally makes a whole lot of sense, providing a whole new niche for polymorphism. In that regard, it turns out, there is a whole research event made: http://research.microsoft.com/pubs/65409/scopedlabels.pdf 
It might make sense to think differently how dependencies should be handled for libraries and applications/binaries. For libraries I can pretty much see both points of view and still don't quite know which one to prefer, because I can see pain on the user side for both cases. But for binaries - especially in conjunction with cabal sandboxes - it could even make some sense to "freeze" (or fix on the first digits: &gt;= 1.2.3 &amp;&amp; &lt; 1.3) all of your dependencies, if this would be possible to do for also the non direct dependencies. Perhaps there's a middle ground, that libraries leave out upper bounds (as long there aren't now issues) and applications/binaries are able to specify the lower and upper bounds of all of their dependencies, also the non direct ones. 
Maybe I wasn't clear, but I was only considering libraries here. No one has a build depedency on an executable so there are fewer implications for your choice there. I'd argue for upper bounds for the same reasons there, you still have users. Yes, users will come across dependency issues either way, but like i wrote: With upper bounds they get a clear indication (a dependency resolution error) that is easily testable, whereas they will get a possibly hard to grok build failure if the upper bound is missing. 
Oh right, I forgot about this! 
 &gt; Yes, users will come across dependency issues either way, but like i wrote: With upper bounds they get a clear indication (a dependency resolution error) that is easily testable, whereas they will get a possibly hard to grok build failure if the upper bound is missing. Yes, they might get better error messages, but this doesn't mean, that it's easier for them to fix the issues. Just having to raise one bound wouldn't be a problem, but if you have to handle multiple bound issues, than this can get quite painful and at the end you still can't be sure that everything compiles. To get really useful, tested and maintained lower and upper bounds you would need to have some kind of versioned hackage distribution, which haskell platform tries to be, but only for the packages they include. 
If the user gets a version mismatch because of an upper bound he gets a clear error message and can then decide for himself whether he wants to try to bump it or just stick to the old version. If he tries to bump it and it turns out to give a build failure because of breaking api changes he knows it's because he explicitly chose to experiment. Without an upper bound he gets the worse error message by default, and there is no way to immediately tell if the package is defect or if it was because of a new dependency. He'll have to manually binary search down from the version of the dependency that failed hoping he will find a solution. You can't make sure that an upper bounded package will play well with unrelated packages (neither can you without upper bounds), but you have the guarantee that it works by itself with the given versions, something you don't have without upper bounds. 
I see what you're saying here, except that it is an inadvertent argument *for* upper bounds: of the client library, to be precise, but upper bounds nonetheless. If cabal were to choose the newest installable version of a provider-library, it would be necessary for the client-library's author to put an upper bound on the dependency to prevent task creep. Example: Let's remember that the specific task in the blog post was updating fay to support a new version of optparse-applicative. Given a hypothetical scenario where: 1. You had released haskell-packages-0.3.0 in the last week, 2. fay specified no upper bound on haskell-packages, 3. cabal was able (dependency-wise) to use haskell-packages-0.3.0, and decides to do so due to an always-newest policy, bergmark must now add an upper bound to haskell-packages, after figuring out that his compile errors were due to an API change. Or, of course, he could expand his task to include making the changes necessary to support a new version of haskell-packages. I would predict, therefore, that if cabal *did* always use the latest available version, the use of upper bounds would increase to avoid this situation. :) Or perhaps cabal could provide a snazzy PVP-aware feature of always choosing the latest z in x.y.z!
Maybe .cabal files could differentiate known upper bound from unknown upper bound? This would let users choose whether they want a known-good build configuration, or if no such configuration exists, then choose a possibly-good configuration.
I like it a lot! You'd start out with all upper bounds as unknown, and then relax/constrain as you test newer versions. It would play well together with these: https://github.com/haskell/hackage-server/issues/52 https://github.com/haskell/cabal/pull/1519 https://github.com/haskell/cabal/issues/949 
Unresolvable scenarios like this are a large part of why I switched back to PVP compliance. Without upper bounds I have no way to move something out of a dependency of one of my packages by promoting the package past the version bound of another one of my packages, or without having version combinations that can't work together despite cabal believing they will.
Yes, that line of research has been a heavy part of discussion taking place over record systems in Haskell. Many people that have played and are playing a role in the current effort are very familiar with it.
J read the title and wondered why maxBound needed a whole article to itself!
Nice write up. I am looking forward to the next one.
Do you really see any way to implement your proposal other than "HList-with-syntactic-sugar"?
I don't think it's stalled. There are several new features which are being added (or already have been added?) to Cabal that would help: * The ability to refer to different revisions of the same package version. * The ability to use a cabal command option or local `cabal.config` parameter to ignore constraints specified in the cabal file, just like you can currently add new constraints. * A freeze facility to provide exactly reproducible builds. Once those are in, I don't think there is any objection to enabling package description edits. That said, Peaker's suggestion is a good one, in addition to the above. It's not the first time that this suggestion has been brought up. It might be more of an invasive change to Cabal though.
Thanks Bas (and all the others) for organizing, it was a great success!
That's because you will never understand the violence and fear that comes with being trans, not merely changing one's name. 
let m be the monad (-&gt;) a. Then the type signatures specialize as: join :: m (m (a,a)) -&gt; m (a,a) (,) :: m (m (a,a)) join (,) :: m (a,a)
We can write (e -&gt; a) as (-&gt;) e a When m = (-&gt;) e then join :: (-&gt;) e ((-&gt;) e a) -&gt; ((-&gt;) e) a Shuffling those around you get join : (-&gt;) e (e -&gt; a) -&gt; (e -&gt; a) join :: (e -&gt; (e -&gt; a)) -&gt; (e -&gt; a) join :: (e -&gt; e -&gt; a) -&gt; e -&gt; a passing to `join` (,) :: x -&gt; y -&gt; (x, y) means `x = e, y = e, a = (e, e)`, so join (,) :: e -&gt; (e, e) The definition is equivalent to the only reasonable thing that can typecheck.
`(,)` is a function, and the Monad instance for those has `m &gt;&gt;= f = f (m r) r`. Now join = (&gt;&gt;= id) = \m -&gt; m &gt;&gt;= id = \m -&gt; id (m r) r = \m -&gt; (m r) r = \m -&gt; m r r Therefore, for functions, `join f x = f x x`, and for `f = (,)` that's just what you've been asking about.
Also: (,) =&lt;&lt; f $ x == (f x, x) (,) &lt;*&gt; f $ x == (x, f x) (,) &lt;$&gt; f &lt;*&gt; g $ x == (f x, g x)
This post is not accessible by default via T-Mobile devices. It is blocked. You have to call tech support and prove that you're over 18, and they remove the block for you. I think this has something to do with the previous post on Bryan's blog about "big data". :) EDIT: Oh noes it still doesn't work. Perhaps there is some other problem with Bryan's site at the moment.
In my experience, it is wrong to assume that mixed-type objects are rare. Most of our `ToJSON` instances are for ADTs with multiple fields of multiple types. For plain data constructors that are not records, the fields are often serialized as mixed-type arrays.
This reminds me of the movie quote [“The Needs of the Many Outweigh the Needs of the Few”](http://youtu.be/Xa6c3OTr6yA)
Another case of explicit recursion leading to explicit cursing. 
How would record update look?
90% of trickiness with monadic expressions giving surprising types comes from `(-&gt;)`. 9% comes from processing lists in another monad.
I like http-conduit. I don't think there's a hole in the ecosystem. -- Just download an HTML document and print it. import Network.HTTP.Conduit import qualified Data.ByteString.Lazy as L main = simpleHttp "http://www.haskell.org/" &gt;&gt;= L.putStr It's pretty easy. And a quality library. It supports https and you don't have to use its “streaming-IO thing” if you don't want to. I don't know the first thing about conduits but I'm using this library. Like most -conduit libraries, the authors tend to provide simple `ByteString` functions, as you see in the [Perform a request](http://hackage.haskell.org/package/http-conduit-1.9.5.1/docs/Network-HTTP-Conduit.html#g:1) section. It doesn't have a massive number of dependencies, notice that there are a bunch of "or" in the dependencies. It's just a carefully planned dependency set with version options. Here are the dependencies: Standard libraries you tend to use in every non-trivial project anyway: &gt; network, mtl, containers, base, text, time, transformers, transformers-base, utf8-string, random, array, bytestring, monad-control, network-bytestring, lifted-base, filepath, data-default Libraries I'd consider appropriate for a production-ready HTTP(S) library: &gt; asn1-data, base64-bytestring, blaze-builder, blaze-builder-conduit, case-insensitive, certificate, conduit, cookie, cprng-aes, deepseq, failure, http-types, mime-types, publicsuffixlist, regex-compat, resourcet, socks, tls, tls-extra, void, zlib-conduit Note that http-conduit is split into http-types, mime-types, cookie, etc. which is good abstraction.
I sometimes use [the curl bindings](http://hackage.haskell.org/package/curl). The library documentation is a bit sparse and there are a lot of options, but it's relatively easy to hack it to whatever end case you need. It's definitely not what I'd call simple, though.
The “aha!” moment for me was realising that although the *fully general* type of `(,)` doesn’t match the form `m (m a)`, what does match is the partially specialised case `a -&gt; a -&gt; (a,a)`; and then that specialising the type isn’t some weird surprise, it’s what the compiler does all the time to typecheck expressions involving polymorphic terms.
 class Functor f =&gt; Costrong f where costrength :: f (Either a b) -&gt; Either a (f b) type PrismVL a b = forall g f. (Costrong g, Pointed f) =&gt; (g b -&gt; f b) -&gt; g a -&gt; f a [Pointer](http://r6research.livejournal.com/27071.html) 
I looked at this, but yeah, quickly got lost in the guts.
why did you do 98 and not the newer one? I do like them by the way
Good choice. That's what I'd try too. I respect the author as well as the io-streams author a ton. 
Thanks, I was going for a retro look on the stripped one and 98 seemed to fit the theme.
[Here](http://i.imgur.com/snev7Ga.png) is the minimalist one I use. The background color is `#1e1e1e` in case you want to match your status bar.
I've detected a hexadecimal color code in your comment. Please allow me to provide visual representation. [#1e1e1e](http://color.re/1e1e1e.png) *** [^^Learn ^^more ^^about ^^me](http://color.re) ^^| ^^Don't ^^want ^^me ^^replying ^^on ^^your ^^comments ^^again? ^^Respond ^^to ^^this ^^comment ^^with: ^^'colorcodebot ^^leave ^^me ^^alone' 
Yes `http-conduit` is very good. I would say though that it *does* have a massive number of dependencies. Getting all those crypto libraries to build with other can be very tricky if you are trying to build anything other than the very latest version of everything. But there is no way around that to support SSL. EDIT: OK, sorry, there is a way around that: use FFI bindings to OpenSSL. But that comes with its own set of problems. I prefer the Haskell dependencies.
I wish you make Haskell t-shirts too. :) Thank you!
Jeese, there are bots for everything these days.
I've had [this](http://i.imgur.com/HbQBcq1.jpg) one in my collection for a while.
I like them, but I’d enjoy one without the text even more. Any possibility of a textless edition?
Minor quibble: your apostrophe is going the wrong way. It should read ’98.
He posted [1.4 versions](http://www.reddit.com/r/haskell/comments/1om4ie/made_some_haskell_wallpapers_thought_id_share/cctbm88) if you haven't noticed that.
The third one would be incredibly useful if it didn't look so magical :)
Which one? the stripes or spacey polygon one?
I wrote about this a few years ago - I agree it can be useful when writing in a pointfree-style: http://nattermorphisms.blogspot.co.uk/2008/08/join-ing-blogosphere.html
Same here, we use http-conduit in production software for data transfert over https. It is easy to use, and does the job well. It runs on linux and windows without issue. Many thanks to the author btw.
Even better: [void](http://hackage.haskell.org/package/base/docs/Control-Monad.html#v:void).
 h&gt; (+5) &amp;&amp;&amp; (*5) $ 10 (15,50) better?
&gt; Do Extraterrestrials Use Functional Programming? Ooh, I hadn't thought of that. What if widespread statically typed FP is the technological threshold barring them from vaporizing us all into a puff of purple smoke?
I wrote a braindump about [possible rules for mostly-monadic-to-mostly-applicative transformation](https://gist.github.com/ion1/7016798) before finding out this already existed. I’ll link it here because my approach was from a slightly different perspective: my rules work on raw `&gt;&gt;=` applications instead of within `do` desugaring.
Other than which streaming library they're associated with, the main difference between http-streams and http-conduit is that http-streams wraps openSSL for its SSL support whereas http-conduit does everything in haskell. Obviously, the later approach requires rather more haskell dependancies.
Certainly. It is just that, all else being equal, I import `Control.Applicative` more often than `Control.Arrow`.
The second van Laarhoven functional reference law requires for a functional reference `l` that l (Compose . fmap f . g) = Compose . fmap (l f) . (l g) The problem with `Monad` is that it isn't closed under functor composition, so the second law doesn't hold/doesn't make sense.
I don't know what's more surprising to me, that there are ads on HWN or that someone here doesn't use adblock.
I got no answer to that! I really though I had adblock installed on all my browsers. I even sent the dev some money for a job well done. *shrug*
4.) It is an excellent summary of the basic concepts and combinators for someone who's not very well versed with lenses yet has an idea of how the package works.
While that is a somewhat reassuring thought, what I meant was that they might consider *us* too great a liability if we were to switch to FP en masse. Maybe it's in everyone's best interest if people just kept using Java and mutating and nulling all over the place. Think about it! Unless... unless you're one of them. I mean, that *is* a mighty funny username, and you're here reading about FP.
I heard somewhere that "A lot of programmng problems have been solved in mathematics for more than 70 years. It seems only very few programmers realise this." I'm glad a lot of Haskell programmers fall in that category. While I agree it's useless to throw the theory in the faces of new users (unless they ask for it...) I think it sets a high standard for the intermediate to expert programmers, and it provides an already-existing vocabulary to all of us, so we don't have to make up our own terms like object-oriented programmers have done.
Let the great experiment begin! 
That's the main point I want to get across :) 
Agreed - just the logo would be lovely.
Actually, I thought the third example was the only readable one of the bunch. It's a pretty straight-forward lifting of the action of `(,)` into function application.
That would probably be what I stole it from. Thanks for providing the source =)
Perfect timimng. I had to use http-coduit just yesterday. Btw it has a bug in urlEncoding It does not encode commas. simpleHttp "http://www.haskell.org/foo?bar=1,2,3" will return 404 (page not found) But if you manually encode those commas: simpleHttp "http://www.haskell.org/foo?bar=1%2C2%2C3" it works fine. 
Great success! A lot of people working hard on awesome stuff, it rubbed off. Overall it was very much like OdHac (an equally good event!): Hack during the day, socialize in the evenings. The main difference was the presentation of the event. OdHac started with presentations for three main projects that people were encouraged to work on (Cabal, Fay, and Hakyll). ZuriHac started with a round table where everyone talked about what they intended to work on. I think both ways are good. There were more package maintainers at ZuriHac, they mostly worked on their own projects. OdHac had 50-60%(?) working on the recommended projects. One thing I'd like to see in the future is to have perhaps just one or two main projects that are important to the whole Haskell community. Cabal/Hackage is a great example of this and I feel a bit bad that there were so few people working on that at both events. Pretty much everyone in the community benefits from having these improved. It would also serve as a good entry point both for beginners and more experienced users. Someone could take on more of a teaching role where the main goal is to get people to a level where they can keep helping out in the future. I'd love to get into Cabal but haven't been able to motivate myself to start digging by myself. There's always work to do on my own projects. And no, I'm not suggesting we do Fay again ;) I would also encourage the people attending to walk around and ask people what they are currently doing; if they need help, or if they can teach you something. Opportunities like these are far apart even if they are yearly. Thanks Bas &amp; co!
Commas should not require encoding. That sounds like a server bug.
I don't think that's a bug, nor is it `http-conduit`-related (it's `parseURI` from `Network.URI`). The `,` character is a valid reserved URI character: reserved = ";" | "/" | "?" | ":" | "@" | "&amp;" | "=" | "+" | "$" | "," You wouldn't expect it to automatically replace `/` with `%2F`. This is user error. 
Well, I never explained it to Adam, but it is easy enough to take an HList-like data type and use a closed type family to make the field accessor work. You need ~3 instances and either `OverlappingInstances` or the new closed type families and then its the same trick as any of the open sum type tricks that folks use.
Looks like it. It comes from clojure web framework. Most likely its routing cannot handle commas in url properly. Oh well. 
I've used the curl bindings, and would not recommend them. The C curl API is terrible, and the Haskell bindings don't expose all of it, making it even worse. Nowadays I use http-conduit, which, while it has its own problems, is a *lot* better. 
Step 1 is learn magic. Step 2 is invent something yourself. Do not proceed to step 2 until you have thoroughly completed step 1 and know why that isn't going to work for you. As a bonus, you'll be able to get better advice from us, because you'll be able to explain what you want from a solution that magic doesn't provide for you.
Yes
I like this [one](https://lh6.googleusercontent.com/-x3WqN1EozSc/T4ut27hzk6I/AAAAAAAAAHI/U17aILTJuyo/s1152/haskell-pattern.png)
I really love the retro colours of the second, but the first one just shouldn't have the text. It just doesn't work in that font, and It's masking a light part of the image and doesn't contrast enough to be be immediately distinguishable. Is there any chance you can link to the source file? (PS or GIMP) p.s. I'm currently using this one: http://p1.pichost.me/i/21/1443974.png 
This is one of the best talks given at the NY user group and I say that by no means at the expense of our other great talks.
I've never heard a report of that, but if I was to take a guess, were you using something like `res &lt;- withManager $ http req`? Something along those lines would cause the connection to be closed by exiting the `withManager` block, so there'd only be one chunk of data available. Anyway, for future reference, bug reports are much more useful for debugging than Reddit comments.
Do you remember the details of this? I'm not aware of what issues come into play when implementing HTTP digest.
The next time anyone says _anything_ about Haskell not being suitable for one or another thing, direct them to this talk! Here's a video of one of the robots discussed doing its thing: http://www.youtube.com/watch?v=Zlt6EJVdUN8 On Wednesday, October 30, Paul Hudak will speak at NY Haskell on "Functional Reactive Programming for Musical User Interfaces". I'm really excited about this one, and in the practical realm, I sincerely hope his talk will give Anthony's a run for its money :-) http://www.meetup.com/NY-Haskell/events/143452712/
It's fairly odd, but there's a big distinction between genetic algorithms and genetic programming. In GA, individuals are represented using bitstrings, and can encode a finite number of solutions. In GP, individuals are programs, represented as ASTs, of which there are infinitely many. Half the battle in doing GA and GP is picking a good representation. It can be worth it to work through a few examples of mutation and crossover by hand, as the distribution of their output will have a huge effect on your population and whether it will find a solution. If you already know the "structure" of your solution, then GA with its bitstring representation can be a better choice than GP, as a lot of the GP's effort will be spent learning the structure. So, here's some things to know about bitstrings: A bitstring is basically a tuple. So, say you're using GA to learn a turtle-graphics program a la Logo. You know the solution will have exactly three instructions. data Turn = TurnLeft | UTurn | TurnRight data Move = Forward | Backward data Pen = PenUp | PenDown Your "bitstrings" will look something like this: type Individual = (Turn, Move, Pen) You can act directly on the individual type. You can also create more generic operators using, say, type-level lists. Or you can convert this to a more uniform representation, perhaps a list of integers (here you would likely want DeriveDataTypeable, giving access to e.g.: the constrIndex function). The latter is equivalent to the conventional bitstrings. The important thing to realize is that the position of the bit matters, and they can be integers rather than literal "bits." You can accept invalid encodings and simply give them fitness 0, but there's no need to. I'd definitely encourage looking at the Field Guide to Genetic Programming for discussion. It's a great introduction and overview, available free online, and is very readable.
Both (http streams and http conduit) are excellent libraries, each with its own set of pros and cons (IMHO): * Skimming the docs, http conduit seems to be the only one to support timeouts, aka trying to establish an http connection for a certain amount of time before giving up, whereas there is no such a thing for http streams, afaik * as you already noticed, http streams uses (if I remember correctly) OpenSSL via another Haskell lib, http conduit does it in pure Haskell with tsl(? Can't remember and I'm writing from the phone). This last point is up to debate; some Haskellers think that the pure Haskell approach has not been tested/verified enough, and thus might be dangerous.
Functional reactive audio seems interesting!
Actually, there are a lot of differences between the libraries; http-conduit includes support for the following: * Connection pooling, which is an important optimization when you're regularly connecting to the same host (e.g., API access). * Cookies, with a full browser API (http-conduit-browser) * Multipart messages * Proxies * Socks proxies * Basic authentication I really wish there was less fragmentation in the Haskell community. If the goal with http-streams was to create a library with easy interop with io-streams, that could have been done without completely reinventing the wheel. If it's to use OpenSSL instead of tls, that's again doable without reinventing everything. In fact, I recently started a new project called [http-client](https://github.com/snoyberg/http-client) which would make such spin-off libraries much easier, but includes most of the functionality that's been built up in http-conduit over the past 3 years. The fact that the author of http-streams is now talking about also writing a pipes version just makes my fragmentation concerns even bigger.
&gt; I've never heard a report of that, but if I was to take a guess, were you using something like res &lt;- withManager $ http req? Something along those lines would cause the connection to be closed by exiting the withManager block, so there'd only be one chunk of data available. I don't think I was using withManager, but I'm willing to believe that I could have been closing the manager before fully evaluating the result. &gt; for future reference, bug reports are much more useful for debugging than Reddit comments. Fair enough. This was about a year ago. At the time, I wasn't really sure whether the problem was with the client or the server (I was working on both at the same time), and once I figured it out a workable solution (always set the content length on the server) I guess I forgot about it.
Btw, there have been subtle protocol compatibility issues with the pure-Haskell TLS implementation in the past, therefore for production code, I tend to prefer the wrapped OpenSSL implementations.
With respect to fragmentation: What's so bad about trying to explore the design space by trying multiple approaches for the same task? To me, for instance, `conduit`, `pipes`, and `io-streams` seem to be 3 different approaches each with different trade-offs, and no clear winner yet -- and maybe there won't ever be a clear winner, as there's seldom one tool optimized for everything (with one exception: Emacs -- just kidding :) ). 
I wasn't trying to imply that there's a problem with exploring a design space, and streaming libraries falls into that category. I *do* think it's a problem if there's not some ideal goal shared of finding a common solution ultimately, even if that goal doesn't seem to be attainable in the short term. But in the case of an HTTP client, I don't see any design space to be explored. This is pretty straightforward stuff. There are questions about ideal API, but they're relatively trivial. Spinning off three separate HTTP client libraries instead of a single library with three small wrapper libraries seems like unnecessary fragmentation.
Yeah I second that.
Nice wallpapers, and many nice ones in the comments here, although this one will always be my favorite: http://i.imgur.com/PzpzV3O.png ([Source](http://www.haskell.org/haskellwiki/File:NarleyYeeaaahh.jpg); wallpaper is 16:10, use background color `#3f51b5` for hiding screen ratio mismatches.)
I've detected a hexadecimal color code in your comment. Please allow me to provide visual representation. [#3f51b5](http://color.re/3f51b5.png) *** [^^Learn ^^more ^^about ^^me](http://color.re) ^^| ^^Don't ^^want ^^me ^^replying ^^on ^^your ^^comments ^^again? ^^Respond ^^to ^^this ^^comment ^^with: ^^'colorcodebot ^^leave ^^me ^^alone' 
Not in detail. Digests can be hard because while they are standardized, servers tend to implement them all differently. They usually need to collect some (but not all) of the headers, the query string, the content body, interpret them as 1-3 different dictionaries with their own capitalization/sorting rules, and then combine those parameters along with the host, path, and method to form the hash. There's also flexibility as to which of these get encoded at which times prior to hashing. When I tried implementing it with http-conduit I think I had difficulty due to some loss of control over the headers or the formatting or the encoding and I had to wade through the source itself to figure out what was happening. Hash programming tends to be very frustrating because often times the server will just document what the hashing procedure is and then you have to fire away countless attempts to hash with 1-bit responses. In the end, the next time I did it, curl seemed far more complex but gave me much more control over the shape of the request that was being sent.
Thank you for your reply. I have taken it into account and I can see that you may have a point and have been taking a look at "the magic" to see how it can help me. Thanks! 
I really look forward to http-client. While some kind of streaming is useful for some http requests, most just don't need it. I'd rather have a killer HTTP substrate library that everyone can build atop—even if it ends up having to be far more complex due to exposed internals—than 6 incompatible one-stop-shops.
Fair enough, http-conduit *does* add some extra request headers, such as `host` and `accept-encoding`. If you ever get motivated to try that out again, please ping me, I'd be happy to include some extra hooks in the codebase to make implementing digests easier.
Well, currently I have http-conduit building against http-client and passing all tests (or maybe one new test is failing... I don't quite remember). But it could definitely use some more testing. Once I consider it a bit more stable, I'll send an email to web-devel and encourage others to play around with it.
I looked a bit but did not spot a link to the slides. Are they available anywhere?
I just pinged anthony about getting them. I don't think they're online yet, sorry.
Thanks!
Huge number of significant improvements in this release! I love this community. 
According to the author, it's all here: https://github.com/bitonic/ny-haskell-agda If anything seems missing let us know and we'll nudge him.
Thanks! Make more! I'm digging the retro '98 one.
The link goes to a video about, robotics with Anthony Cowley at NY Haskell. The link you provide, goes to one of Francesco Mazzoli repos, and is perhaps for the agda talk that happened at the same event?
Here is the agda video: https://vimeo.com/77168227
 liftA2 (,) (+5) (*5) $ 10 better?
indeed, it really seems great! I think I'd prefer it if MINIMAL were a proper extension of the class mechanism, rather than (just) a pragma. I think its a desirable language feature to be defining minimal complete definitions wherever appropriate, while pragmas typically deal with extralinguistic compiler stuff, like optimizations. Ofc tiny nitpick.
In terms of how efficient the code can be made how does ajhc compares to ghc? Also what haskell standard ajhc follows?
Code from the talk: https://github.com/bitonic/ny-haskell-agda
I tried to cram a bit too much :), you can find the rest of the content here: http://mazzo.li/posts/Lambda.html
Would it be better to add a "do nothing" to each of the instructions? type Individual = (Maybe Turn, Maybe Move, Maybe Pen) 
Any thought on better support for Safe Haskell? In particular, some libraries that ship with GHC, such as time, have modules that are unsafe, which makes using Safe Haskell to check for good style quite difficult. The Safe Haskell docs say that Safe Haskell is useful to enforce good style, but with libraries such as time not using Safe Haskell, I find it difficult to use Safe Haskell for this purpose. I think some design changes to Safe Haskell could make it more usable for this purpose. Alternatively the libraries could better support Safe Haskell (e.g. changing code so that it is Safe Inferred, or making it Trustworthy) or just change the docs so that they don't suggest that Safe Haskell is useful as a -Wall on steroids, because currently it's just not useful for that.
As am I. The API footprint is quite small and easy to grok. I am currently working on a library that provides some low level functions that sit right before http-streams' core. One calls into http-streams as normal. The other operates in a monad with canned responses and request recording so that you can test http client code without actually hitting external APIs. Haven't put it up on github yet because I want to use it in at least 1 project to hammer out API kinks. It was quite simply to write with http-streams' API as the target.
&gt; In 7.8, we're hoping to make GHCi use the system linker by default on supported platforms So it's not for certain?
I can not find it now, but I have seen benchmarks showing that JHC and GHC are very comperable. Some things JHC is better at, others GHC. JHC does heavy whole-program analyses to remove most GC from a program. This is one reason it is apealing to use on "embeded devices". As for the version of haskell it targets, mostly Haskell 98 pluss some extentions. The [manual](http://repetae.net/computer/jhc/manual.html) has more information (see the extentions and differences sections). If you want to use it with cabal, good luck :P.
What about the TypeHoles extension ?
Not a proper solution to this problem but you can use `-fwarn-unsafe` instead of `-XSafe` and then mentally ignore warnings about importing from the time package.
[Yep](https://github.com/ghc/ghc/blob/master/docs/users_guide/7.8.1-notes.xml).
I should add that - thanks!
Well, this is what I have so far: http://pastebin.com/17nAwsdN The problem is the lookupSequenz function: I can't just return a Data a, I have to return a Data Int or a Data function... ^^^I ^^^know ^^^my ^^^code ^^^is ^^^horrible... ^^^:D Edit: This is not the Tree code, btw. I decided to tinker around with a simpler GA first.
&gt; I know my code is horrible... :D
Will do!
Sure. I mean, using JHC as your compiler with cabal packages is a pain.
Err thanks.
Can somebody give more details about **Coercible** thing? Is `map MkAge [0..10]` not free in the same way that `map id [0..10]` isn't?
That's basically it. 
Okay. But how does the new feature plan to get around this?
Magic.
And will it also make `map id [0..10]` free? 
It should perhaps be rewritten a bit. But we don't really want to advertise this a ton anyway (not like we don't push experimental things on users in any case, but still.) The basic design is, when you say: newtype Age = MkAge Int You get an instance of a special class generated automatically by the compiler: class Coercible a b where coerce :: Coercible a b =&gt; a -&gt; b -- Compiler generated instance Coercible Age Int where ... instance Coercible Int Age where ... And this `coerce` lets you safely and freely convert between a newtype and its underlying **representational** form. GHC now has a very basic understanding of the notion of nominal vs representational equality (introduced with the concept of roles,) so this `coerce` is *safe* in that it's specifically restricted to types which are rutime-equivalent-but-typed-differently. And then: Prelude&gt; import GHC.Prim Prelude GHC.Prim&gt; newtype Age = MkAge Int deriving Show Prelude GHC.Prim&gt; coerce $ MkAge 1 :: Int 1 Prelude GHC.Prim&gt; coerce (1::Int) :: Age MkAge 1 Prelude GHC.Prim&gt; The current `Coercible` proposal doesn't actually address the `map MkAge [1..10]` example *yet*, but it can be used to eliminate the runtime overhead of other cases. The thing is, there are still some open questions about the design and what it should do so it's quite preliminary. It also seems to have opened up a lot of latent questions about things like representational equality of types and the design issues around them (something roles intended to address in a different cloak.) But it's something of a move in the right direction, I think.
Is Haskell 2010 in the works?
Doesn't R have some way of outputting TeX? I could swear it does, I just can't for the life of me remember what the name of that feature is.
Partly.
Agda's emacs mode is better than Haskell's. It also has working semantic highlighting and jump-to-definition.
What is "HCAR"?
Can anyone add more info about the new I/O manager? My googling gave me [this](http://www.reddit.com/r/haskell/comments/1k6fsl/mio_a_highperformance_multicore_io_manager_for/) Anything else ?
There are actually lots of way to output TeX from R. Check out the [task view on reproducible research](http://cran.r-project.org/web/views/ReproducibleResearch.html). I highly recommend [knitr](http://yihui.name/knitr/), which I have personally used to great success.
most importantly: theres so much more awesome pending, and you should join in and help contribute! Once you get a teeny bit more familiar with GHC, you'll realize theres *so much* low hanging fruit to make things even better in so many parts of GHC, all it takes is a little bit of time and focus to really understand a narrow bit of GHC! (Its important to have a big picture view, sure, but focused mastery is achievable with just a month or so of digging in, in your spare time). cheers! Seriously, the more people helping out, the better! It can be as simple as taking the time to build head, trying out features, and hitting bugs and reporting them! 
I use this one http://i.imgur.com/HyFyV.jpg
&gt; Carter Schonwald has been involved in talking with many who wish to get involved and inspiring them to get their hands dirty! Ah yep, figures :)
[This](http://aosabook.org/en/posa/warp.html) contains a case study of its effects, and is a worthwhile read in general.
I always noticed a Perl-like enthusiasm to give out commit bits by you - it all make sense now! :)
For instance there are some [rewrite rules](http://www.haskell.org/ghc/docs/7.0.1/html/users_guide/rewrite-rules.html) in there, which are unsafe. I'd suggest they be removed, but I do not know how much this could affect performance.
Currently this is as close to a good solution as you can get, but the problem is that then every module that transitively imports an unsafe module is "tainted," if you will. If time is at the bottom of an import hierarchy, practically every module in a package won't be marked safe-inferred. My idea on how to fix issues like this is to augment the notion of package trust. An extra GHC flag could indicate that if I trust a package, all modules in the package are considered trustworthy. That way library authors who don't want to care about Safe Haskell can keep on not caring, and users who might care get a solution that doesn't require nagging library authors or wrapping unsafe imports in boilerplate shims.
I did find #haskell through #perl. ;)
If I really needed ghc extensions, could I use ghc to output C and call that from the C wrapper? Or will I have trouble getting the haskell runtime to run on android? **edit**: [look like ghc is also a valid path to running Haskell on android](https://github.com/neurocyte/ghc-android).
Haskell Community Activity Report
Care to help us spotting the low hanging fruit? trac has a lot of issues like [add -O3](http://ghc.haskell.org/trac/ghc/ticket/1371), marked with difficulty = "moderate (less than 1 day)" (it's just an additional command line option after all, right?), yet it has been open for 6 years... 
-fdefer-type-errors
s/liftA2/liftM2/ then 
http://www.haskell.org/pipermail/haskell-cafe/2013-October/110839.html
Do these instances reveal the representation of a newtype even if I don't export it from the module it's defined in?
The completion function can use the underlying monad. So I think the idea is that you have sufficient state available in the transformed monad to be able to define the completion function just once. Taken to the extreme, you could even store the whole completion function in the underlying monad and then define it to just read the current state and execute it. 
This is already in 7.6.x?
Is there a way to prove the safety of rewrite rules? Not in the general case of "what if they rewrite in an `UnsafePerformIO`?" but, for simple rules like: "map/append" forall f xs ys. map f (xs ++ ys) = map f xs ++ map f ys Can't we determine whether or not it is safe by determining if the rhs is safe? Or that, no choice of `f`, `xs`, or `ys` makes this rule *more* unsafe?
Thanks, managed to get it to work! (see edit)
Rules are applied after (some) inlining, so they can peak at values which would otherwise be inaccessible. But perhaps the rules can be ignored for Safe Haskell.
Good! Here's the minimal example I put together that you won't need after all. &gt; import System.Console.Haskeline &gt; import Control.Monad.State.Strict &gt; &gt; main = runStateT (runInputT settings loop) 0 &gt; where &gt; loop = do &gt; minput &lt;- getInputLine "% " &gt; loop &gt; &gt; settings :: Settings (StateT Int IO) &gt; settings = setComplete comp defaultSettings &gt; comp _ = do &gt; n &lt;- get &gt; put (n+1) &gt; return ("", [Completion (show n) "" True]) 
Interestingly I got the order of the stack wrong (as well as a small typo) the first time around, and lifted getInputLine. That caused a ghc panic. Funtimes!
Breaking encapsulation *is* unsafe.
I seriously owe you a beer. =)
Wow, that's interesting. Did you report the bug?
Nice work! Maybe you could also upload packages for ghc 7.8? 
A minor criticism about the polymorphism part. Syntax such as `(a :: Int, b :: String)` does two jobs. On the one hand it expresses an exact anon. record type. On the other hand, it expresses a set of anon. record types - all those that provide the required fields with the required types irrespective of any other fields. There is a kind of precedent for that in OOP. A class type in OOP represents the class itself in some contexts, and the set of classes including all subclasses in others - particularly as part of a pointer/reference type. Even so, I was impressed by the way Haskell handles existentials - the "any of these types" is distinct from the "precisely this type", so you get to explicitly choose which behavior you want. I may be worrying over nothing, and I can see a kind of least-upper-bound semilattice-of-fields argument for treating the set-of-types as a single type, but I still think it's a good idea to differentiate between this-exact-type and forall-matching-types. 
thanks for putting this online! i enjoyed it a lot :)
Is it stubborn of me to think that TDD won't gain you much in Haskell? It looks like a lot of extra work.
Prove it - is there any way to break the `map` rule above such that it is unsafe?
I feel that when you develop within a repl in an iterative manner you are kind of doing something similar to TDD without the dogma. When writing simple functions, testing them from ghci is often so trivial that no explicit unit test case needs to be written. Eventually you can define some quick check properties. You might even need a few specialized unit tests cases, but I find that with Haskell, you don't need as many trivial unit test cases as in other languages. For example, you don't need as many negative test cases (like passing nulls) as the type system prevents a lot of invalid inputs.
I'd actually suggest going about it a different way. Choose a part of the runtime system, code gen, optimizer, type checker etc, that you wish to really understand. Spend some time getting familiar with understanding that slice of the code base + doing some relevant background reading. You WILL find code clean up opportunities and ways to augment capabilities. Theres a lot of low hanging fruit in the RTS, code gen, and the like, because they've gotten relatively less eyes on them that the type checker / type system related bits overall. Just dig deep and focused and you'll find stuff :) 
Properties would be much nicer than HSpec, too, probably.
Seems like quite a few of the HSpec cases could be written in just a few short Quickcheck/Smallcheck properties which just enumerated all board configurations, 3x3 isn't that large of a space.
I was hoping the T in TDD were 'Types'.
1. What are you using to have the tests run automatically like that? Is it a Vim plugin? 2. What vim color theme is that? Looks pretty nice 3. How did you manage to show leading white space using that dot character?
In answer to the first, it's [guard](https://github.com/guard/guard). To answer the third, I think that's handled in listchars. In my vimrc, it looks like set listchars=trail:·,precedes:«,extends:»,eol:↲,tab:▸\ 
How far is HaLVM from supporting, say, an Yesod hello world? 
Very nice, to awesome. I'm unfamiliar with xen, where in the code is the 0 copy networking, and looking through the examples couldn't find the sample program that made use of real devices. I want to use this, but by no means am qualified. 
That would be a pretty big change. The layer of Xen the HaLVM integrates with is very specific, and not really astonishingly portable. On the bright side, having the HaLVM sitting as a reference would make implementing a KVM port much easier, because you'd have the basic shape of what you need.
I was thinking more of a weekend hacking thing (I can't do any open-source stuff at my job, let alone Haskell!). So it would have to be easy bugs or small changes to existing features. But I'll take a look at your suggestions. RTS sounds very interesting, but I'd rather code in Haskell so I'll take a look at code gen. Thanks.
Depends what you mean by "real devices." Right now, the HaLVM only supports Xen-virtualized devices. Specifically, the virtual block and network devices exported to Xen domains. With that in mind, the example using the network card is in "examples/XenDevice/VIFTest", and the source code for the driver is in "src/XenDevice/XenDevice/NIC.hsc".
But you do have TCP, right? Does the normal Network.Socket library work, so one could write a purely dynamic web server (i.e. one that doesn't serve any files)? If that much works, then interacting with an Amazon S3 type service (or a normal database) could give a fairly complete web server.
We have TCP only via HaNS, not via standard BSD sockets. HaLVM ==&gt; no POSIX. So no file system, no networking. We have a network card driver written in Haskell that you can hook up to the Haskell network stack. If someone wanted to wire this into the network package, that'd be awesome, but it's not done yet. That being said, writing a simple web server / web service via the HaLVM isn't very hard. But hooking up Yesod, with all its dependencies, is a challenge.
wow - can you provide any details (link would be great) on this study?) - seems to contradict the/a complete industry right now
I think sitting on top of xen is a really clever idea (because it means you don't have to directly interface with hardware). Is that something you came up with or did you see it elsewhere?
&gt; Wed 2:00 AM is the highlight ;)
Well, once GHC 7.8.1 is out I'll definitely do that; in the meantime you can use the `ghc-head` package I've uploaded and consult the new section ["GHC HEAD Snapshots"](https://github.com/hvr/multi-ghc-travis#ghc-head-snapshots) I just added to the `README.md`.
The lack of transformer instances for Haskeline makes it pretty brutally hard to use in my code, since they protect me from myself by preventing me from having access to enough functionality to write them myself.
Out of curiousity, is there a nice place to browse documentation for Halfs or HaNS?
We have discussions about Android NDK apps written with Haskell at today's meeting. You can see the slide at http://www.slideshare.net/master_q/20131020-osc-tokyoajhc .
I will write a paper for ICFP to explain Metasepi and Ajhc project.
Dependency injection in other words. This would be a great addition because it is what makes the Haskell IO monad untestable compared to languages where dependency injection is commonplace.
I think the main benefit of TDD is that it makes you think about what you WANT the program to do before you start writing it. After a certain level of proficiency in programming, everyone does this anyway; there's not much visible benefit because we are capable of putting together small pieces of software without needing a test suite to tell us what we're supposed to be doing. In Haskell, it's doubly unnecessary. Haskell's type system makes testing less of a concern than in other languages (it's still important, don't get me wrong). The common sarcastic remark "my code compiled. It must be working!" is actually not so sarcastic in Haskell; a lot of the time, this is the case.
I don't necessarily think that `Apply` and `Bind` are improvements. Just because a mathematical abstraction has a name doesn't necessarily mean it is worth standardizing on that abstraction and introducing it into the type class hierarchy. Most of the cases where I've seen an `Apply` without an `Applicative`, a `Bind` without a `Monad`, or a `Semigroup` without a `Monoid` it has been a misfeature and the abstraction was improved by adding the corresponding `Return`, `Pure` or `Empty` term. All of these abstractions can be lifted to be proper `Monad`s or `Applicative`s or `Semigroup`s, so why not just put in the extra effort to do so?
The most obvious example would be the free bind (`data Bind f a = KindaImpure (f a) | VeryImpure (f (Bind f a))`). This is essentially the "adification" of non-empty lists (in the sense that the adification of free monoids/lists is free monads).
It really is pretty bad. Is there a better alternative somewhere?
I'm confused &gt; instance Apply OrNot where &gt; Not &lt;*&gt; _ = &gt; Not &gt; Or _ &lt;*&gt; Not = &gt; Not &gt; Or (NEL h t) &lt;*&gt; Or (NEL h' t') = &gt; Or (NEL (h h') (t &lt;*&gt; t')) &gt; The OrNot data is isomorphic to Maybe (NonEmptyList a) and has an Apply instance that is similar to the Applicative for Maybe. However, since this data type holds a non-empty list, there is no possibility for an Applicative instance. What's wrong with instance Applicative OrNot where pure a = Or (NEL a []) `OrNot a` is isomorphic to `Maybe NonEmptyList a` which is isomorphic to `[a]`, no? I mean, the `&lt;*&gt;` is a little different (zip the first elements, product the rest): newtype OrNot' a = OrNot' [a] instance Apply OrNot' where (OrNot' (f:fs)) &lt;*&gt; (OrNot' (a:as)) = OrNot' (f a : (fs &lt;*&gt; as)) _ &lt;*&gt; _ = OrNot' [] But I don't see anything inherent in the definition of the type that precludes a defintion of `pure`. What am I missing here? 
In case anyone is wondering about examples, another instance of a type which is certainly an instance of `Apply` but not of `Applicative` would be expression DSLs. Imagine that you have a DSL parameterized by context and indexed by Haskell types, so that the type of such a term might be `Expr g (String -&gt; String)` or some such. In this example, we do not allow just any Haskell term to be reflected into the DSL: so there is no rule of type `a -&gt; Expr g a`. But on the other hand, one of the rules in our GADT is application, `(:$) : Expr g (a -&gt; b) -&gt; Expr g a -&gt; Expr g b`. If `Expr` were an instance of `Apply`, and Tony's proposal were adopted, we could use do-notation to apply functions within the DSL.
Tekmo, I hadn't read your comment by the time I posted mine, but I think what I've said above is relevant here. Expression DSLs indexed by Haskell types do not (in general) admit `pure`, but they have `&lt;*&gt;` just fine. http://www.reddit.com/r/haskell/comments/1ou06l/improving_applicative_donotation/ccvosgc 
The `Apply` instance of `OrNot` seems to match the `Applicative` instance of `Compose Maybe (Product Identity [])`, so I think `pure` should be pure a = Or (Nel a [a])
Off the top of my head: * makes the class hierarchy more complex and thus harder to learn * introduces (slightly) more boilerplate when declaring instances * breaks existing code (None of these are killer arguments IMO.)
But "breaks existing code" would iinm apply to any attempt to fix the current class hierarchy, so it's not a unique issue with this solution.
Oh, absolutely. But it's still a downside, and we should include it in the accounting of whether this proposed change is worth it.
But is your `Expr` a `Functor`?
I answered this as a direct response to your linked comment.
The downside is that type classes like `Semigroup` encourage incomplete design. The Haskell type class hierarchy should be proscriptive, telling users what interfaces they should strive for. By removing `Semigroup` from that hierarchy it conveys to people that they should make the extra effort to turn something into a `Monoid` rather than settling for a `Semigroup`. As I mentioned before, most instances of `Semigroup` that I've seen usually benefit from being lifted to be a `Monoid`. I want to encourage people to always take the effort to write up a proper `Monoid`.
Ah, that's a really good point which I hadn't considered. Thanks!
If I don't care whether the list may be empty then `[]` is perfectly fine. If I do, `NonEmptyList` is clearly superior. You set up a straw man by arguing that the type of `(&lt;&gt;)` is problematic. If I only care that the list can't be empty, the type is fine anyway. Your `Vector` type does not form a `Monoid` (at least in the sense of the type class), so I don't see the point of that example. On `Map`, your proposal makes the model (originally `k-&gt;(v+1)`) more complicated (`(k-&gt;(v+1))+v`), but I think it would be better to make it simpler instead (`k-&gt;v`). This amounts to making the representation something like `data TMap k v = TMap { partialMap :: Map k v, default :: v }`, and the resulting type is a `Monad`. You can recover partiality with `TMap k (Maybe v)`. I think an important piece of your explanation that's missing is *why* it's worth changing the denotation to make it a `Monoid` instead of a `Semigroup` or a `Monad` instead of a `Bind`. As far as I can tell so far, it's for its own sake, which is not particularly convincing to me. Perhaps you might say that because `Applicative` is richer than `Apply` then you gain more power by changing your type to fit that interface, but I argue that you have, in exchange, lost the ability to say things about what the value *can't* be (in the case of `NonEmptyList`, the value can't be empty, and in the case of `Map k`, the value can't be densely populated without knowing something about `k` and can never be infinitely populated). Adding the ability to do more is not always a good thing. A dynamically typed language is a great example of what can go wrong if you take this to the extreme.
First of, I didn't knew FRP could represent dynamic system, this is great! I spent some hours looking around, learning some basic FRP, and your representation seems fine. Somehow it seemed to be related to integral behaving poorly on loop, so I tried changing to `imIntegral`, and voila, it worked ([see my SO answer](http://stackoverflow.com/a/19481946/1631438))! Still, something bizarre is going on, my 2cents is that the integral algorithm was poorly/wrongly implemented. I'm going to look into some more to have a more conclusive answer. 
I'm not sure I understand this idea that `Monoid` should always be seen as a superior design to `Semigroup`. Sure, any `Semigroup` can be made into a `Monoid` by adjoining an identity; but doing so is generally not guaranteed to play nicely with the semantics or other useful properties of the type. What does persuade me, though, is simplicity. Going back to the original proposal, we're being asked to regularly think about `Functor`, `Apply`, `Applicative`, `Bind`, and `Monad`, as five separate ideas. In many cases, it's difficult to come up with compelling examples of things that would uniquely occupy the discrete spots in this space of abstractions. I'm not saying they don't exist -- clearly they do -- but in my mind it's still a very much open question whether the sum of cognitive load from everyone thinking through a class hierarchy of twice the size is less than the cognitive load of working around using an imprecise type class in situations where you might have actually wanted to use, say, `Bind`. In my opinion, some parts of the Haskell community get that calculation very wrong on a regular basis.
Thank you!
(Vector, &lt;&gt;, mempty) is an indexed monoid. 
How much math have you seen? Fact is, interesting monoids don't always come from a free construction. Take the monoid of non-zero integers under multiplication. You'll see that substructures form semigroups, not monoids. E.g. the semigroup of non-zero /even/ integers. (To explore this line of thought further, see the discipline of pseudo-rings aka non-unital rings aka rngs aka rungs.) Everyone knows that you can formally stick in an identity. So what? More isn't always more. 
You get nothing from this modification of applicative do except changes to defaults. Haskell already has rebind-able syntax. Applicative do is about changing how something desugars, which is the reason it is needed. You can obviously define your own `&lt;*&gt;`. I don't know what the laws are supposed to be for `&lt;*&gt;` without `pure`. At least with `&gt;&gt;=` there is an associativity laws. Well, I can come up with a version of the composition law, but that needs the functor instance. And, that takes away some of the most compelling use cases (like `Expr`) which are not functors. IMO, lifting or overloading address these cases sufficiently. There is a case to be made for desugaring to applicative (language level), but not nearly a strong enough case to do this as the default at the library level.
Remember that we are not debating whether or not `NonEmptyList` is a useful type. We're debating whether or not the concatenation operation on `NonEmptyList` is worth type classing (into `Semigroup`). In every case that you have a type like `NonEmptyList` that doesn't form a `Monoid` the mappend operation is usually subtly flawed in the same way as `NonEmptyList` concatenation. So I think we should not encourage a type class whose sole purpose is to endorse the use of flawed concatenation operations. I'm totally fine with defining a concatenation operation for non-empty lists, but I don't think it deserves its own type class. The fact that you can cite other data types which have a similar slightly flawed concatenation operations is not convincing to me. I agree that my map proposal is probably not the correct solution, but this only tells me that I should think some more about what the correct solution should be instead of satisfying myself with the current solution.
I don't know. I think I got that answer incorrect and need to think about it further. If I come up with a better solution I will reply with another comment.
I'm wondering why Naturals aren't 1-based in agda. Now data.Integer has this seemingly Akward representation http://www.cse.chalmers.se/~nad/listings/lib-0.7/Data.Integer.html where something like this would be far more elegant imo. because we don' t have -0 and +0 this seems to make much more sense t me (never wrote agda before and dont have a compiler. so dunno if it' s actually valid) data ℕ : Set where one : ℕ suc : (n : ℕ) → ℕ data ℤ : Set where zero : ℤ _[_] : (a:Sign) → ℕ → ℤ ∣_∣ : ℤ → ℕ ∣ _[n] ∣ = n of course now we can' t pretty write stuff like `+1 - -2` but have to write `+[1] - -[2]` but the definitions become a lot more simple. Opinions?
Man, I can't thank you enough for that! Great find, the difference between `integral` and `imIntegral`. I was looking for some sort of Haskell library to replace Simulink, and Yampa and Netwire seemed to be what was recommended - they get mentioned a lot together in talks/slides.
IIRC, around the time the AMP split was proposed, Tony suggested that the split might go further. He got downvoted quite a bit - from the replies it looked like it was by people who thought he was talking about Pointed - and by the time that got sorted out I think people had moved on from that thread. I remember thinking back then that this could be quite nice, or would at least be worth discussing, but would probably have a better chance at succeeding if it was rolled into the initial AMP proposal rather than being done later.
&gt; __flawed__ concatenation operations o.O
Well, I did already explain why `(&lt;&gt;)` for non-empty lists was subtly flawed. The purpose of type classes is not to uphold *almost* perfect abstractions. If you want to modify Haskell's type class hierarchy you need to make a more compelling argument than "I have a bunch of operations which kind of sort of do the right thing but don't have an identity and are therefore deserving of a type class".
You didn't explain why it's flawed; you explained why it doesn't do something that you'd like it to do. Edit/elaboration: we can easily say "the property of being non-empty is closed under appending" and if being non-empty is all we care about, then we capture exactly what we care about. Not all operations need to state everything they do in their types. Even in Agda this is true, and a lot of taste/design sense goes into deciding what belongs in return types and what belongs in separate proofs. As for examples, `min` and `max` make perfectly good semigroups, but not necessarily monoids. The `Monoid` instance for `Maybe` is atrocious and virtually begs for a `Semigroup` class. `Map` also begs for a `Semigroup`/`Apply` class in several different ways. And it sure as hell was a lot easier for me to do the proofs I wrote in https://github.com/copumpkin/containers in terms of semigroups than having to deal with a special empty case all over the place (which I can handle separately if I so choose). The same is true of fingertree if you structure it correctly.
Seems a great place to start. But simulink is hard to replace. Dropping some boxes, configuring a script with a matfile, and bam, works with variable step size, controlable tolerance, etc etc. I have no Idea how easy is to reimplement that on Yampa.
Oh for sure. By 'replace' I really meant 'replace for small-scale use in assignments and playing around', rather than wholesale power-user full-time substitution. And, of course, in Simulink you can just add a scope anywhere you like to inspect the system at different points... I have no idea how to replicate this functionality.
Actually, this should be possible using `knitr`, and Haskell literate mode. You would write an `.Rnw` document something like this: \documentclass{article} \begin{document} \begin{code} main = do putStrLn "hello" writeFile "output.csv" generateOutput \end{code} &lt;&lt;graph1, echo=FALSE, device=tikz&gt;&gt; library(ggplot2) mydata &lt;- read.csv("output.csv") ggplot(mydata,aes(x,y,colour=someVar)) + geom_bar() @ \end{document} This would be understood by GHC, it could also be understood by `knitr`, which would then generate a compilable `TeX` document. You can use Emacs' [ESS](http://ess.r-project.org/) plugin to make the "knitting" step a lot more comfortable, but you'd have to hack your Emacs a bit to allow direct loading of the code into an interpreter powered by `haskell-mode`. GHCi, however, should have no problems reading this. The only thing this *doesn't* do is transfer values from Haskell to R directly. I know of no way to do that within one document. Neither LaTeX code, nor R code ever starts with a Bird mark, so you're free to use them. You'll be able to compile and run this file. Or you can use the `\begin{code}\end{code}` environment. You can use the `listings` package to make your code output in the LaTeX document prettier. See the [hints at the Wiki](http://www.haskell.org/haskellwiki/Literate_programming#Latex_suggestions_for_literate_programming). NOTE: there are things to be said *against* this style. You're throwing a lot of logic into *one* file, and making it very multi-purpose. At some point, maybe several different files and a `Makefile` might actually be more comfortable in the long run (i.e. easier to maintain.) I think this approach could be sensible for short things you work on yourself, but I wouldn't recommend it for longer projects, or collaborative projects (too many things can go wrong in this pipeline, and it isn't particularly comfortable without Emacs.)
[Paper](http://iit-iti.nrc-cnrc.gc.ca/publications/nrc-47445_e.html) is 404.
Tekmo's map isn't right, but a correct one isn't that hard data Info a = Deleted | Modified a data AppMap k a = AppMap { info :: Map k (Info a) defaultValue :: Maybe a } the `lookup` operation looks something like lookup k a = case M.lookup k (info a) of Just (Modified b) -&gt; Just b Just Deleted -&gt; Nothing Nothing -&gt; defaultValue a We than say two `AppMaps` are "equal" if the lookup operations behave the same on them (quotienting is needed for laws). Everything just works: the `delete` operation simply modifies the underlying map to insert a `Deleted`. The `insert` operation just inserts a `Modified.` `union` is not primitive, but built out of `unionWith`. Most everything works this way: `empty = AppMap M.empty Nothing`. The point for the applicative instance is just pure x = AppMap M.empty (Just x) defining `&lt;*&gt;` is not hard but a bit tedious for a reddit comment.
Usually, you don't want integers in Agda, you want naturals with a zero. Think about length-indexed vectors.
What I mean is that in a world without semigroup operations (i.e. a min/max that operate only on the lifted types) you would not need the Monoid instance for Maybe.
Greater precision isn't always better. Math is about finding commonalities and simplifications between diverse subjects, not about building an increasingly large tower of concepts of greater and greater complexity. Addition and multiplication are pervasive in our lives because they are simple whereas multiplication of non-zero integers is nowhere near as pervasive because it is more complex.
how about an extension for operators: do-many (+) 5 1 3 7 == 5 + 1 + 3 + 7 fmap f $ do-many (&lt;*&gt;) g h w == fmap f g &lt; * &gt; h &lt; * &gt; w do-many (&lt;&gt;) x1 x2 x3 mempty == x1 &lt;&gt; x2 &lt;&gt; x3 &lt;&gt; mempty do-many (&gt;&gt;=) m1 m2 do-many (&gt;&gt;) m3 m4 const $ return () == m1 &gt;&gt;= m2 &gt;&gt;= (m3 &gt;&gt; m4) &gt;&gt;= (const $ return ())
I did my search, Arrow notation seems the way to go. feedback op a b b0 = proc u -&gt; do rec x2 &lt;- (delay dt b0) &gt;&gt;&gt; b -&lt; x1 x1 &lt;- a -&lt; u `op` x2 returnA -&lt; x1 edit: add {-# Language Arrows #-}
This is exactly the sort of argument I have to argue against all the time in mathematics. *No*, rings are not superior to semirings; sometimes you just don't get to have negative numbers. *No*, commutative (semi)rings are not superior to non-commutative (semi)rings; some things simply don't commute. *No*, monoids are not superior to semigroups; you can't always get the luxury of an identity element. There are reasons people've come up with these more general notions. You can't always get all the operators that would make something "pretty". This isn't a flaw; it's a fact of life. Matrices don't commute. Integers don't divide. Naturals don't subtract. u.s.w.
How large is the resulting executable?
You seem to be advocating "roll `Maybe` into your structures that have no identity, because I say `Monoid` is better than `Semigroup`". Is that correct? It seems to be rather un-DRY.
For inspection? I did see lots of places that used arrow notation for Yampa systems, but I didn't find it that advantageous just in terms of readability etc. Also I'm not sure how to use it yet ;P. What is the `delay dt b0` doing there? EDIT: Oh - is it replacing `loopPre` by providing an initial value for the feedback?
Yes, that is what I am advocating.
Youu need a delay because x1 is a recursive binding, so it must have an initial value (b0). It also must have a time delay -- for reasons I cannot explain. using something bigger than dt is making the feedback loop unstable. And yeah, arrows may be the way to inspect something out without reworking too much code.
Exactly, naming intermediate values seems to be the better way to scoop something out without having to rework the entire chain of functions
Anyone knows if this also means that comprehensions will extend to other data types as well?
Let's call `Nothing` `null`, then :)
How about we call it `PositiveInfinity` (in the case of the `max` operation on unbounded integers) and `NegativeInfinity` (in the case of the `min` operation on unbounded integers)?
An example: if arrows represent computations that should be compiled to another medium, such as JavaScript or GPU, then 'arr' can be a pain. Same if you want to statically enforce real-time or bounded space properties. This has been addressed with [Adam Megacz's generalized arrows](http://www.cs.berkeley.edu/~megacz/garrows/), and also the arrows that are effectively available (in small pieces) via the 'categories' package.
&gt; Regarding `Map k`, I believe the correct interface is to extend it with an alternative `Pure` constructor: &gt; &gt; data AMap k v = AMap (Map k v) | Pure v &gt; &gt; That can now be made into a proper `Applicative` and it offers a useful feature: users can now build a map that constitutively returns the same value for all lookups, something that was not possible using the unextended `Map` type. I've actually spent a bit of time looking into a related problem area, so I think I have two things I can contribute here: 1. I agree the best designs are based on `Applicative` instances. I once had an `Apply`-based solution to a similar problem, but ended up refactoring it to a superior version that admitted of `Applicative`. 2. Your `AMap` type, however, isn't very great, because the `Applicative` operations will needlessly build intermediate `Map`s at each step. The best way to go here, IMHO, is to exploit the close relationship between `Map k v` and `k -&gt; Maybe v` and use a version of the latter as our `Applicative`. So something like this: {-# LANGUAGE GeneralizedNewtypeDeriving #-} import Control.Applicative import Data.Functor.Compose import Data.Map (Map) import qualified Data.Map as Map newtype AMap k v = AMap (Compose ((-&gt;) k) Maybe v) deriving (Functor, Applicative) fromMap :: Ord k =&gt; Map k v -&gt; AMap k v fromMap m = AMap (Compose (flip Map.lookup m)) -- | To materialize an AMap you tell it a superset of the keys -- the concrete Map will have. runAMap :: Ord k =&gt; AMap k v -&gt; [k] -&gt; Map k v runAMap (AMap (Compose f)) = Map.fromList . concatMap f' where f' k = case f k of Nothing -&gt; [] Just v -&gt; [(k, v)] Now you can build `AMap`s applicatively without building any intermediate `Map`s. There's also the a special case where, if all of the maps you're working on have the same keys, then what we have semantically is just the function applicative (with no `Maybe`) over a domain type whose membership coincides exactly with the "keys" of the "map." In this case you're basically dealing with a representable functor, and the best solution is to find a way to encode the key set as a type.
My point was to draw a parallel from your proposal to the `null` clusterfuck that most other languages experience. In pure typed FP, we like our types to be precise. We don't like extraneous elements thrown in for the sole purpose of satisfying an overly restrictive interface (`Monoid` in your case). I hate having to throw stupid elements into my types to support `negate` or `(-)` in `Num`, just like I hate having to throw extraneous empty elements into my types to support base's `Monoid`-centrism (`First`, `Last` come to mind). When I do throw in extra elements that don't belong, I now need to deal with them in all sorts of other places in my code, because they don't make sense on their own. My `Integer+Infinity` type now needs to decide how to deal with infinities when I try to do any number of things where infinity makes no sense. And as Wren said elsewhere in this thread, there's nothing fundamentally "better" about `Monoid`. If there were, why stop there? Why not go throw in a `negate :: a -&gt; a` constructor into every type in case we decide that `Group` is the most natural principle of API design? Hell, we can do some awesome stuff with `Field`, so let's just require that everywhere. If I'm merging values in maps with something like `intersectWith`, I don't need a neutral element. I can merge two maps (or partial functions!) using a `min` semigroup and not have to worry about introducing random crap into my types that I then have to unsafely assume isn't there when I project my base type back out. If I want `PositiveInfinity`, I'll add it explicitly to my types in the form of `Maybe`, and write the instance once instead of once for every type. It's not an ideal name, but that's syntax and can be solved in other ways. In general, our code should ask for what it needs, and nothing more. Don't force us to throw garbage into our types to appease an overly restrictive API, when it won't be using that garbage anyway.
Also known as a category. Edit: too hasty, not quite :)
Oh, i hadn't thought of it that way, but yes.
I think it is a different kind of indexed monoid. Perhaps it should be called a "dependent monoid." Or perhaps an "enriched monoid" since (Vector, &lt;&gt;, mempty) defines a single element category enriched over natural numbers. ~~Here natural numbers are the discrete category where the monoidal bifunctor is addition.~~ Enriching categories over monoids is a neat trick, although usually I find I'm using strings (free monoids). Enriching a monoid over a monoid is a rather pathological example, but a useful one in this case. EDIT: Oops, I said how to make a monoidal category where the objects where the elements of a monoid, but this isn't what you want. Instead, you want the objects of the monoidal category to be *collections* of elements from the monoid. In this case it works to define the objects to be *sets* of elements from the monoid, and morphisms to be subset inclusion. Then, the bifunctor is `A tensor B = {x &lt;&gt; y | x in A, y in B}` which obeys all the laws you want with `{mempty}` as the identity object. This works for the "length typed vectors" and for, say, understanding labeled transition systems as categories enriched over free monoids. But, sometimes you want multiple "labelled" arrows between the same object to share a label, in which case you need to replace sets with multisets. Don't be scared: categories enriched over monoids show up all the time, and you never need to worry about the construction.
Could you fix this by making `arr` lift a category other than that of Haskell objects and functions between them?
We need a more universal-algebra-like way of thinking about structures (standard algebraic ones, CT ones, odder ones) so we don't get addicted to particular named combinations of properties.
I'm not quite sure how to communicate this notion of mine, but hear me out. What I notice about `Monoid`/`Monad`/`Applicative` compared to `Semigroup`/`Bind`/`Apply` is that the former seem to be more "stable", in some vague sense of the word. By this I mean that APIs that build on top of them always tend to fall back to the same core set of type classes. However, the latter set of type classes seem to not be "stable", meaning that they are not "fixed points" (again, I'm bastardizing the term) of their own abstraction. Repeated use of them leads to increasingly more complex type classes that don't bring you back to where you started or lossy operations (like my previous discussion of non-empty list concatenation). This is the vague notion I'm trying to communicate when I say that the former type classes are better. Maybe it's not a good point, but that's my rough intuition. Also, `Monoid`s do not always necessitate a `null`. The classic example is integers under addition or multiplication, neither of which requires `null`. Yes, some `Monoid`s do require a `null`, but in those cases I believe the null is justified. That's why I mentioned `max` and `min`, both cases where I think the addition of the "`null`" is an improvement to the type. Same thing for `First` and `Last`.
I think between us he may wind up with blood poisoning.
Don't you mean addition of non-zero integers or multiplication of non-unit integers?
This syntactic convenience is one of the cute features of [Lisk](http://chrisdone.com/posts/lisk-lisp-haskell): all operators are treated as variadic functions. (= main (&gt;&gt;= get-args (. print fib read head))) This desugars to main = getArgs &gt;&gt;= (print . fib . read . head) ---- Or, sticking with plain old Haskell, you can just use lists. mconcat' (+) 0 [ 5 , 1 , 3 , 7 ] mconcat' (&gt;&gt;=) (\_ -&gt; return ()) [ m1 , m2 , mconcat' (&gt;&gt;) (\_ -&gt; return ()) [m3, m4] ] mconcat' (&lt;&gt;) mempty [] = mempty mconcat' (&lt;&gt;) mempty (x:xs) = x &lt;&gt; mconcat' xs Whoops, that recursion pattern looks familiar. I just rewrote foldr. mconcat' (&lt;&gt;) mempty = foldr (&lt;&gt;) mempty
&gt; I have not mentioned the Pointed experiment, because it is not worth mentioning anymore. It was an experiment, executed in both Scala and Haskell, and the result is conclusive. That was an abrupt change of pace. Pointed is dismissed so offhandedly, and yet, my gut instinct is to similarly dismiss Bind and Apply. At least I can write a law for Pointed. fmap f (pure x) = pure (f x) Okay, I suppose if I'm not mistaken, the following would be the laws for Apply/Bind? fmap g (af &lt;*&gt; ax) = fmap (g .) af &lt;*&gt; ax fmap g (f =&lt;&lt; mx) = (fmap g . f) =&lt;&lt; mx (The Pointed law is cooler, because `fmap` disappears, instead of being shuffled around.) ---- It is stated that &gt; Your regular old Data.Map#Map can provide an Apply instance ~~However, this is not the case, since Data.Map#Map does not have a law-abiding Functor instance~~, and Apply is defined as a subclass of Functor.
I've yet to really try it but it seems like I'd have to rework the entire chain of functions anyway, to add an 'inspected datum' to a tuple passed downstream. Unless I misunderstand! This would definitely make it easier, though.
By "inspection" I meant being able to, for example, plot a graph of the velocity rather than the displacement, without redesigning the system.
Wow, GHC is starting to be a real monster! The only thing I don't like about it is the amount of complexity all these features add, and possible incompatibility with other hypothetical compilers.
Let's just be clear that the clever stuff (e.g., generating goal information, case splitting, type-directed term synthesis) is being mediated via emacs but done by Agda. That's to say, it represents positive progress in the API offered by a language implementation, not just improved tooling on top of a bog standard batch-mode compiler. What's happening is that the design effort we put into precision in the types is not lost: rather, it significantly reduces the amount of further information we must supply to determine the program. For people whose minds process type annotations purely as passive(-aggressive?) documentation, this new purpose can take a while to sink in...
Really? Wouldn't that mean, id :: Vector m m (.) :: Vector a n -&gt; Vector m a -&gt; Vector m n Where the size index and the type parameter are in the same class of objects of the category? I don't see how this works out.
This seems semantically good indeed. Runtime properties will be worse than a plain Map depending on the usage though -- you can end up with a big map full of `Deleted` where a plain Map would have given you a small structure.
Lists are homogeneous, though.
True. Although asymptotically it looks pretty similar. `O(n log n)` for n `insert`,`delete`,and `lookup` operations. 
You understood correctly, it just less hassle than tracking how many &amp;&amp;&amp; you need around to fork, how many first you need to add. Arg, I can't even think properly how to manage that. Also, you might want to look Anima or Netwire, I was tipped off by mm_freak on \#haskell that those two libraries are better documented than yampa. 
there is a very concrete way in which, AFAIK, monoids are "superior" to semigroups though. That is, monoids have a representation theorem in the form of a variant of cayley's theorem. I don't know of any similar result for semigroups. That is perhaps just another way of saying that in some particular ways monoids are prettier, but I do think that matters. Mathematics is *about* aesthetics. Being pretty is important (although being interesting is more so). Now, of course this is silly. I find (non-commutative) near-semirings to be a very useful and important construct, thank you very much. 
For a general purpose language, you won't want unary integers either.
A `NonEmpty` emerges as just a `Cofree Maybe`. It has a fundamental reason for existence. I'm generally sympathetic to folks who don't want to care about the `Apply`/`Semiapplicative` or `Bind/`Semimonad` side of things, but the 'why not add an extra case you must consider that can't happen argument is very much the antithesis of how I try to think in Haskell. I try to be very specific in the requirements I put on a function, and moreover there _is_ no graceful law abiding continuous extension of the `Map`-like instances of `Apply` and `Bind` to have a unit. It isn't "enhanced" by adding a unit you simply lose the ability to talk about them at all. `Semigroup` lets me gracefully extend a lot of the existing machinery to just work more permissively for many usecases. instance Ord a =&gt; Semigroup (Min a) instance (Ord a, Bounded a) =&gt; Monoid (Min a) Saying that something has a semigroup means you can reduce a non-empty container with it... without having to deal with the impossible empty case at every iteration of the loop. It means that argument becomes exposed to worker/wrapper, can be unboxed, and we can `getMin . foldMap1 Min` without getting lost in the weeds dealing with cases that can never happen, like e.g. dealing with groups in the output of `group` that are present but empty. The main arguments that hold water for me is that there isn't a way for users to extend their code to work with this without backwards incompatible changes, that Haskell doesn't handle deep class hierarchies well, and so we need to make sacrifices and pick a few good nodes in the lattice of possible constraints. Those arguments have me largely convinced we shouldn't do semigroups/apply/bind today. 
When you go to implement Traversable for this you wind up with problems. Consider when you have a `Map Bool a`. You want this to really model a map from two elements to a result, but you can have to traverse three values, can't provide an index for any kind of `traverseWithIndex` functionality. Traversals of the map can distinguish between the presence or absence of a key based on whether it came from the default, you lose canonical representation... I find this much worse than just admitting that there are somethings that do not have a graceful extension to a Monad. Duality turns the argument around and say you should make sure every container has at least one element so we can always `extract`. ;)
I think this thread has finally given me a good way to express why your design process doesn't resonate with me. Unfortunately, I can't think of a better way to express it than a little exaggeration. I do not intend to ridicule you but to highlight an idea which I find ridiculous. "You know, you could really improve the API for pointers by using `[Ptr a]` instead. Now it's a monoid, and look at all the cool things we can do with it!" "`Const a` isn't a `Monad`. It has a bad interface! Just generate a free monad from it, and now it's way better."
Is this the answer? ls -lh ./bin/cube-debug.apk # =&gt; 179KB
Note that this is the same as wrapping the TMap from my earlier comment in MaybeT. 
Adding nice features to or removing cruddy features from a denotation is never (as far as I can tell so far) free. There is always a tradeoff, although sometimes not one you care about. 
Natural numbers are very common in programming as well. E.g., any kind of cardinality (CARDINAL in Modula, size_t in C/C++) is a natural number. 
Yep, thanks. GHC can do this too (I think), but the apk would probably be about 100 times as large.
Even in general purpose languages, 60% of the time, your integers are being used to represent the size of something, which is always positive or zero.
Woah, you're multiplying non-zero integers! Too complex!
What I meant was that the concept of a`NonEmptyList` was not "stable" under concatenation because there is information you lose if you make the output the same as the inputs. The result should really be a list with at least two elements. With set concatenation, there is no information that you are losing when you concatenate sets because you already didn't know the length. You already knew nothing about the length of the result when concatenating sets, so the resulting type is not "lossy" (meaning that the result type preserves as much information as it reasonably could have).
I'm not saying that `NonEmptyList` is not a useful type and I agree that it is a useful comonad. I'm just saying that the concatenation operator for it is not quite right and should not be type classed (i.e. it should be monomorphic).
&gt; Yes, some Monoids do require a null, but in those cases I believe the null is justified. That's why I mentioned max and min, both cases where I think the addition of the "null" is an improvement to the type You seem to have completely ignored the main point: This is making your type imprecise. The type of integers-and-infinity may not fit for much of your code. Various operations that would be total become partial. This will inevitably result in either implicit or explicit(Maybe) partiality where simple totality could have existed.
This was sort of one of the examples that initially bothered me about `Semigroup`, which is that in order to make it useful you have to invoke an entirely different ecosystem of machinery. For example, you can't fold something using `Semigroup` without a `NonEmptyList`. Then when you introduce `NonEmptyList` you have to introduce a `replicateA`/`replicateM` for that whose constraint becomes `Apply`/`Bind`. So you end up doubling the type class hierarchy just to add non-empty variations on everything.
Don't worry, no offense taken. :) I'm not saying that we should make everything implement every type class. I just mean that if something partially implements a type class then we should take the extra effort to complete it.
So this is sort of related to why I mention in another thread about how `Semigroup`/`Bind`/`Apply` are not "stable". Each of them preserves some concept of non-emptiness by removing the identity. However, the result is that the combining operations for each class end up being lossy in the types. If I combine two things that have 1 or more "units" then I should get something that has 2 or more things, but that information is lost in the types. I feel that in most cases where you want to encode non-emptiness the better solution is to index the length (or a minimum bound on the length) in the type and make it an indexed monoid like `Vector`. Then you get the best of both worlds: a monoid that gives you even greater precision than the non-empty type.
I'm not saying that you should not be able to define numbers without null. I'm just saying that if you do then you should not type class the combining operation for them. I'm perfectly happy with a monomorphic `max` or `min` for unlifted numbers. I agree that you lose precision by adding in the extrema. However, to me this is not a great loss. For me it would be like somebody saying "You will not let me type class my combining operation for numbers which may not be 7". I'm perfectly happy to let you define the type of numbers other than 7, and to define a combining operation for them, but I draw the line at type classing that operation and enshrining it in the type class hierarchy.
In this case I was repeating `ky3`'s own example, not making any connection to the `Monoid`/`Semigroup` debate.
Is it possible for the compiler to infer a 'lower' typeclass? For example, if you define a Monoid, a Semigroup instance is automatically generated. This way you would only have to define the 'highest' typeclass for a datatype and then provide the 'lowest' typeclass needed for a function.
Is that plotting lib online anywhere ?
What do you mean by 'unary'? Are you referring to the fact that all naturals in Agda are represented by one function (`suc`)? In the talk a special parser was imported to handle conventional syntax for naturals. Do you know whether under the hood Agda is always applying manipulations to `suc` or is there a special mode to say, "No really, in this context trust that this optimized addition operation works the same as the `suc` based one."
We still need to normalise with suc. It can be configured to compile to non-unary machine representations like Int, but that's obviously not sound in general.
Can you elaborate on how it is possible for a traversal of the map to determine if a key came from the default? Are you referring to a `Traversable` instance or a monomorphic traversal? 
Yes, as I said, it was an exaggeration. I do understand that you aren't advocating a sort of "must be able to instantiate all the things" approach. However, what you *are* advocating as a design principle is that if a type is "almost, but not quite" an interface you are already comfortable with then it needs improvement. Every feature you add to a type correspondingly removes a different (dual) feature. You add the ability to express empty lists to `NonEmptyList`; I lose the ability to ignore that case when accepting lists as arguments, even when it's totally nonsensical and my only option is to make my function partial. You add the ability to express densely populated maps; I lose the ability to traverse them without violating the model. You add a type argument to lists to make static-length vectors; I lose the ability to write a `filter` function that produces one. When you say somebody should go the extra mile to add some extra feature, you are probably also saying they should *not* support some other feature they already have. You are making arguments elsewhere in this thread that make it sound like you believe there is some mathematical inevitability that makes things that are `Monad`s automatically better than things that are `Bind`s and things that are `Applicative`s automatically better than things that are `Apply`s, but you still have not provided any explanation of why you believe this is so. This seems a fuzzy enough idea that I doubt you will be able to collect your thoughts in time to explain everything in this thread. Maybe you could write a blog post that explains the idea in detail? Just please make sure to address the things one gives up by enriching their interfaces the way you are advocating, or else you will have lost me, and I think others, again.
It is certainly heavy, but it has been necessary so far. What else do you propose to express sequences of IO operations in a purely functional non-strict language like Haskell ?
Absolutely understand why it's there, I'm asking why: - people bang on about type level separation of side-effects so much, when I think it's barely worth mentioning as an advantage - whether people are actually happy it's like this, or it's just price to be paid for the great things Haskell gives you
I understand the tradeoff between precision and generality, but that is not the point I'm making. I'm not saying that you should never implement `NonEmptyList`s and never define a concatenation operation for them. I'm just saying that you should not type class the concatenation operation. I'm very lenient regarding types, but not so lenient regarding type classes. I feel that type classes should strive to be principled, elegant, and "mathematically inevitable". Of course, even I make exception to this rule and allow type classes like `Show`, `Binary`, and `Bounded`, because I understand the practical benefit. However, with `Semigroup` I'm not as convinced about the practical benefit, and I've already made arguments to that effect. The most compelling counterargument anybody in this thread has made so far is that "I want `(&lt;&gt;)` to return an imprecise `NonEmptyList` just because", but that is not convincing enough for me. The reason I haven't written up a blog post on this is that not even I am totally convinced by my own arguments (although I am equally not convinced by opposing arguments either). I'm here to learn and discuss and your arguments don't fall on deaf ears. I just need time to digest what you say and either agree or refine my point.
Probably one of the most immediate examples is `stm`. It's important to have the code within an STM transaction invertible and repeatable and other languages struggle with that when implementing STM. Generally, the result is that you'll get either strange, unpredictable runtime behavior or a runtime error. &gt; I/O and other activities with side-effects should be avoided in transactions, since transactions will be retried. The io! macro can be used to prevent the use of an impure function in a transaction. from http://clojure.org/refs and &gt; Usage: (io! &amp; body) &gt; &gt; If an io! block occurs in a transaction, throws an IllegalStateException, else runs body in an implicit do. If the first expression in body is a literal string, will use that as the exception message. In Haskell, we can isolate IO out of STM transactions by just ensuring that the actions occur in their own STM monad and cannot be created through other means. You cannot insert an IO operation into an STM transaction—the types statically disallow it. 
IO certainly has it's warts, but it seems like any other solution of sequencing impure operations safely would almost certainly look like a monad. I'm interested to hear why you think experimentation is more difficult? Most of my Haskell experimental programs are structured exactly like they are in impure programs with a main function which prints some random values out to the screen, no monad trickery involved. main = do print mything print mything2 For debugging, if I need to "printf" values that are in the middle of pure code I'll use [traceShow](http://hackage.haskell.org/package/base-4.6.0.1/docs/Debug-Trace.html#v:traceShow) just like I would in C or Python. To me the motivating example of monads really comes from seeing how the ST monad lets you implement algorithms that are much more efficient with mutable memory. But where whole "thread" of computation cannot exchange mutable state outside of the statically enforceable `s` thread at the type level which ensures referentially transparency. 
A few observations: * Separation of side effects is not such an alien thing. Keeping the parts of a program which interact with the "real world" as separate as possible is a good idea in general, for ease of reuse / composition as well as of reasoning about / testing the code. Haskell just enforces it through the type system. * Monads are more general than IO. They can be used to capture exception handling, implicit parameter passing, logging and many other patterns in a quite elegant way; and so once you grok monads you will be able to express a lot more than IO. As a bonus, monad transformer machinery allows you to compose such side effects, giving you a layered architecture almost free. * Speaking of elegance, I find `forM_ [1..10] $ print . (3*)` a very neat way to write a for-each loop :)
 Let's look at how many representations we have just for a full map with the two keys set to the same value and see how they differ semantically. AMap [] (Just z) =&gt; (Just z,Just z) -- traverses 1 thing AMap [(False,z)] (Just z) =&gt; (Just z, Just z) -- but traverses 2 things AMap [(True,z)] (Just z) =&gt; (Just z, Just z) -- but traverses 2 things AMap [(False,z),(True,z)] =&gt; (Just z,Just z) -- but traverses 2 things AMap [(False,z),(True,z)] (Just z) =&gt; (Just z, Just z) -- but traverses 3 things Our one configuration now has 5 possible implementations. Traversing now touches an all but random number of elements betwen 1 and 3, in almost arbitrary order. The semantics of being able to delete from the map also differ from each one. You can't just let them delete from the first map and access the ambient value or now those 5 cases are further distinguished and your model has more and more junk. `Foldable` now becomes further removed from the semantic domain as well. If I write a traversal that updates the second target then I get very different results. This model has too many inhabitants to be remotely faithful to what you are trying to describe, and Foldable, Traversable, and the ability to know what the heck deletion means are far more valuable to me than getting some kind of bizarro unit bolted on to support an operation I don't need.
&gt; I feel that type classes should strive to be principled, elegant, and "mathematically inevitable". So do I. Maybe we just disagree on what a principled, elegant, inevitable type class is. I'm willing to embrace most type classes, including `Semigroup`, `Apply`, and `Bind`, that have a few simple laws, but I am likely to reject a type class without any laws at all or with a large number of laws or with laws that are complicated.
&gt; in fact they'll be so second nature you'll be confused in a language where you don't have them. A thousand times this—primarily exceptions. It is so aggravating to have exceptions being thrown every which way.
Please explain why :) I know people like it, I have never seen anything beyond "oh it's great" as an argument pro
You may want to seek out some commentary on the @libraries mailing list. Also, feel free to take a look at my own beginnings of a binding (https://github.com/tel/hs-nanomsg). It's not high enough a priority for me to complete it at this moment, but it may provide some interesting ideas to other people's bindings.
Alright, then let me just take some time to reflect on all that you have said. I appreciate all the time you took to discuss this with me. :)
- Agreed: and separation of effects is something I do anyway - as I said, I've never had a bug with unexpected IO. Having said that - perhaps a personal thing. I do see code that mixes state/algorithms lot (mostly OOP). - I know, but is it worth it for IO? - It's nice... getting less excited about syntax these days :) Making hard things easier vs easy things easier etc
Arrows are just `Strong` monads in the category of `Profunctors` from `Hask -/-&gt; Hask`, which is to say they look like functors from Hask^op * Hask -&gt; Set with a funny composition and the usual couple of monad operations, modified to fit that domain. In this setting, `return = arr`. Using `Hask` as `Set` Hask^op * Hask -&gt; Hask. If you play with a strong monad that is a profunctor from `Foo -/-&gt; Foo` then you'd get Foo^op * Foo -&gt; Hask If `Foo` was something you could fully describe as an EDSL you'd have a form of 'arrow'-like thing that fits your description in a principled way.
I think I get a bit stressed whether I'm doing things the right way. Half the Haskell code I read starts with half a dozen pragmas or abstractions over monads, like lenses.
Because separating side-effects is a good idea whatever the language. The same reasons why it is helpful to keep the database accessing code decoupled from the rest of your program in, say, C#, still apply here. Or, for the closely related issue of mutability, the same reasons why it becomes easier to reason about code if mutable state is kept to a minimum.
On the third bullet, I think the point was exactly that this _isn't_ syntax.
To complement `sclv`'s point, if you can opt-out of `IO` then you can begin to equationally reason about code, which isn't possible if functions aren't referentially transparent. Equational reasoning is something very unique to Haskell, even among functional languages, because of how Haskell delimits `IO`.
this is a good argument for semigroup, but not semi-applicative. Lots of things that are nonempty have legit full applicative instances.
 I gave a reply above citing `stm`. More generally, think of `IO` as "tagging" values which may also cause IO effects. The advantage is that when you see an untagged value you can dramatically reduce the amount of effort you spend thinking about state/side-effects/exceptions/other impurities and focus entirely on the value and the algorithms around it. I say it's *the* advantage because, n of 1, I had no idea how much time I spend thinking about and debugging unknown side effects just on the off-chance that they might exist when using normal languages. Without effect tagging my baseline stress during coding is much higher. Every change requires more effort and thought. Literally, until I began programming in Haskell I never realized how much effort I spent on that kind of stuff. Sometimes I think part of the reason why complex CS stuff happens so frequently in Haskell is because so much more of a Haskell programmer's mental energy is freed up by getting to ignore potential effects that they can use it to think through all the details of their abstractions. As an example, Tekmo above mentioned equational reasoning in Haskell. Equational reasoning is possible in any language, but you need to model all of the effects using, I dunno, Hoare Logic. It's massively more complex and thus never happens. In Haskell it's so easy that I use it constantly to explain the language even to people completely new to Haskell, functional programming, or even programming.
Typeclasses form something more like a lattice than a chain. The problem is when you have multiple superclasses. We live in an open universe where people can add new subclasses to something after the fact. Both Monad and Comonad want to give you a default definition of fmap. Which wins? It actually matters. If you define `return` and `join`, then you can't use `fmap = liftM` without spinning, but if you define `return` and `(&gt;&gt;=)` then `fmap = liftM` is kosher. Symmetrically, if you defined only `extract` and `duplicate`, `fmap = liftW` spins forever, but if you defined `extract` and `extend` then `fmap = liftW` is kosher. Now, you have 2 definitions you can infer, and based on which members of the subclasses you've implemented you should probably pick a different choice otherwise the 'help' you've provided for defining the superclass member for free has now introduced non-termination.
Oops! Yeah, you're right. :)
If we are going to add support for applicative do -- I'd love to also see support for indexed monads added. Sure, we can do that now using rebindable syntax -- but that is a big pill to swallow. 
Thanks. That description was much clearer than that arrows paper (the one describing their theoretical basis). I'm not being sarcastic.
Any comonad is going to fail to be a monoid. I kind of want to be able to keep working with comonads, and many of them admit very useful semigroup-like structure. Without e.g. `ComonadApply` basically comonads are useless for semantics. I'm not trying to foist Apply and Bind off on the rest of the community. (Except through the introduction of relevant traversals and folds in lens 4) I'm not going to try to push them into the class hierarchy. They are too disruptive, there is no code upgrade path that doesn't suck for end users. The time horizon for such a change is too long, etc. I'm just going to keep working with them, as I find they make it easier for me to write certain operations, and provide means to make code more efficient with less slop in my abstractions.
Indeed. Granted, that was just a toy example, but idiomatic monadic code need not be ugly or hard to understand; in general it is quite the opposite.
&gt; I've created a huge number of bugs over the years, but I can't recall "unexpected IO" being the cause of any of them. No, I can't say I have either, but how many times have you created bugs for code that was unnecessarily complicated. Chances are, a not insignificant proportion of the bugs you have created has been for such code. Haskell's brutal purity and uncompromising type system allows you to create rigid, descriptive interfaces, which help to create strict separation between parts. This is a hugely valuable tool when you are trying to keep things simple, and if the IO monad is the price to pay for that, then it's a good deal. Actually, I've been using Haskell for some time and I don't find IO to be much of burden. In fact, it's become second nature. So, stick at it, it gets easier.
"is it worth it for IO?" Sure, because it buys you separation of side effects (which has many pleasant consequences, as the other answers illustrate well), and the cost is surprisingly small once you get through the initial understanding barrier.
&gt; I think I get a bit stressed whether I'm doing things the right way. Half the Haskell code I read starts with half a dozen pragmas or abstractions over monads, like lenses. This is the biggest new-Haskeller dillema: expecting that your Haskell code will need to use all the features of advanced Haskell code. This is a bit like all the people who think Ruby is hard because of all the metaprogramming. I would recommend using standard Haskell98/2010 (no pragmas or extensions) and simple core libraries (base, containers, etc) and get comfortable with that. Add libraries as you need them (transformers, errors, etc). I was coding in Haskell for a year and a half before I touched lenses. Not because they're hard, but because you can't grok an entire ecosystem at once.
Yup, I'm specifically saying: why type-level separation? I try not to mix side-effects in my code anyway (in JS: flat functions, mostly stateless unless very slow; in Ruby, immutable objects unless slow). Why is the type level advantage better, when I don't 'forget' to do this anyway? I completely buy types, I make all sorts of dumb errors compilers would catch, just... not about IO.
Yeah I get it... TBH I just have stateless code, and stateful code, and I don't forget which is which. Third-party code should be a black box anyway, and is normally well tested. It's just not a problem I find myself worrying about - unlike type safety in general to catch lots of bugs. Therefore OCaml seems less effort.
Yeah, weirdly I'm ok with toy examples ;). It's more "ok this code needs to be in 3 Monads at once" because of monadic IO, and I need transformers, and lifting, and etc etc etc. You might think node's abstractions are "crazy", but believe me people find callbacks easy to write (not a good thing, sure, but true). I've met a lot of people who really, really don't want to switch them out for something as "simple" as promises.
I used to do that too. Then I worked with people who didn't care or often did forget and lead to a lot of time in code review. And then I started letting the compiler remember for me and stopped worrying about it entirely.
But in a sense I do get equational reasoning, because I know which bits of my code are doing IO. What specifically do types add above just separating IO from non-IO code? Being 100% sure about it (unsafePerformIO)?
Thanks, good to hear someone saying "yeah it's a pain, but worth it, stick with it" rather than "it's not even a problem, and it's awesome because I say it is" :) Will stick with it!
I guess parallelism/concurrency seems a good argument for it, as compiler can do more for you &amp; your code.
Now I see what you mean, in this light `AMap` seems very messy. Thank you for taking the time to write such a thorough response.
Ah, now I understand your stance more clearly. I think these classes haven't yet been experimented with enough, so it's hard to tell if they'd facilitate good code or not. I don't find the "stable/fixed point" stuff convincing. I think we just have some good ideas about good code enabled by popular classes and less knowledge about what the less popular classes would allow.
&gt; "ok this code needs to be in 3 Monads at once" because of monadic IO, and I need transformers, and lifting, and etc etc etc. Haskell doesn't let you pretend that complex code is simple, which I consider a feature. 
Well, you'd still be able to define `fmap` when defining a Monad, if you so choose. It would still help reduce boiler plate defining `fmap` in the Monad instance instead of having to type out the Functor instance separately. Though, I do see the problem with multiple superclasses defining different `fmap`s. Would it be unreasonable for the compiler to just pick one? They should be semantically equal, no?
I find it exceptionally hard to believe that you've never had a bug that was caused by some code overwriting a value that other code was expecting to stay unchanged.
I can think of cases where IO has happened at an unexpected time, but I can't think of a case where IO has happened and it shouldn't have happened at all. That's how I interpreted the quote, because that's what the IO monad in Haskell most readily prevents.
Shouldn't we try to make complex code *simpler*, not more complex? Some processes are inherently about, for instance, IO and State simultaneously .
You know, but the compiler needs some help. Optimizations like list fusion depend on knowing what code is pure.
Monadic IO was difficult when I first started learning Haskell, but it has since become second nature. The learning curve can be very deceiving. Now that I'm used to IO in Haskell, programs written in languages without explicit labeling of effects seem far more complex than necessary to me.
Ah, that I was. My mistake.
Hmmm, you're right. But maybe we could get around this with rebindable list syntax.
Nope, it is the other way around. I believe Tekmo accidentally left out the "not" in "Evaluating the `getLine` statement does *not* trigger the effect to read user input, so I can safely do code rearrangements like the above."
&gt; Why is the type level advantage better Because it's checked by the compiler, so you don't have to worry about it! &gt; I don't 'forget' to do this anyway? I completely buy types, I make all sorts of dumb errors compilers would catch, just... not about IO. OK, lucky you! I prefer to have the compiler check this for me, and so does pretty much every other Haskell programmer, otherwise they almost certainly wouldn't be using Haskell.
&gt; Third-party code should be a black box anyway I don't think you'll find a lot of support for that idea around here. Haskell programmers actually really like knowing that the library function they're using of type `Int -&gt; Bool` doesn't have any side effects.
It also makes types much more truthful. A type in Haskell that does not involve IO tells you more about the function than any type in any language with implicit IO ever could. Even in many types that do involve IO, the types tell you a more precise story, but that effect is not as strong.
Really? You've never had an exception or a crash because a function call ran out of system resources, when you didn't even know it needed any? You didn't realise it wrote a temporary file, for example. 
Additionally, many rewrite rules are incorrect in the face of effects. For example, consider the equality fmap f . fmap g == fmap (f . g) This is only true because we know f and g don't have any effects. If f and g modify global variables or are performing random IO, then we're interleaving their effects which is not in general correct. edit: added parens
Perhaps the question to ask is "Here is a simple thing I can do with IO in OCaml. Is it simple in Haskell?". The answer will probably be yes, making your other questions moot.
They are not the same line: they are the same instruction for how to retrieve line.
And those processes can be described with (StateT IO). I don't see how you could make that simpler without making something implicit, which is what I think jberryman was referring to by "pretending".
Can I encapsulate IO in Haskell? Could I write (theoretically) something like `foo :: Int -&gt; [a]` like so: foo :: Int -&gt; [a] foo 0 = [] foo x = [ unIO . readNumericInput ] : foo (x-1) (only in non-pseudocode), where readNumericInput is `IO Num` and unIO is `IO Num -&gt; Num`?
I'll explain this another way, using a quote stolen from Haskell Weekly News: &gt; shachaf: getLine :: IO String contains a String in the same way /bin/ls contains a list of files. 
I don't understand what you mean by "encapsulate IO". What is readNumericInput supposed to do?
I've certainly run out of memory before, though that could happen computing the result of a pure function. I can think of perhaps one case where a temporary file was unexpectedly used, but I wasn't really involved with it - I certainly didn't create the bug report. I don't tend to allow unexpected behaviour like this to happen, whether I have a nice type checker at my disposal or not.
I'm out of my element here. I've never fully understood Monads, or done any Haskell IO beyond printing something. ReadNumericInput would do something like `makeThisANumber =&lt;&lt; readLine`, but again in a manner that compiles and does what the names imply.
If they're about IO and State simultaneously, then it's nice to be able to have a type that actually says that: StateT MyState IO It does mean you cannot use IO actions directly in this monad, but rather you have to use "liftIO" instead. This isn't a big deal. i.e: you have to change: do mousePos &lt;- getMousePosition players.position += mousePos - oldMousePos putStrLn "Hello!" to: do mousePos &lt;- liftIO $ getMousePosition players.position += mousePos - oldMousePos liftIO $ putStrLn "Hello!" And this is a very small price to pay to have precise types that tell you a lot more about what the program is doing, and enable things that are impossible in other languages. For example, Haskell's transformers make Python-style generators [ordinary libraries](http://hackage.haskell.org/package/generator-0.5.4/docs/Control-Monad-Generator.html).
Furthermore, monad transformers allow you to write the IO and state-threading parts of the process separately (which in most cases is possible and desirable) and reunite them painlessly into the composed monad `StateT IO`.
The argument is usually that making complex code explicit *does* make it simpler. It doesn't make the act of reading a string from IO, searching non-deterministically, then throwing an exception any "simpler", but instead allows you to reason very straightforwardly about how those combine (or fail to) as something like [`ListT`](http://hackage.haskell.org/package/pipes-4.0.0/docs/Pipes.html#g:5) `IO`.
&gt; Evaluating the getLine statement **doesn't** trigger the effect to read user input ftfy; this is kind of crucial to your point.
Your unIO function is called unsafePerformIO and it has unsafe in its name for a reason.
&gt; You've never had an exception or a crash because Yes, I have those in Haskell. Exactly for the same reason; exceptions.
Maybe you never make mistakes, but I certainly do. I like the separation very much. 
Yeah, I was experimenting a lot with fast, deterministic failure and boxing out the C code. To that end, I made a lot of tradeoffs in the direction of safety that are probably unnecessary in practice. Interestingly, my code probed a few edge cases with nanomsg and found some bugs, but's probably not the best design.
Ha, exceptions. My least favourite thing about Haskell!
Ah right. You're asking about turning an `IO` action into a pure value. You can do this with the `unsafePerformIO` "escape hatch" in Haskell, but it's only really there to allow library writers to do things that they know are safe i.e. not actually externally observable.
If I'm understanding right: `foo` takes an `Int` and returns IO computation that that will yield `[a]` when evaluated. Its type would therefore have to be `Int -&gt; IO [a]`, not `Int -&gt; IO a`. (You can actually write this function really simply, too:) foo :: Int -&gt; IO [a] foo x = replicateM x readNumericInput (Also, I guess you'd have a `(Num a)` constraint in real life.)
Okay, but aren't we then were we are with any other language? If I can then have a function `foo :: Int -&gt; [a]` that uses ` unsafePerformIO`, the compiler actually *has* to parse `foo` and `unsafePerformIO` to figure out that IO takes place, and this is possible in any language in principle. If the presence of an IO type somewhere in a call stack informs the compiler of the presence of IO, then I can provide parsing information to the C compiler that allows the same (say, a list of functions (printf, scanf, ...), and ` asm`). We are getting close to resolving my confusion here, I'm exited. 
&gt; &gt; "it's not even a problem, and it's awesome because I say it is" &gt; &gt; People in this camp have simply gotten used to it, and have perhaps forgotten the pain involved in the learning process Indeed, it *is* hard to understand how to do monadic plumbing when you first come across it. I don't think, though, it's really harder than understanding other concepts that programmers seem to take for granted, such as objects/classes/multiple inheritance/virtual function calls, and pointers. 
The complexities involved: * Having both (-&gt;) and IO types, rather than just (-&gt;) types. * Having different ways to compose pieces of code based on whether they have effects or not (e.g: Use ordinary application or (&gt;&gt;=)). I think this complexity is well worth it, and isn't that big a deal. But it is more complex than just having implicit IO everywhere.
Note that making dumb errors "about IO" isn't just printing hello world or deleting the hard drive unexpectedly. It is also about modifying values that are aliased by others, for example, a common bug not necessarily attributed to "IO".
Thanks! Yeah, that's a pretty critical typo. I fixed it.
It's a good question, but we're not quite where we are with other languages. Other people here have more experience in the depths of Haskell compilers than I do, but as far as I'm aware, the optimizer doesn't treat expressions with unsafePerformIO in them specially. It's going to treat them the same as pure expressions. That means that actually, if you have expressions that use unsafePerformIO, but they may produce different results when run at different times, you have royally scuppered yourself, because the compiler is at liberty to do weird stuff. This is different than a C compiler that will treat everything as side-effecting, unless it can show otherwise.
If you use `unsafePerformIO` routinely without being extremely careful, then you are actually in a much, much *worse* position than you are with other languages. Much worse because the Haskell programming language isn't designed to make guarantees about things like order of evaluation, or even whether code is run at all, or how many times it's run, that other languages have to document carefully. So your use of `unsafePerformIO` may run at unpredictable times and wreak havoc on your program. It's *not* intended to be used as a way to just get rid of pesky `IO` types; `unsafePerformIO` is a tool to be used very carefully in situations where you have a solid proof that your code will behave the same without regard to when and how often it's run.
The great failing, perhaps is that functions like `getLine` have the type `IO String` instead of `(MonadIO m) =&gt; m String`. If it had the latter type, then `foo` would look the same no matter how many monad transformers you decided to use. But, deciding to use monad transformers is what is making the code complicated then -- not the IO model. Anyway, callbacks are easy to write. Debugging them and making them work is the hard part ;) **There is no expedient to which a man will not go to avoid the labor of thinking.** -- *Sir Joshua Reynolds* (popularized by Thomas Edison).
Just to add to this... You're worse off, so you don't do it. The language pushes you away from mistakes due to such design features, so that if you are dead set on shooting yourself in the foot, you can do it, but it's plain to see that you are doing something strange.
But I just reviewed Tarmo's paper, and the examples he gives of `ComonadZip` there all can be given a legit applicative! Similarly Dominic's examples in the codo paper. Not related directly, but I note that the 'idempotency' law in Dominic's paper corresponds somewhat to the notion of 'cartesian-ness' of a product...
Well, to put it simple, when you have a large OOP-based project in which you try to do something simple, you may get lost very quickly, since any piece of thing can go anywhere and do anything. You may very easily find yourself looping through same pieces of code by different paths. In Haskell, you only go up and down in types, so no matter how hard type is, you know that you cannot go beyond it. And it's very often I just write down the "type stack" of things I look at and "meditate" without a need to actually play / inspect behavior of code. Something like that would never be possible in more "traditional" languages.
By the way, I recently had been reading a [parer][0] on language called Disciple, which took an approach you might be interested in. Instead of putting IO and mutable data as monadic computations, they decided to keep mutable data inside a language, Disciple authors took a language which looks very similar to Haskell, and added concept of "regions" and "effects" into it. Regions are, well, regions to which you can contain zero to many variables, while effects are description on what you do on regions. And then, similarly to types, regions and effects are described inside a type, and they're also "derivable" (effects being "empty" by default (code is pure)). So, if you read this paper, you can clearly see what is a tradeoff for using monadic computational style and decide yourself which way is better and "right" for you. Hope I expressed myself not too fuzzy (since my understanding is also very-very limited). [0]: http://cs.anu.edu.au/~Ben.Lippmeier/project/thesis/thesis-lippmeier-sub.pdf
I'm a huge user of ipython notebooks, and this looks like an excellent way of playing around with haskell. Thank you!
If you use `unsafePerformIO` it means "I, the programmer, have investigated this code and can prove that while it appears there is I/O happening, the code is actually pure." For example, lets say you have an external C library, and you import a function from it: foreign import "math.h" c_sin :: CFloat -&gt; IO CFloat We know that this function is actually pure, so we can tell the compiler that information: sin :: Float -&gt; Float sin x = fromCFloat (unsafePerformIO (c_sin (toCFloat x))) `unsafePerformIO` is documenting code that requires auditing -- it's an escape hatch so you can tell the compiler things that it can't prove itself. It is *not* designed to put "read from the console" into a pure expression, and if you do this, you're opening yourself up to strange results, as the compiler is allowed to reorder pure expressions arbitrarily, in ways that may depend on optimization level and the phase of the moon. You can even end up with the same expression evaluating twice to different results, potentially stomping on its own value in memory. That is, consider this code: list :: [Int] list = unsafePerformIO readLn : [1,2,3] main = print (sum list) &gt;&gt; print (sum list) &gt;&gt; print list Passing a file containing `20\n30\n` to this program could print 26, 26, and [20,1,2,3] or 26, 36, then hang, or 36, 36, and [30,1,2,3], or any combination of those, depending how the compiler feels. You've lied to your tools--there's a reason it has `unsafe` in its name.
good point.
This is a bit off-topic, but has anyone thought of using Haskell to nudge into the zeromq / nanomsg space? Either as something similar or something wire-protocol compatible with a robust implementation. I'm not really up to speed on binding C to Haskell rather than the other way around, so I don't know if making C bindings would be worthwhile or if it would just live in the Haskell ecosystem. The more I learn about the implementation of zeromq / nanomsg, the more the thought of using them in production becomes terror-inducing.
See, the argument you are making is the exact reason why there are no numerical preludes in Haskell that I can use for my daily work. Every day I work with semirings, structures built from them, and substructures of them. People who follow your argument that more is always better are the same ones who say "commutative rings are the simplest thing I care to imagine, so let's just start there and then march boldly off towards fields and vector spaces". Many people have made numerical preludes based on this idea, and every single one of them is worthless to me. While elegance may be hard to define, it is widely agreed that there is elegance in simplicity. Semirings are simpler than rings, and therefore in some sense more elegant. When it comes to libraries, elegance is also often defined by the ability to reuse code and to expose the generalizations we weren't aware our code already supported. Semirings live at the pullback of lattices and rings, and therefore generalize order theory and arithmetic, and therefore serve to generalize and unify these two disparate fields of mathematics, allowing theorems to pass from one to the other. While I work a lot with semirings, they are not unique in their generality. The fact that intuitionistic logic generalizes classical logic by removing things like double-negation elimination is another example of focusing on generality. Classical logicians consider classical logic to be "better" because there are more theorems. Whereas I consider intuitionistic logic to be "better" because there are more intutionistic models than there are classical models; therefore, intuitionistic logic is more broadly applicable. In the real world, we don't always get double-negation elimination, nor do we get the law of the excluded middle. Nowhere in here am I aiming to change the language of mathematics. I am *embracing* the language of mathematics. But, what I take to be the core of mathematics is the logical and algebraic theories that lie at the root of mathematical thinking, as opposed to focusing on the applied and analytic side of mathematics. My general interest in simplicity and generality is often dubbed "reverse mathematics", but it is still fundamentally a branch of mathematics.
I think there's a setting you can put in the cabal file, or if not you can request that your package be marked deprecated.
&gt; This is a strange statement. I know that a function doesn't do IO if it doesn't do IO in any language. I think what's confusing you here is that you're thinking about the *implementations* of functions. If you can examine the implementation of a function, then you can tell whether it does IO or not. In the case of Haskell, however, the point is to be able to tell *without* looking at the implementation. If a function has type `Int -&gt; Bool`, then it doesn't have any observable side effects. If it has type `Int -&gt; IO Bool`, then it might have some.
&gt; I find it exceptionally hard to believe that you've never had a bug that was caused by some code overwriting a value that other code was expecting to stay unchanged. I think what's going on here is that we're equivocating over what "IO" means. In Haskell, the term "IO" encompasses memory cell mutation; to mainstream programmers this is rather unexpected and parochial. (Same with "non-termination"—to Haskellers this term includes "throws an exception" as well as the more mainstream "never returns.")
As someone that lives in IPython and GHCi, this would be great for me. I'll have to keep an eye on this project for when it gets beyond the beta stage.
If haskell produce some result, will python use it as an argument? Or vice versa?
Then let's think about how to address that, rather than disallowing `Semigroup` because someone thought of implementing `Monoid` first. 
How does it compare to http://www.reddit.com/r/haskell/comments/1d0151/haskell_in_ipython/
I'm talking about things the IO type delineates, which includes all calculations that fail to be referentially transparent for whatever reason. If someone asks why the type exists, I think any example of bugs it prevents is useful.
It's answered here already but most of the answers require an intermediate/advanced understanding of haskell/programming to totally understand. It's not whether or not monadic IO is "worth it"; monadic IO is a solution to a problem with functional programming. To understand the problem, you need to understand the motivation. The motivation for having a pure language, ie a purely functional, no side effect language is that you can't do IO at all! A prerequisite to a language to be "pure" is that if you call a function with the same parameters you will always get the same result. People want this because (especially with large applications) it makes it much easier to reason about and debug complex programs. But, when you do var user_input = get_user_input(); get_user_input doesn't always give you the same thing! Maybe she gives you a number, or a string, or nothing, or the pipe breaks and instead of a string it returns an error; all things that make a strongly-typed language freak the heck out. In haskell you don't get a string back from getLine; you get an IO String. The IO part handles all the potential errors and what not that the complexity of interacting with the outside world brings; while allowing you to still do things like concat two IO Strings together Monads in general (that is, Haskell's monads, not the ones from category theory) are an invention that allows us to write pure code in a computational context; which is basically just a fancy way of saying you can write code that acts on something that is happening *inside* something else. So making IO a monad is actually not a feature in and of itself, but a solution to a problem. The tl;dr version of the answer to your question is, "it lets the whole language be pure"
My rule doesn't apply to less principled type classes like `Num`. However, even if you restrict it to the semiring operations I am still not sure. I don't claim to understand the benefits of the various ring -like abstractions.
&gt; It must be working!" is actually not so sarcastic in Haskell; a lot of the time, this is the case. Anyone making this kind of claim hasn't written a lot of Haskell. I'd love to live in a world where type checking is isomorphic to correctness, but while Haskell gets us one step closer by making sure your program is sound from a type standpoint, the logic of the code you write is yours and yours alone, and you will mess it up, like all other humans. 
Why treat it as a "rule" then? What makes you think you understand the other abstractions that you're arguing against? Is `Group` principled enough? Because that's all that I need for my `Natural`-to-`Integer` point.
While I don't necessarily know what the right type class granularity should be, I don't believe the correct answer is "as granular as possible".
Ok, then would you allow me to go back and refine my original argument? I will discuss the two types and concatenation operators that have been interleaved throughout our discussion: * NonEmptyLists, with `(&lt;&gt;)` as non-empty list concatenation * Integers, with `(&lt;&gt;)` as `max` I've already suggested an improved interface to the `NonEmptyList` type: Vector m a -- where `m` is a minimum bound on the length According to `kamatsu` this is an "indexed monoid". I don't personally know if that's the right term, but I will just use that term for now unless you know another name which is more correct. However, I buy your argument about the lifted `max` losing precision, so I will use the same indexed monoid trick to refine my original proposal, using something like the following GADT and hypothetical type-level booleans: data Maximum b a where Val :: a -&gt; Maximum 'True a PositiveInfinity :: Maximum 'False a ... with the following signature for the `max` operation which uses hypothetical type-level boolean logic: max :: (Ord a) =&gt; Maximum b1 a -&gt; Maximum b2 a -&gt; Maximum (b1 '&amp;&amp; b2) a ... and `PositiveInfinity` is the `mempty`. **Edit: wait this is wrong and I have it backwards. I retract this argument for now, but give me a second to rethink it because I still think it might be on the right track.** The idea is that the type-level Bool keeps track of whether we have lifted the numbers yet. This gives you the best of both worlds because now it is an indexed monoid (and therefore makes me happy), and also allows you to specify in the types that you don't want a lifted value by constraining the type-level bool to `'True`, which makes you happy. In other words, the refinement of my argument is that indexed monoids with dependent types are preferable to semigroups since you get the extra identity laws, but without losing precision.
Ok, so I revive my last argument and I think these are the type signatures I want for `max` and the `Maximum` type: data Minimum b a where Val :: a -&gt; Minimum 'False a PositiveInfinity :: Minimum 'True a min :: (Ord a) =&gt; Minimum b1 a -&gt; Minimum b2 a -&gt; Minimum (b1 '&amp;&amp; b2) a min PositiveInfinity x = x min x PositiveInfinity = x min (Val x) (Val y) = Data.Ord.min x y mempty = PositiveInfinity So the argument I make is still going to be: indexed monoids replace semigroups. **Edit: Oops. I meant `min`, not `max`, so I just switched it, but still the same argument.**
I've thought about this more, and really think it is just that you can't traverse infinite collections--there is no traversable instance for `(-&gt;) k`. My applicative map is really just an efficent implementation of `Map k a = k -&gt; (Maybe a)`. (EDIT: or not exactly since it can't represent all such functions, but only those that only perform a bounded number of observations on the input). If you know that `k` is finite, than you can easily produce the correct instance of `Traversable`--including in the `Map Bool` instance.
Indexing a type by a `Bool` creates two types. And in the process, you're adding massive complexity to be able to state what I already could with a simple `Maybe` and another typeclass. Why go to all this effort? After a day of arguing, you still can only point to nebulous "Semigroup feels incomplete and inelegant"-style arguments. That's literally the best I've seen in your dozens of responses to this thread. Can you take a step back and look at the straws you're grasping at to avoid rethinking your initial impression? If nothing else, `Semigroup`'s advantage is _not forcing you to write the kinds of code you just wrote in the parent comment_! :) I'm not responding to your specific proposal because you never addressed my other point about the type of `Vector` being too precise, and forcing you to wrap it in a `Sigma` if you want to write anything that isn't a trivial combination of its input parameter types. If I were to respond again, I'd respond with most of the same points.
And to reiterate yet another point I have made at another point on here, and one that you've made too: more precision is not always better! Even in languages like Agda where I go out of my way to prove complex properties about my programs, I still find uses for `Vec`, `List`, and `List+`, which is the Agda std lib's name for `NonEmptyList`. Different types for different needs, and just because you feel that some information is lost in the type of `NonEmptyList` doesn't mean that information is something we care to preserve. If we do, we move to `Vec`, and if not, we stick to `NonEmptyList`. Adding additional precision at the type level leads to all sorts of pain around existentials when you don't need it.
Whose `Vector` type is that? It's not one I've seen proposed, and it's not the standard one. If it's a lower bound, then sure it works for this case. What's an inductive definition of that type, out of curiosity?
I think the way to define it is to build it out of the classical vector type. To avoid ambiguity, I will use `Vector` to refer to the one with the exact length index, and use `VectorMin` to refer to the one with the lower bound. So I think it would be defined using an existential quantification: VectorMin n a = exists (m :: Nat) . Vector (n + m) a
Ah, I was curious if there was an inductive definition of it. You can do one for upper bounds fairly easily (a similar construction to `Fin`) but I hadn't seen one for lower bounds. That `exists` is exactly the `Sigma` I keep referring to, for what it's worth :)
Ah, okay. Now I understand what you mean by `Sigma`. :)
I'd be fine with ido as well. The marriage syntax...
This looks amazing and I'd love to get my hands dirty with it but after I install and run "IHaskell setup", trying to start it with IHaskell notebook or IHaskell console fails with a traceback. ipython runs fine.
I'd be happy to help you get to the bottom of this if you'd like to. Send me an email (see site) or put up a github issue. The following might be helpful: The *.py files in your ~/.config/ipython/profile_haskell or ~/.ipython/profile_haskell directories, if they exist. Your OS and version The IPython version (this requires 1.0, I believe) The traceback you see when runing 'IHaskell setup'. I'd really appreciate your help with this :) 
(Brain dump upcoming) One other definition of it could be data Vector! :: * -&gt; Nat -&gt; * where Rest :: [a] -&gt; Vector! a 0 Cons :: a -&gt; Vector! a n -&gt; Vector! a (1 + n) which you could unroll into data Vector! :: * -&gt; Nat -&gt; * where Nil :: Vector! a 0 Cons1 :: a -&gt; Vector! a 0 -&gt; Vector! a 0 Cons2 :: a -&gt; Vector! a n -&gt; Vector! a (1 + n) And then possibly factor into data Rel :: Nat -&gt; Nat -&gt; * where Zero :: Rel 0 0 Grow :: Rel n (1 + n) data Vector! :: * -&gt; Nat -&gt; * where Nil :: Vector! a 0 Cons :: a -&gt; Rel n m -&gt; Vector! a n -&gt; Vector! a m You could probably keep shuffling it around to get the reflexive transitive closure (free category) of `Rel` (which is just `&lt;=`) and pull out the obviously correct thing, but my Nyquil is kicking in and I can't think straight anymore :(
I am, slowly
Ok, fair enough. I did have a brief look at those two libraries as well. Not sure why Yampa stood out to me (EDIT: Oh, [I remember](https://www.google.com/search?q=haskell+simulink)), but none of them seems to have extensive documentation :P. There must be some way to nicely encapsulate getting output data from inside a wire. I mean, at the very least, you could have a wire that puts its contents through `unsafePerformIO` :P.
You can easily do it from the control panel in Hackage 2.
Actually, IO in Haskell is referentially transparent. When you call the same function multiple times, it'll return the same value of type IO. What happens when that IO actually takes place is outside the scope of the language (it's handed off to the run-time system).
What do you like about ipython that's missing in ghci?
Good luck to you! You will never be bored
[Larry Drebes of Janrain](http://janrain.com/blog/functional-programming-social-web/) illustrates the value of sequestering IO in this blog post. "Thus the code snippet includes data structures, concurrency, and fault tolerance in just two lines of code."
It reminds me my first time working with IO, I used unsafeIO to update/read a nosql database with pure function... Very funny behaviours : a quick way to see the usefulness of defining pure function and why 'unsafeIO' could be unsafe. A nice one to try, really, especially if you like learning from error.
But that's not really `ST`'s role, is it? `ST` handles mutable references, whereas OP was referring specifically to IO effects.
Cokleisli composition gives us a Monoid, but that's like saying every a is a Monoid because Endo a is a Monoid. The issue I'm referring to here is the absurdity of extract mempty :: a
First, make sure not to conflate the concepts of monads and IO. Second, the Haskell type IO probably doesn't mean what you think IO means. I apologise for being terse, but I'm on mobile right now. Keeping the above in mind, IO is Haskell's solution to the problem of how to do side-effects in a purely functional language. It is not perfect and it *is* complicated because it lumps all side-effects under one type. However, it is less complicated than a language which doesn't make the distinction at all; in such languages, composability is severely hindered because the language makes no guarantee that the behaviour of a procedure matches its type at all. In Haskell functions that do not mention IO can be safely composed. If they can't eg. due to misuse of unsafePerformIO, *the function is broken in the worst way possible and the author deserves public shaming*. 
That is ST's role. That's why it originally produced. In fact, under the hood of GHC, IO is ST parameterized over a RealWorld state token. 
Here's a situation that actually pops up a lot in imperative languages: &gt; Does the library function `Data.HashMap.insert` change the existing hash table, or does it create a new one? and variations. Functional programmers find that this sort of nuance is usually not worth keeping in mind -- useless complexity. Haskell's type system enforces that data structures are always copied, never mutated. (Which actually makes copying more efficient, because now you can share large parts of the data structures.)
I thought of this more as a factual statement. Marked the let me tell you part in strikeout now to make it clearer.
I would like to write an addendum to what type classes I'm willing to embrace. The explanation I gave you is basically my old canned answer to the question "when is it worth overloading some functionality with a type class?", but I realized this morning that I have another reason that I sometimes use type classes. Sometimes, even just the parametricity gained by writing polymorphic code instead of monomorphic code is enough to justify the use of a type class.
Third party code should be a black box, but if it is, how can you know for sure that it's not going to interact with something else without you knowing? Besides, monadic IO isn't there so you can tag functions with side effects. It's a way of interacting with the real world in a language that has no concept of state or ordering - instead, you build up a plan to compute something (IO a) in a pure way, with abstractions such as Monad and Applicative, then give it to the runtime to execute the plan by setting main to complete plan. 
Indeed, but we, and a lot more than just us, could be doing our dayjobs in Haskell as well. Haskell is often well suited. But all of the FP literature (and a lot of the packages) is focused elsewhere.
I think you're right here, but you've phrased it in a way that a lot of Haskellers will object to. Let me explain. I couldn't do my day job without postgresql-simple so I'm very glad this IO-directed piece of infrastructure exists. By all means let's improve it further and write even more such packages. However, for said day job the database IO that I do is encoded entirely behind a declarative relational query API (somewhat like HaskellDB). So yes, you're right. To "make it big" Haskell needs more packages that interface with the outside world, and that implies IO. However, what I think Haskellers will object to is that you haven't given due credit to Haskell's marvellous declarative interfaces that *hide* the nitty-gritty of IO. 
Now it just looks really weird if you don't know what happened there. Why not get rid of it?
Ah, sorry, I tried to convey this with "we need to focus on elegant solutions to real world inputs and outputs". Should perhaps have added that this would be playing to one of Haskell's strengths.
Done
&gt; While there is an interesting “pure” core of business logic in there somewhere, it is dwarfed by the amount of I/O-related code. Look at a typical mobile app, how many lines of code should have been separated into pure modules? Not sure I agree here. Have you seen XMonad? Sure, it has lots of mundane plumbing to actually talk to X11, but it has a beautiful pure model around which the entire program is based. Most IO focused apps you can think of do have this pure model, but it's rarely made explicit.
Well, I do not particularly think that IO programming is a big drawback in haskell. In fact, to do a better IO programming, one of the most important consideration in design is what is describable in a pure and declarative way and what kind of side effects/user operations to be dealt with and how to manage them in a maintainable way. Haskell tends to give a great power in making abstraction about how to organize discovered pattern of managing side effects. Okay, I could say my first-hand experience in making hoodle (note taking program : http://ianwookim.org/hoodle) . I participated in developing xournal which is a program of the same kind written in C. It was using GTK and as usual, GTK has lots of callbacks for each IO operation gateway (i.e. buttons, menus and all user interface things). Programming only inside callback functions is a great pain since it clutters my mental design about the flow of program logic. It's often said 'inversion of control.' I was seeking for a good solution to this. At the time I decided to make my own version of xournal in my favorite language haskell, the iteratee, conduit, coroutines was big (and still big now continuing with free monads and pipes) in haskell community, which apparently seemed quite academical in many aspects. But it turned out that following approaches for having a correct abstraction of data flow streaming in haskell community was insightful for me to separation of UI and direct event handling from program/business logic in terms of semi-pure operations. This 'free' monad abstraction that's actively discussed in haskell community gave me a direct idea about how to 'reinvert' the inverted control and it was of great use in designing underlying event handling system in hoodle which became a semi-large project. Carefully thinking about control structure and necessarily purifying a part of components benefits a lot continuously in the long run. Seemingly academic topics being discussed in haskell context is actually of great practical use, particularly for a very IO intensive process. In the end, it's a discussion about programming, and programmers value practicality in any world. 
Well, to be clear, I definitely don't mean that this happens all the time. Obviously logic is the crux of programming and if you get it wrong, you're going to have issues. But, there have been plenty of times when my code has compiled and run correctly in Haskell. When using another language like C++, for example, it's relatively common to run a program and hit a random segmentation fault or something. This has rarely happened to me using Haskell, and I have used a good deal of it. 
What's the straw man?
Any potential synergies with the MirageOS project (http://www.xenproject.org/developers/teams/mirage-os.html)?
&gt; This post is about another, often overlooked, fundamental reason: Most programs are mostly about I/O This is not often overlooked at all. It *always* comes up.
I've always felt like that parsec's error messages were pretty solid when you make good use of the &lt;?&gt; combinator. But yeah I'll definitely take a look at some examples using trifecta, thanks!
ST handles effects which *can* be purified. Many IO effects cannot be compartmentalized so they cannot be in ST.
Without weakening your point at all, the NONINLINABLE pragma opens up windows for slightly less pure code being predictable under unsafePerformIO.
Here be dragons.
No doubt. Bigger dragons than even `unsafePerformIO` itself.
Can you `:set` pragmas in IPython somehow?
The idea is that instead of existentially quantifying the parameter and making it an instance of Monoid, you make it an indexed monoid (I'm not totally sure that is the right word), where min is the indexed mappend and PositiveInfinity is the indexed mempty.
If you write your parser in terms of the [parsers](http://hackage.haskell.org/package/parsers) package it will work with both Parsec and Trifecta (even ReadP!).
Better transfer ownership so the better library can use the better name?
Just ordered this book, it looks really interesting.
The comments here already (“the point they are addressing is completely irrelevant”, “I'm not sure the type of program being discussed in the article actually exists”) illustrate this perfectly: IO intensive systems don't exist, and if they do they don't matter. Perfect. 
Like the analogy :) And I agree with everything except that the IO part is very small. In a lot of common areas, at least.
Graph support is currently non-existent, but planned and possible to implement. The kernel for IPython can give IPython a "display_data" message with many different representations of the same data, including text, HTML, PNG, SVG, etc. So if a command returns some plot, the display_data message can pass along both the text output and the image data, and IPython can decide which to use (and in the notebook, it will use the plot). On the IHaskell side, it's not a terribly difficult exercise to add this, though I haven't yet figured out how to separate out these "display_data" representation plugins from the main code base to keep things modular and easy to extend. Do you have suggestions for which Haskell graphing API to target? Is there a standard one everyone uses?
I wrote exactly what I meant, and very carefully. I didn't say IO was referentially transparent or not. I said that the calculation represented by the IO value was not referentially transparent. I know what I'm talking about.
This is very similar in goals to ihaskell-notebook, and was actually inspired by it. However, when I looked at ihaskell-notebook, it just piped things to GHCi and was generally difficult to use. On the other hand, IHaskell uses the GHC api, so should be more stable (since you're not piping text data between a backgrounded REPL and trying to parse the result) and easier to extend (query GHC api for type data about bound identifiers, etc). I did briefly go down the "embed GHCi" route, but found that it was very fragile. That said, I don't mean to rag on ihaskell-notebook: it was awesome that someone did it first, and it was what got me to write IHaskell, which I think is an improvement.
Can you reformulate that in terms of burritos?
You're right, thanks. Fixed :)
Point well taken, great answer. Have an upvote. Hope we will soon be past the stage where one has to accept or anticipate doing some library work to get ones task done in Haskell.
This doesn't provide any mechanism for Haskell-Python interop. This simply uses IPython's frontends to work with Haskell, because although IPython has "Python" in the name, the protocols and interfaces are meant to work with many languages.
This may be my new favorite type-driven design metaphor.
I realize that the author is criticizing something a speaker said about the role of IO in the design of software. However, when I read articles like this it seems there is an unspoken implication that Haskell is not suitable for programs that consist primarily of IO. I don't understand this... even if a program was 95% IO operations it would benefit from type-level guarantees about *which* 5% could not perform IO (and I would be surprised to find a program that could not be distilled beyond this). Actually, it seems to me that the more complex your IO the more you gain from Haskell's type system. Consider the difference between the types `[Int]`, `[IO Int]`, and `IO [Int]`. One is a pure list, one is a pure list of not-yet-performed IO computations, and the last is an IO computation that will output a list. They have completely different semantics, but the "corporate" languages that the author mentioned do not distinguish between these at the type level. You just have a function returning something like `[Int]` (if the language has even that much compile-time information) and you must depend on documentation to infer if and when it performs IO. As you build larger and larger abstractions it becomes very difficult to remember exactly when each effect takes place. Are there specific reasons why people think Haskell is less suitable for IO, aside from the learning curve associated with monads?
 rsps &lt;- mapM ( proxyHTTPWrap hp ) reqs mapM takeMVar rsps Can be one line: mapM takeMVar =&lt;&lt; mapM (proxyHTTPWrap hp) reqs
This post illustrates the advertised failure mode of Haskell advocates perfectly.
No but I can think of a way to reformulate it in terms of Doritos.
I think “I/O” is the wrong name for what the author is talking about. It's really *integration*. Many “corporate” systems are all about make the right responses in the right order (whatever that means today) on a bunch of heterogenous, unreliable downstream transports, triggered by badly formatted, out–of–sequence, repeated–with–some–missing requests on a bunch of other heterogeneous, unreliable upstream transports, reconciling against various other data sources (can you guess what they are like?), and having a human who thinks they know better diving in to make some manual adjustments half way through. The actual *transformation* might not be much richer than adding up a list of numbers, or replacing a key with its value. It's the integration that really adds value. Knowing that the function that adds up the list of numbers definitely doesn't do any sneaky I/O as a side effect is nice, I suppose, but doesn't speak to the main issue.
Finding "pure" core (or planning one) may be too complex for average programmers. And changing requirements may outdate "pure" core very fast. X11 is fairly constant protocol, but, for example, in mobile or gamedev things do change quite often.
I get that, but my point is that I'm almost going going to end up hiding that parameter *in practice* anyway. The index just makes it harder to use.
I can't really respond here without a concrete example that illustrates your point. I've written pure kernels in almost all haskell applications I write, and requirements change pretty quick in some of those applications.
I'm not sure he thinks Haskell is unsuited, it looks more like he wants more packages/libraries for different kinds of integration.
&gt; The point of the language Haskell is that it makes creating languages easy. you say (SPJ says :p) Haskell will never become mainstream. But there may exist such a DSL on top of Haskell, that makes creating I\O programs easier! Don't you think, that the latter is called "Haskell" in the OP's post? 
I disagree strongly that most programs are mostly I/O. You might perceive it as I/O but most of the code is serialization and deserialization of various formats, which can (and should) be expressed as pure code.
If you're interpreting the same program multiple times, wouldn't you just want to parse it once and run it multiple times?? do let foo = parse "s" foo; foo; foo If you really want to compile an IO action, the short answer is you can't do that. 
That's not news. Twenty years ago people were building C++ apps with a thin layer of transducers around the outside of a business–logic core that did no I/O, and it wasn't news then. And this is why I think “I/O” was a poor choice of terminology in the original post. What Haskell advocates aren't demonstrating is why and how the features of Haskell make exactly the problem of reordering of data, handling unreliability upstream and downstream, and reconciling and reformatting data correctly in the face of frequent and arbitrary changes to all of those things by third parties a quicker, cheaper endeavour for mainstream programmers to do. This shouldn't be hard because the current approaches are terribly slow and awfully expensive. But I don't see it.
Actually, a lot of effort is put in to trimming down GHC, from what I can tell. There doesn't seem to be too much accumulated cruft. When the new type checker was put in, it was a glorious breath of fresh air. Similarly the backends get a lot of helpful refactoring too. 
Comprehensions already extend to other data types. turn on MonadComprehensions or even RebindableSyntax :)
Hah, no, I meant like at different times. On a different day, after a reboot, on a different machine... It also doesn't need to be a high performance compilation. Just some kind of suspended Haskell runtime exported and wrapped in an executable. 
Spot on, there is a large group of programs looking like this. However, we would hope to not have to write the serialization ourselves. Since most data commonly encountered is in a somewhat standard format, we will use a (pure) library for this. Which leaves us with...
Thank you both, you are correct, I should have made this more clear. I/O in terms of FP has a much narrower definition than what I was aiming for.
That would be excellent! This is a really cool project.
Yet you take away SQL or JQuery or LINQ or their BigData mapreduce platforms from those imperative IO coding enterprise developers, and they'll scream bloody murder :)) Functional programming brings an incredibly powerful data slice'n'dice toolset to mainstream programmers. Something they love and cherish in specialized niches (like sql interfacing with databases or Jquery for manipulating the DOM). And it does not even have to be a completely separate language. Most likely large scale functional programming will happen when the mainstream languages like c# and java evolve under the pressure of modern requirements. 
This sounds similar to what you are talking about: http://hackage.haskell.org/package/Workflow *Transparent support for interruptible computations. A workflow can be seen as a persistent thread that executes a monadic computation. Therefore, it can be used in very time consuming computations such are CPU intensive calculations or procedures that are most of the time waiting for the action of a process or an user, that are prone to comunication failures, timeouts or shutdowns. It also can be used if you like to restart your program at the point where the user left it last time.* *The computation can be restarted at the interrupted point thanks to its logged state in permanent storage. The thread state is located in files by default. It can be moved and continued in another computer. Besides that, the package also provides other higher level services associated to workflows: Workflow patterns, and a general configuarion utility, workflow observation events and references to the internal state. The state can be stored maintaining memory references (using the RefSerialize package), so that it is possible to track the modifications of a big structure (for example a document) along the workflow execution.* However it's probably more complex to do it that way than to just write some code to serialize your state after you've finished parsing. Then you can just reload that state to generate whatever data you wanted to, without worrying about the parse step.
Yes, I think the author is arguing that the Haskell community emphasizes the wrong things. I'm not disagreeing so much as bringing up a different yet related issue.
That's good to hear! On the other hand, just the sheer amount of language extensions seems to imply a lot of complexity.
Our company is developing an online game in ActionScript (client) and Java (server). And I often think about benefits of using Haskell instead. * Server - We do use ORM for two reasons: it is very easy to develop DB structure by parts and it will be easy to switch from tables to NoSQL when we go highload (reason to use tables - because we don't know NoSQL). Can Haskell meet such requirements? - adding new "features" was very easy. Second level cache, logging framework, connection pool, load balancer - all require lots of I\O but no one in the company except of me knows that that may be much more complicated in Haskell because of possible core rewritings. - there was, in fact, "pure" abstract core in our server. But then we had added new features, say "produce some game items in chain", "minimize number of player loads from DB", "add support for multitransactional actions", and our core has fouled with hacks. * Client - We DO change features in-game while development. That is, today we are planning RPG system, but tomorrow we have to abandon it and push more efforts to alliance system, because of deadline. Today we are using hardcoded constants, then we are loading constants from server, tomorrow we are using local constants from file, day after tomorrow we are using game update system for constants. Today we need easy login way, tomorrow - complicated and secure. Today we are using simple textures for dialogs, tomorrow we need to minimize graphics space with 9-grid. Today we are developing for one screen resolution, tomorrow we are patching whole game for multiresolution support. - We DO change dialogs and dialog chains often. We DO change interface (we've done in three times, complete change). And we need hackable controls, because we often need custom logic for standard (as we thought) items. And we will change them again, after feedback from players. We rely much on subtyping mechanism and OO abstractions\patterns, and I do not of any functional patterns sutable is this situations. * Too complex - I'm not BOS. I'm an average programmer with 3-4 years of commercial job experience. I'm young, and so are all in my company (except of team lead). We do know only what we're reading in popular resources and rely much on StackOverflow. We use abstractions, that are popular and we use packages that are known to be success. We have no time to develop new abstractions, because we've promised to launch game 8 months before! Now, that our game is very close to be finished, I see some "core" principles, that can be abstracted (and they will be in our next game), but doing that may be a failure - it is too late and what if your game will not be successful? Just waste of money and time.
What is a calculation, and what does it mean for a calculation to be or not be referentially transparent?
Well no, doing so would require serializing everything interesting about the state of the program. A more feasible solution to avoid reparsing would be to just dump the AST into a format specifically designed to be faster to parse. Or just write a compiler. If you compile to some custom bytecode, just dump that and write an interpreter for that instead of directly walking the AST or whatever.
Depending on the types involved, there are hacks you can do with template haskell to acheive this.
I'm not sure any of us are quite getting your spec here. Could you show an example? My initial reaction is that it sounds like you're really trying to generate a new Haskell program, which can then be compiled. For that you've got the haskell-src-exts package or Template Haskell, depending on what you're trying to do. But I can come up with some other interpretations of what you're trying to do, too. Various runtimes over the years have tried the "freeze the program's state and restart it later"; none of them that I know of have ever had any great success. Perl, for instance, used to be able to "dump core" and resume later, but it was always too funky to work. While Haskell does have enough IO isolation that perhaps it could be feasibly used to overcome the problem (in much the same way software STM works in Haskell and very nearly nowhere else), it's not off-the-shelf code if you want to touch any system resources like files or sockets, as far as I know.
No, my issue is with the very premise of the OP's post: &gt; Most programs are mostly about I/O I claim that programs are fundamentally unable to "be about I/O" -- programs are always about manipulating data. The `IO` monad is just one tool among many. It's a means, not an end. Of course, the OP probably means &gt; Most programs are about things that businesses care about, which often involves plumbing together existing solutions. which is true if somewhat tautological. The fallacy is to reformulate this as "programs are about I/O" and link this to a seeming inapplicability of functional/declarative programming to these problems, "because functional programming can't do I/O". The latter assumption is neither true nor relevant. (Haskell can't do I/O? Pfft. /u/augustss has [embedded BASIC as a language inside Haskell][1]) [1]: http://augustss.blogspot.de/2009/02/more-basic-not-that-anybody-should-care.html
&gt; The fallacy is to reformulate this as "programs are about I/O" and link this to a seeming inapplicability of functional/declarative programming to these problems, "because functional programming can't do I/O". On the contrary, the post is a call to action on developing better libraries for integration and I/O, and focusing on these when we evangelize about FP. I somewhat criticise the culture, not the Haskell language.
I upgraded to XCode 5 a couple weeks back and it broke my haskell pipeline. I installed an Ubuntu 13.10 vm with VirtualBox and do all my haskell dev there instead.
Basically, Mavericks removes the old gcc4.2 and replaces it with a version of clang that has a similar frontend to gcc (try gcc --version). This decision has caused a couple different programs to fail to build. GHC is working on a patch to fix their LLVM support, but it isn't quite ready yet. In the meantime the homebrew people have proposed adding a dependency on apple-gcc42 to make it work for GHC 7.6.3. GHC HEAD is supposed to work with clang, if you want to try it out.
&gt;this means that the complexity of counting the neighbors of cell is 8 * O(n) You know, it's not how O(n) works. It's still O(n).
We really need a GHC 7.6.4 release that's compatible with XCode 5. A ticket about the issue: http://ghc.haskell.org/trac/ghc/ticket/8197
Yes, I will unfortunately have to do this given the timing of any new HP release coupled with the Mavericks release. But I have many other things to do too. I may get to it by next week (the biggest limitation is I don't have a Mavericks machine at this exact second.) I hope Apple stops changing crazy amounts of shit in the future (which *always* breaks dozens of minor things,) but given their track record of changing all kinds of stuff for the past few versions of OS X, I won't hold my breath.
Whoah no, I only found some Github repositories that were 2-3 years old. Thanks for that!
You have to rebind for qwerty keyboards out of the box? That does not seem like a sensible default configuration if you want people to try your game.
And now we wait to see which Haskell performance guru tackles this question. :)
alpha-2 will feature an interface for rebinding keys. It's not something I'll be looking at immediately myself, but patches for something like a command line option to switch to qwerty or something similar are of course welcome.
Thank you. That's probably what I was looking for, a way to serialize state and load it in a runner later on. The Workflow package seems to be a good implementation of this, but indeed rather heavyweight. Can I just serialize an `IO ()`and load with some other program later? Keeping references to files or sockets, and threads etc won't be a problem. The interpreted program is intended to be isolated.
Huh? A Haskell program is basically just a giant expression that evaluates down to a single value of type IO (). This value is then passed to the runtime system which executes the side effects described within. In some ways, Haskell can be thought of as just an elaborate macro language for generating imperative programs.
I really enjoy Bird's work. If you want a short digestible example of the kind of thing you will find in that book. * [algebraic identities (pdf)](http://comjnl.oxfordjournals.org/content/32/2/122.full.pdf) and a list of his other publications * [list](http://www.informatik.uni-trier.de/~ley/pers/hd/b/Bird:Richard_S=.html) and * [the sudoku discussion here on /r/haskell](http://www.reddit.com/r/haskell/comments/8ye59/functional_pearl_a_program_to_solve_sudoku_pdf/) 
Isn't it simplified to O(n)?
I actually think IO in Haskell is still much nicer than in any other language I have used. Even in an IO-riddled function, the vast majority of it is still just plain function applications, totally pure, and I still benefit from that, even though the top level signature for the function mentions IO. 
While not exactly the same, you can use the amqp package and RabbitMQ. It works perfectly in production. I have built a high performance RPC wrapper on top of the amqp package, works like a charm.
Unfortunately I don't really know which is the best or standard. On Hackage these three have 200+ downloads and appear to have adequate features: 1. http://hackage.haskell.org/package/plot 2. http://hackage.haskell.org/package/Chart 3. http://hackage.haskell.org/package/gnuplot The gnuplot one probably has the most features since it's just a wrapper to gnuplot. I've never tried to use any of these in anger, however.
Thanks for the info. I really need to make time to play with this.
http://hackage.haskell.org/package/Chart-diagrams seems the obvious choice (after thinking about it for all of 5 seconds) since I assume that there's a simple way to get SVG display in the notebook.
I upgraded, and I've made sure to _not_ upgrade to Xcode 5. (Thanks to all the people who have warned about that!) I haven't noticed any problems with GHC yet.
I like variadic functions of Lisp-like languages and their convenience. Sadly as another comment said, you can't do that with list since they are homogeneous. I think the "do-many" notation as a very simple macro expansion. I don't know if there are any conflicts though. 
&gt; The correct way to describe the cost would be O(8n), actually. Ehh.... but O(n) == O(8n). I don't think the latter is "illegal", but it's certainly unnecessary complexity, like claiming the area of a circle is pi\*r^1+1 . Yes, correct, but.... Generally speaking you can certainly expect to be marked wrong on a test if you claim something is in O(8n), and it would be a correct marking.
&gt; Unnecessary complexity In some cases I'd agree - but I feel in others (like now) it gives a better feel. O(n) feels almost deceitful. But my understanding of O notation is self-taught, I'm sure there are rules I'm violating and I'm in no position to argue with you.
Came here hoping to see comonads, zippers, and infinite grids; was only slightly disappointed, but very intrigued.
I just updated the article. Thanks for the assist!
That's some really useful example code!
Thanks for the explanation, I guess I need to go read up some more on the topic before I open my big mouth next time! EDIT: Although I still like the O(8n) notation here. I'm in the same boat as PokerPirate, I prefer to "completely unhide the constant factor .. When I'm talking to other people about implementing algorithms efficiently".
If you want to talk about the constant factors then you probably really should be talking about specific measurements of time, not "numbers of operations" or something.
&gt;Compilation time seemed somewhere between 1.5 and 8 times slower Glancing at the paper, they say that they did not optimize compile time. 80% of compilation time is spent simplifying expressions by repeatedly applying the same function until it has no effect. They believe that this can be reduced to a single pass. 
Could you have some `Show` like class that could be used to tap into IPythons `display_data`? This way it would be easy to experiment with different graphing/graphics tools they could be implemented as Haskell libraries.
what other data apart from vector can do the job?
I just upgraded and things seem to be working fine after installing a patched GHC from https://github.com/darinmorrison/homebrew-haskell. Note that this does a source install and takes quite a while.
Who is stopping you? I do my everyday job in haskell. I did not ask anyone's permission. I just looked around for libraries that do what i need, read books, and asked questions on forums. I use hdbc to connect to MS Sql Server, Yesod for web and services, i send emails, read files, print files to network printers. I wrote FFI to commercial C library pdflib and create pdfs from thousands tiff files with hierarchical bookmarks. I interface commercial Accounting software via xml services. I exchange information with our other systems via json. Practically any backend job i could think of i can do today with haskell. 
the fastest work around is to use brew to build your own copy of GCC brew install apple-gcc42 is fastest, i've used brew install gcc48 then type "ghc-pkg list". the first line will be a path like /Library/Frameworks/GHC.framework/Versions/7.6.3-x86_64/usr/lib/ghc-7.6.3/package.conf.d go one directory up to /Library/Frameworks/GHC.framework/Versions/7.6.3-x86_64/usr/lib/ghc-7.6.3/ then open the settings file. there'll be a line indicating the path to the c compiler. It'll probably say "/bin/gcc" change that line to "/usr/local/bin/gcc-4.8" (or whichever gcc version you brew installed) enjoy life, haskell and mavericks. NB: this is the power user fix. I recommend that most users instead do one of the approaches in http://www.haskell.org/pipermail/haskell-cafe/2013-September/110320.html
I'd like some help and attention for my Thrift generator, which I've written specifically for Haskell. It's still very much in its early stages. One of the big changes I made is to move all of serialization code into Data.Binary instances, hoping that the binary package can do most of the heavy lifting. This should be interesting to anyone looking for a promising in-the-works Haskell + thrift story. https://github.com/luciferous/vintage
The community definitely needs more people like you, solving everyday enterprise tasks with haskell *and building libraries as they go*. Luckily, I am in this position as well. However, most developers would get in trouble for introducing source code in an "unknown" language in the workplace because it is far harder to maintain should you move on. Also, a lot of employers would think the time spent writing libraries would be better spent using a more mainstream language with these libraries already.
I agree with the basic point of the article, namely that "plumbing" stuff is very important. However... I currently work on what I would consider a very typical IT project. It's a database-backed Java/JSP/JavaScript web application sold to companies and institutions that use it to register data for their business. And my current perspective is that the **reason** that it is tedious to work with, difficult to extend, broken, buggy, and so on -- like *most typical IT projects* -- is **exactly the view that programs are about I/O**. Because this leads to a completely chaotic codebase with no principles, no clarity, no structure, just a vague **I/O-oriented** layer architecture. It's like the macro-scale of the imperative paradigm's focus on the operational. So my counter-hypothesis would be that Haskell's non-I/O-centricity is exactly the reason why it is interesting and beautiful. There are ways of writing imperative OO code in a non-I/O-centric way. I'm thinking mostly of stuff like *Domain-Driven Design*. Few corporate developers are even aware of this. Corporate IT development is not a good model to emulate and pander to. It's an unprincipled quagmire without any theoretical foundation, without functioning best practices, and entirely without beauty. If your system is really just I/O, you could do it as a shell script. But mostly, I think corporate systems are yearning for a clearer separation of domain logic and user manipulation. I think the domains are yearning to be clarified and implemented purely or at least abstractly. But we are taught by the imperative languages and frameworks to not bother with this -- and to see our systems as "mostly I/O."
I love posts about optimizing Haskell code! This post will also be pretty .. accessible, so to speak, for beginners, as a newcomer will probably just scratch his head at the game of life implemented with "comonads, zippers and infinite grids", but will almost definitely know what's going on when it's done with a 2D O(1) random access array. 
I've seen people lament the lack of ipython's notebook thing for Haskell -- I'd hazard a guess that that's what he's referring to. 
I don't disagree with that (well, except for the fact that it doesn't account for unsafePerformIO). What I'm saying is that the whole idea of calling functions and returning values is an imperative concept that you won't find in the lambda calculus.
AFAIK websockets 0.8 also has full Hybi13 support, which 0.7 didn't have.
Ahh, yeah. I should've used the word *apply* instead of *call*.
If your goal was to convey that the cost is roughly 8n then it would be better for you to say that directly --- e.g., by using the phrase "roughly 8n" or "~ 8n" --- rather than using big-O notation, as by definition O(n) means "this function is linear and the coefficient in front doesn't matter for the purposes of this analysis", which is not what you are trying to say.
This is not optimization. This is fighting against Id monsters created by Haskell. Any sufficiently complicated code and you wont have enough life time to optimize it.
I'll just leave this here: http://en.wikipedia.org/wiki/Hashlife
O(n) in Haskell = 1 minute, 10 Gigs of RAM O(n) in C = 0.01 seconds, 50 kb of RAM. There should be a warning sign or something saying that Haskell's On's are orders of magnitude bigger than anyone elses On's.
If you don't like Haskell, why the fuck are you bitching about it on /r/haskell? Go do something useful.
Zippers, as rpglover64 said. Or you can use a more functional tree-based random-access structure, such as an IntMap or a Sequence; those add an O(log n) factor, but that's usually not a problem.
Out of curiosity, how would you structure a GoL program around Data.Map? Would you index it with a tuple, or a custom datatype? How would you design the next generation function?
I actually haven't put much thought into it. Thinking about it, it would be simpler to use a Set of points rather than a map. The neighbor count function would be easy to write: neighbors (x, y) = [(x + a, y + b) | a &lt;- [-1..1], b &lt;- [-1..1]] neighborCount grid (x, y) = sum . map go $ neighbors (x, y) where go (x, y) = if (x, y) `Set.member` grid then 1 else 0 The step function requires a little trick. The set contains the cells that are currently *alive*; however, we also care about cells that are currently *dead*. Happily, the only dead cells that matter are neighbors of alive cells. Moreover, since the rules for currently dead and currently alive cells are different, we want to treat them separately. We'll start by breaking the results of our step function into two parts: step grid = Set.fromList $ alive ++ dead Both `alive` and `dead` are the *result* of applying the appropriate rules to the previously alive and dead cells. `alive` is simpler, so let's start there: alive = filter (\ cell -&gt; neighborCount cell `elem` [2,3]) aliveCells We're just keeping any alive cell that has 2 or 3 neighbors. Since our set already contains alive cells, we just need to turn it into a list: aliveCells = Set.toList aliveCells Dead cells are trickier. Once we have a list of the dead cells we care about, we'll use a very similar filter: dead = filter (\ cell -&gt; neighborCount cell == 3) deadCells however, how do we get a list of those dead cells? What we want is all the neighbors of the alive cells that are not in the grid themselves: deadCells = concatMap (\ cell -&gt; filter (cell `Set.notMember` grid) $ neighbors cell) aliveCells I think this should work, but I haven't really tested it. Take a look at the [actual code](http://lpaste.net/94681) I wrote, which is a teeny bit more idiomatic and actually compiles. I could probably simplify it a bit if it wasn't three in the morning around here :P.
Yeah, you can call ghc as a library. - [http://www.haskell.org/haskellwiki/GHC/As_a_library](http://www.haskell.org/haskellwiki/GHC/As_a_library)
Is this just a troll, or are you commenting about some actual data that you found?
Worth noting that your repa version doesn't use the built-in stencil convolution algorithms. If you use those (which is quite possible), you end up with even better performance because Repa handles fusion in the case of the boundaries better than a hand-coded random-access approach. So, your repa version is not fully optimised.
kalcytriol is a troll. Please ignore.
I found that with 0.7, I couldn't write a client that connected to a Hybi13 server (probably due to the lack of masking) but it works with 0.8, so support wasn't complete. It all works with 0.8 now, so I guess it doesn't really matter anymore.
I'm guessing you're one of those guys that tried to learn Haskell but found the functional programming paradigm too complex because you couldn't pick it up in 2 minutes, and thus decided it was Haskell's fault? 
O(log n) is much worse than O(1) when it is at the core of your algorithm.
While the comment is trollish, maybe it would be worth it to think about ways to improve Haskell to the point where you don't produce horribly inefficient code as a beginner, just because you didn't know about unboxing and strict fields. You should not be an intermediate/proficient Haskell programmer just to improve the speed of the code to what you would have expected in the first place. 
You might be interested in my &lt;http://code.haskell.org/~aavogt/Rlang-QQ/&gt;. It gives you inline R in your haskell which hides the fact that you write and read some files on each side in order to transfer the data between the two languages. To install you need to: darcs get http://code.haskell.org/~aavogt/HList cd HList; cabal install darcs get http://code.haskell.org/~aavogt/Rlang-QQ cd Rlang-QQ; cabal install knitr supports blocks of code which are evaluated by ghc, which is convenient for when you're working on one graph and don't want/need to run everything, but I doubt you get the other nice features (caching, inclusion of graphics) knitr provides. So maybe literate haskell would be just as good. EDIT: knitr+Rlang-QQ is slightly usable &lt;http://code.haskell.org/~aavogt/Rlang-QQ/examples/test6.html&gt; 
It's true that we must spend some thought on better ways to make better performing haskell, but I don't think your argument is valid. Take a C++ begginer. If his code run, it should be efficient, but he have no idea about safety. Take a Haskell begginer. If his code compiles, it should be safe, but has no idea about eficiency. They are at core very different. One enforces you to code fast, and do extra work for safety. The other enforces you to code safe, with extra work for speed. Look the other way. You shouldn't need to be a c++ proficient to write good code.
Speed and safety are hopefully orthogonal for the sake of this discussion. C++ should also be heavily improved so that beginners (or in the case of C++, it does not actually matter what proficiency level you are) do not write horribly unsafe code. So I don't see how your analogy makes my point invalid.
&gt; by definition O(n) means "this function is linear and the coefficient in front doesn't matter for the purposes of this analysis" This function is *at most* linear... sqrt(n) is a O(n) function. f(n) = O(n) &lt;=&gt; For some m and K, for every n &gt; m, |f(n)/n| &lt; K
In addition to geezusfreeek's point, I would point out that it's a false feeling. An O(n) algorithm can be slower than an O(8n) algorithm, without violating the definition. When I said O(n) == O(8n), I meant that; they describe the exact same algorithms, by definition.
Nice changes! The API is a lot simpler than before, I'm already updating my threepenny-gui library.
But the diagnosis is wrong. It's not about I/O at all. By all means, evangelize with examples that are directly relevant to the audience, but keep in mind that all that declarative stuff is still indirectly relevant -- it's about being able to implement directly relevant stuff easily.
Such things as safety does not exists. Erlang programmers know something about it. Huge amount of work and time spent on tests makes your code safe, not static type systems. Static types are only good for beginners. In case of Haskell static type correctness is only a marginal case to be worried of (5% max) the rest is lazy evaluation. And no, C++ program often runs well enough and its performance is good enough just after first introduction of algorithm. Writing correct haskell code almost always (except an unintentional bug or luck) is not good enough. Haskell introduces additional layer of complexity. What is it? It is manual code optimization. Where C++ compilers do this automatically, in Haskell you must do this by hand or your hello world will require 10 gigs of RAM an will execute utterly slow. This is not trolling. This is an observation and facts. Or maybe you will deny this and call the author a liar? Why his primary version of such simple algorithm required 10 gigs of RAM? Another observation: PhD does not guarantee a good programming language or libraries. This is proven by a simple fact that Vector library does not have neither performance nor space complexity of a real world vector known from primitive languages like C. Actually Haskell starts to imitate C++ where standard libraries are just only for laughs and if you want performance you have to write your own. Haskell's Vector should be renamed to Craptor.
Off the top of my head (and I am not very good at ghci, I would be happy to learn that ghci does some of these things better than I am aware) ipythin is better at: * tab completion * editing (especially in the qtconsole or notebook) * syntax coloring * history * help integration (though that one is harder to compare because of the different things you want to know in python and haskell)
The problem as stated is still fundamentally unsolvable. An arbitrary value of type `IO ()` can't necessarily start from a clean state. The value may incorporate, by closure, other values that imply the existence of system resources: file handles, wrappers around C pointers, etc. There is, in general, no way to recreate comparable values at an arbitrary future time period. For a counter-example, suppose you have an I/O device that can only return a fixed sequence of numbers from the beginning, and beeps on each read. Your program opens the device, reads from it 20 times, and then constructs a value of type `IO ()` that reads it again. If you try to serialize that `IO ()` into a compiled binary, it's unclear what that binary should do. Open a brand new handle to the device (and get the wrong answer)? Open the device and repeat the first 20 reads (resulting in an unintended side effect)? Fail on the first attempt to interact with the device (because the open handle is now stale)?
Too bad about the mic, but I'm excited to watch this video. Loved your Days of Haskell series last year. DO IT AGAIN PLEASE?! :D
I used this formula today, but ghci complains about not being able to find libiconv (or finding it, but choking because it's a dylib). EDIT: Just realized I hadn't installed the separate CLT download, so maybe that's my issue.
Do use explicit imports. I've found your Main.hs, and function `play` there. But where is it defined? I have to examine three modules to find entry point 
The memory consumption problem has nothing to do with the Vector library, it has to do with boxing of variables. Vector comes in flavors, unboxed is great for primitive data like double, int, etc. However, you can't unbox complex types like records, which is why the boxed vector exists (random access to complex data). This isn't different from C++, C++ just has different word for it, "Pointers". An unboxed vector of doubles in Haskell is the same as having an array of doubles in C++. A boxed vector of Person records in Haskell is the same as having an array of pointers in C++, where Person objects are on the other end of each pointer. Additionally, the Repa library is built on top of Vector. Which means that even the in-place repa test results demonstrate the capability of the Vector library.
I'm very strict about that when using Python, so I see your point. It's worthwhile to take a moment and go over the source and do this. I'll write a patch (probably) tomorrow.
Very nice. You mention that you've never given a talk before, but it doesn't show -- the pacing is very good, and the content is well-structured. Having never looked into pipes before, I learned quite a bit. Thanks for providing this! 
While that's a good idea, I think it's mostly unnecessary. It adds syntactic baggage. As well as that, much of that syntax is already used in Haskell. `()` for tuples, `[]` for lists, and `{}` for records. The `OverloadedLists` language extension allows using `[]` for lists, vectors, maps, and maybe others that I don't know about. Type inference (or manual type signatures) take care of deciding which type to use. I think the idea of using a letter or symbol in front of `[]` has its merits though.
Thank you so much for the reference. I really appreciate it.
I've found you use `flip`. It is not so bad, but there is an alternative when used in context like `flip (,) 0`. Just use language extension TupleSections and write `fmap (, 0)` instead of `fmap (flip (,) 0)`!
I actually wasn't aware of the OverloadedLists extension. I'll have to start making use of it. The type inference does eliminate most of the need for using various types of brackets. There is that one case were you have to use a signature. ([1..10] :: Vector Int) is a lot of extra typing verses something like V[1..10].
His total comment karma seems to suggest that he is a career troller. :)
I had this originally, but did not know whether using language extensions was a common thing in haskell, or if it is frowned upon. I erred on the side of caution. If it is acceptable and preferable to use the extension, I will use it instead of flip, as I think the relevant code would be far more readable and concise without flip.
I got it working successfully via this method very smoothly. Darin's homebrew formulae were failing unfortunately.
Sure, there are controversial extensions, but TupleSections is very useful IMO. Reading pointfree, `flip` and `uncurry` requires some skills -) btw, your game is pretty hard to play. But that may have reason I do hate Pong-like =)
&gt; Ollie Charles (@acid2) tweeted at 9:51 AM on Wed, Oct 23, 2013: &gt; I'm considering doing another "n days of Hackage" in December - what libraries should I write about? Anything new people loved in 2013? &gt; (https://twitter.com/acid2/status/392921247861846016) I'm excited! :) 
Just for the record, here's a good example where you should leave a semigroup well-alone and not promote it to a monoid (start from *it's quite clumsy to have a bottom element*): http://ghc.haskell.org/trac/ghc/wiki/Hoopl/Cleanup Here we have (complete, distributive) lattices, but projecting out to just the join operation, it's a semigroup. Inserting bottom as the monoid identity like this: data CPFact = Bot | CP (Map LocaReg Const) leads to the rant that *And what is frustrating is that Bot is never, ever used! I don't want to define it and manipulate it when it is never used!* 
&gt; Just use language extension TupleSections There are sometimes nicer ways to do things than flip, and there are sometimes cases when you need language extensions. But activating an extension just for a barely nicer syntax seems silly.
Best talk of tonight's meeting too...
Yes, exactly. I don't think the performance would suffer *that* much at the beginning either, but I'd have to see some benchmarks. This is especially important if you want to support an infinite grid. If you just have two gliders going in opposite directions, you don't want to keep on making your array bigger!
 export DYLD_LIBRARY_PATH=/usr/lib why this fixes libiconv problem is a mystery.
Of course if you have a small enough code base built entirely by yourself or a very small group you can try to make these kinds of guarantees and reason about an entire code base in your head. But that just isn't how software works. We depend on thousands of open source libraries that make you cringe in the worst way if you view the source behind them. Even large platforms like .NET and Java have horrendous code in the standard libraries. Not only that, C++ lacks composability in a large way. So you are constantly forced to re-implement the same things over, and over, and over again. This is why designing a library like pipes is so incredibly difficult (I can only imagine - as I was not involved). It uses equational reasoning to provide composability and guarantees about the code you write vastly increasing modularity. No one will have to solve the problems pipes solves ever again (theoretically, Haskell isn't perfect - just a large step forward).
On the one hand I'm really happy that there is an IO-war going on, because the criticism from both sides seem to be supportive in most cases, so both libraries will probably come out of this thing better and stronger; on the other hand, the IO-war confuses me and makes it hard to choose 'which one is better' for me. I guess it's a matter of personal taste though.
I'm currently dealing with `/Library/Frameworks/GHC.framework/Versions/7.6.3-x86_64/usr/lib/ghc-7.6.3/include/HsFFI.h:30:20: error: stdint.h: No such file or directory` other things like not finding libz and various other headers as well. I tried going in Xcode 4, I made sure not to upgrade, and it had me re-enable developer mode or whatever. But still no luck.
For now, I am doing what schellsan did. I am using vagrant to make it easy, it is nice that they finally have a recent haskell platform in the repo. Though, things don't seem to be playing nicely with shared folders in vagrant..
I just updated one of my apps that uses this library. Quite a bit cleaner to use now.
Thanks!
Agreed. For years the Haskell world has been asking for more of exactly this sort of optimization how-to. OP, I really appreciate your step by step writeup showing results, code changes and thought process. I got lots of new insight. Thank you!
yeah, it fixes most of the issues, but its not a way that has simple instructions that a innocent young engineer will remember after hearing verbally.
Your remark about boxed accesses having a 2x penalty is nonsense (even ignoring the O(1) vs O(2) ) reads on modern hardware have incredibly varied performance characteristics depending one what level of CPU cache and ram the data is currently available. a "2x" cost is true when both the pointer and the value are in the same level of CPU cache, but depending on the algorithm, they wont be, and the constant factors can be 1-3 orders of magnitude. The unit cost ram model is wildly inappropriate guide for writing performant code for modern hardware. I warmly recommend you look up the "intel optimization manual" http://www.intel.com/content/www/us/en/architecture-and-technology/64-ia-32-architectures-optimization-manual.html, it is chock full of great information about the cost model on modern intel hardware, which can be a great guide for performance sensitive engineering.
It might help if you have the full definition of the `O(f)` notation -- a lot of resources skip the mathematical definition in favor of a more intuitive explanation. Let `f` and `g` be functions of `n`. `f` is in `O(g)` iff `f(n) &lt; C*g(n)+k` for all `n` greater than some `N`. This is a precise definition of an upper bound on functions. That is, `O(g)` describes the set of functions that are bounded above by `g`. The `O(g)` notation necessarily throws away the constant factors because it puts functions into equivalence classes based on their upper bounds. If you want more precise notions of running time use big-theta and big-omega notations that describe equivalence and lower-bounds respectively. To see if you get the concept, try and write out the definitions of these sets. EDIT: Theta and Omega classifications are sometimes described as average-case and best-case respectively. These are misnomers at best, and both are useful tools for analyzing running time.
What optimization level did you compile this at? Just curious.
Note ghc -ddump-minimal-imports
That multiline issue with knitr has an easy fix: https://github.com/yihui/knitr/pull/633 The other two problems, caching and including figures automatically, probably will take a lot more work.
If you are not completely and utterly dissatisfied with how long and arduous it is to build custom systems today than I do not know what to say. Custom software is an incredibly expensive, time intensive endeavor. Even for simple things. So much so that extremely valuable behavior that users desire is never materialized due to the cost and effort required that most just cannot afford. Not only that, it is just unprofessional and down right criminal. We waste such an incredible amount of time dealing with failed abstractions and bashing our heads in re-writing the same things over and over again. You see, to me it isn't about making software "bug free" or perfection. It is about finding solid abstractions and building modular software. Anything taking a large step in that direction, such as Haskell, Agda, Coq, Idris, and many others are a huge boon. If we do not stop re-implementing the easy stuff, we'll never get to the really sophisticated problems (there are people working on these problems, but why so few?). We need to free ourselves of the mundane so that more of us can tackle the difficult problems propelling our use of technology forward. This isn't new. We have been trying for decades. The problem is we continue to build software on leaky abstractions that do not compose well. Of course this is just my take on it. I'll fully admit that I could be way off mark.
You're absolutely right; my definition was sloppy. Thanks for the correction!
[Was expecting this.](http://resources2.news.com.au/images/2013/06/18/1226665/335078-bagpipes.jpg)
My recollection is little-o still eats constants, it's just "strictly faster" instead of "no faster than" (much like "&gt;" in place of "&gt;=").
I would have made a pull request for the update and lens integration and refactor if you would have asked me to. ;-) 
I compiled with -O2, the Repa variants are also -threaded. It's also worth mentioning that compiling with -threaded and running with +RTS -N on the non-repa versions actually harmed their performance.
I would expect that normally type inference would take care of that Vector case, as you'd be using vector operations on it (`V.map`, `V.zipWith`, etc.), unless you're only using generalized functions on it like those from `Foldable` and `Traversal`.
So I've decided I want to contribute. I'm a Windows user with a Linux VM that I barely ever use. I've seen comments in the past like "windows is a tier two haskell platform because no one helps out on windows." Specifically for something like the IO manager which IIRC got improved on Linux but not Windows. My question is this: is developing on Windows tier 2? Will it be much easier to ramp up on Linux and then work on Windows only for Windows specific features once I know what I'm doing?
i think you misapprehend. My orignal remark was saying that there exist algorithm where the optimal unboxed code can be 10-1000x faster (robustly over all input sizes) than an Boxed array based algorithm. I don't understand your sentence about boxed types. It has nothing to do with the remarks about UNboxed types. your sentence about how unboxed arrays and CPU caches work is FALSE NONSENSE NONSENSE FALSE pick any three. I've spent the past year working on tools to enable reasoning about memory locality for performance , so i'm a bit OCD about facts on this matter :) any time you read from memory, it'll be loaded in all levels of cache (Except when you're doing using an instruction that does a "streaming" or "nontemporal" read or write. These are often exposed as C compiler intrinsics, though there are plans to add support for them in GHC). Every time you're taking uniform steps indexing into an array, the CPU will say "hah, I notice a pattern, I'll load the place I think you'll load next". The CPU is choosing the Cache eviction scheme and doing "hardware prefetching" of memory ahead of time. All languages people program in have roughly equal lack of (and presence) of control over how cache works. relatedly: GHC 7.8 will have support for prefetch0, prefetch1,prefetch2, and prefetch3! Which corresponds with the associated prefetch intrinsic in GCC. (I added support for software prefetches to all the code gen backends ~ 1-2 months ago). I've written HPC code, both in haskell and in C. Its very very easy to reason about cache behavior (between GC cycles, but thats enough! Most HPC code has so little allocation that the GC stuff wont be a real issue, except for maybe shaving a final 2x constant factor at most). I do agree that depending on the workload, boxed arrays are terrible. At the end of the day, any claims about rules of thumb for performance require very very thoughtful focused experiments and benchmarks. I suspect that theres actually quite a few algoirthms where boxed arrays fair ok, but yes, we agree, loads in cache are complex. understanding systems performance engineering as applied to high level languages is a great adventure! you're going to learn a lot! :) I'm also totally glossing over how to use SIMD registers well on modern hardware. Theres a lot of really really fascinating tricks you can do in that space
good question! honestly just try getting stuff to build on windows and help! :) There are windows related issues and quirks, and having more folks wanting to help "own" keeping that beast a bay is never a bad thing! You can always segue to linuxy parts, or interleave them all. Everyone who's working on ghc does thing differently, so half the fun is figuring out a style of involvement thats fun for yourself and is helping everyone else! :) please feel welcome to ask for help on #ghc on freenode, and email the ghc-devs list. If you're not getting the help you need on one, try the other (and/or both!). GHC is a huge project with shockingly few people working on it, theres so much opportunity for more amazing things the more people who help out! Come aboard! :) 
Thanks a lot for this. It definitely smoothed the transition to Mavericks.
thank you!
Erik uses Conduit to solve his space leak problems. Yeah, I think every Haskeller stumble upon this problem at some point. But on the other hand I don't want to miss laziness anymore and see it as one of the greatest strengths of Haskell.
There is a second part of the [interview](http://www.youtube.com/watch?v=BBm6yfkjkPw) in which Erik mentions a new project. He don't want to release to much detail, it is about data analysis in book reviews (with Haskell of course).
Future versions will give the player more agency, and the physics will be fine-tuned. As a result it should be more fun to play. I can't promise that it will be easier per se, but it will be more skill-based and not feature as many situations in which you are helpless to stop the ball. Thanks for playing.
Thank you! Please give `pipes` a go, it's a lot of fun being able to think in such a modular way about effectful streams :)
Argh, I've been called out!
It looks like I will be doing another series this year - as klrr_ mentioned below, I'm asking for suggestions on Twitter, though I will be doing another call for help on my blog at the start of next month. For now, think about any libraries you'd like me to look at, and we'll see what happens!
I can assure you that you do not want to hear me play bagpipes :)
hehe, pipes ∘ pipes ≃ pipe-organ
I can't believe I've survived for so long without this. What other little-known treasures have you unearthed?
I am worrying if GHC 7.8.* will have the same problem, because a new haskell-platform is [coming](http://trac.haskell.org/haskell-platform/wiki/ReleaseTimetable).
https://github.com/plaimi/bweakfwu/commit/de05dacc288c4f4e2005f85e2aa8701a205f8e04 -- I hope this is satisfactory.
I do hope you keep posting, because you're hilarious. 
You certainly can't do this without runtime support. I don't believe such support currently exists, and it'd be tricky, but it might theoretically be possible. As others have mentioned you'd need some way of recreating/reclaiming any resources the process had when you froze it - naively assuming an open file descriptor is the same just because it has the same number when you restart is certain to lead to problems.
At least for me, what put me off till now was the API stability. I've started looking at pipes during the transition from 3.x -&gt; 4.x and it didn't feel worth to learn an API which was going to change anyway. Are you guys stabilising the API with the 4.x release? :)
Very nice!
Out of curiosity, how does this compare to just (say) installing gcc-4.8 and changing the settings file accordingly?
Ah, cool. I helped that guy on #haskell. The `LaxDouble` instance is mine. For what it's worth to anyone interested, for parsing dates/times, the [thyme](http://hackage.haskell.org/package/thyme) library has been 6x faster than Data.Time due to its attoparsec parser (parses straight from a ByteString) in my experience, and its internal representation is very efficient.
IIRC, it installs library and header files in a place that GCC and GHC can find them.
Man, I read this and I wonder if we are even in the same industry. Where do I see vast amounts of effort wasted? In building web apps, for one. Incredibly effortful and inefficient business. How much of that has to do with, oh, say, being able to write impure functions in ECMAScript, client–side? Almost none. How much has of that has to do with leaky abstractions in the J2EE standard libraries, server side? Almost none. How much as to do with the intrinsic stupidity of trying to use an unreliable high–level application protocol as if it were a reliable low–level transport? How much is to do with writing apps that must run in terms of arbitrarily poor and erratic and idiosyncratic implementations of a standard (running inside arbitrarily poor and erratic and idiosyncratic implementations of another standard)? Much. And why do we put up with all the accidental complexity required to make this work? Because the owners of the hardware that runs the sandbox that runs the sandbox that our apps run in are, as a corporate body, unimaginative, paranoid and stupid—and if we don't, they will lock us out. Please, tell me how low–level libraries of cleanly composable abstractions are the magic bullet that will fix this particular decade–long car crash. &gt;Custom software is an incredibly expensive, time intensive endeavor. Even for simple things. So, please explain how come the world is awash with great masses of apps for phones and tablets: all custom, all doing things that customers want and will pay for, all (all minus ε, anyway) written in Objective C, Java, Python, etc. etc. many written in their spare time by amateurs and, *all working*.
You do need clang to use ghc-ios though, correct? 
Right, the key point is not having broken code look a lot like innocent code.
Downloading now! :)
Cool - does the LLVM backend get rid of the C-- pipeline? Is the LLVM backend in use at the moment?
just use my directions here https://gist.github.com/cartazio/7131371 :) 
Does Elm have a stable release time table?
Arrows exist largely by historical accident -- They were the first thing that Hughes came up with that could handle the parsers of Sweirstra and Duponcheel. If you can get by with an Applicative, use it. It nicely captures a notion of a context-free expression, where the shape of the calculation is pre-determined. A Monad captures a notion of context-sensitivity, where you can choose what to do next as you go. Arrows sit in an awkward middle ground between these two extremes. 
That's awesome, I'm looking forward to learning Elm!
&gt; Cool - does the LLVM backend get rid of the C-- pipeline? No. We emit LLVM from C--. &gt; Is the LLVM backend in use at the moment? Yes. You can optionally turn it on for any of your code using the `-fllvm` flag.
I finally got it to work. I had to `uninstall-hs`, delete `~/Library/Haskell`, reinstall the haskell platform 2013.2, modify the settings line provided (after compiling on another mac and transferring the wrapper over), add the wrapper if not already done, `cabal update` as if the first time, then install the latest `cabal-install`, and finally the packages you want. Though there's plenty of warnings about an unused parameter, and some random `int foo() {}` that gets complaints everywhere. But things seem to work. I really appreciate this, since I'm using Haskell for school as well.
Whoa whoa whoa! where are these helpful people on SO you speak of? I have had my account for about a month and every question I have ever asked has had some form of "RTFM" comment made multiple times.
What do you have to add to Profunctor to get Arrow?
I would look at Cloud Haskell since they appear to be able to serialize closures at least... I don't know how applicable that would be to your case and if IO () can be serialized too. But it's probably your best chance at this in a near future. Of course there are lots of working approaches but they'll all need some more work on your part, no "just frozing this IO() and thawing it on another run".
An `Arrow` is something like a `Strong` monad in the 2-category of profunctors, using the `Strong` class from `profunctors`. There `return = arr`.
Oh, is there any reason for this except adding a C-- to LLVM compiler is much less work than making a Core to LLVM compiler? 
Yes. The thing is, if you wanted a direct Core -&gt; LLVM transform anyway, you'll likely have to do a lot of stuff this post is talking about in any case (realistically you'd more likely go from Core -&gt; STG -&gt; LLVM.) You might not save much in all honesty. C-- better models some things we already have; e.g. proc points and stack maps as mentioned in the post. We also do a lot of optimization on our C-- code, such that LLVM doesn't actually end up doing a whole lot except for the fanciest of analysis (e.g. things like GVN, LICM &amp; vectorization.) And the LLVM optimizers probably can't see some stuff especially after laying out the stack among others, so there's probably some optimization opportunities remaining here at the C-- level. (OTOH, for code that uses e.g. stream fusion, the idea is to basically unbox, fuse and inline the hell out of everything so that you 'push' all the final code into one indivisible peace, e.g. one function call with a loop body. This is typically very small imperative looking code, so LLVM is really good at this stuff. But you may not see improvements for 'average' code over the NCG.) Finally, going the C-- -&gt; LLVM route, as you said, is much simpler given the existing code, keeps the required support work minimal, allows maximal sharing of code and infrastructure between both backends (native and LLVM,) and as a result of this they can use the same ABI and interoperate seamlessly without nearly as much effort.
Also: what better models those parsers? Uuparsinglib's current Monad/observably-different-Applicative combo is pretty awkward.
If a `Profunctor` is `Strong` you get `first`, but then I think you still need to provide `(&gt;&gt;&gt;)` and `arr` to get the full `Arrow` interface. 
Why do you say cartesian? I've always thought they were monoidal categories with tensor given by `(***)` and unit given by `()` (weakened so that `(f *** id) &gt;&gt;&gt; (id *** g)` doesn't need to be `(id *** g) &gt;&gt;&gt; (f *** id)`). In addition they have an embedding of Hask via `arr`. But this monoidal structure isn't cartesian. Is there a way you can see them as cartesian? 
You forgot about "premature optimization is the root of all evil" mentioned (nearly) **always** when someone asks for performance tips…
A strong `Monad` implies you have `return` and `join`. `return` is `arr` n this category. And `join` is `(&gt;&gt;&gt;)`. This generalizes the better known construction in `Hask` if your profunctor is representable, because then it is isomorphic to `a -&gt; f b` for some functor `f`, subject to a join works like `(&gt;=&gt;)` and the identity arrow plays the role of `return`. A comonad in `Hask` also gives rise to a strong `Monad` in this same way with a profunctor that is corepresentable by the comonad. The same argument for why all comonoids are trivial works for showing that all comonads in this 2-category of profunctors are trivial, so there is nothing useful there that is 'splittable'.
You can compute first-sets for the `Applicative` fragment of the parser, and throw in wildcards whenever you don't know due to `(&gt;&gt;=)`. Your asymptotics suffer a bit when you use `(&gt;&gt;=)`, but thats it. This at least keeps you out of `Arrow` territory.
This is what I did and it seems to be working fine. I'm using haskell-platform from homebrew so the settings file is at /usr/local/bin/ghc-7.6.3/settings I installed gcc48 from homebrew (brew tap homebrew/versions; brew install gcc48) and changed the C compiler in the settings file to /usr/local/bin/gcc-4.8.
Just a little proof of concept of using the JSON api from EKG to fetch "upstream" stats from another EKG monitored process. Not sure how useful this is, but could be okay for building a dashboard to present stats on several apps.
I updated the post with a link to a binary version of the 'clang-xcode5-wrapper'.
I installed ghc 7.6 before, and it is working with xcode5 now. So I will wait for a couple of weeks.
Check out this: http://lpaste.net/94839 A bit better: http://lpaste.net/94840 A lot better: (IMHO) http://lpaste.net/94834 See where this is going! So in a sense typing does help you stop hackers! [Example from] (http://net.tutsplus.com/tutorials/ruby/mass-assignment-rails-and-you/)
there is a more detailed version written up [here](http://haskell-workshop.github.io/tutorials/osx/2013-10-23-mavericks-ghc.html)
 data Index = One | Two | Three type Board = Map (Index, Index) Cell You can write a Num instance pretty straightforwardly to make this less of a pain.
`id &amp;&amp;&amp; id`, `arr fst`, and `arr snd` build a cartesian structure.
What about a 1000x1000 grid?
Probably the most comparable thing to your c++ example is using an [unboxed vector](http://hackage.haskell.org/package/vector-0.9.1).
Oh hey, that's my answer :)
well, java could tell if overstepped the bounds
I think you're claiming that given f :: a -&gt; x g :: a -&gt; y then `f &amp;&amp;&amp; g` is the unique map `a -&gt; (x, y)` such that `(f &amp;&amp;&amp; g) &gt;&gt;&gt; arr fst = f` and `(f &amp;&amp;&amp; g) &gt;&gt;&gt; arr g = g`. That would be the translation of the notion of categorical product into the language of `Arrow`s. But this condition does not hold. Take `f = g = Kleisli (putStrLn "Hello")` for example. Those equalities are not satisfied. Thus an `Arrow` need not be cartesian, merely (pre)monoidal with an embedding of Hask. Or have I misunderstood you somehow? 
cool. I was just looking into ekg. Is ekg the way most people do monitoring of haskell processes?
My claim is that every object is a comonoid. This to me is the interesting part of being cartesian. Not that you have products which is just a side effect. given split = id &amp;&amp;&amp; id we have assoc . (split *** id) . split = (id *** split) . split fst . split = snd . split = id the monoidal structure is not a product becuase it isn't really monoidal, not because the "other part" of being cartesian isn't available. So maybe a "cartesian pre-monoidal category"?
I really enjoy HWN, keep up the good work! Would it be possible to reduce the delay between week-end and each HWN post?
Thanks, this gives me more confident in using applicatives for this. It did not answer my question though, or do you imply that applicatives are better use for this than arrows by this statement, if so what makes them better? Yampa md Netwire seem to use arrows and those seems like the most popular of the FRP libraries.
There are some things you can write with arrow that you can't write with applicative. It is hard to classify what those things are.
Interesting. You seem to be using this property outlined at the nLab. http://nlab.mathforge.org/nlab/show/cartesian+monoidal+category#properties_22 The property is that any symmetric monoidal category that satisfies the above condition is automatically cartesian. I've never used it before. However, correct me if I am wrong, but there *are* Haskell `Arrow`s that are symmetric monoidal, such as `Kleisli (Writer [()])`. This one is *still* not cartesian. The condition on the nLab page that fails is that `split` is not a natural transformation, since `split &lt;&lt;&lt; f /= (f &amp;&amp;&amp; f) &lt;&lt;&lt; split`. (The latter writes twice as much as the former). So I don't think "cartesian" is a correct description for `Arrow`s. I would be interested to hear your thoughts. 
I personally use collectd for performance metrics, so I wrote [this](http://hackage.haskell.org/package/hslogstash-0.3.1/docs/Logstash-Counter.html). I'll soon upload something with a few bugs fixed (in the conduits part, not this) and some haddocks ...
The websockets package is a great addition to the Haskell ecosystem. It adds support that was missing (AFAIK), and enables everyone to use Haskell with websockets, instead of making their own implementation/library. Each small new library such as this enables developers to choose, or stick with, Haskell for yet another type of task instead of choosing another tool. What I don't get is why this post has downvotes at all. Not one or two, but 13% of votes are downvotes. What's not to like? 
The `hlogstash` looks very interesting, but [please follow the PVP](https://plus.google.com/117852249512245938101/posts/APGMW1LGxKX)... :-)
Reddit fuzzes the actual number of up and downvotes to deter spam. The difference is correct. Also, I'm sure our favourite /r/haskell trolls like to down vote everything out of spite. 
Help for GHC on Windows! You have provoked me to create a Reddit account solely to reply to you. GHC on Windows badly needs love. Lots of people *use* GHC on Windows, but almost all GHC *developers* use Linux of some variety. Result: the Windows version of GHC has lots of rough edges, and some of them go un-fixed for ages. So I would love for you to roll up your sleeves and help. Here's a [good place to start](http://ghc.haskell.org/trac/ghc/wiki/CodeOwners). Anyone else who uses Windows... we need you too. Thanks!
I'm waiting for that matrix library! 
data Board = Center | N | S | W | E | NW | NE | SW | SE
It seems to me that Apple is trying hard to force a new "standard" which breaks many existing OSS software packages deliberately by providing incompatible "compatibility wrappers" (such as a providing a fake `gcc`-wrapper which isn't even remotely compatible to a real `gcc` executable, or by using a `cpp` implementation which seems to be incompatible with the ISO C standard, requiring software to add tweaks just to support this broken Apple `cpp` implementation) What is Apple trying to accomplish here? Or is it just carelessness? Or are they just trying to repeat the [Maps fiasco](http://techcrunch.com/2012/09/26/the-apple-ios-6-maps-fiasco-clarified-in-3-minutes/) for more publicity?
&gt; Alternatively, assuming my logic isn't flawed, has this been done before? Yes, I've definitely seen it before. See for example [controlling sudo privileges](http://code.haskell.org/~dons/code/cpuperf/design.txt) in shell scripts.
http://donsbot.wordpress.com/2007/03/10/practical-haskell-shell-scripting-with-error-handling-and-privilege-separation/
&gt; class (ExitPermission io, RandomPermission io) =&gt; CanRandomlyExit io where &gt; instance CanRandomlyExit IO where FWIW, this isn't required in recent GHC versions. You can just write: type CanRandomlyExit m = (ExitPermission m,RandomPermission m) f :: CanRandomlyExit m =&gt; m () &gt; has this been done before Sure, similar is used for other monads (MonadReader, MonadWriter). But I think IO-restrictions tend to be used when they're actually important and classes are defined as needed, rather than a general library of it. I suppose you could have a go at re-exporting System.IO with such categorization give it a try, see how it feels.
Hmm....I have had no issue with GHC 7.6.3 since the upgrade to Mavericks. I am using Cabal 1.18.0.1 with sandboxes, and all of my packages build and test just fine, no need to redo or alter my HP installation. I even have have Xcode 5 upgraded...still no issues.
`char board[3][3]` is too complicated for Haskell's type system. We can allocate a 3x3 array, but the fact that the array size is 3x3 won't be visible in its type: emptyBoard :: Array (Int, Int) Cell emptyBoard = array ((1,1),(3,3)) [((i,j), Empty) | i &lt;- [1..3], j &lt;- [1..3]] That might be surprising, given that Haskell's type system is so much more advanced than C's! What makes the type `char[3][3]` so complicated is that it's a type which depends on two values, 3 and another 3. It also depends on another type, `char`, but Haskell handles that kind of dependence just fine; Haskell supports types which depend on other types, but not types which depend on other values. There are more advanced languages, such as Agda, which do support types which depend on values. In that language, I could easily specify that board is a 3x3 grid: board : Vec (Vec Cell 3) 3 That type signature is very similar to your C signature: you say that board is an array of 3 arrays of 3 characters, while I say that a board is a Vector of 3 Vectors of 3 Cells. Anyway, Haskell doesn't support these so-called "dependent types", because they open a whole new can of worms in term of language design and implementation. Still, since Haskell supports types which depend on other types (not values), it's sometimes possible to create types which mimic the values we want to depend on. Here is a typical, but very inefficient way to represent numbers at the type level: -- types with no data constructors! we are only interested in the type constructors. data Zero data Succ a type Three = Succ (Succ (Succ Zero)) Using other tricks, we can implement [`Vec`](http://hackage.haskell.org/package/Vec-1.0.1/docs/Data-Vec-Nat.html) in Haskell, with which we will finally be able to write down a precise type for our 3x3 board: board :: Vec (Vec Cell Three) Three Since `Three` is a type, and not a value, the above type is valid in Haskell. I also hear that ghc has an extension called [type level naturals](http://ghc.haskell.org/trac/ghc/wiki/TypeNats) which should allow you to write `Three` as 3, so with even more tricks, it should be possible to write this: board :: Vec (Vec Cell 3) 3
SafeHaskell is a good idea, but I don't understand why you'd need `liftIO`. The entire point is that a function with a signature mightBeDangerous :: FileWritePermission io =&gt; io () Can only access those methods that write to files, so you know that the danger of using such a "black box" function is restricted. If the signature was mightBeDangerous :: (MonadIO io, FileWritePermission io) =&gt; io () the what would be the point of using permissions in the first place? This is obviously a function that can do more than just write to files. However, you would still be able to use `liftIO` on these specialized functions, since the typeclass would just get converted to `IO` at compile time.
In the context of FRP, arrows vs applicative is mainly a question of implementation difficulty. Arrows are easier to implement, but the price to pay is that the API becomes less convenient -- you need to do a lot of plumbing to get names in scope where you want them. Applicative is harder to implement (sharing, dynamic event switching), but ultimately gives the more convenient API. That's the style I'm using in my reactive-banana library.
Maybe permissions was a bad word. Those are system admin permissions, I'm looking to have a type system that restricts what an `IO` function can do.
Using 7.6.3 (a little dated, but not that much), you have to use `ConstraintKinds`. My approach requires an extra line, but no extensions.
Have a look at Wouter Swierstra's paper, Data Types a la Carte. It ends up using free monads for a similar fine-grained effects approach.
I am waiting for that matrix library too！
I could not have stated it better!
I like this approach. You could also have a ThrowsException permission, which half of IO would need, but that at least makes it explicit.
really inspiring reply！
no, I didn't mean vary size of the grid, I mean fixed size of 1000x1000， which seems hard to define as a data type data Index = One | Two ....| Thounsand
&gt;to break up the monolithic "super monad" that is IO. Why would you do this? Isn't that what the kernel is for? It seems that you are trying to write kernel behaviour into your user-space programme. Writing a kernel in haskell might be a fun thing to do (if you dislike fp and love pain). I/O is inherently about *state*. Making it more granular makes it more stateful. Now, get off my lawn. The mower is lisp-powered.
It requires one extra line per instance, because you have to specialise an instance for every type you want to be `CanRandomlyExit`. But if you have a phobia of extensions, these facts are irrelevant.
I would imagine the reason one would not want to use `ConstraintKinds` is that it's only available on GHC 7.4 and above. `UndecidableInstances` is much more widely available.
OK: Maybe I'm missing something. Explain this to me as if I'm the Unix kernel you're running on. I have a rich and stateful API ready for as much asynchronous message passing as you need.
It's not that I'm excluding extensions entirely, but the fewer the better since it makes it more compatible with other compilers, platforms, and libraries. 
Article completely fails to mention that you have to be a Haskell developer working in finance to be able to afford to park in Westminster!
It would be nice to be able to write custom writers to collection systems.
In theory, I can push these out on Sunday for the previous Sunday-Sat week. Pushing it out on Thursday in a bit late, I know. A push on Thursday is usually because life "got in the way". I've been waiting for Wednesday on purpose to allow posts posted towards the end of the week on reddit to "gain some traction" before being ranked against posts that have had a couple of days on the site to gain some points. That logic might be wrong, but that's why I've been doing it. Any thoughts?
The short but probably beginner-unfriendly version is `replicateM`, using the Monad instance for List. &gt;&gt;&gt; replicateM 3 "ab" ["aaa","aab","aba","abb","baa","bab","bba","bbb"] To explain a bit how this works, consider that `replicateM n = sequence . replicate n`. Hence the input string is first replicated 3 times &gt;&gt;&gt; replicate 3 "ab" ["ab", "ab", "ab"] and then `sequence` for lists returns a list of all possible combinations of the list elements. sequence :: Monad m =&gt; [m a] -&gt; m [a] sequence = mapM id sequence [a0, a1, ..., an] = do r0 &lt;- a0 r1 &lt;- a1 ... rn &lt;- an return [r0, r1, ..., rn]
Constructing in lexigraphic order is easily done with a list comprehension. The rightmost variable (`z`) varies the fastest. Obviously your input list has to be in order... &gt; let chars = "ab" &gt; in [ [x,y,z] | x &lt;- chars, y &lt;- chars, z &lt;- chars ] ["aaa","aab","aba","abb","baa","bab","bba","bbb"]
Consider using lpaste.net, which has no advertisements and a cleaner UI
See Liquid Haskell, which would allow you to use Int indices but check that they don't exceed some given bounds at compile time.
&gt;That might be surprising, given that Haskell's type system is so much more advanced than C's! I think that the comparison is a bit unfair. Note that in `int board[3][3]` both 3's have no semantics at compile time, only at runtime during initialization. It is not a type depending on values, but merely syntax that mixes runtime with compile time code. Also, the equivalent haskell code could be much shorter: board = listArray ((0,0), (2,2)) (repeat 0)
You're thinking the idea is to somehow limit what the kernel can do or impose additional constraints on the kernel, as if that makes programs safer. But it just introduces more ambiguity. Per your reply below: &gt; Why would you do this? Isn't that what the kernel is for? It seems that you are trying to write kernel behaviour into your user-space programme. Not just write kernel behavior, but *encode it* and verify the correctness of it. If you're running a server running Haskell, you *could* configure your operating system to prohibit certain things like creating threads and opening too many handles - but then there's no way to encode that in the Haskell program. There's no way to manifest those constraints, instead it's just like we're running a dynamic programming language where the consequence when we hit those limits will be a hard crash and we'll have no idea why or when it is coming. Instead, with compartmentalized `IO`, we can encode those constraints in programs such that the compiled code *cannot be incorrect* in certain ways. Just as the type system allows us to guarantee that a function `Int -&gt; Int` will never return a value of type `IO FireZeMissiles`, compartmentalizing IO allows us to ensure that, for example, a streaming library is only able to increment its position in a file or socket, but not open new ones. Or a plugin is only able to interact with existing threads, but not fork bomb a system. Or a server-side compiled process is allowed to output to `StdErr` but not `StdOut`. And so on, and so forth. The goal isn't to reimplement kernel behaviour into user-space programs, the goal is to encode it in such a way that someone can compile a program and know *a priori* that it's not incorrect, for certain definitions of incorrect.
Another advantage to this kind of approach is that you can mock the bits you need when you want to test IO code. I'm not sure I like the term "permissions" though, as that implies some kind of security guarantee which you obviously don't provide (given unsafePerformIO).
Haha, thank you :D The best I can offer now is [the list](https://groups.google.com/forum/#!forum/elm-discuss) where I tend to do "private" releases a bit before public ones. We also discuss a lot of language design stuff, so you'll know what is in the pipeline :) [Building from source](https://github.com/evancz/Elm) is also really easy and will let you try out everything. You just clone, move into the root directory, and run `cabal install`. It's pretty much the same as installing from hackage, and a great example of cabal being awesome :) All public branches tend to be quite stable, but I'll always tell the lists if anything weird is going on.
ah, thanks! I was expecting to find a function of type `Ix i =&gt; (i, i) -&gt; (i -&gt; e) -&gt; Array i e`, but `listArray` is just as short in this case.
If I use SafeHaskell then there's a bit more safety guarantee from things like unsafePerformIO, but I agree that permissions might not be the right word. The terminology is half stolen from when I dabbled in Android a while back where you have permissions to access the network, for instance, and another permission to access the filesystem, etc etc. I'm just not the best at naming things. I've considered using `Access` instead, such as `EnvironmentAccess` or `FileReadAccess`, and it's shorter to type than `Permission`, but I haven't settled on anything yet.
No contact details? I'm interested.
Hi, you can private message me on Reddit. 
It will be (and already is if you're willing to parse JSON).
Yay for more Haskell in Kiev.
 data FinTen = Zero | One | Two | Three | Four | Five | Six | Seven | Eight | Nine data FinThousand = FinThousand FinTen FinTen FinTen
What do you want as result when the input is "abc" and "def"? ['abc','abf','aec',...,'dbf','dec','def']? ['abc','abd','abe','abf',...]? As in, all lists where the nth element is the nth element of one of the input lists, or all lists where the nth element lies between the nth element of the first list and the nth element of the second list? Both are instances of a more general pattern. First we have a function that, given the nth element of the first list and the nth element of the second list, produces a list of possible elements in the result: oneOf :: a -&gt; a -&gt; [a] oneOf a b = [a,b] between :: Enum a =&gt; a -&gt; a -&gt; [a] between a b = [a..b] -- between = enumFromTo Now, to combine two lists in a pairwise way, there's the `Prelude` function `zipWith`: zipWith f [a,b,c] [x,y,z] = [f a x, f b y, f c z] In this case `zipWith oneOf list1 list2` and `zipWith between list1 list2` produce a list where each element is a list of all possibilities for that position in the final list. Now we need to combine these lists of lists of possibilities into a list of possible lists (confused yet?). For that, we can use list comprehensions: combine :: [[a]] -&gt; [[a]] combine [] = [[]] combine (firsts:nexts) = [first:next | first &lt;- firsts, next &lt;- combine nexts] The end result is then `combine (zipWith oneOf list1 list2)` or `combine (zipWith between list1 list2)`. The interesting thing is that `combine` already exists in the prelude, where it is known as `sequence` (as described by /u/quchen ). And the combination of `sequence` and `zipWith` exists in `Control.Monad` as `zipWithM`. Open GHCi, and `:m + Control.Monad`, then: &gt; let oneOf a b = [a,b] &gt; let between a b = [a..b] &gt; zipWithM oneOf "abc" "def" ["abc","abf","aec","aef","dbc","dbf","dec","def"] &gt; zipWithM between "abc" "def" ["abc","abd","abe",...,"ded","dee","def"] (the cut in output by me, GHCi will display all 64 elements)
I can't believe this is being upvoted. The OP's suggestion is like a million times simpler and requires no 'plumbing' code since it's using the natural subtyping among typeclass constraints. Data Types a la Carte requires explicit plumbing, introduces a layer of interpretation overhead, and is generally ugly and cumbersome, which is why no one actually uses it much in practice.
What a pity it’s not two weeks later, then I could also attend. The program looks quite interesting!
In general, I think the reason people don't bother with a more fine-grained `IO` type is it's just not that useful (and may even be a hinderance) when the vast majority of your actual logic resides in pure code, and `IO` is just being used as a 'compilation target' for more specialized DSLs. If you're writing a lot of code directly in `IO`, then you start wanting more fine grained effect tracking there, but functional programmers don't usually do that. It's kind of like having typed assembly. If you're writing a lot of code in assembly, having types there is probably helpful. If assembly is your compilation target and you're writing in a higher-level language, typed assembly might not be so helpful or might actively get in the way.
Thank you sir. Exactly what I was looking for.
type Row = (Int,Int,Int) type Board = (Row,Row,Row)
Alright! Haskell jobs in... London... darn.
Yep that's the plan. I've heard if you say his name three times he drops whatever category theory he is doing and comes to your aid. replicateM_ 3 $ putStrLn "/u/edwardkmett"
This does not directly answers your question, but there is the ghc-core package that lets you print core in a much more readable form.
Linkdump incoming: http://www.haskellforall.com/2012/10/hello-core.html http://stackoverflow.com/questions/6121146/reading-ghc-core http://www.haskell.org/haskellwiki/Performance/GHC#Looking_at_the_Core http://book.realworldhaskell.org/read/profiling-and-optimization.html Haskell-mode in emacs can also dump tidy core for you, so you can get lots of practice. :D
meh, bugs. What Jan didn't mention is that the new Cmm statically guarantees that each block begins with a label and ends with a control transfer; GADTs are used to ensure that this invariant can't be broken. Whether this is a good idea or not is debatable though (IMO). What would improve quality and hackability more than anything else is a comprehensive test suite.
Have some sympathy, have some taste. Even Satan values program correctness and run-time safety. 
If you can find a way to introduce cokleisli arrows into that code (without using `Identity`) then you are a better ~~obfuscater~~ programmer than I.
/u/edwardkmett /u/edwardkmett /u/edwardkmett
With a little bit of adjustment for symmetry what you'd've captured would be a Relation, and the ability to run it forward or backwards. Can you put an issue on the `lens` repository? That will let other devs there toss around the idea and keep the discussion open long after this reddit thread dies. The main awkwardness of this is that like you mentioned, it doesn't fit with the other operations we can run on the `lens` types.
See this post: http://justtesting.org/post/64947952690/the-glasgow-haskell-compiler-ghc-on-os-x-10-9
No, it uses typeclasses which are always optimized away at compile time.
This is not true. Type classes are *usually* optimized away, but AFAIK GHC doesn't promise to always do it. There are even some cases (not here) where it can't: data Foo = forall a. Bar a =&gt; Foo a The `Bar` dictionary *must* have a runtime representation for every value of type `Foo`, since the compiler can't statically tell what dictionary to use at call sites.
I don't know about reactive-banana, but you can do FRP without arrows. The Signal/Behavior/Event/Wire datatype just carries the values, no need for inputs or outputs, those are provided by lifting functions and using plain composition instead of &gt;&gt;&gt;
Nothing in NYC?
Well, if all packages follow the PVP, the builds would never fail (that's the whole point of the PVP), unless you throw an additional unsatisfiable constraint into the solver. Or put differently, if a package which followed the PVP worked (and all its dependencies are PVP-adherent and working) at the time it was uploaded to Hackage, then no future uploads of any of its transitive build-dependency packages (assuming all continue to adhere to the PVP) to Hackage, will ever break the build of the said package. That's a really useful property of the PVP.
I have started a series about this on my blog [here](http://alpmestan.com/), currently 2 items about Core, more to come.
I think that would be really useful for judging the popularity of certain versions and the practical upgrade rates. I'd be more than happy to submit build information from my personal builds and, maybe after consideration, think about the work builds as well so far as they're using Hackage libraries. I don't think it's a solution for cabal hell, though.
I think this is a provocative post and I also think I disagree with the entire approach, though not necessarily any particular conclusion. Let's just pick one example! "Constructive logic is the natural vocabulary of computer science. You obtain it by ripping double-negation out of classical logic and watching the system hobble along." That's one way to think of it. Another way to think of it is you obtain it by beginning with things you can construct and only introducing new things you construct. You're not paring down your system, or making a sculpture by "chipping away the rock that doesn't look like David", but just building up things in an internally consistent fashion. As a whole I think the tension between paring down and generalizing is a bit "pared down" in this presentation, because generalizing is often the art of only looking at essentials, or weakening. We want a full range of abstractions at our disposal, and we should just pick the one that matches the semantics we want, pretty much always. I don't see any dichotomy or tension or anything here. The tension just comes because we can only have one "true" hierarchy in base for our core classes, and so folks tend to get strong opinions over just how granular it should be. So in practice, yes, semigroups are pretty common creatures. On the other hand, I think the legit uses for Apply and Bind are terribly rare, and for lengthy reasons I don't find the Map case that compelling. I don't see this as an argument for a "middle ground" because I don't think there's a common debate to be had between these things. Effects, for example, seem dragged in here and I think the debate has been procrustized in order to fit the narrow schema presented. The real story comes from proceeding from the _specifics_ of what typeclass hierarchy we'll find useful and what abstractions we'll find powerful, and then from _that_ finding the "general" principles that seem to apply across our experience. And neither of the poles presented here seem to capture _that_ idea of generalization, which I consider to be the "correct" one, in any proper way. (In fact, we can note that generalization and specification form an adjunction, and argue that progress comes not from proceeding in either direction alone, but shuttling back and forth, just as computation proceeds).
I was going to go through and try to tackle it from the perspective of forgetful and free structures, but I figured I'd lose what little audience I have. The main thing I was trying to address was the notion of right-sizing abstractions, which is a relatively underdiscussed art. The extensible effects were precisely what brought this issue to my attention as the CPS'd Lawvere theory form of extensions is sold as an alternative to monad transformers, but can't subsume many of the use cases we already have. We have to sacrifice those to adopt that solution and this isn't an acceptable alternative to me. This is the crux of the argument. Some things just can't fit or swim around in a space that is too big. Quotienting something that is too big is a matter of taste, but killing useful examples that don't fit is more troubling, when I use those examples every day. The Map as Bind usecase may not matter to you, but it lets us work with sparse vectors in linear, so it has value to me. It however, doesn't have enough value that I'm willing to shove a more refined hierarchy down everyone's throats. Perhaps a more compelling usecase is with relevant traversals in lens HEAD, where we can use it to avoid forcing you to crash or deal with a nonsense corner case when you take the maximum of a non-empty set of things. 
The sparse array/vector story I don't think works out exactly right, but it would probably be something we'd have to go over in more detail. I don't doubt you _can_ use Apply for it, but I also think you don't "want" the Maybe junk in there, and there are other ways to deal with the semantics that don't introduce that noise. Regarding effects, I don't think they're "there yet" and even when they are they may not be right. But I disagree over whether 'size' is the issue, since maybe they're just 'better' for the cases they _do_ cover. (Or maybe they're not, or maybe they are one out of five times -- isn't the "one size fits all" case the one trying to have "one" model for all 'effects'? Wouldn't a true anti-procrustian argue that some things are suited to transformers and some things are suited to other approaches and the fact that effects a la oleg only do "some" things shouldn't be a strike against them at all? :-P) As I said I think there are good points in the post, but its reaching for some ambitious stuff, and it covers enough ground that there's plenty to take issue with. (But provocation can be an asset too).
I'm perfectly okay with offering extensible effects as something that should exist. I wrote `monad-ran` 5 years ago for that very reason. In the end I found I couldn't encode many things I was already using, but when I write a monad by hand I usually CPS it in that fashion whenever I don't need the properties it can't handle. I don't have a problem with it existing. I have a problem with it being sold as a _replacement_ for monad transformers. It is a reasonable abstraction you should be aware of and use when appropriate. I have 'taste' issues with the choice of `Codensity`, but even powered up by enabling you to use `Ran`, I use monads for substitution too much to be sold that I should switch to using something where using it that way is bad. This is a 'taste' argument, though, so I focused merely on the size argument since it is actively being sold as a full on replacement. We don't sell `Applicative` as a replacement for `Monad`, but as a special case you can take advantage of. I fully expected that this post was going to get a rise out of some folks. =) Personally, I find centipede/reverse mathematics to be a good way to think. It gives me insight into precisely the properties that let me decide other properties. I spend a lot of time obsessed with non-associative right seminearrings for parsing, which is a reverse mathematics endeavor if ever their was one. The 'can't we all get along' tone I tried to strike in the end and the overly specific examples ensure that in the end I'll probably get heat from everyone. I'm not about to start advocating for a fully refined class hierarchy like [dibblego](/u/dibblego) or against the existence of `Semigroup` like [Tekmo](/u/Tekmo), but I wanted to make sure that folks couldn't say I was arguing against a non-existent straw man. That said, I don't think too many people take the terms 'generalized abstract nonsense' or 'centipede mathematics' all that seriously these days. It is just a few people who didn't get the memo that the former was a term of endearment.
As good a place as many to mention a favorite quote: "Nonsense, when all is said and done, is still nonsense. But the study of nonsense, now _that_ is science!"
As uttered canonically by Burton Dreben: "Philosophy is garbage, but the history of garbage is scholarship." (Sometimes also just "Garbage is garbage, but ...".)
&gt; char board[3][3] is too complicated for Haskell's type system. Hardly. I have an unpublished linear algebra library that captures exactly the 3-by-3 restriction (and can even remove bounds checks because of the type). I'm not special, zvxr apparently has a similar unpublished library. And, as spaceloop mentions, it's actually C that can't handle this type; the `char[3][3]` "type" is really just a `char*`. It's not very hard to do this sort of thing. The big issue is reducing boilerplate from needing to define ad-hoc index types. Of course, the part of my code that deals with that part has been published: [data-fin](http://hackage.haskell.org/package/data-fin). *Edit:* incidentally, `data-fin` uses an efficient implementation of type-level naturals. Using unary encoding a la Peano is going to make your compile times horrific. Even using binary numbers is going to make your type-checking stack overflow if you're doing anything interesting. Which is why `data-fin` uses decimal numbers: easier to read, and doesn't need a huge stack to compute with.
Use [data-fin](http://hackage.haskell.org/package/data-fin): import Data.Number.Fin import Data.Number.Fin.TyDecimal type Index = Fin (D1:.D0:.D0:.D0) type Grid a = Matrix Index Index a Btw, when `x :: Index` we'll have that `x` is represented as an `Integer` but with automatic bounds checking. For small numbers like these, you may prefer to use the `Fin` from `Data.Number.Fin.Int16` or whatever.
There's a few libs in the open Haskell ecosystem that recurrently give folks cabal hell like no other. My solution is to use and support the alternatives that aren't victims of cabal hell so often. With cabal 1.16 and 1.18, it's actually really hard to get cabal hell, even more so when you're using sandboxing! That said, there is definitely need for better build plan solving and error reporting, especially for large code bases that use many many packages. Though that's a different cabal hell than the standard one I think. 
There is always a shit-ton of shit in NYC
Whereupon I learn, on the subject of the actual value of category theory ... nothing.
Right, I get downvoted because the article doesn't address its purported purpose. Good to know who I'm dealing with.
I would presume you were downvoted because there was no actual category theory involved in the post. It doesn't address that purpose as it is largely unrelated. The use of generalized abstract nonsense was to showcase that even within the mathematics community there are divisions (of admittedly rather varying strength) based on the perceived relative utility of ideas to the practitioner of a domain.
Well it is claimed he is immortal he should place a high value on code maintainability.
hehe, thanks. :) 
Actually our analysis shows that pricing policy in Westminster appears to be random with high tariff lots right next to low tariff lots. See here, the darker the spot the higher the tariff: http://ianozsvald.com/wp-content/uploads/2013/10/whitehall_1to36_capacity_1341parkingbays_bytarrif.png. Sadly I didn't have time to reproduce this in Haskell.
Constructive contents of classical logic was exhibited by Tim Griffin 24 years ago. 
That is actually primarily for /u/bos :-)
It seems to me like you might have two Haskell SDL packages installed at the same time, and that Graphics.UI.SDL imports one of them and Graphics.UI.SDL.Image depends on the other. I'm not fully sure about that though, it's just a hypothesis.
Everybody should have understanding that you can't do things quicker than physically possible, the more so when you're using your spare time for a free convenience service. Also, I like Thursdays.
It sounds very likely to be so, SDL-image doesnt specify any version of SDL it depends on so it's hard to verify which it uses. Iirc ghc-pkg(1) can be used to check these kinds of problems and then you just pass some argument specifying which versions to usr to GHC. I'll fiddle around with that and hoping that I can figure it out.
Is Barclays still a Windows shop?
`liftM2` is pretty much what `liftA2` was before we realised `Applicative` existed. Its existence is almost entirely redundant nowadays.
Typo on page 39. "know" should be "now". Is there an accompanying paper?
If you support that I'd begin to expect that `(`plus`)` would work. 
Indeed, that is why I said that my example was rejected "as well".
Then I don't understand. (`plus`) has only a simple identifier within the backticks, but you seem to be implying that supporting this would require supporting arbitrary expressions within backticks rather than only simple identifiers?
No paper, but there is code: https://github.com/ekmett/structures
On the other hand, (+) has only symbols within parentheses. Accepting the proposed syntax would require supporting simple identifiers between backticks rather than only operator symbols.
That's an entirely different thing. plus -- variable plus as function `plus` -- variable plus as operator + -- variable + as operator (+) -- non-section of variable + as operator
This isn't working for me. I uninstall-hs'd my haskell stuff. deleted ~/Library/Haskell reinstalled haskell platform modified settings using the pre-built binary cabal update cabal install cabal-install And that last step failed with: Downloading cabal-install-1.18.0.2... Configuring cabal-install-1.18.0.2... /var/folders/8n/kc_zqmd56lbffb5ly8hrb58c0000gn/T/41999.c:1:12: warning: control reaches end of non-void function [-Wreturn-type] int foo() {} ^ 1 warning generated. Building cabal-install-1.18.0.2... Preprocessing executable 'cabal' for cabal-install-1.18.0.2... &lt;command line&gt;: cannot satisfy -package-id HTTP-4000.2.8-cdf033f9d7051824f52cd5101df67509 (use -v for more information) Failed to install cabal-install-1.18.0.2 Error while Error while updating world-file. : /Users/dave/Library/Haskell/logs/: openNewBinaryFile: does not exist (No such file or directory) cabal: Error: some packages failed to install: cabal-install-1.18.0.2 failed during the building phase. The exception was: ExitFailure 1 
I believe the above should be simply: plus -- prefix function syntax `plus` -- infix operator syntax + -- infix operator syntax (+) -- prefix function syntax Prefix-infix conversion and sections are separate things. In fact, they are specified in different sections of the Haskell Report ([§3.2](http://www.haskell.org/onlinereport/haskell2010/haskellch3.html#x8-240003.2) and [§3.5](http://www.haskell.org/onlinereport/haskell2010/haskellch3.html#x8-300003.5) respectively). 
Ah it's been a while. uninstall-hs needs --remove :-) Now it's working (so far). Hoping to be able to build Elm today!
There is also an interesting question by Guy Steele at 1:09:15 about composing iterators.
Although the example you gave seems frivolous, here is a slightly modified version that I've wanted to use more than once.... Given a three parameter function "f::A-&gt;B-&gt;C-&gt;Result", curry this once and treat the result as an operator.... IE, since (f a)::B-&gt;C-&gt;Result you should be able to evaluate b `f a` c This doesn't actually work, but it could be nice (or perhaps messy)
Now, I'm no expert on iterators, but perhaps I am an expert on traversals, and if we accept the thesis that traversals are the essence of iterators, then we know how to compose traversals. We compose them with compose. Granted the thesis that traversals are the essence of iterators was only made by Gibbons and Oliveira in 2009 and even then there were questions that were unanswered (but have since been answered).
This I have frequently wanted in practice! I usually just where-bind an infix operator to the function though and that's not a bad solution. Edit: It's worth noting, that while this is occasionally very convenient feeling, I'm a bit afraid of parsing code like it `build x y z` (`sFn m q` b y z) Apostrophes don't direct you toward their partner like parentheses do. Although, to be honest, that example isn't as hideous as I was expecting.
&gt; Arrows are easier to implement What? Given every Arrow defines an Applicative (and are then strictly more powerful than Applicatives), I fail to see how Arrows would be easier to implement...
What is sealing and CBT(?) that he mentions?
&gt; It nicely captures a notion of a context-free expression I don't understand what you mean by "context-free". I certainly doesn't imply that the order of actions shouldn't matter, given that f &lt;$&gt; a &lt;*&gt; b and flip f &lt;$&gt; b &lt;*&gt; a are not equivalent (they may in the case of Applicative-based FRP, but certainly not for Parsec, for instance). What am I missing?
That is commutative, not context free. Here what I mean is that the choice of what operation to do next after 'a' s still fixed. The shape of the computation tree is immutable. This makes it easier to introspect upon. You can inspect the whole tree of (&lt;*&gt;)'s and know that some operation isn't being performed in it, so you can optimize differently. In a monad you have to worry about any function being bound at any time. You can't see inside them, so you can't know all the `m a` expressions you'll encounter. You have to roll with the punches as you get them. It requires online rather than offline analysis.
So if there were such a thing as free Applicatives (and maybe there is), you'd be able to inspect the whole AST before running it, instead of having to do it online (like with free Monads). That's an advantage of Applicatives I hadn't thought about.
cvt* which Liskov mentions in the talk, and apparently stands for "convert". I don't understand this sealing stuff tho, so I'd like to hear the answer to this as well.
http://hackage.haskell.org/package/free-4.1/docs/Control-Applicative-Free.html You can also use `data-reify` to capture all of the sharing involved, so long as the user doesn't build infinite trees.
The main problem is sharing. A basic type for FRP can be implemented in two ways: Arrow: data SignalFunction a b = SF (a -&gt; (b, SignalFunction a b)) Applicative: type Signal a = SignalFunction () a But the second formulation has the problem that duplicating a signal will now duplicate the whole computation. Compare the expression twice' :: SignalFunction Int Int twice' = SF (\x -&gt; (2*x, twice)) to the expression twice :: Signal Int -&gt; Signal Int twice x = (+) &lt;$&gt; x &lt;*&gt; x The second one will compute the "current" value in `x` twice, whereas the first one won't. 
I started the Twitter account as a way for people to get event announcements and news if Google+ isn't your thing. This coincides with the http://ChicagoHaskell.com website we already have going. The group already has almost 40 members.
Shame for the audio and for the angle of the camera :(
As regards the audio, not being a native speaker/listener, when it's so low it's really hard to follow along, especially when the speaker is Ed doing his Haskell wizardry :) I could follow along with slides, but I usually download the talks on my iPad so that I can watch them before going to sleep.
The only context I've heard sealing in is the prevention of redefinitions of functions in an inheritance context. For example, there might be a class class Foo function bar and a class that overrides/redefines/overloads (I shall use "override" for this case from now on) `bar`: class Alice inherits Foo function bar But it may be the case that we don't want `bar` overridden; then we may want to seal `bar`: class Foo sealed function bar Which now prevents overriding it. Similarly, a whole class may be sealed (this is equivalent to Java's final). This at first is counter-intuitive and seems to violate internal models OOP programmers tend to have with which to reason about code, but imagine a class that implements secure access to something; you may want to prevent John Blackhat from inheriting from your class, and overriding your security features. Or more trivially, maybe you don't want inheritance to screw up your careful resource management. Having said that, to me sealing always smelled of "I wish to impose my will on future coders", so I'm not sure that I like it. It has some side-advantages, if you will. Sealed members or classes may simplify dynamic dispatch to a degree, and improve performance slightly (the compiler or interpreter *knows* from the class definition that any `childOfFoo.bar` will call "Foo.bar", not "Alice.bar", or "DynamicallyLoadedObject.bar"), and you can extensible "static interfaces" (in a language without sealing, you may want to emulate it with private functions and the like, and might run into problems when you resolve access issues with lots of friend declarations, because you aren't a seer who knows in advance what somebody 3 years down the line might want to do).
Based on what I understood from earlier in the talk, Liskov's language Clu had a notion of "sealing" which allowed a data type to be viewed both abstractly and concretely (as a representer, by Clu terminology). The trick was that in order to convert an abstract type to its particular concrete representation you needed a "seal" object which behaved much like a secret key used for encryption. Without the correct seal, you could not convert an abstract object to its concrete object which protected its internal structure. The implementer however would of course have the correct seal and be able to open up an abstract object in order to implement its methods. The `(cvt)` bit seemed to be a keyword which Clu used to signal that the sealing or unsealing was to happen at a particular interface.
Ah, I can understand it being a problem for a non-native speaker. :\
If I can get google to do an initial transcription, I can try to spend some time cleaning up the transcript into decent closed captions.
Just use ubuntu 13.10 It has latest haskell platform in packages. 
I shoulda started with the path of least resistant :p
This came up because I was recently using a library that relies on HsOpenSSL, and could not figure out why my program segfaulted any time I used the library, because this important information was not showing up in the types.
I think your solution is the correct way to encode this information in the types. In a sense I think it may even be the more "proper" design, but it sacrifices simplicity. It requires that the entire API is defined in terms of a custom monad transformer over `IO`. There are definitely cases when this is worthwhile, but I think it's too much just for initialization. Especially because it's a bug that should segfault your program every time you run it, so it would never slip past you.
I don't think it's too *imperative* but too *object oriented*. How about something like data Monster = RedMonster | ... data GenericMonster = ... data Entity = ... genericMonsterOfMonster :: Monster -&gt; GenericMonster entityOfGenericMonster :: GenericMonster -&gt; Entity so instead of subtyping just explicity upcast.
[Software Foundations](http://www.cis.upenn.edu/~bcpierce/sf/index.html) is also a great introduction to Coq.
Lots of cool stuff I didn't know about! One thing I didn't understand: why not use the succinct data structures to replace the sorted key arrays entirely? are the succinct operations needed for the binary search too expensive? 'cause in theory, that could save space, and space is time. Is it because keys are not assumed to be Int (though this is obviously the common case)?
I actually do have a variant of this structure that uses a wavelet tree-like structure for storing the key-value pair, which I've been working on. That is much of the design of github.com/analytics, actually, but it isn't yet speed competitive with this version and I was originally trying to aim for a 45 minute talk, and ran over by 2x. If I started from wavelet trees, and started integrating that it'd have been a 3 hour talk. =) It could save space, but it'd also worsen search performance. Typically we pay log n cache misses for a given buffer. Here we'd pay log key-size, which should always dominate the log n, forcing it to be asymptotically slower, even in the best case.
Did you also remember to remove ~/.ghc? I didn't mention it, but I did do that at least once in my various reinstalls. 
You're definitely thinking about this the wrong way around. Monsters that are blue seem to me to be _values_ of type Monster, not _types_. Consider just having: data Monster = Monster { color :: Color, health :: Health, render :: Char} // other properties as needed redDog = Monster Red 100 'd' 
I didn't end up needing to... Did my follow up post go through? I got it going. Safari 7 and Elm are a bit at odds on a few things.
Another nice book (though fast paced) is [Certified Programming with Dependent Types](http://adam.chlipala.net/cpdt/html/toc.html)
If you want to spend your time focusing on programming, you should go with a distro that will have everything working out of the box. If Ubuntu doesn't do it for you, I think Crunchbang is a good one to try. You can always move on to more flexible distros like Slackware and Arch when you want to devote your time to learn Linux. Learning bash/zsh along with the GNU tools can be really helpful when programming. Mainly for testing, prototyping and debugging. There are just so many possibilities. Recently, I used graphviz to debug my graph data structures. Learning a good editor like Vim or Emacs can also make your programming much more effective. Good luck and have fun.
They really need to change the languages name. I like Coq, but I can't say that out loud without people thinking I'm saying something completely different.
awesome its great to see so many haskell jobs popping up
FYI I updated the Wikipedia example for the password verifier that wouldn't type check due to new Strings
So kind of like typeclasses except less code? You check in with either of those three functions depending on which properties you want to have. I like it!
It's not really like type classes. In order for different kinds of entities to support different sets of operations in type classes, you would need them all to be different types. In this scheme, you have something more like duck typing (for better or for worse), and easy access to a collection of "all entities supporting operation `x`".
The [ncurses](http://hackage.haskell.org/package/ncurses) package does exactly what you're proposing. I wish more packages used it. It might add a little type complexity, but it's not *that* bad to have your types be (MonadIO m) =&gt; a -&gt; m b rather than a -&gt; IO b, and it is a strength of Haskell's type system that it lets you enforce initialization like that.
Oh, I see now. Thanks for the clarification!
In addition to the entity scheme I suggested elsewhere in this thread, I also like and in the past have suggested this style, although I would make an amendment: You don't have to actually include a polymorphic field at all. You could just tuple the fields you care about together. That is, instead of: data Monster a = Monster { health :: Int, color :: Color, other :: a } simpleMonster :: Monster () monsterWithInventory :: Monster Inventory You might as well just have: data Monster = Monster { health :: Int, color :: Color } simpleMonster :: Monster monsterWithInventory :: (Monster, Inventory)
It took me a while to get my environment up-to-date, but I finally managed to play. It is hilariously difficult to try to play against myself, so I'll have to challenge someone when I get the chance! Cool combo of mechanics.
Whats the Haskell analog to all of this that Wadler mentioned? I think he said something about Haskell using something similar to cvt right now.
I'm not sure that the French care that much about how the language name sounds in English :)
As a rule of thumb, you don't want to use typeclasses to do object orientation in Haskell. Instead, you represent your vtables explicitly, using first class functions or records of functions. https://lukepalmer.wordpress.com/2010/01/24/haskell-antipattern-existential-typeclass/ In geezusfreak's example this analogy is a bit hard to see because he is only representing getters but if you wanted your enteties to have methods that receive arguments then the maps would need to contain functions: fooableEntities :: Map Entity (Int -&gt; Int)
General rule-of-thumb for functional programming vs. OO programming, FP relies on HAS\_A, OO relies on IS\_A. In your case, a Monster "IS\_A" Entity, and in an OO world, this would probably be solved by derivation. In Haskell, a Monster _has entity behavior_. This turns out to be a clever thing to do in both OO and FP, by saying "a Monster _has a_ Entity subsystem", then you can let Monster be a proxy for that entity subsystem. You can then use a type class to _hide_ underlying implementation _after_ the fact (which I found is a significantly better way to go about working w/ typeclasses). So I might have: data EntityBehavior = EntityB { position :: Position, ... } renderEntity :: EntityBehavior -&gt; Char data Monster = Monster { entity :: EntityBehavior, ... } renderMonster :: Monster -&gt; Char renderMonster m = renderEntity $ entity m Then, rather than having an `Entity` type class, I'd probably go with a bunch of little typeclasses class Renderable a where render :: a -&gt; Char Then if you really need to talk about Entities directly, you could do: class (Renderable a, Foo a, ...) =&gt; Entity a where -- maybe some defaulting here In general, it's better to avoid putting lots of stuff in a class definition, rather, define plain-old-haskell-functions to do that work. So I'd recommend avoiding putting too much entity specific stuff in the class if you can avoid it. 
Something something modules? I'm not really sure.
I think hugs had a notion of a type that behaved like a type alias on the inside, but like a data type everywhere else. Perhaps it was that?
I've spent some time thinking about cache-friendly data structures in functional languages. In my opinion, we'd get more mileage out of giving GHC information about the cache hierarchy and allowing it to lay out our data structures in a good way (which may involve duplicating data), than trying to write cache-oblivious functional data structures. The big problem with cache-obliviousness, is that it makes spatial locality important. This makes sense in terms of a Turing machine, with its infinite reel of tape, but not in terms of the lambda calculus.
Take a look at the FRVR project of Blom [1]. He implemented a "pendulum" which may be similar to your problem. The dissertation was online before, but it doesn't seem to be available any more. Maybe it's on archive.org. [1] http://www.haskell.org/haskellwiki/Yampa#Known_forks -&gt; FRVR
&gt; If Ubuntu doesn't do it for you, I think Crunchbang is a good one to try. That advise does not really make sense to me. CrunchBang is based on Debian, Ubuntu is also based on Debian. If enable all the Ubuntu repositories (Restricted, Universe, Multiverse), you should have exactly the same packages, if not more. Added to that, there are many PPAs (personal package archives) available for Ubuntu that package additional or newer software.
How can this work together with `forkIO`? The forked thread won't be running under `WICXT`, so won't be able to use the library. Perhaps it makes sense to restrict some libraries to being used from a single thread, but surely not all libraries. The library could provide a `forkWICX`, but if you have two libraries using this pattern, you can't use them together in the same forked thread. The library could always supply an `unsafeRunWICXT`, although that's icky. Or even a `getWICXTToken` that runs in the main thread, and a `runWICXT` that runs in the forked thread and takes the token.
Ubuntu doesn't share any of its packages with Debian AFAIK though, but rather they build all their own packages. Which will mean that you will get different versions of various software.
Je ne vois vraiment pas où est le problème.
Here's an alternative approach: {-# LANGUAGE ConstraintKinds #-} {-# LANGUAGE RankNTypes #-} {-# LANGUAGE ImplicitParams #-} -- Do not export this type from your library. It can contain -- state, or contain nothing. data S = S -- This is a constraint that you can use on all your functions -- that require initialization. It makes it a lot nicer to write -- and read the type signatures. type Snarf = (?withNewSnarf :: S) -- We call it `withNewSnarf` to -- self-document a little bit. -- This is the top-level user function which does initialization -- and cleanup necessary and provides the constraint. withNewSnarf :: (Snarf =&gt; IO a) -&gt; IO a withNewSnarf m = do -- ... do some initialization here ... let ?withNewSnarf = S in m -- You use this for any function in your library that requires initialization. withSnarf :: Snarf =&gt; IO a -&gt; IO a withSnarf = id Then you can define your library of functions: snarfoils :: Snarf =&gt; IO () snarfoils = do snarfism (snarferize "Snarfward"); snarferize "Snarfic" snarfism :: Snarf =&gt; IO () -&gt; IO a snarfism m = withSnarf $ do putStrLn "Snarbles: "; m; snarfism m snarferize :: Snarf =&gt; String -&gt; IO () snarferize x = withSnarf $ putStrLn $ "Snarfles: " ++ x Notice that the nesting doesn't matter, e.g. in `snarfoils`, as long as the Snarf constraint is present. Only the top-level `withNewSnarf` is required. Then you can write: main :: IO () main = do putStrLn "Yo!" withNewSnarf $ do snarfoils snarfism $ do -- Forking works. forkIO (do putStrLn "Hey!" -- Notice I can use this freely -- without any transformation or -- liftIO. snarfoils) return () If the `withNewSnarf` is omitted, this won't compile. Some problems to consider (in this approach, and in all approaches): * What happens if I call `withNewSnarf` twice? Should that be disabled? How? * Should forking be allowed, or controlled? * What are the performance differences of Reader/transformer/MonadFoo/ImplicitParams?
It requires the ones I listed: * ConstraintKinds — for the `Snarf` type. * RankNTypes — for `withNewSnarf`'s type. * ImplicitParams — for the implicits. 
Have my downvote. I don't think these naming discussion / joke sessions are interesting. I understand that it is probably a comment you're making in good-faith, but having this discussion again and again is not really respectful of the tool scope and interest.
We could just agree to call it something else in English. Sorta like Honda Fitta was rebranded Honda Jazz in Scandiavia because nobody wanted to drive around in Honda The Cunt (and to make it better, Honda already sounds like "the hand").
How's your network? "premature end of compressed stream" sounds to me like there's something missing from the file.
Yes. 
You only need ConstraintKinds because of the type synonym, right?
Ah, the "reddit argument" didn't occur to me. Tuesday as a compromise perhaps?
Yep.
Does C-- provide advantages over LLVM IR? Are they complementary or different solutions to the same problem?
try the verbose flag for cabal -v and see if theres any missing dependencies
This is an old webpage for a project that has been dormant for a long time. In the beginning of the 2000, there was an agreement between several compiler writers of the functional programming community to experiment with a common low-level backend that would be more suited that C as a target for languages with a GC, etc. I don't think the project ever succeeded in gaining any traction, but you can find bits and pieces that can be linked to that past idea. This community knows about C-- as one of the intermediate languages of the GHC compiler, but the OCaml compiler also has an intermediate language named Cmm roughly at the same level. [Norman Ramsey](http://www.cs.tufts.edu/~nr/#research) did compiler research around this cminusminus idea until fairly recently (notably with his student João Dias); some of this research ended up in GHC as the optimizer framework Hoopl. Xavier Leroy used "one instance of the C-- idea" again when designing the verified [Compcert](http://compcert.inria.fr/) compiler for C (here called Cminor), and this is becoming a new point of collaboration for people working on verified program analysis (eg. Andrew Appel's work on static analysis sitting on top of Cminor). (Similarly in the beginning of the 90s you could find discussion about designers of proof assistants to agree on a common logic to favor interoperability between type-theory-based assistants. It's easy to be enthusiastic about this kind of things, very hard to collaborate on a day-to-day basis when you have different priorities, etc.) PS: more discussion [on Lambda-the-Ultimate](http://lambda-the-ultimate.org/node/4378#comment-67781)
I liked it much more than SF since I like Chlipala's notion of throwing out the big hammers immediately and only peering into the mud over time. It's nice to take the leisurely SF method if you want to learn how Coq works, but it's nice to just sledgehammer (thank you Isabelle) everything if you want to prove programs.
I wonder, too. I guess C-- is older?
Don't listen to them :). Arch is fine, and you would get the same error installing snap with haskell-platform on any other distro. It's network related, your network has to be reliable. Keep retrying the install until it works.
Now I do, I only looked at the inbox and I guess I missed it. Glad things are working for you now. Why does my post have zero points? :-( 
This sort of comment is common. Things that could help: Sharing configurations (NixOS ,Puppet, Chef, vagrant, etc) Ghc-pkg listings known to compile well. Using more sandboxes.
Having read both, I regularly consult CPDT when writing Coq but only rarely check SF. A lot of the tricks in this book (and the attached code) are actually tremendously useful. Plus fully automated proofs rock.
Norman Ramsey was very actively pushing the more general purpose use of C-- while he was over at Harvard. When they (bizarrely!) ultimately denied him tenure, it seems his attention wandered. I'm pretty sure the project died a bit before he left Harvard to go to Tufts University. It does seem to be something that has caught on more in the abstract than in the particular, though, as you mentioned.
My general point about cache-obliviousness is that tuning for all the levels of the cache is -- in the limit -- a losing game. Someone always adds one and doesn't tell you about it.
SF makes me want to write a theorem prover. CPDT makes me want to prove theorems.
The main downside to the name is that every single English discussion you have about it ends up discussing the name more than the tool.
I think SF is not really about Coq, but about foundations in general. Very useful book. It's on par with SICP when it comes to changing the mindset, IMO
No idea. It's plus 1 now. Also don't know why that is. I try not to sweat the reddit karma :-)
I was just going to encourage the `ImplictParams` solution here!
I think they are different solutions to the same problem. I'm not at all an expert here, but my rough understanding is: 1. C-- has better support for integrating GC. Using LLVM as a target for GC languages is quite difficult from what I understand. The C-- docs here state that GC integration is a goal. 2. LLVM has a living and vibrant community. C-- is dormant, I believe.
Could it be faulty HDD? Doing cabal get snap -&gt; ls snap... -&gt; cabal install was working but some dependencies like Either gave me an error &gt; monadRandom transformer random.hi "end of file" Forcing a reinstall of monadRandom I could install either and get snap to compile.
I doubt that, even when there is conflicting dependencies, I'm forcing them away with --force-reinstalls --reinstall (I think I can afford this evilness, since I barelly got anything installed)
Unfortunately, in a functional setting, you get much better bounds by being cache-aware. Compare the insertion complexity of a COLA vs a COW B-Tree, for example, O(log(n)) vs O(log(n)/log(B))
&gt; Or even a getWICXTToken that runs in the main thread, and a runWICXT that runs in the forked thread and takes the token. This suggestion gave me another idea: a token could be used as in chrisdoner's method, but hidden in a normal `ReaderT` for most uses, and extracted when needed for thread-boundary cases, etc.
Define one of your snarf functions using this trick: {-# NOINLINE snarfFoo #-} snarfFoo :: String -&gt; Bool -&gt; IO () snarfFoo = unsafePerformIO $ do p &lt;- initializeSnarf return $ \a b -&gt; do snarf a snurf b The rest of the snarfing functions can be defined as usual. Alternatively if you do not mind having some dummy global variable you can do library initialization like this: {-# NOINLINE dummy #-} dummy :: IORef Int dummy = unsafePerformIO $ do initializeSnarf newIORef 42 This leads to automatic initialization of your foreign library on first module load. SO no need to explicitly initialize it or taint your types. And yes, you will get a lot of flack from haskellers for using unsafePerformIO. But i use it, and it works for me and lets me hide implementation details and simplify my code. 
Harvard denies almost everyone tenure--even amazing people like Norman Ramsey. It is far more common for Harvard to hire someone with tenure (often people in tenured positions at the other ivy league schools), than to promote to tenured rank. There are departments at Harvard that have not promoted within for decades--and probably don't know how to anymore. The CS department is comparatively less bad, but it is still a challenge. Part of the reason for this is structural. The only tenured rank at Harvard is Full Professor, and the process to promote to full is exceedingly difficult. Among other requirements: you must have a degree from Harvard to be a full professor at Harvard, meaning if you are someone like Ramsey who went to Princeton and Cornell, your department must first put you in for an honorary degree. Once that is done with, and your department is convinced, your department has to argue your case in front of the president of the university. Most people suspect that various other rituals are involved--keep in mind that the "President and Fellows of Harvard College" is the oldest corporation in the americas, and that its by laws were written into the constitution of the commonwealth of Massachusetts. Arcane process at Harvard is just a fact of life. It isn't fair, but at least it helps other schools get good people.
Sure, you can come up with lambda calculi for classical logic. And sure, being calculi with explicit proof trees they are therefore constructive. But this is irrelevant. There is a yawning chasm between the way constructive mathematicians look at the world and the way classical mathematicians do. The entire enterprise of creating classical lambda caulculi is of interest to constructivists, but of no interest to any of the classical mathematicians I know. I use the terminology of "constructive" and "classical" to refer to these different communities of practice. And they are exactly that: **communities of practice**. You mislike my terminology because you are too focused on the object of study. But the object does not define the practice. This is why we have anthropology on the one hand and sociology on the other, psychology on the one hand and cognitive science on the other, computational linguistics on the one hand and natural language processing on the other, informatics on the one hand and systems theory on the other, mathematics on the one hand and logic on the other and computer science on the other. When you give me better terms than "constructive" and "classical", and when these terms will be understood by my colleagues engaging in the practices of constructive and classical mathematics, then I will change my terminology. Until then, I humbly request you to stop bringing up classical lambda calculi every time I mention the fact that there are these two different communities of practice.
Lol. Wrong link. http://compgeom.cs.uiuc.edu/~jeffe/teaching/datastructures/notes/01-statictodynamic.pdf
The amortized insertion complexity here is `O((logN)/B)` if you assume ephemeral access and pack the first `log n` levels in a block. The [Stratified B-Tree](http://arxiv.org/abs/1103.4282), which improves on the design of the CoW B-Tree to match this bound and doesn't set about to be cache-aware, merely doesn't try to be purely functional, obtaining its persistence by means similar to the COLA. Using the [Overmars and van Leeuwen](http://compgeom.cs.uiuc.edu/~jeffe/teaching/datastructures/notes/01-statictodynamic.pdf) approach to deamortize search structures, I believe I can make this form of COLA match the insertion complexity of the original one without racing threads. 
I don't object to a distinction between "classical" and "constructive" **mathematics** at all--although I slightly prefer "non-constructive" to "classical." I'm clearly on the constructive side of any cultural divide, and there is clearly a cultural divide--about that I don't disagree. I just find distinction between classical and constructive **logic** frustrating, as we now know that classical logic can also be constructive. Classical mathematicians don't have to give up classical logic to become constructivists. And, constructivists don't need to be so dogmatically intuitionist. I don't object to comparing "intuitionistic" and "classical" logics. It is just that conflating intuitionistic with constructive annoys me. Not all approaches to classical logic are constructive--but intuitionistic is sometimes presented in a non constructive way also. And there are other logics with constructive interpretations: linear logic admits the law of excluded middle, but does not have all the theorems of even minimal structural logic. The problem is conflating these two dimensions. I should not have singled you out. Edward's original post did the same thing.
&gt; Each English discussion with childish people, yes. &gt; That doesn't make them bad people (you can find cock jokes amusing and be a nice person), but that's still the opposite of acting like a grown-up. Children actually don't habitually engage in technical conversations with cock jokes, so labeling this as "childish" is both inaccurate and offensive stereotyping. Some adults do it, other adults don't, some children make these jokes, other children don't. It is beyond me why some people insist on labeling and controlling these things anyway. &gt; PS: and that "Hoare" joke was absolutely terrible and inappropriate for a safe technical communication place. "Safe technical communications place"? What does that even mean?
Yeah, that was the joke.
I've been reading your articles, and I really like your style.
you can find the old page with more info at www.cs.tufts.edu/~nr/c--/ norman ramsy is kindly hosting a backup of the old site!
Thanks guys for the kind words. Let me know if you think about some topic you'd like to see covered. I have some ideas already, and am working on a third post for the series, but I don't know what people would like to learn about first.
I think what you are seeing is just the way GHC deals with names. GHC doesn't keep our variable names and instead uses a "unique name" generator for every variable (and can easily deal with symbol resolution that way I suppose?). Also, it creates separate names for the integers in the list for sharing, so that each time you'll refer to the Int 5, it'll point to the same value on the heap, instead of creating a new boxed 5.
Can I ask why you care? Latex will ignore those newlines won't it? If you still need to change the functionality, all I can suggest is doing a "cabal unpack" and digging into the pandoc source. If I recall, it is quite pleasant!
Looking at the source code, `pre` is always parsed as a code-block [\[1\]](https://github.com/jgm/pandoc/blob/master/src/Text/Pandoc/Readers/HTML.hs#L285) -- and it makes sense since that's the purpose of the tag. `code` is parsed as inline code [\[2\]](https://github.com/jgm/pandoc/blob/master/src/Text/Pandoc/Readers/HTML.hs#L396). So you want to change [..] equation &lt;pre class="rawtex"&gt;$x^2 + y^2 = z^2$&lt;/pre&gt;, and also [..] To [..] equation &lt;code class="rawtex"&gt;$x^2 + y^2 = z^2$&lt;/code&gt;, and also [..] Your filter will need to process inline code. Looking at the [definition of `Block`](http://hackage.haskell.org/package/pandoc-types-1.12.3/docs/Text-Pandoc-Definition.html#t:Block), a number of types have `Inline`s that will need to be traversed. Fortunately, pandoc provides [Text.Pandoc.Walk](http://hackage.haskell.org/package/pandoc-types-1.12.3/docs/Text-Pandoc-Walk.html) to traverse the tree for you. Use, walk :: (a -&gt; a) -&gt; b -&gt; b Or in this case, this specific instance: walk :: (Inline -&gt; Inline) -&gt; Block -&gt; Block So, `rawtex` becomes rawtex :: Block -&gt; Block rawtex (CodeBlock (_,["rawtex"],_) code) = RawBlock (Format "latex") code rawtex x = walk rawtexInline x rawtexInline :: Inline -&gt; Inline rawtexInline (Code (_,["rawtex"],_) code) = RawInline (Format "latex") code rawtexInline x = x
First, change your source document so that the math bits you want to treat as inline have `&lt;code class="rawtex"&gt;...&lt;/code&gt;` instead of `&lt;pre class="rawtex"&gt;...&lt;/pre&gt;`. The `&lt;code&gt;` sections will be parsed as inline code. You can then rewrite `rawtex` with type `Inline -&gt; Inline`. Instead of `CodeBlock`, you'll use `Code`, and instead of `RawBlock`, you'll use `RawInline`. 
This is almost right, except that you don't need to use `walk`. `toJSONFilter` handles all of that for you behind the scenes. So, in your sample code, you can omit `rawtex`, and the main program can be: main = toJSONFilter rawtexInline 
Ah, I see. I thought OP wanted all the math to be treated as inline, but you may be right, in which case your solution is an excellent one.
It's an en-dash. But otherwise you're correct. I feel really embarrassed for whoever put together these pages. 
Haha, that was what my comment about 'lots of punctuation' was about - I haven't grokked applicatives yet so expect another post like this when I do ;P.
if you use cabal, I think the answer might be to do cabal install sdl sdl-image together, and cabal will resolve the correct versions for you.
Or `(| readMatrix getLine |)`... Edit: Apologies. This is non-functioning theoretical Haskell available only via Template Haskell or preprocessors. I should have emphasized the REALLY drink the kool-ade bit.
 y &lt;- C.lift $ readDirectoryWith (readEntry [OptRecursive]) x Notice where apply ($) is placed. Don't ask how I've debugged it - just intuition. Though you should report the GHC bug. Panic inside typechecker is bad.
I will have to study this for a while as the change is non-intuitive to me. The program is working now. Thanks so much for the help!
how can I get that syntax?
Haskell doesn't have idiom brackets...
use [SHE](https://personal.cis.strath.ac.uk/conor.mcbride/pub/she/), a quasi-quoter, or switch to [Idris](http://www.idris-lang.org/). None of which I particularly recommend if your goal is to get work done.
As a learner myself, I really appreciated this post, and would love to see more like it. Thank you!
thank you :)
Very very nice. Have you blogged on this? It's quite applicable to other languages beyond haskell. I had not seen this technique described elsewhere, the world needs to know. 
I had The same "kindFunResult" in a program of mine a while ago, but after reporting the ticket GHC hackers told me is fixed in HEAD. It will now give you a more helpful error message, revealing that the problem was causes by a misplaced lift :)
If you had an MonadIO with effects you want to allow (e.g. curses monad), but wanted to lift from your monad to it, you'd need to write liftYour and hide liftIO.
I've always though that avoiding a system package manager can be problematic further down the line. Best bet is to make sure the packages exist and are up to date. I use FreeBSD which has the ports system for installing most stuff via source including cabal-install for what's not available. I generally prefer to convert anything that missing or out of date to a new port via [hsporter](https://github.com/freebsd-haskell/hsporter) though and use my own fork of a [ports overlay](https://github.com/tim-m89/ports). I would recommend doing the equivalent if its as simple on your os. Also see this [relevant link](http://ivanmiljenovic.wordpress.com/2010/03/15/repeat-after-me-cabal-is-not-a-package-manager/). 
better yet: submit new packages to the os's package upstream (i.e. ports, debian, fedora, etc).
Albert YC Lai has written on this http://www.vex.net/~trebla/haskell/sicp.xhtml "Designate two points of time A,B such that: before A, obtain from the distro exclusively; between A and B, cabal install --global exclusively; after B, cabal install --user exclusively. Never interleave the three."
Childish may have been the wrong term. I'm not a native english speaker and I think the term I was thinking of was rather *puerile* (which I didn't know existed in english until I checked it just now). I'm trying to describe behaviors that are both immature and, yes, quite characteristic of a time of youth were being vulgar is a form of coolness. Apologies if "child" is not the right word for the time in life at which frat jokes seem fun to a large percentage of the population -- which word would you naturally use? I would fine with using "unprofessional" or "immature" instead. Things that are frowned upon in most work environments should also be avoided on Reddit (or most technical places in the internet, in fact). I didn't use "unprofessional" because I realize that to most people, reddit is not a professional communication medium. Reddit doesn't imply professional, but both those environments should be void of immaturity / vulgarity / sexual jokes.
By "safe" I only meant that content here should not be offensive (because it has no reason to; we don't need to make sexual jokes to convey interesting technical content to the Haskell community). *You* are welcome on r/haskell, but jokes about prostitution should not be.
The main thing I learned is that John Reynolds died six months ago. :-(
Well, as I'm in a cynical mood today, I'll just observe that "professional" means "getting paid for X", and I don't see how that correlates with making or not making jokes of whatever. In my country, road pavers are stereotyped as being some of the most foul-mouthed people on the face of this planes, and they're still not doing their job for free. "Immature" would be perhaps suitable if reality were different. If people making jokes (perhaps dumb jokes by someone's standards, but jokes nonetheless) are considered "immature", what about all the adult people who are displaying all sorts of behavior that are much more undesirable in comparison? (Cheating, lying, outright theft, miffiness, instability, sociopathic behavior in executives, etc., etc.) Somehow the predilection of some people (often ones to which one or more of the former qualifiers apply, actually) to become preoccupied with jokes has always seemed to me like a case of misplaced priorities, nor least least because some of them are treating innuendos like outright vulgarisms. I'd have hoped that puritanism would be dead by now since I fail to see its objective, substantial value.
Ah yes, the restricted type synonyms (see [here](http://cvs.haskell.org/Hugs/pages/hugsman/exts.html) in sectio 7.3.5). Long time that I've seen those :-)
I am unfamiliar with your ;-) notation. Which library should I include? (-; "Just kidding" ;-)
I'd use the system package manager to install ghc and the haskell platform if they are sufficiently new (7.6 or so). For the rest, use cabal install, preferably in a sandbox. If there is no new GHC/platform, just get the binary tarball from the ghc site and use that.
Haskell platform preferentially from the system package manager (if it's fresh), everything else via cabal
So do you have to tweak the GHC code to do this, or can you just build it with some flag? 